
MATHEMATICAL
METHODS FOR
PHYSICISTS
SEVENTH EDITION

MATHEMATICAL
METHODS FOR
PHYSICISTS
A Comprehensive Guide
SEVENTH EDITION
George B. Arfken
Miami University
Oxford, OH
Hans J. Weber
University of Virginia
Charlottesville, VA
Frank E. Harris
University of Utah, Salt Lake City, UT
and
University of Florida, Gainesville, FL
AMSTERDAM • BOSTON • HEIDELBERG • LONDON
NEW YORK • OXFORD • PARIS • SAN DIEGO
SAN FRANCISCO • SINGAPORE • SYDNEY • TOKYO
Academic Press is an imprint of Elsevier

Academic Press is an imprint of Elsevier
225 Wyman Street, Waltham, MA 02451, USA
The Boulevard, Langford Lane, Kidlington, Oxford, OX5 1GB, UK
© 2013 Elsevier Inc. All rights reserved.
No part of this publication may be reproduced or transmitted in any form or by any means, electronic or
mechanical, including photocopying, recording, or any information storage and retrieval system, without
permission in writing from the publisher. Details on how to seek permission and further information
about the Publisher’s permissions policies and our arrangements with organizations such as the Copyright
Clearance Center and the Copyright Licensing Agency, can be found at our website:
www.elsevier.com/permissions.
This book and the individual contributions contained in it are protected under copyright by the Publisher
(other than as may be noted herein).
Notices
Knowledge and best practice in this ﬁeld are constantly changing. As new research and experience
broaden our understanding, changes in research methods, professional practices, or medical treatment
may become necessary.
Practitioners and researchers must always rely on their own experience and knowledge in evaluating and
using any information, methods, compounds, or experiments described herein. In using such information
or methods they should be mindful of their own safety and the safety of others, including parties for
whom they have a professional responsibility.
To the fullest extent of the law, neither the Publisher nor the authors, contributors, or editors, assume any
liability for any injury and/or damage to persons or property as a matter of products liability, negligence
or otherwise, or from any use or operation of any methods, products, instructions, or ideas contained in
the material herein.
Library of Congress Cataloging-in-Publication Data
Application submitted.
British Library Cataloguing-in-Publication Data
A catalogue record for this book is available from the British Library.
ISBN: 978-0-12-384654-9
For information on all Academic Press publications,
visit our website: www.elsevierdirect.com
Typeset by: diacriTech, India
Printed in the United States of America
12 13 14
9 8 7 6 5 4 3 2 1

v 
 
CONTENTS 
 
 
 
 
 
 
PREFACE ........................................................................................................................................... XI 
1. 
MATHEMATICAL PRELIMINARIES ...................................................................................................... 1 
1.1. 
Infinite Series .................................................................................................................. 1 
1.2. 
Series of Functions ....................................................................................................... 21 
1.3. 
Binomial Theorem ........................................................................................................ 33 
1.4. 
Mathematical Induction ............................................................................................... 40 
1.5. 
Operations of Series Expansions of Functions .............................................................. 41 
1.6. 
Some Important Series ................................................................................................. 45 
1.7. 
Vectors ......................................................................................................................... 46 
1.8. 
Complex Numbers and Functions ................................................................................. 53 
1.9. 
Derivatives and Extrema .............................................................................................. 62 
1.10. 
Evaluation of Integrals ................................................................................................. 65 
1.11. 
Dirac Delta Functions ................................................................................................... 75 
Additional Readings .................................................................................................... 82 
2. 
DETERMINANTS AND MATRICES .................................................................................................... 83 
2.1 
Determinants ............................................................................................................... 83 
2.2 
Matrices ....................................................................................................................... 95 
Additional Readings .................................................................................................. 121 
3. 
VECTOR ANALYSIS .................................................................................................................... 123 
3.1 
Review of Basics Properties ........................................................................................ 124 
3.2 
Vector in 3 ‐ D Spaces ................................................................................................. 126 
3.3 
Coordinate Transformations ...................................................................................... 133 

vi 
 
3.4 
Rotations in 
3
 ........................................................................................................ 139 
3.5 
Differential Vector Operators ..................................................................................... 143 
3.6 
Differential Vector Operators: Further Properties ...................................................... 153 
3.7 
Vector Integrations .................................................................................................... 159 
3.8 
Integral Theorems ...................................................................................................... 164 
3.9 
Potential Theory ......................................................................................................... 170 
3.10 
Curvilinear Coordinates .............................................................................................. 182 
Additional Readings .................................................................................................. 203 
4. 
TENSOR AND DIFFERENTIAL FORMS .............................................................................................. 205 
4.1           Tensor Analysis .......................................................................................................... 205 
4.2 
Pseudotensors, Dual Tensors ..................................................................................... 215 
4.3 
Tensor in General Coordinates ................................................................................... 218 
4.4 
Jacobians .................................................................................................................... 227 
4.5 
Differential Forms ...................................................................................................... 232 
4.6 
Differentiating Forms ................................................................................................. 238 
4.7 
Integrating Forms ...................................................................................................... 243 
Additional Readings .................................................................................................. 249 
5. 
VECTOR SPACES ....................................................................................................................... 251 
5.1 
Vector in Function Spaces .......................................................................................... 251 
5.2          Gram ‐ Schmidt Orthogonalization ............................................................................. 269 
5.3          Operators ................................................................................................................... 275 
5.4          Self‐Adjoint Operators ................................................................................................ 283 
5.5 
Unitary Operators ...................................................................................................... 287 
5.6 
Transformations of Operators.................................................................................... 292 
5.7 
Invariants ................................................................................................................... 294 
5.8 
Summary – Vector Space Notations ........................................................................... 296 
Additional Readings .................................................................................................. 297 
6. 
EIGENVALUE PROBLEMS ............................................................................................................. 299 
6.1 
Eigenvalue Equations ................................................................................................. 299 
6.2 
Matrix Eigenvalue Problems ...................................................................................... 301 
6.3 
Hermitian Eigenvalue Problems ................................................................................. 310 
6.4 
Hermitian Matrix Diagonalization ............................................................................. 311 
6.5 
Normal Matrices ........................................................................................................ 319 
Additional Readings .................................................................................................. 328 
7. 
ORDINARY DIFFERENTIAL EQUATIONS ........................................................................................... 329 
7.1 
Introduction ............................................................................................................... 329 
7.2 
First ‐ Order Equations ............................................................................................... 331 
7.3 
ODEs with Constant Coefficients ................................................................................ 342 
7.4 
Second‐Order Linear ODEs ......................................................................................... 343 
7.5 
Series Solutions‐ Frobenius‘ Method .......................................................................... 346 
7.6 
Other Solutions .......................................................................................................... 358 

vii 
 
7.7 
Inhomogeneous Linear ODEs ..................................................................................... 375 
7.8 
Nonlinear Differential Equations ................................................................................ 377 
Additional Readings .................................................................................................. 380 
8. 
STURM – LIOUVILLE THEORY ....................................................................................................... 381 
8.1 
Introduction ............................................................................................................... 381 
8.2 
Hermitian Operators .................................................................................................. 384 
8.3 
ODE Eigenvalue Problems .......................................................................................... 389 
8.4 
Variation Methods ..................................................................................................... 395 
8.5 
Summary, Eigenvalue Problems ................................................................................. 398 
Additional Readings .................................................................................................. 399 
9. 
PARTIAL DIFFERENTIAL EQUATIONS .............................................................................................. 401 
9.1 
Introduction ............................................................................................................... 401 
9.2 
First ‐ Order Equations ............................................................................................... 403 
9.3 
Second – Order Equations .......................................................................................... 409 
9.4 
Separation of  Variables ............................................................................................. 414 
9.5 
Laplace and Poisson Equations .................................................................................. 433 
9.6 
Wave Equations ......................................................................................................... 435 
9.7 
Heat – Flow, or Diffution PDE ..................................................................................... 437 
9.8 
Summary .................................................................................................................... 444 
Additional Readings .................................................................................................. 445 
10. 
GREEN’ FUNCTIONS .................................................................................................................. 447 
10.1 
One – Dimensional  Problems .................................................................................... 448 
10.2 
Problems in Two and Three Dimensions .................................................................... 459 
Additional Readings .................................................................................................. 467 
11. 
COMPLEX VARIABLE THEORY ...................................................................................................... 469 
11.1 
Complex Variables and Functions .............................................................................. 470 
11.2 
Cauchy – Riemann Conditions .................................................................................... 471 
11.3 
Cauchy’s Integral Theorem ........................................................................................ 477 
11.4 
Cauchy’s Integral Formula ......................................................................................... 486 
11.5 
Laurent Expansion ...................................................................................................... 492 
11.6 
Singularities ............................................................................................................... 497 
11.7 
Calculus of Residues ................................................................................................... 509 
11.8 
Evaluation of Definite Integrals .................................................................................. 522 
11.9 
Evaluation of Sums ..................................................................................................... 544 
11.10     Miscellaneous Topics .................................................................................................. 547 
Additional Readings .................................................................................................. 550 
12. 
FURTHER TOPICS IN ANALYSIS ..................................................................................................... 551 
12.1 
Orthogonal Polynomials ............................................................................................. 551 
12.2 
Bernoulli Numbers ..................................................................................................... 560 
12.3 
Euler – Maclaurin Integration Formula ...................................................................... 567 
12.4 
Dirichlet Series ........................................................................................................... 571 

viii 
 
12.5 
Infinite Products ......................................................................................................... 574 
12.6 
Asymptotic Series ....................................................................................................... 577 
12.7 
Method of Steepest Descents ..................................................................................... 585 
12.8 
Dispertion Relations ................................................................................................... 591 
Additional Readings .................................................................................................. 598 
13. 
GAMMA FUNCTION ................................................................................................................... 599 
13.1 
Definitions, Properties ................................................................................................ 599 
13.2 
Digamma and Polygamma Functions ........................................................................ 610 
13.3 
The Beta Function ...................................................................................................... 617 
13.4 
Stirling’s Series ........................................................................................................... 622 
13.5 
Riemann Zeta Function .............................................................................................. 626 
13.6 
Other Ralated Function .............................................................................................. 633 
Additional Readings .................................................................................................. 641 
14. 
BESSEL FUNCTIONS ................................................................................................................... 643 
14.1 
Bessel Functions of the First kind, Jν(x) ....................................................................... 643 
14.2 
Orthogonality ............................................................................................................. 661 
14.3 
Neumann Functions, Bessel Functions of  the Second kind ........................................ 667 
14.4 
Hankel Functions ........................................................................................................ 674 
14.5 
Modified Bessel Functions,   Iν(x) and  Kν(x) ................................................................ 680 
14.6 
Asymptotic Expansions .............................................................................................. 688 
14.7 
Spherical Bessel Functions ......................................................................................... 698 
Additional Readings .................................................................................................. 713 
15. 
LEGENDRE FUNCTIONS ............................................................................................................... 715 
15.1 
Legendre Polynomials ................................................................................................ 716 
15.2 
Orthogonality ............................................................................................................. 724 
15.3 
Physical Interpretation of Generating Function ......................................................... 736 
15.4 
Associated Legendre Equation ................................................................................... 741 
15.5 
Spherical Harmonics................................................................................................... 756 
15.6 
Legendre Functions of the Second Kind ...................................................................... 766 
Additional Readings .................................................................................................. 771 
16. 
ANGULAR MOMENTUM ............................................................................................................. 773 
16.1 
Angular Momentum Operators .................................................................................. 774 
16.2 
Angular Momentum Coupling .................................................................................... 784 
16.3 
Spherical Tensors ....................................................................................................... 796 
16.4 
Vector Spherical Harmonics ....................................................................................... 809 
Additional Readings .................................................................................................. 814 
17. 
GROUP THEORY ....................................................................................................................... 815 
17.1 
Introduction to Group Theory .................................................................................... 815 
17.2 
Representation of Groups .......................................................................................... 821 
17.3 
Symmetry and Physics ................................................................................................ 826 
17.4 
Discrete Groups .......................................................................................................... 830 

ix 
 
17.5 
Direct Products ........................................................................................................... 837 
17.6 
Simmetric Group ........................................................................................................ 840 
17.7 
Continous Groups ....................................................................................................... 845 
17.8 
Lorentz Group ............................................................................................................ 862 
17.9 
Lorentz Covariance of Maxwell’s Equantions ............................................................. 866 
17.10      Space Groups ............................................................................................................. 869 
Additional Readings .................................................................................................. 870 
18. 
MORE SPECIAL FUNCTIONS ......................................................................................................... 871 
18.1 
Hermite Functions ...................................................................................................... 871 
18.2 
Applications of Hermite Functions ............................................................................. 878 
18.3 
Laguerre Functions ..................................................................................................... 889 
18.4 
Chebyshev Polynomials .............................................................................................. 899 
18.5 
Hypergeometric Functions ......................................................................................... 911 
18.6 
Confluent Hypergeometric Functions ......................................................................... 917 
18.7 
Dilogarithm ................................................................................................................ 923 
18.8 
Elliptic Integrals .......................................................................................................... 927 
Additional Readings .................................................................................................. 932 
19. 
FOURIER SERIES........................................................................................................................ 935 
19.1 
General Properties ..................................................................................................... 935 
19.2 
Application of Fourier Series ...................................................................................... 949 
19.3 
Gibbs Phenomenon .................................................................................................... 957 
Additional Readings .................................................................................................. 962 
20. 
INTEGRAL TRANSFORMS ............................................................................................................. 963 
20.1 
Introduction ............................................................................................................... 963 
20.2 
Fourier Transforms ..................................................................................................... 966 
20.3 
Properties of Fourier Transforms ............................................................................... 980 
20.4 
Fourier Convolution Theorem ..................................................................................... 985 
20.5 
Signal – Proccesing Applications ................................................................................ 997 
20.6 
Discrete Fourier Transforms ..................................................................................... 1002 
20.7 
Laplace Transforms .................................................................................................. 1008 
20.8 
Properties of Laplace Transforms ............................................................................. 1016 
20.9 
Laplace Convolution Transforms .............................................................................. 1034 
20.10      Inverse Laplace Transforms ...................................................................................... 1038 
Additional Readings ................................................................................................ 1045 
21. 
INTEGRAL EQUATIONS ............................................................................................................. 1047 
21.1 
Introduction ............................................................................................................. 1047 
21.2 
Some Special Methods ............................................................................................. 1053 
21.3 
Neumann Series ....................................................................................................... 1064 
21.4 
Hilbert – Schmidt Theory .......................................................................................... 1069 
Additional Readings ................................................................................................ 1079 
 

x 
 
22. 
CALCULUS OF VARIATIONS ........................................................................................................ 1081 
22.1 
Euler Equation .......................................................................................................... 1081 
22.2 
More General Variations .......................................................................................... 1096 
22.3 
Constrained Minima/Maxima .................................................................................. 1107 
22.4 
Variation with Constraints ....................................................................................... 1111 
Additional Readings ................................................................................................ 1124 
23. 
PROBABILITY AND STATISTICS .................................................................................................... 1125 
23.1 
Probability: Definitions, Simple Properties ............................................................... 1126 
23.2 
Random Variables .................................................................................................... 1134 
23.3 
Binomial Distribution ............................................................................................... 1148 
23.4 
Poisson Distribution ................................................................................................. 1151 
23.5 
Gauss’ Nomal Distribution ....................................................................................... 1155 
23.6 
Transformation of Random Variables ...................................................................... 1159 
23.7 
Statistics ................................................................................................................... 1165 
Additional Readings ................................................................................................ 1179 
 
INDEX  ........................................................................................................................................... 1181 
 
 
 
 
 
 
 
 
 
 
 
 
 

PREFACE
This, the seventh edition of Mathematical Methods for Physicists, maintains the tradition
set by the six previous editions and continues to have as its objective the presentation of all
the mathematical methods that aspiring scientists and engineers are likely to encounter as
students and beginning researchers. While the organization of this edition differs in some
respects from that of its predecessors, the presentation style remains the same: Proofs are
sketched for almost all the mathematical relations introduced in the book, and they are
accompanied by examples that illustrate how the mathematics applies to real-world physics
problems. Large numbers of exercises provide opportunities for the student to develop skill
in the use of the mathematical concepts and also show a wide variety of contexts in which
the mathematics is of practical use in physics.
As in the previous editions, the mathematical proofs are not what a mathematician would
consider rigorous, but they nevertheless convey the essence of the ideas involved, and also
provide some understanding of the conditions and limitations associated with the rela-
tionships under study. No attempt has been made to maximize generality or minimize the
conditions necessary to establish the mathematical formulas, but in general the reader is
warned of limitations that are likely to be relevant to use of the mathematics in physics
contexts.
TO THE STUDENT
The mathematics presented in this book is of no use if it cannot be applied with some skill,
and the development of that skill cannot be acquired passively, e.g., by simply reading the
text and understanding what is written, or even by listening attentively to presentations
by your instructor. Your passive understanding needs to be supplemented by experience
in using the concepts, in deciding how to convert expressions into useful forms, and in
developing strategies for solving problems. A considerable body of background knowledge
xi

xii
Preface
needs to be built up so as to have relevant mathematical tools at hand and to gain experi-
ence in their use. This can only happen through the solving of problems, and it is for this
reason that the text includes nearly 1400 exercises, many with answers (but not methods
of solution). If you are using this book for self-study, or if your instructor does not assign
a considerable number of problems, you would be well advised to work on the exercises
until you are able to solve a reasonable fraction of them.
This book can help you to learn about mathematical methods that are important in
physics, as well as serve as a reference throughout and beyond your time as a student.
It has been updated to make it relevant for many years to come.
WHAT’S NEW
This seventh edition is a substantial and detailed revision of its predecessor; every word of
the text has been examined and its appropriacy and that of its placement has been consid-
ered. The main features of the revision are: (1) An improved order of topics so as to reduce
the need to use concepts before they have been presented and discussed. (2) An introduc-
tory chapter containing material that well-prepared students might be presumed to know
and which will be relied on (without much comment) in later chapters, thereby reducing
redundancy in the text; this organizational feature also permits students with weaker back-
grounds to get themselves ready for the rest of the book. (3) A strengthened presentation of
topics whose importance and relevance has increased in recent years; in this category are
the chapters on vector spaces, Green’s functions, and angular momentum, and the inclu-
sion of the dilogarithm among the special functions treated. (4) More detailed discussion
of complex integration to enable the development of increased skill in using this extremely
important tool. (5) Improvement in the correlation of exercises with the exposition in the
text, and the addition of 271 new exercises where they were deemed needed. (6) Addition
of a few steps to derivations that students found difﬁcult to follow. We do not subscribe
to the precept that “advanced” means “compressed” or “difﬁcult.” Wherever the need has
been recognized, material has been rewritten to enhance clarity and ease of understanding.
In order to accommodate new and expanded features, it was necessary to remove or
reduce in emphasis some topics with signiﬁcant constituencies. For the most part, the
material thereby deleted remains available to instructors and their students by virtue of
its inclusion in the on-line supplementary material for this text. On-line only are chapters
on Mathieu functions, on nonlinear methods and chaos, and a new chapter on periodic sys-
tems. These are complete and newly revised chapters, with examples and exercises, and
are fully ready for use by students and their instuctors. Because there seems to be a sig-
niﬁcant population of instructors who wish to use material on inﬁnite series in much the
same organizational pattern as in the sixth edition, that material (largely the same as in
the print edition, but not all in one place) has been collected into an on-line inﬁnite series
chapter that provides this material in a single unit. The on-line material can be accessed at
www.elsevierdirect.com.

Preface
xiii
PATHWAYS THROUGH THE MATERIAL
This book contains more material than an instructor can expect to cover, even in a
two-semester course. The material not used for instruction remains available for reference
purposes or when needed for speciﬁc projects. For use with less fully prepared students,
a typical semester course might use Chapters 1 to 3, maybe part of Chapter 4, certainly
Chapters 5 to 7, and at least part of Chapter 11. A standard graduate one-semester course
might have the material in Chapters 1 to 3 as prerequisite, would cover at least part of
Chapter 4, all of Chapters 5 through 9, Chapter 11, and as much of Chapters 12 through
16 and/or 18 as time permits. A full-year course at the graduate level might supplement
the foregoing with several additional chapters, almost certainly including Chapter 20 (and
Chapter 19 if not already familiar to the students), with the actual choice dependent on
the institution’s overall graduate curriculum. Once Chapters 1 to 3, 5 to 9, and 11 have
been covered or their contents are known to the students, most selections from the remain-
ing chapters should be reasonably accessible to students. It would be wise, however, to
include Chapters 15 and 16 if Chapter 17 is selected.
ACKNOWLEDGMENTS
This seventh edition has beneﬁted from the advice and help of many people; valuable
advice was provided both by anonymous reviewers and from interaction with students at
the University of Utah. At Elsevier, we received substantial assistance from our Acqui-
sitions Editor Patricia Osborn and from Editorial Project Manager Kathryn Morrissey;
production was overseen skillfully by Publishing Services Manager Jeff Freeland. FEH
gratefully acknowledges the support and encouragement of his friend and partner Sharon
Carlson. Without her, he might not have had the energy and sense of purpose needed to
help bring this project to a timely fruition.

CHAPTER 1
MATHEMATICAL
PRELIMINARIES
This introductory chapter surveys a number of mathematical techniques that are needed
throughout the book. Some of the topics (e.g., complex variables) are treated in more detail
in later chapters, and the short survey of special functions in this chapter is supplemented
by extensive later discussion of those of particular importance in physics (e.g., Bessel func-
tions). A later chapter on miscellaneous mathematical topics deals with material requiring
more background than is assumed at this point. The reader may note that the Additional
Readings at the end of this chapter include a number of general references on mathemati-
cal methods, some of which are more advanced or comprehensive than the material to be
found in this book.
1.1
INFINITE SERIES
Perhaps the most widely used technique in the physicist’s toolbox is the use of inﬁnite
series (i.e., sums consisting formally of an inﬁnite number of terms) to represent functions,
to bring them to forms facilitating further analysis, or even as a prelude to numerical eval-
uation. The acquisition of skill in creating and manipulating series expansions is therefore
an absolutely essential part of the training of one who seeks competence in the mathemat-
ical methods of physics, and it is therefore the ﬁrst topic in this text. An important part of
this skill set is the ability to recognize the functions represented by commonly encountered
expansions, and it is also of importance to understand issues related to the convergence of
inﬁnite series.
1
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

2
Chapter 1 Mathematical Preliminaries
Fundamental Concepts
The usual way of assigning a meaning to the sum of an inﬁnite number of terms is by
introducing the notion of partial sums. If we have an inﬁnite sequence of terms u1, u2, u3,
u4, u5, ..., we deﬁne the ith partial sum as
si =
iX
n=1
un.
(1.1)
This is a ﬁnite summation and offers no difﬁculties. If the partial sums si converge to a
ﬁnite limit as i →∞,
lim
i→∞si = S,
(1.2)
the inﬁnite series P∞
n=1 un is said to be convergent and to have the value S. Note that
we deﬁne the inﬁnite series as equal to S and that a necessary condition for convergence
to a limit is that limn→∞un = 0. This condition, however, is not sufﬁcient to guarantee
convergence.
Sometimes it is convenient to apply the condition in Eq. (1.2) in a form called the
Cauchy criterion, namely that for each ε > 0 there is a ﬁxed number N such that
|s j −si| < ε for all i and j greater than N. This means that the partial sums must cluster
together as we move far out in the sequence.
Some series diverge, meaning that the sequence of partial sums approaches ±∞; others
may have partial sums that oscillate between two values, as for example,
∞
X
n=1
un = 1 −1 + 1 −1 + 1 −··· −(−1)n + ··· .
This series does not converge to a limit, and can be called oscillatory. Often the term
divergent is extended to include oscillatory series as well. It is important to be able to
determine whether, or under what conditions, a series we would like to use is convergent.
Example 1.1.1
THE GEOMETRIC SERIES
The geometric series, starting with u0 = 1 and with a ratio of successive terms r =
un+1/un, has the form
1 + r + r2 + r3 + ··· + rn−1 + ··· .
Its nth partial sum sn (that of the ﬁrst n terms) is1
sn = 1 −rn
1 −r .
(1.3)
Restricting attention to |r| < 1, so that for large n, rn approaches zero, and sn possesses
the limit
lim
n→∞sn =
1
1 −r ,
(1.4)
1Multiply and divide sn = Pn−1
m=0 rm by 1 −r.

1.1 Inﬁnite Series
3
showing that for |r| < 1, the geometric series converges. It clearly diverges (or is oscilla-
tory) for |r| ≥1, as the individual terms do not then approach zero at large n.
■
Example 1.1.2
THE HARMONIC SERIES
As a second and more involved example, we consider the harmonic series
∞
X
n=1
1
n = 1 + 1
2 + 1
3 + 1
4 + ··· + 1
n + ··· .
(1.5)
The terms approach zero for large n, i.e., limn→∞1/n = 0, but this is not sufﬁcient to
guarantee convergence. If we group the terms (without changing their order) as
1 + 1
2 +
1
3 + 1
4

+
1
5 + 1
6 + 1
7 + 1
8

+
1
9 + ··· + 1
16

+ ··· ,
each pair of parentheses encloses p terms of the form
1
p + 1 +
1
p + 2 + ··· +
1
p + p > p
2p = 1
2.
Forming partial sums by adding the parenthetical groups one by one, we obtain
s1 = 1, s2 = 3
2, s3 > 4
2, s4 > 5
2,..., sn > n + 1
2
,
and we are forced to the conclusion that the harmonic series diverges.
Although the harmonic series diverges, its partial sums have relevance among other
places in number theory, where Hn = Pn
m=1 m−1 are sometimes referred to as harmonic
numbers.
■
We now turn to a more detailed study of the convergence and divergence of series,
considering here series of positive terms. Series with terms of both signs are treated later.
Comparison Test
If term by term a series of terms un satisﬁes 0 ≤un ≤an, where the an form a convergent
series, then the series P
n un is also convergent. Letting si and s j be partial sums of the
u series, with j > i, the difference s j −si is P j
n=i+1 un, and this is smaller than the
corresponding quantity for the a series, thereby proving convergence. A similar argument
shows that if term by term a series of terms vn satisﬁes 0 ≤bn ≤vn, where the bn form a
divergent series, then P
n vn is also divergent.
For the convergent series an we already have the geometric series, whereas the harmonic
series will serve as the divergent comparison series bn. As other series are identiﬁed as
either convergent or divergent, they may also be used as the known series for comparison
tests.

4
Chapter 1 Mathematical Preliminaries
Example 1.1.3
A DIVERGENT SERIES
Test P∞
n=1 n−p, p = 0.999, for convergence. Since n−0.999 > n−1 and bn = n−1 forms
the divergent harmonic series, the comparison test shows that P
n n−0.999 is divergent.
Generalizing, P
n n−p is seen to be divergent for all p ≤1.
■
Cauchy Root Test
If (an)1/n ≤r < 1 for all sufﬁciently large n, with r independent of n, then P
n an is
convergent. If (an)1/n ≥1 for all sufﬁciently large n, then P
n an is divergent.
The language of this test emphasizes an important point: The convergence or divergence
of a series depends entirely on what happens for large n. Relative to convergence, it is the
behavior in the large-n limit that matters.
The ﬁrst part of this test is veriﬁed easily by raising (an)1/n to the nth power. We get
an ≤rn < 1.
Since rn is just the nth term in a convergent geometric series, P
n an is convergent by the
comparison test. Conversely, if (an)1/n ≥1, then an ≥1 and the series must diverge. This
root test is particularly useful in establishing the properties of power series (Section 1.2).
D’Alembert (or Cauchy) Ratio Test
If an+1/an ≤r < 1 for all sufﬁciently large n and r is independent of n, then P
n an is
convergent. If an+1/an ≥1 for all sufﬁciently large n, then P
n an is divergent.
This test is established by direct comparison with the geometric series (1+r +r2 +···).
In the second part, an+1 ≥an and divergence should be reasonably obvious. Although not
quite as sensitive as the Cauchy root test, this D’Alembert ratio test is one of the easiest to
apply and is widely used. An alternate statement of the ratio test is in the form of a limit: If
lim
n→∞
an+1
an



< 1,
convergence,
> 1,
divergence,
= 1,
indeterminate.
(1.6)
Because of this ﬁnal indeterminate possibility, the ratio test is likely to fail at crucial points,
and more delicate, sensitive tests then become necessary. The alert reader may wonder how
this indeterminacy arose. Actually it was concealed in the ﬁrst statement, an+1/an ≤r <
1. We might encounter an+1/an < 1 for all ﬁnite n but be unable to choose an r < 1
and independent of n such that an+1/an ≤r for all sufﬁciently large n. An example is
provided by the harmonic series, for which
an+1
an
=
n
n + 1 < 1.
Since
lim
n→∞
an+1
an
= 1,
no ﬁxed ratio r < 1 exists and the test fails.

1.1 Inﬁnite Series
5
Example 1.1.4
D’ALEMBERT RATIO TEST
Test P
n n/2n for convergence. Applying the ratio test,
an+1
an
= (n + 1)/2n+1
n/2n
= 1
2
n + 1
n
.
Since
an+1
an
≤3
4
for n ≥2,
we have convergence.
■
Cauchy (or Maclaurin) Integral Test
This is another sort of comparison test, in which we compare a series with an integral.
Geometrically, we compare the area of a series of unit-width rectangles with the area under
a curve.
Let f (x) be a continuous, monotonic decreasing function in which f (n) = an. Then
P
n an converges if
R ∞
1
f (x)dx is ﬁnite and diverges if the integral is inﬁnite. The ith
partial sum is
si =
iX
n=1
an =
iX
n=1
f (n).
But, because f (x) is monotonic decreasing, see Fig. 1.1(a),
si ≥
i+1
Z
1
f (x)dx.
On the other hand, as shown in Fig. 1.1(b),
si −a1 ≤
iZ
1
f (x)dx.
Taking the limit as i →∞, we have
∞
Z
1
f (x)dx ≤
∞
X
n=1
an ≤
∞
Z
1
f (x)dx + a1.
(1.7)
Hence the inﬁnite series converges or diverges as the corresponding integral converges or
diverges.
This integral test is particularly useful in setting upper and lower bounds on the remain-
der of a series after some number of initial terms have been summed. That is,
∞
X
n=1
an =
N
X
n=1
an +
∞
X
n=N+1
an,
(1.8)

6
Chapter 1 Mathematical Preliminaries
4
3
2
1
f(x)
f(x)
(a)
x
f (1)= a1 
f(1) =a1 
f(2) =a2 
4
3
2
1
(b)
x
i=
FIGURE 1.1
(a) Comparison of integral and sum-blocks leading. (b) Comparison of
integral and sum-blocks lagging.
and
∞
Z
N+1
f (x)dx ≤
∞
X
n=N+1
an ≤
∞
Z
N+1
f (x)dx + aN+1.
(1.9)
To free the integral test from the quite restrictive requirement that the interpolating func-
tion f (x) be positive and monotonic, we shall show that for any function f (x) with a
continuous derivative, the inﬁnite series is exactly represented as a sum of two integrals:
N2
X
n=N1+1
f (n) =
N2
Z
N1
f (x)dx +
N2
Z
N1
(x −[x]) f ′(x)dx.
(1.10)
Here [x] is the integral part of x, i.e., the largest integer ≤x, so x −[x] varies sawtoothlike
between 0 and 1. Equation (1.10) is useful because if both integrals in Eq. (1.10) converge,
the inﬁnite series also converges, while if one integral converges and the other does not,
the inﬁnite series diverges. If both integrals diverge, the test fails unless it can be shown
whether the divergences of the integrals cancel against each other.
We need now to establish Eq. (1.10). We manipulate the contributions to the second
integral as follows:
1.
Using integration by parts, we observe that
N2
Z
N1
x f ′(x)dx = N2 f (N2) −N1 f (N1) −
N2
Z
N1
f (x)dx.
2.
We evaluate
N2
Z
N1
[x] f ′(x)dx =
N2−1
X
n=N1
n
n+1
Z
n
f ′(x)dx =
N2−1
X
n=N1
n
h
f (n + 1) −f (n)
i
= −
N2
X
n=N1+1
f (n) −N1 f (N1) + N2 f (N2).
Subtracting the second of these equations from the ﬁrst, we arrive at Eq. (1.10).

1.1 Inﬁnite Series
7
An alternative to Eq. (1.10) in which the second integral has its sawtooth shifted to be
symmetrical about zero (and therefore perhaps smaller) can be derived by methods similar
to those used above. The resulting formula is
N2
X
n=N1+1
f (n) =
N2
Z
N1
f (x)dx +
N2
Z
N1
(x −[x] −1
2) f ′(x)dx
+ 1
2
h
f (N2) −f (N1)
i
.
(1.11)
Because they do not use a monotonicity requirement, Eqs. (1.10) and (1.11) can be
applied to alternating series, and even those with irregular sign sequences.
Example 1.1.5
RIEMANN ZETA FUNCTION
The Riemann zeta function is deﬁned by
ζ(p) =
∞
X
n=1
n−p,
(1.12)
providing the series converges. We may take f (x) = x−p, and then
∞
Z
1
x−p dx = x−p+1
−p + 1

∞
x=1
,
p ̸= 1,
= ln x

∞
x=1,
p = 1.
The integral and therefore the series are divergent for p ≤1, and convergent for p > 1.
Hence Eq. (1.12) should carry the condition p > 1. This, incidentally, is an independent
proof that the harmonic series (p = 1) diverges logarithmically. The sum of the ﬁrst million
terms P1,000,000
n=1
n−1 is only 14.392 726··· .
■
While the harmonic series diverges, the combination
γ = lim
n→∞
 n
X
m=1
m−1 −lnn
!
(1.13)
converges, approaching a limit known as the Euler-Mascheroni constant.
Example 1.1.6
A SLOWLY DIVERGING SERIES
Consider now the series
S =
∞
X
n=2
1
n lnn .

8
Chapter 1 Mathematical Preliminaries
We form the integral
∞
Z
2
1
x ln x dx =
∞
Z
x=2
d ln x
ln x = lnln x

∞
x=2,
which diverges, indicating that S is divergent. Note that the lower limit of the integral is
in fact unimportant so long as it does not introduce any spurious singularities, as it is the
large-x behavior that determines the convergence. Because n lnn > n, the divergence is
slower than that of the harmonic series. But because lnn increases more slowly than nε,
where ε can have an arbitrarily small positive value, we have divergence even though the
series P
n n−(1+ε) converges.
■
More Sensitive Tests
Several tests more sensitive than those already examined are consequences of a theorem
by Kummer. Kummer’s theorem, which deals with two series of ﬁnite positive terms, un
and an, states:
1.
The series P
n un converges if
lim
n→∞

an
un
un+1
−an+1

≥C > 0,
(1.14)
where C is a constant. This statement is equivalent to a simple comparison test if the
series P
n a−1
n
converges, and imparts new information only if that sum diverges. The
more weakly P
n a−1
n
diverges, the more powerful the Kummer test will be.
2.
If P
n a−1
n
diverges and
lim
n→∞

an
un
un+1
−an+1

≤0,
(1.15)
then P
n un diverges.
The proof of this powerful test is remarkably simple. Part 2 follows immediately from
the comparison test. To prove Part 1, write cases of Eq. (1.14) for n = N + 1 through any
larger n, in the following form:
uN+1 ≤(aNuN −aN+1uN+1)/C,
uN+2 ≤(aN+1uN+1 −aN+2uN+2)/C,
... ≤........................,
un ≤(an−1un−1 −anun)/C.

1.1 Inﬁnite Series
9
Adding, we get
n
X
i=N+1
ui ≤aNuN
C
−anun
C
(1.16)
< aNuN
C
.
(1.17)
This shows that the tail of the series P
n un is bounded, and that series is therefore proved
convergent when Eq. (1.14) is satisﬁed for all sufﬁciently large n.
Gauss’ test is an application of Kummer’s theorem to series un > 0 when the ratios of
successive un approach unity and the tests previously discussed yield indeterminate results.
If for large n
un
un+1
= 1 + h
n + B(n)
n2 ,
(1.18)
where B(n) is bounded for n sufﬁciently large, then the Gauss test states that P
n un con-
verges for h > 1 and diverges for h ≤1: There is no indeterminate case here.
The Gauss test is extremely sensitive, and will work for all troublesome series the physi-
cist is likely to encounter. To conﬁrm it using Kummer’s theorem, we take an = n lnn. The
series P
n a−1
n
is weakly divergent, as already established in Example 1.1.6.
Taking the limit on the left side of Eq. (1.14), we have
lim
n→∞

n lnn

1 + h
n + B(n)
n2

−(n + 1)ln(n + 1)

= lim
n→∞

(n + 1)lnn + (h −1)lnn + B(n)lnn
n
−(n + 1)ln(n + 1)

= lim
n→∞

−(n + 1)ln
n + 1
n

+ (h −1)lnn

.
(1.19)
For h < 1, both terms of Eq. (1.19) are negative, thereby signaling a divergent case of
Kummer’s theorem; for h > 1, the second term of Eq. (1.19) dominates the ﬁrst and is pos-
itive, indicating convergence. At h = 1, the second term vanishes, and the ﬁrst is inherently
negative, thereby indicating divergence.
Example 1.1.7
LEGENDRE SERIES
The series solution for the Legendre equation (encountered in Chapter 7) has successive
terms whose ratio under certain conditions is
a2 j+2
a2 j
= 2 j(2 j + 1) −λ
(2 j + 1)(2 j + 2).
To place this in the form now being used, we deﬁne u j = a2 j and write
u j
u j+1
= (2 j + 1)(2 j + 2)
2 j(2 j + 1) −λ .

10
Chapter 1 Mathematical Preliminaries
In the limit of large j, the constant λ becomes negligible (in the language of the Gauss test,
it contributes to an extent B( j)/j2, where B( j) is bounded). We therefore have
u j
u j+1
→2 j + 2
2 j
+ B( j)
j2
= 1 + 1
j + B( j)
j2 .
(1.20)
The Gauss test tells us that this series is divergent.
■
Exercises
1.1.1
(a)
Prove that if limn→∞n pun = A < ∞, p > 1, the series P∞
n=1 un converges.
(b)
Prove that if limn→∞nun = A > 0, the series diverges. (The test fails for A = 0.)
These two tests, known as limit tests, are often convenient for establishing the
convergence of a series. They may be treated as comparison tests, comparing with
X
n
n−q,
1 ≤q < p.
1.1.2
If limn→∞bn
an = K, a constant with 0 < K < ∞, show that 6nbn converges or diverges
with 6an.
Hint. If 6an converges, rescale bn to b′
n = bn
2K . If 6nan diverges, rescale to b′′
n = 2bn
K .
1.1.3
(a)
Show that the series P∞
n=2
1
n (lnn)2 converges.
(b)
By direct addition P100,000
n=2
[n(lnn)2]−1 = 2.02288. Use Eq. (1.9) to make a ﬁve-
signiﬁcant-ﬁgure estimate of the sum of this series.
1.1.4
Gauss’ test is often given in the form of a test of the ratio
un
un+1
= n2 + a1n + a0
n2 + b1n + b0
.
For what values of the parameters a1 and b1 is there convergence? divergence?
ANS.
Convergent for a1 −b1 > 1,
divergent for a1 −b1 ≤1.
1.1.5
Test for convergence
(a)
∞
X
n=2
(lnn)−1
(d)
∞
X
n=1
[n(n + 1)]−1/2
(b)
∞
X
n=1
n!
10n
(e)
∞
X
n=0
1
2n + 1
(c)
∞
X
n=1
1
2n(2n + 1)

1.1 Inﬁnite Series
11
1.1.6
Test for convergence
(a)
∞
X
n=1
1
n(n + 1)
(d)
∞
X
n=1
ln

1 + 1
n

(b)
∞
X
n=2
1
n lnn
(e)
∞
X
n=1
1
n · n1/n
(c)
∞
X
n=1
1
n2n
1.1.7
For what values of p and q will P∞
n=2
1
n p(lnn)q converge?
ANS.
Convergent for
( p > 1,
all q,
p = 1,
q > 1,
divergent for
( p < 1,
all q,
p = 1,
q ≤1.
1.1.8
Given P1,000
n=1 n−1 = 7.485 470... set upper and lower bounds on the Euler-Mascheroni
constant.
ANS.
0.5767 < γ < 0.5778.
1.1.9
(From Olbers’ paradox.) Assume a static universe in which the stars are uniformly
distributed. Divide all space into shells of constant thickness; the stars in any one shell
by themselves subtend a solid angle of ω0. Allowing for the blocking out of distant
stars by nearer stars, show that the total net solid angle subtended by all stars, shells
extending to inﬁnity, is exactly 4π. [Therefore the night sky should be ablaze with
light. For more details, see E. Harrison, Darkness at Night: A Riddle of the Universe.
Cambridge, MA: Harvard University Press (1987).]
1.1.10
Test for convergence
∞
X
n=1
1 · 3 · 5···(2n −1)
2 · 4 · 6···(2n)
2
= 1
4 + 9
64 + 25
256 + ··· .
Alternating Series
In previous subsections we limited ourselves to series of positive terms. Now, in contrast,
we consider inﬁnite series in which the signs alternate. The partial cancellation due to
alternating signs makes convergence more rapid and much easier to identify. We shall
prove the Leibniz criterion, a general condition for the convergence of an alternating series.
For series with more irregular sign changes, the integral test of Eq. (1.10) is often helpful.
The Leibniz criterion applies to series of the form P∞
n=1(−1)n+1an with an > 0, and
states that if an is monotonically decreasing (for sufﬁciently large n) and limn→∞an = 0,
then the series converges. To prove this theorem, note that the remainder R2n of the series
beyond s2n, the partial sum after 2n terms, can be written in two alternate ways:
R2n = (a2n+1 −a2n+2) + (a2n+3 −a2n+4) + ···
= a2n+1 −(a2n+2 −a2n+3) −(a2n+4 −a2n+5) −··· .

12
Chapter 1 Mathematical Preliminaries
Since the an are decreasing, the ﬁrst of these equations implies R2n > 0, while the second
implies R2n < a2n+1, so
0 < R2n < a2n+1.
Thus, R2n is positive but bounded, and the bound can be made arbitrarily small by taking
larger values of n. This demonstration also shows that the error from truncating an alter-
nating series after a2n results in an error that is negative (the omitted terms were shown to
combine to a positive result) and bounded in magnitude by a2n+1. An argument similar to
that made above for the remainder after an odd number of terms, R2n+1, would show that
the error from truncation after a2n+1 is positive and bounded by a2n+2. Thus, it is generally
true that the error in truncating an alternating series with monotonically decreasing terms
is of the same sign as the last term kept and smaller than the ﬁrst term dropped.
The Leibniz criterion depends for its applicability on the presence of strict sign
alternation. Less regular sign changes present more challenging problems for convergence
determination.
Example 1.1.8
SERIES WITH IRREGULAR SIGN CHANGES
For 0 < x < 2π, the series
S =
∞
X
n=1
cos(nx)
n
= −ln

2sin x
2

(1.21)
converges, having coefﬁcients that change sign often, but not so that the Leibniz criterion
applies easily. To verify the convergence, we apply the integral test of Eq. (1.10), inserting
the explicit form for the derivative of cos(nx)/n (with respect to n) in the second integral:
S =
∞
Z
1
cos(nx)
n
dn +
∞
Z
1

n −[n]
 
−x
n sin(nx) −cos(nx)
n2

dn.
(1.22)
Using integration by parts, the ﬁrst integral in Eq. (1.22) is rearranged to
∞
Z
1
cos(nx)
n
dn =
sin(nx)
nx
∞
1
+ 1
x
∞
Z
1
sin(nx)
n2
dn,
and this integral converges because

∞
Z
1
sin(nx)
n2
dn

<
∞
Z
1
dn
n2 = 1.
Looking now at the second integral in Eq. (1.22), we note that its term cos(nx)/n2 also
leads to a convergent integral, so we need only to examine the convergence of
∞
Z
1

n −[n]
 sin(nx)
n
dn.

1.1 Inﬁnite Series
13
Next, setting (n −[n])sin(nx) = g′(n), which is equivalent to deﬁning g(N) =
R N
1 (n −
[n])sin(nx)dn, we write
∞
Z
1

n −[n]
 sin(nx)
n
dn =
∞
Z
1
g′(n)
n
dn =
g(n)
n
∞
n=1
+
∞
Z
1
g(n)
n2 dn,
where the last equality was obtained using once again an integration by parts. We do not
have an explicit expression for g(n), but we do know that it is bounded because sin x
oscillates with a period incommensurate with that of the sawtooth periodicity of (n −[n]).
This boundedness enables us to determine that the second integral in Eq. (1.22) converges,
thus establishing the convergence of S.
■
Absolute and Conditional Convergence
An inﬁnite series is absolutely convergent if the absolute values of its terms form a con-
vergent series. If it converges, but not absolutely, it is termed conditionally convergent.
An example of a conditionally convergent series is the alternating harmonic series,
∞
X
n=1
(−1)n−1n−1 = 1 −1
2 + 1
3 −1
4 + ··· + (−1)n−1
n
+ ··· .
(1.23)
This series is convergent, based on the Leibniz criterion. It is clearly not absolutely con-
vergent; if all terms are taken with + signs, we have the harmonic series, which we already
know to be divergent. The tests described earlier in this section for series of positive terms
are, then, tests for absolute convergence.
Exercises
1.1.11
Determine whether each of these series is convergent, and if so, whether it is absolutely
convergent:
(a)
ln2
2 −ln3
3 + ln4
4 −ln5
5 + ln6
6 −··· ,
(b)
1
1 + 1
2 −1
3 −1
4 + 1
5 + 1
6 −1
7 −1
8 + ··· ,
(c)
1 −1
2 −1
3 + 1
4 + 1
5 + 1
6 −1
7 −1
8 −1
9 −1
10 + 1
11 ··· + 1
15 −1
16 ··· −1
21 + ··· .
1.1.12
Catalan’s constant β(2) is deﬁned by
β(2) =
∞
X
k=0
(−1)k(2k + 1)−2 = 1
12 −1
32 + 1
52 ··· .
Calculate β(2) to six-digit accuracy.

14
Chapter 1 Mathematical Preliminaries
Hint. The rate of convergence is enhanced by pairing the terms,
(4k −1)−2 −(4k + 1)−2 =
16k
(16k2 −1)2 .
If you have carried enough digits in your summation, P
1≤k≤N 16k/(16k2 −1)2, addi-
tional signiﬁcant ﬁgures may be obtained by setting upper and lower bounds on the tail
of the series, P∞
k=N+1. These bounds may be set by comparison with integrals, as in
the Maclaurin integral test.
ANS.
β(2) = 0.9159 6559 4177··· .
Operations on Series
We now investigate the operations that may be performed on inﬁnite series. In this connec-
tion the establishment of absolute convergence is important, because it can be proved that
the terms of an absolutely convergent series may be reordered according to the familiar
rules of algebra or arithmetic:
•
If an inﬁnite series is absolutely convergent, the series sum is independent of the order
in which the terms are added.
•
An absolutely convergent series may be added termwise to, or subtracted termwise
from, or multiplied termwise with another absolutely convergent series, and the result-
ing series will also be absolutely convergent.
•
The series (as a whole) may be multiplied with another absolutely convergent series.
The limit of the product will be the product of the individual series limits. The product
series, a double series, will also converge absolutely.
No such guarantees can be given for conditionally convergent series, though some of
the above properties remain true if only one of the series to be combined is conditionally
convergent.
Example 1.1.9
REARRANGEMENT OF ALTERNATING HARMONIC SERIES
Writing the alternating harmonic series as
1 −1
2 + 1
3 −1
4 + ··· = 1 −
1
2 −1
3

−
1
4 −1
5

−··· ,
(1.24)
it is clear that P∞
n=1(−1)n−1n−1 < 1. However, if we rearrange the order of the terms, we
can make this series converge to 3
2. We regroup the terms of Eq. (1.24), as

1 + 1
3 + 1
5

−
1
2

+
1
7 + 1
9 + 1
11 + 1
13 + 1
15

−
1
4

+
 1
17 + ··· + 1
25

−
1
6

+
 1
27 + ··· + 1
35

−
1
8

+ ··· .
(1.25)

1.1 Inﬁnite Series
15
Partial sum, sn
1.500
1.400
1.300
1.200
1.100
Number of terms in sum, n
10
9
8
7
6
5
4
3
2
1
FIGURE 1.2
Alternating harmonic series. Terms are rearranged to give
convergence to 1.5.
Treating the terms grouped in parentheses as single terms for convenience, we obtain the
partial sums
s1 = 1.5333
s2 = 1.0333
s3 = 1.5218
s4 = 1.2718
s5 = 1.5143
s6 = 1.3476
s7 = 1.5103
s8 = 1.3853
s9 = 1.5078
s10 = 1.4078.
From this tabulation of sn and the plot of sn versus n in Fig. 1.2, the convergence to 3
2 is
fairly clear. Our rearrangement was to take positive terms until the partial sum was equal
to or greater than 3
2 and then to add negative terms until the partial sum just fell below 3
2
and so on. As the series extends to inﬁnity, all original terms will eventually appear, but
the partial sums of this rearranged alternating harmonic series converge to 3
2.
■
As the example shows, by a suitable rearrangement of terms, a conditionally convergent
series may be made to converge to any desired value or even to diverge. This statement is
sometimes called Riemann’s theorem.
Another example shows the danger of multiplying conditionally convergent series.
Example 1.1.10
SQUARE OF A CONDITIONALLY CONVERGENT SERIES MAY DIVERGE
The series P∞
n=1
(−1)n−1
√n
converges by the Leibniz criterion. Its square,
" ∞
X
n=1
(−1)n−1
√n
#2
=
X
n
(−1)n
 1
√
1
1
√n −1
+ 1
√
2
1
√n −2
+ ··· +
1
√n −1
1
√
1

,

16
Chapter 1 Mathematical Preliminaries
has a general term, in [...], consisting of n −1 additive terms, each of which is bigger than
1
√n−1√n−1, so the entire [...] term is greater than n−1
n−1 and does not go to zero. Hence the
general term of this product series does not approach zero in the limit of large n and the
series diverges.
■
These examples show that conditionally convergent series must be treated with caution.
Improvement of Convergence
This section so far has been concerned with establishing convergence as an abstract math-
ematical property. In practice, the rate of convergence may be of considerable importance.
A method for improving convergence, due to Kummer, is to form a linear combination of
our slowly converging series and one or more series whose sum is known. For the known
series the following collection is particularly useful:
α1 =
∞
X
n=1
1
n(n + 1) = 1,
α2 =
∞
X
n=1
1
n(n + 1)(n + 2) = 1
4,
α3 =
∞
X
n=1
1
n(n + 1)(n + 2)(n + 3) = 1
18,
..............................
αp =
∞
X
n=1
1
n(n + 1)···(n + p) =
1
p p!.
(1.26)
These sums can be evaluated via partial fraction expansions, and are the subject of
Exercise 1.5.3.
The series we wish to sum and one or more known series (multiplied by coefﬁcients)
are combined term by term. The coefﬁcients in the linear combination are chosen to cancel
the most slowly converging terms.
Example 1.1.11
RIEMANN ZETA FUNCTION ζ(3)
From the deﬁnition in Eq. (1.12), we identify ζ(3) as P∞
n=1 n−3. Noting that α2 of
Eq. (1.26) has a large-n dependence ∼n−3, we consider the linear combination
∞
X
n=1
n−3 + aα2 = ζ(3) + a
4.
(1.27)
We did not use α1 because it converges more slowly than ζ(3). Combining the two series
on the left-hand side termwise, we obtain
∞
X
n=1
 1
n3 +
a
n(n + 1)(n + 2)

=
∞
X
n=1
n2(1 + a) + 3n + 2
n3(n + 1)(n + 2) .

1.1 Inﬁnite Series
17
Table 1.1
Riemann Zeta
Function
s
ζ(s)
2
1.64493 40668
3
1.20205 69032
4
1.08232 32337
5
1.03692 77551
6
1.01734 30620
7
1.00834 92774
8
1.00407 73562
9
1.00200 83928
10
1.00099 45751
If we choose a = −1, we remove the leading term from the numerator; then, setting this
equal to the right-hand side of Eq. (1.27) and solving for ζ(3),
ζ(3) = 1
4 +
∞
X
n=1
3n + 2
n3(n + 1)(n + 2).
(1.28)
The resulting series may not be beautiful but it does converge as n−4, faster than n−3.
A more convenient form with even faster convergence is introduced in Exercise 1.1.16.
There, the symmetry leads to convergence as n−5.
■
Sometimes it is helpful to use the Riemann zeta function in a way similar to that
illustrated for the αp in the foregoing example. That approach is practical because the
zeta function has been tabulated (see Table 1.1).
Example 1.1.12
CONVERGENCE IMPROVEMENT
The problem is to evaluate the series P∞
n=1 1/(1 + n2). Expanding (1 + n2)−1 = n−2(1 +
n−2)−1 by direct division, we have
(1 + n2)−1 = n−2

1 −n−2 + n−4 −
n−6
1 + n−2

= 1
n2 −1
n4 + 1
n6 −
1
n8 + n6 .
Therefore
∞
X
n=1
1
1 + n2 = ζ(2) −ζ(4) + ζ(6) −
∞
X
n=1
1
n8 + n6 .
The remainder series converges as n−8. Clearly, the process can be continued as desired.
You make a choice between how much algebra you will do and how much arithmetic the
computer will do.
■

18
Chapter 1 Mathematical Preliminaries
Rearrangement of Double Series
An absolutely convergent double series (one whose terms are identiﬁed by two summation
indices) presents interesting rearrangement opportunities. Consider
S =
∞
X
m=0
∞
X
n=0
an,m.
(1.29)
In addition to the obvious possibility of reversing the order of summation (i.e., doing the m
sum ﬁrst), we can make rearrangements that are more innovative. One reason for doing this
is that we may be able to reduce the double sum to a single summation, or even evaluate
the entire double sum in closed form.
As an example, suppose we make the following index substitutions in our double series:
m = q, n = p −q. Then we will cover all n ≥0, m ≥0 by assigning p the range (0,∞),
and q the range (0, p), so our double series can be written
S =
∞
X
p=0
p
X
q=0
ap−q,q.
(1.30)
In the nm plane our region of summation is the entire quadrant m ≥0, n ≥0; in the pq
plane our summation is over the triangular region sketched in Fig. 1.3. This same pq region
can be covered when the summations are carried out in the reverse order, but with limits
S =
∞
X
q=0
∞
X
p=q
ap−q,q.
The important thing to note here is that these schemes all have in common that, by allowing
the indices to run over their designated ranges, every an,m is eventually encountered, and
is encountered exactly once.
4
q
2
0
0
2
4
p
FIGURE 1.3
The pq index space.

1.1 Inﬁnite Series
19
Another possible index substitution is to set n = s, m = r −2s. If we sum over s ﬁrst,
its range must be (0,[r/2]), where [r/2] is the integer part of r/2, i.e., [r/2] = r/2 for r
even and (r −1)/2 for r odd. The range of r is (0,∞). This situation corresponds to
S =
∞
X
r=0
[r/2]
X
s=0
as,r−2s.
(1.31)
The sketches in Figs. 1.4 to 1.6 show the order in which the an,m are summed when using
the forms given in Eqs. (1.29), (1.30), and (1.31), respectively.
If the double series introduced originally as Eq. (1.29) is absolutely convergent, then all
these rearrangements will give the same ultimate result.
m
2
3
1
0
0
2
4
n
FIGURE 1.4
Order in which terms are summed with m,n index set, Eq. (1.29).
4
m
2
3
1
0
0
2
3
1
4
n
FIGURE 1.5
Order in which terms are summed with p,q index set, Eq. (1.30).

20
Chapter 1 Mathematical Preliminaries
4
6
m
2
0
0
2
3
1
n
FIGURE 1.6
Order in which terms are summed with r,s index set, Eq. (1.31).
Exercises
1.1.13
Show how to combine ζ(2) = P∞
n=1 n−2 with α1 and α2 to obtain a series converging
as n−4.
Note. ζ(2) has the known value π2/6. See Eq. (12.66).
1.1.14
Give a method of computing
λ(3) =
∞
X
n=0
1
(2n + 1)3
that converges at least as fast as n−8 and obtain a result good to six decimal places.
ANS.
λ(3) = 1.051800.
1.1.15
Show that (a) P∞
n=2[ζ(n) −1] = 1,
(b) P∞
n=2(−1)n[ζ(n) −1] = 1
2,
where ζ(n) is the Riemann zeta function.
1.1.16
The convergence improvement of 1.1.11 may be carried out more expediently (in this
special case) by putting α2, from Eq. (1.26), into a more symmetric form: Replacing n
by n −1, we have
α′
2 =
∞
X
n=2
1
(n −1)n(n + 1) = 1
4.
(a)
Combine ζ(3) and α′
2 to obtain convergence as n−5.
(b)
Let α′
4 be α4 with n →n −2. Combine ζ(3),α′
2, and α′
4 to obtain convergence
as n−7.

1.2 Series of Functions
21
(c)
If ζ(3) is to be calculated to six-decimal place accuracy (error 5 × 10−7), how
many terms are required for ζ(3) alone? combined as in part (a)? combined as in
part (b)?
Note. The error may be estimated using the corresponding integral.
ANS.
(a)
ζ(3) = 5
4 −
∞
X
n=2
1
n3(n2 −1).
1.2
SERIES OF FUNCTIONS
We extend our concept of inﬁnite series to include the possibility that each term un may
be a function of some variable, un = un(x). The partial sums become functions of the
variable x,
sn(x) = u1(x) + u2(x) + ··· + un(x),
(1.32)
as does the series sum, deﬁned as the limit of the partial sums:
∞
X
n=1
un(x) = S(x) = lim
n→∞sn(x).
(1.33)
So far we have concerned ourselves with the behavior of the partial sums as a function of
n. Now we consider how the foregoing quantities depend on x. The key concept here is
that of uniform convergence.
Uniform Convergence
If for any small ε > 0 there exists a number N, independent of x in the interval [a,b]
(that is, a ≤x ≤b) such that
|S(x) −sn(x)| < ε,
for all n ≥N,
(1.34)
then the series is said to be uniformly convergent in the interval [a,b]. This says that
for our series to be uniformly convergent, it must be possible to ﬁnd a ﬁnite N so that
the absolute value of the tail of the inﬁnite series,
P∞
i=N+1 ui(x)
, will be less than an
arbitrary small ε for all x in the given interval, including the endpoints.
Example 1.2.1
NONUNIFORM CONVERGENCE
Consider on the interval [0,1] the series
S(x) =
∞
X
n=0
(1 −x)xn.

22
Chapter 1 Mathematical Preliminaries
For 0 ≤x < 1, the geometric series P
n xn is convergent, with value 1/(1 −x), so S(x) =
1 for these x values. But at x = 1, every term of the series will be zero, and therefore
S(1) = 0. That is,
∞
X
n=0
(1 −x)xn = 1,
0 ≤x < 1,
= 0,
x = 1.
(1.35)
So S(x) is convergent for the entire interval [0,1], and because each term is nonnegative,
it is also absolutely convergent. If x ̸= 0, this is a series for which the partial sum sN
is 1 −x N, as can be seen by comparison with Eq. (1.3). Since S(x) = 1, the uniform
convergence criterion is
1 −(1 −x N)
 = x N < ε.
No matter what the values of N and a sufﬁciently small ε may be, there will be an x value
(close to 1) where this criterion is violated. The underlying problem is that x = 1 is the
convergence limit of the geometric series, and it is not possible to have a convergence rate
that is bounded independently of x in a range that includes x = 1.
We note also from this example that absolute and uniform convergence are independent
concepts. The series in this example has absolute, but not uniform convergence. We will
shortly present examples of series that are uniformly, but only conditionally convergent.
And there are series that have neither or both of these properties.
■
Weierstrass M (Majorant) Test
The most commonly encountered test for uniform convergence is the Weierstrass M test.
If we can construct a series of numbers P∞
i=1 Mi, in which Mi ≥|ui(x)| for all x in the
interval [a,b] and P∞
i=1 Mi is convergent, our series ui(x) will be uniformly convergent
in [a,b].
The proof of this Weierstrass M test is direct and simple. Since P
i Mi converges, some
number N exists such that for n + 1 ≥N,
∞
X
i=n+1
Mi < ε.
This follows from our deﬁnition of convergence. Then, with |ui(x)| ≤Mi for all x in the
interval a ≤x ≤b,
∞
X
i=n+1
ui(x) < ε.
Hence S(x) = P∞
n=1 ui(x) satisﬁes
|S(x) −sn(x)| =

∞
X
i=n+1
ui(x)
 < ε,
(1.36)

1.2 Series of Functions
23
we see that P∞
n=1 ui(x) is uniformly convergent in [a,b]. Since we have speciﬁed absolute
values in the statement of the Weierstrass M test, the series P∞
n=1 ui(x) is also seen to
be absolutely convergent. As we have already observed in Example 1.2.1, absolute and
uniform convergence are different concepts, and one of the limitations of the Weierstrass
M test is that it can only establish uniform convergence for series that are also absolutely
convergent.
To further underscore the difference between absolute and uniform convergence, we
provide another example.
Example 1.2.2
UNIFORMLY CONVERGENT ALTERNATING SERIES
Consider the series
S(x) =
∞
X
n=1
(−1)n
n + x2 ,
−∞< x < ∞.
(1.37)
Applying the Leibniz criterion, this series is easily proven convergent for the entire inter-
val −∞< x < ∞, but it is not absolutely convergent, as the absolute values of its terms
approach for large n those of the divergent harmonic series. The divergence of the absolute
value series is obvious at x = 0, where we then exactly have the harmonic series. Never-
theless, this series is uniformly convergent on −∞< x < ∞, as its convergence is for all
x at least as fast as it is for x = 0. More formally,
|S(x) −sn(x)| < |un+1(x)| ≤|un+1(0)|.
Since un+1(0) is independent of x, uniform convergence is conﬁrmed.
■
Abel’s Test
A somewhat more delicate test for uniform convergence has been given by Abel. If un(x)
can be written in the form an fn(x), and
1.
The an form a convergent series, P
n an = A,
2.
For all x in [a,b] the functions fn(x) are monotonically decreasing in n, i.e., fn+1(x) ≤
fn(x),
3.
For all x in [a,b] all the f (n) are bounded in the range 0 ≤fn(x) ≤M, where M is
independent of x,
then P
n un(x) converges uniformly in [a,b].
This test is especially useful in analyzing the convergence of power series. Details of
the proof of Abel’s test and other tests for uniform convergence are given in the works
by Knopp and by Whittaker and Watson (see Additional Readings listed at the end of this
chapter).

24
Chapter 1 Mathematical Preliminaries
Properties of Uniformly Convergent Series
Uniformly convergent series have three particularly useful properties. If a series P
n un(x)
is uniformly convergent in [a,b] and the individual terms un(x) are continuous,
1.
The series sum S(x) = P∞
n=1 un(x) is also continuous.
2.
The series may be integrated term by term. The sum of the integrals is equal to the
integral of the sum:
b
Z
a
S(x) dx =
∞
X
n=1
b
Z
a
un(x) dx.
(1.38)
3.
The derivative of the series sum S(x) equals the sum of the individual-term deriva-
tives:
d
dx S(x) =
∞
X
n=1
d
dx un(x),
(1.39)
provided the following additional conditions are satisﬁed:
dun(x)
dx
is continuous in [a,b],
∞
X
n=1
dun(x)
dx
is uniformly convergent in [a,b].
Term-by-term integration of a uniformly convergent series requires only continuity of
the individual terms. This condition is almost always satisﬁed in physical applications.
Term-by-term differentiation of a series is often not valid because more restrictive condi-
tions must be satisﬁed.
Exercises
1.2.1
Find the range of uniform convergence of the series
(a)
η(x) =
∞
X
n=1
(−1)n−1
nx
,
(b)
ζ(x) =
∞
X
n=1
1
nx .
ANS.
(a) 0 < s ≤x < ∞.
(b) 1 < s ≤x < ∞.
1.2.2
For what range of x is the geometric series P∞
n=0 xn uniformly convergent?
ANS.
−1 < −s ≤x ≤s < 1.
1.2.3
For what range of positive values of x is P∞
n=0 1/(1 + xn)
(a)
convergent?
(b)
uniformly convergent?

1.2 Series of Functions
25
1.2.4
If the series of the coefﬁcients P an and P bn are absolutely convergent, show that the
Fourier series
X
(an cosnx + bn sinnx)
is uniformly convergent for −∞< x < ∞.
1.2.5
The Legendre series P
j even u j(x) satisﬁes the recurrence relations
u j+2(x) = ( j + 1)( j + 2) −l(l + 1)
( j + 2)( j + 3)
x2u j(x),
in which the index j is even and l is some constant (but, in this problem, not a non-
negative odd integer). Find the range of values of x for which this Legendre series is
convergent. Test the endpoints.
ANS.
−1 < x < 1.
1.2.6
A series solution of the Chebyshev equation leads to successive terms having the ratio
u j+2(x)
u j(x)
=
(k + j)2 −n2
(k + j + 1)(k + j + 2)x2,
with k = 0 and k = 1. Test for convergence at x = ±1.
ANS.
Convergent.
1.2.7
A series solution for the ultraspherical (Gegenbauer) function Cα
n (x) leads to the
recurrence
a j+2 = a j
(k + j)(k + j + 2α) −n(n + 2α)
(k + j + 1)(k + j + 2)
.
Investigate the convergence of each of these series at x = ±1 as a function of the
parameter α.
ANS.
Convergent for α < 1,
divergent for α ≥1.
Taylor’s Expansion
Taylor’s expansion is a powerful tool for the generation of power series representations of
functions. The derivation presented here provides not only the possibility of an expansion
into a ﬁnite number of terms plus a remainder that may or may not be easy to evaluate, but
also the possibility of the expression of a function as an inﬁnite series of powers.

26
Chapter 1 Mathematical Preliminaries
We assume that our function f (x) has a continuous nth derivative2 in the interval a ≤
x ≤b. We integrate this nth derivative n times; the ﬁrst three integrations yield
x
Z
a
f (n)(x1)dx1 = f (n−1)(x1)

x
a = f (n−1)(x) −f (n−1)(a),
x
Z
a
dx2
x2
Z
a
f (n)(x1)dx1 =
x
Z
a
dx2
h
f (n−1)(x2) −f (n−1)(a)
i
= f (n−2)(x) −f (n−2)(a) −(x −a) f (n−1)(a),
x
Z
a
dx3
x3
Z
a
dx2
x2
Z
a
f (n)(x1)dx1 = f (n−3)(x) −f (n−3)(a)
−(x −a) f (n−2)(a) −(x −a)2
2!
f (n−1)(a).
Finally, after integrating for the nth time,
x
Z
a
dxn ···
x2
Z
a
f (n)(x1)dx1 = f (x) −f (a) −(x −a) f ′(a) −(x −a)2
2!
f ′′(a)
−··· −(x −a)n−1
(n −1)!
f n−1(a).
Note that this expression is exact. No terms have been dropped, no approximations made.
Now, solving for f (x), we have
f (x) = f (a) + (x −a) f ′(a)
+ (x −a)2
2!
f ′′(a) + ··· + (x −a)n−1
(n −1)!
f (n−1)(a) + Rn,
(1.40)
where the remainder, Rn, is given by the n-fold integral
Rn =
x
Z
a
dxn ···
x2
Z
a
dx1 f (n)(x1).
(1.41)
We may convert Rn into a perhaps more practical form by using the mean value theorem
of integral calculus:
x
Z
a
g(x) dx = (x −a) g(ξ),
(1.42)
2Taylor’s expansion may be derived under slightly less restrictive conditions; compare H. Jeffreys and B. S. Jeffreys, in the
Additional Readings, Section 1.133.

1.2 Series of Functions
27
with a ≤ξ ≤x. By integrating n times we get the Lagrangian form3 of the remainder:
Rn = (x −a)n
n!
f (n)(ξ).
(1.43)
With Taylor’s expansion in this form there are no questions of inﬁnite series convergence.
The series contains a ﬁnite number of terms, and the only questions concern the magnitude
of the remainder.
When the function f (x) is such that limn→∞Rn = 0, Eq. (1.40) becomes Taylor’s
series:
f (x) = f (a) + (x −a) f ′(a) + (x −a)2
2!
f ′′(a) + ···
=
∞
X
n=0
(x −a)n
n!
f (n)(a).
(1.44)
Here we encounter for the ﬁrst time n! with n = 0. Note that we deﬁne 0! = 1.
Our Taylor series speciﬁes the value of a function at one point, x, in terms of the value
of the function and its derivatives at a reference point a. It is an expansion in powers of
the change in the variable, namely x −a. This idea can be emphasized by writing Taylor’s
series in an alternate form in which we replace x by x + h and a by x:
f (x + h) =
∞
X
n=0
hn
n! f (n)(x).
(1.45)
Power Series
Taylor series are often used in situations where the reference point, a, is assigned the
value zero. In that case the expansion is referred to as a Maclaurin series, and Eq. (1.40)
becomes
f (x) = f (0) + x f ′(0) + x2
2! f ′′(0) + ··· =
∞
X
n=0
xn
n! f (n)(0).
(1.46)
An immediate application of the Maclaurin series is in the expansion of various transcen-
dental functions into inﬁnite (power) series.
Example 1.2.3
EXPONENTIAL FUNCTION
Let f (x) = ex. Differentiating, then setting x = 0, we have
f (n)(0) = 1
for all n, n = 1, 2, 3,.... Then, with Eq. (1.46), we have
ex = 1 + x + x2
2! + x3
3! + ··· =
∞
X
n=0
xn
n! .
(1.47)
3An alternate form derived by Cauchy is Rn = (x−ξ)n−1(x−a)
(n−1)!
f (n)(ξ).

28
Chapter 1 Mathematical Preliminaries
This is the series expansion of the exponential function. Some authors use this series to
deﬁne the exponential function.
Although this series is clearly convergent for all x, as may be veriﬁed using the
d’Alembert ratio test, it is instructive to check the remainder term, Rn. By Eq. (1.43) we
have
Rn = xn
n! f (n)(ξ) = xn
n! eξ,
where ξ is between 0 and x. Irrespective of the sign of x,
|Rn| ≤|x|ne|x|
n!
.
No matter how large |x| may be, a sufﬁcient increase in n will cause the denominator of
this form for Rn to dominate over the numerator, and limn→∞Rn = 0. Thus, the Maclaurin
expansion of ex converges absolutely over the entire range −∞< x < ∞.
■
Now that we have an expansion for exp(x), we can return to Eq. (1.45), and rewrite that
equation in a form that focuses on its differential operator characteristics. Deﬁning D as
the operator d/dx, we have
f (x + h) =
∞
X
n=0
hnDn
n!
f (x) = ehD f (x).
(1.48)
Example 1.2.4
LOGARITHM
For a second Maclaurin expansion, let f (x) = ln(1 + x). By differentiating, we obtain
f ′(x) = (1 + x)−1,
f (n)(x) = (−1)n−1 (n −1)! (1 + x)−n.
(1.49)
Equation (1.46) yields
ln(1 + x) = x −x2
2 + x3
3 −x4
4 + ··· + Rn
=
n
X
p=1
(−1)p−1 x p
p + Rn.
(1.50)
In this case, for x > 0 our remainder is given by
Rn = xn
n! f (n)(ξ),
0 ≤ξ ≤x
≤xn
n ,
0 ≤ξ ≤x ≤1.
(1.51)
This result shows that the remainder approaches zero as n is increased indeﬁnitely, pro-
viding that 0 ≤x ≤1. For x < 0, the mean value theorem is too crude a tool to establish a

1.2 Series of Functions
29
meaningful limit for Rn. As an inﬁnite series,
ln(1 + x) =
∞
X
n=1
(−1)n−1 xn
n
(1.52)
converges for −1 < x ≤1. The range −1 < x < 1 is easily established by the d’Alembert
ratio test. Convergence at x = 1 follows by the Leibniz criterion. In particular, at x = 1 we
have the conditionally convergent alternating harmonic series, to which we can now put a
value:
ln2 = 1 −1
2 + 1
3 −1
4 + 1
5 −··· =
∞
X
n=1
(−1)n−1 n−1.
(1.53)
At x = −1, the expansion becomes the harmonic series, which we well know to be
divergent.
■
Properties of Power Series
The power series is a special and extremely useful type of inﬁnite series, and as illustrated
in the preceding subsection, may be constructed by the Maclaurin formula, Eq. (1.44).
However obtained, it will be of the general form
f (x) = a0 + a1x + a2x2 + a3x3 + ··· =
∞
X
n=0
anxn,
(1.54)
where the coefﬁcients ai are constants, independent of x.
Equation (1.54) may readily be tested for convergence either by the Cauchy root test or
the d’Alembert ratio test. If
lim
n→∞

an+1
an
 = R−1,
the series converges for −R < x < R. This is the interval or radius of convergence. Since
the root and ratio tests fail when x is at the limit points ±R, these points require special
attention.
For instance, if an = n−1, then R = 1 and from Section 1.1 we can conclude that the
series converges for x = −1 but diverges for x = +1. If an = n!, then R = 0 and the series
diverges for all x ̸= 0.
Suppose our power series has been found convergent for −R < x < R; then it will be
uniformly and absolutely convergent in any interior interval −S ≤x ≤S, where 0 < S <
R. This may be proved directly by the Weierstrass M test.
Since each of the terms un(x) = anxn is a continuous function of x and f (x) = Panxn
converges uniformly for −S ≤x ≤S, f (x) must be a continuous function in the inter-
val of uniform convergence. This behavior is to be contrasted with the strikingly different
behavior of series in trigonometric functions, which are used frequently to represent dis-
continuous functions such as sawtooth and square waves.

30
Chapter 1 Mathematical Preliminaries
With un(x) continuous and Panxn uniformly convergent, we ﬁnd that term by term dif-
ferentiation or integration of a power series will yield a new power series with continuous
functions and the same radius of convergence as the original series. The new factors in-
troduced by differentiation or integration do not affect either the root or the ratio test.
Therefore our power series may be differentiated or integrated as often as desired within
the interval of uniform convergence (Exercise 1.2.16). In view of the rather severe restric-
tion placed on differentiation of inﬁnite series in general, this is a remarkable and valuable
result.
Uniqueness Theorem
We have already used the Maclaurin series to expand ex and ln(1 + x) into power series.
Throughout this book, we will encounter many situations in which functions are repre-
sented, or even deﬁned by power series. We now establish that the power-series represen-
tation is unique.
We proceed by assuming we have two expansions of the same function whose intervals
of convergence overlap in a region that includes the origin:
f (x) =
∞
X
n=0
an xn,
−Ra < x < Ra
=
∞
X
n=0
bn xn,
−Rb < x < Rb.
(1.55)
What we need to prove is that an = bn for all n.
Starting from
∞
X
n=0
an xn =
∞
X
n=0
bn xn,
−R < x < R,
(1.56)
where R is the smaller of Ra and Rb, we set x = 0 to eliminate all but the constant term of
each series, obtaining
a0 = b0.
Now, exploiting the differentiability of our power series, we differentiate Eq. (1.56),
getting
∞
X
n=1
nan xn−1 =
∞
X
n=1
nbn xn−1.
(1.57)
We again set x = 0, to isolate the new constant terms, and ﬁnd
a1 = b1.
By repeating this process n times, we get
an = bn,

1.2 Series of Functions
31
which shows that the two series coincide. Therefore our power series representation is
unique.
This theorem will be a crucial point in our study of differential equations, in which
we develop power series solutions. The uniqueness of power series appears frequently in
theoretical physics. The establishment of perturbation theory in quantum mechanics is one
example.
Indeterminate Forms
The power-series representation of functions is often useful in evaluating indeterminate
forms, and is the basis of l’Hôpital’s rule, which states that if the ratio of two differentiable
functions f (x) and g(x) becomes indeterminate, of the form 0/0, at x = x0, then
lim
x→x0
f (x)
g(x) = lim
x→x0
f ′(x)
g′(x) .
(1.58)
Proof of Eq. (1.58) is the subject of Exercise 1.2.12.
Sometimes it is easier just to introduce power-series expansions than to evaluate the
derivatives that enter l’Hôpital’s rule. For examples of this strategy, see the following
Example and Exercise 1.2.15.
Example 1.2.5
ALTERNATIVE TO L’HÔPITAL’S RULE
Evaluate
lim
x→0
1 −cos x
x2
.
(1.59)
Replacing cos x by its Maclaurin-series expansion, Exercise 1.2.8, we obtain
1 −cos x
x2
= 1 −(1 −1
2!x2 + 1
4!x4 −···)
x2
= 1
2! −x2
4! + ··· .
Letting x →0, we have
lim
x→0
1 −cos x
x2
= 1
2.
(1.60)
■
The uniqueness of power series means that the coefﬁcients an may be identiﬁed with the
derivatives in a Maclaurin series. From
f (x) =
∞
X
n=0
an xn =
∞
X
m=0
1
n! f (n)(0) xn,
we have
an = 1
n! f (n)(0).

32
Chapter 1 Mathematical Preliminaries
Inversion of Power Series
Suppose we are given a series
y −y0 = a1(x −x0) + a2(x −x0)2 + ··· =
∞
X
n=1
an (x −x0)n.
(1.61)
This gives (y −y0) in terms of (x −x0). However, it may be desirable to have an explicit
expression for (x −x0) in terms of (y −y0). That is, we want an expression of the form
x −x0 =
∞
X
n=1
bn (y −y0)n,
(1.62)
with the bn to be determined in terms of the assumed known an. A brute-force approach,
which is perfectly adequate for the ﬁrst few coefﬁcients, is simply to substitute Eq. (1.61)
into Eq. (1.62). By equating coefﬁcients of (x −x0)n on both sides of Eq. (1.62), and using
the fact that the power series is unique, we ﬁnd
b1 = 1
a1
,
b2 = −a2
a3
1
,
b3 = 1
a5
1

2a2
2 −a1a3

,
b4 = 1
a7
1

5a1a2a3 −a2
1a4 −5a3
2

, and so on.
(1.63)
Some of the higher coefﬁcients are listed by Dwight.4 A more general and much more
elegant approach is developed by the use of complex variables in the ﬁrst and second
editions of Mathematical Methods for Physicists.
Exercises
1.2.8
Show that
(a)
sin x =
∞
X
n=0
(−1)n
x2n+1
(2n + 1)!,
(b)
cos x =
∞
X
n=0
(−1)n x2n
(2n)!.
4H. B. Dwight, Tables of Integrals and Other Mathematical Data, 4th ed. New York: Macmillan (1961). (Compare formula
no. 50.)

1.3 Binomial Theorem
33
1.2.9
Derive a series expansion of cot x in increasing powers of x by dividing the power
series for cos x by that for sin x.
Note. The resultant series that starts with 1/x is known as a Laurent series (cot x does
not have a Taylor expansion about x = 0, although cot(x) −x−1 does). Although the
two series for sin x and cos x were valid for all x, the convergence of the series for cot x
is limited by the zeros of the denominator, sin x.
1.2.10
Show by series expansion that
1
2 ln η0 + 1
η0 −1 = coth−1 η0,
|η0| > 1.
This identity may be used to obtain a second solution for Legendre’s equation.
1.2.11
Show that f (x) = x1/2 (a) has no Maclaurin expansion but (b) has a Taylor expansion
about any point x0 ̸= 0. Find the range of convergence of the Taylor expansion about
x = x0.
1.2.12
Prove l’Hôpital’s rule, Eq. (1.58).
1.2.13
With n > 1, show that
(a)
1
n −ln

n
n −1

< 0,
(b)
1
n −ln
n + 1
n

> 0.
Use these inequalities to show that the limit deﬁning the Euler-Mascheroni constant,
Eq. (1.13), is ﬁnite.
1.2.14
In numerical analysis it is often convenient to approximate d2ψ(x)/dx2 by
d2
dx2 ψ(x) ≈1
h2 [ψ(x + h) −2ψ(x) + ψ(x −h)].
Find the error in this approximation.
ANS.
Error = h2
12ψ(4)(x).
1.2.15
Evaluate lim
x→0
sin(tan x) −tan(sin x)
x7

.
ANS.
−1
30.
1.2.16
A power series converges for −R < x < R. Show that the differentiated series and
the integrated series have the same interval of convergence. (Do not bother about the
endpoints x = ±R.)
1.3
BINOMIAL THEOREM
An extremely important application of the Maclaurin expansion is the derivation of the
binomial theorem.

34
Chapter 1 Mathematical Preliminaries
Let f (x) = (1 + x)m, in which m may be either positive or negative and is not limited
to integral values. Direct application of Eq. (1.46) gives
(1 + x)m = 1 + mx + m(m −1)
2!
x2 + ··· + Rn.
(1.64)
For this function the remainder is
Rn = xn
n! (1 + ξ)m−n m(m −1)···(m −n + 1),
(1.65)
with ξ between 0 and x. Restricting attention for now to x ≥0, we note that for n > m,
(1 + ξ)m−n is a maximum for ξ = 0, so for positive x,
|Rn| ≤xn
n! |m(m −1)···(m −n + 1)|,
(1.66)
with limn→∞Rn = 0 when 0 ≤x < 1. Because the radius of convergence of a power series
is the same for positive and for negative x, the binomial series converges for −1 < x < 1.
Convergence at the limit points ±1 is not addressed by the present analysis, and depends
on m.
Summarizing, we have established the binomial expansion,
(1 + x)m = 1 + mx + m(m −1)
2!
x2 + m(m −1)(m −2)
3!
x3 + ··· ,
(1.67)
convergent for −1 < x < 1. It is important to note that Eq. (1.67) applies whether or not
m is integral, and for both positive and negative m. If m is a nonnegative integer, Rn for
n > m vanishes for all x, corresponding to the fact that under those conditions (1 + x)m is
a ﬁnite sum.
Because the binomial expansion is of frequent occurrence, the coefﬁcients appearing in
it, which are called binomial coefﬁcients, are given the special symbol
m
n

= m(m −1)···(m −n + 1)
n!
,
(1.68)
and the binomial expansion assumes the general form
(1 + x)m =
∞
X
n=0
m
n

xn.
(1.69)
In evaluating Eq. (1.68), note that when n = 0, the product in its numerator is empty (start-
ing from m and descending to m + 1); in that case the convention is to assign the product
the value unity. We also remind the reader that 0! is deﬁned to be unity.
In the special case that m is a positive integer, we may write our binomial coefﬁcient in
terms of factorials:
m
n

=
m!
n! (m −n)!.
(1.70)
Since n! is undeﬁned for negative integer n, the binomial expansion for positive integer
m is understood to end with the term n = m, and will correspond to the coefﬁcients in the
polynomial resulting from the (ﬁnite) expansion of (1 + x)m.

1.3 Binomial Theorem
35
For positive integer m, the
 m
n

also arise in combinatorial theory, being the number
of different ways n out of m objects can be selected. That, of course, is consistent with
the coefﬁcient set if (1 + x)m is expanded. The term containing xn has a coefﬁcient that
corresponds to the number of ways one can choose the “x” from n of the factors (1 + x)
and the 1 from the m −n other (1 + x) factors.
For negative integer m, we can still use the special notation for binomial coefﬁcients, but
their evaluation is more easily accomplished if we set m = −p, with p a positive integer,
and write
−p
n

= (−1)n p(p + 1)···(p + n −1)
n!
= (−1)n (p + n −1)!
n! (p −1)!
.
(1.71)
For nonintegral m, it is convenient to use the Pochhammer symbol, deﬁned for general
a and nonnegative integer n and given the notation (a)n, as
(a)0 = 1, (a)1 = a, (a)n+1 = a(a + 1)···(a + n), (n ≥1).
(1.72)
For both integral and nonintegral m, the binomial coefﬁcient formula can be written
m
n

= (m −n + 1)n
n!
.
(1.73)
There is a rich literature on binomial coefﬁcients and relationships between them and
on summations involving them. We mention here only one such formula that arises if we
evaluate 1/√1 + x, i.e., (1 + x)−1/2. The binomial coefﬁcient
−1
2
n

= 1
n!

−1
2

−3
2

···

−2n −1
2

= (−1)n 1 · 3···(2n −1)
2n n!
= (−1)n (2n −1)!!
(2n)!!
,
(1.74)
where the “double factorial” notation indicates products of even or odd positive integers
as follows:
1 · 3 · 5···(2n −1) = (2n −1)!!
2 · 4 · 6···(2n) = (2n)!!.
(1.75)
These are related to the regular factorials by
(2n)!! = 2n n!
and
(2n −1)!! = (2n)!
2n n! .
(1.76)
Note that these relations include the special cases 0!! = (−1)!! = 1.
Example 1.3.1
RELATIVISTIC ENERGY
The total relativistic energy of a particle of mass m and velocity v is
E = mc2

1 −v2
c2
−1/2
,
(1.77)

36
Chapter 1 Mathematical Preliminaries
where c is the velocity of light. Using Eq. (1.69) with m = −1/2 and x = −v2/c2, and
evaluating the binomial coefﬁcients using Eq. (1.74), we have
E = mc2
"
1 −1
2

−v2
c2

+ 3
8

−v2
c2
2
−5
16

−v2
c2
3
+ ···
#
= mc2 + 1
2mv2 + 3
8mv2
v2
c2

+ 5
16mv2

−v2
c2
2
+ ··· .
(1.78)
The ﬁrst term, mc2, is identiﬁed as the rest-mass energy. Then
Ekinetic = 1
2mv2
"
1 + 3
4
v2
c2 + 5
8

−v2
c2
2
+ ···
#
.
(1.79)
For particle velocity v ≪c, the expression in the brackets reduces to unity and we see that
the kinetic portion of the total relativistic energy agrees with the classical result.
■
The binomial expansion can be generalized for positive integer n to polynomials:
(a1 + a2 + ··· + am)n =
X
n!
n1!n2!···nm!an1
1 an2
2 ···anm
m ,
(1.80)
where the summation includes all different combinations of nonnegative integers
n1,n2, ..., nm with Pm
i=1 ni = n. This generalization ﬁnds considerable use in statisti-
cal mechanics.
In everyday analysis, the combinatorial properties of the binomial coefﬁcients make
them appear often. For example, Leibniz’s formula for the nth derivative of a product of
two functions, u(x)v(x), can be written
 d
dx
n 
u(x)v(x)

=
n
X
i=0
n
i
di u(x)
dxi
dn−i v(x)
dxn−i

.
(1.81)
Exercises
1.3.1
The classical Langevin theory of paramagnetism leads to an expression for the magnetic
polarization,
P(x) = c
cosh x
sinh x −1
x

.
Expand P(x) as a power series for small x (low ﬁelds, high temperature).
1.3.2
Given that
1
Z
0
dx
1 + x2 = tan−1 x

1
0
= π
4 ,

1.3 Binomial Theorem
37
expand the integrand into a series and integrate term by term obtaining5
π
4 = 1 −1
3 + 1
5 −1
7 + 1
9 −··· + (−1)n
1
2n + 1 + ··· ,
which is Leibniz’s formula for π. Compare the convergence of the integrand series and
the integrated series at x = 1. Leibniz’s formula converges so slowly that it is quite
useless for numerical work.
1.3.3
Expand the incomplete gamma function γ (n + 1, x) ≡
x
Z
0
e−ttndt in a series of powers
of x. What is the range of convergence of the resulting series?
ANS.
x
Z
0
e−ttndt = xn+1

1
n + 1 −
x
n + 2 +
x2
2!(n + 3)
−···
(−1)px p
p!(n + p + 1) + ···

.
1.3.4
Develop a series expansion of y = sinh−1 x (that is, sinh y = x) in powers of x by
(a)
inversion of the series for sinh y,
(b)
a direct Maclaurin expansion.
1.3.5
Show that for integral n ≥0,
1
(1 −x)n+1 =
∞
X
m=n
m
n

xm−n.
1.3.6
Show that (1 + x)−m/2 =
∞
X
n=0
(−1)n (m + 2n −2)!!
2nn!(m −2)!! xn, for m = 1, 2, 3, ... .
1.3.7
Using binomial expansions, compare the three Doppler shift formulas:
(a)
ν′ = ν

1 ∓v
c
−1
moving source;
(b)
ν′ = ν

1 ± v
c

moving observer;
(c)
ν′ = ν

1 ± v
c

1 −v2
c2
−1/2
relativistic.
Note. The relativistic formula agrees with the classical formulas if terms of order v2/c2
can be neglected.
1.3.8
In the theory of general relativity there are various ways of relating (deﬁning) a velocity
of recession of a galaxy to its red shift, δ. Milne’s model (kinematic relativity) gives
5The series expansion of tan−1 x (upper limit 1 replaced by x) was discovered by James Gregory in 1671, 3 years before
Leibniz. See Peter Beckmann’s entertaining book, A History of Pi, 2nd ed., Boulder, CO: Golem Press (1971), and L. Berggren,
J. Borwein, and P. Borwein, Pi: A Source Book, New York: Springer (1997).

38
Chapter 1 Mathematical Preliminaries
(a)
v1 = cδ

1 + 1
2δ

,
(b)
v2 = cδ

1 + 1
2δ

(1 + δ)−2,
(c)
1 + δ =
1 + v3/c
1 −v3/c
1/2
.
1.
Show that for δ ≪1 (and v3/c ≪1), all three formulas reduce to v = cδ.
2.
Compare the three velocities through terms of order δ2.
Note. In special relativity (with δ replaced by z), the ratio of observed wavelength λ to
emitted wavelength λ0 is given by
λ
λ0
= 1 + z =
c + v
c −v
1/2
.
1.3.9
The relativistic sum w of two velocities u and v in the same direction is given by
w
c = u/c + v/c
1 + uv/c2 .
If
v
c = u
c = 1 −α,
where 0 ≤α ≤1, ﬁnd w/c in powers of α through terms in α3.
1.3.10
The displacement x of a particle of rest mass m0, resulting from a constant force m0g
along the x-axis, is
x = c2
g



"
1 +

g t
c
2#1/2
−1


,
including relativistic effects. Find the displacement x as a power series in time t.
Compare with the classical result,
x = 1
2gt2.
1.3.11
By use of Dirac’s relativistic theory, the ﬁne structure formula of atomic spectroscopy
is given by
E = mc2

1 +
γ 2
(s + n −|k|)2
−1/2
,
where
s = (|k|2 −γ 2)1/2,
k = ±1,±2,±3, ... .
Expand in powers of γ 2 through order γ 4 (γ 2 = Ze2/4πϵ0 ¯hc, with Z the atomic num-
ber). This expansion is useful in comparing the predictions of the Dirac electron theory
with those of a relativistic Schrödinger electron theory. Experimental results support
the Dirac theory.

1.3 Binomial Theorem
39
1.3.12
In a head-on proton-proton collision, the ratio of the kinetic energy in the center of mass
system to the incident kinetic energy is
R = [
p
2mc2(Ek + 2mc2) −2mc2]/Ek.
Find the value of this ratio of kinetic energies for
(a)
Ek ≪mc2 (nonrelativistic),
(b)
Ek ≫mc2 (extreme-relativistic).
ANS.
(a)
1
2,
(b)
0. The latter answer is a sort of law of diminish-
ing returns for high-energy particle accelerators
(with stationary targets).
1.3.13
With binomial expansions
x
1 −x =
∞
X
n=1
xn,
x
x −1 =
1
1 −x−1 =
∞
X
n=0
x−n.
Adding these two series yields P∞
n=−∞xn = 0.
Hopefully, we can agree that this is nonsense, but what has gone wrong?
1.3.14
(a)
Planck’s theory of quantized oscillators leads to an average energy
⟨ε⟩=
∞
P
n=1
nε0 exp(−nε0/kT )
∞
P
n=0
exp(−nε0/kT )
,
where ε0 is a ﬁxed energy. Identify the numerator and denominator as binomial
expansions and show that the ratio is
⟨ε⟩=
ε0
exp(ε0/kT ) −1.
(b)
Show that the ⟨ε⟩of part (a) reduces to kT, the classical result, for kT ≫ε0.
1.3.15
Expand by the binomial theorem and integrate term by term to obtain the Gregory series
for y = tan−1 x (note tan y = x):
tan−1 x =
x
Z
0
dt
1 + t2 =
x
Z
0
{1 −t2 + t4 −t6 + ···}dt
=
∞
X
n=0
(−1)n x2n+1
2n + 1,
−1 ≤x ≤1.
1.3.16
The Klein-Nishina formula for the scattering of photons by electrons contains a term of
the form
f (ε) = (1 + ε)
ε2
2 + 2ε
1 + 2ε −ln(1 + 2ε)
ε

.

40
Chapter 1 Mathematical Preliminaries
Here ε = hν/mc2, the ratio of the photon energy to the electron rest mass energy. Find
lim
ε→0 f (ε).
ANS.
4
3.
1.3.17
The behavior of a neutron losing energy by colliding elastically with nuclei of mass A
is described by a parameter ξ1,
ξ1 = 1 + (A −1)2
2A
ln A −1
A + 1.
An approximation, good for large A, is
ξ2 =
2
A + 2
3
.
Expand ξ1 and ξ2 in powers of A−1. Show that ξ2 agrees with ξ1 through (A−1)2. Find
the difference in the coefﬁcients of the (A−1)3 term.
1.3.18
Show that each of these two integrals equals Catalan’s constant:
(a)
1
Z
0
arctan t dt
t ,
(b)
−
1
Z
0
ln x
dx
1 + x2 .
Note. The deﬁnition and numerical computation of Catalan’s constant was addressed
in Exercise 1.1.12.
1.4
MATHEMATICAL INDUCTION
We are occasionally faced with the need to establish a relation which is valid for a set of
integer values, in situations where it may not initially be obvious how to proceed. However,
it may be possible to show that if the relation is valid for an arbitrary value of some index n,
then it is also valid if n is replaced by n + 1. If we can also show that the relation is
unconditionally satisﬁed for some initial value n0, we may then conclude (unconditionally)
that the relation is also satisﬁed for n0 + 1, n0 + 2, ... . This method of proof is known
as mathematical induction. It is ordinarily most useful when we know (or suspect) the
validity of a relation, but lack a more direct method of proof.
Example 1.4.1
SUM OF INTEGERS
The sum of the integers from 1 through n, here denoted S(n), is given by the formula
S(n) = n(n + 1)/2. An inductive proof of this formula proceeds as follows:
1.
Given the formula for S(n), we calculate
S(n +1) = S(n)+(n +1) = n(n + 1)
2
+(n +1) =
hn
2 + 1
i
(n +1) = (n + 1)(n + 2)
2
.
Thus, given S(n), we can establish the validity of S(n + 1).

1.5 Operations on Series Expansions of Functions
41
2.
It is obvious that S(1) = 1(2)/2 = 1, so our formula for S(n) is valid for n = 1.
3.
The formula for S(n) is therefore valid for all integers n ≥1.
■
Exercises
1.4.1
Show that
n
X
j=1
j4 = n
30(2n + 1)(n + 1)(3n2 + 3n −1).
1.4.2
Prove the Leibniz formula for the repeated differentiation of a product:
 d
dx
n 
f (x)g(x)

=
n
X
j=0
n
j
" d
dx
 j
f (x)
#" d
dx
n−j
g(x)
#
.
1.5
OPERATIONS ON SERIES EXPANSIONS OF
FUNCTIONS
There are a number of manipulations (tricks) that can be used to obtain series that represent
a function or to manipulate such series to improve convergence. In addition to the proce-
dures introduced in Section 1.1, there are others that to varying degrees make use of the
fact that the expansion depends on a variable. A simple example of this is the expansion
of f (x) = ln(1 + x), which we obtained in 1.2.4 by direct use of the Maclaurin expansion
and evaluation of the derivatives of f (x). An even easier way to obtain this series would
have been to integrate the power series for 1/(1 + x) term by term from 0 to x:
1
1 + x = 1 −x + x2 −x3 + ···
=⇒
ln(1 + x) = x −x2
2 + x3
3 −x4
4 + ··· .
A problem requiring somewhat more deviousness is given by the following example, in
which we use the binomial theorem on a series that represents the derivative of the function
whose expansion is sought.
Example 1.5.1
APPLICATION OF BINOMIAL EXPANSION
Sometimes the binomial expansion provides a convenient indirect route to the Maclaurin
series when direct methods are difﬁcult. We consider here the power series expansion
sin−1 x =
∞
X
n=0
(2n −1)!!
(2n)!!
x2n+1
(2n + 1) = x + x3
6 + 3x5
40 + ··· .
(1.82)
Starting from sin y = x, we ﬁnd dy/dx = 1/
√
1 −x2, and write the integral
sin−1 x = y =
x
Z
0
dt
(1 −t2)1/2 .

42
Chapter 1 Mathematical Preliminaries
We now introduce the binomial expansion of (1 −t2)−1/2 and integrate term by term. The
result is Eq. (1.82).
■
Another way of improving the convergence of a series is to multiply it by a polynomial in
the variable, choosing the polynomial’s coefﬁcients to remove the least rapidly convergent
part of the resulting series. Here is a simple example of this.
Example 1.5.2
MULTIPLY SERIES BY POLYNOMIAL
Returning to the series for ln(1 + x), we form
(1 + a1x)ln(1 + x) =
∞
X
n=1
(−1)n−1 xn
n + a1
∞
X
n=1
(−1)n−1 xn+1
n
= x +
∞
X
n=2
(−1)n−1
1
n −
a1
n −1

xn
= x +
∞
X
n=2
(−1)n−1 n(1 −a1) −1
n(n −1)
xn.
If we take a1 = 1, the n in the numerator disappears and our combined series converges as
n−2; the resulting series for ln(1 + x) is
ln(1 + x) =

x
1 + x
 
1 −
∞
X
n=1
(−1)n
n(n + 1) xn
!
.
■
Another useful trick is to employ partial fraction expansions, which may convert a
seemingly difﬁcult series into others about which more may be known.
If g(x) and h(x) are polynomials in x, with g(x) of lower degree than h(x), and h(x)
has the factorization h(x) = (x −a1)(x −a2)...(x −an), in the case that the factors of
h(x) are distinct (i.e., h has no multiple roots), then g(x)/h(x) can be written in the form
g(x)
h(x) =
c1
x −a1
+
c2
x −a2
+ ··· +
cn
x −an
.
(1.83)
If we wish to leave one or more quadratic factors in h(x), perhaps to avoid the introduction
of imaginary quantities, the corresponding partial-fraction term will be of the form
ax + b
x2 + px + q .
If h(x) has repeated linear factors, such as (x −a1)m, the partial fraction expansion for this
power of x −a1 takes the form
c1,m
(x −a1)m +
c1,m−1
(x −a1)m−1 + ··· +
c1,1
x −a1
.

1.5 Operations on Series Expansions of Functions
43
The coefﬁcients in partial fraction expansions are usually found easily; sometimes it is
useful to express them as limits, such as
ci = lim
x→ai(x −ai)g(x)/h(x).
(1.84)
Example 1.5.3
PARTIAL FRACTION EXPANSION
Let
f (x) =
k2
x(x2 + k2) = c
x + ax + b
x2 + k2 .
We have written the form of the partial fraction expansion, but have not yet determined the
values of a, b, and c. Putting the right side of the equation over a common denominator,
we have
k2
x(x2 + k2) = c(x2 + k2) + x(ax + b)
x(x2 + k2)
.
Expanding the right-side numerator and equating it to the left-side numerator, we get
0(x2) + 0(x) + k2 = (c + a)x2 + bx + ck2,
which we solve by requiring the coefﬁcient of each power of x to have the same value
on both sides of this equation. We get b = 0, c = 1, and then a = −1. The ﬁnal result is
therefore
f (x) = 1
x −
x
x2 + k2 .
(1.85)
■
Still more cleverness is illustrated by the following procedure, due to Euler, for changing
the expansion variable so as to improve the range over which an expansion converges.
Euler’s transformation, the proof of which (with hints) is deferred to Exercise 1.5.4, makes
the conversion:
f (x) =
∞
X
n=0
(−1)ncnxn
(1.86)
=
1
1 + x
∞
X
n=0
(−1)nan

x
1 + x
n
.
(1.87)
The coefﬁcients an are repeated differences of the cn:
a0 = c0,
a1 = c1 −c0,
a2 = c2 −2c1 + c0,
a3 = c3 −3c2 + 3c1 −c0, ...;
their general formula is
an =
n
X
j=0
(−1) j
n
j

cn−j.
(1.88)
The series to which the Euler transformation is applied need not be alternating. The coef-
ﬁcients cn can have a sign factor which cancels that in the deﬁnition.

44
Chapter 1 Mathematical Preliminaries
Example 1.5.4
EULER TRANSFORMATION
The Maclaurin series for ln(1+ x) converges extremely slowly, with convergence only for
|x| < 1. We consider the Euler transformation on the related series
ln(1 + x)
x
= 1 −x
2 + x2
3 −··· ,
(1.89)
so, in Eq. (1.86), cn = 1/(n + 1). The ﬁrst few an are: a0 = 1, a1 = 1
2 −1 = −1
2, a2 =
1
3 −2
  1
2

+ 1 = 1
3, a3 = 1
4 −3
  1
3

+ 3
  1
2

−1 = −1
4, or in general
an = (−1)n
n + 1 .
The converted series is then
ln(1 + x)
x
=
1
1 + x
"
1 + 1
2

x
1 + x

+ 1
3

x
1 + x
2
+ ···
#
,
which rearranges to
ln(1 + x) =

x
1 + x

+ 1
2

x
1 + x
2
+ 1
3

x
1 + x
3
+ ··· .
(1.90)
This new series converges nicely at x = 1, and in fact is convergent for all x < ∞.
■
Exercises
1.5.1
Using a partial fraction expansion, show that for 0 < x < 1,
x
Z
−x
dt
1 −t2 = ln
1 + x
1 −x

.
1.5.2
Prove the partial fraction expansion
1
n(n + 1)···(n + p)
= 1
p!
p
0
1
n −
p
1

1
n + 1 +
p
2

1
n + 2 −··· + (−1)p
p
p

1
n + p

,
where p is a positive integer.
Hint. Use mathematical induction. Two binomial coefﬁcient formulas of use here are
p + 1
p + 1 −j
p
j

=
p + 1
j

,
p+1
X
j=1
(−1) j−1
p + 1
j

= 1.

1.6 Some Important Series
45
1.5.3
The formula for αp, Eq. (1.26), is a summation of the form P∞
n=1 un(p), with
un(p) =
1
n(n + 1)···(n + p).
Applying a partial fraction decomposition to the ﬁrst and last factors of the denominator,
i.e.,
1
n(n + p) = 1
p
1
n −
1
n + p

,
show that un(p) = un(p−1)−un+1(p−1)
p
and that P∞
n=1 un(p) =
1
p p!.
Hint. It is useful to note that u1(p −1) = 1/p!.
1.5.4
Proof of Euler transformation: By substituting Eq. (1.88) into Eq. (1.87), verify that
Eq. (1.86) is recovered.
Hint. It may help to rearrange the resultant double series so that both indices are summed
on the range (0,∞). Then the summation not containing the coefﬁcients c j can be
recognized as a binomial expansion.
1.5.5
Carry out the Euler transformation on the series for arctan(x):
arctan(x) = x −x3
3 + x5
5 −x7
7 + x9
9 −··· .
Check your work by computing arctan(1) = π/4 and arctan(3−1/2) = π/6.
1.6
SOME IMPORTANT SERIES
There are a few series that arise so often that all physicists should recognize them. Here is
a short list that is worth committing to memory.
exp(x) =
∞
X
n=0
xn
n! = 1 + x + x2
2! + x3
3! + x4
4! + ··· ,
−∞< x < ∞,
(1.91)
sin(x) =
∞
X
n=0
(−1)nx2n+1
(2n + 1)!
= x −x3
3! + x5
5! −x7
7! + ··· ,
−∞< x < ∞,
(1.92)
cos(x) =
∞
X
n=0
(−1)nx2n
(2n)!
= 1 −x2
2! + x4
4! −x6
6! + ··· ,
−∞< x < ∞,
(1.93)
sinh(x) =
∞
X
n=0
x2n+1
(2n + 1)! = x + x3
3! + x5
5! + x7
7! + ··· ,
−∞< x < ∞,
(1.94)
cosh(x) =
∞
X
n=0
x2n
(2n)! = 1 + x2
2! + x4
4! + x6
6! + ··· ,
−∞< x < ∞,
(1.95)

46
Chapter 1 Mathematical Preliminaries
1
1 −x =
∞
X
n=0
xn = 1 + x + x2 + x3 + x4 + ··· ,
−1 ≤x < 1,
(1.96)
ln(1 + x) =
∞
X
n=1
(−1)n−1xn
n
= x −x2
2 + x3
3 −x4
4 + ··· , −1 < x ≤1,
(1.97)
(1 + x)p =
∞
X
n=0
p
n

xn =
∞
X
n=0
(p −n + 1)n
n!
xn,
−1 < x < 1.
(1.98)
Reminder. The notation (a)n is the Pochhammer symbol: (a)0 = 1, (a)1 = a, and for inte-
gers n > 1, (a)n = a(a + 1)···(a + n −1). It is not required that a, or p in Eq. (1.98), be
positive or integral.
Exercises
1.6.1
Show that ln
1 + x
1 −x

= 2

x + x3
3 + x5
5 + ···

,
−1 < x < 1.
1.7
VECTORS
In science and engineering we frequently encounter quantities that have algebraic magni-
tude only (i.e., magnitude and possibly a sign): mass, time, and temperature. These we label
scalar quantities, which remain the same no matter what coordinates we may use. In con-
trast, many interesting physical quantities have magnitude and, in addition, an associated
direction. This second group includes displacement, velocity, acceleration, force, momen-
tum, and angular momentum. Quantities with magnitude and direction are labeled vector
quantities. To distinguish vectors from scalars, we usually identify vector quantities with
boldface type, as in V or x.
This section deals only with properties of vectors that are not speciﬁc to three-
dimensional (3-D) space (thereby excluding the notion of the vector cross product and
the use of vectors to describe rotational motion). We also restrict the present discussion to
vectors that describe a physical quantity at a single point, in contrast to the situation where
a vector is deﬁned over an extended region, with its magnitude and/or direction a function
of the position with which it is associated. Vectors deﬁned over a region are called vector
ﬁelds; a familiar example is the electric ﬁeld, which describes the direction and magnitude
of the electrical force on a test charge throughout a region of space. We return to these
important topics in a later chapter.
The key items of the present discussion are (1) geometric and algebraic descriptions of
vectors; (2) linear combinations of vectors; and (3) the dot product of two vectors and its
use in determining the angle between their directions and the decomposition of a vector
into contributions in the coordinate directions.

1.7 Vectors
47
Basic Properties
We deﬁne a vector in a way that makes it correspond to an arrow from a starting point to
another point in two-dimensional (2-D) or 3-D space, with vector addition identiﬁed as
the result of placing the tail (starting point) of a second vector at the head (endpoint) of the
ﬁrst vector, as shown in Fig. 1.7. As seen in the ﬁgure, the result of addition is the same if
the vectors are added in either order; vector addition is a commutative operation. Vector
addition is also associative; if we add three vectors, the result is independent of the order
in which the additions take place. Formally, this means
(A + B) + C = A + (B + C).
It is also useful to deﬁne an operation in which a vector A is multiplied by an ordinary
number k (a scalar). The result will be a vector that is still in the original direction, but
with its length multiplied by k. If k is negative, the vector’s length is multiplied by |k| but
its direction is reversed. This means we can interpret subtraction as illustrated here:
A −B ≡A + (−1)B,
and we can form polynomials such as A + 2B −3C.
Up to this point we are describing our vectors as quantities that do not depend on any
coordinate system that we may wish to use, and we are focusing on their geometric prop-
erties. For example, consider the principle of mechanics that an object will remain in static
equilibrium if the vector sum of the forces on it is zero. The net force at the point O of
Fig. 1.8 will be the vector sum of the forces labeled F1, F2, and F3. The sum of the forces
at static equilibrium is illustrated in the right-hand panel of the ﬁgure.
It is also important to develop an algebraic description for vectors. We can do so by
placing a vector A so that its tail is at the origin of a Cartesian coordinate system and by
noting the coordinates of its head. Giving these coordinates (in 3-D space) the names Ax,
Ay, Az, we have a component description of A. From these components we can use the
Pythagorean theorem to compute the length or magnitude of A, denoted A or |A|, as
A = (A2
x + A2
y + A2
z)1/2.
(1.99)
The components Ax, ... are also useful for computing the result when vectors are added
or multiplied by scalars. From the geometry in Cartesian coordinates, it is obvious that if
C = kA + k′B, then C will have components
Cx = k Ax + k′Bx,
Cy = k Ay + k′By,
Cz = k Az + k′Bz.
At this stage it is convenient to introduce vectors of unit length (called unit vectors) in
the directions of the coordinate axes. Letting ˆex be a unit vector in the x direction, we can
B
C
A
B
A
FIGURE 1.7
Addition of two vectors.

48
Chapter 1 Mathematical Preliminaries
F1
F2
F3
F1
F2
O
F3
wt
FIGURE 1.8
Equilibrium of forces at the point O.
now identify Ax ˆex as a vector of signed magnitude Ax in the x direction, and we see that
A can be represented as the vector sum
A = Ax ˆex + Ayˆey + Azˆez.
(1.100)
If A is itself the displacement from the origin to the point (x, y, z), we denote it by the
special symbol r (sometimes called the radius vector), and Eq. (1.100) becomes
r = xˆex + yˆey + zˆez.
(1.101)
The unit vectors are said to span the space in which our vectors reside, or to form a
basis for the space. Either of these statements means that any vector in the space can be
constructed as a linear combination of the basis vectors. Since a vector A has speciﬁc
values of Ax, Ay, and Az, this linear combination will be unique.
Sometimes a vector will be speciﬁed by its magnitude A and by the angles it makes with
the Cartesian coordinate axes. Letting α, β, γ be the respective angles our vector makes
with the x, y, and z axes, the components of A are given by
Ax = A cosα,
Ay = A cosβ,
Az = A cosγ.
(1.102)
The quantities cosα, cosβ, cosγ (see Fig. 1.9) are known as the direction cosines of A.
Since we already know that A2
x + A2
y + A2
z = A2, we see that the direction cosines are not
entirely independent, but must satisfy the relation
cos2 α + cos2 β + cos2 γ = 1.
(1.103)
While the formalism of Eq. (1.100) could be developed with complex values for the
components Ax, Ay, Az, the geometric situation being described makes it natural to restrict
these coefﬁcients to real values; the space with all possible real values of two coordinates

1.7 Vectors
49
Az
Ay
(Ax, Ay, 0)
(Ax, Ay, Az)
A
y
Ax
x
z
γ
β
α
FIGURE 1.9
Cartesian components and direction cosines of A.
y
Axex
x
A
^
Ayey
^
FIGURE 1.10
Projections of A on the x and y axes.
is denoted by mathematicians (and occasionally by us) IR2; the complete 3-D space is
named IR3.
Dot (Scalar) Product
When we write a vector in terms of its component vectors in the coordinate directions,
as in
A = Ax ˆex + Ayˆey + Azˆez,
we can think of Ax ˆex as its projection in the x direction. Stated another way, it is the
portion of A that is in the subspace spanned by ˆex alone. The term projection corresponds
to the idea that it is the result of collapsing (projecting) a vector onto one of the coordinate
axes. See Fig. 1.10.
It is useful to deﬁne a quantity known as the dot product, with the property that it
produces the coefﬁcients, e.g., Ax, in projections onto the coordinate axes according to
A · ˆex = Ax = A cosα,
A · ˆey = Ay = A cosβ,
A · ˆez = Az = A cosγ,
(1.104)
where cosα, cosβ, cosγ are the direction cosines of A.

50
Chapter 1 Mathematical Preliminaries
We want to generalize the notion of the dot product so that it will apply to arbitrary
vectors A and B, requiring that it, like projections, be linear and obey the distributive and
associative laws
A · (B + C) = A · B + A · C,
(1.105)
A · (kB) = (kA) · B = kA · B,
(1.106)
with k a scalar. Now we can use the decomposition of B into Cartesian components as
in Eq. (1.100), B = Bx ˆex + Byˆey + Bzˆez, to construct the dot product of the vectors A and
B as
A · B = A · (Bx ˆex + Byˆey + Bzˆez)
= BxA · ˆex + ByA · ˆey + BzA · ˆez
= Bx Ax + By Ay + Bz Az.
(1.107)
This leads to the general formula
A · B =
X
i
Bi Ai =
X
i
Ai Bi = B · A,
(1.108)
which is also applicable when the number of dimensions in the space is other than three.
Note that the dot product is commutative, with A · B = B · A.
An important property of the dot product is that A · A is the square of the magnitude
of A:
A · A = A2
x + A2
y + ··· = |A|2.
(1.109)
Applying this observation to C = A + B, we have
|C|2 = C · C = (A + B) · (A + B) = A · A + B · B + 2A · B,
which can be rearranged to
A · B = 1
2
h
|C|2 −|A|2 −|B|2i
.
(1.110)
From the geometry of the vector sum C = A + B, as shown in Fig. 1.11, and recalling
the law of cosines and its similarity to Eq. (1.110), we obtain the well-known formula
A · B = |A| |B|cosθ,
(1.111)
y
x
B
A
C
θ
FIGURE 1.11
Vector sum, C = A + B.

1.7 Vectors
51
where θ is the angle between the directions of A and B. In contrast with the algebraic
formula Eq. (1.108), Eq. (1.111) is a geometric formula for the dot product, and shows
clearly that it depends only on the relative directions of A and B and is therefore indepen-
dent of the coordinate system. For that reason the dot product is sometimes also identiﬁed
as a scalar product.
Equation (1.111) also permits an interpretation in terms of the projection of a vector A
in the direction of B or the reverse. If ˆb is a unit vector in the direction of B, the projection
of A in that direction is given by
Ab ˆb = (ˆb · A)ˆb = (A cosθ)ˆb,
(1.112)
where θ is the angle between A and B. Moreover, the dot product A · B can then be identi-
ﬁed as |B| times the magnitude of the projection of A in the B direction, so A · B = AbB.
Equivalently, A · B is equal to |A| times the magnitude of the projection of B in the A
direction, so we also have A · B = Ba A.
Finally, we observe that since |cosθ| ≤1, Eq. (1.111) leads to the inequality
|A · B| ≤|A| |B|.
(1.113)
The equality in Eq. (1.113) holds only if A and B are collinear (in either the same or
opposite directions). This is the specialization to physical space of the Schwarz inequality,
which we will later develop in a more general context.
Orthogonality
Equation (1.111) shows that A·B becomes zero when cosθ = 0, which occurs at θ = ±π/2
(i.e., at θ = ±90◦). These values of θ correspond to A and B being perpendicular, the
technical term for which is orthogonal. Thus,
A and B are orthogonal if and only if A · B = 0.
Checking this result for two dimensions, we note that A and B are perpendicular if the
slope of B, By/Bx, is the negative of the reciprocal of Ay/Ax, or
By
Bx
= −Ax
Ay
.
This result expands to Ax Bx + Ay By = 0, the condition that A and B be orthogonal.
In terms of projections, A · B = 0 means that the projection of A in the B direction
vanishes (and vice versa). That is of course just another way of saying that A and B are
orthogonal.
The fact that the Cartesian unit vectors are mutually orthogonal makes it possible to
simplify many dot product computations. Because
ˆex · ˆey = ˆex · ˆez = ˆey · ˆez = 0,
ˆex · ˆex = ˆey · ˆey = ˆez · ˆez = 1,
(1.114)

52
Chapter 1 Mathematical Preliminaries
we can evaluate A · B as
(Ax ˆex + Ayˆey+ Azˆez)·(Bx ˆex +Byˆey+Bzˆez) = Ax Bx ˆex ·ˆex + Ay Byˆey·ˆey+ Az Bzˆez·ˆez
+(Ax By + Ay Bx)ˆex · ˆey + (Ax Bz + Az Bx)ˆex · ˆez + (Ay Bz + Az By)ˆey · ˆez
= Ax Bx + Ay By + Az Bz.
See Chapter 3: Vector Analysis, Section 3.2: Vectors in 3-D Space for an introduction
of the cross product of vectors, needed early in Chapter 2.
Exercises
1.7.1
The vector A whose magnitude is 1.732 units makes equal angles with the coordinate
axes. Find Ax, Ay, and Az.
1.7.2
A triangle is deﬁned by the vertices of three vectors A,B and C that extend from the
origin. In terms of A,B, and C show that the vector sum of the successive sides of the
triangle (AB + BC + C A) is zero, where the side AB is from A to B, etc.
1.7.3
A sphere of radius a is centered at a point r1.
(a)
Write out the algebraic equation for the sphere.
(b)
Write out a vector equation for the sphere.
ANS.
(a)
(x −x1)2 + (y −y1)2 + (z −z1)2 = a2.
(b)
r = r1 + a, where a takes on all directions
but has a ﬁxed magnitude a.
1.7.4
Hubble’s law. Hubble found that distant galaxies are receding with a velocity propor-
tional to their distance from where we are on Earth. For the ith galaxy,
vi = H0ri
with us at the origin. Show that this recession of the galaxies from us does not imply
that we are at the center of the universe. Speciﬁcally, take the galaxy at r1 as a new
origin and show that Hubble’s law is still obeyed.
1.7.5
Find the diagonal vectors of a unit cube with one corner at the origin and its three sides
lying along Cartesian coordinates axes. Show that there are four diagonals with length
√
3. Representing these as vectors, what are their components? Show that the diagonals
of the cube’s faces have length
√
2 and determine their components.
1.7.6
The vector r, starting at the origin, terminates at and speciﬁes the point in space (x, y, z).
Find the surface swept out by the tip of r if
(a)
(r −a) · a = 0. Characterize a geometrically.
(b)
(r −a) · r = 0. Describe the geometric role of a.
The vector a is constant (in magnitude and direction).

1.8 Complex Numbers and Functions
53
1.7.7
A pipe comes diagonally down the south wall of a building, making an angle of 45◦with
the horizontal. Coming into a corner, the pipe turns and continues diagonally down a
west-facing wall, still making an angle of 45◦with the horizontal. What is the angle
between the south-wall and west-wall sections of the pipe?
ANS.
120◦.
1.7.8
Find the shortest distance of an observer at the point (2,1,3) from a rocket in free
ﬂight with velocity (1,2,3) km/s. The rocket was launched at time t = 0 from (1,1,1).
Lengths are in kilometers.
1.7.9
Show that the medians of a triangle intersect in the center which is 2/3 of the median’s
length from each vertex. Construct a numerical example and plot it.
1.7.10
Prove the law of cosines starting from A2 = (B −C)2.
1.7.11
Given the three vectors,
P = 3ˆex + 2ˆey −ˆez,
Q = −6ˆex −4ˆey + 2ˆez,
R = ˆex −2ˆey −ˆez,
ﬁnd two that are perpendicular and two that are parallel or antiparallel.
1.8
COMPLEX NUMBERS AND FUNCTIONS
Complex numbers and analysis based on complex variable theory have become extremely
important and valuable tools for the mathematical analysis of physical theory. Though
the results of the measurement of physical quantities must, we ﬁrmly believe, ultimately
be described by real numbers, there is ample evidence that successful theories predicting
the results of those measurements require the use of complex numbers and analysis. In a
later chapter we explore the fundamentals of complex variable theory. Here we introduce
complex numbers and identify some of their more elementary properties.
Basic Properties
A complex number is nothing more than an ordered pair of two real numbers, (a,b). Sim-
ilarly, a complex variable is an ordered pair of two real variables,
z ≡(x, y).
(1.115)
The ordering is signiﬁcant. In general (a,b) is not equal to (b,a) and (x, y) is not equal
to (y, x). As usual, we continue writing a real number (x,0) simply as x, and we call
i ≡(0,1) the imaginary unit. All of complex analysis can be developed in terms of ordered
pairs of numbers, variables, and functions (u(x, y),v(x, y)).
We now deﬁne addition of complex numbers in terms of their Cartesian components as
z1 + z2 = (x1, y1) + (x2, y2) = (x1 + x2, y1 + y2).
(1.116)

54
Chapter 1 Mathematical Preliminaries
Multiplication of complex numbers is deﬁned as
z1z2 = (x1, y1) · (x2, y2) = (x1x2 −y1y2, x1y2 + x2y1).
(1.117)
It is obvious that multiplication is not just the multiplication of corresponding components.
Using Eq. (1.117) we verify that i2 = (0,1)·(0,1) = (−1,0) = −1, so we can also identify
i = √−1 as usual, and further rewrite Eq. (1.115) as
z = (x, y) = (x,0) + (0, y) = x + (0,1) · (y,0) = x + iy.
(1.118)
Clearly, introduction of the symbol i is not necessary here, but it is convenient, in large
part because the addition and multiplication rules for complex numbers are consistent with
those for ordinary arithmetic with the additional property that i2 = −1:
(x1 + iy1)(x2 + iy2) = x1x2 + i2y1y2 + i(x1y2 + y1x2) = (x1x2 −y1y2) + i(x1y2 + y1x2),
in agreement with Eq. (1.117). For historical reasons, i and its multiples are known as
imaginary numbers.
The space of complex numbers, sometimes denoted Z by mathematicians, has the fol-
lowing formal properties:
•
It is closed under addition and multiplication, meaning that if two complex numbers
are added or multiplied, the result is also a complex number.
•
It has a unique zero number, which when added to any complex number leaves it
unchanged and which, when multiplied with any complex number yields zero.
•
It has a unique unit number, 1, which when multiplied with any complex number leaves
it unchanged.
•
Every complex number z has an inverse under addition (known as −z), and every
nonzero z has an inverse under multiplication, denoted z−1 or 1/z.
•
It is closed under exponentiation: if u and v are complex numbers uv is also a complex
number.
From a rigorous mathematical viewpoint, the last statement above is somewhat loose, as it
does not really deﬁne exponentiation, but we will ﬁnd it adequate for our purposes.
Some additional deﬁnitions and properties include the following:
Complex conjugation: Like all complex numbers, i has an inverse under addition,
denoted −i, in two-component form, (0,−1). Given a complex number z = x + iy, it
is useful to deﬁne another complex number, z∗= x −iy, which we call the complex con-
jugate of z.6 Forming
zz∗= (x + iy)(x −iy) = x2 + y2,
(1.119)
we see that zz∗is real; we deﬁne the absolute value of z, denoted |z|, as √zz∗.
6The complex conjugate of z is often denoted z in the mathematical literature.

1.8 Complex Numbers and Functions
55
Division: Consider now the division of two complex numbers: z′/z. We need to manipulate
this quantity to bring it to the complex number form u + iv (with u and v real). We may
do so as follows:
z′
z = z′z∗
zz∗= (x′ + iy′)(x −iy)
x2 + y2
,
or
x′ + iy′
x + iy = xx′ + yy′
x2 + y2 + i xy′ −x′y
x2 + y2 .
(1.120)
Functions in the Complex Domain
Since the fundamental operations in the complex domain obey the same rules as those for
arithmetic in the space of real numbers, it is natural to deﬁne functions so that their real and
complex incarnations are similar, and speciﬁcally so that the complex and real deﬁnitions
agree when both are applicable. This means, among other things, that if a function is repre-
sented by a power series, we should, within the region of convergence of the power series,
be able to use such series with complex values of the expansion variable. This notion is
called permanence of the algebraic form.
Applying this concept to the exponential, we deﬁne
ez = 1 + z + 1
2! z2 + 1
3! z3 + 1
4! z4 + ··· .
(1.121)
Now, replacing z by iz, we have
eiz = 1 + iz + 1
2! (iz)2 + 1
3! (iz)3 + 1
4! (iz)4 + ···
=

1 −1
2! z2 + 1
4! z4 −···

+ i

z −1
3! z3 + 1
5! z5 −···

.
(1.122)
It was permissible to regroup the terms in the series of Eq. (1.122) because that series is
absolutely convergent for all z; the d’Alembert ratio test succeeds for all z, real or complex.
If we now identify the bracketed expansions in the last line of Eq. (1.122) as cos z and sin z,
we have the extremely valuable result
eiz = cos z + i sin z.
(1.123)
This result is valid for all z, real, imaginary, or complex, but is particularly useful when z
is real.
Any function w(z) of a complex variable z = x + iy can in principle be divided into its
real and imaginary parts, just as we did when we added, multiplied, or divided complex
numbers. That is, we can write
w(z) = u(x, y) + iv(x, y),
(1.124)
in which the separate functions u(x, y) and v(x, y) are pure real. For example, if f (z) = z2,
we have
f (z) = (z + iy)2 = (x2 −y2) + i(2xy).

56
Chapter 1 Mathematical Preliminaries
The real part of a function f (z) will be labeled Re f (z), whereas the imaginary part
will be labeled Im f (z). In Eq. (1.124),
Rew(z) = u(x, y),
Imw(z) = v(x, y).
The complex conjugate of our function w(z) is u(x, y) −iv(x, y), and depending on w,
may or may not be equal to w(z∗).
Polar Representation
We may visualize complex numbers by assigning them locations on a planar graph, called
an Argand diagram or, more colloquially, the complex plane. Traditionally the real com-
ponent is plotted horizontally, on what is called the real axis, with the imaginary axis in
the vertical direction. See Fig. 1.12. An alternative to identifying points by their Cartesian
coordinates (x, y) is to use polar coordinates (r,θ), with
x = r cosθ, y = r sinθ,
or
r =
q
x2 + y2, θ = tan−1 y/x.
(1.125)
The arctan function tan−1(y/x) is multiple valued; the correct location on an Argand dia-
gram needs to be consistent with the individual values of x and y.
The Cartesian and polar representations of a complex number can also be related by
writing
x + iy = r(cosθ + i sinθ) = reiθ,
(1.126)
where we have used Eq. (1.123) to introduce the complex exponential. Note that r is
also |z|, so the magnitude of z is given by its distance from the origin in an Argand di-
agram. In complex variable theory, r is also called the modulus of z and θ is termed the
argument or the phase of z.
If we have two complex numbers, z and z′, in polar form, their product zz′ can be written
zz′ = (reiθ)(r′eiθ′) = (rr′)ei(θ+θ′),
(1.127)
showing that the location of the product in an Argand diagram will have argument (polar
angle) at the sum of the polar angles of the factors, and with a magnitude that is the product
x
y
z
r
θ
Jm
Re
FIGURE 1.12
Argand diagram, showing location of z = x + iy = reiθ.

1.8 Complex Numbers and Functions
57
Re
Re
z
Jm
Jm
z
−z*
z −z*
z+ z*
z*
z*
θ
θ
FIGURE 1.13
Left: Relation of z and z∗. Right: z + z∗and z −z∗.
of their magnitudes. Conversely, the quotient z/z′ will have magnitude r/r′ and argument
θ −θ′. These relationships should aid in getting a qualitative understanding of complex
multiplication and division. This discussion also shows that multiplication and division
are easier in the polar representation, whereas addition and subtraction have simpler forms
in Cartesian coordinates.
The plotting of complex numbers on an Argand diagram makes obvious some other
properties. Since addition on an Argand diagram is analogous to 2-D vector addition, it
can be seen that
|z| −|z′|
 ≤|z ± z′| ≤|z| + |z′|.
(1.128)
Also, since z∗= re−iθ has the same magnitude as z but an argument that differs only in
sign, z + z∗will be real and equal to 2Re z, while z −z∗will be pure imaginary and equal
to 2i Im z. See Fig. 1.13 for an illustration of this discussion.
We can use an Argand diagram to plot values of a function w(z) as well as just z itself,
in which case we could label the axes u and v, referring to the real and imaginary parts of
w. In that case, we can think of the function w(z) as providing a mapping from the xy
plane to the uv plane, with the effect that any curve in the xy (sometimes called z) plane
is mapped into a corresponding curve in the uv (= w) plane. In addition, the statements of
the preceding paragraph can be extended to functions:
|w(z)| −|w′(z)|
 ≤|w(z) ± w′(z)| ≤|w(z)| + |w′(z)|,
Rew(z) = w(z) + [w(z)]∗
2
,
Imw(z) = w(z) −[w(z)]∗
2
.
(1.129)
Complex Numbers of Unit Magnitude
Complex numbers of the form
eiθ = cosθ + i sinθ,
(1.130)
where we have given the variable the name θ to emphasize the fact that we plan to restrict
it to real values, correspond on an Argand diagram to points for which x = cosθ, y = sinθ,

58
Chapter 1 Mathematical Preliminaries
Jm
Re
x
y
1
i
eiθ
θ
θ =π
−i
−1
FIGURE 1.14
Some values of z on the unit circle.
and whose magnitude is therefore cos2 θ + sin2 θ = 1. The points exp(iθ) therefore lie on
the unit circle, at polar angle θ. This observation makes obvious a number of relations
that could in principle also be deduced from Eq. (1.130). For example, if θ has the special
values π/2, π, or 3π/2, we have the interesting relationships
eiπ/2 = i,
eiπ = −1,
e3iπ/2 = −i.
(1.131)
We also see that exp(iθ) is periodic, with period 2π, so
e2iπ = e4iπ = ··· = 1,
e3iπ/2 = e−iπ/2 = −i, etc.
(1.132)
A few relevant values of z on the unit circle are illustrated in Fig. 1.14. These relation-
ships cause the real part of exp(iωt) to describe oscillation at angular frequency ω, with
exp(i[ωt + δ]) describing an oscillation displaced from that ﬁrst mentioned by a phase
difference δ.
Circular and Hyperbolic Functions
The relationship encapsulated in Eq. (1.130) enables us to obtain convenient formulas for
the sine and cosine. Taking the sum and difference of exp(+iθ) and exp(−iθ), we have
cosθ = eiθ + e−iθ
2
,
sinθ = eiθ −e−iθ
2i
.
(1.133)
These formulas place the deﬁnitions of the hyperbolic functions in perspective:
coshθ = eθ + e−θ
2
,
sinhθ = eθ −e−θ
2
.
(1.134)
Comparing these two sets of equations, it is possible to establish the formulas
coshiz = cos z,
sinhiz = i sin z.
(1.135)
Proof is left to Exercise 1.8.5.
The fact that exp(inθ) can be written in the two equivalent forms
cosnθ + i sinnθ = (cosθ + i sinθ)n
(1.136)

1.8 Complex Numbers and Functions
59
establishes a relationship known as de Moivre’s Theorem. By expanding the right mem-
ber of Eq. (1.136), we easily obtain trigonometric multiple-angle formulas, of which the
simplest examples are the well-known results
sin(2θ) = 2sinθ cosθ,
cos(2θ) = cos2 θ −sin2 θ.
If we solve the sinθ formula of Eq. (1.133) for exp(iθ), we get (choosing the plus sign
for the radical)
eiθ = i sinθ +
q
1 −sin2 θ.
Setting sinθ = z and θ = sin−1(z), and taking the logarithm of both sides of the above
equation, we express the inverse trigonometric function in terms of logarithms.
sin−1(z) = −i ln
h
iz +
p
1 −z2
i
.
The set of formulas that can be generated in this way includes:
sin−1(z) = −i ln
h
iz +
p
1 −z2
i
,
tan−1(z) = i
2
h
ln(1 −iz) −ln(1 + iz)
i
,
sinh−1(z) = ln
h
z +
p
1 + z2
i
,
tanh−1(z) = 1
2
h
ln(1 + z) −ln(1 −z)
i
.
(1.137)
Powers and Roots
The polar form is very convenient for expressing powers and roots of complex numbers.
For integer powers, the result is obvious and unique:
z = reiϕ,
zn = rneinϕ.
For roots (fractional powers), we also have
z = reiϕ,
z1/n = r1/neiϕ/n,
but the result is not unique. If we write z in the alternate but equivalent form
z = rei(ϕ+2mπ),
where m is an integer, we now get additional values for the root:
z1/n = r1/nei(ϕ+2mπ)/n,
(any integer m).
If n = 2 (corresponding to the square root), different choices of m will lead to two distinct
values of z1/2, both of the same modulus but differing in argument by π. This corresponds
to the well-known result that the square root is double-valued and can be written with
either sign.
In general, z1/n is n-valued, with successive values having arguments that differ by
2π/n. Figure 1.15 illustrates the multiple values of 11/3, i1/3, and (−1)1/3.

60
Chapter 1 Mathematical Preliminaries
(a)
(b)
(c)
−1
−i
1
(1+ 3i)
1
2
( −1+ 3i)
1
2
(−1−3i)
1
2
(1−3i)
1
2
( 3+ i)
1
2
(
3+i)
1
2 −
FIGURE 1.15
Cube roots: (a) 11/3; (b) i1/3; (c) (−1)1/3.
Logarithm
Another multivalued complex function is the logarithm, which in the polar representation
takes the form
ln z = ln(reiθ) = lnr + iθ.
However, it is also true that
ln z = ln

rei(θ+2nπ)
= lnr + i(θ + 2nπ),
(1.138)
for any positive or negative integer n. Thus, ln z has, for a given z, the inﬁnite number of
values corresponding to all possible choices of n in Eq. (1.138).
Exercises
1.8.1
Find the reciprocal of x + iy, working in polar form but expressing the ﬁnal result in
Cartesian form.
1.8.2
Show that complex numbers have square roots and that the square roots are contained
in the complex plane. What are the square roots of i?
1.8.3
Show that
(a)
cosnθ = cosn θ −
n
2

cosn−2 θ sin2 θ +
n
4

cosn−4 θ sin4 θ −···,
(b)
sinnθ =
n
1

cosn−1 θ sinθ −
n
3

cosn−3 θ sin3 θ + ···.
1.8.4
Prove that
(a)
N−1
X
n=0
cosnx = sin(Nx/2)
sin x/2
cos(N −1)x
2 ,
(b)
N−1
X
n=0
sinnx = sin(Nx/2)
sin x/2
sin(N −1)x
2.
These series occur in the analysis of the multiple-slit diffraction pattern.

1.8 Complex Numbers and Functions
61
1.8.5
Assume that the trigonometric functions and the hyperbolic functions are deﬁned for
complex argument by the appropriate power series. Show that
i sin z = sinhiz, siniz = i sinh z,
cos z = coshiz, cosiz = cosh z.
1.8.6
Using the identities
cos z = eiz + e−iz
2
,
sin z = eiz −e−iz
2i
,
established from comparison of power series, show that
(a)
sin(x + iy) = sin x cosh y + i cos x sinh y,
cos(x + iy) = cos x cosh y −i sin x sinh y,
(b)
|sin z|2 = sin2 x + sinh2 y,
|cos z|2 = cos2 x + sinh2 y.
This demonstrates that we may have |sin z|,|cos z| > 1 in the complex plane.
1.8.7
From the identities in Exercises 1.8.5 and 1.8.6 show that
(a)
sinh(x + iy) = sinh x cos y + i cosh x sin y,
cosh(x + iy) = cosh x cos y + i sinh x sin y,
(b)
|sinh z|2 = sinh2 x + sin2 y,
|cosh z|2 = cosh2 x + sin2 y.
1.8.8
Show that
(a)
tanh z
2 = sinh x + i sin y
cosh x + cos y ,
(b)
coth z
2 = sinh x −i sin y
cosh x −cos y .
1.8.9
By comparing series expansions, show that tan−1 x = i
2 ln
1 −ix
1 + ix

.
1.8.10
Find the Cartesian form for all values of
(a)
(−8)1/3,
(b)
i1/4,
(c)
eiπ/4.
1.8.11
Find the polar form for all values of
(a)
(1 + i)3,
(b)
(−1)1/5.

62
Chapter 1 Mathematical Preliminaries
1.9
DERIVATIVES AND EXTREMA
We recall the familiar limit identiﬁed as the derivative, d f (x)/dx, of a function f (x) at a
point x:
d f (x)
dx
= lim
ε=0
f (x + ε) −f (x)
ε
;
(1.139)
the derivative is only deﬁned if the limit exists and is independent of the direction from
which ε approaches zero. The variation or differential of f (x) associated with a change
dx in its independent variable from the reference value x assumes the form
d f = f (x + dx) −f (x) = d f
dx dx,
(1.140)
in the limit that dx is small enough that terms dependent on dx2 and higher powers of dx
become negligible. The mean value theorem (based on the continuity of f ) tells us that
here, d f/dx is evaluated at some point ξ between x and x + dx, but as dx →0, ξ →x.
When a quantity of interest is a function of two or more independent variables, the
generalization of Eq. (1.140) is (illustrating for the physically important three-variable
case):
d f =
h
f (x + dx, y + dy, z + dz) −f (x, y + dy, z + dz)
i
+
h
( f (x, y + dy, z + dz) −f (x, y, z + dz)
i
+
h
f (x, y, z + dz) −f (x, y, z)
i
= ∂f
∂x dx + ∂f
∂y dy + ∂f
∂z dz,
(1.141)
where the partial derivatives indicate differentiation in which the independent variables
not being differentiated are kept ﬁxed. The fact that ∂f/∂x is evaluated at y + dy and
z + dz instead of at y and z alters the derivative by amounts that are of order dy and
dz, and therefore the change becomes negligible in the limit of small variations. It is thus
consistent to interpret Eq. (1.141) as involving partial derivatives that are all evaluated at
the reference point x, y, z.
Further analysis of the same sort as led to Eq. (1.141) can be used to deﬁne higher
derivatives and to establish the useful result that cross derivatives (e.g., ∂2/∂x∂y) are
independent of the order in which the differentiations are performed:
∂
∂y
∂f
∂x

≡∂2 f
∂y∂x = ∂2 f
∂x∂y .
(1.142)
Sometimes it is not clear from the context which variables other than that being dif-
ferentiated are independent, and it is then advisable to attach subscripts to the derivative
notation to avoid ambiguity. For example, if x, y, and z have been deﬁned in a problem,
but only two of them are independent, one might write
∂f
∂x

y
or
∂f
∂x

z
,
whichever is actually meant.

1.9 Derivatives and Extrema
63
For working with functions of several variables, we note two useful formulas that follow
from Eq. (1.141):
1.
The chain rule,
d f
ds = ∂f
∂x
dx
ds + ∂f
∂y
dy
ds + ∂f
∂z
dz
ds ,
(1.143)
which applies when x, y, and z are functions of another variable, s,
2.
A formula obtained by setting d f = 0 (here shown for the case where there are only
two independent variables and the dz term of Eq. (1.141) is absent):
∂y
∂x

f
= −
∂f
∂x

y
∂f
∂y

x
.
(1.144)
In Lagrangian mechanics, one occasionally encounters expressions such as7
d
dt L(x, ˙x,t) =
∂L
∂x ˙x + ∂L
∂˙x ¨x + ∂L
∂t

,
an example of use of the chain rule. Here it is necessary to distinguish between the formal
dependence of L on its three arguments and the overall dependence of L on time. Note the
use of the ordinary (d/dt) and partial (∂/∂t) derivative notation.
Stationary Points
Whether or not a set of independent variables (e.g., x, y, z of our previous discussion)
represents directions in space, one can ask how a function f changes if we move in various
directions in the space of the independent variables; the answer is provided by Eq. (1.143),
where the “direction” is deﬁned by the values of dx/ds, dy/ds, etc.
It is often desired to ﬁnd the minimum of a function f of n variables xi, i = 1,...,n,
and a necessary but not sufﬁcient condition on its position is that
d f
ds = 0 for all directions of ds.
This is equivalent to requiring
∂f
∂xi
= 0,
i = 1,...,n.
(1.145)
All points in the {xi} space that satisfy Eq. (1.145) are termed stationary; for a stationary
point of f to be a minimum, it is also necessary that the second derivatives d2 f/ds2 be
positive for all directions of s. Conversely, if the second derivatives in all directions are
negative, the stationary point is a maximum. If neither of these conditions are satisﬁed, the
stationary point is neither a maximum nor a minimum, and is often called a saddle point
because of the appearance of the surface of f when there are two independent variables
7Here dots indicate time derivatives.

64
Chapter 1 Mathematical Preliminaries
f(x, y)
y
x
FIGURE 1.16
A stationary point that is neither a maximum nor minimum
(a saddle point).
(see Fig. 1.16). It is often obvious whether a stationary point is a minimum or maximum,
but a complete discussion of the issue is nontrivial.
Exercises
1.9.1
Derive the following formula for the Maclaurin expansion of a function of two
variables:
f (x, y) = f (0,0) + x ∂f
∂x + y ∂f
∂y
+ 1
2!
2
0

x2 ∂2 f
∂x2 +
2
1

xy ∂2 f
∂x∂y +
2
2

y2 ∂2 f
∂y2

+ 1
3!
3
0

x3 ∂3 f
∂x3 +
3
1

x2y ∂3 f
∂x2∂y +
3
2

xy2 ∂3 f
∂x∂y2 +
3
3

y3 ∂3 f
∂y3

+ ··· ,
where all the partial derivatives are to be evaluated at the point (0,0).
1.9.2
The result in Exercise 1.9.1 can be generalized to larger numbers of independent vari-
ables. Prove that for an m-variable system, the Maclaurin expansion can be written in

1.10 Evaluation of Integrals
65
the symbolic form
f (x1,..., xm) =
∞
X
n=0
tn
n!
 m
X
i=1
αi
∂
∂xi
!n
f (0,...,0),
where in the right-hand side we have made the substitutions x j = α j t.
1.10
EVALUATION OF INTEGRALS
Proﬁciency in the evaluation of integrals involves a mixture of experience, skill in pat-
tern recognition, and a few tricks. The most familiar include the technique of integration
by parts, and the strategy of changing the variable of integration. We review here some
methods for integrals in one and multiple dimensions.
Integration by Parts
The technique of integration by parts is part of every elementary calculus course, but its
use is so frequent and ubiquitous that it bears inclusion here. It is based on the obvious
relation, for u and v arbitrary functions of x,
d(uv) = u dv + v du.
Integrating both sides of this equation over an interval (a,b), we reach
uv

b
a =
b
Z
a
u dv +
b
Z
a
v du,
which is usually rearranged to the well-known form
b
Z
a
u dv = uv

b
a −
b
Z
a
v du.
(1.146)
Example 1.10.1
INTEGRATION BY PARTS
Consider the integral
Z b
a
x sin x dx. We identify u = x and dv = sin x dx. Differentiating
and integrating, we ﬁnd du = dx and v = −cos x, so Eq. (1.146) becomes
b
Z
a
x sin x dx = (x)(−cos x)

b
a −
b
Z
a
(−cos x)dx = a cosa −b cosb + sinb −sina.
■
The key to the effective use of this technique is to see how to partition an integrand into
u and dv in a way that makes it easy to form du and v and also to integrate
R
v du.

66
Chapter 1 Mathematical Preliminaries
Special Functions
A number of special functions have become important in physics because they arise in fre-
quently encountered situations. Identifying a one-dimensional (1-D) integral as one yield-
ing a special function is almost as good as a straight-out evaluation, in part because it
prevents the waste of time that otherwise might be spent trying to carry out the integration.
But of perhaps more importance, it connects the integral to the full body of knowledge
regarding its properties and evaluation. It is not necessary for every physicist to know
everything about all known special functions, but it is desirable to have an overview per-
mitting the recognition of special functions which can then be studied in more detail if
necessary.
It is common for a special function to be deﬁned in terms of an integral over the range
for which that integral converges, but to have its deﬁnition extended to a larger domain
Table 1.2
Special Functions of Importance in Physics
Gamma function
0(x) =
∞
Z
0
tx−1e−tdt
See Chap. 13.
Factorial (n integral)
n! =
∞
Z
0
tne−tdt
n! = 0(n + 1)
Riemann zeta function
ζ(x) =
1
0(x)
∞
Z
0
tx−1dt
et −1
See Chaps. 1 and 12.
Exponential integrals
En(x) =
∞
Z
1
t−ne−tdt
E1(x) ≡−Ei(−x)
Sine integral
si(x) = −
∞
Z
x
sint
t
dt
Cosine integral
Ci(x) = −
∞
Z
x
cost
t
dt
Error functions
erf(x) =
2
√π
x
Z
0
e−t2dt
erf(∞) = 1
erfc(x) =
2
√π
∞
Z
x
e−t2dt
erfc(x) = 1 −erf(x)
Dilogarithm
Li2(x) = −
x
Z
0
ln(1 −t)
t
dt

1.10 Evaluation of Integrals
67
by analytic continuation in the complex plane (cf. Chapter 11) or by the establishment of
suitable functional relations. We present in Table 1.2 only the most useful integral repre-
sentations of a few functions of frequent occurrence. More detail is provided by a variety of
on-line sources and in material listed under Additional Readings at the end of this chapter,
particularly the compilations by Abramowitz and Stegun and by Gradshteyn and Ryzhik.
A conspicuous omission from the list in Table 1.2 is the extensive family of Bessel func-
tions. A short table cannot sufﬁce to summarize their numerous integral representations; a
survey of this topic is in Chapter 14. Other important functions in more than one variable
or with indices in addition to arguments have also been omitted from the table.
Other Methods
An extremely powerful method for the evaluation of deﬁnite integrals is that of contour
integration in the complex plane. This method is presented in Chapter 11 and will not be
discussed here.
Integrals can often be evaluated by methods that involve integration or differentiation
with respect to parameters, thereby obtaining relations between known integrals and those
whose values are being sought.
Example 1.10.2
DIFFERENTIATE PARAMETER
We wish to evaluate the integral
I =
∞
Z
0
e−x2
x2 + a2 dx.
We introduce a parameter, t, to facilitate further manipulations, and consider the related
integral
J(t) =
∞
Z
0
e−t(x2+a2)
x2 + a2
dx ;
we note that I = ea2 J(1).
We now differentiate J(t) with respect to t and evaluate the resulting integral, which is
a scaled version of Eq. (1.148):
d J(t)
dt
= −
∞
Z
0
e−t(x2 +a2) dx = −e−ta2
∞
Z
0
e−tx2 dx = −1
2
rπ
t e−ta2.
(1.147)
To recover J(t) we integrate Eq. (1.147) between t and ∞, making use of the fact that
J(∞) = 0. To carry out the integration it is convenient to make the substitution u2 = a2t,

68
Chapter 1 Mathematical Preliminaries
so we get
J(t) =
√π
2
∞
Z
t
e−ta2
t1/2 dt =
√π
a
∞
Z
at1/2
e−u2 du,
which we now recognize as J(t) = (π/2a)erfc(at1/2). Thus, our ﬁnal result is
I = π
2a ea2 erfc(a).
■
Many integrals can be evaluated by ﬁrst converting them into inﬁnite series, then
manipulating the resulting series, and ﬁnally either evaluating the series or recognizing
it as a special function.
Example 1.10.3
EXPAND, THEN INTEGRATE
Consider I =
R 1
0
dx
x ln

1+x
1−x

. Using Eq. (1.120) for the logarithm,
I =
1
Z
0
dx 2

1 + x2
3 + x4
5 + ···

= 2

1 + 1
32 + 1
52 + ···

.
Noting that
1
22 ζ(2) = 1
22 + 1
42 + 1
62 + ··· ,
we see that
ζ(2) −1
4 ζ(2) = 1 + 1
32 + 1
52 + ··· ,
so I = 3
2 ζ(2).
■
Simply using complex numbers aids in the evaluation of some integrals. Take, for
example, the elementary integral
I =
Z
dx
1 + x2 .
Making a partial fraction decomposition of (1 + x2)−1 and integrating, we easily get
I =
Z 1
2

1
1 + ix +
1
1 −ix

dx = i
2
h
ln(1 −ix) −ln(1 + ix)
i
.
From Eq. (1.137), we recognize this as tan−1(x).
The complex exponential forms of the trigonometric functions provide interesting
approaches to the evaluation of certain integrals. Here is an example.

1.10 Evaluation of Integrals
69
Example 1.10.4
A TRIGONOMETRIC INTEGRAL
Consider
I =
∞
Z
0
e−at cosbt dt,
where a and b are real and positive. Because cosbt = Re eibt, we note that
I = Re
∞
Z
0
e(−a+ib)t dt.
The integral is now just that of an exponential, and is easily evaluated, leading to
I = Re
1
a −ib = Re a + ib
a2 + b2 ,
which yields I = a/(a2 + b2). As a bonus, the imaginary part of the same integral gives us
∞
Z
0
e−at sinbt dt =
b
a2 + b2 .
■
Recursive methods are often useful in obtaining formulas for a set of related integrals.
Example 1.10.5
RECURSION
Consider
In =
1
Z
0
tn sinπt dt
for positive integer n.
Integrating In by parts twice, taking u = tn and dv = sinπt dt, we have
In = 1
π −n(n −1)
π2
In−2,
with starting values I0 = 2/π and I1 = 1/π.
There is often no practical need to obtain a general, nonrecursive formula, as repeated
application of the recursion is frequently more efﬁcient that a closed formula, even when
one can be found.
■

70
Chapter 1 Mathematical Preliminaries
Multiple Integrals
An expression that corresponds to integration in two variables, say x and y, may be written
with two integral signs, as in
x
f (x, y)dxdy
or
x2
Z
x1
dx
y2(x)
Z
y1(x)
dy f (x, y),
where the right-hand form can be more speciﬁc as to the integration limits, and also gives
an explicit indication that the y integration is to be performed ﬁrst, or with a single integral
sign, as in
Z
S
f (x, y)d A,
where S (if explicitly shown) is a 2-D integration region and d A is an element of “area”
(in Cartesian coordinates, equal to dxdy). In this form we are leaving open both the choice
of coordinate system to be used for evaluating the integral, and the order in which the
variables are to be integrated. In three dimensions, we may either use three integral signs
or a single integral with a symbol dτ indicating a 3-D “volume” element in an unspeciﬁed
coordinate system.
In addition to the techniques available for integration in a single variable, multiple in-
tegrals provide further opportunities for evaluation based on changes in the order of inte-
gration and in the coordinate system used in the integral. Sometimes simply reversing the
order of integration may be helpful. If, before the reversal, the range of the inner integral
depends on the outer integration variable, care must be exercised in determining the inte-
gration ranges after reversal. It may be helpful to draw a diagram identifying the range of
integration.
Example 1.10.6
REVERSING INTEGRATION ORDER
Consider
∞
Z
0
e−rdr
∞
Z
r
e−s
s
ds,
in which the inner integral can be identiﬁed as an exponential integral, suggesting difﬁ-
culty if the integration is approached in a straightforward manner. Suppose we proceed by
reversing the order of integration. To identify the proper coordinate ranges, we draw on
a (r,s) plane, as in Fig. 1.17, the region s > r ≥0, which is covered in the original inte-
gration order as a succession of vertical strips, for each r extending from s = r to s = ∞.
See the left-hand panel of the ﬁgure. If the outer integration is changed from r to s, this
same region is covered by taking, for each s, a horizontal range of r that runs from r = 0 to
r = s. See the right-hand panel of the ﬁgure. The transformed double integral then assumes

1.10 Evaluation of Integrals
71
s
r
s
r
FIGURE 1.17
2-D integration region for Example 1.10.6. Left panel: inner integration
over s; right panel: inner integration over r.
the form
∞
Z
0
e−s
s
ds
s
Z
0
e−r dr,
where the inner integral over r is now elementary, evaluating to 1 −e−s. This leaves us
with a 1-D integral,
∞
Z
0
e−s
s (1 −e−s)ds.
Introducing a power series expansion for 1 −e−s, this integral becomes
∞
Z
0
e−s
s
∞
X
n=1
(−1)n−1sn
n!
=
∞
X
n=1
(−1)n−1
n!
∞
Z
0
sn−1e−s ds =
∞
X
n=1
(−1)n−1
n!
(n −1)!,
where in the last step we have identiﬁed the s integral (cf. Table 1.2) as (n −1)!. We
complete the evaluation by noting that (n −1)!/n! = 1/n, so that the summation can be
recognized as ln2, thereby giving the ﬁnal result
∞
Z
0
e−rdr
∞
Z
r
e−s
s
ds = ln2.
■
A signiﬁcant change in the form of 2-D or 3-D integrals can sometimes be accomplished
by changing between Cartesian and polar coordinate systems.

72
Chapter 1 Mathematical Preliminaries
Example 1.10.7
EVALUATION IN POLAR COORDINATES
In many calculus texts, the evaluation of
R ∞
0 exp(−x2)dx is carried out by ﬁrst converting
it into a 2-D integral by taking its square, which is then written and evaluated in polar
coordinates. Using the fact that dxdy = r drdϕ, we have
∞
Z
0
dx e−x2
∞
Z
0
dy e−y2 =
π/2
Z
0
dϕ
∞
Z
0
r dr e−r2 = π
2
∞
Z
0
1
2 du e−u = π
4 .
This yields the well-known result
∞
Z
0
e−x2dx = 1
2
√π.
(1.148)
■
Example 1.10.8
ATOMIC INTERACTION INTEGRAL
For study of the interaction of a small atom with an electromagnetic ﬁeld, one of the in-
tegrals that arises in a simple approximate treatment using Gaussian-type orbitals is (in
dimensionless Cartesian coordinates)
I =
Z
dτ
z2
(x2 + y2 + z2)3/2 e−(x2+y2+z2),
where the range of the integration is the entire 3-D physical space (IR3). Of course, this is
a problem better addressed in spherical polar coordinates (r,θ,ϕ), where r is the distance
from the origin of the coordinate system, θ is the polar angle (for the Earth, known as
colatitude), and ϕ is the azimuthal angle (longitude). The relevant conversion formulas
are: x2 + y2 + z2 = r2 and z/r = cosθ. The volume element is dτ = r2 sinθ drdθdϕ,
and the ranges of the new coordinates are 0 ≤r < ∞, 0 ≤θ ≤π, and 0 ≤ϕ < 2π. In the
spherical coordinates, our integral becomes
I =
Z
dτ cos2 θ
r
e−r2 =
∞
Z
0
dr re−r2
π
Z
0
dθ cos2 θ sinθ
2π
Z
0
dϕ
=
1
2
 2
3
 
2π

= 2π
3 .
■
Remarks: Changes of Integration Variables
In a 1-D integration, a change in the integration variable from, say, x to y = y(x) involves
two adjustments: (1) the differential dx must be replaced by (dx/dy)dy, and (2) the in-
tegration limits must be changed from x1, x2 to y(x1), y(x2). If y(x) is not single-valued

1.10 Evaluation of Integrals
73
over the entire range (x1, x2), the process becomes more complicated and we do not con-
sider it further at this point.
For multiple integrals, the situation is considerably more complicated and demands
some discussion. Illustrating for a double integral, initially in variables x, y, but trans-
formed to an integration in variables u,v, the differential dx dy must be transformed to
J du dv, where J, called the Jacobian of the transformation and sometimes symbolically
represented as
J = ∂(x, y)
∂(u,v)
may depend on the variables. For example, the conversion from 2-D Cartesian coordinates
x, y to plane polar coordinates r,θ involves the Jacobian
J = ∂(x, y)
∂(r,θ) = r,
so
dx dy = r dr dθ.
For some coordinate transformations the Jacobian is simple and of a well-known form, as
in the foregoing example. We can conﬁrm the value assigned to J by noticing that the
area (in xy space) enclosed by boundaries at r, r + dr, θ, and θ + dθ is an inﬁnitesimally
distorted rectangle with two sides of length dr and two of length rdθ. See Fig. 1.18. For
other transformations we may need general methods for obtaining Jacobians. Computation
of Jacobians will be treated in detail in Section 4.4.
Of interest here is the determination of the transformed region of integration. In prin-
ciple this issue is straightforward, but all too frequently one encounters situations (both
in other texts and in research articles) where misleading and potentially incorrect argu-
ments are presented. The confusion normally arises in cases for which at least a part of
the boundary is at inﬁnity. We illustrate with the conversion from 2-D Cartesian to plane
polar coordinates. Figure 1.19 shows that if one integrates for 0 ≤θ < 2π and 0 ≤r < a,
there are regions in the corners of a square (of side 2a) that are not included. If the integral
is to be evaluated in the limit a →∞, it is both incorrect and meaningless to advance ar-
guments about the “neglect” of contributions from these corner regions, as every point in
these corners is ultimately included as a is increased.
A similar, but slightly less obvious situation arises if we transform an integration over
Cartesian coordinates 0 ≤x < ∞, 0 ≤y < ∞, into one involving coordinates u = x + y,
rdθ
dr
FIGURE 1.18
Element of area in plane polar coordinates.

74
Chapter 1 Mathematical Preliminaries
FIGURE 1.19
2-D integration, Cartesian and plane polar coordinates.
V= a→
u =a
A
B
V= o→
FIGURE 1.20
Integral in transformed coordinates.
v = y, with integration limits 0 ≤u < ∞, 0 ≤v ≤u. See Fig. 1.20. Again it is incorrect
and meaningless to make arguments justifying the “neglect” of the outer triangle (labeled
B in the ﬁgure). The relevant observation here is that ultimately, as the value of u is
increased, any arbitrary point in the quarter-plane becomes included in the region being
integrated.
Exercises
1.10.1
Use a recursive method to show that, for all positive integers n, 0(n) = (n −1)!.
Evaluate the integrals in Exercises 1.10.2 through 1.10.9.
1.10.2
∞
Z
0
sin x
x
dx.
Hint. Multiply integrand by e−ax and take the limit a →0.
1.10.3
∞
Z
0
dx
cosh x .
Hint. Expand the denominator is a way that converges for all relevant x.

1.11 Dirac Delta Function
75
1.10.4
∞
Z
0
dx
eax + 1,
for a > 0.
1.10.5
∞
Z
π
sin x
x2 dx.
1.10.6
∞
Z
0
e−x sin x
x
dx.
1.10.7
x
Z
0
erf(t)dt.
The result can be expressed in terms of special functions in Table 1.2.
1.10.8
x
Z
1
E1(t)dt.
Obtain a result in which the only special function is E1.
1.10.9
∞
Z
0
e−x
x + 1 dx.
1.10.10
Show that
∞
Z
0
tan−1 x
x
2
dx = π ln2.
Hint. Integrate by parts, to linearize in tan−1. Then replace tan−1 x by tan−1 ax and
evaluate for a = 1.
1.10.11
By direct integration in Cartesian coordinates, ﬁnd the area of the ellipse deﬁned by
x2
a2 + y2
b2 = 1.
1.10.12
A unit circle is divided into two pieces by a straight line whose distance of closest
approach to the center is 1/2 unit. By evaluating a suitable integral, ﬁnd the area of
the smaller piece thereby produced. Then use simple geometric considerations to verify
your answer.
1.11
DIRAC DELTA FUNCTION
Frequently we are faced with the problem of describing a quantity that is zero everywhere
except at a single point, while at that point it is inﬁnite in such a way that its integral over

76
Chapter 1 Mathematical Preliminaries
any interval containing that point has a ﬁnite value. For this purpose it is useful to introduce
the Dirac delta function, which is deﬁned to have the properties
δ(x) = 0,
x ̸= 0,
(1.149)
f (0) =
b
Z
a
f (x)δ(x)dx,
(1.150)
where f (x) is any well-behaved function and the integration includes the origin. As a
special case of Eq. (1.150),
∞
Z
−∞
δ(x)dx = 1.
(1.151)
From Eq. (1.150), δ(x) must be an inﬁnitely high, thin spike at x = 0, as in the description
of an impulsive force or the charge density for a point charge. The problem is that no such
function exists, in the usual sense of function. However, the crucial property in Eq. (1.150)
can be developed rigorously as the limit of a sequence of functions, a distribution. For
example, the delta function may be approximated by any of the sequences of functions,
Eqs. (1.152) to (1.155) and Figs. 1.21 and 1.22:
δn(x) =



0,
x < −1
2n
n,
−1
2n < x < 1
2n
0,
x > 1
2n ,
(1.152)
δn(x) =
n
√π exp(−n2x2),
(1.153)
x
y=δn(x)
x
n
π e−n2x2
FIGURE 1.21
δ-Sequence function: left, Eq. (1.152); right, Eq. (1.153).

1.11 Dirac Delta Function
77
x
n
π ⋅
1
1+ n2x2
x
sin nx
πx
FIGURE 1.22
δ-Sequence function: left, Eq. (1.154); right, Eq. (1.155).
δn(x) = n
π
1
1 + n2x2 ,
(1.154)
δn(x) = sinnx
πx
= 1
2π
n
Z
−n
eixt dt.
(1.155)
While all these sequences (and others) cause δ(x) to have the same properties, they dif-
fer somewhat in ease of use for various purposes. Equation (1.152) is useful in providing
a simple derivation of the integral property, Eq. (1.150). Equation (1.153) is convenient to
differentiate. Its derivatives lead to the Hermite polynomials. Equation (1.155) is particu-
larly useful in Fourier analysis and in applications to quantum mechanics. In the theory of
Fourier series, Eq. (1.155) often appears (modiﬁed) as the Dirichlet kernel:
δn(x) = 1
2π
sin[(n + 1
2)x]
sin
  1
2x

.
(1.156)
In using these approximations in Eq. (1.150) and elsewhere, we assume that f (x) is well
behaved—that it offers no problems at large x.
The forms for δn(x) given in Eqs. (1.152) to (1.155) all obviously peak strongly for
large n at x = 0. They must also be scaled in agreement with Eq. (1.151). For the forms
in Eqs. (1.152) and (1.154), veriﬁcation of the scale is the topic of Exercises 1.11.1 and
1.11.2. To check the scales of Eqs. (1.153) and (1.155), we need values of the integrals
∞
Z
−∞
e−n2x2 dx =
rπ
n
and
∞
Z
−∞
sinnx
x
dx = π.
These results are respectively trivial extensions of Eqs. (1.148) and (11.107) (the latter of
which we derive later).

78
Chapter 1 Mathematical Preliminaries
For most physical purposes the forms describing delta functions are quite adequate.
However, from a mathematical point of view the situation is still unsatisfactory. The limits
lim
n→∞δn(x)
do not exist.
A way out of this difﬁculty is provided by the theory of distributions. Recognizing that
Eq. (1.150) is the fundamental property, we focus our attention on it rather than on δ(x)
itself. Equations (1.152) to (1.155) with n = 1,2,3... may be interpreted as sequences of
normalized functions, and we may consistently write
∞
Z
−∞
δ(x) f (x)dx ≡lim
n→∞
Z
δn(x) f (x)dx.
(1.157)
Thus, δ(x) is labeled a distribution (not a function) and is regarded as deﬁned by
Eq. (1.157). We might emphasize that the integral on the left-hand side of Eq. (1.157)
is not a Riemann integral.8
Properties of δ(x)
•
From any of Eqs. (1.152) through (1.155) we see that Dirac’s delta function must be
even in x, δ(−x) = δ(x).
•
If a > 0,
δ(ax) = 1
a δ(x),
a > 0.
(1.158)
Equation (1.158) can be proved by making the substitution x = y/a:
∞
Z
−∞
f (x)δ(ax)dx = 1
a
∞
Z
−∞
f (y/a)δ(y)dy = 1
a f (0).
If a < 0, Eq. (1.158) becomes δ(ax) = δ(x)/|a|.
•
Shift of origin:
∞
Z
−∞
δ(x −x0) f (x)dx = f (x0),
(1.159)
which can be proved by making the substitution y = x −x0 and noting that when y = 0,
x = x0.
8It can be treated as a Stieltjes integral if desired; δ(x)dx is replaced by du(x), where u(x) is the Heaviside step function
(compare Exercise 1.11.9).

1.11 Dirac Delta Function
79
•
If the argument of δ(x) is a function g(x) with simple zeros at points ai on the real axis
(and therefore g′(ai) ̸= 0),
δ

g(x)

=
X
i
δ(x −ai)
|g′(ai)| .
(1.160)
To prove Eq. (1.160), we write
∞
Z
−∞
f (x)δ(x)dx =
X
i
ai+ε
Z
ai−ε
f (x)δ

(x −ai)g′(ai)

dx,
where we have decomposed the original integral into a sum of integrals over small in-
tervals containing the zeros of g(x). In these intervals, we replaced g(x) by the leading
term in its Taylor series. Applying Eqs. (1.158) and (1.159) to each term of the sum,
we conﬁrm Eq. (1.160).
•
Derivative of delta function:
∞
Z
−∞
f (x)δ′(x −x0)dx = −
∞
Z
−∞
f ′(x)δ(x −x0)dx = −f ′(x0).
(1.161)
Equation (1.161) can be taken as deﬁning the derivative δ′(x); it is evaluated by per-
forming an integration by parts on any of the sequences deﬁning the delta function.
•
In three dimensions, the delta function δ(r) is interpreted as δ(x)δ(y)δ(z), so it de-
scribes a function localized at the origin and with unit integrated weight, irrespective
of the coordinate system in use. Thus, in spherical polar coordinates,
y
f (r2)δ(r2 −r1)r2
2dr2 sinθ2dθ2dφ2 = f (r1).
(1.162)
•
Equation (1.155) corresponds in the limit to
δ(t −x) = 1
2π
∞
Z
−∞
exp

iω(t −x)

dω,
(1.163)
with the understanding that this has meaning only when under an integral sign. In that
context it is extremely useful for the simpliﬁcation of Fourier integrals (Chapter 20).
•
Expansions of δ(x) are addressed in Chapter 5. See Example 5.1.7.
Kronecker Delta
It is sometimes useful to have a symbol that is the discrete analog of the Dirac delta func-
tion, with the property that it is unity when the discrete variable has a certain value, and
zero otherwise. A quantity with these properties is known as the Kronecker delta, deﬁned
for indices i and j as
δi j =
(
1,
i = j,
0,
i ̸= j.
(1.164)

80
Chapter 1 Mathematical Preliminaries
Frequent uses of this symbol are to select a special term from a summation, or to have one
functional form for all nonzero values of an index, but a different form when the index is
zero. Examples:
X
i j
fi j δi j =
X
i
fii,
Cn =
1
1 + δn0
2π
L .
Exercises
1.11.1
Let
δn(x) =



0,
x < −1
2n ,
n,
−1
2n < x < 1
2n ,
0,
1
2n < x.
Show that
lim
n→∞
∞
Z
−∞
f (x)δn(x)dx = f (0),
assuming that f (x) is continuous at x = 0.
1.11.2
For
δn(x) = n
π
1
1 + n2x2 ,
show that
∞
Z
−∞
δn(x)dx = 1.
1.11.3
Fejer’s method of summing series is associated with the function
δn(t) =
1
2πn
sin(nt/2)
sin(t/2)
2
.
Show that δn(t) is a delta distribution, in the sense that
lim
n→∞
1
2πn
∞
Z
−∞
f (t)
sin(nt/2)
sin(t/2)
2
dt = f (0).
1.11.4
Prove that
δ[a(x −x1)] = 1
a δ(x −x1).
Note. If δ[a(x −x1)] is considered even, relative to x1, the relation holds for negative
a and 1/a may be replaced by 1/|a|.
1.11.5
Show that
δ[(x −x1)(x −x2)] = [δ(x −x1) + δ(x −x2)]/|x1 −x2|.
Hint. Try using Exercise 1.11.4.

1.11 Dirac Delta Function
81
x
n large
n small
1
n → ∞
un(x)
FIGURE 1.23
Heaviside unit step function.
1.11.6
Using the Gauss error curve delta sequence δn =
n
√π e−n2x2, show that
x d
dx δ(x) = −δ(x),
treating δ(x) and its derivative as in Eq. (1.157).
1.11.7
Show that
∞
Z
−∞
δ′(x) f (x)dx = −f ′(0).
Here we assume that f ′(x) is continuous at x = 0.
1.11.8
Prove that
δ( f (x)) =

d f (x)
dx

−1
x=x0
δ(x −x0),
where x0 is chosen so that f (x0) = 0.
Hint. Note that δ( f )d f = δ(x)dx.
1.11.9
(a)
If we deﬁne a sequence δn(x) = n/(2cosh2 nx), show that
∞
Z
−∞
δn(x)dx = 1,
independent of n.
(b)
Continuing this analysis, show that9
x
Z
−∞
δn(x)dx = 1
2 [1 + tanhnx] ≡un(x)
and
lim
n→∞un(x) =
0,
x < 0,
1,
x > 0.
This is the Heaviside unit step function (Fig. 1.23).
9Many other symbols are used for this function. This is the AMS-55 notation (in Additional Readings, see Abramowitz and
Stegun): u for unit.

82
Chapter 1 Mathematical Preliminaries
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974). Contains a wealth of information about a large number of special functions.
Bender, C. M., and S. Orszag, Advanced Mathematical Methods for Scientists and Engineers. New York:
McGraw-Hill (1978). Particularly recommended for methods of accelerating convergence.
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969), reprinted, Dover (1992). This is an advanced text that presupposes moderate knowledge of
mathematical physics.
Courant, R., and D. Hilbert, Methods of Mathematical Physics, Vol. 1 (1st English ed.). New York: Wiley
(Interscience) (1953). As a reference book for mathematical physics, it is particularly valuable for existence
theorems and discussion of areas such as eigenvalue problems, integral equations, and calculus of variations.
Galambos, J., Representations of Real Numbers by Inﬁnite Series. Berlin: Springer (1976).
Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series, and Products. Corrected and enlarged 7th ed.,
edited by A. Jeffrey and D. Zwillinger. New York: Academic Press (2007).
Hansen, E., A Table of Series and Products. Englewood Cliffs, NJ: Prentice-Hall (1975). A tremendous compi-
lation of series and products.
Hardy, G. H., Divergent Series. Oxford: Clarendon Press (1956), 2nd ed., Chelsea (1992). The standard, com-
prehensive work on methods of treating divergent series. Hardy includes instructive accounts of the gradual
development of the concepts of convergence and divergence.
Jeffrey, A., Handbook of Mathematical Formulas and Integrals. San Diego: Academic Press (1995).
Jeffreys, H. S., and B. S. Jeffreys, Methods of Mathematical Physics, 3rd ed. Cambridge, UK: Cambridge Univer-
sity Press (1972). This is a scholarly treatment of a wide range of mathematical analysis, in which considerable
attention is paid to mathematical rigor. Applications are to classical physics and geophysics.
Knopp, K., Theory and Application of Inﬁnite Series. London: Blackie and Son, 2nd ed. New York: Hafner
(1971), reprinted A. K. Peters Classics (1997). This is a thorough, comprehensive, and authoritative work that
covers inﬁnite series and products. Proofs of almost all the statements about series not proved in this chapter
will be found in this book.
Mangulis, V., Handbook of Series for Scientists and Engineers. New York: Academic Press (1965). A most
convenient and useful collection of series. Includes algebraic functions, Fourier series, and series of the special
functions: Bessel, Legendre, and others.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics, 2 vols. New York: McGraw-Hill (1953). This
work presents the mathematics of much of theoretical physics in detail but at a rather advanced level. It is
recommended as the outstanding source of information for supplementary reading and advanced study.
Rainville, E. D., Inﬁnite Series. New York: Macmillan (1967). A readable and useful account of series constants
and functions.
Sokolnikoff, I. S., and R. M. Redheffer, Mathematics of Physics and Modern Engineering, 2nd ed. New York:
McGraw-Hill (1966). A long chapter 2 (101 pages) presents inﬁnite series in a thorough but very read-
able form. Extensions to the solutions of differential equations, to complex series, and to Fourier series are
included.
Spiegel, M. R., Complex Variables, in Schaum’s Outline Series. New York: McGraw-Hill (1964, reprinted 1995).
Clear, to the point, and with very large numbers of examples, many solved step by step. Answers are provided
for all others. Highly recommended.
Whittaker, E. T., and G. N. Watson, A Course of Modern Analysis, 4th ed. Cambridge, UK: Cambridge University
Press (1962), paperback. Although this is the oldest of the general references (original edition 1902), it still is
the classic reference. It leans strongly towards pure mathematics, as of 1902, with full mathematical rigor.

CHAPTER 2
DETERMINANTS AND
MATRICES
2.1
DETERMINANTS
We begin the study of matrices by solving linear equations that will lead us to determi-
nants and matrices. The concept of determinant and the notation were introduced by the
renowned German mathematician and philosopher Gottfried Wilhelm von Leibniz.
Homogeneous Linear Equations
One of the major applications of determinants is in the establishment of a condition for
the existence of a nontrivial solution for a set of linear homogeneous algebraic equations.
Suppose we have three unknowns x1, x2, x3 (or n equations with n unknowns):
a1x1 + a2x2 + a3x3 = 0,
b1x1 + b2x2 + b3x3 = 0,
(2.1)
c1x1 + c2x2 + c3x3 = 0.
The problem is to determine under what conditions there is any solution, apart from
the trivial one x1 = 0, x2 = 0, x3 = 0. If we use vector notation x = (x1, x2, x3) for the
solution and three rows a = (a1,a2,a3),b = (b1,b2,b3),c = (c1,c2,c3) of coefﬁcients,
then the three equations, Eqs. (2.1), become
a · x = 0,
b · x = 0,
c · x = 0.
(2.2)
These three vector equations have the geometrical interpretation that x is orthogonal
to a,b, and c. If the volume spanned by a,b,c given by the determinant (or triple scalar
83
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

84
Chapter 2 Determinants and Matrices
product, see Eq. (3.12) of Section 3.2)
D3 = (a × b) · c = det(a,b,c) =

a1
a2
a3
b1
b2
b3
c1
c2
c3

(2.3)
is not zero, then there is only the trivial solution x = 0. For an introduction to the cross
product of vectors, see Chapter 3: Vector Analysis, Section 3.2: Vectors in 3-D Space.
Conversely, if the aforementioned determinant of coefﬁcients vanishes, then one of
the row vectors is a linear combination of the other two. Let us assume that c lies in the
plane spanned by a and b, that is, that the third equation is a linear combination of the
ﬁrst two and not independent. Then x is orthogonal to that plane so that x ∼a × b. Since
homogeneous equations can be multiplied by arbitrary numbers, only ratios of the xi are
relevant, for which we then obtain ratios of 2 × 2 determinants
x1
x3
= a2b3 −a3b2
a1b2 −a2b1
,
x2
x3
= −a1b3 −a3b1
a1b2 −a2b1
(2.4)
from the components of the cross product a × b, provided x3 ∼a1b2 −a2b1 ̸= 0. This is
Cramer’s rule for three homogeneous linear equations.
Inhomogeneous Linear Equations
The simplest case of two equations with two unknowns,
a1x1 + a2x2 = a3,
b1x1 + b2x2 = b3,
(2.5)
can be reduced to the previous case by imbedding it in three-dimensional (3-D) space with
a solution vector x = (x1, x2,−1) and row vectors a = (a1,a2,a3),b = (b1,b2,b3). As
before, Eqs. (2.5) in vector notation, a · x = 0 and b · x = 0, imply that x ∼a × b, so the
analog of Eq. (2.4) holds. For this to apply, though, the third component of a × b must not
be zero, that is, a1b2 −a2b1 ̸= 0, because the third component of x is −1 ̸= 0. This yields
the xi as
x1 = a3b2 −b3a2
a1b2 −a2b1
=

a3
a2
b3
b2


a1
a2
b1
b2

,
(2.6)
x2 = a1b3 −a3b1
a1b2 −a2b1
=

a1
a3
b1
b3


a1
a2
b1
b2

.
(2.7)
The determinant in the numerator of x1(x2) is obtained from the determinant of the coef-
ﬁcients

a1
a2
b1
b2
 by replacing the ﬁrst (second) column vector by the vector
a3
b3

of the
inhomogeneous side of Eq. (2.5). This is Cramer’s rule for a set of two inhomogeneous
linear equations with two unknowns.
A full understanding of the above exposition requires now that we introduce a formal
deﬁnition of the determinant and show how it relates to the foregoing.

2.1 Determinants
85
Deﬁnitions
Before deﬁning a determinant, we need to introduce some related concepts and deﬁnitions.
•
When we write two-dimensional (2-D) arrays of items, we identify the item in the nth
horizontal row and the mth vertical column by the index set n,m; note that the row
index is conventionally written ﬁrst.
•
Starting from a set of n objects in some reference order (e.g., the number sequence
1, 2, 3, ..., n), we can make a permutation of them to some other order; the total
number of distinct permutations that are possible is n! (choose the ﬁrst object n ways,
then choose the second in n −1 ways, etc.).
•
Every permutation of n objects can be reached from the reference order by a succession
of pairwise interchanges (e.g., 1234 →4132 can be reached by the successive steps
1234 →1432 →4132). Although the number of pairwise interchanges needed for a
given permutation depends on the path (compare the above example with 1234 →
1243 →1423 →4123 →4132), for a given permutation the number of interchanges
will always either be even or odd. Thus a permutation can be identiﬁed as having either
even or odd parity.
•
It is convenient to introduce the Levi-Civita symbol, which for an n-object system is
denoted by εi j..., where ε has n subscripts, each of which identiﬁes one of the objects.
This Levi-Civita symbol is deﬁned to be +1 if i j ... represents an even permutation
of the objects from a reference order; it is deﬁned to be −1 if i j ... represents an odd
permutation of the objects, and zero if i j ... does not represent a permutation of the
objects (i.e., contains an entry duplication). Since this is an important deﬁnition, we set
it out in a display format:
εi j... = +1,
i j ... an even permutation,
= −1,
i j ... an odd permutation,
=
0,
i j ... not a permutation.
(2.8)
We now deﬁne a determinant of order n to be an n ×n square array of numbers (or func-
tions), with the array conventionally written within vertical bars (not parentheses, braces,
or any other type of brackets), as follows:
Dn =

a11
a12
...
a1n
a21
a22
...
a2n
a31
a32
...
a3n
...
...
...
...
an1
an2
...
ann

.
(2.9)
The determinant Dn has a value that is obtained by
1.
Forming all n! products that can be formed by choosing one entry from each row in
such a way that one entry comes from each column,
2.
Assigning each product a sign that corresponds to the parity of the sequence in which
the columns were used (assuming the rows were used in an ascending sequence),
3.
Adding (with the assigned signs) the products.

86
Chapter 2 Determinants and Matrices
More formally, the determinant in Eq. (2.9) is deﬁned to have the value
Dn =
X
i j...
εi j...a1ia2 j ··· .
(2.10)
The summations in Eq. (2.10) need not be restricted to permutations, but can be assumed
to range independently from 1 through n; the presence of the Levi-Civita symbol will
cause only the index combinations corresponding to permutations to actually contribute to
the sum.
Example 2.1.1
DETERMINANTS OF ORDERS 2 AND 3
To make the deﬁnition more concrete, we illustrate ﬁrst with a determinant of order 2. The
Levi-Civita symbols needed for this determinant are ε12 = +1 and ε21 = −1 (note that
ε11 = ε22 = 0), leading to
D2 =

a11
a12
a21
a22
 = ε12a11a22 + ε21a12a21 = a11a22 −a12a21.
We see that this determinant expands into 2! = 2 terms. A speciﬁc example of a determi-
nant of order 2 is

a1
a2
b1
b2
 = a1b2 −b1a2.
Determinants of order 3 expand into 3! = 6 terms. The relevant Levi-Civita symbols
are ε123 = ε231 = ε312 = +1, ε213 = ε321 = ε132 = −1; all other index combinations have
εi jk = 0, so
D3 =

a11
a12
a13
a21
a22
a23
a31
a32
a33

=
X
i jk
εi jka1ia2 ja3k
= a11a22a33 −a11a23a32 −a13a22a31 −a12a21a33 + a12a23a31 + a13a21a32.
The expression in Eq. (2.3) is the determinant of order 3

a1
a2
a3
b1
b2
b3
c1
c2
c3

= a1b2c3 −a1b3c2 −a2b1c3 + a2b3c1 + a3b1c2 −a3b2c1.
Note that half of the terms in the expansion of a determinant bear negative signs. It is
quite possible that a determinant of large elements will have a very small value. Here is
one example:

8
11
7
9
11
5
8
12
9

= 1.
■

2.1 Determinants
87
Properties of Determinants
The symmetry properties of the Levi-Civita symbol translate into a number of symme-
tries exhibited by determinants. For simplicity, we illustrate with determinants of order 3.
The interchange of two columns of a determinant causes the Levi-Civita symbol multi-
plying each term of the expansion to change sign; the same is true if two rows are inter-
changed. Moreover, the roles of rows and columns may be interchanged; if a determinant
with elements ai j is replaced by one with elements bi j = a ji, we call the bi j determi-
nant the transpose of the ai j determinant. Both these determinants have the same value.
Summarizing:
Interchanging two rows (or two columns) changes the sign of the value of a determi-
nant. Transposition does not alter its value.
Thus,

a11
a12
a13
a21
a22
a23
a31
a32
a33

= −

a12
a11
a13
a22
a21
a23
a32
a31
a33

=

a11
a21
a31
a12
a22
a32
a13
a23
a33

.
(2.11)
Further consequences of the deﬁnition in Eq. (2.10) are:
(1) Multiplication of all members of a single column (or a single row) by a constant k
causes the value of the determinant to be multiplied by k,
(2) If the elements of a column (or row) are actually sums of two quantities, the deter-
minant can be decomposed into a sum of two determinants.
Thus,
k

a11
a12
a13
a21
a22
a23
a31
a32
a33

=

ka11
a12
a13
ka21
a22
a23
ka31
a32
a33

=

ka11
ka12
ka13
a21
a22
a23
a31
a32
a33

,
(2.12)

a11 + b1
a12
a13
a21 + b2
a22
a23
a31 + b3
a32
a33

=

a11
a12
a13
a21
a22
a23
a31
a32
a33

+

b1
a12
a13
b2
a22
a23
b3
a32
a33

.
(2.13)
These basic properties and/or the basic deﬁnition mean that
•
Any determinant with two rows equal, or two columns equal, has the value zero. To
prove this, interchange the two identical rows or columns; the determinant both remains
the same and changes sign, and therefore must have the value zero.
•
An extension of the above is that if two rows (or columns) are proportional, the deter-
minant is zero.
•
The value of a determinant is unchanged if a multiple of one row is added (column
by column) to another row or if a multiple of one column is added (row by row) to
another column. Applying Eq. (2.13), the addition does not contribute to the value of
the determinant.
•
If each element in a row or each element in a column is zero, the determinant has the
value zero.

88
Chapter 2 Determinants and Matrices
Laplacian Development by Minors
The fact that a determinant of order n expands into n! terms means that it is important to
identify efﬁcient means for determinant evaluation. One approach is to expand in terms of
minors. The minor corresponding to ai j, denoted Mi j, or Mi j(a) if we need to identify M
as coming from the ai j, is the determinant (of order n −1) produced by striking out row i
and column j of the original determinant. When we expand into minors, the quantities to
be used are the cofactors of the (i j) elements, deﬁned as (−1)i+ j Mi j. The expansion can
be made for any row or column of the original determinant. If, for example, we expand the
determinant of Eq. (2.9) using row i, we have
Dn =
n
X
j=1
ai j(−1)i+ j Mi j.
(2.14)
This expansion reduces the work involved in evaluation if the row or column selected for
the expansion contains zeros, as the corresponding minors need not be evaluated.
Example 2.1.2
EXPANSION IN MINORS
Consider the determinant (arising in Dirac’s relativistic electron theory)
D ≡

a11
a12
a13
a14
a21
a22
a23
a24
a31
a32
a33
a34
a41
a42
a43
a44

=

0
1
0
0
−1
0
0
0
0
0
0
1
0
0
−1
0

.
Expanding across the top row, only one 3 × 3 matrix survives:
D = (−1)1+2a12M12(a) = (−1) · (1)

−1
0
0
0
0
1
0
−1
0

≡(−1)

b11
b12
b13
b21
b22
b23
b31
b32
b33

.
Expanding now across the second row, we get
D = (−1)(−1)2+3b23M23(b) =

−1
0
0
−1
 = 1.
When we ﬁnally reached a 2 × 2 determinant, it was simple to evaluate it without further
expansion.
■
Linear Equation Systems
We are now ready to apply our knowledge of determinants to the solution of systems of
linear equations. Suppose we have the simultaneous equations
a1x1 + a2x2 + a3x3 = h1,
b1x1 + b2x2 + b3x3 = h2,
c1x1 + c2x2 + c3x3 = h3.
(2.15)

2.1 Determinants
89
To use determinants to help solve this equation system, we deﬁne
D =

a1
a2
a3
b1
b2
b3
c1
c2
c3

.
(2.16)
Starting from x1 D, we manipulate it by (1) moving x1 to multiply the entries of the ﬁrst
column of D, then (2) adding to the ﬁrst column x2 times the second column and x3 times
the third column (neither of these operations change the value). We then reach the second
line of Eq. (2.17) by substituting the right-hand sides of Eqs. (2.15). These operations are
illustrated here:
x1 D =

a1 x1
a2
a3
b1 x1
b2
b3
c1 x1
c2
c3

=

a1 x1 + a2 x2 + a3 x3
a2
a3
b1 x1 + b2 x2 + b3 x3
b2
b3
c1 x1 + c2 x2 + c3 x3
c2
c3

=

h1
a2
a3
h2
b2
b3
h3
c2
c3

.
(2.17)
If D ̸= 0, Eq. (2.17) may now be solved for x1:
x1 = 1
D

h1
a2
a3
h2
b2
b3
h3
c2
c3

.
(2.18)
Analogous procedures starting from x2 D and x3 D give the parallel results
x2 = 1
D

a1
h1
a3
b1
h2
b3
c1
h3
c3

,
x3 = 1
D

a1
a2
h2
b1
b2
h2
c1
c2
h3

.
We see that the solution for xi is 1/D times a numerator obtained by replacing the ith
column of D by the right-hand-side coefﬁcients, a result that can be generalized to an arbi-
trary number n of simultaneous equations. This scheme for the solution of linear equation
systems is known as Cramer’s rule.
If D is nonzero, the above construction of the xi is deﬁnitive and unique, so that there
will be exactly one solution to the equation set. If D ̸= 0 and the equations are homoge-
neous (i.e., all the hi are zero), then the unique solution is that all the xi are zero.
Determinants and Linear Dependence
The preceding subsections go a long way toward identifying the role of the determi-
nant with respect to linear dependence. If n linear equations in n variables, written as
in Eq. (2.15), have coefﬁcients that form a nonzero determinant, the variables are uniquely
determined, meaning that the forms constituting the left-hand sides of the equations must
in fact be linearly independent. However, we would still like to prove the property illus-
trated in the introduction to this chapter, namely that if a set of forms is linearly depen-
dent, the determinant of their coefﬁcients will be zero. But this result is nearly immediate.
The existence of linear dependence means that there exists one equation whose coefﬁcients

90
Chapter 2 Determinants and Matrices
are linear combinations of the coefﬁcients of the other equations, and we may use that fact
to reduce to zero the row of the determinant corresponding to that equation.
In summary, we have therefore established the following important result:
If the coefﬁcients of n linear forms in n variables form a nonzero determinant, the forms
are linearly independent; if the determinant of the coefﬁcients is zero, the forms exhibit
linear dependence.
Linearly Dependent Equations
If a set of linear forms is linearly dependent, we can distinguish three distinct situations
when we consider equation systems based on these forms. First, and of most importance
for physics, is the case in which all the equations are homogeneous, meaning that the
right-hand side quantities hi in equations of the type Eq. (2.15) are all zero. Then, one or
more of the equations in the set will be equivalent to linear combinations of others, and
we will have less than n equations in our n variables. We can then assign one (or in some
cases, more than one) variable an arbitrary value, obtaining the others as functions of the
assigned variables. We thus have a manifold (i.e., a parameterized set) of solutions to our
equation system.
Combining the above analysis with our earlier observation that if a set of homogeneous
linear equations has a nonvanishing determinant it has the unique solution that all the xi
are zero, we have the following important result:
A system of n homogeneous linear equations in n unknowns has solutions that are not
identically zero only if the determinant of its coefﬁcients vanishes. If that determinant
vanishes, there will be one or more solutions that are not identically zero and are
arbitrary as to scale.
A second case is where we have (or combine equations so that we have) the same linear
form in two equations, but with different values of the right-hand quantities hi. In that case
the equations are mutually inconsistent, and the equation system has no solution.
A third, related case, is where we have a duplicated linear form, but with a common
value of hi. This also leads to a solution manifold.
Example 2.1.3
LINEARLY DEPENDENT HOMOGENEOUS EQUATIONS
Consider the equation set
x1 + x2 + x3 = 0,
x1 + 3 x2 + 5 x3 = 0,
x1 + 2 x2 + 3 x3 = 0.
Here
D =

1
1
1
1
3
5
1
2
3

= 1(3)(3) −1(5)(2) −1(3)(1) −1(1)(3) + 1(5)(1) + 1(1)(2) = 0.

2.1 Determinants
91
The third equation is half the sum of the other two, so we drop it. Then,
second equation minus ﬁrst:
2x2 + 4x3 = 0 −→x2 = −2x3,
(3× ﬁrst equation) minus second:
2x1 −2x3 = 0 −→x1 = x3.
Since x3 can have any value, there is an inﬁnite number of solutions, all of the form
(x1, x2, x3) = constant × (1,−2,1).
Our solution illustrates an important property of homogeneous linear equations, namely
that any multiple of a solution is also a solution. The solution only becomes less arbitrary
if we impose a scale condition. For example, in the present case we could require the
squares of the xi to add to unity. Even then, the solution would still be arbitrary as to
overall sign.
■
Numerical Evaluation
There is extensive literature on determinant evaluation. Computer codes and many refer-
ences are given, for example, by Press et al.1 We present here a straightforward method
due to Gauss that illustrates the principles involved in all the modern evaluation methods.
Gauss elimination is a versatile procedure that can be used for evaluating determinants,
for solving linear equation systems, and (as we will see later) even for matrix inversion.
Example 2.1.4
GAUSS ELIMINATION
Our example, a 3×3 linear equation system, can easily be done in other ways, but it is used
here to provide an understanding of the Gauss elimination procedure. We wish to solve
3x + 2y + z = 11
2x + 3y + z = 13
x + y + 4z = 12.
(2.19)
For convenience and for the optimum numerical accuracy, the equations are rearranged so
that, to the extent possible, the largest coefﬁcients run along the main diagonal (upper left
to lower right).
The Gauss technique is to use the ﬁrst equation to eliminate the ﬁrst unknown, x, from
the remaining equations. Then the (new) second equation is used to eliminate y from the
last equation. In general, we work down through the set of equations, and then, with one un-
known determined, we work back up to solve for each of the other unknowns in succession.
1W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes, 2nd ed. Cambridge, UK: Cambridge
University Press (1992), Chapter 2.

92
Chapter 2 Determinants and Matrices
It is convenient to start by dividing each row by its initial coefﬁcient, converting
Eq. (2.19) to
x + 2
3 y + 1
3 z = 11
3
x + 3
2 y + 1
2 z = 13
2
x + y + 4z = 12.
(2.20)
Now, using the ﬁrst equation, we eliminate x from the second and third equations by
subtracting the ﬁrst equation from each of the others:
x + 2
3 y + 1
3 z = 11
3
5
6 y + 1
6 z = 17
6
1
3 y + 11
3 z = 25
3 .
(2.21)
Then we divide the second and third rows by their initial coefﬁcients:
x + 2
3 y + 1
3 z = 11
3
y + 1
5 z = 17
5
y + 11z = 25.
(2.22)
Repeating the technique, we use the new second equation to eliminate y from the third
equation, which can then be solved for z:
x + 2
3 y + 1
3 z = 11
3
y + 1
5 z = 17
5
54
5 z = 108
5
−→
z = 2.
(2.23)
Now that z has been determined, we can return to the second equation, ﬁnding
y + 1
5 × 2 = 17
5
−→
y = 3,
and ﬁnally, continuing to the ﬁrst equation,
x + 2
3 × 3 + 1
3 × 2 = 11
3
−→
x = 1.
The technique may not seem as elegant as the use of Cramer’s rule, but it is well adapted
to computers and is far faster than the time spent with determinants.
If we had not kept the right-hand sides of the equation system, the Gauss elimination
process would have simply brought the original determinant into triangular form (but note

2.1 Determinants
93
that our processes for making the leading coefﬁcients unity cause corresponding changes
in the value of the determinant). In the present problem, the original determinant
D =

3
2
1
2
3
1
1
1
4

was divided by 3 and by 2 going from Eq. (2.19) to (2.20), and multiplied by 6/5 and
by 3 going from Eq. (2.21) to (2.22), so that D and the determinant represented by the
left-hand side of Eq. (2.23) are related by
D = (3)(2)
5
6
1
3


1
2
3
1
3
0
1
1
5
0
0
54
5

= 5
3
54
5 = 18.
(2.24)
Because all the entries in the lower triangle of the determinant explicitly shown in
Eq. (2.24) are zero, the only term that contributes to it is the product of the diagonal
elements: To get a nonzero term, we must use the ﬁrst element of the ﬁrst row, then the
second element of the second row, etc. It is easy to verify that the ﬁnal result obtained in
Eq. (2.24) agrees with the result of evaluating the original form of D.
■
Exercises
2.1.1
Evaluate the following determinants:
(a)

1
0
1
0
1
0
1
0
0

,
(b)

1
2
0
3
1
2
0
3
1

,
(c)
1
√
2

0
√
3
0
0
√
3
0
2
0
0
2
0
√
3
0
0
√
3
0

.
2.1.2
Test the set of linear homogeneous equations
x + 3y + 3z = 0,
x −y + z = 0,
2x + y + 3z = 0
to see if it possesses a nontrivial solution. In any case, ﬁnd a solution to this equation
set.
2.1.3
Given the pair of equations
x + 2y = 3,
2x + 4y = 6,
(a)
Show that the determinant of the coefﬁcients vanishes.
(b)
Show that the numerator determinants, see Eq. (2.18), also vanish.
(c)
Find at least two solutions.
2.1.4
If Ci j is the cofactor of element ai j, formed by striking out the ith row and jth column
and including a sign (−1)i+ j, show that

94
Chapter 2 Determinants and Matrices
(a)
P
i ai jCi j = P
i a jiC ji = |A|, where |A| is the determinant with the elements ai j,
(b)
P
i ai jCik = P
i a jiCki = 0, j ̸= k.
2.1.5
A determinant with all elements of order unity may be surprisingly small. The Hilbert
determinant Hi j = (i + j −1)−1, i, j = 1,2,...,n is notorious for its small values.
(a)
Calculate the value of the Hilbert determinants of order n for n = 1,2, and 3.
(b)
If an appropriate subroutine is available, ﬁnd the Hilbert determinants of order n
for n = 4,5, and 6.
ANS.
n
Det(Hn)
1
1.
2
8.33333 × 10−2
3
4.62963 × 10−4
4
1.65344 × 10−7
5
3.74930 × 10−12
6
5.36730 × 10−18.
2.1.6
Prove that the determinant consisting of the coefﬁcients from a set of linearly dependent
forms has the value zero.
2.1.7
Solve the following set of linear simultaneous equations. Give the results to ﬁve decimal
places.
1.0x1 + 0.9x2 + 0.8x3 + 0.4x4 + 0.1x5 = 1.0
0.9x1 + 1.0x2 + 0.8x3 + 0.5x4 + 0.2x5 + 0.1x6 = 0.9
0.8x1 + 0.8x2 + 1.0x3 + 0.7x4 + 0.4x5 + 0.2x6 = 0.8
0.4x1 + 0.5x2 + 0.7x3 + 1.0x4 + 0.6x5 + 0.3x6 = 0.7
0.1x1 + 0.2x2 + 0.4x3 + 0.6x4 + 1.0x5 + 0.5x6 = 0.6
0.1x2 + 0.2x3 + 0.3x4 + 0.5x5 + 1.0x6 = 0.5.
Note. These equations may also be solved by matrix inversion, as discussed in
Section 2.2.
2.1.8
Show that (in 3-D space)
(a)
X
i
δii = 3,
(b)
X
i j
δi jεi jk = 0,
(c)
X
pq
εipqε jpq = 2δi j,
(d)
X
i jk
εi jkεi jk = 6.

2.2 Matrices
95
Note. The symbol δi j is the Kronecker delta, deﬁned in Eq. (1.164), and εi jk is the
Levi-Civita symbol, Eq. (2.8).
2.1.9
Show that (in 3-D space)
X
k
εi jkεpqk = δipδ jq −δiqδ jp.
Note. See Exercise 2.1.8 for deﬁnitions of δi j and εi jk.
2.2
MATRICES
Matrices are 2-D arrays of numbers or functions that obey the laws that deﬁne matrix
algebra. The subject is important for physics because it facilitates the description of
linear transformations such as changes of coordinate systems, provides a useful formu-
lation of quantum mechanics, and facilitates a variety of analyses in classical and rel-
ativistic mechanics, particle theory, and other areas. Note also that the development of
a mathematics of two-dimensionally ordered arrays is a natural and logical extension of
concepts involving ordered pairs of numbers (complex numbers) or ordinary vectors (one-
dimensional arrays).
The most distinctive feature of matrix algebra is the rule for the multiplication of
matrices. As we will see in more detail later, the algebra is deﬁned so that a set of lin-
ear equations such as
a1x1 + a2x2 = h1
b1x1 + b2x2 = h2
can be written as a single matrix equation of the form
a1
a2
b1
b2
x1
x2

=
h1
h2

.
In order for this equation to be valid, the multiplication indicated by writing the two
matrices next to each other on the left-hand side has to produce the result
a1x1 + a2x2
b1x1 + b2x2

and the statement of equality in the equation has to mean element-by-element agreement of
its left-hand and right-hand sides. Let’s move now to a more formal and precise description
of matrix algebra.
Basic Deﬁnitions
A matrix is a set of numbers or functions in a 2-D square or rectangular array. There are
no inherent limitations on the number of rows or columns. A matrix with m (horizontal)
rows and n (vertical) columns is known as an m × n matrix, and the element of a matrix A
in row i and column j is known as its i, j element, often labeled ai j. As already observed

96
Chapter 2 Determinants and Matrices


u1
u2
u3
u4




4
2
−1
3
0
1


6
7
0
1
4
3

0
1
1
0

 a11
a12

FIGURE 2.1
From left to right, matrices of dimension 4 × 1 (column vector),
3 × 2, 2 × 3, 2 × 2 (square), 1 × 2 (row vector).
when we introduced determinants, when row and column indices or dimensions are men-
tioned together, it is customary to write the row indicator ﬁrst. Note also that order matters,
in general the i, j and j,i elements of a matrix are different, and (if m ̸= n) n × m and
m × n matrices even have different shapes. A matrix for which n = m is termed square;
one consisting of a single column (an m ×1 matrix) is often called a column vector, while
a matrix with only one row (therefore 1 × n) is a row vector. We will ﬁnd that identi-
fying these matrices as vectors is consistent with the properties identiﬁed for vectors in
Section 1.7.
The arrays constituting matrices are conventionally enclosed in parentheses (not vertical
lines, which indicate determinants, or square brackets). A few examples of matrices are
shown in Fig. 2.1. We will usually write the symbols denoting matrices as upper-case
letters in a sans-serif font (as we did when introducing A); when a matrix is known to be a
column vector we often denote it by a lower-case boldface letter in a Roman font (e.g., x).
Perhaps the most important fact to note is that the elements of a matrix are not combined
with one another. A matrix is not a determinant. It is an ordered array of numbers, not a
single number. To refer to the determinant whose elements are those of a square matrix A
(more simply, “the determinant of A”), we can write det(A).
Matrices, so far just arrays of numbers, have the properties we assign to them. These
properties must be speciﬁed to complete the deﬁnition of matrix algebra.
Equality
If A and B are matrices, A = B only if ai j = bi j for all values of i and j. A necessary but
not sufﬁcient condition for equality is that both matrices have the same dimensions.
Addition, Subtraction
Addition and subtraction are deﬁned only for matrices A and B of the same dimensions, in
which case A±B = C, with ci j = ai j ±bi j for all values of i and j, the elements combining
according to the law of ordinary algebra (or arithmetic if they are simple numbers). This
means that C will be a matrix of the same dimensions as A and B. Moreover, we see that
addition is commutative: A + B = B + A. It is also associative, meaning that (A + B) +
C = A + (B + C). A matrix with all elements zero, called a null matrix or zero matrix,
can either be written as O or as a simple zero, with its matrix character and dimensions
determined from the context. Thus, for all A,
A + 0 = 0 + A = A.
(2.25)

2.2 Matrices
97
Multiplication (by a Scalar)
Here what we mean by a scalar is an ordinary number or function (not another matrix).
The multiplication of matrix A by the scalar quantity α produces B = αA, with bi j = α ai j
for all values of i and j. This operation is commutative, with α A = Aα.
Note that the deﬁnition of multiplication by a scalar causes each element of matrix A to
be multiplied by the scalar factor. This is in striking contrast to the behavior of determinants
in which α det(A) is a determinant in which the factor α multiplies only one column or
one row of det(A) and not every element of the entire determinant. If A is an n × n square
matrix, then
det(αA) = αn det(A).
Matrix Multiplication (Inner Product)
Matrix multiplication is not an element-by-element operation like addition or multiplica-
tion by a scalar. Instead, it is a more complicated operation in which each element of the
product is formed by combining elements of a row of the ﬁrst operand with correspond-
ing elements of a column of the second operand. This mode of combination proves to be
that which is needed for many purposes, and gives matrix algebra its power for solving
important problems. This inner product of matrices A and B is deﬁned as
A B = C,
with
ci j =
X
k
aikbkj.
(2.26)
This deﬁnition causes the i j element of C to be formed from the entire ith row of A and
the entire jth column of B. Obviously this deﬁnition requires that A have the same number
of columns (n) as B has rows. Note that the product will have the same number of rows
as A and the same number of columns as B. Matrix multiplication is deﬁned only if these
conditions are met. The summation in Eq. (2.26) is over the range of k from 1 to n, and,
more explicitly, corresponds to
ci j = ai1 b1 j + ai2 b2 j + ··· + a1n bnj.
This combination rule is of a form similar to that of the dot product of the vectors
(ai1,ai2,...,ain) and (b1 j,b2 j,...,bnj). Because the roles of the two operands in a matrix
multiplication are different (the ﬁrst is processed by rows, the second by columns), the
operation is in general not commutative, that is, A B ̸= B A. In fact, A B may even have a
different shape than B A. If A and B are square, it is useful to deﬁne the commutator of
A and B,
[A, B] = A B −B A,
(2.27)
which, as stated above, will in many cases be nonzero.
Matrix multiplication is associative, meaning that (AB)C = A(BC). Proof of this state-
ment is the topic of Exercise 2.2.26.

98
Chapter 2 Determinants and Matrices
Example 2.2.1
MULTIPLICATION, PAULI MATRICES
These three 2 × 2 matrices, which occurred in early work in quantum mechanics by Pauli,
are encountered frequently in physics contexts, so a familiarity with them is highly advis-
able. They are
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
σ3 =
1
0
0
−1

.
(2.28)
Let’s form σ1σ2. The 1,1 element of the product involves the ﬁrst row of σ1 and the ﬁrst
column of σ2; these are shaded and lead to the indicated computation:
0
1
1
0
0
−i
i
0

→0(0) + 1(i) = i.
Continuing, we have
σ1σ2 =
0(0) + 1(i)
0(−i) + 1(0)
1(0) + 0(i)
1(−i) + 0(0)

=
i
0
0
−i

.
(2.29)
In a similar fashion, we can compute
σ2σ1 =
0
−i
i
0
0
1
1
0

=
−i
0
0
i

.
(2.30)
It is clear that σ1 and σ2 do not commute. We can construct their commutator:
[σ1,σ2] = σ1σ2 −σ2σ1 =
i
0
0
−i

−
−i
0
0
i

= 2i
1
0
0
−1

= 2iσ3.
(2.31)
Note that not only have we veriﬁed that σ1 and σ2 do not commute, we have even evaluated
and simpliﬁed their commutator.
■
Example 2.2.2
MULTIPLICATION, ROW AND COLUMN MATRICES
As a second example, consider
A =


1
2
3

,
B =
 4
5
6
.
Let us form A B and B A:
A B =


4
5
6
8
10
12
12
15
18

,
B A = (4 × 1 + 5 × 2 + 6 × 3) = (32).
The results speak for themselves. Often when a matrix operation leads to a 1 × 1 ma-
trix, the parentheses are dropped and the result is treated as an ordinary number or
function.
■

2.2 Matrices
99
Unit Matrix
By direct matrix multiplication, it is possible to show that a square matrix with elements
of value unity on its principal diagonal (the elements (i, j) with i = j), and zeros every-
where else, will leave unchanged any matrix with which it can be multiplied. For example,
the 3 × 3 unit matrix has the form


1
0
0
0
1
0
0
0
1

;
note that it is not a matrix all of whose elements are unity. Giving such a matrix the name 1,
1A = A1 = A.
(2.32)
In interpreting this equation, we must keep in mind that unit matrices, which are square
and therefore of dimensions n × n, exist for all n; the n values for use in Eq. (2.32) must
be those consistent with the applicable dimension of A. So if A is m × n, the unit matrix in
1A must be m × m, while that in A1 must be n × n.
The previously introduced null matrices have only zero elements, so it is also obvious
that for all A,
O A = A O = O.
(2.33)
Diagonal Matrices
If a matrix D has nonzero elements di j only for i = j, it is said to be diagonal; a 3 × 3
example is
D =


1
0
0
0
2
0
0
0
3

.
The rules of matrix multiplication cause all diagonal matrices (of the same size) to com-
mute with each other. However, unless proportional to a unit matrix, diagonal matrices
will not commute with nondiagonal matrices containing arbitrary elements.
Matrix Inverse
It will often be the case that given a square matrix A, there will be a square matrix B such
that A B = B A = 1. A matrix B with this property is called the inverse of A and is given
the name A−1. If A−1 exists, it must be unique. The proof of this statement is simple: If B
and C are both inverses of A, then
A B = B A = A C = C A = 1.
We now look at
C A B = (C A)B = B,
but also
C A B = C(A B) = C.
This shows that B = C.

100
Chapter 2 Determinants and Matrices
Every nonzero real (or complex) number α has a nonzero multiplicative inverse, often
written 1/α. But the corresponding property does not hold for matrices; there exist nonzero
matrices that do not have inverses. To demonstrate this, consider the following:
A =
1
1
0
0

,
B =
 1
0
−1
0

,
so
A B =
0
0
0
0

.
If A has an inverse, we can multiply the equation A B = O on the left by A−1, thereby
obtaining
AB = O −→A−1AB = A−1O −→B = O.
Since we started with a matrix B that was nonzero, this is an inconsistency, and we are
forced to conclude that A−1 does not exist. A matrix without an inverse is said to be singu-
lar, so our conclusion is that A is singular. Note that in our derivation, we had to be careful
to multiply both members of A B = O from the left, because multiplication is noncommu-
tative. Alternatively, assuming B−1 to exist, we could multiply this equation on the right
by B−1, obtaining
AB = O −→ABB−1 = OB−1 −→A = O.
This is inconsistent with the nonzero A with which we started; we conclude that B is also
singular. Summarizing, there are nonzero matrices that do not have inverses and are iden-
tiﬁed as singular.
The algebraic properties of real and complex numbers (including the existence of
inverses for all nonzero numbers) deﬁne what mathematicians call a ﬁeld. The properties
we have identiﬁed for matrices are different; they form what is called a ring.
The numerical inversion of matrices is another topic that has been given much attention,
and computer programs for matrix inversion are widely available. A closed, but cumber-
some formula for the inverse of a matrix exists; it expresses the elements of A−1 in terms of
the determinants that are the minors of det(A); recall that minors were deﬁned in the para-
graph immediately before Eq. (2.14). That formula, the derivation of which is in several of
the Additional Readings, is
(A−1)i j = (−1)i+ j M ji
det(A)
.
(2.34)
We describe here a well-known method that is computationally more efﬁcient than
Eq. (2.34), namely the Gauss-Jordan procedure.
Example 2.2.3
GAUSS-JORDAN MATRIX INVERSION
The Gauss-Jordan method is based on the fact that there exist matrices ML such that the
product MLA will leave an arbitrary matrix A unchanged, except with
(a)
one row multiplied by a constant, or
(b)
one row replaced by the original row minus a multiple of another row, or
(c)
the interchange of two rows.
The actual matrices ML that carry out these transformations are the subject of Exercise 2.2.21.

2.2 Matrices
101
By using these transformations, the rows of a matrix can be altered (by matrix multipli-
cation) in the same ways we were able to change the elements of determinants, so we can
proceed in ways similar to those employed for the reduction of determinants by Gauss elim-
ination. If A is nonsingular, the application of a succession of ML, i.e., M = (... M′′
LM′
LML),
can reduce A to a unit matrix:
M A = 1,
or
M = A−1.
Thus, what we need to do is apply successive transformations to A until these transforma-
tions have reduced A to 1, keeping track of the product of these transformations. The way
in which we keep track is to successively apply the transformations to a unit matrix.
Here is a concrete example. We want to invert the matrix
A =


3
2
1
2
3
1
1
1
4

.
Our strategy will be to write, side by side, the matrix A and a unit matrix of the same size,
and to perform the same operations on each until A has been converted to a unit matrix,
which means that the unit matrix will have been changed to A−1. We start with


3
2
1
2
3
1
1
1
4


and


1
0
0
0
1
0
0
0
1

.
We multiply the rows as necessary to set to unity all elements of the ﬁrst column of the left
matrix:


1
2
3
1
3
1
3
2
1
2
1
1
4


and


1
3
0
0
0
1
2
0
0
0
1


.
Subtracting the ﬁrst row from the second and third rows, we obtain


1
2
3
1
3
0
5
6
1
6
0
1
3
11
3


and


1
3
0
0
−1
3
1
2
0
−1
3
0
1


.

102
Chapter 2 Determinants and Matrices
Then we divide the second row (of both matrices) by 5
6 and subtract 2
3 times it from the
ﬁrst row and 1
3 times it from the third row. The results for both matrices are


1
0
1
5
0
1
1
5
0
0
18
5


and


3
5
−2
5
0
−2
5
3
5
0
−1
5
−1
5
1


.
We divide the third row (of both matrices) by 18
5 . Then as the last step, 1
5 times the third
row is subtracted from each of the ﬁrst two rows (of both matrices). Our ﬁnal pair is


1
0
0
0
1
0
0
0
1


and
A−1 =


11
18
−7
18
−1
18
−7
18
11
18
−1
18
−1
18
−1
18
5
18


.
We can check our work by multiplying the original A by the calculated A−1 to see if we
really do get the unit matrix 1.
■
Derivatives of Determinants
The formula giving the inverse of a matrix in terms of its minors enables us to write a
compact formula for the derivative of a determinant det(A) where the matrix A has ele-
ments that depend on some variable x. To carry out the differentiation with respect to the
x dependence of its element ai j, we write det(A) as its expansion in minors Mi j about the
elements of row i, as in Eq. (2.14), so, appealing also to Eq. (2.34), we have
∂det(A)
∂ai j
= (−1)i+ j Mi j = (A−1) ji det(A).
Applying now the chain rule to allow for the x dependence of all elements of A, we get
d det(A)
dx
= det(A)
X
i j
(A−1) ji
dai j
dx .
(2.35)
Systems of Linear Equations
Using the matrix inverse, we can write down formal solutions to linear equation systems.
To start, we note that if A is a n × n square matrix, and x and h are n × 1 column vectors,
the matrix equation Ax = h is, by the rule for matrix multiplication,
Ax =


a11x1 + a12x2 + ··· + a1nxn
a21x1 + a22x2 + ··· + a2nxn
············
an1x1 + an2x2 + ··· + annxn

= h =


h1
h2
···
hn

,

2.2 Matrices
103
which is entirely equivalent to a system of n linear equations with the elements of A as
coefﬁcients. If A is nonsingular, we can multiply Ax = h on the left by A−1, obtaining the
result x = A−1h.
This result tells us two things: (1) that if we can evaluate A−1, we can compute the
solution x; and (2) that the existence of A−1 means that this equation system has a
unique solution. In our study of determinants we found that a linear equation system had a
unique solution if and only if the determinant of its coefﬁcients was nonzero. We therefore
see that the condition that A−1 exists, i.e., that A is nonsingular, is the same as the condi-
tion that the determinant of A, which we write det(A), be nonzero. This result is important
enough to be emphasized:
A square matrix A is singular if and only if det(A) = 0.
(2.36)
Determinant Product Theorem
The connection between matrices and their determinants can be made deeper by estab-
lishing a product theorem which states that the determinant of a product of two n × n
matrices A and B is equal to the products of the determinants of the individual matrices:
det(A B) = det(A)det(B).
(2.37)
As an initial step toward proving this theorem, let us look at det(A B) with the elements of
the matrix product written out. Showing the ﬁrst two columns explicitly, we have
det(A B) =

a11b11 + a12b21 + ··· + a1nbn1
a11b12 + a12b22 + ··· + a1nbn2
···
a21b11 + a22b21 + ··· + a2nbn1
a21b12 + a22b22 + ··· + a2nbn2
···
···
···
···
···
···
···
an1b11 + an2b21 + ··· + annbn1
an1b12 + an2b22 + ··· + annbn2
···

.
Introducing the notation
a j =


a1 j
a2 j
···
anj

,
this becomes
det(A B) =

X
j1
a j1b j1,1
X
j2
a j2b j2,2
···

,
where the summations over j1, j2, ..., jn run independently from 1 though n. Now, calling
upon Eqs. (2.12) and (2.13), we can move the summations and the factors b outside the
determinant, reaching
det(A B) =
X
j1
X
j2
···
X
jn
b j1,1b j2,2 ···b jn,n det(a j1a j2 ··· a jn).
(2.38)
The determinant on the right-hand side of Eq. (2.38) will vanish if any of the indices jµ
are equal; if all are unequal, that determinant will be ±det(A), with the sign corresponding
to the parity of the column permutation needed to put the a j in numerical order. Both

104
Chapter 2 Determinants and Matrices
of these conditions are met by writing det(a j1a j2 ··· a jn) = ε j1... jn det(A), where ε is the
Levi-Civita symbol deﬁned in Eq. (2.8). The above manipulations bring us to
det(A B) = det(A)
X
j1... jn
ε j1... jnb j1,1b j2,2 ···b jn,n = det(A)det(B),
where the ﬁnal step was to invoke the deﬁnition of the determinant, Eq. (2.10). This result
proves the determinant product theorem.
From the determinant product theorem, we can gain additional insight regarding singular
matrices. Noting ﬁrst that a special case of the theorem is that
det(A A−1) = det(1) = 1 = det(A)det(A−1),
we see that
det(A−1) =
1
det(A).
(2.39)
It is now obvious that if det(A) = 0, then det(A−1) cannot exist, meaning that A−1 cannot
exist either. This is a direct proof that a matrix is singular if and only if it has a vanishing
determinant.
Rank of a Matrix
The concept of matrix singularity can be reﬁned by introducing the notion of the rank
of a matrix. If the elements of a matrix are viewed as the coefﬁcients of a set of linear
forms, as in Eq. (2.1) and its generalization to n variables, a square matrix is assigned a
rank equal to the number of linearly independent forms that its elements describe. Thus, a
nonsingular n × n matrix will have rank n, while a n × n singular matrix will have a rank
r less than n. The rank provides a measure of the extent of the singularity; if r = n −1,
the matrix describes one linear form that is dependent on the others; r = n −2 describes
a situation in which there are two forms that are linearly dependent on the others, etc. We
will in Chapter 6 take up methods for systematically determining the rank of a matrix.
Transpose, Adjoint, Trace
In addition to the operations we have already discussed, there are further operations that
depend on the fact that matrices are arrays. One such operation is transposition. The
transpose of a matrix is the matrix that results from interchanging its row and column
indices. This operation corresponds to subjecting the array to reﬂection about its principal
diagonal. If a matrix is not square, its transpose will not even have the same shape as the
original matrix. The transpose of A, denoted ˜A or sometimes AT , thus has elements
(˜A)i j = a ji.
(2.40)

2.2 Matrices
105
Note that transposition will convert a column vector into a row vector, so
if
x =


x1
x2
...
xn

,
then
˜x = (x1 x2 ... xn).
A matrix that is unchanged by transposition (i.e., ˜A = A) is called symmetric.
For matrices that may have complex elements, the complex conjugate of a matrix is
deﬁned as the matrix resulting if all elements of the original matrix are complex conju-
gated. Note that this does not change the shape or move any elements to new positions.
The notation for the complex conjugate of A is A∗.
The adjoint of a matrix A, denoted A†, is obtained by both complex conjugating and
transposing it (the same result is obtained if these operations are performed in either order).
Thus,
(A†)i j = a∗
ji.
(2.41)
The trace, a quantity deﬁned for square matrices, is the sum of the elements on the
principal diagonal. Thus, for an n × n matrix A,
trace(A) =
n
X
i=1
aii.
(2.42)
From the rule for matrix addition, is is obvious that
trace(A + B) = trace(A) + trace(B).
(2.43)
Another property of the trace is that its value for a product of two matrices A and B is
independent of the order of multiplication:
trace(AB) =
X
i
(AB)ii =
X
i
X
j
ai jb ji =
X
j
X
i
b jiai j
=
X
j
(BA) j j = trace(BA).
(2.44)
This holds even if AB ̸= BA. Equation (2.44) means that the trace of any commutator
[A, B] = AB −BA is zero. Considering now the trace of the matrix product ABC, if we
group the factors as A(BC), we easily see that
trace(ABC) = trace(BCA).
Repeating this process, we also ﬁnd trace(ABC) = trace(CAB). Note, however, that we
cannot equate any of these quantities to trace(CBA) or to the trace of any other noncyclic
permutation of these matrices.

106
Chapter 2 Determinants and Matrices
Operations on Matrix Products
We have already seen that the determinant and the trace satisfy the relations
det(AB) = det(A)det(B) = det(BA),
trace(AB) = trace(BA),
whether or not A and B commute. We also found that trace(A + B) = trace(A) + trace(B)
and can easily show that trace(αA) = α trace(A), establishing that the trace is a linear
operator (as deﬁned in Chapter 5). Since similar relations do not exist for the determinant,
it is not a linear operator.
We consider now the effect of other operations on matrix products. The transpose of a
product, (AB)T, can be shown to satisfy
(AB)T = ˜B˜A,
(2.45)
showing that a product is transposed by taking, in reverse order, the transposes of its fac-
tors. Note that if the respective dimensions of A and B are such as to make AB deﬁned, it
will also be true that ˜B˜A is deﬁned.
Since complex conjugation of a product simply amounts to conjugation of its individual
factors, the formula for the adjoint of a matrix product follows a rule similar to Eq. (2.45):
(AB)† = B†A†.
(2.46)
Finally, consider (AB)−1. In order for AB to be nonsingular, neither A nor B can be
singular (to see this, consider their determinants). Assuming this nonsingularity, we have
(AB)−1 = B−1A−1.
(2.47)
The validity of Eq. (2.47) can be demonstrated by substituting it into the obvious equation
(AB)(AB)−1 = 1.
Matrix Representation of Vectors
The reader may have already noted that the operations of addition and multiplication by a
scalar are deﬁned in identical ways for vectors (Section 1.7) and the matrices we are calling
column vectors. We can also use the matrix formalism to generate scalar products, but in
order to do so we must convert one of the column vectors into a row vector. The operation
of transposition provides a way to do this. Thus, letting a and b stand for vectors in IR3,
a · b
−→
(a1 a2 a3)


b1
b2
b3

= a1b1 + a2b2 + a3b3.
If in a matrix context we regard a and b as column vectors, the above equation assumes
the form
a · b
−→
aT b.
(2.48)
This notation does not really lead to signiﬁcant ambiguity if we note that when dealing with
matrices, we are using lower-case boldface symbols to denote column vectors. Note also
that because aT b is a 1 × 1 matrix, it is synonymous with its transpose, which is bT a. The

2.2 Matrices
107
matrix notation preserves the symmetry of the dot product. As in Section 1.7, the square of
the magnitude of the vector corresponding to a will be aT a.
If the elements of our column vectors a and b are real, then an alternate way of writing
aT b is a†b. But these quantities are not equal if the vectors have complex elements, as will
be the case in some situations in which the column vectors do not represent displacements
in physical space. In that situation, the dagger notation is the more useful because then a†a
will be real and can play the role of a magnitude squared.
Orthogonal Matrices
A real matrix (one whose elements are real) is termed orthogonal if its transpose is equal
to its inverse. Thus, if S is orthogonal, we may write
S−1 = ST, or SST = 1
(S orthogonal).
(2.49)
Since, for S orthogonal, det(SST ) = det(S)det(ST ) = [det(S)]2 = 1, we see that
det(S) = ±1
(S orthogonal).
(2.50)
It is easy to prove that if S and S′ are each orthogonal, then so also are SS′ and S′S.
Unitary Matrices
Another important class of matrices consists of matrices U with the property that U† =
U−1, i.e., matrices for which the adjoint is also the inverse. Such matrices are identiﬁed as
unitary. One way of expressing this relationship is
U U† = U†U = 1
(U unitary).
(2.51)
If all the elements of a unitary matrix are real, the matrix is also orthogonal.
Since for any matrix det(AT ) = det(A), and therefore det(A†) = det(A)∗, application of
the determinant product theorem to a unitary matrix U leads to
det(U)det(U†) = |det(U)|2 = 1,
(2.52)
showing that det(U) is a possibly complex number of magnitude unity. Since such numbers
can be written in the form exp(iθ), with θ real, the determinants of U and U† will, for
some θ, satisfy
det(U) = eiθ,
det(U†) = e−iθ.
Part of the signiﬁcance of the term unitary is associated with the fact that the determinant
has unit magnitude. A special case of this relationship is our earlier observation that if U is
real, and therefore also an orthogonal matrix, its determinant must be either +1 or −1.
Finally, we observe that if U and V are both unitary, then UV and VU will be unitary as
well. This is a generalization of our earlier result that the matrix product of two orthogonal
matrices is also orthogonal.

108
Chapter 2 Determinants and Matrices
Hermitian Matrices
There are additional classes of matrices with useful characteristics. A matrix is identiﬁed as
Hermitian, or, synonymously, self-adjoint, if it is equal to its adjoint. To be self-adjoint,
a matrix H must be square, and in addition, its elements must satisfy
(H†)i j = (H)i j
−→
h∗
ji = hi j
(H is Hermitian).
(2.53)
This condition means that the array of elements in a self-adjoint matrix exhibits a reﬂection
symmetry about the principal diagonal: elements whose positions are connected by reﬂec-
tion must be complex conjugates. As a corollary to this observation, or by direct reference
to Eq. (2.53), we see that the diagonal elements of a self-adjoint matrix must be real.
If all the elements of a self-adjoint matrix are real, then the condition of self-adjointness
will cause the matrix also to be symmetric, so all real, symmetric matrices are self-adjoint
(Hermitian).
Note that if two matrices A and B are Hermitian, it is not necessarily true that AB or BA
is Hermitian; however, AB + BA, if nonzero, will be Hermitian, and AB −BA, if nonzero,
will be anti-Hermitian, meaning that (AB −BA)† = −(AB −BA).
Extraction of a Row or Column
It is useful to deﬁne column vectors ˆei which are zero except for the (i,1) element, which
is unity; examples are
ˆe1 =


1
0
0
···
0


,
ˆe2 =


0
1
0
···
0


,
etc.
(2.54)
One use of these vectors is to extract a single column from a matrix. For example, if A is a
3 × 3 matrix, then
Aˆe2 =


a11
a12
a13
a21
a22
a23
a31
a32
a33




0
1
0

=


a12
a22
a32

.
The row vector ˆeT
i can be used in a similar fashion to extract a row from an arbitrary
matrix, as in
ˆeT
i A = (ai1 ai2 ai3).
These unit vectors will also have many uses in other contexts.
Direct Product
A second procedure for multiplying matrices, known as the direct tensor or Kronecker
product, combines a m × n matrix A and a m′ × n′ matrix B to make the direct product

2.2 Matrices
109
matrix C = A ⊗B, which is of dimension mm′ × nn′ and has elements
Cαβ = Ai j Bkl,
(2.55)
with α = m′(i −1) + k, β = n′( j −1) + l. The direct product matrix uses the indices of
the ﬁrst factor as major and those of the second factor as minor; it is therefore a noncom-
mutative process. It is, however, associative.
Example 2.2.4
DIRECT PRODUCTS
We give some speciﬁc examples. If A and B are both 2 × 2 matrices, we may write, ﬁrst in
a somewhat symbolic and then in a completely expanded form,
A ⊗B =
a11B
a12B
a21B
a22B

=


a11b11
a11b12
a12b11
a12b12
a11b21
a11b22
a12b21
a12b22
a21b11
a21b12
a22b11
a22b12
a21b21
a21b22
a22b21
a22b22

.
Another example is the direct product of two two-element column vectors, x and y.
Again writing ﬁrst in symbolic, and then expanded form,
x1
x2

⊗
y1
y2

=
x1y
x2y

=


x1y1
x1y2
x2y1
x2y2

.
A third example is the quantity AB from Example 2.2.2. It is an instance of the special
case (column vector times row vector) in which the direct and inner products coincide:
AB = A ⊗B.
■
If C and C′ are direct products of the respective forms
C = A ⊗B
and
C′ = A′ ⊗B′,
(2.56)
and these matrices are of dimensions such that the matrix inner products AA′ and BB′ are
deﬁned, then
CC′ = (AA′) ⊗(BB′).
(2.57)
Moreover, if matrices A and B are of the same dimensions, then
C ⊗(A + B) = C ⊗A + C ⊗B
and
(A + B) ⊗C = A ⊗C + B ⊗C.
(2.58)
Example 2.2.5
DIRAC MATRICES
In the original, nonrelativistic formulation of quantum mechanics, agreement between
theory and experiment for electronic systems required the introduction of the concept of
electron spin (intrinsic angular momentum), both to provide a doubling in the number of
available states and to explain phenomena involving the electron’s magnetic moment. The
concept was introduced in a relatively ad hoc fashion; the electron needed to be given
spin quantum number 1/2, and that could be done by assigning it a two-component wave

110
Chapter 2 Determinants and Matrices
function, with the spin-related properties described using the Pauli matrices, which were
introduced in Example 2.2.1:
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
σ3 =
1
0
0
−1

.
Of relevance here is the fact that these matrices anticommute and have squares that are unit
matrices:
σ 2
i = 12,
and
σiσ j + σ jσi = 0,
i ̸= j.
(2.59)
In 1927, P. A. M. Dirac developed a relativistic formulation of quantum mechanics
applicable to spin-1/2 particles. To do this it was necessary to place the spatial and time
variables on an equal footing, and Dirac proceeded by converting the relativistic expression
for the kinetic energy to an expression that was ﬁrst order in both the energy and the
momentum (parallel quantities in relativistic mechanics). He started from the relativistic
equation for the energy of a free particle,
E2 = (p2
1 + p2
2 + p2
3)c2 + m2c4 = p2c2 + m2c4,
(2.60)
where pi are the components of the momentum in the coordinate directions, m is the
particle mass, and c is the velocity of light. In the passage to quantum mechanics, the
quantities pi are to be replaced by the differential operators −i ¯h∂/∂xi, and the entire
equation is applied to a wave function.
It was desirable to have a formulation that would yield a two-component wave function
in the nonrelativistic limit and therefore might be expected to contain the σi. Dirac made
the observation that a key to the solution of his problem was to exploit the fact that the
Pauli matrices, taken together as a vector
σ = σ1ˆe1 + σ2ˆe2 + σ3ˆe3,
(2.61)
could be combined with the vector p to yield the identity
(σ · p)2 = p212,
(2.62)
where 12 denotes a 2 × 2 unit matrix. The importance of Eq. (2.62) is that, at the price of
going to 2×2 matrices, we can linearize the quadratic occurrences of E and p in Eq. (2.60)
as follows. We ﬁrst write
E212 −c2(σ · p)2 = m2c412.
(2.63)
We then factor the left-hand side of Eq. (2.63) and apply both sides of the resulting equation
(which is a 2 × 2 matrix equation) to a two-component wave function that we will call ψ1:
(E12 + c σ · p)(E12 −c σ · p)ψ1 = m2c4ψ1.
(2.64)
The meaning of this equation becomes clearer if we make the additional deﬁnition
(E12 −c σ · p)ψ1 = mc2ψ2.
(2.65)

2.2 Matrices
111
Substituting Eq. (2.65) into Eq. (2.64), we can then write the modiﬁed Eq. (2.64) and the
(unchanged) Eq. (2.65) as the equation set
(E12 + c σ · p)ψ2 = mc2ψ1,
(E12 −c σ · p)ψ1 = mc2ψ2;
(2.66)
both these equations will need to be satisﬁed simultaneously.
To bring Eqs. (2.66) to the form actually used by Dirac, we now make the substitution
ψ1 = ψA + ψB, ψ2 = ψA −ψB, and then add and subtract the two equations from each
other, reaching a set of coupled equations in ψA and ψB:
EψA −cσ · pψB = mc2ψA,
cσ · pψA −EψB = mc2ψB.
In anticipation of what we will do next, we write these equations in the matrix form
E12
0
0
−E12

−

0
cσ · p
−cσ · p
0
ψA
ψB

= mc2
ψA
ψB

.
(2.67)
We can now use the direct product notation to condense Eq. (2.67) into the simpler form
[(σ3 ⊗12)E −γ ⊗c(σ · p)]9 = mc29,
(2.68)
where 9 is the four-component wave function built from the two-component wave
functions:
9 =
ψA
ψB

,
and the terms on the left-hand side have the indicated structure because
σ3 =
1
0
0
−1

and we deﬁne
γ =
 0
1
−1
0

.
(2.69)
It has become customary to identify the matrices in Eq. (2.68) as γ µ and to refer to them
as Dirac matrices, with
γ 0 = σ3 ⊗12 =
12
0
0
−12

=


1
0
0
0
0
1
0
0
0
0
−1
0
0
0
0
−1

.
(2.70)
The matrices resulting from the individual components of σ in Eq. (2.68) are (for
i = 1,2,3)
γ i = γ ⊗σi =
 0
σi
−σi
0

.
(2.71)

112
Chapter 2 Determinants and Matrices
Expanding Eq. (2.71), we have
γ 1 =


0
0
0
1
0
0
1
0
0
−1
0
0
−1
0
0
0

,
γ 2 =


0
0
0
−i
0
0
i
0
0
i
0
0
−i
0
0
0

,
γ 3 =


0
0
1
0
0
0
0
−1
−1
0
0
0
0
1
0
0

.
(2.72)
Now that the γ µ have been deﬁned, we can rewrite Eq. (2.68), expanding σ · p into
components:
h
γ 0E −c(γ 1 p1 + γ 2 p2 + γ 3 p3)
i
9 = mc29.
To put this matrix equation into the speciﬁc form known as the Dirac equation we multiply
both sides of it (on the left) by γ 0. Noting that (γ 0)2 = 1 and giving γ 0γ i the new name
αi, we reach
h
γ 0mc2 + c(α1 p1 + α2 p2 + α3 p3)
i
9 = E9.
(2.73)
Equation (2.73) is in the notation used by Dirac with the exception that he used β as the
name for the matrix here called γ 0.
The Dirac gamma matrices have an algebra that is a generalization of that exhibited
by the Pauli matrices, where we found that the σ 2
i = 1 and that if i ̸= j, then σi and
σ j anticommute. Either by further analysis or by direct evaluation, it is found that, for
µ = 0,1,2,3 and i = 1,2,3,
(γ 0)2 = 1,
(γ i)2 = −1,
(2.74)
γ µγ i + γ iγ µ = 0,
µ ̸= i.
(2.75)
In the nonrelativistic limit, the four-component Dirac equation for an electron reduces
to a two-component equation in which each component satisﬁes the Schrödinger equation,
with the Pauli and Dirac matrices having completely disappeared. See Exercise 2.2.48.
In this limit, the Pauli matrices reappear if we add to the Schrödinger equation an addi-
tional term arising from the intrinsic magnetic moment of the electron. The passage to
the nonrelativistic limit provides justiﬁcation for the seemingly arbitrary introduction of a
two-component wavefunction and use of the Pauli matrices for discussions of spin angular
momentum.
The Pauli matrices (and the unit matrix 12) form what is known as a Clifford algebra,2
with the properties shown in Eq. (2.59). Since the algebra is based on 2 × 2 matrices, it
can have only four members (the number of linearly independent such matrices), and is
said to be of dimension 4. The Dirac matrices are members of a Clifford algebra of dimen-
sion 16. A complete basis for this Clifford algebra with convenient Lorentz transformation
2D. Hestenes, Am. J. Phys. 39: 1013 (1971); and J. Math. Phys. 16: 556 (1975).

2.2 Matrices
113
properties consists of the 16 matrices
14,
γ 5 = iγ 0γ 1γ 2γ 3 =
 0
12
12
0

,
γ µ
(µ = 0,1,2,3),
γ 5γ µ
(µ = 0,1,2,3),
σ µν = iγ µγ ν
(0 ≤µ < ν ≤3).
(2.76)
■
Functions of Matrices
Polynomials with one or more matrix arguments are well deﬁned and occur often. Power
series of a matrix may also be deﬁned, provided the series converges for each matrix ele-
ment. For example, if A is any n × n matrix, then the power series
exp(A) =
∞
X
j=0
1
j! A j,
(2.77)
sin(A) =
∞
X
j=0
(−1) j
(2 j + 1)! A2 j+1,
(2.78)
cos(A) =
∞
X
j=0
(−1) j
(2 j)! A2 j
(2.79)
are well-deﬁned n × n matrices. For the Pauli matrices σk, the Euler identity for real θ
and k = 1, 2, or 3,
exp(iσkθ) = 12 cosθ + iσk sinθ,
(2.80)
follows from collecting all even and odd powers of θ in separate series using σ 2
k = 1. For
the 4 × 4 Dirac matrices σ µν, deﬁned in Eq. (2.76), we have for 1 ≤µ < ν ≤3,
exp(iσ µνθ) = 14 cosθ + iσ µν sinθ,
(2.81)
while
exp(iσ 0kζ) = 14 coshζ + iσ 0k sinhζ
(2.82)
holds for real ζ because (iσ 0k)2 = 1 for k = 1, 2, or 3.
Hermitian and unitary matrices are related in that U, given as
U = exp(iH),
(2.83)
is unitary if H is Hermitian. To see this, just take the adjoint: U† = exp(−iH†) =
exp(−iH) = [exp(iH)]−1 = U−1.
Another result which is important to identify here is that any Hermitian matrix H satisﬁes
a relation known as the trace formula,
det(exp(H)) = exp(trace(H)).
(2.84)
This formula is derived at Eq. (6.27).

114
Chapter 2 Determinants and Matrices
Finally, we note that the multiplication of two diagonal matrices produces a matrix that
is also diagonal, with elements that are the products of the corresponding elements of the
multiplicands. This result implies that an arbitrary function of a diagonal matrix will also
be diagonal, with diagonal elements that are that function of the diagonal elements of the
original matrix.
Example 2.2.6
EXPONENTIAL OF A DIAGONAL MATRIX
If a matrix A is diagonal, then its nth power is also diagonal, with the original diagonal
matrix elements raised to the nth power. For example, given
σ3 =
1
0
0
−1

,
then
(σ3)n =
1
0
0
(−1)n

.
We can now compute
eσ3 =


∞
X
n=0
1
n!
0
0
∞
X
n=0
(−1)n
n!


=
e
0
0
e−1

.
■
A ﬁnal and important result is the Baker-Hausdorff formula, which, among other
places is used in the coupled-cluster expansions that yield highly accurate electronic struc-
ture calculations on atoms and molecules3:
exp(−T)Aexp(T) = A + [A,T] + 1
2! [[A,T], T] + 1
3! [[[A,T], T], T] + ··· .
(2.85)
Exercises
2.2.1
Show that matrix multiplication is associative, (AB)C = A(BC).
2.2.2
Show that
(A + B)(A −B) = A2 −B2
if and only if A and B commute,
[A, B] = 0.
3F. E. Harris, H. J. Monkhorst, and D. L. Freeman, Algebraic and Diagrammatic Methods in Many-Fermion Theory. New York:
Oxford University Press (1992).

2.2 Matrices
115
2.2.3
(a)
Complex numbers, a + ib, with a and b real, may be represented by (or are iso-
morphic with) 2 × 2 matrices:
a + ib ←→
 a
b
−b
a

.
Show that this matrix representation is valid for (i) addition and (ii) multiplication.
(b)
Find the matrix corresponding to (a + ib)−1.
2.2.4
If A is an n × n matrix, show that
det(−A) = (−1)n det A.
2.2.5
(a)
The matrix equation A2 = 0 does not imply A = 0. Show that the most general
2 × 2 matrix whose square is zero may be written as
 ab
b2
−a2
−ab

,
where a and b are real or complex numbers.
(b)
If C = A + B, in general
det C ̸= det A + det B.
Construct a speciﬁc numerical example to illustrate this inequality.
2.2.6
Given
K =


0
0
i
−i
0
0
0
−1
0

,
show that
Kn = KKK···(n factors) = 1
(with the proper choice of n,n ̸= 0).
2.2.7
Verify the Jacobi identity,
[A,[B, C]] = [B,[A, C]] −[C,[A, B]].
2.2.8
Show that the matrices
A =


0
1
0
0
0
0
0
0
0

,
B =


0
0
0
0
0
1
0
0
0

,
C =


0
0
1
0
0
0
0
0
0


satisfy the commutation relations
[A, B] = C,
[A, C] = 0,
and
[B, C] = 0.

116
Chapter 2 Determinants and Matrices
2.2.9
Let
i =


0
1
0
0
−1
0
0
0
0
0
0
1
0
0
−1
0

,
j =


0
0
0
−1
0
0
−1
0
0
1
0
0
1
0
0
0

,
and
k =


0
0
−1
0
0
0
0
1
1
0
0
0
0
−1
0
0

.
Show that
(a)
i2 = j2 = k2 = −1, where 1 is the unit matrix.
(b)
ij = −ji = k,
jk = −kj = i,
ki = −ik = j.
These three matrices (i, j, and k) plus the unit matrix 1 form a basis for quaternions. An
alternate basis is provided by the four 2 × 2 matrices, iσ1,iσ2,−iσ3, and 1, where the
σi are the Pauli spin matrices of Example 2.2.1.
2.2.10
A matrix with elements ai j = 0 for j < i may be called upper right triangular. The
elements in the lower left (below and to the left of the main diagonal) vanish. Show that
the product of two upper right triangular matrices is an upper right triangular matrix.
2.2.11
The three Pauli spin matrices are
σ1 =
0
1
1
0

,
σ2 =
0
−i
i
0

,
and
σ3 =
1
0
0
−1

.
Show that
(a)
(σi)2 = 12,
(b)
σiσ j = iσk, (i, j,k) = (1,2,3) or a cyclic permutation thereof,
(c)
σiσ j + σ jσi = 2δi j12; 12 is the 2 × 2 unit matrix.
2.2.12
One description of spin-1 particles uses the matrices
Mx = 1
√
2


0
1
0
1
0
1
0
1
0

,
My = 1
√
2


0
−i
0
i
0
−i
0
i
0

,
and
Mz =


1
0
0
0
0
0
0
0
−1

.

2.2 Matrices
117
Show that
(a)
[Mx, My] = iMz, and so on (cyclic permutation of indices). Using the Levi-Civita
symbol, we may write
[Mi, M j] = i
X
k
εi jkMk.
(b)
M2 ≡M2
x + M2
y + M2
z = 2 13, where 13 is the 3 × 3 unit matrix.
(c)
[M2, Mi] = 0,
[Mz, L+] = L+,
[L+, L−] = 2Mz,
where L+ ≡Mx + iMy and L−≡Mx −iMy.
2.2.13
Repeat Exercise 2.2.12, using the matrices for a spin of 3/2,
Mx = 1
2


0
√
3
0
0
√
3
0
2
0
0
2
0
√
3
0
0
√
3
0

,
My = i
2


0
−
√
3
0
0
√
3
0
−2
0
0
2
0
−
√
3
0
0
√
3
0

,
and
Mz = 1
2


3
0
0
0
0
1
0
0
0
0
−1
0
0
0
0
−3

.
2.2.14
If A is a diagonal matrix, with all diagonal elements different, and A and B commute,
show that B is diagonal.
2.2.15
If A and B are diagonal, show that A and B commute.
2.2.16
Show that trace(ABC) = trace(CBA) if any two of the three matrices commute.
2.2.17
Angular momentum matrices satisfy a commutation relation
[M j, Mk] = iMl,
j,k,l cyclic.
Show that the trace of each angular momentum matrix vanishes.
2.2.18
A and B anticommute: AB = −BA. Also, A2 = 1, B2 = 1. Show that trace(A) =
trace(B) = 0.
Note. The Pauli and Dirac matrices are speciﬁc examples.
2.2.19
(a)
If two nonsingular matrices anticommute, show that the trace of each one is zero.
(Nonsingular means that the determinant of the matrix is nonzero.)
(b)
For the conditions of part (a) to hold, A and B must be n ×n matrices with n even.
Show that if n is odd, a contradiction results.
2.2.20
If A−1 has elements
(A−1)i j = a(−1)
i j
= C ji
|A| ,

118
Chapter 2 Determinants and Matrices
where C ji is the jith cofactor of |A|, show that
A−1A = 1.
Hence A−1 is the inverse of A (if |A| ̸= 0).
2.2.21
Find the matrices ML such that the product MLA will be A but with:
(a)
The ith row multiplied by a constant k (ai j →kai j, j = 1, 2, 3,...);
(b)
The ith row replaced by the original ith row minus a multiple of the mth row
(ai j →ai j −Kamj, i = 1, 2, 3, ...);
(c)
The ith and mth rows interchanged (ai j →amj, amj →ai j, j = 1,2,3,...).
2.2.22
Find the matrices MR such that the product AMR will be A but with:
(a)
The ith column multiplied by a constant k (a ji →ka ji, j = 1,2,3,...);
(b)
The ith column replaced by the original ith column minus a multiple of the mth
column (a ji →a ji −ka jm, j = 1,2,3,...);
(c)
The ith and mth columns interchanged (a ji →a jm, a jm →a ji, j = 1, 2, 3, ...).
2.2.23
Find the inverse of
A =


3
2
1
2
2
1
1
1
4

.
2.2.24
Matrices are far too useful to remain the exclusive property of physicists. They may
appear wherever there are linear relations. For instance, in a study of population move-
ment the initial fraction of a ﬁxed population in each of n areas (or industries or
religions, etc.) is represented by an n-component column vector P. The movement of
people from one area to another in a given time is described by an n × n (stochastic)
matrix T. Here Ti j is the fraction of the population in the jth area that moves to the ith
area. (Those not moving are covered by i = j.) With P describing the initial population
distribution, the ﬁnal population distribution is given by the matrix equation TP = Q.
From its deﬁnition, Pn
i=1 Pi = 1.
(a)
Show that conservation of people requires that
n
X
i=1
Ti j = 1,
j = 1,2,...,n.
(b)
Prove that
n
X
i=1
Qi = 1
continues the conservation of people.

2.2 Matrices
119
2.2.25
Given a 6 × 6 matrix A with elements ai j = 0.5|i−j|, i, j = 0,1,2,...,5, ﬁnd A−1.
ANS.
A−1 = 1
3


4
−2
0
0
0
0
−2
5
−2
0
0
0
0
−2
5
−2
0
0
0
0
−2
5
−2
0
0
0
0
−2
5
−2
0
0
0
0
−2
4


.
2.2.26
Show that the product of two orthogonal matrices is orthogonal.
2.2.27
If A is orthogonal, show that its determinant = ±1.
2.2.28
Show that the trace of the product of a symmetric and an antisymmetric matrix is zero.
2.2.29
A is 2 × 2 and orthogonal. Find the most general form of
A =
a
b
c
d

.
2.2.30
Show that
det(A∗) = (det A)∗= det(A†).
2.2.31
Three angular momentum matrices satisfy the basic commutation relation
[Jx, Jy] = iJz
(and cyclic permutation of indices). If two of the matrices have real elements, show that
the elements of the third must be pure imaginary.
2.2.32
Show that (AB)† = B†A†.
2.2.33
A matrix C = S†S. Show that the trace is positive deﬁnite unless S is the null matrix, in
which case trace (C) = 0.
2.2.34
If A and B are Hermitian matrices, show that (AB + BA) and i(AB −BA) are also Her-
mitian.
2.2.35
The matrix C is not Hermitian. Show that then C + C† and i(C −C†) are Hermitian.
This means that a non-Hermitian matrix may be resolved into two Hermitian parts,
C = 1
2(C + C†) + 1
2i i(C −C†).
This decomposition of a matrix into two Hermitian matrix parts parallels the decompo-
sition of a complex number z into x + iy, where x = (z + z∗)/2 and y = (z −z∗)/2i.
2.2.36
A and B are two noncommuting Hermitian matrices:
AB −BA = iC.
Prove that C is Hermitian.
2.2.37
Two matrices A and B are each Hermitian. Find a necessary and sufﬁcient condition for
their product AB to be Hermitian.
ANS.
[A, B] = 0.

120
Chapter 2 Determinants and Matrices
2.2.38
Show that the reciprocal (that is, inverse) of a unitary matrix is unitary.
2.2.39
Prove that the direct product of two unitary matrices is unitary.
2.2.40
If σ is the vector with the σi as components given in Eq. (2.61), and p is an ordinary
vector, show that
(σ · p)2 = p212,
where 12 is a 2 × 2 unit matrix.
2.2.41
Use the equations for the properties of direct products, Eqs. (2.57) and (2.58), to show
that the four matrices γ µ, µ = 0,1,2,3, satisfy the conditions listed in Eqs. (2.74) and
(2.75).
2.2.42
Show that γ 5, Eq. (2.76), anticommutes with all four γ µ.
2.2.43
In this problem, the summations are over µ = 0,1,2,3. Deﬁne gµν = gµν by the
relations
g00 = 1;
gkk = −1,
k = 1,2,3;
gµν = 0,
µ ̸= ν;
and deﬁne γµ as P gνµγ µ. Using these deﬁnitions, show that
(a)
Pγµγ αγ µ = −2γ α,
(b)
Pγµγ αγ βγ µ = 4 gαβ,
(c)
Pγµγ αγ βγ νγ µ = −2γ νγ βγ α.
2.2.44
If M = 1
2(1 + γ 5), where γ 5 is given in Eq. (2.76), show that
M2 = M.
Note that this equation is still satisﬁed if γ is replaced by any other Dirac matrix listed
in Eq. (2.76).
2.2.45
Prove that the 16 Dirac matrices form a linearly independent set.
2.2.46
If we assume that a given 4 × 4 matrix A (with constant elements) can be written as a
linear combination of the 16 Dirac matrices (which we denote here as 0i)
A =
16
X
i=1
ci0i,
show that
ci ∼trace(A0i).
2.2.47
The matrix C = iγ 2γ 0 is sometimes called the charge conjugation matrix. Show that
Cγ µC−1 = −(γ µ)T .
2.2.48
(a)
Show that, by substitution of the deﬁnitions of the γ µ matrices from Eqs. (2.70)
and (2.72), that the Dirac equation, Eq. (2.73), takes the following form when
written as 2 × 2 blocks (with ψL and ψS column vectors of dimension 2). Here

Additional Readings
121
L and S stand, respectively, for “large” and “small” because of their relative size
in the nonrelativistic limit):

mc2 −E
c(σ1 p1 + σ2 p2 + σ3 p3)
−c(σ1 p1 + σ2 p2 + σ3 p3)
−mc2 −E
ψL
ψS

= 0.
(b)
To reach the nonrelativistic limit, make the substitution E = mc2 + ε and approx-
imate −2mc2 −ε by −2mc2. Then write the matrix equation as two simultaneous
two-component equations and show that they can be rearranged to yield
1
2m

p2
1 + p2
2 + p2
3

ψL = εψL,
which is just the Schrödinger equation for a free particle.
(c)
Explain why is it reasonable to call ψL and ψS “large” and “small.”
2.2.49
Show that it is consistent with the requirements that they must satisfy to take the Dirac
gamma matrices to be (in 2 × 2 block form)
γ 0 =
 0
12
12
0

,
γ i =
 0
σi
−σi
0

,
(i = 1,2,3).
This choice for the gamma matrices is called the Weyl representation.
2.2.50
Show that the Dirac equation separates into independent 2 × 2 blocks in the Weyl rep-
resentation (see Exercise 2.2.49) in the limit that the mass m approaches zero. This
observation is important in the ultra relativistic regime where the rest mass is inconse-
quential, or for particles of negligible mass (e.g., neutrinos).
2.2.51
(a)
Given r′ = Ur, with U a unitary matrix and r a (column) vector with complex
elements, show that the magnitude of r is invariant under this operation.
(b)
The matrix U transforms any column vector r with complex elements into r′,
leaving the magnitude invariant: r†r = r′†r′. Show that U is unitary.
Additional Readings
Aitken, A. C., Determinants and Matrices. New York: Interscience (1956), reprinted, Greenwood (1983). A read-
able introduction to determinants and matrices.
Barnett, S., Matrices: Methods and Applications. Oxford: Clarendon Press (1990).
Bickley, W. G., and R. S. H. G. Thompson, Matrices—Their Meaning and Manipulation. Princeton, NJ: Van
Nostrand (1964). A comprehensive account of matrices in physical problems, their analytic properties, and
numerical techniques.
Brown, W. C., Matrices and Vector Spaces. New York: Dekker (1991).
Gilbert, J., and L. Gilbert, Linear Algebra and Matrix Theory. San Diego: Academic Press (1995).
Golub, G. H., and C. F. Van Loan, Matrix Computations, 3rd ed. Baltimore: JHU Press (1996). Detailed mathe-
matical background and algorithms for the production of numerical software, including methods for parallel
computation. A classic computer science text.
Heading, J., Matrix Theory for Physicists. London: Longmans, Green and Co. (1958). A readable introduction to
determinants and matrices, with applications to mechanics, electromagnetism, special relativity, and quantum
mechanics.
Vein, R., and P. Dale, Determinants and Their Applications in Mathematical Physics. Berlin: Springer (1998).
Watkins, D.S., Fundamentals of Matrix Computations. New York: Wiley (1991).

CHAPTER 3
VECTOR ANALYSIS
The introductory section on vectors, Section 1.7, identiﬁed some basic properties that are
universal, in the sense that they occur in a similar fashion in spaces of different dimension.
In summary, these properties are (1) vectors can be represented as linear forms, with oper-
ations that include addition and multiplication by a scalar, (2) vectors have a commutative
and distributive dot product operation that associates a scalar with a pair of vectors and
depends on their relative orientations and hence is independent of the coordinate system,
and (3) vectors can be decomposed into components that can be identiﬁed as projections
onto the coordinate directions. In Section 2.2 we found that the components of vectors
could be identiﬁed as the elements of a column vector and that the scalar product of two
vectors corresponded to the matrix multiplication of the transpose of one (the transposition
makes it a row vector) with the column vector of the other.
The current chapter builds on these ideas, mainly in ways that are speciﬁc to three-
dimensional (3-D) physical space, by (1) introducing a quantity called a vector cross
product to permit the use of vectors to represent rotational phenomena and volumes in 3-D
space, (2) studying the transformational properties of vectors when the coordinate system
used to describe them is rotated or subjected to a reﬂection operation, (3) developing math-
ematical methods for treating vectors that are deﬁned over a spatial region (vector ﬁelds),
with particular attention to quantities that depend on the spatial variation of the vector ﬁeld,
including vector differential operators and integrals of vector quantities, and (4) extending
vector concepts to curvilinear coordinate systems, which are very useful when the sym-
metry of the coordinate system corresponds to a symmetry of the problem under study (an
example is the use of spherical polar coordinates for systems with spherical symmetry).
A key idea of the present chapter is that a quantity that is properly called a vector
must have the transformation properties that preserve its essential features under coordinate
transformation; there exist quantities with direction and magnitude that do not transform
appropriately and hence are not vectors. This study of transformation properties will, in a
subsequent chapter, ultimately enable us to generalize to related quantities such as tensors.
123
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

124
Chapter 3 Vector Analysis
Finally, we note that the methods developed in this chapter have direct application in
electromagnetic theory as well as in mechanics, and these connections are explored through
the study of examples.
3.1
REVIEW OF BASIC PROPERTIES
In Section 1.7 we established the following properties of vectors:
1.
Vectors satisfy an addition law that corresponds to successive displacements that can
be represented by arrows in the underlying space. Vector addition is commutative
and associative: A + B = B + A and (A + B) + C = A + (B + C).
2.
A vector A can be multiplied by a scalar k; if k > 0 the result will be a vector in the
direction of A but with its length multiplied by k; if k < 0 the result will be in the
direction opposite to A but with its length mutiplied by |k|.
3.
The vector A−B is interpreted as A+(−1)B, so vector polynomials, e.g., A−2B+
3C, are well-deﬁned.
4.
A vector of unit length in the coordinate direction xi is denoted ˆei. An arbitrary vector
A can be written as a sum of vectors along the coordinate directions, as
A = A1ˆe1 + A2ˆe2 + ··· .
The Ai are called the components of A, and the operations in Properties 1 to 3 cor-
respond to the component formulas
G = A −2B + 3C
=⇒
Gi = Ai −2Bi + 3Ci,
(each i).
5.
The magnitude or length of a vector A, denoted |A| or A, is given in terms of its
components as
|A| =
 A2
1 + A2
2 + ···
1/2.
6.
The dot product of two vectors is given by the formula
A · B = A1B1 + A2B2 + ··· ;
consequences are
|A|2 = A · A,
A · B = |A||B|cosθ,
where θ is the angle between A and B.
7.
If two vectors are perpendicular to each other, their dot product vanishes and they are
termed orthogonal. The unit vectors of a Cartesian coordinate system are orthogonal:
ˆei · ˆej = δi j,
(3.1)
where δi j is the Kronecker delta, Eq. (1.164).
8.
The projection of a vector in any direction has an algebraic magnitude given by its
dot product with a unit vector in that direction. In particular, the projection of A on
the ˆei direction is Ai ˆei, with
Ai = ˆei · A.

3.1 Review of Basic Properties
125
9.
The components of A in R3 are related to its direction cosines (cosines of the angles
that A makes with the coordinate axes) by the formulas
Ax = A cosα,
Ay = A cosβ,
Az = A cosγ,
and cos2 α + cos2 β + cos2 γ = 1.
In Section 2.2 we noted that matrices consisting of a single column could be used to
represent vectors. In particular, we found, illustrating for the 3-D space R3, the following
properties.
10.
A vector A can be represented by a single-column matrix a whose elements are the
components of A, as in
A
=⇒
a =


A1
A2
A3

.
The rows (i.e., individual elements Ai) of a are the coefﬁcients of the individual
members of the basis used to represent A, so the element Ai is associated with the
basis unit vector ˆei.
11.
The vector operations of addition and multiplication by a scalar correspond exactly
to the operations of the same names applied to the single-column matrices represent-
ing the vectors, as illustrated here:
G = A −2B + 3C
=⇒


G1
G2
G3

=


A1
A2
A3

−2


B1
B2
B3

+ 3


C1
C2
C3


=


A1 −2B1 + 3C1
A2 −2B2 + 3C2
A3 −2B3 + 3C3

, or g = a −2b + 3c.
It is therefore appropriate to call these single-column matrices column vectors.
12.
The transpose of the matrix representing a vector A is a single-row matrix, called a
row vector:
aT = (A1 A2 A3).
The operations illustrated in Property 11 also apply to row vectors.
13.
The dot product A · B can be evaluated as aT b, or alternatively, because a and b are
real, as a†b. Moreover, aT b = bT a.
A · B = aT b = (A1 A2 A3)


B1
B2
B3

= A1B1 + A2B2 + A3B3.

126
Chapter 3 Vector Analysis
3.2
VECTORS IN 3-D SPACE
We now proceed to develop additional properties for vectors, most of which are applicable
only for vectors in 3-D space.
Vector or Cross Product
A number of quantities in physics are related to angular motion or the torque required to
cause angular acceleration. For example, angular momentum about a point is deﬁned as
having a magnitude equal to the distance r from the point times the component of the
linear momentum p perpendicular to r—the component of p causing angular motion (see
Fig. 3.1). The direction assigned to the angular momentum is that perpendicular to both
r and p, and corresponds to the axis about which angular motion is taking place. The
mathematical construction needed to describe angular momentum is the cross product,
deﬁned as
C = A × B = (AB sinθ)ˆec.
(3.2)
Note that C, the result of the cross product, is stated to be a vector, with a magnitude that
is the product of the magnitudes of A, B and the sine of the angle θ ≤π between A and B.
The direction of C, i.e., that of ˆec, is perpendicular to the plane of A and B, such that A, B,
and C form a right-handed system.1 This causes C to be aligned with the rotational axis,
with a sign that indicates the sense of the rotation.
From Fig. 3.2, we also see that A × B has a magnitude equal to the area of the parallel-
ogram formed by A and B, and with a direction normal to the parallelogram.
Other places the cross product is encountered include the formulas
v = ω × r
and
FM = qv × B.
The ﬁrst of these equations is the relation between linear velocity v and and angular veloc-
ity ω, and the second equation gives the force FM on a particle of charge q and velocity v
in the magnetic induction ﬁeld B (in SI units).
y
p sinθ
x
r
θ
p
FIGURE 3.1
Angular momentum about the origin, L = r × p.
L has magnitude rp sinθ and is directed out of the plane of the paper.
1The inherent ambiguity in this statement can be resolved by the following anthropomorphic prescription: Point the right hand
in the direction A, and then bend the ﬁngers through the smaller of the two angles that can cause the ﬁngers to point in the
direction B; the thumb will then point in the direction of C.

3.2 Vectors in 3-D Space
127
x
B
A
y
B sinθ
θ
FIGURE 3.2
Parallelogram of A × B.
We can get our right hands out of the analysis by compiling some algebraic properties of
the cross product. If the roles of A and B are reversed, the cross product changes sign, so
B × A = −A × B
(anticommutation).
(3.3)
The cross product also obeys the distributive laws
A × (B + C) = A × B + A × C,
k(A × B) = (kA) × B,
(3.4)
and when applied to unit vectors in the coordinate directions, we get
ˆei × ˆe j =
X
k
εi jk ˆek.
(3.5)
Here εi jk is the Levi-Civita symbol deﬁned in Eq. (2.8); Eq. (3.5) therefore indicates, for
example, that ˆex × ˆex = 0, ˆex × ˆey = ˆez, but ˆey × ˆex = −ˆez.
Using Eq. (3.5) and writing A and B in component form, we can expand A×B to obtain
C = A × B = (Ax ˆex + Ayˆey + Azˆez) × (Bx ˆex + Byˆey + Bzˆez)
= (Ax By −Ay Bx)(ˆex × ˆey) + (Ax Bz −Az Bx)(ˆex × ˆez)
+ (Ay Bz −Az By)(ˆey × ˆez)
= (Ax By −Ay Bx)ˆez + (Ax Bz −Az Bx)(−ˆey) + (Ay Bz −Az By)ˆex.
(3.6)
The components of C are important enough to be displayed prominently:
Cx = Ay Bz −Az By,
Cy = Az Bx −Ax Bz,
Cz = Ax By −Ay Bx,
(3.7)
equivalent to
Ci =
X
jk
εi jk A j Bk.
(3.8)

128
Chapter 3 Vector Analysis
Yet another way of expressing the cross product is to write it as a determinant. It is
straightforward to verify that Eqs. (3.7) are reproduced by the determinantal equation
C =

ˆex
ˆey
ˆez
Ax
Ay
Az
Bx
By
Bz

.
(3.9)
when the determinant is expanded in minors of its top row. The anticommutation of the
cross product now clearly follows if the rows for the components of A and B are inter-
changed.
We need to reconcile the geometric form of the cross product, Eq. (3.2), with the alge-
braic form in Eq. (3.6). We can conﬁrm the magnitude of A × B by evaluating (from the
component form of C)
(A × B) · (A × B) = A2B2 −(A · B)2 = A2B2 −A2B2 cos2 θ
= A2B2 sin2 θ.
(3.10)
The ﬁrst step in Eq. (3.10) can be veriﬁed by expanding its left-hand side in component
form, then collecting the result into the terms constituting the central member of the ﬁrst
line of the equation.
To conﬁrm the direction of C = A × B, we can check that A · C = B · C = 0, showing
that C (in component form) is perpendicular to both A and B. We illustrate for A · C:
A · C = Ax(Ay Bz −Az By) + Ay(Az Bx −Ax Bz) + Az(Ax By −Ay Bx) = 0.
(3.11)
To verify the sign of C, it sufﬁces to check special cases (e.g., A = ˆex, B = ˆey, or Ax =
By = 1, all other components zero).
Next, we observe that it is obvious from Eq. (3.2) that if C = A×B in a given coordinate
system, then that equation will also be satisﬁed if we rotate the coordinates, even though
the individual components of all three vectors will thereby be changed. In other words, the
cross product, like the dot product, is a rotationally invariant relationship.
Finally, note that the cross product is a quantity speciﬁcally deﬁned for 3-D space. It is
possible to make analogous deﬁnitions for spaces of other dimensionality, but they do not
share the interpretation or utility of the cross product in R3.
Scalar Triple Product
While the various vector operations can be combined in many ways, there are two combi-
nations involving three operands that are of particular importance. We call attention ﬁrst
to the scalar triple product, of the form A · (B × C). Taking (B × C) in the determinantal
form, Eq. (3.9), one can see that taking the dot product with A will cause the unit vector ˆex
to be replaced by Ax, with corresponding replacements to ˆey and ˆez. The overall result is
A · (B × C) =

Ax
Ay
Az
Bx
By
Bz
Cx
Cy
Cz

.
(3.12)
We can draw a number of conclusions from this highly symmetric determinantal form.
To start, we see that the determinant contains no vector quantities, so it must evaluate

3.2 Vectors in 3-D Space
129
y
x
B× C
B
C
A
z
FIGURE 3.3
A · (B × C) parallelepiped.
to an ordinary number. Because the left-hand side of Eq. (3.12) is a rotational invariant,
the number represented by the determinant must also be rotationally invariant, and can
therefore be identiﬁed as a scalar. Since we can permute the rows of the determinant (with
a sign change for an odd permutation, and with no sign change for an even permutation),
we can permute the vectors A, B, and C to obtain
A · B × C = B · C × A = C · A × B = −A · C × B, etc.
(3.13)
Here we have followed common practice and dropped the parentheses surrounding the
cross product, on the basis that they must be understood to be present in order for the
expressions to have meaning. Finally, noting that B × C has a magnitude equal to the area
of the BC parallelog ram and a direction perpendicular to it, and that the dot product with
A will multiply that area by the projection of A on B × C, we see that the scalar triple
product gives us (±) the volume of the parallelepiped deﬁned by A, B, and C; see Fig. 3.3.
Example 3.2.1
RECIPROCAL LATTICE
Let a, b, and c (not necessarily mutually perpendicular) represent the vectors that deﬁne a
crystal lattice. The displacements from one lattice point to another may then be written
R = naa + nbb + ncc,
(3.14)

130
Chapter 3 Vector Analysis
with na, nb, and nc taking integral values. In the band theory of solids,2 it is useful to
introduce what is called a reciprocal lattice a′, b′, c′ such that
a · a′ = b · b′ = c · c′ = 1,
(3.15)
and with
a · b′ = a · c′ = b · a′ = b · c′ = c · a′ = c · b′ = 0.
(3.16)
The reciprocal-lattice vectors are easily constructed by calling on the fact that for any u
and v, u × v is perpendicular to both u and v; we have
a′ =
b × c
a · b × c,
b′ =
c × a
a · b × c,
c′ =
a × b
a · b × c.
(3.17)
The scalar triple product causes these expressions to satisfy the scale condition of
Eq. (3.15).
■
Vector Triple Product
The other triple product of importance is the vector triple product, of the form A ×
(B × C). Here the parentheses are essential since, for example, (ˆex × ˆex) × ˆey = 0, while
ˆex × ( ˆex × ˆey) = ˆex × ˆez = −ˆey. Our interest is in reducing this triple product to a simpler
form; the result we seek is
A × (B × C) = B(A · C) −C(A · B).
(3.18)
Equation (3.18), which for convenience we will sometimes refer to as the BAC–CAB rule,
can be proved by inserting components for all vectors and evaluating all the products, but it
is instructive to proceed in a more elegant fashion. Using the formula for the cross product
in terms of the Levi-Civita symbol, Eq. (3.8), we write
A × (B × C) =
X
i
ˆei
X
jk
εi jk A j
 X
pq
εkpq BpCq
!
=
X
i j
X
pq
ˆei A j BpCq
X
k
εi jkεkpq.
(3.19)
The summation over k of the product of Levi-Civita symbols reduces, as shown in
Exercise 2.1.9, to δipδ jq −δiqδ jp; we are left with
A × (B × C) =
X
i j
ˆei A j(BiC j −B jCi) =
X
i
ˆei

Bi
X
j
A jC j −Ci
X
j
A j B j

,
which is equivalent to Eq. (3.18).
2It is often chosen to require a · a′, etc., to be 2π rather than unity, because when Bloch states for a crystal (labeled by k) are
set up, a constituent atomic function in cell R enters with coefﬁcient exp(ik · R), and if k is changed by a reciprocal lattice step
(in, say, the a′ direction), the coefﬁcient becomes exp(i[k + a′] · R), which reduces to exp(2πina)exp(ik · R) and therefore,
because exp(2πina) = 1, to its original value. Thus, the reciprocal lattice identiﬁes the periodicity in k. The unit cell of the k
vectors is called the Brillouin zone

3.2 Vectors in 3-D Space
131
Exercises
3.2.1
If P = ˆex Px + ˆey Py and Q = ˆex Qx + ˆey Qy are any two nonparallel (also nonantiparal-
lel) vectors in the xy-plane, show that P × Q is in the z-direction.
3.2.2
Prove that (A × B) · (A × B) = (AB)2 −(A · B)2.
3.2.3
Using the vectors
P = ˆex cosθ + ˆey sinθ,
Q = ˆex cosϕ −ˆey sinϕ,
R = ˆex cosϕ + ˆey sinϕ,
prove the familiar trigonometric identities
sin(θ + ϕ) = sinθ cosϕ + cosθ sinϕ,
cos(θ + ϕ) = cosθ cosϕ −sinθ sinϕ.
3.2.4
(a)
Find a vector A that is perpendicular to
U = 2ˆex + ˆey −ˆez,
V = ˆex −ˆey + ˆez.
(b)
What is A if, in addition to this requirement, we demand that it have unit
magnitude?
3.2.5
If four vectors a,b,c, and d all lie in the same plane, show that
(a × b) × (c × d) = 0.
Hint. Consider the directions of the cross-product vectors.
3.2.6
Derive the law of sines (see Fig. 3.4):
sinα
|A| = sinβ
|B| = sinγ
|C| .
3.2.7
The magnetic induction B is deﬁned by the Lorentz force equation,
F = q(v × B).
A
C
B
β
α
γ
FIGURE 3.4
Plane triangle.

132
Chapter 3 Vector Analysis
Carrying out three experiments, we ﬁnd that if
v = ˆex,
F
q = 2ˆez −4ˆey,
v = ˆey,
F
q = 4ˆex −ˆez,
v = ˆez,
F
q = ˆey −2ˆex.
From the results of these three separate experiments calculate the magnetic induction B.
3.2.8
You are given the three vectors A, B, and C,
A = ˆex + ˆey,
B = ˆey + ˆez,
C = ˆex −ˆez.
(a)
Compute the scalar triple product, A · B × C. Noting that A = B + C, give a
geometric interpretation of your result for the scalar triple product.
(b)
Compute A × (B × C).
3.2.9
Prove Jacobi’s identity for vector products:
a × (b × c) + b × (c × a) + c × (a × b) = 0.
3.2.10
A vector A is decomposed into a radial vector Ar and a tangential vector At. If ˆr is a
unit vector in the radial direction, show that
(a)
Ar = ˆr(A · ˆr) and
(b)
At = −ˆr × (ˆr × A).
3.2.11
Prove that a necessary and sufﬁcient condition for the three (nonvanishing) vectors A,
B, and C to be coplanar is the vanishing of the scalar triple product
A · B × C = 0.
3.2.12
Three vectors A, B, and C are given by
A = 3ˆex −2ˆey + 2ˆz,
B = 6ˆex + 4ˆey −2ˆz,
C = −3ˆex −2ˆey −4ˆz.
Compute the values of A · B × C and A × (B × C),C × (A × B) and B × (C × A).
3.2.13
Show that
(A × B) · (C × D) = (A · C)(B · D) −(A · D)(B · C).
3.2.14
Show that
(A × B) × (C × D) = (A · B × D)C −(A · B × C)D.

3.3 Coordinate Transformations
133
3.2.15
An electric charge q1 moving with velocity v1 produces a magnetic induction B
given by
B = µ0
4π q1
v1 × ˆr
r2
(mks units),
where ˆr is a unit vector that points from q1 to the point at which B is measured (Biot
and Savart law).
(a)
Show that the magnetic force exerted by q1 on a second charge q2, velocity v2, is
given by the vector triple product
F2 = µ0
4π
q1q2
r2 v2 ×
 v1 × ˆr

.
(b)
Write out the corresponding magnetic force F1 that q2 exerts on q1. Deﬁne your
unit radial vector. How do F1 and F2 compare?
(c)
Calculate F1 and F2 for the case of q1 and q2 moving along parallel trajectories
side by side.
ANS.
(b)
F1 = −µ0
4π
q1q2
r2 v1 ×
 v2 × ˆr

.
In general, there is no simple relation between
F1 and F2. Speciﬁcally, Newton’s third law,
F1 = −F2, does not hold.
(c)
F1 = µ0
4π
q1q2
r2 v2ˆr = −F2.
Mutual attraction.
3.3
COORDINATE TRANSFORMATIONS
As indicated in the chapter introduction, an object classiﬁed as a vector must have speciﬁc
transformation properties under rotation of the coordinate system; in particular, the com-
ponents of a vector must transform in a way that describes the same object in the rotated
system.
Rotations
Considering initially R2, and a rotation of the coordinate axes as shown in Fig. 3.5, we
wish to ﬁnd how the components Ax and Ay of a vector A in the unrotated system are
related to A′
x and A′
y, its components in the rotated coordinate system. Perhaps the easiest
way to answer this question is by ﬁrst asking how the unit vectors ˆex and ˆey are represented
in the new coordinates, after which we can perform vector addition on the new incarnations
of Ax ˆex and Ayˆey.
From the right-hand part of Fig. 3.5, we see that
ˆex = cosϕˆe′
x −sinϕˆe′
y,
and
ˆey = sinϕˆe′
x + cosϕˆe′
y,
(3.20)

134
Chapter 3 Vector Analysis
ex
ˆ
ex
ˆ
ey
ˆ
ey
ˆ
ϕ
ϕ
ϕ
ϕ
ˆe′
y
ˆe′
x
sinϕ
−ˆe′
y
cosϕ
ˆe′
x
sinϕ
ˆe′
x
cosϕ
ˆe′
y
FIGURE 3.5
Left: Rotation of two-dimensional (2-D) coordinate axes through angle ϕ.
Center and right: Decomposition of ˆex and ˆey into their components in the rotated system.
so the unchanged vector A now takes the changed form
A = Ax ˆex + Ayˆey = Ax(cosϕˆe′
x −sinϕˆe′
y) + Ay(sinϕˆe′
x + cosϕˆe′
y)
= (Ax cosϕ + Ay sinϕ)ˆe′
x + (−Ax sinϕ + Ay cosϕ)ˆe′
y.
(3.21)
If we write the vector A in the rotated (primed) coordinate system as
A = A′
x ˆe′
x + A′
yˆe′
y,
we then have
A′
x = Ax cosϕ + Ay sinϕ,
A′
y = −Ax sinϕ + Ay cosϕ,
(3.22)
which is equivalent to the matrix equation
A′ =
A′
x
A′
y

=
 cosϕ
sinϕ
−sinϕ
cosϕ
Ax
Ay

.
(3.23)
Suppose now that we start from A as given by its components in the rotated system,
(A′
x, A′
y), and rotate the coordinate system back to its original orientation. This will entail
a rotaton in the amount −ϕ, and corresponds to the matrix equation
Ax
Ay

=
 cos(−ϕ)
sin(−ϕ)
−sin(−ϕ)
cos(−ϕ)
A′
x
A′
y

=
cosϕ
−sinϕ
sinϕ
cosϕ
A′
x
A′
y

.
(3.24)
Assigning the 2 × 2 matrices in Eqs. (3.23) and (3.24) the respective names S and S′, we
see that these two equations are equivalent to A′ = SA and A = S′A′, with
S =
 cosϕ
sinϕ
−sinϕ
cosϕ

and
S′ =
cosϕ
−sinϕ
sinϕ
cosϕ

.
(3.25)
Now, applying S to A and then S′ to SA (corresponding to ﬁrst rotating the coordinate
system an amount +ϕ and then an amount −ϕ), we recover A, or
A = S′SA.
Since this result must be valid for any A, we conclude that S′ = S−1. We also see that
S′ = ST . We can check that SS′ = 1 by matrix multiplication:
SS′ =
 cosϕ
sinϕ
−sinϕ
cosϕ
cosϕ
−sinϕ
sinϕ
cosϕ

=
1
0
0
1

.

3.3 Coordinate Transformations
135
Since S is real, the fact that S−1 = ST means that it is orthogonal. In summary, we have
found that the transformation connecting A and A′ (the same vector, but represented in the
rotated coordinate system) is
A′ = SA,
(3.26)
with S an orthogonal matrix.
Orthogonal Transformations
It was no accident that the transformation describing a rotation in R2 was orthogonal, by
which we mean that the matrix effecting the transformation was an orthogonal matrix.
An instructive way of writing the transformation S is, returning to Eq. (3.20), to rewrite
those equations as
ˆex = (ˆe′
x · ˆex)ˆe′
x + (ˆe′
y · ˆex)ˆe′
y,
ˆey = (ˆe′
x · ˆey)ˆe′
x + (ˆe′
y · ˆey)ˆe′
y.
(3.27)
This corresponds to writing ˆex and ˆey as the sum of their projections on the orthogonal
vectors ˆe′
x and ˆe′
y. Now we can rewrite S as
S =
ˆe′
x · ˆex
ˆe′
x · ˆey
ˆe′
y · ˆex
ˆe′
y · ˆey

.
(3.28)
This means that each row of S contains the components (in the unprimed coordinates) of
a unit vector (either ˆe′
x or ˆe′
y) that is orthogonal to the vector whose components are in the
other row. In turn, this means that the dot products of different row vectors will be zero,
while the dot product of any row vector with itself (because it is a unit vector) will be unity.
That is the deeper signiﬁcance of an orthogonal matrix S; the µν element of SST is the
dot product formed from the µth row of S and the νth column of ST (which is the same as
the νth row of S). Since these row vectors are orthogonal, we will get zero if µ ̸= ν, and
because they are unit vectors, we will get unity if µ = ν. In other words, SST will be a unit
matrix.
Before leaving Eq. (3.28), note that its columns also have a simple interpretation: Each
contains the components (in the primed coordinates) of one of the unit vectors of the
unprimed set. Thus the dot product formed from two different columns of S will van-
ish, while the dot product of any column with itself will be unity. This corresponds to the
fact that, for an orthogonal matrix, we also have ST S = 1.
Summarizing part of the above,
The transformation from one orthogonal Cartesian coordinate system to another Carte-
sian system is described by an orthogonal matrix.
In Chapter 2 we found that an orthogonal matrix must have a determinant that is real
and of magnitude unity, i.e., ±1. However, for rotations in ordinary space the value of the
determinant will always be +1. One way to understand this is to consider the fact that any
rotation can be built up from a large number of small rotations, and that the determinant
must vary continuously as the amount of rotation is changed. The identity rotation (i.e.,
no rotation at all) has determinant +1. Since no value close to +1 except +1 itself is a
permitted value for the determinant, rotations cannot change the value of the determinant.

136
Chapter 3 Vector Analysis
Reﬂections
Another possibility for changing a coordinate system is to subject it to a reﬂection
operation. For simplicity, consider ﬁrst the inversion operation, in which the sign of each
coordinate is reversed. In R3, the transformation matrix S will be the 3 × 3 analog of
Eq. (3.28), and the transformation under discussion is to set ˆe′
µ = −ˆeµ, with µ = x, y,
and z. This will lead to
S =


−1
0
0
0
−1
0
0
0
−1

,
which clearly results in det S = −1. The change in sign of the determinant corresponds
to the change from a right-handed to a left-handed coordinate system (which obviously
cannot be accomplished by a rotation). Reﬂection about a plane (as in the image produced
by a plane mirror) also changes the sign of the determinant and the handedness of the
coordinate system; for example, reﬂection in the xy-plane changes the sign of ˆez, leaving
the other two unit vectors unchanged; the transformation matrix S for this transformation is
S =


1
0
0
0
1
0
0
0
−1

.
Its determinant is also −1.
The formulas for vector addition, multiplication by a scalar, and the dot product are
unaffected by a reﬂection transformation of the coordinates, but this is not true of the cross
product. To see this, look at the formula for any one of the components of A × B, and how
it would change under inversion (where the same, unchanged vectors in physical space
now have sign changes to all their components):
Cx:
Ay Bz −Az By
−→
(−Ay)(−Bz) −(−Az)(−By) = Ay Bz −Az By.
Note that this formula says that the sign of Cx should not change, even though it must in
order to describe the unchanged physical situation. The conclusion is that our transforma-
tion law fails for the result of a cross-product operation. However, the mathematics can
be salvaged if we classify B × C as a different type of quantity than B and C. Many texts
on vector analysis call vectors whose components change sign under coordinate reﬂec-
tion polar vectors, and those whose components do not then change sign axial vectors.
The term axial doubtless arises from the fact that cross products frequently describe phe-
nomena associated with rotation about the axis deﬁned by the axial vector. Nowadays, it
is becoming more usual to call polar vectors just vectors, because we want that term to
describe objects that obey for all S the transformation law
A′ = SA
(vectors),
(3.29)
(and speciﬁcally without a restriction to S whose determinants are +1). Axial vectors, for
which the vector transformation law fails for coordinate reﬂections, are then referred to
as pseudovectors, and their transformation law can be expressed in the somewhat more
complicated form
C′ = det(S) SC
(pseudovectors).
(3.30)

3.3 Coordinate Transformations
137
A
z
x
B
B
y
A
x
A
y
z
x
FIGURE 3.6
Inversion (right) of original coordinates (left) and the effect
on a vector A and a pseudovector B.
The effect of an inversion operation on a coordinate system and on a vector and a pseu-
dovector are shown in Fig. 3.6.
Since vectors and pseudovectors have different transformation laws, it is in general with-
out physical meaning to add them together.3 It is also usually meaningless to equate quan-
tities of different transformational properties: in A = B, both quantities must be either
vectors or pseudovectors.
Pseudovectors, of course, enter into more complicated expressions, of which an example
is the scalar triple product A·B×C. Under coordinate reﬂection, the components of B×C
do not change (as observed earlier), but those of A are reversed, with the result that
A · B × C changes sign. We therefore need to reclassify it as a pseudoscalar. On the
other hand, the vector triple product, A × (B × C), which contains two cross products,
evaluates, as shown in Eq. (3.18), to an expression containing only legitimate scalars and
(polar) vectors. It is therefore proper to identify A × (B × C) as a vector. These cases
illustrate the general principle that a product with an odd number of pseudo quantities is
“pseudo,” while those with even numbers of pseudo quantities are not.
Successive Operations
One can carry out a succession of coordinate rotations and/or reﬂections by applying the
relevant orthogonal transformations. In fact, we already did this in our introductory discus-
sion for R2 where we applied a rotation and then its inverse. In general, if R and R′ refer to
such operations, the application to A of R followed by the application of R′ corresponds to
A′ = S(R′)S(R)A,
(3.31)
and the overall result of the two transformations can be identiﬁed as a single transformation
whose matrix S(R′R) is the matrix product S(R′)S(R).
3The big exception to this is in beta-decay weak interactions. Here the universe distinguishes between right- and left-handed
systems, and we add polar and axial vector interactions.

138
Chapter 3 Vector Analysis
Two points should be noted:
1.
The operations take place in right-to-left order: The rightmost operator is the one
applied to the original A; that to its left then applies to the result of the ﬁrst opera-
tion, etc.
2.
The combined operation R′R is a transformation between two orthogonal coordinate
systems and therefore can be described by an orthogonal matrix: The product of two
orthogonal matrices is orthogonal.
Exercises
3.3.1
A rotation ϕ1 + ϕ2 about the z-axis is carried out as two successive rotations ϕ1 and
ϕ2, each about the z-axis. Use the matrix representation of the rotations to derive the
trigonometric identities
cos(ϕ1 + ϕ2) = cosϕ1 cosϕ2 −sinϕ1 sinϕ2,
sin(ϕ1 + ϕ2) = sinϕ1 cosϕ2 + cosϕ1 sinϕ2.
3.3.2
A corner reﬂector is formed by three mutually perpendicular reﬂecting surfaces. Show
that a ray of light incident upon the corner reﬂector (striking all three surfaces) is
reﬂected back along a line parallel to the line of incidence.
Hint. Consider the effect of a reﬂection on the components of a vector describing the
direction of the light ray.
3.3.3
Let x and y be column vectors. Under an orthogonal transformation S, they become
x′ = Sx and y′ = Sy. Show that (x′)T y′ = xT y, a result equivalent to the invariance of
the dot product under a rotational transformation.
3.3.4
Given the orthogonal transformation matrix S and vectors a and b,
S =


0.80
0.60
0.00
−0.48
0.64
0.60
0.36
−0.48
0.80

,
a =


1
0
1

,
b =


0
2
−1

,
(a)
Calculate det(S).
(b)
Verify that a · b is invariant under application of S to a and b.
(c)
Determine what happens to a × b under application of S to a and b. Is this what
is expected?
3.3.5
Using a and b as deﬁned in Exercise 3.3.4, but with
S =


0.60
0.00
0.80
−0.64
−0.60
0.48
−0.48
0.80
0.36


and
c =


2
1
3

,
(a)
Calculate det(S).
Apply S to a, b, and c, and determine what happens to
(b)
a × b,

3.4 Rotations in R3
139
(c)
(a × b) · c,
(d)
a × (b × c).
(e)
Classify the expressions in (b) through (d) as scalar, vector, pseudovector, or pseu-
doscalar.
3.4
ROTATIONS IN R3
Because of its practical importance, we discuss now in some detail the treatment of
rotations in R3. An obvious starting point, based on our experience in R2, would be to
write the 3 × 3 matrix S of Eq. (3.28), with rows that describe the orientations of a rotated
(primed) set of unit vectors in terms of the original (unprimed) unit vectors:
S =


ˆe′
1 · ˆe1
ˆe′
1 · ˆe2
ˆe′
1 · ˆe3
ˆe′
2 · ˆe1
ˆe′
2 · ˆe2
ˆe′
2 · ˆe3
ˆe′
3 · ˆe1
ˆe′
3 · ˆe2
ˆe′
3 · ˆe3


(3.32)
We have switched the coordinate labels from x, y, z to 1, 2, 3 for convenience in some of
the formulas that use Eq. (3.32). It is useful to make one observation about the elements
of S, namely sµν = ˆe′
µ · ˆeν. This dot product is the projection of ˆe′
µ onto the ˆeν direction,
and is therefore the change in xν that is produced by a unit change in x′
µ. Since the relation
between the coordinates is linear, we can identify ˆe′
µ · ˆeν as ∂xν/∂x′
µ, so our transformation
matrix S can be written in the alternate form
S =


∂x1/∂x′
1
∂x2/∂x′
1
∂x3/∂x′
1
∂x1/∂x′
2
∂x2/∂x′
2
∂x3/∂x′
2
∂x1/∂x′
3
∂x2/∂x′
3
∂x3/∂x′
3

.
(3.33)
The argument we made to evaluate ˆe′
µ · ˆeν could as easily have been made with the roles
of the two unit vectors reversed, yielding instead of ∂xν/∂x′
µ the derivative ∂x′
µ/∂xν. We
then have what at ﬁrst may seem to be a surprising result:
∂xν
∂x′µ
=
∂x′
µ
∂xν
.
(3.34)
A superﬁcial look at this equation suggests that its two sides would be reciprocals. The
problem is that we have not been notationally careful enough to avoid ambiguity: the
derivative on the left-hand side is to be taken with the other x′ coordinates ﬁxed, while that
on the right-hand side is with the other unprimed coordinates ﬁxed. In fact, the equality in
Eq. (3.34) is needed to make S an orthogonal matrix.
We note in passing that the observation that the coordinates are related linearly restricts
the current discussion to Cartesian coordinate systems. Curvilinear coordinates are treated
later.
Neither Eq. (3.32) nor Eq. (3.33) makes obvious the possibility of relations among the
elements of S. In R2, we found that all the elements of S depended on a single variable,
the rotation angle. In R3, the number of independent variables needed to specify a general
rotation is three: Two parameters (usually angles) are needed to specify the direction of
ˆe′
3; then one angle is needed to specify the direction of ˆe′
1 in the plane perpendicular to ˆe′
3;

140
Chapter 3 Vector Analysis
at this point the orientation of ˆe′
2 is completely determined. Therefore, of the nine elements
of S, only three are in fact independent. The usual parameters used to specify R3 rotations
are the Euler angles.4 It is useful to have S given explicitly in terms of them, as the
Lagrangian formulation of mechanics requires the use of a set of independent variables.
The Euler angles describe an R3 rotation in three steps, the ﬁrst two of which have
the effect of ﬁxing the orientation of the new ˆe3 axis (the polar direction in spherical
coordinates), while the third Euler angle indicates the amount of subsequent rotation about
that axis. The ﬁrst two steps do more than identify a new polar direction; they describe
rotations that cause the realignment. As a result, we can obtain the matrix representations
of these (and the third rotation), and apply them sequentially (i.e., as a matrix product) to
obtain the overall effect of the rotation.
The three steps describing rotation of the coordinate axes are the following (also illus-
trated in Fig. 3.7):
1.
The coordinates are rotated about the ˆe3 axis counterclockwise (as viewed from posi-
tive ˆe3) through an angle α in the range 0 ≤α < 2π, into new axes denoted ˆe′
1, ˆe′
2, ˆe′
3.
(The polar direction is not changed; the ˆe3 and ˆe′
3 axes coincide.)
2.
The coordinates are rotated about the ˆe′
2 axis counterclockwise (as viewed from posi-
tive ˆe′
2) through an angle β in the range 0 ≤β ≤π, into new axes denoted ˆe′′
1, ˆe′′
2, ˆe′′
3.
(This tilts the polar direction toward the ˆe′
1 direction, but leaves ˆe′
2 unchanged.)
3.
The coordinates are now rotated about the ˆe′′
3 axis counterclockwise (as viewed from
positive ˆe′′
3) through an angle γ in the range 0 ≤γ < 2π, into the ﬁnal axes, denoted
ˆe′′′
1 , ˆe′′′
2 , ˆe′′′
3 . (This rotation leaves the polar direction, ˆe′′
3, unchanged.)
In terms of the usual spherical polar coordinates (r,θ,ϕ), the ﬁnal polar axis is at the
orientation θ = β, ϕ = α. The ﬁnal orientations of the other axes depend on all three Euler
angles.
We now need the transformation matrices. The ﬁrst rotation causes ˆe′
1 and ˆe′
2 to
remain in the xy-plane, and has in its ﬁrst two rows and columns exactly the same form
(b)
(c)
x′′
3
x′′
1
x2
x2
x1
x′
1
x′
1
x′
2
β
β
β
α
α
α
x3= x′
3
x3= x′
3
x3= x′
3
x′
2 =
(a)
γ
γ
=
x′′
3
x′′
1
x′′
2
x′′′
3
=
x′
2
x′′
2
x′′′
1
FIGURE 3.7
Euler angle rotations: (a) about ˆe3 through angle α; (b) about ˆe′
2 through
angle β; (c) about ˆe′′
3 through angle γ .
4There are almost as many deﬁnitions of the Euler angles as there are authors. Here we follow the choice generally made by
workers in the area of group theory and the quantum theory of angular momentum.

3.4 Rotations in R3
141
as S in Eq. (3.25):
S1(α) =


cosα
sinα
0
−sinα
cosα
0
0
0
1

.
(3.35)
The third row and column of S1 indicate that this rotation leaves unchanged the ˆe3 com-
ponent of any vector on which it operates. The second rotation (applied to the coordinate
system as it exists after the ﬁrst rotation) is in the ˆe′
3ˆe′
1 plane; note that the signs of sinβ
have to be consistent with a cyclic permutation of the axis numbering:
S2(β) =


cosβ
0
−sinβ
0
1
0
sinβ
0
cosβ

.
The third rotation is like the ﬁrst, but with rotation amount γ :
S3(γ ) =


cosγ
sinγ
0
−sinγ
cosγ
0
0
0
1

.
The total rotation is described by the triple matrix product
S(α,β,γ ) = S3(γ )S2(β)S1(α).
(3.36)
Note the order: S1(α) operates ﬁrst, then S2(β), and ﬁnally S3(γ ). Direct multiplication
gives
S(α,β,γ ) =


cosγ cosβ cosα −sinγ sinα
cosγ cosβ sinα + sinγ cosα
−cosγ sinβ
−sinγ cosβ cosα −cosγ sinα
−sinγ cosβ sinα + cosγ cosα
sinγ sinβ
sinβ cosα
sinβ sinα
cosβ

.
(3.37)
In case they are wanted, note that the elements si j in Eq. (3.37) give the explicit forms of
the dot products ˆe′′′
i · ˆe j (and therefore also the partial derivatives ∂xi/∂x′′′
j ).
Note that each of S1, S2, and S3 are orthogonal, with determinant +1, so that the overall
S will also be orthogonal with determinant +1.
Example 3.4.1
AN R3 ROTATION
Consider a vector originally with components (2,−1,3). We want its components in
a coordinate system reached by Euler angle rotations α = β = γ = π/2. Evaluating
S(α,β,γ ):
S(α,β,γ ) =


−1
0
0
0
0
1
0
1
0

.
A partial check on this value of S is obtained by verifying that det(S) = +1.

142
Chapter 3 Vector Analysis
Then, in the new coordinates, our vector has components


−1
0
0
0
0
1
0
1
0




2
−1
3

=


−2
3
−1

.
The reader should check this result by visualizing the rotations involved.
■
Exercises
3.4.1
Another set of Euler rotations in common use is
(1)
a rotation about the x3-axis through an angle ϕ, counterclockwise,
(2)
a rotation about the x′
1-axis through an angle θ, counterclockwise,
(3)
a rotation about the x′′
3 -axis through an angle ψ, counterclockwise.
If
α = ϕ −π/2
ϕ = α + π/2
β = θ
or
θ = β
γ = ψ + π/2
ψ = γ −π/2,
show that the ﬁnal systems are identical.
3.4.2
Suppose the Earth is moved (rotated) so that the north pole goes to 30◦north, 20◦west
(original latitude and longitude system) and the 10◦west meridian points due south
(also in the original system).
(a)
What are the Euler angles describing this rotation?
(b)
Find the corresponding direction cosines.
ANS.
(b)
S =


0.9551
−0.2552
−0.1504
0.0052
0.5221
−0.8529
0.2962
0.8138
0.5000

.
3.4.3
Verify that the Euler angle rotation matrix, Eq. (3.37), is invariant under the transfor-
mation
α →α + π,
β →−β,
γ →γ −π.
3.4.4
Show that the Euler angle rotation matrix S(α,β,γ ) satisﬁes the following relations:
(a)
S−1(α,β,γ ) = ˜S(α,β,γ ),
(b)
S−1(α,β,γ ) = S(−γ,−β,−α).
3.4.5
The coordinate system (x, y, z) is rotated through an angle 8 counterclockwise about an
axis deﬁned by the unit vector ˆn into system (x′, y′, z′). In terms of the new coordinates

3.5 Differential Vector Operators
143
the radius vector becomes
r′ = rcos8 + r × nsin8 + ˆn(ˆn · r)(1 −cos8).
(a)
Derive this expression from geometric considerations.
(b)
Show that it reduces as expected for ˆn = ˆez. The answer, in matrix form, appears
in Eq. (3.35).
(c)
Verify that r′2 = r2.
3.5
DIFFERENTIAL VECTOR OPERATORS
We move now to the important situation in which a vector is associated with each point
in space, and therefore has a value (its set of components) that depends on the coordinates
specifying its position. A typical example in physics is the electric ﬁeld E(x, y, z), which
describes the direction and magnitude of the electric force if a unit “test charge” was placed
at x, y, z. The term ﬁeld refers to a quantity that has values at all points of a region; if the
quantity is a vector, its distribution is described as a vector ﬁeld. While we already have
a standard name for a simple algebraic quantity which is assigned a value at all points of
a spatial region (it is called a function), in physics contexts it may also be referred to as a
scalar ﬁeld.
Physicists need to be able to characterize the rate at which the values of vectors (and also
scalars) change with position, and this is most effectively done by introducing differential
vector operator concepts. It turns out that there are a large number of relations between
these differential operators, and it is our current objective to identify such relations and
learn how to use them.
Gradient, ∇
Our ﬁrst differential operator is that known as the gradient, which characterizes the change
of a scalar quantity, here ϕ, with position. Working in R3, and labeling the coordinates x1,
x2, x3, we write ϕ(r) as the value of ϕ at the point r = x1ˆe1 + x2ˆe2 + x3ˆe3, and consider
the effect of small changes dx1, dx2, dx3, respectively, in x1, x2, and x3. This situation
corresponds to that discussed in Section 1.9, where we introduced partial derivatives to
describe how a function of several variables (there x, y, and z) changes its value when these
variables are changed by respective amounts dx, dy, and dz. The equation governing this
process is Eq. (1.141).
To ﬁrst order in the differentials dxi, ϕ in our present problem changes by an amount
dϕ =
 ∂ϕ
∂x1

dx1 +
 ∂ϕ
∂x2

dx2 +
 ∂ϕ
∂x3

dx3,
(3.38)
which is of the form corresponding to the dot product of
∇ϕ =


∂ϕ/∂x1
∂ϕ/∂x2
∂ϕ/∂x3


and
dr =


dx1
dx2
dx3

.

144
Chapter 3 Vector Analysis
These quantities can also be written
∇ϕ =
 ∂ϕ
∂x1

ˆe1 +
 ∂ϕ
∂x2

ˆe2 +
 ∂ϕ
∂x3

ˆe3,
(3.39)
dr = dx1ˆe1 + dx2ˆe2 + dx3ˆe3,
(3.40)
in terms of which we have
dϕ = (∇ϕ) · dr.
(3.41)
We have given the 3 × 1 matrix of derivatives the name ∇ϕ (often referred to in speech as
“del phi” or “grad phi”); we give the differential of position its customary name dr.
The notation of Eqs. (3.39) and (3.41) is really only appropriate if ∇ϕ is actually a
vector, because the utility of the present approach depends on our ability to use it in coor-
dinate systems of arbitrary orientation. To prove that ∇ϕ is a vector, we must show that it
transforms under rotation of the coordinate system according to
(∇ϕ)′ = S(∇ϕ).
(3.42)
Taking S in the form given in Eq. (3.33), we examine S(∇ϕ). We have
S(∇ϕ) =


∂x1/∂x′
1
∂x2/∂x′
1
∂x3/∂x′
1
∂x1/∂x′
2
∂x2/∂x′
2
∂x3/∂x′
2
∂x1/∂x′
3
∂x2/∂x′
3
∂x3/∂x′
3




∂ϕ/∂x1
∂ϕ/∂x2
∂ϕ/∂x3


=


3
X
ν=1
∂xν
∂x′
1
∂ϕ
∂xν
3
X
ν=1
∂xν
∂x′
2
∂ϕ
∂xν
3
X
ν=1
∂xν
∂x′
3
∂ϕ
∂xν


.
(3.43)
Each of the elements in the ﬁnal expression in Eq. (3.43) is a chain-rule expression for
∂ϕ/∂x′
µ, µ = 1,2,3, showing that the transformation did produce (∇ϕ)′, the representa-
tion of ∇ϕ in the rotated coordinates.
Having now established the legitimacy of the form ∇ϕ, we proceed to give ∇a life of
its own. We therefore deﬁne (calling the coordinates x, y, z)
∇= ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z .
(3.44)
We note that ∇is a vector differential operator, capable of operating on a scalar (such
as ϕ) to produce a vector as the result of the operation. Because a differential operator
only operates on what is to its right, we have to be careful to maintain the correct order in
expressions involving ∇, and we have to use parentheses when necessary to avoid ambi-
guity as to what is to be differentiated.

3.5 Differential Vector Operators
145
The gradient of a scalar is extremely important in physics and engineering, as it
expresses the relation between a force ﬁeld F(r) experienced by an object at r and the
related potential V (r),
F(r) = −∇V (r).
(3.45)
The minus sign in Eq. (3.45) is important; it causes the force exerted by the ﬁeld to be in
a direction that lowers the potential. We consider later (in Section 3.9) the conditions that
must be satisﬁed if a potential corresponding to a given force can exit.
The gradient has a simple geometric interpretation. From Eq. (3.41), we see that, if
dr is constrained to have a ﬁxed magnitude, the direction of dr that maximizes dϕ will
be when ∇ϕ and dr are collinear. So, the direction of most rapid increase in ϕ is the
gradient direction, and the magnitude of the gradient is the directional derivative of ϕ in
that direction. We now see that −∇V, in Eq. (3.45), is the direction of most rapid decrease
in V, and is the direction of the force associated with the potential V.
Example 3.5.1
GRADIENT OF rn
As a ﬁrst step toward computation of ∇rn, let’s look at the even simpler ∇r. We begin by
writing r = (x2 + y2 + z2)1/2, from which we get
∂r
∂x =
x
(x2 + y2 + z2)1/2 = x
r ,
∂r
∂y = y
r ,
∂r
∂z = z
r .
(3.46)
From these formulas we construct
∇r = x
r ˆex + y
r ˆey + z
r ˆez = 1
r (xˆex + yˆey + zˆez) = r
r .
(3.47)
The result is a unit vector in the direction of r, denoted ˆr. For future reference, we note
that
ˆr = x
r ˆex + y
r ˆey + z
r ˆez
(3.48)
and that Eq. (3.47) takes the form
∇r = ˆr.
(3.49)
The geometry of r and ˆr is illustrated in Fig. 3.8.
y
ϕ
x
r
y
rˆ
x
x/r
y/r
FIGURE 3.8
Unit vector ˆr (in xy-plane).

146
Chapter 3 Vector Analysis
Continuing now to ∇rn, we have
∂rn
∂x = nrn−1 ∂r
∂x ,
with corresponding results for the y and z derivatives. We get
∇rn = nrn−1∇r = nrn−1ˆr.
(3.50)
■
Example 3.5.2
COULOMB’S LAW
In electrostatics, it is well known that a point charge produces a potential proportional
to 1/r, where r is the distance from the charge. To check that this is consistent with the
Coulomb force law, we compute
F = −∇
1
r

.
This is a case of Eq. (3.50) with n = −1, and we get the expected result
F = 1
r2 ˆr.
■
Example 3.5.3
GENERAL RADIAL POTENTIAL
Another situation of frequent occurrence is that the potential may be a function only of the
radial distance from the origin, i.e., ϕ = f (r). We then calculate
∂ϕ
∂x = d f (r)
dr
∂r
∂x , etc.,
which leads, invoking Eq. (3.49), to
∇ϕ = d f (r)
dr
∇r = d f (r)
dr
ˆr.
(3.51)
This result is in accord with intuition; the direction of maximum increase in ϕ must be
radial, and numerically equal to dϕ/dr.
■
Divergence, ∇·
The divergence of a vector A is deﬁned as the operation
∇· A = ∂Ax
∂x + ∂Ay
∂y + ∂Az
∂z .
(3.52)
The above formula is exactly what one might expect given both the vector and differential-
operator character of ∇.
After looking at some examples of the calculation of the divergence, we will discuss its
physical signiﬁcance.

3.5 Differential Vector Operators
147
Example 3.5.4
DIVERGENCE OF COORDINATE VECTOR
Calculate ∇· r:
∇· r =

ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z

·
 ˆexx + ˆey y + ˆezz

= ∂x
∂x + ∂y
∂y + ∂z
∂z ,
which reduces to ∇· r = 3.
■
Example 3.5.5
DIVERGENCE OF CENTRAL FORCE FIELD
Consider next ∇· f (r)ˆr. Using Eq. (3.48), we write
∇· f (r)ˆr =

ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z

·
x f (r)
r
ˆex + y f (r)
r
ˆey + zf (r)
r
ˆez

.
= ∂
∂x
x f (r)
r

+ ∂
∂y
 y f (r)
r

+ ∂
∂z
zf (r)
r

.
Using
∂
∂x
x f (r)
r

= f (r)
r
−x f (r)
r2
∂r
∂x + x
r
d f (r)
dr
∂r
∂x = f (r)
1
r −x2
r3

+ x2
r2
d f (r)
dr
and corresponding formulas for the y and z derivatives, we obtain after simpliﬁcation
∇· f (r)ˆr = 2 f (r)
r
+ d f (r)
dr
.
(3.53)
In the special case f (r) = rn, Eq. (3.53) reduces to
∇· rnˆr = (n + 2)rn−1.
(3.54)
For n = 1, this reduces to the result of Example 3.5.4. For n = −2, corresponding to the
Coulomb ﬁeld, the divergence vanishes, except at r = 0, where the differentiations we
performed are not deﬁned.
■
If a vector ﬁeld represents the ﬂow of some quantity that is distributed in space, its
divergence provides information as to the accumulation or depletion of that quantity at the
point at which the divergence is evaluated. To gain a clearer picture of the concept, let us
suppose that a vector ﬁeld v(r) represents the velocity of a ﬂuid5 at the spatial points r,
and that ρ(r) represents the ﬂuid density at r at a given time t. Then the direction and
magnitude of the ﬂow rate at any point will be given by the product ρ(r)v(r).
Our objective is to calculate the net rate of change of the ﬂuid density in a volume
element at the point r. To do so, we set up a parallelepiped of dimensions dx, dy, dz
centered at r and with sides parallel to the xy, xz, and yz planes. See Fig. 3.9. To ﬁrst
order (inﬁnitesimal dr and dt), the density of ﬂuid exiting the parallelepiped per unit time
5It may be helpful to think of the ﬂuid as a collection of molecules, so the number per unit volume (the density) at any point is
affected by the ﬂow in and out of a volume element at the point.

148
Chapter 3 Vector Analysis
−ρvx⏐x−dx/2
+ρvx⏐x+dx/2
dx
dy
dz
FIGURE 3.9
Outward ﬂow of ρv from a volume element in the ±x directions. The
quantities ±ρvx must be multiplied by dy dz to represent the total ﬂux through the
bounding surfaces at x ± dx/2.
through the yz face located at x −(dx/2) will be
Flow out, face at x −dx
2 :
−(ρvx)

(x−dx/2,y,z) dy dz.
Note that only the velocity component vx is relevant here. The other components of v
will not cause motion through a yz face of the parallelepiped. Also, note the following:
dy dz is the area of the yz face; the average of ρvx over the face is to ﬁrst order its value at
(x −dx/2, y, z), as indicated, and the amount of ﬂuid leaving per unit time can be identiﬁed
as that in a column of area dy dz and height vx. Finally, keep in mind that outward ﬂow
corresponds to that in the −x direction, explaining the presence of the minus sign.
We next compute the outward ﬂow through the yz planar face at x + dx/2. The result is
Flow out, face at x + dx
2 :
+ (ρvx)

(x+dx/2,y,z) dy dz.
Combining these, we have for both yz faces

−(ρvx)

x−dx/2 + (ρvx)

x+dx/2

dy dz =
∂(ρvx)
∂x

dx dy dz.
Note that in combining terms at x −dx/2 and x + dx/2 we used the partial derivative
notation, because all the quantities appearing here are also functions of y and z. Finally,
adding corresponding contributions from the other four faces of the parallelepiped, we
reach
Net ﬂow out
per unit time =
 ∂
∂x (ρvx) + ∂
∂y (ρvy) + ∂
∂z (ρvz)

dx dy dz
= ∇· (ρv)dx dy dz.
(3.55)
We now see that the name divergence is aptly chosen. As shown in Eq. (3.55), the
divergence of the vector ρv represents the net outﬂow per unit volume, per unit time. If
the physical problem being described is one in which ﬂuid (molecules) are neither created
or destroyed, we will also have an equation of continuity, of the form
∂ρ
∂t + ∇· (ρv) = 0.
(3.56)
This equation quantiﬁes the obvious statement that a net outﬂow from a volume element
results in a smaller density inside the volume.
When a vector quantity is divergenceless (has zero divergence) in a spatial region, we
can interpret it as describing a steady-state “ﬂuid-conserving” ﬂow (ﬂux) within that region

3.5 Differential Vector Operators
149
A
B
C
(b)
(a)
FIGURE 3.10
Flow diagrams: (a) with source and sink; (b) solenoidal. The divergence
vanishes at volume elements A and C, but is negative at B.
(even if the vector ﬁeld does not represent material that is moving). This is a situation that
arises frequently in physics, applying in general to the magnetic ﬁeld, and, in charge-free
regions, also to the electric ﬁeld. If we draw a diagram with lines that follow the ﬂow paths,
the lines (depending on the context) may be called stream lines or lines of force. Within a
region of zero divergence, these lines must exit any volume element they enter; they cannot
terminate there. However, lines will begin at points of positive divergence (sources) and
end at points where the divergence is negative (sinks). Possible patterns for a vector ﬁeld
are shown in Fig. 3.10.
If the divergence of a vector ﬁeld is zero everywhere, its lines of force will consist
entirely of closed loops, as in Fig. 3.10(b); such vector ﬁelds are termed solenoidal. For
emphasis, we write
∇· B = 0 everywhere
−→
B is solenoidal.
(3.57)
Curl, ∇×
Another possible operation with the vector operator ∇is to take its cross product with a
vector. Using the established formula for the cross product, and being careful to write the
derivatives to the left of the vector on which they are to act, we obtain
∇× V = ˆex
 ∂
∂y Vz −∂
∂z Vy

+ ˆey
 ∂
∂z Vx −∂
∂x Vz

+ ˆez
 ∂
∂x Vy −∂
∂y Vx

=

ˆex
ˆey
ˆez
∂/∂x ∂/∂y ∂/∂z
Vx
Vy
Vz

.
(3.58)
This vector operation is called the curl of V. Note that when the determinant in Eq. (3.58)
is evaluated, it must be expanded in a way that causes the derivatives in the second row to
be applied to the functions in the third row (and not to anything in the top row); we will
encounter this situation repeatedly, and will identify the evaluation as being from the top
down.
Example 3.5.6
CURL OF A CENTRAL FORCE FIELD
Calculate ∇× [ f (r)ˆr]. Writing
ˆr = x
r ˆex + y
r ˆey + z
r ˆez,

150
Chapter 3 Vector Analysis
and remembering that ∂r/∂y = y/r and ∂r/∂z = z/r, the x-component of the result is
found to be

∇× [ f (r)ˆr]

x = ∂
∂y
zf (r)
r
−∂
∂z
y f (r)
r
= z
 d
dr
f (r)
r
 ∂r
∂y −y
 d
dr
f (r)
r
 ∂r
∂z
= z
 d
dr
f (r)
r
 y
r −y
 d
dr
f (r)
r
 z
r = 0.
By symmetry, the other components are also zero, yielding the ﬁnal result
∇× [ f (r)ˆr] = 0.
(3.59)
■
Example 3.5.7
A NONZERO CURL
Calculate F = ∇× (−yˆex + xˆey), which is of the form ∇× b, where bx = −y, by = x,
bz = 0. We have
Fx = ∂bz
∂y −∂by
∂z = 0,
Fy = ∂bx
∂z −∂vz
∂x = 0,
Fz = ∂by
∂x −∂bx
∂y = 2,
so F = 2ˆez.
■
The results of these two examples can be better understood from a geometric interpreta-
tion of the curl operator. We proceed as follows: Given a vector ﬁeld B, consider the line
integral
H
B · ds for a small closed path. The circle through the integral sign is a signal
that the path is closed. For simplicity in the computations, we take a rectangular path in
the xy-plane, centered at a point (x0, y0), of dimensions 1x × 1y, as shown in Fig. 3.11.
We will traverse this path in the counterclockwise direction, passing through the four seg-
ments labeled 1 through 4 in the ﬁgure. Since everywhere in this discussion z = 0, we do
not show it explicitly.
x
y
4
3
2
1
2
2
x0−Δx , y0−Δy
2
2
x0+Δx , y0−Δy
2
2
x0+Δx , y0+ Δy
2
2
x0−Δx , y0+ Δy
FIGURE 3.11
Path for computing circulation at (x0, y0).

3.5 Differential Vector Operators
151
Segment 1 of the path contributes to the integral
Segment 1 =
x0+1x/2
Z
x0−1x/2
Bx(x, y0 −1y/2)dx ≈Bx(x0, y0 −1y/2)1x,
where the approximation, replacing Bx by its value at the middle of the segment, is good
to ﬁrst order. In a similar fashion, we have
Segment 2 =
y0+1y/2
Z
y0−1y/2
By(x0 + 1x/2, y)dy ≈By(x0 + 1x/2, y0)1y,
Segment 3 =
x0−1x/2
Z
x0+1x/2
Bx(x, y0 + 1y/2)dx ≈−Bx(x0, y0 + 1y/2)1x,
Segment 4 =
y0−1y/2
Z
y0+1y/2
By(x0 −1x/2, y)dy ≈−By(x0 −1x/2, y0)1y.
Note that because the paths of segments 3 and 4 are in the direction of decrease in the value
of the integration variable, we obtain minus signs in the contributions of these segments.
Combining the contributions of Segments 1 and 3, and those of Segments 2 and 4, we have
Segments 1 + 3 =
 Bx(x0, y0 −1y/2) −Bx(x0, y0 + 1y/2)

1x ≈−∂Bx
∂y 1y1x,
Segments 2 + 4 =
 By(x0 + 1x/2, y0) −By(x0 −1x/2, y0)

1y ≈+∂By
∂x 1x 1y.
Combining these contributions to obtain the value of the entire line integral, we have
I
B · ds ≈
∂By
∂x −∂Bx
∂y

1x1y ≈[∇× B]z1x1y.
(3.60)
The thing to note is that a nonzero closed-loop line integral of B corresponds to a nonzero
value of the component of ∇× B normal to the loop. In the limit of a small loop, the line
integral will have a value proportional to the loop area; the value of the line integral per
unit area is called the circulation (in ﬂuid dynamics, it is also known as the vorticity).
A nonzero circulation corresponds to a pattern of stream lines that form closed loops.
Obviously, to form a closed loop, a stream line must curl; hence the name of the ∇×
operator.
Returning now to Example 3.5.6, we have a situation in which the lines of force must
be entirely radial; there is no possibility to form closed loops. Accordingly, we found this
example to have a zero curl. But, looking next at Example 3.5.7, we have a situation in
which the stream lines of −yˆex + xˆey form counterclockwise circles about the origin, and
the curl is nonzero.

152
Chapter 3 Vector Analysis
We close the discussion by noting that a vector whose curl is zero everywhere is termed
irrotational. This property is in a sense the opposite of solenoidal, and deserves a parallel
degree of emphasis:
∇× B = 0 everywhere
−→
B is irrotational.
(3.61)
Exercises
3.5.1
If S(x, y, z) =
 x2 + y2 + z2−3/2, ﬁnd
(a)
∇S at the point (1,2,3),
(b)
the magnitude of the gradient of S, |∇S| at (1,2,3), and
(c)
the direction cosines of ∇S at (1,2,3).
3.5.2
(a)
Find a unit vector perpendicular to the surface
x2 + y2 + z2 = 3
at the point (1,1,1).
(b)
Derive the equation of the plane tangent to the surface at (1,1,1).
ANS.
(a)
 ˆex + ˆey + ˆez

/
√
3,
(b)
x + y + z = 3.
3.5.3
Given a vector r12 = ˆex(x1 −x2)+ ˆey(y1 −y2)+ ˆez(z1 −z2), show that ∇1r12 (gradient
with respect to x1, y1, and z1 of the magnitude r12) is a unit vector in the direction of
r12.
3.5.4
If a vector function F depends on both space coordinates (x, y, z) and time t, show that
dF = (dr · ∇)F + ∂F
∂t dt.
3.5.5
Show that ∇(uv) = v∇u + u∇v, where u and v are differentiable scalar functions of
x, y, and z.
3.5.6
For a particle moving in a circular orbit r = ˆexr cosωt + ˆeyr sinωt:
(a)
Evaluate r × ˙r, with ˙r = dr/dt = v.
(b)
Show that ¨r + ω2r = 0 with ¨r = dv/dt.
Hint. The radius r and the angular velocity ω are constant.
ANS.
(a)
ˆezωr2.
3.5.7
Vector A satisﬁes the vector transformation law, Eq. (3.26). Show directly that its time
derivative dA/dt also satisﬁes Eq. (3.26) and is therefore a vector.
3.5.8
Show, by differentiating components, that
(a)
d
dt (A · B) = dA
dt · B + A · dB
dt ,

3.6 Differential Vector Operators: Further Properties
153
(b)
d
dt (A × B) = dA
dt × B + A × dB
dt ,
just like the derivative of the product of two algebraic functions.
3.5.9
Prove ∇· (a × b) = b · (∇× a) −a · (∇× b).
Hint. Treat as a scalar triple product.
3.5.10
Classically, orbital angular momentum is given by L = r × p, where p is the lin-
ear momentum. To go from classical mechanics to quantum mechanics, p is replaced
(in units with ¯h = 1) by the operator −i∇. Show that the quantum mechanical angular
momentum operator has Cartesian components
Lx = −i

y ∂
∂z −z ∂
∂y

,
L y = −i

z ∂
∂x −x ∂
∂z

,
Lz = −i

x ∂
∂y −y ∂
∂x

.
3.5.11
Using the angular momentum operators previously given, show that they satisfy com-
mutation relations of the form
[Lx, L y] ≡Lx L y −L yLx = i Lz
and hence
L × L = iL.
These commutation relations will be taken later as the deﬁning relations of an angular
momentum operator.
3.5.12
With the aid of the results of Exercise 3.5.11, show that if two vectors a and b commute
with each other and with L, that is, [a,b] = [a,L] = [b,L] = 0, show that
[a · L,b · L] = i(a × b) · L.
3.5.13
Prove that the stream lines of b in of Example 3.5.7 are counterclockwise circles.
3.6
DIFFERENTIAL VECTOR OPERATORS: FURTHER
PROPERTIES
Successive Applications of ∇
Interesting results are obtained when we operate with ∇on the differential vector operator
forms we have already introduced. The possible results include the following:
(a) ∇· ∇ϕ
(b) ∇× ∇ϕ
(c) ∇(∇· V)
(d) ∇· (∇× V)
(e) ∇× (∇× V).

154
Chapter 3 Vector Analysis
All ﬁve of these expressions involve second derivatives, and all ﬁve appear in the
second-order differential equations of mathematical physics, particularly in electromag-
netic theory.
Laplacian
The ﬁrst of these expressions, ∇· ∇ϕ, the divergence of the gradient, is named the
Laplacian of ϕ. We have
∇· ∇ϕ =

ˆex
∂
∂x + ˆey
∂
∂y + ˆez
∂
∂z

·

ˆex
∂ϕ
∂x + ˆey
∂ϕ
∂y + ˆez
∂ϕ
∂z

= ∂2ϕ
∂x2 + ∂2ϕ
∂y2 + ∂2ϕ
∂z2 .
(3.62)
When ϕ is the electrostatic potential, we have
∇· ∇ϕ = 0
(3.63)
at points where the charge density vanishes, which is Laplace’s equation of electrostatics.
Often the combination ∇· ∇is written ∇2, or 1 in the older European literature.
Example 3.6.1
LAPLACIAN OF A CENTRAL FIELD POTENTIAL
Calculate ∇2ϕ(r). Using Eq. (3.51) to evaluate ∇ϕ and then Eq. (3.53) for the divergence,
we have
∇2ϕ(r) = ∇· ∇ϕ(r) = ∇· dϕ(r)
dr
ˆer = 2
r
dϕ(r)
dr
+ d2ϕ(r)
dr2
.
We get a term in addition to d2ϕ/dr2 because ˆer has a direction that depends on r.
In the special case ϕ(r) = rn, this reduces to
∇2rn = n(n + 1)rn−2.
This vanishes for n = 0 (ϕ =constant) and for n = −1 (Coulomb potential). For n = −1,
our derivation fails for r = 0, where the derivatives are undeﬁned.
■
Irrotational and Solenoidal Vector Fields
Expression (b), the second of our ﬁve forms involving two ∇operators, may be written as
a determinant:
∇× ∇ϕ =

ˆex
ˆey
ˆez
∂/∂x
∂/∂y
∂/∂z
∂ϕ/∂x ∂ϕ/∂y ∂ϕ/∂z

=

ˆex
ˆey
ˆez
∂/∂x
∂/∂y
∂/∂z
∂/∂x
∂/∂y
∂/∂z

ϕ = 0.
Because the determinant is to be evaluated from the top down, it is meaningful to move
ϕ outside and to its right, leaving a determinant with two identical rows and yielding the
indicated value of zero. We are thereby actually assuming that the order of the partial

3.6 Differential Vector Operators: Further Properties
155
differentiations can be reversed, which is true so long as these second derivatives of ϕ are
continuous.
Expression (d) is a scalar triple product that may be written
∇· (∇× V) =

∂/∂x
∂/∂y
∂/∂z
∂/∂x
∂/∂y
∂/∂z
Vx
Vy
Vz

= 0.
This determinant also has two identical rows and yields zero if V has sufﬁcient continuity.
These two vanishing results tell us that any gradient has a vanishing curl and is therefore
irrotational, and that any curl has a vanishing divergence, and is therefore solenoidal.
These properties are of such importance that we set them out here in display form:
∇× ∇ϕ = 0,
all ϕ,
(3.64)
∇· (∇× V) = 0,
all V.
(3.65)
Maxwell’s Equations
The uniﬁcation of electric and magnetic phenomena that is encapsulated in Maxwell’s
equations provides an excellent example of the use of differential vector operators. In SI
units, these equations take the form
∇· B = 0,
(3.66)
∇· E = ρ
ε0
,
(3.67)
∇× B = ε0µ0
∂E
∂t + µ0J,
(3.68)
∇× E = −∂B
∂t .
(3.69)
Here E is the electric ﬁeld, B is the magnetic induction ﬁeld, ρ is the charge density, J is
the current density, ε0 is the electric permittivity, and µ0 is the magnetic permeability, so
ε0µ0 = 1/c2, where c is the velocity of light.
Vector Laplacian
Expressions (c) and (e) in the list at the beginning of this section satisfy the relation
∇× (∇× V) = ∇(∇· V) −∇· ∇V.
(3.70)
The term ∇· ∇V, which is called the vector Laplacian and sometimes written ∇2V, has
prior to this point not been deﬁned; Eq. (3.70) (solved for ∇2V) can be taken to be its
deﬁnition. In Cartesian coordinates, ∇2V is a vector whose i component is ∇2Vi, and that
fact can be conﬁrmed either by direct component expansion or by applying the BAC–CAB
rule, Eq. (3.18), with care always to place V so that the differential operators act on it.
While Eq. (3.70) is general, ∇2V separates into Laplacians for the components of V only
in Cartesian coordinates.

156
Chapter 3 Vector Analysis
Example 3.6.2
ELECTROMAGNETIC WAVE EQUATION
Even in vacuum, Maxwell’s equations can describe electromagnetic waves. To derive an
electromagnetic wave equation, we start by taking the time derivative of Eq. (3.68) for the
case J = 0, and the curl of Eq. (3.69). We then have
∂
∂t ∇× B = ϵ0µ0
∂2E
∂t2 ,
∇× (∇× E) = −∂
∂t ∇× B = −ϵ0µ0
∂2E
∂t2 .
We now have an equation that involves only E; it can be brought to a more convenient
form by applying Eq. (3.70), dropping the ﬁrst term on the right of that equation because,
in vacuum, ∇· E = 0. The result is the vector electromagnetic wave equation for E,
∇2E = ϵ0µ0
∂2E
∂t2 = 1
c2
∂2E
∂t2 .
(3.71)
Equation (3.71) separates into three scalar wave equations, each involving the (scalar)
Laplacian. There is a separate equation for each Cartesian component of E.
■
Miscellaneous Vector Identities
Our introduction of differential vector operators is now formally complete, but we present
two further examples to illustrate how the relationships between these operators can be
manipulated to obtain useful vector identities.
Example 3.6.3
DIVERGENCE AND CURL OF A PRODUCT
First, simplify ∇· ( f V), where f and V are, respectively, scalar and vector functions.
Working with the components,
∇· ( f V) = ∂
∂x ( f Vx) + ∂
∂y ( f Vy) + ∂
∂z ( f Vz)
= ∂f
∂x Vx + f ∂Vx
∂x + ∂f
∂y Vy + f ∂Vy
∂y + ∂f
∂z Vz + f ∂Vz
∂z
= (∇f ) · V + f ∇· V.
(3.72)
Now simplify ∇× ( f V). Consider the x-component:
∂
∂y ( f Vz) −∂
∂z ( f Vy) = f
∂Vz
∂y −∂Vy
∂z

+
∂f
∂y Vz −∂f
∂z Vy

.
This is the x-component of f (∇× V) + (∇f ) × V, so we have
∇× ( f V) = f (∇× V) + (∇f ) × V.
(3.73)
■

3.6 Differential Vector Operators: Further Properties
157
Example 3.6.4
GRADIENT OF A DOT PRODUCT
Verify that
∇(A · B) = (B · ∇)A + (A · ∇)B + B × (∇× A) + A × (∇× B).
(3.74)
This problem is easier to solve if we recognize that ∇(A · B) is a type of term that appears
in the BAC–CAB expansion of a vector triple product, Eq. (3.18). From that equation,
we have
A × (∇× B) = ∇B(A · B) −(A · ∇)B,
where we placed B at the end of the ﬁnal term because ∇must act on it. We write ∇B to
indicate an operation our notation is not really equipped to handle. In this term, ∇acts only
on B, because A appeared to its left on the left-hand side of the equation. Interchanging
the roles of A and B, we also have
B × (∇× A) = ∇A(A · B) −(B · ∇)A,
where ∇A acts only on A. Adding these two equations together, noting that ∇B + ∇A is
simply an unrestricted ∇, we recover Eq. (3.74).
■
Exercises
3.6.1
Show that u × v is solenoidal if u and v are each irrotational.
3.6.2
If A is irrotational, show that A × r is solenoidal.
3.6.3
A rigid body is rotating with constant angular velocity ω. Show that the linear velocity
v is solenoidal.
3.6.4
If a vector function V(x, y, z) is not irrotational, show that if there exists a scalar func-
tion g(x, y, z) such that gV is irrotational, then
V · ∇× V = 0.
3.6.5
Verify the vector identity
∇× (A × B) = (B · ∇)A −(A · ∇)B −B(∇· A) + A(∇· B).
3.6.6
As an alternative to the vector identity of Example 3.6.4 show that
∇(A · B) = (A × ∇) × B + (B × ∇) × A + A(∇· B) + B(∇· A).
3.6.7
Verify the identity
A × (∇× A) = 1
2∇(A2) −(A · ∇)A.
3.6.8
If A and B are constant vectors, show that
∇(A · B × r) = A × B.

158
Chapter 3 Vector Analysis
3.6.9
Verify Eq. (3.70),
∇× (∇× V) = ∇(∇· V) −∇· ∇V,
by direct expansion in Cartesian coordinates.
3.6.10
Prove that ∇× (ϕ∇ϕ) = 0.
3.6.11
You are given that the curl of F equals the curl of G. Show that F and G may differ by
(a)
a constant and
(b)
a gradient of a scalar function.
3.6.12
The Navier-Stokes equation of hydrodynamics contains a nonlinear term of the form
(v · ∇)v. Show that the curl of this term may be written as −∇× [v × (∇× v)].
3.6.13
Prove that (∇u) × (∇v) is solenoidal, where u and v are differentiable scalar functions.
3.6.14
The function ϕ is a scalar satisfying Laplace’s equation, ∇2ϕ = 0. Show that ∇ϕ is
both solenoidal and irrotational.
3.6.15
Show that any solution of the equation
∇× (∇× A) −k2A = 0
automatically satisﬁes the vector Helmholtz equation
∇2A + k2A = 0
and the solenoidal condition
∇· A = 0.
Hint. Let ∇· operate on the ﬁrst equation.
3.6.16
The theory of heat conduction leads to an equation
∇29 = k |∇8|2 ,
where 8 is a potential satisfying Laplace’s equation: ∇28 = 0. Show that a
solution of this equation is 9 = k82/2.
3.6.17
Given the three matrices
Mx =


0
0
0
0
0
−i
0
i
0

,
My =


0
0
i
0
0
0
−i
0
0

,
and
Mz =


0
−i
0
i
0
0
0
0
0

,
show that the matrix-vector equation

M · ∇+ 13
1
c
∂
∂t

ψ = 0

3.7 Vector Integration
159
reproduces Maxwell’s equations in vacuum. Here ψ is a column vector with compo-
nents ψ j = B j −i E j/c, j = x, y, z. Note that ε0µ0 = 1/c2 and that 13 is the 3 × 3 unit
matrix.
3.6.18
Using the Pauli matrices σ i of Eq. (2.28), show that
(σ · a)(σ · b) = (a · b)12 + iσ · (a × b).
Here
σ ≡ˆexσ 1 + ˆeyσ 2 + ˆezσ 3,
a and b are ordinary vectors, and 12 is the 2 × 2 unit matrix.
3.7
VECTOR INTEGRATION
In physics, vectors occur in line, surface, and volume integrals. At least in principle, these
integrals can be decomposed into scalar integrals involving the vector components; there
are some useful general observations to make at this time.
Line Integrals
Possible forms for line integrals include the following:
Z
C
ϕdr,
Z
C
F · dr,
Z
C
V × dr.
(3.75)
In each of these the integral is over some path C that may be open (with starting and
endpoints distinct) or closed (forming a loop). Inserting the form of dr, the ﬁrst of these
integrals reduces immediately to
Z
C
ϕdr = ˆex
Z
C
ϕ(x, y, z)dx + ˆey
Z
C
ϕ(x, y, z)dy + ˆez
Z
C
ϕ(x, y, z)dz.
(3.76)
The unit vectors need not remain within the integral beause they are constant in both mag-
nitude and direction.
The integrals in Eq. (3.76) are one-dimensional scalar integrals. Note, however, that
the integral over x cannot be evaluated unless y and z are known in terms of x; similar
observations apply for the integrals over y and z. This means that the path C must be
speciﬁed. Unless ϕ has special properties, the value of the integral will depend on the path.
The other integrals in Eq. (3.75) can be handled similarly. For the second integral, which
is of common occurrence, being that which evaluates the work associated with displace-
ment on the path C, we have:
W =
Z
C
F · dr =
Z
C
Fx(x, y, z)dx +
Z
C
Fy(x, y, z)dy +
Z
C
Fz(x, y, z)dz.
(3.77)

160
Chapter 3 Vector Analysis
Example 3.7.1
LINE INTEGRALS
We consider two integrals in 2-D space:
IC =
Z
C
ϕ(x, y)dr,
with ϕ(x, y) = 1,
JC =
Z
C
F(x, y) · dr,
with F(x, y) = −yˆex + xˆey.
We perform integrations in the xy-plane from (0,0) to (1,1) by the two different paths
shown in Fig. 3.12:
Path C1 is (0,0) →(1,0) →(1,1),
Path C2 is the straight line (0,0) →(1,1).
For the ﬁrst segment of C1, x ranges from 0 to 1 while y is ﬁxed at zero. For the second
segment, y ranges from 0 to 1 while x = 1. Thus,
IC1 = ˆex
1
Z
0
dxϕ(x,0) + ˆey
1
Z
0
dyϕ(1, y) = ˆex
1
Z
0
dx + ˆey
1
Z
0
dy = ˆex + ˆey,
JC1 =
1
Z
0
dx Fx(x,0) +
1
Z
0
dy Fy(1, y) =
1
Z
0
=
1
Z
0
dx(0) +
1
Z
0
dy(1) = 1.
On Path 2, both dx and dy range from 0 to 1, with x = y at all points of the path. Thus,
IC2 = ˆex
1
Z
0
dx ϕ(x, x) + ˆey
1
Z
0
dy ϕ(y, y) = ˆex + ˆey,
JC2 =
1
Z
0
dx Fx(x, x) +
1
Z
0
dyFy(y, y) =
1
Z
0
dx(−x) +
1
Z
0
dy(y) = −1
2 + 1
2 = 0.
We see that integral I is independent of the path from (0,0) to (1,1), a nearly trivial special
case, while the integral J is not.
■
y
1
1
x
C2
C1
C1
FIGURE 3.12
Line integration paths.

3.7 Vector Integration
161
FIGURE 3.13
Positive normal directions: left, disk; right, spherical surface with hole.
Surface Integrals
Surface integrals appear in the same forms as line integrals, the element of area being a
vector, dσ, normal to the surface:
Z
ϕdσ,
Z
V · dσ,
Z
V × dσ.
Often dσ is written ˆndA, where ˆn is a unit vector indicating the normal direction. There are
two conventions for choosing the positive direction. First, if the surface is closed (has no
boundary), we agree to take the outward normal as positive. Second, for an open surface,
the positive normal depends on the direction in which the perimeter of the surface is tra-
versed. Starting from an arbitrary point on the perimeter, we deﬁne a vector u to be in the
direction of travel along the perimeter, and deﬁne a second vector v at our perimeter point
but tangent to and lying on the surface. We then take u×v as the positive normal direction.
This corresponds to a right-hand rule, and is illustrated in Fig. 3.13. It is necessary to deﬁne
the orientation carefully so as to deal with cases such as that of Fig. 3.13, right.
The dot-product form is by far the most commonly encountered surface integral, as it
corresponds to a ﬂow or ﬂux through the given surface.
Example 3.7.2
A SURFACE INTEGRAL
Consider a surface integral of the form I =
R
S B · dσ over the surface of a tetrahe-
dron whose vertices are at the origin and at the points (1,0,0), (0,1,0), and (0,0,1), with
B = (x + 1)ˆex + yˆey −zˆez. See Fig. 3.14.
The surface consists of four triangles, which can be identiﬁed and their contributions
evaluated, as follows:
1.
On the xy-plane (z = 0), vertices at (x, y) = (0,0), (1,0), and (0,1); direction of out-
ward normal is −ˆez, so dσ = −ˆezd A (d A = element of area on this triangle). Here,
B = (x + 1)ˆex + yˆey, and B · dσ = 0. So there is no contribution to I.
2.
On the xz plane (y = 0), vertices at (x, z) = (0,0), (1,0), and (0,1); direction of out-
ward normal is −ˆey, so dσ = −ˆeyd A. On this triangle, B = (x + 1)ˆex −zˆez, Again,
B · dσ = 0. There is no contribution to I.
3.
On the yz plane (x = 0), vertices at (y, z) = (0,0), (1,0), and (0,1); direction
of outward normal is −ˆex, so dσ = −ˆexd A. Here, B = ˆex + yˆey −zˆez, and

162
Chapter 3 Vector Analysis
B
O
1
1
1
A
x
C
C
z
y
z=1
z =0
A
B
2
2
3
2
2
FIGURE 3.14
Tetrahedron, and detail of the oblique face.
B · dσ = (−1)d A; the contribution to I is −1 times the area of the triangle (=1/2),
or I3 = −1/2.
4.
Obliquely oriented, vertices at (x, y, z) = (1,0,0), (0,1,0), (0,0,1); direction of out-
ward normal is ˆn = (ˆex + ˆey + ˆez)/
√
3, and dσ = ˆnd A. Using also B = (x + 1)ˆex +
yˆey −zˆez, this contribution to I becomes
I4 =
Z
14
x + 1 + y −z
√
3
d A =
Z
14
2(1 −z)
√
3
d A,
where we have used the fact that on this triangle, x + y + z = 1.
To complete the evaluation, we note that the geometry of the triangle is as shown
in Fig. 3.14, that the width of the triangle at height z is
√
2(1 −z), and a change dz in
z produces a displacement √3/2dz on the triangle. I4 therefore can be written
I4 =
1
Z
0
2(1 −z)2dz = 2
3.
Combining the nonzero contributions I3 and I4, we obtain the ﬁnal result
I = −1
2 + 2
3 = 1
6.
■
Volume Integrals
Volume integrals are somewhat simpler, because the volume element dτ is a scalar
quantity. Sometimes dτ is written d3r, or d3x when the coordinates were designated
(x1, x2, x3). In the literature, the form dr is frequently encountered, but in contexts that
usually reveal that it is a synonym for dτ, and not a vector quantity. The volume integrals
under consideration here are of the form
Z
Vdτ = ˆex
Z
Vx dτ + ˆey
Z
Vy dτ + ˆez
Z
Vz dτ.
The integral reduces to a vector sum of scalar integrals.

3.7 Vector Integration
163
Some volume integrals contain vector quantities in combinations that are actually scalar.
Often these can be rearranged by applying techniques such as integration by parts.
Example 3.7.3
INTEGRATION BY PARTS
Consider an integral over all space of the form
R
A(r)∇· f (r)d3r in the frequently occur-
ring special case in which either f or A vanish sufﬁciently strongly at inﬁnity. Expanding
the integrand into components,
Z
A(r) · ∇f (r)d3r =
x
dy dz

Ax f
 ∞
x=−∞−
Z
f ∂Ax
∂x dx

+ ···
= −
y
f ∂Ax
∂x dx dy dz−
y
f ∂Ay
∂y dx dy dz−
y
f ∂Az
∂z dx dy dz
= −
Z
f (r)∇· A(r)d3r.
(3.78)
For example, if A = eikz ˆp describes a photon with a constant polarization vector in the
direction ˆp and ψ(r) is a bound-state wave function (so it vanishes at inﬁnity), then
Z
eikz ˆp · ∇ψ(r)d3r = −(ˆp · ˆez)
Z
ψ(r)deikz
dz d3r = −ik(ˆp · ˆez)
Z
ψ(r)eikzd3r.
Only the z-component of the gradient contributes to the integral.
Analogous rearrangements (assuming the integrated terms vanish at inﬁnity) include
Z
f (r)∇· A(r)d3r = −
Z
A(r) · ∇f (r)d3r,
(3.79)
Z
C(r) ·
 ∇× A(r

d3r =
Z
A(r) ·
 ∇× C(r)

d3r.
(3.80)
In the cross-product example, the sign change from the integration by parts combines with
the signs from the cross product to give the result shown.
■
Exercises
3.7.1
The origin and the three vectors A, B, and C (all of which start at the origin) deﬁne a
tetrahedron. Taking the outward direction as positive, calculate the total vector area of
the four tetrahedral surfaces.
3.7.2
Find the work
H
F·dr done moving on a unit circle in the xy-plane, doing work against
a force ﬁeld given by
F = −ˆex y
x2 + y2 +
ˆeyx
x2 + y2 :
(a)
Counterclockwise from 0 to π,
(b)
Clockwise from 0 to −π.
Note that the work done depends on the path.

164
Chapter 3 Vector Analysis
3.7.3
Calculate the work you do in going from point (1,1) to point (3,3). The force you exert
is given by
F = ˆex(x −y) + ˆey(x + y).
Specify clearly the path you choose. Note that this force ﬁeld is nonconservative.
3.7.4
Evaluate
H
r · dr for a closed path of your choosing.
3.7.5
Evaluate
1
3
Z
s
r · dσ
over the unit cube deﬁned by the point (0,0,0) and the unit intercepts on the positive
x-, y-, and z-axes. Note that r · dσ is zero for three of the surfaces and that each of the
three remaining surfaces contributes the same amount to the integral.
3.8
INTEGRAL THEOREMS
The formulas in this section relate a volume integration to a surface integral on its boundary
(Gauss’ theorem), or relate a surface integral to the line deﬁning its perimeter (Stokes’
theorem). These formulas are important tools in vector analysis, particularly when the
functions involved are known to vanish on the boundary surface or perimeter.
Gauss’ Theorem
Here we derive a useful relation between a surface integral of a vector and the volume
integral of the divergence of that vector. Let us assume that a vector A and its ﬁrst deriva-
tives are continuous over a simply connected region of R3 (regions that contain holes,
like a donut, are not simply connected). Then Gauss’ theorem states that
I
∂V
A · dσ =
Z
V
∇· Adτ.
(3.81)
Here the notations V and ∂V respectively denote a volume of interest and the closed sur-
face that bounds it. The circle on the surface integral is an additional indication that the
surface is closed.
To prove the theorem, consider the volume V to be subdivided into an arbitrary large
number of tiny (differential) parallelepipeds, and look at the behavior of ∇·A for each. See
Fig. 3.15. For any given parallelepiped, this quantity is a measure of the net outward ﬂow
(of whatever A describes) through its boundary. If that boundary is interior (i.e., is shared
by another parallelepiped), outﬂow from one parallelepiped is inﬂow to its neighbor; in a
summation of all the outﬂows, all the contributions of interior boundaries cancel. Thus, the
sum of all the outﬂows in the volume will just be the sum of those through the exterior
boundary. In the limit of inﬁnite subdivision, these sums become integrals: The left-hand
side of Eq. (3.81) becomes the total outﬂow to the exterior, while its right-hand side is the
sum of the outﬂows of the differential elements (the parallelepipeds).

3.8 Integral Theorems
165
FIGURE 3.15
Subdivision for Gauss’ theorem.
A simple alternate explanation of Gauss’ theorem is that the volume integral sums the
outﬂows ∇· A from all elements of the volume; the surface integral computes the same
thing, by directly summing the ﬂow through all elements of the boundary.
If the region of interest is the complete R3, and the volume integral converges, the
surface integral in Eq. (3.81) must vanish, giving the useful result
Z
∇· Adτ = 0, integration over R3 and convergent.
(3.82)
Example 3.8.1
TETRAHEDRON
We check Gauss’ theorem for a vector B = (x + 1)ˆex + yˆey −zˆez, comparing
Z
V
∇· Bdτ
vs.
Z
∂V
B · dσ,
where V is the tetrahedron of Example 3.7.2. In that example we computed the surface
integral needed here, obtaining the value 1/6. For the integral over V , we take the diver-
gence, obtaining ∇· B = 1. The volume integral therefore reduces to the volume of the
tetrahedron that, with base of area 1/2 and height 1, has volume 1/3 × 1/2 × 1 = 1/6.
This instance of Gauss’ theorem is conﬁrmed.
■
Green’s Theorem
A frequently useful corollary of Gauss’ theorem is a relation known as Green’s theorem.
If u and v are two scalar functions, we have the identities
∇· (u∇v) = u∇2v + (∇u) · (∇v),
(3.83)
∇· (u∇v) = u∇2v + (∇u) · (∇v).
(3.84)

166
Chapter 3 Vector Analysis
Subtracting Eq. (3.84) from Eq. (3.83), integrating over a volume V on which u, v, and
their derivatives are continuous, and applying Gauss’ theorem, Eq. (3.81), we obtain
Z
V
(u∇2v −v∇2u)dτ =
I
∂V
(u∇v −v∇u) · dσ.
(3.85)
This is Green’s theorem. An alternate form of Green’s theorem, obtained from Eq. (3.83)
alone, is
I
∂V
u∇v · dσ =
Z
V
u∇2v dτ +
Z
V
∇u · ∇v dτ.
(3.86)
While the results already obtained are by far the most important forms of Gauss’ theo-
rem, volume integrals involving the gradient or the curl may also appear. To derive these,
we consider a vector of the form
B(x, y, z) = B(x, y, z)a,
(3.87)
in which a is a vector with constant magnitude and constant but arbitrary direction. Then
Eq. (3.81) becomes, applying Eq. (3.72),
a ·
I
∂V
B dσ =
Z
V
∇· (Ba)dτ = a
Z
V
∇B dτ.
This may be rewritten
a ·


I
∂V
B dσ −
Z
V
∇B dτ

= 0.
(3.88)
Since the direction of a is arbitrary, Eq. (3.88) cannot always be satisﬁed unless the quan-
tity in the square brackets evaluates to zero.6 The result is
I
∂V
B dσ =
Z
V
∇B dτ.
(3.89)
In a similar manner, using B = a × P in which a is a constant vector, we may show
I
∂V
dσ × P =
Z
V
∇× Pdτ.
(3.90)
These last two forms of Gauss’ theorem are used in the vector form of Kirchoff diffraction
theory.
6This exploitation of the arbitrary nature of a part of a problem is a valuable and widely used technique.

3.8 Integral Theorems
167
Stokes’ Theorem
Stokes’ theorem is the analog of Gauss’ theorem that relates a surface integral of a deriva-
tive of a function to the line integral of the function, with the path of integration being the
perimeter bounding the surface.
Let us take the surface and subdivide it into a network of arbitrarily small rectangles.
In Eq. (3.60) we saw that the circulation of a vector B about such a differential rectan-
gles (in the xy-plane) is ∇× Bzˆez dx dy. Identifying dx dy ˆez as the element of area dσ,
Eq. (3.60) generalizes to
X
four sides
B · dr = ∇× B · dσ.
(3.91)
We now sum over all the little rectangles; the surface contributions, from the right-hand
side of Eq. (3.91), are added together. The line integrals (left-hand side) of all interior
line segments cancel identically. See Fig. 3.16. Only the line integral around the perimeter
survives. Taking the limit as the number of rectangles approaches inﬁnity, we have
I
∂S
B · dr =
Z
S
∇× B · dσ.
(3.92)
Here ∂S is the perimeter of S. This is Stokes’ theorem. Note that both the sign of the
line integral and the direction of dσ depend on the direction the perimeter is traversed,
so consistent results will always be obtained. For the area and the line-integral direction
shown in Fig. 3.16, the direction of σ for the shaded rectangle will be out of the plane of
the paper.
Finally, consider what happens if we apply Stokes’ theorem to a closed surface. Since it
has no perimeter, the line integral vanishes, so
Z
S
∇× B · dσ = 0,
for S a closed surface.
(3.93)
As with Gauss’ theorem, we can derive additional relations connecting surface integrals
with line integrals on their perimeter. Using the arbitrary-vector technique employed to
FIGURE 3.16
Direction of normal for the shaded rectangle when perimeter of the surface
is traversed as indicated.

168
Chapter 3 Vector Analysis
reach Eqs. (3.89) and (3.90), we can obtain
Z
S
dσ × ∇ϕ =
I
∂S
ϕdr,
(3.94)
Z
S
(dσ × ∇) × P =
I
∂S
dr × P.
(3.95)
Example 3.8.2
OERSTED’S AND FARADAY’S LAWS
Consider the magnetic ﬁeld generated by a long wire that carries a time-independent cur-
rent I (meaning that ∂E/∂t = ∂B/∂t = 0). The relevant Maxwell equation, Eq. (3.68),
then takes the form ∇× B = µ0J. Integrating this equation over a disk S perpendicular to
and surrounding the wire (see Fig. 3.17), we have
I =
Z
S
J · dσ = 1
µ0
Z
S
(∇× B) · dσ.
Now we apply Stokes’ theorem, obtaining the result I = (1/µ0)
H
∂S B · dr, which is
Oersted’s law.
Similarly, we can integrate Maxwell’s equation for ∇× E, Eq. (3.69). Imagine moving
a closed loop (∂S) of wire (of area S) across a magnetic induction ﬁeld B. We have
Z
S
(∇× E) · dσ = −d
dt
Z
S
B · dσ = −d8
dt ,
where 8 is the magnetic ﬂux through the area S. By Stokes’ theorem, we have
Z
∂S
E · dr = −d8
dt .
This is Faraday’s law. The line integral represents the voltage induced in the wire loop; it is
equal in magnitude to the rate of change of the magnetic ﬂux through the loop. There is no
sign ambiguity; if the direction of ∂S is reversed, that causes a reversal of the direction of
dσ and thereby of 8.
■
B
I
FIGURE 3.17
Direction of B given by Oersted’s law.

3.8 Integral Theorems
169
Exercises
3.8.1
Using Gauss’ theorem, prove that
I
S
dσ = 0
if S = ∂V is a closed surface.
3.8.2
Show that
1
3
I
S
r · dσ = V,
where V is the volume enclosed by the closed surface S = ∂V.
Note. This is a generalization of Exercise 3.7.5.
3.8.3
If B = ∇× A, show that
I
S
B · dσ = 0
for any closed surface S.
3.8.4
From Eq. (3.72), with V the electric ﬁeld E and f the electrostatic potential ϕ, show
that, for integration over all space,
Z
ρϕdτ = ε0
Z
E2dτ.
This corresponds to a 3-D integration by parts.
Hint. E = −∇ϕ,∇· E = ρ/ε0. You may assume that ϕ vanishes at large r at least as
fast as r−1.
3.8.5
A particular steady-state electric current distribution is localized in space. Choosing a
bounding surface far enough out so that the current density J is zero everywhere on the
surface, show that
Z
Jdτ = 0.
Hint. Take one component of J at a time. With ∇· J = 0, show that Ji = ∇· (xiJ) and
apply Gauss’ theorem.
3.8.6
Given a vector t = −ˆexy + ˆeyx, show, with the help of Stokes’ theorem, that the integral
of t around a continuous closed curve in the xy-plane satisﬁes
1
2
I
t · dλ = 1
2
I
(x dy −y dx) = A,
where A is the area enclosed by the curve.

170
Chapter 3 Vector Analysis
3.8.7
The calculation of the magnetic moment of a current loop leads to the line integral
I
r × dr.
(a)
Integrate around the perimeter of a current loop (in the xy-plane) and show that
the scalar magnitude of this line integral is twice the area of the enclosed surface.
(b)
The perimeter of an ellipse is described by r = ˆexa cosθ + ˆeyb sinθ. From part (a)
show that the area of the ellipse is πab.
3.8.8
Evaluate
H
r × dr by using the alternate form of Stokes’ theorem given by Eq. (3.95):
Z
S
(dσ × ∇) × P =
I
dλ × P.
Take the loop to be entirely in the xy-plane.
3.8.9
Prove that
I
u∇v · dλ = −
I
v∇u · dλ.
3.8.10
Prove that
I
u∇v · dλ =
Z
S
(∇u) × (∇v) · dσ.
3.8.11
Prove that
I
∂V
dσ × P =
Z
V
∇× Pdτ.
3.8.12
Prove that
Z
S
dσ × ∇ϕ =
I
∂S
ϕdr.
3.8.13
Prove that
Z
S
(dσ × ∇) × P =
I
∂S
dr × P.
3.9
POTENTIAL THEORY
Much of physics, particularly electromagnetic theory, can be treated more simply by intro-
ducing potentials from which forces can be derived. This section deals with the deﬁnition
and use of such potentials.

3.9 Potential Theory
171
Scalar Potential
If, over a given simply connected region of space (one with no holes), a force can be
expressed as the negative gradient of a scalar function ϕ,
F = −∇ϕ,
(3.96)
we call ϕ a scalar potential, and we beneﬁt from the feature that the force can be described
in terms of one function instead of three. Since the force is a derivative of the scalar poten-
tial, the potential is only determined up to an additive constant, which can be used to adjust
its value at inﬁnity (usually zero) or at some other reference point. We want to know what
conditions F must satisfy in order for a scalar potential to exist.
First, consider the result of computing the work done against a force given by −∇ϕ
when an object subject to the force is moved from a point A to a point B. This is a line
integral of the form
−
B
Z
A
F · dr =
B
Z
A
∇ϕ · dr.
(3.97)
But, as pointed out in Eq. (3.41), ∇ϕ · dr = dϕ, so the integral is in fact independent of the
path, depending only on the endpoints A and B. So we have
−
B
Z
A
F · dr = ϕ(rB) −ϕ(rA),
(3.98)
which also means that if A and B are the same point, forming a closed loop,
I
F · dr = 0.
(3.99)
We conclude that a force (on an object) described by a scalar potential is a conservative
force, meaning that the work needed to move the object between any two points is inde-
pendent of the path taken, and that ϕ(r) is the work needed to move to the point r from a
reference point where the potential has been assigned the value zero.
Another property of a force given by a scalar potential is that
∇× F = −∇× ∇ϕ = 0
(3.100)
as prescribed by Eq. (3.64). This observation is consistent with the notion that the lines of
force of a conservative F cannot form closed loops.
The three conditions, Eqs. (3.96), (3.99), and (3.100), are all equivalent. If we take
Eq. (3.99) for a differential loop, its left side and that of Eq. (3.100) must, according
to Stokes’ theorem, be equal. We already showed both these equations followed from
Eq. (3.96). To complete the establishment of full equivalence, we need only to derive
Eq. (3.96) from Eq. (3.99). Going backward to Eq. (3.97), we rewrite it as
B
Z
A
(F + ∇ϕ) · dr = 0,

172
Chapter 3 Vector Analysis
which must be satisﬁed for all A and B. This means its integrand must be identically zero,
thereby recovering Eq. (3.96).
Example 3.9.1
GRAVITATIONAL POTENTIAL
We have previously, in Example 3.5.2, illustrated the generation of a force from a scalar
potential. To perform the reverse process, we must integrate. Let us ﬁnd the scalar potential
for the gravitational force
FG = −Gm1m2ˆr
r2
= −kˆr
r2 ,
radially inward. Setting the zero of scalar potential at inﬁnity, we obtain by integrating
(radially) from inﬁnity to position r,
ϕG(r) −ϕG(∞) = −
r
Z
∞
FG · dr = +
∞
Z
r
FG · dr.
The minus sign in the central member of this equation arises because we are calculating
the work done against the gravitational force. Evaluating the integral,
ϕG(r) = −
∞
Z
r
kdr
r2 = −k
r = −Gm1m2
r
.
The ﬁnal negative sign corresponds to the fact that gravity is an attractive force.
■
Vector Potential
In some branches of physics, especially electrodynamics, it is convenient to introduce a
vector potential A such that a (force) ﬁeld B is given by
B = ∇× A.
(3.101)
An obvious reason for introducing A is that it causes B to be solenoidal; if B is the mag-
netic induction ﬁeld, this property is required by Maxwell’s equations. Here we want to
develop a converse, namely to show that when B is solenoidal, a vector potential A exists.
We demonstrate the existence of A by actually writing it.
Our construction is
A = ˆey
x
Z
x0
Bz(x, y, z)dx + ˆez


y
Z
y0
Bx(x0, y, z)dy −
x
Z
x0
By(x, y, z)dx

.
(3.102)

3.9 Potential Theory
173
Checking the y- and z-components of ∇× A ﬁrst, noting that Ax = 0,
(∇× A)y = −∂Az
∂x = + ∂
∂x
x
Z
x0
By(x, y, z)dx = By,
(∇× A)z = +∂Ay
∂x == ∂
∂x
x
Z
x0
Bz(x, y, z)dx = Bz.
The x-component of ∇× A is a bit more complicated. We have
(∇× A)x = ∂Az
∂y −∂Ay
∂z
= ∂
∂y


y
Z
y0
Bx(x0, y, z)dy −
x
Z
x0
By(x, y, z)dx

−∂
∂z
x
Z
x0
Bz(x, y, z)dx
= Bx(x0, y, z) −
x
Z
x0
∂By(x, y, z)
∂y
+ ∂Bz(x, y, z)
∂z

dx.
To go further, we must use the fact that B is solenoidal, which means ∇· B = 0. We can
therefore make the replacement
∂By(x, y, z)
∂y
+ ∂Bz(x, y, z)
∂z
= −∂Bx(x, y, z)
∂x
,
after which the x integration becomes trivial, yielding
+
Z x
x0
∂Bx(x, y, z)
∂x
dx = Bx(x, y, z) −Bx(x0, y, z),
leading to the desired ﬁnal result (∇× A)x = Bx.
While we have shown that there exists a vector potential A such that ∇× A = B subject
only to the condition that B be solenoidal, we have in no way established that A is unique.
In fact, A is far from unique, as we can add to it not only an arbitrary constant, but also the
gradient of any scalar function, ∇ϕ, without affecting B at all. Moreover, our veriﬁcation
of A was independent of the values of x0 and y0, so these can be assigned arbitrarily
without affecting B. In addition, we can derive another formula for A in which the roles of
x and y are interchanged:
A = −ˆex
y
Z
y0
Bz(x, y, z)dy −ˆez


x
Z
x0
By(x, y0, z)dx −
y
Z
y0
Bx(x, y, z)dy

.
(3.103)

174
Chapter 3 Vector Analysis
Example 3.9.2
MAGNETIC VECTOR POTENTIAL
We consider the construction of the vector potential for a constant magnetic induction ﬁeld
B = Bzˆez.
(3.104)
Using Eq. (3.102), we have (choosing the arbitrary value of x0 to be zero)
A = ˆey
x
Z
0
Bz dx = ˆeyx Bz.
(3.105)
Alternatively, we could use Eq. (3.103) for A, leading to
A′ = −ˆex yBz.
(3.106)
Neither of these is the form for A found in many elementary texts, which for B from
Eq. (3.104) is
A′′ = 1
2 (B × r) = Bz
2 (xˆey −yˆex).
(3.107)
These disparate forms can be reconciled if we use the freedom to add to A any expression
of the form ∇ϕ. Taking ϕ = Cxy, the quantity that can be added to A will be of the form
∇ϕ = C(yˆex + xˆey).
We now see that
A −Bz
2 (yˆex + xˆey) = A′ + Bz
2 (yˆex + xˆey) = A′′,
showing that all these formulas predict the same value of B.
■
Example 3.9.3
POTENTIALS IN ELECTROMAGNETISM
If we introduce suitably deﬁned scalar and vector potentials ϕ and A into Maxwell’s
equations, we can obtain equations giving these potentials in terms of the sources of the
electromagnetic ﬁeld (charges and currents). We start with B = ∇× A, thereby assuring
satisfaction of the Maxwell’s equation ∇· B = 0. Substitution into the equation for ∇× E
yields
∇× E = −∇× ∂A
∂t
−→
∇×

E + ∂A
∂t

= 0,
showing that E + ∂A/∂t is a gradient and can be written as −∇ϕ, thereby deﬁning ϕ. This
preserves the notion of an electrostatic potential in the absence of time dependence, and
means that A and ϕ have now been deﬁned to give
B = ∇× A,
E = −∇ϕ −∂A
∂t .
(3.108)
At this point A is still arbitrary to the extent of adding any gradient, which is equivalent to
making an arbitrary choice of ∇· A. A convenient choice is to require
1
c2
∂ϕ
∂t + ∇· A = 0.
(3.109)

3.9 Potential Theory
175
This gauge condition is called the Lorentz gauge, and transformations of A and ϕ to
satisfy it or any other legitimate gauge condition are called gauge transformations. The
invariance of electromagnetic theory under gauge transformation is an important precursor
of contemporary directions in fundamental physical theory.
From Maxwell’s equation for ∇· E and the Lorentz gauge condition, we get
ρ
ε0
= ∇· E = −∇2E −∂
∂t ∇· A = −∇2ϕ + 1
c2
∂2ϕ
∂t2 ,
(3.110)
showing that the Lorentz gauge permitted us to decouple A and ϕ to the extent that we
have an equation for ϕ in terms only of the charge density ρ; neither A nor the current
density J enters this equation.
Finally, from the equation for ∇× B, we obtain
1
c2
∂2A
∂t2 −∇2A = µ0J.
(3.111)
Proof of this formula is the subject of Exercise 3.9.11.
■
Gauss’ Law
Consider a point charge q at the origin of our coordinate system. It produces an electric
ﬁeld E, given by
E =
qˆr
4πε0r2 .
(3.112)
Gauss’ law states that for an arbitrary volume V,
I
∂V
E · dσ =
( q
ε0
if ∂V encloses q,
0
if ∂V does not enclose q.
(3.113)
The case that ∂V does not enclose q is easily handled. From Eq. (3.54), the r−2 central
force E is divergenceless everywhere except at r = 0, and for this case, throughout the
entire volume V . Thus, we have, invoking Gauss’ theorem, Eq. (3.81),
Z
V
∇· E = 0
−→
E · dσ = 0.
If q is within the volume V , we must be more devious. We surround r = 0 by a small
spherical hole (of radius δ), with a surface we designate S′, and connect the hole with the
boundary of V via a small tube, thereby creating a simply connected region V ′ to which
Gauss’ theorem will apply. See Fig. 3.18. We now consider
H
E · dσ on the surface of
this modiﬁed volume. The contribution from the connecting tube will become negligible
in the limit that it shrinks toward zero cross section, as E is ﬁnite everywhere on the
tube’s surface. The integral over the modiﬁed ∂V will thus be that of the original ∂V (over
the outer boundary, which we designate S), plus that of the inner spherical surface (S′).

176
Chapter 3 Vector Analysis
FIGURE 3.18
Making a multiply connected region simply connected.
But note that the “outward” direction for S′ is toward smaller r, so dσ ′ = −ˆrd A. Because
the modiﬁed volume contains no charge, we have
I
∂V ′
E · dσ =
I
S
E · dσ +
q
4πε0
I
S′
ˆr · dσ ′
δ2
= 0,
(3.114)
where we have inserted the explicit form of E in the S′ integral. Because S′ is a sphere of
radius δ, this integral can be evaluated. Writing d as the element of solid angle, so d A =
δ2d,
I
S′
ˆr · dσ ′
δ2
=
Z
ˆr
δ2 · (−ˆrδ2 d) = −
Z
d = −4π,
independent of the value of δ. Returning now to Eq. (3.114), it can be rearranged into
I
S
E · dσ = −
q
4πε0
(−4π) = + q
ε0
,
the result needed to conﬁrm the second case of Gauss’ law, Eq. (3.113).
Because the equations of electrostatics are linear, Gauss’ law can be extended to collec-
tions of charges, or even to continuous charge distributions. In that case, q can be replaced
by
R
V ρ dτ, and Gauss’ law becomes
Z
∂V
E · dσ =
Z
V
ρ
ε0
dτ.
(3.115)
If we apply Gauss’ theorem to the left side of Eq. (3.115), we have
Z
V
∇· Edτ =
Z
V
ρ
ε0
dτ.
Since our volume is completely arbitrary, the integrands of this equation must be equal, so
∇· E = ρ
ε0
.
(3.116)
We thus see that Gauss’ law is the integral form of one of Maxwell’s equations.

3.9 Potential Theory
177
Poisson’s Equation
If we return to Eq. (3.116) and, assuming a situation independent of time, write E = −∇ϕ,
we obtain
∇2ϕ = −ρ
ε0
.
(3.117)
This equation, applicable to electrostatics,7 is called Poisson’s equation. If, in addition,
ρ = 0, we have an even more famous equation,
∇2ϕ = 0,
(3.118)
Laplace’s equation.
To make Poisson’s equation apply to a point charge q, we need to replace ρ by a con-
centration of charge that is localized at a point and adds up to q. The Dirac delta function
is what we need for this purpose. Thus, for a point charge q at the origin, we write
∇2ϕ = −q
ε0
δ(r),
(charge q at r = 0).
(3.119)
If we rewrite this equation, inserting the point-charge potential for ϕ, we have
q
4πε0
∇2
1
r

= −q
ε0
δ(r),
which reduces to
∇2
1
r

= −4π δ(r).
(3.120)
This equation circumvents the problem that the derivatives of 1/r do not exist at r = 0,
and gives appropriate and correct results for systems containing point charges. Like the
deﬁnition of the delta function itself, Eq. (3.120) is only meaningful when inserted into an
integral. It is an important result that is used repeatedly in physics, often in the form
∇2
1
 1
r12

= −4π δ(r1 −r2).
(3.121)
Here r12 = |r1 −r2|, and the subscript in ∇1 indicates that the derivatives apply to r1.
Helmholtz’s Theorem
We now turn to two theorems that are of great formal importance, in that they establish
conditions for the existence and uniqueness of solutions to time-independent problems in
electromagnetic theory. The ﬁrst of these theorems is:
A vector ﬁeld is uniquely speciﬁed by giving its divergence and its curl within a simply
connected region and its normal component on the boundary.
7For general time dependence, see Eq. (3.110).

178
Chapter 3 Vector Analysis
Note that both for this theorem and the next (Helmholtz’s theorem), even if there are points
in the simply connected region where the divergence or the curl is only deﬁned in terms of
delta functions, these points are not to be removed from the region.
Let P be a vector ﬁeld satisfying the conditions
∇· P = s,
∇× P = c,
(3.122)
where s may be interpreted as a given source (charge) density and c as a given circulation
(current) density. Assuming that the normal component Pn on the boundary is also given,
we want to show that P is unique.
We proceed by assuming the existence of a second vector, P′, which satisﬁes Eq. (3.122)
and has the same value of Pn. We form Q = P −P′, which must have ∇· Q, ∇× Q, and
Qn all identically zero. Because Q is irrotational, there must exist a potential ϕ such that
Q = −∇ϕ, and because ∇· Q = 0, we also have
∇2ϕ = 0.
Now we draw on Green’s theorem in the form given in Eq. (3.86), letting u and v each
equal ϕ. Because Qn = 0 on the boundary, Green’s theorem reduces to
Z
V
(∇ϕ) · (∇ϕ)dτ =
Z
V
Q · Qdτ = 0.
This equation can only be satisﬁed if Q is identically zero, showing that P′ = P, thereby
proving the theorem.
The second theorem we shall prove, Helmholtz’s theorem, is
A vector P with both source and circulation densities vanishing at inﬁnity may be writ-
ten as the sum of two parts, one of which is irrotational, the other of which is solenoidal.
Helmholtz’s theorem will clearly be satisﬁed if P can be written in the form
P = −∇ϕ + ∇× A,
(3.123)
since −∇ϕ is irrotational, while ∇× A is solenoidal. Because P is known, so are also s
and c, deﬁned as
s = ∇· P,
c = ∇× P.
We proceed by exhibiting expressions for ϕ and A that enable the recovery of s and c.
Because the region here under study is simply connected and the vector involved vanishes
at inﬁnity (so that the ﬁrst theorem of this subsection applies), having the correct s and c
guarantees that we have properly reproduced P.
The formulas proposed for ϕ and A are the following, written in terms of the spatial
variable r1:
ϕ(r1) = 1
4π
Z s(r2)
r12
dτ2,
(3.124)
A(r1) = 1
4π
Z c(r2)
r12
dτ2.
(3.125)
Here r12 = |r1 −r2|.

3.9 Potential Theory
179
If Eq. (3.123) is to be satisﬁed with the proposed values of ϕ and A, it is necessary that
∇· P = −∇· ∇ϕ + ∇· (∇× A) = −∇2ϕ = s,
∇× P = −∇× ∇ϕ + ∇× (∇× A) = ∇× (∇× A) = c.
To check that −∇2ϕ = s, we examine
−∇2
1 ϕ(r1) = −1
4π
Z
∇2
1
 1
r12

s(r2)dτ2
= −1
4π
Z 
−4πδ(r1 −r2)

s(r2)dτ2 = s(r1).
(3.126)
We have written ∇1 to make clear that it operates on r1 and not r2, and we have used the
delta-function property given in Eq. (3.121). So s has been recovered.
We now check that ∇× (∇× A) = c. We start by using Eq. (3.70) to convert this
condition to a more easily utilized form:
∇× (∇× A) = ∇(∇· A) −∇2A = c.
Taking r1 as the free variable, we look ﬁrst at
∇1
 ∇1 · A(r1)

= 1
4π ∇1
Z
∇1 ·
c(r2)
r12

dτ2
= 1
4π ∇1
Z
c(r2) · ∇1
 1
r12

dτ2
= 1
4π ∇1
Z
c(r2) ·

−∇2
 1
r12

dτ2.
To reach the second line of this equation, we used Eq. (3.72) for the special case that the
vector in that equation is not a function of the variable being differentiated. Then, to obtain
the third line, we note that because the ∇1 within the integral acts on a function of r1 −r2,
we can change ∇1 into ∇2 and introduce a sign change.
Now we integrate by parts, as in Example 3.7.3, reaching
∇1

∇1 · A(r1)

= 1
4π ∇1
Z  ∇2 · c(r2)
 1
r12

dτ2.
At last we have the result we need: ∇2 · c(r2) vanishes, because c is a curl, so the entire
∇(∇· A) term is zero and may be dropped. This reduces the condition we are checking to
−∇2A = c.
The quantity −∇2A is a vector Laplacian and we may individually evaluate its Cartesian
components. For component j,
−∇2
1 A j(r1) = −1
4π
Z
c j(r2)∇2
1
 1
r12

dτ2
= −1
4π
Z
c j(r2)

−4πδ(r1 −r2)

dτ2 = c j(r1).
This completes the proof of Helmholtz’s theorem.
Helmholtz’s theorem legitimizes the division of the quantities appearing in electromag-
netic theory into an irrotational vector ﬁeld E and a solenoidal vector ﬁeld B, together

180
Chapter 3 Vector Analysis
with their respective representations using scalar and vector potentials. As we have seen
in numerous examples, the source s is identiﬁed as the charge density (divided by ε0) and
the circulation c is the current density (multiplied by µ0).
Exercises
3.9.1
If a force F is given by
F = (x2 + y2 + z2)n(ˆexx + ˆeyy + ˆezz),
ﬁnd
(a)
∇· F.
(b)
∇× F.
(c)
A scalar potential ϕ(x, y, z) so that F = −∇ϕ.
(d)
For what value of the exponent n does the scalar potential diverge at both the
origin and inﬁnity?
ANS.
(a)
(2n + 3)r2n
(b)
0
(c)
−r2n+2/(2n + 2), n ̸= −1
(d)
n = −1, ϕ = −lnr.
3.9.2
A sphere of radius a is uniformly charged (throughout its volume). Construct the elec-
trostatic potential ϕ(r) for 0 ≤r < ∞.
3.9.3
The origin of the Cartesian coordinates is at the Earth’s center. The moon is on the
z-axis, a ﬁxed distance R away (center-to-center distance). The tidal force exerted by
the moon on a particle at the Earth’s surface (point x, y, z) is given by
Fx = −GMm x
R3 ,
Fy = −GMm y
R3 ,
Fz = +2GMm z
R3 .
Find the potential that yields this tidal force.
ANS.
−GMm
R3

z2 −1
2x2 −1
2 y2

.
3.9.4
A long, straight wire carrying a current I produces a magnetic induction B with com-
ponents
B = µ0I
2π

−
y
x2 + y2 ,
x
x2 + y2 ,0

.
Find a magnetic vector potential A.
ANS.
A = −ˆz(µ0I/4π)ln(x2 + y2). (This solution is not unique.)
3.9.5
If
B = ˆr
r2 =
 x
r3 , y
r3 , z
r3

,
ﬁnd a vector A such that ∇× A = B.
ANS.
One possible solution is A =
ˆex yz
r(x2 + y2) −
ˆeyxz
r(x2 + y2).

3.9 Potential Theory
181
3.9.6
Show that the pair of equations
A = 1
2(B × r),
B = ∇× A,
is satisﬁed by any constant magnetic induction B.
3.9.7
Vector B is formed by the product of two gradients
B = (∇u) × (∇v),
where u and v are scalar functions.
(a)
Show that B is solenoidal.
(b)
Show that
A = 1
2(u ∇v −v ∇u)
is a vector potential for B, in that
B = ∇× A.
3.9.8
The magnetic induction B is related to the magnetic vector potential A by B = ∇× A.
By Stokes’ theorem
Z
B · dσ =
I
A · dr.
Show that each side of this equation is invariant under the gauge transformation, A →
A + ∇ϕ.
Note. Take the function ϕ to be single-valued.
3.9.9
Show that the value of the electrostatic potential ϕ at any point P is equal to the average
of the potential over any spherical surface centered on P, provided that there are no
electric charges on or within the sphere.
Hint. Use Green’s theorem, Eq. (3.85), with u = r−1, the distance from P, and v = ϕ.
Equation (3.120) will also be useful.
3.9.10
Using Maxwell’s equations, show that for a system (steady current) the magnetic vector
potential A satisﬁes a vector Poisson equation,
∇2A = −µJ,
provided we require ∇· A = 0.
3.9.11
Derive, assuming the Lorentz gauge, Eq. (3.109):
1
c2
∂2A
∂t2 −∇2A = µ0J.
Hint. Eq. (3.70) will be helpful.

182
Chapter 3 Vector Analysis
3.9.12
Prove that an arbitrary solenoidal vector B can be described as B = ∇× A, with
A = −ˆex
y
Z
y0
Bz(x, y, z)dy −ˆez


x
Z
x0
By(x, y0, z)dx −
y
Z
y0
Bx(x, y, z)dy

.
3.10
CURVILINEAR COORDINATES
Up to this point we have treated vectors essentially entirely in Cartesian coordinates; when
r or a function of it was encountered, we wrote r as
p
x2 + y2 + z2, so that Cartesian
coordinates could continue to be used. Such an approach ignores the simpliﬁcations that
can result if one uses a coordinate system that is appropriate to the symmetry of a problem.
Central force problems are frequently easiest to deal with in spherical polar coordinates.
Problems involving geometrical elements such as straight wires may be best handled in
cylindrical coordinates. Yet other coordinate systems (of use too infrequent to be described
here) may be appropriate for other problems.
Naturally, there is a price that must be paid for the use of a non-Cartesian coordinate sys-
tem. Vector operators become different in form, and their speciﬁc forms may be position-
dependent. We proceed here to examine these questions and derive the necessary formulas.
Orthogonal Coordinates in R3
In Cartesian coordinates the point (x0, y0, z0) can be identiﬁed as the intersection of three
planes: (1) the plane x = x0 (a surface of constant x), (2) the plane y = y0 (constant y), and
(3) the plane z = z0 (constant z). A change in x corresponds to a displacement normal to
the surface of constant x; similar remarks apply to changes in y or z. The planes of constant
coordinate value are mutually perpendicular, and have the obvious feature that the normal to
any given one of them is in the same direction, no matter where on the plane it is constructed
(a plane of constant x has a normal that is, of course, everywhere in the direction of ˆex).
Consider now, as an example of a curvilinear coordinate system, spherical polar coor-
dinates (see Fig. 3.19). A point r is identiﬁed by r (distance from the origin), θ (angle of
r relative to the polar axis, which is conventionally in the z direction), and ϕ (dihedral
angle between the zx plane and the plane containing ˆez and r). The point r is therefore at
the intersection of (1) a sphere of radius r, (2) a cone of opening angle θ, and (3) a half-
plane through equatorial angle ϕ. This example provides several observations: (1) general
θ
ϕ
y
r
z
x
FIGURE 3.19
Spherical polar coordinates.

3.10 Curvilinear Coordinates
183
θ
keθ
ˆ
r
r ′
FIGURE 3.20
Effect of a “large” displacement in the direction ˆeθ. Note that r′ ̸= r.
coordinates need not be lengths, (2) a surface of constant coordinate value may have a
normal whose direction depends on position, (3) surfaces with different constant values of
the same coordinate need not be parallel, and therefore also (4) changes in the value of a
coordinate may move r in both an amount and a direction that depends on position.
It is convenient to deﬁne unit vectors ˆer, ˆeθ, ˆeϕ in the directions of the normals to the
surfaces, respectively, of constant r, θ, and ϕ. The spherical polar coordinate system has
the feature that these unit vectors are mutually perpendicular, meaning that, for example, ˆeθ
will be tangent to both the constant-r and constant-ϕ surfaces, so that a small displacement
in the ˆeθ direction will not change the values of either the r or the ϕ coordinate. The
reason for the restriction to “small” displacements is that the directions of the normals
are position-dependent; a “large” displacement in the ˆeθ direction would change r (see
Fig. 3.20). If the coordinate unit vectors are mutually perpendicular, the coordinate system
is said to be orthogonal.
If we have a vector ﬁeld V (so we associate a value of V with each point in a region of
R3), we can write V(r) in terms of the orthogonal set of unit vectors that are deﬁned for
the point r; symbolically, the result is
V(r) = Vr ˆer + Vθ ˆeθ + Vϕ ˆeϕ.
It is important to realize that the unit vectors ˆei have directions that depend on the value
of r. If we have another vector ﬁeld W(r) for the same point r, we can perform algebraic
processes8 on V and W by the same rules as for Cartesian coordinates. For example, at the
point r,
V · W = Vr Wr + Vθ Wθ + VϕWϕ.
However, if V and W are not associated with the same r, we cannot carry out such opera-
tions in this way, and it is important to realize that
r ̸= r ˆer + θ ˆeθ + ϕˆeϕ.
Summarizing, the component formulas for V or W describe component decompositions
applicable to the point at which the vector is speciﬁed; an attempt to decompose r as
illustrated above is incorrect because it uses ﬁxed unit-vector orientations where they do
not apply.
Dealing for the moment with an arbitrary curvilinear system, with coordinates labeled
(q1,q2,q3), we consider how changes in the qi are related to changes in the Cartesian
coordinates. Since x can be thought of as a function of the qi, namely x(q1,q2,q3), we have
dx = ∂x
∂q1
dq1 + ∂x
∂q2
dq2 + ∂x
∂q3
dq3,
(3.127)
with similar formulas for dy and dz.
8Addition, multiplication by a scalar, dot and cross products (but not application of differential or integral operators).

184
Chapter 3 Vector Analysis
We next form a measure of the differential displacement, dr, associated with changes
dqi. We actually examine
(dr)2 = (dx)2 + (dy)2 + (dz)2.
Taking the square of Eq. (3.127), we get
(dx)2 =
X
i j
∂x
∂qi
∂x
∂q j
dqi dq j
and similar expressions for (dy)2 and (dz)2. Combining these and collecting terms with
the same dqi dq j, we reach the result
(dr)2 =
X
i j
gi j dqi dq j,
(3.128)
where
gi j(q1,q2,q3) = ∂x
∂qi
∂x
∂q j
+ ∂y
∂qi
∂y
∂q j
+ ∂z
∂qi
∂z
∂q j
.
(3.129)
Spaces with a measure of distance given by Eq. (3.128) are called metric or Riemannian.
Equation (3.129) can be interpreted as the dot product of a vector in the dqi direction, of
components (∂x/∂qi, ∂y/∂qi, ∂z/∂qi), with a similar vector in the dq j direction. If the
qi coordinates are perpendicular, the coefﬁcients gi j will vanish when i ̸= j.
Since it is our objective to discuss orthogonal coordinate systems, we specialize
Eqs. (3.128) and (3.129) to
(dr)2 = (h1 dq1)2 + (h2 dq2)2 + (h3 dq3)2,
(3.130)
h2
i =
 ∂x
∂qi
2
+
 ∂y
∂qi
2
+
 ∂y
∂qi
2
.
(3.131)
If we consider Eq. (3.130) for a case dq2 = dq3 = 0, we see that we can identify h1dq1
as dr1, meaning that the element of displacement in the q1 direction is h1dq1. Thus, in
general,
dri = hidqi,
or
∂r
∂qi
= hi ˆei.
(3.132)
Here ˆei is a unit vector in the qi direction, and the overall dr takes the form
dr = h1dq1 ˆe1 + h2dq2 ˆe2 + h3dq3 ˆe3.
(3.133)
Note that hi may be position-dependent and must have the dimension needed to cause
hidqi to be a length.
Integrals in Curvilinear Coordinates
Given the scale factors hi for a set of coordinates, either because they have been tabulated
or because we have evaluated them via Eq. (3.131), we can use them to set up formulas for
integration in the curvilinear coordinates. Line integrals will take the form
Z
C
V · dr =
X
i
Z
C
Vihidqi.
(3.134)

3.10 Curvilinear Coordinates
185
Surface integrals take the same form as in Cartesian coordinates, with the exception that
instead of expressions like dx dy we have (h1dq1)(h2dq2) = h1h2 dq1 dq2 etc. This means
that
Z
S
V · dσ =
Z
S
V1h2h3 dq2dq3 +
Z
S
V2h3h1 dq3dq1 +
Z
S
V3h1h2 dq1dq2.
(3.135)
The element of volume in orthogonal curvilinear coordinates is
dτ = h1h2h3 dq1dq2dq3,
(3.136)
so volume integrals take the form
Z
V
ϕ(q1,q2,q3)h1h2h3dq1dq2dq3,
(3.137)
or the analogous expression with ϕ replaced by a vector V(q1,q2,q3).
Differential Operators in Curvilinear Coordinates
We continue with a restriction to orthogonal coordinate systems.
Gradient—Because our curvilinear coordinates are orthogonal, the gradient takes the
same form as for Cartesian coordinates, providing we use the differential displacements
dri = hi dqi in the formula. Thus, we have
∇ϕ(q1,q2,q3) = ˆe1
1
h1
∂ϕ
∂q1
+ ˆe2
1
h2
∂ϕ
∂q2
+ ˆe3
1
h3
∂ϕ
∂q3
,
(3.138)
this corresponds to writing ∇as
∇= ˆe1
1
h1
∂
∂q1
+ ˆe2
1
h2
∂
∂q2
+ ˆe3
1
h3
∂
∂q3
.
(3.139)
Divergence—This operator must have the same meaning as in Cartesian coordinates,
so ∇· V must give the net outward ﬂux of V per unit volume at the point of evaluation.
The key difference from the Cartesian case is that an element of volume will no longer be
a parallelepiped, as the scale factors hi are in general functions of position. See Fig. 3.21.
To compute the net outﬂow of V in the q1 direction from a volume element deﬁned by
+B1h2dq2h3dq3|q1+dq1/2
h2dq2
−B1h2dq2h3dq3|q1−dq1/2
h3dq3
FIGURE 3.21
Outﬂow of B1 in the q1 direction from a curvilinear volume element.

186
Chapter 3 Vector Analysis
dq1, dq2, dq3 and centered at (q1,q2,q3), we must form
Net q1 outﬂow = −V1h2h3 dq2dq3

q1−dq1/2,q2,q3
+ V1h2h3 dq2dq3

q1+dq1/2,q2,q3
.
(3.140)
Note that not only V1, but also h2h3 must be evaluated at the displaced values of q1; this
product may have different values at q1 + dq1/2 and q1 −dq1/2. Rewriting Eq. (3.140) in
terms of a derivative with respect to q1, we have
Net q1 outﬂow = ∂
∂q1
(V1h2h3)dq1dq2dq3.
Combining this with the q2 and q3 outﬂows and dividing by the differential volume
h1h2h3 dq1dq2dq3, we get the formula
∇· V(q1,q2,q3) =
1
h1h2h3
 ∂
∂q1
(V1h2h3) + ∂
∂q2
(V2h3h1) + ∂
∂q3
(V3h1h2)

.
(3.141)
Laplacian—From the formulas for the gradient and divergence, we can form the Laplacian
in curvilinear coordinates:
∇2ϕ(q1,q2,q3) = ∇· ∇ϕ =
1
h1h2h3
 ∂
∂q1
h2h3
h1
∂ϕ
∂q1

+ ∂
∂q2
h3h1
h2
∂ϕ
∂q2

+ ∂
∂q3
h1h2
h3
∂ϕ
∂q3

.
(3.142)
Note that the Laplacian contains no cross derivatives, such as ∂2/∂q1∂q2. They do not
appear because the coordinate system is orthogonal.
Curl—In the same spirit as our treatment of the divergence, we calculate the circulation
around an element of area in the q1q2 plane, and therefore associated with a vector in
the q3 direction. Referring to Fig. 3.22, the line integral
H
B · dr consists of four segment
−B1h1dq1|q2+ dq2/2
B2h2dq2|q1+ dq1/2
−B2h2dq2|q1−dq1/2
+B1h1dq1|q2−dq2/2
FIGURE 3.22
Circulation
H
B · dr around curvilinear element of area on
a surface of constant q3.

3.10 Curvilinear Coordinates
187
contributions, which to ﬁrst order are
Segment 1 = (h1B1)

q1,q2−dq2/2,q3
dq1,
Segment 2 = (h2B2)

q1+dq1/2,q2,q3
dq2,
Segment 3 = −(h1B1)

q1,q2+dq2/2,q3
dq1,
Segment 4 = −(h2B2)

q1−dq1/2,q2,q3
dq2.
Keeping in mind that the hi are functions of position, and that the loop has area
h1h2 dq1dq2, these contributions combine into a circulation per unit area
(∇× B)3 =
1
h1h2

−∂
∂q2
(h1B1) + ∂
∂q1
(h2B2)

.
The generalization of this result to arbitrary orientation of the circulation loop can be
brought to the determinantal form
∇× B =
1
h1h2h3

ˆe1h1
ˆe2h2
ˆe3h3
∂
∂q1
∂
∂q2
∂
∂q3
h1B1
h2B2
h3B3

.
(3.143)
Just as for Cartesian coordinates, this determinant is to be evaluated from the top down, so
that the derivatives will act on its bottom row.
Circular Cylindrical Coordinates
Although there are at least 11 coordinate systems that are appropriate for use in solving
physics problems, the evolution of computers and efﬁcient programming techniques have
greatly reduced the need for most of these coordinate systems, with the result that the dis-
cussion in this book is limited to (1) Cartesian coordinates, (2) spherical polar coordinates
(treated in the next subsection), and (3) circular cylindrical coordinates, which we discuss
here. Speciﬁcations and details of other coordinate systems will be found in the ﬁrst two
editions of this work and in Additional Readings at the end of this chapter (Morse and
Feshbach, Margenau and Murphy).
In the circular cylindrical coordinate system the three curvilinear coordinates are labeled
(ρ,ϕ, z). We use ρ for the perpendicular distance from the z-axis because we reserve r for
the distance from the origin. The ranges of ρ, ϕ, and z are
0 ≤ρ < ∞,
0 ≤ϕ < 2π,
−∞< z < ∞.
For ρ = 0, ϕ is not well deﬁned. The coordinate surfaces, shown in Fig. 3.23, follow:
1.
Right circular cylinders having the z-axis as a common axis,
ρ =

x2 + y21/2
= constant.

188
Chapter 3 Vector Analysis
ρ
ϕ
y
x
z
FIGURE 3.23
Cylindrical coordinates ρ, ϕ, z.
2.
Half-planes through the z-axis, at an angle ϕ measured from the x direction,
ϕ = tan−1  y
x

= constant.
The arctangent is double valued on the range of ϕ, and the correct value of ϕ must be
determined by the individual signs of x and y.
3.
Planes parallel to the xy-plane, as in the Cartesian system,
z = constant.
Inverting the preceding equations, we can obtain
x = ρ cosϕ,
y = ρ sinϕ,
z = z.
(3.144)
This is essentially a 2-D curvilinear system with a Cartesian z-axis added on to form
a 3-D system.
The coordinate vector r and a general vector V are expressed as
r = ρ ˆeρ + z ˆez,
V = Vρ ˆeρ + Vϕ ˆeϕ + Vz ˆez.
From Eq. (3.131), the scale factors for these coordinates are
hρ = 1,
hϕ = ρ,
hz = 1,
(3.145)

3.10 Curvilinear Coordinates
189
so the elements of displacement, area, and volume are
dr = ˆeρ dρ + ρ ˆeϕ dϕ + ˆez dz,
dσ = ρ ˆeρ dϕ dz + ˆeϕdρ dz + ρ ˆez dρ dϕ,
(3.146)
dτ = ρ dρ dϕ dz.
It is perhaps worth emphasizing that the unit vectors ˆeρ and ˆeϕ have directions that vary
with ϕ; if expressions containing these unit vectors are differentiated with respect to ϕ, the
derivatives of these unit vectors must be included in the computations.
Example 3.10.1
KEPLER’S AREA LAW FOR PLANETARY MOTION
One of Kepler’s laws states that the radius vector of a planet, relative to an origin at the
sun, sweeps out equal areas in equal time. It is instructive to derive this relationship using
cylindrical coordinates. For simplicity we consider a planet of unit mass and motion in the
plane z = 0.
The gravitational force F is of the form f (r) ˆer, and hence the torque about the origin,
r × F, vanishes, so angular momentum L = r × dr/dt is conserved. To evaluate dr/dt,
we start from dr as given in Eq. (3.146), writing
dr
dt = ˆeρ ˙ρ + ˆeϕρ ˙ϕ,
where we have used the dot notation (invented by Newton) to indicate time derivatives.
We now form
L = ρ ˆeρ ×
 ˆeρ ˙ρ + ˆeϕρ ˙ϕ

= ρ2 ˙ϕˆez.
We conclude that ρ2 ˙ϕ is constant. Making the identiﬁcation ρ2 ˙ϕ = 2d A/dt, where A is
the area swept out, we conﬁrm Kepler’s law.
■
Continuing now to the vector differential operators, using Eqs. (3.138), (3.141), (3.142),
and (3.143), we have
∇ψ(ρ,ϕ, z) = ˆeρ
∂ψ
∂bρ + ˆeϕ
1
ρ
∂ψ
∂ϕ + ˆez
∂ψ
∂z ,
(3.147)
∇· V = 1
ρ
∂
∂ρ (ρVρ) + 1
ρ
∂Vϕ
∂ϕ + ∂Vz
∂z ,
(3.148)
∇2ψ = 1
ρ
∂
∂ρ

ρ ∂ψ
∂ρ

+ 1
ρ2
∂2ψ
∂ϕ2 + ∂2ψ
∂z2 ,
(3.149)
∇× V = 1
ρ

ˆeρ ρˆeϕ ˆez
∂
∂ρ
∂
∂ϕ
∂
∂z
Vρ ρVϕ Vz

.
(3.150)

190
Chapter 3 Vector Analysis
Finally, for problems such as circular wave guides and cylindrical cavity resonators, one
needs the vector Laplacian ∇2V. From Eq. (3.70), its components in cylindrical coordi-
nates can be shown to be
∇2V

ρ = ∇2Vρ −1
ρ2 Vρ −2
ρ2
∂Vϕ
∂ϕ ,
∇2V

ϕ = ∇2Vϕ −1
ρ2 Vϕ + 2
ρ2
∂Vρ
∂ϕ ,
(3.151)
∇2V

z = ∇2Vz.
Example 3.10.2
A NAVIER-STOKES TERM
The Navier-Stokes equations of hydrodynamics contain a nonlinear term
∇×

v × (∇× v)

,
where v is the ﬂuid velocity. For ﬂuid ﬂowing through a cylindrical pipe in the z direction,
v = ˆezv(ρ).
From Eq. (3.150),
∇× v = 1
ρ

ˆeρ
ρˆeϕ
ˆez
∂
∂ρ
∂
∂ϕ
∂
∂z
0
0
v(ρ)

= −ˆeϕ
∂v
∂ρ ,
v × (∇× v) =

ˆeρ
ˆeϕ
ˆez
0
0
v
0
−∂v
∂ρ
0

= ˆeρ v(ρ) ∂v
∂ρ .
Finally,
∇×
 v × (∇× v)

= 1
ρ

ˆeρ
ρˆeϕ
ˆez
∂
∂ρ
∂
∂ϕ
∂
∂z
v ∂v
∂ρ
0
0

= 0.
For this particular case, the nonlinear term vanishes.
■
Spherical Polar Coordinates
Spherical polar coordinates were introduced as an initial example of a curvilinear coordi-
nate system, and were illustrated in Fig. 3.19. We reiterate: The coordinates are labeled
(r,θ,ϕ). Their ranges are
0 ≤r < ∞,
0 ≤θ ≤π,
0 ≤ϕ < 2π.

3.10 Curvilinear Coordinates
191
For r = 0, neither θ nor ϕ is well deﬁned. Additionally, ϕ is ill-deﬁned for θ = 0 and
θ = π. The coordinate surfaces follow:
1.
Concentric spheres centered at the origin,
r =

x2 + y2 + z21/2
= constant.
2.
Right circular cones centered on the z (polar) axis with vertices at the origin,
θ = arccos z
r = constant.
3.
Half-planes through the z (polar) axis, at an angle ϕ measured from the x direction,
ϕ = arctan y
x = constant.
The arctangent is double valued on the range of ϕ, and the correct value of ϕ must be
determined by the individual signs of x and y.
Inverting the preceding equations, we can obtain
x = r sinθ cosϕ,
y = r sinθ sinϕ,
z = r cosθ.
(3.152)
The coordinate vector r and a general vector V are expressed as
r = r ˆer,
V = Vr ˆer + Vθ ˆeθ + Vϕ ˆeϕ.
From Eq. (3.131), the scale factors for these coordinates are
hr = 1,
hθ = r,
hϕ = r sinθ,
(3.153)
so the elements of displacement, area, and volume are
dr = ˆer dr + r ˆeθ dθ + r sinθ ˆeϕ dϕ,
dσ = r2 sinθ ˆer dθ dϕ + r sinθ ˆeθ dr dϕ + r ˆeϕ dr dθ,
(3.154)
dτ = r2 sinθ dρ dθ dϕ.
Frequently one encounters a need to perform a surface integration over the angles, in which
case the angular dependence of dσ reduces to
d = sinθ dθ dϕ,
(3.155)
where d is called an element of solid angle, and has the property that its integral over all
angles has the value
Z
d = 4π.
Note that for spherical polar coordinates, all three of the unit vectors have directions that
depend on position, and this fact must be taken into account when expressions containing
the unit vectors are differentiated.

192
Chapter 3 Vector Analysis
The vector differential operators may now be evaluated, using Eqs. (3.138), (3.141),
(3.142), and (3.143):
∇ψ(r,θ,ϕ) = ˆer
∂ψ
∂r + ˆeθ
1
r
∂ψ
∂θ + ˆeϕ
1
r sinθ
∂ψ
∂ϕ ,
(3.156)
∇· V =
1
r2 sinθ

sinθ ∂
∂r (r2Vr) + r ∂
∂θ (sinθ Vθ) + r ∂Vϕ
∂ϕ

,
(3.157)
∇2ψ =
1
r2 sinθ

sinθ ∂
∂r

r2 ∂ψ
∂r

+ ∂
∂θ

sinθ ∂ψ
∂θ

+
1
sinθ
∂2ψ
∂ϕ2

,
(3.158)
∇× V =
1
r2 sinθ

ˆer
r ˆeθ
r sinθ ˆeϕ
∂
∂r
∂
∂θ
∂
∂ϕ
Vr
rVθ
r sinθVϕ

.
(3.159)
Finally, again using Eq. (3.70), the components of the vector Laplacian ∇2V in spherical
polar coordinates can be shown to be
∇2V

r = ∇2Vr −2
r2 Vr −2
r2 cotθVθ −2
r2
∂Vθ
∂θ −
2
r2 sinθ
∂Vϕ
∂ϕ ,
∇2V

θ = ∇2Vθ −
1
r2 sin2 θ
Vθ + 2
r2
∂Vr
∂θ −2cosθ
r2 sin2 θ
∂Vϕ
∂ϕ ,
(3.160)
∇2V

ϕ = ∇2Vϕ −
1
r2 sin2 θ
Vϕ +
2
r2 sinθ
∂Vr
∂ϕ + 2cosθ
r2 sin2 θ
∂Vϕ
∂ϕ .
Example 3.10.3
∇, ∇·, ∇× FOR A CENTRAL FORCE
We can now easily derive some of the results previously obtained more laboriously in
Cartesian coordinates:
From Eq. (3.156),
∇f (r) = ˆer
d f
dr ,
∇rn = ˆernrn−1.
(3.161)
Specializing to the Coulomb potential of a point charge at the origin, V = Ze/(4πε0r), so
the electric ﬁeld has the expected value E = −∇V = (Ze/4πε0r2)ˆer.
Taking next the divergence of a radial function, we have from Eq. (3.157),
∇·
 ˆer f (r)

= 2
r f (r) + d f
dr ,
∇· (ˆer rn) = (n + 2)rn−1.
(3.162)
Specializing the above to the Coulomb force (n = −2), we have (except for r = 0)
∇· r−2 = 0, which is consistent with Gauss’ law.
Continuing now to the Laplacian, from Eq. (3.158) we have
∇2 f (r) = 2
r
d f
dr + d2 f
dr2 ,
∇2rn = n(n + 1)rn−2,
(3.163)
in contrast to the ordinary second derivative of rn involving n −1.

3.10 Curvilinear Coordinates
193
Finally, from Eq. (3.159),
∇×
 ˆer f (r)

= 0,
(3.164)
which conﬁrms that central forces are irrotational.
■
Example 3.10.4
MAGNETIC VECTOR POTENTIAL
A single current loop in the xy-plane has a vector potential A that is a function only of r
and θ, is entirely in the ˆeϕ direction and is related to the current density J by the equation
µ0J = ∇× B = ∇×

∇× ˆeϕ Aϕ(r,θ)

.
In spherical polar coordinates this reduces to
µ0J = ∇×
1
r2 sinθ

ˆer
r ˆeθ
r sinθ ˆeϕ
∂
∂r
∂
∂θ
∂
∂ϕ
0
0
r sinθ Aϕ

= ∇×
1
r2 sinθ

ˆer
∂
∂θ (r sinθ Aϕ) −r ˆeθ
∂
∂r (r sinθ Aϕ)

.
Taking the curl a second time, we obtain
µ0J =
1
r2 sinθ

ˆer
r ˆeθ
r sinθ ˆeϕ
∂
∂r
∂
∂θ
∂
∂ϕ
1
r sinθ
∂
∂θ (sinθ Aϕ)
−1
r
∂
∂r (r Aϕ)
0

.
Expanding this determinant from the top down, we reach
µ0J = −ˆeϕ
∂2Aϕ
∂r2 + 2
r
∂Aϕ
∂r
+
1
r2 sinθ
∂
∂θ

sinθ ∂Aϕ
∂θ

−
1
r2 sin2 θ
Aϕ

.
(3.165)
Note that we get, in addition to ∇2Aϕ, one more term: −Aϕ/r2 sin2 θ.
■
Example 3.10.5
STOKES’ THEOREM
As a ﬁnal example, let’s compute
H
B · dr for a closed loop, comparing the result with
integrals
R
(∇× B) · dσ for two different surfaces having the same perimeter. We use
spherical polar coordinates, taking B = e−r ˆeϕ.
The loop will be a unit circle about the origin in the xy-plane; the line integral about
it will be taken in a counterclockwise sense as viewed from positive z, so the normal to
the surfaces it bounds will pass through the xy-plane in the direction of positive z. The
surfaces we consider are (1) a circular disk bounded by the loop, and (3) a hemisphere
bounded by the loop, with its surface in the region z < 0. See Fig. 3.24.

194
Chapter 3 Vector Analysis
n
n
n
n
FIGURE 3.24
Surfaces for Example 3.10.5: (left) S1, disk; (right) S2, hemisphere.
For the line integral, dr = r sinθ ˆeϕ dϕ, which reduces to dr = ˆeϕ dϕ since θ = π/2 and
r = 1 on the entire loop. We then have
I
B · dr =
2π
Z
ϕ=0
e−1 ˆeϕ · ˆeϕ dϕ = 2π
e .
For the surface integrals, we need ∇× B:
∇× B =
1
r2 sinθ
 ∂
∂θ (r sinθ e−r)ˆer −r ∂
∂r (r sinθ e−r)ˆeθ

= e−r cosθ
r sinθ
ˆer −(1 −r)e−r ˆeθ.
Taking ﬁrst the disk, at all points of which θ = π/2, with integration range 0 ≤r ≤1,
and 0 ≤ϕ < 2π, we note that dσ = −ˆeθ r sinθ dr dϕ = −ˆeθ r dr dϕ. The minus sign arises
because the positive normal is in the direction of decreasing θ. Then,
Z
S1
−(∇× B) · ˆeθ r dr dϕ =
2π
Z
0
dϕ
1
Z
0
dr (1 −r)e−r = 2π
e .
For the hemisphere, deﬁned by r = 1, π/2 ≤θ < π, and 0 ≤ϕ < 2π, we have dσ =
−ˆer r2 sinθ dθ dϕ = −ˆer sinθ dθ dϕ (the normal is in the direction of decreasing r), and
Z
S2
−(∇× B) · ˆer sinθ dθ dϕ = −
π
Z
π/2
dθe−1 cosθ
2π
Z
0
dϕ = 2π
e .
The results for both surfaces agree with that from the line integral of their common
perimeter. Because ∇× B is solenoidal, all the ﬂux that passes through the disk in the
xy-plane must continue through the hemispherical surface, and for that matter, through
any surface with the same perimeter. That is why Stokes’ theorem is indifferent to features
of the surface other than its perimeter.
■

3.10 Curvilinear Coordinates
195
Rotation and Reﬂection in Spherical Coordinates
It is infrequent that rotational coordinate transformations need be applied in curvilinear
coordinate systems, and they usually arise only in contexts that are compatible with the
symmetry of the coordinate system. We limit the current discussion to rotations (and
reﬂections) in spherical polar coordinates.
Rotation—Suppose a coordinate rotation identiﬁed by Euler angles (α,β,γ ) converts the
coordinates of a point from (r,θ,ϕ) to (r,θ′,ϕ′). It is obvious that r retains its original
value. Two questions arise: (1) How are θ′ and ϕ′ related to θ and ϕ? and (2) How do the
components of a vector A, namely (Ar, Aθ, Aϕ), transform?
It is simplest to proceed, as we did for Cartesian coordinates, by analyzing the three
consecutive rotations implied by the Euler angles. The ﬁrst rotation, by an angle α about
the z-axis, leaves θ unchanged, and converts ϕ into ϕ −α. However, it causes no change
in any of the components of A.
The second rotation, which inclines the polar direction by an angle β toward the (new)
x-axis, does change the values of both θ and ϕ and, in addition, changes the directions
of ˆeθ and ˆeϕ. Referring to Fig. 3.25, we see that these two unit vectors are subjected to
a rotation χ in the plane tangent to the sphere of constant r, thereby yielding new unit
vectors ˆe′
θ and ˆe′
ϕ such that
ˆeθ = cosχ ˆe′
θ −sinχ ˆe′
ϕ,
ˆeϕ = sinχ ˆe′
θ + cosχ ˆe′
ϕ.
This transformation corresponds to
S2 =
 cosχ
sinχ
−sinχ
cosχ

.
Carrying out the spherical trigonometry corresponding to Fig. 3.25, we have the new
coordinates
cosθ′ = cosβ cosθ + sinβ sinθ cos(ϕ −α),
cosϕ′ = cosβ cosθ′ −cosθ
sinβ sinθ′
,
(3.166)
P
x
x
z
z ′
ϕ −α
θ
θ ′
ϕ ′
β
π −ϕ ′
ˆeϕ ′
ˆeϕ
ˆeθ′
eθ
FIGURE 3.25
Rotation and unit vectors in spherical polar coordinates, shown on a sphere
of radius r. The original polar direction is marked z; it is moved to the direction z′, at an
inclination given by the Euler angle β. The unit vectors ˆeθ and ˆeϕ at the point P are
thereby rotated through the angle χ.

196
Chapter 3 Vector Analysis
and
cosχ = cosβ −cosθ cosθ′
sinθ sinθ′
.
(3.167)
The third rotation, by an angle γ about the new z-axis, leaves the components of A
unchanged but requires the replacement of ϕ′ by ϕ′ −γ .
Summarizing,


A′
r
A′
θ
A′
ϕ

=


1
0
0
0
cosχ
sinχ
0
−sinχ
cosχ




Ar
Aθ
Aϕ

.
(3.168)
This equation speciﬁes the components of A in the rotated coordinates at the point
(r,θ′,ϕ′ −γ ) in terms of the original components at the same physical point, (r,θ,ϕ).
Reﬂection—Inversion of the coordinate system reverses the sign of each Cartesian coor-
dinate. Taking the angle ϕ as that which moves the new +x coordinate toward the new +y
coordinate, the system (which was originally right-handed) now becomes left-handed. The
coordinates (r,θ,ϕ) of a (ﬁxed) point become, in the new system, (r,π −θ,π + ϕ). The
unit vectors ˆer and ˆeϕ are invariant under inversion, but ˆeθ changes sign, so


A′
r
A′
θ
A′
ϕ

=


Ar
−Aθ
Aϕ

,
coordinate inversion.
(3.169)
Exercises
3.10.1
The u-, v-, z-coordinate system frequently used in electrostatics and in hydrodynamics
is deﬁned by
xy = u,
x2 −y2 = v,
z = z.
This u-, v-, z-system is orthogonal.
(a)
In words, describe brieﬂy the nature of each of the three families of coordinate
surfaces.
(b)
Sketch the system in the xy-plane showing the intersections of surfaces of constant
u and surfaces of constant v with the xy-plane.
(c)
Indicate the directions of the unit vectors ˆeu and ˆev in all four quadrants.
(d)
Finally, is this u-, v-, z-system right-handed (ˆeu × ˆev = +ˆez) or left-handed (ˆeu ×
ˆev = −ˆez)?
3.10.2
The elliptic cylindrical coordinate system consists of three families of surfaces:
(1)
x2
a2 cosh2 u
+
y2
a2 sinh2 u
= 1;
(2)
x2
a2 cos2 v −
y2
a2 sin2 v
= 1;
(3)
z = z.
Sketch the coordinate surfaces u = constant and v = constant as they intersect the ﬁrst
quadrant of the xy-plane. Show the unit vectors ˆeu and ˆev. The range of u is 0 ≤u < ∞.
The range of v is 0 ≤v ≤2π.

3.10 Curvilinear Coordinates
197
3.10.3
Develop arguments to show that dot and cross products (not involving ∇) in orthogonal
curvilinear coordinates in R3 proceed, as in Cartesian coordinates, with no involvement
of scale factors.
3.10.4
With ˆe1 a unit vector in the direction of increasing q1, show that
(a)
∇· ˆe1 =
1
h1h2h3
∂(h2h3)
∂q1
(b)
∇× ˆe1 = 1
h1

ˆe2
1
h3
∂h1
∂q3
−ˆe3
1
h2
∂h1
∂q2

.
Note that even though ˆe1 is a unit vector, its divergence and curl do not necessarily
vanish.
3.10.5
Show that a set of orthogonal unit vectors ˆei may be deﬁned by
ˆei = 1
hi
∂r
∂qi
.
In particular, show that ˆei · ˆei = 1 leads to an expression for hi in agreement with
Eq. (3.131).
The above equation for ˆei may be taken as a starting point for deriving
∂ˆei
∂q j
= ˆe j
1
hi
∂h j
∂qi
,
i ̸= j
and
∂ˆei
∂qi
= −
X
j̸=i
ˆe j
1
h j
∂hi
∂q j
.
3.10.6
Resolve the circular cylindrical unit vectors into their Cartesian components (see
Fig. 3.23).
ANS.
ˆeρ = ˆex cosϕ + ˆey sinϕ,
ˆeϕ = −ˆex sinϕ + ˆey cosϕ,
ˆez = ˆez.
3.10.7
Resolve the Cartesian unit vectors into their circular cylindrical components (see
Fig. 3.23).
ANS.
ˆex = ˆeρ cosϕ −ˆeϕ sinϕ,
ˆey = ˆeρ sinϕ + ˆeϕ cosϕ,
ˆez = ˆez.
3.10.8
From the results of Exercise 3.10.6, show that
∂ˆeρ
∂ϕ = ˆeϕ,
∂ˆeϕ
∂ϕ = −ˆeρ
and that all other ﬁrst derivatives of the circular cylindrical unit vectors with respect to
the circular cylindrical coordinates vanish.

198
Chapter 3 Vector Analysis
3.10.9
Compare ∇· V as given for cylindrical coordinates in Eq. (3.148) with the result of its
computation by applying to V the operator
∇= ˆeρ
∂
∂ρ + ˆeϕ
1
ρ
∂
∂ϕ + ˆez
∂
∂z
Note that ∇acts both on the unit vectors and on the components of V.
3.10.10
(a)
Show that r = ˆeρρ + ˆezz.
(b)
Working entirely in circular cylindrical coordinates, show that
∇· r = 3
and
∇× r = 0.
3.10.11
(a)
Show that the parity operation (reﬂection through the origin) on a point (ρ,ϕ, z)
relative to ﬁxed x-, y-, z-axes consists of the transformation
ρ →ρ,
ϕ →ϕ ± π,
z →−z.
(b)
Show that ˆeρ and ˆeϕ have odd parity (reversal of direction) and that ˆez has even
parity.
Note. The Cartesian unit vectors ˆex, ˆey, and ˆez remain constant.
3.10.12
A rigid body is rotating about a ﬁxed axis with a constant angular velocity ω. Take ω
to lie along the z-axis. Express the position vector r in circular cylindrical coordinates
and using circular cylindrical coordinates,
(a)
calculate v = ω × r,
(b)
calculate ∇× v.
ANS.
(a)
v = ˆeϕωρ
(b)
∇× v = 2ω.
3.10.13
Find the circular cylindrical components of the velocity and acceleration of a moving
particle,
vρ = ˙ρ,
aρ = ¨ρ −ρ ˙ϕ2,
vϕ = ρ ˙ϕ,
aϕ = ρ ¨ϕ + 2 ˙ρ ˙ϕ,
vz = ˙z,
az = ¨z.
Hint.
r(t) = ˆeρ(t)ρ(t) + ˆezz(t)
= [ˆex cosϕ(t) + ˆey sinϕ(t)]ρ(t) + ˆezz(t).
Note. ˙ρ = dρ/dt, ¨ρ = d2ρ/dt2, and so on.
3.10.14
In right circular cylindrical coordinates, a particular vector function is given by
V(ρ,ϕ) = ˆeρVρ(ρ,ϕ) + ˆeϕVϕ(ρ,ϕ).
Show that ∇× V has only a z-component. Note that this result will hold for any vector
conﬁned to a surface q3 = constant as long as the products h1V1 and h2V2 are each
independent of q3.

3.10 Curvilinear Coordinates
199
3.10.15
A conducting wire along the z-axis carries a current I. The resulting magnetic vector
potential is given by
A = ˆez
µI
2π ln
 1
ρ

.
Show that the magnetic induction B is given by
B = ˆeϕ
µI
2πρ .
3.10.16
A force is described by
F = −ˆex
y
x2 + y2 + ˆey
x
x2 + y2 .
(a)
Express F in circular cylindrical coordinates.
Operating entirely in circular cylindrical coordinates for (b) and (c),
(b)
Calculate the curl of F and
(c)
Calculate the work done by F in encircling the unit circle once counter-clockwise.
(d)
How do you reconcile the results of (b) and (c)?
3.10.17
A calculation of the magnetohydrodynamic pinch effect involves the evaluation of
(B · ∇)B. If the magnetic induction B is taken to be B = ˆeϕ Bϕ(ρ), show that
(B · ∇)B = −ˆeρ B2
ϕ/ρ.
3.10.18
Express the spherical polar unit vectors in terms of Cartesian unit vectors.
ANS.
ˆer = ˆex sinθ cosϕ + ˆey sinθ sinϕ + ˆez cosθ,
ˆeθ = ˆex cosθ cosϕ + ˆey cosθ sinϕ −ˆez sinθ,
ˆeϕ = −ˆex sinϕ + ˆey cosϕ.
3.10.19
Resolve the Cartesian unit vectors into their spherical polar components:
ˆex = ˆer sinθ cosϕ + ˆeθ cosθ cosϕ −ˆeϕ sinϕ,
ˆey = ˆer sinθ sinϕ + ˆeθ cosθ sinϕ + ˆeϕ cosϕ,
ˆez = ˆer cosθ −ˆeθ sinθ.
3.10.20
(a)
Explain why it is not possible to relate a column vector r (with components x,
y, z) to another column vector r′ (with components r, θ, ϕ), via a matrix equation
of the form r′ = Br.
(b)
One can write a matrix equation relating the Cartesian components of a vector to
its components in spherical polar coordinates. Find the transformation matrix and
determine whether it is orthogonal.
3.10.21
Find the transformation matrix that converts the components of a vector in spherical
polar coordinates into its components in circular cylindrical coordinates. Then ﬁnd the
matrix of the inverse transformation.
3.10.22
(a)
From the results of Exercise 3.10.18, calculate the partial derivatives of ˆer, ˆeθ, and
ˆeϕ with respect to r, θ, and ϕ.

200
Chapter 3 Vector Analysis
(b)
With ∇given by
ˆer
∂
∂r + ˆeθ
1
r
∂
∂θ + ˆeϕ
1
r sinθ
∂
∂ϕ
(greatest space rate of change), use the results of part (a) to calculate ∇· ∇ψ. This
is an alternate derivation of the Laplacian.
Note. The derivatives of the left-hand ∇operate on the unit vectors of the right-hand ∇
before the dot product is evaluated.
3.10.23
A rigid body is rotating about a ﬁxed axis with a constant angular velocity ω. Take ω to
be along the z-axis. Using spherical polar coordinates,
(a)
calculate v = ω × r.
(b)
calculate ∇× v.
ANS.
(a)
v = ˆeϕωr sinθ.
(b)
∇× v = 2ω.
3.10.24
A certain vector V has no radial component. Its curl has no tangential components.
What does this imply about the radial dependence of the tangential components of V?
3.10.25
Modern physics lays great stress on the property of parity (whether a quantity remains
invariant or changes sign under an inversion of the coordinate system). In Cartesian
coordinates this means x →−x, y →−y, and z →−z.
(a)
Show that the inversion (reﬂection through the origin) of a point (r,θ,ϕ) relative
to ﬁxed x-, y-, z-axes consists of the transformation
r →r,
θ →π −θ,
ϕ →ϕ ± π.
(b)
Show that ˆer and ˆeϕ have odd parity (reversal of direction) and that ˆeθ has even
parity.
3.10.26
With A any vector,
A · ∇r = A.
(a)
Verify this result in Cartesian coordinates.
(b)
Verify this result using spherical polar coordinates. Equation (3.156) provides ∇.
3.10.27
Find the spherical coordinate components of the velocity and acceleration of a moving
particle:
vr = ˙r,
ar = ¨r −r ˙θ2 −r sin2 θ ˙ϕ2,
vθ = r ˙θ,
aθ = r ¨θ + 2˙r ˙θ −r sinθ cosθ ˙ϕ2,
vϕ = r sinθ ˙ϕ,
aϕ = r sinθ ¨ϕ + 2˙r sinθ ˙ϕ + 2r cosθ ˙θ ˙ϕ.

3.10 Curvilinear Coordinates
201
Hint.
r(t) = ˆer(t)r(t)
= [ˆex sinθ(t)cosϕ(t) + ˆey sinθ(t)sinϕ(t) + ˆez cosθ(t)]r(t).
Note. The dot in ˙r, ˙θ, ˙ϕ means time derivative: ˙r = dr/dt, ˙θ = dθ/dt,
˙ϕ = dϕ/dt.
3.10.28
Express ∂/∂x, ∂/∂y, ∂/∂z in spherical polar coordinates.
ANS.
∂
∂x = sinθ cosϕ ∂
∂r + cosθ cosϕ 1
r
∂
∂θ −sinϕ
r sinθ
∂
∂ϕ ,
∂
∂y = sinθ sinϕ ∂
∂r + cosθ sinϕ 1
r
∂
∂θ + cosϕ
r sinθ
∂
∂ϕ ,
∂
∂z = cosθ ∂
∂r −sinθ 1
r
∂
∂θ .
Hint. Equate ∇xyz and ∇rθϕ.
3.10.29
Using results from Exercise 3.10.28, show that
−i

x ∂
∂y −y ∂
∂x

= −i ∂
∂ϕ .
This is the quantum mechanical operator corresponding to the z-component of orbital
angular momentum.
3.10.30
With the quantum mechanical orbital angular momentum operator deﬁned as L =
−i(r × ∇), show that
(a)
Lx + i L y = eiϕ
 ∂
∂θ + i cotθ ∂
∂ϕ

,
(b)
Lx −i L y = −e−iϕ
 ∂
∂θ −i cotθ ∂
∂ϕ

.
3.10.31
Verify that L × L = iL in spherical polar coordinates. L = −i(r × ∇), the quantum
mechanical orbital angular momentum operator.
Written in component form, this relation is
L yLz −LzL y = i Lx,
LzLx −Lx Lz = −L y,
Lx L y −L yLx = i Lz.
Using the commutator notation, [A, B] = AB −B A, and the deﬁnition of the Levi-
Civita symbol εi jk, the above can also be written
[Li, L j] = i εi jk Lk,
where i, j, k are x, y, z in any order.
Hint. Use spherical polar coordinates for L but Cartesian components for the cross
product.

202
Chapter 3 Vector Analysis
3.10.32
(a)
Using Eq. (3.156) show that
L = −i(r × ∇) = i

ˆeθ
1
sinθ
∂
∂ϕ −ˆeϕ
∂
∂θ

.
(b)
Resolving ˆeθ and ˆeϕ into Cartesian components, determine Lx, L y, and Lz in
terms of θ, ϕ, and their derivatives.
(c)
From L2 = L2
x + L2
y + L2
z show that
L2 = −1
sinθ
∂
∂θ

sinθ ∂
∂θ

−
1
sin2 θ
∂2
∂ϕ2
= −r2∇2 + ∂
∂r

r2 ∂
∂r

.
3.10.33
With L = −ir × ∇, verify the operator identities
(a)
∇= ˆer
∂
∂r −i r × L
r2
,
(b)
r∇2 −∇

1 + r ∂
∂r

= i∇× L.
3.10.34
Show that the following three forms (spherical coordinates) of ∇2ψ(r) are equivalent:
(a)
1
r2
d
dr

r2 dψ(r)
dr

;
(b)
1
r
d2
dr2 [rψ(r)];
(c)
d2ψ(r)
dr2
+ 2
r
dψ(r)
dr
.
The second form is particularly convenient in establishing a correspondence between
spherical polar and Cartesian descriptions of a problem.
3.10.35
A certain force ﬁeld is given in spherical polar coordinates by
F = ˆer
2P cosθ
r3
+ ˆeθ
P
r3 sinθ,
r ≥P/2.
(a)
Examine ∇× F to see if a potential exists.
(b)
Calculate
H
F · dr for a unit circle in the plane θ = π/2. What does this indicate
about the force being conservative or nonconservative?
(c)
If you believe that F may be described by F = −∇ψ, ﬁnd ψ. Otherwise simply
state that no acceptable potential exists.
3.10.36
(a)
Show that A = −ˆeϕ cotθ/r is a solution of ∇× A = ˆer/r2.
(b)
Show that this spherical polar coordinate solution agrees with the solution given
for Exercise 3.9.5:
A = ˆex
yz
r(x2 + y2) −ˆey
xz
r(x2 + y2).
Note that the solution diverges for θ = 0, π corresponding to x, y = 0.
(c)
Finally, show that A = −ˆeθϕ sinθ/r is a solution. Note that although this solution
does not diverge (r ̸= 0), it is no longer single-valued for all possible azimuth
angles.

Additional Readings
203
3.10.37
An electric dipole of moment p is located at the origin. The dipole creates an electric
potential at r given by
ψ(r) =
p · r
4πε0r3 .
Find the electric ﬁeld, E = −∇ψ at r.
Additional Readings
Borisenko, A. I., and I. E. Tarpov, Vector and Tensor Analysis with Applications. Englewood Cliffs, NJ: Prentice-
Hall (1968), reprinting, Dover (1980).
Davis, H. F., and A. D. Snider, Introduction to Vector Analysis, 7th ed. Boston: Allyn & Bacon (1995).
Kellogg, O. D., Foundations of Potential Theory. Berlin: Springer (1929), reprinted, Dover (1953). The classic
text on potential theory.
Lewis, P. E., and J. P. Ward, Vector Analysis for Engineers and Scientists. Reading, MA: Addison-Wesley (1989).
Margenau, H., and G. M. Murphy, The Mathematics of Physics and Chemistry, 2nd ed. Princeton NJ: Van Nos-
trand (1956). Chapter 5 covers curvilinear coordinates and 13 speciﬁc coordinate systems.
Marion, J. B., Principles of Vector Analysis. New York: Academic Press (1965). A moderately advanced presen-
tation of vector analysis oriented toward tensor analysis. Rotations and other transformations are described
with the appropriate matrices.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Chapter 5
includes a description of several different coordinate systems. Note that Morse and Feshbach are not above
using left-handed coordinate systems even for Cartesian coordinates. Elsewhere in this excellent (and difﬁ-
cult) book there are many examples of the use of the various coordinate systems in solving physical problems.
Eleven additional fascinating but seldom-encountered orthogonal coordinate systems are discussed in the sec-
ond (1970) edition of Mathematical Methods for Physicists.
Spiegel, M. R., Vector Analysis. New York: McGraw-Hill (1989).
Tai, C.-T., Generalized Vector and Dyadic Analysis. Oxford: Oxford University Press (1966).
Wrede, R. C., Introduction to Vector and Tensor Analysis. New York: Wiley (1963), reprinting, Dover (1972).
Fine historical introduction. Excellent discussion of differentiation of vectors and applications to mechanics.

CHAPTER 4
TENSORS AND
DIFFERENTIAL FORMS
4.1
TENSOR ANALYSIS
Introduction, Properties
Tensors are important in many areas of physics, ranging from topics such as general relativ-
ity and electrodynamics to descriptions of the properties of bulk matter such as stress (the
pattern of force applied to a sample) and strain (its response to the force), or the moment
of inertia (the relation between a torsional force applied to an object and its resultant angu-
lar acceleration). Tensors constitute a generalization of quantities previously introduced:
scalars and vectors. We identiﬁed a scalar as an quantity that remained invariant under
rotations of the coordinate system and which could be speciﬁed by the value of a sin-
gle real number. Vectors were identiﬁed as quantities that had a number of real compo-
nents equal to the dimension of the coordinate system, with the components transforming
like the coordinates of a ﬁxed point when a coordinate system is rotated. Calling scalars
tensors of rank 0 and vectors tensors of rank 1, we identify a tensor of rank n in a
d-dimensional space as an object with the following properties:
•
It has components labeled by n indices, with each index assigned values from 1 through
d, and therefore having a total of dn components;
•
The components transform in a speciﬁed manner under coordinate transformations.
The behavior under coordinate transformation is of central importance for tensor anal-
ysis and conforms both with the way in which mathematicians deﬁne linear spaces and
205
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

206
Chapter 4 Tensors and Differential Forms
with the physicist’s notion that physical observables must not depend on the choice of
coordinate frames.
Covariant and Contravariant Tensors
In Chapter 3, we considered the rotational transformation of a vector A = A1ˆe1 + A2ˆe2 +
A3ˆe3 from the Cartesian system deﬁned by ˆei (i = 1,2,3) into a rotated coordinate system
deﬁned by ˆe′
i, with the same vector A then represented as A′ = A′
1ˆe′
1 + A′
2ˆe′
2 + A′
3e′
3. The
components of A and A′ are related by
A′
i =
X
j
(ˆe′
i · ˆe j)A j,
(4.1)
where the coefﬁcients (ˆe′
i · ˆe j) are the projections of ˆe′
i in the ˆe j directions. Because the ˆei
and the ˆe j are linearly related, we can also write
A′
i =
X
j
∂x′
i
∂x j
A j.
(4.2)
The formula of Eq. (4.2) corresponds to the application of the chain rule to convert the set
A j into the set A′
i, and is valid for A j and A′
i of arbitrary magnitude because both vectors
depend linearly on their components.
We have also previously noted that the gradient of a scalar ϕ has in the unrotated Carte-
sian coordinates the components (∇ϕ) j = (∂ϕ/∂x j)ˆe j, meaning that in a rotated system
we would have
(∇ϕ)′
i ≡∂ϕ
∂x′
i
=
X
j
∂x j
∂x′
i
∂ϕ
∂x j
,
(4.3)
showing that the gradient has a transformation law that differs from that of Eq. (4.2) in
that ∂x′
i/∂x j has been replaced by ∂x j/∂x′
i. Remembering that these two expressions, if
written in detail, correspond, respectively, to (∂x′
i/∂x j)xk and (∂x j/∂x′
i)x′
k, where k runs
over the index values other than that already in the denominator, and also noting that (in
Cartesian coordinates) they are two different ways of computing the same quantity (the
magnitude and sign of the projection of one of these unit vectors upon the other), we see
that it was legitimate to identify both A and ∇ϕ as vectors, as we did in Chapter 3.
However, as the alert reader may note from the repeated insertion of the word
“Cartesian,” the partial derivatives in Eqs. (4.2) and (4.3) are only guaranteed to be equal
in Cartesian coordinate systems, and since there is sometimes a need to use non-Cartesian
systems it becomes necessary to distinguish these two different transformation rules. Quan-
tities transforming according to Eq. (4.2) are called contravariant vectors, while those
transforming according to Eq. (4.3) are termed covariant. When non-Cartesian systems
may be in play, it is therefore customary to distinguish these transformation properties by
writing the index of a contravariant vector as a superscript and that of a covariant vector as
a subscript. This means, among other things, that the components of the position vector r,

4.1 Tensor Analysis
207
which is contravariant, must now be written (x1, x2, x3). Thus, summarizing,
(A′)i =
X
j
∂(x′)i
∂x j
A j
A, a contravariant vector,
(4.4)
A′
i =
X
j
∂x j
∂(x′)i A j
A, a covariant vector.
(4.5)
It is useful to note that the occurrence of subscripts and superscripts is systematic; the
free (i.e., unsummed) index i occurs as a superscript on both sides of Eq. (4.4), while it
appears as a subscript on both sides of Eq. (4.5), if we interpret an upper index in the
denominator as equivalent to a lower index. The summed index occurs once as upper
and once as lower (again treating an upper index in the denominator as a lower index).
A frequently used shorthand (the Einstein convention) is to omit the summation sign in
formulas like Eqs. (4.4) and (4.5) and to understand that when the same symbol occurs
both as an upper and a lower index in the same expression, it is to be summed. We will
gradually back into the use of the Einstein convention, giving the reader warnings as we
start to do so.
Tensors of Rank 2
Now we proceed to deﬁne contravariant, mixed, and covariant tensors of rank 2 by the
following equations for their components under coordinate transformations:
(A′)i j =
X
kl
∂(x′)i
∂xk
∂(x′) j
∂xl
Akl,
(B′)i
j =
X
kl
∂(x′)i
∂xk
∂xl
∂(x′) j Bk
l ,
(4.6)
(C′)i j =
X
kl
∂xk
∂(x′)i
∂xl
∂(x′) j Ckl.
Clearly, the rank goes as the number of partial derivatives (or direction cosines) in the
deﬁnition: 0 for a scalar, 1 for a vector, 2 for a second-rank tensor, and so on. Each index
(subscript or superscript) ranges over the number of dimensions of the space. The number
of indices (equal to the rank of tensor) is not limited by the dimensionality of the space. We
see that Akl is contravariant with respect to both indices, Ckl is covariant with respect to
both indices, and Bk
l transforms contravariantly with respect to the index k but covariantly
with respect to the index l. Once again, if we are using Cartesian coordinates, all three
forms of the tensors of second rank, contravariant, mixed, and covariant are the same.
As with the components of a vector, the transformation laws for the components of a
tensor, Eq. (4.6), cause its physically relevant properties to be independent of the choice
of reference frame. This is what makes tensor analysis important in physics. The inde-
pendence relative to reference frame (invariance) is ideal for expressing and investigating
universal physical laws.

208
Chapter 4 Tensors and Differential Forms
The second-rank tensor A (with components Akl) may be conveniently represented by
writing out its components in a square array (3 × 3 if we are in three-dimensional (3-D)
space):
A =


A11 A12 A13
A21 A22 A23
A31 A32 A33

.
(4.7)
This does not mean that any square array of numbers or functions forms a tensor. The
essential condition is that the components transform according to Eq. (4.6).
We can view each of Eq. (4.6) as a matrix equation. For A, it takes the form
(A′)i j =
X
kl
Sik Akl(ST )lj,
or
A′ = SAST,
(4.8)
a construction that is known as a similarity transformation and is discussed in
Section 5.6.
In summary, tensors are systems of components organized by one or more indices that
transform according to speciﬁc rules under a set of transformations. The number of
indices is called the rank of the tensor.
Addition and Subtraction of Tensors
The addition and subtraction of tensors is deﬁned in terms of the individual elements, just
as for vectors. If
A + B = C,
(4.9)
then, taking as an example A, B, and C to be contravariant tensors of rank 2,
Ai j + Bi j = Ci j.
(4.10)
In general, of course, A and B must be tensors of the same rank (of both contra- and
co-variance) and in the same space.
Symmetry
The order in which the indices appear in our description of a tensor is important. In general,
Amn is independent of Anm, but there are some cases of special interest. If, for all m and n,
Amn = Anm,
A is symmetric.
(4.11)
If, on the other hand,
Amn = −Anm,
A is antisymmetric.
(4.12)
Clearly, every (second-rank) tensor can be resolved into symmetric and antisymmetric
parts by the identity
Amn = 1
2(Amn + Anm) + 1
2(Amn −Anm),
(4.13)
the ﬁrst term on the right being a symmetric tensor, the second, an antisymmetric tensor.

4.1 Tensor Analysis
209
Isotropic Tensors
To illustrate some of the techniques of tensor analysis, let us show that the now-familiar
Kronecker delta, δkl, is really a mixed tensor of rank 2, δk
l .1 The question is: Does δk
l trans-
form according to Eq. (4.6)? This is our criterion for calling it a tensor. If δk
l is the mixed
tensor corresponding to this notation, it must satisfy (using the summation convention,
meaning that the indices k and l are to be summed)
(δ′)i
j = ∂(x′)i
∂xk
∂xl
∂(x′) j δk
l = ∂(x′)i
∂xk
∂xk
∂(x′) j ,
where we have performed the l sum and used the deﬁnition of the Kronecker delta. Next,
∂(x′)i
∂xk
∂xk
∂(x′) j = ∂(x′)i
∂(x′) j ,
where we have identiﬁed the k summation on the left-hand side as an instance of the
chain rule for differentiation. However, (x′)i and (x′) j are independent coordinates, and
therefore the variation of one with respect to the other must be zero if they are different,
unity if they coincide; that is,
∂(x′)i
∂(x′) j = (δ′)i
j.
(4.14)
Hence
(δ′)i
j = ∂(x′)i
∂xk
∂xl
∂(x′) j δk
l ,
(4.15)
showing that the δk
l are indeed the components of a mixed second-rank tensor. Note that
this result is independent of the number of dimensions of our space.
The Kronecker delta has one further interesting property. It has the same components in
all of our rotated coordinate systems and is therefore called isotropic. In Section 4.2 and
Exercise 4.2.4 we shall meet a third-rank isotropic tensor and three fourth-rank isotropic
tensors. No isotropic ﬁrst-rank tensor (vector) exists.
Contraction
When dealing with vectors, we formed a scalar product by summing products of corre-
sponding components:
A · B =
X
i
Ai Bi.
The generalization of this expression in tensor analysis is a process known as contraction.
Two indices, one covariant and the other contravariant, are set equal to each other, and then
(as implied by the summation convention) we sum over this repeated index. For example,
1It is common practice to refer to a tensor A by specifying a typical component, such as Ai j , thereby also conveying information
as to its covariant vs. contravariant nature. As long as you refrain from writing nonsense such as A = Ai j , no harm is done.

210
Chapter 4 Tensors and Differential Forms
let us contract the second-rank mixed tensor Bi
j by setting j to i, then summing over i. To
see what happens, let’s look at the transformation formula that converts B into B′. Using
the summation convention,
(B′)i
i = ∂(x′)i
∂xk
∂xl
∂(x′)i Bk
l = ∂xl
∂xk Bk
l ,
where we recognized the i summation as an instance of the chain rule for differentiation.
Then, because the xi are independent, we may use Eq. (4.14) to reach
(B′)i
i = δl
k Bk
l = Bk
k .
(4.16)
Remembering that the repeated index (i or k) is summed, we see that the contracted B
is invariant under transformation and is therefore a scalar.2 In general, the operation of
contraction reduces the rank of a tensor by 2.
Direct Product
The components of two tensors (of any ranks and covariant/contravariant characters) can
be multiplied, component by component, to make an object with all the indices of both
factors. The new quantity, termed the direct product of the two tensors, can be shown to be
a tensor whose rank is the sum of the ranks of the factors, and with covariant/contravariant
character that is the sum of those of the factors. We illustrate:
Ci j
klm = Ai
k B j
lm,
Fi j
kl = A j Bi
lk.
Note that the index order in the direct product can be deﬁned as desired, but the covari-
ance/contravariance of the factors must be maintained in the direct product.
Example 4.1.1
DIRECT PRODUCT OF TWO VECTORS
Let’s form the direct product of a covariant vector ai (rank-1 tensor) and a contravariant
vector b j (also a rank-1 tensor) to form a mixed tensor of rank 2, with components C j
i =
aib j. To verify that C j
i is a tensor, we consider what happens to it under transformation:
(C′) j
i = (a′)i(b′) j = ∂xk
∂(x′)i
ak
∂(x′) j
∂xl
bl = ∂xk
∂(x′)i
∂(x′) j
∂xl
Cl
k,
(4.17)
conﬁrming that C j
i is the mixed tensor indicated by its notation.
If we now form the contraction Ci
i (remember that i is summed), we obtain the scalar
product aibi. From Eq. (4.17) it is easy to see that aibi = (a′)i(b′)i, indicating the invari-
ance required of a scalar product.
■
Note that the direct product concept gives a meaning to quantities such as ∇E, which
was not deﬁned within the framework of vector analysis. However, this and other tensor-
like quantities involving differential operators must be used with caution, because their
2In matrix analysis this scalar is the trace of the matrix whose elements are the Bi
j .

4.1 Tensor Analysis
211
transformation rules are simple only in Cartesian coordinate systems. In non-Cartesian
systems, operators ∂/∂xi act also on the partial derivatives in the transformation expres-
sions and alter the tensor transformation rules.
We summarize the key idea of this subsection:
The direct product is a technique for creating new, higher-rank tensors.
Inverse Transformation
If we have a contravariant vector Ai, which must have the transformation rule (using sum-
mation convention)
(A′) j = ∂(x′) j
∂xi
Ai,
the inverse transformation (which can be obtained simply by interchanging the roles of the
primed and unprimed quantities) is
Ai =
∂xi
∂(x′) j (A′) j,
(4.18)
as may also be veriﬁed by applying ∂(x′)k/∂xi (and summing i) to Ai as given by
Eq. (4.18):
∂(x′)k
∂xi
Ai = ∂(x′)k
∂xi
∂xi
∂(x′) j (A′) j = δk
j(A′) j = (A′)k.
(4.19)
We see that (A′)k is recovered. Incidentally, note that
∂xi
∂(x′) j ̸=
∂(x′) j
∂xi
−1
;
as we have previously pointed out, these derivatives have different other variables held
ﬁxed. The cancellation in Eq. (4.19) only occurs because the product of derivatives is
summed. In Cartesian systems, we do have
∂xi
∂(x′) j = ∂(x′) j
∂xi
,
both equal to the direction cosine connecting the xi and (x′) j axes, but this equality does
not extend to non-Cartesian systems.
Quotient Rule
If, for example, Ai j and Bkl are tensors, we have already observed that their direct product,
Ai j Bkl, is also a tensor. Here we are concerned with the inverse problem, illustrated by

212
Chapter 4 Tensors and Differential Forms
equations such as
Ki Ai = B,
K j
i A j = Bi,
K j
i A jk = Bik,
(4.20)
Ki jkl Ai j = Bkl,
K i j Ak = Bi jk.
In each of these expressions A and B are known tensors of ranks indicated by the number
of indices, A is arbitrary, and the summation convention is in use. In each case K is an
unknown quantity. We wish to establish the transformation properties of K. The quotient
rule asserts:
If the equation of interest holds in all transformed coordinate systems, then K is a tensor
of the indicated rank and covariant/contravariant character.
Part of the importance of this rule in physical theory is that it can establish the tensor
nature of quantities. For example, the equation giving the dipole moment m induced in an
anisotropic medium by an electric ﬁeld E is
mi = Pi j E j.
Since presumably we know that m and E are vectors, the general validity of this equation
tells us that the polarization matrix P is a tensor of rank 2.
Let’s prove the quotient rule for a typical case, which we choose to be the second of
Eqs. (4.20). If we apply a transformation to that equation, we have
K j
i A j = Bi
−→
(K ′) j
i A′
j = B′
i.
(4.21)
We now evaluate B′
i, reaching the last member of the equation below by using Eq. (4.18)
to convert A j into components of A′ (note that this is the inverse of the transformation to
the primed quantities):
B′
i = ∂xm
∂(x′)i Bm = ∂xm
∂(x′)i K j
m A j = ∂xm
∂(x′)i K j
m
∂(x′)n
∂x j
A′
n.
(4.22)
It may lessen possible confusion if we rename the dummy indices in Eq. (4.22), so we
interchange n and j, causing that equation to then read
B′
i = ∂xm
∂(x′)i
∂(x′) j
∂xn
K n
m A′
j.
(4.23)
It has now become clear that if we subtract the expression for B′
i in Eq. (4.23) from that in
Eq. (4.21) we will get

(K ′) j
i −∂xm
∂(x′)i
∂(x′) j
∂xn
K n
m

A′
j = 0.
(4.24)
Since A′ is arbitrary, the coefﬁcient of A′
j in Eq. (4.24) must vanish, showing that K has
the transformation properties of the tensor corresponding to its index conﬁguration.

4.1 Tensor Analysis
213
Other cases may be treated similarly. One minor pitfall should be noted: The quotient
rule does not necessarily apply if B is zero. The transformation properties of zero are
indeterminate.
Example 4.1.2
EQUATIONS OF MOTION AND FIELD EQUATIONS
In classical mechanics, Newton’s equations of motion m˙v = F tell us on the basis of the
quotient rule that, if the mass is a scalar and the force a vector, then the acceleration a ≡˙v
is a vector. In other words, the vector character of the force as the driving term imposes its
vector character on the acceleration, provided the scale factor m is scalar.
The wave equation of electrodynamics can be written in relativistic four-vector form as
 1
c2
∂2
∂t2 −∇2

Aµ = J µ,
where J µ is the external charge/current density (a four-vector) and Aµ is the four-
component vector potential. The second-derivative expression in square brackets can be
shown to be a scalar. From the quotient rule, we may then infer that Aµ must be a tensor
of rank 1, i.e., also a four-vector.
■
The quotient rule is a substitute for the illegal division of tensors.
Spinors
It was once thought that the system of scalars, vectors, tensors (second-rank), and so on
formed a complete mathematical system, one that is adequate for describing a physics
independent of the choice of reference frame. But the universe and mathematical physics
are not that simple. In the realm of elementary particles, for example, spin-zero particles3
(π mesons, α particles) may be described with scalars, spin 1 particles (deuterons) by
vectors, and spin 2 particles (gravitons) by tensors. This listing omits the most common
particles: electrons, protons, and neutrons, all with spin 1
2. These particles are properly
described by spinors. A spinor does not have the properties under rotation consistent with
being a scalar, vector, or tensor of any rank. A brief introduction to spinors in the context
of group theory appears in Chapter 17.
Exercises
4.1.1
Show that if all the components of any tensor of any rank vanish in one particular
coordinate system, they vanish in all coordinate systems.
Note. This point takes on special importance in the four-dimensional (4-D) curved space
of general relativity. If a quantity, expressed as a tensor, exists in one coordinate sys-
tem, it exists in all coordinate systems and is not just a consequence of a choice of a
coordinate system (as are centrifugal and Coriolis forces in Newtonian mechanics).
3The particle spin is intrinsic angular momentum (in units of ¯h). It is distinct from classical (often called orbital) angular
momentum that arises from the motion of the particle.

214
Chapter 4 Tensors and Differential Forms
4.1.2
The components of tensor A are equal to the corresponding components of tensor B in
one particular coordinate system denoted, by the superscript 0; that is,
A0
i j = B0
i j.
Show that tensor A is equal to tensor B, Ai j = Bi j, in all coordinate systems.
4.1.3
The last three components of a 4-D vector vanish in each of two reference frames. If the
second reference frame is not merely a rotation of the ﬁrst about the x0 axis, meaning
that at least one of the coefﬁcients ∂(x′)i/∂x0 (i = 1,2,3) is nonzero, show that the
zeroth component vanishes in all reference frames. Translated into relativistic mechan-
ics, this means that if momentum is conserved in two Lorentz frames, then energy is
conserved in all Lorentz frames.
4.1.4
From an analysis of the behavior of a general second-rank tensor under 90◦and 180◦
rotations about the coordinate axes, show that an isotropic second-rank tensor in 3-D
space must be a multiple of δi
j.
4.1.5
The 4-D fourth-rank Riemann-Christoffel curvature tensor of general relativity, Riklm,
satisﬁes the symmetry relations
Riklm = −Rikml = −Rkilm.
With the indices running from 0 to 3, show that the number of independent components
is reduced from 256 to 36 and that the condition
Riklm = Rlmik
further reduces the number of independent components to 21. Finally, if the components
satisfy an identity Riklm + Rilmk + Rimkl = 0, show that the number of independent
components is reduced to 20.
Note. The ﬁnal three-term identity furnishes new information only if all four indices are
different.
4.1.6
Tiklm is antisymmetric with respect to all pairs of indices. How many independent com-
ponents has it (in 3-D space)?
4.1.7
If T...i is a tensor of rank n, show that ∂T...i/∂x j is a tensor of rank n + 1 (Cartesian
coordinates).
Note. In non-Cartesian coordinate systems the coefﬁcients ai j are, in general, functions
of the coordinates, and the derivatives the components of a tensor of rank n do not
form a tensor except in the special case n = 0. In this case the derivative does yield a
covariant vector (tensor of rank 1).
4.1.8
If Ti jk... is a tensor of rank n, show that P
j ∂Ti jk.../∂x j is a tensor of rank n −1
(Cartesian coordinates).
4.1.9
The operator
∇2 −1
c2
∂2
∂t2

4.2 Pseudsotensors, Dual Tensors
215
may be written as
4
X
i=1
∂2
∂x2
i
,
using x4 = ict. This is the 4-D Laplacian, sometimes called the d’Alembertian and
denoted by □2. Show that it is a scalar operator, that is, invariant under Lorentz trans-
formations, i.e., under rotations in the space of vectors (x1, x2, x3, x4).
4.1.10
The double summation Ki j Ai B j is invariant for any two vectors Ai and B j. Prove that
Ki j is a second-rank tensor.
Note. In the form ds2 (invariant) = gi j dxi dx j, this result shows that the matrix gi j is
a tensor.
4.1.11
The equation Ki j A jk = Bk
i holds for all orientations of the coordinate system. If A and
B are arbitrary second-rank tensors, show that K is a second-rank tensor also.
4.2
PSEUDOTENSORS, DUAL TENSORS
The topics of this section will be treated for tensors restricted for practical reasons to Carte-
sian coordinate systems. This restriction is not conceptually necessary but simpliﬁes the
discussion and makes the essential points easy to identify.
Pseudotensors
So far the coordinate transformations in this chapter have been restricted to passive rota-
tions, by which we mean rotation of the coordinate system, keeping vectors and tensors at
ﬁxed orientations. We now consider the effect of reﬂections or inversions of the coordinate
system (sometimes also called improper rotations).
In Section 3.3, where attention was restricted to orthogonal systems of Cartesian coor-
dinates, we saw that the effect of a coordinate rotation on a ﬁxed vector could be described
by a transformation of its components according to the formula
A′ = SA,
(4.25)
where S was an orthogonal matrix with determinant +1. If the coordinate transformation
included a reﬂection (or inversion), the transformation matrix was still orthogonal, but
had determinant −1. While the transformation rule of Eq. (4.25) was obeyed by vectors
describing quantities such as position in space or velocity, it produced the wrong sign
when vectors describing angular velocity, torque, and angular momentum were subject to
improper rotations. These quantities, called axial vectors, or nowadays pseudovectors,
obeyed the transformation rule
A′ = det(S)SA
(pseudovector).
(4.26)
The extension of this concept to tensors is straightforward. We insist that the designation
tensor refer to objects that transform as in Eq. (4.6) and its generalization to arbitrary

216
Chapter 4 Tensors and Differential Forms
rank, but we also accommodate the possibility of having, at arbitrary rank, objects whose
transformation requires an additional sign factor to adjust for the effect associated with
improper rotations. These objects are called pseudotensors, and constitute a generalization
of the objects already identiﬁed as pseudoscalars and pseudovectors.
If we form a tensor or pseudotensor as a direct product or identify one via the quotient
rule, we can determine its pseudo status by what amounts to a sign rule. Letting T be a
tensor and P a pseudotensor, then, symbolically,
T ⊗T = P ⊗P = T,
T ⊗P = P ⊗T = P.
(4.27)
Example 4.2.1
LEVI-CIVITA SYMBOL
The three-index version of the Levi-Civita symbol, introduced in Eq. (2.8), has the values
ε123 = ε231 = ε312 = +1,
ε132 = ε213 = ε321 = −1,
(4.28)
all other εi jk = 0.
Suppose now that we have a rank-3 pseudotensor ηi jk, which in one particular Cartesian
coordinate system is equal to εi jk. Then, letting A stand for the matrix of coefﬁcients in an
orthogonal transformation of R3, we have in the transformed coordinate system
η′
i jk = det(A)
X
pqr
aipa jqakrεpqr,
(4.29)
by deﬁnition of pseudotensor. All terms of the pqr sum will vanish except those where
pqr is a permutation of 123, and when pqr is such a permutation the sum will correspond
to the determinant of A except that its rows will have been permuted from 123 to i jk. This
means that the pqr sum will have the value εi jk det(A), and
η′
i jk = εi jk [det(A)]2 = εi jk,
(4.30)
where the ﬁnal result depends on the fact that |det(A)| = 1. If the reader is uncomfortable
with the above analysis, the result can be checked by enumeration of the contributions of
the six permutations that correspond to nonzero values of η′
i jk.
Equation (4.30) not only shows that ε is a rank-3 pseudotensor, but that it is also
isotropic. In other words, it has the same components in all rotated Cartesian coordinate
systems, and −1 times those component values in all Cartesian systems that are reached
by improper rotations.
■
Dual Tensors
With any antisymmetric second-rank tensor C (in 3-D space) we may associate a pseu-
dovector C with components deﬁned by
Ci = 1
2εi jkC jk.
(4.31)

4.2 Pseudsotensors, Dual Tensors
217
In matrix form the antisymmetric C may be written
C =


0
C12 −C31
−C12
0
C23
C31 −C23
0

.
(4.32)
We know that Ci must transform as a vector under rotations because it was obtained from
the double contraction of εi jkC jk, but that it is really a pseudovector because of the pseudo
nature of εi jk. Speciﬁcally, the components of C are given by
(C1,C2,C3) = (C23,C31,C12).
(4.33)
Note the cyclic order of the indices that comes from the cyclic order of the components
of εi jk.
We identify the pseudovector of Eq. (4.33) and the antisymmetric tensor of Eq. (4.32)
as dual tensors; they are simply different representations of the same information. Which
of the dual pair we choose to use is a matter of convenience.
Here is another example of duality. If we take three vectors A, B, and C, we may deﬁne
the direct product
V i jk = Ai B jCk.
(4.34)
V i jk is evidently a rank-3 tensor. The dual quantity
V = εi jkV i jk
(4.35)
is clearly a pseudoscalar. By expansion it is seen that
V =

A1 B1 C1
A2 B2 C2
A3 B3 C3

(4.36)
is our familiar scalar triple product.
Exercises
4.2.1
An antisymmetric square array is given by


0
C3 −C2
−C3
0
C1
C2 −C1
0

=


0
C12 C13
−C12
0
C23
−C13 −C23
0

,
where (C1,C2,C3) form a pseudovector. Assuming that the relation
Ci = 1
2! εi jkC jk
holds in all coordinate systems, prove that C jk is a tensor. (This is another form of the
quotient theorem.)
4.2.2
Show that the vector product is unique to 3-D space, that is, only in three dimensions can
we establish a one-to-one correspondence between the components of an antisymmetric
tensor (second-rank) and the components of a vector.

218
Chapter 4 Tensors and Differential Forms
4.2.3
Write ∇·∇×A and ∇×∇ϕ in tensor (index) notation in IR3 so that it becomes obvious
that each expression vanishes.
AN S.
∇· ∇× A = εi jk
∂
∂xi
∂
∂x j Ak
(∇× ∇ϕ)i = εi jk
∂
∂x j
∂
∂xk ϕ.
4.2.4
Verify that each of the following fourth-rank tensors is isotropic, that is, that it has the
same form independent of any rotation of the coordinate systems.
(a)
Aik
jl = δi
jδk
l ,
(b)
Bi j
kl = δi
kδ j
l + δi
l δ j
k ,
(c)
Ci j
kl = δi
kδ j
l −δi
l δ j
k .
4.2.5
Show that the two-index Levi-Civita symbol εi j is a second-rank pseudotensor (in two-
dimensional [2-D] space). Does this contradict the uniqueness of δi
j (Exercise 4.1.4)?
4.2.6
Represent εi j by a 2 × 2 matrix, and using the 2 × 2 rotation matrix of Eq. (3.23), show
that εi j is invariant under orthogonal similarity transformations.
4.2.7
Given Ak = 1
2 εi jk Bi j with Bi j = −B ji, antisymmetric, show that
Bmn = εmnk Ak.
4.3
TENSORS IN GENERAL COORDINATES
Metric Tensor
The distinction between contravariant and covariant transformations was established in
Section 4.1, where we also observed that it only became meaningful when working with
coordinate systems that are not Cartesian. We now want to examine relationships that can
systematize the use of more general metric spaces (also called Riemannian spaces). Our
initial illustrations will be for spaces with three dimensions.
Letting qi denote coordinates in a general coordinate system, writing the index as a
superscript to reﬂect the fact that coordinates transform contravariantly, we deﬁne covari-
ant basis vectors εi that describe the displacement (in Euclidean space) per unit change
in qi, keeping the other q j constant. For the situations of interest here, both the direction
and magnitude of εi may be functions of position, so it is deﬁned as the derivative
εi = ∂x
∂qi ˆex + ∂y
∂qi ˆey + ∂z
∂qi ˆez.
(4.37)
An arbitrary vector A can now be formed as a linear combination of the basis vectors,
multiplied by coefﬁcients:
A = A1 ε1 + A2 ε2 + A3 ε3.
(4.38)

4.3 Tensors in General Coordinates
219
At this point we have a linguistic ambiguity: A is a ﬁxed object (usually called a vector)
that may be described in various coordinate systems. But it is also customary to call the
collection of coefﬁcients Ai a vector (more speciﬁcally, a contravariant vector), while
we have already called εi a covariant basis vector. The important thing to observe here is
that A is a ﬁxed object that is not changed by our transformations, while its representation
(the Ai) and the basis used for the representation (the εi) change in mutually inverse ways
(as the coordinate system is changed) so as to keep A ﬁxed.
Given our basis vectors, we can compute the displacement (change in position) associ-
ated with changes in the qi. Because the basis vectors depend on position, our computation
needs to be for small (inﬁnitesimal) displacements ds. We have
(ds)2 =
X
i j
(εi dqi) · (ε j dq j),
which, using the summation convention, can be written
(ds)2 = gi jdqidq j,
(4.39)
with
gi j = εi · ε j.
(4.40)
Since (ds)2 is an invariant under rotational (and reﬂection) transformations, it is a scalar,
and the quotient rule permits us to identify gi j as a covariant tensor. Because of its role in
deﬁning displacement, gi j is called the covariant metric tensor.
Note that the basis vectors can be deﬁned by their Cartesian components, but they are, in
general, neither unit vectors nor mutually orthogonal. Because they are often not unit vec-
tors we have identiﬁed them by the symbol ε, not ˆe. The lack of both a normalization and
an orthogonality requirement means that gi j, though manifestly symmetric, is not required
to be diagonal, and its elements (including those on the diagonal) may be of either sign.
It is convenient to deﬁne a contravariant metric tensor that satisﬁes
gikgkj = g jkgki = δi
j,
(4.41)
and is therefore the inverse of the covariant metric tensor. We will use gi j and gi j to make
conversions between contravariant and covariant vectors that we then regard as related.
Thus, we write
gi j F j = Fi
and
gi j Fj = Fi.
(4.42)
Returning now to Eq. (4.38), we can manipulate it as follows:
A = Ai εi = Ai δk
i εk =

Aigi j

g jkεk

= A j ε j,
(4.43)
showing that the same vector can be represented either by contravariant or covariant com-
ponents, with the two sets of components related by the transformation in Eq. (4.42).

220
Chapter 4 Tensors and Differential Forms
Covariant and Contravariant Bases
We now deﬁne the contravariant basis vectors
εi = ∂qi
∂x ˆex + ∂qi
∂y ˆey + ∂qi
∂z ˆez,
(4.44)
giving them this name in anticipation of the fact that we can prove them to be the con-
travariant versions of the εi. Our ﬁrst step in this direction is to verify that
εi · ε j = ∂qi
∂x
∂x
∂q j + ∂qi
∂y
∂y
∂q j + ∂qi
∂z
∂z
∂q j = δi
j,
(4.45)
a consequence of the chain rule and the fact that qi and q j are independent variables.
We next note that
(εi · ε j)(ε j · εk) = δi
k,
(4.46)
also proved using the chain rule; the terms can be collected so that groups of them corre-
spond to the identities in Eq. (4.45). Equation (4.46) shows that
gi j = εi · ε j.
(4.47)
Multiplying both sides of Eq. (4.47) on the right by ε j and performing the implied sum-
mation, the left-hand side of that equation, gi j ε j, becomes the formula for εi, while the
right-hand side simpliﬁes to the expression in Eq. (4.44), thereby proving that the con-
travariant vector in that equation was appropriately named.
We illustrate now some typical metric tensors and basis vectors in both covariant and
contravariant form.
Example 4.3.1
SOME METRIC TENSORS
In spherical polar coordinates, (q1,q2,q3) ≡(r,θ,ϕ), and x = r sinθ cosϕ, y = r sinθ sinϕ,
z = r cosθ. The covariant basis vectors are
εr = sinθ cosϕ ˆex + sinθ sinϕ ˆey + cosθ ˆez,
εθ = r cosθ cosϕ ˆex + r cosθ sinϕ ˆey −r sinθ ˆez,
εϕ = −r sinθ sinϕ ˆex + r sinθ cosϕ ˆey,
and the contravariant basis vectors, which can be obtained in many ways, one of which is
to start from r2 = x2 + y2 + z2, cosθ = z/r, tanϕ = y/x, are
εr = sinθ cosϕ ˆex + sinθ sinϕ ˆey + cosθ ˆez,
εθ = r−1 cosθ cosϕ ˆex + r−1 cosθ sinϕ ˆey −r−1 sinθ ˆez,
εϕ = −sinϕ
r sinθ ˆex + cosϕ
r sinθ ˆey,

4.3 Tensors in General Coordinates
221
leading to
g11 = εr · εr = 1,
g22 = εθ · εθ = r2,
g33 = εϕ · εϕ = r2 sin2 θ;
all other gi j vanish. Combining these to make gi j and taking the inverse (to make gi j), we
have
(gi j) =


1
0
0
0
r2
0
0
0
r2 sin2 θ

,
(gi j) =


1
0
0
0
r−2
0
0
0
(r sinθ)−2

.
We can check that we have inverted gi j correctly by comparing the expression given for
gi j from that built directly from εi · ε j. This check is left for the reader.
The Minkowski metric of special relativity has the form
(gi j) = (gi j) =


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

.
The motivation for including it in this example is to emphasize that for some met-
rics important in physics, distances ds2 need not be positive (meaning that ds can be
imaginary).
■
The relation between the covariant and contravariant basis vectors is useful for writing
relationships between vectors. Let A and B be vectors with contravariant representations
(Ai) and (Bi). We may convert the representation of B to Bi = gi j B j, after which the
scalar product A · B takes the form
A · B = (Ai εi) · (B j ε j) = Ai B j(εi · ε j) = Ai Bi.
(4.48)
Another application is in writing the gradient in general coordinates. If a function ψ is
given in a general coordinate system (qi), its gradient ∇ψ is a vector with Cartesian com-
ponents
(∇ψ) j = ∂ψ
∂qi
∂qi
∂x j .
(4.49)
In vector notation, Eq. (4.49) becomes
∇ψ = ∂ψ
∂qi εi,
(4.50)
showing that the covariant representation of ∇ψ is the set of derivatives ∂ψ/∂qi. If we
have reason to use a contravariant representation of the gradient, we can convert its com-
ponents using Eq. (4.42).

222
Chapter 4 Tensors and Differential Forms
Covariant Derivatives
Moving on to the derivatives of a vector, we ﬁnd that the situation is much more compli-
cated because the basis vectors εi are in general not constant, and the derivative will not
be a tensor whose components are the derivatives of the vector components.
Starting from the transformation rule for a contravariant vector,
(V ′)i = ∂xi
∂qk
V k,
and differentiating with respect to q j, we get (for each i)
∂(V ′)i
∂q j
= ∂xi
∂qk
∂V k
∂q j +
∂2xi
∂q j∂qk V k,
(4.51)
which appears to differ from the transformation law for a second-rank tensor because it
contains a second derivative.
To see what to do next, let’s write Eq. (4.51) as a single vector equation in the xi coor-
dinates, which we take to be Cartesian. The result is
∂V′
∂q j = ∂V k
∂q j εk + V k ∂εk
∂q j .
(4.52)
We now recognize that ∂εk/∂q j must be some vector in the space spanned by the set of
all εi and we therefore write
∂εk
∂q j = 0µ
jk εµ.
(4.53)
The quantities 0µ
jk are known as Christoffel symbols of the second kind (those of the ﬁrst
kind will be encountered shortly). Using the orthogonality property of the ε, Eq. (4.45),
we can solve Eq. (4.53) by taking its dot product with any εm, reaching
0m
jk = εm · ∂εk
∂q j .
(4.54)
Moreover, we note that 0m
kj = 0m
jk, which can be demonstrated by writing out the compo-
nents of ∂εk/∂q j.
Returning now to Eq. (4.52) and inserting Eq. (4.53), we initially get
∂V′
∂q j = ∂V k
∂q j εk + V k0µ
jkεµ.
(4.55)
Interchanging the dummy indices k and µ in the last term of Eq. (4.55), we get the ﬁnal
result
∂V′
∂q j =
∂V k
∂q j + V µ0k
jµ

εk.
(4.56)
The parenthesized quantity in Eq. (4.56) is known as the covariant derivative of V , and
it has (unfortunately) become standard to identify it by the awkward notation
V k
; j = ∂V k
∂q j + V µ 0k
jµ, so ∂V′
∂q j = V k
; j εk.
(4.57)

4.3 Tensors in General Coordinates
223
If we rewrite Eq. (4.56) in the form
dV′ =
h
V k
; j dq ji
εk,
and take note that dq j is a contravariant vector, while εk is covariant, we see that the
covariant derivative, V k
; j is a mixed second-rank tensor.4 However, it is important to realize
that although they bristle with indices, neither ∂V k/∂q j nor 0k
jν have individually the
correct transformation properties to be tensors. It is only the combination in Eq. (4.57) that
has the requisite transformational attributes.
It can be shown (see Exercise 4.3.6) that the covariant derivative of a covariant vector
Vi is given by
Vi; j = ∂Vi
∂q j −Vk0k
i j.
(4.58)
Like V i
; j, Vi; j is a second-rank tensor.
The physical importance of the covariant derivative is that it includes the changes in the
basis vectors pursuant to a general dqi, and is therefore more appropriate for describing
physical phenomena than a formulation that considers only the changes in the coefﬁcients
multiplying the basis vectors.
Evaluating Christoffel Symbols
It may be more convenient to evaluate the Christoffel symbols by relating them to the
metric tensor than simply to use Eq. (4.54). As an initial step in this direction, we deﬁne
the Christoffel symbol of the ﬁrst kind [i j,k] by
[i j,k] ≡gmk0m
i j,
(4.59)
from which the symmetry [i j,k] = [ ji,k] follows. Again, this [i j,k] is not a third-rank
tensor. Inserting Eq. (4.54) and applying the index-lowering transformation, Eq. (4.42),
we have
[i j,k] = gmk εm · ∂εi
∂q j
= εk · ∂εi
∂q j .
(4.60)
Next, we write gi j = εi · ε j as in Eq. (4.40) and differentiate it, identifying the result with
the aid of Eq. (4.60):
∂gi j
∂qk = ∂εi
∂qk · ε j + εi · ∂ε j
∂qk
= [ik, j] + [ jk,i].
4V′ does not contribute to the covariant/contravariant character of the equation as its implicit index labels the Cartesian coordi-
nates, as is also the case for εk.

224
Chapter 4 Tensors and Differential Forms
We then note that we can combine three of these derivatives with different index sets, with
a result that simpliﬁes to give
1
2
∂gik
∂q j + ∂g jk
∂qi −∂gi j
∂qk

= [i j,k].
(4.61)
We now return to Eq. (4.59), which we solve for 0m
i j by multiplying both sides by
gnk, summing over k, and using the fact that (gµν) and (gµν) are mutually inverse, see
Eq. (4.41):
0n
i j =
X
k
gnk[i j,k].
(4.62)
Finally, substituting for [i j,k] from Eq. (4.61), and once again using the summation con-
vention, we get:
0n
i j = gnk[i j,k] = 1
2 gnk
∂gik
∂q j + ∂g jk
∂qi −∂gi j
∂qk

.
(4.63)
The apparatus of this subsection becomes unnecessary in Cartesian coordinates, because
the basis vectors have vanishing derivatives, and the covariant and ordinary partial deriva-
tives then coincide.
Tensor Derivative Operators
With covariant differentiation now available, we are ready to derive the vector differential
operators in general tensor form.
Gradient—We have already discussed it, with the result from Eq. (4.50):
∇ψ = ∂ψ
∂qi εi.
(4.64)
Divergence—A vector V whose contravariant representation is V iεi has divergence
∇· V = ε j · ∂(V iεi)
∂q j
= ε j ·
∂V i
∂q j + V k0i
jk

εi = ∂V i
∂qi + V k0i
ik.
(4.65)
Note that the covariant derivative has appeared here. Expressing 0i
ik by Eq. (4.63), we
have
0i
ik = 1
2 gim
∂gim
∂qk + ∂gkm
∂qi −∂gik
∂qm

= 1
2 gim ∂gim
∂qk ,
(4.66)
where we have recognized that the last two terms in the bracket will cancel because by
changing the names of their dummy indices they can be identiﬁed as identical except in
sign.
Because (gim) is the matrix inverse to (gim), we note that the combination of matrix
elements on the right-hand side of Eq. (4.66) is similar to those in the formula for the
derivative of a determinant, Eq. (2.35); remember that g is symmetric: gim = gmi. In the
present notation, the relevant formula is
d det(g)
dqk
= det(g) gim ∂gim
∂qk ,
(4.67)

4.3 Tensors in General Coordinates
225
where det(g) is the determinant of the covariant metric tensor (gµν). Using Eq. (4.67),
Eq. (4.66) becomes
0i
ik =
1
2det(g)
d det(g)
dqk
=
1
[det(g)]1/2
∂[det(g)]1/2
∂qk
.
(4.68)
Combining the result in Eq. (4.68) with Eq. (4.65), we obtain a maximally compact formula
for the divergence of a contravariant vector V:
∇· V = V i
;i =
1
[det(g)]1/2
∂
∂qk

[det(g)]1/2 V k
.
(4.69)
To compare this result with that for an orthogonal coordinate system, Eq. (3.141), note that
det(g) = (h1h2h3)2 and that the k component of the vector represented by V in Eq. (3.141)
is, in the present notation, equal to V k|εk| = hkV k (no summation).
Laplacian—We can form the Laplacian ∇2ψ by inserting an expression for the gradi-
ent ∇ψ into the formula for the divergence, Eq. (4.69). However, that equation uses the
contravariant coefﬁcients V k, so we must describe the gradient in its contravariant rep-
resentation. Since Eq. (4.64) shows that the covariant coefﬁcients of the gradient are the
derivatives ∂ψ/∂qi, its contravariant coefﬁcients have to be
gki ∂ψ
∂qi .
Insertion into Eq. (4.69) then yields
∇2ψ =
1
[det(g)]1/2
∂
∂qk

[det(g)]1/2gki ∂ψ
∂qi

.
(4.70)
For orthogonal systems the metric tensor is diagonal and the contravariant gii = (hi)−2
(no summation). Equation (4.70) then reduces to
∇· ∇ψ =
1
h1h2h3
∂
∂qi
 
h1h2h3
h2
i
∂ψ
∂qi
!
,
in agreement with Eq. (3.142).
Curl—The difference of derivatives that appears in the curl has components that can be
written
∂Vi
∂q j −∂Vj
∂qi = ∂Vi
∂q j −Vk0k
i j −∂Vj
∂qi + Vk0k
ji = Vi; j −Vj;i,
(4.71)
where we used the symmetry of the Christoffel symbols to obtain a cancellation. The rea-
son for the manipulation in Eq. (4.71) is to bring all the terms on its right-hand side to
tensor form. In using Eq. (4.71), it is necessary to remember that the quantities Vi are coef-
ﬁcients of the possibly nonunit εi and are therefore not components of V in the orthonor-
mal basis ˆei.

226
Chapter 4 Tensors and Differential Forms
Exercises
4.3.1
For the special case of 3-D space (ε1, ε2, ε3 deﬁning a right-handed coordinate system,
not necessarily orthogonal), show that
εi =
ε j × εk
ε j × εk · εi
,
i, j,k = 1, 2, 3 and cyclic permutations.
Note. These contravariant basis vectors εi deﬁne the reciprocal lattice space of
Example 3.2.1.
4.3.2
If the covariant vectors εi are orthogonal, show that
(a)
gi j is diagonal,
(b)
gii = 1/gii (no summation),
(c)
|εi| = 1/|εi|.
4.3.3
Prove that (εi · ε j)(ε j · εk) = δi
k.
4.3.4
Show that 0m
jk = 0m
kj.
4.3.5
Derive the covariant and contravariant metric tensors for circular cylindrical coordi-
nates.
4.3.6
Show that the covariant derivative of a covariant vector is given by
Vi; j ≡∂Vi
∂q j −Vk0k
i j.
Hint. Differentiate
εi · ε j = δi
j.
4.3.7
Verify that Vi; j = gikV k
; j by showing that
∂Vi
∂q j −Vk0k
i j = gik
∂V k
∂q j + V m0k
mj

.
4.3.8
From the circular cylindrical metric tensor gi j, calculate the 0k
i j for circular cylindrical
coordinates.
Note. There are only three nonvanishing 0.
4.3.9
Using the 0k
i j from Exercise 4.3.8, write out the covariant derivatives V i
; j of a vector V
in circular cylindrical coordinates.
4.3.10
Show that for the metric tensor gi j;k = gi j
;k = 0.
4.3.11
Starting with the divergence in tensor notation, Eq. (4.70), develop the divergence of a
vector in spherical polar coordinates, Eq. (3.157).
4.3.12
The covariant vector Ai is the gradient of a scalar. Show that the difference of covariant
derivatives Ai; j −A j;i vanishes.

4.4 Jacobians
227
4.4
JACOBIANS
In the preceding chapters we have considered the use of curvilinear coordinates, but have
not placed much focus on transformations between coordinate systems, and in particular
on the way in which multidimensional integrals must transform when the coordinate sys-
tem is changed. To provide formulas that will be useful in spaces with arbitrary numbers
of dimensions, and with transformations involving coordinate systems that are not orthog-
onal, we now return to the notion of the Jacobian, introduced but not fully developed in
Chapter 1.
As already mentioned in Chapter 1, changes of variables in multiple integrations, say
from variables x1, x2,... to u1, u2,... requires that we replace the differential dx1dx2 ...
with J du1du2 ..., where J, called the Jacobian, is the quantity (usually dependent on
the variables) needed to make these expressions mutually consistent. More speciﬁcally,
we identify dτ = J du1du2 ... as the “volume” of a region of width du1 in u1, du2 in
u2, ..., where the “volume” is to be computed in the x1, x2, ...space, treated as Cartesian
coordinates.
To obtain a formula for J we start by identifying the displacement (in the Cartesian
system deﬁned by the xi) that corresponds to a change in each variable ui. Letting ds(ui)
be that displacement (which is a vector), we can decompose it into Cartesian components
as follows:
ds(u1) =
∂x1
∂u1

ˆe1 +
∂x2
∂u1

ˆe2 + ···

du1,
ds(u2) =
∂x1
∂u2

ˆe1 +
∂x2
∂u2

ˆe2 + ···

du2,
(4.72)
ds(u3) =
∂x1
∂u3

ˆe1 +
∂x2
∂u3

ˆe2 + ···

du3,
...... =
.........
The partial derivatives (∂xi/∂u j) in Eq. (4.72) must be understood to be evaluated with
the other uk held constant. It would clutter the formula an unreasonable amount to indicate
this explicitly.
If we had only two variables, u1 and u2, the differential area would simply be |ds(u1)|
times the component of ds(u2) that is perpendicular to ds(u1). If there were a third vari-
able, u3, we would further multiply by the component of ds(u3) that was perpendicular to
both ds(u1) and ds(u2). Extension to arbitrary numbers of dimensions is obvious.
What is less obvious is an explicit formula for the “volume” for an arbitrary number of
dimensions. Let’s start by writing Eq. (4.72) in matrix form:


ds(u1)
du1
ds(u2)
du2
ds(u3)
du3
...


=


∂x1
∂u1
∂x2
∂u1
∂x3
∂u1
···
∂x1
∂u2
∂x2
∂u2
∂x3
∂u2
···
∂x1
∂u3
∂x2
∂u3
∂x3
∂u3
···
...
...
... ...




ˆe1
ˆe2
ˆe3
...

.
(4.73)

228
Chapter 4 Tensors and Differential Forms
We now proceed to make changes to the second and succeeding rows of the square matrix
in Eq. (4.73) that may destroy the relation to the ds(ui)/dui, but which will leave the
“volume” unchanged. In particular, we subtract from the second row of the derivative
matrix that multiple of the ﬁrst row which will cause the ﬁrst element of the modiﬁed
second row to vanish. This will not change the “volume” because it modiﬁes ds(u2)/du2
by adding or subtracting a vector in the ds(u1)/du1 direction, and therefore does not affect
the component of ds(u2)/du2 perpendicular to ds(u1)/du1. See Fig. 4.1.
The alert reader will recall that this modiﬁcation of the second row of our matrix is an
operation that was used when evaluating determinants, and was there justiﬁed because it
did not change the value of the determinant. We have a similar situation here; the operation
will not change the value of the differential “volume” because we are changing only the
component of ds(u2)/du2 that is in the ds(u1)/du1 direction. In a similar fashion, we
can carry out further operations of the same kind that will lead to a matrix in which all
the elements below the principal diagonal have been reduced to zero. The situation at this
point is indicated schematically for an 4-D space as the transition from the ﬁrst to the
second matrix in Fig. 4.2. These modiﬁed ds(ui)/dui will lead to the same differential
volume as the original ds(ui)/dui. This modiﬁed matrix will no longer provide a faithful
representation of the differential region in the ui space, but that is irrelevant since our only
objective is to evaluate the differential “volume.”
We next take the ﬁnal (nth) row of our modiﬁed matrix, which will be entirely zero
except for its last element, and subtract a suitable multiple of it from all the other rows
to introduce zeros in the last element of every row above the principal diagonal. These
operations correspond to changes in which we modify only the components of the other
ds(ui)/dui that are in the direction of ds(un), and therefore will not change the differential
“volume.” Then, using the next-to-last row (which now has only a diagonal element), we
can in a similar fashion introduce zeros in the next-to-last column of all the preceding
rows. Continuing this process, we will ultimately have a set of modiﬁed ds(ui)/dui that
will have the structure shown as the last matrix in Fig. 4.2. Because our modiﬁed matrix is
diagonal, with each nonzero element associated with a single different ˆei, the “volume” is
u1
u2
ku1
h
u1
u2
h
FIGURE 4.1
Area remains unchanged when vector proportional to u1 is added to u2.


a11
a12
a13
a14
a21
a22
a23
a24
a31
a32
a33
a34
a41
a42
a43
a44

→


a11
a12
a13
a14
0
b22
b23
b24
0
0
b33
b34
0
0
0
b44

→


a11
0
0
0
0
b22
0
0
0
0
b33
0
0
0
0
b44


FIGURE 4.2
Manipulation of Jacobian matrix. Here ai j = (∂x j/∂ui), and bi j are formed
by combining rows (see text).

4.4 Jacobians
229
then easily computed as the product of the diagonal elements. This product of the diagonal
elements of a diagonal matrix is an evaluation of its determinant.
Reviewing what we have done, we see that we have identiﬁed the differential “volume”
as a quantity which is equal to the determinant of the original derivative set. This must be
so, because we obtained our ﬁnal result by carrying out operations each of which leaves a
determinant unchanged. The ﬁnal result can be expressed as the well-known formula for
the Jacobian:
dτ = J du1du2 ...,
J =

∂x1
∂u1
∂x2
∂u1
∂x3
∂u1
···
∂x1
∂u2
∂x2
∂u2
∂x3
∂u2
···
∂x1
∂u3
∂x2
∂u3
∂x3
∂u3
···
...
...
... ...

≡∂(x1, x2,...)
∂(u1,u2,...) .
(4.74)
The standard notation for the Jacobian, shown as the last member of Eq. (4.74), is a conve-
nient reminder of the way in which the partial derivatives appear in it. Note also that when
the standard notation for J is inserted in the expression for dτ, the overall expression has
du1du2 ... in the numerator, while ∂(u1,u2,...) appears in the denominator. This feature
can help the user to make a proper identiﬁcation of the Jacobian.
A few words about nomenclature: The matrix in Eq. (4.73) is sometimes called the
Jacobian matrix, with the determinant in Eq. (4.74) then distinguished by calling it
the Jacobian determinant. Unless within a discussion in which both these quantities
appear and need to be separately identiﬁed, most authors simply call J, the determinant in
Eq. (4.74), the Jacobian. That is the usage we follow in this book.
We close with one ﬁnal observation. Since J is a determinant, it will have a sign that
depends on the order in which the xi and ui are speciﬁed. This ambiguity corresponds
to our freedom to choose either right- or left-handed coordinates. In typical applications
involving a Jacobian, it is usual to take its absolute value and to choose the ranges of the
individual ui integrals in a way that gives the correct sign for the overall integral.
Example 4.4.1
2-D and 3-D JACOBIANS
In two dimensions, with Cartesian coordinates x, y and transformed coordinates u,v, the
element of area d A has, following Eq. (4.74), the form
d A = du dv
∂x
∂u
∂y
∂v

−
∂x
∂v
∂y
∂u

.
This is the expected result, as the quantity in square brackets is the formula for the z
component of the cross product of the two vectors
∂x
∂u

ˆex +
∂y
∂u

ˆey
and
∂x
∂v

ˆex +
∂y
∂v

ˆey,
and it is well known that the magnitude of the cross product of two vectors is a measure of
the area of the parallelogram with sides formed by the vectors.

230
Chapter 4 Tensors and Differential Forms
In three dimensions, the determinant in the Jacobian corresponds exactly with the for-
mula for the scalar triple product, Eq. (3.12). Letting Ax, Ay, Az in that formula refer to the
derivatives (∂x/∂u), (∂y/∂u), (∂z/∂u), with the components of B and C similarly related
to derivatives with respect to v and w, we recover the formula for the volume within the
parallelepiped deﬁned by three vectors.
■
Inverse of Jacobian
Since the xi and the ui are arbitrary sets of coordinates, we could have carried out the
entire analysis of the preceding subsection regarding the ui as the fundamental coordinate
system, with the xi as coordinates reached by a change of variables. In that case, our
Jacobian (which we choose to label J −1), would be
J −1 = ∂(u1,u2,...)
∂(x1, x2,...) .
(4.75)
It is clear that if dx1dx2 ... = J du1du2 ..., then it must also be true that du1du2 ... =
(1/J)dx1dx2 .... Let’s verify that the quantity we have called J −1 is in fact 1/J.
Let’s represent the two Jacobian matrices involved here as
A =


∂x1
∂u1
∂x2
∂u1
∂x3
∂u1
···
∂x1
∂u2
∂x2
∂u2
∂x3
∂u2
···
∂x1
∂u3
∂x2
∂u3
∂x3
∂u3
···
...
...
... ...


,
B =


∂u1
∂x1
∂u2
∂x1
∂u3
∂x1
···
∂u1
∂x2
∂u2
∂x2
∂u3
∂x2
···
∂u1
∂x3
∂u2
∂x3
∂u3
∂x3
···
...
...
... ...


.
We then have J = det(A) and J −1 = det(B). We would like to show that J J −1 =
det(A)det(B) = 1. The proof is fairly simple if we use the determinant product theorem.
Thus, we write
det(A)det(B) = det(AB),
and now all we need show is that the matrix product AB is a unit matrix. Carrying out the
matrix multiplication, we ﬁnd, as a result of the chain rule,
(AB)i j =
X
k
∂xk
∂ui
∂u j
∂xk

=
∂u j
∂ui

= δi j,
(4.76)
verifying that AB is indeed a unit matrix.
The relation between the Jacobian and its inverse is of practical interest. It may turn out
that the derivatives ∂ui/∂x j are easier to compute than ∂xi/∂u j, making it convenient to
obtain J by ﬁrst constructing and evaluating the determinant for J −1.

4.4 Jacobians
231
Example 4.4.2
DIRECT AND INVERSE APPROACHES TO JACOBIAN
Suppose we need the Jacobian ∂(r,θ,ϕ)
∂(x, y, z), where x, y, and z are Cartesian coordinates and
r, θ, ϕ are spherical polar coordinates. Using Eq. (4.74) and the relations
r =
q
x2 + y2 + z2,
θ = cos−1
 
z
p
x2 + y2 + z2
!
,
ϕ = tan−1  y
x

,
we ﬁnd after signiﬁcant effort (letting ρ2 = x2 + y2),
J = ∂(r,θ,ϕ)
∂(x, y, z) =

x
r
y
r
z
r
xz
r2ρ
yz
r2ρ −ρ
r2
−y
ρ2
x
ρ2
0

= 1
rρ =
1
r2 sinθ .
It is much less effort to use the relations
x = r sinθ cosϕ,
y = r sinθ sinϕ,
z = r cosθ,
and then to evaluate (easily),
J −1 = ∂(x, y, z)
∂(r,θ,ϕ) =

sinθ cosϕ
sinθ sinϕ
cosθ
r cosθ cosϕ r cosθ sinϕ −r sinθ
−r sinθ sinϕ r sinθ cosϕ
0

= r2 sinθ.
We ﬁnish by writing J = 1/J −1 = 1/r2 sinθ.
■
Exercises
4.4.1
Assuming the functions u and v to be differentiable,
(a)
Show that a necessary and sufﬁcient condition that u(x, y, z) and v(x, y, z) are
related by some function f (u,v) = 0 is that (∇u) × (∇v) = 0;
(b)
If u = u(x, y) and v = v(x, y), show that the condition (∇u) × (∇v) = 0 leads to
the 2-D Jacobian
J = ∂(u,v)
∂(x, y) =

∂u
∂x
∂u
∂y
∂v
∂x
∂v
∂y

= 0.
4.4.2
A 2-D orthogonal system is described by the coordinates q1 and q2. Show that the
Jacobian J satisﬁes the equation
J ≡∂(x, y)
∂(q1,q2) ≡∂x
∂q1
∂y
∂q2
−∂x
∂q2
∂y
∂q1
= h1h2.
Hint. It’s easier to work with the square of each side of this equation.

232
Chapter 4 Tensors and Differential Forms
4.4.3
For the transformation u = x + y, v = x/y, with x ≥0 and y ≥0, ﬁnd the Jacobian
∂(x, y)
∂(u,v)
(a)
By direct computation,
(b)
By ﬁrst computing J −1.
4.5
DIFFERENTIAL FORMS
Our study of tensors has indicated that signiﬁcant complications arise when we leave Carte-
sian coordinate systems, even in traditional contexts such as the introduction of spherical
or cylindrical coordinates. Much of the difﬁculty arises from the fact that the metric (as
expressed in a coordinate system) becomes position-dependent, and that the lines or sur-
faces of constant coordinate values become curved. Many of the most vexing problems can
be avoided if we work in a geometry that deals with inﬁnitesimal displacements, because
the situations of most importance in physics then become locally similar to the simpler and
more familiar conditions based on Cartesian coordinates.
The calculus of differential forms, of which the leading developer was Elie Cartan, has
become recognized as a natural and very powerful tool for the treatment of curved coordi-
nates, both in classical settings and in contemporary studies of curved space-time. Cartan’s
calculus leads to a remarkable uniﬁcation of concepts and theorems of vector analysis that
is worth pursuing, with the result that in differential geometry and in theoretical physics
the use of differential forms is now widespread.
Differential forms provide an important entry to the role of geometry in physics, and the
connectivity of the spaces under discussion (technically, referred to as their topology) has
physical implications. Illustrations are provided already by situations as simple as the fact
that a coordinate deﬁned on a circle cannot be single-valued and continuous at all angles.
More sophisticated consequences of topology in physics, largely beyond the scope of the
present text, include gauge transformations, ﬂux quantization, the Bohm-Aharanov effect,
emerging theories of elementary particles, and phenomena of general relativity.
Introduction
For simplicity we begin our discussion of differential forms in a notation appropriate for
ordinary 3-D space, though the real power of the methods under study is that they are
not limited either by the dimensionality of the space or by its metric properties (and are
therefore also relevant to the curved space-time of general relativity). The basic quantities
under consideration are the differentials dx, dy, dz (identiﬁed with linearly independent
directions in the space), linear combinations thereof, and more complicated quantities built
from these by combination rules we will shortly discuss in detail. Taking for example dx,
it is essential to understand that in our current context it is not just an inﬁnitesimal number
describing a change in the x coordinate, but is to be viewed as a mathematical object with
certain operational properties (which, admittedly, may include its eventual use in contexts
such as the evaluation of line, surface, or volume integrals). The rules by which dx and

4.5 Differential Forms
233
related quantities can be manipulated have been designed to permit expressions such as
ω = A(x, y, z)dx + B(x, y, z)dy + C(x, y, z)dz,
(4.77)
which are called 1-forms, to be related to quantities that occur as the integrands of line
integrals, to permit expressions of the type
ω = F(x, y, z)dx ∧dy + G(x, y, z)dx ∧dz + H(x, y, z)dy ∧dz,
(4.78)
which are called 2-forms, to be related to the integrands of surface integrals, and to permit
expressions like
ω = K(x, y, z)dx ∧dy ∧dz,
(4.79)
known as 3-forms, to be related to the integrands of volume integrals.
The ∧symbol (called “wedge”) indicates that the individual differentials are to be com-
bined to form more complicated objects using the rules of exterior algebra (sometimes
called Grassmann algebra), so more is being implied by Eqs. (4.77) to (4.79) than the
somewhat similar formulas that might appear in the conventional notation for various kinds
of integrals. To maintain contact with other presentations on differential forms, we note
that some authors omit the wedge symbol, thereby assuming that the reader knows that
the differentials are to be combined according to the rules of exterior algebra. In order
to minimize potential confusion, we will continue to write the wedge symbol for these
combinations of differentials (which are called exterior, or wedge products).
To write differential forms in ways that do not presuppose the dimension of the under-
lying space, we sometimes write the differentials as dxi, designating a form as a p-form
if it contains p factors dxi. Ordinary functions (containing no dxi) can be identiﬁed as
0-forms.
The mathematics of differential forms was developed with the aim of systematizing the
application of calculus to differentiable manifolds, loosely deﬁned as sets of points that
can be identiﬁed by coordinates that locally vary “smoothly” (meaning that they are differ-
entiable to whatever degree is needed for analysis).5 We are presently focusing attention
on the differentials that appear in the forms; one could also consider the behavior of the
coefﬁcients. For example, when we write the 1-form
ω = Ax dx + Ay dy + Az dz,
Ax, Ay, Az will behave under a coordinate transformation like the components of a vec-
tor, and in the older differential-forms literature the differentials and the coefﬁcients were
referred to as contravariant and covariant vector components, since these two sets of quan-
tities must transform in mutually inverse ways under rotations of the coordinate system.
What is relevant for us at this point is that relationships we develop for differential forms
can be translated into related relationships for their vector coefﬁcients, yielding not only
various well-known formulas of vector analysis but also showing how they can be gener-
alized to spaces of higher dimension.
5A manifold deﬁned on a circle or sphere must have a coordinate that cannot be globally smooth (in the usual coordinate systems
it will jump somewhere by 2π). This and related issues connect topology and physics, and are for the most part outside the scope
of this text.

234
Chapter 4 Tensors and Differential Forms
Exterior Algebra
The central idea in exterior algebra is that the operations are designed to create permuta-
tional antisymmetry. Assuming the basis 1-forms are dxi, that ω j are arbitrary p-forms
(of respective orders p j), and that a and b are ordinary numbers or functions, the wedge
product is deﬁned to have the properties
(aω1 + bω2) ∧ω3 = a ω1 ∧ω3 + b ω2 ∧ω3
(p1 = p2),
(ω1 ∧ω2) ∧ω3 = ω1 ∧(ω2 ∧ω3),
a(ω1 ∧ω2) = (aω1) ∧ω2,
(4.80)
dxi ∧dx j = −dx j ∧dxi.
We thus have the usual associative and distributive laws, and each term of an arbitrary
differential form can be reduced to a coefﬁcient multiplying a dxi or a wedge product of
the generic form
dxi ∧dx j ∧··· ∧dx p.
Moreover, the properties in Eq. (4.80) permit all the coefﬁcient functions to be collected at
the beginning of a form. For example,
a dx1 ∧b dx2 = −a(b dx2 ∧dx1) = −ab(dx2 ∧dx1) = ab(dx1 ∧dx2).
We therefore generally do not need parentheses to indicate the order in which products are
to be carried out.
We can use the last of Eqs. (4.80) to bring the index set into any desired order. If any two
of the dxi are the same, the expression will vanish because dxi ∧dxi = −dxi ∧dxi = 0;
otherwise, the ordered-index form will have a sign determined by the parity of the index
permutation needed to obtain the ordering. It is not a coincidence that this is the sign rule
for the terms of a determinant, compare Eq. (2.10). Letting εP stand for the Levi-Civita
symbol for the permutation to ascending index order, an arbitrary wedge product of dxi
can, for example, be brought to the form
εP dxh1 ∧dxh2 ∧··· ∧dxh p,
1 ≤h1 < h2 < ··· < h p.
If any of the dxi in a differential form is linearly dependent on the others, then its
expansion into linearly independent terms will produce a duplicated dx j and cause the
form to vanish. Since the number of linearly independent dx j cannot be larger than the
dimension of the underlying space, we see that in a space of dimension d we only need to
consider p-forms with p ≤d. Thus, in 3-D space, only up through 3-forms are relevant;
for Minkowski space (ct, x, y, z), we will also have 4-forms.
Example 4.5.1
SIMPLIFYING DIFFERENTIAL FORMS
Consider the wedge product
ω = (3dx + 4dy −dz) ∧(dx −dy + 2dz) = 3dx ∧dx −3dx ∧dy + 6dx ∧dz
+ 4dy ∧dx −4dy ∧dy + 8dy ∧dz −dz ∧dx + dz ∧dy −2dz ∧dz.

4.5 Differential Forms
235
The terms with duplicate differentials, e.g., dx ∧dx, vanish, and products that differ only
in the order of the 1-forms can be combined, changing the sign of the product when we
interchange its factors. We get
ω = −7dx ∧dy + 7dx ∧dz + 7dy ∧dz = 7(dy ∧dz −dz ∧dx −dx ∧dy).
We will shortly see that in three dimensions there are some advantages to bringing the
1-forms into cyclic order (rather than ascending or descending order) in the wedge prod-
ucts, and we did so in the ﬁnal simpliﬁcation of ω.
■
The antisymmetry built into the exterior algebra has an important purpose: It causes
p-forms to depend on the differentials in ways appropriate (in three dimensions) for
the description of elements of length, area, and volume, in part because the fact that
dxi ∧dxi = 0 prevents the appearance of duplicated differentials. In particular, 1-forms
can be associated with elements of length, 2-forms with area, and 3-forms with volume.
This feature carries forward to spaces of arbitrary dimensionality, thereby resolving poten-
tially difﬁcult questions that would otherwise have to be handled on a case-by-case basis.
In fact, one of the virtues of the differential-forms approach is that there now exists a con-
siderable body of general mathematical results that is pretty much completely absent from
tensor analysis. For example, we will shortly ﬁnd that the rules for differentiation in the
exterior algebra cause the derivative of a p-form to be a (p + 1)-form, thereby avoiding
a pitfall that arises in tensor calculus: When the transformation coefﬁcients are position-
dependent, simply differentiating the coefﬁcients representing a tensor of rank p does not
yield another tensor. As we have seen, this dilemma is resolved in tensor analysis by intro-
ducing the notion of covariant derivative. Another consequence of the antisymmetry is
that lengths, areas, volumes, and (at higher dimensionality) hypervolumes are oriented
(meaning that they have signs that depend on the way the p-forms deﬁning them are writ-
ten), and the orientation must be taken into account when making computations based on
differential forms.
Complementary Differential Forms
Associated with each differential form is a complementary (or dual) form that contains the
differentials not included in the original form. Thus, if our underlying space has dimension
d, the form dual to a p-form will be a (d−p)-form. In three dimensions, the complement
to a 1-form will be a 2-form (and vice versa), while the complement to a 3-form will be
a 0-form (a scalar). It is useful to work with these complementary forms, and this is done
by introducing an operator known as the Hodge operator; it is usually designated nota-
tionally as an asterisk (preceding the quantity to which it is applied, not as a superscript),
and is therefore also referred to either as the Hodge star operator or simply as the star
operator. Formally, its deﬁnition requires the introduction of a metric and the selection
of an orientation (chosen by specifying the standard order of the differentials comprising
the 1-form basis), and if the 1-form basis is not orthogonal there result complications we
shall not discuss. For orthogonal bases, the dual forms depend on the index positions of
the factors and on the metric tensor.6
6In the current discussion, restricted to Euclidean and Minkowski metrics, the metric tensor is diagonal, with diagonal elements
±1, and the relevant quantities are the signs of the diagonal elements.

236
Chapter 4 Tensors and Differential Forms
To ﬁnd ∗ω, where ω is a p-form, we start by writing the wedge product ω′ of all mem-
bers of the 1-form basis not represented in ω, with the sign corresponding to the permuta-
tion that is needed to bring the index set
(indices of ω) followed by (indices of ω′)
to standard order. Then ∗ω consists of ω′ (with the sign we just found), but also multi-
plied by (−1)µ, where µ is the number of differentials in ω′ whose metric-tensor diagonal
element is −1. For R3, ordinary 3-D space, the metric tensor is a unit matrix, so this ﬁnal
multiplication can be omitted, but it becomes relevant for our other case of current interest,
the Minkowski metric.
For Euclidean 3-D space, we have
∗1 = dx1 ∧dx2 ∧dx3,
∗dx1 = dx2 ∧dx3,
∗dx2 = dx3 ∧dx1,
∗dx3 = dx1 ∧dx2,
(4.81)
∗(dx1 ∧dx2) = dx3,
∗(dx3 ∧dx1) = dx2,
∗(dx2 ∧dx3) = dx1,
∗(dx1 ∧dx2 ∧dx3) = 1.
Cases not shown above are linearly dependent on those that were shown and can be
obtained by permuting the differentials in the above formulas and taking the resulting sign
changes into account.
At this point, two observations are in order. First, note that by writing the indices 1, 2, 3
in cyclic order, we have caused all the starred quantities to have positive signs. This
choice makes the symmetry more evident. Second, it can be seen that all the formulas
in Eq. (4.81) are consistent with ∗(∗ω) = ω. However, this is not universally true; com-
pare with the formulas for Minkowski space, which are in the example we next consider.
See also Exercise 4.5.1.
Example 4.5.2
HODGE OPERATOR IN MINKOWSKI SPACE
Taking the oriented 1-form basis (dt,dx1,dx2,dx3), and the metric tensor


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

,
let’s determine the effect of the Hodge operator on the various possible differential forms.
Consider initially *1, for which the complementary form contains dt ∧dx1 ∧dx2 ∧dx3.
Since we took these differentials in the basis order, they are assigned a plus sign. Since ω =
1 contains no differentials, its number µ of negative metric-tensor diagonal elements is
zero, so (−1)µ = (−1)0 = 1 and there is no sign change arising from the metric. Therefore,
∗1 = dt ∧dx1 ∧dx2 ∧dx3.
Next, take ∗(dt ∧dx1 ∧dx2 ∧dx3). The complementary form is just unity, with no
sign change due to the index ordering, as the differentials are already in standard order.

4.5 Differential Forms
237
However, this time we have three entries in the quantity being starred with negative metric-
tensor diagonal elements; this generates (−1)3 = −1, so
∗(dt ∧dx1 ∧dx2 ∧dx3) = −1.
Moving next to ∗dx1, the complementary form is dt ∧dx2 ∧dx3, and the index order-
ing (based on dx1,dt,dx2,dx3) requires one pair interchange to reach the standard order
(thereby yielding a minus sign). But the quantity being starred contains one differential
that generates a minus sign, namely dx1, so
∗dx1 = dt ∧dx2 ∧dx3.
Looking explicitly at one more case, consider ∗(dt ∧dx1), for which the complementary
form is dx2 ∧dx3. This time the indices are in standard order, but the dx1 being starred
generates a minus sign, so
∗(dt ∧dx1) = −dx2 ∧dx3.
Development of the remaining possibilities is left to Exercise 4.5.1; the results are summa-
rized below, where i, j,k denotes any cyclic permutation of 1,2,3.
∗1 = dt ∧dx1 ∧dx2 ∧dx3,
∗dxi = dt ∧dx j ∧dxk,
∗dt = dx1 ∧dx2 ∧dx3,
∗(dx j ∧dxk) = dt ∧dxi,
∗(dt ∧dxi) = −dx j ∧dxk,
(4.82)
∗(dx1 ∧dx2 ∧dx3) = dt,
∗(dt ∧dxi ∧dx j) = dxk,
∗(dt ∧dx1 ∧dx2 ∧dx3) = −1.
Note that all the starred forms in Eq. (4.82) with an even number of differentials have
the property that ∗(∗ω) = −ω, conﬁrming our earlier statement that complementing twice
does not always restore the original form with its original sign.
■
We now consider some examples illustrating the utility of the star operator.
Example 4.5.3
MISCELLANEOUS DIFFERENTIAL FORMS
In the Euclidean space R3, consider the wedge product A ∧B of the two 1-forms A =
Ax dx + Ay dy + Az dz and B = Bx dx + By dy + Bz dz. Simplifying using the rules for
exterior products,
A ∧B = (Ay Bz −Az By)dy ∧dz + (Az Bx −Ax Bz)dz ∧dx + (Ax By −Ay Bx)dx ∧dy.
If we now apply the star operator and use the formulas in Eq. (4.81) we get
∗(A ∧B) = (Ay Bz −Az By)dx + (Az Bx −Ax Bz)dy + (Ax By −Ay Bx)dz,
showing that in R3, ∗(A ∧B) forms an expression that is analogous to the cross product
A × B of vectors Ax ˆex + Ayˆey + Azˆez and Bx ˆex + Byˆey + Bzˆez. In fact, we can write
∗(A ∧B) = (A × B)x dx + (A × B)y dy + (A × B)z dz.
(4.83)

238
Chapter 4 Tensors and Differential Forms
Note that the sign of ∗(A ∧B) is determined by our implicit choice that the standard
ordering of the basis differentials is (dx,dy,dz).
Next, consider the exterior product A ∧B ∧C, where C is a 1-form with coefﬁcients
Cx,Cy,Cz. Applying the evaluation rules, we ﬁnd that every surviving term in the product
is proportional to dx ∧dy ∧dz, and we obtain
A ∧B ∧C = (Ax ByCz −Ax BzCy −Ay BxCz
+ Ay BzCx + Az BxCy −Az ByCx) dx ∧dy ∧dz,
which we recognize can be written in the form
A ∧B ∧C =

Ax Ay Az
Bx By Bz
Cx Cy Cz

dx ∧dy ∧dz.
Applying now the star operator, we reach
∗(A ∧B ∧C) =

Ax Ay Az
Bx By Bz
Cx Cy Cz

= A · (B × C).
(4.84)
Not only were the results in Eqs. (4.83) and (4.84) easily obtained, they also generalize
nicely to spaces of arbitrary dimension and metric, while the traditional vector notation,
which uses the cross product, is applicable only to R3.
■
Exercises
4.5.1
Using the rules for the application of the Hodge star operator, verify the results given in
Eq. (4.82) for its application to all linearly independent differential forms in Minkowski
space.
4.5.2
If the force ﬁeld is constant and moving a particle from the origin to (3,0,0) requires a
units of work, from (−1,−1,0) to (−1,1,0) takes b units of work, and from (0,0,4)
to (0,0,5) c units of work, ﬁnd the 1-form of the work.
4.6
DIFFERENTIATING FORMS
Exterior Derivatives
Having introduced differential forms and their exterior algebra, we next develop their prop-
erties under differentiation. To accomplish this, we deﬁne the exterior derivative, which
we consider to be an operator identiﬁed by the traditional symbol d. We have, in fact,
already introduced that operator when we wrote dxi, stating at the time that we intended
to interpret dxi as a mathematical object with speciﬁed properties and not just as a small

4.6 Differentiating Forms
239
change in xi. We are now reﬁning that statement to interpret dxi as the result of applying
the operator d to the quantity xi. We complete our deﬁnition of the operator d by requir-
ing it to have the following properties, where ω is a p-form, ω′ is a p′-form, and f is an
ordinary function (a 0-form):
d(ω + ω′) = dω + dω′
(p = p′),
d( f ω) = (d f ) ∧ω + f dω,
d(ω ∧ω′) = dω ∧ω′ + (−1)p ω ∧dω′,
(4.85)
d(dω) = 0,
d f =
X
j
∂f
∂x j
dx j,
where the sum over j spans the underlying space. The formula for the derivative of the
wedge product is sometime called by mathematicians an antiderivation, referring to the
fact that when applied to the right-hand factor an antisymmetry-motivated minus sign
appears.
Example 4.6.1
EXTERIOR DERIVATIVE
Equations (4.85) are axioms, so they are not subject to proof, though they are required
to be consistent. It is of interest to verify that the sign for the derivative of the second
term in a wedge product is needed. Taking ω and ω′ to be monomials, we ﬁrst bring their
coefﬁcients to the left and then apply the differentiation operator (which, irrespective of
the choice of sign, gives zero when applied to any of the differentials). Thus,
d(ω ∧ω′) = d(AB)
h
dx1 ∧··· ∧dx p
i
∧
h
dx1 ∧··· ∧dx p′
i
=
X
µ
 ∂A
∂xµ
B + A ∂B
∂xµ

dxµ ∧
h
dx1 ∧··· ∧dx p
i
∧
h
dx1 ∧··· ∧dx p′
i
.
On expanding the sum, the ﬁrst term is clearly dω ∧ω′; to make the second term look like
ω ∧dω′, it is necessary to permute dxµ through the p differentials in ω, yielding the sign
factor (−1)p. Extension to general polynomial forms is trivial.
One might also ask whether the fourth of the above axioms, d(dω) = 0, sometimes
referred to as Poincaré’s lemma, is necessary or consistent with the others. First, it pro-
vides new information, as otherwise we have no way of reducing d(dxi). Next, to see why
the axiom set is consistent, we illustrate by examining (in R2)
d f = ∂f
∂x dx + ∂f
∂y dy,

240
Chapter 4 Tensors and Differential Forms
from which we form
d(d f ) = ∂
∂x
∂f
∂x

dx ∧dx + ∂
∂y
∂f
∂x

dy ∧dx
+ ∂
∂x
∂f
∂y

dx ∧dy + ∂
∂y
∂f
∂y

dy ∧dy = 0.
We obtain the zero result because of the antisymmetry of the wedge product and because
the mixed second derivatives are equal. We see that the central reason for the validity of
Poincaré’s lemma is that the mixed derivatives of a sufﬁciently differentiable function are
invariant with respect to the order in which the differentiations are carried out.
■
To catalog the possibilities for the action of the d operator in ordinary 3-D space, we
ﬁrst note that the derivative of an ordinary function (a 0-form) is
d f = ∂f
∂x dx + ∂f
∂y dy + ∂f
∂z dz = (∇f )x dx + (∇f )y dy + (∇f )z dz.
(4.86)
We next differentiate the 1-form ω = Ax dx + Ay dy + Az dz. After simpliﬁcation,
dω =
∂Az
∂y −∂Ay
∂z

dy ∧dz +
∂Ax
∂z −∂Az
∂x

dz ∧dx +
∂Ay
∂x −∂Ax
∂y

dx ∧dy.
We recognize this as
d(Ax dx + Ay dy + Az dz) =
(∇× A)x dy ∧dz + (∇× A)y dz ∧dx + (∇× A)z dx ∧dy,
(4.87)
which is equivalent to
∗d
 Ax dx + Ay dy + Az dz

= (∇× A)x dx + (∇× A)y dy + (∇× A)z dz.
(4.88)
Finally we differentiate the 2-form Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy, obtaining
the three-form
d
 Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy

=
∂Bx
∂x + ∂By
∂y + ∂Bz
∂z

dx ∧dy ∧dz,
equivalent to
d
 Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy

= (∇· B)dx ∧dy ∧dz
(4.89)
and
∗d
 Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy

= ∇· B.
(4.90)
We see that application of the d operator directly generates all the differential operators of
traditional vector analysis.

4.6 Differentiating Forms
241
If now we return to Eq. (4.87) and take the 1-form on its left-hand side to be d f , so that
A = ∇f, we have, inserting Eq. (4.86),
d(d f ) =

∇× (∇f )

x dy ∧dz +

∇× (∇f )

y dz ∧dx +

∇× (∇f )

z dx ∧dy = 0.
(4.91)
We have invoked Poincaré’s lemma to set this expression to zero. The result is equivalent
to the well-known identity ∇× (∇f ) = 0.
Another identity is obtained if we start from Eq. (4.89) and take the 2-form on its left-
hand side to be d(Ax dx + Ay dy + Az dz). Then, with the aid of Eq. (4.88), we have
d

d(Ax dx + Ay dy + Az dz)

= ∇· (∇× A)dx ∧dy ∧dz = 0,
(4.92)
where once again the zero result follows from Poincaré’s lemma and we have established
the well-known formula ∇· (∇× A) = 0. Part of the importance of the derivation of these
formulas using differential-forms methods is that these are merely the ﬁrst members of
hierarchies of identities that can be derived for spaces with higher numbers of dimensions
and with different metric properties.
Example 4.6.2
MAXWELL’S EQUATIONS
Maxwell’s equations of electromagnetic theory can be written in an extremely compact and
elegant way using differential forms notation. In that notation, the independent elements of
the electromagnetic ﬁeld tensor can be written as the coefﬁcients of a 2-form in Minkowski
space with oriented basis (dt,dx,dy,dz):
F = −Ex dt ∧dx −Ey dt ∧dy −Ez dt ∧dz
+ Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy.
(4.93)
Here E and B are respectively the electric ﬁeld and the magnetic induction. The sources of
the ﬁeld, namely the charge density ρ and the components of the current density J, become
the coefﬁcients of the 3-form
J = ρ dx ∧dy ∧dz −Jx dt ∧dy ∧dz −Jy dt ∧dz ∧dx −Jz dt ∧dx ∧dy.
(4.94)
For simplicity we work in units with the permitivity, magnetic permeability, and velocity
of light all set to unity (ε = µ = c = 1). Note that it is natural that the charge and current
densities occur in a 3-form; although they have together the number of components needed
to constitute a four-vector, they are of dimension inverse volume. Note also that some of
the signs in the formulas of this example depend on the details of the metric, and are chosen
to be correct for the Minkowski metric as given in Example 4.5.2. This Minkowski metric
has signature (1,3), meaning that it has one positive and three negative diagonal elements.
Some workers deﬁne the Minkowski metric to have signature (3,1), reversing all its signs.
Either choice will give correct results to problems of physics if used consistently; trouble
only arises if material from inconsistent sources is combined.
The two homogeneous Maxwell equations are obtained from the simple formula
dF = 0. This equation is not a mathematical requirement on F; it is a statement of the

242
Chapter 4 Tensors and Differential Forms
physical properties of electric and magnetic ﬁelds. To relate our new formula to the more
usual vector equations, we simply apply the d operator to F:
dF = −
∂Ex
∂y dy + ∂Ex
∂z dz

∧dt ∧dx −
∂Ey
∂x dx + ∂Ey
∂z dz

∧dt ∧dy
−
∂Ez
∂x dx + ∂Ez
∂y dy

∧dt ∧dz +
∂Bx
∂t dt + ∂Bx
∂x dx

∧dy ∧dz
+
∂By
∂t dt + ∂By
∂y dy

∧dz ∧dx +
∂Bz
∂t dt + ∂Bz
∂z dz

∧dx ∧dy = 0.
(4.95)
Equation (4.95) is easily simpliﬁed to
dF =
∂Ez
∂y −∂Ey
∂z + ∂Bx
∂t

dt ∧dy ∧dz +
∂Ex
∂z −∂Ez
∂x + ∂By
∂t

dt ∧dz ∧dx
+
∂Ey
∂x −∂Ex
∂y + ∂Bz
∂t

dt ∧dx ∧dy +
∂Bx
∂x + ∂By
∂y + ∂Bz
∂z

dx ∧dy ∧dz = 0.
(4.96)
Since the coefﬁcient of each 3-form monomial must individually vanish, we obtain from
Eq. (4.96) the vector equations
∇× E + ∂B
∂t = 0
and
∇· B = 0.
We now go on to obtain the two inhomogeneous Maxwell equations from the almost
equally simple formula d(∗F) = J. To verify this, we ﬁrst form ∗F, evaluating the starred
quantities using the formulas in Eqs. (4.82):
∗F = Ex dy ∧dz + Ey dz ∧dx + Ez dx ∧dy + Bx dt ∧dx + By dt ∧dy + Bz dt ∧dz.
We now apply the d operator, reaching after steps similar to those taken while obtaining
Eq. (4.96):
d(∗F) = ∇· E dx ∧dy ∧dz +
∂Ex
∂t
−(∇× Bx

dt ∧dy ∧dz
+
∂Ey
∂t
−(∇× By

dt ∧dz ∧dx +
∂Ez
∂t −(∇× Bz

dt ∧dx ∧dy. (4.97)
Setting d(∗F) from Eq. (4.97) equal to J as given in Eq. (4.94), we obtain the remaining
Maxwell equations
∇· E = ρ
and
∇× B −∂E
∂t = J.
We close this example by applying the d operator to J. The result must vanish because
d J = d(d(∗F)). We get, starting from Eq. (4.94),
d J =
∂ρ
∂t + ∂Jx
∂x + ∂Jy
∂y + ∂Jz
∂z

dt ∧dx ∧dy ∧dz = 0,

4.7 Integrating Forms
243
showing that
∂ρ
∂t + ∇· J = 0.
(4.98)
Summarizing, the differential-forms approach has reduced Maxwell’s equations to the
two simple formulas
dF = 0
and
d(∗F) = J,
(4.99)
and we have also shown that J must satisfy an equation of continuity.
■
Exercises
4.6.1
Given the two 1-forms ω1 = x dy + y dx and ω2 = x dy −y dx, calculate
(a)
dω1,
(b)
dω2.
(c)
For each of your answers to (a) or (b) that is nonzero, apply the operator d a second
time and verify that d(dωi) = 0.
4.6.2
Apply the operator d twice to ω3 = xy dz + xz dy −yz dx. Verify that the second
application of d yields a zero result.
4.6.3
For ω2 and ω3 the 1-forms with these names in Exercises 4.6.1 and 4.6.2, evaluate
d(ω2 ∧ω3):
(a)
By forming the exterior product and then differentiating, and
(b)
Using the formula for differentiating a product of two forms.
Verify that both approaches give the same result.
4.7
INTEGRATING FORMS
It is natural to deﬁne the integrals of differential forms in a way that preserves our usual
notions of integration. The integrals with which we are concerned are over regions of the
manifolds on which our differential forms are deﬁned; this fact and the antisymmetry of
the wedge product need to be taken into account in developing deﬁnitions and properties
of integrals. For convenience, we illustrate in two or three dimensions; the notions extend
to spaces of arbitrary dimensionality.
Consider ﬁrst the integral of a 1-form ω in 2-D space, integrated over a curve C from a
start-point P to an endpoint Q:
Z
C
ω =
Z
C
h
Ax dx + Ay dy
i
.
We interpret the integration as a conventional line integral. If the curve is described para-
metrically by x(t), y(t) as t increases monotonically from tP to tQ, our integral takes the

244
Chapter 4 Tensors and Differential Forms
elementary form
Z
C
ω =
tQ
Z
tP
h
Ax(t) dx
dt + Ay(t) dy
dt
i
dt,
and (at least in principle) the integral can be evaluated by the usual methods.
Sometimes the integral will have a value that will be independent of the path from P to
Q; in physics this situation arises when a 1-form with coefﬁcients A = (Ax, Ay) describes
what is known as a conservative force (i.e., one that can be written as the gradient of a
potential). In our present language, we then call ω exact, meaning that there exists some
function f such that
ω = d f (x, y)
(4.100)
for a region that includes the points P, Q, and all other points through which the path may
pass.
To check the signiﬁcance of Eq. (4.100), note that it implies
ω = ∂f
∂x dx + ∂f
∂y dy,
showing that ω has as coefﬁcients the components of the gradient of f . Given Eq. (4.100),
we also see that
if ω = d f,
Q
Z
P
ω = f (Q) −f (P).
(4.101)
This admittedly obvious result is independent of the dimension of the space, and is of
importance to the remainder of this section.
Looking next at 2-forms, we have (in 2-D space) integrals such as
Z
S
ω =
Z
S
B(x, y)dx ∧dy.
(4.102)
We interpret dx ∧dy as the element of area corresponding to displacements dx and dy
in mutually orthogonal directions, so in the usual notation of integral calculus we would
write dx dy.
Let’s now return to the wedge product notation and consider what happens if we make
a change of variables from x, y to u,v, with x = au + bv, y = eu + f v. Then dx =
a du + b dv, dy = e du + f dv, and
dx ∧dy = (a du + b dv) ∧(e du + f dv) = (af −be)du ∧dv.
(4.103)
We note that the coefﬁcient of du ∧dv is just the Jacobian of the transformation from x, y
to u,v, which becomes clear if we write a = ∂x/∂u, etc., after which we have
af −be =

∂x
∂u
∂x
∂v
∂y
∂u
∂y
∂v

=

a b
e f
.
(4.104)

4.7 Integrating Forms
245
We now see a fundamental reason why the wedge product has been introduced; it has the
algebraic properties needed to generate in a natural fashion the relations between elements
of area (or its higher-dimension analogs) in different coordinate systems. To emphasize
that observation, note that the Jacobian occurred as a natural consequence of the transfor-
mation; we did not have to take additional steps to insert it, and it was generated simply
by evaluating the relevant differential forms. In addition, the present formulation has one
new feature: because dx ∧dy and dy ∧dx are opposite in sign, areas must be assigned
algebraic signs, and it is necessary to retain the sign of the Jacobian if we make a change
of variables. We therefore take as the element of area corresponding to dx ∧dy the ordi-
nary product ±dxdy, with a choice of sign known as the orientation of the area. Then,
Eq. (4.102) becomes
Z
S
ω =
Z
S
B(x, y)(±dxdy),
(4.105)
and if elsewhere in the same computation we had dy ∧dx, we must convert it to dxdy
using the sign opposite to that used for dx ∧dy.
For p-forms with p > 2, a corresponding analysis applies: If we transform from
(x, y,...) to (u,v,...), the wedge product dx ∧dy ∧··· becomes J du ∧dv ∧···, where
J is the (signed) Jacobian of the transformation. Since the p-space volumes are oriented,
the sign of the Jacobian is relevant and must be retained. Exercise 4.7.1 shows that the
change of variables from the 3-form dx ∧dy ∧dz to du ∧dv ∧dw yields the determinant
which is the (signed) Jacobian of the transformation.
Stokes’ Theorem
A key result regarding the integration of differential forms is a formula known as Stokes’
theorem, a restricted form of which we encountered in our study of vector analysis in
Chapter 3. Stokes’ theorem, in its simplest form, states that if
•
R is a simply-connected region (i.e., one with no holes) of a p-dimensional differen-
tiable manifold in a n-dimensional space (n ≥p);
•
R has a boundary denoted ∂R, of dimension p −1;
•
ω is a (p −1)-form deﬁned on R and its boundary, with derivative dω;
then
Z
R
dω =
Z
∂R
ω.
(4.106)
This is the generalization, to p dimensions, of Eq. (4.101). Note that because dω results
from applying the d operator to ω, the differentials in dω consist of all those in ω, in
the same order, but preceded by that produced by the differentiation. This observation is
relevant for identifying the signs to be associated with the integrations.

246
Chapter 4 Tensors and Differential Forms
A rigorous proof of Stokes’ theorem is somewhat complicated, but an indication of its
validity is not too involved. It is sufﬁcient to consider the case that ω is a monomial:
ω = A(x1,..., x p)dx2 ∧···dx p,
dω = ∂A
∂x1
dx1 ∧dx2 ···dx p.
(4.107)
We start by approximating the portion of R adjacent to the boundary by a set of small
p-dimensional parallelepipeds whose thickness in the x1 direction is δ, with δ having for
each parallelepiped the sign that makes x1 →x1 −δ in the interior of R. For each such
parallepiped (symbolically denoted 1, with faces of constant x1 denoted ∂1), we integrate
dω in x1 from x1 −δ to x1 and over the full range of the other xi, obtaining
Z
1
dω =
Z
∂1
x1
Z
x1−δ
 ∂A
∂x1

dx1 ∧dx2 ∧···dx p
=
Z
∂1
A(x1, x2,...)dx2 ∧···dx p −
Z
∂1
A(x1 −δ, x2,...)dx2 ∧···dx p.
(4.108)
Equation (4.108) indicates the validity of Stokes’ theorem for a laminar region whose
exterior boundary is ∂R; if we perform the same process repeatedly, we can collapse the
inner boundary to a region of zero volume, thereby reaching Eq. (4.106).
Stokes’ theorem applies for manifolds of any dimension; different cases of this single
theorem in two and three dimensions correspond to results originally identiﬁed as distinct
theorems. Some examples follow.
Example 4.7.1
GREEN’S THEOREM IN THE PLANE
Consider in a 2-D space the 1-form ω and its derivative:
ω = P(x, y)dx + Q(x, y)dy,
(4.109)
dω = ∂P
∂y dy ∧dx + ∂Q
∂x dx ∧dy =
∂Q
∂x −∂P
∂y

dx ∧dy,
(4.110)
where we have without comment discarded terms containing dx ∧dx or dy ∧dy.
We apply Stokes’ theorem for this ω to a region S with boundary C, obtaining
Z
S
∂Q
∂x −∂P
∂y

dx ∧dy =
Z
C
(P dx + Q dy).
With orientation such that dx ∧dy = dS (ordinary element of area), we have the formula
usually identiﬁed as Green’s theorem in the plane:
Z
C

P dx + Q dy

=
Z
S
∂Q
∂x −∂P
∂y

dS.
(4.111)

4.7 Integrating Forms
247
Some cases of this theorem: taking P = 0, Q = x, we have the well-known formula
Z
C
x dy =
Z
S
dS = A,
where A is the area enclosed by C with the line integral evaluated in the mathematically
positive (counterclockwise) direction.
If we take P = y, Q = 0, we get instead another familiar formula:
Z
C
y dx =
Z
S
(−1)dS = −A.
■
When working Example 4.7.1, we assumed (without comment) that the line integral on
the closed curve C was to be evaluated for travel in the counterclockwise direction, and
we also related area to the conversion from dx ∧dy to +dxdy. These are choices that
were not dictated by the theory of differential forms but by our intention to make its results
correspond to computation in the usual system of planar Cartesian coordinates. What is
certainly true is that the differential forms calculus gives a different sign for the integral
of y dx than it gave for the integral of x dy; the user of the calculus has the responsibility
to make deﬁnitions corresponding to the situation for which the results are claimed to be
relevant.
Example 4.7.2
STOKES’ THEOREM (USUAL 3-D CASE)
Let the vector potential A be represented by the differential form ω, with it and its deriva-
tive of the forms
ω = Ax dx + Ay dy + Az dz,
(4.112)
dω =
∂Az
∂y −∂Ay
∂z

dy ∧dz +
∂Ax
∂z −∂Az
∂x

dz ∧dx +
∂Ay
∂x −∂Ax
∂y

dx ∧dy
= (∇× A)x dy ∧dz + (∇× A)y dz ∧dx + (∇× A)z dx ∧dy.
(4.113)
Applying Stokes’ theorem to a region S with boundary C and noting that if the standard
order for orienting the differentials is dx,dy,dz, then dy ∧dz →dσ x, dz ∧dz →dσ y,
dx ∧dy →dσ z, and Stokes’ theorem takes the familiar form
Z
C
 Ax dx + Ay dy + Az dz

=
Z
C
A · dr =
Z
S
(∇× A) · dσ.
(4.114)
■
Once again we have results whose interpretation depends on how we have chosen to
deﬁne the quantities involved. The differential forms calculus does not know whether we
intend to use a right-handed coordinate system, and that choice is implicit in our identiﬁ-
cation of the elements of area dσj. In fact, the mathematics does not even tell us that the
quantities we identiﬁed as components of ∇× A actually correspond to anything physical

248
Chapter 4 Tensors and Differential Forms
in their indicated directions. So, once again, we emphasize that the mathematics of differ-
ential forms provides a structure appropriate to the physics to which we apply it, but part
of what the physicist brings to the table is the correlation between mathematical objects
and the physical quantities they represent.
Example 4.7.3
GAUSS’ THEOREM
As a ﬁnal example, consider a 3-D region V with boundary ∂V, containing an electric ﬁeld
given on ∂V as the 2-form ω, with
ω = Ex dy ∧dz + Ey dz ∧dz + Ez dx ∧dy,
(4.115)
dω =
∂Ex
∂x + ∂Ey
∂y + ∂Ez
∂z

dx ∧dy ∧dz = (∇· E)dx ∧dy ∧dz.
(4.116)
For this case, Stokes’ theorem is
Z
V
dω =
Z
V
(∇· E)dx ∧dy ∧dz =
Z
V
(∇· E)dτ =
Z
∂V
E · dσ,
(4.117)
where dx ∧dy ∧dz →dτ and, just as in Example 4.7.2, dy ∧dz →dσ x, etc. We have
recovered Gauss’ theorem.
■
Exercises
4.7.1
Use differential-forms relations to transform the integral A(x, y, z)dx ∧dy ∧dz to
the equivalent expression in du ∧dv ∧dw, where u,v,w is a linear transformation of
x, y, z, and thereby ﬁnd the determinant that can be identiﬁed as the Jacobian of the
transformation.
4.7.2
Write Oersted’s law,
Z
∂S
H · dr =
Z
S
∇× H · da ∼I,
in differential form notation.
4.7.3
A 1-form Adx + Bdy is deﬁned as closed if ∂A
∂y = ∂B
∂x . It is called exact if there is a
function f such that ∂f
∂x = A and ∂f
∂y = B. Determine which of the following 1-forms
are closed, or exact, and ﬁnd the corresponding functions f for those that are exact:
y dx + x dy,
y dx + x dy
x2 + y2
,
[ln(xy) + 1]dx + x
y dy,
−
y dx
x2 + y2 +
x dy
x2 + y2 ,
f (z)dz with z = x + iy.

Additional Readings
249
Additional Readings
Dirac, P. A. M., General Theory of Relativity. Princeton, NJ: Princeton University Press (1996).
Edwards, H. M., Advanced Calculus: A Differential Forms Approach. Boston, MA: Birkhäuser (1994).
Flanders, H., Differential Forms with Applications to the Physical Sciences. New York: Dover (1989).
Hartle, J. B., Gravity. San Francisco: Addison-Wesley (2003). This text uses a minimum of tensor analysis.
Hassani, S., Foundations of Mathematical Physics. Boston, MA: Allyn and Bacon (1991).
Jeffreys, H., Cartesian Tensors. Cambridge: Cambridge University Press (1952). This is an excellent discussion
of Cartesian tensors and their application to a wide variety of ﬁelds of classical physics.
Lawden, D. F., An Introduction to Tensor Calculus, Relativity and Cosmology, 3rd ed. New York: Wiley (1982).
Margenau, H., and G. M. Murphy, The Mathematics of Physics and Chemistry, 2nd ed. Princeton, NJ: Van
Nostrand (1956). Chapter 5 covers curvilinear coordinates and 13 speciﬁc coordinate systems.
Misner, C. W., K. S. Thorne, and J. A. Wheeler, Gravitation. San Francisco: W. H. Freeman (1973). A leading
text on general relativity and cosmology.
Moller, C., The Theory of Relativity. Oxford: Oxford University Press (1955), reprinting, (1972). Most texts on
general relativity include a discussion of tensor analysis. Chapter 4 develops tensor calculus, including the
topic of dual tensors. The extension to non-Cartesian systems, as required by general relativity, is presented
in Chapter 9.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Chapter 5
includes a description of several different coordinate systems. Note that Morse and Feshbach are not above
using left-handed coordinate systems even for Cartesian coordinates. Elsewhere in this excellent (and difﬁcult)
book there are many examples of the use of the various coordinate systems in solving physical problems.
Eleven additional fascinating but seldom encountered orthogonal coordinate systems are discussed in the
second (1970) edition of Mathematical Methods for Physicists.
Ohanian, H. C., and R. Rufﬁni, Gravitation and Spacetime, 2nd ed. New York: Norton & Co. (1994). A well-
written introduction to Riemannian geometry.
Sokolnikoff, I. S., Tensor Analysis—Theory and Applications, 2nd ed. New York: Wiley (1964). Particularly
useful for its extension of tensor analysis to non-Euclidean geometries.
Weinberg, S., Gravitation and Cosmology. Principles and Applications of the General Theory of Relativity. New
York: Wiley (1972). This book and the one by Misner, Thorne, and Wheeler are the two leading texts on
general relativity and cosmology (with tensors in non-Cartesian space).
Young, E. C., Vector and Tensor Analysis, 2nd ed. New York: Dekker (1993).

CHAPTER 5
VECTOR SPACES
A large body of physical theory can be cast within the mathematical framework of vector
spaces. Vector spaces are far more general than vectors in ordinary space, and the analogy
may to the uninitiated seem somewhat strained. Basically, this subject deals with quantities
that can be represented by expansions in a series of functions, and includes the methods by
which such expansions can be generated and used for various purposes. A key aspect of
the subject is the notion that a more or less arbitrary function can be represented by such
an expansion, and that the coefﬁcients in these expansions have transformation properties
similar to those exhibited by vector components in ordinary space. Moreover, operators
can be introduced to describe the application of various processes to a function, thereby
converting it (and also the coefﬁcients deﬁning it) into other functions within our vector
space. The concepts presented in this chapter are crucial to an understanding of quan-
tum mechanics, to classical systems involving oscillatory motion, transport of material or
energy, even to fundamental particle theory. Indeed, it is not excessive to claim that vector
spaces are one of the most fundamental mathematical structures in physical theory.
5.1
VECTORS IN FUNCTION SPACES
We now seek to extend the concepts of classical vector analysis (from Chapter 3) to more
general situations. Suppose that we have a two-dimensional (2-D) space in which the two
coordinates, which are real (or in the most general case, complex) numbers that we will
call a1 and a2, are, respectively, associated with the two functions ϕ1(s) and ϕ2(s). It is
important at the outset to understand that our new 2-D space has nothing whatsoever to do
with the physical xy space. It is a space in which the coordinate point (a1,a2) corresponds
to the function
f (s) = a1ϕ1(s) + a2ϕ2(s).
(5.1)
The analogy with a physical 2-D vector space with vectors A = A1ˆe1 + A2ˆe2 is that ϕi(s)
corresponds to ˆei, while ai ←→Ai, and f (s) ←→A. In other words, the coordinate
251
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

252
Chapter 5 Vector Spaces
values are the coefﬁcients of the ϕi(s), so each point in the space identiﬁes a different
function f (s). Both f and ϕ are shown above as dependent on an independent variable we
call s. We choose the name s to emphasize the fact that the formulation is not restricted
to the spatial variables x, y, z, but can be whatever variable, or set of variables, is needed
for the problem at hand. Note further that the variable s is not a continuous analog of the
discrete variables xi of an ordinary vector space. It is a parameter reminding the reader that
the ϕi that correspond to the dimensions of our vector space are usually not just numbers,
but are functions of one or more variables. The variable(s) denoted by s may sometimes
correspond to physical displacements, but that is not always the case. What should be clear
is that s has nothing to do with the coordinates in our vector space; that is the role of the ai.
Equation (5.1) deﬁnes a set of functions (a function space) that can be built from the
basis ϕ1, ϕ2; we call this space a linear vector space because its members are linear com-
binations of the basis functions and the addition of its members corresponds to component
(coefﬁcient) addition. If f (s) is given by Eq. (5.1) and g(s) is given by another linear
combination of the same basis functions,
g(s) = b1ϕ1(s) + b2ϕ2(s),
with b1 and b2 the coefﬁcients deﬁning g(s), then
h(s) = f (s) + g(s) = (a1 + b1)ϕ1(s) + (a2 + b2)ϕ2(s)
(5.2)
deﬁnes h(s), the member of our space (i.e., the function), which is the sum of the members
f (s) and g(s). In order for our vector space to be useful, we consider only spaces in which
the sum of any two members of the space is also a member.
In addition, the notion of linearity includes the requirement that if f (s) is a member
of our vector space, then u(s) = k f (s), where k is a real or complex number, is also a
member, and we can write
u(s) = k f (s) = ka1ϕ1(s) + ka2ϕ2(s).
(5.3)
Vector spaces for which addition of two members or multiplication of a member by scalar
always produces a result that is also a member are termed closed under these operations.
We can summarize our ﬁndings up to this point as follows: addition of two members
of our vector space causes the coefﬁcients of the sum, h(s) in Eq. (5.2), to be the sum
of the coefﬁcients of the addends, namely f (s) and g(s); multiplication of f (s) by a
ordinary number k (which, by analogy with ordinary vectors, we call a scalar), results
in the multiplication of the coefﬁcients by k. These are exactly the operations we would
carry out to form the sum of two ordinary vectors, A + B, or the multiplication of a vector
by a scalar, as in kA. However, here we have the coefﬁcients ai and bi, which combine
under vector addition and multiplication by a scalar in exactly the same way that we would
combine the ordinary vector components Ai and Bi.
The functions that form the basis of our vector space can be ordinary functions, and may
be as simple as powers of s, or more complicated, as for example ϕ1 = (1 + 3s + 3s2)es,
ϕ2 = (1 −3s + 3s2)e−s, or compound quantities such as the Pauli matrices σi, or even
completely abstract quantities that are deﬁned only by certain properties they may possess.
The number of basis functions (i.e., the dimension of our basis) may be a small number
such as 2 or 3, a larger but ﬁnite integer, or even denumerably inﬁnite (as would arise in an

5.1 Vectors in Function Spaces
253
untruncated power series). The main universal restriction on the form of a basis is that the
basis members be linearly independent, so that any function (member) of our vector space
will be described by a unique linear combination of the basis functions. We illustrate the
possibilities with some simple examples.
Example 5.1.1
SOME VECTOR SPACES
1.
We consider ﬁrst a vector space of dimension 3, which is spanned by (meaning that it
has a basis that consists of) the three functions P0(s) = 1, P1(s) = s, P2(s) = 3
2s2 −1
2.
Some members of this vector space include the functions
s + 3 = 3P0(s) + P1(s),
s2 = 1
3 P0(s) + 2
3 P2(s),
4 −3s = 4P0(s) −3P1(s).
In fact, because we can write 1, s, and s2 in terms of our basis, we can see that any
quadratic form in s will be a member of our vector space, and that our space includes
only functions of s that can be written in the form c0 + c1s + c2s2.
To illustrate our vector-space operations, we can form
s2 −2(s + 3) =
1
3 P0(s) + 2
3 P2(s)

−2
h
3P0(s) + P1(s)
i
=
1
3 −6

P0(s) −2P1(s) + 2
3 P2(s).
This calculation involves only operations on the coefﬁcients; we do not need to refer
to the deﬁnitions of the Pn to carry it out.
Note that we are free to deﬁne our basis any way we want, so long as its members
are linearly independent. We could have chosen as our basis for this same vector space
ϕ0 = 1, ϕ1 = s, ϕ2 = s2, but we chose not to do so.
2.
The set of functions ϕn(s) = sn (n = 0,1,2,...) is a basis for a vector space whose
members consist of functions that can be represented by a Maclaurin series. To avoid
difﬁculties with this inﬁnite-dimensional basis, we will usually need to restrict consid-
eration to functions and ranges of s for which the Maclaurin series converges. Conver-
gence and related issues are of great interest in pure mathematics; in physics problems
we usually proceed in ways such that convergence is assured.
The members of our vector space will have representations
f (s) = a0 + a1 s + a2 s2 + ··· =
∞
X
n=0
an sn,
and we can (at least in principle) use the rules for making power series expansions to
ﬁnd the coefﬁcients that correspond to a given f (s).
3.
The spin space of an electron is spanned by a basis that consists of a linearly indepen-
dent set of possible spin states. It is well known that an electron can have two linearly
independent spin states, and they are often denoted by the symbols α and β. One pos-
sible spin state is f = a1α + a2β, and another is g = b1α + b2β. We do not even need

254
Chapter 5 Vector Spaces
to know what α and β really stand for to discuss the 2-D vector space spanned by
these functions, nor do we need to know the role of any parametric variable such as
s. We can, however, state that the particular spin state corresponding to f + ig must
have the form
f + ig = (a1 + ib1)α + (a2 + ib2)β.
■
Scalar Product
To make the vector space concept useful and parallel to that of vector algebra in ordinary
space, we need to introduce the concept of a scalar product in our function space. We shall
write the scalar product of two members of our vector space, f and g, as ⟨f |g⟩. This is the
notation that is almost universally used in physics; various other notations can be found in
the mathematics literature; examples include [ f, g] and ( f, g).
The scalar product has two main features, the full meaning of which may only become
clear as we proceed. They are:
1.
The scalar product of a member with itself, e.g., ⟨f | f ⟩, must evaluate to a numeri-
cal value (not a function) that plays the role of the square of the magnitude of that
member, corresponding to the dot product of an ordinary vector with itself, and
2.
The scalar product must be linear in each of the two members.1
There exists an extremely wide range of possibilities for deﬁning scalar products that
meet these criteria. The situation that arises most often in physics is that the members
of our vector space are ordinary functions of the variable s (as in the ﬁrst vector space
of Example 5.1.1), and the scalar product of the two members f (s) and g(s) is computed
as an integral of the type
⟨f |g⟩=
b
Z
a
f ∗(s)g(s)w(s)ds,
(5.4)
with the choice of a, b, and w(s) dependent on the particular deﬁnition we wish to adopt
for our scalar product. In the special case ⟨f | f ⟩, the scalar product is to be interpreted as
the square of a “length,” and this scalar product must therefore be positive for any f that is
not itself identically zero. Since the integrand in the scalar product is then f ∗(s) f (s)w(s)
and f ∗(s) f (s) ≥0 for all s (even if f (s) is complex), we can see that w(s) must be
positive over the entire range [a,b] except possibly for zeros at isolated points.
Let’s review some of the implications of Eq. (5.4). It is not appropriate to interpret that
equation as a continuum analog of the ordinary dot product, with the variable s thought
of as the continuum limit of an index labeling vector components. The integral actually
arises pursuant to a decision to compute a “squared length” as a possibly weighted average
over the range of values of the parameter s. We can illustrate this point by considering the
1If the members of the vector space are complex, this statement will need adjustment; see the formal deﬁnitions in the next
subsection.

5.1 Vectors in Function Spaces
255
other situation that arises occasionally in physics, and illustrated by the third vector space
in Example 5.1.1. Here we simply deﬁne the scalar products of the individual α and β to
have values
⟨α|α⟩= ⟨β|β⟩= 1,
⟨α|β⟩= ⟨β|α⟩= 0,
and then, taking the simple one-electron functions
f = a1α + a2β,
g = b1α + b2β,
and assuming ai and bi to be real, we expand ⟨f |g⟩(using its linearity property) to reach
⟨f |g⟩= a1b1⟨α|α⟩+ a1b2⟨α|β⟩+ a2b1⟨β|α⟩+ a2b2⟨β|β⟩= a1b1 + a2b2.
(5.5)
These equations show that the introduction of an integral is not an indispensible step toward
generalization of the scalar product; they also show that the ﬁnal formula in Eq. (5.5),
which is analogous to ordinary vector algebra, arises from the expansion of ⟨f |g⟩in a
basis whose two members, α and β, are orthogonal (i.e., have a zero scalar product). Thus,
the analogy to ordinary vector algebra is that the “unit vectors” of this spin system deﬁne
an orthogonal “coordinate system” and that the “dot product” then has the expected form.
Vector spaces that are closed under addition and multiplication by a scalar and which
have a scalar product that exists for all pairs of its members are termed Hilbert spaces;
these are the vector spaces of primary importance in physics.
Hilbert Space
Proceeding now somewhat more formally (but still without complete rigor), and includ-
ing the possibility that our function space may require more than two basis functions, we
identify a Hilbert space H as having the following properties:
•
Elements (members) f , g, or h of H are subject to two operations, addition, and
multiplication by a scalar (here k, k1, or k2). These operations produce quantities that
are also members of the space.
•
Addition is commutative and associative:
f (s) + g(s) = g(s) + f (s),
[ f (s) + g(s)] + h(s) = f (s) + [g(s) + h(s)].
•
Multiplication by a scalar is commutative, associative, and distributive:
k f (s) = f (s)k,
k[ f (s) + g(s)] = k f (s) + kg(s),
(k1 + k2) f (s) = k1 f (s) + k2 f (s),
k1[k2 f (s)] = k1k2 f (s).
•
H is spanned by a set of basis functions ϕi, where for the purposes of this book the
number of such basis functions (the range of i) can either be ﬁnite or denumerably inﬁ-
nite (like the positive integers). This means that every function in H can be represented
by the linear form f (s) = P
n anϕn(s). This property is also known as completeness.
We require that the basis functions be linearly independent, so that each function in the
space will be a unique linear combination of the basis functions.

256
Chapter 5 Vector Spaces
•
For all functions f (s) and g(s) in H, there exists a scalar product, denoted as ⟨f |g⟩,
which evaluates to a ﬁnite real or complex numerical value (i.e., does not contain s)
and which has the properties that
1.
⟨f | f ⟩≥0, with the equality holding only if f is identically zero.2 The quantity
⟨f | f ⟩1/2 is called the norm of f and is written || f ||.
2.
⟨g| f ⟩∗= ⟨f |g⟩, ⟨f |g + h⟩= ⟨f |g⟩+ ⟨f |h⟩, and ⟨f |kg⟩= k⟨f |g⟩.
Consequences of these properties are that ⟨f |k1g + k2h⟩= k1⟨f |g⟩+ k2⟨f |h⟩, but
⟨k f |g⟩= k∗⟨f |g⟩and ⟨k1 f + k2g|h⟩= k∗
1⟨f |h⟩+ k∗
2⟨g|h⟩.
Example 5.1.2
SOME SCALAR PRODUCTS
Continuing with the ﬁrst vector space of Example 5.1.1, let’s assume that our scalar product
of any two functions f (s) and g(s) takes the form
⟨f |g⟩=
1
Z
−1
f ∗(s) g(s)ds,
(5.6)
i.e., the formula given as Eq. (5.4) with a = −1, b = 1, and w(s) = 1. Since all the mem-
bers of this vector space are quadratic forms and the integral in Eq. (5.6) is over the ﬁnite
range from −1 to +1, the scalar product will always exist and our three basis functions
indeed deﬁne a Hilbert space. Before we make a few sample computations, let’s note that
the brackets in the left member of Eq. (5.6) do not show the detailed form of the scalar
product, thereby concealing information about the integration limits, the number of vari-
ables (here we have only one, s), the nature of the space involved, the presence or absence
of a weight factor w(s), and even the exact operation that forms the product. All these
features must be inferred from the context or by a previously provided deﬁnition.
Now let’s evaluate two scalar products:
⟨P0|s2⟩=
1
Z
−1
P∗
0 (s)s2 ds =
1
Z
−1
(1)(s2)dx =
s3
3
1
−1
= 2
3,
⟨P0|P2⟩=
1
Z
−1
(1)
3
2 s2 −1
2

ds =
3
2
s3
3 −1
2 s
1
−1
= 0.
(5.7)
Looking further at the scalar product deﬁnition of the present example, we note that it
is consistent with the general requirements for a scalar product, as (1) ⟨f | f ⟩is formed as
the integral of an inherently nonnegative integrand, and will be positive for all nonzero
2To be rigorous, the phrase “identically zero” needs to be replaced by “zero except on a set of measure zero,” and other conditions
need to be more tightly speciﬁed. These are niceties that are important for a precise formulation of the mathematics but are not
often of practical importance to the working physicist. We note, however, that discontinuous functions do arise in applications
of Fourier series, with consequences that are discussed in Chapter 19.

5.1 Vectors in Function Spaces
257
f ; and (2) the placement of the complex-conjugate asterisk makes it obvious that
⟨g| f ⟩∗= ⟨f |g⟩.
■
Schwarz Inequality
Any scalar product that meets the Hilbert space conditions will satisfy the Schwarz
inequality, which can be stated as
|⟨f |g⟩|2 ≤⟨f | f ⟩⟨g|g⟩.
(5.8)
Here there is equality only if f and g are proportional. In ordinary vector space, the equiv-
alent result is, referring to Eq. (1.113),
(A · B)2 = |A|2|B|2 cos2 θ ≤|A|2|B|2,
(5.9)
where θ is the angle between the directions of A and B. As observed previously, the equal-
ity only holds if A and B are collinear. If we also require A to be of unit length, we have the
intuitively obvious result that the projection of B onto a noncollinear A direction will have
a magnitude less than that of B. The Schwarz inequality extends this property to functions;
their norms shrink on nontrivial projection.
The Schwarz inequality can be proved by considering
I = ⟨f −λg| f −λg⟩≥0,
(5.10)
where λ is an as yet undetermined constant. Treating λ and λ∗as linearly independent,3 we
differentiate I with respect to λ∗(remember that the left member of the product is complex
conjugated) and set the result to zero, to ﬁnd the λ value for which I is a minimum:
−⟨g| f −λg⟩= 0
=⇒
λ = ⟨g| f ⟩
⟨g|g⟩.
Substituting this λ value into Eq. (5.10), we get (using properties of the scalar product)
⟨f | f ⟩−⟨f |g⟩⟨g| f ⟩
⟨g|g⟩
≥0.
Noting that ⟨g|g⟩must be positive, and rewriting ⟨g| f ⟩as ⟨f |g⟩∗, we conﬁrm the Schwarz
inequality, Eq. (5.8).
Orthogonal Expansions
With now a well-behaved scalar product in hand, we can make the deﬁnition that two func-
tions f and g are orthogonal if ⟨f |g⟩= 0, which means that ⟨g| f ⟩will also vanish. An
example of two functions that are orthogonal under the then-applicable deﬁnition of the
scalar product are P0(s) and P2(s), where the scalar product is that deﬁned in Eq. (5.6)
and P0, P2 are the functions from Example 5.1.1; the orthogonality is shown by Eq. (5.7).
We further deﬁne a function f as normalized if the scalar product ⟨f | f ⟩= 1; this is the
3 It is not obvious that one can do this, but consider λ = µ + iν, λ∗= µ −iν, with µ and ν real. Then 1
2 [∂/∂µ + i∂/∂ν] is
equivalent to taking ∂/∂λ∗keeping λ constant.

258
Chapter 5 Vector Spaces
function-space equivalent of a unit vector. We will ﬁnd that great convenience results if
the basis functions for our function space are normalized and mutually orthogonal, cor-
responding to the description of a 2-D or three-dimensional (3-D) physical vector space
based on orthogonal unit vectors. A set of functions that is both normalized and mutually
orthogonal is called an orthonormal set. If a member f of an orthogonal set is not nor-
malized, it can be made so without disturbing the orthogonality: we simply rescale it to
f = f/⟨f | f ⟩1/2, so any orthogonal set can easily be made orthonormal if desired.
If our basis is orthonormal, the coefﬁcients for the expansion of an arbitrary function in
that basis take a simple form. We return to our 2-D example, with the assumption that the
ϕi are orthonormal, and consider the result of taking the scalar product of f (s), as given
by Eq. (5.1), with ϕ1(s):
⟨ϕ1| f ⟩= ⟨ϕ1|(a1ϕ1 + a2ϕ2)⟩= a1⟨ϕ1|ϕ1⟩+ a2⟨ϕ1|ϕ2⟩.
(5.11)
The orthonormality of the ϕ now comes into play; the scalar product multiplying a1 is
unity, while that multiplying a2 is zero, so we have the simple and useful result ⟨ϕ1| f ⟩=
a1. Thus, we have a rather mechanical means of identifying the components of f. The
general result corresponding to Eq. (5.11) follows:
If ⟨ϕi|ϕ j⟩= δi j
and
f =
n
X
i=1
aiϕi,
then ai = ⟨ϕi| f ⟩.
(5.12)
Here the Kronecker delta, δi j, is unity if i = j and zero otherwise. Looking once again
at Eq. (5.11), we consider what happens if the ϕi are orthogonal but not normalized. Then
instead of Eq. (5.12) we would have:
If the ϕi are orthogonal and f =
n
X
i=1
aiϕi,
then ai = ⟨ϕi| f ⟩
⟨ϕi|ϕi⟩.
(5.13)
This form of the expansion will be convenient when normalization of the basis introduces
unpleasant factors.
Example 5.1.3
EXPANSION IN ORTHONORMAL FUNCTIONS
Consider the set of functions χn(x) = sinnx, for n = 1,2,..., to be used for x in the
interval 0 ≤x ≤π with scalar product
⟨f |g⟩=
π
Z
0
f ∗(x)g(x)dx.
(5.14)
We wish to use these functions for the expansion of the function x2(π −x).
First, we check that they are orthogonal:
Snm =
π
Z
0
χ∗
n (x)χm(x)dx =
π
Z
0
sinnx sinmx dx.

5.1 Vectors in Function Spaces
259
For n ̸= m this integral can be shown to vanish, either by symmetry considerations or by
consulting a table of integrals. To determine normalization, we need Snn; from symmetry
considerations, the integrand, sin2 nx = 1
2(1 −cos2nx), can be seen to have average value
1/2 over the range (0,π), leading to Snn = π/2 for all integer n. This means the χn are not
normalized, but can be made so if we multiply by √2/π. So our orthonormal basis will be
ϕn(x) =
 2
π
1/2
sinnx,
n = 1,2,3,....
(5.15)
To expand x2(π −x), we apply Eq. (5.2), which requires the evaluation of
an = ⟨ϕn|x2(π −x)⟩=
 2
π
1/2
π
Z
0
(sinnx) x2(π −x) dx,
(5.16)
for use in the expansion
x2(π −x) =
 2
π
1/2 ∞
X
n=0
an sinnx.
(5.17)
Evaluating cases of Eq. (5.16) by hand or using a computer for symbolic computation, we
have for the ﬁrst few an: a1 = 5.0132, a2 = −1.8300, a3 = 0.1857, a4 = −0.2350. The
convergence is not very fast.
■
Example 5.1.4
SPIN SPACE
A system of four spin- 1
2 particles in a triplet state has the following three linearly indepen-
dent spin functions:
χ1 = αβαα −βααα,
χ2 = αααβ −ααβα,
χ3 = αααβ + ααβα −αβαα −βααα.
The four symbols in each term of these expressions refer to the spin assignments of the
four particles, in numerical order.
The scalar product in the spin space has the form, for monomials,
⟨abcd|wxyz⟩= δawδbxδcyδdz,
meaning that the scalar product is unity if the two monomials are identical, and is zero if
they are not. Scalar products involving polynomials can be evaluated by expanding them
into sums of monomial products. It is easy to conﬁrm that this deﬁnition meets the require-
ments for a valid scalar product.
Our mission will be (1) verify that the χi are orthogonal; (2) convert them, if neces-
sary, to normalized form to make an orthonormal basis for the spin space; and (3) expand
the following triplet spin function as a linear combination of the orthonormal spin basis
functions:
χ0 = ααβα −αβαα.
The functions χ1 and χ2 are orthogonal, as they have no terms in common. Although χ1
and χ3 have two terms in common, they occur in sign combinations leading to a vanishing

260
Chapter 5 Vector Spaces
scalar product. The same observation applies to ⟨χ2|χ3⟩. However, none of the χi are nor-
malized. We ﬁnd ⟨χ1|χ1⟩= ⟨χ2|χ2⟩= 2, ⟨χ3|χ3⟩= 4, so an orthonormal basis would be
ϕ1 = 2−1/2χ1,
ϕ2 = 2−1/2χ2,
ϕ3 = 1
2 χ3.
Finally, we obtain the coefﬁcients for the expansion of χ0 by forming a1 = ⟨ϕ1|χ0⟩=
−1/
√
2, a2 = ⟨ϕ2|χ0⟩= −1/
√
2, and a3 = ⟨ϕ3|χ0⟩= 1. Thus, the desired expansion is
χ0 = −1
√
2
ϕ1 −1
√
2
ϕ2 + ϕ3.
■
Expansions and Scalar Products
If we have found the expansions of two functions,
f =
X
µ
aµϕµ
and
g =
X
ν
bνϕν,
then their scalar product can be written
⟨f |g⟩=
X
µν
a∗
µ bν⟨ϕµ|ϕν⟩.
If the ϕ set is orthonormal, the above reduces to
⟨f |g⟩=
X
µ
a∗
µ bµ.
(5.18)
In the special case g = f , this reduces to
⟨f | f ⟩=
X
µ
|aµ|2,
(5.19)
consistent with the requirement that ⟨f | f ⟩≥0, with equality only if f is zero “almost
everywhere.”
If we regard the set of expansion coefﬁcients aµ as the elements of a column vector
a representing f , with column vector b similarly representing g, Eqs. (5.18) and (5.19)
correspond to the matrix equations
⟨f |g⟩= a†b,
⟨f | f ⟩= a†a.
(5.20)
Note that by taking the adjoint of a, we both complex conjugate it and convert it into a row
vector, so that the matrix products in Eq. (5.20) collapse to scalars, as required.

5.1 Vectors in Function Spaces
261
Example 5.1.5
COEFFICIENT VECTORS
A set of functions that is orthonormal on 0 ≤x ≤π is
ϕn(x) =
r
2 −δn0
π
cosnx,
n = 0,1,2,....
First, let us expand in terms of this basis the two functions
ψ1 = cos3 x + sin2 x + cos x + 1
and
ψ2 = cos2 x −cos x.
We write the expansions as vectors a1 and a2 with components n = 0,...,3:
a1 =


⟨ϕ0|ψ1⟩
⟨ϕ1|ψ1⟩
⟨ϕ2|ψ1⟩
⟨ϕ3|ψ1⟩

,
a2 =


⟨ϕ0|ψ2⟩
⟨ϕ1|ψ2⟩
⟨ϕ2|ψ2⟩
⟨ϕ3|ψ2⟩

.
All components beyond n = 3 vanish and need not be shown. It is straightforward to
evaluate these scalar products. Alternatively, we can rewrite the ψi using trigonometric
identities, reaching the forms
ψ1 = cos3x
4
−cos2x
2
+ 7
4 cos x + 3
2,
ψ2 = cos2x
2
−cos x + 1
2.
These expressions are now easily recognized as equivalent to
ψ1 =
rπ
2
 
ϕ3
4 −ϕ2
2 + 7ϕ1
4
+ 3
√
2ϕ0
2
!
,
ψ2 =
rπ
2
 
ϕ2
2 −ϕ1 +
√
2ϕ0
2
!
,
so
a1 =
rπ
2


3
√
2/2
7/4
−1/2
1/4

,
a2 =
rπ
2


√
2/2
−1
1/2
0

.
We see from the above that the general formula for ﬁnding the coefﬁcients in an orthonor-
mal expansion, Eq. (5.12), is a systematic way of doing what sometimes can be carried out
in other ways.
We can now evaluate the scalar products ⟨ψi|ψ j⟩. Identifying these ﬁrst as matrix prod-
ucts that we then evaluate,
⟨ψ1|ψ1⟩= a†
1a1 = 63π
16 ,
⟨ψ1|ψ2⟩= a†
1a2 = −π
4 ,
⟨ψ2|ψ2⟩= a†
2a2 = 7π
8 .
■

262
Chapter 5 Vector Spaces
Bessel’s Inequality
Given a set of basis functions and the deﬁnition of a space, it is not necessarily assured that
the basis functions span the space (a property sometimes referred to as completeness). For
example, we might have a space deﬁned to be that containing all functions possessing a
scalar product of a given deﬁnition, while the basis functions have been speciﬁed by giving
their functional form. This issue is of some importance, because we need to know whether
an attempt to expand a function in a given basis can be guaranteed to converge to the correct
result. Totally general criteria are not available, but useful results have been obtained if
the function being expanded has, at worst, a ﬁnite number of ﬁnite discontinuities, and
results are accepted as “accurate” if deviations from the correct value occur only at isolated
points. Power series and trigonometric series have been proved complete for the expansion
of square integrable functions f (those for which ⟨f | f ⟩as deﬁned in Eq. (5.7) exists;
mathematicians identify such spaces by the designation L2). Also proved complete are the
orthonormal sets of functions that arise as the solutions to Hermitian eigenvalue problems.4
A not too practical test for completeness is provided by Bessel’s inequality, which states
that if a function f has been expanded in an orthonormal basis as P
n anϕn, then
⟨f | f ⟩≥
X
n
|an|2,
(5.21)
with the inequality occurring if the expansion of f is incomplete. The impracticality of
this as a completeness test is that one needs to apply it for all f before using it to claim
completeness of the space.
We establish Bessel’s inequality by considering
I =
*
f −
X
i
ai ϕi
 f −
X
j
a j ϕ j
+
≥0,
(5.22)
where I = 0 represents what is termed convergence in the mean, a criterion that per-
mits the integrand to deviate from zero at isolated points. Expanding the scalar product,
and eliminating terms that vanish because the ϕ are orthonormal, we arrive at Eq. (5.21),
with equality only resulting if the expansion converges to f . We note in passing that con-
vergence in the mean is a less stringent requirement than uniform convergence, but is
adequate for almost all physical applications of basis-set expansions.
Example 5.1.6
EXPANSION OF A DISCONTINUOUS FUNCTION
The functions cosnx (n = 0,1,2,...) and sinnx (n = 1,2,...) have (together) been
shown to form a complete set on the interval −π < x < π. Since this determination is
obtained subject to convergence in the mean, there is the possibility of deviation at iso-
lated points, thereby permitting the description of functions with isolated discontinuities.
4See R. Courant and D. Hilbert, Methods of Mathematical Physics (English translation), Vol. 1, New York: Interscience (1953),
reprinting, Wiley (1989), chapter 6, section 3.

5.1 Vectors in Function Spaces
263
We illustrate with the square-wave function
f (x) =



h
2,
0 < x < π
−h
2,
−π < x < 0.
(5.23)
The functions cosnx and sinnx are orthogonal on the expansion interval (with unit weight
in the scalar product), and the expansion of f (x) takes the form
f (x) = a0 +
∞
X
n=1
(an cosnx + bn sinnx).
Because f (x) is an odd function of x, all the an vanish, and we only need to compute
bn = 1
π
π
Z
−π
f (t)sinnt dt.
The factor 1/π preceding the integral arises because the expansion functions are not
normalized.
Upon substitution of ±h/2 for f (t), we ﬁnd
bn = h
nπ (1 −cosnπ) =



0,
n even,
2h
nπ ,
n odd.
Thus, the expansion of the square wave is
f (x) = 2h
π
∞
X
n=0
sin(2n + 1)x
2n + 1
.
(5.24)
To give an idea of the rate at which the series in Eq. (5.24) converges, some of its partial
sums are plotted in Fig. 5.1.
■
Expansions of Dirac Delta Function
Orthogonal expansions provide opportunities to develop additional representations of the
Dirac delta function. In fact, such a representation can be built from any complete set of
functions ϕn(x). For simplicity we assume the ϕn to be orthonormal with unit weight on
the interval (a,b), and consider the expansion
δ(x −t) =
∞
X
n=0
cn(t)ϕn(x),
(5.25)
where, as indicated, the coefﬁcients must be functions of t. From the rule for determining
the coefﬁcients, we have, for t also in the interval (a,b),
cn(t) =
b
Z
a
ϕ∗
n(x)δ(x −t)dx = ϕ∗
n(t),
(5.26)

264
Chapter 5 Vector Spaces
FIGURE 5.1
Expansion of square wave. Computed using Eq. (5.24) with summation
terminated after n = 4, 8, 12, and 20. Curves are at different vertical scales to
enhance visibility.
where the evaluation has used the deﬁning property of the delta function. Substituting this
result back into Eq. (5.25), we have
δ(x −t) =
∞
X
n=0
ϕ∗
n(t)ϕn(x).
(5.27)
This result is clearly not uniformly convergent at x = t. However, remember that it is not
to be used by itself, but has meaning only when it appears as part of an integrand. Note
also that Eq. (5.27) is only valid when x and t are within the range (a,b).
Equation (5.27) is called the closure relation for the Dirac delta function (with respect
to the ϕn) and obviously depends on the completeness of the ϕ set. If we apply Eq. (5.27)
to an arbitrary function F(t) that we assume to have the expansion F(t) = P
p cp ϕp(t),
we have
b
Z
a
F(t)δ(x −t)dt =
b
Z
a
dt
∞
X
p=0
cp ϕp(t)
∞
X
n=0
ϕ∗
n(t)ϕn(x)
=
∞
X
p=0
cp ϕp(x) = F(x),
(5.28)
which is the expected result. However, if we replace the integration limits (a,b) by (t1,t2)
such that a ≤t1 < t2 ≤b, we get a more general result that reﬂects the fact that our

5.1 Vectors in Function Spaces
265
0
20
40
60
80
0.2
1
0.8
0.6
0.4
FIGURE 5.2
Approximation at N = 80 to δ(t −x), Eq. (5.30), for t = 0.4.
representation of δ(x −t) is negligible except when x ≈t:
t2
Z
t1
F(t)δ(x −t)dt =
(F(x),
t1 < x < t2,
0,
x < t1 or x > t2.
(5.29)
Example 5.1.7
DELTA FUNCTION REPRESENTATION
To illustrate an expansion of the Dirac delta function in an orthonormal basis, take ϕn(x) =
√
2 sinnπx, which are orthonormal and complete on x = (0,1) for n = 1,2,.... Then the
Dirac delta function has representation, valid for 0 < x < 1, 0 < t < 1,
δ(x −t) = lim
N→∞
N
X
n=1
2sinnπt sinnπx.
(5.30)
Plotting this with N = 80 for t = 0.4 and 0 < x < 1 gives the result shown in Fig. 5.2. ■
Dirac Notation
Much of what we have discussed can be brought to a form that promotes clarity and
suggests possibilities for additional analysis by using a notational device invented by
P. A. M. Dirac. Dirac suggested that instead of just writing a function f , it be written
enclosed in the right half of an angle-bracket pair, which he named a ket. Thus f →| f ⟩,
ϕi →|ϕi⟩, etc. Then he suggested that the complex conjugates of functions be enclosed
in left half-brackets, which he named bras. An example of a bra is ϕ∗
i →⟨ϕi|. Finally, he

266
Chapter 5 Vector Spaces
suggested that when the sequence (bra followed by ket = bra+ket ∼bracket) is encoun-
tered, the pair should be interpreted as a scalar product (with the dropping of one of the two
adjacent vertical lines). As an initial example of the use of this notation, take Eq. (5.12),
which we now write as
| f ⟩=
X
j
a j|ϕ j⟩=
X
j
|ϕ j⟩⟨ϕ j| f ⟩=

X
j
|ϕ j⟩⟨ϕ j|

| f ⟩.
(5.31)
This notational rearrangement shows that we can view the expansion in the ϕ basis as the
insertion of a set of basis members in a way which, in sum, has no effect. If the sum is over
a complete set of ϕ j, the ket-bra sum in Eq. (5.31) will have no net effect when inserted
before any ket in the space, and therefore we can view the sum as a resolution of the
identity. To emphasize this, we write
1 =
X
j
|ϕ j⟩⟨ϕ j|.
(5.32)
Many expressions involving expansions in orthonormal sets can be derived by the insertion
of resolutions of the identity.
Dirac notation can also be applied to expressions involving vectors and matrices, where
it illuminates the parallelism between physical vector spaces and the function spaces here
under study. If a and b are column vectors and M is a matrix, then we can write |b⟩as a
synonym for b, we can write ⟨a| to mean a†, and then ⟨a|b⟩is interpreted as equivalent to
a†b, which (when the vectors are real) is matrix notation for the (scalar) dot product a · b.
Other examples are expressions such as
a = Mb ↔|a⟩= |Mb⟩= M|b⟩
or
a†Mb = (M†a)†b ↔⟨a|Mb⟩= ⟨M†a|b⟩.
Exercises
5.1.1
A function f (x) is expanded in a series of orthonormal functions
f (x) =
∞
X
n=0
anϕn(x).
Show that the series expansion is unique for a given set of ϕn(x). The functions ϕn(x)
are being taken here as the basis vectors in an inﬁnite-dimensional Hilbert space.
5.1.2
A function f (x) is represented by a ﬁnite set of basis functions ϕi(x),
f (x) =
N
X
i=1
ciϕi(x).
Show that the components ci are unique, that no different set c′
i exists.
Note. Your basis functions are automatically linearly independent. They are not neces-
sarily orthogonal.

5.1 Vectors in Function Spaces
267
5.1.3
A function f (x) is approximated by a power series Pn−1
i=0 ci xi over the interval [0,1].
Show that minimizing the mean square error leads to a set of linear equations
Ac = b,
where
Ai j =
1
Z
0
xi+ j dx =
1
i + j + 1,
i, j = 0,1,2,...,n −1
and
bi =
1
Z
0
xi f (x)dx,
i = 0,1,2,...,n −1.
Note. The Ai j are the elements of the Hilbert matrix of order n. The determinant of this
Hilbert matrix is a rapidly decreasing function of n. For n = 5, det A = 3.7 × 10−12 and
the set of equations Ac = b is becoming ill-conditioned and unstable.
5.1.4
In place of the expansion of a function F(x) given by
F(x) =
∞
X
n=0
anϕn(x),
with
an =
b
Z
a
F(x)ϕn(x)w(x)dx,
take the ﬁnite series approximation
F(x) ≈
m
X
n=0
cnϕn(x).
Show that the mean square error
b
Z
a
"
F(x) −
m
X
n=0
cnϕn(x)
#2
w(x)dx
is minimized by taking cn = an.
Note. The values of the coefﬁcients are independent of the number of terms in the ﬁnite
series. This independence is a consequence of orthogonality and would not hold for a
least-squares ﬁt using powers of x.
5.1.5
From Example 5.1.6,
f (x) =



h
2,
0 < x < π
−h
2,
−π < x < 0



= 2h
π
∞
X
n=0
sin(2n + 1)x
2n + 1
.

268
Chapter 5 Vector Spaces
(a)
Show that
π
Z
−π
h
f (x)
i 2
dx = π
2 h2 = 4h2
π
∞
X
n=0
(2n + 1)−2.
For a ﬁnite upper limit this would be Bessel’s inequality. For the upper limit ∞,
this is Parseval’s identity.
(b)
Verify that
π
2 h2 = 4h2
π
∞
X
n=0
(2n + 1)−2
by evaluating the series.
Hint. The series can be expressed in terms of the Riemann zeta function ζ(2) = π2/6.
5.1.6
Derive the Schwarz inequality from the identity


b
Z
a
f (x)g(x)dx


2
=
b
Z
a
h
f (x)
i 2
dx
b
Z
a
[
h
g(x)
i 2
dx
−1
2
b
Z
a
dx
b
Z
a
dy
h
f (x)g(y) −f (y)g(x)
i 2
.
5.1.7
Starting from I =
*
f −
X
i
ai ϕi
 f −
X
j
a j ϕ j
+
≥0,
derive Bessel’s inequality, ⟨f | f ⟩≥
X
n
|an|2.
5.1.8
Expand the function sinπx in a series of functions ϕi that are orthogonal (but not nor-
malized) on the range 0 ≤x ≤1 when the scalar product has deﬁnition
⟨f |g⟩=
1
Z
0
f ∗(x)g(x)dx.
Keep the ﬁrst four terms of the expansion. The ﬁrst four ϕi are:
ϕ0 = 1,
ϕ1 = 2x −1,
ϕ2 = 6x2 −6x + 1,
ϕ3 = 20x3 −30x2 + 12x −1.
Note. The integrals that are needed are the subject of Example 1.10.5.
5.1.9
Expand the function e−x in Laguerre polynomials Ln(x), which are orthonormal on the
range 0 ≤x < ∞with scalar product
⟨f |g⟩=
∞
Z
0
f ∗(x)g(x)e−x dx.

5.2 Gram-Schmidt Orthogonalization
269
Keep the ﬁrst four terms of the expansion. The ﬁrst four Ln(x) are
L0 = 1,
L1 = 1 −x,
L2 = 2 −4x + x2
2
,
L3 = 6 −18x + 9x2 −x3
6
.
5.1.10
The explicit form of a function f is not known, but the coefﬁcients an of its expan-
sion in the orthonormal set ϕn are available. Assuming that the ϕn and the members of
another orthonormal set, χn, are available, use Dirac notation to obtain a formula for
the coefﬁcients for the expansion of f in the χn set.
5.1.11
Using conventional vector notation, evaluate
X
j
|ˆe j⟩⟨ˆe j|a⟩, where a is an arbitrary vec-
tor in the space spanned by the ˆe j.
5.1.12
Letting a = a1ˆe1 + a2ˆe2 and b = b1ˆe1 + b2ˆe2 be vectors in R2, for what values of k, if
any, is
⟨a|b⟩= a1b1 −a1b2 −a2b1 + ka2b2
a valid deﬁnition of a scalar product?
5.2
GRAM-SCHMIDT ORTHOGONALIZATION
Crucial to carrying out the expansions and transformations under discussion is the avail-
ability of useful orthonormal sets of functions. We therefore proceed to the description of
a process whereby a set of functions that is neither orthogonal or normalized can be used
to construct an orthonormal set that spans the same function space. There are many ways
to accomplish this task. We present here the method called the Gram-Schmidt orthogo-
nalization process.
The Gram-Schmidt process assumes the availability of a set of functions χµ and an
appropriately deﬁned scalar product ⟨f |g⟩. We orthonormalize sequentially to form the
orthonormal functions ϕν, meaning we make the ﬁrst orthonormal function, ϕ0, from χ0,
the next, ϕ1, from χ0 and χ1, etc. If, for example, the χµ are powers xµ, the orthonormal
function ϕν will be a polynomial of degree ν in x. Because the Gram-Schmidt process is
often applied to powers, we have chosen to number both the χ and the ϕ sets starting from
zero (rather than 1).
Thus, our ﬁrst orthonormal function will simply be a normalized version of χ0.
Speciﬁcally,
ϕ0 =
χ0
⟨χ0|χ0⟩1/2 .
(5.33)
To check that Eq. (5.33) is correct, we form
⟨ϕ0|ϕ0⟩=
*
χ0
⟨χ0|χ0⟩1/2

χ0
⟨χ0|χ0⟩1/2
+
= 1.
Next, starting from ϕ0 and χ1, we form a function that is orthogonal to ϕ0. We use ϕ0 rather
than χ0 to be consistent with what we will do in later steps of the process. Thus, we write
ψ1 = χ1 −a1,0ϕ0.
(5.34)

270
Chapter 5 Vector Spaces
What we are doing here is the removal from χ1 of its projection onto ϕ0, leaving a remain-
der that will be orthogonal to ϕ0. Remembering that ϕ0 is normalized (of “unit length”),
that projection is identiﬁed as ⟨ϕ0|χ1⟩ϕ0, so that
a1,0 = ⟨ϕ0|χ1⟩.
(5.35)
In case Eq. (5.35) is not intuitively obvious, we can conﬁrm it by writing the requirement
that ψ1 be orthogonal to ϕ0:
⟨ϕ0|ψ1⟩=
D
ϕ0


χ1 −a1,0ϕ0
E
= ⟨ϕ0|χ1⟩−a1,0⟨ϕ0|ϕ0⟩= 0,
which, because ϕ0 is normalized, reduces to Eq. (5.35). The function ψ1 is not in general
normalized. To normalize it and thereby obtain ϕ1, we form
ϕ1 =
ψ1
⟨ψ1|ψ1⟩1/2 .
(5.36)
To continue further, we need to make, from ϕ0, ϕ1, and χ2, a function that is orthogonal
to both ϕ0 and ϕ1. It will have the form
ψ2 = χ2 −a0,2ϕ0 −a1,2ϕ1.
(5.37)
The last two terms of Eq. (5.37), respectively, remove from χ2 its projections on ϕ0 and ϕ1;
these projections are independent because ϕ0 and ϕ1 are orthogonal. Thus, either from our
knowledge of projections or by setting to zero the scalar products ⟨ϕi|ψ2⟩(i = 0 and 1),
we establish
a0,2 = ⟨ϕ0|χ2⟩,
a1,2 = ⟨ϕ1|χ2⟩.
(5.38)
Finally, we make ϕ2 = ψ2/⟨ψ2|ψ2⟩1/2.
The generalization for which the above is the ﬁrst few terms is that, given the prior
formation of ϕi, i = 0,...,n −1, the orthonormal function ϕn is obtained from χn by the
following two steps:
ψn = χn −
n−1
X
µ=0
⟨ϕµ|χn⟩ϕµ,
ϕn =
ψn
⟨ψn|ψn⟩1/2 .
(5.39)
Reviewing the above process, we note that different results would have been obtained if
we used the same set of χi, but simply took them in a different order. For example, if we
had started with χ3, one of our orthonormal functions would have been a multiple of χ3,
while the set we constructed yielded ϕ3 as a linear combination of χµ, µ = 0,1,2,3.
Example 5.2.1
LEGENDRE POLYNOMIALS
Let us form an orthonormal set, taking the χµ as xµ, and making the deﬁnition
⟨f |g⟩=
1
Z
−1
f ∗(x)g(x)dx.
(5.40)

5.2 Gram-Schmidt Orthogonalization
271
This scalar product deﬁnition will cause the members of our set to be orthogonal, with
unit weight, on the range (−1,1). Moreover, since the χµ are real, the complex conjugate
asterisk has no operational signiﬁcance here.
The ﬁrst orthonormal function, ϕ0, is
ϕ0(x) =
1
⟨1|1⟩1/2 =
1
"
1R
−1
dx
#1/2 = 1
√
2
.
To obtain ϕ1, we ﬁrst obtain ψ1 by evaluating
ψ1(x) = x −⟨ϕ0|x⟩ϕ0(x) = x,
where the scalar product vanishes because ϕ0 is an even function of x, whereas x is odd,
and the range of integration is even. We then ﬁnd
ϕ1(x) =
x
"
1R
−1
x2dx
#1/2 =
r
3
2 x.
The next step is less trivial. We form
ψ2(x) = x2 −⟨ϕ0|x2⟩ϕ0(x) −⟨ϕ1|x2⟩ϕ1(x) = x2 −
 1
√
2
 x2
 1
√
2

= x2 −1
3,
where we have used symmetry to set ⟨ϕ1|x2⟩to zero and evaluated the scalar product
 1
√
2
 x2

= 1
√
2
1
Z
−1
x2dx =
√
2
3 .
Then,
ϕ2(x) =
x2 −1
3
"
1R
−1
 x2 −1
3
2dx
#1/2 =
r
5
2
3
2 x2 −1
2

.
Continuation to one more orthonormal function yields
ϕ3(x) =
r
7
2
5
2 x3 −3
2 x

.
Reference to Chapter 15 will show that
ϕn(x) =
r
2n + 1
2
Pn(x),
(5.41)
where Pn(x) is the nth degree Legendre polynomial. Our Gram-Schmidt process provides
a possible but very cumbersome method of generating the Legendre polynomials; other,
more efﬁcient approaches exist.
■

272
Chapter 5 Vector Spaces
Table 5.1
Orthogonal Polynomials Generated by Gram-Schmidt Orthogonalization of
un(x) = xn, n = 0, 1, 2,... .
Polynomials
Scalar Products
Table
Legendre
1
Z
−1
Pn(x)Pm(x)dx = 2δmn/(2n + 1)
Table 15.1
Shifted Legendre
1
Z
0
P∗
n (x)P∗
m(x)dx = δmn/(2n + 1)
Table 15.2
Chebyshev I
1
Z
−1
Tn(x)Tm(x)
 1 −x2−1/2dx = δmnπ/(2 −δn0)
Table 18.4
Shifted Chebyshev I
1
Z
0
T ∗
n (x)T ∗
m(x)[x(1 −x)]−1/2dx = δmnπ/(2 −δn0)
Table 18.5
Chebyshev II
1
Z
−1
Un(x)Um(x)
 1 −x21/2dx = δmnπ/2
Table 18.4
Laguerre
∞
Z
0
Ln(x)Lm(x)e−xdx = δmn
Table 18.2
Associated Laguerre
∞
Z
0
Lk
n(x)Lk
m(x)e−xdx = δmn(n + k)!/n!
Table 18.3
Hermite
∞
Z
−∞
Hn(x)Hm(x)e−x2dx = 2nδmnπ1/2n!
Table 18.1
The intervals, weights, and conventional normalization can be deduced from the forms of the scalar products.
Tables of explicit formulas for the ﬁrst few polynomials of each type are included in the indicated tables
appearing in Chapters 15 and 18 of this book.
The Legendre polynomials are, except for sign and scale, uniquely deﬁned by the Gram-
Schmidt process, the use of successive powers of x, and the deﬁnition adopted for the
scalar product. By changing the scalar product deﬁnition (different weight or range), we
can generate other useful sets of orthogonal polynomials. A number of these are presented
in Table 5.1. For various reasons most of these polynomial sets are not normalized to
unity. The scalar product formulas in the table give the conventional normalizations, and
are those of the explicit formulas referenced in the table.
Orthonormalizing Physical Vectors
The Gram-Schmidt process also works for ordinary vectors that are simply given by their
components, it being understood that the scalar product is just the ordinary dot product.

5.2 Gram-Schmidt Orthogonalization
273
Example 5.2.2
ORTHONORMALIZING A 2-D MANIFOLD
A 2-D manifold (subspace) in 3-D space is deﬁned by the two vectors a1 = ˆe1 + ˆe2 −2ˆe3
and a2 = ˆe1 + 2ˆe2 −3ˆe3. In Dirac notation, these vectors (written as column matrices) are
|a1⟩=


1
1
−2

,
|a2⟩=


1
2
−3

.
Our task is to span this manifold with an orthonormal basis.
We proceed exactly as for functions: Our ﬁrst orthonormal basis vector, which we call
b1, will be a normalized version of a1, and therefore formed as
|b1⟩=
a1
⟨a1|a1⟩1/2 =
1
61/2 |a1⟩=
1
61/2


1
1
−2

.
An unnormalized version of a second orthonormal function will have the form
|b′
2⟩= |a2⟩−⟨b1|a2⟩|b1⟩= |a2⟩−
9
61/2 |b1⟩=


−1/2
1/2
0

.
Normalizing, we reach
|b2⟩=
b′
2
⟨b′
2|b′
2⟩1/2 = 1
√
2


−1
1
0

.
■
Exercises
For the Gram-Schmidt constructions in Exercises 5.2.1 through 5.2.6, use a scalar prod-
uct of the form given in Eq. (5.7) with the speciﬁed interval and weight.
5.2.1
Following the Gram-Schmidt procedure, construct a set of polynomials P∗
n (x) orthog-
onal (unit weighting factor) over the range [0,1] from the set [1, x, x2, ...]. Scale so
that P∗
n (1) = 1.
ANS.
P∗
n (x) = 1,
P∗
1 (x) = 2x −1,
P∗
2 (x) = 6x2 −6x + 1,
P∗
3 (x) = 20x3 −30x2 + 12x −1.
These are the ﬁrst four shifted Legendre polynomials.
Note. The “*” is the standard notation for “shifted”: [0,1] instead of [−1,1]. It does not
mean complex conjugate.
5.2.2
Apply the Gram-Schmidt procedure to form the ﬁrst three Laguerre polynomials:
un(x) = xn,
n = 0,1,2,...,
0 ≤x < ∞,
w(x) = e−x.

274
Chapter 5 Vector Spaces
The conventional normalization is
∞
Z
0
Lm(x)Ln(x)e−xdx = δmn.
ANS.
L0 = 1,
L1 = (1 −x),
L2 = 2 −4x + x2
2
.
5.2.3
You are given
(a)
a set of functions un(x) = xn, n = 0,1,2,... ,
(b)
an interval (0,∞),
(c)
a weighting function w(x) = xe−x. Use the Gram-Schmidt procedure to construct
the ﬁrst three orthonormal functions from the set un(x) for this interval and this
weighting function.
ANS.
ϕ0(x) = 1,
ϕ1(x) = (x −2)/
√
2,
ϕ2(x) = (x2 −6x + 6)/2
√
3.
5.2.4
Using the Gram-Schmidt orthogonalization procedure, construct the lowest three
Hermite polynomials:
un(x) = xn,
n = 0,1,2,...,
−∞< x < ∞,
w(x) = e−x2.
For this set of polynomials the usual normalization is
∞
Z
−∞
Hm(x)Hn(x)w(x)dx = δmn2mm!π1/2.
ANS.
H0 = 1,
H1 = 2x,
H2 = 4x2 −2.
5.2.5
Use the Gram-Schmidt orthogonalization scheme to construct the ﬁrst three Chebyshev
polynomials (type I):
un(x) = xn,
n = 0,1,2,...,
−1 ≤x ≤1,
w(x) = (1 −x2)−1/2.
Take the normalization
1
Z
−1
Tm(x)Tn(x)w(x)dx = δmn



π,
m = n = 0,
π
2 ,
m = n ≥1.
Hint. The needed integrals are given in Exercise 13.3.2.
ANS.
T0 = 1,
T1 = x,
T2 = 2x2 −1,
(T3 = 4x3 −3x).
5.2.6
Use the Gram-Schmidt orthogonalization scheme to construct the ﬁrst three Chebyshev
polynomials (type II):
un(x) = xn,
n = 0,1,2,...,
−1 ≤x ≤1,
w(x) = (1 −x2)+1/2.
Take the normalization to be
1
Z
−1
Um(x)Un(x)w(x)dx = δmn
π
2 .

5.3 Operators
275
Hint.
1
Z
−1
(1 −x2)1/2x2n dx = π
2 × 1 · 3 · 5···(2n −1)
4 · 6 · 8···(2n + 2),
n = 1,2,3,...
= π
2 ,
n = 0.
ANS.
U0 = 1,
U1 = 2x,
U2 = 4x2 −1.
5.2.7
As a modiﬁcation of Exercise 5.2.5, apply the Gram-Schmidt orthogonalization proce-
dure to the set un(x) = xn, n = 0,1,2,..., 0 ≤x < ∞. Take w(x) to be exp(−x2).
Find the ﬁrst two nonvanishing polynomials. Normalize so that the coefﬁcient of the
highest power of x is unity. In Exercise 5.2.5, the interval (−∞,∞) led to the Hermite
polynomials. The functions found here are certainly not the Hermite polynomials.
ANS.
ϕ0 = 1,
ϕ1 = x −π−1/2.
5.2.8
Form a set of three orthonormal vectors by the Gram-Schmidt process using these input
vectors in the order given:
c1 =


1
1
1

,
c2 =


1
1
2

,
c3 =


1
0
2

.
5.3
OPERATORS
An operator is a mapping between functions in its domain (those to which it can be
applied) and functions in its range (those it can produce). While the domain and the range
need not be in the same space, our concern here is for operators whose domain and range
are both in all or part of the same Hilbert space. To make this discussion more concrete,
here are a few examples of operators:
•
Multiplication by 2: Converts f into 2 f ;
•
For a space containing algebraic functions of a variable x, d/dx: Converts f (x) into
d f/dx;
•
An integral operator A deﬁned by A f (x) =
R
G(x, x′) f (x′)dx′: A special case of this
is a projection operator |ϕi⟩⟨ϕi|, which converts f into ⟨ϕi| f ⟩ϕi.
In addition to the abovementioned restriction on domain and range, we also for our present
purposes restrict attention to operators that are linear, meaning that if A and B are linear
operators, f and g functions, and k a constant, then
(A + B) f = Af + B f,
A( f + g) = Af + Ag,
Ak = k A.
For both electromagnetic theory and quantum mechanics, an important class of operators
are differential operators, those that include differentiation of the functions to which they
are applied. These operators arise when differential equations are written in operator form;

276
Chapter 5 Vector Spaces
for example, the operator
L(x) =
 1 −x2 d2
dx2 −2x d
dx
enables us to write Legendre’s differential equation,
 1 −x2d2y
dx2 −2x dy
dx + λy = 0,
in the form L(y)y = −λy. When no confusion thereby results, this can be shortened to
Ly = −λy.
Commutation of Operators
Because differential operators act on the function(s) to their right, they do not necessarily
commute with other operators containing the same independent variable. This fact makes
it useful to consider the commutator of operators A and B,
[A, B] = AB −B A.
(5.42)
We can often reduce AB −B A to a simpler operator expression. When we write an
operator equation, its meaning is that the operator on the left-hand side of the equation
produces the same effect on every function in its domain as is produced by the opera-
tor on the right-hand side. Let’s illustrate this point by evaluating the commutator [x, p],
where p = −i d/dx. The imaginary unit i and the name p appear because this operator
is that corresponding in quantum mechanics to momentum (in a system of units such that
¯h = h/2π = 1). The operator x stands for multiplication by x.
To carry out the evaluation, we apply [x, p] to an arbitrary function f (x). Inserting the
explicit form of p, we have
[x, p] f (x) = (xp −px) f (x) = −ix d f (x)
dx
−

−i d
dx

x f (x)

= −ix f ′(x) + i

f (x) + x f ′(x)

= i f (x),
indicating that
[x, p] = i.
(5.43)
As indicated before, this means [x, p] f (x) = i f (x) for all f.
We can carry out various algebraic manipulations on commutators. In general, if A, B,
C are operators and k is a constant,
[A, B] = −[B, A],
[A, B + C] = [A, B] + [A,C],
k[A, B] = [k A, B] = [A,kB].
(5.44)

5.3 Operators
277
Example 5.3.1
OPERATOR MANIPULATION
Given [x, p], we can simplify the commutator [x, p2]. We write, being careful about the
operator ordering and using Eq. (5.43),
[x, p2] = xp2 −pxp + pxp −p2x = [x, p]p + p[x, p] = 2i p,
(5.45)
a result also obtainable from
x

−d2
dx2

f (x) −

−d2
dx2

x f (x) = 2 f ′(x) = 2i

−i d
dx

f (x).
However, note that Eq. (5.45) follows solely from the validity of Eq. (5.43), and will apply
to any quantities x and p that satisfy that commutation relation, whether or not we are
operating with ordinary functions and their derivatives. Put another way, if x and p are
operators in some abstract Hilbert space and all we know about them is Eq. (5.43), we may
still conclude that Eq. (5.45) is also valid.
■
Identity, Inverse, Adjoint
An operator that is generally available is the identity operator, namely one that leaves
functions unchanged. Depending on the context, this operator will be denoted either I or
simply 1. Some, but not all operators will have an inverse, namely an operator that will
“undo” its effect. Letting A−1 denote the inverse of A, if A−1 exists, it will have the
property
A−1A = AA−1 = 1.
(5.46)
Associated with many operators will be another operator, called its adjoint and denoted
A†, which will be such that for all functions f and g in the Hilbert space,
⟨f |Ag⟩= ⟨A† f |g⟩.
(5.47)
Thus, we see that A† is an operator that, applied to the left member of any scalar product,
produces the same result as is obtained if A is applied to the right member of the same
scalar product. Equation (5.47) is, in essence, the deﬁning equation for A†.
Depending on the speciﬁc operator A, and the deﬁnitions in use of the Hilbert space
and the scalar product, A† may or may not be equal to A. If A = A†, A is referred to as
self-adjoint, or equivalently, Hermitian. If A† = −A, A is called anti-Hermitian. This
deﬁnition is worth emphasis:
If
H† = H,
H is Hermitian.
(5.48)
Another situation of frequent occurrence is that the adjoint of an operator is equal to
its inverse, in which case the operator is called unitary. A unitary operator U is therefore
deﬁned by the following statement:
If
U† = U−1,
U is unitary.
(5.49)
In the special case that U is both real and unitary, it is called orthogonal.

278
Chapter 5 Vector Spaces
The reader will doubtless note that the nomenclature for operators is similar to that
previously introduced for matrices. This is not accidental; we shall shortly develop corre-
spondences between operator and matrix expressions.
Example 5.3.2
FINDING THE ADJOINT
Consider an operator A = x(d/dx) whose domain is the Hilbert space whose members f
have a ﬁnite value of ⟨f | f ⟩when the scalar product has deﬁnition
⟨f |g⟩=
∞
Z
−∞
f ∗(x)g(x)dx.
This space is often referred to as L2 on (−∞,∞). Starting from ⟨f |A g⟩, we integrate by
parts as needed to move the operator out of the right half of the scalar product. Because f
and g must vanish at ±∞, the integrated terms vanish, and we get
⟨f |A g⟩=
∞
Z
−∞
f ∗x dg
dx dx =
∞
Z
−∞
 x f ∗dg
dx dx = −
∞
Z
−∞
d
 x f ∗
dx
g dx
=

−
 d
dx

x f
 g

.
We see from the above that A† = −(d/dx)x, from which we can ﬁnd A† = −A −1.
This A is clearly neither Hermitian nor unitary (with the speciﬁed deﬁnition of the scalar
product).
■
Example 5.3.3
ADJOINT DEPENDS ON SCALAR PRODUCT
For the Hilbert space and scalar product of Example 5.3.2, an integration by parts easily
establishes that an operator A = −i(d/dx) is self-adjoint, i.e., A† = A. But now let’s
consider the same operator A, but for the L2 space with −1 ≤x ≤1 (and with a scalar
product of the same form, but with integration limits ±1). In this space, the integrated
terms from the integration by parts do not vanish, but we can incorporate them into an
operator on the left half of the scalar product by adding delta-function terms:

f
−i d
dx
 g

= −i f ∗g

1
−1 +
1
Z
−1

−i d f
dx
∗
g dx
=
1
Z
−1
h
iδ(x −1) −iδ(x + 1) −i d
dx
i
f (x)
∗
g(x)dx.
In this truncated space the operator A is not self-adjoint.
■

5.3 Operators
279
Basis Expansions of Operators
Because we are dealing only with linear operators, we can write the effect of an operator on
an arbitrary function if we know the result of its action on all members of a basis spanning
our Hilbert space. In particular, assume that the action of an operator A on member ϕµ of
an orthonormal basis has the result, also expanded in that basis,
Aϕµ =
X
ν
aνµϕν.
(5.50)
Assuming this form for the result of operation with A is not a major restriction; all it says
is that the result is in our Hilbert space. Formally, the coefﬁcients aνµ can be obtained by
taking scalar products:
aνµ = ⟨ϕν|Aϕµ⟩= ⟨ϕν|A|ϕµ⟩.
(5.51)
Following common usage, we have inserted an optional (operationally meaningless) verti-
cal line between A and ϕµ. This notation has the aesthetic effect of separating the operator
from the two functions entering the scalar product, and also emphasizes the possibility
that instead of evaluating the scalar product as written, we can without changing its value
evaluate it using the adjoint of A, as ⟨A†ϕν|ϕµ⟩.
We now apply Eq. (5.50) to a function ψ whose expansion in the ϕ basis is
ψ =
X
µ
cµϕµ,
cµ = ⟨ϕµ|ψ⟩.
(5.52)
The result is
Aψ =
X
µ
cµ Aϕµ =
X
µ
cµ
X
ν
aνµϕν =
X
ν
 X
µ
aνµcµ
!
ϕν.
(5.53)
If we think of Aψ as a function χ in our Hilbert space, with expansion
χ =
X
ν
bνϕν,
(5.54)
we then see from Eq. (5.53) that the coefﬁcients bν are related to cµ and aνµ in a way
corresponding to matrix multiplication. To make this more concrete,
•
Deﬁne c as a column vector with elements ci, representing the function ψ,
•
Deﬁne b as a column vector with elements bi, representing the function χ,
•
Deﬁne A as a matrix with elements ai j, representing the operator A,
•
The operator equation χ = Aψ then corresponds to the matrix equation b = Ac.
In other words, the expansion of the result of applying A to any function ψ can be com-
puted (by matrix multiplication) from the expansions of A and ψ. In effect, that means that
the operator A can be thought of as completely deﬁned by its matrix elements, while ψ
and χ = Aψ are completely characterized by their coefﬁcients.

280
Chapter 5 Vector Spaces
We obtain an interesting expression if we introduce Dirac notation for all the quantities
entering Eq. (5.53). We then have, moving the ket representing ϕν to the left,
Aψ =
X
νµ
|ϕν⟩⟨ϕν|A|ϕµ⟩⟨ϕµ|ψ⟩,
(5.55)
which leads us to identify A as
A =
X
νµ
|ϕν⟩⟨ϕν|A|ϕµ⟩⟨ϕµ|,
(5.56)
which we note is nothing other than A, multiplied on each side by a resolution of the
identity, of the form given in Eq. (5.32).
Another interesting observation results if we reintroduce into Eq. (5.56) the coefﬁcient
aνµ, bringing us to
A =
X
νµ
|ϕν⟩aνµ⟨ϕµ|.
(5.57)
Here we have the general form for an operator A, with a speciﬁc behavior that is deter-
mined entirely by the set of coefﬁcients aνµ. The special case A = 1 has already been seen
to be of the form of Eq. (5.57) with aνµ = δνµ.
Example 5.3.4
MATRIX ELEMENTS OF AN OPERATOR
Consider the expansion of the operator x in a basis consisting of functions ϕn(x) =
Cn Hn(x)e−x2/2, n = 0,1,..., where the Hn are Hermite polynomials, with scalar product
⟨f |g⟩=
∞
Z
−∞
f ∗(x)g(x)dx.
From Table 5.1, we can see that the ϕn are orthogonal and that they will also be normalized
if Cn = (2nn!√π)−1/2. The matrix elements of x, which we denote xνµ and are written
collectively as a matrix denoted x, are given by
xνµ = ⟨ϕν|x|ϕµ⟩= CνCµ
∞
Z
−∞
Hν(x) x Hµ(x)e−x2 dx.
The integral leading to xνµ can be evaluated in general by using the properties of the
Hermite polynomials, but our present purposes are adequately served by a straightfor-
ward case-by-case computation. From the table of Hermite polynomials in Table 18.1, we
identify
H0 = 1,
H1 = 2x,
H2 = 4x2 −2,
H3 = 8x3 −12x,
...,
and we take note of the integration formula
In =
∞
Z
−∞
x2ne−x2 dx = (2n −1)!!√π
2n
.

5.3 Operators
281
Making use of the parity (even/odd symmetry) of the Hn and the fact that the matrix x
is symmetric, we note that many matrix elements are either zero or equal to others. We
illustrate with the explicit computation of one matrix element, x12:
x12 = C1C2
∞
Z
−∞
(2x)x(4x2 −2)e−x2 dx = C1C2
∞
Z
−∞
 8x4 −4x2
e−x2 dx
= C1C2
h
8I2 −4I1
i
= 1.
Evaluating other matrix elements, we ﬁnd that x, the matrix of x, has the form
x =


0
√
2/2
0
0
···
√
2/2
0
1
0
···
0
1
0
√
6/2
···
0
0
√
6/2
0
···
···
···
···
···
···


.
(5.58)
■
Basis Expansion of Adjoint
We now look at the adjoint of our operator A as an expansion in the same basis. Our
starting point is the deﬁnition of the adjoint. For arbitrary functions ψ and χ,
⟨ψ|A|χ⟩= ⟨A†ψ|χ⟩= ⟨χ|A†|ψ⟩∗,
where we reached the last member of the equation by using the complex conjugation prop-
erty of the scalar product. This is equivalent to
⟨χ|A†|ψ⟩= ⟨ψ|A|χ⟩∗=
"
⟨ψ|
 X
νµ
|ϕν⟩aνµ⟨ϕµ|
!
|χ⟩
#∗
=
X
νµ
⟨ψ|ϕν⟩∗a∗
νµ⟨ϕµ|χ⟩∗
=
X
νµ
⟨χ|ϕµ⟩a∗
νµ⟨ϕν|ψ⟩,
(5.59)
where in the last line we have again used the scalar product complex conjugation property
and have reordered the factors in the sum.
We are now in a position to note that Eq. (5.59) corresponds to
A† =
X
νµ
|ϕν⟩a∗
µν⟨ϕµ|.
(5.60)
In writing Eq. (5.60) we have changed the dummy indices to make the formula as similar
as possible to Eq. (5.57). It is important to note the differences: The coefﬁcient aνµ of
Eq. (5.57) has been replaced by a∗
µν, so we see that the index order has been reversed and

282
Chapter 5 Vector Spaces
the complex conjugate taken. This is the general recipe for forming the basis set expansion
of the adjoint of an operator. The relation between the matrix elements of A and of A†
is exactly that which relates a matrix A to its adjoint A†, showing that the similarity in
nomenclature is purposeful. We thus have the important and general result:
•
If A is the matrix representing an operator A, then the operator A†, the adjoint of A, is
represented by the matrix A†.
Example 5.3.5
ADJOINT OF SPIN OPERATOR
Consider a spin space spanned by functions we call α and β, with a scalar product com-
pletely deﬁned by the equations ⟨α|α⟩= ⟨β|β⟩= 1, ⟨α|β⟩= 0. An operator B is such
that
B α = 0,
B β = α.
Taking all possible linearly independent scalar products, this means that
⟨α|Bα⟩= 0,
⟨β|Bα⟩= 0,
⟨α|Bβ⟩= 1,
⟨β|Bβ⟩= 0.
It is therefore necessary that
⟨B†α|α⟩= 0,
⟨B†β|α⟩= 0,
⟨B†α|β⟩= 1,
⟨B†β|β⟩= 0,
which means that B† is an operator such that
B† α = β,
B† β = 0.
The above equations correspond to the matrices
B =
0
1
0
0

,
B† =
0
0
1
0

.
We see that B† is the adjoint of B, as required.
■
Functions of Operators
Our ability to represent operators by matrices also implies that the observations made in
Chapter 3 regarding functions of matrices also apply to linear operators. Thus, we have
deﬁnite meanings for quantities such as exp(A), sin(A), or cos(A), and can also apply to
operators various identities involving matrix commutators. Important examples include the
Jacobi identity (Exercise 2.2.7), and the Baker-Hausdorff formula, Eq. (2.85).
Exercises
5.3.1
Show (without introducing matrix representations) that the adjoint of the adjoint of an
operator restores the original operator, i.e., that (A†)† = A.

5.4 Self-Adjoint Operators
283
5.3.2
U and V are two arbitrary operators. Without introducing matrix representations of
these operators, show that
(UV )† = V †U†.
Note the resemblance to adjoint matrices.
5.3.3
Consider a Hilbert space spanned by the three functions ϕ1 = x1, ϕ2 = x2, ϕ3 = x3, and
a scalar product deﬁned by ⟨xν|xµ⟩= δνµ.
(a)
Form the 3 × 3 matrix of each of the following operators:
A1 =
3
X
i=1
xi
 ∂
∂xi

,
A2 = x1
 ∂
∂x2

−x2
 ∂
∂x1

.
(b)
Form the column vector representing ψ = x1 −2x2 + 3x3.
(c)
Form the matrix equation corresponding to χ = (A1 −A2)ψ and verify that the
matrix equation reproduces the result obtained by direct application of A1 −A2
to ψ.
5.3.4
(a)
Obtain the matrix representation of A = x(d/dx) in a basis of Legendre polyno-
mials, keeping terms through P3. Use the orthonormal forms of these polynomials
as given in 5.2.1 and the scalar product deﬁned there.
(b)
Expand x3 in the orthonormal Legendre polynomial basis.
(c)
Verify that Ax3 is given correctly by its matrix representation.
5.4
SELF-ADJOINT OPERATORS
Operators that are self-adjoint (Hermitian) are of particular importance in quantum
mechanics because observable quantities are associated with Hermitian operators. In
particular, the average value of an observable A in a quantum mechanical state described
by any normalized wave function ψ is given by the expectation value of A, deﬁned as
⟨A⟩= ⟨ψ|A|ψ⟩.
(5.61)
This, of course, only makes sense if it can be assured that ⟨A⟩is real, even if ψ and/or
A is complex. Using the fact that A is postulated to be Hermitian, we take the complex
conjugate of ⟨A⟩:
⟨A⟩∗= ⟨ψ|A|ψ⟩∗= ⟨Aψ|ψ⟩,
which reduces to ⟨A⟩because A is self-adjoint.
We have already seen that if A and A† are expanded in a basis, the matrix A† must be
the matrix adjoint of the matrix A. This means that the coefﬁcients in its expansion must
satisfy
aνµ = a∗
µν
(coefﬁcients of self-adjoint A).
(5.62)

284
Chapter 5 Vector Spaces
Thus, we have the nearly self-evident result: A matrix representing a Hermitian operator
is a Hermitian matrix. It is also obvious from Eq. (5.62) that the diagonal elements of a
Hermitian matrix (which are expectation values for the basis functions) are real.
We can easily verify from basis expansions that ⟨A⟩must be real. Letting c be the vector
of expansion coefﬁcients of ψ in the basis for which aνµ are the matrix elements of A, then
⟨A⟩= ⟨ψ|A|ψ⟩=
*X
ν
cνϕν
 A

X
µ
cµϕµ
+
=
X
νµ
c∗
ν⟨ϕν|A|ϕµ⟩cµ
=
X
νµ
c∗
νaνµcµ = c†Ac,
which reduces, as it must, to a scalar. Because A is a self-adjoint matrix, c†Ac is easily seen
to be a self-adjoint 1×1 matrix, i.e., a real scalar (use the facts that (BAC)† = C†A†B† and
that A† = A).
Example 5.4.1
SOME SELF-ADJOINT OPERATORS
Consider the operators x and p introduced earlier, with a scalar product of deﬁnition
⟨f |g⟩=
∞
Z
−∞
f ∗(x)g(x)dx,
(5.63)
where our Hilbert space is the set of all functions f for which ⟨f | f ⟩exists (i.e., ⟨f | f ⟩is
ﬁnite). This is the L2 space on the interval (−∞,∞). To test whether x is self-adjoint, we
compare ⟨f |xg⟩and ⟨x f |g⟩. Writing these out as integrals, we consider
∞
Z
−∞
f ∗(x)x g(x)dx
vs.
∞
Z
−∞
[x f (x)]∗g(x)dx.
Because the order of ordinary functions (including x) can be changed without affecting the
value of an integral, and because x is inherently real, these two expressions are equal and
x is self-adjoint.
Turning next to p = −i(d/dx), the comparison we must make is
∞
Z
−∞
f ∗(x)

−i d g(x)
dx

dx
vs.
∞
Z
−∞

−i d f (x)
dx
∗
g(x)dx.
(5.64)
We can bring these expressions into better correspondence if we integrate the ﬁrst by parts,
differentiating f ∗(x) and integrating dg(x)/dx. Doing so, the ﬁrst expression above be-
comes
∞
Z
−∞
f ∗(x)

−i d g(x)
dx

dx = −i f ∗(x)g(x)

∞
−∞−
∞
Z
−∞
d f (x)
dx
∗h
−ig(x)
i
dx.

5.4 Self-Adjoint Operators
285
The boundary terms to be evaluated at ±∞must vanish because ⟨f | f ⟩and ⟨g|g⟩are ﬁnite,
which assures also (from the Schwarz inequality) that ⟨f |g⟩is ﬁnite as well. Upon moving
i within the complex conjugate in the remaining integral, we verify agreement with the
second expression in Eq. (5.64). Thus both x and p are self-adjoint. Note that if p had not
contained the factor i, it would not have been self-adjoint, as we obtained a needed sign
change when i was moved within the scope of the complex conjugate.
■
Example 5.4.2
EXPECTATION VALUE OF p
Because p, though Hermitian, is also imaginary, consider what happens when we compute
its expectation value for a wave function of the form ψ(x) = eiθ f (x), where f (x) is a
real L2 wave function and θ is a real phase angle. Using the scalar product as deﬁned in
Eq. (5.63), and remembering that p = −i(d/dx), we have
⟨p⟩= −i
∞
Z
−∞
f (x)d f (x)
dx
dx = −i
2
∞
Z
−∞
d
dx
h
f (x)
i 2
dx
= −i
2
h
f (+∞)2 −f (−∞)2i
= 0.
As shown, this integral vanishes because f (x) = 0 at ±∞(this is fortunate because expec-
tation values must be real). This result corresponds to the well-known property that wave
functions that describe time-dependent phenomena (nonzero momentum) cannot either be
real or real except for a constant (complex) phase factor.
■
The relations between operators and their adjoints provide opportunities for rearrange-
ments of operator expressions that may facilitate their evaluation. Some examples follow.
Example 5.4.3
OPERATOR EXPRESSIONS
(a)
Suppose we wish to evaluate ⟨(x2 + p2)ψ|ϕ⟩, with ψ of a complicated functional
form that might be unpleasant to differentiate (as required to apply p2), whereas ϕ is
simple. Because x is self-adjoint, so also is x2:
⟨x2ψ|ϕ⟩= ⟨xψ|xϕ⟩= ⟨ψ|x2ϕ⟩.
The same is true of p2, so ⟨(x2 + p2)ψ|ϕ⟩= ⟨ψ|(x2 + p2)ϕ⟩.
(b)
Look next at ⟨(x + ip)ψ|(x + ip)ψ⟩, which is the expression to be evaluated if we
want the norm of (x + ip)ψ. Note that x + ip is not self-adjoint, but has adjoint
x −ip. Our norm rearranges to
⟨(x + ip)ψ|(x + ip)ψ⟩= ⟨ψ|(x −ip)(x + ip)|ψ⟩
= ⟨ψ|x2 + p2 + i(xp −px)|ψ⟩
= ⟨ψ|x2 + p2 + i(i)|ψ⟩= ⟨ψ|x2 + p2 −1|ψ⟩.

286
Chapter 5 Vector Spaces
To reach the last line of the above equation, we recognized the commutator [x, p] = i,
as established in Eq. (5.43).
(c)
Suppose that A and B are self-adjoint. What can we say about the self-adjointness of
AB? Consider
⟨ψ|AB|ϕ⟩= ⟨Aψ|B|ϕ⟩= ⟨B Aψ|ϕ⟩.
Note that because we moved A to the left ﬁrst (with no dagger needed because it is
self-adjoint), it is part of what the subsequently moved B must operate on. So we see
that the adjoint of AB is B A. We conclude that AB is only self-adjoint if A and B
commute (so that B A = AB). Note that if A and B were not individually self-adjoint,
their commutation would not be sufﬁcient to make AB self-adjoint.
■
Exercises
5.4.1
(a)
A is a non-Hermitian operator. Show that the operators A + A† and i(A −A†) are
Hermitian.
(b)
Using the preceding result, show that every non-Hermitian operator may be
written as a linear combination of two Hermitian operators.
5.4.2
Prove that the product of two Hermitian operators is Hermitian if and only if the two
operators commute.
5.4.3
A and B are noncommuting quantum mechanical operators, and C is given by the
formula
AB −B A = iC.
Show that C is Hermitian. Assume that appropriate boundary conditions are satisﬁed.
5.4.4
The operator L is Hermitian. Show that ⟨L2⟩≥0, meaning that for all ψ in the space in
which L is deﬁned, ⟨ψ|L2|ψ⟩≥0.
5.4.5
Consider a Hilbert space whose members are functions deﬁned on the surface of the
unit sphere, with a scalar product of the form
⟨f |g⟩=
Z
d f ∗g,
where d is the element of solid angle. Note that the total solid angle of the sphere is
4π. We work here with the three functions ϕ1 = Cx/r, ϕ2 = Cy/r, ϕ3 = Cz/r, with C
assigned a value that makes the ϕi normalized.
(a)
Find C, and show that the ϕi are also mutually orthogonal.
(b)
Form the 3 × 3 matrices of the angular momentum operators
Lx = −i

y ∂
∂z −z ∂
∂y

,
L y = −i

z ∂
∂x −x ∂
∂z

,
Lz = −i

x ∂
∂y −y ∂
∂x

.

5.5 Unitary Operators
287
(c)
Verify that the matrix representations of the components of L satisfy the angular
momentum commutator [Lx, L y] = i Lz.
5.5
UNITARY OPERATORS
One of the reasons unitary operators are important in physics is that they can be used to
describe transformations between orthonormal bases. This property is the generalization
to the complex domain of the rotational transformations of ordinary (physical) vectors that
we analyzed in Chapter 3.
Unitary Transformations
Suppose we have a function ψ that has been expanded in the orthonormal basis ϕ:
ψ =
X
µ
cµϕµ =
 X
µ
|ϕµ⟩⟨ϕµ|
!
|ψ⟩.
(5.65)
We now wish to convert this expansion to a different orthonormal basis, with functions
ϕ′
ν. A possible starting point is to recognize that each of the original basis functions can be
expanded in the primed basis. We can obtain the expansion by inserting a resolution of the
identity in the primed basis:
ϕµ =
X
ν
uνµϕ′
ν =
 X
ν
|ϕ′
ν⟩⟨ϕ′
ν|
!
|ϕµ⟩=
X
ν
⟨ϕ′
ν|ϕµ⟩ϕ′
ν.
(5.66)
Comparing the second and fourth members of this equation, we identify uνµ as the ele-
ments of a matrix U:
uνµ = ⟨ϕ′
ν|ϕµ⟩.
(5.67)
Note how the use of resolutions of the identity makes these formulas obvious, and that
Eqs. (5.65) to (5.67) are only valid because the ϕµ and the ϕ′
ν are complete orthonormal
sets.
Inserting the expansion for ϕµ from Eq. (5.66) into Eq. (5.65), we reach
ψ =
X
µ
cµ
X
ν
uνµϕ′
ν =
X
ν
 X
µ
uνµcµ
!
ϕ′
ν = c′
νϕ′
ν,
(5.68)
where the coefﬁcients c′
ν of the expansion in the primed basis form a column vector c′ that
is related to the coefﬁcient vector c in the unprimed basis by the matrix equation
c′ = Uc,
(5.69)
with U the matrix whose elements are given in Eq. (5.67).
If we now consider the reverse transformation, from an expansion in the primed basis
to one in the unprimed basis, starting from
ϕ′
µ =
X
ν
vνµϕν =
X
ν
⟨ϕν|ϕ′
µ⟩ϕν,
(5.70)

288
Chapter 5 Vector Spaces
we see that V, the matrix of the transformation inverse to U, has elements
vνµ = ⟨ϕν|ϕ′
µ⟩= (U∗)µν = (U†)νµ.
(5.71)
In other words,
V = U†.
(5.72)
If we now transform the expansion of ψ, given in the unprimed basis by the coefﬁcient
vector c, ﬁrst to the primed basis and then back to the original unprimed basis, the coefﬁ-
cients will transform, ﬁrst to c′ and then back to c, according to
c = V Uc = U†Uc.
(5.73)
In order for Eq. (5.73) to be consistent it is necessary that U†U be a unit matrix, meaning
that U must be unitary. We thus have the important following result:
The transformation that converts the expansion of a vector c in any orthonormal basis
{ϕµ} to its expansion c′ in any other orthonormal basis {ϕ′ν} is described by the
matrix equation c′ = Uc, where the transformation matrix U is unitary and has ele-
ments uνµ = ⟨ϕ′ν|ϕµ⟩. A transformation between orthonormal bases is called a unitary
transformation.
Equation (5.69) is a direct generalization of the ordinary 2-D vector rotational transfor-
mation equation, Eq. (3.26),
A′ = SA.
For further emphasis, we compare the transformation matrix U introduced here (at right,
below) with the matrix S (at left) from Eq. (3.28), for rotations in ordinary 2-D space:
S =
 
ˆe′
1 · ˆe1
ˆe′
1 · ˆe2
ˆe′
2 · ˆe1
ˆe′
2 · ˆe2
!
U =


⟨ϕ′
1|ϕ1⟩
⟨ϕ′
1|ϕ2⟩
···
⟨ϕ′
2|ϕ1⟩
⟨ϕ′
2|ϕ2⟩
···
···
···
···

.
The resemblance becomes even more striking if we recognize that in Dirac notation, the
quantities ˆe′
i · ˆe j assume the form ⟨ˆe′
i|ˆe j⟩.
As for ordinary vectors (except that the quantities involved here are complex), the ith
row of U contains the (complex conjugated) components (a.k.a. coefﬁcients) of ϕ′
i in terms
of the unprimed basis; the orthonormality of the primed ϕ is consistent with the fact that
U U† is a unit matrix. The columns of U contain the components of the ϕ j in terms of the
primed basis; that also is analogous to our earlier observations. The matrix S is orthogonal;
U is unitary, which is the generalization to a complex space of the orthogonality condition.
Summarizing, we see that unitary transformations are analogous, in vector spaces, to the
orthogonal transformations that describe rotations (or reﬂections) in ordinary space.

5.5 Unitary Operators
289
Example 5.5.1
A UNITARY TRANSFORMATION
A Hilbert space is spanned by ﬁve functions deﬁned on the surface of a unit sphere and
expressed in spherical polar coordinates θ,ϕ:
χ1 =
r
15
4π sinθ cosθ cosϕ,
χ2 =
r
15
4π sinθ cosθ sinϕ,
χ3 =
r
15
4π sin2 θ sinϕ cosϕ,
χ4 =
r
15
16π sin2 θ(cos2 ϕ −sin2 ϕ),
χ5 =
r
5
16π (3cos2 θ −1).
These are orthonormal when the scalar product is deﬁned as
⟨f |g⟩=
π
Z
0
sinθ dθ
2π
Z
0
dϕ f ∗(θ,ϕ) g(θ,ϕ).
This Hilbert space can alternatively be spanned by the orthonormal set of functions
χ′
1 = −
r
15
8π sinθ cosθ eiϕ,
χ′
2 =
r
15
8π sinθ cosθ e−iϕ,
χ′
3 =
r
15
32π sin2 θ e2iϕ,
χ′
4 =
r
15
32π sin2 θ e−2iϕ,
χ′
5 = χ5.
The matrix U describing the transformation from the unprimed to the primed basis has
elements uνµ = ⟨χ′
ν|χµ⟩. Working out a representative matrix element,
u22 = ⟨χ′
2|χ2⟩=
15
4π
√
2
π
Z
0
sinθ dθ
2π
Z
0
dϕ sin2 θ cos2 θ e+iϕ sinϕ
=
15
4π
√
2
π
Z
0
sin3 θ cos2 θ dθ
2π
Z
0
dϕ e+iϕ eiϕ −e−iϕ
2i
=
15
4π
√
2
 4
15
 −2π
2i
=
i√
2
.

290
Chapter 5 Vector Spaces
We obtained this result by using the formula
R 2π
0
eniϕ dϕ = 2π δn0 and by looking up a
tabulated value for the θ integral. We evaluate explicitly one more matrix element:
u21 = ⟨χ′
2|χ1⟩=
15
4π
√
2
π
Z
0
sin3 θ cos2 θ dθ
2π
Z
0
dϕ e+iϕ eiϕ + e−iϕ
2
=
15
4π
√
2
 4
15
 2π
2 = 1
√
2
.
Evaluating the remaining elements of U, we reach
U =


−1/
√
2
−i/
√
2
0
0
0
1/
√
2
−i/
√
2
0
0
0
0
0
i/
√
2
1/
√
2
0
0
0
−i/
√
2
1/
√
2
0
0
0
0
0
1


.
As a check, note that the ith column of U should yield the components of χi in the primed
basis. For the ﬁrst column, we have
r
15
4π sinθ cosθ cosϕ = −1
√
2
 
−
r
15
8π sinθ cosθ eiϕ
!
+ 1
√
2
 r
15
8π sinθ cosθ e−iϕ
!
,
which simpliﬁes easily to an identity. Further checks are left as Exercise 5.5.1.
■
Successive Transformations
It is possible to make two or more successive unitary transformations, each of which will
convert an input orthonormal basis to an output basis that is also orthonormal. Just as for
ordinary vectors, the successive transformations are applied in right-to-left order, and the
product of the transformations can be viewed as a resultant unitary transformation.
Exercises
5.5.1
Show that the matrix U of Example 5.5.1 correctly transforms the vector f (θ,ϕ) =
3χ1 + 2iχ2 −χ3 + χ5 to the {χ′
i} basis by
(a)
(1) Making a column vector c that represents f (θ,ϕ) in the {χi} basis,
(2) forming c′ = Uc, and
(3) comparing the expansion P
i c′
iχ′
i(θ,ϕ) with f (θ,ϕ);
(b)
Verifying that U is unitary.
5.5.2
(a)
Given (in R3) the basis ϕ1 = x, ϕ2 = y, ϕ3 = z, consider the basis transformation
x →z, y →y, z →−x. Find the 3 × 3 matrix U for this transformation.

5.5 Unitary Operators
291
(b)
This transformation corresponds to a rotation of the coordinate axes. Identify
the rotation and reconcile your transformation matrix with an appropriate matrix
S(α,β,γ ) of the form given in Eq. (3.37).
(c)
Form the column vector c representing (in the original basis) f = 2x −3y + z,
ﬁnd the result of applying U to c, and show that this is consistent with the basis
transformation of part (a).
Note. You do not need to be able to form scalar products to handle this exercise; a
knowledge of the linear relationship between the original and transformed functions is
sufﬁcient.
5.5.3
Construct the matrix representing the inverse of the transformation in Exercise 5.5.2,
and show that this matrix and the transformation matrix of that exercise are matrix
inverses of each other.
5.5.4
The unitary transformation U that converts an orthonormal basis {ϕi} into the basis {ϕ′
i}
and the unitary transformation V that converts the basis {ϕ′
i} into the basis {χi} have
matrix representations
U =


i sinθ
cosθ
0
−cosθ
i sinθ
0
0
0
1

,
V =


1
0
0
0
cosθ
i sinθ
0
cosθ
−i sinθ

.
Given the function f (x) = 3ϕ1(x) −ϕ2(x) −2ϕ3(x),
(a)
By applying U, form the vector representing f (x) in the {ϕ′
i} basis and then by
applying V form the vector representing f (x) in the {χi} basis. Use this result to
write f (x) as a linear combination of the χi.
(b)
Form the matrix products UV and VU and then apply each to the vector represent-
ing f (x) in the {ϕi} basis. Verify that the results of these applications differ and
that only one of them gives the result corresponding to part (a).
5.5.5
Three functions which are orthogonal with unit weight on the range −1 ≤x ≤1 are
P0 = 1, P1 = x, and P2 = 3
2x2 −1
2. Another set of functions that are orthogonal and
span the same space are F0 = x2, F1 = x, F2 = 5x2 −3. Although much of this exercise
can be done by inspection, write down and evaluate all the integrals that lead to the
results when they are obtained in terms of scalar products.
(a)
Normalize each of the Pi and Fi.
(b)
Find the unitary matrix U that transforms from the normalized Pi basis to the
normalized Fi basis.
(c)
Find the unitary matrix V that transforms from the normalized Fi basis to the
normalized Pi basis.
(d)
Show that U and V are unitary, and that V = U−1.
(e)
Expand f (x) = 5x2 −3x + 1 in terms of the normalized versions of both bases,
and verify that the transformation matrix U converts the P-basis expansion of
f (x) into its F-basis expansion.

292
Chapter 5 Vector Spaces
5.6
TRANSFORMATIONS OF OPERATORS
We have seen how unitary transformations can be used to transform the expansion of a
function from one orthonormal basis set to another. We now consider the corresponding
transformation for operators. Given an operator A, which when expanded in the ϕ basis
has the form
A =
X
µν
|ϕµ⟩aµν⟨ϕν|,
we convert it to the ϕ′ basis by the simple expedient of inserting resolutions of the identity
(written in terms of the primed basis) on both sides of the above expression. This is an
excellent example of the beneﬁts of using Dirac notation. Remembering that this does not
change A (but of course does change its appearance), we get
A =
X
µνστ
|ϕ′
σ⟩⟨ϕ′
σ|ϕµ⟩aµν⟨ϕν|ϕ′
τ⟩⟨ϕ′
τ|,
which we simplify by identifying ⟨ϕ′
σ|ϕµ⟩= uσµ, as deﬁned in Eq. (5.67), and ⟨ϕν|ϕ′
τ⟩=
u∗
τν. Thus,
A =
X
µνστ
|ϕ′
σ⟩uσµaµνu∗
τν⟨ϕ′
τ| =
X
στ
|ϕ′
σ⟩a′
στ⟨ϕ′
τ|,
(5.74)
where a′
στ is the στ matrix element of A in the primed basis, related to the unprimed
values by
a′
στ =
X
µν
uσµaµνu∗
τν.
(5.75)
If we now note that u∗
τν = (U†)ντ , we can write Eq. (5.75) as the matrix equation
A′ = U A U† = U A U−1,
(5.76)
where in the ﬁnal member of the equation we used the fact that U is unitary.
Another way of getting at Eq. (5.76) is to consider the operator equation Aψ = χ, where
initially A, ψ, and χ are all regarded as expanded in the orthonormal set ϕ, with A having
matrix elements aµν, and with ψ and χ having the forms ψ = P
ν cµϕµ and χ = P
ν bνϕν.
This state of affairs corresponds to the matrix equation
Ac = b.
Now we simply insert U−1U between A and c, and multiply both sides of the equation on
the left by U. The result is

U A U−1
Uc

= Ub
−→
A′c′ = b′,
(5.77)
showing that the operator and the functions are properly related when the functions have
been transformed by applying U and the operator has been transformed as required by
Eq. (5.76). Since this relationship is valid for any choice of c and U, it conﬁrms the trans-
formation equation for A.

5.6 Transformations of Operators
293
Nonunitary Transformations
It is possible to consider transformations similar to that illustrated by Eq. (5.77), but using
a transformation matrix G that must be nonsingular, but is not required to be unitary. Such
more general transformations occasionally appear in physics applications, are called simi-
larity transformations, and lead to an equation deceptively similar to Eq. (5.77):

G A G−1
Gc

= Gb.
(5.78)
There is one important difference: Although a general similarity transformation preserves
the original operator equation, corresponding items do not describe the same quantity in a
different basis. Instead, they describe quantities that have been systematically (but consis-
tently) altered by the transformation.
Sometimes we encounter a need for transformations that are not even similarity trans-
formations. For example, we may have an operator whose matrix elements are given in a
nonorthogonal basis, and we consider the transformation to an orthonormal basis generated
by use of the Gram-Schmidt procedure.
Example 5.6.1
GRAM-SCHMIDT TRANSFORMATION
The Gram-Schmidt process describes the transformation from an initial function set χi to
an orthonormal set ϕµ according to equations that can be brought to the form
ϕµ =
µ
X
i=1
tiµχi,
µ = 1,2,....
Because the Gram-Schmidt process only generates coefﬁcients tiµ with i ≤µ, the transfor-
mation matrix T can be described as upper triangular, i.e., a square matrix with nonzero
elements tiµ only on and above its principal diagonal. Deﬁning S as a matrix with elements
si j = ⟨χi|χ j⟩(often called an overlap matrix), the orthonormality of the ϕµ is evidenced
by the equation
⟨ϕµ|ϕν⟩=
X
i j
⟨tiµχi|t jνχ j⟩=
X
i j
t∗
iµ⟨χi|χ j⟩t jν = (T†ST)µν = δµν.
(5.79)
Note that because T is upper triangular, T† must be lower triangular. In writing Eq. (5.79)
we did not have to restrict the i and j summations, as the coefﬁcients outside the contribut-
ing ranges of i and j are present, but set to zero.
From Eq. (5.79) we can obtain a representation of S:
S = (T†)−1T−1 = (TT†)−1.
(5.80)
Moreover, if we replace S from Eq. (5.79) by the matrix of a general operator A (in the χi
basis), we ﬁnd that in the orthonormal ϕ basis its representation A′ is
A′ = T†AT.
(5.81)
In general, T† will not be equal to T−1, so this equation does not deﬁne a similarity trans-
formation.
■

294
Chapter 5 Vector Spaces
Exercises
5.6.1
(a)
Using the two spin functions ϕ1 = α and ϕ2 = β as an orthonormal basis (so
⟨α|α⟩= ⟨β|β⟩= 1, ⟨α|β⟩= 0), and the relations
Sxα = 1
2β, Sxβ = 1
2α, Syα = 1
2iβ, Syβ = −1
2iα, Szα = 1
2α, Szβ = −1
2β,
construct the 2 × 2 matrices of Sx, Sy, and Sz.
(b)
Taking now the basis ϕ′
1 = C(α + β), ϕ′
2 = C(α −β):
(i)
Verify that ϕ′
1 and ϕ′
2 are orthogonal,
(ii)
Assign C a value that makes ϕ′
1 and ϕ′
2 normalized,
(iii)
Find the unitary matrix for the transformation {ϕi} →{ϕ′
i}.
(c)
Find the matrices of Sx, Sy, and Sz in the {ϕ′
i} basis.
5.6.2
For the basis ϕ1 = Cxe−r2, ϕ2 = Cye−r2, ϕ3 = Cze−r2, where r2 = x2 + y2 + z2,
with the scalar product deﬁned as an unweighted integral over R3 and with C chosen
to make the ϕi normalized:
(a)
Find the 3 × 3 matrix of Lx = −i

y ∂
∂z −z ∂
∂y

;
(b)
Using the transformation matrix U =


1
0
0
0
1/
√
2
−i/
√
2
0
1/
√
2
i/
√
2

,
ﬁnd the trans-
formed matrix of Lx;
(c)
Find the new basis functions ϕ′
i deﬁned by the transformation U, and write explic-
itly (in terms of x, y, and z) the functional forms of Lxϕ′
i, i = 1, 2, 3.
Hint. Use
R
e−r2d3r = π3/2,
R
x2e−r2d3r = 1
2π3/2; the integrals are over R3.
5.6.3
The Gram-Schmidt process for converting an arbitrary basis χν into an orthonormal
set ϕν is described in Section 5.2 in a way that introduces coefﬁcients of the form
−⟨ϕµ|χν⟩. For bases consisting of three functions, convert the formulation so that ϕν
is expressed entirely in terms of the χµ, thereby obtaining an expression for the upper-
triangular matrix T appearing in Eq. (5.81).
5.7
INVARIANTS
Just as coordinate rotations leave invariant the essential properties of physical vectors,
we can expect unitary transformations to preserve essential features of our vector spaces.
These invariances are most directly observed in the basis-set expansions of operators
and functions.
Consider ﬁrst a matrix equation of the form b = Ac, where all quantities have been
evaluated using a particular orthonormal basis ϕi. Now suppose that we wish to use a
basis χi which can be reached from the original basis by applying a unitary transformation

5.7 Invariants
295
such that
c′ = Uc
and
b′ = Ub.
In the new basis, the matrix A becomes A′ = UAU−1, and the invariance we seek corre-
sponds to b′ = A′c′. In other words, all the quantities must change coherently so that their
relationship is unaltered. It is easy to verify that this is the case. Substituting for the primed
quantities,
Ub = (UAU−1)(Uc)
−→
Ub = U Ac,
from which we can recover b = Ac by multiplying from the left by U−1.
Scalar quantities should remain invariant under unitary transformation; the prime
example here is the scalar product. If f and g are represented in some orthonormal basis,
respectively, by a and b, their scalar product is given by a†b. Under a unitary transfor-
mation whose matrix representation is U, a becomes a′ = Ua and b becomes b′ = Ub,
and
⟨f |g⟩= (a′)†b′ = (Ua)†(Ub) = (a†U†)(Ub) = a†b.
(5.82)
The fact that U† = U−1 enables us to conﬁrm the invariance.
Another scalar that should remain invariant under basis transformation is the expectation
value of an operator.
Example 5.7.1
EXPECTATION VALUE IN TRANSFORMED BASIS
Suppose that ψ =
X
i
ciϕi, and that we wish to compute the expectation value of A for
this ψ, where A, the matrix corresponding to A, has elements aνµ = ⟨ϕν|A|ϕµ⟩. We have
⟨A⟩= ⟨ψ|A|ψ⟩
−→
c†Ac.
If we now choose to use a basis obtained from the ϕi by a unitary transformation U, the
expression for ⟨A⟩becomes
(Uc)†(UAU−1)(Uc) = c†U†UAU−1Uc,
which, because U is unitary and therefore U† = U−1, reduces, as it must, to the previously
obtained value of ⟨A⟩.
■
Vector spaces have additional useful matrix invariants. The trace of a matrix is invariant
under unitary transformation. If A′ = U A U−1, then
trace(A′) =
X
ν
(U A U−1)νν =
X
νµτ
uνµaµτ(U−1)τν =
X
µτ
 X
ν
(U−1)τνuνµ
!
aµτ
=
X
µτ
δµτaµτ =
X
µ
aµµ = trace(A).
(5.83)
Here we simply used the property U−1U = 1.
Another matrix invariant is the determinant. From the determinant product theorem,
det(U A U−1) = det(U−1U A) = det(A). Further invariants will be identiﬁed when we study
matrix eigenvalue problems in Chapter 6.

296
Chapter 5 Vector Spaces
Exercises
5.7.1
Using the formal properties of unitary transformations, show that the commutator
[x, p] = i is invariant under unitary transformation of the matrices representing x and p.
5.7.2
The Pauli matrices
σ 1 =
0
1
1
0

,
σ 2 =
0
−i
i
0

,
σ 3 =
1
0
0
−1

,
have commutator [σ 1,σ 2] = 2iσ 3. Show that this relationship continues to be valid if
these matrices are transformed by
U =
 cosθ
sinθ
−sinθ
cosθ
!
.
5.7.3
(a)
The operator Lx is deﬁned as
Lx = −i

y ∂
∂z −z ∂
∂y

.
Verify that the basis ϕ1 = Cxe−r2, ϕ2 = Cye−r2, ϕ3 = Cze−r2, where r2 = x2 +
y2 + z2, forms a closed set under the operation of Lx, meaning that when Lx is
applied to any member of this basis the result is a function within the basis space,
and construct the 3 × 3 matrix of Lx in this basis from the result of the application
of Lx to each basis function.
(b)
Verify that Lx
h
(x + iy)e−r2i
= −ze−r2, and note that this result, using the {ϕi}
basis, can be written Lx(ϕ1 + iϕ2) = −ϕ3.
(c)
Express the equation of part (b) in matrix form, and write the matrix equation that
results when each of the quantities is transformed using the transformation matrix
U =


1
0
0
0
1/
√
2
−i/
√
2
0
1/
√
2
i/
√
2

.
(d)
Regarding the transformation U as producing a new basis {ϕ′
i}, ﬁnd the explicit
form (in x, y, z) of the ϕ′
i.
(e)
Using the operator form of Lx and the explicit forms of the ϕ′
i, verify the validity
of the transformed equation found in part (c).
Hint. The results of Exercise 5.6.2 may be useful.
5.8
SUMMARY—VECTOR SPACE NOTATION
It may be useful to summarize some of the relationships found in this chapter, highlighting
the essentially complete mathematical parallelism between the properties of vectors and
those of basis expansions in vector spaces. We do so here, using Dirac notation wherever
appropriate.

5.8 Summary—Vector Space Notation
297
1.
Scalar product:
⟨ϕ|ψ⟩=
b
Z
a
ϕ∗(t)ψ(t)w(t)dt ⇐⇒⟨u|v⟩= u†v = u∗· v.
(5.84)
The result of the scalar product operation is a scalar (i.e., a real or complex number).
Here u†v represents the product of a row and a column vector; it is equivalent to the
dot-product notation also shown.
2.
Expectation value:
⟨ϕ|A|ϕ⟩=
b
Z
a
ϕ∗(t)Aϕ(t)w(t)dt ⇐⇒⟨u|A|u⟩= u†Au.
(5.85)
3.
Adjoint:
⟨ϕ|A|ψ⟩= ⟨A†ϕ|ψ⟩⇐⇒⟨u|A|v⟩= ⟨A†u|v⟩= [A†u]†v = u†Av.
(5.86)
Note that the simpliﬁcation of [A†u]†v shows that the matrix A† has the property
expected of an operator adjoint.
4.
Unitary transformation:
ψ = Aϕ −→Uψ = (U AU−1)(Uϕ) ⇐⇒w = Av −→Uw = (UAU−1)(Uv). (5.87)
5.
Resolution of identity:
1 =
X
i
|ϕi⟩⟨ϕi| ⇐⇒1 =
X
i
|ˆei⟩⟨ˆei|,
(5.88)
where the ϕi are orthonormal and the ˆei are orthogonal unit vectors. Applying
Eq. (5.88) to a function (or vector):
ψ =
X
i
|ϕi⟩⟨ϕi|ψ⟩=
X
i
aiϕi ⇐⇒w =
X
i
|ˆei⟩⟨ˆei|w⟩=
X
i
wi ˆei,
(5.89)
where ai = ⟨ϕi|ψ⟩and wi = ⟨ˆei|w⟩= ˆei · w.
Additional Readings
Brown, W. A., Matrices and Vector Spaces. New York: M. Dekker (1991).
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969), reprinting, Dover (1992).
Dennery, P., and A. Krzywicki, Mathematics for Physicists. New York: Harper & Row, reprinting, Dover (1996).
Halmos, P. R., Finite-Dimensional Vector Spaces, 2nd ed. Princeton, NJ: Van Nostrand (1958), reprinting,
Springer (1993).
Jain, M. C., Vector Spaces and Matrices in Physics, 2nd ed. Oxford: Alpha Science International (2007).
Kreyszig, E., Advanced Engineering Mathematics, 6th ed. New York: Wiley (1988).
Lang, S., Linear Algebra. Berlin: Springer (1987).
Roman, S., Advanced Linear Algebra, Graduate Texts in Mathematics 135, 2nd ed. Berlin: Springer (2005).

CHAPTER 6
EIGENVALUE PROBLEMS
6.1
EIGENVALUE EQUATIONS
Many important problems in physics can be cast as equations of the generic form
Aψ = λψ,
(6.1)
where A is a linear operator whose domain and range is a Hilbert space, ψ is a function in
the space, and λ is a constant. The operator A is known, but both ψ and λ are unknown,
and the task at hand is to solve Eq. (6.1). Because the solutions to an equation of this
type yield functions ψ that are unchanged by the operator (except for multiplication by a
scale factor λ), they are termed eigenvalue equations: Eigen is German for “[its] own.” A
function ψ that solves an eigenvalue equation is called an eigenfunction, and the value of
λ that goes with an eigenfunction is called an eigenvalue.
The formal deﬁnition of an eigenvalue equation may not make its essential content
totally apparent. The requirement that the operator A leaves ψ unchanged except for a
scale factor constitutes a severe restriction upon ψ. The possibility that Eq. (6.1) has any
solutions at all is in many cases not intuitively obvious.
To see why eigenvalue equations are common in physics, let’s cite a few examples:
1.
The resonant standing waves of a vibrating string will be those in which the restor-
ing force on the elements of the string (represented by Aψ) are proportional to their
displacements ψ from equilibrium.
2.
The angular momentum L and the angular velocity ω of a rigid body are three-
dimensional (3-D) vectors that are related by the equation
L = Iω,
where I is the 3×3 moment of inertia matrix. Here the direction of ω deﬁnes the axis
of rotation, while the direction of L deﬁnes the axis about which angular momentum is
299
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

300
Chapter 6 Eigenvalue Problems
generated. The condition that these two axes be in the same direction (thereby deﬁn-
ing what are known as the principal axes of inertia) is that L = λω, where λ is a
proportionality constant. Combining with the formula for L, we obtain
Iω = λω,
which is an eigenvalue equation in which the operator is the matrix I and the eigen-
function (then usually called an eigenvector) is the vector ω.
3.
The time-independent Schrödinger equation in quantum mechanics is an eigenvalue
equation, with A the Hamiltonian operator H, ψ a wave function and λ = E the
energy of the state represented by ψ.
Basis Expansions
A powerful approach to eigenvalue problems is to express them in terms of an orthonormal
basis whose members we designate ϕi, using the formulas developed in Chapter 5. Then
the operator A and the function ψ are represented by a matrix A and a vector c whose
elements are obtained, according to Eqs. (5.51) and (5.52), as the scalar products
ai j = ⟨ϕi|A|ϕ j⟩,
ci = ⟨ϕi|ψ⟩.
Our original eigenvalue equation has now been reduced to a matrix equation:
Ac = λc.
(6.2)
When an eigenvalue equation is presented in this form, we can call it a matrix eigenvalue
equation and call the vectors c that solve it eigenvectors. As we shall see in later sections
of this chapter, there is a well-developed technology for the solution of matrix eigenvalue
equations, so a route always available for solving eigenvalue equations is to cast them
in matrix form. Once a matrix eigenvalue problem has been solved, we can recover the
eigenfunctions of the original problem from their expansion:
ψ =
X
i
ciϕi.
Sometimes, as in the moment of inertia example mentioned above, our eigenvalue prob-
lem originates as a matrix problem. Then, of course, we do not have to begin its solution
process by introducing a basis and converting it into matrix form, and our solutions will be
vectors that do not need to be interpreted as expansions in a basis.
Equivalence of Operator and Matrix Forms
It is important to note that we are dealing with eigenvalue equations in which the operator
involved is linear and that it operates on elements of a Hilbert space. Once these conditions
are met, the operator and function involved can always be expanded in a basis, leading to
a matrix eigenvalue equation that is totally equivalent to our original problem. Among
other things, this means that any theorems about properties of eigenvectors or eigenvalues
that are developed from basis-set expansions of an eigenvalue problem must apply also to
the original problem, and that solution of the matrix eigenvalue equation provides also a

6.2 Matrix Eigenvalue Problems
301
solution to the original problem. These facts, plus the practical observation that we know
how to solve matrix eigenvalue problems, strongly suggest that the detailed investigation
of the matrix problems should be on our agenda.
When we explore matrix eigenvalue problems, we will ﬁnd that certain properties of the
matrix inﬂuence the nature of the solutions, and that in particular signiﬁcant simpliﬁcations
become available when the matrix is Hermitian. Many eigenvalue equations of interest in
physics involve differential operators, so it is of importance to understand whether (or
under what conditions) these operators are Hermitian. That issue is taken up in Chapter 8.
Finally, we note that the introduction of a basis-set expansion is not the only possibility
for solving an eigenvalue equation. Eigenvalue equations involving differential operators
can also be approached by the general methods for solving differential equations. That
topic is also discussed in Chapter 8.
6.2
MATRIX EIGENVALUE PROBLEMS
While in principle the notion of an eigenvalue problem is already fully deﬁned, we open
this section with a simple example that may help to make it clearer how such problems are
set up and solved.
A Preliminary Example
We consider here a simple problem of two-dimensional (2-D) motion in which a particle
slides frictionlessly in an ellipsoidal basin (see Fig. 6.1). If we release the particle (initially
at rest) at an arbitrary point in the basin, it will start to move downhill in the (negative)
gradient direction, which in general will not aim directly at the potential minimum at the
bottom of the basin. The particle’s overall trajectory will then be a complicated path, as
sketched in the bottom panel of Fig. 6.1. Our objective is to ﬁnd the positions, if any,
from which the trajectories will aim at the potential minimum, and will therefore represent
simple one-dimensional oscillatory motion.
This problem is sufﬁciently elementary that we can analyze it without great difﬁculty.
We take a potential of the form
V (x, y) = ax2 + bxy + cy2,
with parameters a, b, c in ranges that describe an ellipsoidal basin with a minimum in V
at x = y = 0. We then calculate the x and y components of the force on our particle when
at (x, y):
Fx = −∂V
∂x = −2ax −by,
Fy = −∂V
∂y = −bx −2cy.
It is pretty clear that, for most values of x and y, Fx/Fy ̸= x/y, so the force will not be
directed toward the minimum at x = y = 0.
To search for directions in which the force is directed toward x = y = 0, we begin by
writing the equations for the force in matrix form:
Fx
Fy

=
−2a
−b
−b
−2c
x
y

,
or
f = Hr,

302
Chapter 6 Eigenvalue Problems
–5
0
5
y
−10
0
10
x
−5
0
5
y
−10
0
10
x
FIGURE 6.1
Top: Contour lines of basin potential V = x2 −
√
5xy + 3y2. Bottom:
Trajectory of sliding particle of unit mass starting from rest at (8.0,−1.92).
where f, H, and r are deﬁned as indicated. Now the condition Fx/Fy = x/y is equivalent
to the statement that f and r are proportional, and therefore we can write
Hr = λr,
(6.3)
where, as already suggested, H is a known matrix, while λ and r are to be determined. This
is an eigenvalue equation, and the column vectors r that are its solutions are its eigenvec-
tors, while the corresponding values of λ are its eigenvalues.
Equation (6.3) is a homogeneous linear equation system, as becomes more obvious if
written as
(H −λ1)r = 0,
(6.4)
and we know from Chapter 2 that it will have the unique solution r = 0 unless det(H −
λ1) = 0. However, the value of λ is at our disposal, so we can search for values of λ that
cause this determinant to vanish. Proceeding symbolically, we look for λ such that
det(H −λ1) =

h11 −λ
h12
h21
h22 −λ
 = 0.
Expanding the determinant, which is sometimes called a secular determinant (the name
arising from early applications in celestial mechanics), we have an algebraic equation, the
secular equation,
(h11 −λ)(h22 −λ) −h12h21 = 0,
(6.5)

6.2 Matrix Eigenvalue Problems
303
which can be solved for λ. The left hand side of Eq. (6.5) is also called the characteristic
polynomial (in λ) of H, and Eq. (6.5) is for that reason also known as the characteristic
equation of H.
Once a value of λ that solves Eq. (6.5) has been obtained, we can return to the homo-
geneous equation system, Eq. (6.4), and solve it for the vector r. This can be repeated for
all λ that are solutions to the secular equation, thereby giving a set of eigenvalues and the
associated eigenvectors.
Example 6.2.1
2-D ELLIPSOIDAL BASIN
Let’s continue with our ellipsoidal basin example, with the speciﬁc parameter values a = 1,
b = −
√
5, c = 3. Then our matrix H has the form
H =
−2
√
5
√
5
−6

,
and the secular equation takes the form
det(H −λ1) =

−2 −λ
√
5
√
5
−6 −λ
 = λ2 + 8λ + 7 = 0.
Since λ2 + 8λ + 7 = (λ + 1)(λ + 7), we see that the secular equation has as solutions the
eigenvalues λ = −1 and λ = −7.
To get the eigenvector corresponding to λ = −1, we return to Eq. (6.4), which, written
in great detail, is
(H −λ1)r =
−2 −(−1)
√
5
√
5
−6 −(−1)
x
y

=
−1
√
5
√
5
−5
x
y

= 0,
which expands into a linearly dependent pair of equations:
−x +
√
5 y = 0
√
5 x −5y = 0.
This is, of course, the intention associated with the secular equation, because if these equa-
tions were linearly independent they would inexorably lead to the solution x = y = 0.
Instead, from either equation, we have x =
√
5 y, so we have the eigenvalue/eigenvector
pair
λ1 = −1,
r1 = C
√
5
1

,
where C is a constant that can assume any value. Thus, there is an inﬁnite number of x, y
pairs that deﬁne a direction in the 2-D space, with the magnitude of the displacement in
that direction arbitrary. The arbitrariness of scale is a natural consequence of the fact that
the equation system was homogeneous; any multiple of a solution of a linear homogeneous
equation set will also be a solution. This eigenvector corresponds to trajectories that start
from the particle at rest anywhere on the line deﬁned by r1. A trajectory of this sort is
illustrated in the top panel of Fig. 6.2.

304
Chapter 6 Eigenvalue Problems
−5
0
5
y
−10
0
10
x
−5
0
5
y
−10
0
10
x
FIGURE 6.2
Trajectories starting at rest. Top: At a point on the line x = y
√
5.
Bottom: At a point on the line y = −x
√
5.
We have not yet considered the possibility that λ = −7. This leads to a different eigen-
vector, obtained by solving
(H −λ1)r =
−2 + 7
√
5
√
5
−6 + 7
x
y

=
 5
√
5
√
5
1
x
y

= 0,
corresponding to y = −x
√
5. This deﬁnes the eigenvalue/eigenvector pair
λ2 = −7,
r2 = C′
−1
√
5

.
A trajectory of this sort is shown in the bottom panel of Fig. 6.2.
We thus have two directions in which the force is directed toward the minimum, and
they are mutually perpendicular: the ﬁrst direction has dy/dx = 1/
√
5 ; for the second,
dy/dx = −
√
5.
We can easily check our eigenvectors and eigenvalues. For λ1 and r1,
Hr1 =
−2
√
5
√
5
−6

C
√
5
C

= C

−
√
5
−1

= (−1)

C
√
5
C

= λ1r1.
It is often useful to normalize eigenvectors, which we can do by choosing the constant
(C or C′) to make r of magnitude unity. In the present example,
r1 =
 √5/6
√1/6
!
,
r2 =
 
−√1/6
√5/6
!
.
(6.6)

6.2 Matrix Eigenvalue Problems
305
Each of these normalized eigenvectors is still arbitrary as to overall sign (or if we accept
complex coefﬁcients, as to an arbitrary complex factor of magnitude unity).
Before leaving this example, we make three further observations: (1) the number of
eigenvalues was equal to the dimension of the matrix H. This is a consequence of the
fundamental theorem of algebra, namely that an equation of degree n will have n roots;
(2) although the secular equation was of degree 2 and quadratic equations can have com-
plex roots, our eigenvalues were real; and (3) our two eigenvectors are orthogonal.
■
Our 2-D example is easily understood physically. The directions in which the displace-
ment and the force are collinear are the symmetry directions of the elliptical potential ﬁeld,
and they are associated with different eigenvalues (the proportionality constant between
position and force) because the ellipses have axes of different lengths. We have, in fact,
identiﬁed the principal axes of our basin. With the parameters of Example 6.2.1, the poten-
tial could have been written (using the normalized eigenvectors)
V = 1
2
 √
5 x + y
√
6
!2
+ 7
2
 
x −
√
5 y
√
6
!2
= 1
2(x′)2 + 7
2(y′)2,
which shows that V divides into two quadratic terms, each dependent on a parenthesized
quantity (a new coordinate) proportional to one of our eigenvectors. The new coordinates
are related to the original x, y by a rotation with unitary transformation U:
Ur =
√5/6
√1/6
√1/6
−√5/6
x
y

=
 
(
√
5 x + y)/
√
6
(x −
√
5 y)/
√
6
!
=
x′
y′

.
Finally, we note that when we calculate the force in the primed coordinate system, we get
Fx′ = −x′,
Fy′ = −7y′,
corresponding to the eigenvalues we found.
Another Eigenproblem
Example 6.2.1 is not complicated enough to provide a full illustration of the matrix eigen-
value problem. Consider next the following example.
Example 6.2.2
BLOCK-DIAGONAL MATRIX
Find the eigenvalues and eigenvectors of
H =


0
1
0
1
0
0
0
0
2

.
(6.7)
Writing the secular equation and expanding in minors using the third row, we have

−λ
1
0
1
−λ
0
0
0
2 −λ

= (2 −λ)

−λ
1
1
−λ
 = (2 −λ)(λ2 −1) = 0.
(6.8)
We see that the eigenvalues are 2, +1, and −1.

306
Chapter 6 Eigenvalue Problems
To obtain the eigenvector corresponding to λ = 2, we examine the equation set
[H −2(1)]c = 0:
−2c1 + c2 = 0,
c1 −2c2 = 0,
0 = 0.
The ﬁrst two equations of this set lead to c1 = c2 = 0. The third obviously conveys no
information, and we are led to the conclusion that c3 is arbitrary. Thus, at this point we
have
λ1 = 2,
c1 =


0
0
C

.
(6.9)
Taking next λ = +1, our matrix equation is [H −1(1)]c = 0, which is equivalent to the
ordinary equations
−c1 + c2 = 0,
c1 −c2 = 0,
c3 = 0.
We clearly have c1 = c2 and c3 = 0, so
λ2 = +1,
c2 =


C
C
0

.
(6.10)
Similar operations for λ = −1 yield
λ3 = −1,
c3 =


C
−C
0

.
(6.11)
Collecting our results, and normalizing the eigenvectors (often useful, but not in general
necessary), we have
λ1 = 2,
c1 =


0
0
1

,
λ2 = 1,
c2 =


2−1/2
2−1/2
0

,
λ3 = −1,
c3 =


2−1/2
−2−1/2
0

.
Note that because H was block-diagonal, with an upper-left 2×2 block and a lower-
right 1×1 block, the secular equation separated into a product of the determinants for the
two blocks, and its solutions corresponded to those of an individual block, with coefﬁ-
cients of value zero for the other block(s). Thus, λ = 2 was a solution for the 1×1 block
in row/column 3, and its eigenvector involved only the coefﬁcient c3. The λ values ±1
came from the 2×2 block in rows/columns 1 and 2, with eigenvectors involving only
coefﬁcients c1 and c2.
■
In the case of a 1×1 block in row/column i, we saw, for i = 3 in Example 6.2.2, that its
only element was the eigenvalue, and that the corresponding eigenvector is proportional

6.2 Matrix Eigenvalue Problems
307
to ˆei (a unit vector whose only nonzero element is ci = 1). A generalization of this obser-
vation is that if a matrix H is diagonal, its diagonal elements hii will be the eigenvalues λi,
and that the eigenvectors ci will be the unit vectors ˆei.
Degeneracy
If the secular equation has a multiple root, the eigensystem is said to be degenerate or to
exhibit degeneracy. Here is an example.
Example 6.2.3
DEGENERATE EIGENPROBLEM
Let’s ﬁnd the eigenvalues and eigenvectors of
H =


0
0
1
0
1
0
1
0
0

.
(6.12)
The secular equation for this problem is

−λ
0
1
0
1 −λ
0
1
0
−λ

= λ2(1 −λ) −(1 −λ) = (λ2 −1)(1 −λ) = 0
(6.13)
with the three roots +1, +1, and −1. Let’s consider ﬁrst λ = −1. Then we have
c1 + c3 = 0,
2c2 = 0,
c1 + c3 = 0.
Thus,
λ1 = −1,
c1 = C


1
0
−1

.
(6.14)
For the double root λ = +1,
−c1 + c3 = 0,
0 = 0,
c1 −c3 = 0.
Note that of the three equations, only one is now linearly independent; the double root sig-
nals two linear dependencies, and we have solutions for any values of c1 and c2, with only
the condition that c3 = c1. The eigenvectors for λ = +1 thus span a 2-D manifold (= sub-
space), in contrast to the trivial one-dimensional manifold characteristic of nondegenerate

308
Chapter 6 Eigenvalue Problems
solutions. The general form for these eigenvectors is
λ = +1,
c =


C
C′
C

.
(6.15)
It is convenient to describe the degenerate eigenspace for λ = 1 by identifying two mutu-
ally orthogonal vectors that span it. We can pick the ﬁrst vector by choosing arbitrary val-
ues of C and C′ (an obvious choice is to set one of these, say C′, to zero). Then, using the
Gram-Schmidt process (or in this case by simple inspection), we ﬁnd a second eigenvector
orthogonal to the ﬁrst. Here, this leads to
λ2 = λ3 = +1,
c2 = C


1
0
1

,
c3 = C′


0
1
0

.
(6.16)
Normalizing, our eigenvalues and eigenvectors become
λ1 = −1,
c1 =


2−1/2
0
−2−1/2

,
λ2 = λ3 = 1,
c2 =


2−1/2
0
2−1/2

,
c3 =


0
1
0

.
■
The eigenvalue problems we have used as examples all led to secular equations with
simple solutions; realistic applications frequently involve matrices of large dimension and
secular equations of high degree. The solution of matrix eigenvalue problems has been
an active ﬁeld in numerical analysis and very sophisticated computer programs for this
purpose are now available. Discussion of the details of such programs is outside the scope
of this book, but the ability to use such programs should be part of the technology available
to the working physicist.
Exercises
Find the eigenvalues and corresponding normalized eigenvectors of the matrices in
Exercises 6.2.1 through 6.2.14. Orthogonalize any degenerate eigenvectors.
6.2.1
A =


1
0
1
0
1
0
1
0
1

.
ANS.
λ = 0,1,2.
6.2.2
A =


1
√
2
0
√
2
0
0
0
0
0

.
ANS.
λ = −1,0,2.
6.2.3
A =


1
1
0
1
0
1
0
1
1

.
ANS.
λ = −1,1,2.

6.2 Matrix Eigenvalue Problems
309
6.2.4
A =


1
√
8
0
√
8
1
√
8
0
√
8
1

.
ANS.
λ = −3,1,5.
6.2.5
A =


1
0
0
0
1
1
0
1
1

.
ANS.
λ = 0,1,2.
6.2.6
A =


1
0
0
0
1
√
2
0
√
2
0

.
ANS.
λ = −1,1,2.
6.2.7
A =


0
1
0
1
0
1
0
1
0

.
ANS.
λ = −
√
2,0,
√
2.
6.2.8
A =


2
0
0
0
1
1
0
1
1

.
ANS.
λ = 0,2,2.
6.2.9
A =


0
1
1
1
0
1
1
1
0

.
ANS.
λ = −1,−1,2.
6.2.10
A =


1
−1
−1
−1
1
−1
−1
−1
1

.
ANS.
λ = −1,2,2.
6.2.11
A =


1
1
1
1
1
1
1
1
1

.
ANS.
λ = 0,0,3.
6.2.12
A =


5
0
2
0
1
0
2
0
2

.
ANS.
λ = 1,1,6.
6.2.13
A =


1
1
0
1
1
0
0
0
0

.
ANS.
λ = 0,0,2.

310
Chapter 6 Eigenvalue Problems
6.2.14
A =


5
0
√
3
0
3
0
√
3
0
3

.
ANS.
λ = 2,3,6.
6.2.15
Describe the geometric properties of the surface
x2 + 2xy + 2y2 + 2yz + z2 = 1.
How is it oriented in 3-D space? Is it a conic section? If so, which kind?
6.3
HERMITIAN EIGENVALUE PROBLEMS
All the illustrative problems we have thus far examined have turned out to have real eigen-
values; this was also true of all the exercises at the end of Section 6.2. We also found,
whenever we bothered to check, that the eigenvectors corresponding to different eigenval-
ues were orthogonal. The purpose of the present section is to show that these properties
are consequences of the fact that all the eigenvalue problems we have considered were for
Hermitian matrices.
We remind the reader that the check for Hermiticity is simple: We simply verify that H
is equal to its adjoint, H†; if a matrix is real, this condition is simply that it be symmetric.
All the matrices to which we referred are clearly Hermitian.
We now proceed to characterize the eigenvalues and eigenvectors of Hermitian matri-
ces. Let H be a Hermitian matrix, with ci and c j two of its eigenvectors corresponding,
respectively, to the eigenvalues λi and λ j. Then, using Dirac notation,
H|ci⟩= λi|ci⟩,
H|c j⟩= λ j|c j⟩.
(6.17)
Multiplying on the left the ﬁrst of these by c†
j, which in Dirac notation is ⟨c j|, and the
second by ⟨ci|,
⟨c j|H|ci⟩= λi⟨c j|ci⟩,
⟨ci|H|c j⟩= λ j⟨ci|c j⟩.
(6.18)
We next take the complex conjugate of the second of these equations, noting that ⟨ci|c j⟩∗=
⟨c j|ci⟩, that we must complex conjugate the occurrence of λ j, and that
⟨ci|H|c j⟩∗= ⟨Hc j|ci⟩= ⟨c j|H|ci⟩.
(6.19)
Note that the ﬁrst member of Eq. (6.19) contains the scalar product of ci with Hc j. Com-
plex conjugating this scalar product yields the second member of that equation. The ﬁnal
member of the equation follows because H is Hermitian.
The complex conjugation therefore converts Eqs. (6.18) into
⟨c j|H|ci⟩= λi⟨c j|ci⟩,
⟨c j|H|ci⟩= λ∗
j⟨c j|ci⟩.
(6.20)
Equations (6.20) permit us to obtain two important results: First, if i = j, the scalar product
⟨c j|ci⟩becomes ⟨ci|ci⟩, which is an inherently positive quantity. This means that the two
equations are only consistent if λi = λ∗
i , meaning that λi must be real. Thus,
The eigenvalues of a Hermitian matrix are real.

6.4 Hermitian Matrix Diagonalization
311
Next, if i ̸= j, combining the two equations of Eq. (6.20), and remembering that the λi
are real,
(λi −λ j)⟨c j|ci⟩= 0,
(6.21)
so that either λi = λ j or ⟨c j|ci⟩= 0. This tells us that
Eigenvectors of a Hermitian matrix corresponding to different eigenvalues are
orthogonal.
Note, however, that if λi = λ j, which will occur if i and j refer to two degenerate eigen-
vectors, we know nothing about their orthogonality. In fact, in Example 6.2.3 we examined
a pair of degenerate eigenvectors, noting that they spanned a two-dimensional manifold
and were not required to be orthogonal. However, we also noted in that context that we
could choose them to be orthogonal. Sometimes (as in Example 6.2.3), it is obvious how
to choose orthogonal degenerate eigenvectors. When it is not obvious, we can start from
any linearly independent set of degenerate eigenvectors and orthonormalize them by the
Gram-Schmidt process.
Since the total number of eigenvectors of a Hermitian matrix is equal to its dimension,
and since (whether or not there is degeneracy) we can make from them an orthonormal set
of eigenvectors, we have the following important result:
It is possible to choose the eigenvectors of a Hermitian matrix in such a way that they
form an orthonormal set that spans the space of the matrix basis. This situation is often
referred to by the statement, “The eigenvectors of a Hermitian matrix form a complete
set.” This means that if the matrix is of order n, any vector of dimension n can be written
as a linear combination of the orthonormal eigenvectors, with coefﬁcients determined
by the rules for orthogonal expansions.
We close this section by reminding the reader that theorems which have been established
for an arbitrary basis-set expansion of a Hermitian eigenvalue equation apply also to that
eigenvalue equation in its original form. Therefore, this section has also shown that:
If H is a linear Hermitian operator on an arbitrary Hilbert space,
1.
The eigenvalues of H are real.
2.
Eigenfunctions corresponding to different eigenvalues of H are orthogonal.
3.
It is possible to choose the eigenfunctions of H in a way such that they form a
orthonormal basis for the Hilbert space. In general, the eigenfunctions of a Her-
mitian operator form a complete set (i.e., a complete basis for the Hilbert space).
6.4
HERMITIAN MATRIX DIAGONALIZATION
In Section 6.2 we observed that if a matrix is diagonal, the diagonal elements are its eigen-
values. This observation opens an alternative approach to the matrix eigenvalue problem.
Given the matrix eigenvalue equation
Hc = λc,
(6.22)

312
Chapter 6 Eigenvalue Problems
where H is a Hermitian matrix, consider what happens if we insert unity between H and c,
as follows, with U a unitary matrix, and then left-multiply the resulting equation by U:
HU−1Uc = λc
−→
UHU−1 Uc

= λ
 Uc

.
(6.23)
Equation (6.23) shows that our original eigenvalue equation has been converted into one
in which H has been replaced by its unitary transformation (by U) and the eigenvector c
has also been transformed by U, but the value of λ remains unchanged. We thus have the
important result:
The eigenvalues of a matrix remain unchanged when the matrix is subjected to a unitary
transformation.
Next, suppose that we choose U in such a way that the transformed matrix UHU−1 is
in the eigenvector basis. While we may or may not know how to construct this U, we
know that such a unitary matrix exists because the eigenvectors form a complete orthog-
onal set, and can be speciﬁed to be normalized. If we transform with the chosen U, the
matrix UHU−1 will be diagonal, with the eigenvalues as diagonal elements. Moreover, the
eigenvector Uc of UHU−1 corresponding to the eigenvalue λi = (UHU−1)ii is ˆei (a column
vector with all elements zero except for unity in the ith row). We may ﬁnd the eigenvector
ci of Eq. (6.22) by solving the equation Uci = ˆei, obtaining ci = U−1ˆei.
These observations correspond to the following:
For any Hermitian matrix H, there exists a unitary transformation U that will cause
UHU−1 to be diagonal, with the eigenvalues of H as its diagonal elements.
This is an extremely important result. Another way of stating it is:
A Hermitian matrix can be diagonalized by a unitary transformation, with its eigenval-
ues as the diagonal elements.
Looking next at the ith eigenvector U−1ˆei, we have


(U−1)11
...
(U−1)1i
...
(U−1)1n
(U−1)21
...
(U−1)2i
...
(U−1)2n
···
···
···
···
···
···
···
···
···
···
(U−1)n1
...
(U−1)ni
...
(U−1)nn




0
···
1
···
0


=


(U−1)1i
(U−1)2i
···
···
(U−1)ni


.
(6.24)
We see that the columns of U−1 are the eigenvectors of H, normalized because U−1 is
a unitary matrix. It is also clear from Eq. (6.24) that U−1 is not entirely unique; if its
columns are permuted, all that will happen is that the order of the eigenvectors are changed,
with a corresponding permutation of the diagonal elements of the diagonal matrix UHU−1.
Summarizing,
If a unitary matrix U is such that, for a Hermitian matrix H, UHU−1 is diagonal, the
normalized eigenvector of H corresponding to the eigenvalue (UHU−1)ii will be the
ith column of U−1.
If H is not degenerate, U−1 (and also U) will be unique except for a possible permutation
of the columns of U−1 (and a corresponding permutation of the rows of U). However, if H is
degenerate (has a repeated eigenvalue), then the columns of U−1 corresponding to the same

6.4 Hermitian Matrix Diagonalization
313
eigenvalue can be transformed among themselves, thereby giving additional ﬂexibility to
U and U−1.
Finally, calling on the fact that both the determinant and the trace of a matrix are
unchanged when the matrix is subjected to a unitary transformation (shown in Section 5.7),
we see that the determinant of a Hermitian matrix can be identiﬁed as the product of its
eigenvalues, and its trace will be their sum. Apart from the individual eigenvalues them-
selves, these are the most useful of the invariants that a matrix has with respect to unitary
transformation.
We illustrate the ideas thus far introduced in this section in the next example.
Example 6.4.1
TRANSFORMING A MATRIX TO DIAGONAL FORM
We return to the matrix H of Example 6.2.2:
H =


0
1
0
1
0
0
0
0
2

.
We note that it is Hermitian, so there exists a unitary transformation U that will diagonalize
it. Since we already know the eigenvectors of H, we can use them to construct U. Noting
that we need normalized eigenvectors, and consulting Eqs. (6.9) to (6.11), we have
λ = 2,


0
0
1

;
λ = 1,


1/
√
2
1/
√
2
0

;
λ = −1,


1/
√
2
−1/
√
2
0

.
Combining these as columns into U−1,
U−1 =


0
1/
√
2
1/
√
2
0
1/
√
2
−1/
√
2
1
0
0

.
Since U = (U−1)†, we easily form
U =


0
0
1
1/
√
2
1/
√
2
0
1/
√
2
−1/
√
2
0


and
UHU−1 =


2
0
0
0
1
0
0
0
−1

.
The trace of H is 2, as is the sum of the eigenvalues; det(H) is −2, equal to 2×1×(−1).
■
Finding a Diagonalizing Transformation
As Example 6.4.1 shows, a knowledge of the eigenvectors of a Hermitian matrix H enables
the direct construction of a unitary matrix U that transforms H into diagonal form. But we
are interested in diagonalizing matrices for the purpose of ﬁnding their eigenvectors and
eigenvalues, so the construction illustrated in Example 6.4.1 does not meet our present
needs. Applied mathematicians (and even theoretical chemists!) have over many years

314
Chapter 6 Eigenvalue Problems
given attention to numerical methods for diagonalizing matrices of order large enough that
direct, exact solution of the secular equation is not possible, and computer programs for
carrying out these methods have reached a high degree of sophistication and efﬁciency.
In varying ways, such programs involve processes that approach diagonalization via suc-
cessive approximations. That is to be expected, since explicit formulas for the solution of
high-degree algebraic equations (including, of course, secular equations) do not exist. To
give the reader a sense of the level that has been reached in matrix diagonalization technol-
ogy, we identify a computation1 that determined some of the eigenvalues and eigenvectors
of a matrix whose dimension exceeded 109.
One of the older techniques for diagonalizing a matrix is due to Jacobi. It has now been
supplanted by more efﬁcient (but less transparent) methods, but we discuss it brieﬂy here
to illustrate the ideas involved. The essence of the Jacobi method is that if a Hermitian
matrix H has a nonzero value of some off-diagonal hi j (and thus also hji), a unitary trans-
formation that alters only rows/columns i and j can reduce hi j and hji to zero. While
this transformation may cause other, previously zeroed elements to become nonzero, it can
be shown that the resulting matrix is closer to being diagonal (meaning that the sum of
the squared magnitudes of its off-diagonal elements has been reduced). One may therefore
apply Jacobi-type transformations repeatedly to reduce individual off-diagonal elements to
zero, continuing until there is no off-diagonal element larger than an acceptable tolerance.
If one constructs the unitary matrix that is the product of the individual transformations,
one obtains thereby the overall diagonalizing transformation. Alternatively, one can use
the Jacobi method only for retrieval of the eigenvalues, after which the method presented
previously can be used to obtain the eigenvectors.
Simultaneous Diagonalization
It is of interest to know whether two Hermitian matrices A and B can have a common set
of eigenvectors. It turns out that this is possible if and only if they commute. The proof is
simple if the eigenvectors of either A or B are nondegenerate.
Assume that ci are a set of eigenvectors of both A and B with respective eigenvalues ai
and bi. Then form, for any i,
BAci = Baici = biaici,
ABci = Abici = aibici.
These equations show that BAci = ABci for every ci. Since any vector v can be written as
a linear combination of the ci, we ﬁnd that (BA −AB)v = 0 for all v, which means that
BA = AB. We have found that the existence of a common set of eigenvectors implies com-
mutation. It remains to prove the converse, namely that commutation permits construction
of a common set of eigenvectors.
For the converse, we assume that A and B commute, that ci is an eigenvector of A with
eigenvalue ai, and that this eigenvector of A is nondegenerate. Then we form
ABci = BAci = Baici,
or
A(Bci) = ai(Bci).
1J. Olsen, P. Jørgensen, and J. Simons, Passing the one-billion limit in full conﬁguration-interaction calculations, Chem. Phys.
Lett. 169: 463 (1990).

6.4 Hermitian Matrix Diagonalization
315
This equation shows that Bci is also an eigenvector of A with eigenvalue ai. Since the
eigenvector of A was assumed nondegenerate, Bci must be proportional to ci, meaning
that ci is also an eigenvector of B. This completes the proof that if A and B commute, they
have common eigenvectors.
The proof of this theorem can be extended to include the case in which both opera-
tors have degenerate eigenvectors. Including that extension, we summarize by stating the
general result:
Hermitian matrices have a complete set of eigenvectors in common if and only if they
commute.
It may happen that we have three matrices A, B, and C, and that [A, B] = 0 and [A, C] =
0, but [B, C] ̸= 0. In that case, which is actually quite common in atomic physics, we
have a choice. We can insist upon a set of ci that are simultaneous eigenvectors of A and
B, in which case not all the ci can be eigenvectors of C, or we can have simultaneous
eigenvectors of A and C, but not B. In atomic physics these choices typically correspond
to descriptions in which different angular momenta are required to have deﬁnite values.
Spectral Decomposition
Once the eigenvalues and eigenvectors of a Hermitian matrix H have been found, we
can express H in terms of these quantities. Since mathematicians call the set of eigen-
values of H its spectrum, the expression we now derive for H is referred to as its spectral
decomposition.
As previously noted, in the orthonormal eigenvector basis the matrix H will be diagonal.
Then, instead of the general form for the basis expansion of an operator, H will be of the
diagonal form
H =
X
µ
|cµ⟩λµ⟨cµ|,
each cµ satisﬁes Hcµ = λµcµ and ⟨cµ|cµ⟩= 1.
(6.25)
This result, the spectral decomposition of H, is easily checked by applying it to any eigen-
vector cν.
Another result related to the spectral decomposition of H can be obtained if we multiply
both sides of the equation Hcµ = λµcµ on the left by H, reaching
H2cµ = (λµ)2cµ;
further applications of H show that all positive powers of H have the same eigenvectors
as H, so if f (H) is any function of H that has a power-series expansion, it has spectral
decomposition
f (H) =
X
µ
|cµ⟩f (λµ)⟨cµ|.
(6.26)
Equation (6.26) can be extended to include negative powers if H is nonsingular; to do so,
multiply Hcµ = λµcµ on the left by H−1 and rearrange, to obtain
H−1cµ = 1
λµ
cµ,
showing that negative powers of H also have the same eigenvectors as H.

316
Chapter 6 Eigenvalue Problems
Finally, we can now easily prove the trace formula, Eq. (2.84). In the eigenvector basis,
det

exp(A)

=
Y
µ
eλµ = exp
 X
µ
λµ
!
= exp

trace(A)

.
(6.27)
Since the determinant and trace are basis-independent, this proves the trace formula.
Expectation Values
The expectation value of a Hermitian operator H associated with the normalized function
ψ was deﬁned in Eq. (5.61) as
⟨H⟩= ⟨ψ|H|ψ⟩,
(6.28)
where it was shown that if an orthonormal basis was introduced, with H then represented
by a matrix H and ψ represented by a vector a, this expectation value assumed the form
⟨H⟩= a†Ha = ⟨a|H|a⟩=
X
νµ
a∗
νhνµaµ.
(6.29)
If these quantities are expressed in the orthonormal eigenvector basis, Eq. (6.29) becomes
⟨H⟩=
X
µ
a∗
µλµaµ =
X
µ
|aµ|2λµ,
(6.30)
where aµ is the coefﬁcient of the eigenvector cµ (with eigenvalue λµ) in the expansion
of ψ. We note that the expectation value is a weighted sum of the eigenvalues, with the
weights nonnegative, and adding to unity because
⟨a|a⟩=
X
µ
a∗
µaµ =
X
µ
|aµ|2 = 1.
(6.31)
An obvious implication of Eq. (6.30) is that the expectation value ⟨H⟩cannot be
smaller than the smallest eigenvalue nor larger than the largest eigenvalue. The quantum-
mechanical interpretation of this observation is that if H corresponds to a physical quantity,
measurements of that quantity will yield the values λµ with relative probabilities given by
|aµ|2, and hence with an average value corresponding to the weighted sum, which is the
expectation value.
Hermitian operators arising in physical problems often have ﬁnite smallest eigenvalues.
This, in turn, means that the expectation value of the physical quantity associated with the
operator has a ﬁnite lower bound. We thus have the frequently useful relation
If the algebraically smallest eigenvalue of H is ﬁnite, then, for any ψ, ⟨ψ|H|ψ⟩will
be greater than or equal to this eigenvalue, with the equality occurring only if ψ is an
eigenfunction corresponding to the smallest eigenvalue.

6.4 Hermitian Matrix Diagonalization
317
Positive Deﬁnite and Singular Operators
If all the eigenvalues of an operator A are positive, it is termed positive deﬁnite. If and
only if A is positive deﬁnite, its expectation value for any nonzero ψ, namely ⟨ψ|A|ψ⟩,
will also be positive, since (when ψ is normalized) it must be equal to or larger than the
smallest eigenvalue.
Example 6.4.2
OVERLAP MATRIX
Let S be an overlap matrix of elements sνµ = ⟨χν|χµ⟩, where the χν are members of a
linearly independent, but nonorthogonal basis. If we assume an arbitrary nonzero function
ψ to be expanded in terms of the χν, according to ψ = P
ν bνχν, the scalar product ⟨ψ|ψ⟩
will be given by
⟨ψ|ψ⟩=
X
νµ
b∗
νsνµbµ,
which is of the form of an expectation value for the matrix S. Since ⟨ψ|ψ⟩is an inherently
positive quantity, we conclude that S is positive deﬁnite.
■
If, on the other hand, the rows (or the columns) of a square matrix represent linearly
dependent forms, either as coefﬁcients in a basis-set expansion or as the coefﬁcients of
a linear expression in a set of variables, the matrix will be singular, and that fact will be
signaled by the presence of eigenvalues that are zero. The number of zero eigenvalues
provides an indication of the extent of the linear dependence; if an n ×n matrix has m zero
eigenvalues, its rank will be n −m.
Exercises
6.4.1
Show that the eigenvalues of a matrix are unaltered if the matrix is transformed by a
similarity transformation—a transformation that need not be unitary, but of the form
given in Eq. (5.78).
This property is not limited to symmetric or Hermitian matrices. It holds for any
matrix satisfying an eigenvalue equation of the type Ax = λx. If our matrix can be
brought into diagonal form by a similarity transformation, then two immediate conse-
quences are that:
1.
The trace (sum of eigenvalues) is invariant under a similarity transformation.
2.
The determinant (product of eigenvalues) is invariant under a similarity
transformation.
Note. The invariance of the trace and determinant are often demonstrated by using the
Cayley-Hamilton theorem, which states that a matrix satisﬁes its own characteristic
(secular) equation.
6.4.2
As a converse of the theorem that Hermitian matrices have real eigenvalues and that
eigenvectors corresponding to distinct eigenvalues are orthogonal, show that if

318
Chapter 6 Eigenvalue Problems
(a)
the eigenvalues of a matrix are real and
(b)
the eigenvectors satisfy x†
i x j = δi j,
then the matrix is Hermitian.
6.4.3
Show that a real matrix that is not symmetric cannot be diagonalized by an orthogonal
or unitary transformation.
Hint. Assume that the nonsymmetric real matrix can be diagonalized and develop a
contradiction.
6.4.4
The matrices representing the angular momentum components Lx, L y, and Lz are all
Hermitian. Show that the eigenvalues of L2, where L2 = L2
x + L2
y + L2
z, are real and
nonnegative.
6.4.5
A has eigenvalues λi and corresponding eigenvectors |xi⟩. Show that A−1 has the same
eigenvectors but with eigenvalues λ−1
i
.
6.4.6
A square matrix with zero determinant is labeled singular.
(a)
If A is singular, show that there is at least one nonzero column vector v such that
A|v⟩= 0.
(b)
If there is a nonzero vector |v⟩such that
A|v⟩= 0,
show that A is a singular matrix. This means that if a matrix (or operator) has
zero as an eigenvalue, the matrix (or operator) has no inverse and its determinant
is zero.
6.4.7
Two Hermitian matrices A and B have the same eigenvalues. Show that A and B are
related by a unitary transformation.
6.4.8
Find the eigenvalues and an orthonormal set of eigenvectors for each of the matrices of
Exercise 2.2.12.
6.4.9
The unit process in the iterative matrix diagonalization procedure known as the Jacobi
method is a unitary transformation that operates on rows/columns i and j of a real
symmetric matrix A to make ai j = a ji = 0. If this transformation (from basis functions
ϕi and ϕ j to ϕ′
i and ϕ′
j) is written
ϕ′
i = ϕi cosθ −ϕ j sinθ,
ϕ′
j = ϕi sinθ + ϕ j cosθ,
(a)
Show that ai j is transformed to zero if tan2θ =
2ai j
a j j −aii
,
(b)
Show that aµν remains unchanged if neither µ nor ν is i or j,
(c)
Find a′
ii and a′
j j and show that the trace of A is not changed by the transformation,
(d)
Find a′
iµ and a′
jµ (where µ is neither i nor j) and show that the sum of the squares
of the off-diagonal elements of A is reduced by the amount 2a2
i j.

6.5 Normal Matrices
319
6.5
NORMAL MATRICES
Thus far the discussion has been centered on Hermitian eigenvalue problems, which we
showed to have real eigenvalues and orthogonal eigenvectors, and therefore capable of
being diagonalized by a unitary transformation. However, the class of matrices which can
be diagonalized by a unitary transformation contains, in addition to Hermitian matrices, all
other matrices that commute with their adjoints; a matrix A with this property, namely
[A, A†] = 0,
is termed normal.2 Clearly Hermitian matrices are normal, as H† = H. Unitary matrices
are also normal, as U commutes with its inverse. Anti-Hermitian matrices (with A† = −A)
are also normal. And there exist normal matrices that are not in any of these categories.
To show that normal matrices can be diagonalized by a unitary transformation, it suf-
ﬁces to prove that their eigenvectors can form an orthonormal set, which reduces to the
requirement that eigenvectors of different eigenvalues be orthogonal. The proof proceeds
in two steps, of which the ﬁrst is to demonstrate that a normal matrix A and its adjoint have
the same eigenvectors.
Assuming |x⟩to be an eigenvector of A with eigenvalue λ, we have the equation
(A −λ1)|x⟩= 0.
Multiplying this equation on its left by ⟨x|(A† −λ∗1), we have
⟨x|(A† −λ∗1)(A −λ1)|x⟩= 0,
after which we use the normal property to interchange the two parenthesized quantities,
bringing us to
⟨x|(A −λ1)(A† −λ∗1)|x⟩= 0.
Moving the ﬁrst parenthesized quantity into the left half-bracket, we have
⟨(A† −λ∗1)x|(A† −λ∗1)|x⟩= 0,
which we identify as a scalar product of the form ⟨f | f ⟩. The only way this scalar product
can vanish is if
(A† −λ∗1)|x⟩= 0,
showing that |x⟩is an eigenvector of A† in addition to being an eigenvector of A. However,
the eigenvalues of A and A† are complex conjugates; for general normal matrices λ need
not be real.
A demonstration that the eigenvectors are orthogonal proceeds along the same lines are
for Hermitian matrices. Letting |xi⟩and |x j⟩be two eigenvectors (of both A and A†), we
form
⟨x j|A|xi⟩= λi⟨x j|xi⟩,
⟨xi|A†|x j⟩= λ∗
j⟨xi|x j⟩.
(6.32)
2Normal matrices are the largest class of matrices that can be diagonalized by unitary transformations. For an extensive discus-
sion of normal matrices, see P. A. Macklin, Normal matrices for physicists, Am. J. Phys. 52: 513 (1984).

320
Chapter 6 Eigenvalue Problems
We now take the complex conjugate of the second of these equations, noting that
⟨xi|x j⟩∗= ⟨x j|xi⟩. To form the complex conjugate of ⟨xi|A†|x j⟩, we convert it ﬁrst to
⟨Axi|x j⟩and then interchange the two half-brackets. Equations (6.32) then become
⟨x j|A|xi⟩= λi⟨x j|xi⟩,
⟨x j|A|xi⟩= λ j⟨x j|xi⟩.
(6.33)
These equations indicate that if λi ̸= λ j, we must have ⟨x j|xi⟩= 0, thus proving
orthogonality.
The fact that the eigenvalues of a normal matrix A† are complex conjugates of the eigen-
values of A enables us to conclude that
•
the eigenvalues of an anti-Hermitian matrix are pure imaginary (because A† = −A,
λ∗= −λ), and
•
the eigenvalues of a unitary matrix are of unit magnitude (because λ∗= 1/λ, equivalent
to λ∗λ = 1).
Example 6.5.1
A NORMAL EIGENSYSTEM
Consider the unitary matrix
U =


0
0
1
1
0
0
0
1
0

.
This matrix describes a rotational transformation in which z →x, x →y, and y →z.
Because it is unitary, it is also normal, and we may ﬁnd its eigenvalues from the secular
equation
det(U −λ1) =

−λ
0
1
1
−λ
0
0
1
−λ

= −λ3 + 1 = 0,
which has solutions λ = 1, ω, and ω∗, where ω = e2πi/3. (Note that ω3 = 1, so ω∗=
1/ω = ω2.) Because U is real, unitary and describes a rotation, its eigenvalues must fall on
the unit circle, their sum (the trace) must be real, and their product (the determinant) must
be +1. This means that one of the eigenvalues must be +1, and the remaining two may be
real (both +1 or both −1) or form a complex conjugate pair. We see that the eigenvalues
we have found satisfy these criteria. The trace of U is zero, as is the sum 1 + ω + ω∗(this
may be veriﬁed graphically; see Fig. 6.3).
Proceeding to the eigenvectors, substitution into the equation
(U −λ1)c = 0

6.5 Normal Matrices
321
ω
ω*
1
FIGURE 6.3
Eigenvalues of the matrix U, Example 6.5.1.
yields (in unnormalized form)
λ1 = 1,
c1 =


1
1
1

,
λ2 = ω,
c2 =


1
ω∗
ω

,
λ3 = ω2,
c3 =


1
ω
ω∗

.
The interpretation of this result is interesting. The eigenvector c1 is unchanged by U
(application of U multiplies it by unity), so it must lie in the direction of the axis of the
rotation described by U. The other two eigenvectors are complex linear combinations of
the coordinates that are invariant in “direction,” but not in phase under application of U. We
write “direction” in quotes, since the complex coefﬁcients in the eigenvectors cause them
not to identify directions in physical space. Nevertheless, they do form quantities that are
invariant except for multiplication by the eigenvalue (which we identify as a phase, since
it is of magnitude unity). The argument of ω, 2π/3, identiﬁes the amount of the rotation
about the c1 axis. Coming back to physical reality, we note that we have found that U
corresponds to a rotation of amount 2π/3 about an axis in the (1,1,1) direction; the reader
can verify that this indeed takes x into y, y into z, and z into x.
Because U is normal, its eigenvectors must be orthogonal. Since we now have complex
quantities, in order to check this we must compute the scalar product of two vectors a and
b from the formula a†b. Our eigenvectors pass this test.
Finally, let’s verify that U and U† have the same eigenvectors, and that corresponding
eigenvalues are complex conjugates. Taking the adjoint of U, we have
U† =


0
1
0
0
0
1
1
0
0

.
Using the eigenvectors we have already found to form U†ci, the veriﬁcation is easily estab-
lished. We illustrate with c2:


0
1
0
0
0
1
1
0
0




1
ω∗
ω

=


ω∗
ω
1

= ω∗


1
ω∗
ω

,
as required.
■

322
Chapter 6 Eigenvalue Problems
Nonnormal Matrices
Matrices that are not even normal sometimes enter problems of importance in physics.
Such a matrix, A, still has the property that the eigenvalues of A† are the complex conju-
gates of the eigenvalues of A, because det(A†) = [det(A)]∗, so
det(A −λ1) = 0
−→
det(A† −λ∗1) = 0,
for the same λ, but it is no longer true that the eigenvectors are orthogonal or that A and A†
have common eigenvectors.
Here is an example arising from the analysis of vibrations in mechanical systems. We
consider the vibrations of a classical model of the CO2 molecule. Even though the model is
classical, it is a good representation of the actual quantum-mechanical system, as to good
approximation the nuclei execute small (classical) oscillations in the Hooke’s-law potential
generated by the electron distribution. This problem is an illustration of the application of
matrix techniques to a problem that does not start as a matrix problem. It also provides an
example of the eigenvalues and eigenvectors of an asymmetric real matrix.
Example 6.5.2
NORMAL MODES
Consider three masses on the x-axis joined by springs as shown in Fig. 6.4. The spring
forces are assumed to be linear in the displacements from equilibrium (small displace-
ments, Hooke’s law), and the masses are constrained to stay on the x-axis.
Using a different coordinate for the displacement of each mass from its equilibrium
position, Newton’s second law yields the set of equations
¨x1 = −k
M (x1 −x2)
¨x2 = −k
m (x2 −x1) −k
m (x2 −x3)
(6.34)
¨x3 = −k
M (x3 −x2),
where ¨x stands for d2x/dt2. We seek the frequencies, ω, such that all the masses vibrate
at the same frequency. These are called the normal modes of vibration,3 and are solutions
to Eqs. (6.34) with
xi(t) = xi sin ωt,
i = 1, 2, 3.
C
k
x1
k
O
O
x2
x3
FIGURE 6.4
The three-mass spring system representing the CO2 molecule.
3For detailed discussion of normal modes of vibration, see E. B. Wilson, Jr., J. C. Decius, and P. C. Cross, Molecular
Vibrations—The Theory of Infrared and Raman Vibrational Spectra. New York: Dover (1980).

6.5 Normal Matrices
323
Substituting this solution set into Eqs. (6.34), these equations, after cancellation of the
common factor sinωt, become equivalent to the matrix equation
Ax ≡


k
M
−k
M
0
−k
m
2k
m
−k
m
0
−k
M
k
M




x1
x2
x3

= +ω2


x1
x2
x3

.
(6.35)
We can ﬁnd the eigenvalues of A by solving the secular equation

k
M −ω2
−k
M
0
−k
m
2k
m −ω2
−k
m
0
−k
M
k
M −ω2

= 0,
(6.36)
which expands to
ω2
 k
M −ω2

ω2 −2k
m −k
M

= 0.
The eigenvalues are
ω2 = 0,
k
M ,
k
M + 2k
m .
For ω2 = 0, substitution back into Eq. (6.35) yields
x1 −x2 = 0,
−x1 + 2x2 −x3 = 0,
−x2 + x3 = 0,
which corresponds to x1 = x2 = x3. This describes pure translation with no relative motion
of the masses and no vibration.
For ω2 = k/M, Eq. (6.35) yields
x1 = −x3,
x2 = 0.
The two outer masses are moving in opposite directions. The central mass is stationary. In
CO2 this is called the symmetric stretching mode.
Finally, for ω2 = k/M + 2k/m, the eigenvector components are
x1 = x3,
x2 = −2M
m x1.
In this antisymmetric stretching mode, the two outer masses are moving, together, in a
direction opposite to that of the central mass, so one CO bond stretches while the other
contracts the same amount. In both of these stretching modes, the net momentum of the
motion is zero.
Any displacement of the three masses along the x-axis can be described as a linear
combination of these three types of motion: translation plus two forms of vibration.

324
Chapter 6 Eigenvalue Problems
The matrix A of Eq. (6.35) is not normal; the reader can check that AA† ̸= A†A. As a
result, the eigenvectors we have found are not orthogonal, as is obvious by examination of
the unnormalized eigenvectors:
ω2 = 0,
x =


1
1
1

,
ω2 = k
M ,
x =


1
0
−1

,
ω2 = k
M + 2k
m ,
x =


1
−2M/m
1

.
Using the same λ values, we can solve the simultaneous equations

A† −λ∗1

y = 0.
The resulting eigenvectors are
ω2 = 0,
x =


1
m/M
1

,
ω2 = k
M ,
x =


1
0
1

,
ω2 = k
M + 2k
m ,
x =


1
−2
1

.
These vectors are neither orthogonal nor the same as the eigenvectors of A.
■
Defective Matrices
If a matrix is not normal, it may not even have a full complement of eigenvectors. Such
matrices are termed defective. By the fundamental theorem of algebra, a matrix of dimen-
sion N will have N eigenvalues (when their multiplicity is taken into account). It can also
be shown that any matrix will have at least one eigenvector corresponding to each of its
distinct eigenvalues. But it is not always true that that an eigenvalue of multiplicity k > 1
will have k eigenvectors. We give as a simple example a matrix with the doubly degenerate
eigenvalue λ = 1:
1
1
0
1

has only the single eigenvector
1
0

.
Exercises
6.5.1
Find the eigenvalues and corresponding eigenvectors for
2
4
1
2

.
Note that the eigenvectors are not orthogonal.
ANS.
λ1 = 0, c1 = (2,−1);
λ2 = 4, c2 = (2, 1).
6.5.2
If A is a 2×2 matrix, show that its eigenvalues λ satisfy the secular equation
λ2 −λ trace(A) + det(A) = 0.
6.5.3
Assuming a unitary matrix U to satisfy an eigenvalue equation Uc = λc, show that the
eigenvalues of the unitary matrix have unit magnitude. This same result holds for real
orthogonal matrices.

6.5 Normal Matrices
325
6.5.4
Since an orthogonal matrix describing a rotation in real 3-D space is a special case
of a unitary matrix, such an orthogonal matrix can be diagonalized by a unitary
transformation.
(a)
Show that the sum of the three eigenvalues is 1 + 2cosϕ, where ϕ is the net angle
of rotation about a single ﬁxed axis.
(b)
Given that one eigenvalue is 1, show that the other two eigenvalues must be eiϕ
and e−iϕ.
Our orthogonal rotation matrix (real elements) has complex eigenvalues.
6.5.5
A is an nth-order Hermitian matrix with orthonormal eigenvectors |xi⟩and real eigen-
values λ1 ≤λ2 ≤λ3 ≤··· ≤λn. Show that for a unit magnitude vector |y⟩,
λ1 ≤⟨y|A|y⟩≤λn.
6.5.6
A particular matrix is both Hermitian and unitary. Show that its eigenvalues are all ±1.
Note. The Pauli and Dirac matrices are speciﬁc examples.
6.5.7
For his relativistic electron theory Dirac required a set of four anticommuting matrices.
Assume that these matrices are to be Hermitian and unitary. If these are n ×n matrices,
show that n must be even. With 2×2 matrices inadequate (why?), this demonstrates
that the smallest possible matrices forming a set of four anticommuting, Hermitian,
unitary matrices are 4×4.
6.5.8
A is a normal matrix with eigenvalues λn and orthonormal eigenvectors |xn⟩. Show that
A may be written as
A =
X
n
λn|xn⟩⟨xn|.
Hint. Show that both this eigenvector form of A and the original A give the same result
acting on an arbitrary vector |y⟩.
6.5.9
A has eigenvalues 1 and −1 and corresponding eigenvectors
1
0

and
0
1

.
Construct A.
ANS.
A =
1
0
0
−1

.
6.5.10
A non-Hermitian matrix A has eigenvalues λi and corresponding eigenvectors |ui⟩. The
adjoint matrix A† has the same set of eigenvalues but different corresponding eigen-
vectors, |vi⟩. Show that the eigenvectors form a biorthogonal set in the sense that
⟨vi|u j⟩= 0
for
λ∗
i ̸= λ j.
6.5.11
You are given a pair of equations:
A|fn⟩= λn|gn⟩
˜A|gn⟩= λn|fn⟩
with A real.

326
Chapter 6 Eigenvalue Problems
(a)
Prove that |fn⟩is an eigenvector of (˜AA) with eigenvalue λ2
n.
(b)
Prove that |gn⟩is an eigenvector of (A˜A) with eigenvalue λ2
n.
(c)
State how you know that
(1)
The |fn⟩form an orthogonal set.
(2)
The |gn⟩form an orthogonal set.
(3)
λ2
n is real.
6.5.12
Prove that A of the preceding exercise may be written as
A =
X
n
λn|gn⟩⟨fn|,
with the |gn⟩and ⟨fn| normalized to unity.
Hint. Expand an arbitrary vector as a linear combination of |fn⟩.
6.5.13
Given
A = 1
√
5
2
2
1
−4

,
(a)
Construct the transpose ˜A and the symmetric forms ˜AA and A˜A.
(b)
From A˜A|gn⟩= λ2
n|gn⟩, ﬁnd λn and |gn⟩. Normalize the |gn⟩.
(c)
From ˜AA|fn⟩= λ2
n|gn⟩, ﬁnd λn [same as (b)] and |fn⟩. Normalize the |fn⟩.
(d)
Verify that A|fn⟩= λn|gn⟩and ˜A|gn⟩= λn|fn⟩.
(e)
Verify that A = P
n λn|gn⟩⟨fn|.
6.5.14
Given the eigenvalues λ1 = 1,λ2 = −1 and the corresponding eigenvectors
|f1⟩=
1
0

,
|g1⟩= 1
√
2
1
1

,
|f2⟩=
0
1

,
and
|g2⟩= 1
√
2
 1
−1

,
(a)
construct A;
(b)
verify that A|fn⟩= λn|gn⟩;
(c)
verify that ˜A|gn⟩= λn|fn⟩.
ANS.
A = 1
√
2
1
−1
1
1

.
6.5.15
Two matrices U and H are related by
U = eiaH,
with a real.

6.5 Normal Matrices
327
(a)
If H is Hermitian, show that U is unitary.
(b)
If U is unitary, show that H is Hermitian. (H is independent of a.)
(c)
If trace H = 0, show that det U = +1.
(d)
If det U = +1, show that trace H = 0.
Hint. H may be diagonalized by a similarity transformation. Then U is also diagonal.
The corresponding eigenvalues are given by u j = exp(iah j).
6.5.16
An n ×n matrix A has n eigenvalues Ai. If B = eA, show that B has the same eigen-
vectors as A with the corresponding eigenvalues Bi given by Bi = exp(Ai).
6.5.17
A matrix P is a projection operator satisfying the condition
P2 = P.
Show that the corresponding eigenvalues (ρ2)λ and ρλ satisfy the relation
(ρ2)λ = (ρλ)2 = ρλ.
This means that the eigenvalues of P are 0 and 1.
6.5.18
In the matrix eigenvector-eigenvalue equation
A|xi⟩= λi|xi⟩,
A is an n ×n Hermitian matrix. For simplicity assume that its n real eigenvalues are
distinct, λ1 being the largest. If |x⟩is an approximation to |x1⟩,
|x⟩= |x1⟩+
n
X
i=2
δi|xi⟩,
show that
⟨x|A|x⟩
⟨x|x⟩≤λ1
and that the error in λ1 is of the order |δi|2. Take |δi| << 1.
Hint. The n vectors |xi⟩form a complete orthogonal set spanning the n-dimensional
(complex) space.
6.5.19
Two equal masses are connected to each other and to walls by springs as shown in
Fig. 6.5. The masses are constrained to stay on a horizontal line.
(a)
Set up the Newtonian acceleration equation for each mass.
(b)
Solve the secular equation for the eigenvectors.
(c)
Determine the eigenvectors and thus the normal modes of motion.
6.5.20
Given a normal matrix A with eigenvalues λ j, show that A† has eigenvalues λ∗
j, its
real part (A + A†)/2 has eigenvalues ℜe(λ j), and its imaginary part (A −A†)/2i has
eigenvalues ℑm(λ j).

328
Chapter 6 Eigenvalue Problems
k
k
k
m
m
FIGURE 6.5
Triple oscillator.
6.5.21
Consider a rotation given by Euler angles α = π/4, β = π/2, γ = 5π/4.
(a)
Using the formula of Eq. (3.37), construct the matrix U representing this rotation.
(b)
Find the eigenvalues and eigenvectors of U, and from them describe this rotation
by specifying a single rotation axis and an angle of rotation about that axis.
Note. This technique provides a representation of rotations alternative to the Euler
angles.
Additional Readings
Bickley, W. G., and R. S. H. G. Thompson, Matrices—Their Meaning and Manipulation. Princeton, NJ: Van
Nostrand (1964). A comprehensive account of matrices in physical problems, and their analytic properties and
numerical techniques.
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969), reprinting, Dover (1992).
Gilbert, J. and L. Gilbert, Linear Algebra and Matrix Theory. San Diego: Academic Press (1995).
Golub, G. H., and C. F. Van Loan, Matrix Computations, 3rd ed. Baltimore: JHU Press (1996). Detailed mathe-
matical background and algorithms for the production of numerical software, including methods for parallel
computation. A classic computer science text.
Halmos, P. R., Finite-Dimensional Vector Spaces, 2nd ed. Princeton, NJ: Van Nostrand (1958), reprinting,
Springer (1993).
Hirsch, M., Differential Equations, Dynamical Systems, and Linear Algebra. San Diego: Academic Press (1974).
Heading, J., Matrix Theory for Physicists. London: Longmans, Green and Co. (1958). A readable introduction to
determinants and matrices, with applications to mechanics, electromagnetism, special relativity, and quantum
mechanics.
Jain, M. C., Vector Spaces and Matrices in Physics, 2nd ed. Oxford: Alpha Science International (2007).
Watkins, D. S., Fundamentals of Matrix Computations. New York: Wiley (1991).
Wilkinson, J. H., The Algebraic Eigenvalue Problem. London: Oxford University Press (1965), reprinting (2004).
Classic treatise on numerical computation of eigenvalue problems. Perhaps the most widely read book in the
ﬁeld of numerical analysis.

CHAPTER 7
ORDINARY DIFFERENTIAL
EQUATIONS
Much of theoretical physics is originally formulated in terms of differential equations in the
three-dimensional physical space (and sometimes also time). These variables (e.g., x, y, z,
t) are usually referred to as independent variables, while the function or functions being
differentiated are referred to as dependent variable(s). A differential equation involving
more than one independent variable is called a partial differential equation, often abbre-
viated PDE. The simpler situation considered in the present chapter is that of an equation
in a single independent variable, known as an ordinary differential equation, abbreviated
ODE. As we shall see in a later chapter, some of the most frequently used methods for solv-
ing PDEs involve their expression in terms of the solutions to ODEs, so it is appropriate to
begin our study of differential equations with ODEs.
7.1
INTRODUCTION
To start, we note that the taking of a derivative is a linear operation, meaning that
d
dx
 aϕ(x) + bψ(x)

= a dϕ
dx + bdψ
dx ,
and the derivative operation can be viewed as deﬁning a linear operator: L = d/dx. Higher
derivatives are also linear operators, as for example
d2
dx2
 aϕ(x) + bψ(x)

= a d2ϕ
dx2 + bd2ψ
dx2 .
329
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

330
Chapter 7 Ordinary Differential Equations
Note that the linearity under discussion is that of the operator. For example, if we deﬁne
L = p(x) d
dx + q(x),
it is identiﬁed as linear because
L
 aϕ(x) + bψ(x)

= a

p(x)dϕ
dx + q(x)ϕ

+ b

p(x)dψ
dx + q(x)ψ

= aLϕ + bLψ.
We see that the linearity of L imposes no requirement that either p(x) or q(x) be a linear
function of x. Linear differential operators therefore include those of the form
L ≡
n
X
ν=0
pν(x)
 dν
dxν

,
where the functions pν(x) are arbitrary.
An ODE is termed homogeneous if the dependent variable (here ϕ) occurs to the same
power in all its terms, and inhomogeneous otherwise; it is termed linear if it can be written
in the form
Lϕ(x) = F(x),
(7.1)
where L is a linear differential operator and F(x) is an algebraic function of x (i.e., not
a differential operator). An important class of ODEs are those that are both linear and
homogeneous, and thereby of the form Lϕ = 0.
The solutions to ODEs are in general not unique, and if there are multiple solutions it
is useful to identify those that are linearly independent (linear dependence is discussed in
Section 2.1). Homogeneous linear ODEs have the general property that any multiple of a
solution is also a solution, and that if there are multiple linearly independent solutions, any
linear combination of those solutions will also solve the ODE. This statement is equivalent
to noting that if L is linear, then, for all a and b,
Lϕ = 0
and
Lψ = 0
−→
L(aϕ + bψ) = 0.
The Schrödinger equation of quantum mechanics is a homogeneous linear ODE (or if in
more than one dimension, a homogeneous linear PDE), and the property that any linear
combination of its solutions is also a solution is the conceptual basis for the well-known
superposition principle in electrodynamics, wave optics and quantum theory.
Notationally, it is often convenient to use the symbols x and y to refer, respectively,
to independent and dependent variables, and a typical linear ODE then takes the form
Ly = F(x). It is also customary to use primes to indicate derivatives: y′ ≡dy/dx. In
terms of this notation, the superposition property of solutions y1 and y2 of a homogeneous
linear ODE tells us that the ODE also has as solutions c1y1, c2y2, and c1y1 + c2y2, with
the ci arbitrary constants.
Some physically important problems (particularly in ﬂuid mechanics and in chaos the-
ory) give rise to nonlinear differential equations. A well-studied example is the Bernoulli
equation
y′ = p(x)y + q(x)yn,
n ̸= 0, 1,
which cannot be written in terms of a linear operator applied to y.

7.2 First-Order Equations
331
Further terms used to classify ODEs include their order (highest derivative appear-
ing therein), and degree (power to which the highest derivative appears after the ODE is
rationalized if that is necessary). For many applications, the concept of linearity is more
relevant than that of degree.
7.2
FIRST-ORDER EQUATIONS
Physics involves some ﬁrst-order differential equations. For completeness it seems desir-
able to touch upon them brieﬂy. We consider the general form
dy
dx = f (x, y) = −P(x, y)
Q(x, y).
(7.2)
While there is no systematic way to solve the most general ﬁrst-order ODE, there are
a number of techniques that are often useful. After reviewing some of these techniques,
we proceed to a more detailed treatment of linear ﬁrst-order ODEs, for which systematic
procedures are available.
Separable Equations
Frequently Eq. (7.2) will have the special form
dy
dx = −P(x)
Q(y).
(7.3)
Then it may be rewritten as
P(x)dx + Q(y)dy = 0.
Integrating from (x0, y0) to (x, y) yields
x
Z
x0
P(x)dx +
y
Z
y0
Q(y)dy = 0.
Since the lower limits, x0 and y0, contribute constants, we may ignore them and simply add
a constant of integration. Note that this separation of variables technique does not require
that the differential equation be linear.
Example 7.2.1
PARACHUTIST
We want to ﬁnd the velocity of a falling parachutist as a function of time and are partic-
ularly interested in the constant limiting velocity, v0, that comes about by air drag, taken
to be quadratic, −bv2, and opposing the force of the gravitational attraction, mg, of the
Earth on the parachutist. We choose a coordinate system in which the positive direction
is downward so that the gravitational force is positive. For simplicity we assume that the
parachute opens immediately, that is, at time t = 0, where v(t) = 0, our initial condition.
Newton’s law applied to the falling parachutist gives
m ˙v = mg −bv2,
(7.4)
where m includes the mass of the parachute.

332
Chapter 7 Ordinary Differential Equations
The terminal velocity, v0, can be found from the equation of motion as t →∞; when
there is no acceleration, ˙v = 0, and
bv2
0 = mg,
or
v0 =
rmg
b .
It simpliﬁes further work to rewrite Eq. (7.4) as
m
b ˙v = v2
0 −v2.
This equation is separable, and we write it in the form
dv
v2
0 −v2 = b
m dt.
(7.5)
Using partial fractions to write
1
v2
0 −v2 = 1
2v0

1
v + v0
−
1
v −v0

,
it is straightforward to integrate both sides of Eq. (7.5) (the left-hand side from v = 0 to v,
the right-hand side from t = 0 to t), yielding
1
2v0
ln v0 + v
v0 −v = b
m t.
Solving for the velocity, we have
v = e2t/T −1
e2t/T + 1v0 = v0
sinh(t/T )
cosh(t/T ) = v0 tanh t
T ,
where T = √m/gb is the time constant governing the asymptotic approach of the velocity
to its limiting value, v0.
Inserting numerical values, g = 9.8 m/s2, and taking b = 700 kg/m, m = 70 kg, gives
v0 = √9.8/10 ≈1 m/s ≈3.6 km/h ≈2.234 mi/h, the walking speed of a pedestrian at
landing, and T = √m/bg = 1/
√
10 · 9.8 ≈0.1 s. Thus, the constant speed v0 is reached
within a second. Finally, because it is always important to check the solution, we verify
that our solution satisﬁes the original differential equation:
˙v = cosh(t/T )
cosh(t/T )
v0
T −sinh2(t/T )
cosh2(t/T )
v0
T = v0
T −v2
T v0
= g −b
m v2.
A more realistic case, where the parachutist is in free fall with an initial speed v(0) > 0
before the parachute opens, is addressed in Exercise 7.2.16.
■

7.2 First-Order Equations
333
Exact Differentials
Again we rewrite Eq. (7.2) as
P(x, y)dx + Q(x, y)dy = 0.
(7.6)
This equation is said to be exact if we can match the left-hand side of it to a differential
dϕ, and thereby reach
dϕ = ∂ϕ
∂x dx + ∂ϕ
∂y dy = 0.
(7.7)
Exactness therefore implies that there exists a function ϕ(x, y) such that
∂ϕ
∂x = P(x, y)
and
∂ϕ
∂y = Q(x, y),
(7.8)
because then our ODE corresponds to an instance of Eq. (7.7), and its solution will be
ϕ(x, y) = constant.
Before seeking to ﬁnd a function ϕ satisfying Eq. (7.8), it is useful to determine whether
such a function exists. Taking the two formulas from Eq. (7.8), differentiating the ﬁrst with
respect to y and the second with respect to x, we ﬁnd
∂2ϕ
∂y∂x = ∂P(x, y)
∂y
and
∂2ϕ
∂x∂y = ∂Q(x, y)
∂x
,
and these are consistent if and only if
∂P(x, y)
∂y
= ∂Q(x, y)
∂x
.
(7.9)
We therefore conclude that Eq. (7.6) is exact only if Eq. (7.9) is satisﬁed. Once exactness
has been veriﬁed, we can integrate Eqs. (7.8) to obtain ϕ and therewith a solution to the
ODE.
The solution takes the form
ϕ(x, y) =
x
Z
x0
P(x, y)dx +
y
Z
y0
Q(x0, y)dy = constant.
(7.10)
Proof of Eq. (7.10) is left to Exercise 7.2.7.
We note that separability and exactness are independent attributes. All separable ODEs
are automatically exact, but not all exact ODEs are separable.
Example 7.2.2
A NONSEPARABLE EXACT ODE
Consider the ODE
y′ +

1 + y
x

= 0.
Multiplying by x dx, this ODE becomes
(x + y)dx + x dy = 0,

334
Chapter 7 Ordinary Differential Equations
which is of the form
P(x, y)dx + Q(x, y)dy = 0,
with P(x, y) = x + y and Q(x, y) = x. The equation is not separable. To check if it is
exact, we compute
∂P
∂y = ∂(x + y)
∂y
= 1,
∂Q
∂x = ∂x
∂x = 1.
These partial derivatives are equal; the equation is exact, and can be written in the form
dϕ = P dx + Q dy = 0.
The solution to the ODE will be ϕ = C, with ϕ computed according to Eq. (7.10):
ϕ =
x
Z
x0
(x + y)dx +
y
Z
y0
x0dy =
 
x2
2 + xy −x2
0
2 −x0y
!
+ (xoy −x0y0)
= x2
2 + xy + constant terms.
Thus, the solution is
x2
2 + xy = C,
which if desired can be solved to give y as a function of x. We can also check to make sure
that our solution actually solves the ODE.
■
It may well turn out that Eq. (7.6) is not exact and that Eq. (7.9) is not satisﬁed. However,
there always exists at least one and perhaps an inﬁnity of integrating factors α(x, y) such
that
α(x, y)P(x, y)dx + α(x, y)Q(x, y)dy = 0
is exact. Unfortunately, an integrating factor is not always obvious or easy to ﬁnd. A sys-
tematic way to develop an integrating factor is known only when a ﬁrst-order ODE is
linear; this will be discussed in the subsection on linear ﬁrst-order ODEs.
Equations Homogeneous in x and y
An ODE is said to be homogeneous (of order n) in x and y if the combined powers of
x and y add to n in all the terms of P(x, y) and Q(x, y) when the ODE is written as in
Eq. (7.6). Note that this use of the term “homogeneous” has a different meaning than when
it was used to describe a linear ODE as given in Eq. (7.1) with the term F(x) equal to zero,
because it now applies to the combined power of x and y.
A ﬁrst-order ODE, which is homogeneous of order n in the present sense (and not nec-
essarily linear), can be made separable by the substitution y = xv, with dy = x dv + v dx.
This substitution causes the x dependence of all the terms of the equation containing dv to
be xn+1, with all the terms containing dx having x-dependence xn. The variables x and v
can then be separated.

7.2 First-Order Equations
335
Example 7.2.3
AN ODE HOMOGENEOUS IN x AND y
Consider the ODE
(2x + y)dx + x dy = 0,
which is homogeneous in x and y. Making the substitution y = xv, with dy = x dv +v dx,
the ODE becomes
(2v + 2)dx + x dv = 0,
which is separable, with solution ln x + 1
2 ln(v+1) = C, which is equivalent to x2(v+1) =
C. Forming y = xv, the solution can be rearranged into
y = C
x −x.
■
Isobaric Equations
A generalization of the preceding subsection is to modify the deﬁnition of homogeneity by
assigning different weights to x and y (note that corresponding weights must then also be
assigned to dx and dy). If assigning unit weight to each instance of x or dx and a weight
m to each instance of y or dy makes the ODE homogeneous as deﬁned here, then the
substitution y = xmv will make the equation separable. We illustrate with an example.
Example 7.2.4
AN ISOBARIC ODE
Here is an isobaric ODE:
(x2 −y)dx + x dy = 0.
Assigning x weight 1, and y weight m, the term x2dx has weight 3; the other two terms
have weight 1 + m. Setting 3 = 1 + m, we ﬁnd that all terms can be assigned equal weight
if we take m = 2. This means that we should make the substitution y= x2v. Doing so,
we get
(1 −v)dx + x dv = 0,
which separates into
dx
x +
dv
v + 1 = 0
−→
ln x + ln(v + 1) = lnC,
or
x(v + 1) = C.
From this, we get v = C
x −1. Since y = x2v, the ODE has solution y = Cx −x2.
■

336
Chapter 7 Ordinary Differential Equations
Linear First-Order ODEs
While nonlinear ﬁrst-order ODEs can often (but not always) be solved using the strategies
already presented, the situation is different for the linear ﬁrst-order ODE because proce-
dures exist for solving the most general equation of this type, which we write in the form
dy
dx + p(x)y = q(x).
(7.11)
If our linear ﬁrst-order ODE is exact, its solution is straightforward. If it is not exact, we
make it exact by introducing an integrating factor α(x), so that the ODE becomes
α(x)dy
dx + α(x)p(x)y = α(x)q(x).
(7.12)
The reason for multiplication by α(x) is to cause the left-hand side of Eq. (7.12) to become
a perfect differential, so we require that α(x) be such that
d
dx

α(x)y

= α(x)dy
dx + α(x)p(x)y.
(7.13)
Expanding the left-hand side of Eq. (7.13), that equation becomes
α(x)dy
dx + dα
dx y = α(x)dy
dx + α(x)p(x)y,
so α must satisfy
dα
dx = α(x)p(x).
(7.14)
This is a separable equation and therefore soluble. Separating the variables and integrat-
ing, we obtain
α
Z dα
α =
x
Z
p(x)dx.
We need not consider the lower limits of these integrals because they combine to yield a
constant that does not affect the performance of the integrating factor and can be set to
zero. Completing the evaluation, we reach
α(x) = exp


x
Z
p(x)dx

.
(7.15)
With α now known we proceed to integrate Eq. (7.12), which, because of Eq. (7.13),
assumes the form
d
dx [α(x)y(x)] = α(x)q(x),
which can be integrated (and divided through by α) to yield
y(x) =
1
α(x)


x
Z
α(x)q(x)dx + C

≡y2(x) + y1(x).
(7.16)

7.2 First-Order Equations
337
The two terms of Eq. (7.16) have an interesting interpretation. The term y1 = C/α(x)
is the general solution of the homogeneous equation obtained by replacing q(x) with zero.
To see this, write the homogeneous equation as
dy
y = −p(x)dx,
which integrates to
ln y = −
x
Z
p(x)dx + C = −lnα + C.
Taking the exponential of both sides and renaming eC as C, we get just y = C/α(x). The
other term of Eq. (7.16),
y2 =
1
α(x)
x
Z
α(x)q(x)dx
(7.17)
corresponds to the right-hand side (source) term q(x), and is a solution of the original
inhomogeneous equation (as is obvious because C can be set to zero). We thus have the
general solution to the inhomogeneous equation presented as a particular solution (or,
in ODE parlance, a particular integral) plus the general solution to the corresponding
homogeneous equation.
The above observations illustrate the following theorem:
The solution of an inhomogeneous ﬁrst-order linear ODE is unique except for an arbi-
trary multiple of the solution of the corresponding homogeneous ODE.
To show this, suppose y1 and y2 both solve the inhomogeneous ODE, Eq. (7.11). Then,
subtracting the equation for y2 from that for y1, we have
y′
1 −y′
2 + p(x)(y1 −y2) = 0.
This shows that y1 −y2 is (at some scale) a solution of the homogeneous ODE. Remember
that any solution of the homogeneous ODE remains a solution when multiplied by an
arbitrary constant.
We also have the theorem:
A ﬁrst-order linear homogeneous ODE has only one linearly independent solution.
Two functions y1(x) and y2(x) are linearly dependent if there exist two constants a and
b, both nonzero, that cause ay1 + by2 to vanish for all x. In the present situation, this is
equivalent to the statement that y1 and y2 are linearly dependent if they are proportional to
each other.
To prove the theorem, assume that the homogeneous ODE has the linearly independent
solutions y1 and y2. Then, from the homogeneous ODE, we have
y′
1
y1
= −p(x) = y′
2
y2
.
Integrating the ﬁrst and last members of this equation, we obtain
ln y1 = ln y2 + C,
equivalent to
y1 = Cy2,
contradicting our assumption that y1 and y2 are linearly independent.

338
Chapter 7 Ordinary Differential Equations
Example 7.2.5
RL CIRCUIT
For a resistance-inductance circuit Kirchoff’s law leads to
L dI (t)
dt
+ RI (t) = V (t),
where I (t) is the current, L and R are, respectively, constant values of the inductance and
the resistance, and V (t) is the time-dependent input voltage.
From Eq. (7.15), our integrating factor α(t) is
α(t) = exp
tZ R
L dt = eRt/L.
Then, by Eq. (7.16),
I (t) = e−Rt/L


tZ
eRt/L V (t)
L
dt + C

,
with the constant C to be determined by an initial condition.
For the special case V (t) = V0, a constant,
I (t) = e−Rt/L
V0
L . L
R eRt/L + C

= V0
R + Ce−Rt/L.
If the initial condition is I (0) = 0, then C = −V0/R and
I (t) = V0
R
h
1 −e−Rt/Li
.
■
We close this section by pointing out that the inhomogeneous linear ﬁrst-order ODE can
also be solved by a method called variation of the constant, or alternatively variation of
parameters, as follows. First, we solve the homogeneous ODE y′ + py = 0 by separation
of variables as before, giving
y′
y = −p,
ln y = −
x
Z
p(X)d X + lnC,
y(x) = C exp

−
x
Z
p(X)d X

.
Next we allow the integration constant to become x-dependent, that is, C →C(x). This is
the reason the method is called “variation of the constant.” To prepare for substitution into
the inhomogeneous ODE, we calculate y′:
y′ = exp

−
x
Z
p(X)d X


−pC(x) + C′(x)

= −py(x) + C′(x)exp

−
x
Z
p(X)d X

.
Making the substitution for y′ into the inhomogeneous ODE y′ + py = q, some cancella-
tion occurs, and we are left with
C′(x)exp

−
x
Z
p(X)d X

= q,

7.2 First-Order Equations
339
which is a separable ODE for C(x) that integrates to yield
C(x) =
x
Z
exp


X
Z
p(Y)dY

q(X)d X
and
y = C(x)exp

−
x
Z
p(X)d X

.
This particular solution of the inhomogeneous ODE is in agreement with that called y2 in
Eq. (7.17).
Exercises
7.2.1
From Kirchhoff’s law the current I in an RC (resistance-capacitance) circuit (Fig. 7.1)
obeys the equation
R dI
dt + 1
C I = 0.
(a)
Find I (t).
(b)
For a capacitance of 10,000 µF charged to 100 V and discharging through a resis-
tance of 1 M, ﬁnd the current I for t = 0 and for t = 100 seconds.
Note. The initial voltage is I0R or Q/C, where Q =
R ∞
0
I (t)dt.
7.2.2
The Laplace transform of Bessel’s equation (n = 0) leads to
(s2 + 1) f ′(s) + s f (s) = 0.
Solve for f (s).
7.2.3
The decay of a population by catastrophic two-body collisions is described by
dN
dt = −kN 2.
This is a ﬁrst-order, nonlinear differential equation. Derive the solution
N(t) = N0

1 + t
τ0
−1
,
where τ0 = (kN0)−1. This implies an inﬁnite population at t = −τ0.
C
+
−
R
FIGURE 7.1
RC circuit.

340
Chapter 7 Ordinary Differential Equations
7.2.4
The rate of a particular chemical reaction A + B →C is proportional to the concentra-
tions of the reactants A and B:
dC(t)
dt
= α[A(0) −C(t)][B(0) −C(t)].
(a)
Find C(t) for A(0) ̸= B(0).
(b)
Find C(t) for A(0) = B(0).
The initial condition is that C(0) = 0.
7.2.5
A boat, coasting through the water, experiences a resisting force proportional to vn,v
being the boat’s instantaneous velocity. Newton’s second law leads to
m dv
dt = −kvn.
With v(t = 0) = v0, x(t = 0) = 0, integrate to ﬁnd v as a function of time and v as a
function of distance.
7.2.6
In the ﬁrst-order differential equation dy/dx = f (x, y), the function f (x, y) is a func-
tion of the ratio y/x:
dy
dx = g(y/x).
Show that the substitution of u = y/x leads to a separable equation in u and x.
7.2.7
The differential equation
P(x, y)dx + Q(x, y)dy = 0
is exact. Show that its solution is of the form
ϕ(x, y) =
x
Z
x0
P(x, y)dx +
y
Z
y0
Q(x0, y)dy = constant.
7.2.8
The differential equation
P(x, y)dx + Q(x, y)dy = 0
is exact. If
ϕ(x, y) =
x
Z
x0
P(x, y)dx +
y
Z
y0
Q(x0, y)dy,
show that
∂ϕ
∂x = P(x, y),
∂ϕ
∂y = Q(x, y).
Hence, ϕ(x, y) = constant is a solution of the original differential equation.
7.2.9
Prove that Eq. (7.12) is exact in the sense of Eq. (7.9), provided that α(x) satisﬁes
Eq. (7.14).

7.2 First-Order Equations
341
7.2.10
A certain differential equation has the form
f (x)dx + g(x)h(y)dy = 0,
with none of the functions f (x), g(x),h(y) identically zero. Show that a necessary and
sufﬁcient condition for this equation to be exact is that g(x) = constant.
7.2.11
Show that
y(x) = exp

−
x
Z
p(t)dt





x
Z
exp


s
Z
p(t)dt

q(s)ds + C



is a solution of
dy
dx + p(x)y(x) = q(x)
by differentiating the expression for y(x) and substituting into the differential equation.
7.2.12
The motion of a body falling in a resisting medium may be described by
m dv
dt = mg −bv
when the retarding force is proportional to the velocity, v. Find the velocity. Evaluate
the constant of integration by demanding that v(0) = 0.
7.2.13
Radioactive nuclei decay according to the law
dN
dt = −λN,
N being the concentration of a given nuclide and λ, the particular decay constant. In
a radioactive series of two different nuclides, with concentrations N1(t) and N2(t), we
have
dN1
dt
= −λ1N1,
dN2
dt
= λ1N1 −λ2N2.
Find N2(t) for the conditions N1(0) = N0 and N2(0) = 0.
7.2.14
The rate of evaporation from a particular spherical drop of liquid (constant density) is
proportional to its surface area. Assuming this to be the sole mechanism of mass loss,
ﬁnd the radius of the drop as a function of time.
7.2.15
In the linear homogeneous differential equation
dv
dt = −av
the variables are separable. When the variables are separated, the equation is exact.
Solve this differential equation subject to v(0) = v0 by the following three methods:
(a)
Separating variables and integrating.
(b)
Treating the separated variable equation as exact.

342
Chapter 7 Ordinary Differential Equations
(c)
Using the result for a linear homogeneous differential equation.
ANS.
v(t) = v0e−at.
7.2.16
(a)
Solve Example 7.2.1, assuming that the parachute opens when the parachutist’s
velocity has reached vi = 60 mi/h (regard this time as t = 0). Find v(t).
(b)
For a skydiver in free fall use the friction coefﬁcient b = 0.25 kg/m and mass
m = 70 kg. What is the limiting velocity in this case?
7.2.17
Solve the ODE
(xy2 −y)dx + x dy = 0.
7.2.18
Solve the ODE
(x2 −y2ey/x)dx + (x2 + xy)ey/xdy = 0.
Hint. Note that the quantity y/x in the exponents is of combined degree zero and does
not affect the determination of homogeneity.
7.3
ODES WITH CONSTANT COEFFICIENTS
Before addressing second-order ODEs, the main topic of this chapter, we discuss a special-
ized, but frequently occurring class of ODEs that are not constrained to be of speciﬁc order,
namely those that are linear and whose homogeneous terms have constant coefﬁcients. The
generic equation of this type is
dny
dxn + an−1
dn−1y
dxn−1 + ··· + a1
dy
dx + a0y = F(x).
(7.18)
The homogeneous equation corresponding to Eq. (7.18) has solutions of the form y = emx,
where m is a solution of the algebraic equation
mn + an−1mn−1 + ··· + a1m + a0 = 0,
as may be veriﬁed by substitution of the assumed form of the solution.
In the case that the m equation has a multiple root, the above prescription will not yield
the full set of n linearly independent solutions for the original n th order ODE. If one then
considers the limiting process in which two roots approach each other, it is possible to
conclude that if emx is a solution, then so is d emx/dm = xemx. A triple root would have
solutions emx, xemx, x2emx, etc.
Example 7.3.1
HOOKE’S LAW SPRING
A mass M attached to a Hooke’s Law spring (of spring constant k) is in oscillatory motion.
Letting y be the displacement of the mass from its equilibrium position, Newton’s law of
motion takes the form
M d2y
dt2 = −ky,

7.4 Second-Order Linear ODEs
343
which is an ODE of the form y′′ + a0y = 0, with a0 = +k/M. The general solution to this
ODE is of the form C1em1t + C2em2t, where m1 and m2 are the solutions of the algebraic
equation m2 + a0 = 0.
The values of m1 and m2 are ±iω, where ω = √k/M, so the ODE has solution
y(t) = C1e+iωt + C2e−iωt.
Since the ODE is homogeneous, we may alternatively describe its general solution using
arbitrary linear combinations of the above two terms. This permits us to combine them to
obtain forms that are real and therefore appropriate to the current problem. Noting that
eiωt + e−iωt
2
= cosωt
and
eiωt −e−iωt
2i
= sinωt,
a convenient alternate form is
y(t) = C1 cosωt + C2 sinωt.
The solution to a speciﬁc oscillation problem will now involve ﬁtting the coefﬁcients
C1 and C2 to the initial conditions, as for example y(0) and y′(0).
■
Exercises
Find the general solutions to the following ODEs. Write the solutions in forms that are
entirely real (i.e., that contain no complex quantities).
7.3.1
y′′′ −2y′′ −y′ + 2y = 0.
7.3.2
y′′′ −2y′′ + y′ −2y = 0.
7.3.3
y′′′ −3y′ + 2y = 0.
7.3.4
y′′ + 2y′ + 2y = 0.
7.4
SECOND-ORDER LINEAR ODES
We now turn to the main topic of this chapter, second-order linear ODEs. These are of
particular importance because they arise in the most frequently used methods for solving
PDEs in quantum mechanics, electromagnetic theory, and other areas in physics. Unlike
the ﬁrst-order linear ODE, we do not have a universally applicable closed-form solution,
and in general it is found advisable to use methods that produce solutions in the form of
power series. As a precursor to the general discussion of series-solution methods, we begin
by examining the notion of singularity as applied to ODEs.
Singular Points
The concept of singularity of an ODE is important to us for two reasons: (1) it is useful
for classifying ODEs and identifying those that can be transformed into common forms
(discussed later in this subsection), and (2) it bears on the feasibility of ﬁnding series

344
Chapter 7 Ordinary Differential Equations
solutions to the ODE. This feasibility is the topic of Fuchs’ theorem (to be discussed
shortly).
When a linear homogeneous second-order ODE is written in the form
y′′ + P(x)y′ + Q(x)y = 0,
(7.19)
points x0 for which P(x) and Q(x) are ﬁnite are termed ordinary points of the ODE.
However, if either P(x) or Q(x) diverge as x →x0, the point x0 is called a singular point.
Singular points are further classiﬁed as regular or irregular (the latter also sometimes
called essential singularities):
•
A singular point x0 is regular if either P(x) or Q(x) diverges there, but (x −x0)P(x)
and (x −x0)2Q(x) remain ﬁnite.
•
A singular point x0 is irregular if P(x) diverges faster than 1/(x −x0) so that (x −
x0)P(x) goes to inﬁnity as x →x0, or if Q(x) diverges faster than 1/(x −x0)2 so that
(x −x0)2Q(x) goes to inﬁnity as x →x0.
These deﬁnitions hold for all ﬁnite values of x0. To analyze the behavior at x →∞, we
set x = 1/z, substitute into the differential equation, and examine the behavior in the limit
z →0. The ODE, originally in the dependent variable y(x), will now be written in terms
of w(z), deﬁned as w(z) = y(z−1). Converting the derivatives,
y′ = dy(x)
dx
= dy(z−1)
dz
dz
dx = dw(z)
dz

−1
x2

= −z2w′,
(7.20)
y′′ = dy′
dz
dz
dx = (−z2) d
dz

−z2w′
= z4w′′ + 2z3w′.
(7.21)
Using Eqs. (7.20) and (7.21), we transform Eq. (7.19) into
z4w′′ +

2z3 −z2P(z−1)

w′ + Q(z−1)w = 0.
(7.22)
Dividing through by z4 to place the ODE in standard form, we see that the possibility of a
singularity at z = 0 depends on the behavior of
2z −P(z−1)
z2
and
Q(z−1)
z4
.
If these two expressions remain ﬁnite at z = 0, the point x = ∞is an ordinary point. If
they diverge no more rapidly than 1/z and 1/z2, respectively, x = ∞is a regular singular
point; otherwise it is an irregular singular point (an essential singularity).
Example 7.4.1
BESSEL’S EQUATION
Bessel’s equation is
x2y′′ + xy′ + (x2 −n2)y = 0.

7.4 Second-Order Linear ODEs
345
Comparing it with Eq. (7.19), we have
P(x) = 1
x ,
Q(x) = 1 −n2
x2 ,
which shows that the point x = 0 is a regular singularity. By inspection we see that there
are no other singularities in the ﬁnite range. As x →∞(z →0), from Eq. (7.22) we have
the coefﬁcients
2z −z
z2
and
1 −n2z2
z4
.
Since the latter expression diverges as 1/z4, the point x = ∞is an irregular, or essential,
singularity.
■
Table 7.1 lists the singular points of a number of ODEs of importance in physics. It
will be seen that the ﬁrst three equations in Table 7.1, the hypergeometric, Legendre, and
Chebyshev, all have three regular singular points. The hypergeometric equation, with reg-
ular singularities at 0, 1, and ∞, is taken as the standard, the canonical form. The solutions
of the other two may then be expressed in terms of its solutions, the hypergeometric func-
tions. This is done in Chapter 18.
In a similar manner, the conﬂuent hypergeometric equation is taken as the canonical
form of a linear second-order differential equation with one regular and one irregular sin-
gular point.
Table 7.1
Singularities of Some Important ODEs.
Equation
Regular
Irregular
Singularity
Singularity
x =
x =
1. Hypergeometric
0,1,∞
···
x(x −1)y′′ + [(1 + a + b)x + c]y′ + aby = 0
2. Legendrea
−1,1,∞
···
(1 −x2)y′′ −2xy′ + l(l + 1)y = 0
3. Chebyshev
−1,1,∞
···
(1 −x2)y′′ −xy′ + n2y = 0
4. Conﬂuent hypergeometric
0
∞
xy′′ + (c −x)y′ −ay = 0
5. Bessel
0
∞
x2y′′ + xy′ + (x2 −n2)y = 0
6. Laguerrea
0
∞
xy′′ + (1 −x)y′ + ay = 0
7. Simple harmonic oscillator
···
∞
y′′ + ω2y = 0
8. Hermite
···
∞
y′′ −2xy′ + 2αy = 0
aThe associated equations have the same singular points.

346
Chapter 7 Ordinary Differential Equations
Exercises
7.4.1
Show that Legendre’s equation has regular singularities at x = −1, 1, and ∞.
7.4.2
Show that Laguerre’s equation, like the Bessel equation, has a regular singularity at
x = 0 and an irregular singularity at x = ∞.
7.4.3
Show that Chebyshev’s equation, like the Legendre equation, has regular singularities
at x = −1, 1, and ∞.
7.4.4
Show that Hermite’s equation has no singularity other than an irregular singularity at
x = ∞.
7.4.5
Show that the substitution
x →1 −x
2
,
a = −l,
b = l + 1,
c = 1
converts the hypergeometric equation into Legendre’s equation.
7.5
SERIES SOLUTIONS—FROBENIUS’ METHOD
In this section we develop a method of obtaining solution(s) of the linear, second-order,
homogeneous ODE. For the moment, we develop the mechanics of the method. After
studying examples, we return to discuss the conditions under which we can expect these
series solutions to exist.
Consider a linear, second-order, homogeneous ODE, in the form
d2y
dx2 + P(x)dy
dx + Q(x)y = 0.
(7.23)
In this section we develop (at least) one solution of Eq. (7.23) by expansion about the point
x = 0. In the next section we develop the second, independent solution and prove that
no third, independent solution exists. Therefore the most general solution of Eq. (7.23)
may be written in terms of the two independent solutions as
y(x) = c1y1(x) + c2y2(x).
(7.24)
Our physical problem may lead to a nonhomogeneous, linear, second-order ODE,
d2y
dx2 + P(x)dy
dx + Q(x)y = F(x).
(7.25)
The function on the right, F(x), typically represents a source (such as electrostatic charge)
or a driving force (as in a driven oscillator). Methods for solving this inhomogeneous
ODE are also discussed later in this chapter and, using Laplace transform techniques, in
Chapter 20. Assuming a single particular integral (i.e., speciﬁc solution), yp, of the in-
homogeneous ODE to be available, we may add to it any solution of the corresponding
homogeneous equation, Eq. (7.23), and write the most general solution of Eq. (7.25) as
y(x) = c1y1(x) + c2y2(x) + yp(x).
(7.26)
In many problems, the constants c1 and c2 will be ﬁxed by boundary conditions.

7.5 Series Solutions—Frobenius’ Method
347
For the present, we assume that F(x) = 0, and that therefore our differential equation is
homogeneous. We shall attempt to develop a solution of our linear, second-order, homoge-
neous differential equation, Eq. (7.23), by substituting into it a power series with undeter-
mined coefﬁcients. Also available as a parameter is the power of the lowest nonvanishing
term of the series. To illustrate, we apply the method to two important differential equa-
tions.
First Example—Linear Oscillator
Consider the linear (classical) oscillator equation
d2y
dx2 + ω2y = 0,
(7.27)
which we have already solved by another method in Example 7.3.1. The solutions we
found there were y = sinωx and cosωx.
We try
y(x) = xs(a0 + a1x + a2x2 + a3x3 + ···)
=
∞
X
j=0
a j xs+ j,
a0 ̸= 0,
(7.28)
with the exponent s and all the coefﬁcients a j still undetermined. Note that s need not be
an integer. By differentiating twice, we obtain
dy
dx =
∞
X
j=0
a j(s + j)xs+ j−1,
d2y
dx2 =
∞
X
j=0
a j(s + j)(s + j −1)xs+ j−2.
By substituting into Eq. (7.27), we have
∞
X
j=0
a j(s + j)(s + j −1)xs+ j−2 + ω2
∞
X
j=0
a j xs+ j = 0.
(7.29)
From our analysis of the uniqueness of power series (Chapter 1), we know that the coef-
ﬁcient of each power of x on the left-hand side of Eq. (7.29) must vanish individually, xs
being an overall factor.
The lowest power of x appearing in Eq. (7.29) is xs−2, occurring only for j = 0 in the
ﬁrst summation. The requirement that this coefﬁcient vanish yields
a0s(s −1) = 0.
Recall that we chose a0 as the coefﬁcient of the lowest nonvanishing term of the series in
Eq. (7.28), so that, by deﬁnition, a0 ̸= 0. Therefore we have
s(s −1) = 0.
(7.30)

348
Chapter 7 Ordinary Differential Equations
This equation, coming from the coefﬁcient of the lowest power of x, is called the indicial
equation. The indicial equation and its roots are of critical importance to our analysis.
Clearly, in this example it informs us that either s = 0 or s = 1, so that our series solution
must start either with an x0 or an x1 term.
Looking further at Eq. (7.29), we see that the next lowest power of x, namely xs−1, also
occurs uniquely (for j = 1 in the ﬁrst summation). Setting the coefﬁcient of xs−1 to zero,
we have
a1(s + 1)s = 0.
This shows that if s = 1, we must have a1 = 0. However, if s = 0, this equation imposes
no requirement on the coefﬁcient set.
Before considering further the two possibilities for s, we return to Eq. (7.29) and demand
that the remaining net coefﬁcients vanish. The contributions to the coefﬁcient of xs+ j,
( j ≥0), come from the term containing a j+2 in the ﬁrst summation and from that with a j
in the second. Because we have already dealt with j = 0 and j = 1 in the ﬁrst summation,
when we have used all j ≥0, we will have used all the terms of both series. For each value
of j, the vanishing of the net coefﬁcient of xs+ j results in
a j+2(s + j + 2)(s + j + 1) + ω2a j = 0,
equivalent to
a j+2 = −a j
ω2
(s + j + 2)(s + j + 1).
(7.31)
This is a two-term recurrence relation.1 In the present problem, given a j, Eq. (7.31)
permits us to compute a j+2 and then a j+4,a j+6, and so on, continuing as far as desired.
Thus, if we start with a0, we can make the even coefﬁcients a2, a4, ..., but we obtain no
information about the odd coefﬁcients a1, a3, a5, .... But because a1 is arbitrary if s = 0
and necessarily zero if s = 1, let us set it equal to zero, and then, by Eq. (7.31),
a3 = a5 = a7 = ··· = 0;
the result is that all the odd-numbered coefﬁcients vanish.
Returning now to Eq. (7.30), our indicial equation, we ﬁrst try the solution s = 0. The
recurrence relation, Eq. (7.31), becomes
a j+2 = −a j
ω2
( j + 2)( j + 1),
(7.32)
1In some problems, the recurrence relation may involve more than two terms; its exact form will depend on the functions P(x)
and Q(x) of the ODE.

7.5 Series Solutions—Frobenius’ Method
349
which leads to
a2 = −a0
ω2
1 · 2 = −ω2
2! a0,
a4 = −a2
ω2
3 · 4 = +ω4
4! a0,
a6 = −a4
ω2
5 · 6 = −ω6
6! a0,
and so on.
By inspection (and mathematical induction, see Section 1.4),
a2n = (−1)n ω2n
(2n)!a0,
(7.33)
and our solution is
y(x)s=0 = a0

1 −(ωx)2
2!
+ (ωx)4
4!
−(ωx)6
6!
+ ···

= a0 cosωx.
(7.34)
If we choose the indicial equation root s = 1 from Eq. (7.30), the recurrence relation of
Eq. (7.31) becomes
a j+2 = −a j
ω2
( j + 3)( j + 2).
(7.35)
Evaluating this successively for j = 0, 2, 4, ..., we obtain
a2 = −a0
ω2
2 · 3 = −ω2
3! a0,
a4 = −a2
ω2
4 · 5 = +ω4
5! a0,
a6 = −a4
ω2
6 · 7 = −ω6
7! a0,
and so on.
Again, by inspection and mathematical induction,
a2n = (−1)n
ω2n
(2n + 1)!a0.
(7.36)
For this choice, s = 1, we obtain
y(x)s=1 = a0x

1 −(ωx)2
3!
+ (ωx)4
5!
−(ωx)6
7!
+ ···

= a0
ω

(ωx) −(ωx)3
3!
+ (ωx)5
5!
−(ωx)7
7!
+ ···

= a0
ω sinωx.
(7.37)

350
Chapter 7 Ordinary Differential Equations
I
II
III
IV
a0k(k−1)
a1(k +1)k
a2(k +2)(k +1)
a0ω2
a3(k+ 3)(k+ 2)
a1ω2
xk+
xk+
xk+ 1+…
xk−2+
xk +1+
+
xk+ 1+… = 0
=0
=0
=0
=0
FIGURE 7.2
Schematics of series solution.
For future reference we note that the ODE solution from the indicial equation root s = 0
consisted only of even powers of x, while the solution from the root s = 1 contained only
odd powers.
To summarize this approach, we may write Eq. (7.29) schematically as shown in
Fig. 7.2. From the uniqueness of power series (Section 1.2), the total coefﬁcient of
each power of x must vanish—all by itself. The requirement that the ﬁrst coefﬁcient
vanish (I) leads to the indicial equation, Eq. (7.30). The second coefﬁcient is han-
dled by setting a1 = 0 (II). The vanishing of the coefﬁcients of xs (and higher pow-
ers, taken one at a time) is ensured by imposing the recurrence relation, Eq. (7.31),
(III), (IV).
This expansion in power series, known as Frobenius’ method, has given us two series
solutions of the linear oscillator equation. However, there are two points about such series
solutions that must be strongly emphasized:
1.
The series solution should always be substituted back into the differential equation, to
see if it works, as a precaution against algebraic and logical errors. If it works, it is a
solution.
2.
The acceptability of a series solution depends on its convergence (including asymp-
totic convergence). It is quite possible for Frobenius’ method to give a series solution
that satisﬁes the original differential equation when substituted in the equation but
that does not converge over the region of interest. Legendre’s differential equation
(examined in Section 8.3) illustrates this situation.
Expansion about x0
Equation (7.28) is an expansion about the origin, x0 = 0. It is perfectly possible to replace
Eq. (7.28) with
y(x) =
∞
X
j=0
a j(x −x0)s+ j,
a0 ̸= 0.
(7.38)
Indeed, for the Legendre, Chebyshev, and hypergeometric equations, the choice x0 = 1
has some advantages. The point x0 should not be chosen at an essential singularity, or
Frobenius’ method will probably fail. The resultant series (x0 an ordinary point or regular
singular point) will be valid where it converges. You can expect a divergence of some sort
when |x −x0| = |z1 −x0|, where z1 is the ODE’s closest singularity to x0 (in the complex
plane).

7.5 Series Solutions—Frobenius’ Method
351
Symmetry of Solutions
Let us note that for the classical oscillator problem we obtained one solution of even sym-
metry, y1(x) = y1(−x), and one of odd symmetry, y2(x) = −y2(−x). This is not just an
accident but a direct consequence of the form of the ODE. Writing a general homogeneous
ODE as
L(x)y(x) = 0,
(7.39)
in which L(x) is the differential operator, we see that for the linear oscillator equation,
Eq. (7.27), L(x) is even under parity; that is,
L(x) = L(−x).
Whenever the differential operator has a speciﬁc parity or symmetry, either even or odd,
we may interchange +x and −x, and Eq. (7.39) becomes
±L(x)y(−x) = 0.
Clearly, if y(x) is a solution of the differential equation, y(−x) is also a solution. Then,
either y(x) and y(−x) are linearly dependent (i.e., proportional), meaning that y is either
even or odd, or they are linearly independent solutions that can be combined into a pair of
solutions, one even, and one odd, by forming
yeven = y(x) + y(−x),
yodd = y(x) −y(−x).
For the classical oscillator example, we obtained two solutions; our method for ﬁnding
them caused one to be even, the other odd.
If we refer back to Section 7.4 we can see that Legendre, Chebyshev, Bessel, simple har-
monic oscillator, and Hermite equations are all based on differential operators with even
parity; that is, their P(x) in Eq. (7.19) is odd and Q(x) even. Solutions of all of them
may be presented as series of even powers of x or separate series of odd powers of x.
The Laguerre differential operator has neither even nor odd symmetry; hence its solutions
cannot be expected to exhibit even or odd parity. Our emphasis on parity stems primarily
from the importance of parity in quantum mechanics. We ﬁnd that in many problems wave
functions are either even or odd, meaning that they have a deﬁnite parity. Most interac-
tions (beta decay is the big exception) are also even or odd, and the result is that parity is
conserved.
A Second Example—Bessel’s Equation
This attack on the linear oscillator was perhaps a bit too easy. By substituting the power
series, Eq. (7.28), into the differential equation, Eq. (7.27), we obtained two independent
solutions with no trouble at all.
To get some idea of other things that can happen, we try to solve Bessel’s equation,
x2y′′ + xy′ + (x2 −n2)y = 0.
(7.40)

352
Chapter 7 Ordinary Differential Equations
Again, assuming a solution of the form
y(x) =
∞
X
j=0
a j xs+ j,
we differentiate and substitute into Eq. (7.40). The result is
∞
X
j=0
a j(s + j)(s + j −1)xs+ j +
∞
X
j=0
a j(s + j)xs+ j
+
∞
X
j=0
a j xs+ j+2 −
∞
X
j=0
a jn2xs+ j = 0.
(7.41)
By setting j = 0, we get the coefﬁcient of xs, the lowest power of x appearing on the
left-hand side,
a0

s(s −1) + s −n2
= 0,
(7.42)
and again a0 ̸= 0 by deﬁnition. Equation (7.42) therefore yields the indicial equation
s2 −n2 = 0,
(7.43)
with solutions s = ±n.
We need also to examine the coefﬁcient of xs+1. Here we obtain
a1[(s + 1)s + s + 1 −n2] = 0,
or
a1(s + 1 −n)(s + 1 + n) = 0.
(7.44)
For s = ±n, neither s + 1 −n nor s + 1 + n vanishes and we must require a1 = 0.
Proceeding to the coefﬁcient of xs+ j for s = n, we see that it is the term containing a j
in the ﬁrst, second, and fourth terms of Eq. (7.41), but is that containing a j−2 in the third
term. By requiring the overall coefﬁcient of xs+ j to vanish, we obtain
a j[(n + j)(n + j −1) + (n + j) −n2] + a j−2 = 0.
When j is replaced by j + 2, this can be rewritten for j ≥0 as
a j+2 = −a j
1
( j + 2)(2n + j + 2),
(7.45)
which is the desired recurrence relation. Repeated application of this recurrence relation
leads to
a2 = −a0
1
2(2n + 2) = −
a0n!
221!(n + 1)!,
a4 = −a2
1
4(2n + 4) =
a0n!
242!(n + 2)!,
a6 = −a4
1
6(2n + 6) = −
a0n!
263!(n + 3)!,
and so on,

7.5 Series Solutions—Frobenius’ Method
353
and in general,
a2p = (−1)p
a0n!
22p p!(n + p)!.
(7.46)
Inserting these coefﬁcients in our assumed series solution, we have
y(x) = a0xn

1 −
n! x2
221!(n + 1)! +
n! x4
242!(n + 2)! −···

.
(7.47)
In summation form,
y(x) = a0
∞
X
j=0
(−1) j
n! xn+2 j
22 j j!(n + j)!
= a0 2nn!
∞
X
j=0
(−1) j
1
j!(n + j)!
x
2
n+2 j
.
(7.48)
In Chapter 14 the ﬁnal summation (with a0 = 1/2nn!) is identiﬁed as the Bessel function
Jn(x):
Jn(x) =
∞
X
j=0
(−1) j
1
j!(n + j)!
x
2
n+2 j
.
(7.49)
Note that this solution, Jn(x), has either even or odd symmetry,2 as might be expected
from the form of Bessel’s equation.
When s = −n and n is not an integer, we may generate a second distinct series, to be
labeled J−n(x). However, when −n is a negative integer, trouble develops. The recurrence
relation for the coefﬁcients a j is still given by Eq. (7.45), but with 2n replaced by −2n.
Then, when j + 2 = 2n or j = 2(n −1), the coefﬁcient a j+2 blows up and Frobenius’
method does not produce a solution consistent with our assumption that the series starts
with x−n.
By substituting in an inﬁnite series, we have obtained two solutions for the linear oscil-
lator equation and one for Bessel’s equation (two if n is not an integer). To the questions
“Can we always do this? Will this method always work?” the answer is “No, we cannot
always do this. This method of series solution will not always work.’’
Regular and Irregular Singularities
The success of the series substitution method depends on the roots of the indicial equation
and the degree of singularity of the coefﬁcients in the differential equation. To understand
better the effect of the equation coefﬁcients on this naive series substitution approach,
2Jn(x) is an even function if n is an even integer, and an odd function if n is an odd integer. For nonintegral n, Jn has no such
simple symmetry.

354
Chapter 7 Ordinary Differential Equations
consider four simple equations:
y′′ −6
x2 y = 0,
(7.50)
y′′ −6
x3 y = 0,
(7.51)
y′′ + 1
x y′ −b2
x2 y = 0,
(7.52)
y′′ + 1
x2 y′ −b2
x2 y = 0.
(7.53)
The reader may show easily that for Eq. (7.50) the indicial equation is
s2 −s −6 = 0,
giving s = 3 and s = −2. Since the equation is homogeneous in x (counting d2/dx2 as
x−2), there is no recurrence relation. However, we are left with two perfectly good solu-
tions, x3 and x−2.
Equation (7.51) differs from Eq. (7.50) by only one power of x, but this sends the indicial
equation to
−6a0 = 0,
with no solution at all, for we have agreed that a0 ̸= 0. Our series substitution worked for
Eq. (7.50), which had only a regular singularity, but broke down at Eq. (7.51), which has
an irregular singular point at the origin.
Continuing with Eq. (7.52), we have added a term y′/x. The indicial equation is
s2 −b2 = 0,
but again, there is no recurrence relation. The solutions are y = xb and x−b, both perfectly
acceptable one-term series.
When we change the power of x in the coefﬁcient of y′ from −1 to −2, in Eq. (7.53),
there is a drastic change in the solution. The indicial equation (with only the y′ term con-
tributing) becomes
s = 0.
There is a recurrence relation,
a j+1 = +a j
b2 −j( j −1)
j + 1
.
Unless the parameter b is selected to make the series terminate, we have
lim
j→∞

a j+1
a j
 = lim
j→∞
j( j + 1)
j + 1
= lim
j→∞
j2
j = ∞.

7.5 Series Solutions—Frobenius’ Method
355
Hence our series solution diverges for all x ̸= 0. Again, our method worked for
Eq. (7.52) with a regular singularity but failed when we had the irregular singularity of
Eq. (7.53).
Fuchs’ Theorem
The answer to the basic question as to when the method of series substitution can be
expected to work is given by Fuchs’ theorem, which asserts that we can always obtain
at least one power-series solution, provided that we are expanding about a point which is
an ordinary point or at worst a regular singular point.
If we attempt an expansion about an irregular or essential singularity, our method may
fail as it did for Eqs. (7.51) and (7.53). Fortunately, the more important equations of mathe-
matical physics, listed in Section 7.4, have no irregular singularities in the ﬁnite plane.
Further discussion of Fuchs’ theorem appears in Section 7.6.
From Table 7.1, Section 7.4, inﬁnity is seen to be a singular point for all the equations
considered. As a further illustration of Fuchs’ theorem, Legendre’s equation (with inﬁnity
as a regular singularity) has a convergent series solution in negative powers of the argu-
ment (Section 15.6). In contrast, Bessel’s equation (with an irregular singularity at inﬁnity)
yields asymptotic series (Sections 12.6 and 14.6). Although only asymptotic, these solu-
tions are nevertheless extremely useful.
Summary
If we are expanding about an ordinary point or at worst about a regular singularity, the
series substitution approach will yield at least one solution (Fuchs’ theorem).
Whether we get one or two distinct solutions depends on the roots of the indicial
equation.
1.
If the two roots of the indicial equation are equal, we can obtain only one solution by
this series substitution method.
2.
If the two roots differ by a nonintegral number, two independent solutions may be
obtained.
3.
If the two roots differ by an integer, the larger of the two will yield a solution, while the
smaller may or may not give a solution, depending on the behavior of the coefﬁcients.
The usefulness of a series solution for numerical work depends on the rapidity of con-
vergence of the series and the availability of the coefﬁcients. Many ODEs will not yield
nice, simple recurrence relations for the coefﬁcients. In general, the available series will
probably be useful for very small |x| (or |x −x0|). Computers can be used to determine
additional series coefﬁcients using a symbolic language, such as Mathematica3 or Maple.4
Often, however, for numerical work a direct numerical integration will be preferred.
3S. Wolfram, Mathematica: A System for Doing Mathematics by Computer. Reading, MA. Addison Wesley (1991).
4A. Heck, Introduction to Maple. New York: Springer (1993).

356
Chapter 7 Ordinary Differential Equations
Exercises
7.5.1
Uniqueness theorem. The function y(x) satisﬁes a second-order, linear, homogeneous
differential equation. At x = x0, y(x) = y0 and dy/dx = y′
0. Show that y(x) is unique,
in that no other solution of this differential equation passes through the points (x0, y0)
with a slope of y′
0.
Hint. Assume a second solution satisfying these conditions and compare the Taylor
series expansions.
7.5.2
A series solution of Eq. (7.23) is attempted, expanding about the point x = x0. If x0 is
an ordinary point, show that the indicial equation has roots s = 0,1.
7.5.3
In the development of a series solution of the simple harmonic oscillator (SHO) equa-
tion, the second series coefﬁcient a1 was neglected except to set it equal to zero. From
the coefﬁcient of the next-to-the-lowest power of x, xs−1, develop a second-indicial
type equation.
(a)
(SHO equation with s = 0). Show that a1, may be assigned any ﬁnite value
(including zero).
(b)
(SHO equation with s = 1). Show that a1 must be set equal to zero.
7.5.4
Analyze the series solutions of the following differential equations to see when a1 may
be set equal to zero without irrevocably losing anything and when a1 must be set equal
to zero.
(a)
Legendre,
(b)
Chebyshev,
(c)
Bessel,
(d)
Hermite.
ANS.
(a)
Legendre,
(b)
Chebyshev, and
(d)
Hermite: For s = 0, a1
may be set equal to zero; for s = 1, a1 must be set equal to zero.
(c)
Bessel: a1 must be set equal to zero (except for s = ±n = −1
2).
7.5.5
Obtain a series solution of the hypergeometric equation
x(x −1)y′′ + [(1 + a + b)x −c]y′ + aby = 0.
Test your solution for convergence.
7.5.6
Obtain two series solutions of the conﬂuent hypergeometric equation
xy′′ + (c −x)y′ −ay = 0.
Test your solutions for convergence.
7.5.7
A quantum mechanical analysis of the Stark effect (parabolic coordinates) leads to the
differential equation
d
dξ

ξ du
dξ

+
1
2 Eξ + α −m2
4ξ −1
4 Fξ2

u = 0.
Here α is a constant, E is the total energy, and F is a constant such that Fz is the
potential energy added to the system by the introduction of an electric ﬁeld.

7.5 Series Solutions—Frobenius’ Method
357
Using the larger root of the indicial equation, develop a power-series solution about
ξ = 0. Evaluate the ﬁrst three coefﬁcients in terms of ao.
ANS.
Indicial equation
s2 −m2
4 = 0,
u(ξ) = a0ξm/2

1 −
α
m + 1ξ +

α2
2(m + 1)(m + 2) −
E
4(m + 2)

ξ2 + ···

.
Note that the perturbation F does not appear until a3 is included.
7.5.8
For the special case of no azimuthal dependence, the quantum mechanical analysis of
the hydrogen molecular ion leads to the equation
d
dη

(1 −η2)du
dη

+ αu + βη2u = 0.
Develop a power-series solution for u(η). Evaluate the ﬁrst three nonvanishing coefﬁ-
cients in terms of a0.
ANS.
Indicial equation
s(s −1) = 0,
uk=1 = a0η

1 + 2 −α
6
η2 +
(2 −α)(12 −α)
120
−β
20

η4 + ···

.
7.5.9
To a good approximation, the interaction of two nucleons may be described by a
mesonic potential
V = Ae−ax
x
,
attractive for A negative. Show that the resultant Schrödinger wave equation
¯h2
2m
d2ψ
dx2 + (E −V )ψ = 0
has the following series solution through the ﬁrst three nonvanishing coefﬁcients:
ψ = a0

x + 1
2 A′x2 + 1
6
1
2 A′2 −E′ −aA′

x3 + ···

,
where the prime indicates multiplication by 2m/¯h2.
7.5.10
If the parameter b2 in Eq. (7.53) is equal to 2, Eq. (7.53) becomes
y′′ + 1
x2 y′ −2
x2 y = 0.
From the indicial equation and the recurrence relation, derive a solution y = 1 + 2x +
2x2. Verify that this is indeed a solution by substituting back into the differential
equation.

358
Chapter 7 Ordinary Differential Equations
7.5.11
The modiﬁed Bessel function I0(x) satisﬁes the differential equation
x2 d2
dx2 I0(x) + x d
dx I0(x) −x2I0(x) = 0.
Given that the leading term in an asymptotic expansion is known to be
I0(x) ∼
ex
√
2πx
,
assume a series of the form
I0(x) ∼
ex
√
2πx
n
1 + b1x−1 + b2x−2 + ···
o
.
Determine the coefﬁcients b1 and b2.
ANS.
b1 = 1
8,
b2 =
9
128.
7.5.12
The even power-series solution of Legendre’s equation is given by Exercise 8.3.1. Take
a0 = 1 and n not an even integer, say n = 0.5. Calculate the partial sums of the series
through x200, x400, x600, ..., x2000 for x = 0.95(0.01)1.00. Also, write out the individ-
ual term corresponding to each of these powers.
Note. This calculation does not constitute proof of convergence at x = 0.99 or diver-
gence at x = 1.00, but perhaps you can see the difference in the behavior of the sequence
of partial sums for these two values of x.
7.5.13
(a)
The odd power-series solution of Hermite’s equation is given by Exercise 8.3.3.
Take a0 = 1. Evaluate this series for α = 0, x = 1,2,3. Cut off your calculation
after the last term calculated has dropped below the maximum term by a factor of
106 or more. Set an upper bound to the error made in ignoring the remaining terms
in the inﬁnite series.
(b)
As a check on the calculation of part (a), show that the Hermite series yodd(α = 0)
corresponds to
R x
0 exp(x2)dx.
(c)
Calculate this integral for x = 1,2,3.
7.6
OTHER SOLUTIONS
In Section 7.5 a solution of a second-order homogeneous ODE was developed by substi-
tuting in a power series. By Fuchs’ theorem this is possible, provided the power series is
an expansion about an ordinary point or a nonessential singularity.5 There is no guarantee
that this approach will yield the two independent solutions we expect from a linear second-
order ODE. In fact, we shall prove that such an ODE has at most two linearly independent
solutions. Indeed, the technique gave only one solution for Bessel’s equation (n an integer).
In this section we also develop two methods of obtaining a second independent solution:
an integral method and a power series containing a logarithmic term. First, however, we
consider the question of independence of a set of functions.
5This is why the classiﬁcation of singularities in Section 7.4 is of vital importance.

7.6 Other Solutions
359
Linear Independence of Solutions
In Chapter 2 we introduced the concept of linear dependence for forms of the type a1x1 +
a2x2 + ..., and identiﬁed a set of such forms as linearly dependent if any one of the forms
could be written as a linear combination of others. We need now to extend the concept to
a set of functions ϕλ. The criterion for linear dependence of a set of functions of a variable
x is the existence of a relation of the form
X
λ
kλϕλ(x) = 0,
(7.54)
in which not all the coefﬁcients kλ are zero. The interpretation we attach to Eq. (7.54) is that
it indicates linear dependence if it is satisﬁed for all relevant values of x. Isolated points or
partial ranges of satisfaction of Eq. (7.54) do not sufﬁce to indicate linear dependence. The
essential idea being conveyed here is that if there is linear dependence, the function space
spanned by the ϕλ(x) can be spanned using less than all of them. On the other hand, if the
only global solution of Eq. (7.54) is kλ = 0 for all λ, the set of functions ϕλ(x) is said to
be linearly independent.
If the members of a set of functions are mutually orthogonal, then they are automatically
linearly independent. To establish this, consider the evaluation of
S =
*X
λ
kλϕλ

X
µ
kµϕµ
+
for a set of orthonormal ϕλ and with arbitrary values of the coefﬁcients kλ. Because of the
orthonormality, S evaluates to P
λ |kλ|2, and will be nonzero (showing that P
λ kλϕλ ̸= 0)
unless all the kλ vanish.
We now proceed to consider the ramiﬁcations of linear dependence for solutions of
ODEs, and for that purpose it is appropriate to assume that the functions ϕλ(x) are differ-
entiable as needed. Then, differentiating Eq. (7.54) repeatedly, with the assumption that it
is valid for all x, we generate a set of equations
X
λ
kλϕ′
λ(x) = 0,
X
λ
kλϕ′′
λ(x) = 0,
continuing until we have generated as many equations as the number of λ values. This
gives us a set of homogeneous linear equations in which kλ are the unknown quantities.
By Section 2.1 there is a solution other than all kλ = 0 only if the determinant of the
coefﬁcients of the kλ vanishes. This means that the linear dependence we have assumed by
accepting Eq. (7.54) implies that

ϕ1
ϕ2
...
ϕn
ϕ′
1
ϕ′
2
...
ϕ′
n
...
...
...
...
ϕ(n−1)
1
ϕ(n−1)
2
...
ϕ(n−1)
n

= 0.
(7.55)

360
Chapter 7 Ordinary Differential Equations
This determinant is called the Wronskian, and the analysis leading to Eq. (7.55) shows
that:
1.
If the Wronskian is not equal to zero, then Eq. (7.54) has no solution other than kλ = 0.
The set of functions ϕλ is therefore linearly independent.
2.
If the Wronskian vanishes at isolated values of the argument, this does not prove linear
dependence. However, if the Wronskian is zero over the entire range of the variable,
the functions ϕλ are linearly dependent over this range.6
Example 7.6.1
LINEAR INDEPENDENCE
The solutions of the linear oscillator equation, Eq. (7.27), are ϕ1 = sinωx, ϕ2 = cosωx.
The Wronskian becomes

sinωx
cosωx
ω cosωx
−ω sinωx
 = −ω ̸= 0.
These two solutions, ϕ1 and ϕ2, are therefore linearly independent. For just two functions
this means that one is not a multiple of the other, which is obviously true here.
Incidentally, you know that
sinωx = ±(1 −cos2 ωx)1/2,
but this is not a linear relation, of the form of Eq. (7.54).
■
Example 7.6.2
LINEAR DEPENDENCE
For an illustration of linear dependence, consider the solutions of the ODE
d2ϕ(x)
dx2
= ϕ(x).
This equation has solutions ϕ1 = ex and ϕ2 = e−x, and we add ϕ3 = cosh x, also a solution.
The Wronskian is

ex
e−x
cosh x
ex
−e−x
sinh x
ex
e−x
cosh x

= 0.
The determinant vanishes for all x because the ﬁrst and third rows are identical. Hence
ex, e−x, and cosh x are linearly dependent, and, indeed, we have a relation of the form of
Eq. (7.54):
ex + e−x −2cosh x = 0
with
kλ ̸= 0.
■
6Compare H. Lass, Elements of Pure and Applied Mathematics, New York: McGraw-Hill (1957), p. 187, for proof of this
assertion. It is assumed that the functions have continuous derivatives and that at least one of the minors of the bottom row of
Eq. (7.55) (Laplace expansion) does not vanish in [a,b], the interval under consideration.

7.6 Other Solutions
361
Number of Solutions
Now we are ready to prove the theorem that a second-order homogeneous ODE has two
linearly independent solutions.
Suppose y1, y2, y3 are three solutions of the homogeneous ODE, Eq. (7.23). Then we
form the Wronskian W jk = y j y′
k −y′
j yk of any pair y j, yk of them and note also that
W ′
jk = (y′
j y′
k + y j y′′
k ) −(y′′
j yk + y′
j y′
k)
= y j y′′
k −y′′
j yk.
(7.56)
Next we divide the ODE by y and move Q(x) to its right-hand side (where it becomes
−Q(x)), so, for solutions y j and yk:
y′′
j
y j
+ P(x)
y′
j
y j
= −Q(x) = y′′
k
yk
+ P(x) y′k
yk
.
Taking now the ﬁrst and third members of this equation, multiplying by y j yk and rearrang-
ing, we ﬁnd that
(y j y′′
k −y′′
j yk) + P(x)(y j y′
k −y′
j yk) = 0,
which simpliﬁes for any pair of solutions to
W ′
jk = −P(x)W jk.
(7.57)
Finally, we evaluate the Wronskian of all three solutions, expanding it by minors along the
second row and identifying each term as containing a W ′
i j as given by Eq. (7.56):
W =

y1
y2
y3
y′
1
y′
2
y′
3
y′′
1
y′′
2
y′′
3

= −y′
1W ′
23 + y′
2W ′
13 −y′
3W ′
12.
We now use Eq. (7.57) to replace each W ′
i j by −P(x)Wi j and then reassemble the minors
into a 3 × 3 determinant, which vanishes because it contains two identical rows:
W = P(x)
 y′
1W23 −y′
2W13 + y′
3W12

= −P(x)

y1
y2
y3
y′
1
y′
2
y′
3
y′
1
y′
2
y′
3

= 0.
We therefore have W = 0, which is just the condition for linear dependence of the solutions
y j. Thus, we have proved the following:
A linear second-order homogeneous ODE has at most two linearly independent solu-
tions. Generalizing, a linear homogeneous nth-order ODE has at most n linearly inde-
pendent solutions yj, and its general solution will be of the form y(x) = Pn
j=1 c j y j(x).

362
Chapter 7 Ordinary Differential Equations
Finding a Second Solution
Returning to our linear, second-order, homogeneous ODE of the general form
y′′ + P(x)y′ + Q(x)y = 0,
(7.58)
let y1 and y2 be two independent solutions. Then the Wronskian, by deﬁnition, is
W = y1y′
2 −y′
1y2.
(7.59)
By differentiating the Wronskian, we obtain, as already demonstrated in Eq. (7.57),
W ′ = −P(x)W.
(7.60)
In the special case that P(x) = 0, that is,
y′′ + Q(x)y = 0,
(7.61)
the Wronskian
W = y1y′
2 −y′
1y2 = constant.
(7.62)
Since our original differential equation is homogeneous, we may multiply the solutions y1
and y2 by whatever constants we wish and arrange to have the Wronskian equal to unity
(or −1). This case, P(x) = 0, appears more frequently than might be expected. Recall that
∇2(ψ/r) in spherical polar coordinates contains no ﬁrst radial derivative. Finally, every
linear second-order differential equation can be transformed into an equation of the form
of Eq. (7.61) (compare Exercise 7.6.12).
For the general case, let us now assume that we have one solution of Eq. (7.58) by a
series substitution (or by guessing). We now proceed to develop a second, independent
solution for which W ̸= 0. Rewriting Eq. (7.60) as
dW
W = −Pdx,
we integrate over the variable x, from a to x, to obtain
ln W(x)
W(a) = −
x
Z
a
P(x1)dx1,
or7
W(x) = W(a)exp

−
x
Z
a
P(x1)dx1

.
(7.63)
7If P(x) remains ﬁnite in the domain of interest, W(x) ̸= 0 unless W(a) = 0. That is, the Wronskian of our two solutions is
either identically zero or never zero. However, if P(x) does not remain ﬁnite in our interval, then W(x) can have isolated zeros
in that domain and one must be careful to choose a so that W(a) ̸= 0.

7.6 Other Solutions
363
Now we make the observation that
W(x) = y1y′
2 −y′
1y2 = y2
1
d
dx
 y2
y1

,
(7.64)
and, by combining Eqs. (7.63) and (7.64), we have
d
dx
 y2
y1

= W(a)exp[−
R x
a P(x1)dx1]
y2
1
.
(7.65)
Finally, by integrating Eq. (7.65) from x2 = b to x2 = x we get
y2(x) = y1(x)W(a)
x
Z
b
exp

−
R x2
a
P(x1)dx1

[y1(x2)]2
dx2.
(7.66)
Here a and b are arbitrary constants and a term y1(x)y2(b)/y1(b) has been dropped,
because it is a multiple of the previously found ﬁrst solution y1. Since W(a), the Wronskian
evaluated at x = a, is a constant and our solutions for the homogeneous differential equa-
tion always contain an arbitrary normalizing factor, we set W(a) = 1 and write
y2(x) = y1(x)
x
Z exp[−
R x2 P(x1)dx1]
[y1(x2)]2
dx2.
(7.67)
Note that the lower limits x1 = a and x2 = b have been omitted. If they are retained,
they simply make a contribution equal to a constant times the known ﬁrst solution, y1(x),
and hence add nothing new. If we have the important special case P(x) = 0, Eq. (7.67)
reduces to
y2(x) = y1(x)
x
Z
dx2
[y1(x2)]2 .
(7.68)
This means that by using either Eq. (7.67) or Eq. (7.68) we can take one known solution and
by integrating can generate a second, independent solution of Eq. (7.58). This technique is
used in Section 15.6 to generate a second solution of Legendre’s differential equation.
Example 7.6.3
A SECOND SOLUTION FOR THE LINEAR OSCILLATOR EQUATION
From d2y/dx2 + y = 0 with P(x) = 0 let one solution be y1 = sin x. By applying
Eq. (7.68), we obtain
y2(x) = sin x
x
Z
dx2
sin2 x2
= sin x(−cot x) = −cos x,
which is clearly independent (not a linear multiple) of sin x.
■

364
Chapter 7 Ordinary Differential Equations
Series Form of the Second Solution
Further insight into the nature of the second solution of our differential equation may be
obtained by the following sequence of operations.
1.
Express P(x) and Q(x) in Eq. (7.58) as
P(x) =
∞
X
i=−1
pi xi,
Q(x) =
∞
X
j=−2
q j x j.
(7.69)
The leading terms of the summations are selected to create the strongest possible
regular singularity (at the origin). These conditions just satisfy Fuchs’ theorem and
thus help us gain a better understanding of that theorem.
2.
Develop the ﬁrst few terms of a power-series solution, as in Section 7.5.
3.
Using this solution as y1, obtain a second series-type solution, y2, from Eq. (7.67), by
integrating it term by term.
Proceeding with Step 1, we have
y′′ + (p−1x−1 + p0 + p1x + ···)y′ + (q−2x−2 + q−1x−1 + ···)y = 0,
(7.70)
where x = 0 is at worst a regular singular point. If p−1 = q−1 = q−2 = 0, it reduces to an
ordinary point. Substituting
y =
∞
X
λ=0
aλxs+λ
(Step 2), we obtain
∞
X
λ=0
(s + λ)(s + λ −1)aλxs+λ−2 +
∞
X
i=−1
pi xi
∞
X
λ=0
(s + λ)aλxs+λ−1
+
∞
X
j=−2
q j x j
∞
X
λ=0
aλxs+λ = 0.
(7.71)
Assuming that p−1 ̸= 0, our indicial equation is
s(s −1) + p−1k + q−2 = 0,
which sets the net coefﬁcient of xs−2 equal to zero. This reduces to
s2 + (p−1 −1)s + q−2 = 0.
(7.72)
We denote the two roots of this indicial equation by s = α and s = α −n, where n is zero
or a positive integer. (If n is not an integer, we expect two independent series solutions by
the methods of Section 7.5 and we are done.) Then
(s −α)(s −α + n) = 0,
(7.73)
or
s2 + (n −2α)s + α(α −n) = 0,

7.6 Other Solutions
365
and equating coefﬁcients of s in Eqs. (7.72) and (7.73), we have
p−1 −1 = n −2α.
(7.74)
The known series solution corresponding to the larger root s = α may be written as
y1 = xα
∞
X
λ=0
aλxλ.
Substituting this series solution into Eq. (7.67) (Step 3), we are faced with
y2(x) = y1(x)
x
Z  
exp
 −
R x2
a
P∞
i=−1 pi xi
1 dx1

x2α
2
 P∞
λ=0 aλxλ
2
2
!
dx2,
(7.75)
where the solutions y1 and y2 have been normalized so that the Wronskian W(a) = 1.
Tackling the exponential factor ﬁrst, we have
x2
Z
a
∞
X
i=−1
pi xi
1dx1 = p−1 ln x2 +
∞
X
k=0
pk
k + 1xk+1
2
+ f (a),
(7.76)
with f (a) an integration constant that may depend on a. Hence,
exp

−
x2
Z
a
X
i
pi xi
1dx1

= exp[−f (a)]x−p−1
2
exp
 
−
∞
X
k=0
pk
k + 1xk+1
2
!
= exp[−f (a)]x−p−1
2

1 −
∞
X
k=0
pk
k + 1xk+1
2
+ 1
2!
 
−
∞
X
k=0
pk
k + 1xk+1
2
!2
+ ···

. (7.77)
This ﬁnal series expansion of the exponential is certainly convergent if the original expan-
sion of the coefﬁcient P(x) was uniformly convergent.
The denominator in Eq. (7.75) may be handled by writing

x2α
2
 ∞
X
λ=0
aλxλ
2
!2

−1
= x−2α
2
 ∞
X
λ=0
aλxλ
2
!−2
= x−2α
2
∞
X
λ=0
bλxλ
2.
(7.78)
Neglecting constant factors, which will be picked up anyway by the requirement that
W(a) = 1, we obtain
y2(x) = y1(x)
x
Z
x−p−1−2α
2
 ∞
X
λ=0
cλxλ
2
!
dx2.
(7.79)
Applying Eq. (7.74),
x−p−1−2α
2
= x−n−1
2
,
(7.80)

366
Chapter 7 Ordinary Differential Equations
and we have assumed here that n is an integer. Substituting this result into Eq. (7.79), we
obtain
y2(x) = y1(x)
x
Z 
c0x−n−1
2
+ c1x−n
2
+ c2x−n+1
2
+ ··· + cnx−1
2
+ ···

dx2.
(7.81)
The integration indicated in Eq. (7.81) leads to a coefﬁcient of y1(x) consisting of two
parts:
1.
A power series starting with x−n.
2.
A logarithm term from the integration of x−1 (when λ = n). This term always appears
when n is an integer, unless cn fortuitously happens to vanish.8
If we choose to combine y1 and the power series starting with x−n, our second solution
will assume the form
y2(x) = y1(x)ln|x| +
∞
X
j=−n
d j x j+α.
(7.82)
Example 7.6.4
A SECOND SOLUTION OF BESSEL’S EQUATION
From Bessel’s equation, Eq. (7.40), divided by x2 to agree with Eq. (7.59), we have
P(x) = x−1
Q(x) = 1
for the case
n = 0.
Hence p−1 = 1, q0 = 1; all other pi and q j vanish. The Bessel indicial equation, Eq. (7.43)
with n = 0, is
s2 = 0.
Hence we verify Eqs. (7.72) to (7.74) with n and α set to zero.
Our ﬁrst solution is available from Eq. (7.49). It is9
y1(x) = J0(x) = 1 −x2
4 + x4
64 −O(x6).
(7.83)
Now, substituting all this into Eq. (7.67), we have the speciﬁc case corresponding to
Eq. (7.75):
y2(x) = J0(x)
x
Z


exp
h
−
R x2 x−1
1 dx1
i

1 −x2
2
4 + x4
2
64 −···
2

dx2.
(7.84)
8For parity considerations, ln x is taken to be ln |x|, even.
9The capital O (order of) as written here means terms proportional to x6 and possibly higher powers of x.

7.6 Other Solutions
367
From the numerator of the integrand,
exp

−
x2
Z dx1
x1

= exp[−ln x2] = 1
x2
.
This corresponds to the x−p−1
2
in Eq. (7.77). From the denominator of the integrand, using
a binomial expansion, we obtain
"
1 −x2
2
4 + x4
2
64
#−2
= 1 + x2
2
2 + 5x4
2
32 + ··· .
Corresponding to Eq. (7.79), we have
y2(x) = J0(x)
x
Z
1
x2
"
1 + x2
2
2 + 5x4
2
32 + ···
#
dx2
= J0(x)

ln x + x2
4 + 5x4
128 + ···

.
(7.85)
Let us check this result. From Eq. (14.62), which gives the standard form of the second
solution, which is called a Neumann function and designated Y0,
Y0(x) = 2
π

ln x −ln2 + γ

J0(x) + 2
π
x2
4 −3x4
128 + ···

.
(7.86)
Two points arise: (1) Since Bessel’s equation is homogeneous, we may multiply y2(x) by
any constant. To match Y0(x), we multiply our y2(x) by 2/π. (2) To our second solution,
(2/π)y2(x), we may add any constant multiple of the ﬁrst solution. Again, to match Y0(x)
we add
2
π

−ln2 + γ

J0(x),
where γ is the Euler-Mascheroni constant, deﬁned in Eq. (1.13).10 Our new, modiﬁed
second solution is
y2(x) = 2
π

ln x −ln2 + γ

J0(x) + 2
π J0(x)
x2
4 + 5x4
128 + ···

.
(7.87)
Now the comparison with Y0(x) requires only a simple multiplication of the series for
J0(x) from Eq. (7.83) and the curly bracket of Eq. (7.87). The multiplication checks,
through terms of order x2 and x4, which is all we carried. Our second solution from
Eqs. (7.67) and (7.75) agrees with the standard second solution, the Neumann function
Y0(x).
■
The analysis that indicated the second solution of Eq. (7.58) to have the form given in
Eq. (7.82) suggests the possibility of just substituting Eq. (7.82) into the original differen-
tial equation and determining the coefﬁcients d j. However, the process has some features
different from that of Section 7.5, and is illustrated by the following example.
10The Neumann function Y0 is deﬁned as it is in order to achieve convenient asymptotic properties; see Sections 14.3 and 14.6.

368
Chapter 7 Ordinary Differential Equations
Example 7.6.5
MORE NEUMANN FUNCTIONS
We consider here second solutions to Bessel’s ODE of integer orders n > 0, using the
expansion given in Eq. (7.82). The ﬁrst solution, designated Jn and presented in Eq. (7.49),
arises from the value α = n from the indicial equation, while the quantity called n in
Eq. (7.82), the separation of the two roots of the indicial equation, has in the current context
the value 2n. Thus, Eq. (7.82) takes the form
y2(x) = Jn(x)ln|x| +
∞
X
j=−2n
d j x j+n,
(7.88)
where y2 must, apart from scale and a possible multiple of Jn, be the second solution
Yn of the Bessel equation. Substituting this form into Bessel’s equation, carrying out the
indicated differentiations and using the fact that Jn(x) is a solution of our ODE, we get
after combining similar terms
x2y′′
2 + xy′
2 + (x2 −n2)y2 =
2x J ′
n(x) +
X
j≥−2n
j( j + 2n)d j x j+n +
X
j≥−2n
d j x j+n+2 = 0.
(7.89)
We next insert the power-series expansion
2x J ′
n(x) =
X
j≥0
a j x j+n,
(7.90)
where the coefﬁcients can be obtained by differentiation of the expansion of Jn, see
Eq. (7.49), and have the values (for j ≥0)
a2 j =
(−1) j(n + 2 j)
j!(n + j)!2n+2 j−1 ,
a2 j+1 = 0.
(7.91)
This, and a redeﬁnition of the index j in the last term, bring Eq. (7.89) to the form
X
j≥0
a j x j+n +
X
j≥−2n
j( j + 2n)d j x j+n +
X
j≥−2n+2
d j−2x j+n = 0.
(7.92)
Considering ﬁrst the coefﬁcient of x−n+1 (corresponding to j = −2n + 1), we note that
its vanishing requires that d−2n+1 vanish, as the only contribution comes from the middle
summation. Since all a j of odd j vanish, the vanishing of d−2n+1 implies that all other d j
of odd j must also vanish. We therefore only need to give further consideration to even j.
We next note that the coefﬁcient d0 is arbitrary, and may without loss of generality
be set to zero. This is true because we may bring d0 to any value by adding to y2 an
appropriate multiple of the solution Jn, whose expansion has an xn leading term. We have
then exhausted all freedom in specifying y2; its scale is determined by our choice of its
logarithmic term.
Now, taking the coefﬁcient of xn (terms with j = 0), and remembering that d0 = 0, we
have
d−2 = −a0,

7.6 Other Solutions
369
and we may recur downward in steps of 2, using formulas based on the coefﬁcients of
xn−2, xn−4, ..., corresponding to
d j−2 = −j(2n + j)d j,
j = −2,−4,...,−2n + 2.
To obtain d j with positive j, we recur upward, obtaining from the coefﬁcient of xn+ j
d j = −a j −d j−2
j(2n + j) ,
j = 2,4, ...,
again remembering that d0 = 0.
Proceeding to n = 1 as a speciﬁc example, we have from Eq. (7.91) a0 = 1, a2 = −3/8,
and a4 = 5/192, so
d−2 = −1,
d2 = −a2
8 = 3
64,
d4 = −a4 −d2
24
= −
7
2304;
thus
y2(x) = J1(x)ln|x| −1
x + 3
64x3 −
7
2304x5 + ··· ,
in agreement (except for a multiple of J1 and a scale factor) with the standard form of the
Neumann function Y1:
Y1(x) = 2
π

ln
x
2
 + γ −1
2

J1(x) + 2
π

−1
x + 3
64x3 −
7
2304x5 + ···

.
(7.93)
■
As shown in the examples, the second solution will usually diverge at the origin because
of the logarithmic factor and the negative powers of x in the series. For this reason y2(x) is
often referred to as the irregular solution. The ﬁrst series solution, y1(x), which usually
converges at the origin, is called the regular solution. The question of behavior at the
origin is discussed in more detail in Chapters 14 and 15, in which we take up Bessel
functions, modiﬁed Bessel functions, and Legendre functions.
Summary
The two solutions of both sections (together with the exercises) provide a complete solu-
tion of our linear, homogeneous, second-order ODE, assuming that the point of expansion
is no worse than a regular singularity. At least one solution can always be obtained by
series substitution (Section 7.5). A second, linearly independent solution can be con-
structed by the Wronskian double integral, Eq. (7.67). This is all there are: No third,
linearly independent solution exists (compare Exercise 7.6.10).
The inhomogeneous, linear, second-order ODE will have a general solution formed by
adding a particular solution to the complete inhomogeneous equation to the general solu-
tion of the corresponding homogeneous ODE. Techniques for ﬁnding particular solutions
of linear but inhomogeneous ODEs are the topic of the next section.

370
Chapter 7 Ordinary Differential Equations
Exercises
7.6.1
You know that the three unit vectors ˆex, ˆey, and ˆez are mutually perpendicular
(orthogonal). Show that ˆex, ˆey, and ˆez are linearly independent. Speciﬁcally, show that
no relation of the form of Eq. (7.54) exists for ˆex, ˆey, and ˆez.
7.6.2
The criterion for the linear independence of three vectors A, B, and C is that the
equation
aA + bB + cC = 0,
analogous to Eq. (7.54), has no solution other than the trivial a = b = c = 0. Using
components A = (A1, A2, A3), and so on, set up the determinant criterion for the exis-
tence or nonexistence of a nontrivial solution for the coefﬁcients a,b, and c. Show that
your criterion is equivalent to the scalar triple product A · B × C ̸= 0.
7.6.3
Using the Wronskian determinant, show that the set of functions

1, xn
n!(n = 1,2,..., N)

is linearly independent.
7.6.4
If the Wronskian of two functions y1 and y2 is identically zero, show by direct integra-
tion that
y1 = cy2,
that is, that y1 and y2 are linearly dependent. Assume the functions have continuous
derivatives and that at least one of the functions does not vanish in the interval under
consideration.
7.6.5
The Wronskian of two functions is found to be zero at x0 −ε ≤x ≤x0 +ε for arbitrarily
small ε > 0. Show that this Wronskian vanishes for all x and that the functions are
linearly dependent.
7.6.6
The three functions sin x,ex, and e−x are linearly independent. No one function can be
written as a linear combination of the other two. Show that the Wronskian of sin x,ex,
and e−x vanishes but only at isolated points.
ANS.
W = 4sin x,
W = 0 for x = ±nπ,
n = 0,1,2,... .
7.6.7
Consider two functions ϕ1 = x and ϕ2 = |x|. Since ϕ′
1 = 1 and ϕ′
2 = x/|x|, W(ϕ1,ϕ2) =
0 for any interval, including [−1,+1]. Does the vanishing of the Wronskian over
[−1,+1] prove that ϕ1 and ϕ2 are linearly dependent? Clearly, they are not. What is
wrong?
7.6.8
Explain that linear independence does not mean the absence of any dependence. Illus-
trate your argument with cosh x and ex.
7.6.9
Legendre’s differential equation
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0

7.6 Other Solutions
371
has a regular solution Pn(x) and an irregular solution Qn(x). Show that the Wronskian
of Pn and Qn is given by
Pn(x)Q′
n(x) −P′
n(x)Qn(x) =
An
1 −x2 ,
with An independent of x.
7.6.10
Show, by means of the Wronskian, that a linear, second-order, homogeneous ODE of
the form
y′′(x) + P(x)y′(x) + Q(x)y(x) = 0
cannot have three independent solutions.
Hint. Assume a third solution and show that the Wronskian vanishes for all x.
7.6.11
Show the following when the linear second-order differential equation py′′+qy′+ry =
0 is expressed in self-adjoint form:
(a)
The Wronskian is equal to a constant divided by p:
W(x) =
C
p(x).
(b)
A second solution y2(x) is obtained from a ﬁrst solution y1(x) as
y2(x) = Cy1(x)
x
Z
dt
p(t)[y1(t)]2 .
7.6.12
Transform our linear, second-order ODE
y′′ + P(x)y′ + Q(x)y = 0
by the substitution
y = z exp

−1
2
x
Z
P(t)dt


and show that the resulting differential equation for z is
z′′ + q(x)z = 0,
where
q(x) = Q(x) −1
2 P′(x) −1
4 P2(x).
Note. This substitution can be derived by the technique of Exercise 7.6.25.
7.6.13
Use the result of Exercise 7.6.12 to show that the replacement of ϕ(r) by rϕ(r) may be
expected to eliminate the ﬁrst derivative from the Laplacian in spherical polar coordi-
nates. See also Exercise 3.10.34.

372
Chapter 7 Ordinary Differential Equations
7.6.14
By direct differentiation and substitution show that
y2(x) = y1(x)
x
Z exp[−
R s P(t)dt]
[y1(s)]2
ds
satisﬁes, like y1(x), the ODE
y′′
2(x) + P(x)y′
2(x) + Q(x)y2(x) = 0.
Note. The Leibniz formula for the derivative of an integral is
d
dα
h(α)
Z
g(α)
f (x,α)dx =
h(α)
Z
g(α)
∂f (x,α)
∂α
dx + f [h(α), α]dh(α)
dα
−f [g(α), α]dg(α)
dα .
7.6.15
In the equation
y2(x) = y1(x)
x
Z exp[−
R s P(t)dt]
[y1(s)]2
ds,
y1(x) satisﬁes
y′′
1 + P(x)y′
1 + Q(x)y1 = 0.
The function y2(x) is a linearly independent second solution of the same equation.
Show that the inclusion of lower limits on the two integrals leads to nothing new, that
is, that it generates only an overall constant factor and a constant multiple of the known
solution y1(x).
7.6.16
Given that one solution of
R′′ + 1
r R′ −m2
r2 R = 0
is R = rm, show that Eq. (7.67) predicts a second solution, R = r−m.
7.6.17
Using
y1(x) =
∞
X
n=0
(−1)n
(2n + 1)!x2n+1
as a solution of the linear oscillator equation, follow the analysis that proceeds through
Eq. (7.81) and show that in that equation cn = 0, so that in this case the second solution
does not contain a logarithmic term.
7.6.18
Show that when n is not an integer in Bessel’s ODE, Eq. (7.40), the second solution of
Bessel’s equation, obtained from Eq. (7.67), does not contain a logarithmic term.

7.6 Other Solutions
373
7.6.19
(a)
One solution of Hermite’s differential equation
y′′ −2xy′ + 2αy = 0
for α = 0 is y1(x) = 1. Find a second solution, y2(x), using Eq. (7.67). Show that
your second solution is equivalent to yodd (Exercise 8.3.3).
(b)
Find a second solution for α = 1, where y1(x) = x, using Eq. (7.67). Show that
your second solution is equivalent to yeven (Exercise 8.3.3).
7.6.20
One solution of Laguerre’s differential equation
xy′′ + (1 −x)y′ + ny = 0
for n = 0 is y1(x) = 1. Using Eq. (7.67), develop a second, linearly independent solu-
tion. Exhibit the logarithmic term explicitly.
7.6.21
For Laguerre’s equation with n = 0,
y2(x) =
x
Z es
s ds.
(a)
Write y2(x) as a logarithm plus a power series.
(b)
Verify that the integral form of y2(x), previously given, is a solution of Laguerre’s
equation (n = 0) by direct differentiation of the integral and substitution into the
differential equation.
(c)
Verify that the series form of y2(x), part (a), is a solution by differentiating the
series and substituting back into Laguerre’s equation.
7.6.22
One solution of the Chebyshev equation
(1 −x2)y′′ −xy′ + n2y = 0
for n = 0 is y1 = 1.
(a)
Using Eq. (7.67), develop a second, linearly independent solution.
(b)
Find a second solution by direct integration of the Chebyshev equation.
Hint. Let v = y′ and integrate. Compare your result with the second solution given in
Section 18.4.
ANS.
(a)
y2 = sin−1 x.
(b)
The second solution, Vn(x), is not deﬁned for n = 0.
7.6.23
One solution of the Chebyshev equation
(1 −x2)y′′ −xy′ + n2y = 0
for n = 1 is y1(x) = x. Set up the Wronskian double integral solution and derive a
second solution, y2(x).
ANS.
y2 = −(1 −x2)1/2.

374
Chapter 7 Ordinary Differential Equations
7.6.24
The radial Schrödinger wave equation for a spherically symmetric potential can be writ-
ten in the form
"
−¯h2
2m
d2
dr2 + l(l + 1)
¯h2
2mr2 + V (r)
#
y(r) = Ey(r).
The potential energy V (r) may be expanded about the origin as
V (r) = b−1
r
+ b0 + b1r + ··· .
(a)
Show that there is one (regular) solution y1(r) starting with rl+1.
(b)
From Eq. (7.69) show that the irregular solution y2(r) diverges at the origin as r−l.
7.6.25
Show that if a second solution, y2, is assumed to be related to the ﬁrst solution, y1,
according to y2(x) = y1(x) f (x), substitution back into the original equation
y′′
2 + P(x)y′
2 + Q(x)y2 = 0
leads to
f (x) =
x
Z exp[−
R s P(t)dt]
[y1(s)]2
ds,
in agreement with Eq. (7.67).
7.6.26
(a)
Show that
y′′ + 1 −α2
4x2
y = 0
has two solutions:
y1(x) = a0x(1+α)/2,
y2(x) = a0x(1−α)/2.
(b)
For α = 0 the two linearly independent solutions of part (a) reduce to the single
solution y1′ = a0x1/2. Using Eq. (7.68) derive a second solution,
y2′(x) = a0x1/2 ln x.
Verify that y2′ is indeed a solution.
(c)
Show that the second solution from part (b) may be obtained as a limiting case
from the two solutions of part (a):
y2′(x) = lim
α→0
 y1 −y2
α

.

7.7 Inhomogeneous Linear ODEs
375
7.7
INHOMOGENEOUS LINEAR ODES
We frame the discussion in terms of second-order ODEs, although the methods can be
extended to equations of higher order. We thus consider ODEs of the general form
y′′ + P(x)y′ + Q(x)y = F(x),
(7.94)
and proceed under the assumption that the corresponding homogeneous equation, with
F(x) = 0, has been solved, thereby obtaining two independent solutions designated y1(x)
and y2(x).
Variation of Parameters
The method of variation of parameters (variation of the constant) starts by writing a par-
ticular solution of the inhomogeneous ODE, Eq. (7.94), in the form
y(x) = u1(x)y1(x) + u2(x)y2(x).
(7.95)
We have speciﬁcally written u1(x) and u2(x) to emphasize that these are functions of the
independent variable, and not constant coefﬁcients. This, of course, means that Eq. (7.95)
does not constitute a restriction to the functional form of y(x). For clarity and compactness,
we will usually write these functions just as u1 and u2.
In preparation for inserting y(x), from Eq. (7.95), into the inhomogeneous ODE, we
compute its derivative:
y′ = u1y′
1 + u2y′
2 + (y1u′
1 + y2u′
2),
and take advantage of the redundancy in the form assumed for y by choosing u1 and u2 in
such a way that
y1u′
1 + y2u′
2 = 0,
(7.96)
where Eq. (7.96) is assumed to be an identity (i.e., to apply for all x). We will shortly show
that requiring Eq. (7.96) does not lead to an inconsistency.
After applying Eq. (7.96), y′, and its derivative y′′, are found to be
y′ = u1y′
1 + u2y′
2,
y′′ = u1y′′
1 + u2y′′
2 + u′
1y′
1 + u′
2y′
2,
and substitution into Eq. (7.94) yields
(u1y′′
1 + u2y′′
2 + u′
1y′
1 + u′
2y′
2) + P(x)(u1y′
1 + u2y′
2) + Q(x)(u1y1 + u2y2) = F(x),
which, because y1 and y2 are solutions of the homogeneous equation, reduces to
u′
1y′
1 + u′
2y′
2 = F(x).
(7.97)
Equations (7.96) and (7.97) are, for each value of x, a set of two simultaneous algebraic
equations in the variables u′
1 and u′
2; to emphasize this point we repeat them here:
y1u′
1 + y2u′
2 = 0,
y′
1u′
1 + y′
2u′
2 = F(x).
(7.98)

376
Chapter 7 Ordinary Differential Equations
The determinant of the coefﬁcients of these equations is

y1
y2
y′
1
y′
2
,
which we recognize as the Wronskian of the linearly independent solutions to the homo-
geneous equation. That means this determinant is nonzero, so there will, for each x, be
a unique solution to Eqs. (7.98), i.e., unique functions u′
1 and u′
2. We conclude that the
restriction implied by Eq. (7.96) is permissible.
Once u′
1 and u′
2 have been identiﬁed, each can be integrated, respectively yielding u1
and u2, and, via Eq. (7.95), a particular solution of our inhomogeneous ODE.
Example 7.7.1
AN INHOMOGENEOUS ODE
Consider the ODE
(1 −x)y′′ + xy′ −y = (1 −x)2.
(7.99)
The corresponding homogeneous ODE has solutions y1 = x and y2 = ex. Thus, y′
1 = 1,
y′
2 = ex, and the simultaneous equations for u′
1 and u′
2 are
x u′
1 + exu′
2 = 0,
u′
1 + exu′
2 = F(x).
(7.100)
Here F(x) is the inhomogeneous term when the ODE has been written in the standard
form, Eq. (7.94). This means that we must divide Eq. (7.99) through by 1 −x (the coefﬁ-
cient of y′′), after which we see that F(x) = 1 −x.
With the above choice of F(x), we solve Eqs. (7.100), obtaining
u′
1 = 1,
u′
2 = −xe−x,
which integrate to
u1 = x,
u2 = (x + 1)e−x.
Now forming a particular solution to the inhomogeneous ODE, we have
yp(x) = u1y1 + u2y2 = x(x) +
 (x + 1)e−x
ex = x2 + x + 1.
Because x is a solution to the homogeneous equation, we may remove it from the above
expression, leaving the more compact formula yp = x2 + 1.
The general solution to our ODE therefore takes the ﬁnal form
y(x) = C1x + C2ex + x2 + 1.
■

7.8 Nonlinear Differential Equations
377
Exercises
7.7.1
If our linear, second-order ODE is inhomogeneous, that is, of the form of Eq. (7.94),
the most general solution is
y(x) = y1(x) + y2(x) + yp(x),
where y1 and y2 are independent solutions of the homogeneous equation.
Show that
yp(x) = y2(x)
x
Z
y1(s)F(s)ds
W{y1(s), y2(s)} −y1(x)
x
Z
y2(s)F(s)ds
W{y1(s), y2(s)},
with W{y1(x), y2(x)} the Wronskian of y1(s) and y2(s).
Find the general solutions to the following inhomogeneous ODEs:
7.7.2
y′′ + y = 1.
7.7.3
y′′ + 4y = ex.
7.7.4
y′′ −3y′ + 2y = sin x.
7.7.5
xy′′ −(1 + x)y′ + y = x2.
7.8
NONLINEAR DIFFERENTIAL EQUATIONS
The main outlines of large parts of physical theory have been developed using mathe-
matics in which the objects of concern possessed some sort of linearity property. As a
result, linear algebra (matrix theory) and solution methods for linear differential equations
were appropriate mathematical tools, and the development of these mathematical topics
has progressed in the directions illustrated by most of this book. However, there is some
physics that requires the use of nonlinear differential equations (NDEs). The hydrodynam-
ics of viscous, compressible media is described by the Navier-Stokes equations, which are
nonlinear. The nonlinearity evidences itself in phenomena such as turbulent ﬂow, which
cannot be described using linear equations. Nonlinear equations are also at the heart of
the description of behavior known as chaotic, in which the evolution of a system is so
sensitive to its initial conditions that it effectively becomes unpredictable.
The mathematics of nonlinear ODEs is both more difﬁcult and less developed than that
of linear ODEs, and accordingly we provide here only an extremely brief survey. Much of
the recent progress in this area has been in the development of computational methods for
nonlinear problems; that is also outside the scope of this text.
In this ﬁnal section of the present chapter we discuss brieﬂy some speciﬁc NDEs, the
classical Bernoulli and Riccati equations.

378
Chapter 7 Ordinary Differential Equations
Bernoulli and Riccati Equations
Bernoulli equations are nonlinear, having the form
y′(x) = p(x)y(x) + q(x)[y(x)]n,
(7.101)
where p and q are real functions and n ̸= 0, 1 to exclude ﬁrst-order linear ODEs. However,
if we substitute
u(x) = [y(x)]1−n,
then Eq. (7.101) becomes a ﬁrst-order linear ODE,
u′ = (1 −n)y−ny′ = (1 −n)

p(x)u(x) + q(x)

,
(7.102)
which we can solve (using an integrating factor) as described in Section 7.2.
Riccati equations are quadratic in y(x):
y′ = p(x)y2 + q(x)y + r(x),
(7.103)
where we require p ̸= 0 to exclude linear ODEs and r ̸= 0 to exclude Bernoulli equations.
There is no known general method for solving Riccati equations. However, when a special
solution y0(x) of Eq. (7.103) is known by a guess or inspection, then one can write the
general solution in the form y = y0 + u, with u satisfying the Bernoulli equation
u′ = pu2 + (2py0 + q)u,
(7.104)
because substitution of y = y0 + u into Eq. (7.103) removes r(x) from the resulting
equation.
There are no general methods for obtaining exact solutions of most nonlinear ODEs.
This fact makes it more important to develop methods for ﬁnding the qualitative behavior
of solutions. In Section 7.5 of this chapter we mentioned that power-series solutions of
ODEs exist except (possibly) at essential singularities of the ODE. The coefﬁcients in
the power-series expansions provide us with the asymptotic behavior of the solutions. By
making expansions of solutions to NDEs and retaining only the linear terms, it will often
be possible to understand the qualitative behavior of the solutions in the neighborhood of
the expansion point.
Fixed and Movable Singularities, Special Solutions
A ﬁrst step in analyzing the solutions of NDEs is to identify their singularity structures.
Solutions of NDEs may have singular points that are independent of the initial or bound-
ary conditions; these are called ﬁxed singularities. But in addition they may have spon-
taneous, or movable, singularities that vary with the initial or boundary conditions. This
feature complicates the asymptotic analysis of NDEs.

7.8 Nonlinear Differential Equations
379
Example 7.8.1
MOVEABLE SINGULARITY
Compare the linear ODE
y′ +
y
x −1 = 0,
(which has an obvious regular singularity at x = 1), with the NDE y′ = y2. Both have the
same solution with initial condition y(0) = 1, namely y(x) = 1/(1 −x). But for y(0) = 2,
the linear ODE has solution y = 1 + 1/(1 −x), while the NDE now has solution y(x) =
2/(1 −2x). The singularity in the solution of the NDE has moved to x = 1/2.
■
For a linear second-order ODE we have a complete description of its solutions and their
asymptotic behavior when two linearly independent solutions are known. But for NDEs
there may still be special solutions whose asymptotic behavior is not obtainable from two
independent solutions. This is another characteristic property of NDEs, which we illustrate
again by an example.
Example 7.8.2
SPECIAL SOLUTION
The NDE y′′ = yy′/x has two linearly independent solutions that deﬁne the two-parameter
family of curves
y(x) = 2c1 tan(c1 ln x + c2) −1,
(7.105)
where the ci are integration constants. However, this NDE also has the special solution y =
c3 = constant, which cannot be obtained from Eq. (7.105) by any choice of the parameters
c1, c2.
The “general solution” in Eq. (7.105) can be obtained by making the substitution x = et,
and then deﬁning Y(t) ≡y(et) so that x(dy/dx) = dY/dt, thereby obtaining the ODE
Y ′′ = Y ′(Y + 1). This ODE can be integrated once to give Y ′ = 1
2Y 2 + Y + c with c =
2(c2
1 + 1/4) an integration constant. The equation for Y ′ is separable and can be integrated
again to yield Eq. (7.105).
■
Exercises
7.8.1
Consider the Riccati equation y′ = y2 −y −2. A particular solution to this equation is
y = 2. Find a more general solution.
7.8.2
A particular solution to y′ = y2/x3 −y/x + 2x is y = x2. Find a more general solution.
7.8.3
Solve the Bernoulli equation y′ + xy = xy3.
7.8.4
ODEs of the form y = xy′ + f (y′) are known as Clairaut equations. The ﬁrst step in
solving an equation of this type is to differentiate it, yielding
y′ = y′ + xy′′ + f ′(y′)y′′,
or
y′′ x + f ′(y′)

= 0.
Solutions may therefore be obtained both from y′′ = 0 and from f ′(y′) = −x. The
so-called general solution comes from y′′ = 0.

380
Chapter 7 Ordinary Differential Equations
For f (y′) = (y′)2,
(a)
Obtain the general solution (note that it contains a single constant).
(b)
Obtain the so-called singular solution from f ′(y′) = −x. By substituting back into
the original ODE show that this singular solution contains no adjustable constants.
Note. The singular solution is the envelope of the general solutions.
Additional Readings
Cohen, H., Mathematics for Scientists and Engineers. Englewood Cliffs, NJ: Prentice-Hall (1992).
Golomb, M., and M. Shanks, Elements of Ordinary Differential Equations. New York: McGraw-Hill (1965).
Hubbard, J., and B. H. West, Differential Equations. Berlin: Springer (1995).
Ince, E. L., Ordinary Differential Equations. New York: Dover (1956). The classic work in the theory of ordinary
differential equations.
Jackson, E. A., Perspectives of Nonlinear Dynamics. Cambridge: Cambridge University Press (1989).
Jordan, D. W., and P. Smith, Nonlinear Ordinary Differential Equations, 2nd ed. Oxford: Oxford University
Press (1987).
Margenau, H., and G. M. Murphy, The Mathematics of Physics and Chemistry, 2nd ed. Princeton, NJ: Van
Nostrand (1956).
Miller, R. K., and A. N. Michel, Ordinary Differential Equations. New York: Academic Press (1982).
Murphy, G. M., Ordinary Differential Equations and Their Solutions. Princeton, NJ: Van Nostrand (1960).
A thorough, relatively readable treatment of ordinary differential equations, both linear and nonlinear.
Ritger, P. D., and N. J. Rose, Differential Equations with Applications. New York: McGraw-Hill (1968).
Sachdev, P. L., Nonlinear Differential Equations and their Applications. New York: Marcel Dekker (1991).
Tenenbaum, M., and H. Pollard, Ordinary Differential Equations. New York: Dover (1985). Detailed and read-
able (over 800 pages). This is a reprint of a work originally published in 1963, and stresses formal manipula-
tions. Its references to numerical methods are somewhat dated.

CHAPTER 8
STURM-LIOUVILLE THEORY
8.1
INTRODUCTION
Chapter 7 examined methods for solving ordinary differential equations (ODEs), with
emphasis on techniques that can generate the solutions. In the present chapter we shift
the focus to the general properties that solutions must have to be appropriate for speciﬁc
physics problems, and to discuss the solutions using the notions of vector spaces and eigen-
value problems that were developed in Chapters 5 and 6.
A typical physics problem controlled by an ODE has two important properties: (1) Its
solution must satisfy boundary conditions, and (2) It contains a parameter whose value
must be set in a way that satisﬁes the boundary conditions. From a vector-space perspec-
tive, the boundary conditions (plus continuity and differentiability requirements) deﬁne the
Hilbert space of our problem, while the parameter normally occurs in a way that permits
the ODE to be written as an eigenvalue equation within that Hilbert space.
These ideas can be made clearer by examining a speciﬁc example. The standing waves
of a vibrating string clamped at its ends are governed by the ODE
d2ψ
dx2 + k2 ψ = 0,
(8.1)
where ψ(x) is the amplitude of the transverse displacement at the point x along the string,
and k is a parameter. This ODE has solutions for any value of k, but the solutions of
relevance to the string problem must have ψ(x) = 0 for the values of x at the ends of the
string.
The boundary conditions of this problem can be interpreted as deﬁning a Hilbert space
whose members are differentiable functions with zeros at the boundary values of x; the
ODE itself can be written as the eigenvalue equation
Lψ = k2 ψ,
L = −d2
dx2 .
(8.2)
381
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

382
Chapter 8 Sturm-Liouville Theory
For practical reasons the eigenvalue is given the name k2. It is required to ﬁnd functions
ψ(x) that solve Eq. (8.2) subject to the boundary conditions, i.e., to ﬁnd members ψ(x) of
our Hilbert space that solve the eigenvalue equation.
We could now follow the procedures developed in Chapter 5, namely (1) choose a basis
for our Hilbert space (a set of functions with zeros at the boundary values of x), (2) deﬁne
a scalar product for our space, (3) expand L and ψ in terms of our basis, and (4) solve the
resulting matrix equation. However, that procedure makes no use of any speciﬁc features
of the current ODE, and in particular ignores the fact that it is easily solved.
Instead, we continue with the example deﬁned by Eq. (8.1), using our ability to solve
the ODE involved.
Example 8.1.1
STANDING WAVES, VIBRATING STRING
We consider a string clamped at x = 0 and x = l and undergoing transverse vibrations.
As already indicated, its standing wave amplitudes ψ(x) are solutions of the differential
equation
d2ψ(x)
dx2
+ k2ψ(x) = 0,
(8.3)
where k is not initially known and ψ(x) is subject to the boundary conditions that the ends
of the string be ﬁxed in position: ψ(0) = ψ(l) = 0. This is the eigenvalue problem deﬁned
in Eq. (8.2).
The general solution to this differential equation is ψ(x) = A sinkx + B coskx, and in
the absence of the boundary conditions solutions would exist for all values of k, A, and
B. However, the boundary condition at x = 0 requires us to set B = 0, leaving ψ(x) =
A sinkx. We have yet to satisfy the boundary condition at x = l. The fact that A is as yet
unspeciﬁed is not helpful for this purpose, as A = 0 leaves us with only the trivial solution
ψ = 0. We must, instead, require sinkl = 0, which is accomplished by setting kl = nπ,
where n is a nonzero integer, leading to
ψn(x) = A sin
nπx
l

,
k2 = n2π2
l2
,
n = 1,2,....
(8.4)
Because Eq. (8.3) is homogeneous, it will have solutions of arbitrary scale, so A can have
any value. Since our purpose is usually to identify linearly independent solutions, we disre-
gard changes in the sign or magnitude of A. In the vibrating string problem, these quantities
control the amplitude and phase of the standing waves. Since changing the sign of n sim-
ply changes the sign of ψ, +n and −n in Eq. (8.4) are regarded here as equivalent, so we
restricted n to positive values. The ﬁrst few ψn are shown in Fig. 8.1. Note that the number
of nodes increases with n: ψn has n + 1 nodes (including the two nodes at the ends of the
string).
The fact that our problem has solutions only for discrete values of k is typical of eigen-
value problems, and in this problem the discreteness in k can be traced directly to the
presence of the boundary conditions. Figure 8.2 shows what happens when k is varied
in either direction from the acceptable value π/l, with the boundary condition at x = 0
maintained for all k. It is obvious that the eigenvalues (here k2) lie at separated points, and

8.1 Introduction
383
FIGURE 8.1
Standing wave patterns of a vibrating string.
a
b
c
d
e
x
0
FIGURE 8.2
Solutions to Eq. (8.3) on the range 0 ≤x ≤l for: (a) k = 0.9π/l,
(b) k = π/l, (c) k = 1.2π/l, (d) k = 1.5π/l, (e) k = 1.9π/l.
that the boundary condition at x = l cannot be satisﬁed for k < π/l. Moreover, the ﬁrst
acceptable k value larger than π/l is clearly larger than 1.9π/l (it is actually 2π/l).
As already noted, the solution to this eigenvalue problem is undetermined as to scale
because the underlying equation (together with its boundary conditions) is homogeneous.
However, if we introduce a scalar product of deﬁnition
⟨f |g⟩=
lZ
0
f ∗(x)g(x)dx,
(8.5)
we can deﬁne solutions that are normalized; requiring ⟨ψn|ψn⟩= 1, we have, with arbitrary
sign,
ψn(x) =
r
2
l sin
nπx
l

.
(8.6)
Although we did not solve Eq. (8.2) by an expansion technique, the solutions (the eigen-
functions) will still have properties that depend on whether the operator L is Hermitian. As
we saw in Chapter 5, the Hermitian property depends both on L and the deﬁnition of the
scalar product, and a topic for discussion in the present chapter is the identiﬁcation of con-
ditions making an operator Hermitian. This issue is important because Hermiticity implies
real eigenvalues as well as orthogonality and completeness of the eigenfunctions.
■

384
Chapter 8 Sturm-Liouville Theory
Summarizing, the matters of interest here, and the subject matter of the current chapter,
include:
1.
The conditions under which an ODE can be written as an eigenvalue equation with a
self-adjoint (Hermitian) operator,
2.
Methods for the solution of ODEs subject to boundary conditions, and
3.
The properties of the solutions to ODE eigenvalue equations.
8.2
HERMITIAN OPERATORS
Characterization of the general features of eigenproblems arising from second-order dif-
ferential equations is known as Sturm-Liouville theory. It therefore deals with eigenvalue
problems of the form
Lψ(x) = λψ(x),
(8.7)
where L is a linear second-order differential operator, of the general form
L(x) = p0(x) d2
dx2 + p1(x) d
dx + p2(x).
(8.8)
The key matter at issue here is to identify the conditions under which L is a Hermitian
operator.
Self-Adjoint ODEs
L is known in differential equation theory as self-adjoint if
p ′
0 (x) = p1(x).
(8.9)
This feature enables L(x) to be written
L(x) = d
dx

p0(x) d
dx

+ p2(x),
(8.10)
and the operation of L on a function u(x) then takes the form
Lu = (p0u′)′ + p2u.
(8.11)
Inserting Eq. (8.11) into an integral of the form
R b
a v∗(x)Lu(x)dx, we proceed by applying
an integration by parts to the p0 term (assuming that p0 is real):
b
Z
a
v∗(x)Lu(x)dx =
b
Z
a
h
v∗ p0u′′ + v∗p2u
i
dx
=
h
v∗p0u′i b
a +
b
Z
a

−(v∗)′ p0u′ + v∗p2u

dx.

8.2 Hermitian Operators
385
Another integration by parts leads to
b
Z
a
v∗(x)Lu(x)dx =
h
v∗p0u′ −(v∗)′ p0u
i b
a +
b
Z
a
h
p0(v∗)′′ u + v∗p2u
i
dx
=
h
v∗p0u′ −(v∗)′ p0u
i b
a +
b
Z
a
(Lv)∗u dx.
(8.12)
Equation (8.12) shows that, if the boundary terms

···
 b
a vanish and the scalar product
is an unweighted integral from a to b, then the operator L is self-adjoint, as that term
was deﬁned for operators. In passing, we observe that the notion of self-adjointness in
differential equation theory is weaker than the corresponding concept for operators in our
Hilbert spaces, due to the lack of a requirement on the boundary terms. We again stress
that the Hilbert-space deﬁnition of self-adjoint depends not only on the form of L but also
on the deﬁnition of the scalar product and the boundary conditions.
Looking further at the boundary terms, we see that they are surely zero if u and v both
vanish at the endpoints x = a and x = b (a case of what are termed Dirichlet boundary
conditions). The boundary terms are also zero if both u′ and v′ vanish at a and b (Neu-
mann boundary conditions). Even if neither Dirichlet nor Neumann boundary conditions
apply, it may happen (particularly in a periodic system, such as a crystal lattice) that the
boundary terms vanish because v∗p0u′
a = v∗p0u′
b for all u and v.
Specializing Eq. (8.12) to the case that u and v are eigenfunctions of L with respective
real eigenvalues λu and λv, that equation reduces to
(λu −λv)
b
Z
a
v∗u dx =
h
p0(v∗u′ −(v∗)′u)
i b
a.
(8.13)
It is thus apparent that if the boundary terms vanish and λu ̸= λv, then u and v must
be orthogonal on the interval (a,b). This is a speciﬁc illustration of the orthogonality
requirement for eigenfunctions of a Hermitian operator in a Hilbert space.
Making an ODE Self-Adjoint
Some of the differential equations that are important in physics involve operators L that are
self-adjoint in the differential-equation sense, meaning that they satisfy Eq. (8.9); others
are not. However, if an operator does not satisfy Eq. (8.9), it is known how to multiply it
by a quantity that converts it into self-adjoint form. Letting such a quantity be designated
w(x), the Sturm-Liouville eigenvalue problem of Eq. (8.7) becomes
w(x)L(x)ψ(x) = w(x)λψ(x),
(8.14)
an equation that has the same eigenvalues λ and eigenfunctions ψ(x) as the original prob-
lem in Eq. (8.7). If now w(x) is chosen to be
w(x) = p−1
0
exp
Z
p1(x)
p0(x) dx

,
(8.15)

386
Chapter 8 Sturm-Liouville Theory
where p0 and p1 are the quantities in L as given in Eq. (8.8), we can by direct evaluation
ﬁnd that
w(x)L(x) = p0
d2
dx2 + p1
d
dx + w(x)p2(x),
(8.16)
where
p0 = exp
Z
p1(x)
p0(x) dx

,
p1 = p1
p0
exp
Z
p1(x)
p0(x) dx

.
(8.17)
It is then straightforward to show that p ′
0 = p1, so wL satisﬁes the self-adjoint condition.
If we now apply the process represented by Eq. (8.12) to wL, we get
b
Z
a
v∗(x)w(x)Lu(x)dx =
h
v∗p0u′ −
 v∗′ p0u
i b
a +
b
Z
a
w(x)(Lv)∗u dx.
(8.18)
If the boundary terms vanish, Eq. (8.18) is equivalent to ⟨v|L|u⟩= ⟨Lv|u⟩when the scalar
product is deﬁned to be
⟨v|u⟩=
b
Z
a
v∗(x)u(x)w(x)dx.
(8.19)
Again considering the case that u and v are eigenfunctions of L, with respective eigen-
values λu and λv, Eq. (8.18) reduces to
(λu −λv)
b
Z
a
v∗u w dx =
h
wp0
 v∗u′ −(v∗)′u
i b
a,
(8.20)
where p0 is the coefﬁcient of y′′ in the original ODE. We thus see that if the right-hand
side of Eq. (8.20) vanishes, then u and v are orthogonal on (a,b) with weight factor w
when λu ̸= λv. In other words, our choice of scalar product deﬁnition and boundary con-
ditions have made L a self-adjoint operator in our Hilbert space, thereby producing an
eigenfunction orthogonality condition.
Summarizing, we have the useful and important result:
If a second-order differential operator L has coefﬁcients p0(x) and p1(x) that sat-
isfy the self-adjoint condition, Eq. (8.9), then it is Hermitian, given (a) a scalar prod-
uct of uniform weight and (b) boundary conditions that remove the endpoint terms of
Eq. (8.12).
If Eq. (8.9) is not satisﬁed, then L is Hermitian if (a) the scalar product is deﬁned
to include the weight factor given in Eq. (8.15), and (b) boundary conditions cause
removal of the endpoint terms in Eq. (8.18).
Note that once the problem has been deﬁned such that L is Hermitian, then the general
properties proved for Hermitian problems apply: the eigenvalues are real; the eigenfunc-
tions are (or if degenerate can be made) orthogonal, using the relevant scalar product
deﬁnition.

8.2 Hermitian Operators
387
Example 8.2.1
LAGUERRE FUNCTIONS
Consider the eigenvalue problem Lψ = λψ, with
L = x d2
dx2 + (1 −x) d
dx ,
(8.21)
subject to (a) ψ nonsingular on 0 ≤x < ∞, and (b) limx→∞ψ(x) = 0. Condition (a) is
simply a requirement that we use the solution of the differential equation that is regular at
x = 0; and condition (b) is a typical Dirichlet boundary condition.
The operator L is not self-adjoint, with p0 = x and p1 = 1 −x. But we can form
w(x) = 1
x exp
Z 1 −x
x
dx

= 1
x eln x−x = e−x.
(8.22)
The boundary terms, for arbitrary eigenfunctions u and v, are of the form
h
xe−x
v∗u′ −(v∗)′u
i ∞
0 ;
their contributions at x = ∞vanish because u and v go to zero; the common factor x
causes the x = 0 contribution to vanish also. We therefore have a self-adjoint problem,
with u and v of different eigenvalues orthogonal under the deﬁnition
⟨v|u⟩=
∞
Z
0
v∗(x)u(x)e−xdx.
The eigenvalue equation of this example is that whose solutions are the Laguerre
polynomials; what we have shown here is that they are orthogonal on (0,∞) with
weight e−x.
■
Exercises
8.2.1
Show that Laguerre’s ODE, Table 7.1, may be put into self-adjoint form by multiplying
by e−x and that w(x) = e−x is the weighting function.
8.2.2
Show that the Hermite ODE, Table 7.1, may be put into self-adjoint form by multiplying
by e−x2 and that this gives w(x) = e−x2 as the appropriate weighting function.
8.2.3
Show that the Chebyshev ODE, Table 7.1, may be put into self-adjoint form by mul-
tiplying by (1 −x2)−1/2 and that this gives w(x) = (1 −x2)−1/2 as the appropriate
weighting function.
8.2.4
The Legendre, Chebyshev, Hermite, and Laguerre equations, given in Table 7.1, have
solutions that are polynomials. Show that ranges of integration that guarantee that the
Hermitian operator boundary conditions will be satisﬁed are
(a)
Legendre [−1,1],
(b)
Chebyshev [−1,1],
(c)
Hermite (−∞,∞),
(d)
Laguerre [0,∞).

388
Chapter 8 Sturm-Liouville Theory
8.2.5
The functions u1(x) and u2(x) are eigenfunctions of the same Hermitian operator but
for distinct eigenvalues λ1 and λ2. Prove that u1(x) and u2(x) are linearly independent.
8.2.6
Given that
P1(x) = x
and
Q0(x) = 1
2ln
1 + x
1 −x

are solutions of Legendre’s differential equation (Table 7.1) corresponding to different
eigenvalues:
(a)
Evaluate their orthogonality integral
1
Z
−1
x
2 ln
1 + x
1 −x

dx.
(b)
Explain why these two functions are not orthogonal, that is, why the proof of
orthogonality does not apply.
8.2.7
T0(x) = 1 and V1(x) = (1 −x2)1/2 are solutions of the Chebyshev differential equation
corresponding to different eigenvalues. Explain, in terms of the boundary conditions,
why these two functions are not orthogonal on the range (−1,1) with the weighting
function found in Exercise 8.2.3.
8.2.8
A set of functions un(x) satisﬁes the Sturm-Liouville equation
d
dx

p(x) d
dx un(x)

+ λnw(x)un(x) = 0.
The functions um(x) and un(x) satisfy boundary conditions that lead to orthogonality.
The corresponding eigenvalues λm and λn are distinct. Prove that for appropriate bound-
ary conditions, u′
m(x) and u′
n(x) are orthogonal with p(x) as a weighting function.
8.2.9
Linear operator A has n distinct eigenvalues and n corresponding eigenfunctions:
Aψi = λiψi. Show that the n eigenfunctions are linearly independent. Do not assume
A to be Hermitian.
Hint. Assume linear dependence, i.e., that ψn = Pn−1
i=1 aiψi. Use this relation and the
operator-eigenfunction equation ﬁrst in one order and then in the reverse order. Show
that a contradiction results.
8.2.10
The ultraspherical polynomials C(α)
n (x) are solutions of the differential equation

(1 −x2) d2
dx2 −(2α + 1)x d
dx + n(n + 2α)

C(α)
n (x) = 0.
(a)
Transform this differential equation into self-adjoint form.
(b)
Find an interval of integration and weighting factor that make C(α)
n (x) of the same
α but different n orthogonal.
Note. Assume that your solutions are polynomials.

8.3 ODE Eigenvalue Problems
389
8.3
ODE EIGENVALUE PROBLEMS
Now that we have identiﬁed the conditions that make a second-order ODE eigenvalue
problem Hermitian, let’s examine several such problems to gain further understanding of
the processes involved and to illustrate techniques for ﬁnding solutions.
Example 8.3.1
LEGENDRE EQUATION
The Legendre equation,
Ly(x) = −(1 −x2)y′′(x) + 2xy′(x) = λy(x),
(8.23)
deﬁnes an eigenvalue problem that arises when ∇2 is written in spherical polar coordinates,
with x identiﬁed as cosθ, where θ is the polar angle of the coordinate system. The range
of x in this context is −1 ≤x ≤1, and in typical circumstances one needs solutions to
Eq. (8.23) that are nonsingular on the entire range of x. It turns out that this is a nontrivial
requirement, mainly because x = ±1 are singular points of the Legendre ODE. If we regard
nonsingularity of y at x = ±1 as a set of boundary conditions, we shall ﬁnd that this
requirement is sufﬁcient to deﬁne eigenfunctions of the Legendre operator.
This eigenvalue problem, namely Eq. (8.23) plus nonsingularity at x = ±1, is conve-
niently handled by the method of Frobenius. We assume solutions of the form
y =
∞
X
j=0
a j xs+ j,
(8.24)
with indicial equation s(s −1) = 0, whose solutions are s = 0 and s = 1. For s = 0, we
obtain the following recurrence relation for the coefﬁcients a j:
a j+2 =
j( j + 1) −λ
( j + 1)( j + 2)a j.
(8.25)
We may set a1 = 0, thereby causing all a j of odd j to vanish, so (for s = 0) our series will
contain only even powers of x. The boundary condition comes into play because Eq. (8.24)
diverges at x = ±1 for all λ except those that actually cause the series to terminate after a
ﬁnite number of terms.
To see how the divergence arises, note that for large j and |x| = 1 the ratio of successive
terms of the series approaches
a j x j
a j+2x j+2 →
j( j + 1)
( j + 1)( j + 2) →1,
so the ratio test is indeterminate. However, application of the Gauss test shows that this
series diverges, as was discussed in more detail in Example 1.1.7.
The series in Eq. (8.24) can be made to terminate after al for some even l by choosing
λ = l(l + 1), a value that makes al+2 = 0. Then al+4, al+6,... will also vanish, and our
solution will be a polynomial, which is clearly nonsingular for all |x| ≤1. Summarizing,
we have, for even l, solutions that are polynomials of degree l as eigenfunctions, and the
corresponding eigenvalues are l(l + 1).

390
Chapter 8 Sturm-Liouville Theory
For s = 1 we must set a1 = 0 and the recurrence relation is
a j+2 = ( j + 1)( j + 2) −λ
( j + 2)( j + 3)
a j,
(8.26)
which also leads to divergence at |x| = 1. However, the divergence can now be avoided
by setting λ = (l + 1)(l + 2) for some even value of l, thereby causing al+2, al+4,...
to vanish. The result will be a polynomial of degree l + s, i.e., of an odd degree l + 1.
These solutions can be described equivalently as, for odd l, polynomials of degree l with
eigenvalues λ = l(l + 1), so the overall set of eigenfunctions consists of polynomials of all
integer degrees l, with respective eigenvalues l(l + 1). When given the conventional scal-
ing, these polynomials are called Legendre polynomials. Veriﬁcation of these properties
of solutions to the Legendre equation is left to Exercise 8.3.1.
Before leaving the Legendre equation, note that its ODE is self-adjoint, and that the
coefﬁcient of d2/dx2 in the Legendre operator is p0 = −(1 −x2), which vanishes at x =
±1. Comparing with Eq. (8.12), we see that this value of p0 causes the vanishing of the
boundary terms when we take the adjoint of L, so the Legendre operator on the range
−1 ≤x ≤1 is Hermitian, and therefore has orthogonal eigenfunctions. In other words, the
Legendre polynomials are orthogonal with unit weight on (−1,1).
■
Let’s examine one more ODE that leads to an interesting eigenvalue problem.
Example 8.3.2
HERMITE EQUATION
Consider the Hermite differential equation,
Ly = −y′′ + 2xy′ = λy,
(8.27)
which we wish to regard as an eigenvalue problem on the range −∞< x < ∞. To make
L Hermitian, we deﬁne a scalar product with a weight factor as given by Eq. (8.15),
⟨f |g⟩=
∞
Z
−∞
f ∗(x)g(x)e−x2 dx,
(8.28)
and demand (as a boundary condition) that our eigenfunctions yn have ﬁnite norms using
this scalar product, meaning that ⟨yn|yn⟩< ∞.
Again we obtain a solution by the method of Frobenius, as a series of the form given
in Eq. (8.24). Again the indicial equation is s(s −1) = 0, and for s = 0 we can develop a
series of even powers of x with coefﬁcients satisfying the recurrence relation
a j+2 =
2 j −λ
( j + 1)( j + 2)a j.
(8.29)
This series converges for all x, but (assuming it does not terminate) it behaves asymptoti-
cally for large |x| as ex2 and therefore does not describe a function of ﬁnite norm, even with
the e−x2 weight factor in the scalar product. Thus, even though the series solution always
converges, our boundary conditions require that we arrange to terminate the series, thereby
producing polynomial solutions. From Eq. (8.29) we see that the condition for obtaining
an even polynomial of degree j is that λ = 2 j. Odd polynomial solutions can be obtained

8.3 ODE Eigenvalue Problems
391
using the indicial equation solution s = 1. Details of both the solutions and the asymptotic
properties are the subject of Exercise 8.3.3.
Since we have established that this is a Hermitian eigenvalue problem with the scalar
product as deﬁned in Eq. (8.28), its solutions (when scaled conventionally they are called
Hermite polynomials) are orthogonal using that scalar product.
■
Some ODE eigenvalue problems can be attacked by dividing the space in which they
reside into regions that are most naturally treated in different ways. The following example
illustrates this situation, with a potential that is assumed nonzero only within a ﬁnite region.
Example 8.3.3
DEUTERON GROUND STATE
The deuteron is a bound state of a neutron and a proton. Due to the short range of the
nuclear force, the deuteron properties do not depend much on the detailed shape of the
interaction potential. Thus, this system may be modeled by a spherically symmetric square
well potential with the value V = V0 < 0 when the nucleons are within a distance a of each
other, but with V = 0 when the internucleon distance is greater than a. The Schrödinger
equation for the relative motion of the two nucleons assumes the form
−¯h2
2µ∇2ψ + V ψ = Eψ,
where µ is the reduced mass of the system (approximately half the mass of either particle).
This eigenvalue equation must be solved subject to the boundary conditions that ψ be ﬁnite
at r = 0 and approach zero at r = ∞sufﬁciently rapidly to be a member of an L2 Hilbert
space. The eigenfunctions ψ must also be continuous and differentiable for all r, including
r = a.
It can be shown that if there is to be a bound state, E will have to have a negative value
in the range V0 < E < 0, and the lowest state (the ground state) will be described by a
wave function ψ that is spherically symmetric (thereby having no angular momentum).
Thus, taking ψ = ψ(r) and using a result from Exercise 3.10.34 to write
∇2ψ = 1
r
d2u
dr2 ,
with u(r) = rψ(r),
the Schrödinger equation reduces to an ODE that assumes the form, for r < a,
d2u1
dr2 + k2
1 u1 = 0,
with
k2
1 = 2µ
¯h2 (E −V0) > 0,
while, for r > a,
d2u2
dr2 −k2
2 u2 = 0,
with
k2
2 = −2µ E
¯h2
> 0.
The solutions for these two ranges of r must connect smoothly, meaning that both u
and du/dr must be continuous across r = a, and therefore must satisfy the matching
conditions u1(a) = u2(a), u′
1(a) = u′
2(a). In addition, the requirement that ψ be ﬁnite
at r = 0 dictates that u1(0) = 0, and the boundary condition at r = ∞requires that
limr→∞u2(r) = 0.

392
Chapter 8 Sturm-Liouville Theory
For r < a, our Schrödinger equation has the general solution
u1(r) = A sink1r + C cosk1r,
and the boundary condition at r = 0 is only met if we set C = 0. The Schrödinger equation
for r > a has the general solution
u2(r) = C′ exp(k2r) + B exp(−k2r),
(8.30)
and the boundary condition at r = ∞requires us to set C′ = 0. The matching conditions
at r = a then take the form
A sink1a = B exp(−k2a)
and
Ak1 cosk1a = −k2B exp(−k2a).
Using the second of these equations to eliminate B exp(−k2a) from the ﬁrst, we reach
A sink1a = −A k1
k2
cosk1a,
(8.31)
showing that the overall scale of the solution (i.e., A) is arbitrary, which is of course a
consequence of the fact that the Schrödinger equation is homogeneous.
Rearranging Eq. (8.31), and inserting values for k1 and k2, our matching conditions
become
tank1a = −k1
k2
,
or
tan
2µa2
¯h2 (E −V0)
1/2
= −
r
E −V0
−E
.
(8.32)
This is an admittedly unpleasant implicit equation for E; if it has solutions with E in the
range V0 < E < 0, our model predicts deuteron bound state(s).
One way to search for solutions to Eq. (8.32) is to plot its left- and right-hand sides
as a function of E, identifying the E values, if any, for which they are equal. Taking
V0 = −4.046 × 10−12 J, a = 2.5 fermi,1 µ = 0.835 × 10−27 kg, and ¯h = 1.05 × 10−34 J-s
(joule-seconds), the two sides of Eq. (8.32) are plotted in Fig. 8.3 for the range of E in
which a bound state is possible. The E values have been plotted in MeV (mega electron
volts), the energy unit most frequently used in nuclear physics (1 MeV ≈1.6 × 10−13 J).
The curves cross at only one point, indicating that the model predicts just one bound state.
Its energy is at approximately E = −2.2 MeV.
It is instructive to see what happens if we take E values that may or may not solve
Eq. (8.32), using u(r) = A sink1r for r < a (thereby satisfying the r = 0 boundary condi-
tion) but for r > a using the general form of u(r) as given in Eq. (8.30), with the coefﬁcient
values B and C′ that are required by the matching conditions for the chosen E value. Let-
ting E−and E+, respectively, denote values of E less than and greater than the eigenvalue
E, we ﬁnd that by forcing a smooth connection at r = a we lose the required asymptotic
behavior except at the eigenvalue. See Fig. 8.4.
■
11 fermi = 10−15 m.

8.3 ODE Eigenvalue Problems
393
0
LHS
RHS
LHS
–20
–25
–10
–15
0
5
E (MeV)
FIGURE 8.3
Left- and right-hand sides of Eq. (8.32) as a function of E for the model
parameters given in the text.
–0.5
0
0.5
1
u(r)
4
8
12
16
E–
E
E+
r (fermi)
FIGURE 8.4
Wavefunctions for the deuteron problem when the energy is chosen to be
less than the eigenvalue E (E−< E) or greater than E (E+ > E).
Exercises
8.3.1
Solve the Legendre equation
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0
by direct series substitution.
(a)
Verify that the indicial equation is
s(s −1) = 0.
(b)
Using s = 0 and setting the coefﬁcient a1 = 0, obtain a series of even powers of x:
yeven = a0

1 −n(n + 1)
2!
x2 + (n −2)n(n + 1)(n + 3)
4!
x4 + ···

,

394
Chapter 8 Sturm-Liouville Theory
where
a j+2 = j( j + 1) −n(n + 1)
( j + 1)( j + 2)
a j.
(c)
Using s = 1 and noting that the coefﬁcient a1 must be zero, develop a series of
odd powers of x:
yodd =a0

x −(n −1)(n + 2)
3!
x3
+ (n −3)(n −1)(n + 2)(n + 4)
5!
x5 + ···

,
where
a j+2 = ( j + 1)( j + 2) −n(n + 1)
( j + 2)( j + 3)
a j.
(d)
Show that both solutions, yeven and yodd, diverge for x = ±1 if the series continue
to inﬁnity. (Compare with Exercise 1.2.5.)
(e)
Finally, show that by an appropriate choice of n, one series at a time may be con-
verted into a polynomial, thereby avoiding the divergence catastrophe. In quantum
mechanics this restriction of n to integral values corresponds to quantization of
angular momentum.
8.3.2
Show that with the weight factor exp(−x2) and the interval −∞< x < ∞for the scalar
product, the Hermite ODE eigenvalue problem is Hermitian.
8.3.3
(a)
Develop series solutions for Hermite’s differential equation
y′′ −2xy′ + 2αy = 0.
ANS.
s(s −1) = 0, indicial equation.
For s = 0,
a j+2 = 2a j
j −α
( j + 1)( j + 2)
( j even),
yeven = a0

1 + 2(−α)x2
2!
+ 22(−α)(2 −α)x4
4!
+ ···

.
For s = 1,
a j+2 = 2a j
j + 1 −α
( j + 2)( j + 3)
( j even),
yodd = a1

x + 2(1 −α)x3
3!
+ 22(1 −α)(3 −α)x5
5!
+ ···

.
(b)
Show that both series solutions are convergent for all x, the ratio of successive
coefﬁcients behaving, for a large index, like the corresponding ratio in the expan-
sion of exp(x2).

8.4 Variation Method
395
(c)
Show that by appropriate choice of α, the series solutions may be cut off and
converted to ﬁnite polynomials. (These polynomials, properly normalized, become
the Hermite polynomials in Section 18.1.)
8.3.4
Laguerre’s ODE is
xL′′
n(x) + (1 −x)L′
n(x) + nLn(x) = 0.
Develop a series solution and select the parameter n to make your series a polynomial.
8.3.5
Solve the Chebyshev equation
(1 −x2)T ′′
n −xT ′
n + n2Tn = 0,
by series substitution. What restrictions are imposed on n if you demand that the series
solution converge for x = ±1?
ANS.
The inﬁnite series does converge for x = ±1 and no
restriction on n exists (compare with Exercise 1.2.6).
8.3.6
Solve
(1 −x2)U′′
n (x) −3xU′
n(x) + n(n + 2)Un(x) = 0,
choosing the root of the indicial equation to obtain a series of odd powers of x. Since
the series will diverge for x = 1, choose n to convert it into a polynomial.
8.4
VARIATION METHOD
We saw in Chapter 6 that the expectation value of a Hermitian operator H for the normal-
ized function ψ can be written as
⟨H⟩≡⟨ψ|H|ψ⟩,
and that the expansion of this quantity in a basis consisting of the orthonormal eigenfunc-
tions of H had the form given in Eq. (6.30):
⟨H⟩=
X
µ
|aµ|2λµ,
where aµ is the coefﬁcient of the µth eigenfunction of H and λi is the corresponding
eigenvalue. As we noted when we obtained this result, one of its consequences is that ⟨H⟩
is a weighted average of the eigenvalues of H, and therefore is at least as large as the small-
est eigenvalue, and equal to the smallest eigenvalue only if ψ is actually an eigenfunction
to which that eigenvalue corresponds.
The observations of the foregoing paragraph hold true even if we do not actually make
an expansion of ψ and even if we do not actually know or have available the eigenfunctions
or eigenvalues of H. The knowledge that ⟨H⟩is an upper limit to the smallest eigenvalue
of H is sufﬁcient to enable us to devise a method for approximating that eigenvalue and
the associated eigenfunction. This eigenfunction will be the member of the Hilbert space
of our problem that yields the smallest expectation value of H, and a strategy for ﬁnding
it is to search for the minimum in ⟨H⟩within our Hilbert space. This is the essential idea

396
Chapter 8 Sturm-Liouville Theory
behind what is known as the variation method for the approximate solution of eigenvalue
problems.
Since in many problems (including most that arise in quantum mechanics) it is imprac-
tical to compute ⟨H⟩for all members of a Hilbert space, the actual approach is to deﬁne a
portion of the Hilbert space by introducing an assumed functional form for ψ that contains
parameters, and then to minimize ⟨H⟩with respect to the parameters; this is the source
of the name “variation method.” The success of the method will depend on whether the
functional form that is chosen is capable of representing functions that are “close” to the
desired eigenfunction (meaning that its coefﬁcient in the expansion is relatively large, with
other coefﬁcients much smaller). The great advantage of the variation method is that we
do not need to know anything about the exact eigenfunction and we do not actually have
to make an expansion; we simply choose a suitable functional form and minimize ⟨H⟩.
Since eigenvalue equations for energies and related quantities in quantum mechanics
usually have ﬁnite smallest eigenvalues (e.g., ground energy levels), the variation method
is frequently applicable. We point out that it is not a method having only academic inter-
est; it is at the heart of some of the most powerful methods for solving the Schrödinger
eigenvalue equation for complex quantum systems.
Example 8.4.1
VARIATION METHOD
Given a single-electron wave function (in three-dimensional space) of the form
ψ =
ζ 3
π
1/2
e−ζr,
(8.33)
where the factor (ζ/π)3/2 makes ψ normalized, it can be shown that, in units with the
electron mass, its charge, and ¯h (Planck’s constant divided by 2π) all set to unity (so-called
Hartree atomic units), the quantum-mechanical kinetic energy operator has expectation
value ⟨ψ|T |ψ⟩= ζ 2/2, and the potential energy of interaction between the electron and a
ﬁxed nucleus of charge +Z has ⟨ψ|V |ψ⟩= −Zζ. For a one-electron atom with a nucleus
of charge +Z at r = 0, the total energy will be less than or equal to the expectation value
of the Hamiltonian H = T + V , given for the ψ of Eq. (8.33) as
⟨H⟩= ⟨T ⟩+ ⟨V ⟩= ζ 2
2 −Zζ.
(8.34)
As is customary when the meaning is clear, we no longer explicitly show ψ within all
the angle brackets. We can now optimize our upper bound to the lowest eigenvalue of H
by minimizing the expectation value ⟨H⟩with respect to the parameter ζ in ψ. To do so,
we set
d
dζ
ζ 2
2 −Zζ

= 0,
leading to ζ −Z = 0, or ζ = Z. This tells us that the wave function yielding the energy
closest to the smallest eigenvalue is that with ζ = Z, and the energy expectation value for
this value of ζ is Z2/2 −Z2 = −Z2/2.

8.4 Variation Method
397
The result we have just found is exact, because, with malice aforethought and with
appropriate knowledge, we chose a functional form that included the exact wave function.
But now let us continue to a two-electron atom, taking a wave function of the form 9 =
ψ(1)ψ(2), with both ψ of the same ζ value. For this two-electron atom, the scalar product
is deﬁned as integration over the coordinates of both electrons, and the Hamiltonian is
now H = T (1) + T (2) + V (1) + V (2) + U(1,2), where T (i) and V (i) denote the kinetic
energy and the electron-nuclear potential energy for electron i; U(1,2) is the electron-
electron repulsion energy operator, equal in Hartree units to 1/r12, where r12 is the distance
between the positions of the two electrons. For the wave function in use here, the electron-
electron repulsion has expectation value ⟨U⟩= 5ζ/8 and the expectation value ⟨H⟩(for
Z = 2, thereby representing the He atom) is
⟨H⟩= ζ 2
2 + ζ 2
2 −Zζ −Zζ + 5ζ
8 = ζ 2 −27ζ
8 .
Minimizing ⟨H⟩with respect to ζ, we obtain the optimum value ζ = 27/16, and for this
value of ζ we have ⟨H⟩= −(27/16)2 = −2.8477 hartree. This is the best approximation
available using a wave function of the form we chose. It cannot be exact, as the exact solu-
tion for this system with two interacting electrons cannot be a product of two one-electron
functions. We have therefore not included in our variational search the exact ground-state
eigenfunction. A highly precise value of the smallest eigenvalue for this problem can only
be obtained numerically, and in fact was produced by using the variation method with a
trial function containing thousands of parameters and yielding a result accurate to about
40 decimal places.2 The value found here by very simple means is higher than the exact
value, −2.9037··· hartree, by only about 2%, and already conveys much physically rele-
vant information. If the two electrons did not interact, they would each have had an opti-
mum wave function with ζ = 2; the fact that the optimum ζ is somewhat smaller shows
that each electron partially screens the nucleus from the other electron.
From the viewpoint of the mathematical method in use here, it is desirable to note that
we did not need to assume any relation between the trial wave function and the exact form
of the eigenfunction; the variational optimization adjusts the trial function to give an ener-
getically optimum ﬁt. The quality of the ﬁnal result of course depends on the degree to
which the trial function can mimic the actual eigenfunction, and trial functions are ordi-
narily chosen in a way that balances inherent quality against convenience of use.
■
Exercises
8.4.1
A function that is normalized on the interval 0 ≤x < ∞with an unweighted scalar
product is
ψ = 2α3/2xe−αx.
(a)
Verify the normalization.
(b)
Verify that for this ψ, ⟨x−1⟩= α.
2C. Schwartz, Experiment and theory in computations of the He atom ground state, Int. J. Mod. Phys. E: Nuclear Physics
15: 877 (2006).

398
Chapter 8 Sturm-Liouville Theory
(c)
Verify that for this ψ, ⟨d2/dx2⟩= −α2.
(d)
Use the variation method to ﬁnd the value of α that minimizes

ψ
−1
2
d2
dx2 −1
x
ψ

,
and ﬁnd the minimum value of this expectation value.
8.5
SUMMARY, EIGENVALUE PROBLEMS
Because any Hermitian operator on a Hilbert space can be expanded in a basis and is there-
fore mathematically equivalent to a matrix, all the properties derived for matrix eigenvalue
problems automatically apply whether or not a basis-set expansion is actually carried out.
It may be helpful to summarize some of those results, along with some that were developed
in the present chapter.
1.
A second-order differential operator is Hermitian if it is self-adjoint in the differential-
equation sense and the functions on which it operates are required to satisfy appropri-
ate boundary conditions. In that event, the scalar product consistent with Hermiticity
is an unweighted integral over the range between its boundaries.
2.
If a second-order differential operator is not self-adjoint in the differential-equation
sense, it will nevertheless be Hermitian if it satisﬁes appropriate boundary condi-
tions and if the scalar product includes the weight function that makes the original
differential equation self-adjoint.
3.
A Hermitian operator on a Hilbert space has a complete set of eigenfunctions. Thus,
they span the space and can be used as basis for an expansion.
4.
The eigenvalues of a Hermitian operator are real.
5.
The eigenfunctions of a Hermitian operator corresponding to different eigenvalues
are orthogonal, using the appropriate scalar product.
6.
Degenerate eigenfunctions of a Hermitian operator can be orthogonalized using the
Gram-Schmidt or any other orthogonalization process.
7.
Two operators have a common set of eigenfunctions if and only if they commute.
8.
An algebraic function of an operator has the same eigenfunctions as the original
operator, and its eigenvalues are the corresponding function of the eigenvalues of the
original operator.
9.
Eigenvalue problems involving a differential operator may be solved either by
expressing the problem in any basis and solving the resulting matrix problem or by
using relevant properties of the differential equation.
10.
The matrix representation of a Hermitian operator can be brought to diagonal form by
a unitary transformation. In diagonal form, the diagonal elements are the eigenvalues,
and the eigenvectors are the basis functions. The orthonormal eigenvectors are the
columns of the unitary matrix U−1 when a Hermitian matrix H is transformed to the
diagonal matrix UHU−1.

Additional Readings
399
11.
Hermitian-operator eigenvalue problems which have a ﬁnite smallest eigenvalue
may have their solutions approximated by the variation method, which is based on
the theorem that for all members of the relevant Hilbert space, the expectation value
of the operator will be larger than its smallest eigenvalue (or equal to it only if the
Hilbert space member is actually a corresponding eigenfunction).
Additional Readings
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969).
Dennery, P., and A. Krzywicki, Mathematics for Physicists. Reprinted. New York: Dover (1996).
Hirsch, M., Differential Equations, Dynamical Systems, and Linear Algebra. San Diego: Academic Press (1974).
Miller, K. S., Linear Differential Equations in the Real Domain. New York: Norton (1963).
Titchmarsh, E. C., Eigenfunction Expansions Associated with Second-Order Differential Equations, Part 1. 2nd
ed. London: Oxford University Press (1962).
Titchmarsh, E. C., Eigenfunction Expansions Associated with Second-Order Differential Equations. Part 2.
London: Oxford University Press (1958).

CHAPTER 9
PARTIAL DIFFERENTIAL
EQUATIONS
9.1
INTRODUCTION
As mentioned in Chapter 7, partial differential equations (PDEs) involve derivatives with
respect to more than one independent variable; if the independent variables are x and y,
a PDE in a dependent variable ϕ(x, y) will contain partial derivatives, with the mean-
ing discussed in Eq. (1.141). Thus, ∂ϕ/∂x implies an x derivative with y held constant,
∂2ϕ/∂x2 is the second derivative with respect to x (again keeping y constant), and we may
also have mixed derivatives
∂2ϕ
∂x∂y = ∂
∂x
∂ϕ
∂y

.
Like ordinary derivatives, partial derivatives (of any order, including mixed derivatives)
are linear operators, since they satisfy equations of the type
∂[ϕ(x, y) + bϕ(x, y)]
∂x
= a ∂ϕ(x, y)
∂x
+ b∂ϕ(x, y)
∂x
.
Similar to the situation for ODEs, general differential operators, L, which may contain
partial derivatives of any order, pure or mixed, multiplied by arbitrary functions of the
independent variables, are linear operators, and equations of the form
Lϕ(x, y) = F(x, y)
are linear PDEs. If the source term F(x, y) vanishes, the PDE is termed homogeneous;
if F(x, y) is nonzero, it is inhomogeneous.
401
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

402
Chapter 9 Partial Differential Equations
Homogeneous PDEs have the property, previously noted in other contexts, that any
linear combination of solutions will also be a solution to the PDE. This is the superposition
principle that is fundamental in electrodynamics and quantum mechanics, and which also
permits us to build speciﬁc solutions by the linear combination of suitable members of the
set of functions constituting the general solution to the homogeneous PDE.
Example 9.1.1
VARIOUS TYPES OF PDES
Laplace
∇2ψ = 0,
linear, homogeneous
Poisson
∇2ψ = f (r),
linear, inhomogeneous
Euler (inviscid ﬂow)
∂u
∂t + u · ∇u = −∇P
ρ
nonlinear, inhomogeneous
■
Since the dynamics of many physical systems involve just two derivatives, for exam-
ple, acceleration in classical mechanics, and the kinetic energy operator ∼∇2 in quantum
mechanics, differential equations of second order occur most frequently in physics. Even
when the deﬁning equations are ﬁrst order, they may, as in Maxwell’s equations, involve
two coupled unknown vector functions (they are the electric and magnetic ﬁelds), and
the elimination of one unknown vector yields a second-order PDE for the other (compare
Example 3.6.2).
Examples of PDEs
Among the most frequently encountered PDEs are the following:
1.
Laplace’s equation, ∇2ψ = 0.
This very common and very important equation occurs in studies of
(a)
electromagnetic phenomena, including electrostatics, dielectrics, steady currents,
and magnetostatics,
(b)
hydrodynamics (irrotational ﬂow of perfect ﬂuid and surface waves),
(c)
heat ﬂow,
(d)
gravitation.
2.
Poisson’s equation, ∇2ψ = −ρ/ε0.
This inhomogeneous equation describes electrostatics with a source term −ρ/ε0.
3.
Helmholtz and time-independent diffusion equations, ∇2ψ ± k2ψ = 0.
These equations appear in such diverse phenomena as
(a)
elastic waves in solids, including vibrating strings, bars, membranes,
(b)
acoustics (sound waves),
(c)
electromagnetic waves,
(d)
nuclear reactors.

9.2 First-Order Equations
403
4.
The time-dependent diffusion equation, ∇2ψ = 1
a2
∂ψ
∂t .
5.
The time-dependent classical wave equation, 1
c2
∂2ψ
∂t2 = ∇2ψ.
6.
The Klein-Gordon equation, ∂2ψ = −µ2ψ, and the corresponding vector equations in
which the scalar function ψ is replaced by a vector function. Other, more complicated
forms are also common.
7.
The time-dependent Schrödinger wave equation,
−¯h2
2m ∇2ψ + V ψ = i ¯h ∂ψ
∂t
and its time-independent form
−¯h2
2m ∇2ψ + V ψ = Eψ.
8.
The equations for elastic waves and viscous ﬂuids and the telegraphy equation.
9.
Maxwell’s coupled partial differential equations for electric and magnetic ﬁelds and
those of Dirac for relativistic electron wave functions.
We begin our study of PDEs by considering ﬁrst-order equations, which illustrate some
of the most important principles involved. We then continue to classiﬁcation and prop-
erties of second-order PDEs, and a preliminary discussion of prototypical homogeneous
equations of the different classes. Finally, we examine a very useful and powerful method
for obtaining solutions to homogeneous PDEs, namely the method of separation of
variables.
This chapter is mainly devoted to general properties of homogeneous PDEs; full detail
on speciﬁc equations is for the most part postponed to chapters that discuss the spe-
cial functions involved. Questions arising from the extension to inhomogeneous PDEs
(i.e., problems involving sources or driving terms) are also deferred, mainly to later chap-
ters on Green’s functions and integral transforms.
Occasionally, we encounter equations of higher order. In both the theory of the slow
motion of a viscous ﬂuid and the theory of an elastic body we ﬁnd the equation
(∇2)2ψ = 0.
Fortunately, these higher-order differential equations are relatively rare and are not dis-
cussed here. Sometimes, particularly in ﬂuid mechanics, we encounter nonlinear PDEs.
9.2
FIRST-ORDER EQUATIONS
While the most important PDEs arising in physics are linear and second order, many
involving three spatial variables plus possibly a time variable, ﬁrst-order PDEs do arise
(e.g., the Cauchy-Riemann equations of complex variable theory). Part of the motivation
for studying these easily solved equations is that the study provides insights that apply also
to higher-order problems.

404
Chapter 9 Partial Differential Equations
Characteristics
Let us start by considering the following homogeneous linear ﬁrst-order equation in two
independent variables x and y, with constant coefﬁcients a and b, and with dependent
variable ϕ(x, y):
Lϕ = a ∂ϕ
∂x + b∂ϕ
∂y = 0.
(9.1)
This equation would be easier to solve if we could rearrange it so that it contained only one
derivative; one way to do this is would be to rewrite our PDE in terms of new coordinates
(s,t) such that one of them, say s, is such that (∂/∂s)t would expand into the linear combi-
nation of ∂/∂x and ∂/∂y in the original PDE, while the other new coordinate, t, is such that
(∂/∂t)s does not occur in the PDE. It is easily veriﬁed that deﬁnitions of s and t consistent
with these objectives for the PDE in Eq. (9.1) are s = ax + by and t = bx −ay. To check
this, write ϕ(x, y) = ϕ(x(s,t), y(s,t)) = ˆϕ(s,t), and we can verify that
∂ϕ
∂x

y
= a
∂ϕ
∂s

t
+ b
∂ϕ
∂t

s
and
∂ϕ
∂y

x
= b
∂ϕ
∂s

t
−a
∂ϕ
∂t

s
,
so
a ∂ϕ
∂x + b∂ϕ
∂y = (a2 + b2)∂ˆϕ
∂s .
We see that the PDE does not contain a derivative with respect to t. Since our PDE now
has the simple form
(a2 + b2)∂ˆϕ
∂s = 0,
it clearly has solution
ˆϕ(s,t) = f (t),
with f (t) completely arbitrary.
(9.2)
In terms of the original variables,
ϕ(x, y) = f (bx −ay),
(9.3)
where we again stress that f (t) is an arbitrary function of its argument.
Checking our work to this point, we note that
Lϕ = a ∂f (bx −ay)
∂x
+ b∂f (bx −ay)
∂y
= abf ′(bx −ay) + b

−af ′(bx −ay)

= 0.
Since the satisfaction of this equation does not depend on the properties of the function f ,
we verify that ϕ(x, y) as given in Eq. (9.3) is a solution of our PDE, irrespective of the
choice of the function f . In fact, it is the general solution of our PDE.
It is useful to visualize the signiﬁcance of what we have just observed. Note that holding
t = bx −ay to a ﬁxed value deﬁnes a line in the xy plane on which our solution ϕ is con-
stant, with individual points on this line corresponding to different values of s = ax + by.
In addition, we observe that the lines of constant s are orthogonal to those of constant t, and
that s has the same coefﬁcients as the derivatives in the PDE. The general solution to our
PDE can thus be characterized as independent of s and with arbitrary dependence on t.

9.2 First-Order Equations
405
The curves of constant t are called characteristic curves, or more frequently just char-
acteristics of our PDE. An alternative and insightful way of describing the characteristic
curves is to observe that they are the stream lines (ﬂow lines) of s. Put another way, they
are the lines that are traced out as the value of s is changed, keeping t constant. The char-
acteristic can also be characterized by its slope,
dy
dx = b
a ,
for L in Eq. (9.1).
(9.4)
For our present ﬁrst-order PDE, the solution ϕ is constant along each characteristic. We
shall shortly see that more general PDEs can be solved using ODE methods on charac-
teristic lines, a feature that causes it to be said that PDE solutions propagate along the
characteristics, giving further signiﬁcance to the notion that in some sense these are lines
of ﬂow. In the present problem this translates into the statement that if we know ϕ at any
point on a characteristic, we know it on the entire characteristic line.
The characteristics have one additional (but related) property of importance. Ordinarily,
if a PDE solution ϕ(x, y) is speciﬁed on a curve segment (a boundary condition), one
can deduce from it the values of the solution at nearby points that are not on the curve. If
one introduces a Taylor expansion about some point (x0, y0) on the curve (thereby tacitly
assuming that there are no singularities that invalidate the expansion), the value of ϕ at a
nearby point (x, y) will be given by
ϕ(x, y) = ϕ(x0, y0) + ∂ϕ(x0, y0)
∂x
(x −x0) + ∂ϕ(x0, y0)
∂y
(y −y0) + ··· .
(9.5)
To use Eq. (9.5), we need values of the derivatives of ϕ. To obtain these derivatives, note
the following:
•
The speciﬁcation of ϕ on a given curve, with the curve parametrically described by
x(l), y(l), means that the curve direction, i.e., dx/dl and dy/dl, is known, as is the
derivative of ϕ along the curve, namely
dϕ
dl = ∂ϕ
∂x
dx
dl + ∂ϕ
∂y
dy
dl .
(9.6)
Equation (9.6) therefore provides us with a linear equation satisﬁed by the two deriva-
tives ∂ϕ/∂x and ∂ϕ/∂y.
•
The PDE supplies a second linear equation, in this case
a ∂ϕ
∂x + b∂ϕ
∂y = 0.
(9.7)
•
Providing that the determinant of their coefﬁcients is not zero, we can solve Eqs. (9.6)
and (9.7) for ∂ϕ/∂x and ∂ϕ/∂y at (x0, y0) and therefore evaluate the leading terms of
the Taylor series for ϕ(x, y).1 The determinant of coefﬁcients of Eqs. (9.6) and (9.7)
takes the form
D =

dx
dl
dy
dl
a
b

= bdx
dl −a dy
dl .
1The linear terms are all that are necessary; one can choose x and y close enough to (x0, y0) that second- and higher-order terms
can be made negligible relative to those retained.

406
Chapter 9 Partial Differential Equations
Now we make the observation that if ϕ was speciﬁed along a characteristic (for which
t = bx −ay =constant), we have
b dx −a dy = 0,
or
bdx
dl −a dy
dl = 0,
so that D = 0 and we cannot solve for the derivatives of ϕ. Our conclusions relative to
characteristics, which can be extended to more general equations, are:
1.
If the dependent variable ϕ of the PDE in Eq. (9.1) is speciﬁed along a curve (i.e., ϕ
has a boundary condition speciﬁed on a boundary curve), this ﬁxes the value of ϕ
at a point of each characteristic that intersects the boundary curve, and hence at all
points of each such characteristic;
2.
If the boundary curve is along a characteristic, the boundary condition on it will ordi-
narily lead to inconsistency, and therefore, unless the boundary condition is redundant
(i.e., coincidentally equal everywhere to the solution constructed from the value of ϕ
at any one point on the characteristic), the PDE will not have a solution;
3.
If the boundary curve has more than one intersection with the same characteristic,
this will usually lead to an inconsistency, as the PDE may not have a solution that is
simultaneously consistent with the values of ϕ at both intersections; and
4.
Only if the boundary curve is not a characteristic can a boundary condition ﬁx the
value of ϕ at points not on the curve. Values of ϕ speciﬁed only on a character-
istic of the PDE provide no information as to the value of ϕ at points not on that
characteristic.
In the above example, the argument t of the arbitrary function f was a linear combina-
tion of x and y, which worked because the coefﬁcients of the derivatives in the PDE were
constants. If these coefﬁcients were more general functions of x and y, the foregoing type
of analysis could still be carried out, but the form of t would have to be different. This
more complicated case is illustrated in Exercises 9.2.5 and 9.2.6.
More General PDEs
Consider now a ﬁrst-order PDE of a form more general than Eq. (9.1),
Lϕ = a ∂ϕ
∂x + b∂ϕ
∂y + q(x, y)ϕ = F(x, y).
(9.8)
We may identify its characteristic curves just as before, which amounts to making a trans-
formation to new variables s = ax +by, t = bx −ay, in terms of which our PDE becomes,
compare Eq. (9.5),
(a2 + b2)
∂ϕ
∂s

+ ˆq(s,t) ˆϕ = ˆF(s,t).
(9.9)
Here ˆq(s,t) is obtained by converting q(x, y) to the new coordinates:
ˆq(s,t) = q
as + bt
a2 + b2 , bs −at
a2 + b2

,

9.2 First-Order Equations
407
and ˆF is related in a similar fashion to F. Equation (9.9) is really an ODE in s (containing
what can be viewed as a parameter, t), and its general solution can be obtained by the usual
procedures for solving ODEs.
Example 9.2.1
ANOTHER FIRST-ORDER PDE
Consider the PDE
∂ϕ
∂x + ∂ϕ
∂y + (x + y)ϕ = 0.
Applying a transformation to the characteristic direction t = x −y and the direction
orthogonal thereto s = x + y, our PDE becomes
2∂ϕ
∂s + sϕ = 0.
This equation separates into
2dϕ
ϕ + s ds = 0,
with general solution
lnϕ = −s2
4 + C(t),
or
ϕ = e−s2/4 f (t),
where f (t), originally exp[C(t)], is completely arbitrary. One can simplify the result
slightly by noting that s2/4 = t2/4 + xy; then exp(−t2/4) can be absorbed into f (t),
leaving the compact result (in terms of x and y)
ϕ(x, y) = e−xy f (x −y),
( f arbitrary).
■
More Than Two Independent Variables
It is useful to consider how the concept of characteristic can be generalized to PDEs with
more than two independent variables. Given the three-dimensional (3-D) differential form
a ∂ϕ
∂x + b∂ϕ
∂y + c∂ϕ
∂z ,
we apply a transformation to convert our PDE to the new variables s = ax + by + cz,
t = α1x + α2y + α3z, u = β1x + β2y + β3z, with αi and βi such that (s,t,u) form an
orthogonal coordinate system. Then our 3-D differential form is found equivalent to
(a2 + b2 + c2)∂ϕ
∂s ,
and the stream lines of s (those with t and u constant) are our characteristics, along which
we can propagate a solution ϕ by solving an ODE. Each characteristic can be identiﬁed by

408
Chapter 9 Partial Differential Equations
its ﬁxed values of t and u. For the 3-D analog of Eq. (9.1),
a ∂ϕ
∂x + b∂ϕ
∂y + c∂ϕ
∂z = 0,
(9.10)
we have
(a2 + b2 + c2)∂ϕ
∂s = 0,
with solution ϕ = f (t,u), with f a completely arbitrary function of its two arguments.
Consider next an attempt to solve our 3-D PDE subject to a boundary condition ﬁxing
the values of the PDE solution ϕ on a surface. If the characteristic through a point on
the surface lies in the surface, we have a potential inconsistency between the boundary
condition and the solution propagated along the characteristic. We are then also unable to
extend ϕ away from the boundary surface because the data on the surface is insufﬁcient
to yield values of the derivatives that are needed for a Taylor expansion. To see this, note
that the derivatives ∂ϕ/∂x, ∂ϕ/∂y, and ∂ϕ/∂z can only be determined if we can ﬁnd
two directions (parametrically designated l and l′) such that we can solve simultaneously
Eq. (9.10) and
∂ϕ
∂l = ∂ϕ
∂x
dx
dl + ∂ϕ
∂y
dy
dl + ∂ϕ
∂z
dz
dl ,
∂ϕ
∂l′ = ∂ϕ
∂x
dx
dl′ + ∂ϕ
∂y
dy
dl′ + ∂ϕ
∂z
dz
dl′ .
A solution can be obtained only if
D =

dx
dl
dy
dl
dz
dl
dx
dl′
dy
dl′
dz
dl′
a
b
c

̸= 0.
If a characteristic, with dx/dl′′ = a, dy/dl′′ = b, and dz/dl′′ = c, lies in the two-
dimensional (2-D) surface, there will only be one further linearly independent direction
l, and D will necessarily be zero.
Summarizing, our earlier observations extend to the 3-D case:
A boundary condition is effective in determining a unique solution to a ﬁrst-order PDE
only if the boundary does not include a characteristic, and inconsistencies may arise if
a characteristic intersects a boundary more than once.
Exercises
Find the general solutions of the PDEs in Exercises 9.2.1 to 9.2.4.
9.2.1
∂ψ
∂x + 2∂ψ
∂y + (2x −y)ψ = 0.
9.2.2
∂ψ
∂x −2∂ψ
∂y + x + y = 0.

9.3 Second-Order Equations
409
9.2.3
∂ψ
∂x + ∂ψ
∂y = ∂ψ
∂z .
9.2.4
∂ψ
∂x + ∂ψ
∂y + ∂ψ
∂z = x −y.
9.2.5
(a)
Show that the PDE
y ∂ψ
∂x + x ∂ψ
∂y = 0
can be transformed into a readily soluble form by writing it in the new variables
u = xy, v = x2 −y2, and ﬁnd its general solution.
(b)
Discuss this result in terms of characteristics.
9.2.6
Find the general solution to the PDE
x ∂ψ
∂x −y ∂ψ
∂y = 0.
Hint. The solution to Exercise 9.2.5 may provide a suggestion as to how to proceed.
9.3
SECOND-ORDER EQUATIONS
Classes of PDEs
We consider here extending the notion of characteristics to second-order PDEs. This can
sometimes be done in a useful fashion. As a preliminary example, consider the following
homogeneous second-order equation
a2 ∂2ϕ(x, y)
∂x2
−c2 ∂2ϕ(x, y)
∂y2
= 0,
(9.11)
where a and c are assumed to be real. This equation can be written in the factored form

a ∂
∂x + c ∂
∂y

a ∂
∂x −c ∂
∂y

ϕ = 0,
(9.12)
and, since the two operator factors commute, we see that Eq. (9.12) will be satisﬁed if ϕ is
a solution to either of the ﬁrst-order equations
a ∂ϕ
∂x + c∂ϕ
∂y = 0
or
a ∂ϕ
∂x −c∂ϕ
∂y = 0.
(9.13)
However, these ﬁrst-order equations are of just the type discussed in the preceding subsec-
tion, so we can identify their respective general solutions as
ϕ1(x, y) = f (cx −ay),
ϕ2(x, y) = g(cx + ay),
(9.14)
where f and g are arbitrary (and totally unrelated) functions. Moreover, we can iden-
tify the stream lines of ax + cy and ax −cy as characteristics, with implications as to
the effectiveness and possible consistency of boundary conditions. For some PDEs with

410
Chapter 9 Partial Differential Equations
second derivatives as given in Eq. (9.11), it will also be practical to propagate solutions
along the characteristics.
Look next at the superﬁcially similar equation
a2 ∂2ϕ(x, y)
∂x2
+ c2 ∂2ϕ(x, y)
∂y2
= 0,
(9.15)
with a and c again assumed to be real. If we factor this, we get

a ∂
∂x + ic ∂
∂y

a ∂
∂x −ic ∂
∂y

ϕ = 0.
(9.16)
This factorization is of less practical value, as it leads to complex characteristics, which
do not have an obvious relevance to boundary conditions. In addition, propagation along
such characteristics does not provide a solution to the PDE for physically relevant (i.e.,
real) coordinate values.
It is customary to identify second-order PDEs as hyperbolic if they are of (or can be
transformed into) the form given in Eq. (9.11), with real values of a and c. PDEs that are of
(or can be transformed into) the form given in Eq. (9.15) are called elliptic. The designation
is useful because it correlates with the existence (or nonexistence) of real characteristics,
and therefore with the behavior of the PDE relative to boundary conditions, with further
implications as to convenient methods for solving the PDE. The terms elliptic and hyper-
bolic have been introduced based on an analogy to quadratic forms, where a2x2 +c2y2 = d
is the equation of an ellipse, while a2x2 −c2y2 = d is that of a hyperbola.
More general PDEs will have second derivatives of the differential form
L = a ∂2ϕ
∂x2 + 2b ∂2ϕ
∂x∂y + c∂2ϕ
∂y2 .
(9.17)
The form in Eq. (9.17) has the following factorization:
L =
 
b +
√
b2 −ac
c1/2
∂
∂x + c1/2 ∂
∂y
! 
b −
√
b2 −ac
c1/2
∂
∂x + c1/2 ∂
∂y
!
.
(9.18)
Equation (9.18) is easily veriﬁed by expanding the product. The equation also shows
that the characteristics of Eq. (9.17) are real if and only if b2 −ac ≥0. This quantity
is well known from elementary algebra, being the discriminant of the quadratic form
at2 + 2bt + c. If b2 −ac > 0, the two factors identify two linearly independent real charac-
teristics, as were found for the prototype hyperbolic PDE discussed in Eqs. (9.11) to (9.14).
If b2 −ac < 0, the characteristics will, as for the prototype elliptic PDE in Eqs. (9.15)
and (9.16), form a complex conjugate pair. We now have, however, one new possibility:
If b2 −ac = 0 (a case that for quadratic forms is that of a parabola), we have a PDE that
has exactly one linearly independent characteristic; such PDEs are termed parabolic, and
the canonical form adopted for them is
a ∂ϕ
∂x = ∂2ϕ
∂y2 .
(9.19)
If the original PDE lacked a ∂/∂x term, it would in effect be an ODE in y that depends
on x only parametrically and need not be considered further in the context of methods for
PDEs.

9.3 Second-Order Equations
411
To complete our discussion of the second-order form in Eq. (9.17), we need to show
that it can be transformed into the canonical form for the PDE of its classiﬁcation. For this
purpose we consider the transformation to new variables ξ, η, deﬁned as
ξ = c1/2x −c−1/2by,
η = c−1/2y.
(9.20)
By systematic application of the chain rule to evaluate ∂2/∂x2, ∂2/∂x∂y, and ∂2/∂y2, it
can be shown that
L = (ac −b2)∂2ϕ
∂ξ2 + ∂2ϕ
∂η2 .
(9.21)
Veriﬁcation of Eq. (9.21) is the subject of Exercise 9.3.1.
Equation (9.21) shows that the classiﬁcation of our PDE remains invariant under trans-
formation, and is hyperbolic if b2 −ac > 0, elliptic if b2 −ac < 0, and parabolic if
b2 −ac = 0. Perhaps better seen from Eq. (9.18), we see that the stream lines of the char-
acteristics have slope
dy
dx =
c
b ±
√
b2 −ac
.
(9.22)
More than Two Independent Variables
While we will not carry out a full analysis, it is important to note that many problems
in physics involve more than two dimensions (often, three spatial dimensions or several
spatial dimensions plus time). Often, the behavior in the multiple spatial dimensions is
similar, and we apply the terms hyperbolic, elliptic, and parabolic in a way that relates the
spatial to the time derivatives when the latter occur. Thus, these equations are classiﬁed as
indicated:
Laplace equation
∇2ψ = 0
elliptic
Poisson equation
∇2ψ = −ρ/ε0
elliptic
Wave equation
∇2ψ = 1
c2
∂2ψ
∂t2
hyperbolic
Diffusion equation
a ∂ψ
∂t = ∇2ψ
parabolic
The speciﬁc equations mentioned here are very important in physics and will be further
discussed in later sections of this chapter. These examples, of course, do not represent the
full range of second-order PDEs, and do not include cases where the coefﬁcients in the
differential operator are functions of the coordinates. In that case, the classiﬁcation into
elliptic, hyperbolic, and parabolic is only local; the class may change as the coordinates
vary.
Boundary Conditions
Usually, when we know a physical system at some time and the law governing the phys-
ical process, then we are able to predict the subsequent development. Such initial val-
ues are the most common boundary conditions associated with ODEs and PDEs. Finding

412
Chapter 9 Partial Differential Equations
solutions that match given points, curves, or surfaces corresponds to boundary value prob-
lems. Solutions usually are required to satisfy certain imposed (for example, asymptotic)
boundary conditions. These boundary conditions ordinarily take one of three forms:
1.
Cauchy boundary conditions. The value of a function and normal derivative speci-
ﬁed on the boundary. In electrostatics this would mean ϕ, the potential, and En, the
normal component of the electric ﬁeld.
2.
Dirichlet boundary conditions. The value of a function speciﬁed on the boundary.
In electrostatics, this would mean the potential ϕ.
3.
Neumann boundary conditions. The normal derivative (normal gradient) of a func-
tion speciﬁed on the boundary. In the electrostatic case this would be En and therefore
σ, the surface charge density.
Because the three classes of second-order PDEs have different patterns of character-
istics, the boundary conditions needed to specify (in a consistent way) a unique solution
will depend on the equation class. An exact analysis of the role of boundary conditions is
complicated and beyond the scope of the present text. However, a summary of the relation
of these three types of boundary conditions to the three classes of 2-D partial differential
equations is given in Table 9.1. For a more extended discussion of these partial differ-
ential equations the reader may consult Morse and Feshbach, Chapter 6 (see Additional
Readings).
Parts of Table 9.1 are simply a matter of maintaining internal consistency or of common
sense. For instance, for Poisson’s equation with a closed surface, Dirichlet conditions lead
Table 9.1
Relation between PDE and Boundary Conditions
Boundary
Class of Partial Differential Equation
Conditions
Elliptic
Hyperbolic
Parabolic
Laplace, Poisson
Wave equation in
Diffusion equation
in (x, y)
(x,t)
in (x,t)
Cauchy
Open surface
Unphysical results
Unique, stable
Too restrictive
(instability)
solution
Closed surface
Too restrictive
Too restrictive
Too restrictive
Dirichlet
Open surface
Insufﬁcient
Insufﬁcient
Unique, stable
solution in one
direction
Closed surface
Unique, stable
Solution not unique
Too restrictive
solution
Neumann
Open surface
Insufﬁcient
Insufﬁcient
Unique, stable
solution in one
direction
Closed surface
Unique, stable
Solution not unique
Too restrictive
solution

9.3 Second-Order Equations
413
to a unique, stable solution. Neumann conditions, independent of the Dirichlet conditions,
likewise lead to a unique stable solution independent of the Dirichlet solution. There-
fore, Cauchy boundary conditions (meaning Dirichlet plus Neumann) could lead to an
inconsistency.
The term boundary conditions includes as a special case the concept of initial condi-
tions. For instance, specifying the initial position x0 and the initial velocity v0 in some
dynamical problem would correspond to the Cauchy boundary conditions. Note, how-
ever, that an initial condition corresponds to applying the condition at only one end of
the allowed range of the (time) variable.
Finally, we note that Table 9.1 oversimpliﬁes the situation in various ways. For example,
the Helmholtz PDE,
∇2ψ ± k2ψ = 0,
(which could be thought of as the reduction of a parabolic time-dependent equation to its
spatial part) has solution(s) for Dirichlet conditions on a closed boundary only for certain
values of its parameter k. The determination of k and the characterization of these solutions
is an eigenvalue problem and is important for physics.
Nonlinear PDEs
Nonlinear ODEs and PDEs are a rapidly growing and important ﬁeld. We encountered
earlier the simplest linear wave equation,
∂ψ
∂t + c∂ψ
∂x = 0,
as the ﬁrst-order PDE of the wavefronts of the wave equation. The simplest nonlinear wave
equation,
∂ψ
∂t + c(ψ)∂ψ
∂x = 0,
(9.23)
results if the local speed of propagation, c, is not constant but depends on the wave ψ.
When a nonlinear equation has a solution of the form ψ(x,t) = A cos(kx −ωt), where
ω(k) varies with k so that ω′′(k) ̸= 0, then it is called dispersive. Perhaps the best-known
nonlinear dispersive equation is the Korteweg-deVries equation,
∂ψ
∂t + ψ ∂ψ
∂x + ∂3ψ
∂x3 = 0,
(9.24)
which models the lossless propagation of shallow water waves and other phenomena. It is
widely known for its soliton solutions. A soliton is a traveling wave with the property of
persisting through an interaction with another soliton: After they pass through each other,
they emerge in the same shape and with the same velocity and acquire no more than a
phase shift. Let ψ(ξ = x −ct) be such a traveling wave. When substituted into Eq. (9.24)
this yields the nonlinear ODE
(ψ −c)dψ
dξ + d3ψ
dξ3 = 0,
(9.25)

414
Chapter 9 Partial Differential Equations
which can be integrated to yield
d2ψ
dξ2 = cψ −ψ2
2 .
(9.26)
There is no additive integration constant in Eq. (9.26), because the solution must be such
that d2ψ/dξ2 →0 with ψ →0 for large ξ. This causes ψ to be localized at the character-
istic ξ = 0, or x = ct. Multiplying Eq. (9.26) by dψ/dξ and integrating again yields
dψ
dξ
2
= cψ2 −ψ3
3 ,
(9.27)
where dψ/dξ →0 for large ξ. Taking the root of Eq. (9.27) and integrating again yields
the soliton solution
ψ(x −ct) =
3c
cosh2   1
2
√c(x −ct)
.
(9.28)
Exercises
9.3.1
Show that by making a change of variables to ξ = c1/2x −c−1/2by, η = c−1/2y, the
operator L of Eq. (9.18) can be brought to the form
L = (ac −b2) ∂2
∂ξ2 + ∂2
∂η2 .
9.4
SEPARATION OF VARIABLES
Partial differential equations are clearly important in physics, as evidenced by the PDEs
listed in Section 9.1, and of equal importance is the development of methods for their
solution. Our discussion of characteristics has suggested an approach that will be useful
for some problems. Other general techniques for solving PDEs can be found, for example,
in the books by Bateman and by Gustafson listed in the Additional Readings at the end of
this chapter. However, the technique described in the present section is probably that most
widely used.
The method developed in this section for solution of a PDE splits a partial differential
equation of n variables into n ordinary differential equations, with the intent that an overall
solution to the PDE will be a product of single-variable functions which are solutions to
the individual ODEs. In problems amenable to this method, the boundary conditions are
usually such that they separate at least partially into conditions that can be applied to the
separate ODEs.
Further discussion of the method depends on the nature of the problem we seek to solve,
so we now make the observation that PDEs occur in physics in two contexts, either as
•
An equation with no unknown parameters for which there is expected to be a unique
solution consistent with the boundary conditions (typical example: Laplace equation
for the electrostatic potential with the potential speciﬁed on the boundary), or

9.4 Separation of Variables
415
•
An eigenvalue problem which will have solutions consistent with the boundary con-
ditions only for certain values of an embedded but initially unknown parameter (the
eigenvalue).
In the ﬁrst of these two cases, the unique solution is typically approached by ﬁrst applying
boundary conditions to the separate ODEs to specialize their solutions as much as possible.
The solution is at this point normally not unique, and we have a (usually inﬁnite) number
of product solutions that satisfy the boundary conditions thus far applied. We then regard
these product solutions as a basis that can be used to form an expansion that satisﬁes the
remaining boundary condition(s). We illustrate with the ﬁrst and fourth examples of this
section.
In the second case identiﬁed above, we typically have homogeneous boundary condi-
tions (solution equal to zero on the boundary), and in favorable situations can satisfy all
the boundary conditions by imposing them on the separate ODEs. At this point we usually
ﬁnd that each product solves our PDE with a different value of its embedded parameter,
so that we are obtaining eigenfunctions and eigenvalues. This process is illustrated in the
second and third examples of the present section.
The method of separation of variables proceeds by dividing the PDE into pieces each
of which can be set equal to a constant of separation. If our PDE has n independent
variables, there will be n −1 independent separation constants (though we often prefer
a more symmetric formulation with n separation constants plus an equation connecting
them). The separation constants may have values that are restricted by invoking boundary
conditions.
To get a broad understanding of the method of separation of variables, it is useful to see
how it is carried out in a variety of coordinate systems. Here we examine the process in
Cartesian, cylindrical, and spherical polar coordinates. For application to other coordinate
systems we refer the reader to the second edition of this text.
Cartesian Coordinates
In Cartesian coordinates the Helmholtz equation becomes
∂2ψ
∂x2 + ∂2ψ
∂y2 + ∂2ψ
∂z2 + k2ψ = 0,
(9.29)
using Eq. (3.62) for the Laplacian. For the present, let k2 be a constant. As stated in the
introductory paragraphs of this section, our strategy will be to split Eq. (9.29) into a set of
ordinary differential equations. To do so, let
ψ(x, y, z) = X(x)Y(y)Z(z)
(9.30)
and substitute back into Eq. (9.29). How do we know Eq. (9.30) is valid? When the dif-
ferential operators in various variables are additive in the PDE, that is, when there are no
products of differential operators in different variables, the separation method has a chance
to succeed. For success, it is usually also necessary that at least some of the boundary con-
ditions separate into conditions on the separate factors. At any rate, we are proceeding in
the spirit of let’s try and see if it works. If our attempt succeeds, then Eq. (9.30) will be

416
Chapter 9 Partial Differential Equations
justiﬁed. If it does not succeed, we shall ﬁnd out soon enough and then we can try another
attack, such as Green’s functions, integral transforms, or brute-force numerical analysis.
With ψ assumed given by Eq. (9.30), Eq. (9.29) becomes
Y Z d2X
dx2 + X Z d2Y
dy2 + XY d2Z
dz2 + k2XY Z = 0.
(9.31)
Dividing by ψ = XY Z and rearranging terms, we obtain
1
X
d2X
dx2 = −k2 −1
Y
d2Y
dy2 −1
Z
d2Z
dz2 .
(9.32)
Equation (9.32) exhibits one separation of variables. The left-hand side is a function of
x alone, whereas the right-hand side depends only on y and z and not on x. But x, y,
and z are all independent coordinates. The equality of two sides that depend on different
variables can only be attained if each side must be equal to the same constant, a constant
of separation. We choose2
1
X
d2X
dx2 = −l2,
(9.33)
−k2 −1
Y
d2Y
dy2 −1
Z
d2Z
dz2 = −l2.
(9.34)
Now, turning our attention to Eq. (9.34), we obtain
1
Y
d2Y
dy2 = −k2 + l2 −1
Z
d2Z
dz2 ,
(9.35)
and a second separation has been achieved. Here we have a function of y equated to a
function of z. We resolve it, as before, by equating each side to another constant of sepa-
ration, −m2,
1
Y
d2Y
dy2 = −m2,
(9.36)
−k2 + l2 −1
Z
d2Z
dz2 = −m2.
(9.37)
The separation is now complete, but to make the formulation more symmetrical, we will set
1
Z
d2Z
dz2 = −n2,
(9.38)
and then consistency with Eq. (9.37) leads to the condition
l2 + m2 + n2 = k2.
(9.39)
Now we have three ODEs, Eqs. (9.33), (9.36), and (9.38), to replace Eq. (9.29). Our
assumption, Eq. (9.30), has succeeded in splitting the PDE; if we can also use the fac-
tored form to satisfy the boundary conditions, our solution of the PDE will be complete.
2The choice of sign for separation constants is completely arbitrary, and will be ﬁxed in speciﬁc problems by the need to satisfy
speciﬁc boundary conditions, and particularly to avoid the unnecessary introduction of complex numbers.

9.4 Separation of Variables
417
It is convenient to label the solution according to the choice of our constants l,m, and n;
that is,
ψlmn(x, y, z) = Xl(x)Ym(y)Zn(z).
(9.40)
Subject to the boundary conditions of the problem being solved and to the condition
k2 = l2 + m2 + n2, we may choose l, m, and n as we like, and Eq. (9.40) will still be a
solution of Eq. (9.29), provided only that Xl(x) is a solution of Eq. (9.33), and so on.
Because our original PDE is homogeneous and linear, we may develop the most general
solution of Eq. (9.29) by taking a linear combination of solutions ψlmn,
9 =
X
l,m
almψlmn,
(9.41)
where it is understood that n will be given a value consistent with Eq. (9.39) and with the
values of l and m.
Finally, the constant coefﬁcients alm must be chosen to permit 9 to satisfy the boundary
conditions of the problem, leading usually to a discrete set of values l,m.
Reviewing what we have done, it can be seen that the separation into ODEs could still
have been achieved if k2 were replaced by any function that depended additively on the
variables, i.e., if
k2 −→f (x) + g(y) + h(z).
A case of practical importance would be the choice k2 −→C(x2 + y2 + z2), leading to
the problem of a 3-D quantum harmonic oscillator. Replacing the constant term k2 by
a separable function of the variables will, of course, change the ODEs we obtain in the
separation process and may have implications relative to the boundary conditions.
Example 9.4.1
LAPLACE EQUATION FOR A PARALLELEPIPED
As a concrete example we take Eq. (9.29) with k = 0, which makes it a Laplace equation,
and ask for its solution in a parallelepiped deﬁned by the planar surfaces x = 0, x = c,
y = 0, y = c, z = 0, z = L, with the Dirichlet boundary condition ψ = 0 on all the bound-
aries except that at z = L; on that boundary ψ is given the constant value V. See Fig. 9.1.
This is a problem in which the PDE contains no unknown parameters and should have a
unique solution.
We expect a solution of the generic form given by Eq. (9.41), with ψlmn given by
Eq. (9.40). To proceed further, we need to develop the actual functional forms of X(x),
Y(y), and Z(z). For X and Y , the ODEs, written in conventional form, are
X′′ = −l2X,
Y ′′ = −m2Y,
with general solutions
X = A sinlx + B coslx,
Y = A′ sinmy + B′ cosmy.
We could have written X and Y as complex exponentials, but that choice would be less
convenient when we consider the boundary conditions. To satisfy the boundary condition
at x = 0, we set X(0) = 0, which can be accomplished by choosing B = 0; to satisfy

418
Chapter 9 Partial Differential Equations
x
c
c
L
y
z
FIGURE 9.1
Parallelepiped for solution of Laplace equation.
the boundary condition at x = c, we set X(c) = 0, which causes us to choose l such that
lc = λπ, where λ must be a nonzero integer. Without loss of generality, we can restrict λ to
positive values, as −X and X are linearly dependent. Moreover, we can include whatever
scale factor is ultimately needed in our solution for Z(z), so we may set A = 1. Similar
remarks apply to the solution Y(y), so our solutions for X and Y take the ﬁnal form
Xλ(x) = sin
λπx
c

,
Yµ(y) = sin
µπy
c

,
(9.42)
with λ = 1,2,3,... and µ = 1,2,3,....
Next we consider the ODE for Z. It must be solved with a value of n2, calculated from
Eq. (9.39) with k = 0 as
n2 = −π2
c2 (λ2 + µ2).
This equation suggests that n will be imaginary, but that is unimportant here. Returning to
the ODE for Z, we now see that it becomes
Z′′ = +π2
c2 (λ2 + µ2)Z,
and the general solution for Z(z) for given λ and µ is then easily identiﬁed as
Zλµ(z) = A eρλµz + B e−ρλµz,
with ρλµ = π
c
p
λ2 + µ2.
(9.43)
We now specialize Eq. (9.43) in a way that makes Zλµ(0) = 0 and Zλµ(L) = V. Noting
that sinh(ρλµz) is a linear combination of eρλµz and e−ρλµz, we write
Zλµ(z) = V sinh(ρλµz)
sinh(ρλµL).
(9.44)
At this point, we have made choices that cause all the boundary conditions to be satisﬁed
except that at z = L, and we are now ready to select the coefﬁcients aλµ as required by the

9.4 Separation of Variables
419
remaining boundary condition, which because of Eq. (9.44) corresponds to
1
V 9(x, y, L) =
X
λµ
aλµ sin
λπx
c

sin
µπy
c

= 1.
(9.45)
The symmetry of this expression suggests that we write aλµ = bλbµ, and ﬁnd the coefﬁ-
cients bλ from the equation
X
λ
bλ sin
λπx
c

= 1.
(9.46)
Because the sine functions in Eq. (9.46) are the eigenfunctions of the one-dimensional
(1-D) equation for X, which is a Hermitian eigenproblem, they form an orthogonal set on
the interval (0,c), so the bλ can be computed by the following formulas:
bλ =
*
sin
λπx
c
1
+
*
sin
λπx
c
 sin
λπx
c
+ =
c
Z
0
sin(λπx/c)dx
c
Z
0
sin2(λπx/c)dx
= 4
λπ ,
λ odd,
= 0,
λ even,
and our complete solution for the potential in the parallelepiped becomes
9(x, y, z) = V
X
λµ
bλbµ sin
λπx
c

sin
µπy
c
 sinh(ρλµz)
sinh(ρλµL).
(9.47)
■
As brieﬂy mentioned earlier, PDEs also occur as eigenvalue problems. Here is a simple
example.
Example 9.4.2
QUANTUM PARTICLE IN A BOX
We consider a particle of mass m trapped in a box with planar faces at x = 0, x = a, y = 0,
y = b, z = 0, z = c. The quantum stationary states of this system are the eigenfunctions of
the Schrödinger equation
−1
2∇2ψ(x, y, z) = Eψ(x, y, z),
(9.48)
where this PDE is subject to the Dirichlet boundary condition ψ = 0 on the walls of the
box. We identify E as the stationary-state energy (the eigenvalue), in a system of units with
m = ¯h = 1. This is a Helmholtz equation with the new wrinkle that E is not initially known.
The boundary conditions are such that this PDE has no solution except for a set of discrete
values of E. We want to ﬁnd both those values and the corresponding eigenfunctions.

420
Chapter 9 Partial Differential Equations
Separating the variables in Eq. (9.48) by assuming a solution of the form Eq. (9.30), the
PDE becomes
−
 X′′
X + Y ′′
Y + Z′′
Z

= 2E,
(9.49)
and the separation yields
X′′
X = −l2,
with solution X = A sinlx + B coslx.
After applying the boundary conditions at x = 0 and x = a we get (scaling to A = 1)
Xλ = sin
λπx
a

,
λ = 1,2,3,...,
so l = λπ/a.
(9.50)
Because the X equation is a 1-D Hermitian eigenvalue problem, these functions Xλ(x) are
orthogonal on 0 ≤x ≤a.
Similar processing of the Y and Z equations, with separation constants −m2 and −n2,
yields
Yµ = sin
µπy
b

,
µ = 1,2,3,...,
so m = µπ/b,
Zν = sin
νπz
c

,
ν = 1,2,3,...,
so n = νπ/c,
(9.51)
yielding two additional 1-D eigenvalue problems.
Replacing X′′/X, Y ′′/Y , Z′′/Z in Eq. (9.49), respectively, by −l2, −m2, −n2, and then
evaluating these quantities from Eqs. (9.50) and (9.51), we have
l2 + m2 + n2 = 2E,
or
E = π2
2
λ2
a2 + µ2
b2 + ν2
c2

,
(9.52)
with λ, µ, and µ arbitrary positive integers. The situation is quite different from our
solution, Example 9.4.1, of the Laplace equation. Instead of a unique solution we have
an inﬁnite set of solutions, corresponding to all positive integer triples (λ,µ,ν), each with
its own value of E. Making the observation that the differential operator on the left-hand
side of Eq. (9.47) is Hermitian in the presence of the chosen boundary conditions, we have
found a complete orthogonal set of its eigenfunctions. The orthogonality is obvious, as it
can be conﬁrmed from the orthogonality of the Xλ, Yµ, and Zν on their respective 1-D
intervals. Because we set the coefﬁcients of all the sine functions to unity, our overall
eigenfunctions are not normalized, but we can easily normalize them if we so choose.
We close this example with the observation that this boundary-value problem will not
have a solution for arbitrarily chosen values of E, as the E values must satisfy Eq. (9.52)
with integer values of λ, µ, and ν. This will cause the E values of the problem solutions to
be a discrete set; using terminology introduced in a previous chapter, our boundary-value
problem can be said to have a discrete spectrum.
■

9.4 Separation of Variables
421
Circular Cylindrical Coordinates
Curvilinear coordinate systems introduce additional nuances into the process for separating
variables. Again we consider the Helmholtz equation, now in circular cylindrical coordi-
nates. With our unknown function ψ dependent on ρ,ϕ, and z, that equation becomes,
using Eq. (3.149) for ∇2:
∇2ψ(ρ,ϕ, z) + k2ψ(ρ,ϕ, z) = 0,
(9.53)
or
1
ρ
∂
∂ρ

ρ ∂ψ
∂ρ

+ 1
ρ2
∂2ψ
∂ϕ2 + ∂2ψ
∂z2 + k2ψ = 0.
(9.54)
As before, we assume a factored form3 for ψ,
ψ(ρ,ϕ, z) = P(ρ)8(ϕ)Z(z).
(9.55)
Substituting into Eq. (9.46), we have
8Z
ρ
d
dρ

ρ d P
dρ

+ P Z
ρ2
d28
dϕ2 + P8d2Z
dz2 + k2P8Z = 0.
(9.56)
All the partial derivatives have become ordinary derivatives. Dividing by P8Z and mov-
ing the z derivative to the right-hand side yields
1
ρP
d
dρ

ρ d P
dρ

+
1
ρ28
d28
dϕ2 + k2 = −1
Z
d2Z
dz2 .
(9.57)
Again, a function of z on the right appears to depend on a function of ρ and ϕ on the
left. We resolve this by setting each side of Eq. (9.57) equal to the same constant. Let us
choose4 −l2. Then
d2Z
dz2 = l2Z
(9.58)
and
1
ρP
d
dρ

ρ d P
dρ

+
1
ρ28
d28
dϕ2 + k2 = −l2.
(9.59)
Setting
k2 + l2 = n2,
(9.60)
multiplying by ρ2, and rearranging terms, we obtain
ρ
P
d
dρ

ρ d P
dρ

+ n2ρ2 = −1
8
d28
dϕ2 .
(9.61)
3For those with limited familiarity with the Greek alphabet, we point out that the symbol P is the upper-case form of ρ.
4Again, the choice of sign of the separation constant is arbitrary. However, the minus sign chosen for the axial coordinate z is
optimum if we expect exponential dependence on z, from Eq. (9.58). A positive sign is chosen for the azimuthal coordinate ϕ in
expectation of a periodic dependence on ϕ, from Eq. (9.62).

422
Chapter 9 Partial Differential Equations
We set the right-hand side equal to m2, so
d28
dϕ2 = −m28,
(9.62)
and the left-hand side of Eq. (9.61) rearranges into a separate equation for ρ:
ρ d
dρ

ρ d P
dρ

+ (n2ρ2 −m2)P = 0.
(9.63)
Typically, Eq. (9.62) will be subject to the boundary condition that 8 have periodicity 2π
and will therefore have solutions
e±imϕ or, equivalently sinmϕ, cosmϕ, with integer m.
The ρ equation, Eq. (9.63), is Bessel’s differential equation (in the independent variable
nρ), originally encountered in Chapter 7. Because of its occurrence here (and in many
other places relevant to physics), it warrants extensive study and is the topic of Chapter 14.
The separation of variables of Laplace’s equation in parabolic coordinates also gives rise
to Bessel’s equation. It may be noted that the Bessel equation is notorious for the variety
of disguises it may assume. For an extensive tabulation of possible forms the reader is
referred to Tables of Functions by Jahnke and Emde.5
Summarizing, we have found that the original Helmholtz equation, a 3-D PDE, can be
replaced by three ODEs, Eqs. (9.58), (9.62), and (9.63). Noting that the ODE for ρ contains
the separation constants from the z and ϕ equations, the solutions we have obtained for the
Helmholtz equation can be written, with labels, as
ψlm(ρ,ϕ, z) = Plm(ρ)8m(ϕ)Zl(z),
(9.64)
where we probably should recall that the n in Eq. (9.63) for P is a function of l (specif-
ically, n2 = l2 + k2). The most general solution of the Helmholtz equation can now be
constructed as a linear combination of the product solutions:
9(ρ,ϕ, z) =
X
l,m
alm Plm(ρ)8m(ϕ)Zl(z).
(9.65)
Reviewing what we have done, we note that the separation could still have been achieved
if k2 had been replaced by any additive function of the form
k2
−→
f (r) + g(ϕ)
ρ2
+ h(z).
Example 9.4.3
CYLINDRICAL EIGENVALUE PROBLEM
In this example we regard Eq. (9.53) as an eigenvalue problem, with Dirichlet boundary
conditions ψ = 0 on all boundaries of a ﬁnite cylinder, with k2 initially unknown and to be
determined. Our region of interest will be a cylinder with curved boundaries at ρ = R and
with end caps at z = ±L/2, as shown in Fig. 9.2. To emphasize that k2 is an eigenvalue,
5E. Jahnke and F. Emde, Tables of Functions, 4th rev. ed., New York: Dover (1945), p. 146; also, E. Jahnke, F. Emde, and
F. Lösch, Tables of Higher Functions, 6th ed., New York: McGraw-Hill (1960).

9.4 Separation of Variables
423
R
+ L/2
−L/2
z
FIGURE 9.2
Cylindrical region for solution of the Helmholtz equation.
we rename it λ, and our eigenvalue equation is, symbolically,
−∇2ψ = λψ,
(9.66)
with boundary conditions ψ = 0 at ρ = R and at z = ±L/2. Apart from constants, this is
the time-independent Schrödinger equation for a particle in a cylindrical cavity. We limit
the present example to the determination of the smallest eigenvalue (the ground state).
This will be the solution to the PDE with the smallest number of oscillations, so we seek a
solution without zeros (nodes) in the interior of the cylindrical region.
Again, we seek separated solutions of the form given in Eq. (9.55). The ODEs for Z and
8, Eqs. (9.58) and (9.62), have the simple forms
Z′′ = l2Z,
8′′ = −m28,
with general solutions
Z = A elz + B e−lz,
8 = A′ sinmϕ + B′ cosmϕ.
We now need to specialize these solutions to satisfy the boundary conditions. The condition
on 8 is simply that it be periodic in ϕ with period 2π; this result will be obtained if m is
any integer (including m = 0, which corresponds to the simple solution 8 = constant).
Since our objective here is to obtain the least oscillatory solution, we choose that form,
8 = constant, for 8.
Looking next at Z, we note that the arbitrary choice of sign for the separation constant
l2 has led to a form of solution that appears not to be optimum for fulﬁlling conditions
requiring Z = 0 at the boundaries. But, writing l2 = −ω2, l = iω, Z becomes a linear
combination of sinωz and cosωz; the least oscillatory solution with Z(±L/2) = 0 is Z =
cos(πz/L), so ω = π/L, and l2 = −π2/L2.
The functions Z(z) and 8(ϕ) that we have found satisfy the boundary conditions in z
and ϕ but it remains to choose P(ρ) in a way that produces P = 0 at ρ = R with the least
oscillation in P. The equation governing P, Eq. (9.63), is
ρ2P′′ + ρP′ + n2ρ2P = 0,
(9.67)

424
Chapter 9 Partial Differential Equations
where n was introduced as satisfying (in the current notation) n2 = λ + l2, see Eq. (9.60).
Continuing now with Eq. (9.67), we identify as the Bessel equation of order zero in x = nρ.
As we learned in Chapter 7, this ODE has two linearly independent solutions, of which
only the one designated J0 is nonsingular at the origin. Since we need here a solution that
is regular over the entire range 0 ≤x ≤nR, the solution we must choose is J0(nρ).
We can now see what is necessary to satisfy the boundary condition at ρ = R, namely
that J0(nR) vanish. This is a condition on the parameter n. Remembering that we want
the least oscillatory function P, we need for n to be such that nR will be the location of
the smallest zero of J0. Giving this point the name α (which by numerical methods can
be found to be approximately 2.4048), our boundary condition takes the form nR = α, or
n = α/R, and our complete solution to the Helmholtz equation can be written
ψ(ρ,ϕ, z) = J0
αρ
R

cos
πz
L

.
(9.68)
To complete our analysis, we must ﬁgure out how to arrange that n = α/R. Since the
condition connecting n, l, and λ rearranges to
λ = n2 −l2,
(9.69)
we see that the condition on n translates into one on λ. Our PDE has a unique ground-
state solution consistent with the boundary conditions, namely an eigenfunction whose
eigenvalue can be computed from Eq. (9.69), yielding
λ = α2
R2 + π2
L2 .
If we had not restricted consideration to the ground state (by choosing the least
oscillatory solution), we would have (in principle) been able to obtain a complete set of
eigenfunctions, each with its own eigenvalue.
■
Spherical Polar Coordinates
As a ﬁnal exercise in the separation of variables in PDEs, let us try to separate
the Helmholtz equation, again with k2 constant, in spherical polar coordinates. Using
Eq. (3.158), our PDE is
1
r2 sinθ

sinθ ∂
∂r

r2 ∂ψ
∂r

+ ∂
∂θ

sinθ ∂ψ
∂θ

+
1
sinθ
∂2ψ
∂ϕ2

= −k2ψ.
(9.70)
Now, in analogy with Eq. (9.30) we try
ψ(r,θ,ϕ) = R(r)2(θ)8(ϕ).
(9.71)
By substituting back into Eq. (9.70) and dividing by R28, we have
1
R r2
d
dr

r2 d R
dr

+
1
2r2 sinθ
d
dθ

sinθ d2
dθ

+
1
8r2 sin2 θ
d28
dϕ2 = −k2.
(9.72)

9.4 Separation of Variables
425
Note that all derivatives are now ordinary derivatives rather than partials. By multiplying
by r2 sin2 θ, we can isolate (1/8)(d28/dϕ2) to obtain
1
8
d28
dϕ2 = r2 sin2 θ

−k2 −
1
R r2
d
dr

r2 d R
dr

−
1
2r2 sinθ
d
dθ

sinθ d2
dθ

.
(9.73)
Equation (9.73) relates a function of ϕ alone to a function of r and θ alone. Since r, θ,
and ϕ are independent variables, we equate each side of Eq. (9.73) to a constant. In almost
all physical problems, ϕ will appear as an azimuth angle. This suggests a periodic solution
rather than an exponential. With this in mind, let us use −m2 as the separation constant,
which then must be an integer squared. Then
1
8
d28(ϕ)
dϕ2
= −m2
(9.74)
and
1
R r2
d
dr

r2 d R
dr

+
1
2r2 sinθ
d
dθ

sinθ d2
dθ

−
m2
r2 sin2 θ
= −k2.
(9.75)
Multiplying Eq. (9.75) by r2 and rearranging terms, we obtain
1
R
d
dr

r2 d R
dr

+ r2k2 = −
1
2sinθ
d
dθ

sinθ d2
dθ

+
m2
sin2 θ
.
(9.76)
Again, the variables are separated. We equate each side to a constant, λ, and ﬁnally obtain
1
sinθ
d
dθ

sinθ d2
dθ

−
m2
sin2 θ
2 + λ2 = 0,
(9.77)
1
r2
d
dr

r2 d R
dr

+ k2R −λR
r2 = 0.
(9.78)
Once more we have replaced a partial differential equation of three variables by three
ODEs.
The ODE for 8 is the same as that encountered in cylindrical coordinates, with solutions
exp(±imϕ) or sinmϕ, cosmϕ. The 2 ODE can be made less forbidding by changing the
independent variable from θ to t = cosθ, after which Eq. (9.77), with 2(θ) now written
as P(cosθ) = P(t), becomes
(1 −t2)P′′(t) −2t P′(t) −
m2
1 −t2 P(t) + λP(t) = 0.
(9.79)
This is the associated Legendre equation (called the Legendre equation if m = 0), and is
discussed in detail in Chapter 15. We normally require solutions for P(t) that do not have
singularities in the region within the range of the spherical polar coordinate θ (namely
that it be nonsingular for the entire range 0 ≤θ ≤π, equivalent to −1 ≤t ≤+1). The
solutions satisfying these conditions, called associated Legendre functions, are tradition-
ally denoted Pm
l , with l a nonnegative integer. In Section 8.3 we discussed the Legendre
equation as a 1-D eigenvalue problem, ﬁnding that the requirement of nonsingularity
at t = ±1 is a sufﬁcient boundary condition to make its solutions well deﬁned. We
found also that its eigenfunctions are the Legendre polynomials and that its eigenvalues

426
Chapter 9 Partial Differential Equations
(λ in the present notation) have the values l(l +1), where l is an integer. The generalization
of these ﬁndings to the associated Legendre equation (that with nonzero m) shows that λ
continues to be given as l(l +1), but with the additional restriction that l ≥|m|. Details are
deferred to Chapter 15.
Before continuing to the R equation, Eq. (9.78), let us observe that in deriving the 8 and
2 equations we have assumed that k2 was a constant. However, if k2 was not a constant,
but an additive expression of the form
k2
−→
f (r) + g(θ)
r2
+
h(ϕ)
r2 sin2 θ
,
we could still carry out the separation of variables, but the relatively familiar 8 and 2
equations we have identiﬁed will be changed in ways that make them different, and prob-
ably less tractable. However, if the departure of k2 from a constant value is restricted to
the form k2 = k2(r), then the angular parts of the separation will remain as presented
in Eqs. (9.74) and (9.79), and we only need to deal with increased generality in the R
equation.
It is worth stressing that the great importance of this separation of variables in spherical
polar coordinates stems from the fact that the case k2 = k2(r) covers a tremendous amount
of physics, such as a great deal of the theories of gravitation, electrostatics and atomic,
nuclear, and particle physics. Problems with k2 = k2(r) can be characterized as central
force problems, and the use of spherical polar coordinates is natural in such problems.
From both a practical and a theoretical point of view, it is a key observation that the angu-
lar dependence is isolated in Eqs. (9.74) and (9.77), or its equivalent, Eq. (9.79), that these
equations are the same for all central force problems, and that they can be solved exactly.
A detailed discussion of the angular properties of central force problems in quantum me-
chanics is deferred to Chapter 16.
Returning now to the remaining separated ODE, namely the R equation, we consider in
some depth two special cases: (1) The case k2 = 0, corresponding to the Laplace equation,
and (2) k2 a nonzero constant, corresponding to the Helmholtz equation. For both cases we
assume that the 8 and 2 equations have been solved subject to the boundary conditions
already discussed, so that the separation constant λ must have the value l(l + 1) for some
nonnegative integer l. Continuing on the assumption that k2 is a (possibly zero) constant,
Eq. (9.79) expands into
r2R′′ + 2r R′ +
h
k2r2 −l(l + 1)
i
R = 0.
(9.80)
Taking ﬁrst the case of the Laplace equation, for which k2 = 0, Eq. (9.80) is easy to
solve. Either by inspection or by attempting to carry out a series solution by the method
of Frobenius, it is found that the initial term of the series, a0rs, is by itself a complete
solution to Eq. (9.80). In fact, substituting the assumed solution R = rs into Eq. (9.80),
that equation reduces to
s(s −1)rs + 2s rs −l(l + 1)rs = 0,
showing that s(s + 1) = l(l + 1), which has two solutions, s = l (obviously), and s =
−l −1. In other words, given the value l from the choice of solution to the 2 equation,

9.4 Separation of Variables
427
we ﬁnd that the R equation (for the Laplace equation) has the two solutions rl and r−l−1,
so its general solution takes the form
R(r) = Arl + B r−l−1.
(9.81)
Combining the solutions to the separated ODEs, and summing over all choices of the
separation constants, we see that the most general solution of the Laplace equation that has
a nonsingular angular dependence can be written
ψ(r,θ,ϕ) =
X
l,m
(Almrl + Blmr−l−1)Pm
l (cosθ)(A′
lm sinmϕ + B′
lm cosmϕ).
(9.82)
If our problem now has Dirichlet or Neumann boundary conditions on a spherical surface
(with the region under study either within or outside the sphere), we may be able (by meth-
ods more fully articulated in later chapters) to choose the coefﬁcients in Eq. (9.82) so that
the boundary conditions are satisﬁed. Note that if the region in which we are to solve the
Laplace equation includes the origin, r = 0, then only the rl term should be retained and
we set Blm to zero. If our region for the Laplace equation is, say, external to a sphere of
some ﬁnite radius, then we must avoid the large-r divergence of rl and set Alm to zero,
retaining only r−l−1. More complicated cases, e.g., where we study the annular region
between two concentric spheres, will require the retention of both Alm and Blm and will in
general be somewhat more difﬁcult.
We continue now to the case of nonzero but constant k2. Equation (9.80) looks a lot
like a Bessel equation, but differs therefrom by the coefﬁcient “2” in the R′ term and the
factor k2 that multiplies r2 in the coefﬁcient of R. Both these differences can be resolved
by rewriting R(r) as
R(r) = Z(kr)
(kr)1/2 ,
(9.83)
which will then give us a differential equation for Z. Carrying out the differentiations to
obtain R′ and R′′ in terms of Z, and changing the independent variable from r to x = kr,
Eq. (9.83) becomes
x2Z′′ + x Z′ +
h
x2 −
 l + 1
2
2i
Z = 0,
(9.84)
showing that Z is a Bessel function, of order l + 1
2. Returning to Eq. (9.83), we can now
identify R(r) in terms of quantities known as spherical Bessel functions, where jl(x), the
spherical Bessel functions that are regular at x = 0, have deﬁnition
jl(x) =
r π
2x Jl+1/2(x).
Since the status of R(r) as the solution to a homogeneous ODE is not affected by the scale
factor in the deﬁnition of jl(x), we see that Eq. (9.83) is equivalent to the observation that
Eq. (9.80) has a solution jl(kr). The spherical Bessel function that is the second solution of
Eq. (9.80) is designated yl, so that solution is yl(kr), and the general solution of Eq. (9.80)
can be written
R(r) = Ajl(kr) + Byl(kr).
(9.85)

428
Chapter 9 Partial Differential Equations
We note here that the properties of spherical Bessel functions are discussed more fully in
Chapter 14.
With the solutions to the radial ODE in hand, we can now write that the general solution
to the Helmholtz equation in spherical polar coordinates takes the form
ψ(r,θ,ϕ) =
X
l,m

Alm jl(kr) + Blm yl(kr)

× Pm
l (cosθ)(A′
lm sinmϕ + B′
lm cosmϕ).
(9.86)
The above discussion assumes that k2 > 0; negative values of k2 (and therefore
imaginary values of k) simply correspond to our identifying an equation of the form
(∇2 −k2)ψ = 0 as a somewhat peculiar case of (∇2 + k2)ψ = 0. For negative k2, we
can see we then get solutions that involve jl(kr) or yl(kr) with imaginary k. In order
to avoid notations that unnecessarily involve imaginary quantities, it is usual to deﬁne a
new set of functions il(x) that are proportional to jl(ix), and are called modiﬁed spher-
ical Bessel functions. The modiﬁed solutions parallel to yl(ix) are denoted kl(x). These
functions are also discussed in Chapter 14.
The cases we have just surveyed do not, of course, cover all possibilities, and various
other choices of k2(r) lead to problems that are of importance in physics. Without pro-
ceeding to a detailed analysis here, we cite a couple:
•
Taking k2 = A/r + λ yields (with boundary condition that ψ vanish in the limit
r →∞) the time-independent Schrödinger equation for the hydrogen atom; the R
equation can then be identiﬁed as the associated Laguerre differential equation, dis-
cussed in Chapter 18.
•
Taking k2 = Ar2 + λ yields (with boundary condition at r = ∞) the equation for the
3-D quantum harmonic oscillator, for which the R equation can be reduced to the
Hermite ODE, also discussed in Chapter 18.
Some other boundary-value problems lead to well-studied ODEs. However, sometimes the
practicing physicist will encounter a radial equation that may have to be solved using the
techniques presented in Chapter 7, or if all else fails, by numerical methods.
We close this subsection with an example that is a simple boundary-value problem in
spherical coordinates.
Example 9.4.4
SPHERE WITH BOUNDARY CONDITION
In this example we solve the Laplace equation for the electrostatic potential ψ(r) in a
region interior to a sphere of radius a, using spherical polar coordinates (r,θ,ϕ) with
origin at the center of the sphere. Our solution is to be subject to the Neumann boundary
condition dψ/dn = −V0 cosθ on the spherical surface. See Fig. 9.3.
To start, we note that totally arbitrary Neumann boundary conditions will not be consis-
tent with our assumption of a charge-free sphere, as the integral of the normal derivative on
the spherical surface gives, according to Gauss’ law, a measure of the total charge within.

9.4 Separation of Variables
429
z
zero
zero
FIGURE 9.3
Arrows indicate sign and relative magnitude of the (inward) normal
derivative of the electrostatic potential on a spherical surface (boundary condition
for Example 9.4.4).
The present example is internally consistent, as
Z
S
cosθdσ =
π
Z
0
dθ
2π
Z
0
dϕ cosθ = 0.
Next, we need to take the general solution for the Laplace equation within a sphere, as
given by Eq. (9.82), and calculate therefrom the inward normal derivative at r = a. Since
the normal is in the −r direction, we need only compute −∂ψ/∂r, evaluated at r = a.
Noting that for the present problem Blm = 0, our boundary condition becomes
−V cosθ = −
X
l,m
l Almal−1Pm
l (cosθ)(A′
lm sinmϕ + B′
lm cosmϕ).
Since the left-hand side of this equation is independent of ϕ, its right-hand side has nonzero
coefﬁcients only for m = 0, for which we only have the term originally containing B′
l0,
because sin(0) = 0. Thus, consolidating the constants, the boundary condition becomes
the simpler form
−V cosθ = −
X
l
l Alal−1Pl(cosθ),
(9.87)
Without having made a detailed study of the properties of Legendre functions, the solution
of an equation of this type might need to be deferred to Chapter 15, but this one is easy
to solve because P1(cosθ) = cosθ (see Legendre polynomials in Table 15.1) Thus, from
Eq. (9.87),
l Alal−1 = V δl1,
so A1 = V and all the other coefﬁcients except A0 vanish. The coefﬁcient A0 is not deter-
mined by the boundary conditions and represents an arbitrary constant that may be added

430
Chapter 9 Partial Differential Equations
to the potential. Thus, the potential within the sphere has the form
ψ = Vr P1(cosθ) + A0 = Vr cosθ + A0 = V z + A0,
corresponding to a uniform electric ﬁeld within the sphere, in the −z direction and of
magnitude V. The electric ﬁeld is, of course, unaffected by the arbitrary value of the
constant A0.
■
Summary: Separated-Variable Solutions
For convenient reference, the forms of the solutions of Laplace’s and Helmholtz’s equa-
tions for spherical polar coordinates are collected in Table 9.2. Although the ODEs
obtained from the separation of variables are the same irrespective of the boundary con-
ditions, the ODE solutions to be used, and the constants of separation, do depend on the
boundaries. Boundaries with less than spherical symmetry may lead to values of m and
l that are not integral, and may also require use of the second solution of the Legendre
equation (quantities normally denoted Qm
l ). Engineering applications frequently require
solutions to PDEs for regions of low symmetry, but such problems are nowadays almost
universally approached using numerical, rather than analytical methods. Consequently,
Table 9.2 only contains data that are relevant for problems inside or outside a spherical
boundary, or between two concentric spherical boundaries. This restriction to spherical
symmetry causes the angular portion of the solutions to be uniquely of the form we have
already identiﬁed.
In contrast to the unique angular solution, both linearly independent solutions to the
radial ODE are relevant, with the choice of solution dependent on the geometry. Solutions
within a sphere must employ only the radial functions that are regular at the origin, i.e.,
rl, jl, or il. Solutions external to a sphere may employ r−l−1, kl (deﬁned so that it will
decay exponentially to zero at large r), or a linear combination of jl and yl (both of which
are oscillatory and decay as r−1/2). Solutions between concentric spheres can use both the
radial functions appropriate to the PDE.
It is also possible to summarize the forms of solution to the Laplace and Helmholtz
equations in circular cylindrical coordinates, if we restrict attention to problems that have
circular symmetry about the axial direction of the coordinate system. However, the situa-
tion is considerably more complicated than for spherical coordinates, as we now have two
Table 9.2
Solutions of PDEs in Spherical Polar Co-
ordinatesa
ψ =
X
l,m
fl(r)Pm
l (cosθ)



alm cosmϕ + blm sinmϕ)
or
clmeimϕ



∇2ψ = 0
fl(r) = rl, r−l−1
∇2ψ + k2ψ = 0
fl(r) = jl(kr), yl(kr)
∇2ψ −k2ψ = 0
fl(r) = il(kr), kl(kr)
a For il, jl, kl, yl, see Chapter 14; for Pm
l , see Chapters 15 and 16.

9.4 Separation of Variables
431
Table 9.3
Solutions of PDEs in Circular Cylindrical Coordinatesa
ψ =
X
m,α
fmα(ρ)gα(z)



amα cosmϕ + bmα sinmϕ)
or
cmαeimϕ



∇2ψ = 0
fmα(ρ) = Jm(αρ),Ym(αρ)
gα(z) = eαz, e−αz
or
fmα(ρ) = Im(αρ), Km(αρ)
gα(z) = sin(αz),cos(αz) or eiαz
or
fmα(ρ) = ρm, ρ−m
gα(z) = 1
∇2ψ + λψ = 0
fmα(ρ) = Jm(αρ),Ym(αρ)
if β2 = α2 −λ > 0,
gα(z) = eβz, e−βz
if β2 = λ −α2 > 0,
gα(z) = sin(βz),cos(βz) or eiβz
if λ = α2,
gα(z) = 1
or
fmα(ρ) = Im(αρ), Km(αρ)
if β2 = −λ −α2 > 0,
gα(z) = eβz, e−βz
if β2 = λ + α2 > 0,
gα(z) = sin(βz),cos(βz) or eiβz
if λ = −α2,
gα(z) = 1
or
fmα(ρ) = ρm, ρ−m
if β2 = −λ > 0,
gα(z) = eβz, e−βz
if β2 = λ > 0,
gα(z) = sin(βz),cos(βz) or eiβz
a The parameter α can have any real values consistent with the boundary conditions. For Im,
Jm, Km, Ym, see Chapter 14.
coordinates (ρ and z) that can have a variety of boundary conditions, in contrast to the
single such coordinate (r) in the spherical system. In spherical coordinates the form of the
radial function is completely determined by the PDE, and speciﬁc problems differ only
in the choice (or relative weight) of the two linearly independent radial solutions. But in
cylindrical coordinates the forms of the ρ and z solutions, as well as their coefﬁcients, are
determined by the boundary conditions, and not entirely by the value of the constant in the
Helmholtz equation. Choices of the ρ and z solutions, though coupled, can vary widely.
For details, the reader is referred to Table 9.3.
Our ﬁnal observations of this section deal with the functions we encountered in the
course of the separations in cylindrical and spherical coordinates. For the purpose of this
discussion, it is useful to think of our PDE as an operator equation subject to boundary
conditions. If, in cylindrical coordinates, we restrict attention to PDEs in which the param-
eter k2 is independent of ϕ (and with boundary conditions that do not depend upon ϕ), we
have chosen our operator equation as one that has circular symmetry. Moreover, we will
then always get the same 8 equation, with (of course) the same solutions. In these cir-
cumstances, the solutions will have symmetry properties derived from those of our overall
boundary-value problem.6 The 8 equation can also be thought of as an operator equa-
tion, and we can go further and identify the operator as L2
z = −∂2/∂ϕ2, where Lz is the
z component of the angular momentum. The solutions of the 8 equation are eigenfunc-
tions of this operator; the reason they can occur as part of the PDE solution is because
6Note that the solutions to a boundary-value problem need not have the full problem symmetry (a point that will be elaborated
in great detail when we develop group-theoretical methods). An obvious example is that the Sun-Earth gravitational potential is
spherically symmetric, while the most familiar solution (the Earth’s orbit) is planar. The dilemma is resolved by noting that the
spherical symmetry manifests itself in the possible existence of Earth orbits at all angular orientations.

432
Chapter 9 Partial Differential Equations
L2
z commutes with the operator deﬁning the PDE (clearly so, because the PDE operator
does not contain ϕ). In other words, because L2
z and the PDE operator commute, they will
have simultaneous eigenfunctions, and the overall solutions of the PDE can be labeled to
identify the L2
z eigenfunction that was chosen.
Looking now at the situation in spherical polar coordinates, we note that if k2 is inde-
pendent of the angles, i.e., k2 = k2(r), then our PDE always has the same angular solutions
2lm(θ)8m(ϕ). Looking further at the angular terms of our PDE, we can identify them as
the operator L2, and we see that the angular solutions we have found are eigenfunctions of
this operator. When the PDE operator is independent of the angles, it will commute with
L2 and the solutions to the PDE can be labeled accordingly. These symmetry features are
very important and are discussed in great detail in Chapter 16.
Exercises
9.4.1
By letting the operator ∇2 + k2 act on the general form a1ψ1(x, y, z) + a2ψ2(x, y, z),
show that it is linear, i.e., that (∇2 + k2)(a1ψ1 + a2ψ2) = a1(∇2 + k2)ψ1 + a2(∇2 +
k2)ψ2.
9.4.2
Show that the Helmholtz equation,
∇2ψ + k2ψ = 0,
is still separable in circular cylindrical coordinates if k2 is generalized to k2 + f (ρ) +
(1/ρ2)g(ϕ) + h(z).
9.4.3
Separate variables in the Helmholtz equation in spherical polar coordinates, splitting off
the radial dependence ﬁrst. Show that your separated equations have the same form as
Eqs. (9.74), (9.77), and (9.78).
9.4.4
Verify that
∇2ψ(r,θ,ϕ) +

k2 + f (r) + 1
r2 g(θ) +
1
r2 sin2 θ
h(ϕ)

ψ(r,θ,ϕ) = 0
is separable (in spherical polar coordinates). The functions f , g, and h are functions
only of the variables indicated; k2 is a constant.
9.4.5
An atomic (quantum mechanical) particle is conﬁned inside a rectangular box of sides
a,b, and c. The particle is described by a wave function ψ that satisﬁes the Schrödinger
wave equation
−¯h2
2m ∇2ψ = Eψ.
The wave function is required to vanish at each surface of the box (but not to be identi-
cally zero). This condition imposes constraints on the separation constants and therefore
on the energy E. What is the smallest value of E for which such a solution can be
obtained?
ANS.
E = π2¯h2
2m
 1
a2 + 1
b2 + 1
c2

.

9.5 Laplace and Poisson Equations
433
9.4.6
The quantum mechanical angular momentum operator is given by L =
−i(r × ∇). Show that
L · Lψ = l(l + 1)ψ
leads to the associated Legendre equation.
Hint. Section 8.3 and Exercise 8.3.1 may be helpful.
9.4.7
The 1-D Schrödinger wave equation for a particle in a potential ﬁeld V = 1
2kx2 is
−¯h2
2m
d2ψ
dx2 + 1
2kx2ψ = Eψ(x).
(a)
Deﬁning
a =
mk
¯h2
1/4
,
λ = 2E
¯h
m
k
1/2
,
and setting ξ = ax, show that
d2ψ(ξ)
dξ2
+ (λ −ξ2)ψ(ξ) = 0.
(b)
Substituting
ψ(ξ) = y(ξ)e−ξ2/2,
show that y(ξ) satisﬁes the Hermite differential equation.
9.5
LAPLACE AND POISSON EQUATIONS
The Laplace equation can be considered the prototypical elliptic PDE. At this point we
supplement the discussion motivated by the method of separation of variables with some
additional observations. The importance of Laplace’s equation for electrostatics has stim-
ulated the development of a great variety of methods for its solution in the presence of
boundary conditions ranging from simple and symmetrical to complicated and convoluted.
Techniques for present-day engineering problems tend to rely heavily on computational
methods. The thrust of this section, however, will be on general properties of the Laplace
equation and its solutions.
The basic properties of the Laplace equation are independent of the coordinate system
in which it is expressed; we assume for the moment that we will use Cartesian coordinates.
Then, because the PDE sets the sum of the second derivatives, ∂2ψ/∂x2
i , to zero, it is
obvious that if any of the second derivatives has a positive sign, at least one of the others
must be negative. This point is illustrated in Example 9.4.1, where the x and y dependence
of a solution to the Laplace equation was sinusoidal, and as a result, the z dependence was
exponential (corresponding to different signs for the second derivative). Since the second
derivative is a measure of curvature, we conclude that if ψ has positive curvature in any
coordinate direction, it must have negative curvature in some other coordinate direction.
That observation, in turn, means that all the stationary points of ψ (points where its
ﬁrst derivatives in all directions vanish) must be saddle points, not maxima or minima.

434
Chapter 9 Partial Differential Equations
Since the Laplace equation describes the static electric potential in charge-free regions, we
conclude that the potential cannot have an extremum at a point where there is no charge.
A corollary to this observation is that the extrema of the electrostatic potential in a charge-
free region must be on the boundary of the region.
A related property of the Laplace equation is that its solution, subject to Dirichlet bound-
ary conditions for the entire closed boundary of its region, is unique. This property applies
also to its inhomogeneous generalization, the Poisson equation. The proof is simple: Sup-
pose there are two distinct solutions ψ1 and ψ2 for the same boundary conditions. Then,
their difference ψ = ψ1 −ψ2 (for either the Laplace or Poisson equation) will be a solution
to the Laplace equation with ψ = 0 on the boundary. Since ψ cannot have extrema within
the bounded region, it must be zero everywhere, meaning that ψ1 = ψ2.
If we have a Laplace or Poisson equation subject to Neumann boundary conditions on
the entire closed boundary of its region, then the difference ψ = ψ1 −ψ2 of two solutions
will also be a solution to the Laplace equation with a zero Neumann boundary condition.
To analyze this situation, we invoke Green’s Theorem, in the form provided by Eq. (3.86),
taking both u and v of that equation to be ψ. Equation (3.86) then becomes
Z
S
ψ ∂ψ
∂n dS =
Z
V
ψ∇2ψ dτ +
Z
V
∇ψ · ∇ψ dτ.
(9.88)
The boundary condition causes the left-hand side of Eq. (9.88) to vanish, the ﬁrst integral
on the right-hand side vanishes because ψ is a solution of the Laplace equation, and the
remaining integral on the right-hand side must therefore also vanish. But that integral can
only vanish if ∇ψ is zero everywhere, which can only be true if ψ is constant. Thus,
solutions to the Laplace equation with Neumann boundary conditions are also unique,
except for an additive constant to the potential.
An oft-cited application of this uniqueness theorem is the solution of electrostatics prob-
lems by the method of images, which replaces a problem containing boundaries by one
without a boundary but with additional charge added in such a way that the potential at
the boundary location has the desired value. For example, a positive charge in front of a
grounded boundary (one with ψ = 0) can be augmented by a negative charge at the mirror-
image position behind the boundary. Then the two-charge system (ignoring the boundary)
will yield the desired zero potential at the boundary location, and the uniqueness theorem
tells us that the potential calculated for the two-charge system must be the same (within
the original region) as that for the original system.
Exercises
9.5.1
Verify that the following are solutions of Laplace’s equation:
(a)
ψ1 = 1/r, r ̸= 0,
(b)
ψ2 = 1
2r ln r + z
r −z .
9.5.2
If 9 is a solution of Laplace’s equation, ∇29 = 0, show that ∂9/∂z is also a solution.
9.5.3
Show that an argument based on Eq. (9.88) can be used to prove that the Laplace and
Poisson equations with Dirichlet boundary conditions have unique solutions.

9.6 Wave Equation
435
9.6
WAVE EQUATION
The wave equation is the prototype hyperbolic PDE. As we have seen earlier in this chap-
ter, hyperbolic PDEs have two characteristics, and for the equation
1
c2
∂2ψ
∂t2 = ∂2ψ
∂x2 ,
(9.89)
the characteristics are lines of constant x −ct and those of constant x + ct. This means
that the general solution to Eq. (9.89) takes the form
ψ(x,t) = f (x −ct) + g(x + ct),
(9.90)
with f and g completely arbitrary.
Viewing x as a position variable and t as the time, we can interpret f (x −ct) as a wave,
moving with velocity c, in the +x direction. By this we mean that the entire proﬁle of f,
as a function of x at t = 0, will be shifted uniformly toward positive x by an amount c
when t = 1. See Fig. 9.4. Similarly, g(x + ct) describes a wave moving at velocity c in the
−x direction. Because f and g are arbitrary, the traveling waves they describe need not
be sinusoidal or periodic, but may be entirely irregular; moreover, there is no requirement
that f and g have any particular relationship to each other.
An obvious special case of the general situation described above is that when f (x −ct)
is chosen to be sinusoidal, f = sin(x −ct). For simplicity we have taken f to have unit
amplitude and wavelength 2π. We also take g(x + ct) to be g = sin(x + ct), a sinusoidal
wave of the same wavelength and amplitude traveling in the direction opposite to f. At a
point x and time t, these two waves add to produce a resultant
ψ(x,t) = sin(x −ct) + sin(x + ct),
which, using trigonometric identities, can be rearranged to
ψ(x,t) = (sin x cosct −cos x sinct) + (sin x cosct + cos x sinct) = 2sin x cosct.
This form for ψ can be identiﬁed as a standing wave distribution, meaning that the time
evolution of the wave’s proﬁle in x is an oscillation in amplitude, with the wave pattern
not moving in either direction. An obvious point of difference from a traveling wave is
that for a standing wave, the nodes (points where ψ = 0) are stationary in time, while in a
traveling wave they are moving in time at velocity ±c.
Our current interest in traveling vs. standing waves is their relation to solutions to
the wave equation that we might ﬁnd using the method of separation of variables. That
method would obviously lead us to standing-wave solutions. However, it is useful to note
that the totality of the solution set from the separated variables has the same content as
x
⇒
⇒
FIGURE 9.4
Traveling wave f (x −ct). Dashed line is proﬁle at t = 0; full line is
proﬁle at a time t > 0.

436
Chapter 9 Partial Differential Equations
the traveling-wave solutions. For example, the products sin x cosct and cos x sinct are
solutions we would get by separating the variables, and linear combinations of these yield
sin(x ± ct).
d’Alembert’s Solution
While all ways of writing the general solution to the wave equation are mathemati-
cally equivalent, diverse forms differ in their convenience of use for various purposes.
To illustrate this, we consider how we might construct a solution to the wave equation,
given, as an initial condition, (1) the entire spatial distribution of the wave amplitude at
t = 0 and (2) the time derivative of the wave amplitude at t = 0 for the entire spatial distri-
bution. The solution to this problem is generally referred to as d’Alembert’s solution of
the wave equation; it was also (and slightly earlier) found by Euler.
We start by using Eq. (9.90) to write our initial conditions in terms of the presently
unknown functions f and g:
ψ(x,0) = f (x) + g(x),
(9.91)
∂ψ(x,t)
∂t

t=0 = −cf ′(x) + cg′(x).
(9.92)
We now integrate Eq. (9.92) between the limits x −ct and x + ct (and divide the result
by 2c), obtaining
1
2c
x+ct
Z
x−ct
∂ψ(x,0)
∂t
dx = 1
2

−f (x + ct) + f (x −ct) + g(x + ct) −g(x −ct)

.
(9.93)
From Eq. (9.91), we also have
1
2

ψ(x + ct,0) + ψ(x −ct,0)

=
1
2

f (x + ct) + g(x + ct) + f (x −ct) + g(x −ct)

.
(9.94)
Adding together the right-hand sides of Eqs. (9.93) and (9.94), half the terms cancel, and
those that survive combine to give the result
f (x −ct) + g(x + ct),
which is ψ(x,t).
Therefore, from the left-hand sides of Eqs. (9.93) and (9.94), we obtain the ﬁnal result
ψ(x,t) = 1
2

ψ(x + ct,0) + ψ(x −ct,0)

+ 1
2c
x+ct
Z
x−ct
∂ψ(x,0)
∂t
dx.
(9.95)
This equation gives ψ(x,t) in terms of data at t = 0 that are within the distance ct of
the point x. This is a reasonable result, since ct is the distance that waves in this problem
can move between times t = 0 and t = t. More speciﬁcally, Eq. (9.95) contains terms that
represent half the t = 0 amplitude at distances ±ct from x (half, because a disturbance that
starts at these points is split between propagation in both directions), plus an additional
integral that accumulates the effect of the initial amplitude derivative over the region of
inﬂuence.

9.7 Heat-Flow, or Diffusion PDE
437
Exercises
Solve the wave equation, Eq. (9.89), subject to the indicated conditions.
9.6.1
Determine ψ(x,t) given that at t = 0 ψ0(x) = sin x and ∂ψ(x)/∂t = cos x.
9.6.2
Determine ψ(x,t) given that at t = 0 ψ0(x) = δ(x) (Dirac delta function) and the initial
time derivative of ψ is zero.
9.6.3
Determine ψ(x,t) given that at t = 0 ψ0(x) is a single square-wave pulse as deﬁned
below, and the initial time derivative of ψ is zero.
ψ0(x) = 0, |x| > a/2,
ψ0(x) = 1/a, |x| < a/2.
9.6.4
Determine ψ(x,t) given that at t = 0 ψ0 = 0 for all x, but ∂ψ/∂t = sin(x).
9.7
HEAT-FLOW, OR DIFFUSION PDE
Here we return to a parabolic PDE to develop methods that adapt a special solution of a
PDE to boundary conditions by introducing parameters. The methods are fairly general
and apply to other second-order PDEs with constant coefﬁcients as well. To some extent,
they are complementary to the earlier basic separation method for ﬁnding solutions in a
systematic way.
We consider the 3-D time-dependent diffusion PDE for an isotropic medium, using it
to describe heat ﬂow subject to given boundary conditions. Assuming isotropy actually is
not much of a restriction because, in case we have different (constant) rates of diffusion in
different directions, for example, in wood, our heat-ﬂow PDE takes the form
∂ψ
∂t = a2 ∂2ψ
∂x2 + b2 ∂2ψ
∂y2 + c2 ∂2ψ
∂z2 ,
(9.96)
if we put the coordinate axes along the principal directions of anisotropy. Now we sim-
ply rescale the coordinates using the substitutions x = aξ, y = bη, z = cζ to get back the
original isotropic form of Eq. (9.96),
∂8
∂t = ∂28
∂ξ2 + ∂28
∂η2 + ∂28
∂ζ 2 ,
(9.97)
for the temperature distribution function 8(ξ,η,ζ,t) = ψ(x, y, z,t).
For simplicity, we ﬁrst solve the time-dependent PDE for a homogeneous one-
dimensional medium, a long metal rod in the x-direction, for which the PDE is
∂ψ
∂t = a2 ∂2ψ
∂x2 ,
(9.98)
where the constant a measures the diffusivity, or heat conductivity, of the medium. We
obtain solutions to this linear PDE with constant coefﬁcients by the method of separation
of variables, for which we set ψ(x,t) = X(x)T (t), leading to the separate equations
1
T
dT
dt = β,
1
X
d2X
dx2 = β
a2 .
These equations have, for any nonzero value of β, solutions T = eβt and X = e±αx, with
α2 = β/a2. We seek solutions whose time dependence decays exponentially at large t,

438
Chapter 9 Partial Differential Equations
that is, solutions with negative values of β, and therefore set α = iω, a2 = −ω2 for real ω,
and have
ψ(x,t) = eiωxe−ω2a2t = (cosωx ± i sinωx)e−ω2a2t.
(9.99)
Note that β = 0, for which
ψ(x,t) = C′
0x + C0,
(9.100)
is also included in the solution set for the PDE. If we use this solution for a rod of inﬁnite
length, we must set C′
0 = 0 to avoid a nonphysical divergence; in any case, the value of C0
is then the constant value that the temperature approaches at long times.
Forming real linear combinations of sinωx and cosωx with arbitrary coefﬁcients, and
keeping the β = 0 solution, we obtain from Eq. (9.99) for any choice of A, B, ω, C′
0, and
C0, a solution
ψ(x,t) = (A cosωx + B sinωx)e−ω2a2t + C′
0x + C0.
(9.101)
Solutions for different values of these parameters can now be combined as needed to form
an overall solution consistent with the required boundary conditions.
If the rod we are studying is ﬁnite in length, it may be that the boundary conditions can
be satisﬁed if we restrict ω to discrete nonzero values that are multiples of a basic value ω0.
For a rod of inﬁnite length, it may be better to let ω assume a continuous range of values,
so that ψ(x,t) will have the general form
ψ(x,t) =
Z
[A(ω)cosωx + B(ω)sinωx]e−a2ω2tdω + C0.
(9.102)
We call speciﬁc attention to the fact that
•
Forming linear combinations of solutions by summation or integration over parameters
is a powerful and standard method for generalizing speciﬁc PDE solutions in order to
adapt them to boundary conditions.
Example 9.7.1
A SPECIFIC BOUNDARY CONDITION
Let us solve a 1-D case explicitly, where the temperature at time t = 0 is ψ0(x) = 1 =
constant in the interval between x = +1 and x = −1 and zero for x > 1 and x < −1. At
the ends, x = ±1, the temperature is always held at zero. Note that this problem, including
its initial conditions, has even parity, ψ0(x) = ψ0(−x), so ψ(x,t) must also be even.
We choose the spatial solutions of Eq. (9.98) to be of the form given in Eq. (9.101),
but restricted to C′
0 = C0 = 0 (since the t →∞limit of ψ(x,t) is zero for the entire
range −1 ≤x ≤1), and to cos(lπx/2) for odd integer l, because these functions are the
even-parity members of an orthonormal basis for the interval −1 ≤x ≤1 that satisfy the
boundary condition ψ = 0 at x = ±1. Then, at t = 0 our solution takes the form
ψ(x,0) =
∞
X
l=1
al cos πlx
2 ,
−1 < x < 1,
and we need to choose the coefﬁcients al so that ψ(x,0) = 1.

9.7 Heat-Flow, or Diffusion PDE
439
Using the orthonormality, we compute
al =
1
Z
−1
1 · cos πlx
2
= 2
lπ sin πlx
2

1
x=−1
= 4
πl sin lπ
2 =
4(−1)m
(2m + 1)π ,
l = 2m + 1.
Including its time dependence, the full solution is given by the series
ψ(x,t) = 4
π
∞
X
m=0
(−1)m
2m + 1 cos
h
(2m + 1)πx
2
i
e−t((2m+1)πa/2)2,
(9.103)
which converges absolutely for t > 0 but only conditionally at t = 0, as a result of the
discontinuity at x = ±1.
■
We are now ready to consider the diffusion equation in three dimensions. We start by
assuming a solution of the form ψ = f (x, y, z)T (t), and separate the spatial from the time
dependence. As in the 1-D case, T (t) will have exponentials as solutions, and we can
choose the solution that decays exponentially at large t. Assigning the separation constant
the value −k2, so that the time dependence is exp(−k2t), the separated equation in the
spatial coordinates takes the form
∂2 f
∂x2 + ∂2 f
∂y2 + ∂2 f
∂z2 + k2 f = 0,
(9.104)
which we recognize as the Helmholtz equation. Assuming that we can solve this equation
for various values of k2 by further separations of variables or by other means, we can form
whatever sum or integral of individual solutions that may be needed to satisfy the boundary
conditions.
Alternate Solutions
In an alternative approach to the heat ﬂow equation, we now return to the one-dimensional
PDE, Eq. (9.98), seeking solutions of a new functional form ψ(x,t) = u(x/√t), which
is suggested by dimensional considerations and experimental data. Substituting u(ξ), ξ =
x/√t, into Eq. (9.98) using
∂ψ
∂x = u′
√t ,
∂2ψ
∂x2 = u′′
t ,
∂ψ
∂t = −
x
2
√
t3 u′
(9.105)
with the notation u′(ξ) ≡du/dξ, the PDE is reduced to the ODE
2a2u′′(ξ) + ξu′(ξ) = 0.
(9.106)
Writing this ODE as
u′′
u′ = −ξ
2a2 ,

440
Chapter 9 Partial Differential Equations
we can integrate it once to get lnu′ = −ξ2/4a2+lnC1, where C1 is an integration constant.
Exponentiating and integrating again we ﬁnd the general solution
u(ξ) = C1
ξ
Z
0
e−ξ2/4a2dξ + C2,
(9.107)
which contains two integration constants Ci. We initialize this solution at time t = 0 to
temperature +1 for x > 0 and −1 for x < 0, corresponding to u(∞) = +1 and u(−∞) =
−1. Noting that
∞
Z
0
e−ξ2/4a2dξ = a√π,
a case of the integral evaluated in Eq. (1.148), we obtain
u(∞) = a√πC1 + C2 = 1,
u(−∞) = −a√πC1 + C2 = −1,
which ﬁxes the constants C1 = 1/a√π, C2 = 0. We therefore have the speciﬁc solution
ψ =
1
a√π
x/√t
Z
0
e−ξ2/4a2dξ =
2
√π
x/2a√t
Z
0
e−v2dv = erf

x
2a√t

,
(9.108)
where erf is the standard name for Gauss’ error function (one of the special functions
listed in Table 1.2). We need to generalize this speciﬁc solution to adapt it to boundary
conditions.
To this end we now generate new solutions of the PDE with constant coefﬁcients
by differentiating the special solution given in Eq. (9.108). In other words, if ψ(x,t)
solves the PDE in Eq. (9.98), so do ∂ψ/∂t and ∂ψ/∂x, because these derivatives and
the differentiations of the PDE commute; that is, the order in which they are carried out
does not matter. Note carefully that this method no longer works if any coefﬁcient of the
PDE depends on t or x explicitly. However, PDEs with constant coefﬁcients dominate
in physics. Examples are Newton’s equations of motion in classical mechanics, the wave
equations of electrodynamics, and Poisson’s and Laplace’s equations in electrostatics and
gravity. Even Einstein’s nonlinear ﬁeld equations of general relativity take on this special
form in local geodesic coordinates.
Therefore, by differentiating Eq. (9.108) with respect to x, we ﬁnd the simpler, more
basic solution,
ψ1(x,t) =
1
a√tπ e−x2/4a2t,
(9.109)
and, repeating the process, another basic solution
ψ2(x,t) =
x
2a3√
t3π
e−x2/4a2t.
(9.110)
Again, these solutions have to be generalized to adapt them to boundary conditions. And
there is yet another method of generating new solutions of a PDE with constant coefﬁ-
cients: We can translate a given solution, for example, ψ1(x,t) →ψ1(x −α,t), and then

9.7 Heat-Flow, or Diffusion PDE
441
integrate over the translation parameter α. Therefore,
ψ(x,t) =
1
2a√tπ
∞
Z
−∞
C(α)e−(x−α)2/4a2tdα
(9.111)
is again a solution, which we rewrite using the substitution
ξ = x −α
2a√t ,
α = x −2aξ
√
t,
dα = −2a
√
tdξ.
(9.112)
These substitutions lead to
ψ(x,t) =
1
√π
∞
Z
−∞
C(x −2aξ
√
t)e−ξ2dξ,
(9.113)
a solution of our PDE. Equation (9.113) is in a form permitting us to understand the sig-
niﬁcance of the weight function C(x) from the translation method. If we set t = 0 in that
equation, the function C in the integrand then becomes independent of ξ, and the integral
can then be recognized as
∞
Z
−∞
e−ξ2dξ = √π,
a well-known result equivalent to Eq. (1.148). Equation (9.113) then becomes the simpler
form
ψ(x,0) = C(x),
or
C(x) = ψ0(x),
where ψ0 is the initial spatial distribution of ψ. Using this notation, we can write the
solution to our PDE as
ψ(x,t) =
1
√π
∞
Z
−∞
ψ0(x −2aξ
√
t)e−ξ2dξ,
(9.114)
a form that explicitly displays the role of the boundary (initial) condition. From Eq. (9.114)
we see that the initial temperature distribution, ψ0(x), spreads out over time and is damped
by the Gaussian weight function.
Example 9.7.2
SPECIAL BOUNDARY CONDITION AGAIN
We consider now a problem similar to Example 9.7.1, but instead of keeping ψ = 0 at
all times at x = ±1, we regard the system as inﬁnite in length, with ψ0 = 0 everywhere
except for |x| < 1, where ψ0 = 1. This change makes Eq. (9.114) usable, because our PDE
now applies over the range (−∞,∞), and heat will ﬂow (and temporarily increase the
temperature) at and beyond |x| = 1.
The range of ψ0(x) corresponds to a range of ξ with endpoints found from x −2aξ√t =
±1, so our solution becomes
ψ(x,t) =
1
√π
(x+1)/2a√t
Z
(x−1)/2a√t
e−ξ2dξ.

442
Chapter 9 Partial Differential Equations
In terms of the error function, we can also write this solution as
ψ(x,t) = 1
2

erf
 x + 1
2a√t

−erf
 x −1
2a√t

.
(9.115)
Equation (9.115) applies for all x, including |x| > 1.
■
Next we consider the problem of heat ﬂow for an extended spherically symmetric
medium centered at the origin, suggesting that we should use polar coordinates r,θ,ϕ. We
expect a solution of the form u(r,t). Using Eq. (3.158) for the Laplacian, we ﬁnd the PDE
∂u
∂t = a2
∂2u
∂r2 + 2
r
∂u
∂r

,
(9.116)
which we transform to the 1-D heat-ﬂow PDE by the substitution
u = v(r,t)
r
,
∂u
∂r = 1
r
∂v
∂r −v
r2 ,
∂u
∂t = 1
r
∂v
∂t ,
∂2u
∂r2 = 1
r
∂2v
∂r2 −2
r2
∂v
∂r + 2v
r3 .
(9.117)
This yields the PDE
∂v
∂t = a2 ∂2v
∂r2 .
(9.118)
Example 9.7.3
SPHERICALLY SYMMETRIC HEAT FLOW
Let us apply the 1-D heat-ﬂow PDE to a spherically symmetric heat ﬂow under fairly
common boundary conditions, where x is replaced by the radial variable. Initially we have
zero temperature everywhere. Then, at time t = 0, a ﬁnite amount of heat energy Q is
released at the origin, spreading evenly in all directions. What is the resulting spatial and
temporal temperature distribution?
Inspecting our special solution in Eq. (9.110) we see that, for t →0, the temperature
v(r,t)
r
= C
√
t3 e−r2/4a2t
(9.119)
goes to zero for all r ̸= 0, so zero initial temperature is guaranteed. As t →∞, the temper-
ature v/r →0 for all r including the origin, which is implicit in our boundary conditions.
The constant C can be determined from energy conservation, which gives (for arbitrary t)
the constraint
Q = σρ
Z v
r dτ = 4πσρC
√
t3
∞
Z
0
r2e−r2/4a2tdr = 8
p
π3σρa3C,
(9.120)

9.7 Heat-Flow, or Diffusion PDE
443
where ρ is the constant density of the medium and σ is its speciﬁc heat. The ﬁnal result
in Eq. (9.120) is obtained by ﬁrst making a change of variable from r to ξ = r/2a√t,
obtaining
∞
Z
0
e−r2/4a2tr2dr = (2a
√
t)3
∞
Z
0
e−ξ2ξ2dξ,
then evaluating the ξ integral via an integration by parts:
∞
Z
0
e−ξ2ξ2dξ = −ξ
2e−ξ2
∞
0
+ 1
2
∞
Z
0
e−ξ2dξ =
√π
4 .
The temperature, as given by Eq. (9.119) at any moment, i.e., at ﬁxed t, is a Gaussian
distribution that ﬂattens out as time increases, because its width is proportional to √t.
As a function of time the temperature at any ﬁxed point is proportional to t−3/2e−T/t,
with T ≡r2/4a2. This functional form shows that the temperature rises from zero to a
maximum and then falls off to zero again for large times. To ﬁnd the maximum, we set
d
dt

t−3/2e−T/t
= t−5/2e−T/t
T
t −3
2

= 0,
(9.121)
from which we ﬁnd tmax = 2T/3 = r2/6a2. The temperature maximum arrives at later
times at larger distances from the origin.
■
In the case of cylindrical symmetry (in the plane z = 0 in plane polar coordinates ρ =
p
x2 + y2,ϕ), we look for a temperature ψ = u(ρ,t) that then satisﬁes the ODE (using
Eq. (2.35) in the diffusion equation)
∂u
∂t = a2
∂2u
∂ρ2 + 1
ρ
∂u
∂ρ

,
(9.122)
which is the planar analog of Eq. (9.118). This ODE also has solutions with the functional
dependence ρ/√t ≡r. Upon substituting
u = v
 ρ
√t

,
∂u
∂t = −ρv′
2t3/2 ,
∂u
∂ρ = v′
√t ,
∂2u
∂ρ2 = v′
t
(9.123)
into Eq. (9.122) with the notation v′ ≡dv/dr, we ﬁnd the ODE
a2v′′ +
a2
r + r
2

v′ = 0.
(9.124)
This is a ﬁrst-order ODE for v′, which we can integrate when we separate the variables v
and r as
v′′
v′ = −
1
r + r
2a2

.
(9.125)
This yields
v(r) = C
r e−r2/4a2 = C
√t
ρ e−ρ2/4a2t.
(9.126)

444
Chapter 9 Partial Differential Equations
This special solution for cylindrical symmetry can be similarly generalized and adapted to
boundary conditions, as for the spherical case. Finally, the z-dependence can be factored
in, because z separates from the plane polar radial variable ρ.
Exercises
9.7.1
For a homogeneous spherical solid with constant thermal diffusivity, K, and no heat
sources, the equation of heat conduction becomes
∂T (r,t)
∂t
= K∇2T (r,t).
Assume a solution of the form
T = R(r)T (t)
and separate variables. Show that the radial equation may take on the standard form
r2 d2R
dr2 + 2r d R
dr + α2r2R = 0,
and that sinαr/r and cosαr/r are its solutions.
9.7.2
Separate variables in the thermal diffusion equation of Exercise 9.7.1 in circular cylin-
drical coordinates. Assume that you can neglect end effects and take T = T (ρ,t).
9.7.3
Solve the PDE
∂ψ
∂t = a2 ∂2ψ
∂x2 ,
to obtain ψ(x,t) for a rod of inﬁnite extent (in both the +x and −x directions), with a
heat pulse at time t = 0 that corresponds to ψ0(x) = Aδ(x).
9.7.4
Solve the same PDE as in Exercise 9.7.3 for a rod of length L, with position on the rod
given by the variable x, with the two ends of the rod at x = 0 and x = L kept (at all
times t) at the respective temperatures T = 1 and T = 0, and with the rod initially at
T (x) = 0, for 0 < x ≤L.
9.8
SUMMARY
This chapter has provided an overview of methods for the solution of ﬁrst- and second-
order linear PDEs, with emphasis on homogeneous second-order PDEs subject to bound-
ary conditions that either determine unique solutions or deﬁne eigenvalue problems. We
found that the usual boundary conditions are identiﬁed as of Dirichlet type (solution spec-
iﬁed on boundary), Neumann type (normal derivative of solution speciﬁed on boundary),
or Cauchy type (both solution and its normal derivative speciﬁed). Applicable types of
boundary conditions depend on the classiﬁcation of the PDE; second-order PDEs are clas-
siﬁed as hyperbolic (e.g., wave equation), elliptic (e.g., Laplace equation), or parabolic
(e.g., heat/diffusion equation).

Additional Readings
445
The method of widest applicability to the solution of PDEs is the method of separation
of variables, which, when effective, reduces a PDE to a set of ODEs. The chapter has pre-
sented a very small number of complete PDE solutions to illustrate the technique. A wider
variety of examples only becomes possible when we are prepared to exploit the proper-
ties of the special functions that are the solutions of various ODEs, and, as a result, fuller
illustration of PDE solutions will be provided in the chapters that discuss these special
functions. We point out, in particular, that general PDEs with spherical symmetry all have
the same angular solutions, known as spherical harmonics. These, and the functions from
which they are constructed (Legendre polynomials and associated Legendre functions),
are the subject matter of Chapters 15 and 16. Some spherically symmetric problems have
radial solutions that can be identiﬁed as spherical Bessel functions; these are treated in
the Bessel function chapter (Chapter 14).
PDE problems with cylindrical symmetry usually involve Bessel functions, often in
ways more complex than in the examples of the present chapter. Further illustrations appear
in Chapter 14.
This chapter has not attempted to discuss methods for the solution of inhomogeneous
PDEs. That topic deserves its own chapter, and will be developed in Chapter 10.
Finally, we repeat an earlier observation: Fourier expansions (Chapter 19) and integral
transforms (Chapter 20) can also have a role in the solution of PDEs, and applications of
these techniques to PDEs are included in the appropriate chapters of this book.
Additional Readings
Bateman, H., Partial Differential Equations of Mathematical Physics. New York: Dover (1944), 1st ed. (1932).
A wealth of applications of various partial differential equations in classical physics. Excellent examples of
the use of different coordinate systems, including ellipsoidal, paraboloidal, toroidal coordinates, and so on.
Cohen, H., Mathematics for Scientists and Engineers. Englewood Cliffs, NJ: Prentice-Hall (1992).
Folland, G. B., Introduction to Partial Differential Equations, 2nd ed. Princeton, NJ: Princeton University Press
(1995).
Guckenheimer, J., P. Holmes, and F. John, Nonlinear Oscillations, Dynamical Systems and Bifurcations of Vector
Fields, revised ed. New York: Springer-Verlag (1990).
Gustafson, K. E., Partial Differential Equations and Hilbert Space Methods, 2nd ed., New York: Wiley (1987),
reprinting Dover (1998).
Margenau, H., and G. M. Murphy, The Mathematics of Physics and Chemistry, 2nd ed. Princeton, NJ: Van
Nostrand (1956). Chapter 5 covers curvilinear coordinates and 13 speciﬁc coordinate systems.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Chapter 5
includes a description of several different coordinate systems. Note that Morse and Feshbach are not above
using left-handed coordinate systems even for Cartesian coordinates. Elsewhere in this excellent (and difﬁcult)
book are many examples of the use of the various coordinate systems in solving physical problems. Chapter 6
discusses characteristics in detail.

CHAPTER 10
GREEN’S FUNCTIONS
In contrast to the linear differential operators that have been our main concern when
formulating problems as differential equations, we now turn to methods involving inte-
gral operators, and in particular to those known as Green’s functions. Green’s-function
methods enable the solution of a differential equation containing an inhomogeneous term
(often called a source term) to be related to an integral operator containing the source. As
a preliminary and elementary example, consider the problem of determining the potential
ψ(r) generated by a charge distribution whose charge density is ρ(r). From the Poisson
equation, we know that ψ(r) satisﬁes
−∇2ψ(r) = 1
ε0
ρ(r).
(10.1)
We also know, applying Coulomb’s law to the potential at r1 produced by each element of
charge ρ(r2)d3r2, and assuming the space is empty except for the charge distribution, that
ψ(r1) =
1
4πε0
Z
d3r2
ρ(r2)
|r1 −r2|.
(10.2)
Here the integral is over the entire region where ρ(r2) ̸= 0. We can view the right-hand
side of Eq. (10.2) as an integral operator that converts ρ into ψ, and identify the kernel
(the function of two variables, one of which is to be integrated) as the Green’s function for
this problem. Thus, we write
G(r1,r2) =
1
4πε
1
|r1 −r2|,
(10.3)
ψ(r1) =
Z
d3r2 G(r1,r2)ρ(r2),
(10.4)
assigning our Green’s function the symbol G (for “Green”).
447
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

448
Chapter 10 Green’s Functions
This example is preliminary because the response of more general problems to an
inhomogeneous term will depend on the boundary conditions. For example, an electro-
statics problem may include conductors whose surfaces will contain charge layers with
magnitudes that depend on ρ and which will also contribute to the potential at general r.
It is elementary because the form of the Green’s function will also depend on the differen-
tial equation to be solved, and often it will not be possible to obtain a Green’s function in
a simple, closed form.
The essential feature of any Green’s function is that it provides a way to describe the
response of the differential-equation solution to an arbitrary source term (in the presence
of the boundary conditions). In our present example, G(r1,r2) gives us the contribution
to ψ at the point r1 produced by a point source of unit magnitude (a delta function) at the
point r2. The fact that we can determine ψ everywhere by an integration is a consequence
of the fact that our differential equation is linear, so each element of the source contributes
additively. In the more general context of a PDE that depends on both spatial and time
coordinates, Green’s functions also appear as responses of the PDE solution to impulses at
given positions and times.
The aim of this chapter is to identify some general properties of Green’s functions, to
survey methods for ﬁnding them, and to begin building connections between differential-
operator and integral-operator methods for the description of physics problems. We start
by considering problems in one dimension.
10.1
ONE-DIMENSIONAL PROBLEMS
Let’s consider the second-order self-adjoint inhomogeneous ODE
Ly ≡d
dx

p(x) dy
dx

+ q(x) y = f (x),
(10.5)
which is to be satisﬁed on the range a ≤x ≤b subject to homogeneous boundary condi-
tions at x = a and x = b that will cause L to be Hermitian.1 Our Green’s function for this
problem needs to satisfy the boundary conditions and the ODE
LG(x,t) = δ(x −t),
(10.6)
so that y(x), the solution to Eq. (10.5) with its boundary conditions, can be obtained as
y(x) =
b
Z
a
G(x,t) f (t)dt.
(10.7)
To verify Eq. (10.7), simply apply L:
Ly(x) =
b
Z
a
L G(x,t) f (t)dt =
b
Z
a
δ(x −t) f (t)dt = f (x).
1A homogeneous boundary condition is one that continues to be satisﬁed if the function satisfying it is multiplied by a scale
factor. Most of the more commonly encountered types of boundary conditions are homogeneous, e.g., y = 0, y′ = 0, even
c1y + c2y′ = 0. However, y = c with c a nonzero constant is not homogeneous.

10.1 One-Dimensional Problems
449
General Properties
To gain an understanding of the properties G(x,t) must have, we ﬁrst consider the result
of integrating Eq. (10.6) over a small range of x that includes x = t. We have
t+ε
Z
t−ε
d
dx

p(x)dG(x,t)
dx

dx +
t+ε
Z
t−ε
q(x) G(x,t)dx =
t+ε
Z
t−ε
δ(t −x)dx,
which, carrying out some of the integrations, simpliﬁes to
p(x)dG(x,t)
dx

t+ε
t−ε
+
t+ε
Z
t−ε
q(x) G(x,t)dx = 1.
(10.8)
It is clear that Eq. (10.8) cannot be satisﬁed in the limit of small ε if G(x,t) and
dG(x,t)/dx are both continuous (in x) at x = t, but we can satisfy that equation if we
require G(x,t) to be continuous but accept a discontinuity in dG(x,t)/dx at x = t. In
particular, continuity in G will cause the integral containing q(x) to vanish in the limit
ε →0, and we are left with the requirement
lim
ε→0+
"
dG(x,t)
dx

x=t+ε
−dG(x,t)
dx

x=t−ε
#
=
1
p(t).
(10.9)
Thus, the discontinuous impulse at x = t leads to a discontinuity in the x derivative of
G(x,t) at that x value. Note, however, that because of the integration in Eq. (10.7), the
singularity in dG/dx does not lead to a similar singularity in the overall solution y(x) in
the usual case that f (x) is continuous.
As a next step toward reaching understanding of the properties of Green’s functions, let’s
expand G(x,t) in the eigenfunctions of our operator L, obtained subject to the boundary
conditions already identiﬁed. Since L is Hermitian, its eigenfunctions can be chosen to be
orthonormal on (a,b), with
Lϕn(x) = λnϕn(x),
⟨ϕn|ϕm⟩= δnm.
(10.10)
Expanding both the x and the t dependence of G(x,t) in this orthonormal set (using the
complex conjugates of the ϕn for the t expansion),
G(x,t) =
X
nm
gnmϕn(x)ϕ∗
m(t).
(10.11)
We also expand δ(x −t) in the same orthonormal set, according to Eq. (5.27):
δ(x −t) =
X
m
ϕm(x)ϕ∗
m(t).
(10.12)
Inserting both these expansions into Eq. (10.6), we have before any simpliﬁcation
L
X
nm
gnmϕn(x)ϕ∗
m(t) =
X
m
ϕm(x)ϕ∗
m(t).
(10.13)

450
Chapter 10 Green’s Functions
Applying L, which operates only on ϕn(x), Eq. (10.13) reduces to
X
nm
λngnmϕn(x)ϕ∗
m(t) =
X
m
ϕm(x)ϕ∗
m(t).
Taking scalar products in the x and t domains, we ﬁnd that gnm = δnm/λn, so G(x,t) must
have the expansion
G(x,t) =
X
n
ϕ∗
n(t)ϕn(x)
λn
.
(10.14)
The above analysis fails in the case that any λn is zero, but we shall not pursue that special
case further.
The importance of Eq. (10.14) does not lie in its dubious value as a computational tool,
but in the fact that it reveals the symmetry of G:
G(x,t) = G(t, x)∗.
(10.15)
Form of Green’s Function
The properties we have identiﬁed for G are sufﬁcient to enable its more complete identiﬁ-
cation, given a Hermitian operator L and its boundary conditions. We continue with the
study of problems on an interval (a,b) with one homogeneous boundary condition at each
endpoint of the interval.
Given a value of t, it is necessary for x in the range a ≤x < t that G(x,t) have an x
dependence y1(x) that is a solution to the homogeneous equation L = 0 and that also satis-
ﬁes the boundary condition at x = a. The most general G(x,t) satisfying these conditions
must have the form
G(x,t) = y1(x)h1(t),
(x < t),
(10.16)
where h1(t) is presently unknown. Conversely, in the range t < x ≤b, it is necessary that
G(x,t) have the form
G(x,t) = y2(x)h2(t),
(x > t),
(10.17)
where y2 is a solution of L = 0 that satisﬁes the boundary condition at x = b. The sym-
metry condition, Eq. (10.15), permits Eqs. (10.16) and (10.17) to be consistent only if
h∗
2 = A y1 and h∗
1 = A y2, with A a constant that is still to be determined. Assuming that
y1 and y2 can be chosen to be real, we are led to the conclusion that
G(x,t) =
( A y1(x)y2(t),
x < t,
A y2(x)y1(t),
x > t,
(10.18)
where Lyi = 0, with y1 satisfying the boundary condition at x = a and y2 satisfying that at
x = b. The value of A in Eq. (10.18) depends, of course, on the scale at which the yi have
been speciﬁed, and must be set to a value that is consistent with Eq. (10.9). As applied
here, that condition reduces to
A
h
y′
2(t)y1(t) −y′
1(t)y2(t)
i
=
1
p(t),

10.1 One-Dimensional Problems
451
equivalent to
A =
 p(t)

[y′
2(t)y1(t) −y′
1(t)y2(t)
−1 .
(10.19)
Despite its appearance, A does not depend on t. The expression involving the yi is their
Wronskian, and it has a value proportional to 1/p(t). See Exercise 7.6.11.
It is instructive to verify that the form for G(x,t) given by Eq. (10.18) causes Eq. (10.7)
to generate the desired solution to the ODE Ly = f. To this end, we obtain an explicit form
for y(x):
y(x) = A y2(x)
x
Z
a
y1(t) f (t)dt + A y1(x)
b
Z
x
y2(t) f (t)dt.
(10.20)
From Eq. (10.20) it is easy to verify that the boundary conditions on y(x) are satisﬁed; if
x = a the ﬁrst of the two integrals vanishes, and the second is proportional to y1; corre-
sponding remarks apply at x = b.
It remains to show that Eq. (10.20) yields Ly = f. Differentiating with respect to x, we
ﬁrst have
y′(x) = A y′
2(x)
x
Z
a
y1(t) f (t)dt + A y2(x)y1(x) f (x)
+ A y′
1(x)
b
Z
x
y2(t) f (t)dt −A y1(x)y2(x) f (x)
= A y′
2(x)
x
Z
a
y1(t) f (t)dt + A y′
1(x)
b
Z
x
y2(t) f (t)dt.
(10.21)
Proceeding to (py′)′:
h
p(x)y′(x)
i ′
= A
h
p(x)y′
2(x)
i ′
x
Z
a
y1(t) f (t)dt + A
h
p(x)y′
2(x)
i
y1(x) f (x)
+ A
h
p(x)y′
1(x)
i ′
b
Z
x
y2(t) f (t)dt −A
h
p(x)y′
1(x)
i
y2(x) f (x). (10.22)
Combining Eq. (10.22) and q(x) times Eq. (10.20), many terms drop because Ly1 =
Ly2 = 0, leaving
Ly(x) = A p(x)
h
y′
2(x)y1(x) −y′
1(x)y2(x)
i
f (x) = f (x),
(10.23)
where the ﬁnal simpliﬁcation took place using Eq. (10.19).

452
Chapter 10 Green’s Functions
Example 10.1.1
SIMPLE SECOND-ORDER ODE
Consider the ODE
−y′′ = f (x),
with boundary conditions y(0) = y(1) = 0. The corresponding homogeneous equation
−y′′ = 0 has general solution y0 = c0 + c1x; from these we construct the solution y1 = x
that satisﬁes y1(0) = 0 and the solution y2 = 1 −x, satisfying y2(1) = 0. For this ODE,
the coefﬁcient p(x) = −1, y′
1(x) = 1, y′
2(x) = −1, and the constant A in the Green’s
function is
A =
h
(−1)[(−1)(x) −(1)(1 −x)]
i −1
= 1.
Our Green’s function is therefore
G(x,t) =
(x(1 −t),
0 ≤x < t,
t(1 −x),
t < x ≤1.
Assuming we can perform the integral, we can now solve this ODE with boundary condi-
tions for any function f (x). For example, if f (x) = sinπx, our solution would be
y(x) =
1
Z
0
G(x,t) sinπt dt = (1 −x)
x
Z
0
t sinπt dt + x
1
Z
x
(1 −t)sinπt dt
= 1
π2 sinπx.
The correctness of this result is easily checked.
One advantage of the Green’s function formalism is that we do not need to repeat most
of our work if we change the function f (x). If we now take f (x) = cosπx, we get
y(x) = 1
π2

2x −1 + cosπx

.
Note that our solution takes full account of the boundary conditions.
■
Other Boundary Conditions
Occasionally one encounters problems other than the Hermitian second-order ODEs we
have been considering. Some, but not always all of the Green’s-function properties we
have identiﬁed, carry over to such problems.
Consider ﬁrst the possibility that we may have nonhomogeneous boundary conditions,
such as the problem Ly = f with y(a) = c1 and y(b) = c2, with one or both ci nonzero.
This problem can be converted into one with homogeneous boundary conditions by making
a change of the dependent variable from y to
u = y −c1(b −x) + c2(x −a)
b −a
.

10.1 One-Dimensional Problems
453
In terms of u, the boundary conditions are homogeneous: u(a) = u(b) = 0. A nonhomo-
geneous condition on the derivative, e.g., y′(a) = c, can be treated analogously.
Another possibility for a second-order ODE is that we may have two boundary condi-
tions at one endpoint and none at the other; this situation corresponds to an initial-value
problem, and has lost the close connection to Sturm-Liouville eigenvalue problems. The
result is that Green’s functions can still be constructed by invoking the condition of conti-
nuity in G(x,t) at x = t and the prescribed discontinuity in ∂G/∂x, but they will no longer
be symmetric.
Example 10.1.2
INITIAL VALUE PROBLEM
Consider
Ly = d2y
dx2 + y = f (x),
(10.24)
with the initial conditions y(0) = 0 and y′(0) = 0. This operator L has p(x) = 1.
We start by noting that the homogeneous equation Ly = 0 has the two linearly indepen-
dent solutions y1 = sin x and y2 = cos x. However, the only linear combination of these
solutions that satisﬁes the boundary condition at x = 0 is the trivial solution y = 0, so our
Green’s function for x < t can only be G(x,t) = 0. On the other hand, for the region x > t
there are no boundary conditions to serve as constraints, and in that region we are free to
write
G(x,t) = C1(t)y1 + C2(t)y2,
or
G(x,t) = C1(t)sin x + C2(t)cos x,
x > t.
We now impose the requirements
G(t−,t) = G(t+,t) −→0 = C1(t)sint + C2(t)cost,
∂G
∂x (t+,t) −∂G
∂x (t−,t) =
1
p(t) = 1 −→C1(t)cost −C2(t)sint −(0) = 1.
These equations can now be solved, yielding C1(t) = cost, C2(t) = −sint, so for x > t
G(x,t) = cost sin x −sint cos x = sin(x −t).
Thus, the complete speciﬁcation of G(x,t) is
G(x,t) =
(0,
x < t,
sin(x −t),
x > t.
(10.25)
The lack of correspondence to a Sturm-Liouville problem is reﬂected in the lack of sym-
metry of the Green’s function. Nevertheless, the Green’s function can be used to construct

454
Chapter 10 Green’s Functions
the solution to Eq. (10.24) subject to its initial conditions:
y(x) =
∞
Z
0
G(x,t) f (t)dt
=
x
Z
0
sin(x −t) f (t)dt.
(10.26)
Note that if we regard x as a time variable, our solution at “time” x is only inﬂuenced by
source contributions from times t prior to x, so Eq. (10.24) obeys causality.
We conclude this example by observing that we can verify that y(x) as given by
Eq. (10.26) is the correct solution to our problem. Details are left as Exercise 10.1.3.
■
Example 10.1.3
BOUNDARY AT INFINITY
Consider
 d2
dx2 + k2

ψ(x) = g(x),
(10.27)
an equation essentially similar to one we have already studied several times, but now with
boundary conditions that correspond (when multiplied by e−iωt) to an outgoing wave.
The general solution to Eq. (10.27) with g = 0 is spanned by the two functions
y1 = e−ikx
and
y2 = e+ikx.
The outgoing wave boundary condition means that for large positive x we must have the
solution y2, while for large negative x the solution must be y1. This information sufﬁces
to indicate that the Green’s function for this problem must have the form
G(x, x′) =
( Ay1(x′)y2(x),
x > x′,
Ay2(x′)y1(x),
x < x′.
We ﬁnd the coefﬁcient A from Eq. (10.19), in which p(x) = 1:
A =
1
y′
2(x)y1(x) −y′
1(x)y2(x) =
1
ik + ik = −i
2k .
Combining these results, we reach
G(x, x′) = −i
2k exp

i|x −x′|

.
(10.28)
This result is yet another illustration that the Green’s function depends on boundary con-
ditions as well as on the differential equation.
Veriﬁcation that this Green’s function yields the desired problem solution is the topic of
Exercise 10.1.8.
■

10.1 One-Dimensional Problems
455
Relation to Integral Equations
Consider now an eigenvalue equation of the form
Ly(x) = λy(x),
(10.29)
where we assume L to be self-adjoint and subject to the boundary conditions y(a) =
y(b) = 0. We can proceed formally by treating Eq. (10.29) as an inhomogeneous equa-
tion whose right-hand side is the particular function λy(x). To do so, we would ﬁrst ﬁnd
the Green’s function G(x,t) for the operator L and the given boundary conditions, after
which, as in Eq. (10.7), we could write
y(x) = λ
b
Z
a
G(x,t) y(t)dt.
(10.30)
Equation (10.30) is not a solution to our eigenvalue problem, since the unknown function
y(x) appears on both sides and, moreover, it does not tell us the possible values of the
eigenvalue λ. What we have accomplished, however, is to convert our eigenvalue ODE
and its boundary conditions into an integral equation which we can regard as an alternate
starting point for solution of our eigenvalue problem.
Our generation of Eq. (10.30) shows that it is implied by Eq. (10.29). If we can also
show that we can connect these equations in the reverse order, namely that Eq. (10.30)
implies Eq. (10.29), we can then conclude that they are equivalent formulations of the
same eigenvalue problem. We proceed by applying L to Eq. (10.30), labeling it Lx to
make clear that it is an operator on x, not t:
Lx y(x) = λLx
b
Z
a
G(x,t)y(t)dt
= λ
b
Z
a
LxG(x,t)y(t)dt = λ
b
Z
a
δ(x −t)y(t)dt
= λy(x).
(10.31)
The above analysis shows that under rather general circumstances we will be able to
convert an eigenvalue equation based on an ODE into an entirely equivalent eigenvalue
equation based on an integral equation. Note that to specify completely the ODE eigen-
value equation we had to make an explicit identiﬁcation of the accompanying boundary
conditions, while the corresponding integral equation appears to be entirely self-contained.
Of course, what has happened is that the effect of the boundary conditions has inﬂuenced
the speciﬁcation of the Green’s function that is the kernel of the integral equation.
Conversion to an integral equation may be useful for two reasons, the more practical
of which is that the integral equation may suggest different computational procedures for
solution of our eigenvalue problem. There is also a fundamental mathematical reason why
an integral-equation formulation may be preferred: It is that integral operators, such as that
in Eq. (10.30), are bounded operators (meaning that their application to a function y of

456
Chapter 10 Green’s Functions
ﬁnite norm produces a result whose norm is also ﬁnite). On the other hand, differential
operators are unbounded; their application to a function of ﬁnite norm can produce a result
of unbounded norm. Stronger theorems can be developed for operators that are bounded.
We close by making the now obvious observation that Green’s functions provide the
link between differential-operator and integral-operator formulations of the same problem.
Example 10.1.4
DIFFERENTIAL VS. INTEGRAL FORMULATION
Here we return to an eigenvalue problem we have already treated several times in various
contexts, namely
−y′′(x) = λy(x),
subject to boundary conditions y(0) = y(1) = 0. In Example 10.1.1 we found the Green’s
function for this problem to be
G(x,t) =
(x(1 −t),
0 ≤x < t,
t(1 −x),
t < x ≤1,
and, following Eq. (10.30), our eigenvalue problem can be rewritten as
y(x) = λ
1
Z
0
G(x,t) y(t)dt.
(10.32)
Methods for solution of integral equations will not be discussed until Chapter 21, but we
can easily verify that the well-known solution set for this problem,
y = sinnπx,
λn = n2π2,
n = 1, 2, ... ,
also solves Eq. (10.32).
■
Exercises
10.1.1
Show that
G(x,t) =
(x,
0 ≤x < t,
t,
t < x ≤1,
is the Green’s function for the operator L = −d2/dx2 and the boundary conditions
y(0) = 0, y′(1) = 0.
10.1.2
Find the Green’s function for
(a)
Ly(x) = d2y(x)
dx2
+ y(x),
(
y(0) = 0,
y′(1) = 0.
(b)
Ly(x) = d2y(x)
dx2
−y(x),
y(x) ﬁnite for −∞< x < ∞.

10.1 One-Dimensional Problems
457
10.1.3
Show that the function y(x) deﬁned by Eq. (10.26) satisﬁes the initial-value problem
deﬁned by Eq. (10.24) and its initial conditions y(0) = y′(0) = 0.
10.1.4
Find the Green’s function for the equation
−d2y
dx2 −y
4 = f (x),
with boundary conditions y(0) = y(π) = 0.
ANS.
G(x,t) =
(2sin(x/2)cos(t/2),
0 ≤x < t,
2cos(x/2)sin(t/2),
t < x ≤π.
10.1.5
Construct the Green’s function for
x2 d2y
dx2 + x dy
dx + (k2x2 −1)y = 0,
subject to the boundary conditions y(0) = 0, y(1) = 0.
10.1.6
Given that
L = (1 −x2) d2
dx2 −2x d
dx
and that G(±1,t) remains ﬁnite, show that no Green’s function can be constructed by
the techniques of this section.
Note. The solutions to L = 0 needed for the regions x < t and x > t are linearly depen-
dent.
10.1.7
Find the Green’s function for
d2ψ
dt2 + k dψ
dt = f (t),
subject to the initial conditions ψ(0) = ψ′(0) = 0, and solve this ODE for t > 0 given
f (t) = exp(−t).
10.1.8
Verify that the Green’s function
G(x, x′) = −i
2k exp

ik|x −x′|

yields an outgoing wave solution to the ODE
 d2
dx2 + k2

ψ(x) = g(x).
Note. Compare with Example 10.1.3.
10.1.9
Construct the 1-D Green’s function for the modiﬁed Helmholtz equation,
 d2
dx2 −k2

ψ(x) = f (x).

458
Chapter 10 Green’s Functions
The boundary conditions are that the Green’s function must vanish for x →∞and
x →−∞.
ANS.
G(x1, x2) = −1
2k exp

−k|x1 −x2|

.
10.1.10
From the eigenfunction expansion of the Green’s function show that
(a)
2
π2
∞
X
n=1
sin nπx sin nπt
n2
=
(x(1 −t),
0 ≤x < t,
t(1 −x),
t < x ≤1.
(b)
2
π2
∞
X
n=0
sin(n + 1
2)πx sin(n + 1
2)πt
(n + 1
2)2
=
(x,
0 ≤x < t,
t,
t < x ≤1.
10.1.11
Derive an integral equation corresponding to
y′′(x) −y(x) = 0,
y(1) = 1,
y(−1) = 1,
(a)
by integrating twice.
(b)
by forming the Green’s function.
ANS.
y(x) = 1 −
1
Z
−1
K(x,t) y(t)dt,
K(x,t) =
( 1
2(1 −x)(t + 1),
x > t,
1
2(1 −t)(x + 1),
x < t.
10.1.12
The general second-order linear ODE with constant coefﬁcients is
y′′(x) + a1y′(x) + a2y(x) = 0.
Given the boundary conditions y(0) = y(1) = 0, integrate twice and develop the inte-
gral equation
y(x) =
1
Z
0
K(x,t) y(t)dt,
with
K(x,t) =
(a2t(1 −x) + a1(x −1),
t < x,
a2x(1 −t) + a1x,
x < t.
Note that K(x,t) is symmetric and continuous if a1 = 0. How is this related to self-
adjointness of the ODE?
10.1.13
Transform the ODE
d2y(r)
dr2
−k2y(r) + V0
e−r
r
y(r) = 0

10.2 Problems in Two and Three Dimensions
459
and the boundary conditions y(0) = y(∞) = 0 into an integral equation of the form
y(r) = −V0
∞
Z
0
G(r,t) e−t
t
y(t)dt.
The quantities V0 and k2 are constants. The ODE is derived from the Schrödinger wave
equation with a mesonic potential:
G(r,t) =



−1
k e−kt sinhkr,
0 ≤r < t,
−1
k e−kr sinhkt,
t < r < ∞.
10.2
PROBLEMS IN TWO AND THREE DIMENSIONS
Basic Features
The principles, but unfortunately not all the details of our analysis of Green’s functions in
one dimension, extend to problems of higher dimensionality. We summarize here proper-
ties of general validity for the case where L is a linear second-order differential operator
in two or three dimensions.
1.
A homogeneous PDE Lψ(r1) = 0 and its boundary conditions deﬁne a Green’s
function G(r1,r2), which is the solution of the PDE
LG(r1,r2) = δ(r1 −r2)
subject to the relevant boundary conditions.
2.
The inhomogeneous PDE Lψ(r) = f (r) has, subject to the boundary conditions of
Item 1, the solution
ψ(r1) =
Z
G(r1,r2) f (r2)d3r2,
where the integral is over the entire space relevant to the problem.
3.
When L and its boundary conditions deﬁne the Hermitian eigenvalue problem
Lψ = λψ with eigenfunctions ϕn(r) and corresponding eigenvalues λn, then
•
G(r1,r2) is symmetric, in the sense that
G(r1,r2) = G∗(r2,r1), and
•
G(r1,r2) has the eigenfunction expansion
G(r1,r2) =
X
n
ϕ∗
n(r2)ϕn(r1)
λn
.

460
Chapter 10 Green’s Functions
4.
G(r1,r2) will be continuous and differentiable at all points such that r1 ̸= r2. We
cannot even require continuity in a strict sense at r1 = r2 (because our Green’s func-
tion may become inﬁnite there), but we can have the weaker condition that G remain
continuous in regions that surround, but do not include r1 = r2. G must have more
serious singularities in its ﬁrst derivatives, so that the second-order derivatives in L
will generate the delta-function singularity characteristic of G and speciﬁed in Item 1.
What does not carry over from the 1-D case are the explicit formulas we used to con-
struct Green’s functions for a variety of problems.
Self-Adjoint Problems
In more than one dimension, a second-order differential equation is self-adjoint if it has
the form
Lψ(r) = ∇·
h
p(r)∇ψ(r)
i
+ q(r)ψ(r) = f (r),
(10.33)
with p(r) and q(r) real. This operator will deﬁne a Hermitian problem if its boundary
conditions are such that ⟨ϕ|Lψ⟩= ⟨Lϕ|ψ⟩. See Exercise 10.2.2.
Assuming we have a Hermitian problem, consider the scalar product
D
G(r,r1)
LG(r,r2)
E
=
D
LG(r,r1)
G(r,r2)
E
.
(10.34)
Here the scalar product and L both refer to the variable r, and the Hermitian property is
responsible for this equality. The points r1 and r2 are arbitrary. Noting that LG results in
a delta function, we have, from the left-hand side of Eq. (10.34),
D
G(r,r1)
LG(r,r2)
E
=
D
G(r,r1)
δ(r −r2)
E
= G∗(r2,r1).
(10.35)
But, from the right-hand side of Eq. (10.34),
D
LG(r,r1)
G(r,r2)
E
=
D
δ(r −r1)
G(r,r2)
E
= G(r1,r2).
(10.36)
Substituting Eqs. (10.35) and (10.36) into Eq. (10.34), we recover the symmetry condition
G(r1,r2) = G∗(r2,r1).
Eigenfunction Expansions
We already saw, in 1-D Hermitian problems, that the Green’s function of a Hermitian
problem can be written as an eigenfunction expansion. If L, with its boundary conditions,
has normalized eigenfunctions ϕn(r) and corresponding eigenvalues λn, our expansion
took the form
G(r1,r2) =
X
n
ϕ∗
n(r2)ϕn(r1)
λn
.
(10.37)
It turns out to be useful to consider the somewhat more general equation
Lψ(r1) −λψ(r1) = δ(r2 −r1),
(10.38)

10.2 Problems in Two and Three Dimensions
461
where λ is a parameter (not an eigenvalue of L). In this more general case, an expansion in
the ϕn yields for the Green’s function of the entire left-hand side of Eq. (10.38) the formula
G(r1,r2) =
X
n
ϕ∗
n(r2)ϕn(r1)
λn −λ
.
(10.39)
Note that Eq. (10.39) will be well-deﬁned only if the parameter λ is not equal to any of the
eigenvalues of L.
Form of Green’s Functions
In spaces of more than one dimension, we cannot divide the region under consideration
into two intervals, one on each side of a point (here designated r2), then choosing for
each interval a solution to the homogeneous equation appropriate to its outer boundary.
A more fruitful approach will often be to obtain a Green’s function for an operator L
subject to some particularly convenient boundary conditions, with a subsequent plan to
add to it whatever solution to the homogeneous equation Lψ(r) = 0 that may be needed
to adapt to the boundary conditions actually under consideration. This approach is clearly
legitimate, as the addition of any solution to the homogeneous equation will not affect the
(dis)continuity properties of the Green’s function.
We consider ﬁrst the Laplace operator in three dimensions, with the boundary condition
that G vanish at inﬁnity. We therefore seek a solution to the inhomogeneous PDE
∇2
1G(r1,r2) = δ(r1 −r2)
(10.40)
with limr1→∞G(r1,r2) = 0. We have added a subscript “1” to ∇to remind the reader that
it operates on r1 and not on r2. Since our boundary conditions are spherically symmetric
and at an inﬁnite distance from r1 and r2, we may make the simplifying assumption that
G(r1,r2) is a function only of r12 = |r1 −r2|.
Our ﬁrst step in processing Eq. (10.40) is to integrate it over a spherical volume of radius
a centered at r2:
Z
r12<a
∇1 · ∇1G(r1,r2)d3r1 = 1,
(10.41)
where we have reduced the right-hand side using the properties of the delta function and
written the left-hand side in a form making it ready for the application of Gauss’ theorem.
We now apply that theorem to the left-hand side of Eq. (10.41), reaching
Z
r12=a
∇1G(r1,r2) · dσ 1 = 4πa2 dG
dr12

r12=a
= 1.
(10.42)
Since Eq. (10.42) must be satisﬁed for all values of a, it is necessary that
d
dr12
G(r1,r2) =
1
4πr2
12
,

462
Chapter 10 Green’s Functions
which can be integrated to yield
G(r1,r2) = −1
4π
1
|r1 −r2|.
(10.43)
We do not need to add a constant of integration because this form for G vanishes at inﬁnity.
At this point it may be useful to note that the sign of G(r1,r2) depends on the sign asso-
ciated with the differential operator of which it is a Green’s function. Some texts (including
previous editions of this book) have deﬁned G as produced by a negative delta function
so that Eq. (10.43) when associated with +∇2 would not need a minus sign. There is,
of course, no ambiguity in any physical results because a change in the sign of G must
be accompanied by a change in the sign of the integral in which G is combined with the
inhomogeneous term of a differential equation.
The Green’s function of Eq. (10.43) is only going to be appropriate for an inﬁnite system
with G = 0 at inﬁnity but, as mentioned already, it can be converted into the Green’s func-
tions of another problem by addition of a suitable solution to the homogeneous equation
(in this case, Laplace’s equation). Since that is a reasonable starting point for a variety
of problems, the form given in Eq. (10.43) is sometimes called the fundamental Green’s
function of Laplace’s equation (in three dimensions).
Let’s now repeat our analysis for the Laplace operator in two dimensions for a region
of inﬁnite extent, using circular coordinates ρ = (ρ,ϕ). The integral in Eq. (10.41) is then
over a circular area, and the 2-D analog of Eq. (10.42) becomes
Z
ρ12=a
∇1G(ρ1,ρ2) · dσ 1 = 2πa dG
dρ12

ρ12=a
= 1,
leading to
d
dρ12
G(ρ1,ρ2) =
1
2πρ12
,
which has the indeﬁnite integral
G(ρ1,ρ2) = 1
2π ln|ρ1 −ρ2|.
(10.44)
The form given in Eq. (10.44) becomes inﬁnite at inﬁnity, but it nevertheless can be
regarded as a fundamental 2-D Green’s function. However, note that we will generally
need to add to it a suitable solution to the 2-D Laplace equation to obtain the form needed
for speciﬁc problems.
The above analysis indicates that the Green’s function for the Laplace equation in 2-D
space is rather different than the 3-D result. This observation illustrates the fact that there
is a real difference between ﬂatland (2-D) physics and actual (3-D) physics, even when the
latter is applied to problems with translational symmetry in one direction.
This is also a good time to note that the symmetry in the Green’s function corresponds
to the notion that a source at r2 produces a result (a potential) at r1 that is the same as the
potential at r2 from a similar source at r1. This property will persist in more complicated
problems so long as their deﬁnition makes them Hermitian.

10.2 Problems in Two and Three Dimensions
463
Table 10.1
Fundamental Green’s Functionsa
Laplace
Helmholtzb
Modiﬁed
∇2
∇2+ k2
Helmholtzc
∇2−k2
1-D
1
2 |x1 −x2|
−i
2k exp(ik|x1 −x2|)
−1
2k exp(−k|x1 −x2|)
2-D
1
2π ln|ρ1 −ρ2|
−i
4 H(1)
0
(k|ρ1 −ρ2|)
−1
2π K0(k|ρ1 −ρ2|)
3-D
−1
4π
1
|r1 −r2|
−exp(ik|r1 −r2|)
4π|r1 −r2|
−exp(−k|r1 −r2|)
4π|r1 −r2|
a Boundary conditions: For the Helmholtz equation, outgoing wave;
for modiﬁed Helmholtz and 3-D Laplace equations, G →0 at inﬁnity;
for 1-D and 2-D Laplace equation, arbitrary.
b H1
0 is a Hankel function, Section 14.4.
c K0 is a modiﬁed Bessel function, Section 14.5.
Because they occur rather frequently, it is useful to have Green’s functions for the
Helmholtz and modiﬁed Helmholtz equations in two and three dimensions (for one dimen-
sion these Green’s functions were introduced in Example 10.1.3 and Exercise 10.1.9). For
the Helmholtz equation, a convenient fundamental form results if we take boundary con-
ditions corresponding to an outgoing wave, meaning that the asymptotic r dependence
must be of the form exp(+ikr). For the modiﬁed Helmholtz equation, the most convenient
boundary condition (for one, two, and three dimensions) is that G decay to zero in all direc-
tions at large r. The one-, two-, and three-dimensional (3-D) fundamental Green’s functions
for the Laplace, Helmholtz, and modiﬁed Helmholtz operators are listed in Table 10.1.
We shall not derive here the forms of the Green’s functions for the Helmholtz equations;
in fact, for two dimensions, they involve Bessel functions and are best treated in detail in a
later chapter. However, for three dimensions, the Green’s functions are of relatively simple
form, and the veriﬁcation that they return correct results is the topic of Exercises 10.2.4
and 10.2.6. The fundamental Green’s function for the 1-D Laplace equation may not be
instantly recognizable in comparison to the formulas we derived in Section 10.1, but con-
sistency with our earlier analysis is the topic of Example 10.2.1
Sometimes it is useful to represent Green’s functions as expansions that take advantage
of the speciﬁc properties of various coordinate systems. The so-called spherical Green’s
function is the radial part of such an expansion in spherical polar coordinates. For the
Laplace operator, it takes a form developed in Eqs. (16.65) and (16.66). We write it here
only to show that it exhibits the two-region character that provides a convenient represen-
tation of the discontinuity in the derivative:
−1
4π
1
|r1 −r2| =
∞
X
l=0
2l + 1
4π
g(r1,r2)Pl(cosχ),

464
Chapter 10 Green’s Functions
where χ is the angle between r1 and r2, Pl is a Legendre polynomial, and the spherical
Green’s function g(r1,r2) is
gl(r1,r2) =



−
1
2l + 1
rl
1
rl+1
2
,
r1 < r2,
−
1
2l + 1
rl
2
rl+1
1
,
r1 > r2.
An explicit derivation of the formula for gl is given in Example 16.3.2.
In cylindrical coordinates (ρ,ϕ, z) one encounters an axial Green’s function gm(ρ1,ρ2),
in terms of which the fundamental Green’s function for the Laplace operator takes the form
(also involving a continuous parameter k)
G(r1,r2) = −1
4π
1
|r1 −r2|
=
1
2π2
∞
X
m=−∞
eim(ϕ1−ϕ2)
∞
Z
0
gm(kρ1,kρ2)cosk(z1 −z2)dk .
Here
gm(kρ1,kρ2) = −Im(kρ<)Km(kρ>),
where ρ< and ρ> are, respectively, the smaller and larger of ρ1 and ρ2. The quantities Im
and Km are modiﬁed Bessel functions, deﬁned in Chapter 14. This expansion is discussed
in more detail in Example 14.5.1. Again we note the two-region character.
Example 10.2.1
ACCOMMODATING BOUNDARY CONDITIONS
Let’s use the fundamental Green’s function of the 1-D Laplace equation,
d2ψ(x)
dx2
= 0,
namely
G(x1, x2) = 1
2 |x1 −x2|,
to illustrate how we can modify it to accommodate speciﬁc boundary conditions. We return
to the oft-used example with Dirichlet conditions ψ = 0 at x = 0 and x = 1. The continu-
ity of G and the discontinuity in its derivative are unaffected if we add to the above G one
or more terms of the form f (x1)g(x2), where f and g are solutions of the 1-D Laplace
equation, i.e., any functions of the form ax + b.
For the boundary conditions we have speciﬁed, the Green’s function we require has the
form
G(x1, x2) = −1
2(x1 + x2) + x1x2 + 1
2 |x1 −x2|.
The continuous and differentiable terms we have added to the fundamental form bring us
to the result
G(x1, x2) =
(
−1
2(x1 + x2) + x1x2 + 1
2(x2 −x1) = −x1(1 −x2),
x1 < x2,
−1
2(x1 + x2) + x1x2 + 1
2(x1 −x2) = −x2(1 −x1),
x2 < x1.
This result is consistent with what we found in Example 10.1.1.
■

10.2 Problems in Two and Three Dimensions
465
Example 10.2.2
QUANTUM MECHANICAL SCATTERING: BORN APPROXIMATION
The quantum theory of scattering provides a nice illustration of Green’s function tech-
niques and the use of the Green’s function to obtain an integral equation. Our physical
picture of scattering is as follows. A beam of particles moves along the negative z-axis
toward the origin. A small fraction of the particles is scattered by the potential V (r) and
goes off as an outgoing spherical wave. Our wave function ψ(r) must satisfy the time-
independent Schrödinger equation
−¯h2
2m ∇2ψ(r) + V (r)ψ(r) = Eψ(r),
(10.45)
or
∇2ψ(r) + k2ψ(r) =
2m
¯h2 V (r)ψ(r)

,
k2 = 2mE
¯h2 .
(10.46)
From the physical picture just presented we look for a solution having the asymptotic
form
ψ(r) ∼eik0·r + fk(θ,ϕ)eikr
r ,
(10.47)
where eik0·r is an incident plane wave2 with the propagation vector k0 carrying the sub-
script 0 to indicate that it is in the θ = 0 (z-axis) direction. The eikr/r term describes an
outgoing spherical wave with an angular and energy-dependent amplitude factor fk(θ,ϕ),3
and its 1/r radial dependence causes its asymptotic total ﬂux to be independent of r. This
is a consequence of the fact that the scattering potential V (r) becomes negligible at large r.
Equation (10.45) contains nothing describing the internal structure or possible motion of
the scattering center and therefore can only represent elastic scattering, so the propagation
vector of the incoming wave, k0, must have the same magnitude, k, as the scattered wave.
In quantum mechanics texts it is shown that the differential probability of scattering, called
the scattering cross section, is given by | fk(θ,ϕ|2.
We now need to solve Eq. (10.46) to obtain ψ(r) and the scattering cross section. Our
approach starts by writing the solution in terms of the Green’s function for the operator
on the left-hand side of Eq. (10.46), obtaining an integral equation because the inhomoge-
neous term of that equation has the form (2m/¯h2)V (r)ψ(r):
ψ(r1) =
Z 2m
¯h2 V (r2)ψ(r2) G(r1,r2)d3r2.
(10.48)
We intend to take the Green’s function to be the fundamental form given for the Helmholtz
equation in Table 10.1. We then recover the exp(ikr)/r part of the desired asymptotic
form, but the incident-wave term will be absent. We therefore modify our tentative for-
mula, Eq. (10.48), by adding to its right-hand side the term exp(ik0 · r), which is legiti-
mate because this quantity is a solution to the homogeneous (Helmholtz) equation. That
2For simplicity we assume a continuous incident beam. In a more sophisticated and more realistic treatment, Eq. (10.47) would
be one component of a wave packet.
3If V (r) represents a central force, fk will be a function of θ only, independent of the azimuthal angle ϕ.

466
Chapter 10 Green’s Functions
approach leads us to
ψ(r1) = eik0·r1 −
Z 2m
¯h2 V (r2)ψ(r2) eik|r1−r2|
4π|r1 −r2| d3r2.
(10.49)
This integral equation analog of the original Schrödinger wave equation is exact. It is
called the Lippmann-Schwinger equation, and is an important starting point for studies
of quantum-mechanical scattering phenomena.
We will later study methods for solving integral equations such as that in Eq. (10.49).
However, in the special case that the unscattered amplitude
ψ0(r1) = eik0·r1
(10.50)
dominates the solution, it is a satisfactory approximation to replace ψ(r2) by ψ0(r2) within
the integral, obtaining
ψ1(r1) = eik0·r1 −
Z 2m
¯h2 V (r2) eik|r1−r2|
4π|r1 −r2| eik0·r2d3r2.
(10.51)
This is the famous Born approximation. It is expected to be most accurate for weak
potentials and high incident energy.
■
Exercises
10.2.1
Show that the fundamental Green’s function for the 1-D Laplace equation, |x1 −x2|/2,
is consistent with the form found in Example 10.1.1.
10.2.2
Show that if
Lψ(r) ≡∇·
h
p(r)∇ψ(r)
i
+ q(r)ψ(r),
then L is Hermitian for p(r) and q(r) real, assuming Dirichlet boundary conditions on
the boundary of a region and that the scalar product is an integral over that region with
unit weight.
10.2.3
Show that the terms +k2 in the Helmholtz operator and −k2 in the modiﬁed Helmholtz
operator do not affect the behavior of G(r1,r2) in the immediate vicinity of the singular
point r1 = r2. Speciﬁcally, show that
lim
|r1−r2|→0
Z
k2G(r1,r2)d3r2 = −1.
10.2.4
Show that
−exp(ik|r1 −r2|)
4π|r1 −r2|
satisﬁes the appropriate criteria and therefore is a Green’s function for the Helmholtz
equation.

Additional Readings
467
10.2.5
Find the Green’s function for the 3-D Helmholtz equation, Exercise 10.2.4, when the
wave is a standing wave.
10.2.6
Verify that the formula given for the 3-D Green’s function of the modiﬁed Helmholtz
equation in Table 10.1 is correct when the boundary conditions of the problem are that
G vanish at inﬁnity.
10.2.7
An electrostatic potential (mks units) is
ϕ(r) =
Z
4πε0
e−ar
r
.
Reconstruct the electrical charge distribution that will produce this potential. Note that
ϕ(r) vanishes exponentially for large r, showing that the net charge is zero.
ANS.
ρ(r) = Zδ(r) −Za2
4π
e−ar
r
.
Additional Readings
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969), reprinting, Dover (1992). This book contains nearly 100 pages on Green’s functions, starting
with some good introductory material.
Courant, R., and D. Hilbert, Methods of Mathematical Physics, Vol. 1 (English edition). New York: Interscience
(1953). This is one of the classic works of mathematical physics. Originally published in German in 1924,
the revised English edition is an excellent reference for a rigorous treatment of integral equations, Green’s
functions, and a wide variety of other topics on mathematical physics.
Jackson, J. D., Classical Electrodynamics, 3rd ed. New York: Wiley (1999). Contains applications to electro-
magnetic theory.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics, 2 vols. New York: McGraw-Hill (1953). Chapter
7 is a particularly detailed, complete discussion of Green’s functions from the point of view of mathematical
physics. Note, however, that Morse and Feshbach frequently choose a source of 4πδ(r −r′) in place of our
δ(r −r′). Considerable attention is devoted to bounded regions.
Stakgold, I., Green’s Functions and Boundary Value Problems. New York: Wiley (1979).

CHAPTER 11
COMPLEX VARIABLE
THEORY
The imaginary numbers are a wonderful
ﬂight of God’s spirit; they are almost an
amphibian between being and not being.
GOTTFRIED WILHELM VON LEIBNIZ, 1702
We turn now to a study of complex variable theory. In this area we develop some of the
most powerful and widely useful tools in all of analysis. To indicate, at least partly, why
complex variables are important, we mention brieﬂy several areas of application.
1.
In two dimensions, the electric potential, viewed as a solution of Laplace’s equation,
can be written as the real (or the imaginary) part of a complex-valued function, and this
identiﬁcation enables the use of various features of complex variable theory (speciﬁ-
cally, conformal mapping) to obtain formal solutions to a wide variety of electrostatics
problems.
2.
The time-dependent Schrödinger equation of quantum mechanics contains the imagi-
nary unit i, and its solutions are complex.
3.
In Chapter 9 we saw that the second-order differential equations of interest in physics
may be solved by power series. The same power series may be used in the complex
plane to replace x by the complex variable z. The dependence of the solution f (z) at
a given z0 on the behavior of f (z) elsewhere gives us greater insight into the behavior
of our solution and a powerful tool (analytic continuation) for extending the region in
which the solution is valid.
4.
The change of a parameter k from real to imaginary, k →ik, transforms the Helmholtz
equation into the time-independent diffusion equation. The same change connects the
spherical and hyperbolic trigonometric functions, transforms Bessel functions into
their modiﬁed counterparts, and provides similar connections between other super-
ﬁcially dissimilar functions.
469
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

470
Chapter 11 Complex Variable Theory
5.
Integrals in the complex plane have a wide variety of useful applications:
•
Evaluating deﬁnite integrals and inﬁnite series,
•
Inverting power series,
•
Forming inﬁnite products,
•
Obtaining solutions of differential equations for large values of the variable
(asymptotic solutions),
•
Investigating the stability of potentially oscillatory systems,
•
Inverting integral transforms.
6.
Many physical quantities that were originally real become complex as a simple physi-
cal theory is made more general. The real index of refraction of light becomes a com-
plex quantity when absorption is included. The real energy associated with an energy
level becomes complex when the ﬁnite lifetime of the level is considered.
11.1
COMPLEX VARIABLES AND FUNCTIONS
We have already seen (in Chapter 1) the deﬁnition of complex numbers z = x + iy as
ordered pairs of two real numbers, x and y. We reviewed there the rules for their arithmetic
operations, identiﬁed the complex conjugate z∗of the complex number z, and discussed
both the Cartesian and polar representations of complex numbers, introducing for that pur-
pose the Argand diagram (complex plane). In the polar representation z = reiθ, we noted
that r (the magnitude of the complex number) is also called its modulus, and the angle
θ is known as its argument. We proved that eiθ satisﬁes the important equation
eiθ = cosθ + i sinθ.
(11.1)
This equation shows that for real θ, eiθ is of unit magnitude and is therefore situated on
the unit circle, at an angle θ from the real axis.
Our focus in the present chapter is on functions of a complex variable and on their
analytical properties. We have already noted that by deﬁning complex functions f (z) to
have the same power-series expansion (in z) as the expansion (in x) of the correspond-
ing real function f (x), the real and complex deﬁnitions coincide when z is real. We also
showed that by use of the polar representation, z = reiθ, it becomes clear how to com-
pute powers and roots of complex quantities. In particular, we noted that roots, viewed as
fractional powers, become multivalued functions in the complex domain, due to the fact
that exp(2nπi) = 1 for all positive and negative integers n. We thus found z1/2 to have
two values (not a surprise, since for positive real x, we have ±√x). But we also noted
that z1/m will have m different complex values. We also noted that the logarithm becomes
multivalued when extended to complex values, with
ln z = ln(reiθ) = lnr + i(θ + 2nπ),
(11.2)
with n any positive or negative integer (including zero).
If necessary, the reader should review the topics mentioned above by rereading
Section 1.8.

11.2 Cauchy-Riemann Conditions
471
11.2
CAUCHY-RIEMANN CONDITIONS
Having established complex functions of a complex variable, we now proceed to differen-
tiate them. The derivative of f (z), like that of a real function, is deﬁned by
lim
δz→0
f (z + δz) −f (z)
(z + δz) −z
= lim
δz→0
δf (z)
δz
= d f
dz = f ′(z),
(11.3)
provided that the limit is independent of the particular approach to the point z. For real
variables we require that the right-hand limit (x →x0 from above) and the left-hand limit
(x →x0 from below) be equal for the derivative d f (x)/dx to exist at x = x0. Now, with z
(or z0) some point in a plane, our requirement that the limit be independent of the direction
of approach is very restrictive.
Consider increments δx and δy of the variables x and y, respectively. Then
δz = δx + iδy.
(11.4)
Also, writing f = u + iv,
δf = δu + iδv,
(11.5)
so that
δf
δz = δu + iδv
δx + iδy .
(11.6)
Let us take the limit indicated by Eq. (11.3) by two different approaches, as shown in
Fig. 11.1. First, with δy = 0, we let δx →0. Equation (11.3) yields
lim
δz→0
δf
δz = lim
δx→0
δu
δx + i δv
δx

= ∂u
∂x + i ∂v
∂x ,
(11.7)
assuming that the partial derivatives exist. For a second approach, we set δx = 0 and then
let δy →0. This leads to
lim
δz→0
δf
δz = lim
δy→0

−i δu
δy + δv
δy

= −i ∂u
∂y + ∂v
∂y .
(11.8)
If we are to have a derivative d f/dz, Eqs. (11.7) and (11.8) must be identical. Equating
real parts to real parts and imaginary parts to imaginary parts (like components of vectors),
we obtain
∂u
∂x = ∂v
∂y ,
∂u
∂y = −∂v
∂x .
(11.9)
y
x
dx → 0
z0
dy = 0
dx = 0
dy → 0
FIGURE 11.1
Alternate approaches to z0.

472
Chapter 11 Complex Variable Theory
These are the famous Cauchy-Riemann conditions. They were discovered by Cauchy
and used extensively by Riemann in his development of complex variable theory. These
Cauchy-Riemann conditions are necessary for the existence of a derivative of f (z). That
is, in order for d f/dz to exist, the Cauchy-Riemann conditions must hold.
Conversely, if the Cauchy-Riemann conditions are satisﬁed and the partial derivatives
of u(x, y) and v(x, y) are continuous, the derivative d f/dz exists. To show this, we start
by writing
δf =
∂u
∂x + i ∂v
∂x

δx +
∂u
∂y + i ∂v
∂y

δy,
(11.10)
where the justiﬁcation for this expression depends on the continuity of the partial deriva-
tives of u and v. Using the Cauchy-Riemann equations, Eq. (11.9), we convert Eq. (11.10)
to the form
δf =
∂u
∂x + i ∂v
∂x

δx +

−∂v
∂x + i ∂u
∂x

δy
=
∂u
∂x + i ∂v
∂x

(δx + iδy).
(11.11)
Replacing δx + iδy by δz and bringing it to the left-hand side of Eq. (11.11), we reach
δf
δz = ∂u
∂x + i ∂v
∂x ,
(11.12)
an equation whose right-hand side is independent of the direction of δz (i.e., the relative
values of δx and δy). This independence of directionality meets the condition for the exis-
tence of the derivative, d f/dz.
Analytic Functions
If f (z) is differentiable and single-valued in a region of the complex plane, it is said to
be an analytic function in that region.1 Multivalued functions can also be analytic under
certain restrictions that make them single-valued in speciﬁc regions; this case, which is
of great importance, is taken up in detail in Section 11.6. If f (z) is analytic everywhere
in the (ﬁnite) complex plane, we call it an entire function. Our theory of complex vari-
ables here is one of analytic functions of a complex variable, which points up the crucial
importance of the Cauchy-Riemann conditions. The concept of analyticity carried on in
advanced theories of modern physics plays a crucial role in the dispersion theory (of ele-
mentary particles). If f ′(z) does not exist at z = z0, then z0 is labeled a singular point;
singular points and their implications will be discussed shortly.
To illustrate the Cauchy-Riemann conditions, consider two very simple examples.
1Some writers use the term holomorphic or regular.

11.2 Cauchy-Riemann Conditions
473
Example 11.2.1
z2 IS ANALYTIC
Let f (z) = z2. Multiplying out (x −iy)(x −iy) = x2 −y2 +2ixy, we identify the real part
of z2 as u(x, y) = x2 −y2 and its imaginary part as v(x, y) = 2xy. Following Eq. (11.9),
∂u
∂x = 2x = ∂v
∂y ,
∂u
∂y = −2y = −∂v
∂x .
We see that f (z) = z2 satisﬁes the Cauchy-Riemann conditions throughout the complex
plane. Since the partial derivatives are clearly continuous, we conclude that f (z) = z2 is
analytic, and is an entire function.
■
Example 11.2.2
z∗IS NOT ANALYTIC
Let f (z) = z∗, the complex conjugate of z. Now u = x and v = −y. Applying the Cauchy-
Riemann conditions, we obtain
∂u
∂x = 1 ̸= ∂v
∂y = −1.
The Cauchy-Riemann conditions are not satisﬁed for any values of x or y and f (z) = z∗
is nowhere an analytic function of z. It is interesting to note that f (z) = z∗is continu-
ous, thus providing an example of a function that is everywhere continuous but nowhere
differentiable in the complex plane.
■
The derivative of a real function of a real variable is essentially a local characteristic, in
that it provides information about the function only in a local neighborhood, for instance,
as a truncated Taylor expansion. The existence of a derivative of a function of a com-
plex variable has much more far-reaching implications, one of which is that the real and
imaginary parts of our analytic function must separately satisfy Laplace’s equation in two
dimensions, namely
∂2ψ
∂x2 + ∂2ψ
∂y2 = 0.
To verify the above statement, we differentiate the ﬁrst Cauchy-Riemann equation in
Eq. (11.9) with respect to x and the second with respect to y, obtaining
∂2u
∂x2 = ∂2v
∂x∂y ,
∂2u
∂y2 = −∂2v
∂y∂x .
Combining these two equations, we easily reach
∂2u
∂x2 + ∂2u
∂y2 = 0,
(11.13)
conﬁrming that u(x, y), the real part of a differentiable complex function, satisﬁes the
Laplace equation. Either by recognizing that if f (z) is differentiable, so is −i f (z) =
v(x, y) −iu(x, y), or by steps similar to those leading to Eq. (11.13), we can conﬁrm
that v(x, y) also satisﬁes the two-dimensional (2-D) Laplace equation. Sometimes u and
v are referred to as harmonic functions (not to be confused with spherical harmonics,
which we will later encounter as the angular solutions to central force problems).

474
Chapter 11 Complex Variable Theory
The solutions u(x, y) and v(x, y) are complementary in that the curves of constant
u(x, y) make orthogonal intersections with the curves of constant v(x, y). To conﬁrm this,
note that if (x0, y0) is on the curve u(x, y) = c, then x0 +dx, y0 +dy is also on that curve if
∂u
∂x dx + ∂u
∂y dy = 0,
meaning that the slope of the curve of constant u at (x0, y0) is
dy
dx

u
= −∂u/∂x
∂u/∂y ,
(11.14)
where the derivatives are to be evaluated at (x0, y0). Similarly, we can ﬁnd that the slope
of the curve of constant v at (x0, y0) is
dy
dx

v
= −∂v/∂x
∂v/∂y = ∂u/∂y
∂u/∂x ,
(11.15)
where the last member of Eq. (11.15) was reached using the Cauchy-Riemann equations.
Comparing Eqs. (11.14) and (11.15), we note that at the same point, the slopes they
describe are orthogonal (to check, verify that dxudxv + dyudyv = 0).
The properties we have just examined are important for the solution of 2-D electrostatics
problems (governed by the Laplace equation). If we have identiﬁed (by methods outside
the scope of the present text) an appropriate analytic function, its lines of constant u will
describe electrostatic equipotentials, while those of constant v will be the stream lines of
the electric ﬁeld.
Finally, the global nature of our analytic function is also illustrated by the fact that it
has not only a ﬁrst derivative, but in addition, derivatives of all higher orders, a property
which is not shared by functions of a real variable. This property will be demonstrated in
Section 11.4.
Derivatives of Analytic Functions
Working with the real and imaginary parts of an analytic function f (z) is one way to take
its derivative; an example of that approach is to use Eq. (11.12). However, it is usually
easier to use the fact that complex differentiation follows the same rules as those for real
variables. As a ﬁrst step in establishing this correspondence, note that, if f (z) is analytic,
then, from Eq. (11.12),
f ′(z) = ∂f
∂x ,
and that
h
f (z)g(z)
i ′
=
 d
dz
h
f (z)g(z)
i
=
 ∂
∂x
h
f (z)g(z)
i
=
∂f
∂x

g(z) + f (z)
∂g
∂x

= f ′(z)g(z) + f (z)g′(z),

11.2 Cauchy-Riemann Conditions
475
the familiar rule for differentiating a product. Given also that
dz
dz = ∂z
∂x = 1,
we can easily establish that
dz2
dz = 2z,
and, by induction, dzn
dz = nzn−1.
Functions deﬁned by power series will then have differentiation rules identical to those
for the real domain. Functions not ordinarily deﬁned by power series also have the same
differentiation rules as for the real domain, but that will need to be demonstrated case by
case. Here is an example that illustrates the establishment of a derivative formula.
Example 11.2.3
DERIVATIVE OF LOGARITHM
We want to verify that d ln z/dz = 1/z. Writing, as in Eq. (1.138),
ln z = lnr + iθ + 2nπi,
we note that if we write ln z = u + iv, we have u = lnr, v = θ + 2nπ. To check whether
ln z satisﬁes the Cauchy-Riemann equations, we evaluate
∂u
∂x = 1
r
∂r
∂x = x
r2 ,
∂u
∂y = 1
r
∂r
∂y = y
r2 ,
∂v
∂x = ∂θ
∂x = −y
r2 ,
∂v
∂y = ∂θ
∂y = x
r2 .
The derivatives of r and θ with respect to x and y are obtained from the equations connect-
ing Cartesian and polar coordinates. Except at r = 0, where the derivatives are undeﬁned,
the Cauchy-Riemann equations can be conﬁrmed.
Then, to obtain the derivative, we can simply apply Eq. (11.12),
d ln z
dz
= ∂u
∂x + i ∂v
∂x = x −iy
r2
=
1
x + iy = 1
z .
Because ln z is multivalued, it will not be analytic except under conditions restricting it to
single-valuedness in a speciﬁc region. This topic will be taken up in Section 11.6.
■
Point at Inﬁnity
In complex variable theory, inﬁnity is regarded as a single point, and behavior in its neigh-
borhood is discussed after making a change of variable from z to w = 1/z. This transfor-
mation has the effect that, for example, z = −R, with R large, lies in the w plane close
to z = +R, thereby among other things inﬂuencing the values computed for derivatives.
An elementary consequence is that entire functions, such as z or ez, have singular points
at z = ∞. As a trivial example, note that at inﬁnity the behavior of z is identiﬁed as that of
1/w as w →0, leading to the conclusion that z is singular there.

476
Chapter 11 Complex Variable Theory
Exercises
11.2.1
Show whether or not the function f (z) = ℜ(z) = x is analytic.
11.2.2
Having shown that the real part u(x, y) and the imaginary part v(x, y) of an analytic
function w(z) each satisfy Laplace’s equation, show that neither u(x, y) nor v(x, y) can
have either a maximum or a minimum in the interior of any region in which w(z) is
analytic. (They can have saddle points only.)
11.2.3
Find the analytic function
w(z) = u(x, y) + iv(x, y)
(a)
if u(x, y) = x3 −3xy2,
(b)
if v(x, y) = e−y sin x.
11.2.4
If there is some common region in which w1 = u(x, y) + iv(x, y) and w2 = w∗
1 =
u(x, y) −iv(x, y) are both analytic, prove that u(x, y) and v(x, y) are constants.
11.2.5
Starting from f (z) = 1/(x + iy), show that 1/z is analytic in the entire ﬁnite z plane
except at the point z = 0. This extends our discussion of the analyticity of zn to negative
integer powers n.
11.2.6
Show that given the Cauchy-Riemann equations, the derivative f ′(z) has the same value
for dz = a dx + ib dy (with neither a nor b zero) as it has for dz = dx.
11.2.7
Using f (reiθ) = R(r,θ)ei2(r,θ), in which R(r,θ) and 2(r,θ) are differentiable real
functions of r and θ, show that the Cauchy-Riemann conditions in polar coordinates
become
(a)
∂R
∂r = R
r
∂2
∂θ ,
(b)
1
r
∂R
∂θ = −R ∂2
∂r .
Hint. Set up the derivative ﬁrst with δz radial and then with δz tangential.
11.2.8
As an extension of Exercise 11.2.7 show that 2(r,θ) satisﬁes the 2-D Laplace equation
in polar coordinates,
∂22
∂r2 + 1
r
∂2
∂r + 1
r2
∂22
∂θ2 = 0.
11.2.9
For each of the following functions f (z), ﬁnd f ′(z) and identify the maximal region
within which f (z) is analytic.
(a)
f (z) = sin z
z
,
(d)
f (z) = e−1/z,
(b)
f (z) =
1
z2 + 1,
(e)
f (z) = z2 −3z + 2,
(c)
f (z) =
1
z(z + 1),
(f)
f (z) = tan(z),
(g)
f (z) = tanh(z).

11.3 Cauchy’s Integral Theorem
477
11.2.10
For what complex values do each of the following functions f (z) have a derivative?
(a)
f (z) = z3/2,
(b)
f (z) = z−3/2,
(c)
f (z) = tan−1(z),
(d)
f (z) = tanh−1(z).
11.2.11
Two-dimensional irrotational ﬂuid ﬂow is conveniently described by a complex poten-
tial f (z) = u(x,v) + iv(x, y). We label the real part, u(x, y), the velocity potential,
and the imaginary part, v(x, y), the stream function. The ﬂuid velocity V is given by
V = ∇u. If f (z) is analytic:
(a)
Show that d f/dz = Vx −iVy.
(b)
Show that ∇· V = 0 (no sources or sinks).
(c)
Show that ∇× V = 0 (irrotational, nonturbulent ﬂow).
11.2.12
The function f (z) is analytic. Show that the derivative of f (z) with respect to z∗does
not exist unless f (z) is a constant.
Hint. Use the chain rule and take x = (z + z∗)/2, y = (z −z∗)/2i.
Note. This result emphasizes that our analytic function f (z) is not just a complex func-
tion of two real variables x and y. It is a function of the complex variable x + iy.
11.3
CAUCHY’S INTEGRAL THEOREM
Contour Integrals
With differentiation under control, we turn to integration. The integral of a complex vari-
able over a path in the complex plane (known as a contour) may be deﬁned in close
analogy to the (Riemann) integral of a real function integrated along the real x-axis.
We divide the contour, from z0 to z′
0, designated C, into n intervals by picking n −1
intermediate points z1, z2,... on the contour (Fig. 11.2). Consider the sum
Sn =
n
X
j=1
f (ζ j)(z j −z j−1),
where ζ j is a point on the curve between z j and z j−1. Now let n →∞with
|z j −z j−1| →0
for all j. If limn→∞Sn exists, then
lim
n→∞
n
X
j=1
f (ζ j)(z j −z j−1) =
z′
0
Z
z0
f (z)dz =
Z
C
f (z)dz.
(11.16)
The right-hand side of Eq. (11.16) is called the contour integral of f (z) (along the speciﬁed
contour C from z = z0 to z = z′
0).

478
Chapter 11 Complex Variable Theory
y
x
z3
z2
z1
z0
ζ1
ζ0
z¢0=zn 
FIGURE 11.2
Integration path.
As an alternative to the above, the contour integral may be deﬁned by
z2
Z
z1
f (z)dz =
x2,y2
Z
x1,y1
[u(x, y) + iv(x, y)][dx + i dy]
=
x2,y2
Z
x1,y1
[u(x, y)dx −v(x, y)dy] + i
x2,y2
Z
x1,y1
[v(x, y)dx + u(x, y)dy],
(11.17)
with the path joining (x1, y1) and (x2, y2) speciﬁed. This reduces the complex integral to
the complex sum of real integrals. It is somewhat analogous to the replacement of a vector
integral by the vector sum of scalar integrals.
Often we are interested in contours that are closed, meaning that the start and end of the
contour are at the same point, so that the contour forms a closed loop. We normally deﬁne
the region enclosed by a contour as that which lies to the left when the contour is traversed
in the indicated direction; thus a contour intended to surround a ﬁnite area will normally be
deemed to be traversed in the counterclockwise direction. If the origin of a polar coordinate
system is within the contour, this convention will cause the normal direction of travel on
the contour to be that in which the polar angle θ increases.
Statement of Theorem
Cauchy’s integral theorem states that:
If f(z) is an analytic function at all points of a simply connected region in the complex
plane and if C is a closed contour within that region, then
I
C
f (z)dz = 0.
(11.18)

11.3 Cauchy’s Integral Theorem
479
To clarify the above, we need the following deﬁnition:
•
A region is simply connected if every closed curve within it can be shrunk continu-
ously to a point that is within the region.
In everyday language, a simply connected region is one that has no holes. We also need to
explain that the symbol
H
will be used from now on to indicate an integral over a closed
contour; a subscript (such as C) is attached when further speciﬁcation of the contour is
desired. Note also that for the theorem to apply, the contour must be “within” the region of
analyticity. That means it cannot be on the boundary of the region.
Before proving Cauchy’s integral theorem, we look at some examples that do (and do
not) meet its conditions.
Example 11.3.1
zn ON CIRCULAR CONTOUR
Let’s examine the contour integral
H
C zndz, where C is a circle of radius r > 0 around the
origin z = 0 in the positive mathematical sense (counterclockwise). In polar coordinates,
cf. Eq. (1.125), we parameterize the circle as z = reiθ and dz = ireiθdθ. For n ̸= −1,n an
integer, we then obtain
I
C
zndz = i rn+1
2π
Z
0
exp[i(n + 1)θ]dθ
= i rn+1
"
ei(n+1)θ
i(n + 1)
#2π
0
= 0
(11.19)
because 2π is a period of ei(n+1)θ. However, for n = −1
I
C
dz
z = i
2π
Z
0
dθ = 2πi,
(11.20)
independent of r but nonzero.
The fact that Eq. (11.19) is satisﬁed for all integers n ≥0 is required by Cauchy’s the-
orem, because for these n values zn is analytic for all ﬁnite z, and certainly for all points
within a circle of radius r. Cauchy’s theorem does not apply for any negative integer n
because, for these n, zn is singular at z = 0. The theorem therefore does not prescribe any
particular values for the integrals of negative n. We see that one such integral (that for
n = −1) has a nonzero value, and that others (for integral n ̸= −1) do vanish.
■
Example 11.3.2
zn ON SQUARE CONTOUR
We next examine the integration of zn for a different contour, a square with vertices at
± 1
2 ± 1
2i. It is somewhat tedious to perform this integration for general integer n, so we
illustrate only with n = 2 and n = −1.

480
Chapter 11 Complex Variable Theory
y
x
−1+i
2
1+ i
2
1−i
2
−1−i
2
FIGURE 11.3
Square integration contour.
For n = 2, we have z2 = x2 −y2 + 2ixy. Referring to Fig. 11.3, we identify the con-
tour as consisting of four line segments. On Segment 1, dz = dx (y = −1
2 and dy = 0);
on Segment 2, dz = i dy, x = 1
2, dx = 0; on Segment 3, dz = dx, y = 1
2, dy = 0; and on
Segment 4, dz = i dy, x = −1
2, dx = 0. Note that for Segments 3 and 4 the integration is
in the direction of decreasing value of the integration variable. These segments therefore
contribute as follows to the integral:
Segment 1:
1
2
Z
−1
2
dx(x2 −1
4 −ix) = 1
3
1
8 −

−1
8

−1
4 −i
2(0) = −1
6,
Segment 2:
1
2
Z
−1
2
i dy( 1
4 −y2 + iy) = i
4 −i
3
1
8 −

−1
8

−1
2(0) = i
6,
Segment 3:
−1
2
Z
1
2
(dx)(x2 −1
4 + ix) = −1
3
1
8 −

−1
8

+ 1
4 −i
2(0) = 1
6,
Segment 4:
−1
2
Z
1
2
(i dy)( 1
4 −y2 −iy) = −i
4 + i
3
1
8 −

−1
8

−1
2(0) = −i
6.
We ﬁnd that the integral of z2 over the square vanishes, just as it did over the circle. This
is required by Cauchy’s theorem.
For n = −1, we have, in Cartesian coordinates,
z−1 = x −iy
x2 + y2 ,

11.3 Cauchy’s Integral Theorem
481
and the integral over the four segments of the square contour takes the form
1
2
Z
−1
2
x + i/2
x2 + 1
4
dx +
1
2
Z
−1
2
1
2 −iy
y2 + 1
4
(i dy) +
−1
2
Z
1
2
x −i/2
x2 + 1
4
dx +
−1
2
Z
1
2
1
2 + iy
y2 + 1
4
(i dy).
Several of the terms vanish because they involve the integration of an odd integrand over
an even interval, and others simply cancel. All that remains is
Z
□
z−1dz = i
1
2
Z
−1
2
dx
x2 + 1
4
= 2i
1
Z
−1
du
u2 + 1 = 2i
hπ
2 −

−π
2
i
= 2πi,
the same result as was obtained for the integration of z−1 around a circle of any radius.
Cauchy’s theorem does not apply here, so the nonzero result is not problematic.
■
Cauchy’s Theorem: Proof
We now proceed to a proof of Cauchy’s integral theorem. The proof we offer is subject to
a restriction originally accepted by Cauchy but later shown unnecessary by Goursat. What
we need to show is that
I
C
f (z)dz = 0,
subject to the requirement that C is a closed contour within a simply connected region R
where f (z) is analytic. See Fig. 11.4. The restriction needed for Cauchy’s (and the present)
proof is that if we write f (z) = u(x, y) + iv(x, y), the partial derivatives of u and v are
continuous.
y
x
C
R
FIGURE 11.4
A closed-contour C within a simply connected region R.

482
Chapter 11 Complex Variable Theory
We intend to prove the theorem by direct application of Stokes’ theorem (Section 3.8).
Writing dz = dx + i dy,
I
C
f (z)dz =
I
C
(u + iv)(dx + i dy)
=
I
C
(u dx −v dy) + i
I
C
(v dx + u dy).
(11.21)
These two line integrals may be converted to surface integrals by Stokes’ theorem, a pro-
cedure that is justiﬁed because we have assumed the partial derivatives to be continuous
within the area enclosed by C. In applying Stokes’ theorem, note that the ﬁnal two integrals
of Eq. (11.21) are real.
To proceed further, we note that all the integrals involved here can be identiﬁed as
having integrands of the form (Vx ˆex + Vyˆey) · dr, the integration is around a loop in the
xy plane, and the value of the integral will be the surface integral, over the enclosed area,
of the z component of ∇× (Vx ˆex + Vyˆey). Thus, Stokes’ theorem says that
I
C
(Vx dx + Vy dy) =
Z
A
∂Vy
∂x −∂Vx
∂y

dx dy,
(11.22)
with A being the 2-D region enclosed by C.
For the ﬁrst integral in the second line of Eq. (11.21), let u = Vx and v = −Vy.2 Then
I
C
(u dx −v dy) =
I
C
(Vx dx + Vy dy)
=
Z
A
∂Vy
∂x −∂Vx
∂y

dx dy = −
Z
A
∂v
∂x + ∂u
∂y

dx dy.
(11.23)
For the second integral on the right side of Eq. (11.21) we let u = Vy and v = Vx. Using
Stokes’ theorem again, we obtain
I
C
(v dx + u dy) =
Z
A
∂u
∂x −∂v
∂y

dx dy.
(11.24)
Inserting Eqs. (11.23) and (11.24) into Eq. (11.21), we now have
I
C
f (z)dz = −
Z
A
∂v
∂x + ∂u
∂y

dx dy + i
Z
A
∂u
∂x −∂v
∂y

dx dy = 0.
(11.25)
Remembering that f (z) has been assumed analytic, we ﬁnd that both the surface integrals
in Eq. (11.25) are zero because application of the Cauchy-Riemann equations causes their
integrands to vanish. This establishes the theorem.
2For Stokes’ theorem, Vx and Vy are any two functions with continuous partial derivatives, and they need not be connected by
any relations stemming from complex variable theory.

11.3 Cauchy’s Integral Theorem
483
Multiply Connected Regions
The original statement of Cauchy’s integral theorem demanded a simply connected region
of analyticity. This restriction may be relaxed by the creation of a barrier, a narrow region
we choose to exclude from the region identiﬁed as analytic. The purpose of the barrier
construction is to permit, within a multiply connected region, the identiﬁcation of curves
that can be shrunk to a point within the region, that is, the construction of a subregion that
is simply connected.
Consider the multiply connected region of Fig. 11.5, in which f (z) is only analytic in
the unshaded area labeled R. Cauchy’s integral theorem is not valid for the contour C,
as shown, but we can construct a contour C′ for which the theorem holds. We draw a
barrier from the interior forbidden region, R′, to the forbidden region exterior to R and
then run a new contour, C′, as shown in Fig. 11.6.
The new contour, C′, through ABDEFGA, never crosses the barrier that converts R into
a simply connected region. Incidentally, the three-dimensional analog of this technique
was used in Section 3.9 to prove Gauss’ law. Because f (z) is in fact continuous across the
barrier dividing DE from G A and the line segments DE and G A can be arbitrarily close
together, we have
A
Z
G
f (z)dz = −
D
Z
E
f (z)dz.
(11.26)
y
x
C
R
R′
FIGURE 11.5
A closed contour C in a multiply connected region.
y
x
D
A
F
B
C′2
C′1
E
G
FIGURE 11.6
Conversion of a multiply connected region into a simply connected region.

484
Chapter 11 Complex Variable Theory
Then, invoking Cauchy’s integral theorem, because the contour is now within a simply
connected region, and using Eq. (11.26) to cancel the contributions of the segments along
the barrier,
I
C′
f (z)dz =
Z
ABD
f (z)dz +
Z
EFG
f (z)dz = 0.
(11.27)
Now that we have established Eq. (11.27), we note that A and D are only inﬁnitesimally
separated and that f (z) is actually continuous across the barrier. Hence, integration on the
path ABD will yield the same result as a truly closed contour ABDA. Similar remarks apply
to the path EFG, which can be replaced by EFGE. Renaming ABDA as C′
l and EFGE as
−C′
2, we have the simple result
I
C′
1
f (z)dz =
I
C′
2
f (z)dz,
(11.28)
in which C′
1 and C′
2 are both traversed in the same (counterclockwise, that is, positive)
direction.
This result calls for some interpretation. What we have shown is that the integral of an
analytic function over a closed contour surrounding an “island” of nonanalyticity can be
subjected to any continuous deformation within the region of analyticity without changing
the value of the integral. The notion of continuous deformation means that the change
in contour must be able to be carried out via a series of small steps, which precludes
processes whereby we “jump over” a point or region of nonanalyticity. Since we already
know that the integral of an analytic function over a contour in a simply connected region
of analyticity has the value zero, we can make the more general statement
The integral of an analytic function over a closed path has a value that remains
unchanged over all possible continuous deformations of the contour within the region
of analyticity.
Looking back at the two examples of this section, we see that the integrals of z2 vanished
for both the circular and square contours, as prescribed by Cauchy’s integral theorem for
an analytic function. The integrals of z−1 did not vanish, and vanishing was not required
because there was a point of nonanalyticity within the contours. However, the integrals of
z−1 for the two contours had the same value, as either contour can be reached by continuous
deformation of the other.
We close this section with an extremely important observation. By a trivial extension to
Example 11.3.1 plus the fact that closed contours in a region of analyticity can be deformed
continuously without altering the value of the integral, we have the valuable and useful
result:
The integral of (z −z0)n around any counterclockwise closed path C that encloses z0
has, for any integer n, the values
I
C
(z −z0)n dz =
0,
n ̸= −1,
2πi,
n = −1.
(11.29)

11.3 Cauchy’s Integral Theorem
485
Exercises
11.3.1
Show that
z2
Z
z1
f (z)dz = −
z1
Z
z2
f (z)dz.
11.3.2
Prove that

Z
C
f (z)dz
 ≤| f |max · L,
where | f |max is the maximum value of | f (z)| along the contour C and L is the length
of the contour.
11.3.3
Show that the integral
4−3i
Z
3+4i
(4z2 −3iz)dz
has the same value on the two paths: (a) the straight line connecting the integration
limits, and (b) an arc on the circle |z| = 5.
11.3.4
Let F(z) =
z
Z
π(1+i)
cos2ζ dζ.
Show that F(z) is independent of the path connecting the limits of integration, and
evaluate F(πi).
11.3.5
Evaluate
H
C(x2 −iy2)dz, where the integration is (a) clockwise around the unit circle,
(b) on a square with vertices at ±1 ± i. Explain why the results of parts (a) and (b) are
or are not identical.
11.3.6
Verify that
1+i
Z
0
z∗dz
depends on the path by evaluating the integral for the two paths shown in Fig. 11.7.
Recall that f (z) = z∗is not an analytic function of z and that Cauchy’s integral theorem
therefore does not apply.
11.3.7
Show that
I
C
dz
z2 + z = 0,
in which the contour C is a circle deﬁned by |z| = R > 1.
Hint. Direct use of the Cauchy integral theorem is illegal. The integral may be evaluated
by expanding into partial fractions and then treating the two terms individually. This
yields 0 for R > 1 and 2πi for R < 1.

486
Chapter 11 Complex Variable Theory
y
x
2
1
2
1
(1,1)
FIGURE 11.7
Contours for Exercise 11.3.6.
11.4
CAUCHY’S INTEGRAL FORMULA
As in the preceding section, we consider a function f (z) that is analytic on a closed contour
C and within the interior region bounded by C. This means that the contour C is to be
traversed in the counterclockwise direction. We seek to prove the following result, known
as Cauchy’s integral formula:
1
2πi
I
C
f (z)
z −z0
dz = f (z0),
(11.30)
in which z0 is any point in the interior region bounded by C. Note that since z is on the
contour C while z0 is in the interior, z −z0 ̸= 0 and the integral Eq. (11.30) is well deﬁned.
Although f (z) is assumed analytic, the integrand is f (z)/(z −z0) and is not analytic at
z = z0 unless f (z0) = 0. We now deform the contour, to make it a circle of small radius
r about z = z0, traversed, like the original contour, in the counterclockwise direction. As
shown in the preceding section, this does not change the value of the integral. We therefore
write z = z0 + reiθ, so dz = ireiθdθ, the integration is from θ = 0 to θ = 2π, and
I
C
f (z)
z −z0
dz =
2π
Z
0
f (z0 + reiθ)
reiθ
ireiθ dθ.
Taking the limit r →0, we obtain
I
C
f (z)
z −z0
dz = i f (z0)
2π
Z
0
dθ = 2πi f (z0),
(11.31)
where we have replaced f (z) by its limit f (z0) because it is analytic and therefore contin-
uous at z = z0. This proves the Cauchy integral formula.
Here is a remarkable result. The value of an analytic function f (z) is given at an arbitrary
interior point z = z0 once the values on the boundary C are speciﬁed.

11.4 Cauchy’s Integral Formula
487
It has been emphasized that z0 is an interior point. What happens if z0 is exterior to C?
In this case the entire integrand is analytic on and within C. Cauchy’s integral theorem,
Section 11.3, applies and the integral vanishes. Summarizing, we have
1
2πi
I
C
f (z)dz
z −z0
=
 f (z0),
z0 within the contour,
0,
z0 exterior to the contour.
Example 11.4.1
AN INTEGRAL
Consider
I =
I
C
dz
z(z + 2),
where the integration is counterclockwise over the unit circle. The factor 1/(z + 2) is
analytic within the region enclosed by the contour, so this is a case of Cauchy’s integral
formula, Eq. (11.30), with f (z) = 1/(z + 2) and z0 = 0. The result is immediate:
I = 2πi

1
z + 2

z=0
= πi.
■
Example 11.4.2
INTEGRAL WITH TWO SINGULAR FACTORS
Consider now
I =
I
C
dz
4z2 −1,
also integrated counterclockwise over the unit circle. The denominator factors into
4
 z −1
2
 z + 1
2

, and it is apparent that the region of integration contains two singular fac-
tors. However, we may still use Cauchy’s integral formula if we make the partial fraction
expansion
1
4z2 −1 = 1
4
 
1
z −1
2
−
1
z + 1
2
!
,
after which we integrate the two terms individually. We have
I = 1
4


I
C
dz
z −1
2
−
I
C
dz
z + 1
2

.
Each integral is a case of Cauchy’s formula with f (z) = 1, and for both integrals the
point z0 = ± 1
2 is within the contour, so each evaluates to 2πi, and their sum is zero. So
I = 0.
■

488
Chapter 11 Complex Variable Theory
Derivatives
Cauchy’s integral formula may be used to obtain an expression for the derivative of f (z).
Differentiating Eq. (11.30) with respect to z0, and interchanging the differentiation and the
z integration,3
f ′(z0) =
1
2πi
I
f (z)
(z −z0)2 dz.
(11.32)
Differentiating again,
f ′′(z0) =
2
2πi
I
f (z)dz
(z −z0)3 .
Continuing, we get4
f (n)(z0) = n!
2πi
I
f (z)dz
(z −z0)n+1 ;
(11.33)
that is, the requirement that f (z) be analytic guarantees not only a ﬁrst derivative but
derivatives of all orders as well! The derivatives of f (z) are automatically analytic. As
indicated in a footnote, this statement assumes the Goursat version of the Cauchy integral
theorem. This is a reason why Goursat’s contribution is so signiﬁcant in the development
of the theory of complex variables.
Example 11.4.3
USE OF DERIVATIVE FORMULA
Consider
I =
I
C
sin2 z dz
(z −a)4 ,
where the integral is counterclockwise on a contour that encircles the point z = a. This is
a case of Eq. (11.33) with n = 3 and f (z) = sin2 z. Therefore,
I = 2πi
3!
 d3
dz3 sin2 z

z=a
= πi
3
h
−8sin z cos z
i
z=a = −8πi
3 sina cosa.
■
3The interchange can be proved legitimate, but the proof requires that Cauchy’s integral theorem not be subject to the continuous
derivative restriction in Cauchy’s original proof. We are therefore now depending on Goursat’s proof of the integral theorem.
4This expression is a starting point for deﬁning derivatives of fractional order. See A. Erdelyi, ed., Tables of Integral Trans-
forms, Vol. 2. New York: McGraw-Hill (1954). For more recent applications to mathematical analysis, see T. J. Osler, An inte-
gral analogue of Taylor’s series and its use in computing Fourier transforms, Math. Comput. 26: 449 (1972), and references
therein.

11.4 Cauchy’s Integral Formula
489
Morera’s Theorem
A further application of Cauchy’s integral formula is in the proof of Morera’s theorem,
which is the converse of Cauchy’s integral theorem. The theorem states the following:
If a function f (z) is continuous in a simply connected region R and
H
C f (z)dz = 0 for
every closed contour C within R, then f (z) is analytic throughout R.
To prove the theorem, let us integrate f (z) from z1 to z2. Since every closed-path inte-
gral of f (z) vanishes, this integral is independent of path and depends only on its end-
points. We may therefore write
F(z2) −F(z1) =
z2
Z
z1
f (z)dz,
(11.34)
where F(z), presently unknown, can be called the indeﬁnite integral of f (z). We then
construct the identity
F(z2) −F(z1)
z2 −z1
−f (z1) =
1
z2 −z1
z2
Z
z1
h
f (t) −f (z1)
i
dt,
(11.35)
where we have introduced another complex variable, t. Next, using the fact that f (t) is
continuous, we write, keeping only terms to ﬁrst order in t −z1,
f (t) −f (z1) = f ′(z1)(t −z1) + ··· ,
which implies that
z2
Z
z1
h
f (t) −f (z1)
i
dt =
z2
Z
z1
h
f ′(z1)(t −z1) + ···
i
dt = f ′(z1)
2
(z2 −z1)2 + ··· .
It is thus apparent that the right-hand side of Eq. (11.35) approaches zero in the limit
z2 →z1, so
f (z1) = lim
z2→z1
F(z2) −F(z1)
z2 −z1
= F′(z1).
(11.36)
Equation (11.36) shows that F(z), which by construction is single-valued, has a derivative
at all points within R and is therefore analytic in that region. Since F(z) is analytic, then
so also must be its derivative, f (z), thereby proving Morera’s theorem.
At this point, one comment might be in order. Morera’s theorem, which establishes
the analyticity of F(z) in a simply connected region, cannot be extended to prove that
F(z), as well as f (z), is analytic throughout a multiply connected region via the device of
introducing a barrier. It is not possible to show that F(z) will have the same value on both
sides of the barrier, and in fact it does not always have that property. Thus, if extended
to a multiply connected region, F(z) may fail to have the single-valuedness that is one
of the requirements for analyticity. Put another way, a function which is analytic in a

490
Chapter 11 Complex Variable Theory
multiply connected region will have analytic derivatives of all orders in that region, but its
integral is not guaranteed to be analytic in the entire multiply connected region. This issue
is elaborated in Section 11.6.
The proof of Morera’s theorem has given us something additional, namely that the
indeﬁnite integral of f (z) is its antiderivative, showing that:
The rules for integration of complex functions are the same as those for real functions.
Further Applications
An important application of Cauchy’s integral formula is the following Cauchy inequal-
ity. If f (z) = P anzn is analytic and bounded, | f (z)| ≤M on a circle of radius r about
the origin, then
|an|rn ≤M
(Cauchy’s inequality)
(11.37)
gives upper bounds for the coefﬁcients of its Taylor expansion. To prove Eq. (11.37) let us
deﬁne M(r) = max|z|=r | f (z)| and use the Cauchy integral for an = f (n)(z)/n!,
|an| = 1
2π

I
|z|=r
f (z)
zn+1 dz
 ≤M(r)
2πr
2πrn+1 .
An immediate consequence of the inequality, Eq. (11.37), is Liouville’s theorem: If
f (z) is analytic and bounded in the entire complex plane it is a constant. In fact, if
| f (z)| ≤M for all z, then Cauchy’s inequality Eq. (11.37), applied for |z| = r, gives
|an| ≤Mr−n. If now we choose to let r approach ∞, we may conclude that for all n > 0,
|an| = 0. Hence f (z) = a0.
Conversely, the slightest deviation of an analytic function from a constant value implies
that there must be at least one singularity somewhere in the inﬁnite complex plane. Apart
from the trivial constant functions then, singularities are a fact of life, and we must learn to
live with them. As pointed out when introducing the concept of the point at inﬁnity, even
innocuous functions such as f (z) = z have singularities at inﬁnity; we now know that this
is a property of every entire function that is not simply a constant. But we shall do more
than just tolerate the existence of singularities. In the next section, we show how to expand
a function in a Laurent series at a singularity, and we go on to use singularities to develop
the powerful and useful calculus of residues in a later section of this chapter.
A famous application of Liouville’s theorem yields the fundamental theorem of alge-
bra (due to C. F. Gauss), which says that any polynomial P(z) = Pn
ν=0 aνzν with n > 0
and an ̸= 0 has n roots. To prove this, suppose P(z) has no zero. Then 1/P(z) is analytic
and bounded as |z| →∞, and, because of Liouville’s theorem, P(z) would have to be a
constant. To resolve this contradiction, it must be the case that P(z) has at least one root λ
that we can divide out, forming P(z)/(z −λ), a polynomial of degree n −1. We can repeat
this process until the polynomial has been reduced to degree zero, thereby ﬁnding exactly
n roots.

11.4 Cauchy’s Integral Formula
491
Exercises
Unless explicitly stated otherwise, closed contours occurring in these exercises are to
be understood as traversed in the mathematically positive (counterclockwise) direction.
11.4.1
Show that
1
2πi
I
zm−n−1dz,
m and n integers
(with the contour encircling the origin once), is a representation of the Kronecker δmn.
11.4.2
Evaluate
I
C
dz
z2 −1,
where C is the circle |z −1| = 1.
11.4.3
Assuming that f (z) is analytic on and within a closed contour C and that the point z0
is within C, show that
I
C
f ′(z)
z −z0
dz =
I
C
f (z)
(z −z0)2 dz.
11.4.4
You know that f (z) is analytic on and within a closed contour C. You suspect that the
nth derivative f (n)(z0) is given by
f (n)(z0) = n!
2πi
I
C
f (z)
(z −z0)n+1 dz.
Using mathematical induction (Section 1.4), prove that this expression is correct.
11.4.5
(a)
A function f (z) is analytic within a closed contour C (and continuous on C). If
f (z) ̸= 0 within C and | f (z)| ≤M on C, show that
| f (z)| ≤M
for all points within C.
Hint. Consider w(z) = 1/f (z).
(b)
If f (z) = 0 within the contour C, show that the foregoing result does not hold
and that it is possible to have | f (z)| = 0 at one or more points in the interior with
| f (z)| > 0 over the entire bounding contour. Cite a speciﬁc example of an analytic
function that behaves this way.
11.4.6
Evaluate
I
C
eiz
z3 dz,
for the contour a square with sides of length a > 1, centered at z = 0.

492
Chapter 11 Complex Variable Theory
11.4.7
Evaluate
I
C
sin2 z −z2
(z −a)3 dz,
where the contour encircles the point z = a.
11.4.8
Evaluate
I
C
dz
z(2z + 1),
for the contour the unit circle.
11.4.9
Evaluate
I
C
f (z)
z(2z + 1)2 dz,
for the contour the unit circle.
Hint. Make a partial fraction expansion.
11.5
LAURENT EXPANSION
Taylor Expansion
The Cauchy integral formula of the preceding section opens up the way for another deriva-
tion of Taylor’s series (Section 1.2), but this time for functions of a complex variable.
Suppose we are trying to expand f (z) about z = z0 and we have z = z1 as the nearest
point on the Argand diagram for which f (z) is not analytic. We construct a circle C cen-
tered at z = z0 with radius less than |z1 −z0| (Fig. 11.8). Since z1 was assumed to be the
nearest point at which f (z) was not analytic, f (z) is necessarily analytic on and within C.
From the Cauchy integral formula, Eq. (11.30),
f (z) =
1
2πi
I
C
f (z′)dz′
z′ −z
=
1
2πi
I
C
f (z′)dz′
(z′ −z0) −(z −z0)
=
1
2πi
I
C
f (z′)dz′
(z′ −z0)[1 −(z −z0)/(z′ −z0)].
(11.38)
Here z′ is a point on the contour C and z is any point interior to C. It is not legal yet
to expand the denominator of the integrand in Eq. (11.38) by the binomial theorem, for

11.5 Laurent Expansion
493
z′
C
z1
z
z
|z1−z0|
|z′−z0|
z0
FIGURE 11.8
Circular domains for Taylor expansion.
we have not yet proved the binomial theorem for complex variables. Instead, we note the
identity
1
1 −t = 1 + t + t2 + t3 + ··· =
∞
X
n=0
tn,
(11.39)
which may easily be veriﬁed by multiplying both sides by 1 −t. The inﬁnite series, fol-
lowing the methods of Section 1.2, is convergent for |t| < 1.
Now, for a point z interior to C,|z −z0| < |z′ −z0|, and, using Eq. (11.39), Eq. (11.38)
becomes
f (z) =
1
2πi
I
C
∞
X
n=0
(z −z0)n f (z′)dz′
(z′ −z0)n+1
.
(11.40)
Interchanging the order of integration and summation, which is valid because Eq. (11.39)
is uniformly convergent for |t| < 1 −ε, with 0 < ε < 1, we obtain
f (z) =
1
2πi
∞
X
n=0
(z −z0)n
I
C
f (z′)dz′
(z′ −z0)n+1 .
(11.41)
Referring to Eq. (11.33), we get
f (z) =
∞
X
n=0
f (n)(z0)
n!
(z −z0)n,
(11.42)
which is our desired Taylor expansion.
It is important to note that our derivation not only produces the expansion given in
Eq. (11.41); it also shows that this expansion converges when |z −z0| < |z1 −z0|. For this
reason the circle deﬁned by |z −z0| = |z1 −z0| is called the circle of convergence of our

494
Chapter 11 Complex Variable Theory
z
R
C2
C1
z¢(C1)
z¢(C2)
z0
r
Contour
line
FIGURE 11.9
Annular region for Laurent series.
|z′ −z0|C1 > |z −z0|;|z′ −z0|C2 < |z −z0|.
Taylor series. Alternatively, the distance |z1 −z0| is sometimes referred to as the radius of
convergence of the Taylor series. In view of the earlier deﬁnition of z1, we can say that:
The Taylor series of a function f (z) about any interior point z0 of a region in which
f (z) is analytic is a unique expansion that will have a radius of convergence equal to
the distance from z0 to the singularity of f (z) closest to z0, meaning that the Taylor
series will converge within this circle of convergence. The Taylor series may or may
not converge at individual points on the circle of convergence.
From the Taylor expansion for f (z) a binomial theorem may be derived. That task is
left to Exercise 11.5.2.
Laurent Series
We frequently encounter functions that are analytic in an annular region, say, between
circles of inner radius r and outer radius R about a point z0, as shown in Fig. 11.9. We
assume f (z) to be such a function, with z a typical point in the annular region. Draw-
ing an imaginary barrier to convert our region into a simply connected region, we apply
Cauchy’s integral formula to evaluate f (z), using the contour shown in the ﬁgure. Note
that the contour consists of the two circles centered at z0, labeled C1 and C2 (which can be
considered closed since the barrier is ﬁctitious), plus segments on either side of the barrier
whose contributions will cancel. We assign C2 and C1 the radii r2 and r1, respectively,
where r < r2 < r1 < R. Then, from Cauchy’s integral formula,
f (z) =
1
2πi
I
C1
f (z′)dz′
z′ −z
−
1
2πi
I
C2
f (z′)dz′
z′ −z .
(11.43)
Note that in Eq. (11.43)) an explicit minus sign has been introduced so that the contour
C2 (like C1) is to be traversed in the positive (counterclockwise) sense. The treatment of

11.5 Laurent Expansion
495
Eq. (11.43) now proceeds exactly like that of Eq. (11.38) in the development of the Taylor
series. Each denominator is written as (z′ −z0) −(z −z0) and expanded by the binomial
theorem, which is now regarded as proven (see Exercise 11.5.2).
Noting that for C1, |z′ −z0| > |z −z0|, while for C2, |z′ −z0| < |z −z0|, we ﬁnd
f (z) =
1
2πi
∞
X
n=0
(z −z0)n
I
C1
f (z′)dz′
(z′ −z0)n+1 +
1
2πi
∞
X
n=1
(z −z0)−n
I
C2
(z′ −z0)n−1 f (z′)dz′.
(11.44)
The minus sign of Eq. (11.43) has been absorbed by the binomial expansion. Labeling the
ﬁrst series S1 and the second S2 we have
S1 =
1
2πi
∞
X
n=0
(z −z0)n
I
C1
f (z′)dz′
(z′ −z0)n+1 ,
(11.45)
which has the same form as the regular Taylor expansion, convergent for |z −z0| < |z′ −
z0| = r1, that is, for all z interior to the larger circle, C1. For the second series in Eq. (6.65)
we have
S2 =
1
2πi
∞
X
n=1
(z −z0)−n
I
C2
(z′ −z0)n−1 f (z′)dz′,
(11.46)
convergent for |z −z0| > |z′ −z0| = r2, that is, for all z exterior to the smaller circle, C2.
Remember, C2 now goes counterclockwise.
These two series are combined into one series,5 known as a Laurent series, of the form
f (z) =
∞
X
n=−∞
an(z −z0)n,
(11.47)
where
an =
1
2πi
I
C
f (z′)dz′
(z′ −z0)n+1 .
(11.48)
Since convergence of a binomial expansion is not relevant to the evaluation of Eq. (11.48),
C in that equation may be any contour within the annular region r < |z −z0| < R that
encircles z0 once in a counterclockwise sense. If such an annular region of analyticity does
exist, then Eq. (11.47) is the Laurent series, or Laurent expansion, of f (z).
The Laurent series differs from the Taylor series by the obvious feature of negative
powers of (z −z0). For this reason the Laurent series will always diverge at least at z = z0
and perhaps as far out as some distance r. In addition, note that Laurent series coefﬁcients
need not come from evaluation of contour integrals (which may be very intractable). Other
techniques, such as ordinary series expansions, may provide the coefﬁcients.
Numerous examples of Laurent series appear later in this book. We limit ourselves here
to one simple example to illustrate the application of Eq. (11.47).
5Replace n by −n in S2 and add.

496
Chapter 11 Complex Variable Theory
Example 11.5.1
LAURENT EXPANSION
Let f (z) = [z(z −1)]−1. If we choose to make the Laurent expansion about z0 = 0, then
r > 0 and R < 1. These limitations arise because f (z) diverges both at z = 0 and z = 1.
A partial fraction expansion, followed by the binomial expansion of (1 −z)−1, yields the
Laurent series
1
z(z −1) = −
1
1 −z −1
z = −1
z −1 −z −z2 −z3 −··· = −
∞
X
n=−1
zn.
(11.49)
From Eqs. (11.49), (11.47), and (11.48), we then have
an =
1
2πi
I
dz′
(z′)n+2(z′ −1) =
−1
for n ≥−1,
0
for n < −1,
(11.50)
where the contour for Eq. (11.50) is counterclockwise in the annular region between z′ = 0
and |z′| = 1.
The integrals in Eq. (11.50) can also be directly evaluated by insertion of the geometric-
series expansion of (1 −z′)−1:
an = −1
2πi
I
∞
X
m=0
(z′)m
dz′
(z′)n+2 .
(11.51)
Upon interchanging the order of summation and integration (permitted because the series
is uniformly convergent), we have
an = −1
2πi
∞
X
m=0
I
(z′)m−n−2 dz′.
(11.52)
The integral in Eq. (11.52) (including the initial factor 1/2πi, but not the minus sign) was
shown in Exercise 11.4.1 to be an integral representation of the Kronecker delta, and is
therefore equal to δm,n+1. The expression for an then reduces to
an = −
∞
X
m=0
δm,n+1 =
−1,
n ≥−1,
0,
n < −1,
in agreement with Eq. (11.50).
■
Exercises
11.5.1
Develop the Taylor expansion of ln(1 + z).
ANS.
∞
X
n=1
(−1)n−1 zn
n .

11.6 Singularities
497
11.5.2
Derive the binomial expansion
(1 + z)m = 1 + mz + m(m −1)
1 · 2
z2 + ··· =
∞
X
n=0
m
n

zn
for m, any real number. The expansion is convergent for |z| < 1. Why?
11.5.3
A function f (z) is analytic on and within the unit circle. Also, | f (z)| < 1 for |z| ≤1
and f (0) = 0. Show that | f (z)| < |z| for |z| ≤1.
Hint. One approach is to show that f (z)/z is analytic and then to express [ f (z0)/z0]n
by the Cauchy integral formula. Finally, consider absolute magnitudes and take the nth
root. This exercise is sometimes called Schwarz’s theorem.
11.5.4
If f (z) is a real function of the complex variable z = x + iy, that is, f (x) = f ∗(x), and
the Laurent expansion about the origin, f (z) = P anzn, has an = 0 for n < −N, show
that all of the coefﬁcients an are real.
Hint. Show that zN f (z) is analytic (via Morera’s theorem, Section 11.4).
11.5.5
Prove that the Laurent expansion of a given function about a given point is unique;
that is, if
f (z) =
∞
X
n=−N
an(z −z0)n =
∞
X
n=−N
bn(z −z0)n,
show that an = bn for all n.
Hint. Use the Cauchy integral formula.
11.5.6
Obtain the Laurent expansion of ez/z2 about z = 0.
11.5.7
Obtain the Laurent expansion of zez/(z −1) about z = 1.
11.5.8
Obtain the Laurent expansion of (z −1)e1/z about z = 0.
11.6
SINGULARITIES
Poles
We deﬁne a point z0 as an isolated singular point of the function f (z) if f (z) is not
analytic at z = z0 but is analytic at all neighboring points. There will therefore be a Laurent
expansion about an isolated singular point, and one of the following statements will be true:
1.
The most negative power of z −z0 in the Laurent expansion of f (z) about z = z0 will
be some ﬁnite power, (z −z0)−n, where n is an integer, or
2.
The Laurent expansion of f (z) about z −z0 will continue to negatively inﬁnite powers
of z −z0.

498
Chapter 11 Complex Variable Theory
In the ﬁrst case, the singularity is called a pole, and is more speciﬁcally identiﬁed as
a pole of order n. A pole of order 1 is also called a simple pole. The second case is not
referred to as a “pole of inﬁnite order,” but is called an essential singularity.
One way to identify a pole of f (z) without having available its Laurent expansion is to
examine
lim
z→z0(z −z0)n f (z0)
for various integers n. The smallest integer n for which this limit exists (i.e., is ﬁnite) gives
the order of the pole at z = z0. This rule follows directly from the form of the Laurent
expansion.
Essential singularities are often identiﬁed directly from their Laurent expansions. For
example,
e1/z = 1 + 1
z + 1
2!
1
z
2
+ ···
=
∞
X
n=0
1
n!
1
z
n
clearly has an essential singularity at z = 0. Essential singularities have many pathologi-
cal features. For instance, we can show that in any small neighborhood of an essential
singularity of f (z) the function f (z) comes arbitrarily close to any (and therefore every)
preselected complex quantity w0.6 Here, the entire w-plane is mapped by f into the neigh-
borhood of the point z0.
The behavior of f (z) as z →∞is deﬁned in terms of the behavior of f (1/t) as t →0.
Consider the function
sin z =
∞
X
n=0
(−1)nz2n+1
(2n + 1)! .
(11.53)
As z →∞, we replace the z by 1/t to obtain
sin
1
t

=
∞
X
n=0
(−1)n
(2n + 1)!t2n+1 .
(11.54)
It is clear that sin(1/t) has an essential singularity at t = 0, from which we conclude that
sin z has an essential singularity at z = ∞. Note that although the absolute value of sin x
for all real x is equal to or less than unity, the absolute value of siniy = i sinh y increases
exponentially without limit as y increases.
A function that is analytic throughout the ﬁnite complex plane except for isolated poles
is called meromorphic. Examples are ratios of two polynomials, also tan z and cot z. As
previously mentioned, functions that have no singularities in the ﬁnite complex plane are
called entire functions. Examples are exp z, sin z, and cos z.
6This theorem is due to Picard. A proof is given by E. C. Titchmarsh, The Theory of Functions, 2nd ed. New York: Oxford
University Press (1939).

11.6 Singularities
499
Branch Points
In addition to the isolated singularities identiﬁed as poles or essential singularities, there
are singularities uniquely associated with multivalued functions. It is useful to work with
these functions in ways that to the maximum possible extent remove ambiguity as to the
function values. Thus, if at a point z0 (at which f (z) has a derivative) we have chosen a
speciﬁc value of the multivalued function f (z), then we can assign to f (z) values at nearby
points in a way that causes continuity in f (z). If we think of a succession of closely spaced
points as in the limit of zero spacing deﬁning a path, our current observation is that a given
value of f (z0) then leads to a unique deﬁnition of the value of f (z) to be assigned to
each point on the path. This scheme creates no ambiguity so long as the path is entirely
open, meaning that the path does not return to any point previously passed. But if the path
returns to z0, thereby forming a closed loop, our prescription might lead, upon the return,
to a different one of the multiple values of f (z0).
Example 11.6.1
VALUE OF z1/2 ON A CLOSED LOOP
We consider f (z) = z1/2 on the path consisting of counterclockwise passage around the
unit circle, starting and ending at z = +1. At the start point, where z1/2 has the multiple
values +1 and −1, let us choose f (z) = +1. See Fig. 11.10. Writing f (z) = eiθ/2, we
note that this form (with θ = 0) is consistent with the desired starting value of f (z), +1.
In the ﬁgure, the start point is labeled A. Next, we note that passage counterclockwise on
the unit circle corresponds to an increase in θ, so that at the points marked B, C, and D in
the ﬁgure, the respective values of θ are π/2, π, and 3π/2. Note that because of the path
we have decided to take, we cannot assign to point C the θ value −π or to point D the θ
value −π/2. Continuing further along the path, when we return to point A the value of θ
has become 2π (not zero).
Now that we have identiﬁed the behavior of θ, let’s examine what happens to f (z). At
the points B, C, and D, we have
f (zB) = eiθB/2 = eiπ/4 = 1 + i
√
2
,
f (zC) = eiπ/2 = +i,
f (zD) = e3iπ/4 = −1 + i
√
2
.
y
B
C
A
D
x
θ
FIGURE 11.10
Path encircling z = 0 for evaluation of z1/2.

500
Chapter 11 Complex Variable Theory
y
B
C
A
D
x
θ
FIGURE 11.11
Path not encircling z = 0 for evaluation of z1/2.
When we return to point A, we have f (+1) = eiπ = −1, which is the other value of the
multivalued function z1/2.
If we continue for a second counterclockwise circuit of the unit circle, the value of θ
would continue to increase, from 2π to 4π (reached when we arrive at point A after the
second loop). We now have f (+1) = e(4πi)/2 = e2πi = 1, so a second circuit has brought
us back to the original value. It should now be clear that we are only going to be able to
obtain two different values of z1/2 for the same point z.
■
Example 11.6.2
ANOTHER CLOSED LOOP
Let’s now see what happens to the function z1/2 as we pass counterclockwise around a
circle of unit radius centered at z = +2, starting and ending at z = +3. See Fig. 11.11.
At z = 3, the values of f (z) are +
√
3 and −
√
3; let’s start with f (z A) = +
√
3. As we
move from point A through point B to point C, note from the ﬁgure that the value of θ
ﬁrst increases (actually, to 30◦) and then decreases again to zero; further passage from C
to D and back to A causes θ ﬁrst to decrease (to −30◦) and then to return to zero at A. So
in this example the closed loop does not bring us to a different value of the multivalued
function z1/2.
■
The essential difference between these two examples is that in the ﬁrst, the path encircled
z = 0; in the second it did not. What is special about z = 0 is that (from a complex-variable
viewpoint) it is singular; the function z1/2 does not have a derivative there. The lack of a
well-deﬁned derivative means that ambiguity in the function value will result from paths
that circle such a singular point, which we call a branch point. The order of a branch
point is deﬁned as the number of paths around it that must be taken before the function
involved returns to its original value; in the case of z1/2, we saw that the branch point at
z = 0 is of order 2.
We are now ready to see what must be done to cause a multivalued function to be
restricted to single-valuedness on a portion of the complex plane. We simply need to pre-
vent its evaluation on paths that encircle a branch point. We do so by drawing a line (known
as a branch line, or more commonly, a branch cut) that the evaluation path cannot cross;
the branch cut must start from our branch point and continue to inﬁnity (or if consistent
with maintaining single-valuedness) to another ﬁnite branch point. The precise path of a
branch cut can be chosen freely; what must be chosen appropriately are its endpoints.
Once appropriate branch cut(s) have been drawn, the originally multivalued function has
been restricted to being single-valued in the region bounded by the branch cut(s); we call
the function as made single-valued in this way a branch of our original function. Since we

11.6 Singularities
501
could construct such a branch starting from any one of the values of the original function
at a single arbitrary point in our region, we identify our multivalued function as having
multiple branches. In the case of z1/2, which is double-valued, the number of branches
is two.
Note that a function with a branch point and a corresponding branch cut will not be
continuous across the cut line. Hence line integrals in opposite directions on the two sides
of the branch cut will not generally cancel each other. Branch cuts, therefore, are real
boundaries to a region of analyticity, in contrast to the artiﬁcial barriers we introduced in
extending Cauchy’s integral theorem to multiply connected regions.
While from a fundamental viewpoint all branches of a multivalued function f (z) are
equally legitimate, it is often convenient to agree on the branch to be used, and such a
branch is sometimes called the principal branch, with the value of f (z) on that branch
called its principal value. It is common to take the branch of z1/2 which is positive for
real, positive z as its principal branch.
An observation that is important for complex analysis is that by drawing appropriate
branch cut(s), we have restricted a multivalued function to single-valuedness, so that it
can be an analytic function within the region bounded by the branch cut(s), and we can
therefore apply Cauchy’s two theorems to contour integrals within the region of analyticity.
Example 11.6.3
ln z HAS AN INFINITE NUMBER OF BRANCHES
Here we examine the singularity structure of ln z. As we already saw in Eq. (1.138), the
logarithm is multivalued, with the polar representation
ln z = ln

rei(θ+2nπ)
= lnr + i(θ + 2nπ),
(11.55)
where n can have any positive or negative integer value.
Noting that ln z is singular at z = 0 (it has no derivative there), we now identify z = 0
as a branch point. Let’s consider what happens if we encircle it by a counterclockwise
path on a circle of radius r, starting from the initial value lnr, at z = r = reiθ with θ = 0.
Every passage around the circle will add 2π to θ, and after n complete circuits the value
we have for ln z will be lnr + 2nπi. The branch point of ln z at z = 0 is of inﬁnite order,
corresponding to the inﬁnite number of its multiple values. (By encircling z = 0 repeatedly
in the clockwise direction, we can also reach all negative integer values of n.)
We can make ln z single-valued by drawing a branch cut from z = 0 to z = ∞in any
way (though there is ordinarily no reason to use cuts that are not straight lines). It is typical
to identify the branch with n = 0 as the principal branch of the logarithm. Incidentally, we
note that the inverse trigonometric functions, which can be written in terms of logarithms,
as in Eq. (1.137), will also be inﬁnitely multivalued, with principal values that are usually
chosen on a branch that will yield real values for real z. Compare with the usual choices of
the values assigned the real-variable forms of sin−1 x = arcsin x, etc.
■
Using the logarithm, we are now in a position to look at the singularity structures of
expressions of the form z p, where both z and p may be complex. To do so, we write
z = eln z,
so z p = ep ln z,
(11.56)
which is single-valued if p is an integer, t-valued if p is a real rational fraction (in lowest
terms) of the form s/t, and inﬁnitely multivalued otherwise.

502
Chapter 11 Complex Variable Theory
Example 11.6.4
MULTIPLE BRANCH POINTS
Consider the function
f (z) = (z2 −1)1/2 = (z + 1)1/2(z −1)1/2.
The ﬁrst factor on the right-hand side, (z +1)1/2, has a branch point at z = −1. The second
factor has a branch point at z = +1. At inﬁnity f (z) has a simple pole. This is best seen
by substituting z = 1/t and making a binomial expansion at t = 0:
(z2 −1)1/2 = 1
t (1 −t2)1/2 = 1
t
∞
X
n=0
1/2
n

(−1)nt2n = 1
t −1
2t −1
8t3 + ··· .
We want to make f (z) single-valued by making appropriate branch cut(s). There are many
ways to accomplish this, but one we wish to investigate is the possibility of making a
branch cut from z = −1 to z = +1, as shown in Fig. 11.12.
To determine whether this branch cut makes our f (z) single-valued, we need to see what
happens to each of the multivalent factors in f (z) as we move around on its Argand dia-
gram. Figure 11.12 also identiﬁes the quantities that are relevant for this purpose, namely
those that relate a point P to the branch points. In particular, we have written the position
relative to the branch point at z = 1 as z −1 = ρeiϕ, with the position relative to z = −1
denoted z + 1 = reiθ. With these deﬁnitions, we have
f (z) = r1/2ρ1/2e(θ+ϕ)/2.
Our mission is to note how ϕ and θ change as we move along the path, so that we can use
the correct value of each for evaluating f (z).
We consider a closed path starting at point A in Fig. 11.13, proceeding via points B
through F, then back to A. At the start point, we choose θ = ϕ = 0, thereby causing the
multivalued f (z A) to have the speciﬁc value +
√
3. As we pass above z = +1 on the way
to point B, θ remains essentially zero, but ϕ increases from zero to π. These angles do not
change as we pass from B to C, but on going to point D, θ increases to π, and then, passing
below z = −1 on the way to point E, it further increases to 2π (not zero!). Meanwhile, ϕ
remains essentially at π. Finally, returning to point A below z = +1, ϕ increases to 2π,
so that upon the return to point A both ϕ and θ have become 2π. The behavior of these
angles and the values of (θ + ϕ)/2 (the argument of f (z)) are tabulated in Table 11.1.
y
P
p
t
x
θ
ϕ
−1
+1
FIGURE 11.12
Possible branch cut for Example 11.6.4 and the quantities relating
a point P to the branch points.

11.6 Singularities
503
A
D
B
C
F
E
FIGURE 11.13
Path around the branch cut in Example 11.6.4.
Table 11.1
Phase Angles, Path in Fig. 11.13
Point
θ
ϕ
(θ + ϕ)/2
A
0
0
0
B
0
π
π/2
C
0
π
π/2
D
π
π
π
E
2π
π
π/2
F
2π
π
3π/2
A
2π
2π
2π
Two features emerge from this analysis:
1.
The phase of f (z) at points B and C is not the same as that at points E and F. This
behavior can be expected at a branch cut.
2.
The phase of f (z) at point A′ (the return to A) exceeds that at point A by 2π, meaning
that the function f (z) = (z2 −1)1/2 is single-valued for the contour shown, encircling
both branch points.
What actually happened is that each of the two multivalued factors contributed a sign
change upon passage around the closed loop, so the two factors together restored the origi-
nal sign of f (z).
Another way we could have made f (z) single-valued would have been to make a sepa-
rate branch cut from each branch point to inﬁnity; a reasonable way to do this would be to
make cuts on the real axis for all x > 1 and for all x < −1. This alternative is explored in
Exercises 11.6.2 and 11.6.4.
■
Analytic Continuation
We saw in Section 11.5 that a function f (z) which is analytic within a region can be
uniquely expanded in a Taylor series about any interior point z0 of the region of analyti-
city, and that the resulting expansion will be convergent within a circle of convergence
extending to the singularity of f (z) closest to z0. Since

504
Chapter 11 Complex Variable Theory
•
The coefﬁcients in the Taylor series are proportional to the derivatives of f (z),
•
An analytic function has derivatives of all orders that are independent of direction, and
therefore
•
The values of f (z) on a single ﬁnite line segment with z0 as a interior point will sufﬁce
to determine all derivatives of f (z) at z = z0,
we conclude that if two apparently different analytic functions (e.g., a closed expression
vs. an integral representation or a power series) have values that coincide on a range as
restricted as a single ﬁnite line segment, then they are actually the same function within
the region where both functional forms are deﬁned.
The above conclusion will provide us with a technique for extending the deﬁnition of an
analytic function beyond the range of any particular functional form initially used to deﬁne
it. All we will need to do is to ﬁnd another functional form whose range of deﬁnition is not
entirely included in that of the initial form and which yields the same function values on at
least a ﬁnite line segment within the area where both functional forms are deﬁned.
To make the approach more concrete, consider the situation illustrated in Fig. 11.14,
where a function f (z) is deﬁned by its Taylor expansion about a point z0 with a circle
of convergence C0 deﬁned by the singularity nearest to z0, labeled zs. If we now make
a Taylor expansion about some point z1 within C0 (which we can do because f (z) has
known values in the neighborhood of z1), this new expansion may have a circle of con-
vergence C1 that is not entirely within C0, thereby deﬁning a function that is analytic in
the region that is the union of C1 and C2. Note that if we need to obtain actual values of
f (z) for z within the intersection of C0 and C1 we may use either Taylor expansion, but
in the region within only one circle we must use the expansion that is valid there (the other
expansion will not converge). A generalization of the above analysis leads to the beautiful
and valuable result that if two analytic functions coincide in any region, or even on any
ﬁnite line segment, they are the same function, and therefore deﬁned over the entire range
of both function deﬁnitions.
After Weierstrass this process of enlarging the region in which we have the speciﬁcation
of an analytic function is called analytic continuation, and the process may be carried out
repeatedly to maximize the region in which the function is deﬁned. Consider the situation
pictured in Fig. 11.15, where the only singularity of f (z) is at zs and f (z) is originally
deﬁned by its Taylor expansion about z0, with circle of convergence C0. By making ana-
lytic continuations as shown by the series of circles C1,..., we can cover the entire annular
region of analyticity shown in the ﬁgure, and can use the original Taylor series to generate
new expansions that apply to regions within the other circles.
C0
C1
z0
z1
zs
FIGURE 11.14
Analytic continuation. One step.

11.6 Singularities
505
C1
C2
C3
C4
C5
C0
z0
zs
FIGURE 11.15
Analytic continuation. Many steps.
i
P
1
FIGURE 11.16
Radii of convergence of power-series expansions for Example 11.6.5.
Example 11.6.5
ANALYTIC CONTINUATION
Consider these two power-series expansions:
f1(z) =
∞
X
n=0
(−1)n(z −1)n,
(11.57)
f2(z) =
∞
X
n=0
in−1(z −i)n.
(11.58)
Each has a unit radius of convergence; the circles of convergence overlap, as can be seen
from Fig. 11.16.
To determine whether these expansions represent the same analytic function in over-
lapping domains, we can check to see if f1(z) = f2(z) for at least a line segment in the
region of overlap. A suitable line is the diagonal that connects the origin with 1 + i, pass-
ing through the intermediate point (1 + i)/2. Setting z = (α + 1
2)(1 + i) (chosen to make
α = 0 an interior point of the overlap region), we expand f1 and f2 about α = 0 to ﬁnd out

506
Chapter 11 Complex Variable Theory
whether their power series coincide. Initially we have (as functions of α)
f1 =
∞
X
n=0
(−1)n

(1 + i)α −1 −i
2
n
,
f2 =
∞
X
n=0
in−1

(1 + i)α + 1 −i
2
n
.
Applying the binomial theorem to obtain power series in α, and interchanging the order of
the two sums,
f1 =
∞
X
j=0
(−1) j(1 + i) jα j
∞
X
n= j
n
j
1 −i
2
n−j
,
f2 =
∞
X
j=0
i j−1(1 + i) jα j
∞
X
n= j
in−j
n
j
1 −i
2
n−j
=
∞
X
j=0
1
i (−1) j(1 −i) jα j
∞
X
n= j
n
j
1 + i
2
n−j
.
To proceed further we need to evaluate the summations over n. Referring to Exercise 1.3.5,
where it was shown that
∞
X
n= j
n
j

xn−j =
1
(1 −x) j+1 ,
we get
f1 =
∞
X
j=0
(−1) j(1 + i) jα j

2
1 + i
 j+1
=
∞
X
j=0
(−1) j 2 j+1α j
1 + i
,
f2 =
∞
X
j=0
1
i (−1) j(1 −i) jα j

2
1 −i
 j+1
=
∞
X
j=0
(−1) j 2 j+1α j
i(1 −i)
= f1,
conﬁrming that f1 and f2 are the same analytic function, now deﬁned over the union of
the two circles in Fig. 11.16.
Incidentally, both f1 and f2 are expansions of 1/z (about the respective points 1 and i),
so 1/z could also be regarded as an analytic continuation of f1, f2, or both to the entire
complex plane except the singular point at z = 0. The expansion in powers of α is also
a representation of 1/z, but its range of validity is only a circle of radius 1/
√
2 about
(1 + i)/2 and it does not analytically continue f (z) outside the union of C1 and C2.
■
The use of power series is not the only mechanism for carrying out analytic continu-
ations; an alternative and powerful method is the use of functional relations, which are
formulas that relate values of the same analytic function f (z) at different z. As an exam-
ple of a functional relation, the integral representation of the gamma function, given in

11.6 Singularities
507
Table 1.2, can be manipulated (see Chapter 13) to show that 0(z + 1) = z0(z), consis-
tent with the elementary result that n! = n(n −1)!. This functional relation can be used
to analytically continue 0(z) to values of z for which the integral representation does not
converge.
Exercises
11.6.1
As an example of an essential singularity consider e1/z as z approaches zero. For any
complex number z0, z0 ̸= 0, show that
e1/z = z0
has an inﬁnite number of solutions.
11.6.2
Show that the function
w(z) = (z2 −1)1/2
is single-valued if we make branch cuts on the real axis for x > 1 and for x < −1.
11.6.3
A function f (z) can be represented by
f (z) = f1(z)
f2(z),
in which f1(z) and f2(z) are analytic. The denominator, f2(z), vanishes at z = z0,
showing that f (z) has a pole at z = z0. However, f1(z0) ̸= 0, f ′
2(z0) ̸= 0. Show that
a−1, the coefﬁcient of (z −z0)−1 in a Laurent expansion of f (z) at z = z0, is given by
a−1 = f1(z0)
f ′
2(z0).
11.6.4
Determine a unique branch for the function of Exercise 11.6.2 that will cause the value
it yields for f (i) to be the same as that found for f (i) in Example 11.6.4. Although
Exercise 11.6.2 and Example 11.6.4 describe the same multivalued function, the speciﬁc
values assigned for various z will not agree everywhere, due to the difference in the
location of the branch cuts. Identify the portions of the complex plane where both these
descriptions do and do not agree, and characterize the differences.
11.6.5
Find all singularities of
z−1/3 +
z−1/4
(z −3)3 + (z −2)1/2,
and identify their types (e.g., second-order branch point, ﬁfth-order pole, ...). Include
any singularities at the point at inﬁnity.
Note. A branch point is of nth order if it requires n, but no fewer, circuits around the
point to restore the original value.
11.6.6
The function F(z) = ln(z2 + 1) is made single-valued by straight-line branch cuts
from (x, y) = (0,−1) to (−∞,−1) and from (0,+1) to (0,+∞). See Fig. 11.17. If
F(0) = −2πi, ﬁnd the value of F(i −2).

508
Chapter 11 Complex Variable Theory
i−2
i
o
−i
FIGURE 11.17
Branch cuts for Exercise 11.6.6.
11.6.7
Show that negative numbers have logarithms in the complex plane. In particular, ﬁnd
ln(−1).
ANS.
ln(−1) = iπ.
11.6.8
For noninteger m, show that the binomial expansion of Exercise 11.5.2 holds only for a
suitably deﬁned branch of the function (1 + z)m. Show how the z-plane is cut. Explain
why |z| < 1 may be taken as the circle of convergence for the expansion of this branch,
in light of the cut you have chosen.
11.6.9
The Taylor expansion of Exercises 11.5.2 and 11.6.8 is not suitable for branches other
than the one suitably deﬁned branch of the function (1 + z)m for noninteger m. (Note
that other branches cannot have the same Taylor expansion since they must be distin-
guishable.) Using the same branch cut of the earlier exercises for all other branches,
ﬁnd the corresponding Taylor expansions, detailing the phase assignments and Taylor
coefﬁcients.
11.6.10
(a)
Develop a Laurent expansion of f (z) = [z(z −1)]−1 about the point z = 1 valid
for small values of |z −1|. Specify the exact range over which your expansion
holds. This is an analytic continuation of the inﬁnite series in Eq. (11.49).
(b)
Determine the Laurent expansion of f (z) about z = 1 but for |z −1| large.
Hint. Make a partial fraction decomposition of this function and use the geometric
series.
11.6.11
(a)
Given f1(z) =
R ∞
0 e−ztdt (with t real), show that the domain in which f1(z) exists
(and is analytic) is Re(z) > 0.
(b)
Show that f2(z) = 1/z equals f1(z) over Re(z) > 0 and is therefore an analytic
continuation of f1(z) over the entire z-plane except for z = 0.
(c)
Expand 1/z about the point z = −i. You will have
f3(z) =
∞
X
n=0
an(z + i)n.
What is the domain of this formula for f3(z)?
ANS.
1
z = i
∞
X
n=0
i−n(z + i)n,
|z + i| < 1.

11.7 Calculus of Residues
509
11.7
CALCULUS OF RESIDUES
Residue Theorem
If the Laurent expansion of a function,
f (z) =
∞
X
n=−∞
an(z −z0)n,
is integrated term by term by using a closed contour that encircles one isolated singular
point z0 once in a counterclockwise sense, we obtain, applying Eq. (11.29),
an
I
(z −z0)ndz = 0,
n ̸= −1.
(11.59)
However, for n = −1, Eq. (11.29) yields
a−1
I
(z −z0)−1dz = 2πia−1.
(11.60)
Summarizing Eqs. (11.59) and (11.60), we have
I
f (z)dz = 2πia−1.
(11.61)
The constant a−1, the coefﬁcient of (z −z0)−1 in the Laurent expansion, is called the
residue of f (z) at z = z0.
Now consider the evaluation of the integral, over a closed contour C, of a function that
has isolated singularities at points z1, z2, .... We can handle this integral by deforming our
contour as shown in Fig. 11.18. Cauchy’s integral theorem (Section 11.3) then leads to
I
C
f (z)dz +
I
C1
f (z)dz +
I
C2
f (z)dz + ··· = 0,
(11.62)
C2
C1
C0
C
ℑz=y
ℜz= x
FIGURE 11.18
Excluding isolated singularities.

510
Chapter 11 Complex Variable Theory
where C is in the positive, counterclockwise direction, but the contours C1,C2,... , that,
respectively, encircle z1, z2,... are all clockwise. Thus, referring to Eq. (11.61), the inte-
grals Ci about the individual isolated singularities have the values
I
Ci
f (z)dz = −2πia−1,i,
(11.63)
where a−1,i is the residue obtained from the Laurent expansion about the singular point
z = zi. The negative sign comes from the clockwise integration. Combining Eqs. (11.62)
and (11.63), we have
I
C
f (z)dz = 2πi(a−1,1 + a−1,2 + ···)
= 2πi (sum of the enclosed residues).
(11.64)
This is the residue theorem. The problem of evaluating a set of contour integrals is
replaced by the algebraic problem of computing residues at the enclosed singular points.
Computing Residues
It is, of course, not necessary to obtain an entire Laurent expansion of f (z) about z = z0
to identify a−1, the coefﬁcient of (z −z0)−1 in the expansion. If f (z) has a simple pole at
z −z0, then, with an the coefﬁcients in the expansion of f (z),
(z −z0) f (z) = a−1 + a0(z −z0) + a1(z −z0)2 + ··· ,
(11.65)
and, recognizing that (z −z0) f (z) may not have a form permitting an obvious cancellation
of the factor z −z0, we take the limit of Eq. (11.65) as z →z0:
a−1 = lim
z→z0

(z −z0) f (z)

.
(11.66)
If there is a pole of order n > 1 at z −z0, then (z −z0)n f (z) must have the expansion
(z −z0)n f (z) = a−n + ··· + a−1(z −z0)n−1 + a0(z −z0)n + ··· .
(11.67)
We see that a−1 is the coefﬁcient of (z −z0)n−1 in the Taylor expansion of (z −z0)n f (z),
and therefore we can identify it as satisfying
a−1 =
1
(n −1)! lim
z→z0
 dn−1
dzn−1

(z −z0)n f (z)

,
(11.68)
where a limit is indicated to take account of the fact that the expression involved may
be indeterminate. Sometimes the general formula, Eq. (11.68), is found to be more com-
plicated than the judicious use of power-series expansions. See items 4 and 5 in Exam-
ple 11.7.1 below.
Essential singularities will also have well-deﬁned residues, but ﬁnding them may be
more difﬁcult. In principle, one can use Eq. (11.48) with n = −1, but the integral involved
may seem intractable. Sometimes the easiest route to the residue is by ﬁrst ﬁnding the
Laurent expansion.

11.7 Calculus of Residues
511
Example 11.7.1
COMPUTING RESIDUES
Here are some examples:
1.
Residue of
1
4z+1 at z = −1
4 is limz=−1
4
 
z+ 1
4
4z+1
!
= 1
4,
2.
Residue of
1
sin z at z = 0 is limz→0

z
sin z

= 1,
3.
Residue of
ln z
z2+4 at z = 2eπi is
lim
z→2eπi
(z −2eπi)ln z
z2 + 4

= (ln2 + πi)
4i
= π
4 −i ln2
4
,
4.
Residue of
z
sin2 z at z = π; the pole is second order, and the residue is given by
1
1! lim
z→π
 d
dz
z(z −π)
sin2 z

.
However, it may be easier to make the substitution w = z −π, to note that sin2 z =
sin2 w, and to identify the residue as the coefﬁcient of 1/w in the expansion of (w +
π)/sin2 w about w = 0. This expansion can be written
w + π

w −w3
3! + ···
2 =
w + π
w2 −w4
3 + ···
.
The denominator expands entirely into even powers of w, so the π in the numerator
cannot contribute to the residue. Then, from the w in the numerator and the leading
term of the denominator, we ﬁnd the residue to be 1.
5.
Residue of f (z) = cotπz
z(z+2) at z = 0.
The pole at z = 0 is second-order, and direct application of Eq. (11.48) leads to a
complicated indeterminate expression requiring multiple applications of l’Hôpital’s
rule. Perhaps easier is to introduce the initial terms of the expansions about z = 0:
cotπz = (πz)−1 + O(z), 1/(z + 2) = 1
2[1 −(z/2) + O(z2)], reaching
f (z) = 1
z
 1
πz + O(z)
1
2
h
1 −z
2 + O(z2)
i
,
from which we can read out the residue as the coefﬁcient of z−1, namely −1/4π.
6.
Residue of e−1/z at z = 0. This is at an essential singularity; from the Taylor series of
ew with w = −1/z, we have
e−1/z = 1 −1
z + 1
2!

−1
z
2
+ ··· ,
from which we read out the value of the residue, −1.
■

512
Chapter 11 Complex Variable Theory
Cauchy Principal Value
Occasionally an isolated pole will be directly on the contour of an integration, causing the
integral to diverge. A simple example is provided by an attempt to evaluate the real integral
b
Z
−a
dx
x ,
(11.69)
which is divergent because of the logarithmic singularity at x = 0; note that the indeﬁnite
integral of x−1 is ln x. However, the integral in Eq. (11.69) can be given a meaning if we
obtain a convergent form when replaced by a limit of the form
lim
δ→0+
−δ
Z
−a
dx
x +
b
Z
δ
dx
x .
(11.70)
To avoid issues with the logarithm of negative values of x, we change the variable in
the ﬁrst integral to y = −x, and the two integrals are then seen to have the respective
values lnδ −lna and lnb −lnδ, with sum lnb −lna. What has happened is that the
increase toward +∞as 1/x approaches zero from positive values of x is compensated by
a decrease toward −∞as 1/x approaches zero from negative x. This situation is illustrated
graphically in Fig. 11.19.
Note that the procedure we have described does not make the original integral of
Eq. (11.69) convergent. In order for that integral to be convergent, it would be necessary
F(x)≈a−1
x−x0
x0−d
x0+d
x
x0
FIGURE 11.19
Cauchy principal value cancellation, integral of 1/z.

11.7 Calculus of Residues
513
that
lim
δ1,δ2→0+


−δ1
Z
−a
dx
x +
b
Z
δ2
dx
x


exist (meaning that the limit has a unique value) when δ1 and δ2 approach zero indepen-
dently. However, different rates of approach to zero by δ1 and δ2 will cause a change in
value of the integral. For example, if δ2 = 2δ1, then an evaluation like that of Eq. (11.70)
would yield the result (lnδ1 −lna) + (lnb −lnδ2) = lnb −lna −ln2. The limit then has
no deﬁnite value, conﬁrming our original statement that the integral diverges.
Generalizing from the above example, we deﬁne the Cauchy principal value of the real
integral of a function f (x) with an isolated singularity on the integration path at the point
x0 as the limit
lim
δ→0+
x0−δ
Z
f (x)dx +
Z
x0+δ
f (x)dx.
(11.71)
The Cauchy principal value is sometimes indicated by preceding the integral sign by P or
by drawing a horizontal line through the integration sign, as in
P
Z
f (x)dx
or
Z
f (x)dx.
This notation, of course, presumes that the location of the singularity is known.
Example 11.7.2
A CAUCHY PRINCIPAL VALUE
Consider the integral
I =
∞
Z
0
sin x
x
dx.
(11.72)
If we substitute for sin x the equivalent formula
sin x = eix −e−ix
2i
,
we then have
I =
∞
Z
0
eix −e−ix
2ix
dx.
(11.73)
We would like to separate this expression for I into two terms, but if we do so, each will
become a logarithmically divergent integral. However, if we change the integration range
in Eq. (11.72), originally (0,∞), to (δ,∞), that integral remains unchanged in the limit

514
Chapter 11 Complex Variable Theory
of small δ, and the integrals in Eq. (11.73) remain convergent so long as δ is not precisely
zero. Then, rewriting the second of the two integrals in Eq. (11.73), to reach
∞
Z
δ
e−ix
2ix dx =
−δ
Z
−∞
eix
2ix dx,
we see that the two integrals which together form I can be written (in the limit δ →0+) as
the Cauchy principal value integral
I =
∞
Z
−∞
eix
2ix dx.
(11.74)
■
The Cauchy principal value has implications for complex variable theory. Suppose now
that, instead of having a break in the integration path from x0 −δ to x0 + δ, we connect
the two parts of the path by a circular arc passing, in the complex plane, either above or
below the singularity at x0. Let’s continue the discussion in conventional complex-variable
notation, denoting the singular point as z0, so our arc will be a half circle (of radius δ)
passing either counterclockwise below the singularity at z0 or clockwise above z0. We
restrict further analysis to singularities no stronger than 1/(z −z0), so we are dealing with
a simple pole. Looking at the Laurent expansion of the function f (z) to be integrated,
it will have initial terms
a−1
z −z0
+ a0 + ··· ,
and the integration over a semicircle of radius δ will take (in the limit δ →0+) one of the
two forms (in the polar representation z −z0 = reiθ, with dz = ireiθdθ and r = δ):
Iover =
0
Z
π
dθ iδeiθ h a−1
δeiθ + a0 + ···
i
=
0
Z
π

ia−1 + iδeiθa0 + ···

dθ →−iπa−1,
(11.75)
Iunder =
2π
Z
π
dθ iδeiθ h a−1
δeiθ + a0 + ···
i
=
2π
Z
π

ia−1 + iδeiθa0 + ···

dθ →iπa−1 .
(11.76)
Note that all but the ﬁrst term of each of Eqs. (11.75) and (11.76) vanishes in the limit
δ →0+, and that each of these equations yields a result that is in magnitude half the value
that would have been obtained by a full circuit around the pole. The signs associated with
the semicircles correspond as expected to the direction of travel, and the two semicircular
integrals average to zero.
We occasionally will want to evaluate a contour integral of a function f (z) on a closed
path that includes the two pieces of a Cauchy principal value integral
R
f (z)dz with a
simple pole at z0, a semicircular arc connecting them at the singularity, and whatever other
curve C is needed to close the contour (see Fig. 11.20).

11.7 Calculus of Residues
515
C1
C2
x
r
R
FIGURE 11.20
A contour including a Cauchy principal value integral.
These contributions combine as follows, noting that in the ﬁgure the contour passes over
the point z0:
Z
f (z)dz + Iover +
Z
C2
f (z)dz = 2πi
X
residues (other than at z0),
which rearranges to give
Z
f (z)dz = −Iover −
Z
C2
f (z)dz + 2πi
X
residues (other than at z0).
(11.77)
On the other hand, we could have chosen the contour to pass under z0, in which case,
instead of Eq. (11.77) we would get
Z
f (z)dz = −Iunder −
Z
C2
f (z)dz + 2πi
X
residues (other than at z0) + 2πia−1,
(11.78)
where the residue denoted a−1 is from the pole at z0. Equations (11.77) and (11.78) are in
agreement because 2πia−1 −Iunder = −Iover, so for the purpose of evaluating the Cauchy
principal value integral, it makes no difference whether we go below or above the singu-
larity on the original integration path.
Pole Expansion of Meromorphic Functions
Analytic functions f (z) that have only isolated poles as singularities are called meromor-
phic. Mittag-Lefﬂer showed that, instead of making an expansion about a single regu-
lar point (a Taylor expansion) or about an isolated singular point (a Laurent expansion),
it was also possible to make an expansion each of whose terms arises from a different
pole of f (z). Mittag-Lefﬂer’s theorem assumes that f (z) is analytic at z =0 and at all
other points (excluding inﬁnity) with the exception of discrete simple poles at points z1,
z2, ... , with respective residues b1, b2, ... . We choose to order the poles in a way such
that 0 < |z1| ≤|z2| ≤···, and we assume that in the limit of large z, | f (z)/z| →0. Then,

516
Chapter 11 Complex Variable Theory
Mittag-Lefﬂer’s theorem states that
f (z) = f (0) +
∞
X
n=1
bn

1
z −zn
+ 1
zn

.
(11.79)
To prove the theorem, we make the preliminary observation that the quantity being
summed in Eq. (11.79) can be written
z bn
zn(zn −z),
suggesting that it might be useful to consider a contour integral of the form
IN =
I
CN
f (w)dw
w(w −z),
where w is another complex variable and CN is a circle enclosing the ﬁrst N poles of f (z).
Since CN, which has a radius we denote RN, has total arc length 2π RN, and the absolute
value of the integrand asymptotically approaches | f (RN)|/R2
N, the large-z behavior of
f (z) guarantees that limRN →∞IN = 0.
We now obtain an alternate expression for IN using the residue theorem. Recognizing
that CN encircles simple poles at w = 0, w = z, and w = zn, n = 1... N, that f (w) is
nonsingular at w = 0 and w = z, and that the residue of f (z)/w(w −z) at zn is just
bn/zn(zn −z), we have
IN = 2πi f (0)
−z + 2πi f (z)
z
+
N
X
n=1
2πibn
zn(zn −z).
Taking the large-N limit, in which IN = 0, we recover Mittag-Lefﬂer’s theorem,
Eq. (11.79). The pole expansion converges when the condition limz→∞| f (z)/z| = 0 is
satisﬁed.
Mittag-Lefﬂer’s theorem leads to a number of interesting pole expansions. Consider the
following examples.
Example 11.7.3
POLE EXPANSION OF tan z
Writing
tan z =
eiz −e−iz
i(eiz + e−iz),
we easily see that the only singularities of tan z are for real values of z, and they occur
at the zeros of cos x, namely at ±π/2, ±3π/2, ... , or in general at zn = ±(2n + 1)π/2.

11.7 Calculus of Residues
517
To obtain the residues at these points, we take the limit (using l’Hôpital’s rule)
bn =
lim
z →(2n+1)π
2
(z −(2n + 1)π/2)sin z
cos z
= sin z + (z −(2n + 1)π/2)cos z
−sin z
z = (2n+1)π
2
= −1,
the same value for every pole.
Noting that tan(0) = 0, and that the poles within a circle of radius (N + 1)π will be
those (of both signs) referred to here by n values 0 through N, Eq. (11.79) for the current
case (but only through N) yields
tan z =
N
X
n=0
(−1)

1
z −(2n + 1)π/2 +
1
(2n + 1)π/2

+
N
X
n=0
(−1)

1
z + (2n + 1)π/2 +
1
−(2n + 1)π/2

=
N
X
n=0
(−1)

1
z −(2n + 1)π/2 +
1
z + (2n + 1)π/2

.
Combining terms over a common denominator, and taking the limit N →∞, we reach the
usual form of the expansion:
tan z = 2z

1
(π/2)2 −z2 +
1
(3π/2)2 −z2 +
1
(5π/2)2 −z2 + ···

.
(11.80)
■
Example 11.7.4
POLE EXPANSION OF cot z
This example proceeds much as the preceding one, except that cot z has a simple pole at
z = 0, with residue +1. We therefore consider instead cot z −1/z, thereby removing the
singularity. The singular points are now simple poles at ±nπ (n ̸= 0), with residues (again
obtained via l’Hôpital’s rule)
bn = lim
z→nπ(z −nπ)cot z = lim
z→nπ
(z −nπ)(z cos z −sin z)
z sin z
= z cos z −sin z + (z −nπ)(−z sin z)
sin z + z cos z

z=nπ
= +1.
Noting that cot z −1/z is zero at z = 0 (the second term in the expansion of cot z is −z/3),
we have
cot z −1
z =
N
X
n=1

1
z −nπ + 1
nπ +
1
z + nπ +
1
−nπ

,

518
Chapter 11 Complex Variable Theory
which rearranges to
cot z = 1
z + 2z

1
z2 −π2 +
1
z2 −(2π)2 +
1
z2 −(3π)2 + ···

.
(11.81)
■
In addition to Eqs. (11.80) and (11.81), two other pole expansions of importance are
sec z = π

1
(π/2)2 −z2 −
3
(3π/2)2 −z2 +
5
(5π/2)2 −z2 −

,
(11.82)
csc z = 1
z −2z

1
z2 −π2 −
1
z2 −(2π)2 +
1
z2 −(3π)2 + ···

.
(11.83)
Counting Poles and Zeros
It is possible to obtain information about the numbers of poles and zeros of a function
f (z) that is otherwise analytic within a closed region by consideration of its logarithmic
derivative, namely f ′(z)/f (z). The starting point for this analysis is to write an expression
for f (z) relative to a point z0 where there is either a zero or a pole in the form
f (z) = (z −z0)µg(z),
with g(z) ﬁnite and nonzero at z = z0. That requirement identiﬁes the limiting behavior of
f (z) near z0 as proportional to (z −z0)µ, and also causes f ′/f to assume near z = z0 the
form
f ′(z)
f (z) = µ(z −z0)µ−1g(z) + (z −z0)µg′(z)
(z −z0)µg(z)
=
µ
z −z0
+ g′(z)
g(z) .
(11.84)
Equation (11.84) shows that, for all nonzero µ (i.e., if z0 is either a zero or a pole), f ′/f
has a simple pole at z = z0 with residue µ. Note that because g(z) is required to be nonzero
and ﬁnite, the second term of Eq. (11.84) cannot be singular.
Applying now the residue theorem to Eq. (11.84) for a closed region within which f (z)
is analytic except possibly at poles, we see that the integral of f ′/f around a closed contour
yields the result
I
C
f ′(z)
f (z) dz = 2πi

N f −Pf

,
(11.85)
where Pf is the number of poles of f (z) within the region enclosed by C, each multiplied
by its order, and N is the number of zeros of f (z) enclosed by C, each multiplied by its
multiplicity.
The counting of zeros is often facilitated by using Rouché’s theorem, which states
If f (z) and g(z) are analytic in the region bounded by a curve C and | f (z)| > |g(z)|
on C, then f (z) and f (z) + g(z) have the same number of zeros in the region bounded
by C.

11.7 Calculus of Residues
519
To prove Rouché’s theorem, we ﬁrst write, from Eq. (11.85),
I
C
f ′(z)
f (z) dz = 2πi N f
and
I
C
f ′(z) + g′(z)
f (z) + g(z) dz = 2πi N f +g,
where N f designates the number of zeros of f within C. Then we observe that because
the indeﬁnite integral of f ′/f is ln f , N f is the number of times the argument of f cycles
through 2π when C is traversed once in the counterclockwise direction. Similarly, we note
that N f +g is the number of times the argument of f + g cycles through 2π on traversal of
the contour C.
We next write
f + g = f

1 + g
f

and
arg( f + g) = arg( f ) + arg

1 + g
f

,
(11.86)
using the fact that the argument of a product is the sum of the arguments of its factors. It
is then clear that the number of cycles through 2π of arg( f + g) is equal to the number
of cycles of arg( f ) plus the number of cycles of arg(1 + g/f ). But because |g/f | < 1,
the real part of 1 + g/f never becomes negative, and its argument is therefore restricted to
the range −π/2 < arg(1 + g/f ) < π/2. Therefore arg(1 + g/f ) cannot cycle through 2π,
the number of cycles of arg( f + g) must be equal to the number of cycles of arg f , and
f + g and f must have the same number of zeros within C. This completes the proof of
Rouché’s theorem.
Example 11.7.5
COUNTING ZEROS
Our problem is to determine the number of zeros of F(z) = z3 −2z + 11 with moduli
between 1 and 3. Since F(z) is analytic for all ﬁnite z, we could in principle simply apply
Eq. (11.85) for the contour consisting of the circles |z| = 1 (clockwise) and |z| = 3 (coun-
terclockwise), setting PF = 0 and solving for NF. However, that approach will in practice
prove difﬁcult. Instead, we simplify the problem by using Rouché’s theorem.
We ﬁrst compute the number of zeros within |z| = 1, writing F(z) = f (z) + g(z), with
f (z) = 11 and g(z) = z3 −2z. It is clear that | f (z)| > |g(z)| when |z| = 1, so, by Rouché’s
theorem, f and f + g have the same number of zeros within this circle. Since f (z) = 11
has no zeros, we conclude that all the zeros of F(z) are outside |z| = 1.
Next we compute the number of zeros within |z| = 3, taking for this purpose f (z) = z3,
g(z) = 11 −2z. When |z| = 3, we have | f (z)| = 27 > |g(z)|, so F and f have the same
number of zeros, namely three (the three-fold zero of f at z = 0). Thus, the answer to our
problem is that F has three zeros, all with moduli between 1 and 3.
■
Product Expansion of Entire Functions
We remind the reader that a function f (z) that is analytic for all ﬁnite z is called an entire
function. Referring to Eq. (11.84), we see that if f (z) is an entire function, then f ′(z)/f (z)
will be meromorphic, with all its poles simple. Assuming for simplicity that the zeros of f

520
Chapter 11 Complex Variable Theory
are simple and at points zn, so that µ in Eq. (11.84) is 1, we can invoke the Mittag-Lefﬂer
theorem to write f ′/f as the pole expansion
f ′(z)
f (z) = f ′(0)
f (0) +
∞
X
n=1

1
z −zn
+ 1
zn

.
(11.87)
Integrating Eq. (11.87) yields
z
Z
0
f ′(z)
f (z) dz = ln f (z) −ln f (0)
= zf ′(0)
f (0) +
∞
X
n=1

ln(z −zn) −ln(−zn) + z
zn

.
Exponentiating, we obtain the product expansion
f (z) = f (0)exp
zf ′(0)
f (0)
 ∞
Y
n=1

1 −z
zn

ez/zn.
(11.88)
Examples are the product expansions for
sin z = z
∞
Y
n = −∞
n ̸= 0

1 −z
nπ

ez/nπ = z
∞
Y
n=1

1 −
z2
n2π2

,
(11.89)
cos z =
∞
Y
n=1

1 −
z2
(n −1/2)2π2

.
(11.90)
The expansion of sin z cannot be obtained directly from Eq. (11.88), but its derivation is the
subject of Exercise 11.7.5. We also point out here that the gamma function has a product
expansion, discussed in Chapter 13.
Exercises
11.7.1
Determine the nature of the singularities of each of the following functions and evaluate
the residues (a > 0).
(a)
1
z2 + a2 .
(b)
1
(z2 + a2)2 .
(c)
z2
(z2 + a2)2 .
(d)
sin1/z
z2 + a2 .
(e)
ze+iz
z2 + a2 .
(f)
ze+iz
z2 −a2 .
(g)
e+iz
z2 −a2 .
(h)
z−k
z + 1,
0 < k < 1.

11.7 Calculus of Residues
521
Hint. For the point at inﬁnity, use the transformation w = 1/z for |z| →0. For the
residue, transform f (z)dz into g(w)dw and look at the behavior of g(w).
11.7.2
Evaluate the residues at z = 0 and z = −1 of π cotπz/z(z + 1).
11.7.3
The classical deﬁnition of the exponential integral Ei(x) for x > 0 is the Cauchy prin-
cipal value integral
Ei(x) =
x
Z
−∞
et
t dt,
where the integration range is cut at x = 0. Show that this deﬁnition yields a convergent
result for positive x.
11.7.4
Writing a Cauchy principal value integral to deal with the singularity at x = 1, show
that, if 0 < p < 1,
∞
Z
0
x−p
x −1 dx = −π cot pπ.
11.7.5
Explain why Eq. (11.88) is not directly applicable to the product expansion of sin z.
Show how the expansion, Eq. (11.89), can be obtained by expanding instead sin z/z.
11.7.6
Starting from the observations
1.
f (z) = anzn has n zeros, and
2.
for sufﬁciently large |R|, |Pn−1
m=0 am Rm| < |an Rn|,
use Rouché’s theorem to prove the fundamental theorem of algebra (namely that every
polynomial of degree n has n roots).
11.7.7
Using Rouché’s theorem, show that all the zeros of F(z) = z6 −4z3 + 10 lie between
the circles |z| = 1 and |z| = 2.
11.7.8
Derive the pole expansions of sec z and csc z given in Eqs. (11.82) and (11.83).
11.7.9
Given that f (z) = (z2 −3z + 2)/z, apply a partial fraction decomposition to f ′/f and
show directly that
H
C f ′(z)/f (z)dz = 2πi(N f −Pf ), where N f and Pf are, respec-
tively, the numbers of zeros and poles encircled by C (including their multiplicities).
11.7.10
The statement that the integral halfway around a singular point is equal to one-half the
integral all the way around was limited to simple poles. Show, by a speciﬁc example,
that
Z
Semicircle
f (z)dz = 1
2
I
Circle
f (z)dz
does not necessarily hold if the integral encircles a pole of higher order.
Hint. Try f (z) = z−2.

522
Chapter 11 Complex Variable Theory
11.7.11
A function f (z) is analytic along the real axis except for a third-order pole at z = x0.
The Laurent expansion about z = x0 has the form
f (z) =
a−3
(z −x0)3 +
a−1
z −x0
+ g(z),
with g(z) analytic at z = x0. Show that the Cauchy principal value technique is appli-
cable, in the sense that
(a)
limδ→0
nR x0−δ
−∞
f (x)dx +
R ∞
x0+δ f (x)dx
o
is ﬁnite.
(b)
R
Cx0 f (z)dz = ±iπa−1,
where Cx0 denotes a small semicircle about z = x0.
11.7.12
The unit step function is deﬁned as (compare Exercise 1.15.13)
u(s −a) =
0,
s < a
1,
s > a.
Show that u(s) has the integral representations
(a)
u(s) = limε→0+
1
2πi
∞
Z
−∞
eixs
x −iε dx.
(b)
u(s) = 1
2 +
1
2πi
∞
Z
−∞
eixs
x
dx.
Note. The parameter s is real.
11.8
EVALUATION OF DEFINITE INTEGRALS
Deﬁnite integrals appear repeatedly in problems of mathematical physics as well as in
pure mathematics. In Chapter 1 we reviewed several methods for integral evaluation, there
noting that contour integration methods were powerful and deserved detailed study. We
have now reached a point where we can explore these methods, which are applicable to a
wide variety of deﬁnite integrals with physically relevant integration limits. We start with
applications to integrals containing trigonometric functions, which we can often convert to
forms in which the variable of integration (originally an angle) is converted into a complex
variable z, with the integration integral becoming a contour integral over the unit circle.
Trigonometric Integrals, Range (0,2π)
We consider here integrals of the form
I =
2π
Z
0
f (sinθ,cosθ)dθ,
(11.91)

11.8 Evaluation of Deﬁnite Integrals
523
where f is ﬁnite for all values of θ. We also require f to be a rational function of sinθ
and cosθ so that it will be single-valued. We make a change of variable to
z = eiθ,
dz = ieiθdθ,
with the range in θ, namely (0,2π), corresponding to eiθ moving counterclockwise around
the unit circle to form a closed contour. Then we make the substitutions
dθ = −i dz
z ,
sinθ = z −z−1
2i
,
cosθ = z + z−1
2
,
(11.92)
where we have used Eq. (1.133) to represent sinθ and cosθ. Our integral then becomes
I = −i
I
f
z −z−1
2i
, z + z−1
2
 dz
z ,
(11.93)
with the path of integration the unit circle. By the residue theorem, Eq. (11.64),
I = (−i)2πi
X
residues within the unit circle.
(11.94)
Note that we must use the residues of f/z. Here are two preliminary examples.
Example 11.8.1
INTEGRAL OF cos IN DENOMINATOR
Our problem is to evaluate the deﬁnite integral
I =
2π
Z
0
dθ
1 + a cosθ ,
|a| < 1.
By Eq. (11.93) this becomes
I = −i
I
unit circle
dz
z[1 + (a/2)(z + z−1)]
= −i 2
a
I
dz
z2 + (2/a)z + 1.
The denominator has roots
z1 = −1 +
√
1 −a2
a
and
z2 = −1 −
√
1 −a2
a
.
Noting that z1z2 = 1, it is easy to see that z2 is within the unit circle and z1 is outside.
Writing the integral in the form
I
dz
(z −z1)(z −z2),
we see that the residue of the integrand at z = z2 is 1/(z2 −z1), so application of the
residue theorem yields
I = −i 2
a · 2πi
1
z2 −z1
.

524
Chapter 11 Complex Variable Theory
Inserting the values of z1 and z2, we obtain the ﬁnal result
2π
Z
0
dθ
1 + a cosθ =
2π
√
1 −a2 ,
|a| < 1.
■
Example 11.8.2
ANOTHER TRIGONOMETRIC INTEGRAL
Consider
I =
2π
Z
0
cos2θ dθ
5 −4cosθ .
Making the substitutions identiﬁed in Eqs. (11.92) and (11.93), the integral I assumes the
form
I =
I
1
2(z2 + z−2)
5 −2(z + z−1)
−i dz
z

= i
4
I
(z4 + 1)dz
z2  z −1
2

(z −2)
,
where the integration is around the unit circle. Note that we identiﬁed cos2θ as (z2 +
z−2)/2, which is simpler than reducing it ﬁrst to its equivalent in terms of sin z and cos z.
We see that the integrand has poles at z = 0 (of order 2), and simple poles at z = 1/2 and
z = 2. Only the poles at z = 0 and z = 1/2 are within the contour.
At z = 0 the residue of the integrand is
d
dz
"
z4 + 1
 z −1
2

(z −2)
#
z=0
= 5
2,
while its residue at z = 1/2 is
 z4 + 1
z2(z −2)

z=1/2
= −17
6 .
Applying the residue theorem, we have
I = i
4 (2πi)
5
2 −17
6

= π
6 .
■
We stress that integrals of the type now under consideration are evaluated after trans-
forming them so that they can be identiﬁed as exactly equivalent to contour integrals to
which we can apply the residue theorem. Further examples are in the exercises.

11.8 Evaluation of Deﬁnite Integrals
525
Integrals, Range −∞to ∞
Consider now deﬁnite integrals of the form
I =
∞
Z
−∞
f (x)dx,
(11.95)
where it is assumed that
•
f (z) is analytic in the upper half-plane except for a ﬁnite number of poles. For the
moment will be assumed that there are no poles on the real axis. Cases not satisfying
this condition will be considered later.
•
In the limit |z| →∞in the upper half-plane (0 ≤arg z ≤π), f (z) vanishes more
strongly than 1/z.
Note that there is nothing unique about the upper half-plane. The method described here
can be applied, with obvious modiﬁcations, if f (z) vanishes sufﬁciently strongly on the
lower half-plane.
The second assumption stated above makes it useful to evaluate the contour integral
H
f (z)dz on the contour shown in Fig. 11.21, because the integral I is given by the inte-
gration along the real axis, while the arc, of radius R, with R →∞, gives a negligible
contribution to the contour integral. Thus,
I =
I
f (z)dz,
and the contour integral can be evaluated by applying the residue theorem.
Situations of this sort are of frequent occurrence, and we therefore formalize the condi-
tions under which the integral over a large arc becomes negligible:
If limR→∞zf(z) = 0 for all z = Reiθ with θ in the range θ1 ≤θ ≤θ2, then
lim
R→∞
Z
C
f (z)dz = 0,
(11.96)
where C is the arc over the angular range θ1 to θ2 on a circle of radius R with
center at the origin.
Poles
FIGURE 11.21
A contour closed by a large semicircle in the upper half-plane.

526
Chapter 11 Complex Variable Theory
To prove Eq. (11.96), simply write the integral over C in polar form:
lim
R→∞

Z
C
f (z)dz

≤
θ2
Z
θ1
lim
R→∞
 f (Reiθ)i Reiθ dθ
≤(θ2 −θ1) lim
R→∞
 f (Reiθ)Reiθ = 0.
Now, using the contour of Fig. 11.21, letting C denote the semicircular arc from θ = 0
to θ = π,
I
f (z)dz = lim
R→∞
R
Z
−R
f (x)dx + lim
R→∞
Z
C
f (z)dz
= 2πi
X
residues (upper half-plane),
(11.97)
where our second assumption has caused the vanishing of the integral over C.
Example 11.8.3
INTEGRAL OF MEROMORPHIC FUNCTION
Evaluate
I =
∞
Z
0
dx
1 + x2 .
This is not in the form we require, but it can be made so by noting that the integrand is
even and we can write
I = 1
2
∞
Z
−∞
dx
1 + x2 .
(11.98)
We note that f (z) = 1/(1 + z2) is meromorphic; all its singularities for ﬁnite z are poles,
and it also has the property that zf (z) vanishes in the limit of large |z|. Therefore, we may
apply Eq. (11.97), so
1
2
∞
Z
−∞
dx
1 + x2 = 1
2(2πi)
X
residues of
1
1 + z2 (upper half-plane).
Here and in every other similar problem we have the question: Where are the poles?
Rewriting the integrand as
1
z2 + 1 =
1
(z + i)(z −i),
we see that there are simple poles (order 1) at z = i and z = −i. The residues are
at z = i:
1
z + i

z=i = 1
2i ,
and
at z = −i:
1
z −i

z=−i = −1
2i .

11.8 Evaluation of Deﬁnite Integrals
527
However, only the pole at z = +i is enclosed by the contour, so our result is
∞
Z
0
dx
1 + x2 = 1
2(2πi) 1
2i = π
2 .
(11.99)
This result is hardly a surprise, as we presumably already know that
∞
Z
0
dx
1 + x2 = tan−1 x

∞
0 = arctan x

∞
0 = π
2 ,
but, as shown in later examples, the techniques illustrated here are also easy to apply when
more elementary methods are difﬁcult or impossible.
Before leaving this example, note that we could equally well have closed the contour
with a semicircle in the lower half-plane, as zf (z) vanishes on that arc as well as that in the
upper half-plane. Then, taking the contour so the real axis is traversed from −∞to +∞,
the path would be clockwise (see Fig. 11.22), so we would need to take −2πi times the
residue of the pole that is now encircled (at z = −i). Thus, we have I = −1
2(2πi)(−1/2i),
which (as it must) evaluates to the same result we obtained previously, namely π/2.
■
Integrals with Complex Exponentials
Consider the deﬁnite integral
I =
∞
Z
−∞
f (x)eiax dx,
(11.100)
with a real and positive. (This is a Fourier transform; see Chapter 19.) We assume the
following two conditions:
•
f (z) is analytic in the upper half-plane except for a ﬁnite number of poles.
•
lim|z|→∞f (z) = 0,
0 ≤arg z ≤π.
Note that this is a less restrictive condition than the second condition imposed on f (z) for
our previous integration of
R ∞
−∞f (x)dx.
Pole
FIGURE 11.22
A contour closed by a large semicircle in the lower half-plane.

528
Chapter 11 Complex Variable Theory
We again employ the half-circle contour shown in Fig. 11.21. The application of the
calculus of residues is the same as the example just considered, but here we have to work
harder to show that the integral over the (inﬁnite) semicircle goes to zero. This integral
becomes, for a semicircle of radius R,
IR =
π
Z
0
f (Reiθ)eiaR cosθ−aR sinθi Reiθ dθ,
where the θ integration is over the upper half-plane, 0 ≤θ ≤π. Let R be sufﬁciently large
that | f (z)| = | f (Reiθ)| < ε for all θ within the integration range. Our second assumption
on f (z) tells us that as R →∞, ε →0. Then
|IR| ≤εR
π
Z
0
e−aR sinθ dθ = 2εR
π/2
Z
0
e−aR sinθ dθ.
(11.101)
We now note that in the range [0,π/2],
2
π θ ≤sinθ,
as is easily seen from Fig. 11.23. Substituting this inequality into Eq. (11.101), we have
|IR| ≤2εR
π/2
Z
0
e−2aRθ/π dθ = 2εR 1 −e−aR
2aR/π
< π
a ε,
showing that
lim
R→∞IR = 0.
This result is also important enough to commemorate; it is sometimes known as Jordan’s
lemma. Its formal statement is
If limR=∞f (z) = 0 for all z = Reiθ in the range 0 ≤θ ≤π, then
lim
R→∞
Z
C
eiaz f (z)dz = 0,
(11.102)
where a > 0 and C is a semicircle of radius R in the upper half-plane with center at
the origin.
Note that for Jordan’s lemma the upper and lower half-planes are not equivalent, because
the condition a > 0 causes the exponent −aR sinθ only to be negative and yield a neg-
ligible result in the upper half-plane. In the lower half-plane, the exponential is positive
and the integral on a large semicircle there would diverge. Of course, we could extend the
theorem by considering the case a < 0, in which event the contour to be used would then
be a semicircle in the lower half-plane.

11.8 Evaluation of Deﬁnite Integrals
529
(a)
(b)
1
y
2
π
θ
FIGURE 11.23
(a) y = (2/π)θ, (b) y = sinθ.
Returning now to integrals of the type represented by Eq. (11.100), and using the contour
shown in Fig. 11.21, application of the residue theorem yields the general result (for a > 0),
∞
Z
−∞
f (x)eiaxdx = 2πi
X
residues of eiaz f (z) (upper half-plane),
(11.103)
where we have used Jordan’s lemma to set to zero the contribution to the contour integral
from the large semicircle.
Example 11.8.4
OSCILLATORY INTEGRAL
Consider
I =
∞
Z
0
cos x
x2 + 1 dx,
which we initially manipulate, introducing cos x = (eix + e−ix)/2, as follows:
I = 1
2
∞
Z
0
eix dx
x2 + 1 + 1
2
∞
Z
0
e−ix dx
x2 + 1
= 1
2
∞
Z
0
eix dx
x2 + 1 + 1
2
−∞
Z
0
eix d(−x)
(−x)2 + 1 = 1
2
∞
Z
−∞
eix dx
x2 + 1,
thereby bringing I to the form presently under discussion.
We now note that in this problem f (z) = 1/(z2 + 1), which certainly approaches zero
for large |z|, and the exponential factor is of the form eiaz, with a = +1. We may therefore
evaluate the integral using Eq. (11.103), with the contour shown in Fig. 11.21.
The quantity whose residues are needed is
eiz
z2 + 1 =
eiz
(z + i)(z −i),

530
Chapter 11 Complex Variable Theory
and we note that the exponential, an entire function, contributes no singularities. So our
singularities are simple poles at z = ±i. Only the pole at z = +i is within the contour, and
its residue is ei2/2i, which reduces to 1/2ie. Our integral therefore has the value
I = 1
2 (2πi) 1
2ie = π
2e.
■
Our next example is an important integral, the evaluation of which involves the
principal-value concept and a contour that apparently needs to go through a pole.
Example 11.8.5
SINGULARITY ON CONTOUR OF INTEGRATION
We now consider the evaluation of
I =
∞
Z
0
sin x
x
dx.
(11.104)
Writing the integrand as (eiz −e−iz)/2iz, an attempt to do as we did in Example 11.8.4
leads to the problem that each of the two integrals into which I can be separated is individ-
ually divergent. This is a problem we have already encountered in discussing the Cauchy
principal value of this integral. Referring to (11.74), we write I as
I =
∞
Z
−∞
eix dx
2ix ,
(11.105)
suggesting that we consider the integral of eiz/2iz over a suitable closed contour.
We now note that although the gap at x = 0 is inﬁnitesimal, that point is a pole of
eiz/2iz, and we must draw a contour which avoids it, using a small semicircle to con-
nect the points at −δ and +δ. Compare with the discussion at Eqs. (11.75) and (11.76).
Choosing the small semicircle above the pole, as in Fig. 11.20, we then have a contour
that encloses no singularities.
The integral around this contour can now be identiﬁed as consisting of (1) the two semi-
inﬁnite segments constituting the principal value integral in Eq. (11.105), (2) the large
semicircle CR of radius R (R →∞), and (3) a semicircle Cr of radius r (r →0), traversed
clockwise, so
I
eiz
2iz dz = I +
Z
Cr
eiz
2iz dz +
Z
CR
eiz
2iz dz = 0.
(11.106)
By Jordan’s lemma, the integral over CR vanishes. As discussed at Eq. (11.75), the clock-
wise path Cr half-way around the pole at z = 0 contributes half the value of a full circuit,
namely (allowing for the clockwise direction of travel) −πi times the residue of eiz/2iz at
z = 0. This residue has value 1/2i, so
R
Cr = −πi(1/2i) = −π/2, and, solving Eq. (11.106)

11.8 Evaluation of Deﬁnite Integrals
531
for I, we then obtain
I =
∞
Z
0
sin x
x
dx = π
2 .
(11.107)
Note that it was necessary to close the contour in the upper half-plane. On a large circle
in the lower half-plane, eiz becomes inﬁnite and Jordan’s lemma cannot be applied.
■
Another Integration Technique
Sometimes we have an integral on the real range (0,∞) that lacks the symmetry needed
to extend the integration range to (−∞,∞). However, it may be possible to identify a
direction in the complex plane on which the integrand has a value identical to or conve-
niently related to that of the original integral, thereby permitting construction of a contour
facilitating the evaluation.
Example 11.8.6
EVALUATION ON A CIRCULAR SECTOR
Our problem is to evaluate the integral
I =
∞
Z
0
dx
x3 + 1,
which we cannot convert easily into an integral on the range (−∞,∞). However, we note
that along a line with argument θ = 2π/3, z3 will have the same values as at corresponding
points on the real line; note that (re2πi/3)3 = r3e2πi = r3. We therefore consider
I
dz
z3 + 1
on the contour shown in Fig. 11.24. The part of the contour along the positive real axis,
labeled A, simply yields our integral I. The integrand approaches zero sufﬁciently rapidly
for large |z| that the integral on the large circular arc, labeled C in the ﬁgure, vanishes. On
B
A
C
2π/3
FIGURE 11.24
Contour for Example 11.8.6.

532
Chapter 11 Complex Variable Theory
the remaining segment of the contour, labeled B, we note that dz = e2πi/3dr, z3 = r3, and
Z
B
dz
z3 + 1 =
0
Z
∞
e2πi/3dr
r3 + 1 = −e2πi/3
∞
Z
0
dr
r3 + 1 = −e2πi/3I.
Therefore,
I
dz
z3 + 1 =

1 −e2πi/3
I.
(11.108)
We now need to evaluate our complete contour integral using the residue theorem. The
integrand has simple poles at the three roots of z3 + 1, which are at z1 = eπi/3, z2 = eπi,
and z3 = e5πi/3, as marked in Fig. 11.24. Only the pole at z1 is enclosed by our contour.
The residue at z = z1 is
lim
z=z1
z −z1
z3 + 1 = 1
3z2

z=z1
=
1
3e2πi/3 .
Equating 2πi times this result to the value of the contour integral as given in Eq. (11.108),
we have

1 −e2πi/3
I = 2πi

1
3e2πi/3

.
Solution for I is facilitated if we multiply through by e−πi/3, obtaining initially

e−πi/3 −eπi/3
I = 2πi

−1
3

,
which is easily rearranged to
I =
π
3sinπ/3 =
π
3
√
3/2
= 2π
3
√
3
.
■
Avoidance of Branch Points
Sometimes we must deal with integrals whose integrands have branch points. In order to
use contour integration methods for such integrals we must choose contours that avoid the
branch points, enclosing only point singularities.
Example 11.8.7
INTEGRAL CONTAINING LOGARITHM
We now look at
I =
∞
Z
0
ln x dx
x3 + 1 .
(11.109)
The integrand in Eq. (11.109) is singular at x = 0, but the integration converges (the indef-
inite integral of ln x is x ln x −x). However, in the complex plane this singularity manifests

11.8 Evaluation of Deﬁnite Integrals
533
A
B
C
FIGURE 11.25
Contour for Example 11.8.7.
itself as a branch point, so if we are to recast this problem in a way involving a contour
integral, we must avoid z = 0 and a branch cut from that point to z = ∞. It turns out to be
convenient to use a contour similar to that for Example 11.8.6, except that we must make a
small circular detour about z = 0 and then draw the branch cut in a direction that remains
outside our chosen contour. Noting also that the integrand has poles at the same points as
those of Example 11.8.6, we consider a contour integral
I ln z dz
z3 + 1,
where the contour and the locations of the singularities of the integrand are as illustrated
in Fig. 11.25.
The integral over the large circular arc, labeled C, vanishes, as the factor z3 in the
denominator dominates over the weakly divergent factor ln z in the numerator (which
diverges more weakly than any positive power of z). We also get no contribution to the
contour integral from the arc at small r, since we have there
lim
r→0
2π/3
Z
0
ln(reiθ)
1 + r3e3iθ ireiθ dθ,
which vanishes because r lnr →0.
The integrals over the segments labeled A and B do not vanish. To evaluate the integral
over these segments, we need to make an appropriate choice of the branch of the multi-
valued function ln z. It is natural to choose the branch so that on the real axis we have
ln z = ln x (and not ln x + 2nπi with some nonzero n). Then the integral over the segment
labeled A will have the value I.7
To compute the integral over B, we note that on this segment z3 = r3 and dz = e2πi/3dr
(as in Example 11.8.6), also but note that ln z = lnr +2πi/3. There is little temptation here
to use a different one of the multiple values of the logarithm, but for future reference note
that we must use the value that is reached continuously from the value we already chose
on the positive real axis, moving in a way that does not cross the branch cut. Thus, we
cannot reach segment A by clockwise travel from the positive real axis (thereby getting
7Because the integration converges at x = 0, the value is not affected by the fact that this segment terminates inﬁnitesimally
before reaching that point.

534
Chapter 11 Complex Variable Theory
ln z = lnr −4πi/3) or any other value that would require multiple circuits around the
branch point z = 0.
Based on the foregoing, we have
Z
B
ln z dz
z3 + 1 =
0
Z
∞
lnr + 2πi/3
r3 + 1
e2πi/3 dr = −e2πi/3I −2πi
3 e2πi/3
∞
Z
0
dr
r3 + 1.
(11.110)
Referring to Example 11.8.6 for the value of the integral in the ﬁnal term of Eq. (11.110),
and combining the contributions to the overall contour integral,
I ln z dz
z3 + 1 =

1 −e2πi/3
I −2πi
3 e2πi/3
 2π
3
√
3

.
(11.111)
Our next step is to use the residue theorem to evaluate the contour integral. Only the
pole at z = z1 lies within the contour. The residue we must compute is
lim
z=z1
(z −z1)ln z
z3 + 1
= ln z
3z2

z=z1
= πi/3
3e2πi/3 = πi
9 e−2πi/3,
and application of the residue theorem to Eq. (11.111) yields

1 −e2πi/3
I −2πi
3 e2πi/3
 2π
3
√
3

= (2πi)
πi
9

e−2πi/3.
(11.112)
Solving for I, we get
I = −2π2
27 .
(11.113)
Veriﬁcation of the passage from Eq. (11.112) to (11.113) is left to Exercise 11.8.6.
■
Exploiting Branch Cuts
Sometimes, rather than being an annoyance, a branch cut provides an opportunity for a
creative way of evaluating difﬁcult integrals.
Example 11.8.8
USING A BRANCH CUT
Let’s evaluate
I =
∞
Z
0
x p dx
x2 + 1,
0 < p < 1.
Consider the contour integral
I
z p dz
z2 + 1,
where the contour is that shown in Fig. 11.26. Note that z = 0 is a branch point, and we
have taken the cut along the positive real axis. We assign z p its usual principal value

11.8 Evaluation of Deﬁnite Integrals
535
A
B
FIGURE 11.26
Contour for Example 11.8.8.
(which is x p) just above the cut, so that the segment of the contour labeled A, which
actually extends from ε to ∞, converges in the limit of small ε to the integral I. Neither
the circle of radius ε nor that at R →∞contributes to the value of the contour integral.
On the remaining segment of the contour, labeled B, we have z = re2πi, written this way
so we can see that z p = r pe2pπi. We use this value for z p on segment B because we must
get to B by encircling z = 0 in the counterclockwise, mathematically positive direction.
The contribution of segment B to the contour integral is then seen to be
0
Z
∞
r pe2pπi dr
r2 + 1
= −e2pπi I,
so
I
z p dz
z2 + 1 =

1 −e2pπi
I.
(11.114)
To apply the residue theorem, we note that there are simple poles at z1 = i and z2 = −i;
to use these for evaluation of z p we need to identify these as z1 = eπi/2 and z2 = e3πi/2.
It would be a serious mistake to use z2 = e−πi/2 when evaluating z p
2 . We now ﬁnd the
residues to be:
Residue at z1: epπi/2
2i
,
Residue at z2: e3pπi/2
−2i
,
and we have, referring to Eq. (11.114),

1 −e2pπi
I = (2πi) 1
2i

epπi/2 −e3pπi/2
.
(11.115)
This equation simpliﬁes to
I = π sin(pπ/2)
sin pπ
=
π
2cos(pπ/2).
(11.116)
The details of the evaluation are left to Exercise 11.8.7.
■
The use of a branch cut, as illustrated in Example 11.8.8, is so helpful that sometimes it
is advisable to insert a factor into a contour integral to create one that would not otherwise
exist. To illustrate this, we return to an integral we evaluated earlier by another method.

536
Chapter 11 Complex Variable Theory
Example 11.8.9
INTRODUCING A BRANCH POINT
Let’s evaluate once again the integral
I =
∞
Z
0
dx
x3 + 1,
which we previously considered in Example 11.8.6. This time, we proceed by setting up
the contour integral
I ln z dz
z3 + 1,
taking the contour to be that depicted in Fig. 11.26. Note that in the present problem the
poles of the integrand are not those shown in Fig. 11.26, which was originally drawn to
illustrate a different problem; for the locations of the poles of the present integrand, see
Fig. 11.24.
The virtue of the introduction of the factor ln z is that its presence causes the integral
segments above and below the positive real axis not to cancel completely, but to yield
a net contribution corresponding to an integral of interest. In the present problem (using
the labeling in Fig. 11.26), we again have vanishing contributions from the small and large
circles, and (taking the usual principal value for the logarithm on segment A), that segment
contributes to the contour integral the expected value
Z
A
ln z dz
z3 + 1 =
∞
Z
0
ln x dx
x3 + 1 .
(11.117)
However, segment B make the contribution
Z
B
ln z dz
z3 + 1 =
0
Z
∞
(ln x + 2πi)dx
x3 + 1
,
(11.118)
and when Eqs. (11.117) and (11.118) are combined, the logarithmic terms cancel, and we
are left with
I ln z dz
z3 + 1 =
Z
A+B
ln z dz
z3 + 1 = −2πi
∞
Z
0
dx
x3 + 1 = −2πi I.
(11.119)
Note that what has happened is that the logarithm has disappeared (its contributions can-
celed), but its presence caused the integral of current interest to be proportional to the value
of the contour integral we introduced.
To complete the evaluation, we need to evaluate the contour integral using the residue
theorem. Note that the residues are those of the integrand, including the logarithmic factor,
and this factor must be computed taking account of the branch cut. In the present problem,
we identify poles at z1 = eπi/3, z2 = eπi, and z3 = e5πi/3 (not e−πi/3). The contour now

11.8 Evaluation of Deﬁnite Integrals
537
in use encircles all three poles. Their respective residues (denoted Ri) are
R1 =
πi
3

1
3e2πi/3 ,
R2 = (πi)
1
3e6πi/3 ,
and
R3 =
5πi
3

1
3e10πi/3 ,
where the ﬁrst parenthesized factor of each residue comes from the logarithm.
Continuing, we have, referring to Eq. (11.119),
−2πi I = 2πi (R1 + R2 + R3);
I = −(R1 + R2 + R3) = −πi
9
h
e−2πi/3 + 3 + 5e2πi/3i
= 2π
3
√
3
.
More robust examples involving the introduction of ln z appear in the exercises.
■
Exploiting Periodicity
The periodicity of the trigonometric functions (and that, in the complex plane, of the hyper-
bolic functions) creates opportunities to devise contours in which multiple contributions
corresponding to an integral of interest can be used to encircle singularities and enable use
of the residue theorem. We illustrate with one example.
Example 11.8.10
INTEGRAND PERIODIC ON IMAGINARY AXIS
We wish to evaluate
I =
∞
Z
0
x dx
sinh x .
Taking account of the sinusoidal behavior of the hyperbolic sine in the imaginary direction,
we consider
I
z dz
sinh z
(11.120)
on the contour shown in Fig.11.27. In drawing the contour we needed to be mindful of the
singularities of the integrand, which are poles associated with the zeros of sinh z. Recog-
nizing that
sinh(x + iy) = sinh x coshiy + cosh x sinhiy = sinh x cos y + i cosh x sin y,
(11.121)
B
A
O
C
B′
πi−R
−R
πi + R
R
FIGURE 11.27
Contour for Example 11.8.10.

538
Chapter 11 Complex Variable Theory
and that for all x, cosh x ≥1, we see that sinh z is zero only for z = nπi, with n an integer.
Moreover, because limz→0 z/sinh z = 1, the integrand of our present contour integral will
not have a pole at z = 0, but will have poles at z = nπi for all nonzero integral n. For that
reason, the lower horizontal line of the contour in Fig. 11.27, marked A, continues through
z = 0 as a straight line on the real axis, but the upper horizontal line (for which y = π),
marked B and B′, has an inﬁnitesimal semicircular detour, marked C, around the pole at
z = πi.
Because the integrand in Eq. (11.120) is an even function of z, the integral on segment A,
which extends from −∞to +∞, has the value 2I. To evaluate the integral on segments
B and B′, we ﬁrst note, using Eq. (11.121), that sinh(x + iπ) = −sinh x, and that the
integral on these segments is in the direction of negative x. Recognizing the integral on
these segments as a Cauchy principal value, we write
Z
B+B′
z dz
sinh z =
∞
Z
−∞
x + iπ
sinh x dx.
Because x/sinh x is even and nonsingular at z = 0, while iπ/sinh x is odd, this integral
reduces to
∞
Z
−∞
x + iπ
sinh x dx = 2I.
Combining what we have up to this point, invoking the residue theorem, and noting that
the integrand is negligible on the vertical connections at x = ±∞. We have
I
z dz
sinh z = 4I +
Z
C
z dz
sinh z = 2πi (residue of z/sinh z at z = πi).
(11.122)
To complete the evaluation, we now note that the residue we need is
lim
z→πi
z(z −πi)
sinh z
=
πi
coshπi = −πi,
and, cf. Eqs. (11.75) and (11.76), the counterclockwise semicircle C evaluates to πi times
this residue. We have then
4I + (πi)(−πi) = (2πi)(−πi),
so I = π2
4 .
■
Exercises
11.8.1
Generalizing Example 11.8.1, show that
2π
Z
0
dθ
a ± b cosθ =
2π
Z
0
dθ
a ± b sinθ =
2π
(a2 −b2)1/2 ,
for a > |b|.
What happens if |b| > |a|?

11.8 Evaluation of Deﬁnite Integrals
539
11.8.2
Show that
π
Z
0
dθ
(a + cosθ)2 =
πa
(a2 −1)3/2 ,
a > 1.
11.8.3
Show that
2π
Z
0
dθ
1 −2t cosθ + t2 =
2π
1 −t2 ,
for |t| < 1.
What happens if |t| > 1?
What happens if |t| = 1?
11.8.4
Evaluate
2π
Z
0
cos3θ dθ
5 −4cosθ .
ANS.
π/12.
11.8.5
With the calculus of residues, show that
π
Z
0
cos2n θ dθ = π
(2n)!
22n(n!)2 = π (2n −1)!!
(2n)!!
,
n = 0,1,2,... .
The double factorial notation is deﬁned in Eq. (1.76).
Hint. cosθ = 1
2(eiθ + e−iθ) = 1
2(z + z−1),
|z| = 1.
11.8.6
Verify that simpliﬁcation of the expression in Eq. (11.112) yields the result given in
Eq. (11.113).
11.8.7
Complete the details of Example 11.8.8 by verifying that there is no contribution to
the contour integral from either the small or the large circles of the contour, and that
Eq. (11.115) simpliﬁes to the result given as (11.116).
11.8.8
Evaluate
∞
Z
−∞
cosbx −cosax
x2
dx,
a > b > 0.
ANS.
π(a −b).
11.8.9
Prove that
∞
Z
−∞
sin2 x
x2
dx = π
2 .
Hint. sin2 x = 1
2(1 −cos2x).
11.8.10
Show that
∞
Z
0
x sin x
x2 + 1 dx = π
2e.

540
Chapter 11 Complex Variable Theory
11.8.11
A quantum mechanical calculation of a transition probability leads to the function
f (t,ω) = 2(1 −cosωt)/ω2. Show that
∞
Z
−∞
f (t,ω)dω = 2πt.
11.8.12
Show that (a > 0):
(a)
∞
Z
−∞
cos x
x2 + a2 dx = π
a e−a.
How is the right side modiﬁed if cos x is replaced by coskx?
(b)
∞
Z
−∞
x sin x
x2 + a2 dx = π e−a.
How is the right side modiﬁed if sin x is replaced by sinkx?
11.8.13
Use the contour shown (Fig. 11.28) with R →∞to prove that
∞
Z
−∞
sin x
x
dx = π.
11.8.14
In the quantum theory of atomic collisions, we encounter the integral
I =
∞
Z
−∞
sint
t
eipt dt,
R
R
.
R
R
FIGURE 11.28
Contour for Exercise 11.8.13.

11.8 Evaluation of Deﬁnite Integrals
541
in which p is real. Show that
I = 0, |p| > 1
I = π, |p| < 1.
What happens if p = ±1?
11.8.15
Show that
∞
Z
0
dx
(x2 + a2)2 = π
4a3 ,
a > 0.
11.8.16
Evaluate
∞
Z
−∞
x2
1 + x4 dx.
ANS.
π/
√
2.
11.8.17
Evaluate
∞
Z
0
x p ln x
x2 + 1 dx,
0 < p < 1.
ANS.
π2
4
sin(πp/2)
cos2(πp/2).
11.8.18
Evaluate
∞
Z
0
(ln x)2
1 + x2 dx,
(a)
by appropriate series expansion of the integrand to obtain
4
∞
X
n=0
(−1)n(2n + 1)−3,
(b)
and by contour integration to obtain π3
8 .
Hint. x →z = et. Try the contour shown in Fig. 11.29, letting R →∞.
−R+iπ
−R
R +iπ
y
R
x
FIGURE 11.29
Contour for Exercise 11.8.18.

542
Chapter 11 Complex Variable Theory
11.8.19
Prove that
∞
Z
0
ln(1 + x2)
1 + x2
dx = π ln2.
11.8.20
Show that
∞
Z
0
xa
(x + 1)2 dx =
πa
sinπa ,
where −1 < a < 1.
Hint. Use the contour shown in Fig. 11.26, noting that z = 0 is a branch point and the
positive x-axis can be chosen to be a cut line.
11.8.21
Show that
∞
Z
−∞
x2dx
x4 −2x2 cos2θ + 1 =
π
2sinθ =
π
21/2(1 −cos2θ)1/2 .
Exercise 11.8.16 is a special case of this result.
11.8.22
Show that
∞
Z
0
dx
1 + xn =
π/n
sin(π/n).
Hint. Try the contour shown in Fig. 11.30, with θ = 2π/n.
11.8.23
(a)
Show that
f (z) = z4 −2z2 cos2θ + 1
has zeros at eiθ,e−iθ,−eiθ, and −e−iθ.
(b)
Show that
∞
Z
−∞
dx
x4 −2x2 cos2θ + 1 =
π
2sinθ =
π
21/2(1 −cos2θ)1/2 .
Exercise 11.8.22 (n = 4) is a special case of this result.
R
R
θ
FIGURE 11.30
Sector contour.

11.8 Evaluation of Deﬁnite Integrals
543
11.8.24
Show that
∞
Z
0
x−a
x + 1 dx =
π
sinaπ ,
where 0 < a < 1.
Hint. You have a branch point and you will need a cut line. Try the contour shown in
Fig. 11.26.
11.8.25
Show that
∞
Z
0
coshbx
cosh x dx =
π
2cos(πb/2),
|b| < 1.
Hint. Choose a contour that encloses one pole of cosh z.
11.8.26
Show that
∞
Z
0
cos(t2)dt =
∞
Z
0
sin(t2)dt =
√π
2
√
2
.
Hint. Try the contour shown in Fig. 11.30, with θ = π/4.
Note. These are the Fresnel integrals for the special case of inﬁnity as the upper limit.
For the general case of a varying upper limit, asymptotic expansions of the Fresnel
integrals are the topic of Exercise 12.6.1.
11.8.27
Show that
1
Z
0
1
(x2 −x3)1/3 dx = 2π/
√
3.
Hint. Try the contour shown in Fig. 11.31.
11.8.28
Evaluate
∞
Z
−∞
tan−1 ax dx
x(x2 + b2) , for a and b positive, with ab < 1.
Explain why the integrand does not have a singularity at x = 0.
0
1
FIGURE 11.31
Contour for Exercise 11.8.27.

544
Chapter 11 Complex Variable Theory
Hint. Try the contour shown in Fig. 11.32, and use Eq. (1.137) to represent tan−1 az.
After cancellation, the integrals on segments B and B′ combine to give an elementary
integral.
11.9
EVALUATION OF SUMS
The fact that the cotangent is a meromorphic function with regularly spaced poles, all with
the same residue, enables us to use it to write a wide variety of inﬁnite summations in terms
of contour integrals. To start, note that π cotπz has simple poles at all integers on the real
axis, each with residue
lim
z→n
π cosπz
sinπz
= 1.
Suppose that we now evaluate the integral
IN =
I
CN
f (z)π cotπz dz,
where the contour is a circle about z = 0 of radius N + 1
2 (thereby not passing close to
the singularities of cotπz). Assuming also that f (z) has only isolated singularities, at
points z j other than real integers, we get by application of the residue theorem (see also
Exercise 11.9.1),
IN = 2πi
N
X
n=−N
f (n) + 2πi
X
j
(residues of f (z)π cotπz at singularities z j of f ).
This integral over the circular contour CN will be negligible for large |z| if zf (z) →0 at
large |z|.8 When that condition is met, limN→∞IN = 0, and we have the useful result
∞
X
n=−∞
f (n) = −
X
j
(residues of f (z)π cotπz at singularities z j of f ).
(11.123)
The condition required of f (z) will usually be satisﬁed if the summation of Eq. (11.123)
converges.
A
ib
B′
B
x
y
i
a
FIGURE 11.32
Contour for Exercise 11.8.28.
8See also Exercise 11.9.2.

11.9 Evaluation of Sums
545
Example 11.9.1
EVALUATING A SUM
Consider the summation
S =
∞
X
n=1
1
n2 + a2 ,
where, for simplicity, we assume that a is nonintegral. To bring our problem to the form
we know how to treat, we note that also
−1
X
n=−∞
1
n2 + a2 = S,
so that
∞
X
n=−∞
1
n2 + a2 = 2 S + 1
a2 ,
(11.124)
where we have added on the right-hand side the contribution from n = 0 that was not
included in S.
The summation is now identiﬁed as of the form of Eq. (11.123), with f (z) = 1/(z2 +
a2); f (z) approaches zero at large z rapidly enough to make Eq. (11.123) applicable. We
therefore proceed to the observation that the only singularities of f (z) are simple poles at
z = ±ia. The residues we need are those of π cot(πz)/(z2 + a2); they are
π cotiπa
2ia
= −π cothπa
2a
and
π cot(−iπa)
−2ia
= −π coth(−πa)
−2a
.
These are equal, so from Eqs. (11.123) and (11.124),
2S + 1
a2 = π cothπa
a
,
which we easily solve to reach S = π cothπa
2a
−
1
2a2 .
■
Additional types of summations can be performed if we replace cotπz by functions
with other regularly repeating patterns of residues. For example, π cscπz has residues for
integer z that alternate in sign between +1 and −1; π tanπz has residues that are all +1,
but occur at the points n + 1
2. And π secπz has residues ±1 at the half-integers with a sign
alternation. For convenience, we list in Table 11.2 the contour-integral formulas for the
four types of summations we have just discussed.
We close this section with another example, this time illustrating what can be done if
f (z) has a pole at an integer value of z.
Example 11.9.2
ANOTHER SUM
Consider now the summation
S =
∞
X
n=1
1
n(n + 1).

546
Chapter 11 Complex Variable Theory
Table 11.2
Contour-Integral-Based Formulas for Summations
Summation
Formula
∞
X
n=−∞
f (n)
−
X
(residues of f (z)π cotπz at singularities of f ).
∞
X
n=−∞
(−1)n f (n)n
−
X
(residues of f (z)π cscπz at singularities of f ).
∞
X
n=−∞
f

n + 1
2

X
(residues of f (z)π tanπz at singularities of f ).
∞
X
n=−∞
(−1)n f

n + 1
2

X
(residues of f (z)π secπz at singularities of f ).
To extend the summation to n = −∞, we note that S =
−2
X
n=−∞
1
n(n + 1), so that
2S =
∞
X
n=−∞
′
1
n(n + 1),
(11.125)
where the prime on the sum indicates that the terms for n = 0 and n = −1 are to be omit-
ted. The derivation of Eq. (11.123) indicates that this equation will apply if we omit the
(singular) n = 0 and n = −1 terms from the sum and include the points z = 0 and z = −1
as points where the residues of f (z)π cotπz are to be included.
Based on that insight, we ﬁnd that in the present problem,
2S = −(sum of residues of π cotπz/z(z + 1) at z = 0 and z = −1).
The singularities at z = 0 and z = −1 are second-order poles, at which the residues are
most easily computed by the method illustrated in item 5 of Example 11.7.1. In Exer-
cise 11.7.2 it is shown that the residue at each pole has value −1. Completing the problem,
2S = −(−1 −1) = 2,
so S = 1.
In this instance the result is easily veriﬁed by making the partial fraction expansion
1
n(n + 1) = 1
n −
1
n + 1.
When inserted in the summation S, all terms cancel except the initial term of the 1/n
summation, yielding S = 1.
■
Exercises
11.9.1
Show that if f (z) is analytic at z=z0 and g(z) has a simple pole at z=z0 with residue
b0, then f (z)g(z) also has a simple pole at z=z0, with residue f (z0)b0.
11.9.2
Show that cot z has magnitude of order 1 for large |z| when not extremely close to one
of its poles and does not affect the limiting behavior of IN.

11.10 Miscellaneous Topics
547
11.9.3
Evaluate 1
13 −1
33 + 1
53 −··· .
11.9.4
Evaluate P∞
n=1
1
n(n+2).
11.9.5
Evaluate P∞
n=−∞
(−1)n
(n+a)2 , where a is real and not an integer.
11.9.6
(a)
Using a method based on contour integration, evaluate P∞
n=0
1
(2n+1)2 .
(b)
Check your work by relating your answer to an appropriate expression involving
zeta functions.
11.9.7
Show that
1
cosh(π/2) −
1
3cosh(3π/2) +
1
5cosh(5π/2) −··· = π
8 .
11.9.8
For −π ≤ϕ ≤+π, show that P∞
n=1(−1)n sinnϕ
n3
= ϕ
12(ϕ2 −π2).
11.10
MISCELLANEOUS TOPICS
Schwarz Reﬂection Principle
Our starting point for this topic is the observation that g(z) = (z −x0)n for integral n and
real x0 satisﬁes
g∗(z) = [(z −x0)n]∗= (z∗−x0)n = g(z∗).
(11.126)
A generalization of the result in Eq. (11.126) is the Schwarz reﬂection principle:
If a function f (z) is (1) analytic over some region including a portion of the real axis
and (2) real when z is real, then
f ∗(z) = f (z∗).
(11.127)
Expanding f (z) about some point x0 within the region of analyticity on the real axis,
f (z) =
∞
X
n=0
(z −x0)n f (n)(x0)
n!
.
Since f (z) is analytic at z = x0, this Taylor expansion exists. Since f (z) is real when z is
real, f (n)(x0) must be real for all n. Then, invoking Eq. (11.126), the Schwarz reﬂection
principle, Eq. (11.127), follows immediately. This completes the proof within a circle of
convergence. Analytic continuation then permits the extension of this result to the entire
region of analyticity.
Note that the reﬂection principle can also be derived by the consideration of Laurent
expansions. See Exercise 11.10.2.
Mapping
An analytic function w(z) = u(x, y) + iv(x, y) can be regarded as a mapping in which
points or curves in an xy plane can be associated with the corresponding points or curves in
a uv plane. As a relatively simple example, consider the transformation w = 1/z. From an

548
Chapter 11 Complex Variable Theory
examination of its polar form, with z = reiθ, w = ρeiϕ, we see that ρ = 1/r and ϕ = −θ,
leading to the conclusion that the interior of the unit circle maps into its exterior (see
Fig. 11.33). Circles in other locations in the z plane are transformed by w = 1/z into
other circles (or straight lines, which can be thought of as circles of inﬁnite radius). This
statement is the subject of Exercise 11.10.6. The transformation of two such circles are
shown in the four panels of Fig. 11.34. Compare the way in which the interiors of the
circles transform in Figs. 11.33 and 11.34. Note that the transformation does not preserve
lengths, as can be seen in the ﬁgure from the labeling of various points and their locations
when mapped.
1/r
r
-θ
θ
FIGURE 11.33
Mapping w = 1/z. The shaded areas transform into each other.
y
x
x
a
b
3
1
v
u
b
a
1
b =i
d
a
c
y
d
d
b=−i
u
v
a
c
1
3
FIGURE 11.34
Left panels: circles in z plane. Right panels: their transformations
in w plane under w = 1/z.

11.10 Miscellaneous Topics
549
Historically, the notion of mapping was useful for identifying and carrying out transfor-
mations that would facilitate the solution of 2-D problems in electrostatics, ﬂuid dynamics,
and other areas of classical physics. An important aspect of such mappings is that they are
conformal, meaning that (except at singularities of the transformation) the angles at which
curves intersect remain unchanged when transformed. This feature preserves relations, e.g.,
between equipotentials and lines of force (stream lines). With the nearly universal use of
high-speed computers, procedures based on conformal mapping are no longer central to
the practical solution of most physics and engineering problems, and as a consequence
will not be explored here in further detail. For problems where these techniques are still
relevant, we refer the reader to earlier editions of this book and to sources identiﬁed under
Additional Readings. In that connection, we call particular attention to the book by Spiegel,
which contains (in chapter 8) descriptions of a large number of mappings and (in chapter 9)
many applications to problems of ﬂuid ﬂow, electrostatics, and heat conduction.
Exercises
11.10.1
A function f (z) = u(x, y) + iv(x, y) satisﬁes the conditions for the Schwarz reﬂection
principle. Show that
(a)
u is an even function of y.
(b)
v is an odd function of y.
11.10.2
A function f (z) can be expanded in a Laurent series about the origin with the coefﬁ-
cients an real. Show that the complex conjugate of this function of z is the same function
of the complex conjugate of z; that is,
f ∗(z) = f (z∗).
Verify this explicitly for
(a)
f (z) = zn,n an integer.
(b)
f (z) = sin z.
If f (z) = iz(a1 = i), show that the foregoing statement does not hold.
11.10.3
The function f (z) is analytic in a domain that includes the real axis. When z is real
(z = x), f (x) is pure imaginary.
(a)
Show that
f (z∗) = −[ f (z)]∗.
(b)
For the speciﬁc case f (z) = iz, develop the Cartesian forms of f (z), f (z∗), and
f ∗(z). Do not quote the general result of part (a).
11.10.4
How do circles centered on the origin in the z-plane transform for
(a)
w1(z) = z + 1
z ,
(b)
w2(z) = z −1
z ,
for z ̸= 0?
What happens when |z| →1?

550
Chapter 11 Complex Variable Theory
11.10.5
What part of the z-plane corresponds to the interior of the unit circle in the w-plane if
(a)
w = z −1
z + 1?
(b)
w = z −i
z + i ?
11.10.6
(a)
Writing z = x + iy, w = u + iv, show that if w = 1/z, the circle in the xy plane
deﬁned by (x −a)2 + (y −b)2 = r2 transforms into (u −A)2 + (v −B)2 = R2.
(b)
Does the center of the circle in the z plane transform into the center of the corre-
sponding circle in the w plane?
11.10.7
Assume that a curve in the xy plane passes through point z0 in the direction dz = eiθds,
where s indicates arc length on the curve. Then, if w = f (z), with f (z) analytic at z =
z0, we have dw = (dw/dz)dz = f ′(z)eiθds, where dw is in the direction the mapping
of the xy curve passes through w0 = f (z0) in the w plane. Use this observation to
prove that if f ′(z0) ̸= 0, the angle at which two curves intersect in the z plane is the
same (both in magnitude and direction) as the angle of intersection of their mappings in
the w plane.
Additional Readings
Ahlfors, L. V., Complex Analysis, 3rd ed. New York: McGraw-Hill (1979). This text is detailed, thorough, rigor-
ous, and extensive.
Churchill, R. V., J. W. Brown, and R. F. Verkey, Complex Variables and Applications, 5th ed. New York:
McGraw-Hill (1989). This is an excellent text for both the beginning and advanced student. It is readable and
quite complete. A detailed proof of the Cauchy-Goursat theorem is given in Chapter 5.
Greenleaf, F. P., Introduction to Complex Variables. Philadelphia: Saunders (1972). This very readable book has
detailed, careful explanations.
Kurala, A., Applied Functions of a Complex Variable. New York: Wiley (Interscience) (1972). An intermediate-
level text designed for scientists and engineers. Includes many physical applications.
Levinson, N., and R. M. Redheffer, Complex Variables. San Francisco: Holden-Day (1970). This text is written
for scientists and engineers who are interested in applications.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Chapter 4 is
a presentation of portions of the theory of functions of a complex variable of interest to theoretical physicists.
Remmert, R., Theory of Complex Functions. New York: Springer (1991).
Sokolnikoff, I. S., and R. M. Redheffer, Mathematics of Physics and Modern Engineering, 2nd ed. New York:
McGraw-Hill (1966). Chapter 7 covers complex variables.
Spiegel, M. R., Complex Variables, in Schaum’s Outline Series. New York: McGraw-Hill (original 1964,
reprinted 1995). An excellent summary of the theory of complex variables for scientists.
Titchmarsh, E. C., The Theory of Functions, 2nd ed. New York: Oxford University Press (1958). A classic.
Watson, G. N., Complex Integration and Cauchy’s Theorem. New York: Hafner (original 1917, reprinted 1960).
A short work containing a rigorous development of the Cauchy integral theorem and integral formula. Appli-
cations to the calculus of residues are included. Cambridge Tracts in Mathematics, and Mathematical Physics,
No. 15.

CHAPTER 12
FURTHER TOPICS IN
ANALYSIS
The broader perspective and additional tools made available through complex variable
theory enable us to consider fruitfully a number of topics in analysis that have wide appli-
cation in areas of relevance to physics. In this chapter we survey several such topics.
12.1
ORTHOGONAL POLYNOMIALS
Many physical problems lead to second-order differential equations corresponding to
Sturm-Liouville problems, and often the solutions of interest in physics are polynomials,
deﬁned on a range and with weighting factors that make them eigenfunctions of Hermitian
problems. A number of interesting features of such problems can be approached with the
aid of complex variable theory.
Rodrigues Formulas
Odile Rodrigues showed that a large class of second-order Sturm-Liouville ordinary dif-
ferential equations (ODEs) had polynomial solutions which could be put in a compact and
useful form now generally called a Rodrigues formula. While such formulas could be pre-
sented case by case with an aura of coincidence or mystery, the approach we take here is
to develop them from a general viewpoint, after which we can proceed to more detailed
discussion of well-known special cases.
Consider a second-order Sturm-Liouville ODE of the general form
p(x)y′′ + q(x)y′ + λy = 0,
(12.1)
551
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

552
Chapter 12 Further Topics in Analysis
with p(x) and q(x) restricted to the polynomial forms
p(x) = αx2 + βx + γ,
q(x) = µx + ν.
(12.2)
The forms of p and q are sufﬁciently general to include most of the ODEs with classi-
cal sets of polynomials as solutions (the Legendre, Hermite, and Laguerre ODEs, among
others). When Eq. (12.1) has as a solution a polynomial of degree n, we can write
yn(x) =
n
X
j=0
g j x j,
(12.3)
with coefﬁcient gn nonzero. Setting to zero the coefﬁcient of xn when yn is inserted into
the ODE, we have
n(n −1)αgn + nµgn + λgn = 0,
(12.4)
showing that the eigenvalue λn which corresponds to yn must have the value
λn = −n(n −1)α −nµ.
(12.5)
In Chapter 7 we identiﬁed an ODE of the form of Eq. (12.1) as self-adjoint if p′(x) =
q(x), and also showed that if an ODE was not already self-adjoint as written, it could be
converted to self-adjoint form by multiplying all its terms by a weight factor w(x), which
must be such that
(wp)′ = wq,
or
w′ = wq −p′
p
.
(12.6)
As shown previously, this equation is separable and has solution
w(x) = p−1 exp


x
Z q(x)
p(x)dx

.
(12.7)
The introduction of w enables the ODE to assume the form
d
dx

w(x)p(x)y′
+ λw(x)y = 0,
(12.8)
which was useful for discussing orthogonality properties of its solutions.
Our current interest in w(x), however, is in the observation by Rodrigues that its par-
ticular form permits the solutions yn(x) to be written in the compact and interesting form
that is now called its Rodrigues formula:
yn(x) =
1
w(x)
 d
dx
n 
wp(x)n
.
(12.9)
The proof of Eq. (12.9) is both simple and ingenious. Using the deﬁning condition for
w(x), Eq. (12.6), we ﬁrst obtain
p

wpn′ = wpn
(n −1)p′ + q

.
(12.10)

12.1 Orthogonal Polynomials
553
We then differentiate this equation n + 1 times and divide by w. Because p is only
quadratic in x and q is linear, application of Leibniz’s formula to the multiple differen-
tiations leads to only three terms on the left-hand side and two on the right:
p
w
 d
dx
n+2 
wpn
+ (n + 1)p′
w
 d
dx
n+1 
wpn
+ n(n + 1)p′′
2w
 d
dx
n 
wpn
= (n −1)p′ + q
w
 d
dx
n+1 
wpn
+ (n + 1)[(n −1)p′′ + q′]
w
 d
dx
n 
wpn
.
(12.11)
Our objective is to manipulate Eq. (12.11) into a form showing that yn as given in
Eq. (12.9) is a solution to the ODE of Eq. (12.1). We start by identifying the terms with yn
where that is possible, and then, combining or canceling similar terms, we reach
p
w
 d
dx
n+2 
wpn
+ 2p′ −q
w
 d
dx
n+1 
wpn
−
n2 −n −2
2
p′′ + (n + 1)q′

yn = 0.
(12.12)
To complete our analysis we now need to move the factors 1/w so that only n differenti-
ations appear to their right, enabling identiﬁcation of the remaining terms of the equation
with yn or its derivatives. We note the identity
p
w
 d
dx
n+2 
wpn
= p
 1
w
 d
dx
n 
wpn′′
−2p
dw−1
dx
 d
dx
n+1 
wpn
−p
d2w−1
dx2
 d
dx
n 
wpn
,
which reduces, using Eq. (12.6), to
p
w
 d
dx
n+2 
wpn
= py′′
n + 2(q −p′)
w
 d
dx
n+1 
wpn
−

p′′ −q′ −q −p′
p

yn.
(12.13)
Substituting Eq. (12.13) into Eq. (12.12), some further simpliﬁcation results:
py′′
n + q
w
 d
dx
n+1 
wpn
−
n2 −n
2
p′′ + nq′ −q(q −p′)
p

yn = 0.
(12.14)
Our ﬁnal step is to use the identity
q
w
 d
dx
n+1 
wpn
= qy′
n −q(q −p′)
p
yn,
(12.15)
which brings us to
py′′
n + qy′
n −
n2 −n
2
p′′ + nq′

yn = 0.
(12.16)

554
Chapter 12 Further Topics in Analysis
Noting that p′′ = 2α and q′ = µ, we conﬁrm that yn is a solution of Eq. (12.1) with the
eigenvalue given in Eq. (12.5).
Finally, we need to show that Rodrigues’ formula, Eq. (12.9), results in an expression
that is a polynomial of degree n. We note that a typical term of that formula will in-
volve a j-fold differentiation of w and an (n −j)-fold differentiation of pn. After the
differentiation of pn, we are left with p j times a polynomial. The differentiation of w
will, applying Eq. (12.6), leave (w/p j) times a polynomial, and the numerator and de-
nominator factors p j cancel. In addition, the w from the differentiation cancels against
the initial factor w−1, leaving each term of yn in polynomial form. When all terms of yn
are combined, the resulting polynomial must have the degree consistent with Eq. (12.5),
namely n.
Example 12.1.1
RODRIGUES FORMULA FOR HERMITE ODE
The Hermite ODE is
y′′ −2xy′ + λy = 0,
or
py′′ + qy′ + λy = 0
with p = 1, q = −2x. We easily ﬁnd
w = exp


x
Z
(−2x)dx

= e−x2.
The Rodrigues formula is therefore (with a factor (−1)n to obtain the Hermite polynomials
with their conventional signs)
yn(x) = (−1)n
w
 d
dx
n 
wpn
= (−1)nex2  d
dx
n
e−x2.
(12.17)
■
Schlaeﬂi Integral
One of the nice features of the Rodrigues formulas is that the multiple differentiations can
be converted to a convenient form by use of Cauchy’s integral formula. Using Eq. (11.33),
we have
yn(x) =
1
w(x)
n!
2πi
I
C
w(z)[p(z)]n
(z −x)n+1 dz,
(12.18)
where the contour C encloses the point x, and must be such that w(z)[p(z)]n is analytic
everywhere on and within C. This formula is known as the Schlaeﬂi integral for yn(x).
It is possible to introduce the Schlaeﬂi integral as the deﬁnition of a set of functions yn
and, from that deﬁnition, prove that yn is a solution to the corresponding ODE. Because
we created the Schaeﬂi integral to represent a function already known to be a solution,
veriﬁcation that it solves the ODE becomes redundant.

12.1 Orthogonal Polynomials
555
Generating Functions
Many sets of functions arising in mathematical physics can be deﬁned in terms of gener-
ating functions. Such functions include, but are not limited to the orthogonal polynomials
yn that have been the subject of our discussion of Rodrigues formulas. For now, we make
no assumptions as to the source of the functions involved.
If fn(x) is a set of functions, deﬁned for integer values of the index n, it may be the case
that the fn(x) can be described as the coefﬁcients of the powers of an auxiliary variable, t,
in the expansion of a function g(x,t), which is called a generating function:
g(x,t) =
X
n
cn fn(x)tn.
(12.19)
The range of n may be semi-inﬁnite, with n ≥0, thereby describing a Taylor series, or it
may extend from −∞to +∞, thus describing a Laurent series. The additional coefﬁcient,
cn, permits adjustment of the function set to an agreed-upon scaling. Different choices of
cn will also lead to different generating functions g(x,t) for the same set of fn.
Applying the residue theorem, we can see that the generating function expansion is
closely related to contour integral representations of the functions fn:
cn fn(x) =
1
2πi
I g(x,t)
tn+1 dt,
(12.20)
where the contour encircles t = 0 but no other singularities of the integrand (with respect
to t).
A generating function may be regarded as providing the deﬁnition of a function set
fn(x), or alternatively it may have been obtained as the encapsulation of the fn which
were already deﬁned in some other way (e.g., as polynomial solutions of a Sturm-Liouville
ODE). We shall later take up the issue of obtaining generating functions for previously
speciﬁed fn, focusing for now only on ways in which they can be used.
It is obvious that by explicitly evaluating the implied expansion one can extract the
members of a function set from its generating function. However, a more important feature
of generating functions is that they can be very useful in deriving relationships between
members of the set fn. For example,
∂g(x,t)
∂t
=
X
n
ncn fn(x)tn−1 =
X
n
(n + 1)cn+1 fn+1(x)tn,
and if we can relate g and ∂g/∂t we have a corresponding relation between fn and fn+1.
Relations between the fn(x) and their derivatives f ′
n(x) can be deduced by differentiating
g(x,t) with respect to x.
Example 12.1.2
HERMITE POLYNOMIALS
A generating function formula for the Hermite polynomials Hn(x) (at their conventional
scaling) is
e−t2+2tx =
∞
X
n=0
Hn(x)tn
n!.
(12.21)

556
Chapter 12 Further Topics in Analysis
To develop a recurrence formula connecting Hn of contiguous index values, we compute
∂
∂t e−t2+2tx = (2x −2t)e−t2+2tx =
∞
X
n=0
nHn(x)tn−1
n! .
(12.22)
Expanding the exponential in the central member of Eq. (12.22) (and suppressing tem-
porarily the argument of Hn),
∞
X
n=0
2x Hn
tn
n! −
∞
X
n=0
2Hn
tn+1
n!
=
∞
X
n=0
nHn
tn−1
n! .
Extracting the coefﬁcient of tn from each of these summations, we reach (for each n)
2x Hn
n!
−2Hn−1
(n −1)! = (n + 1)Hn+1
(n + 1)!
,
which reduces to
2x Hn(x) −2nHn−1(x) = Hn+1(x).
(12.23)
Equation (12.23) is called a recurrence formula; it permits the construction of the en-
tire series of Hn from starting values (typically H0 and H1, which are easily computed
directly).
A derivative formula can be obtained by differentiating Eq. (12.21) with respect to x.
We have
∂
∂x e−t2+2tx = 2te−t2+2tx =
∞
X
n=0
H′
n(x)tn
n!.
Substituting Eq. (12.21) into the central member of this equation, we get
∞
X
n=0
2Hn(x)tn+1
n!
=
∞
X
n=0
H′
n(x)tn
n!,
which leads directly to
2nHn−1(x) = H′
n(x).
(12.24)
■
In later chapters we illustrate the application of these ideas to a variety of special func-
tions; in the next section of this chapter we apply them to a generating function that leads
to quantities known as Bernoulli numbers.
Finding Generating Functions
To take generating functions out of the realm of magic, we next consider how they might
be obtained. For a more or less arbitrary function set, this question has been a topic of
current interest in mathematical research, with methods of several sorts devised during the
past century by Rainville, Weisner, Truesdell, and others. See the works by McBride and
Talman in Additional Readings.

12.1 Orthogonal Polynomials
557
For sets of polynomials arising in Sturm-Liouville problems and described by Rodrigues
formulas, we can be more explicit. Using the Schlaeﬂi integral, Eq. (12.18), we can form
g(x,t) =
1
w(x)
∞
X
n=0
cntn n!
2πi
I
C
w(z)[p(z)]n
(z −x)n+1 dz.
(12.25)
Recall that C encloses x and that wpn must be analytic throughout the region within the
contour.
In principle Eq. (12.25) can be evaluated to obtain g(x,t), for example by choosing C
to be such that the summation can be brought inside the z integral and (after specifying
cn) evaluating ﬁrst the sum and then the contour integral. In practice the difﬁculty of doing
this may depend on the problem, including the choice of cn. We provide one example of
the process.
Example 12.1.3
LEGENDRE POLYNOMIALS
We use the formal process described above to obtain a generating function for the Legendre
polynomials. The Legendre ODE is of the form discussed in Eq. (12.1),
(1 −x2)y′′ −2xy′ + λy = 0,
implying that
p(x) = 1 −x2,
q(x) = −2x,
and the equation is, as written, self-adjoint, so w(x) = 1. From the generating-function
formula based on the Schlaeﬂi integral, Eq. (12.25), we choose cn = (−1)n/2nn!, thereby
reaching
g(x,t) =
∞
X
n=0
(−1)ntn
2nn!
 n!
2πi
I
C
(1 −z2)n
(z −x)n+1 dz.
Interchanging the summation and integration (which we will justify later), the factors de-
pendent on n form a geometric series, which we can sum:
∞
X
n=0
(z2 −1)t
2(z −x)
n
1
z −x =
1
z −x −1
2(z2 −1)t
= −2
t

z2 −2z
t + 2x −t
t
−1
.
Inserting this result into the formula for g(x,t), we now have
g(x,t) = −2
t
1
2πi
I
C

z2 −2z
t + 2x −t
t
−1
dz
= −2
t
1
2πi
I
C
dz
(z −z1)(z −z2),
(12.26)

558
Chapter 12 Further Topics in Analysis
where z1 and z2 are the roots of the quadratic form in the ﬁrst line of the equation:
z1 = 1
t −
√
1 −2xt + t2
t
,
z2 = 1
t +
√
1 −2xt + t2
t
.
In order for Eq. (12.26) to be valid, it must have been legitimate to interchange the sum-
mation and integration, which is the case only if the summation is uniformly convergent
(with respect to z) for all points at which it is used (i.e., everywhere on the contour C). It is
convenient to analyze the convergence for small t and x and for a contour with |z| = 1.
Once a ﬁnal formula has been obtained, its range of validity can be extended by appeal to
analytic continuation.
On the assumed contour and for small x, there will be a range of |t| ≪1 for which

(z2 −1)t
2(z −x)
 < 1,
guaranteeing convergence of the geometric series. We now return to the evaluation of the
contour integral in Eq. (12.26). It has two poles, at z = z1 and z = z2. For small x and |t|,
z2 will be approximately 2/t and will be exterior to the contour, while z1 will be close
to the origin of z. Thus, only the residue of the integrand at z = z1 will contribute to the
contour integral, which will have the value
g(x,t) = −2
t
1
z1 −z2
.
Since
z1 −z2 = −2
t
p
1 −2xt + t2,
we obtain the Legendre polynomial generating function as
g(x,t) =
1
√
1 −2xt + t2 .
(12.27)
■
Summary—Orthogonal Polynomials
For ﬁve classical sets of orthogonal polynomials, we summarize in Table 12.1 their ODEs,
Rodrigues formulas, and generating functions. Omitted from the list are important sub-
sidiary polynomial sets (e.g., those connected with the associated Legendre and associated
Laguerre ODEs).
Exercises
12.1.1
Starting from the Rodrigues formula in Table 12.1 for the Hermite polynomials Hn,
derive the generating function for the Hn given in that table.

12.1 Orthogonal Polynomials
559
Table 12.1
Orthogonal Polynomials: ODEs, Rodrigues Formulas, and Generating
Functions
Rodrigues Formula
Generating Function
Legendre:
(1 −x2)y′′ −2xy′ + n(n + 1)y = 0
Pn(x) =
1
2nn!
 d
dx
n
(x2 −1)n
(1 −2xt + t2)−1/2 =
∞
X
n=0
Pn(x)tn
Hermite:
y′′ −2xy′ + 2ny = 0
Hn(x) = (−1)nex2  d
dx
n
e−x2
e−t2+2xt =
∞
X
n=0
1
n! Hn(x)tn
Laguerre:
xy′′ + (1 −x)y′ + ny = 0
Ln(x) = ex
n!
 d
dx
n  xne−x
e−xt/(1−t)
1 −t
=
∞
X
n=0
Ln(x)tn
Chebyshev I:
(1 −x2)y′′ −xy′ + n2y = 0
Tn(x) = (−1)n(1 −x2)1/2
(2n −1)!!
 d
dx
n
(1 −x2)n−1/2
1 −t2
1 −2xt + t2 = T0(x) + 2
∞
X
n=1
Tn(x)tn
Chebyshev II:
(1 −x2)y′′ −3xy′ + n(n + 2)y = 0
Un(x) =
(−1)n(n + 1)
(2n + 1)!!(1 −x2)1/2
 d
dx
n
(1 −x2)n+1/2
1
1 −2xt + t2 =
∞
X
n=0
Un(x)tn
12.1.2
(a)
Starting from the Laguerre ODE,
xy′′ + (1 −x)y′ + λy = 0,
obtain the Rodrigues formula for its polynomial solutions Ln(x).
(b)
From the Rodrigues formula, scaled as in Table 12.1, derive the generating func-
tion for the Ln(x) given in that table.
12.1.3
Carry out in detail the steps needed to conﬁrm that the (n + 1)-fold differentiation of
Eq. (12.10) leads to Eq. (12.12).
12.1.4
Conﬁrm the algebraic steps that convert Eq. (12.12) into Eq. (12.16).
12.1.5
Given the following integral representations, in which the contours encircle the origin
but no other singular points, derive the corresponding generating functions:
(a)
Bessel functions:
Jn(x) =
1
2πi
I
e(x/2)(t−1/t)t−n−1dt.
(b)
Modiﬁed Bessel functions:
In(x) =
1
2πi
I
e(x/2)(t+1/t)t−n−1dt.

560
Chapter 12 Further Topics in Analysis
12.1.6
Expand the generating function for the Legendre polynomials, (1 −2tz + t2)−1/2, in
powers of t. Assume that t is small. Collect the coefﬁcients of t0,t1, and t2.
ANS.
a0= P0(z) = 1,
a1= P1(z) = z,
a2= P2(z) = 1
2(3z2 −1).
12.1.7
The set of Chebyshev polynomials usually denoted Un(x) has the generating-function
formula
1
1 −2xt + t2 =
∞
X
n=0
Un(x)tn.
Derive a recurrence formula (for integer n ≥0) connecting three Un of consecutive n.
12.2
BERNOULLI NUMBERS
A generating-function approach is a convenient way to introduce the set of numbers ﬁrst
used in mathematics by Jacques (James, Jacob) Bernoulli. These quantities have been de-
ﬁned in a number of different ways, so extreme care must be taken in combining formulas
from works by different authors. Our deﬁnition corresponds to that used in the reference
work Handbook of Mathematical Functions (AMS-55). See Additional Readings.
Since the Bernoulli numbers, denoted Bn, do not depend on a variable, their generating
function depends only on a single (complex) variable, and the generating-function formula
has the speciﬁc form
t
et −1 =
∞
X
n=0
Bntn
n! .
(12.28)
The inclusion of the factor 1/n! in the deﬁnition is just one of the ways some deﬁnitions of
Bernoulli numbers differ. We defer for the moment the important question as to the circle
of convergence of the expansion in Eq. (12.28).
Since Eq. (12.28) is a Taylor series, we may identify the Bn as successive derivatives of
the generating function:
Bn =
 dn
dtn

t
et −1

t=0
.
(12.29)
To obtain B0, we must take the limit of t/(et −1) as t →0, easily ﬁnding B0 = 1. Applying
Eq. (12.29), we also have
B1 = d
dt

t
et −1

t=0
= lim
t→0

1
et −1 −
tet
(et −1)2

= −1
2.
(12.30)

12.2 Bernoulli Numbers
561
In principle we could continue to obtain further Bn, but it is more convenient to proceed
in a more sophisticated fashion. Our starting point is to examine
∞
X
n=2
Bntn
n!
=
t
et −1 −B0 −B1t =
t
et −1 −1 + t
2
=
−t
e−t −1 −1 −t
2,
(12.31)
where we have used the fact that
t
et −1 =
−t
e−t −1 −t.
(12.32)
Equation (12.31) shows that the summation on its left-hand side is an even function of t,
leading to the conclusion that all Bn of odd n (other than B1) must vanish.
We next use the generating function to obtain a recursion relation for the Bernoulli
numbers. We form
et −1
t
t
et −1 = 1 =
" ∞
X
m=0
tm
(m + 1)!
#"
1 −t
2 +
∞
X
n=1
B2n
t2n
(2n)!
#
= 1 +
∞
X
m=1
tm

1
(m + 1)! −
1
2m!

+
∞
X
N=2
t N
≤N/2
X
n=1
B2n
(2n)!(N −2n + 1)!
= 1 +
∞
X
N=2
t N
(N + 1)!

−N −1
2
+
≤N/2
X
n=1
N + 1
2n

B2n

.
(12.33)
Since the coefﬁcient of each power of t in the ﬁnal summation of Eq. (12.33) must vanish,
we may set to zero for each N the expression in its square brackets. Changing N, if even,
to 2N and if odd, to 2N −1, Eq. (12.33) leads to the pair of equations
N −1
2 =
N
X
n=1
2N + 1
2n

B2n,
N −1 =
N−1
X
n=1
2N
2n

B2n.
(12.34)
Either of these equations can be used to obtain the B2n sequentially, starting from B2. The
ﬁrst few Bn are listed in Table 12.2.
To obtain additional relations involving the Bernoulli numbers, we next consider the
following representation of cott:
cott = cost
sint = i
eit + e−it
eit −e−it

= i
e2it + 1
e2it −1

= i

1 +
2
e2it −1

.

562
Chapter 12 Further Topics in Analysis
Table 12.2
Bernoulli Numbers
n
Bn
Bn
0
1
1.000000000
1
−1
2
−0.500000000
2
1
6
0.166666667
4
−1
30
−0.033333333
6
1
42
0.023809524
8
−1
30
−0.033333333
10
5
66
0.075757576
Note. Further values are given in AMS-55;
see Abramowitz in Additional Readings.
Multiplying by t and rearranging slightly,
t cott = 2it
2 +
2it
e2it −1 =
∞
X
n=0
B2n
(2it)2n
(2n)!
=
∞
X
n=0
(−1)n B2n
(2t)2n
(2n)! ,
(12.35)
where the term 2it/2 has canceled the B1 term that would otherwise appear in the expan-
sion.
Now that we have our Bernoulli-number expansion identiﬁed with t cott, we can see
that it represents a function with singularities (poles) at t = mπ, where m = ±1, ±2,....
There is no singularity at t = 0 (due to the presence of the factor t), so the singularity
nearest the expansion point (the origin) is at |t| = π. Since the argument in the expansion
is 2t, we conclude that the generating series for the Bernoulli numbers, Eq. (12.28), will
have the radius of convergence |2t| = 2π. This observation is, of course, consistent with
the fact that the zeros of et −1 are for t at integer multiples of 2πi.
To obtain another representation of the Bernoulli numbers, we write Bn using the
contour-integration formula, Eq. (12.20). Noting that for use in this equation cn fn(x) =
Bn/n!, we have
Bn = n!
2πi
I
t
et −1
dt
tn+1 ,
(12.36)
where the integral is a circle within the radius of convergence of the generating series. We
can, at least in principle, evaluate the integral using the residue theorem. For n = 0 we
have a simple pole with a residue of +1, and
B0 = 0!
2πi · 2πi(+1) = 1.

12.2 Bernoulli Numbers
563
−2πi
−4πi
−8πi
−6πi
2πi
4πi
6πi
8πi
R
C
x
y
A
A¢
FIGURE 12.1
Contour of integration for Bernoulli numbers.
For n = 1 the singularity at t = 0 becomes a second-order pole, and the limiting process
prescribed by Eq. (11.68) yields the residue −1
2, so
B1 = 1!
2πi · 2πi

−1
2

= −1
2,
consistent with our previous result. For n ≥2 the poles at t = 0 are of increasing order and
this procedure becomes rather tedious, so we resort to a different approach. We deform
the contour of our integral representation as shown in Fig. 12.1, which differs from the
original circular contour in that it surrounds all the poles of the integrand at t = ±2πmi,
m = 1,2,..., while avoiding the inclusion of the pole at t = 0. In contrast to the high-order
pole at t = 0, the other poles are all ﬁrst-order, with residues that are easily evaluated.
To use the new contour, we need to identify the contributions from its constituent parts.
The direction of travel around the contour causes the small circle about t = 0 to contribute
+2πi times the residue of the integrand at t = 0, i.e., the result that when multiplied by
n!/2πi is equal to Bn. The remainder of the contour makes no contribution to the integral:
(1) Because the integrand is analytic along the real axis and there is no branch cut there,
the segments A and A′, which are in opposite directions of travel, cancel; and (2) the
large circle contributes negligibly (for n ≥2) because at large |t| the integrand behaves
asymptotically as 1/|t|n. Noting that the poles at nonzero t are encircled in a clockwise
sense, we have the following relatively simple result (for n ≥2):
Bn = −n!
2πi
X
2πi

residues of
t−n
et −1 at poles t ̸= 0

.
(12.37)
Since the residue at t = 2πmi is simply (2πmi)−n, Eq. (12.37) becomes
Bn = −
n!
(2πi)n
∞
X
m=1
 1
mn +
1
(−m)n

,

564
Chapter 12 Further Topics in Analysis
which further reduces, for 2n ≥2, to
B2n = (−1)n+1 (2n)!
(2π)2n
∞
X
m=1
2
m2n = (−1)n+1 2(2n)!
(2π)2n ζ(2n),
B2n+1 = 0.
(12.38)
Note that the Bn of odd n > 1 are correctly shown to vanish, and that the Bernoulli numbers
of even n > 0 are identiﬁed as proportional to Riemann zeta functions, which ﬁrst appeared
in this book at Eq. (1.12). We repeat the deﬁnition:
ζ(z) =
∞
X
m=1
1
mz .
Equation (12.38) is an important result because we already have a straightforward way
to obtain values of the Bn, via Eq. (12.34), and Eq. (12.38) can be inverted to give a closed
expression for ζ(2n), which otherwise was known only as a summation. This representa-
tion of the Bernoulli numbers was discovered by Euler.
It is readily seen from Eq. (12.38) that |B2n| increases without limit as n →∞. Numeri-
cal values have been calculated by Glaisher.1 Illustrating the divergent behavior of the
Bernoulli numbers, we have
B20 = −5.291 × 102
B200 = −3.647 × 10215.
Some authors prefer to deﬁne the Bernoulli numbers with a modiﬁed version of Eq. (12.38)
by using
Bn = 2(2n)!
(2π)2n ζ(2n),
(12.39)
the subscript being just half of our subscript and all signs positive. Again, when using other
texts or references, you must check to see exactly how the Bernoulli numbers are deﬁned.
The Bernoulli numbers occur frequently in number theory. The von Staudt-Clausen the-
orem states that
B2n = An −1
p1
−1
p2
−1
p3
−··· −1
pk
,
(12.40)
in which An is an integer and p1, p2,..., pk are all the prime numbers such that pi −1 is
a divisor of 2n. It may readily be veriﬁed that this holds for
B6
(A3 = 1, p = 2,3,7),
B8
(A4 = 1, p = 2,3,5),
B10
(A5 = 1, p = 2,3,11),
and other special cases.
1J. W. L. Glaisher, table of the ﬁrst 250 Bernoulli numbers (to nine ﬁgures) and their logarithms (to ten ﬁgures). Trans. Cam-
bridge Philos. Soc. 12: 390 (1871-1879).

12.2 Bernoulli Numbers
565
The Bernoulli numbers appear in the summation of integral powers of the integers,
N
X
j=1
j p,
p integral,
and in numerous series expansions of the transcendental functions, including tan x, cot x,
ln|sin x|, (sin x)−1, ln|cos x|, ln|tan x|, (cosh x)−1, tanh x, and coth x. For example,
tan x = x + x3
3 + 2
15x5 + ··· + (−1)n−122n(22n −1)B2n
(2n)!
x2n−1 + ··· .
(12.41)
The Bernoulli numbers are likely to appear in such series expansions because of the deﬁ-
nition, Eq. (12.28), the form of Eq. (12.35), and the relation to the Riemann zeta function,
Eq. (12.38).
Bernoulli Polynomials
If Eq. (12.28) is generalized slightly, we have
tets
et −1 =
∞
X
n=0
Bn(s)tn
n!
(12.42)
deﬁning the Bernoulli polynomials, Bn(s). It is clear that Bn(s) will be a polynomial of
degree n, since the Taylor expansion of the generating function will contain contributions
in which each instance of t may (or may not) be accompanied by a factor s. The ﬁrst seven
Bernoulli polynomials are given in Table 12.3.
If we set s = 0 in the generating function formula, Eq. (12.42), we have
Bn(0) = Bn,
n = 0,1,2,...,
(12.43)
showing that the Bernoulli polynomial evaluated at zero equals the corresponding
Bernoulli number.
Table 12.3
Bernoulli Polynomials
B0 = 1
B1 = x −1
2
B2 = x2 −x + 1
6
B3 = x3 −3
2 x2 + 1
2 x
B4 = x4 −2x3 + x2 −1
30
B5 = x5 −5
2 x4 + 5
3 x3 −1
6 x
B6 = x6 −3x5 + 5
2 x4 −1
2 x2 + 1
42

566
Chapter 12 Further Topics in Analysis
Two other important properties of the Bernoulli polynomials follow from the deﬁning
relation, Eq. (12.42). If we differentiate both sides of that equation with respect to s, we
have
t2ets
et −1 =
∞
X
n=0
B′
n(s)tn
n!
=
∞
X
n=0
Bn(s)tn+1
n!
=
∞
X
n=1
Bn−1(s)
tn
(n −1)!,
(12.44)
where the second line of Eq. (12.44) is obtained by rewriting its left-hand side using the
generating-function formula. Equating the coefﬁcients of equal powers of t in the two lines
of Eq. (12.44), we obtain the differentiation formula
d
ds Bn(s) = nBn−1(s),
n = 1,2,3,....
(12.45)
We also have a symmetry relation, which we can obtain by setting s = 1 in Eq. (12.42).
The left-hand side of that equation then becomes
tet
et −1 =
−t
e−t −1.
(12.46)
Thus, equating Eq. (12.42) for s = 1 with the Bernoulli-number expansion (in −t) of the
right-hand side of Eq. (12.46), we reach
∞
X
n=0
Bn(1)tn
n! =
∞
X
n=0
Bn
(−t)n
n!
,
which is equivalent to
Bn(1) = (−1)n Bn(0).
(12.47)
These relations are used in the development of the Euler-Maclaurin integration formula.
Exercises
12.2.1
Verify the identities, Eqs. (12.32) and (12.46).
12.2.2
Show that the ﬁrst Bernoulli polynomials are
B0(s) = 1
B1(s) = s −1
2
B2(s) = s2 −s + 1
6.
Note that Bn(0) = Bn, the Bernoulli number.

12.3 Euler-Maclaurin Integration Formula
567
12.2.3
Show that
tan x =
∞
X
n=1
(−1)n−122n(22n −1)B2n
(2n)!
x2n−1,
−π
2 < x < π
2 .
Hint. tan x = cot x −2cot2x.
12.3
EULER-MACLAURIN INTEGRATION FORMULA
One use of the Bernoulli polynomials is in the derivation of the Euler-Maclaurin integra-
tion formula. This formula is used both to develop asymptotic expansions (treated later in
this chapter) and to obtain approximate values for summations. An important application
of the Euler-Maclaurin formula, presented in Chapter 13, is its use to derive Stirling’s
formula, an asymptotic expression for the gamma function.
The technique we use to develop the Euler-Maclaurin formula is repeated integration by
parts, using Eq. (12.45) to create new derivatives. We start with
1
Z
0
f (x)dx =
1
Z
0
f (x)B0(x)dx,
(12.48)
where we have, for reasons that will shortly become apparent, inserted the redundant factor
B0(x) = 1. From Eq. (12.45), we note that
B0(x) = B′
1(x),
and we substitute B′
1(x) for B0(x) in Eq. (12.48), integrate by parts, and identify B1(1) =
−B1(0) = 1
2, thereby obtaining
1
Z
0
f (x)dx = f (1)B1(1) −f (0)B1(0) −
1
Z
0
f ′(x)B1(x)dx
= 1
2

f (1) + f (0)

−
1
Z
0
f ′(x)B1(x)dx.
(12.49)
Again using Eq. (12.45), we have
B1(x) = 1
2 B′
2(x).
Inserting B′
2(x) and integrating by parts again, we get
1
Z
0
f (x)dx = 1
2
h
f (1) + f (0)
i
−1
2
h
f ′(1)B2(1) −f ′(0)B2(0)
i
+ 1
2
1
Z
0
f (2)(x)B2(x)dx.
(12.50)

568
Chapter 12 Further Topics in Analysis
Using the relation
B2n(1) = B2n(0) = B2n,
n = 0,1,2,...,
(12.51)
Eq. (12.50) simpliﬁes to
1
Z
0
f (x)dx = 1
2
h
f (1) + f (0)
i
−B2
2
h
f ′(1) −f ′(0)
i
+ 1
2
1
Z
0
f (2)(x)B2(x)dx. (12.52)
Continuing, we replace B2(x) by B′
3(x)/3 and once again integrate by parts. Because
B2n+1(1) = B2n+1(0) = 0,
n = 1,2,3,...,
(12.53)
the integration by parts produces no integrated terms, and
1
2
1
Z
0
f (2)(x)B2(x)dx =
1
2 · 3
1
Z
0
f (2)(x)B′
3(x)dx = −1
3!
1
Z
0
f (3)(x)B3(x)dx.
(12.54)
Substituting B3(x) = B′
4(x)/4 and carrying out one more partial integration, we get inte-
grated terms containing B4(x), which simplify according to Eq. (12.51). The result is
−1
3!
1
Z
0
f (3)(x)B3(x)dx = B4
4!
h
f (3)(1) −f (3)(0)
i
+ 1
4!
1
Z
0
f (4)(x)B4(x)dx.
(12.55)
We may continue this process, with steps that are entirely analogous to those that led
to Eqs. (12.54) and (12.55). After steps leading to derivatives of f of order 2q −1,
we have
1
Z
0
f (x)dx = 1
2

f (1) + f (0)

−
q
X
p=1
1
(2p)! B2p

f (2p−1)(1) −f (2p−1)(0)

+
1
(2q)!
1
Z
0
f (2q)(x)B2q(x)dx.
(12.56)
This is the Euler-Maclaurin integration formula. It assumes that the function f (x) has the
required derivatives.

12.3 Euler-Maclaurin Integration Formula
569
The range of integration in Eq. (12.56) may be shifted from [0,1] to [1,2] by replacing
f (x) by f (x + 1). Adding such results up to [n −1,n], we obtain
n
Z
0
f (x)dx = 1
2 f (0) + f (1) + f (2) + ··· + f (n −1) + 1
2 f (n)
−
q
X
p=1
1
(2p)! B2p
h
f (2p−1)(n) −f (2p−1)(0)
i
+
1
(2q)!
1
Z
0
B2q(x)
n−1
X
ν=0
f (2q)(x + ν)dx.
(12.57)
Note that the derivative terms at the intermediate integer arguments all cancel. However,
the intermediate terms f ( j) do not, and 1
2 f (0) + f (1) + ··· + 1
2 f (n) appear exactly as
in trapezoidal integration, or quadrature, so the summation over p may be interpreted as a
correction to the trapezoidal approximation. Equation (12.57) may therefore be seen as a
generalization of Eq. (1.10).
In many applications of Eq. (12.57) the ﬁnal integral containing f (2q), though small,
will not approach zero as q is increased without limit, and the Euler-Maclaurin formula
then has an asymptotic, rather than convergent character. Such series, and the implications
regarding their use, are the topic of a later section of this chapter.
One of the most important uses of the Euler-Maclaurin formula is in summing series by
converting them to integrals plus correction terms.2 Here is an illustration of the process.
Example 12.3.1
ESTIMATION OF ζ(3)
A straightforward application of Eq. (12.57) to ζ(3) proceeds as follows (noting that all
derivatives of f (x) = 1/x3 vanish in the limit x →∞):
ζ(3) =
∞
X
n=1
1
n3 = 1
2 f (1) +
∞
Z
1
dx
x3 −
q
X
p=1
B2p
(2p)! f (2p−1)(1) + remainder.
(12.58)
Evaluating the integral, setting f (1) = 1, and inserting
f (2n−1)(x) = −(2n + 1)!
2x2n+2
with x = 1, Eq. (12.58) becomes
ζ(3) = 1
2 + 1
2 +
q
X
p=1
(2p + 1)B2p
2x2p+2
+ remainder.
(12.59)
2See R. P. Boas and C. Stutz, Estimating sums with integrals. Am. J. Phys. 39: 745 (1971), for a number of examples.

570
Chapter 12 Further Topics in Analysis
Table 12.4
Contributions to ζ(3) of Terms in
Euler-Maclaurin Formula
n0 = 1
n0 = 2
n0 = 4
Explicit terms
0.500000
1.062500
1.169849
R ∞
n0 x−3dx
0.500000
0.125000
0.031250
B2 term
0.250000
0.015615
0.000977
B4 term
−0.083333
−0.001302
−0.000020
B6 term
0.083333
0.000326
0.000001
B8 term
−0.150000
−0.000146
−0.000000
B10 term
0.416667
0.000102
0.000000
B12 term
−1.645238
−0.000100
−0.000000
B14 term
8.750000
0.000134
0.000000
Suma
1.166667
1.201995
1.202057
a Sums only include data above horizontal marker.
Left column: formula applied to entire summation; central column:
formula applied starting from second term; right column: formula
starting from fourth term.
To assess the quality of this result, we list, in the ﬁrst data column of Table 12.4, the con-
tributions to it. The line marked “explicit terms” consists presently of only the term 1
2 f (1).
We note that the individual terms start to increase after the B4 term; since it is our inten-
tion not to evaluate the remainder, the accuracy of the expansion is limited. As discussed
more extensively in the section on asymptotic expansions, the best result available from
these data is obtained by truncating the expansion before the terms start to increase; adding
the contributions above the marker line in the table, we get the value listed as “Sum.” For
reference, the accurate value of ζ(3) is 1.202057.
We can improve the result available from the Euler-Maclaurin formula by explicitly
calculating some initial terms and applying the formula only to those that remain. This
stratagem causes the derivatives entering the formula to be smaller and diminishes the
correction from the trapezoid-rule estimate. Simply starting the formula at n = 2 instead
of n = 1 reduces the error markedly; see the second data column of Table 12.4. Now the
“explicit terms” consist of f (1) + 1
2 f (2). Starting the Euler-Maclaurin formula at n = 4
further improves the result, then reaching better than seven-ﬁgure accuracy.
■
When the Euler-Maclaurin formula is applied to sums whose summands have a ﬁnite
number of nonzero derivatives, it can evaluate them exactly. See Exercise 12.3.1.
Exercises
12.3.1
The Euler-Maclaurin integration formula may be used for the evaluation of ﬁnite series:
n
X
m=1
f (m) =
n
Z
1
f (x)dx + 1
2 f (1) + 1
2 f (n) + B2
2!
h
f ′(n) −f ′(1)
i
+ ··· .

12.4 Dirichlet Series
571
Show that
(a)
n
X
m=1
m = 1
2n(n + 1).
(b)
n
X
m=1
m2 = 1
6n(n + 1)(2n + 1).
(c)
n
X
m=1
m3 = 1
4n2(n + 1)2.
(d)
n
X
m=1
m4 = 1
30n(n + 1)(2n + 1)(3n2 + 3n −1).
12.3.2
The Euler-Maclaurin integration formula provides a way of calculating the Euler-
Mascheroni constant γ to high accuracy. Using f (x) = 1/x in Eq. (12.57) (with interval
[1,n]) and the deﬁnition of γ , Eq. (1.13), we obtain
γ =
n
X
s=1
s−1 −lnn −1
2n +
N
X
k=1
B2k
(2k)n2k .
Using double-precision arithmetic, calculate γ for N = 1,2,....
Note. See D. E. Knuth, Euler’s constant to 1271 places. Math. Comput. 16: 275 (1962).
ANS.
For n = 1000, N = 2
γ = 0.5772 1566 4901.
12.4
DIRICHLET SERIES
Series expansions of the general form
S(s) =
X
n
an
ns
are known as Dirichlet series, and our knowledge of contour integration methods and
Bernoulli numbers enables us to evaluate a variety of expressions of this type. One of the
most important Dirichlet series is that of the Riemann zeta function,
ζ(s) =
∞
X
n=1
1
ns .
(12.60)
We have already evaluated a sum from which ζ(2) can be extracted.

572
Chapter 12 Further Topics in Analysis
Example 12.4.1
EVALUATION OF ζ(2)
From Example 11.9.1, we have
S(a) =
∞
X
n=1
1
n2 + a2 = π cothπa
2a
−
1
2a2 .
Simply by taking the limit a →0, we have
ζ(2) = lim
a→0 S(a) = lim
a→0
 π
2a
 1
πa + πa
3 + ···

−
1
2a2

= π2
6 .
(12.61)
■
From the relation with the Bernoulli numbers, or alternatively (and perhaps less conve-
niently) by contour-integration methods, we ﬁnd
ζ(4) = π4
90 .
Values of ζ(2n) through ζ(10) are listed in Exercise 12.4.1. The zeta functions of odd
integer argument seem unamenable to evaluation in closed form, but are easy to compute
numerically (see Example 12.3.1).
Other useful Dirichlet series, in the notation of AMS-55 (see Additional Readings),
include
η(s) =
∞
X
n=1
(−1)n−1n−s = (1 −21−s)ζ(s),
(12.62)
λ(s) =
∞
X
n=0
(2n −1)−s = (1 −2−s)ζ(s),
(12.63)
β(s) =
∞
X
n=0
(−1)n(2n + 1)−s.
(12.64)
Closed expressions are available (for integer n ≥1) for ζ(2n), η(2n), and λ(2n), and for
β(2n −1). The sums with exponents of opposite parity cannot be reduced to ζ(2n) or per-
formed by the contour-integral methods we discussed in Chapter 11. An important series
that can only be evaluated numerically is that whose result is Catalan’s constant, which is
β(2) = 1 −1
32 + 1
52 −··· = 0.91596559....
(12.65)

12.4 Dirichlet Series
573
For reference, we list a few of these summable Dirichlet series:
ζ(2) = 1 + 1
22 + 1
32 + ··· = π2
6 ,
(12.66)
ζ(4) = 1 + 1
24 + 1
34 + ··· = π4
90 ,
(12.67)
η(2) = 1 −1
22 + 1
32 + ··· = π2
12 ,
(12.68)
η(4) = 1 −1
24 + 1
34 + ··· = 7π4
720 ,
(12.69)
λ(2) = 1 + 1
32 + 1
52 + ··· = π2
8 ,
(12.70)
λ(4) = 1 + 1
34 + 1
54 + ··· = π4
96 ,
(12.71)
β(1) = 1 −1
3 + 1
5 −··· = π
4 ,
(12.72)
β(3) = 1 −1
33 + 1
53 −··· = π3
32 .
(12.73)
Exercises
12.4.1
From B2n = (−1)n−1 2(2n)!
(2π)2n ζ(2n), show that
(a) ζ(2) = π2
6 ,
(d) ζ(8) = π8
9450,
(b) ζ(4) = π4
90 ,
(e) ζ(10) =
π10
93,555.
(c) ζ(6) = π6
945,
12.4.2
The integral
1
Z
0
[ln(1 −x)]2 dx
x
appears in the fourth-order correction to the magnetic moment of the electron. Show
that it equals 2ζ(3).
Hint. Let 1 −x = e−t.

574
Chapter 12 Further Topics in Analysis
12.4.3
(a) Show that
∞
Z
0
(ln z)2
1 + z2 dz = 4

1 −1
33 + 1
53 −1
73 + ···

.
(b) By contour integration show that this series evaluates to π3/8.
12.4.4
Show that Catalan’s constant, β(2), may be written as
β(2) = 2
∞
X
k=1
(4k −3)−2 −π2
8 .
Hint. π2 = 6ζ(2).
12.4.5
Show that
(a)
Z 1
0
ln(1 + x)
x
dx = 1
2ζ(2),
(b) lim
a→1
Z a
0
ln(1 −x)
x
dx = ζ(2).
Note that the integrand in part (b) diverges for a = 1 but that the integral is convergent.
12.4.6
(a)
Show that the equation ln2 = P∞
s=1(−1)s+1s−1, Eq. (1.53), may be rewritten as
ln2 =
n
X
s=2
2−sζ(s) +
∞
X
p=1
(2p)−n−1

1 −1
2p
−1
.
Hint. Take the terms in pairs.
(b)
Calculate ln2 to six signiﬁcant ﬁgures.
12.4.7
(a)
Show that the equation π/4 = P∞
s=1(−1)s+1(2s −1)−1, Eq. (12.72), may be
rewritten as
π
4 = 1 −2
n
X
s=1
4−2sζ(2s) −2
∞
X
p=1
(4p)−2n−2

1 −
1
(4p)2
−1
.
(b)
Calculate π/4 to six signiﬁcant ﬁgures.
12.5
INFINITE PRODUCTS
We saw in Chapter 11 that complex variable theory can be used to generate inﬁnite-product
representations of analytic functions. Here we develop some of their properties. For that
purpose it is convenient to write these products in the form
P =
∞
Y
n=1
(1 + an).
The inﬁnite product may be related to an inﬁnite series by the obvious method of taking
the logarithm:
ln
∞
Y
n=1
(1 + an) =
∞
X
n=1
ln(1 + an).
(12.74)

12.5 Inﬁnite Products
575
The main theorem regarding convergence of inﬁnite products is the following:
If 0 ≤an < 1, the inﬁnite products Q∞
n=1(1 + an) and Q∞
n=1(1 −an) converge if
P∞
n=1 an converges and diverge if P∞
n=1 an diverges.
For the inﬁnite product Q(1 + an), note that
1 + an ≤ean,
which means that the partial product consisting of the ﬁrst n factors satisﬁes
pn ≤esn,
where sn is the sum of the ﬁrst n an. Letting n →∞,
∞
Y
n=1
(1 + an) ≤exp
∞
X
m=1
an,
(12.75)
thereby giving an upper bound for the inﬁnite product.
To develop a lower bound, we note that, because all ai > 0,
pn = 1 +
n
X
i=1
ai +
n
X
i=1
n
X
j=1
aia j + ··· ≥sn.
Hence
∞
Y
n=1
(1 + an) ≥
∞
X
n=1
an.
(12.76)
If the inﬁnite sum remains ﬁnite, the inﬁnite product will also. But if the inﬁnite sum
diverges, so will the inﬁnite product.
The case Q(1 −an) is complicated by the negative signs, but a proof similar to the
foregoing may be developed by noting that for an < 1
2,
(1 −an) ≤(1 + an)−1
and
(1 −an) ≥(1 + 2an)−1.
Example 12.5.1
CONVERGENCE OF INFINITE PRODUCTS FOR sin z AND cos z
These products, developed in Eqs. (11.89) and (11.90), are
sin z = z
∞
Y
n=1

1 −
z2
n2π2

,
cos z =
∞
Y
n=1

1 −
z2
(n −1/2)2π2

.
(12.77)
The product expansion of sin z converges for all z, because, writing the factors as (1−an),
∞
X
n=1
an = z2
π2
∞
X
n=1
n−2 = z2
π2 ζ(2) = z2
6 ,

576
Chapter 12 Further Topics in Analysis
a convergent result. For the expansion of cos z, we have
∞
X
n=1
an = 4z2
π2
∞
X
n=1
(2n −1)−2 = 4z2
π2 λ(2) = z2
2 ,
also convergent for all z. Note, however, that if z is large, many terms of the product will
have to be taken before either of these series approaches convergence. In fact, the main
use of these series is in establishing mathematical results rather than for precise numerical
work in physics.
■
We close this section with one further example illustrating a technique for working with
inﬁnite products.
Example 12.5.2
AN INTERESTING PRODUCT
We wish to evaluate the inﬁnite product
P =
∞
Y
n=2

1 −1
n2

.
We note that the product we seek is equivalent to all but the ﬁrst term of the product
expansion of sin z with z = π as given in Eq. (12.77). In fact, the missing ﬁrst term, which
is zero, guarantees that we will get the correct result for sinπ. For general z, we move
the ﬁrst term (and the prefactor z) to the left-hand side of the product formula for sin z,
reaching
sin z
z(1 −z2/π2) =
∞
Y
n=2

1 −
z2
n2π2

.
We now take the limits of the two sides of this equation as z →π, applying l’Hôpital’s
rule to evaluate the left-hand side and recognizing the right-hand side as P. Thus,
P = lim
z→π
sin z
z(1 −z2/π2) =
cos z
1 −3z2/π2

z=π
= −1
1 −3 = +1
2.
■
Exercises
12.5.1
Using
ln
∞
Y
n=1
(1 ± an) =
∞
X
n=1
ln(1 ± an)
and the Maclaurin expansion of ln(1±an), show that the inﬁnite product Q∞
n=1(1±an)
converges or diverges with the inﬁnite series P∞
n=1 an.

12.6 Asymptotic Series
577
12.5.2
An inﬁnite product appears in the form
∞
Y
n=1
1 + a/n
1 + b/n

,
where a and b are constants. Show that this inﬁnite product converges only if a = b.
12.5.3
Show that the inﬁnite product representations of sin x and cos x are consistent with the
identity 2sin x cos x = sin2x.
12.5.4
Determine the limit to which Q∞
n=2

1 + (−1)n
n

converges.
12.5.5
Show that Q∞
n=2
h
1 −
2
n(n+1)
i
= 1
3.
12.5.6
Prove that Q∞
n=2

1 −1
n2

= 1
2.
12.5.7
Verify the Euler identity Q∞
p=1(1 + z p) = Q∞
q=1(1 −z2q−1)−1,
|z| < 1.
12.5.8
Show that Q∞
r=1(1 + x/r)e−x/r converges for all ﬁnite x (except for the zeros of 1 +
x/r).
Hint. Write the nth factor as 1 + an.
12.5.9
Derive the formula, valid for small x,
lnsin x = ln x +
X
anxn,
giving the explicit form for the coefﬁcients an.
Hint. d(lnsin x)/dx = cot x.
12.5.10
Using the inﬁnite product representations of sin z, show that
z cot z = 1 −2
∞
X
m,n=1
 z
nπ
2m
,
and hence that the Bernoulli numbers are given by the formula
B2n = (−1)n−1 2(2n)!
(2π)2n ζ(2n).
This is an alternate route to Eq. (12.38).
Hint. The result of Exercise 12.5.9 will be helpful.
12.6
ASYMPTOTIC SERIES
Asymptotic series frequently occur in physics. In fact, one of the earliest and still impor-
tant approximations of quantum mechanics, the WKB expansion (the initials stand for its
originators, Wenzel, Kramers, and Brillouin), is an asymptotic series. In numerical com-
putations, these series are employed for the accurate computation of a variety of functions.

578
Chapter 12 Further Topics in Analysis
We consider here two types of integrals that lead to asymptotic series: ﬁrst, integrals of
the form
I1(x) =
∞
Z
x
e−u f (u)du,
where the variable x appears as the lower limit of an integral. Second, we consider the
form
I2(x) =
∞
Z
0
e−u f
u
x

du,
with the function f to be expanded as a Taylor series (binomial series). Asymptotic series
often occur as solutions of differential equations; we encounter many examples in later
chapters of this book.
Exponential Integral
The nature of an asymptotic series is perhaps best illustrated by a speciﬁc example. Sup-
pose that we have the exponential integral function3
Ei(x) =
x
Z
−∞
eu
u du,
(12.78)
which we ﬁnd more convenient to write in the form
−Ei(−x) =
∞
Z
x
e−u
u du = E1(x),
(12.79)
to be evaluated for large values of x. This function has a series expansion that converges
for all x, namely
E1(x) = −γ −ln x −
∞
X
n=1
(−1)nxn
nn!
,
(12.80)
which we derive in Chapter 13, but the series is totally useless for numerical evalua-
tion when x is large. We need another approach, for which it is convenient to generalize
Eq. (12.79) to
I (x, p) =
∞
Z
x
e−u
u p du,
(12.81)
where we restrict consideration to cases in which x and p are positive. As already stated,
we seek an evaluation for large values of x.
3This function occurs frequently in astrophysical problems involving gas with a Maxwell-Boltzmann energy distribution.

12.6 Asymptotic Series
579
Integrating by parts, we obtain
I (x, p) = e−x
x p −p
∞
Z
x
e−u
u p+1 du = e−x
x p −pe−x
x p+1 + p(p + 1)
∞
Z
x
e−u
u p+2 du.
Continuing to integrate by parts, we develop the series
I (x, p) = e−x
 1
x p −
p
x p+1 + p(p + 1)
x p+2
−··· + (−1)n−1
(p + n −2)!
(p −1)!x p+n−1

+ (−1)n (p + n −1)!
(p −1)!
∞
Z
x
e−u
u p+n du.
(12.82)
This is a remarkable series. Checking the convergence by the d’Alembert ratio test, we
ﬁnd
lim
n→∞
|un+1|
|un|
= lim
n→∞
(p + n)!
(p + n −1)! · 1
x = lim
n→∞
p + n
x
= ∞
(12.83)
for all ﬁnite values of x. Therefore our series as an inﬁnite series diverges everywhere!
Before discarding Eq. (12.83) as worthless, let us see how well a given partial sum approxi-
mates our function I (x, p). Taking sn as the partial sum of the series through n terms and
Rn as the corresponding remainder,
I (x, p) −sn(x, p) = (−1)n+1 (p + n)!
(p −1)!
∞
Z
x
e−u
u p+n+1 du = Rn(x, p).
In absolute value
|Rn(x, p)| ≤(p + n)!
(p −1)!
∞
Z
x
e−u
u p+n+1 du.
When we substitute u = v + x, the integral becomes
∞
Z
x
e−u
u p+n+1 du = e−x
∞
Z
0
e−v
(v + x)p+n+1 dv
=
e−x
x p+n+1
∞
Z
0
e−v 
1 + v
x
−p−n−1
dv.
For large x the ﬁnal integral approaches 1 and
|Rn(x, p)| ≈(p + n)!
(p −1)!
e−x
x p+n+1 .
(12.84)

580
Chapter 12 Further Topics in Analysis
This means that if we take x large enough, our partial sum sn will be an arbitrarily good
approximation to the function I (x, p). Our divergent series, Eq. (12.82), therefore is per-
fectly good for computations of partial sums. For this reason it is sometimes called a semi-
convergent series. Note that the power of x in the denominator of the remainder, namely
p +n +1, is higher than the power of x in the last term included in sn(x, p), namely p +n.
Thus, our asymptotic series for E1(x) assumes the form
ex E1(x) = ex
∞
Z
x
e−u
u du
≈sn(x) = 1
x −1!
x2 + 2!
x2 −3!
x4 + ··· + (−1)n n!
xn+1 ,
(12.85)
where we must choose to terminate the series after some n.
Since the remainder Rn(x, p) alternates in sign, the successive partial sums give alter-
nately upper and lower bounds for I (x, p). The behavior of the series (with p = 1) as a
function of the number of terms included is shown in Fig. 12.2, where we have plotted
partial sums of ex E1(x) for the value x = 5. The optimum determination of ex E1(x) is
given by the closest approach of the upper and lower bounds, that is, for x = 5, between
s6 = 0.1664 and s5 = 0.1741. Therefore
0.1664 ≤ex E1(x)

x=5 ≤0.1741.
(12.86)
Actually, from tables,
ex E1(x)

x=5 = 0.1704,
(12.87)
0.1704
0.1714
0.1664
0.20
0.19
0.18
0.17
0.16
0.15
0.14
sn(x= 5)
n
10
9
8
7
6
5
4
3
2
1
FIGURE 12.2
Partial sums of ex E1(x) |x=5.

12.6 Asymptotic Series
581
within the limits established by our asymptotic expansion. Note that inclusion of addi-
tional terms in the series expansion beyond the optimum point reduces the accuracy of
the representation. As x is increased, the spread between the lowest upper bound and the
highest lower bound will diminish. By taking x large enough, one may compute ex E1(x)
to any desired degree of accuracy. Other properties of E1(x) are derived and discussed in
Section 13.6.
Cosine and Sine Integrals
Asymptotic series may also be developed from deﬁnite integrals, provided that the inte-
grand has the required behavior. As an example, the cosine and sine integrals (in Table 1.2)
are deﬁned by
Ci(u) = −
∞
Z
u
cost
t
dt,
(12.88)
si(u) = −
∞
Z
u
sint
t
dt.
(12.89)
Combining these, using the formula for eit,
Ci(u) + isi(u) = −
∞
Z
u
e−it
t
dt,
and then changing the integration variable from t to z, we reach
F(u) = Ci(u) + isi(u) = −eiu
∞
Z
0
eizdz
u + z .
(12.90)
To further process F(u), we now consider the contour integral
−eiu
I
C
eizdz
u + z ,
where the contour C is that shown in Fig. 12.3. Since we are interested in evaluation for
large positive (and real) u, our integrand has as its only singularity a pole on the negative
real axis, so the region enclosed by the contour is entirely analytic and the contour integral
therefore vanishes. The exponential and the denominator cause the arc at inﬁnity (labeled
B) not to contribute to the contour integral, so the integral we seek is obtained from seg-
ment A and must be equal to the negative of the integral on segment D. Therefore, we
have
F(u) = −eiu
∞
Z
0
e−yidy
u + iy ,
(12.91)

582
Chapter 12 Further Topics in Analysis
A
D
B
FIGURE 12.3
Contour for sine and cosine integrals.
which is already helpful since we have converted an oscillatory integral into one with a
monotonically and exponentially decreasing integrand. To obtain an asymptotic expansion,
we continue by expanding the denominator of the integrand using the binomial theorem,
writing
1
u + iy = 1
u
"
1 −iy
u +
iy
u
2
−···
#
.
We plan to integrate in y from zero to inﬁnity, and the proposed expansion will be diver-
gent when y > u, but we proceed anyway, because the terms of the series will initially
be decreasing and will be satisfactory as an asymptotic expansion. Formally, we take the
viewpoint that we are writing 1/(u + iy) as a ﬁnite series plus a remainder, and we will
abandon the expansion at or before the point that the remainder is a minimum.
Inserting the expansion, and integrating termwise using the formula
∞
Z
0
yne−ydy = n!,
we get
F(u) ≈−ieiu
u

1 −i
1!
u

−
 2!
u2

+ i
 3!
u3

+
 4!
u4

−···

.
(12.92)
As for our earlier example, the exponential integral, this series will diverge for all u, but if
u is sufﬁciently large the terms will initially decrease to very small values before increasing
again toward divergence.
To go from the expansion of F(u) to those of Ci and si, we need to separate it into real
and imaginary parts. Writing eiu = cosu + i sinu and collecting terms appropriately, we

12.6 Asymptotic Series
583
get as the desired asymptotic expansions
Ci(u) ≈sinu
u
N
X
n=0
(−1)n (2n)!
u2n −cosu
u
N
X
n=0
(−1)n (2n + 1)!
u2n+1
,
(12.93)
si(u) ≈−cosu
u
N
X
n=0
(−1)n (2n)!
u2n −sinu
u
N
X
n=0
(−1)n (2n + 1)!
u2n+1
.
(12.94)
Deﬁnition of Asymptotic Series
Poincaré has introduced a formal deﬁnition for an asymptotic series.4 Following Poincaré,
we consider a function f (x) whose asymptotic expansion is sought, the partial sums sn in
its expansion, and the corresponding remainders Rn(x). Though the expansion need not be
a power series, we assume that form for simplicity in the present discussion. Thus,
xn Rn(x) = xn[ f (x) −sn(x)],
(12.95)
where
sn(x) = a0 + a1
x + a2
x2 + ··· + an
xn .
(12.96)
The asymptotic expansion of f (x) is deﬁned to have the properties that
lim
x→∞xn Rn(x) = 0, for ﬁxed n,
(12.97)
and
lim
n→∞xn Rn(x) = ∞, for ﬁxed x.
(12.98)
These conditions were met for our examples, Eqs. (12.85), (12.93), and (12.94).5
For power series, as assumed in the form of sn(x), Rn(x) ≈x−n−1. With the conditions
of Eqs. (12.97) and (12.98) satisﬁed, we write
f (x) ∼
∞
X
n=0
anx−n.
(12.99)
Note the use of ∼in place of =. The function f (x) is equal to the series only in the limit
as x →∞and with the restriction to a ﬁnite number of terms in the series.
Asymptotic expansions of two functions may be multiplied together, and the result will
be an asymptotic expansion of the product of the two functions. The asymptotic expansion
of a given function f (t) may be integrated term by term (just as in a uniformly convergent
series of continuous functions) from x ≤t < ∞, and the result will be an asymptotic
4Poincaré’s deﬁnition allows (or neglects) exponentially decreasing functions. The reﬁnement of his deﬁnition is of considerable
importance for the advanced theory of asymptotic expansions, particularly for extensions into the complex plane. However, for
purposes of an introductory treatment and especially for numerical computation of expansions for which the variable is real and
positive, Poincaré’s approach is perfectly satisfactory.
5Some writers feel that the requirement of Eq. (12.98), which excludes convergent series of inverse powers of x, is artiﬁcial and
unnecessary.

584
Chapter 12 Further Topics in Analysis
expansion of
R ∞
x
f (t)dt. Term-by-term differentiation, however, is valid only under very
special conditions.
Some functions do not possess an asymptotic expansion; ex is an example of such a
function. However, if a function has an asymptotic expansion of the power-series form in
Eq. (12.99), it has only one. The correspondence is not one to one; many functions may
have the same asymptotic expansion.
One of the most useful and powerful methods of generating asymptotic expansions, the
method of steepest descents, is developed in the next section of this text.
Exercises
12.6.1
Integrating by parts, develop asymptotic expansions of the Fresnel integrals
(a) C(x) =
Z x
0
cos πu2
2 du,
(b) s(x) =
Z x
0
sin πu2
2 du.
These integrals appear in the analysis of a knife-edge diffraction pattern.
12.6.2
Rederive the asymptotic expansions of Ci(x) and si(x) by repeated integration by parts.
Hint. Ci(x) + isi(x) = −
Z ∞
x
eit
t dt.
12.6.3
Derive the asymptotic expansion of the Gauss error function
erf(x) =
2
√π
x
Z
0
e−t2dt
≈1 −e−x2
√πx

1 −
1
2x2 + 1 · 3
22x4 −1 · 3 · 5
23x6
+ ··· + (−1)n (2n −1)!!
2nx2n

.
Hint. erf(x) = 1 −erfc(x) = 1 −
2
√π
Z ∞
x
e−t2dt.
Normalized so that erf(∞) = 1, this function plays an important role in probability
theory. It may be expressed in terms of the Fresnel integrals (Exercise 12.6.1), the in-
complete gamma functions (Section 13.6), or the conﬂuent hypergeometric functions
(Section 18.5).
12.6.4
The asymptotic expressions for the various Bessel functions, Section 14.6, contain the
series
Pν(z) ∼1 +
∞
X
n=1
(−1)n
Q2n
s=1[4ν2 −(2s −1)2]
(2n)!(8z)2n
,
Qν(z) ∼
∞
X
n=1
(−1)n+1
Q2n−1
s=1 [4ν2 −(2s −1)2]
(2n −1)!(8z)2n−1
.
Show that these two series are indeed asymptotic series.

12.7 Method of Steepest Descents
585
12.6.5
For x > 1,
1
1 + x =
∞
X
n=0
(−1)n
1
xn+1 .
Test this series to see if it is an asymptotic series.
12.6.6
Derive the following Bernoulli-number asymptotic series for the Euler-Mascheroni con-
stant, deﬁned in Eq. (1.13):
γ ∼
n
X
s=1
s−1 −lnn −1
2n +
∞
X
k=1
B2k
(2k)n2k .
Here n plays the role of x.
Hint. Apply the Euler-Maclaurin integration formula to f (x) = x−1 over the interval
[1,n] for N = 1,2,... .
12.6.7
Develop an asymptotic series for
∞
Z
0
e−xv
(1 + v2)2 dv.
Take x to be real and positive.
ANS.
1
x −2!
x3 + 4!
x5 −··· + (−1)n(2n)!
x2n+1
.
12.7
METHOD OF STEEPEST DESCENTS
In this section we consider the frequently occurring situation that we require the asymptotic
behavior (for large t, assumed real) of a function f (t), where
•
f (t) is represented by an integral of the generic form
f (t) =
Z
C
F(z,t)dz,
with F(z,t) analytic in z, but also parametrically dependent on t;
•
The integration path C is, or can be deformed to be, such that for large t the dominant
contribution to the integral arises from a small range of z in the neighborhood of the
point z0 where |F(z0,t)| is a maximum on the path;
•
The integration path will pass through z0 in the orientation that causes the most rapid
decrease in |F| on departure from z0 in either direction along the path (hence the name
steepest descents); and
•
In the limit of large t the contribution to the integral from the neighborhood of z0
asymptotically approaches the exact value of f (t).

586
Chapter 12 Further Topics in Analysis
While the above conditions seem rather restrictive, they can in fact be met for many of the
important special functions of mathematical physics, including, among others, the gamma
function and various Bessel functions.
Saddle Points
The integration path supplied with the original deﬁnition of an integral representation
deﬁning a function f (t) will not usually meet the conditions outlined above, and we need
to consider the features of the integrand F(z,t) that will be useful in deﬁning a more suit-
able path which, even if the original formulation is entirely real, may be a more general
contour in the complex plane. We already know (Exercise 11.2.2) that neither the real nor
the imaginary part of an analytic function can have an extremum (either a minimum or
maximum) within the region of analyticity, and the same is also true of its modulus (this
result is Jensen’s theorem; see Exercise 12.7.1). To better understand that, let us represent
F(z,t) (in a region where it is assumed nonzero) in the form
F(z,t) = ew(z,t) = eu(z,t)+iv(z,t),
(12.100)
where u and v are the real and imaginary parts of an analytic function w; this representation
permits us to identify u as ln|F|; the fact that u cannot have an extremum makes Jensen’s
theorem obvious.
Although u cannot have an extremum, it can have a saddle point (a point at which
w′ = 0; then also du/ds = 0 for all directions ds, but with higher derivatives that are
positive in some directions and negative in others (see Fig. 12.4). Let us examine some
general features of w and its components u and v in the neighborhood of a saddle point of
u, which we designate z0. We proceed by expanding w(z,t) in a Taylor series about z0.
Because w′ = 0 there, the ﬁrst two nonzero terms of the expansion are
w(z,t) = w(z0,t) + w′′(z0,t)
2!
(z −z0)2 + ··· .
(12.101)
FIGURE 12.4
Saddle point of u (= |F|); see Eq. (12.100).

12.7 Method of Steepest Descents
587
It could be that w′′(z0,t) = 0, but that possibility makes the analysis more complicated
without changing it in a fundamental way, so we proceed under the assumption that
w′′(z0,t) ̸= 0. Using the abbreviated notations w0 = w(z0,t), w′′(z0,t) = w′′
0, and in-
troducing the polar forms w′′
0 = |w′′
0|eiα, z −z0 = reiθ, Eq. (12.101) becomes
w(z,t) = w0 + 1
2|w′′
0|ei(α+2θ)r2 + ···
(12.102)
= w0 + 1
2|w′′
0|r2h
cos(α + 2θ) + i sin(α + 2θ)
i
+ ···.
(12.103)
For later reference we note that α is the argument of w′′(z0,t). We see that, in general
at a saddle point, u (the real part of w) will increase most rapidly when α + 2θ = 2nπ,
corresponding to the opposite directions θ = −α/2 and θ = −α/2 + π. On the other hand,
u will decrease most rapidly when α + 2θ = (2n + 1)π, i.e., θ = −α/2 + ( 1
2π or 3
2π), the
two directions perpendicular to those of maximum increase. And u will (to second order)
remain constant (so-called level lines) in the directions θ = −α/2 + ( 1
4π, 3
4π, 5
4π, 7
4π).
See the left panel of Fig. 12.5.
The behavior of v (the imaginary part of w) will be similar to that of u, but displaced in
angle by 45◦. The level lines of v will be in the directions θ = −αs + (0, π/2, π, 3π/2),
and therefore will coincide with the directions of maximum increase or decrease in u. See
the right panel of Fig. 12.5.
We are now ready to identify an optimum contour for evaluating the integral repre-
sentation of f (t), namely one that passes through the saddle point z0 in the directions of
maximum rate of decrease in u with distance from z0, and therefore also in |F|. These
directions have the additional advantage that they are level lines of v, so that the factor eiv
will not produce changes of phase (oscillatory behavior and therefore numerical instabil-
ity) in F as we leave the saddle point. If we had chosen z0 to be a point other than a saddle
point, the expansion of w would have contained a nonzero linear term in r, and it would
not have been possible to construct a curve through z0 that would cause |F| to decrease in
both path directions, or to keep the phase of F constant.
Level
Level
u
v
+
+
+
+
−
−
−
−
Level
Level
FIGURE 12.5
Near a saddle point in w = u + iv: When features of u are oriented as in
the left panel, those of v are as shown in the right panel. Arrows indicate ascending
directions.

588
Chapter 12 Further Topics in Analysis
Saddle Point Method
Now that we have identiﬁed z0 and the directions of steepest descent in |F(z,t)|, we com-
plete the speciﬁcation of the method of steepest descents, also called the saddle point
method of asymptotic approximation, by assuming that the signiﬁcant contributions to the
integral are from a small range of 0 ≤r ≤a in each of the two directions along the path.
Before obtaining a ﬁnal result, we must make one more observation. Looking at the way
in which the contour had to be deformed to pass through z0, we need to determine the
sense of the path (i.e., we must decide whether the direction of travel is at θ = −α/2 + 1
2π
or at θ = −α/2 + 3
2π). Assuming that this has been decided, we can then identify, for
the portion of the path in which we descend from F(z0), dz = eiθdr. The contribution in
which we ascend to F(z0) will have the opposite sign for dz but we can handle it simply
by multiplying the descending contribution by two. Then, noting that ei(α+2θ) = −1, our
approximation to f (t) is
f (t) ≈2ew0+iθ
a
Z
0
e−|w′′
0|r2/2dr,
(12.104)
where the initial “2” causes inclusion of the ascent to z0. We now make the key assumption
of the method, namely that |w′′
0|, the measure of the rate of decrease in |F| as we leave z0,
is large enough that the bulk of the value of the integral has already been attained for small
a, and that the exponential decrease in the value of the integrand enables us to replace a
by inﬁnity without making signiﬁcant error. In problems where the saddle point method
is applicable, this condition is met when t is sufﬁciently large. We complete the present
analysis by remembering that ew0 = F(z0,t) and by evaluating the integral for a = ∞,
where, cf. Eq. (1.148), it has the value
q
π/2|w′′
0|. We get
f (t) ≈F(z0,t)eiθ
s
2π
|w′′(z0,t)|.
(12.105)
We remind the reader that
θ = −arg(w′′(z0,t))
2
+
π
2 or 3π
2

,
(12.106)
with the choice (which affects only the sign of the ﬁnal result) determined from the sense
in which the contour passes through the saddle point z0.
Sometimes it is sufﬁcient to apply the method of steepest descents only to the rapidly
varying part of an integral. This corresponds to assuming that we may make the approxi-
mation
f (t) =
Z
C
g(z,t)F(z,t)dz ≈g(z0,t)
Z
C
F(z,t)dz,
(12.107)

12.7 Method of Steepest Descents
589
after which we proceed as before. Note that this causes g not to be considered when we
deﬁne w or w′′, and our ﬁnal formula is replaced by
f (t) ≈g(z0,t)F(z0,t)eiθ
s
2π
|w′′(z0,t)|.
(12.108)
A ﬁnal note of warning: We assumed that the only signiﬁcant contribution to the inte-
gral came from the immediate vicinity of the saddle point z = z0. This condition must be
checked for each new problem.
Example 12.7.1
ASYMPTOTIC FORM OF THE GAMMA FUNCTION
In many physical problems, particularly in the ﬁeld of statistical mechanics, it is desir-
able to have an accurate approximation of the gamma or factorial function of very large
numbers. As listed in Table 1.2, the factorial function may be deﬁned by the Euler integral
t! = 0(t + 1) =
∞
Z
0
ρte−ρdρ = tt+1
∞
Z
0
et(ln z−z)dz.
(12.109)
Here we have made the substitution ρ = zt in order to convert the integral to the form given
in Eq. (12.108). As before, we assume that t is real and positive, from which it follows that
the integrand vanishes at the limits 0 and ∞. By differentiating the exponent, which we
call w(z,t), we obtain
dw
dz = t d
dz (ln z −z) = t
z −t,
w′′ = −t
z2 ,
which shows that the point z = 1 is a saddle point and argw′′(1,t) = arg(−t) = π. Apply-
ing Eq. (12.106), we see that the direction of travel through the saddle point is
θ = −argw′′
2
+
π
2 or 3π
2

= 0 or π;
the choice θ = 0 is that consistent with deformation from a path that was originally along
the real axis. In fact, what we have found is that the direction of steepest descent is along the
real axis, a conclusion that we might have reached more or less intuitively.
Direct substitution into Eq. (12.108) with g = tt+1, F = e−t, θ = 0, and |w′′| = −t
yields
t! = 0(t + 1) ≈
r
2π
t tt+1e−t =
√
2πtt+1/2e−t.
(12.110)
This result is the leading term in Stirling’s expansion of the gamma function. The method
of steepest descents is probably the easiest way of obtaining this term. Further terms in the
asymptotic expansion are developed in Section 13.4.
In this example the calculation was carried out assuming t to be real. This assumption
is not necessary. We may show (Exercise 12.7.3) that Eq. (12.110) also holds when t is
complex, provided only that its real part be required to be large and positive.
■

590
Chapter 12 Further Topics in Analysis
Sometimes the application of the saddle point method to a real integral results in a con-
tour that goes through a saddle point that is not on the real axis. Here is a relatively simple
example. A more complicated case of practical importance appears in the chapter on Bessel
functions (see Section 14.6).
Example 12.7.2
SADDLE POINT METHOD AVOIDS OSCILLATIONS
As a second example of the method of steepest descents, consider the integral
H(t) =
∞
Z
−∞
e−t(z2−1/4) costz
1 + z2
dz,
(12.111)
which we wish to evaluate for large positive t. When t is large, the integrand oscillates very
rapidly, and ordinary quadrature methods become difﬁcult. We proceed by bringing H(t)
to a form appropriate for applying the saddle point method, replacing costz by costz +
i sintz = eitz (a replacement that does not change the value of the integral because we
added an odd term to the previously even integrand). We then have
H(t) =
Z
C
g(z)e−t(z2−iz−1/4)dz,
(12.112)
with g(z) = 1/(1 + z2). This form corresponds to w(z) = −t(z2 −iz −1
4), so we have
w′(z) = −t(2z −i),
which has a zero at z0 = i/2.
(12.113)
Then, at z0, which is a saddle point,
w0 = 0,
w′′(z0) = −2t,
g(z0) = 4
3.
(12.114)
We also need the phase θ of the steepest-descent direction. Noting that arg(w′′(z0)) = π
and applying Eq. (12.106), we ﬁnd θ = 0 (or π).
We are now ready to apply Eq. (12.108). The result is
H(t) ≈
√
2π(4/3)(e0)
| −2t|
= 4
3
rπ
t .
(12.115)
As a check, we compare this approximate formula for H(t) with the result of a tedious
numerical integration: For t = 100, Hexact = 0.23284, and Hsaddle = 0.23633.
■
Exercises
We present here a rather small number of exercises on the method of steepest descents.
Several additional exercises appear elsewhere in this book, in particular in Section 14.6,
where the technique is applied to the contour integral representations of Bessel func-
tions.
12.7.1
Prove Jensen’s theorem (that |F(z)|2 can have no extremum in the interior of a region
in which F is analytic) by showing that the mean value of |F|2 on a circle about any

12.8 Dispersion Relations
591
point z0 is equal to |F(z0)|2. Explain why you can then conclude that there cannot be
an extremum of |F| at z0.
12.7.2
Find the steepest path and leading asymptotic expansion for the Fresnel integrals
Z s
0
cos x2dx,
Z s
0
sin x2dx.
Hint. Use
Z 1
0
eitz2dz.
12.7.3
Show that the formula
0(1 + s) ≈
√
2πssse−s
holds for complex values of s (with ℜe(s) large and positive).
Hint. This involves assigning a phase to s and then demanding that Im[s f (z)] be con-
stant in the vicinity of the saddle point.
12.8
DISPERSION RELATIONS
The concept of dispersion relations entered physics with the work of Kronig and Kramers
in optics. The name dispersion comes from optical dispersion, a result of the dependence
of the index of refraction on wavelength, or angular frequency. As we shall soon see, the
index of refraction n may have a real part determined by the phase velocity and a (negative)
imaginary part determined by the absorption. Kronig and Kramers showed in 1926–1927
that the real part of (n2 −1) could be expressed as an integral of the imaginary part.
Generalizing this, we shall apply the label dispersion relations to any pair of equations
giving the real part of a function as an integral of its imaginary part and the imaginary
part as an integral of its real part (we develop this in more detail below). The existence of
such integral relations might be suspected as an integral analog of the Cauchy-Riemann
differential equations, Eq. (11.9).
The applications in modern physics are widespread. For instance, the real part of the
function might describe the forward scattering of a gamma ray in a nuclear Coulomb ﬁeld
(a dispersive process). Then the imaginary part would describe the electron-positron pair
production in that same Coulomb ﬁeld (the absorptive process). As will be seen later, the
dispersion relations may be taken as a consequence of causality and therefore are indepen-
dent of the details of the particular interaction.
We consider a complex function f (z) that is analytic in the upper half-plane and on
the real axis. We also require that f (z) approach zero for large |z| in the upper half-plane
sufﬁciently rapidly that its integral over the semicircular part of the contour in Fig. 12.6
will be negligible. The point of these conditions is that we may express f (z) by the Cauchy
integral formula, Eq. (11.30), using this contour, obtaining
f (z0) =
1
2πi
∞
Z
−∞
f (x)
x −z0
dx.
(12.116)
The integral over the contour shown in Fig. 12.6 has become an integral along the x-axis.

592
Chapter 12 Further Topics in Analysis
x
z
y
−R
−∞
R
∞
FIGURE 12.6
Contour for dispersion integral.
Equation (12.116) assumes that z0 is in the upper half-plane, interior to the closed con-
tour. If z0 were in the lower half-plane, the integral would yield zero by the Cauchy in-
tegral theorem, Section 11.3. Now, if we move z0 onto the real axis (then calling it x0)
and pass it via a small clockwise semicircle s in the upper half-plane, the contour integral
(which would contain no singularities) would have nonzero contributions corresponding to
a Cauchy principal value integral minus half the usual contribution from the pole at x0, or
0 =
Z
f (x)
x −x0
dx +
Z
s
f (z)
z −x0
dz
=
Z
f (x)
x −x0
dx −πi f (x0),
equivalent to the ﬁnal formula
f (x0) = 1
πi
∞
Z
−∞
f (x)
x −x0
dx.
(12.117)
Note that the cut integral sign denotes the Cauchy principal value. Splitting Eq. (12.117)
into real and imaginary parts6 yields
f (x0) = u(x0) + iv(x0)
= 1
π
∞
Z
−∞
v(x)
x −x0
dx −i
π
∞
Z
−∞
u(x)
x −x0
dx.
Finally, equating real part to real part and imaginary part to imaginary part, we obtain
u(x0) = 1
π
∞
Z
−∞
v(x)
x −x0
dx,
v(x0) = −1
π
∞
Z
−∞
u(x)
x −x0
dx.
(12.118)
6The second argument, y = 0, is dropped: u(x0,0) →u(x0).

12.8 Dispersion Relations
593
These are the dispersion relations. The real part of our complex function is expressed as an
integral over the imaginary part. The imaginary part is expressed as an integral over the real
part. Alternatively, the real part can be called an integral transform of the imaginary part
(and vice versa); the particular transform involved is known as a Hilbert transform, and
we note that (apart from a minus sign) the Hilbert transform is its own inverse. Note that
these relations are meaningful only when f (x) is a complex function of the real variable x.
Compare Exercise 12.8.1.
From a physical point of view u(x) and/or v(x) may represent some physical measure-
ments. Then f (z) = u(z) + iv(z) is an analytic continuation over the upper half-plane,
with the value on the real axis serving as a boundary condition.
Symmetry Relations
On occasion f (x) will satisfy a symmetry relation and the integral from −∞to +∞
may be replaced by an integral over positive values only. This is of considerable physical
importance because the variable x might represent a frequency and only zero and positive
frequencies are available for physical measurements. Suppose7
f (−x) = f ∗(x).
(12.119)
Then
u(−x) + iv(−x) = u(x) −iv(x).
(12.120)
The real part of f (x) is even and the imaginary part is odd.8 In quantum mechanical scat-
tering problems these relations, Eq. (12.120), are called crossing conditions. To exploit
these crossing conditions, we rewrite the ﬁrst of Eqs. (12.118) as
u(x0) = 1
π
0
Z
−∞
v(x)
x −x0
dx + 1
π
∞
Z
0
v(x)
x −x0
dx.
(12.121)
Letting x →−x in the ﬁrst integral on the right-hand side of Eq. (12.121) and substituting
v(−x) = −v(x) from Eq. (12.120), we obtain
u(x0) = 1
π
∞
Z
0
v(x)

1
x + x0
+
1
x −x0

dx
= 2
π
∞
Z
0
xv(x)
x2 −x2
0
dx.
(12.122)
7This is not just a curiosity. It ensures that the Fourier transform of f (x) will be real. Or conversely, Eq. (12.119) is a conse-
quence when f (x) is obtained as the Fourier transform of a real function.
8u(x,0) = u(−x,0),v(x,0) = −v(−x,0). Compare these symmetry conditions with those that follow from the Schwarz reﬂec-
tion principle, Section 11.10.

594
Chapter 12 Further Topics in Analysis
Similarly,
v(x0) = −2
π
∞
Z
0
x0u(x)
x2 −x2
0
dx.
(12.123)
The original Kronig-Kramers optical dispersion relations were in this form. The asymptotic
behavior (x0 →∞) of Eqs. (12.122) and (12.123) lead to quantum mechanical sum rules.
See Exercise 12.8.4.
Optical Dispersion
The function exp[i(kx −ωt)] can describe an electromagnetic wave moving along the x-
axis in the positive direction with velocity v = ω/k; ω is the angular frequency, k the wave
number or propagation vector, and n = ck/ω, the index of refraction. From Maxwell’s
equations with electric permittivity ε and magnetic permeability unity, and using Ohm’s
law with conductivity σ, the propagation vector k for a dielectric becomes9
k2 = εω2
c2

1 + i 4πσ
ωε

.
(12.124)
The presence of the conductivity (which means absorption) causes k2 to have an imagi-
nary part. The propagation vector k (and therefore the index of refraction n) have become
complex.
For poor conductivity (4πσ/ωε ≪1) a binomial expansion yields
k = √εω
c + i 2πσ
c√ε
and
ei(kx−ωt) = eiω(x√ε/c−t)e−2πσ x/c√ε,
an attenuated wave.
Returning to the general expression for k2, Eq. (12.124), we ﬁnd that the index of re-
fraction becomes
n2 = c2k2
ω2 = ε + i 4πσ
ω .
(12.125)
We take n2 to be a function of the complex variable ω (with ε and σ depending on ω).
However, n2 does not vanish as ω →∞but instead approaches unity. It therefore does
not satisfy the condition needed for a dispersion relation, but this difﬁculty can be cir-
cumvented by working with f (ω) = n2(ω) −1. The Kronig-Kramers relations then take
9See J. D. Jackson, Classical Electrodynamics, 3rd ed. New York: Wiley (1999), Sections 7.7 and 7.10. Equation (12.124) is in
Gaussian units.

12.8 Dispersion Relations
595
the form
ℜe[n2(ω0) −1] = 2
π
∞
Z
0
ωIm[n2(ω) −1]
ω2 −ω2
0
dω,
Im[n2(ω0) −1] = −2
π
∞
Z
0
ω0ℜe[n2(ω) −1]
ω2 −ω2
0
dω.
(12.126)
Knowledge of the absorption coefﬁcient at all frequencies speciﬁes the real part of the
index of refraction, and vice versa.
The Parseval Relation
When the functions u(x) and v(x) are Hilbert transforms of each other, given by
Eqs. (12.118), and each is square integrable,10 the two functions satisfy the scaling condi-
tion
∞
Z
−∞
|u(x)|2dx =
∞
Z
−∞
|v(x)|2dx.
(12.127)
This is the Parseval relation.
To derive Eq. (12.127), we start with
∞
Z
−∞
|u(x)|2dx =
∞
Z
−∞
dx

1
π
∞
Z
−∞
v(s)ds
s −x



1
π
∞
Z
−∞
v(t)dt
t −x

,
using the formula for u(x) from Eq. (12.118) twice. Integrating ﬁrst with respect to x, we
have
∞
Z
−∞
|u(x)|2dx =
∞
Z
−∞
v(s)ds
∞
Z
−∞
v(t)dt 1
π2
∞
Z
−∞
dx
(s −x)(t −x),
(12.128)
where both principal-value limits at the singularities of the integrand must now be taken for
the x integration. As shown in Exercise 12.8.8, that integration yields a delta function11:
1
π2
∞
Z
−∞
dx
(s −x)(t −x) = δ(s −t).
Thus,
∞
Z
−∞
|u(x)|2dx =
∞
Z
−∞
v(t)dt
∞
Z
−∞
v(s)δ(s −t)ds.
(12.129)
10This means that
R ∞
−∞|u(x)|2dx and
R ∞
−∞|v(x)|2dx are ﬁnite.
11Note that when s = t, the integrand has the same sign (for small ε) at x = s −ε and at x = s + ε, so the limit deﬁning the
principal value then does not exist. The singularity in the integration is that which is needed to represent a delta function.

596
Chapter 12 Further Topics in Analysis
Then the s integration is carried out by inspection, using the deﬁning property of the delta
function:
∞
Z
−∞
v(s)δ(s −t)ds = v(t).
(12.130)
Substituting Eq. (12.130) into Eq. (12.129), we have Eq. (12.127), the Parseval relation.
Again, in terms of optics, the presence of refraction over some frequency range (n ̸= 1)
implies the existence of absorption, and vice versa.
Exercises
12.8.1
Assume that the function f (z) satisﬁes the conditions for the dispersion relations. In
addition, assume that f (z) = f ∗(z∗), i.e., that it meets the conditions of the Schwarz
reﬂection principle, Eq. (11.127). Show that f (z) is identically zero.
12.8.2
For f (z) such that we may replace the closed contour of the Cauchy integral formula
by an integral over the real axis we have
f (x0) =
1
2πi



x0−δ
Z
−∞
f (x)
x −x0
dx +
∞
Z
x0+δ
f (x)
x −x0
dx



+
1
2πi
Z
C
f (x)
x −x0
dx.
Here we take C to be a small semicircle about x0 in the lower half-plane. Show that the
formula for f (x0) reduces to
f (x0) = 1
πi
∞
Z
−∞
f (x)
x −x0
dx,
which is Eq. (12.117).
12.8.3
(a)
The function f (z) = eiz does not vanish at the endpoints of the range of arg z, a
and π. Show, with the help of Jordan’s lemma, Eq. (11.102), that Eq. (12.116) still
holds.
(b)
For f (z) = eiz verify by direct integration the dispersion relations, Eq. (12.117)
or Eqs. (12.118).
12.8.4
With f (x) = u(x) + iv(x) and f (x) = f ∗(−x), show that as x0 →∞,
(a)
u(x0) ∼−2
πx2
0
Z ∞
0
xv(x)dx,
(b)
v(x0) ∼
2
πx0
Z ∞
0
u(x)dx.
In quantum mechanics relations of this form are often called sum rules.

12.8 Dispersion Relations
597
12.8.5
(a)
Given the integral equation (valid for all real x0)
1
1 + x2
0
= 1
π
∞
Z
−∞
u(x)
x −x0
dx,
use Hilbert transforms to determine u(x0).
(b)
Verify that the u(x0) found as your answer to part (a) actually satisﬁes the integral
equation.
(c)
From f (z) |y=0= u(x) + iv(x), replace x by z and determine f (z). Verify that
the conditions for the Hilbert transforms are satisﬁed.
(d)
Are the crossing conditions satisﬁed?
ANS.
(a) u(x0) =
x0
1 + x2
0
,
(c) f (z) = (z + i)−1.
12.8.6
(a)
If the real part of the complex index of refraction (squared) is constant (no optical
dispersion), show that the imaginary part is zero (no absorption).
(b)
Conversely, if there is absorption, show that there must be dispersion. In other
words, if the imaginary part of n2 −1 is not zero, show that the real part of n2 −1
is not constant.
12.8.7
Given u(x) = x/(x2 + 1) and v(x) = −1/(x2 + 1), show by direct evaluation of each
integral that
∞
Z
−∞
|u(x)|2dx =
∞
Z
−∞
|v(x)|2dx.
ANS.
Z ∞
−∞
|u(x)|2dx =
Z ∞
−∞
|v(x)|2dx = π
2 .
12.8.8
Take u(x) = δ(x), a delta function, and assume that the Hilbert transform equations
hold.
(a)
Show that
δ(w) = 1
π2
∞
Z
−∞
dy
y(y −w).
(b)
With changes of variables w = s −t and x = s −y, transform the δ representation
of part (a) into
δ(s −t) = 1
π2
∞
Z
−∞
dx
(x −s)(x −t).
Note. The δ function is discussed in Section 1.11.

598
Chapter 12 Further Topics in Analysis
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974).
Lewin, L., Polylogarithms and Associated Functions. New York: North-Holland (1981). This is a deﬁnitive
resource for the dilogarithm and its generalizations up through its publication date. It is clear and no more
difﬁcult than necessary.
McBride, E. B., Obtaining Generating Functions. New York: Springer-Verlag (1971). An introduction to meth-
ods of obtaining generating functions, both for sets of functions arising from ODEs and for those that do
not.
Nussenzveig, H. M., Causality and Dispersion Relations, Mathematics in Science and Engineering Series, Vol.
95. New York: Academic Press (1972). This is an advanced text covering causality and dispersion relations in
the ﬁrst chapter and then moving on to develop implications in a variety of areas of theoretical physics.
Talman, J. D., Special Functions. New York: W. A. Benjamin (1968). Develops the theory of a number of
special functions using their underlying group-theoretical properties, including presentation of their generating
functions.
Wyld, H. W., Mathematical Methods for Physics. Reading, MA: Benjamin/Cummings (1976), Perseus Books
(1999). This is a relatively advanced text that contains an extensive discussion of dispersion relations.

CHAPTER 13
GAMMA FUNCTION
The gamma function is probably the special function that occurs most frequently in the
discussion of problems in physics. For integer values, as the factorial function, it appears
in every Taylor expansion. As we shall later see, it also occurs frequently with half-integer
arguments, and is needed for general nonintegral values in the expansion of many func-
tions, e.g., Bessel functions of noninteger order.
It has been shown that the gamma function is one of a general class of functions that
do not satisfy any differential equation with rational coefﬁcients. Speciﬁcally, the gamma
function is one of very few functions of mathematical physics that do not satisfy either
the hypergeometric differential equation (Section 18.5) or the conﬂuent hypergeomet-
ric equation (Section 18.6). Since most physical theories involve quantities governed by
differential equations, the gamma function (by itself) does not usually describe a physi-
cal quantity of interest, but rather tends to appear as a factor in expansions of physically
relevant quantities.
13.1
DEFINITIONS, PROPERTIES
At least three different convenient deﬁnitions of the gamma function are in common use.
Our ﬁrst task is to state these deﬁnitions, to develop some simple, direct consequences, and
to show the equivalence of the three forms.
Inﬁnite Limit (Euler)
The ﬁrst deﬁnition, named after Euler, is
0(z) ≡lim
n→∞
1 · 2 · 3···n
z(z + 1)(z + 2)···(z + n)nz,
z ̸= 0,−1,−2,−3,....
(13.1)
599
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

600
Chapter 13 Gamma Function
This deﬁnition of 0(z) is useful in developing the Weierstrass inﬁnite-product form of
0(z), Eq. (13.16), and in obtaining the derivative of ln0(z) (Section 13.2). Here and else-
where in this chapter z may be either real or complex. Replacing z with z + 1, we have
0(z + 1) = lim
n→∞
1 · 2 · 3···n
(z + 1)(z + 2)(z + 3)···(z + n + 1)nz+1
= lim
n→∞
nz
z + n + 1 ·
1 · 2 · 3···n
z(z + 1)(z + 2)···(z + n)nz
= z0(z).
(13.2)
This is the basic functional relation for the gamma function. It should be noted that it is a
difference equation.
Also, from the deﬁnition,
0(1) = lim
n→∞
1 · 2 · 3···n
1 · 2 · 3···n(n + 1)n = 1.
(13.3)
Now, repeated application of Eq. (13.2) gives
0(2) = 1,
0(3) = 20(2) = 2,
0(4) = 30(3) = 2 · 3,
etc.,
so
0(n) = 1 · 2 · 3···(n −1) = (n −1)!.
(13.4)
Deﬁnite Integral (Euler)
A second deﬁnition, also frequently called the Euler integral, and already presented in
Table 1.2, is
0(z) ≡
∞
Z
0
e−ttz−1dt,
ℜe(z) > 0.
(13.5)
The restriction on z is necessary to avoid divergence of the integral. When the gamma
function does appear in physical problems, it is often in this form or some variation, such as
0(z) = 2
∞
Z
0
e−t2t2z−1dt,
ℜe(z) > 0,
(13.6)
or
0(z) =
1
Z
0

ln
1
t
z−1
dt,
ℜe(z) > 0.
(13.7)

13.1 Deﬁnitions, Properties
601
When z = 1
2, Eq. (13.6) is just the Gauss error integral, and, cf. Eq. (1.148), we have the
interesting result
0
1
2

= √π.
(13.8)
Generalizations of Eq. (13.6), the Gaussian integrals, are considered in Exercise 13.1.10.
To show the equivalence of these two deﬁnitions, Eqs. (13.1) and (13.5), consider the
function of two variables
F(z,n) =
n
Z
0

1 −t
n
n
tz−1dt,
ℜe(z) > 0,
(13.9)
with n a positive integer. This form was chosen because the exponential has the deﬁnition
lim
n→∞

1 −t
n
n
≡e−t.
(13.10)
Inserting Eq. (13.10) into Eq. (13.9), we see that the inﬁnite-n limit of F(z,n) corresponds
to 0(z) as given by Eq. (13.5):
lim
n→∞F(z,n) = F(z,∞) =
∞
Z
0
e−ttz−1 dt ≡0(z).
(13.11)
Our remaining task is to identify this limit also with Eq. (13.1).
Returning to F(z,n), we evaluate it by carrying out successive integrations by parts. For
convenience we make the substitution u = t/n. Then
F(z,n) = nz
1
Z
0
(1 −u)nuz−1 du.
(13.12)
The ﬁrst integration by parts yields
F(z,n)
nz
= (1 −u)n uz
z

1
0
+ n
z
1
Z
0
(1 −u)n−1uz du ;
(13.13)
note that (because z ̸= 0) the integrated part vanishes at both endpoints. Repeating this n
times, with the integrated part vanishing at both endpoints each time, we ﬁnally get
F(z,n) = nz
n(n −1)···1
z(z + 1)···(z + n −1)
1
Z
0
uz+n−1 du
=
1 · 2 · 3···n
z(z + 1)(z + 2)···(z + n)nz.
(13.14)
This is identical with the expression on the right side of Eq. (13.1). Hence
lim
n→∞F(z,n) = F(z,∞) ≡0(z),
where 0(z) is in the form given by Eq. (13.1), thereby completing the proof.

602
Chapter 13 Gamma Function
Inﬁnite Product (Weierstrass)
The third deﬁnition (Weierstrass’ form) is the inﬁnite product
1
0(z) ≡zeγ z
∞
Y
n=1

1 + z
n

e−z/n,
(13.15)
where γ is the Euler-Mascheroni constant
γ = 0.5772156619···,
(13.16)
which was introduced as a limit in Eq. (1.13). Existence of the limit was the topic of
Exercise 1.2.13.
This inﬁnite-product form is useful for proving various properties of 0(z). It can be
derived from the original deﬁnition, Eq. (13.1), by rewriting it as
0(z) = lim
n→∞
1 · 2 · 3···n
z(z + 1)···(z + n)nz = lim
n→∞
1
z
n
Y
m=1

1 + z
m
−1
nz.
(13.17)
Taking the reciprocal of Eq. (13.17) and using
n−z = e(−lnn)z,
(13.18)
we obtain
1
0(z) = z lim
n→∞e(−lnn)z
n
Y
m=1

1 + z
m

.
(13.19)
Multiplying and dividing the right-hand side of Eq. (13.19) by
exp

1 + 1
2 + 1
3 + ··· + 1
n

z

=
n
Y
m=1
ez/m,
(13.20)
we get
1
0(z) = z

lim
n→∞exp

1 + 1
2 + 1
3 + ··· + 1
n −lnn

z

×
"
lim
n→∞
n
Y
m=1

1 + z
m

e−z/m
#
.
(13.21)
Comparing with Eq. (1.13), we see that the parenthesized quantity in the exponent
approaches as a limit the Euler-Mascheroni constant, thereby conﬁrming Eq. (13.15).

13.1 Deﬁnitions, Properties
603
Functional Relations
In Eq. (13.2) we already obtained the most important functional relation for the gamma
function,
0(z + 1) = z 0(z).
(13.22)
Viewed as a complex-valued function, this formula permits the extension to negative z of
values obtained via numerical evaluation of the integral representation, Eq. (13.5). While
the Euler limit formula already tells us that 0(z) is an analytic function for all z except 0,
−1,..., stepwise extrapolation from the integral is a more efﬁcient numerical approach.
The gamma function satisﬁes several other functional relations, of which one of the most
interesting is the reﬂection formula,
0(z)0(1 −z) =
π
sin zπ .
(13.23)
This relation connects (for nonintegral z) values of 0(z) that are related by reﬂection about
the line z = 1/2.
One way to prove the reﬂection formula starts from the product of Euler integrals,
0(z + 1)0(1 −z) =
∞
Z
0
sze−s ds
∞
Z
0
t−ze−t dt
=
∞
Z
0
vz dv
(v + 1)2
∞
Z
0
u e−u du.
(13.24)
In obtaining the second line of Eq. (13.24) we transformed from the variables s, t to
u = s + t, v = s/t, as suggested by combining the exponentials and the powers in the
integrands. We also needed to insert the Jacobian of this transformation,
J −1 = −

1
1
1
t −s
t2

= s + t
t2
= (v + 1)2
u
;
the ﬁnal substitution becomes obvious if we note that v + 1 = u/t.
Returning to Eq. (13.24), the u integration is elementary, being equal to 1!, while
the v integration can be evaluated by contour-integration methods; it was the topic of
Exercise 11.8.20, and has the value
∞
Z
0
vz dv
(v + 1)2 =
πz
sinπz .
(13.25)
Using these results, and then replacing 0(z + 1) in Eq. (13.24) by z0(z) and canceling z
from the two sides of the resulting equation, we complete the demonstration of Eq. (13.23).
A special case of Eq. (13.23) results if we set z = 1/2. Then (taking the positive square
root), we get
0
  1
2

= √π,
(13.26)
in agreement with Eq. (13.8).

604
Chapter 13 Gamma Function
Another functional relation is Legendre’s duplication formula,
0(1 + z)0

z + 1
2

= 2−2z√π 0(2z + 1),
(13.27)
which we prove for general z in Section 13.3. However, it is instructive to prove it now for
integer values of z. Assuming z to be a nonnegative integer n, we start the proof by writing
0(n + 1) = n!, 0(2n + 1) = (2n)!, and
0

n + 1
2

= 0
1
2

·
1
2 · 3
2 ··· 2n −1
2

= √π 1 · 3···(2n −1)
2n
= √π (2n −1)!!
2n
,
(13.28)
where we have used Eq. (13.26) and the double factorial notation ﬁrst introduced in
Eqs. (1.75) and (1.76). The double factorial notation is used frequently enough in physics
applications that a familiarity with it is essential, and will from here on be used without
comment. Making the further observation that n! = 2−n(2n)!!, Eq. (13.27) follows directly.
Incidentally, we call attention to the fact that gamma functions with half-integer argu-
ments appear frequently in physics problems, and Eq. (13.28) shows how to write them in
closed form.
Analytic Properties
The Weierstrass deﬁnition shows immediately that 0(z) has simple poles at z = 0, −1, −2,
−3, ... and that [0(z)]−1 has no poles in the ﬁnite complex plane, which means that 0(z)
has no zeros. This behavior may also be seen in Eq. (13.23), if we note that π/(sinπz) is
never equal to zero. A plot of 0(z) for real z is shown in Fig. 13.1. We note sign changes
for each unit interval of negative z, that 0(1) = 0(2) = 1, and that the gamma function has
a minimum between z = 1 and z = 2, at z0 = 0.46143..., with 0(z0) = 0.88560.... The
residues Rn at the poles z = −n (n an integer ≥0) are
Rn = lim
ε→0

ε0(−n + ε)

= lim
ε→0
ε 0(−n + 1 + ε)
−n + ε
= lim
ε→0
ε 0(−n + 2 + ε)
(−n + ε)(−n + 1 + ε)
= lim
ε→0
ε0(1 + ε)
(−n + ε)···(ε) = (−1)n
n!
,
(13.29)
showing that the residues alternate in sign, with that at z = −n having magnitude 1/n!.
Schlaeﬂi Integral
A contour integral representation of the gamma function that we will ﬁnd useful in devel-
oping asymptotic series for the Bessel functions is the Schlaeﬂi integral
Z
C
e−ttν dt = (e2πiν −1)0(ν + 1),
(13.30)

13.1 Deﬁnitions, Properties
605
5
G (x+ 1)
4
3
2
1
1
2
3
4
5
−5
−4
−3
−2
−1
−1
−3
−5
x
FIGURE 13.1
Gamma function 0(x + 1) for real x.
0
Cut line
A
B
D
x
y
ε
+∞
FIGURE 13.2
Gamma function contour.
where C is the contour shown in Fig. 13.2. This contour integral representation is only
useful when ν is not an integer. For integer ν, the integrand is an entire function; both
sides of Eq. (13.30) vanish and it yields no information. However, for noninteger ν, t = 0
is a branch point of the integrand and the right-hand side of Eq. (13.30) then evaluates
to a nonzero result. Note that, unlike the contour representations we considered in earlier
chapters, the present contour is open; we cannot close it at z = +∞because of the branch
cut, nor can we close it with a large circle, as e−t becomes inﬁnite in the limit of large
negative t.
To verify Eq. (13.30), we proceed (for ν + 1 > 0) by evaluating the contributions from
the various parts of the integration path. The integral from ∞to +ε on the real axis yields
−0(ν + 1), choosing arg(z) = 0. The integral +ε to ∞(in the fourth quadrant) then yields
e2πiν0(ν + 1), the argument of z having increased to 2π. Since the circle around the
origin contributes nothing when ν > −1, Eq. (13.30) follows. Now that this equation is
established, we can deform the contour as desired (providing that we avoid the branch
point and cut), since there are no other singularities we must avoid.

606
Chapter 13 Gamma Function
It is often convenient to cast Eq. (13.30) into the more symmetrical form
Z
C
e−ttν dt = 2ieiνπ0(ν + 1) sin(νπ),
(13.31)
where C can be the contour of Fig. 13.2 or any deformation thereof that encircles the
origin, does not cross the branch cut, and begins and ends at any points respectively above
and below the cut for which x = +∞.
The above analysis establishes Eqs. (13.30) and (13.31) for ν > −1. However, we note
that the integral exists for ν < −1 as long as we stay away from the origin, and therefore
it remains valid for all nonintegral ν. What we have found is that this contour integral
representation provides an analytic continuation of the Euler integral, Eq. (13.5), to all
nonintegral ν.
Factorial Notation
Our discussion of the gamma function has been presented in terms of the classical notation,
which was ﬁrst introduced by Legendre. In an attempt to make a closer correspondence to
the factorial notation (traditionally used for integers), and to simplify the Euler integral
representation of the gamma function, Eq. (13.5), some authors have chosen to use the
notation z! as a synonym for 0(z + 1) even when z has an arbitrary complex value. Occa-
sionally one even encounters Gauss’ notation, Q(z), for the factorial function:
Y
(z) = z! = 0(z + 1).
Neither the factorial (for nonintegral arguments) nor the Gauss notation are currently
favored by most serious investigators, and we will not use them in this book.
Example 13.1.1
MAXWELL-BOLTZMANN DISTRIBUTION
In classical statistical mechanics, a state of energy E is occupied, according to the equa-
tion of Maxwell-Boltzmann statistics, with a probability proportional to e−E/kT, where k
is Boltzmann’s constant and T is the absolute temperature; it is usual to deﬁne β = 1/kT
and to write the probability of occupancy of a state of energy E as p(E) = Ce−βE. If the
number of states in a small energy interval dE at energy E is given, using a density distri-
bution function n(E), as n(E)dE, then the total probability of states at energy E assumes
the form C n(E)e−βE dE. Under those conditions, the total probability of occupancy in
any state (namely, unity) must be
1 = C
Z
n(E)e−βE dE,
(13.32)
which enables us to set the normalization constant C, and the average energy ⟨E⟩of such
a classical system will be
⟨E⟩= C
Z
E n(E)e−βE dE.
(13.33)

13.1 Deﬁnitions, Properties
607
For a structureless ideal gas, it can be shown that n(E) is proportional to E1/2, with E, the
kinetic energy of a gas molecule, in the range (0,∞). Then we may ﬁnd C from
1 = C
∞
Z
0
E1/2 e−βE dE = C 0( 3
2)
β3/2 = C
√π
2β3/2 ,
or C = 2β3/2
√π ,
and
⟨E⟩= C
∞
Z
0
E3/2 e−βE dE = C 0( 5
2)
β5/2 =
2β3/2
√π
 √π
β5/2
1
2 · 3
2

= 3
2 kT,
the known value of the average kinetic energy per molecule for a structureless classical
gas at temperature T .
In probability theory, the distribution used here is known as a gamma distribution; it
is further discussed in Chapter 23.
■
Exercises
13.1.1
Derive the recurrence relations
0(z + 1) = z0(z)
from the Euler integral, Eq. (13.5),
0(z) =
∞
Z
0
e−ttz−1dt.
13.1.2
In a power-series solution for the Legendre functions of the second kind we encounter
the expression
(n + 1)(n + 2)(n + 3)···(n + 2s −1)(n + 2s)
2 · 4 · 6 · 8···(2s −2)(2s) · (2n + 3)(2n + 5)(2n + 7)···(2n + 2s + 1),
in which s is a positive integer.
(a)
Rewrite this expression in terms of factorials.
(b)
Rewrite this expression using Pochhammer symbols; see Eq. (1.72).
13.1.3
Show that 0(z) may be written
0(z) = 2
∞
Z
0
e−t2t2z−1 dt,
ℜe(z) > 0,
0(z) =
1
Z
0

ln
1
t
z−1
dt,
ℜe(z) > 0.

608
Chapter 13 Gamma Function
13.1.4
In a Maxwellian distribution the fraction of particles of mass m with speed between v
and v + dv is
dN
N = 4π

m
2πkT
3/2
exp

−mv2
2kT

v2 dv,
where N is the total number of particles, k is Boltzmann’s constant, and T is the
absolute temperature. The average or expectation value of vn is deﬁned as ⟨vn⟩=
N −1 R
vndN. Show that
⟨vn⟩=
2kT
m
n/2 0( n+3
2 )
0( 3
2)
.
This is an extension of Example 13.1.1, in which the distribution was in kinetic energy
E = mv2/2, with dE = mv dv.
13.1.5
By transforming the integral into a gamma function, show that
−
1
Z
0
xk ln x dx =
1
(k + 1)2 ,
k > −1.
13.1.6
Show that
∞
Z
0
e−x4dx = 0
5
4

.
13.1.7
Show that
lim
x→0
0(ax)
0(x) = 1
a .
13.1.8
Locate the poles of 0(z). Show that they are simple poles and determine the residues.
13.1.9
Show that the equation 0(x) = k, k ̸= 0, has an inﬁnite number of real roots.
13.1.10
Show that, for integer s,
(a)
∞
Z
0
x2s+1 exp(−ax2)dx =
s!
2as+1 .
(b)
∞
Z
0
x2s exp(−ax2)dx = 0(s + 1
2)
2as+1/2 = (2s −1)!!
2s+1as
rπ
a .
These Gaussian integrals are of major importance in statistical mechanics.
13.1.11
Express the coefﬁcient of the nth term of the expansion of (1 + x)1/2 in powers of x
(a)
in terms of factorials of integers,
(b)
in terms of the double factorial (!!) functions.

13.1 Deﬁnitions, Properties
609
ANS.
an = (−1)n+1
(2n −3)!
22n−2n!(n −2)! = (−1)n+1 (2n −3)!!
(2n)!!
, n = 2,3,....
13.1.12
Express the coefﬁcient of the nth term of the expansion of (1 + x)−1/2 in powers of x
(a)
in terms of the factorials of integers,
(b)
in terms of the double factorial (!!) functions.
ANS.
an = (−1)n
(2n)!
22n(n!)2 = (−1)n (2n −1)!!
(2n)!!
,
n = 1,2,3....
13.1.13
The Legendre polynomial Pn may be written as
Pn(cosθ) = 2(2n −1)!!
(2n)!!

cosnθ + 1
1 ·
n
2n −1 cos(n −2)θ
+ 1 · 3
1 · 2
n(n −1)
(2n −1)(2n −3) cos(n −4)θ
+ 1 · 3 · 5
1 · 2 · 3
n(n −1)(n −2)
(2n −1)(2n −3)(2n −5) cos(n −6)θ + ···

.
Let n = 2s + 1. Then the above can be written
Pn(cosθ) = P2s+1(cosθ) =
s
X
m=0
am cos(2m + 1)θ.
Find am in terms of factorials and double factorials.
13.1.14
(a)
Show that 0
  1
2 −n

0
  1
2 + n

= (−1)nπ, where n is an integer.
(b)
Express 0
  1
2 + n

and 0( 1
2 −n) separately in terms of π1/2 and a double factorial
function.
ANS.
0( 1
2 + n) = (2n −1)!!
2n
π1/2.
13.1.15
Show that if 0(x + iy) = u + iv, then 0(x −iy) = u −iv.
This is a special case of the Schwarz reﬂection principle, Section 11.10.
13.1.16
Prove that |0(α + iβ)| = |0(α)|
∞
Y
n=0

1 +
β2
(α + n)2
−1/2
.
This equation has been useful in calculations of beta decay theory.
13.1.17
Show that for n, a positive integer,
|0(n + ib + 1)| =

πb
sinhπb
1/2
n
Y
s=1
(s2 + b2)1/2.
13.1.18
Show that for all real values of x and y, |0(x)| ≥|0(x + iy)|.

610
Chapter 13 Gamma Function
13.1.19
Show that |(0( 1
2 + iy)|2 =
π
coshπy .
13.1.20
The probability density associated with the normal distribution of statistics is given by
f (x) =
1
σ(2π)1/2 exp

−(x −µ)2
2σ 2

,
with (−∞,∞) for the range of x. Show that
(a)
⟨x⟩, the mean value of x, is equal to µ,
(b)
the standard deviation (⟨x2⟩−⟨x⟩2)1/2 is given by σ.
13.1.21
For the gamma distribution
f (x) =



1
βα0(α) xα−1e−x/β,
x > 0,
0,
x ≤0,
show that
(a)
⟨x⟩, the mean value of x, is equal to αβ,
(b)
σ 2, its variance, deﬁned as ⟨x2⟩−⟨x⟩2, has the value αβ2.
13.1.22
The wave function of a particle scattered by a Coulomb potential is ψ(r,θ). Given that
at the origin the wave function becomes
ψ(0) = e−πγ/2 0(1 + iγ ),
where γ > 0 is a dimensionless parameter, show that
|ψ(0)|2 =
2πγ
e2πγ −1.
13.1.23
Derive the contour integral representation of Eq. (13.31),
2i0(ν + 1)sinνπ =
Z
C
e−t(−t)ν dt.
13.2
DIGAMMA AND POLYGAMMA FUNCTIONS
Digamma Function
As may be noted from the three deﬁnitions in Section 13.1, it is inconvenient to deal with
the derivatives of the gamma function directly. It is more productive to take the natural
logarithm of the gamma function as given by Eq. (13.1), thereby converting the product
to a sum, and then to differentiate. The most useful results are obtained if we start with
0(z + 1):

13.2 Digamma and Polygamma Functions
611
0(z + 1) = z0(z) = lim
n→∞
n!
(z + 1)(z + 2)···(z + n)nz,
(13.34)
ln0(z + 1) = lim
n→∞
h
ln(n!) + z lnn −ln(z + 1)
−ln(z + 2) −··· −ln(z + n)
i
,
(13.35)
in which the logarithm of the limit is equal to the limit of the logarithm. Differentiating
with respect to z, we obtain
d
dz ln0(z + 1) ≡ψ(z + 1) = lim
n→∞

lnn −
1
z + 1 −
1
z + 2 −··· −
1
z + n

,
(13.36)
which deﬁnes ψ(z + 1), the digamma function. Note that this deﬁnition also corre-
sponds to
ψ(z + 1) = [0(z + 1)]′
0(z + 1) .
(13.37)
To bring Eq. (13.36) to a better form, we add and subtract the harmonic number
Hn =
n
X
m=1
1
m ,
thereby obtaining
ψ(z + 1) = lim
n→∞
"
(lnn −Hn) −
n
X
m=1

1
z + m −1
m
#
= −γ +
∞
X
m=1
z
m(m + z).
(13.38)
We have now arranged the contributions in a way that causes each group of terms to
approach a ﬁnite limit as n →∞: in that limit lnn −Hn became (minus) the Euler-
Mascheroni constant, deﬁned in Eq. (1.13), and the summation is convergent.
Setting z = 0, we ﬁnd1
ψ(1) = −γ = −0.577 215 664 901···.
(13.39)
For integer n > 0, Eq. (13.38) reduces to a form that is good for revealing its structure but
less desirable for actual computation:
ψ(n + 1) = −γ + Hn = −γ +
n
X
m=1
1
m .
(13.40)
1γ has been computed to 1271 places by D. E. Knuth, Math. Comput. 16: 275 (1962), and to 3566 decimal places by D. W.
Sweeney, ibid. 17: 170 (1963). It may be of interest that the fraction 228/395 gives γ accurate to six places.

612
Chapter 13 Gamma Function
Polygamma Function
The digamma function may be differentiated repeatedly, giving rise to the polygamma
function:
ψ(m)(z + 1) ≡dm+1
dzm+1 ln0(z + 1)
= (−1)m+1m!
∞
X
n=1
1
(z + n)m+1 ,
m = 1, 2, 3,....
(13.41)
Plots of 0(x), ψ(x), and ψ′(x) are presented in Fig. 13.3.
If we set z = 0 in Eq. (13.41), the series in that equation is that deﬁning the Riemann
zeta function,2
ζ(m) ≡
∞
X
n=1
1
nm ,
(13.42)
and we have
ψ(m)(1) = (−1)m+1m!ζ(m + 1),
m = 1,2,3,....
(13.43)
The values of polygamma functions of the positive integral argument, ψ(m)(n + 1), may
be calculated recursively; see Exercise 13.2.8.
6
5
4
3
2
1
0
−1.0
−1.0
0
1.0
2.0
3.0
4.0
x
In Γ (x+ 1)
d
dx
In Γ(x + 1)
d
dx
In Γ(x + 1)
d2
dx2
In Γ (x+ 1)
d2
dx2
Γ(x +1)
FIGURE 13.3
Gamma function and its ﬁrst two logarithmic derivatives.
2For z ̸= 0 this series has been used to deﬁne a generalization of ζ(m) known as the Hurwitz zeta function.

13.2 Digamma and Polygamma Functions
613
Maclaurin Expansion
It is now possible to write a Maclaurin expansion for ln0(z + 1):
ln0(z + 1) =
∞
X
n=1
zn
n! ψ(n−1)(1) = −γ z +
∞
X
n=2
(−1)n zn
n ζ(n).
(13.44)
This expansion is convergent for |z| < 1; for z = x, the range is −1 < x ≤1. Alternate
forms of this series appear in Exercise 13.2.2. Equation (13.44) is a possible means of
computing 0(z + 1) for real or complex z, but Stirling’s series (Section 13.4) is usually
better, and in addition, an excellent table of values of the gamma function for complex
arguments based on the use of Stirling’s series and the functional relation, Eq. (13.22), is
now available.3
Series Summation
The digamma and polygamma functions may also be used in summing series. If the general
term of the series has the form of a rational fraction (with the highest power of the index in
the numerator at least two less than the highest power of the index in the denominator), it
may be transformed by the method of partial fractions; see Eq. (1.83). This transformation
permits the inﬁnite series to be expressed as a ﬁnite sum of digamma and polygamma
functions. The usefulness of this method depends on the availability of tables of digamma
and polygamma functions. Such tables and examples of series summation are given in
AMS-55, chapter 6 (see Additional Readings for the reference).
Example 13.2.1
CATALAN’S CONSTANT
Catalan’s constant, β(2), Eq. (12.65), is given by
K = β(2) =
∞
X
k=0
(−1)k
(2k + 1)2 .
Grouping the positive and negative terms separately and starting with the unit index, to
match the form of ψ(1), Eq. (13.41), we obtain
K = 1 +
∞
X
n=1
1
(4n + 1)2 −1
9 −
∞
X
n=1
1
(4n + 3)2 .
Now, identifying the summations in terms of ψ(1), we get
K = 8
9 + 1
16 ψ(1)

1 + 1
4

−1
16ψ(1)

1 + 3
4

.
3Table of the Gamma Function for Complex Arguments, Applied Mathematics Series No. 34. Washington, DC: National Bureau
of Standards (1954).

614
Chapter 13 Gamma Function
Using the values of ψ(1) from Table 6.1 of AMS-55 (see Additional Readings for the
reference), we obtain
K = 0.91596559....
Compare this calculation of Catalan’s constant with those carried out in earlier chapters
(Exercises 1.1.12 and 12.4.4).
■
Exercises
13.2.1
For “small” values of x,
ln0(x + 1) = −γ x +
∞
X
n=2
(−1)n ζ(n)
n
xn,
where γ is the Euler-Mascheroni constant and ζ(n) the Riemann zeta function. For what
values of x does this series converge?
ANS.
−1 < x ≤1.
Note that if x = 1, we obtain
γ =
∞
X
n=2
(−1)n ζ(n)
n
,
a series for the Euler-Mascheroni constant. The convergence of this series is exceed-
ingly slow. For actual computation of γ , other, indirect, approaches are far superior
(see Exercise 12.3.2).
13.2.2
Show that the series expansion of ln0(x + 1) (Exercise 13.2.1) may be written as
(a)
ln0(x + 1) = 1
2 ln
 πx
sinπx

−γ x −
∞
X
n=1
ζ(2n + 1)
2n + 1 x2n+1,
(b)
ln0(x + 1) = 1
2 ln
 πx
sinπx

−1
2 ln
1 + x
1 −x

+ (1 −γ )x
−
∞
X
n=1
h
ζ(2n + 1) −1
i x2n+1
2n + 1.
Determine the range of convergence of each of these expressions.
13.2.3
Verify that for n, a positive integer, the following two forms of the digamma function
are equal to each other:
ψ(n + 1) =
n
X
j=1
1
j −γ
and
ψ(n + 1) =
∞
X
j=1
n
j(n + j) −γ.

13.2 Digamma and Polygamma Functions
615
13.2.4
Show that ψ(z + 1) has the series expansion
ψ(z + 1) = −γ +
∞
X
n=2
(−1)nζ(n) zn−1.
13.2.5
For a power-series expansion of ln0(z + 1), AMS-55 (see Additional Readings for the
reference) lists
ln0(z + 1) = −ln(1 + z) + z(1 −γ ) +
∞
X
n=2
(−1)nh
ζ(n) −1
i zn
n .
(a)
Show that this agrees with Eq. (13.44) for |z| < 1.
(b)
What is the range of convergence of this new expression?
13.2.6
Show that
1
2 ln
 πz
sinπz

=
∞
X
n=1
ζ(2n)
2n
z2n,
|z| < 1.
Hint. Use Eqs. (13.23) and (13.35).
13.2.7
Write out a Weierstrass inﬁnite-product deﬁnition of ln0(z + 1). Without differentiat-
ing, show that this leads directly to the Maclaurin expansion of ln0(z +1), Eq. (13.44).
13.2.8
Derive the difference relation for the polygamma function,
ψ(m)(z + 2) = ψ(m)(z + 1) + (−1)m
m!
(z + 1)m+1 ,
m = 0,1,2,....
13.2.9
The Pochhammer symbol (a)n is deﬁned (for integral n) as
(a)n = a(a + 1)···(a + n −1),
(a)0 = 1.
(a)
Express (a)n in terms of factorials.
(b)
Find (d/da)(a)n in terms of (a)n and digamma functions.
ANS.
d
da (a)n = (a)n[ψ(a + n) −ψ(a)].
(c)
Show that
(a)n+k = (a + n)k · (a)n.

616
Chapter 13 Gamma Function
13.2.10
Verify the following special values of the ψ form of the digamma and polygamma
functions:
ψ(1) = −γ,
ψ(1)(1) = ζ(2),
ψ(2)(1) = −2ζ(3).
13.2.11
Verify:
(a)
∞
Z
0
e−r lnr dr = −γ .
(b)
∞
Z
0
re−r lnr dr = 1 −γ .
(c)
∞
Z
0
rne−r lnr dr = (n −1)! + n
∞
Z
0
rn−1e−r lnr dr,
n = 1,2,3,....
Hint. These may be veriﬁed by integration by parts, or by differentiating the Euler
integral formula for 0(n + 1) with respect to n.
13.2.12
Dirac relativistic wave functions for hydrogen involve factors such as 0[2(1 −
α2Z2)1/2 +1] where α, the ﬁne structure constant, is 1/137 and Z is the atomic number.
Expand 0[2(1 −α2Z2)1/2 + 1] in a series of powers of α2Z2.
13.2.13
The quantum mechanical description of a particle in a Coulomb ﬁeld requires a knowl-
edge of the argument of 0(z) when z is complex. Determine the argument of 0(1 + ib)
for small, real b.
13.2.14
Using digamma and polygamma functions, sum the series
(a)
∞
X
n=1
1
n(n + 1),
(b)
∞
X
n=2
1
n2 −1.
Note. You can use Exercise 13.2.8 to calculate the needed digamma functions.
13.2.15
Show that
∞
X
n=1
1
(n + a)(n + b) =
1
(b −a)
h
ψ(1 + b) −ψ(1 + a)
i
,
where a ̸= b, and neither a nor b is a negative integer. It is of some interest to compare
this summation with the corresponding integral,
∞
Z
1
dx
(x + a)(x + b) =
1
b −a
h
ln(1 + b) −ln(1 + a)
i
.
The relation between ψ(x) and ln x is made explicit in the analysis leading to Stirling’s
formula.

13.3 The Beta Function
617
13.3
THE BETA FUNCTION
Products of gamma functions can be identiﬁed as describing an important class of deﬁnite
integrals involving powers of sine and cosine functions, and these integrals, in turn, can
be further manipulated to evaluate a large number of algebraic deﬁnite integrals. These
properties make it useful to deﬁne the beta function, deﬁned as
B(p,q) = 0(p)0(q)
0(p + q) .
(13.45)
For whatever it is worth, note that the B in Eq. (13.45) is an upper-case beta.
To understand the virtue of this deﬁnition, let us write the product 0(p)0(q) using the
integral representation given as Eq. (13.6), valid for ℜe(p), ℜe(q) > 0:
0(p)0(q) = 4
∞
Z
0
s2p−1e−s2 ds
∞
Z
0
t2q−1e−t2 dt.
(13.46)
The reason for using this integral representation is that the quadratic terms in the exponent,
s2 and t2, combine in a convenient way if we change the integration variables from s,t
to polar coordinates r,θ, with s = r cosθ, t = r sinθ, r2 = s2 + t2, and ds dt = r dr dθ.
Equation (13.46) becomes
0(p)0(q) = 4
∞
Z
0
r2p+2q−1e−r2 dr
π/2
Z
0
cos2p−1θ sin2q−1θ dθ
= 20(p + q)
π/2
Z
0
cos2p−1θ sin2q−1θ dθ,
where we have used Eq. (13.6) to recognize the r integration as 0(p + q). This gives us
our ﬁrst integral evaluation based on the beta function:
B(p,q) = 2
π/2
Z
0
cos2p−1θ sin2q−1θ dθ.
(13.47)
Because Eq. (13.47) is often used when p and q are integers, we rewrite for the case
p = m + 1, q = n + 1,
m!n!
(m + n + 1)! = 2
π/2
Z
0
cos2m+1θ sin2n+1θ dθ.
(13.48)
Because gamma functions of a half-integral argument are available in closed form,
Eq. (13.47) also provides a route to these trigonometric integrals for even powers of the
sine and/or cosine. Note also that from its deﬁnition it is obvious that B(p,q) = B(q, p),
showing that the integral in Eq. (13.47) does not change in value if the powers of the sine
and cosine are interchanged.

618
Chapter 13 Gamma Function
Alternate Forms, Deﬁnite Integrals
The substitution t = cos2 θ converts Eq. (13.47) to
B(p + 1,q + 1) =
1
Z
0
t p(1 −t)q dt.
(13.49)
Replacing t by x2, we obtain
B(p = 1,q + 1) = 2
1
Z
0
x2p+1(1 −x2)q dx.
(13.50)
The substitution t = u/(1 + u) in Eq. (13.49) yields still another useful form,
B(p + 1,q + 1) =
∞
Z
0
u p
(1 + u)p+q+2 du.
(13.51)
The beta function as a deﬁnite integral is useful in establishing integral representations of
the Bessel function (Exercise 14.1.17) and the hypergeometric function (Exercise 18.5.12).
Derivation of Legendre Duplication Formula
The Legendre duplication formula involves products of gamma functions, which suggests
that the beta function may provide a useful route to its proof. We start by using Eq. (13.49)
for B
 z + 1
2, z + 1
2

:
B

z + 1
2, z + 1
2

=
1
Z
0
tz−1/2(1 −t)z−1/2 dt.
(13.52)
Making the substitution t = (1 + s)/2, we have
B

z + 1
2, z + 1
2

= 2−2z
1
Z
−1
(1 −s2)z−1/2 ds
= 2−2z+1
1
Z
0
(1 −s2)z−1/2 ds = 2−2z B
1
2, z + 1
2

,
(13.53)
where we used the fact that the s integrand was even to change the integration range to
(0,1), and then used Eq. (13.50) to evaluate the resulting integral. Now, inserting the
deﬁnition, Eq. (13.45), for both instances of B in Eq. (13.53), we reach
0(z + 1
2)0(z + 1
2)
0(2z + 1)
= 2−2z 0( 1
2)0(z + 1
2)
0(z + 1)
,

13.3 The Beta Function
619
which is easily rearranged into
0(z + 1)0

z + 1
2

=
√π
22z 0(2z + 1),
(13.54)
the Legendre duplication formula, originally introduced as Eq. (13.27), but proved at that
time only for integer values of z.
Although the integrals used in this derivation are deﬁned only for ℜe(z) > −1, the
result, Eq. (13.54), holds, by analytic continuation, for all z where the gamma functions
are analytic.
Exercises
13.3.1
Verify the following beta function identities:
(a)
B(a,b) = B(a + 1,b) + B(a,b + 1),
(b)
B(a,b) = a + b
b
B(a,b + 1),
(c)
B(a,b) = b −1
a
B(a + 1,b −1),
(d)
B(a,b)B(a + b,c) = B(b,c)B(a,b + c).
13.3.2
(a)
Show that
1
Z
−1
(1 −x2)1/2x2n dx =



π/2,
n = 0
π (2n −1)!!
(2n + 2)!!,
n = 1,2,3,....
(b)
Show that
1
Z
−1
(1 −x2)−1/2x2n dx =



π,
n = 0,
π (2n −1)!!
(2n)!!
,
n = 1,2,3,....
13.3.3
Show that
1
Z
−1
(1 −x2)n dx =
2(2n)!!
(2n + 1)!!,
n = 0,1,2,....
13.3.4
Evaluate
1
Z
−1
(1 + x)a(1 −x)b dx in terms of the beta function.
ANS.
2a+b+1B(a + 1,b + 1).

620
Chapter 13 Gamma Function
13.3.5
Show, by means of the beta function, that
z
Z
t
dx
(z −x)1−α(x −t)α =
π
sinπα ,
0 < α < 1.
13.3.6
Show that the Dirichlet integral
Z Z
x pyq dx dy =
p!q!
(p + q + 2)! = B(p + 1,q + 1)
p + q + 2
,
where the range of integration is the triangle bounded by the positive x- and y-axes and
the line x + y = 1.
13.3.7
Show that
∞
Z
0
∞
Z
0
e−(x2+y2+2xy cosθ) dx dy =
θ
2sinθ .
What are the limits on θ?
Hint. Consider oblique xy-coordinates.
ANS.
−π < θ < π.
13.3.8
Evaluate (using the beta function)
(a)
π/2
Z
0
cos1/2θ dθ =
(2π)3/2
16[0(5/4)]2 ,
(b)
π/2
Z
0
cosn θ dθ =
π/2
Z
0
sinn θ dθ =
√π[(n −1)/2]!
2(n/2)!
=



(n −1)!!
n!!
for n odd,
π
2 · (n −1)!!
n!!
for n even.
13.3.9
Evaluate
1
Z
0
(1 −x4)−1/2dx as a beta function.
ANS.
[0(5/4)]2 · 4
(2π)1/2
= 1.311028777.
13.3.10
Using beta functions, show that the integral representation
Jν(z) =
2
π1/2 0(ν + 1
2)
 z
2
ν
π/2
Z
0
sin2ν θ cos(z cosθ)dθ,
ℜ(ν) > −1
2,

13.3 The Beta Function
621
reduces to the Bessel series
Jν(z) =
∞
X
s=0
(−1)s
1
s!0(s + ν + 1)
 z
2
2s+ν
,
thereby conﬁrming its validity.
13.3.11
Given the associated Legendre function, deﬁned in Chapter 15,
Pm
m (x) = (2m −1)!!(1 −x2)m/2,
show that
(a)
1
Z
−1
[Pm
m (x)]2 dx =
2
2m + 1 (2m)!,
m = 0,1,2,...,
(b)
1
Z
−1
[Pm
m (x)]2
dx
1 −x2 = 2 · (2m −1)!,
m = 1,2,3,... .
13.3.12
Show that, for integers p and q,
(a)
1
Z
0
x2p+1(1 −x2)−1/2 dx =
(2p)!!
(2p + 1)!!,
(b)
1
Z
0
x2p (1 −x2)q dx = (2p −1)!! (2q)!!
(2p + 2q + 1)!! .
13.3.13
A particle of mass m moving in a symmetric potential that is well described by V (x) =
A|x|n has a total energy 1
2m(dx/dt)2 + V (x) = E. Solving for dx/dt and integrating
we ﬁnd that the period of motion is
τ = 2
√
2m
xmax
Z
0
dx
(E −Axn)1/2 ,
where xmax is a classical turning point given by Axn
max = E. Show that
τ = 2
n
r
2πm
E
 E
A
1/n
0(1/n)
0(1/n + 1
2)
.
13.3.14
Referring to Exercise 13.3.13,
(a)
Determine the limit as n →∞of
2
n
r
2πm
E
 E
A
1/n
0(1/n)
0(1/n + 1
2)
.

622
Chapter 13 Gamma Function
(b)
Find limn→∞τ from the behavior of the integrand (E −Axn)−1/2.
(c)
Investigate the behavior of the physical system (potential well) as n →∞. Obtain
the period from inspection of this limiting physical system.
13.3.15
Show that
∞
Z
0
sinhαx
coshβx
dx = 1
2 B
α + 1
2
,
β −α
2

,
−1 < α < β.
Hint. Let sinh2x = u.
13.3.16
The beta distribution of probability theory has a probability density
f (x) = 0(α + β)
0(α)0(β) xα−1(1 −x)β−1,
with x restricted to the interval (0, 1). Show that
(a)
⟨x⟩, the mean value, is
α
α + β .
(b)
σ 2, its variance, is ⟨x2⟩−⟨x⟩2 =
αβ
(α + β)2(α + β + 1).
13.3.17
From
lim
n→∞
Z π/2
0
sin2n θ dθ
Z π/2
0
sin2n+1 θ dθ
= 1,
derive the Wallis formula for π:
π
2 = 2 · 2
1 · 3 · 4 · 4
3 · 5 · 6 · 6
5 · 7 ··· .
13.4
STIRLING’S SERIES
In statistical mechanics we encounter the need to evaluate ln(n!) for very large values of
n, and we occasionally need ln0(z) for nonintegral z when |z| is large enough that it is
inconvenient or impractical to use the Maclaurin series, Eq. (13.44), possibly followed
by repeated use of the functional relation 0(z + 1) = z0(z). These needs can be met by
the asymptotic expansion for ln0(z) known as Stirling’s series or Stirling’s formula.
While it is in principle possible to develop such an asymptotic formula by the method of
steepest descents (and in fact we have already obtained the leading term of the expansion
in this way; see Example 12.7.1), a relatively simple way of obtaining the full asymptotic
expansion is by use of the Euler-Maclaurin integration formula in Section 12.3.

13.4 Stirling’s Series
623
Derivation from Euler-Maclaurin Integration Formula
The Euler-Maclaurin formula for evaluating a deﬁnite integral on the range (0,∞),
obtained by specializing Eq. (12.57) and ignoring the remainder, is
∞
Z
0
f (x)dx = 1
2 f (0) + f (1) + f (2) + f (3) + ···
+ B2
2! f ′(0) + B4
4! f (3)(0) + B6
6! f (5)(x) + ···,
(13.55)
where Bn are Bernoulli numbers:
B2 = 1
6,
B4 = −1
30,
B6 = 1
42,
B8 = −1
30,
··· .
We proceed by applying Eq. (13.55) to the deﬁnite integral
∞
Z
0
dx
(z + x)2 = 1
z
(for z not on the negative real axis). We note, by comparing with Eq. (13.41), that
f (1) + f (2) + ··· =
∞
X
n=1
1
(z + n)2 = ψ(1)(z + 1);
this makes a connection to the gamma function and is the reason for our current strategy.
We also note that
f (2n−1)(0) =
 d
dx
2n−1
1
(z + x)2

x=0 = −(2n)!
z2n+1 ,
so the expansion yields
1
z =
∞
Z
0
dx
(z + x)2 = 1
2z2 + ψ(1)(z + 1) −B2
z3 −B4
z5 −···.
Solving for ψ(1)(z + 1), we have
ψ(1)(z + 1) = d
dz ψ(z + 1) = 1
z −1
2z2 + B2
z3 + B4
z5 + ···
= 1
z −1
2z2 +
∞
X
n=1
B2n
z2n+1 .
(13.56)
Since the Bernoulli numbers diverge strongly, this series does not converge. It is a semi-
convergent, or asymptotic, series, useful if one retains a small number of terms (compare
with Section 12.6).

624
Chapter 13 Gamma Function
Integrating once, we get the digamma function
ψ(z + 1) = C1 + ln z + 1
2z −B2
2z2 −B4
4z4 −···
= C1 + ln z + 1
2z −
∞
X
n=1
B2n
2nz2n ,
(13.57)
where C1 has a value still to be determined. In the next subsection we will show that
C1 = 0. Equation (13.57), then, gives us another expression for the digamma function,
often more useful than Eq. (13.38) or Eq. (13.44).
Stirling’s Formula
The indeﬁnite integral of the digamma function, obtained by integrating Eq. (13.57), is
ln0(z + 1) = C2 +

z + 1
2

ln z + (C1 −1)z + B2
2z + ··· +
B2n
2n(2n−1)z2n−1 + ···,
(13.58)
in which C2 is another constant of integration. We are now ready to determine C1 and
C2, which we can do by requiring that the asymptotic expansion be consistent with the
Legendre duplication formula, Eq. (13.54). Substituting Eq. (13.58) into the logarithm of
the duplication formula, we ﬁnd that satisfaction of that formula dictates that C1 = 0 and
that C2 must have the value
C2 = 1
2 ln2π.
(13.59)
Thus, inserting also values of the B2n, our ﬁnal result is
ln0(z + 1) = 1
2 ln2π +

z + 1
2

ln z −z + 1
12z −
1
360z3 +
1
1260z5 −··· .
(13.60)
This is Stirling’s series, an asymptotic expansion. The absolute value of the error is less
than the absolute value of the ﬁrst term neglected.
The leading term in the asymptotic behavior of the gamma function was one of the ex-
amples used to illustrate the method of steepest descents. In Example 12.7.1, we found that
0(z + 1) ∼
√
2π zz+1/2e−z,
corresponding to
ln0(z + 1) ∼1
2 ln2π +

z + 1
2

ln z −z,
yielding all the terms of Eq. (13.60) that do not vanish in the limit of large |z|.
To help convey a feeling of the remarkable precision of Stirling’s series for 0(s + 1),
the ratio of the ﬁrst term of Stirling’s approximation to 0(s + 1) is plotted in Fig. 13.4. In
Table 13.1 we give the ratio of the ﬁrst term in the expansion to 0(s + 1) and a similar
ratio when two terms are kept in the expansion to 0(s + 1). The derivation of these forms
is Exercise 13.4.1.

13.4 Stirling’s Series
625
1.02
1.00
0.99
0.98
0.97
0.96
0.95
0.94
0.93
0.92
1
2
3
4
5
6
7
8
9
S
2π ss+1/2 e−s (1+
1
12s )
0.83% low
Γ(s+ 1)
2 π ss +1/2 e−s
Γ(s+ 1)
FIGURE 13.4
Accuracy of Stirling’s formula.
Table 13.1
Ratios of One- and Two-Term Stirling Series to Exact
Values of 0(s + 1)
s
1
0(s + 1)
√
2πss+1/2e−s
1
0(s + 1)
√
2πss+1/2e−s

1 +
1
12s

1
0.92213
0.99898
2
0.95950
0.99949
3
0.97270
0.99972
4
0.97942
0.99983
5
0.98349
0.99988
6
0.98621
0.99992
7
0.98817
0.99994
8
0.98964
0.99995
9
0.99078
0.99996
10
0.99170
0.99998
Exercises
13.4.1
Rewrite Stirling’s series to give 0(z + 1) instead of ln0(z + 1).
ANS.
0(z + 1) =
√
2π zz+1/2e−z

1 + 1
12z +
1
288z2 −
139
51,840z3 + ···

.
13.4.2
Use Stirling’s formula to estimate 52!, the number of possible rearrangements of cards
in a standard deck of playing cards.
13.4.3
Show that the constants C1 and C2 in Stirling’s formula have the respective values zero
and 1
2 ln2π by using the logarithm of the Legendre duplication formula (see Fig. 3.4).

626
Chapter 13 Gamma Function
13.4.4
Without using Stirling’s series show that
(a)
ln(n!) <
n+1
Z
1
ln xdx,
(b)
ln(n!) >
n
Z
1
ln xdx; n is an integer ≥2.
Note that the arithmetic mean of these two integrals gives a good approximation for
Stirling’s series.
13.4.5
Test for convergence
∞
X
p=0
"
0(p + 1
2)
p!
#2 2p + 1
2p + 2 = π
∞
X
p=0
(2p −1)!! (2p + 1)!!
(2p)!! (2p + 2)!!
.
This series arises in an attempt to describe the magnetic ﬁeld created by and enclosed
by a current loop.
13.4.6
Show that lim
x→∞xb−a 0(x + a + 1)
0(x + b + 1) = 1.
13.4.7
Show that lim
n→∞
(2n −1)!!
(2n)!!
n1/2 = π−1/2.
13.4.8
A set of N distinguishable particles is assigned to states ψi, i = 1, 2,..., M. If the
numbers of particles in the various states are n1, n2,...,nM (with M ≪N), the number
of ways this can be done is
W =
N!
n1!n2!···nM!.
The entropy associated with this assignment is S = k ln W, where k is Boltzmann’s
constant. In the limit N →∞, with ni = pi N (so pi is the fraction of the particles in
state i), ﬁnd S as a function of N and the pi.
(a)
In the limit of large N, ﬁnd the entropy associated with an arbitrary set of ni. Is
the entropy an extensive function of the system size (i.e., is it proportional to N)?
(b)
Find the set of pi that maximize S.
Hint. Remember that P
i pi = 1 and that this is a constrained maximization (see
Section 22.3).
Note. These formulas correspond to classical, or Boltzmann, statistics.
13.5
RIEMANN ZETA FUNCTION
We are now in a position to broaden our earlier survey of ζ(z), the Riemann zeta function.
In so doing, we note an interesting degree of parallelism between some of the properties of
ζ(z) and corresponding properties of the gamma function.

13.5 Riemann Zeta Function
627
We open this section by repeating the deﬁnition of ζ(z), which is valid when the series
converges:
ζ(z) ≡
∞
X
n=1
n−z.
(13.61)
The values of ζ(n) for integral n from 2 to 10 were listed in Table 1.1 on page 17.
We now want to consider the possibility of analytically continuing ζ(z) beyond the
range of convergence of Eq. (13.61). As a ﬁrst step toward doing so, we prove the integral
representation that was given in Table 1.1:
ζ(z) =
1
0(z)
∞
Z
0
tz−1 dt
et −1 .
(13.62)
Equation (13.62) has a range of validity that is limited by the behavior of its integrand at
small t; since the denominator then approaches t, the overall small-t dependence is tz−2.
Writing z = x + iy and tz−2 = tx−2eiy lnt, we see that, like Eq. (13.61), Eq. (13.62) will
only converge when ℜe z > 1.
We start from the right-hand side of Eq. (13.62), denoted I, by multiplying the numer-
ator and denominator of its integrand by e−t and expanding the denominator in powers of
e−t, reaching
I =
1
0(z)
∞
Z
0
tz−1e−t dt
1 −e−t
=
1
0(z)
∞
Z
0
∞
X
m=1
tz−1e−mt dt.
We next change the variable of integration for the individual terms so that all terms contain
an identical factor e−t:
I =
1
0(z)
∞
Z
0
∞
X
m=1
 t
m
z−1
e−t
dt
m

=
1
0(z)
 ∞
X
m=1
1
mz
! ∞
Z
0
tz−1e−t dt
= ζ(z)
1
0(z)
∞
Z
0
tz−1e−t dt = ζ(z).
(13.63)
In the second line of Eq. (13.63) we recognize the summation as a zeta function and the
integral as the Euler integral representation of 0(z), Eq. (13.5). It then cancels against the
initial factor 1/0(z), leaving the desired ﬁnal result, Eq. (13.62). In passing, we note that
the only difference between the integral of Eq. (13.62) and the Euler integral for the gamma
function is that we now have a denominator et −1 instead of simply et.
The next step toward the analytic continuation we seek is to introduce a contour integral
with the same integrand as Eq. (13.62), using the same open contour that was found useful
for the gamma function, shown in Fig. 13.2. Just as for the gamma function, we do not
wish to restrict z to integral values, so the integrand will in general have a branch point
at t = 0, and again we have placed the branch cut on the positive real axis. Restricting
consideration for now to z with ℜe z > 1, we evaluate the contour integral, denoted I, as
the sum of its contributions from the sections of the contour, respectively, labeled A, B,

628
Chapter 13 Gamma Function
and D in Fig. 13.2. For ℜe z > 1, the small circle D makes no contribution to the integral,
while
IA =
1
0(z)
ε
Z
∞
tz−1 dt
et −1 = −ζ(z),
IB =
1
0(z)
∞
Z
ε
tz−1e2πi(z−1) dt
et −1
= e2πi(z−1)ζ(z) = e2πizζ(z).
Combining the above, we get
I =
1
0(z)
Z
C
tz−1 dt
et −1 =

e2πiz −1

ζ(z).
(13.64)
Note that Eq. (13.64) is useful as a relation involving ζ(z) only if z is not an integer.
We now wish to deform the contour of Eq. (13.64) in a way that will remove the
restriction ℜe z > 1, which we originally needed to obtain that equation. The deforma-
tion corresponds to an analytic continuation of ζ(z) to a larger range of z, and will be
effective because the deformation can avoid the divergence in the neighborhood of t = 0.
When we consider possible deformations, we need to make the observation that, unlike
the gamma function, the integrand of Eq. (13.64) has simple poles at the points t = 2nπi,
n = ±1,±2,..., so that if we deform the contour in a way that encloses any of these poles,
we must allow for the change thereby produced in the value of the contour integral.
If we initially deform the contour by expanding the circle D to some ﬁnite radius less
than 2πi, we do not change the value of the integral I but extend its range of validity to
negative z. If, for z < 0, we further expand D until it becomes an open circle of inﬁnite
radius (but not through any of the poles), the value of the contour integral is reduced to
zero, with the change caused by the inclusion of the contribution from the poles that are
then encircled. We therefore have the interesting result that the original contour integral
had a value that was the negative of 2πi times the sum of the residues that were newly
enclosed. Thus,
I =

e2πiz −1

ζ(z) = −2πi
0(z)
∞
X
n=1
(residues of tz−1/(et −1) at t = ±2nπi).
At the pole t = +2πni, the residue is
 2nπeπi/2z−1, while at t = −2πni it is
 2nπe3πi/2z−1. Note that we must evaluate the residues taking cognizance of the branch
cut. Inserting these values and rearranging a bit,

e2πiz −1

ζ(z) = −
 ∞
X
n=1
1
n−z+1
!
(2π)zi
0(z)

eπi(z−1)/2 + e3πi(z−1)/2
= ζ(1 −z)(2π)z
0(z)

e3πiz/2 −eπiz/2
.
(13.65)
Note that because z < 0, the summation over n converges and can be identiﬁed as ζ(1−z).
Equation (13.65) can be simpliﬁed, but we already see its essential feature, namely that it

13.5 Riemann Zeta Function
629
provides a functional relation connecting ζ(z) and ζ(1 −z), parallel to but more compli-
cated than the reﬂection formula for the gamma function, Eq. (13.23). The derivation of
Eq. (13.65) was carried out for z < 0, but now that we have obtained it, we can, appeal-
ing to analytic continuation, assert its validity for all z such that its constituent factors are
nonsingular. This formula, in the simpliﬁed form we shall shortly obtain, was ﬁrst found
by Riemann.
The simpliﬁcation of Eq. (13.65) can be accomplished by recognizing, with the aid of
the gamma-function reﬂection formula, Eq. (13.23), that
e3πiz/2 −eπiz/2
e2πiz −1
= sin(πz/2)
sinπz
=
0(z)0(1 −z)
0(z/2)0(1 −z/2),
so
ζ(z) = ζ(1 −z)
πz 2z 0(1 −z)
0(z/2)0(1 −z/2) = ζ(1 −z) πz−1/20((1 −z)/2)
0(z/2)
,
(13.66)
where the ﬁnal member of Eq. (13.66) was obtained by using the duplication formula,
Eq. (13.27), with the value of z in the duplication formula set to the present −z/2. Equa-
tion (13.66) can now be rearranged to the more symmetrical form
0
 z
2

π−z/2ζ(z) = 0
1 −z
2

π−(1−z)/2ζ(1 −z) .
(13.67)
Equation (13.67), the zeta-function reﬂection formula, enables generation of ζ(z) in the
half-plane ℜe z < 0 from values in the region ℜe z > 1, where the series deﬁnition con-
verges.
It is possible to show that ζ(z) has no zeros in the region where the series deﬁ-
nition converges, and, from Eq. (13.67), this implies that ζ(z) is also nonzero for all
z in the half-plane ℜe z < 0 except at points where 0(z/2) is singular, namely z =
−2,−4,...,−2n,.... 0(z/2) is also singular at z = 0 but, as we shall see shortly, the
singularity at ζ(1) compensates the singularity at 0(0), with the result that ζ(0) is nonzero.
The zeros of ζ(z) at the negative even integers are called its trivial zeros, as they arise
from the singularities of the gamma function. Any other zeros of ζ(z) (and there are an
inﬁnite number of them) must lie in the region 0 ≤ℜe z ≤1, which has been called the
critical strip of the Riemann zeta function.
To obtain values of ζ(z) in the critical strip, we proceed by analytically continuing
toward ℜe z = 0 the formula from Eq. (12.62) that deﬁnes the Dirichlet series η(z) (clearly
valid for ℜe z > 1),
ζ(z) =
η(z)
1 −21−z =
1
1 −21−z
∞
X
n=1
(−1)n−1
nz
.
(13.68)
This alternating series converges for all ℜe z > 0, thereby providing a formula for ζ(z)
throughout the critical strip, but it is best used where the convergence is relatively rapid,
namely for ℜe z ≥1
2. Values of ζ(z) for ℜe z < 1
2 may be more conveniently obtained
from those for ℜe z ≥1
2 using the reﬂection formula, Eq. (13.67).

630
Chapter 13 Gamma Function
Equation (13.68) may be used to verify that the singularity of ζ(z) at z = 1 is a simple
pole and to ﬁnd its residue. We proceed as follows:
(Residue at z = 1) = lim
z→1(z −1)ζ(z) = lim
z→1

z −1
1 −21−z
 ∞
X
n=1
(−1)n−1
n
=
 1
ln2

(ln2) = 1,
(13.69)
where we used l’Hôpital’s rule, recognized that d 21−z/dz = −21−z ln2, and identiﬁed the
summation as that of Eq. (1.53). Returning now to Eq. (13.67), noting that
lim
z→0
ζ(1 −z)
0(z/2) = −residue of ζ(s) at s = 1
2(residue of 0(s) at s = 0) = −1
2,
we obtain the nonzero result
ζ(0) = 0(1/2)π−1/2

−1
2

= −1
2.
(13.70)
In addition to the practical utility we have already noted for the Riemann zeta function,
it plays a major role in current developments in analytic number theory. A starting point
for such investigations is the celebrated Euler prime number product formula, which can
be developed by forming
ζ(s)(1 −2−s) = 1 + 1
2s + 1
3s + ··· −
 1
2s + 1
4s + 1
6s + ···

,
(13.71)
eliminating all the n−s, where n is a multiple of 2. Then we write
ζ(s)(1 −2−s)(1 −3−s) = 1 + 1
3s + 1
5s + 1
7s + 1
9s + ···
−
 1
3s + 1
9s + 1
15s + ···

,
eliminating all the remaining terms in which n is a multiple of 3. Continuing, we have
ζ(s)(1 −2−s)(1 −3−s)(1 −5−s)···(1 −P−s), where P is a prime number, and all terms
n−s, in which n is a multiple of any integer up through P, are canceled out. In the limit
P →∞, we reach
ζ(s)(1 −2−s)(1 −3−s)···(1 −P−s) −→ζ(s)
∞
Y
P(prime)=2
(1 −P−s) = 1.
Therefore
ζ(s) =
∞
Y
P(prime)=2
(1 −P−s)−1,
(13.72)

13.5 Riemann Zeta Function
631
giving ζ(s) as an inﬁnite product.4 Incidentally, the cancellation procedure in the above
derivation has a clear application in numerical computation. For example, Eq. (13.71) will
give ζ(s)(1 −2−s) to the same accuracy as Eq. (13.61) gives ζ(s), but with only half as
many terms.
The asymptotic distribution of prime numbers can be related to the poles of ζ ′/ζ, and
in particular to the nontrivial zeros of the zeta function. Riemann conjectured that all the
nontrivial zeros were on the critical line ℜe z = 1
2, and there are potentially important
results that can be proved if Riemann’s conjecture is correct. Numerical work has veriﬁed
that the ﬁrst 300 × 109 nontrivial zeros of ζ(z) are simple and indeed fall on the critical
line. See J. Van de Lune, H. J. J. Te Riele, and D. T. Winter, “On the zeros of the Riemann
zeta function in the critical strip. IV,” Math. Comput. 47, 667 (1986).
Although many gifted mathematicians have attempted to establish what has come to
be known as the Riemann hypothesis, it has for about 150 years remained unproven
and is considered one of the premier unsolved problems in modern mathematics. Pop-
ular accounts of this fascinating problem can be found in M. du Santoy, The Music of
the Primes: Searching to Solve the Greatest Mystery in Mathematics, New York: Harper-
Collins (2003); J. Derbyshire, Prime Obsession: Bernhard Riemann and the Greatest Un-
solved Problem in Mathematics, Washington, DC: Joseph Henry Press (2003); and K. Sab-
bagh, The Riemann Hypothesis: The Greatest Unsolved Problem in Mathematics, New
York: Farrar, Straus and Giroux (2003).
Exercises
13.5.1
Show that the symmetrical functional relation
0
 z
2

π−z/2ζ(z) = 0
1 −z
2

π−(1−z)/2ζ(1 −z)
follows from the equation

e2πiz −1

ζ(z) = ζ(1 −z)(2π)z
0(z)

e3πiz/2 −eπiz/2
.
13.5.2
Prove that
∞
Z
0
xnexdx
(ex −1)2 = n!ζ(n).
Assuming n to be real, show that each side of the equation diverges if n = 1. Hence
the preceding equation carries the condition n > 1. Integrals such as this appear in the
quantum theory of transport effects: thermal and electrical conductivity.
4For further discussion, the reader is referred to the works by Edwards, Ivíc, Patterson, and Titchmarsh in Additional Readings.

632
Chapter 13 Gamma Function
13.5.3
The Bloch-Grüneisen approximation for the resistance in a monovalent metal at abso-
lute temperature T is
ρ = C T 5
26
2/T
Z
0
x5dx
(ex −1)(1 −e−x),
where 2 is the Debye temperature characteristic of the metal.
(a)
For T →∞, show that
ρ ≈C
4 · T
22 .
(b)
For T →0, show that
ρ ≈5!ζ(5)C T 5
26 .
13.5.4
Derive the following expansion of the Debye function for n ≥1:
x
Z
0
tndt
et −1 = xn
"
1
n −
x
2(n + 1) +
∞
X
k=1
B2kx2k
(2k + n)(2k)!
#
, |x| < 2π.
The complete integral (0,∞) equals n!ζ(n + 1) (Exercise 13.5.6).
13.5.5
The total energy radiated by a blackbody is given by
u = 8πk4T 4
c3h3
∞
Z
0
x3
ex −1 dx.
Show that the integral in this expression is equal to 3!ζ(4). The ﬁnal result is the Stefan-
Boltzmann law.
13.5.6
As a generalization of the result in Exercise 13.5.5, show that
∞
Z
0
xsdx
ex −1 = s!ζ(s + 1),
ℜe(s) > 0.
13.5.7
Prove that
∞
Z
0
xsdx
ex + 1 = s!(1 −2−s)ζ(s + 1),
ℜe(s) > 0.
Exercises 13.5.6 and 13.5.7 give the Mellin integral transform of 1/(ex ± 1); this trans-
form is deﬁned in Eq. (20.9).

13.6 Other Related Functions
633
13.5.8
The neutrino energy density (Fermi distribution) in the early history of the universe is
given by
ρν = 4π
h3
∞
Z
0
x3
exp(x/kT ) + 1 dx.
Show that
ρν = 7π5
30h3 (kT )4.
13.5.9
Prove that
ψ(n)(z) = (−1)n+1
∞
Z
0
tne−zt
1 −e−t dt,
ℜe(z) > 0.
13.5.10
Show that ζ(s) is analytic in the entire ﬁnite complex plane except at s = 1, where it
has a simple pole with a residue of +1.
Hint. The contour integral representation will be useful.
13.6
OTHER RELATED FUNCTIONS
Incomplete Gamma Functions
Generalizing the Euler-integral deﬁnition of the gamma function, Eq. (13.5), we deﬁne
incomplete gamma functions by the variable-limit integrals
γ (a, x) =
x
Z
0
e−tta−1 dt,
ℜ(a) > 0,
(13.73)
0(a, x) =
∞
Z
x
e−tta−1 dt.
Clearly, these two functions are related, for
γ (a, x) + 0(a, x) = 0(a).
(13.74)
The choice of employing γ (a, x) or 0(a, x) is purely a matter of convenience. If the
parameter a is a positive integer, Eqs. (13.73) may be integrated completely to yield
γ (n, x) = (n −1)!
 
1 −e−x
n−1
X
s=0
xs
s!
!
,
(13.75)
0(n, x) = (n −1)!e−x
n−1
X
s=0
xs
s! .

634
Chapter 13 Gamma Function
While the above expressions are valid only for positive integer n, the function 0(n, x) is
well deﬁned (providing x > 0) for n = 0 and corresponds to an exponential integral (see
later subsection).
For nonintegral a, a power-series expansion of γ (a, x) for small x and an asymptotic
expansion of 0(a, x) are developed in Exercises 1.3.3 and 13.6.4:
γ (a, x) = xa
∞
X
n=0
(−1)n
xn
n!(a + n),
small x,
0(a, x) ∼xa−1e−x
∞
X
n=0
0(a)
0(a −n) · 1
xn
(13.76)
∼xa−1e−x
∞
X
n=0
(a −n)n
1
xn ,
large x,
where (a −n)n is a Pochhammer symbol. The ﬁnal expression in Eq. (13.76) makes it
clear how to obtain an asymptotic expansion for 0(0, x). Noting that (−n)n = (−1)n n!,
we have
0(0, x) ∼e−x
x
∞
X
n=0
(−1)n n!
xn .
(13.77)
These incomplete gamma functions may also be expressed quite elegantly in terms of
conﬂuent hypergeometric functions (compare Section 18.6).
Incomplete Beta Function
Just as there are incomplete gamma functions, there is also an incomplete beta function,
customarily deﬁned for 0 ≤x ≤1, p > 0 (and, if x = 1, also q > 0) as
Bx(p,q) =
x
Z
0
t p−1(1 −t)q−1 dt.
(13.78)
Clearly, Bx=1(p,q) becomes the regular (complete) beta function, Eq. (13.49). A power-
series expansion of Bx(p,q) is the subject of Exercise 13.6.5. The relation to hypergeo-
metric functions appears in Section 18.5.
The incomplete beta function makes an appearance in probability theory in calculating
the probability of at most k successes in n independent trials.5
Exponential Integral
Although the incomplete gamma function 0(a, x) in its general form, Eq. (13.73), is only
infrequently encountered in physical problems, a special case is quite common and very
5W. Feller, An Introduction to Probability Theory and Its Applications, 3rd ed. New York: Wiley (1968), Section VI.10.

13.6 Other Related Functions
635
E1(x)
3
2
1
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
x
FIGURE 13.5
The exponential integral, E1(x) = −Ei(−x).
useful. We deﬁne the exponential integral by6
−Ei(−x) ≡
∞
Z
x
e−t
t
dt ≡E1(x).
(13.79)
For a graph of this function, see Fig. 13.5. To obtain a series expansion of E1(x) for
small x, we will need to proceed with caution, because the integral in Eq. (13.78) diverges
logarithmically as x →0. We start from
E1(x) = 0(0, x) = lim
a→0
h
0(a) −γ (a, x)
i
.
(13.80)
Setting a = 0 in the convergent terms (those with n ≥1) in the expansion of γ (a, x) and
moving them outside the scope of the limiting process, we rearrange Eq. (13.80) to
E1(x) = lim
a→0
a0(a) −xa
a

−
∞
X
n=1
(−1)nxn
n · n!
.
(13.81)
Using l’Hôpital’s rule, Eq. (1.58), writing a0(a) = 0(a + 1), and noting that d xa/da =
xa ln x, the limit in Eq. (13.81) reduces to
h d
da 0(a + 1) −d
da xai
a=1 = 0(1)ψ(1) −ln x = −γ −ln x,
(13.82)
where γ (without arguments) is the Euler-Mascheroni constant.7 From Eqs. (13.81) and
(13.82) we obtain the rapidly converging series
E1(x) = −γ −ln x −
∞
X
n=1
(−1)nxn
n · n!
.
(13.83)
6The appearance of the two minus signs in −Ei(−x) is a historical monstrosity. AMS-55, chapter 5, denotes this integral as
E1(x). See Additional Readings for the reference.
7Having the notations γ (a, x) and γ in the same discussion and with different meanings may seem unfortunate, but these are
the traditional notations and should not lead to confusion if the reader is alert.

636
Chapter 13 Gamma Function
1.0
10
Ci(x)
si(x)
x
−1.0
FIGURE 13.6
Sine and cosine integrals.
The asymptotic expansion for E1(x) is simply that given in Eq. (13.77) for 0(0, x). We
repeat it here:
E1(x) ∼e−x
1
x −1!
x2 + 2!
x3 −3!
x4 + ···

.
(13.84)
Further special forms related to the exponential integral are the sine integral, cosine
integral (for both see Fig. 13.6), and the logarithmic integral, deﬁned by8
si(x) = −
∞
Z
x
sint
t
dt,
Ci(x) = −
∞
Z
x
cost
t
dt,
(13.85)
li(x) =
x
Z
0
dt
lnt = Ei(ln x).
Viewed as functions of a complex variable, Ci(z) and li(z) are multivalued, with a branch
cut conventionally chosen to be along the negative real axis from the branch point at z = 0.
By transforming from real to imaginary argument, we can show that
si(x) = 1
2i
h
Ei(ix) −Ei(−ix)
i
= 1
2i
h
E1(ix) −E1(−ix)
i
,
(13.86)
whereas
Ci(x) = 1
2
h
Ei(ix) + Ei(−ix)
i
= −1
2
h
E1(ix) + E1(−ix)
i
,
|arg x| < π
2 .
(13.87)
Adding these two relations, we obtain
Ei(ix) = Ci(x) + isi(x),
(13.88)
8Another sine integral is denoted Si(x)= si(x) + π/2.

13.6 Other Related Functions
637
showing that the relation among these integrals is exactly analogous to that among eix,
cos x, and sin x. In terms of E1,
E1(ix) = −Ci(x) + i si(x).
(13.89)
Asymptotic expansions of Ci(x) and si(x) were developed in Section 12.6, with explicit
formulas in Eqs. (12.93) and (12.94). Power-series expansions about the origin for Ci(x),
si(x), and li(x) may be obtained from those for the exponential integral, E1(x), or by direct
integration, Exercise 13.6.13. The exponential, sine, and cosine integrals are tabulated in
AMS-55, chapter 5 (see Additional Readings for the reference), and can also be accessed
by symbolic software such as Mathematica, Maple, Mathcad, and Reduce.
Error Function
The error function erf(z) and the complementary error function erfc(z) are deﬁned by
the integrals
erf z =
2
√π
z
Z
0
e−t2 dt,
erfc z = 1 −erf z =
2
√π
∞
Z
z
e−t2 dt.
(13.90)
The factors 2/√π cause these functions to be scaled so that erf ∞= 1. For a plot of erf x,
see Fig. 13.7.
The power-series expansion of erf x follows directly from the expansion of the expo-
nential in the integrand:
erf x =
2
√π
∞
X
n=0
(−1)n x2n+1
(2n + 1)n! .
(13.91)
Its asymptotic expansion, the subject of Exercise 12.6.3, is
erf x ≈1 −e−x2
√π x

1 −
1
2x2 + 1 · 3
22x4 −1 · 3 · 5
23x6
+ ··· + (−1)n (2n −1)!!
2nx2n

.
(13.92)
x
erf x
−2
−1
−1
1
2
1
FIGURE 13.7
Error function, erf x.

638
Chapter 13 Gamma Function
From the general form of the integrands and Eq. (13.6) we expect that erf z and erfc z may
be written as incomplete gamma functions with a = 1
2. The relations are
erf z = π−1/2γ ( 1
2, z2),
erfc z = π−1/20( 1
2, z2).
(13.93)
Exercises
13.6.1
Show that γ (a, x) = e−x
∞
X
n=0
(a −1)!
(a + n)! xa+n
(a)
by repeatedly integrating by parts,
(b)
by transforming it into Eq. (13.76).
13.6.2
Show that
(a)
dm
dxm [x−aγ (a, x)] = (−1)mx−a−mγ (a + m, x),
(b)
dm
dxm [exγ (a, x)] = ex
0(a)
0(a −m)γ (a −m, x).
13.6.3
Show that γ (a, x) and 0(a, x) satisfy the recurrence relations
(a)
γ (a + 1, x) = a γ (a, x) −xae−x,
(b)
0(a + 1, x) = a 0(a, x) + xae−x.
13.6.4
Show that the asymptotic expansion (for large x) of the incomplete gamma function
0(a, x) has the form
0(a, x) ∼xa−1e−x
∞
X
n=0
0(a)
0(a −n) · 1
xn ,
and that the above expression is equivalent to
0(a, x) ∼xa−1e−x
∞
X
n=0
(a −n)n
1
xn .
13.6.5
A series expansion of the incomplete beta function yields
Bx(p,q) = x p
 1
p + 1 −q
p + 1x + (1 −q)(2 −q)
2!(p + 2)
x2 + ···
+ (1 −q)(2 −q)···(n −q)
n!(p + n)
xn + ···

.

13.6 Other Related Functions
639
Given that 0 ≤x ≤1, p > 0, and q > 0, test this series for convergence. What happens
at x = 1?
13.6.6
Using the deﬁnitions of the various functions, show that
(a)
si(x) = 1
2i [E1(ix) −E1(−ix)],
(b)
Ci(x) = −1
2[E1(ix) + E1(−ix)],
(c)
E1(ix) = −Ci(x) + i si(x).
13.6.7
The potential produced by a 1s hydrogen electron is given by
V (r) =
q
4πε0a0
 1
2r γ (3,2r) + 0(2,2r)

.
(a)
For r ≪1, show that
V (r) =
q
4πε0a0

1 −2
3r2 + ···

.
(b)
For r ≫1, show that
V (r) =
q
4πε0a0
· 1
r .
Here r is expressed in units of a0, the Bohr radius.
Note. V (r) is illustrated in Fig. 13.8.
Distributed
charge
potential
Point charge potential
1/r
r
FIGURE 13.8
Distributed charge potential produced by a 1s hydrogen electron,
Exercise 13.6.7.

640
Chapter 13 Gamma Function
13.6.8
The potential produced by a 2p hydrogen electron can be shown to be
V (r) =
1
4πε0
·
q
24a0
1
r γ (5,r) + 0(4,r)

−
1
4πε0
·
q
120a0
 1
r3 γ (7,r) + r20(2,r)

P2(cosθ).
Here r is expressed in units of a0, the Bohr radius. P2(cosθ) is a Legendre polynomial
(Section 15.1).
(a)
For r ≪1, show that
V (r) =
1
4πε0
· q
a0
1
4 −
1
120r2P2(cosθ) + ···

.
(b)
For r ≫1, show that
V (r) =
1
4πε0
· q
a0r

1 −6
r2 P2(cosθ) + ···

.
13.6.9
Prove that the exponential integral has the expansion
∞
Z
x
e−t
t
dt = −γ −ln x −
∞
X
n=1
(−1)nxn
n · n!
,
where γ is the Euler-Mascheroni constant.
13.6.10
Show that E1(z) may be written as
E1(z) = e−z
∞
Z
0
e−zt
1 + t dt.
Show also that we must impose the condition |arg z| ≤π/2.
13.6.11
Related to the exponential integral by a simple change of variable is the function
En(x) =
∞
Z
1
e−xt
tn
dt.
Show that En(x) satisﬁes the recurrence relation
En+1(x) = 1
n e−x −x
n En(x),
n = 1,2,3,··· .
13.6.12
With En(x) as deﬁned in Exercise 13.6.11, show that for n > 1,
En(0) = 1/(n −1).
13.6.13
Develop the following power-series expansions:

13.6 Other Related Functions
641
(a)
si(x) = −π
2 +
∞
X
n=0
(−1)nx2n+1
(2n + 1)(2n + 1)!,
(b)
Ci(x) = γ + ln x +
∞
X
n=1
(−1)nx2n
2n(2n)! .
13.6.14
An analysis of a center-fed linear antenna leads to the expression
x
Z
0
1 −cost
t
dt.
Show that this is equal to γ + ln x −Ci(x).
13.6.15
Using the relation
0(a) = γ (a, x) + 0(a, x),
show that if γ (a, x) satisﬁes the relations of Exercise 13.6.2, then 0(a, x) must satisfy
the same relations.
13.6.16
For x > 0, show that
∞
Z
x
tndt
et −1 =
∞
X
k=1
e−kx
xn
k + nxn−1
k2
+ n(n −1)xn−2
k3
+ ··· +
n!
kn+1

.
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974). Contains a wealth of information about gamma functions, incomplete gamma functions, exponential
integrals, error functions, and related functions in chapters 4 to 6.
Artin, E., The Gamma Function (translated by M. Butler). New York: Holt, Rinehart and Winston (1964). Demon-
strates that if a function f (x) is smooth (log convex) and equal to (n −1)! when x = n = integer, it is the
gamma function.
Davis, H. T., Tables of the Higher Mathematical Functions. Bloomington, IN: Principia Press (1933). Volume I
contains extensive information on the gamma function and the polygamma functions.
Edwards, H. M., Riemann’s Zeta Function. New York: Academic Press (1974) and Dover (2003).
Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series, and Products. New York: Academic Press (1980).
Ivi´c, A., The Riemann Zeta Function. New York: Wiley (1985).
Luke, Y. L., The Special Functions and Their Approximations, Vol. 1. New York: Academic Press (1969).
Luke, Y. L., Mathematical Functions and Their Approximations. New York: Academic Press (1975). This is
an updated supplement to Handbook of Mathematical Functions with Formulas, Graphs, and Mathematical
Tables (AMS-55). Chapter 1 deals with the gamma function. Chapter 4 treats the incomplete gamma function
and a host of related functions.
Patterson, S. J., Introduction to the Theory of the Reimann Zeta Function. Cambridge: Cambridge University
Press (1988).
Titchmarsh, E. C., and D. R. Heath-Brown, The Theory of the Riemann Zeta-Function. Oxford: Clarendon Press
(1986). A detailed, classic work.

CHAPTER 14
BESSEL FUNCTIONS
Bessel functions appear in a wide variety of physical problems. In Section 9.4 we saw
that separation of the Helmholtz, or wave, equation in circular cylindrical coordinates led
to Bessel’s equation in the coordinate describing distance from the axis of the cylindrical
system. In that same section, we also identiﬁed spherical Bessel functions (closely related
to Bessel functions of half-integral order) in Helmholtz equations in spherical coordinates.
In summarizing the forms of solutions to partial differential equations (PDEs) in these
coordinate systems, we not only identiﬁed the original and spherical Bessel functions,
but also those of imaginary argument (usually expressed as modiﬁed Bessel functions to
avoid the explicit use of imaginary quantities). Since these PDEs can describe many types
of problems ranging from stationary problems in quantum mechanics to those of spherical
or cylindrical wave propagation, a good familiarity with Bessel functions is important to
the practicing physicist.
Often problems in physics involve integrals that can be identiﬁed as Bessel functions,
even when the original problem did not explicitly involve cylindrical or spherical geom-
etry. Moreover, Bessel and closely related functions form a rich area of mathematical
analysis with many representations, many interesting and useful properties, and many
interrelations. Some of the major interrelations are developed in the present chapter.
In addition to the material presented here, we call attention to further relations in terms
of conﬂuent hypergeometric functions; see Section 18.6.
14.1
BESSEL FUNCTIONS OF THE FIRST KIND, Jν(x)
Bessel functions of the ﬁrst kind, normally labeled Jν, are those obtained by the Frobenius
method for solution of the Bessel ODE,
x2J ′′
ν + x J ′
ν + (x2 −ν2)Jν = 0.
(14.1)
The term “ﬁrst kind” reﬂects the fact that Jν(x) includes the functions that, for non-
negative integer ν, are regular at x = 0. All solutions to the Bessel ordinary differential
643
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

644
Chapter 14 Bessel Functions
equation (ODE) that are linearly independent of Jν(x) are irregular at x = 0 for all ν;
a speciﬁc choice for a second solution is denoted Yν(x) and is called a Bessel function of
the second kind.1
Generating Function for Integral Order
We start our detailed study of Bessel functions by introducing a generating function yield-
ing the Jn for integer n (of either sign). Because the Jn are not polynomials, the generating
function cannot be found by the methods of Section 12.1, but we will be able to show that
the functions deﬁned by the generating function are indeed the solutions of the Bessel ODE
obtained by the Frobenius method.
Our generating function formula, a Laurent series, is
g(x,t) = e(x/2)(t−1/t) =
∞
X
n=−∞
Jn(x)tn.
(14.2)
Although the Bessel ODE is homogeneous and its solutions are of arbitrary scale,
Eq. (14.2) ﬁxes a speciﬁc scale for Jn(x). To relate Eq. (14.2) to the Frobenius solution,
Eq. (7.48), we manipulate the exponential as follows:
g(x,t) = ext/2 · e−x/2t =
∞
X
r=0
x
2
r tr
r!
∞
X
s=0
(−1)sx
2
s t−s
s!
=
∞
X
r=0
∞
X
s=0
(−1)s x
2
r+s tr−s
r!s! .
We now change the summation index r to n = r −s, yielding
g(x,t) =
∞
X
n=−∞
"X
s
(−1)s
(n + s)!s!
x
2
n+2s
#
tn,
(14.3)
where the s summation starts at max(0,−n). For n ≥0, the coefﬁcient of tn is seen to be
Jn(x) =
∞
X
s=0
(−1)s
s!(n + s)!
x
2
n+2s
.
(14.4)
Comparing with Eq. (7.48), we conﬁrm that for n ≥0, Jn as given by Eq. (14.4) is the
Frobenius solution, at the speciﬁc scale given here.
If now we replace n by −n, the summation in Eq. (14.3) becomes
J−n(x) =
∞
X
s=n
(−1)s
s!(s −n)!
x
2
−n+2s
;
1We use the notation of AMS-55, also used by Watson in his deﬁnitive treatise (for both sources, see Additional Readings). The
Yν are sometimes also called Neumann functions; for that reason some workers write them as Nν. They were denoted Nν in
previous editions of this book.

14.1 Bessel Functions of the First Kind, Jν(x)
645
changing s to s + n, we reach
J−n(x) =
∞
X
s=0
(−1)s+n
s!(s + n)!
x
2
n+2s
= (−1)n Jn(x)
(integral n),
(14.5)
conﬁrming both that J−n(x) is a solution to the Bessel ODE and that it is linearly dependent
on Jn.
If we now consider Jν with ν nonintegral, we get no information from the generating
function, but the Frobenius method then gives linearly independent solutions for both +ν
and −ν, which are both solutions of the Bessel ODE, Eq. (14.1), for the same value of ν2.
Looking at the details of the development of Eqs. (7.46) to (7.48), we see that the generali-
zation of Eq. (14.4) to noninteger ν is
Jν(x) =
∞
X
s=0
(−1)s
s!0(ν + s + 1)
x
2
ν+2s
,
(ν ̸= −1,−2,...),
(14.6)
and that Jν(x) as given in Eq. (14.6) is a solution to the Bessel ODE.
For ν ≥0 the series of Eq. (14.6) is convergent for all x, and for small x is a practi-
cal way to evaluate Jν(x). Graphs of J0, J1, and J2 are shown in Fig. 14.1. The Bessel
functions oscillate but are not periodic, except in the limit x →∞, with the amplitude of
the oscillation decreasing asymptotically as x−1/2. This behavior is discussed further in
Section 14.6.
Recurrence Relations
The Bessel functions Jn(x) satisfy recurrence relations connecting functions of contigu-
ous n, as well as some connecting the derivative J ′
n to various Jn. Such recurrence rela-
tions may all be obtained by operating on the series, Eq. (14.6), although this requires a bit
of clairvoyance (or a lot of trial and error). However, if the recurrence relations are already
known, their veriﬁcation is straightforward; see Exercise 14.1.8. Our approach here will
be to obtain them from the generating function g(x,t), using a process similar to that
illustrated in Example 12.1.2.
x
1.0
J0(x)
J1(x)
J2(x)
1
2
3
6
7
J
0
8
9
5
4
FIGURE 14.1
Bessel functions J0(x), J1(x), and J2(x).

646
Chapter 14 Bessel Functions
We start by differentiating g(x,t):
∂
∂t g(x,t) = x
2

1 + 1
t2

e(x/2)(t−1/t) =
∞
X
n=−∞
nJn(x)tn−1,
∂
∂x g(x,t) = 1
2

t −1
t

e(x/2)(t−1/t) =
∞
X
n=−∞
J ′
n(x)tn.
Inserting the right-hand side of Eq. (14.2) in place of the exponentials and equating the
coefﬁcients of equal powers of t (as illustrated in Example 12.1.2), we obtain the two
basic Bessel-function recurrence formulas:
Jn−1(x) + Jn+1(x) = 2n
x Jn(x),
(14.7)
Jn−1(x) −Jn+1(x) = 2J ′
n(x).
(14.8)
Because Eq. (14.7) is a three-term recurrence relation, its use to generate Jn will require
two starting values. For example, given J0 and J1, then J2 (and any other integral order Jn
including those for n < 0) may be computed.
An important special case of Eq. (14.8) is
J ′
0(x) = −J1(x).
(14.9)
Equations (14.7) and (14.8) can also be combined (Exercise 14.1.4) to form the useful
additional formulas
d
dx

xn Jn(x)

= xn Jn−1(x),
(14.10)
d
dx

x−n Jn(x)

= −x−n Jn+1(x),
(14.11)
Jn(x) = ±J ′
n±1 + n ± 1
x
Jn±1(x).
(14.12)
Bessel’s Differential Equation
Suppose we consider a set of functions Zν(x) that satisﬁes the basic recurrence relations,
Eqs. (14.7) and (14.8), but with ν not necessarily an integer and Zν not necessarily given by
the series in Eq. (14.6). It is our objective to show that any functions that satisfy these recur-
rence relations must also be solutions to Bessel’s ODE. We start by forming (1) x2Zν′′(x)
from x2/2 times the derivative of Eq. (14.8), (2) x Z′
ν(x) from Eq. (14.8) multiplied by
x/2, and (3) ν2Zν(x) from Eq. (14.7) multiplied by νx/2. Putting these together we obtain
x2Zν′′(x) + x Z′
ν(x) −ν2Zν(x)
= x2
2

Z′
ν−1(x) −Z′
ν+1(x) −ν −1
x
Zν−1(x) −ν + 1
x
Zν+1(x)

.
(14.13)

14.1 Bessel Functions of the First Kind, Jν(x)
647
The terms within square brackets in Eq. (14.13) can now by use of Eq. (14.12) be simpliﬁed
to −2Zν(x), so Eq. (14.13) can be rewritten
x2Zν′′(x) + x Z′
ν(x) + (x2 −ν2)Zν(x) = 0,
(14.14)
which is Bessel’s ODE. Reiterating, we have shown that any functions Zν(x) that satisfy
the basic recurrence formulas, Eqs. (14.7) and (14.8), also satisfy Bessel’s equation; that
is, the Zν are Bessel functions. For later use, we note that if the argument of Zν is kρ rather
than x, Eq. (14.14) becomes
ρ2 d2
dρ2 Zν(kρ) + ρ d
dρ Zν(kρ) + (k2ρ2 −ν2)Zν(kρ) = 0.
(14.15)
Integral Representation
It is of great value to have integral representations of Bessel functions. Starting from the
generating-function formula, we can apply the residue theorem to evaluate the contour
integral
I
C
e(x/2)(t+1/t)
tn+1
dt =
I
C
X
m
Jm(x)tm−n−1dt = 2πi Jn(x),
(14.16)
where the contour C encircles the singularity at t = 0. The integral on the left-hand side
of Eq. (14.16) can now be brought to a convenient form by taking the contour to be the
unit circle and changing the integration variable by making the substitution t = eiθ. Then
dt = ieiθdθ, e(x/2)(t−1/t) = eix sinθ, and we have
2πi Jn(x) =
2π
Z
0
eix sinθ
e(n+1)iθ ieiθdθ =
2π
Z
0
ei(x sinθ−nθ)idθ.
(14.17)
Assuming x to be real and taking the imaginary parts of both sides of Eq. (14.17), we ﬁnd
Jn(x) = 1
2π
2π
Z
0
cos(x sinθ −nθ)dθ = 1
π
π
Z
0
cos(x sinθ −nθ)dθ,
(14.18)
where the last equality only holds because we are assuming n to be an integer. Though we
will not need it now, the real part of this equation also gives an interesting formula:
2π
Z
0
sin(x sinθ −nθ)dθ = 0.
(14.19)
An oft-occurring special case of Eq. (14.18) is
J0(x) = 1
2π
2π
Z
0
eix cosθdθ = 1
π
π
Z
0
cos(x sinθ)dθ.
(14.20)

648
Chapter 14 Bessel Functions
Table 14.1
Zeros of the Bessel Functions and Their First Derivatives
Number
of zeros
J0(x)
J1(x)
J2(x)
J3(x)
J4(x)
J5(x)
1
2.4048
3.8317
5.1356
6.3802
7.5883
8.7715
2
5.5201
7.0156
8.4172
9.7610
11.0647
12.3386
3
8.6537
10.1735
11.6198
13.0152
14.3725
15.7002
4
11.7915
13.3237
14.7960
16.2235
17.6160
18.9801
5
14.9309
16.4706
17.9598
19.4094
20.8269
22.2178
J′
0(x)
J′
1(x)
J′
2(x)
J′
3(x)
J′
4(x)
J′
5(x)
1
3.8317
1.8412
3.0542
4.2012
5.3176
6.4156
2
7.0156
5.3314
6.7061
8.0152
9.2824
10.5199
3
10.1735
8.5363
9.9695
11.3459
12.6819
13.9872
4
13.3237
11.7060
13.1704
14.5858
15.9641
17.3128
5
16.4706
14.8636
16.3475
17.7887
19.1960
20.5755
Equation (14.18) is only one of many integral representations of Jn, and some of these
can be derived (using an appropriately modiﬁed contour) for Jν of a nonintegral order.
This topic is explored in the subsection below entitled “Bessel Functions of Nonintegral
Order”.
Zeros of Bessel Functions
In many physical problems in which phenomena are described by Bessel functions, we are
interested in the points where these functions (which have oscillatory character) are zero.
For example, in a problem involving standing waves, these zeros identify the positions of
the nodes. And in boundary value problems, we may need to choose the argument of our
Bessel function to put a zero at an appropriate point.
There are no closed formulas for the zeros of Bessel functions; they must be found by
numerical methods. Because the need for them arises frequently, tables of the zeros are
available, both in compilations such as AMS-55 (see Additional Readings) and at a variety
of sources online.2 Table 14.1 lists the ﬁrst few zeros of Jn(x) for integer n from n = 0
through n = 5, giving also the positions of the zeros of J ′
n.
Example 14.1.1
FRAUNHOFER DIFFRACTION, CIRCULAR APERTURE
In the theory of diffraction of radiation of wavelength λ, incident normal to a circular
aperture of radius a, we encounter the integral
8 ∼
a
Z
0
r dr
2π
Z
0
eibr cosθdθ,
(14.21)
2Additional roots of the Bessel functions and those of their ﬁrst derivatives may be found in C. L. Beattie, Table of ﬁrst 700 zeros
of Bessel functions, Bell Syst. Tech. J. 37, 689 (1958), and Bell Monogr. 3055. Roots may be also be accessed in Mathematica,
Maple, and other symbolic software.

14.1 Bessel Functions of the First Kind, Jν(x)
649
x
θ
α
y
r
Incident waves
FIGURE 14.2
Geometry for Fraunhofer diffraction, circular aperture.
where 8 is the amplitude of the diffracted wave and (r,θ) identiﬁes points in the aperture.
The exponent br cosθ is the phase of the radiation through (r,θ) that is diffracted to an
angle α from the incident direction, with
b = 2π
λ sinα.
(14.22)
The geometry is illustrated in Fig. 14.2. Fraunhofer diffraction, for which the above are
the relevant formulas, applies in the limit that the outgoing radiation is detected at large
distances from the aperture.
The behavior of the complex exponential will cause the amplitude to oscillate as α is
increased, creating (for each wavelength) a diffraction pattern. To understand the patterns
more fully, we need to evaluate the integral in Eq. (14.21). From Eq. (14.20) we may
immediately reduce Eq. (14.21) to
8 ∼2π
a
Z
0
J0(br)rdr,
(14.23)
which can be integrated in r using Eq. (14.10):
8 ∼2π
a
Z
0
1
b2
d
dr

(br)J1(br)

dr = 2π
b2

br J1(br)
 a
0 = 2πa
b
J1(ab),
(14.24)
where we have used the fact that J1(0) = 0. The intensity of the light in the diffraction

650
Chapter 14 Bessel Functions
0
10000
20000
30000
0.0002
Radians
0.0004
FIGURE 14.3
Amplitude of Fraunhofer diffraction vs. deﬂection angle (green light,
aperture of radius 0.5 cm).
pattern is proportional to 82 and, substituting for b from Eq. (14.22),
82 ∼
 J1[(2πa/λ)sinα]
sinα
2
.
(14.25)
For visible light and apertures of reasonable size, 2πa/λ is quite small: for green light
(λ = 5.5 × 10−5 cm) and an aperture with a = 0.5 cm, 2πa/λ = 57120, and these parame-
ter values lead to the pattern for 8 shown in Fig. 14.3. Note that the ﬁgure plots 8 (a plot
of 82 would make the oscillations too small to be observable on the same graph as the
maximum at α = 0). We see that 8 exhibits a central maximum at α = 0 of amplitude
∼30,000, with subsidiary extrema that by α = 0.001 radian have decreased in magnitude
to less than 1% of the central maximum. Remembering that the intensity is 82, we see that
the diffraction spreading of the incident light is exceedingly small. To make a quantitative
analysis of the diffraction pattern, we need to identify the positions of its minima. They
correspond to the zeros of J1; for example, from Table 14.1 we ﬁnd the ﬁrst minimum
to be where (2πa/λ)sinα = 3.8317, or α ≈14 seconds of arc. If this analysis had been
known in the 17th century, the arguments against the wave theory of light would have
collapsed.
In mid-20th century this same diffraction pattern appears in the scattering of nuclear
particles by atomic nuclei, a striking demonstration of the wave properties of the nuclear
particles.
■
Further examples of the use of Bessel functions and their roots are provided by the
following example and by the exercises of this section and Section 14.2.
Example 14.1.2
CYLINDRICAL RESONANT CAVITY
The propagation of electromagnetic waves in hollow metallic cylinders is important in
many practical devices. If the cylinder has end surfaces, it is called a cavity. Resonant
cavities play a crucial role in many particle accelerators.

14.1 Bessel Functions of the First Kind, Jν(x)
651
The resonant frequencies of a cavity are those of the oscillatory solutions to Maxwell’s
equations that correspond to standing wave patterns. By combining Maxwell’s equations,
we derived in Example 3.6.2 the vector Laplace equation for the electric ﬁeld E in a region
free of electric charges and currents. Taking the z-axis along the axis of the cavity, our
concern here is the equation for Ez, which from Eq. (3.71) we found to have the form
∇2Ez = −1
c2
∂2Ez
∂t2 ,
(14.26)
which has standing-wave solutions Ez(x, y, z,t) = Ez(x, y, z) f (t), where f (t) has real
solutions sinωt and cosωt, corresponding to sinusoidal oscillations at angular frequency ω.
We are implicitly assuming that our solution has a nonzero component Ez, and we will also
set Bz = 0, so we intend to obtain solutions that are usually called the TM (for ‘‘transverse
magnetic”) modes of oscillation. Additional solutions, with Ez = 0 and Bz nonzero, corre-
spond to TE (transverse electric) modes and are the subject of Exercise 14.1.25.
Thus, for the present problem, in which our cavity is that shown in Fig. 14.4, we seek
solutions to the spatial PDE:
∇2Ez + k2Ez = 0,
k = ω
c .
(14.27)
The aim of the present example is to ﬁnd the values of ω for which Eq. (14.27) has solutions
consistent with the boundary conditions at the cavity walls. Assuming the metallic walls
to be perfect conductors, the boundary conditions are that the tangential components of the
electric ﬁeld vanish there. Taking the cavity to have planar end caps at z = 0 and z = h, and
(in cylindrical coordinates ρ,ϕ) to be bounded by a curved surface at ρ = a, our boundary
conditions are Ex = Ey = 0 on the end caps, and Eϕ = Ez = 0 on the boundary at ρ = a.
x
y
z
h
a
FIGURE 14.4
Resonant cavity.

652
Chapter 14 Bessel Functions
Once a solution (with Bz = 0) has been found for Ez, then the remaining components of
B and E have deﬁnite values. For further details, see J. D. Jackson, Electrodynamics in
Additional Readings.
Equation (14.27) can be solved by the method of separation of variables, with solutions
of the form given in Eq. (9.64):
Ez(ρ,θ, z) = Plm(ρ)8m(ϕ)Zl(z),
(14.28)
with 8m(θ) = e±imϕ or its equivalent in terms of sines and cosines, while Zl(z) and
Plm(ρ) are solutions of the ODEs
d2Zl
dz2 = −l2Zl,
(14.29)
ρ d
dρ

ρ d Plm
dρ

+

(k2 −l2)ρ2 −m2
Plm = 0.
(14.30)
Equation (14.29) corresponds to Eq. (9.58), but with a different choice of the sign for
the separation constant in anticipation of the fact that Zl will turn out to be oscillatory.
This change causes n2 in Eq. (9.60) to become k2 −l2, and Eq. (14.30) is then seen to
correspond exactly with Eq. (9.63).
Recognizing now Eq. (14.30) as Bessel’s ODE and Eq. (14.29) as the ODE for a classical
harmonic oscillator, we ﬁnd, before imposing boundary conditions,
Ez = Jm(nρ)e±imϕ
A sinlz + B coslz

,
(14.31)
and the general solution will be an arbitrary linear combination of the above for different
values of n, m, and l. We have chosen the solution to Bessel’s ODE to be of the ﬁrst kind
to maintain regularity at ρ = 0, since this ρ value is inside the cavity. We have written the
ϕ dependence of the solution as a complex exponential for notational convenience. The
physically relevant solutions will be arbitrary mixtures of the corresponding real quanti-
ties, sinmϕ and cosmϕ. Continuity and single-valuedness in ϕ dictate that m have integer
values.
The condition that Ez = 0 on the curved boundary translates into the requirement
Jm(na) = 0. Letting αmj stand for the jth positive zero of Jm, we ﬁnd that
na = αmj,
or
k2 −l2 =
αmj
a
2
.
(14.32)
To complete the solution we need to identify the boundary condition on Z. Because
∂Ex/∂x = ∂Ey/∂y = 0 on the end caps, we have from the Maxwell equation for ∇· E:
∂Ex
∂x + ∂Ey
∂y + ∂Ez
∂z = 0
−→
∂Ez
∂z = 0,
(14.33)
so we have the requirement Z′(0) = Z′(h) = 0, and we must choose
Z = B coslz,
with
l = pπ
h ,
p = 0,1,2,....
(14.34)
Combining Eqs. (14.32) and (14.34), we ﬁnd
k2 =
αmj
a
2
+
 pπ
h
2
= ω2
c2 ,
(14.35)

14.1 Bessel Functions of the First Kind, Jν(x)
653
thereby providing an equation for the resonant frequencies:
ωmjp = c
s
α2
mj
a2 + p2π2
h2 ,



m = 0,1,2,...,
j = 1,2,3,...,
p = 0,1,2,....
(14.36)
Recapitulating, the functions we have found, labeled by the indices m, j, and p, are
the spatial parts of standing-wave solutions of TM character whose time dependence and
overall amplitude are of the form Ce±iωmjpt.
■
Bessel Functions of Nonintegral Order
While Jν of noninteger ν are not produced from a generating-function approach, they are
readily identiﬁed from the Taylor series expansion, and they are conventionally given a
scale consistent with that of the Jn of integer n. They then satisfy the same recurrence
relations as those derived from the generating function.
If ν is not an integer, there is actually an important simpliﬁcation. The functions Jν
and J−ν are then independent solutions of the same ODE, and a relation of the form of
Eq. (14.5) does not exist. On the other hand, for ν = n, an integer, we need another solution.
The development of this second solution and an investigation of its properties form the
subject of Section 14.3.
Schlaeﬂi Integral
It is useful to modify the integral representation, Eq. (14.16), so that it can be applied for
Bessel functions of nonintegral order. Our ﬁrst step in doing so is to deform the circular
contour by stretching it to inﬁnity on the negative real axis and opening the contour there,
as shown in Fig. 14.5. Our integral, written
Fν(x) =
1
2πi
Z
C
e(x/2)(t−1/t)
tν+1
dt,
(14.37)
−∞
(t)
(t)
FIGURE 14.5
Contour, Schlaeﬂi integral for Jν.

654
Chapter 14 Bessel Functions
now has a branch point at t = 0, and because we have opened the contour we can place the
branch cut along the negative real axis. We might anticipate that this procedure will not
affect our integral representation, as the integrand vanishes at t = −∞on both sides of the
cut. However, that remains to be proved.
Our ﬁrst step toward a proof that Fν is actually Jν is to verify that Fν still satisﬁes
Bessel’s ODE. If we substitute Fν and its x derivatives into the ODE, we can, after some
manipulation, reach the expression
1
2πi
Z
C
d
dt
(
e(x/2)(t−1/t)
tν

ν + x
2

t + 1
t
)
dt,
(14.38)
and because the integration is within a region of analyticity of the integrand, the integral
reduces to
(
e(x/2)(t−1/t)
tν

ν + x
2

t + 1
t
)
end
−
(
e(x/2)(t−1/t)
tν

ν + x
2

t + 1
t
)
start
.
We therefore conclude that the ODE is satisﬁed if the above expression vanishes; in our
present situation each of the quantities in braces is zero for large negative t and positive x,
conﬁrming that Fν satisﬁes Bessel’s ODE.
We still need to show that Fν is the solution designated Jν; to accomplish this we con-
sider its value for small x > 0. Deforming the contour to a large open circle and making a
change of variable to u = eiπ xt/2, we get (to lowest order in x)
Fν(x) ≈
1
2πi
x
2
ν
eiνπ
Z
C′
e−u
uν+1 du.
(14.39)
Because of the change of variable, the contour C′ becomes that which we introduced
when developing a Schlaeﬂi integral representation of the gamma function, and, using
Eq. (13.31), we reduce Eq. (14.39) to
Fν(x) ≈
x
2
ν sin[(ν + 1)π]0(−ν)
π
=
1
0(ν + 1)
x
2
ν
,
(14.40)
where the last step used the reﬂection formula for the gamma function, Eq. (13.23). Since
this is the leading term of the expansion for Jν, our proof is complete.
Exercises
14.1.1
From the product of the generating functions g(x,t)g(x,−t), show that
1 = [J0(x)]2 + 2[J1(x)]2 + 2[J2(x)]2 + ···
and therefore that |J0(x)| ≤1 and |Jn(x)| ≤1/
√
2,n = 1,2,3,... .
Hint. Use uniqueness of power series, (Section 1.2).
14.1.2
Using a generating function g(x,t) = g(u + v,t) = g(u,t)g(v,t), show that
(a)
Jn(u + v) = P∞
s=−∞Js(u)Jn−s(v),
(b)
J0(u + v) = J0(u)J0(v) + 2P∞
s=1 Js(u)J−s(v).
These are addition theorems for the Bessel functions.

14.1 Bessel Functions of the First Kind, Jν(x)
655
14.1.3
Using only the generating function
e(x/2)(t−1/t) =
∞
X
n=−∞
Jn(x)tn
and not the explicit series form of Jn(x), show that Jn(x) has odd or even parity accord-
ing to whether n is odd or even, that is,
Jn(x) = (−1)n Jn(−x).
14.1.4
Use the basic recurrence formulas, Eqs. (14.7) and (14.8), to prove the following
formulas:
(a)
d
dx [xn Jn(x)] = xn Jn−1(x),
(b)
d
dx [x−n Jn(x)] = −x−n Jn+1(x),
(c)
Jn(x) = J ′
n+1 + n+1
x Jn+1(x).
14.1.5
Derive the Jacobi-Anger expansion
eiρ cosϕ =
∞
X
m=−∞
im Jm(ρ)eimϕ.
This is an expansion of a plane wave in a series of cylindrical waves.
14.1.6
Show that
(a)
cos x = J0(x) + 2P∞
n=1(−1)n J2n(x),
(b)
sin x = 2P∞
n=0(−1)n J2n+1(x).
14.1.7
To help remove the generating function from the realm of magic, show that it can be
derived from the recurrence relation, Eq. (14.7).
Hint. (a) Assume a generating function of the form
g(x,t) =
∞
X
m=−∞
Jm(x)tm.
(b) Multiply Eq. (14.7) by tn and sum over n.
(c) Rewrite the preceding result as

t + 1
t

g(x,t) = 2t
x
∂g(x,t)
∂t
.
(d) Integrate and adjust the “constant” of integration (a function of x)
so that the coefﬁcient of the zeroth power, t0, is J0(x) as given by
Eq. (14.6).

656
Chapter 14 Bessel Functions
14.1.8
Show, by direct differentiation, that
Jν(x) =
∞
X
s=0
(−1)s
s!0(s + ν + 1)
x
2
ν+2s
satisﬁes the two recurrence relations
Jν−1(x) + Jν+1(x) = 2ν
x Jν(x),
Jν−1(x) −Jν+1(x) = 2J ′
ν(x),
and Bessel’s differential equation
x2J ′′
ν (x) + x J ′
ν(x) + (x2 −ν2)Jν(x) = 0.
14.1.9
Prove that
sin x
x
=
π/2
Z
0
J0(x cosθ)cosθ dθ,
1 −cos x
x
=
π/2
Z
0
J1(x cosθ) dθ.
Hint. The deﬁnite integral
π/2
Z
0
cos2s+1 θ dθ =
2 · 4 · 6···(2s)
1 · 3 · 5···(2s + 1)
may be useful.
14.1.10
Derive
Jn(x) = (−1)nxn
1
x
d
dx
n
J0(x).
Hint. Try mathematical induction (Section 1.4).
14.1.11
Show that between any two consecutive zeros of Jn(x) there is one and only one zero
of Jn+1(x).
Hint. Equations (14.10) and (14.11) may be useful.
14.1.12
An analysis of antenna radiation patterns for a system with a circular aperture involves
the equation
g(u) =
1
Z
0
f (r)J0(ur)rdr.
If f (r) = 1 −r2, show that
g(u) = 2
u2 J2(u).

14.1 Bessel Functions of the First Kind, Jν(x)
657
14.1.13
The differential cross section in a nuclear scattering experiment is given by dσ/d =
| f (θ)|2. An approximate treatment leads to
f (θ) = −ik
2π
2π
Z
0
R
Z
0
exp[ikρ sinθ sinϕ]ρ dρ dϕ.
Here θ is an angle through which the scattered particle is scattered. R is the nuclear
radius. Show that
dσ
d = (π R2) 1
π
 J1(kR sinθ)
sinθ
2
.
14.1.14
A set of functions Cn(x) satisﬁes the recurrence relations
Cn−1(x) −Cn+1(x) = 2n
x Cn(x),
Cn−1(x) + Cn+1(x) = 2C′
n(x).
(a)
What linear second-order ODE does the Cn(x) satisfy?
(b)
By a change of variable transform your ODE into Bessel’s equation. This sug-
gests that Cn(x) may be expressed in terms of Bessel functions of transformed
argument.
14.1.15
(a)
Show by direct differentiation and substitution that
Jν(x) =
1
2πi
Z
C
e(x/2)(t−1/t)t−ν−1dt
(this is the Schlaeﬂi integral representation of Jν), and that the equivalent equation,
Jν(x) =
1
2πi
x
2
ν Z
C
es−x2/4ss−ν−1ds,
both satisfy Bessel’s equation. C is the contour shown in Fig. 14.5. The negative
real axis is the cut line.
Hint. This exercise is aimed at providing details of the discussion that starts at
Eq. (14.38).
(b)
Show that the ﬁrst integral (with n an integer) may be transformed into
Jn(x) = 1
2π
2π
Z
0
ei(x sinθ−nθ)dθ = i−n
2π
2π
Z
0
ei(x cosθ+nθ)dθ.
14.1.16
The contour C in Exercise 14.1.15 is deformed to the path −∞to −1, unit circle e−iπ
to eiπ, and ﬁnally −1 to −∞. Show that
Jν(x) = 1
π
π
Z
0
cos(νθ −x sinθ)dθ −sinνπ
π
∞
Z
0
e−νθ−x sinhθdθ.
This is Bessel’s integral.

658
Chapter 14 Bessel Functions
Hint. The negative values of the variable of integration u must be represented in
a manner consistent with the presence of the branch cut, for example, by writing
u = te±ix.
14.1.17
(a)
Show that
Jν(x) =
2
π1/20(ν + 1
2)
x
2
ν
π/2
Z
0
cos(x sinθ)cos2ν θ dθ,
where ν > −1
2.
Hint. Here is a chance to use series expansion and term-by-term integration. The
formulas of Section 13.3 will prove useful.
(b)
Transform the integral in part (a) into
Jν(x) =
1
π1/20(ν + 1
2)
x
2
ν
π
Z
0
cos(x cosθ)sin2ν θ dθ
=
1
π1/20(ν + 1
2)
x
2
ν
π
Z
0
e±ix cosθ sin2ν θ dθ
=
1
π1/20(ν + 1
2)
x
2
ν
1
Z
−1
e±ipx(1 −p2)ν−1/2 dp.
These are alternate integral representations of Jν(x).
14.1.18
Given that C is the contour in Fig. 14.5,
(a)
From
Jν(x) =
1
2πi
x
2
ν Z
C
t−ν−1et−x2/4tdt
derive the recurrence relation
J ′
ν(x) = ν
x Jν(x) −Jν+1(x).
(b)
From
Jν(x) =
1
2πi
Z
C
t−ν−1e(x/2)(t−1/t)dt
derive the recurrence relation
J ′
ν(x) = 1
2

Jν−1(x) −Jν+1(x)

.

14.1 Bessel Functions of the First Kind, Jν(x)
659
14.1.19
Show that the recurrence relation
J ′
n(x) = 1
2

Jn−1(x) −Jn+1(x)

follows directly from differentiation of
Jn(x) = 1
π
π
Z
0
cos(nθ −x sinθ) dθ.
14.1.20
Evaluate
∞
Z
0
e−ax J0(bx)dx,
a,b > 0.
Actually the results hold for a ≥0, −∞< b < ∞. This is a Laplace transform of J0.
Hint. Either an integral representation of J0 or a series expansion will be helpful.
14.1.21
Using the symmetries of the trigonometric functions, conﬁrm that for integer n,
1
2π
2π
Z
0
cos(x sinθ −nθ)dθ = 1
π
π
Z
0
cos(x sinθ −nθ)dθ.
14.1.22
(a)
Plot the intensity, 82 of Eq. (14.25), as a function of (sinα/λ) along a diameter
of the circular diffraction pattern. Locate the ﬁrst two minima.
(b)
Estimate the fraction of the total light intensity that falls within the central
maximum.
Hint. [J1(x)]2/x may be written as a derivative and the area integral of the intensity
integrated by inspection.
14.1.23
The fraction of light incident on a circular aperture (normal incidence) that is transmitted
is given by
T = 2
2ka
Z
0
J2(x)dx
x −
1
2ka
2ka
Z
0
J2(x)dx.
Here a is the radius of the aperture and k is the wave number, 2π/λ. Show that
(a)
T = 1 −1
ka
∞
X
n=0
J2n+1(2ka),
(b)
T = 1 −
1
2ka
2ka
Z
0
J0(x)dx.
14.1.24
The amplitude U(ρ,ϕ,t) of a vibrating circular membrane of radius a satisﬁes the wave
equation
∇2U ≡∂2U
∂ρ2 + 1
ρ
∂U
∂ρ + 1
ρ2
∂2U
∂ϕ2 = 1
v2
∂2U
∂t2 .
Here v is the phase velocity of the wave, determined by the properties of the membrane.

660
Chapter 14 Bessel Functions
(a)
Show that a physically relevant solution is
U(ρ,ϕ,t) = Jm(kρ)

c1eimϕ + c2e−imϕ
b1eiωt + b2e−iωt
.
(b)
From the Dirichlet boundary condition Jm(ka) = 0, ﬁnd the allowable values of k.
14.1.25
Example 14.1.2 describes the TM modes of electromagnetic cavity oscillation. To
obtain the transverse electric (TE) modes, we set Ez = 0 and work from the z com-
ponent of the magnetic induction B:
∇2Bz + α2Bz = 0
with boundary conditions
Bz(0) = Bz(l) = 0
and
∂Bz
∂ρ

ρ=a
= 0.
Show that the TE resonant frequencies are given by
ωmnp = c
s
β2mn
a2 + p2π2
l2
,
p = 1,2,3,...,
and identify the quantities βmn.
14.1.26
A conducting cylinder can accommodate traveling electromagnetic waves; when used
for this purpose it is called a wave guide. The equations describing traveling waves are
the same as those of Example 14.1.2, but there is no boundary condition on Ez at z = 0
or z = h other than that its z dependence be oscillatory. For each TM mode (values
of m and j of Example 14.1.2), there is a minimum frequency that can be transmitted
through a wave guide of radius a. Explain why this is so, and give a formula for the
cutoff frequencies.
14.1.27
Plot the three lowest TM and the three lowest TE angular resonant frequencies, ωmnp,
as a function of the ratio radius/length (a/l) for 0 ≤a/l ≤1.5.
Hint. Try plotting ω2 (in units of c2/a2) vs. (a/l)2. Why this choice?
14.1.28
Show that the integral
a
Z
0
xm Jn(x)dx,
m ≥n ≥0,
(a)
is integrable for m + n odd in terms of Bessel functions and powers of x, i.e., is
expressible as linear combinations of a p Jq(a);
(b)
may be reduced for m + n even to integrated terms plus
R a
0 J0(x)dx.
14.1.29
Show that
α0n
Z
0

1 −
y
α0n

J0(y)ydy = 1
α0n
α0n
Z
0
J0(y)dy.

14.2 Orthogonality
661
Here α0n is the nth zero of J0(y). This relation is useful (see Exercise 14.2.9): The expres-
sion on the right is easier and quicker to evaluate, and is much more accurate. Taking
the difference of two terms in the expression on the left leads to a large relative error.
14.2
ORTHOGONALITY
To identify the orthogonality properties of Bessel functions, it is convenient to start by
writing Bessel’s ODE in a form that we can recognize as a Sturm-Liouville eigenvalue
problem, the general properties of which were discussed in detail starting from Eq. (8.15).
If we divide Eq. (14.15) through by ρ2 and rearrange slightly, we have
−
 d2
dρ2 + 1
ρ
d
dρ −ν2
ρ2

Zν(kρ) = k2Zν(kρ),
(14.41)
showing that Zν(kρ) is an eigenfunction of the operator
L = −
 d2
dρ2 + 1
ρ
d
dρ −ν2
ρ2

(14.42)
with eigenvalue k2. Since we are most often interested in problems whose solutions in
cylindrical coordinates (ρ,ϕ, z) separate into products P(ρ)8(ϕ)Z(z) and which are for
the region within a cylindrical boundary at some ρ = a, we usually have 8(ϕ) = eimϕ with
m an integer (thereby causing ν2 →m2), and ﬁnd that P(ρ) = Jm(kρ). We choose P to
be a Bessel function of the ﬁrst kind because ρ = 0 is interior to our region and we want a
solution that is nonsingular there.
From Sturm-Liouville theory, we ﬁnd that the weight factor needed to make L of
Eq. (14.42) self-adjoint (as an ODE) is w(ρ) = ρ, and the orthogonality integral for the
two eigenfunctions Jν(kρ) and Jν(k′ρ), a case of Eq. (8.20), is (whether or not ν is an
integer)
a

k′Jν(ka)J ′
ν(k′a) −k J ′
ν(ka)Jν(k′a)

k2 −k′2
=
a
Z
0
ρ Jν(kρ)Jν(k′ρ)dρ.
(14.43)
In writing Eq. (14.43) we have used the fact that the presence of a factor ρ in the boundary
terms causes there to be no contribution from the lower limit ρ = 0.3
Equation (14.43) shows us that the Jν(k) of different k will be orthogonal (with weight
factor ρ) if we can cause the left-hand side of that equation to vanish. We may do so by
choosing k and k′ in such a way that Jν(ka) = Jν(k′a) = 0. In other words, we can require
that k and k′ be such that ka and k′a are zeros of Jν, and our Bessel functions will then
satisfy Dirichlet boundary conditions.
If now we let ανi denote the ith zero of Jν, the above analysis corresponds to the fol-
lowing orthogonality formula for the interval [0,a]:
a
Z
0
ρ Jν

ανi
ρ
a

Jν

ανj
ρ
a

dρ = 0,
i ̸= j.
(14.44)
3This will be true for all ν ≥−1, as will become more evident when we discuss Bessel functions of the second kind.

662
Chapter 14 Bessel Functions
−0.2
0
0.2
0.4
0.6
x
0.2
0.6
0.8
1
0.4
FIGURE 14.6
Bessel functions J1(α1nρ), n = 1,2,3 on range 0 ≤ρ ≤1.
Note that all members of our orthogonal set of Bessel functions have the same value of
the index ν, differing only in the scale of the argument of Jν. Successive members of the
orthogonal set will have increasing numbers of oscillations in the interval (0,a). Note also
that the weight factor, ρ, is just that which corresponds to unweighted orthogonality over
the region within a circle of radius a. We show in Fig. 14.6 the ﬁrst three Bessel functions
of order ν = 1 that are orthogonal within the unit circle.
An alternative to the foregoing analysis would be to ensure the vanishing of the bound-
ary term of Eq. (14.43) at ρ = a by choosing values of k corresponding to the Neumann
boundary condition J ′
ν(ka) = 0. The functions obtained in this way would also form an
orthogonal set.
Normalization
Our orthogonal sets of Bessel functions are not normalized, and to use them in expansions
we need their normalization integrals. These integrals may be developed by returning to
Eq. (14.43), which is valid for all k and k′, whether or not the boundary terms vanish. We
take the limits of both sides of that equation as k′ →k, evaluating the limit on the left-hand
side using l’Hôpital’s rule, which here corresponds to taking the derivatives of numerator
and denominator with respect to k′:
a
Z
0
ρ [Jν(kρ)]2 dρ = lim
k′→k
a

Jν(ka) d
dk′

k′J ′
ν(k′a)

−k J ′
ν(ka) d
dk′

Jν(k′a)

d
dk′ (k2 −k′2)
.

14.2 Orthogonality
663
We now simplify this equation for the case that ka = ανi, so we set Jν(ka) = 0 and reach
a
Z
0
ρ
h
Jν

ανi
ρ
a
i2
dρ = −a2k

J ′
ν(ka)
 2
−2k
= a2
2

J ′
ν(ανi)
 2.
(14.45)
Now, because ανi is a zero of Jν, Eq. (14.12) permits us to recognize that J ′
ν(ανi) =
−Jν+1(ανi). We then obtain from Eq. (14.45) the desired result,
a
Z
0
ρ
h
Jν

ανi
ρ
a
i2
dρ = a2
2

Jν+1(ανi)
 2.
(14.46)
Bessel Series
If we assume that the set of Bessel functions Jν(ανjρ/a) for ﬁxed ν and for j = 1,2,3,...
is complete, then any well-behaved but otherwise arbitrary function f (ρ) may be expanded
in a Bessel series
f (ρ) =
∞
X
j=1
cνj Jν

ανj
ρ
a

,
0 ≤ρ ≤a,
ν > −1.
(14.47)
The coefﬁcients cνj are determined by the usual rules for orthogonal expansions. With the
aid of Eq. (14.46) we have
cνj =
2
a2[Jν+1(ανj)]2
a
Z
0
f (ρ)Jν

ανj
ρ
a

ρdρ.
(14.48)
As pointed out earlier, it is also possible to obtain an orthogonal set of Bessel functions
of given order ν by imposing the Neumann boundary condition J ′
ν(kρ) = 0 at ρ = a,
corresponding to k = βνj/a, where βνj is the jth zero of J ′
ν. These functions can also be
used for orthogonal expansions. This approach is explored in Exercises 14.2.2 and 14.2.5.
The following example illustrates the usefulness of Bessel series.
Example 14.2.1
ELECTROSTATIC POTENTIAL IN A HOLLOW CYLINDER
We consider a hollow cylinder, which in cylindrical coordinates (ρ,ϕ, z) is bounded by
a curved surface at ρ = a and end caps at z = 0 and z = h. The base (z = 0) and curved
surface are assumed to be grounded, and therefore at potential ψ = 0, while the end cap
at z = h has a known potential distribution V (ρ,ϕ,h). Our problem is to determine the
potential V (ρ,ϕ, z) throughout the interior of the cylinder.
We proceed by ﬁnding separated-variable solutions to the Laplace equation in cylindri-
cal coordinates, along the lines discussed in Section 9.4. Our ﬁrst step is to identify product

664
Chapter 14 Bessel Functions
solutions, which, as in Eq. (9.64), must take the form4
ψlm(ρ,ϕ, z) = Plm(ρ)8m(ϕ)Zl(z),
(14.49)
with 8m = e±imϕ, and
d2
dz2 Zl(z) = l2Zl(z),
(14.50)
ρ2 d2
dρ2 Plm + ρ d
dρ Plm + (l2ρ2 −m2)Plm = 0.
(14.51)
The equation for Plm is Bessel’s ODE, with solutions of relevance here Jm(lρ). To satisfy
the boundary condition at ρ = a we need to choose l = αmj/a, where j can be any positive
integer and αmj is the jth zero of Jm.
The equation for Zl has solutions e±lz; to satisfy the boundary condition at z = 0 we
need to take the linear combination of these solutions that is equivalent to sinhlz. Combin-
ing these observations, we see that possible solutions to the Laplace equation that satisfy
all the boundary conditions other than that at z = h can be written
ψmj = cmj Jm

αmj
ρ
a

eimϕ sinh

αmj
z
a

.
(14.52)
Since Laplace’s equation is homogeneous, any linear combination of the ψmj with arbi-
trary values of the cmj will be a solution, and our remaining task is to ﬁnd the linear
combination of such solutions that satisﬁes the boundary condition at z = h. Therefore,
V (ρ,ϕ, z) =
∞
X
m=−∞
∞
X
j=1
ψmj,
(14.53)
with the boundary condition at z = h expressed as
∞
X
m=−∞
∞
X
j=1
cmj Jm

αmj
ρ
a

eimϕ sinh

αmj
h
a

= V (ρ,ϕ,h).
(14.54)
Our solution is both a trigonometric series and a Bessel series, each with orthogonality
properties that can be used to determine the coefﬁcients. From Eq. (14.48) and the formula
2π
Z
0
e−imϕeim′ϕ = 2πδmm′,
(14.55)
we ﬁnd
cmj =

πa2 sinh

αmj
h
a

J 2
m+1(αmj)
−1
2π
Z
0
dϕ
a
Z
0
V (ρ,ϕ,h)Jm

αmj
ρ
a

e−imϕρdρ.
(14.56)
4 Note that here Zl is a function of z arising from the separation of variables; the notation is not intended to identify it as a
Bessel function.

14.2 Orthogonality
665
These are deﬁnite integrals, that is, numbers. Substituting back into Eq. (14.52), the series
in Eq. (14.53) is speciﬁed and the potential V (ρ,ϕ, z) is determined.
■
Exercises
14.2.1
Show that
(k2 −k′2)
a
Z
0
Jν(kx)Jν(k′x)xdx = a[k′Jν(ka)J ′
ν(k′a) −k J ′
ν(ka)Jν(k′a)],
where J ′
ν(ka) =
d
d(kx) Jν(kx) |x=a, and that
a
Z
0
[Jν(kx)]2xdx = a2
2

[J ′
ν(ka)]2 +

1 −ν2
k2a2

[Jν(ka)]2

,
ν > −1.
These two integrals are usually called the ﬁrst and second Lommel integrals.
14.2.2
(a)
If βνm is the mth zero of (d/dρ)Jν(βνmρ/a), show that the Bessel functions are
orthogonal over the interval [0,a] with an orthogonality integral
a
Z
0
Jν

βνm
ρ
a

Jν

βνn
ρ
a

ρ dρ = 0,
m ̸= n,
ν > −1.
(b)
Derive the corresponding normalization integral (m = n).
ANS.
(b) a2
2

1 −ν2
β2νm

[Jν(βνm)]2,
ν > −1.
14.2.3
Verify that the orthogonality equation, Eq. (14.44), and the normalization equation,
Eq. (14.46), hold for ν > −1.
Hint. Using power-series expansions, examine the behavior of Eq. (14.43) as ρ →0.
14.2.4
From Eq. (11.49), develop a proof that Jν(z), ν > −1 has no complex roots (with a
nonzero imaginary part).
Hint. (a) Use the series form of Jν(z) to exclude pure imaginary roots.
(b) Assume ανm to be complex and take ανn to be α∗
νm.
14.2.5
(a)
In the series expansion
f (ρ) =
∞
X
m=1
cνm Jν

ανm
ρ
a

,
0 ≤ρ ≤a,
ν > −1,
with Jν(ανm) = 0, show that the coefﬁcients are given by
cνm =
2
a2[Jν+1(ανm)]2
a
Z
0
f (ρ)Jν

ανm
ρ
a

ρ dρ.

666
Chapter 14 Bessel Functions
(b)
In the series expansion
f (ρ) =
∞
X
m=1
dνm Jν

βνm
ρ
a

,
0 ≤ρ ≤a,
ν > −1,
with (d/dρ)Jν(βνmρ/a) |ρ=a= 0, show that the coefﬁcients are given by
dνm =
2
a2(1 −ν2/β2νm)[Jν(βνm)]2
a
Z
0
f (ρ)Jν

βνm
ρ
a

ρ dρ.
14.2.6
A right circular cylinder has an electrostatic potential of ψ(ρ,ϕ) on both ends. The
potential on the curved cylindrical surface is zero. Find the potential at all interior
points.
Hint. Choose your coordinate system and adjust your z dependence to exploit the
symmetry of your potential.
14.2.7
A function f (x) is expressed as a Bessel series:
f (x) =
∞
X
n=1
an Jm(αmnx),
with αmn the nth root of Jm. Prove the Parseval relation,
1
Z
0
[ f (x)]2x dx = 1
2
∞
X
n=1
a2
n[Jm+1(αmn)]2.
14.2.8
Prove that
∞
X
n=1
(αmn)−2 =
1
4(m + 1).
Hint. Expand xm in a Bessel series and apply the Parseval relation.
14.2.9
A right circular cylinder of length l and radius a has on its end caps a potential
ψ

z = ± l
2

= 100

1 −ρ
a

.
The potential on the curved surface (the side) is zero. Using the Bessel series from
Exercise 14.2.6, calculate the electrostatic potential for ρ/a = 0.0(0.2)1.0 and z/l =
0.0(0.1)0.5. Take a/l = 0.5.

14.3 Neumann Functions, Bessel Functions of the Second Kind
667
Hint. From Exercise 14.1.29 you have
α0n
Z
0

1 −
y
α0n

J0(y)ydy.
Show that this equals
1
α0n
α0n
Z
0
J0(y)dy.
Numerical evaluation of this latter form rather than the former is both faster and more
accurate.
Note. For ρ/a = 0.0 and z/l = 0.5 the convergence is slow, 20 terms giving only 98.4
rather than 100.
Check value.
For ρ/a = 0.4 and z/l = 0.3,
ψ = 24.558.
14.3
NEUMANN FUNCTIONS, BESSEL FUNCTIONS OF
THE SECOND KIND
From the theory of ODEs, it is known that Bessel’s equation has two independent solutions.
Indeed, for nonintegral order ν we have already found two solutions and labeled them
Jν(x) and J−ν(x) using the inﬁnite series, Eq. (14.6). The trouble is that when ν is integral,
Eq. (14.5) holds and we have but one independent solution. A second solution may be
developed by the methods of Section 7.6. This yields a perfectly good second solution of
Bessel’s equation. However, that solution is not the standard form, which is called a Bessel
function of the second kind or alternatively, a Neumann function.
Deﬁnition and Series Form
The standard deﬁnition of the Neumann functions is the following linear combination of
Jν(x) and J−ν(x):
Yν(x) = cosνπ Jν(x) −J−ν(x)
sinνπ
.
(14.57)
For nonintegral ν,Yν(x) clearly satisﬁes Bessel’s equation, for it is a linear combination
of known solutions, Jν(x) and J−ν(x). The behavior of Yν(x) for small x (and nonintegral
ν) can be determined from the power-series expansion of J−ν, Eq. (14.6); we may write,

668
Chapter 14 Bessel Functions
calling upon Eq. (13.23),
Yν(x) = −
1
sinνπ

1
0(1 −ν)
x
2
−ν
−···

= −0(ν)0(1 −ν)
π

1
0(1 −ν)
x
2
−ν
−···

= −0(ν)
π
x
2
−ν
+ ··· .
(14.58)
However, for integral ν, Eq. (14.57) becomes indeterminate; in fact, Yn(x) for integral
n is deﬁned as
Yn(x) = lim
ν→n Yν(x).
(14.59)
To determine that the limit represented by Eq. (14.59) exists and is not identically
zero (so that Yn(x) has a meaningful deﬁnition), we apply l’Hôpital’s rule to Eq. (14.57),
obtaining initially
Yn(x) = 1
π
d Jν
dν −(−1)n d J−ν
dν

ν=n
.
(14.60)
Inserting the expansions of Jν and J−ν from Eq. (14.6), the differentiations of (x/2)2s±ν
combine to yield (2/π)Jn(x)ln(x/2), while the derivatives of 1/0(s ± n + 1) yield terms
containing ψ(s ± n + 1)/0(s ± n + 1), where ψ is the digamma function (Section 13.2).
The ﬁnal result, whose veriﬁcation is the topic of Exercise 14.3.8, is
Yn(x) = 2
π Jn(x)ln
x
2

−1
π
n−1
X
k=0
(n −k −1)!
k!
x
2
2k−n
−1
π
∞
X
k=0
(−1)k
k!(n + k)!

ψ(k + 1) + ψ(n + k + 1)
 x
2
2k+n
,
(14.61)
An explicit form for ψ(n) for integer n is given in Eq. (13.40).
Equation (14.61) shows that for n > 0, the most divergent term for small x is in agree-
ment with the result for noninteger n given in Eq. (14.58). We also see that all solutions
for integer n contain a logarithmic term with the regular function Jn multiplying the log-
arithm. In our earlier study of ODEs, we found that a second solution will usually have
a contribution of this type when the indicial equation causes the exponents of the power-
series expansion to be integers. We may also conclude from Eq. (14.61) that Yn is linearly
independent of Jn, conﬁrming that we indeed have a second solution to Bessel’s ODE.
It is of some interest to obtain the expansion of Y0(x) in a more explicit form. Returning
to Eq. (14.61), we note that its ﬁrst summation is vacant, and we have the relatively simple
expansion
Y0(x) = 2
π J0(x)ln
x
2

−2
π
∞
X
k=0
(−1)k
k!k! [−γ + Hk]
x
2
2k
= 2
π J0(x)
h
γ + ln
x
2
i
−2
π
∞
X
k=1
(−1)k
k!k! Hk
x
2
2k
,
(14.62)
where Hk is the harmonic number Pk
m=1 m−1 and γ is the Euler-Mascheroni constant.

14.3 Neumann Functions, Bessel Functions of the Second Kind
669
−1.0
0.4
2
4
6
8
Y0(x)
Y1(x)
Y2(x)
x
10
FIGURE 14.7
Neumann functions Y0(x), Y1(x), and Y2(x).
The Neumann functions Yn(x) are irregular at x = 0, but with increasing x become
oscillatory, as may be seen from the graphs of Y0, Y1, and Y2 in Fig. 14.7. The deﬁnition
of Eq. (14.57) was speciﬁcally chosen to cause the oscillatory behavior to be at the same
scale as that of Jn and displaced asymptotically in phase by π/2, similarly to the relative
behavior of the sine and cosine. However, unlike the sine and cosine, Jn and Yn only exhibit
exact periodicity in the asymptotic limit. This point is covered in detail in Section 14.6.
Figure 14.8 compares J0(x) and Y0(x) over a large range of x.
Integral Representations
As with all the other Bessel functions, Yν(x) has integral representations. For Y0(x) we
have
Y0(x) = −2
π
∞
Z
0
cos(x cosht)dt = −2
π
∞
Z
1
cos(xt)
(t2 −1)1/2 dt,
x > 0.
(14.63)
See Exercise 14.3.7, which shows that the above integral is a solution to Bessel’s ODE that
is linearly independent of J0(x). Speciﬁc identiﬁcation as Y0 is the topic of Exercise 14.4.8.
Recurrence Relations
Substituting Eq. (14.57) for Yν(x) (nonintegral ν) into the recurrence relations for Jn(x),
Eqs. (14.7) and (14.8), we see immediately that Yν(x) satisﬁes these same recurrence rela-
tions. This actually constitutes a proof that Yν is a solution to the Bessel ODE. Note that
the converse is not necessarily true. All solutions need not satisfy the same recurrence
relations, as the relations depend on the scales assigned to the solutions of different ν.
An example of this sort of trouble appears in Section 14.5.

670
Chapter 14 Bessel Functions
−0.4
−0.2
0
0.2
0.4
0.6
5
10
20
25
30
15
x
FIGURE 14.8
Oscillatory behavior of J0(x) (solid line) and Y0(x) (dashed line) for
1 ≤x ≤30.
Wronskian Formulas
An ODE p(x)y′′ + q(x)y′ + r(x)y = 0 in self-adjoint form (so q = p′) was found in
Exercise 7.6.1 to have the following Wronskian formula connecting its solutions u and v:
u(x)v′(x) −u′(x)v(x) =
A
p(x).
(14.64)
To bring Bessel’s equation to self-adjoint form, we need to write it as xy′′ + y′ +
(x −ν2/x)y = 0, thereby showing that for our present purposes p(x) = x, and we therefore
have for each noninteger ν
Jν J ′
−ν −J ′
ν J−ν = Aν
x .
(14.65)
Since Aν is a constant but can be expected to depend on ν, it may be identiﬁed for each
ν at any convenient point, such as x = 0. From the power-series expansion, Eq. (14.6),
we obtain the following limiting behaviors for small x:
Jν →
1
0(1 + ν)
x
2
ν
,
J ′
ν →
ν
20(1 + ν)
x
2
ν−1
,
(14.66)
J−ν →
1
0(1 −ν)
x
2
−ν
,
J ′
−ν →
−ν
20(1 −ν)
x
2
−ν−1
.
Substitution into Eq. (14.65) yields
Jν(x)J ′
−ν(x) −J ′
ν(x)J−ν(x) =
−2ν
x0(1 + ν)0(1 −ν) = −2sinνπ
πx
,
(14.67)

14.3 Neumann Functions, Bessel Functions of the Second Kind
671
using Eq. (13.23). Although Eq. (14.67) was obtained for x →0, comparison with
Eq. (14.65) shows that it must be true for all x, and that Aν = −(2/π)sinνπ. Note that Aν
vanishes for integral ν, showing that the Wronskian of Jn and J−n vanishes and that these
Bessel functions are linearly dependent.
Using our recurrence relations, we may readily develop a large number of alternate
forms, among which are
Jν J−ν+1 + J−ν Jν−1 = 2sinνπ
πx
,
(14.68)
Jν J−ν−1 + J−ν Jν+1 = −2sinνπ
πx
,
(14.69)
JνY ′
ν −J ′
νYν = 2
πx ,
(14.70)
JνYν+1 −Jν+1Yν = −2
πx .
(14.71)
Many more will be found in the Additional Readings.
You will recall that in Chapter 7, Wronskians were of great value in two respects: (1)
in establishing the linear independence or linear dependence of solutions of differential
equations, and (2) in developing an integral form of a second solution. Here the speciﬁc
forms of the Wronskians and Wronskian-derived combinations of Bessel functions are
useful primarily in development of the general behavior of the various Bessel functions.
Wronskians are also of great use in checking tables of Bessel functions.
Uses of Neumann Functions
The Neumann functions Yν(x) are of importance for a number of reasons:
1.
They are second, independent solutions of Bessel’s equation, thereby completing the
general solution.
2.
They are needed for physical problems in which they are not excluded by a require-
ment of regularity at x = 0. Speciﬁc examples include electromagnetic waves in coax-
ial cables and quantum mechanical scattering theory.
3.
They lead directly to the two Hankel functions, whose deﬁnition and use, particularly
in studies of wave propagation, are discussed in Section 14.4.
We close with one example in which Neumann functions play a vital role.
Example 14.3.1
COAXIAL WAVE GUIDES
We are interested in an electromagnetic wave conﬁned between the concentric, conducting
cylindrical surfaces ρ = a and ρ = b. The equations governing the wave propagation are
the same as those discussed in Example 14.1.2, but the boundary conditions are now dif-
ferent, and our interest is in solutions that are traveling waves (compare Exercise 14.1.26).

672
Chapter 14 Bessel Functions
For wave propagation problems, it is convenient to write the solution in terms of com-
plex exponentials, with the actual physical quantities involved ultimately identiﬁed as their
real (or imaginary) parts. Thus, in place of Eq. (14.31) (the solution for standing waves in a
cylindrical cavity), we now have for Ez solutions in which the ρ dependence must involve
both Jm and Ym (as the latter is not ruled out by a requirement for regularity at ρ = 0).
Including the time dependence, we have for the TM (transverse magnetic) solutions the
separated-variable forms
Ez =

cmn Jm(γmnρ) + dmnYm(γmnρ)

e±imϕei(lz−ωt),
(14.72)
with l now permitted to have any real value (there is no boundary condition on z). The
index n identiﬁes different possible values of γmn. As in Eq. (14.30), the relation between
γmn, l, and ω is
ω2
c2 = γ 2
mn + l2.
(14.73)
The most general TM traveling-wave solution will be an arbitrary linear combination
of all functions of the form given by Eq. (14.72) with γmn, cmn, and dmn chosen so that
Ez will vanish at ρ = a and ρ = b. A main difference between this problem and that of
Example 14.1.2 is that the condition on Ez is not given by the zeros of the Bessel functions
Jm, but by zeros of linear combinations of Jm and Ym. Speciﬁcally, we require that
cmn Jm(γmna) + dmnYm(γmna) = 0,
(14.74)
cmn Jm(γmnb) + dmnYm(γmnb) = 0.
(14.75)
These transcendental equations may be solved, for each relevant m, to yield an inﬁnite set
of solutions (indexed by n) for γmn and the ratio dmn/cmn. An example of this process is
in Exercise 14.3.10.
Returning now to the equation for ω, we observe that the smallest value it can attain for
the solution indexed by m and n is cγmn, showing that TM waves can only propagate if the
angular frequency ω of the electromagnetic radiation is equal to or larger than this cutoff.
In general, larger values of γmn correspond to higher degrees of transverse oscillation, and
modes with greater transverse oscillation will therefore have higher cutoff frequencies.
As for the circular wave guide (the subject of Exercise 14.1.26, there will also be TE
modes of propagation, also with mode-dependent cutoffs. However, the coaxial guide can
also support traveling waves in TEM (transverse electric and magnetic) modes. These
modes, not possible for a circular waveguide, do not exhibit a cutoff, are the conﬁned
equivalent of plane waves, and correspond to the ﬂow of current (in opposite directions)
on the coaxial conductors.
■
Exercises
14.3.1
Prove that the Neumann functions Yn (with n an integer) satisfy the recurrence relations
Yn−1(x) + Yn+1(x) = 2n
x Yn(x),
Yn−1(x) −Yn+1(x) = 2Y ′
n(x).

14.3 Neumann Functions, Bessel Functions of the Second Kind
673
Hint. These relations may be proved by differentiating the recurrence relations for Jν
or by using the limit form of Yν but not dividing everything by zero.
14.3.2
Show that for integer n
Y−n(x) = (−1)nYn(x).
14.3.3
Show that
Y ′
0(x) = −Y1(x).
14.3.4
If X and Z are any two solutions of Bessel’s equation, show that
Xν(x)Z′
ν(x) −X′
ν(x)Zν(x) = Aν
x ,
in which Aν may depend on ν but is independent of x. This is a special case of Exer-
cise 7.6.11.
14.3.5
Verify the Wronskian formulas
Jν(x)J−ν+1(x) + J−ν(x)Jν−1(x) = 2sinνπ
πx
,
Jν(x)Y ′
ν(x) −J ′
ν(x)Yν(x) = 2
πx .
14.3.6
As an alternative to letting x approach zero in the evaluation of the Wronskian constant,
we may invoke the uniqueness of power-series expansions. The coefﬁcient of x−1 in
the series expansion of uν(x)v′
ν(x) −u′
ν(x)vν(x) is then Aν. Show by series expansion
that the coefﬁcients of x0 and x1 of Jν(x)J ′
−ν(x) −J ′
ν(x)J−ν(x) are each zero.
14.3.7
(a)
By differentiating and substituting into Bessel’s ODE for ν = 0, show that
R ∞
0 cos(x cosht)dt is a solution.
Hint. Rearrange the ﬁnal integral to
R ∞
0
d
dt

x sin(x cosht)sinht

dt.
(b)
Show that Y0(x) = −2
π
R ∞
0 cos(x cosht)dt is linearly independent of J0(x).
14.3.8
Verify the expansion formula for Yn(x) given in Eq. (14.61).
Hint. Start from Eq. (14.60) and perform the indicated differentiations on the power-
series expansions of Jν and J−ν. The digamma functions ψ arise from the differen-
tiation of the gamma function. You will need the identity (not derived in this book)
lim
z→−n ψ(z)/0(z) = (−1)n−1n!, where n is a positive integer.
14.3.9
If Bessel’s ODE (with solution Jν) is differentiated with respect to ν, one obtains
x2 d2
dx2
∂Jν
∂ν

+ x d
dx
∂Jν
∂ν

+ (x2 −ν2)∂Jν
∂ν = 2ν Jν.
Use the above equation to show that Yn(x) is a solution to Bessel’s ODE.
Hint. Equation (14.60) will be useful.
14.3.10
For the case m =0, a =1, and b =2, the coaxial wave-guide TM boundary conditions
become f (λ) = 0, with
f (x) = J0(2x)
Y0(2x) −J0(x)
Y0(x).

674
Chapter 14 Bessel Functions
5
−5
x
J0 (2x)
J0 (x)
Y0 (x)
Y0 (2x)
2
4
6
8
10
FIGURE 14.9
The function f (x) of Exercise 14.3.10.
This function is plotted in Fig. 14.9.
(a)
Calculate f (x) for x = 0.0(0.1)10.0 and plot f (x) vs. x to ﬁnd the approximate
location of the roots.
(b)
Call a root-ﬁnding program to determine the ﬁrst three roots to higher precision.
ANS.
(b) 3.1230, 6.2734, 9.4182.
Note. The higher roots can be expected to appear at intervals whose length approaches π.
Why? AMS-55 (see Additional Readings) gives an approximate formula for the roots.
The function g(x) = J0(x)Y0(2x) −J0(2x)Y0(x) is much better behaved than the f (x)
previously discussed.
14.4
HANKEL FUNCTIONS
Hankel functions are solutions of Bessel’s ODE with asymptotic properties that make
them particularly useful in problems involving the propagation of spherical or cylindrical
waves. Since the functions Jν and Yν form the complete solution of this ODE, the Hankel
functions cannot be anything completely new; they must be linear combinations of the
solutions we have already found. We introduce them here via straightforward algebraic
deﬁnitions; later in this section we identify integral representations that some authors have
used as a starting point.

14.4 Hankel Functions
675
Deﬁnitions
Starting from the Bessel functions of the ﬁrst and second kinds, namely Jν(x) and Yν(x),
we deﬁne the two Hankel functions H(1)
ν (x) and H(2)
ν (x) (sometimes, but nowadays
infrequently referred to as Bessel functions of the third kind) as follows:
H(1)
ν (x) = Jν(x) + iYν(x),
(14.76)
H(2)
ν (x) = Jν(x) −iYν(x).
(14.77)
This is exactly analogous to taking
e±iθ = cosθ ± i sinθ.
(14.78)
For real arguments, H(1)
ν
and H(2)
ν
are complex conjugates. The extent of the analogy
will be seen even better when their asymptotic forms are considered. Indeed, it is their
asymptotic behavior that makes the Hankel functions useful. This behavior is discussed in
Section 14.6, and in that section we provide an illustrative example in which the asymptotic
properties play a key role.
Series expansion of H(1)
ν (x) and H(2)
ν (x) may be obtained by combining Eqs. (14.6) and
(14.62). Often only the ﬁrst term is of interest; it is given by
H(1)
0 (x) ≈i 2
π ln x + 1 + i 2
π (γ −ln2) + ···,
(14.79)
H(1)
ν (x) ≈−i 0(ν)
π
2
x
ν
+ ···,
ν > 0,
(14.80)
H(2)
0 (x) ≈−i 2
π ln x + 1 −i 2
π (γ −ln2) + ···,
(14.81)
H(2)
ν (x) ≈i 0(ν)
π
2
x
ν
+ ···,
ν > 0.
(14.82)
In these equations γ is the Euler-Mascheroni constant, deﬁned in Eq. (1.13).
Since the Hankel functions are linear combinations (with constant coefﬁcients) of Jν
and Yν, they satisfy the same recurrence relations, Eqs. (14.7) and (14.8). For both H(1)
ν (x)
and H(2)
ν (x),
Hν−1(x) + Hν+1(x) = 2ν
x Hν(x),
(14.83)
Hν−1(x) −Hν+1(x) = 2H′
ν(x).
(14.84)
A variety of Wronskian formulas can be developed, including:
H(2)
ν
H(1)
ν+1 −H(1)
ν
H(2)
ν+1 =
4
iπx ,
(14.85)
Jν−1H(1)
ν
−Jν H(1)
ν−1 =
2
iπx ,
(14.86)
Jν−1H(2)
ν
−Jν H(2)
ν−1 = −2
iπx .
(14.87)

676
Chapter 14 Bessel Functions
Contour Integral Representation of
the Hankel Functions
The integral representation (Schlaeﬂi integral) for Jν(x) was introduced in Section 14.1,
where we established that
Jν(x) =
1
2πi
Z
C
e(x/2)(t−1/t) dt
tν+1 ,
(14.88)
with C the contour shown in Fig. 14.5. Recall that when ν is nonintegral, the integrand has
a branch point at t = 0 and the contour had to avoid a cut line that was drawn along the
negative real axis. In developing the Schlaeﬂi integral for general ν, we began by showing
that Bessel’s ODE was satisﬁed for any open contour for which an expression of the form
e(x/2)(t−1/t)
tν

ν + x
2

t + 1
t

(14.89)
vanished at both endpoints of the contour.
We now make further use of those observations by noting that the expression in
Eq. (14.89) not only vanishes at t = −∞on the real axis both below and above the cut,
but that it also vanishes at t = 0 when that point is approached from positive t.
We therefore consider the contour shown in Fig. 14.10, calling attention to the fact that
the upper half of the contour (from t = 0+ to t = ∞eπi), labeled C1, meets the conditions
necessary to yield a solution to Bessel’s ODE, and that the remaining (lower) half of the
contour, labeled C2, also yields a solution. What remains to be determined is the identi-
ﬁcation of these solutions: We will show that they are the Hankel functions. For x > 0,
we assert that
H(1)
ν (x) = 1
πi
Z
C1
e(x/2)(t−1/t) dt
tν+1 ,
(14.90)
H(2)
ν (x) = 1
πi
Z
C2
e(x/2)(t−1/t) dt
tν+1 .
(14.91)
C1
C1
C2
C2
∞eiπ
∞e −iπ
t =i
t = −i 
(t)
(t)
FIGURE 14.10
Hankel function contours.

14.4 Hankel Functions
677
These expressions are particularly convenient because they may be handled by the method
of steepest descents (Section 12.7). H(1)
ν (x) has a saddle point at t = +i, whereas H(2)
ν (x)
has a saddle point at t = −i.
There remains the problem of relating Eqs. (14.90) and (14.91) to our earlier deﬁnition
of the Hankel functions, Eqs. (14.76) and (14.77). Since the contours of Eqs. (14.90) and
(14.91) combine to produce a contour yielding Jν, Eq. (14.88), we have, from the integral
representations,
Jν(x) = 1
2
h
H(1)
ν (x) + H(2)
ν (x)
i
.
(14.92)
If we can show (also from the integral representations) that
Yν(x) = 1
2i
h
H(1)
ν (x) −H(2)
ν (x)
i
,
(14.93)
we will be able to recover the original deﬁnitions of the H(i)
ν .
We therefore rewrite Eq. (14.90) by replacing the integration variable t by eiπ/s, so
the integrand of that equation becomes −e(x/2)(s−1/s)e−iνπsν−1. After the substitution the
contour (in s) is found to be the same as C1, but traversed in the opposite direction (thereby
compensating the initial minus sign in the transformed integrand). The result, with details
left as Exercise 14.4.3, is that the contour integral representation of H(1) is consistent with
the identiﬁcation
H(1)
ν (x) = e−iνπ H(1)
−ν (x).
(14.94)
Similar processing of Eq. (14.91), with t = e−iπ/s, leads to
H(2)
ν (x) = eiνπ H(2)
−ν (x).
(14.95)
We now combine Eqs. (14.94) and (14.95) to reach
J−ν(x) = 1
2
h
eiνπ H(1)
ν (x) + e−iνπ H(2)
ν (x)
i
,
(14.96)
where again the H(i)
ν
refer to the contour integral representations. Substituting Eqs. (14.92)
and (14.96) into the deﬁning equation for Yν, Eq. (14.57), we conﬁrm that Yν is described
properly when the H(i)
ν
stand for their contour integral representations. This completes
the proof that Eqs. (14.90) and (14.91) are consistent with the original deﬁnitions of the
Hankel functions.
The reader may wonder why so much stress is placed on the development of integral
representations. There are several reasons. The ﬁrst is simply aesthetic appeal. Second,
the integral representations facilitate manipulations, analysis, and the development of rela-
tions among the various special functions. We have already seen an example of this in the
development of Eqs. (14.94) to (14.96). And, probably most important of all, integral rep-
resentations are extremely useful in developing asymptotic expansions. Such expansions
can often be obtained using the method of steepest descents (Section 12.7), or by methods
involving expansion in negative powers of the expansion variable, as in Section 12.6.

678
Chapter 14 Bessel Functions
In conclusion, the Hankel functions are introduced here for the following reasons:
•
As analogs of e±ix they are useful for describing traveling waves. These applications
are best studied when the asymptotic properties of the functions are in hand, and there-
fore are postponed to Section 14.6.
•
They offer an alternate (contour integral) and rather elegant deﬁnition of Bessel
functions.
•
We will see in Section 14.5 that they offer a route to the deﬁnition of the quantities
known as modiﬁed Bessel functions, and that in Section 14.6 they are useful for the
development of the asymptotic properties of Bessel functions.
Exercises
14.4.1
Verify the Wronskian formulas
(a)
Jν(x)H(1)′
ν
(x) −J ′
ν(x)H(1)
ν (x) = 2i
πx ,
(b)
Jν(x)H(2)′
ν
(x) −J ′
ν(x)H(2)
ν (x) = −2i
πx ,
(c)
Yν(x)H(1)′
ν
(x) −Y ′
ν(x)H(1)
ν (x) = −2
πx ,
(d)
Yν(x)H(2)′
ν
(x) −Y ′
ν(x)H(2)
ν (x) = −2
πx ,
(e)
H(1)
ν (x)H(2)′
ν
(x) −H(1)′
ν
(x)H(2)
ν (x) = −4i
πx ,
(f)
H(2)
ν (x)H(1)
ν+1(x) −H(1)
ν (x)H(2)
ν+1(x) =
4
iπx ,
(g)
Jν−1(x)H(1)
ν (x) −Jν(x)H(1)
ν−1(x) =
2
iπx .
14.4.2
Show that the integral forms
(a)
1
iπ
∞eiπ
Z
0C1
e(x/2)(t−1/t) dt
tν+1 = H(1)
ν (x),
(b)
1
iπ
0
Z
∞e−iπC2
e(x/2)(t−1/t) dt
tν+1 = H(2)
ν (x)
satisfy Bessel’s ODE. The contours C1 and C2 are shown in Fig. 14.10.
14.4.3
Show that the substitution t = eiπ/s into Eq. (14.90) for H(1)
ν (x) not only produces the
integrand for the similar integral representation of H(1)
−ν (x) but that the contour in s is
identical to the original contour in t.
14.4.4
Using the integrals and contours given in Exercise 14.4.2, show that
1
2i [H(1)
ν (x) −H(2)
ν (x)] = Yν(x).

14.4 Hankel Functions
679
iπ
C3
C4
∞−iπ
∞+iπ 
−iπ
(γ )
(γ )
FIGURE 14.11
Hankel function contours for Exercise 14.4.5.
14.4.5
Show that the integrals in Exercise 14.4.2 may be transformed to yield
(a)
H(1)
ν (x) = 1
πi
R
C3 ex sinhγ −νγ dγ,
(b)
H(2)
ν (x) = 1
πi
R
C4 ex sinhγ −νγ dγ,
where C3 and C4 are the contours in Fig. 14.11.
14.4.6
(a)
Transform H(1)
0 (x), Eq. (14.90), into
H(1)
0 (x) = 1
iπ
Z
C
eix coshsds,
where the contour C runs from −∞−iπ/2 through the origin of the s-plane to
∞+ iπ/2.
(b)
Justify rewriting H(1)
0 (x) as
H(1)
0 (x) = 2
iπ
∞+iπ/2
Z
0
eix coshsds.
(c)
Verify that this integral representation actually satisﬁes Bessel’s differential equa-
tion. (The iπ/2 in the upper limit is not essential. It serves as a convergence factor.
We can replace it by iaπ/2 and take the limit a →0.)
14.4.7
From
H(1)
0 (x) = 2
iπ
∞
Z
0
eix coshsds
show that
(a) J0(x) = 2
π
R ∞
0 sin(x coshs)ds,
(b) J0(x) = 2
π
R ∞
1
sin(xt)
√
t2−1dt.
This last result is a Fourier sine transform.

680
Chapter 14 Bessel Functions
14.4.8
From H(1)
0 (x) = 2
iπ
∞
Z
0
eix coshsds (see Exercises 14.4.5 and 14.4.6), show that
(a)
Y0(x) = −2
π
∞
Z
0
cos(x coshs)ds,
(b)
Y0(x) = −2
π
∞
Z
1
cos(xt)
p
t2 −1)
dt.
These are the integral representations in Eq. (14.63). This last result is a Fourier cosine
transform.
14.5
MODIFIED BESSEL FUNCTIONS, Iν(x) AND Kν(x)
The Laplace and Helmholtz equations, when separated in circular cylindrical coordinates,
may lead to Bessel’s ODE in the coordinate ρ that describes distance from the cylindrical
axis. When that is the case, the behavior of the solutions as a function of ρ is inherently
oscillatory; as we have already seen, the Bessel functions Jν(kρ), and also Yν(kρ), have
for any value of ν an inﬁnite number of zeros, and this property may be useful in causing
satisfaction of boundary conditions. However, as already shown in Section 9.4, the con-
nection constants arising when the variables are separated may have a sign opposite to that
required to yield Bessel’s ODE, and the equation in the ρ coordinate then assumes the
form
ρ2 d2
dρ2 Pν(kρ) + ρ d
dρ Pν(kρ) −(k2ρ2 + ν2)Pν(kρ) = 0.
(14.97)
Equation (14.97), known as the modiﬁed Bessel equation, differs from the Bessel ODE
only in the sign of the quantity k2ρ2, but this small change is sufﬁcient to alter the nature
of the solutions. As we shall shortly discuss in more detail, the solutions to Eq. (14.97),
called modiﬁed Bessel functions, are not oscillatory and have behavior that is exponential
(rather than trigonometric) in character.
Fortunately, the knowledge we have developed regarding the Bessel ODE can be put
to good use for the modiﬁed Bessel equation, since the substitution k →ik converts the
conventional Bessel ODE to its modiﬁed form, and shows that if Pν(kρ) is a solution to
the Bessel ODE, then Pν(ikρ) must be a solution to the modiﬁed Bessel equation. One
way of stating this fact is to note that the solutions of Eq. (14.97) are Bessel functions of
imaginary argument.

14.5 Modiﬁed Bessel Functions, Iν(x) and Kν(x)
681
Series Solution
Since any solution of Bessel’s ODE can be converted into a solution of the modiﬁed ODE
by insertion of i into its argument, let’s start by looking at the series expansion
Jν(ix) =
∞
X
s=0
(−1)s
s!0(s + ν + 1)
ix
2
ν+2s
= iν
∞
X
s=0
1
s!0(s + ν + 1)
x
2
ν+2s
.
(14.98)
Since all the terms of the summation have the same sign, it is evident that Jν(ix) cannot
exhibit oscillatory behavior. It is convenient to choose the solutions of the modiﬁed Bessel
equation in a way that causes them to be real, and we accordingly deﬁned the modiﬁed
Bessel functions of the ﬁrst kind, denoted Iν(x), as
Iν(x) = i−ν Jν(ix) = e−iνπ/2Jν(xeiπ/2) =
∞
X
s=0
1
s!0(s + ν + 1)
x
2
ν+2s
.
(14.99)
Like Jν for ν ≥0, Iν is ﬁnite at the origin, with a power-series expansion that is convergent
for all x. At small x, its limiting behavior will be of the form
Iν(x) =
xν
2ν0(ν + 1) + ··· .
(14.100)
From the relation between Jν and J−ν, we may also conclude that Iν and I−ν are linearly
independent unless ν is an integer n; taking cognizance of the factor i−n in the deﬁnition
of In, the linear dependence takes the form
In(x) = I−n(x).
(14.101)
Graphs of I0 and I1 are shown in Fig. 14.12.
Recurrence Relations for Iν
The recurrence relations satisﬁed by Iν(x) may be developed from the series expansions,
but it is perhaps easier to work from the existing recurrence relations for Jν(x). Our starting
point is Eq. (14.7), written for ix:
Jν−1(ix) + Jν+1(ix) = 2n
ix Jn(ix).
(14.102)
We change J to I, related according to Eq. (14.99) by
Jν(ix) = iν Iν(x),
(14.103)
thereby obtaining
iν−1Iν−1(x) + iν+1Iν+1(x) = 2ν
ix iν Iν(x),
which simpliﬁes to
Iν−1(x) −Iν+1(x) = 2ν
x Iν(x).
(14.104)

682
Chapter 14 Bessel Functions
2.4
2.0
1.6
1.2
0.8
0.4
1
2
3
x
K0 K1
I1
I0
FIGURE 14.12
Modiﬁed Bessel functions.
In a similar fashion, Eq. (14.8) transforms into
Iν−1(x) + Iν+1(x) = 2I ′
ν(x).
(14.105)
The above analysis is also the topic of Exercise 14.1.14.
Second Solution Kν
As already pointed out we have but one independent solution when ν is an integer, exactly
as for the Bessel functions Jν. The choice of a second, independent solution of Eq. (14.97)
is essentially a matter of convenience. The second solution given here is selected on the
basis of its asymptotic behavior, which we examine in the next section. The confusion of
choice and notation for this solution is perhaps greater than anywhere else in this ﬁeld.5
There is also no universal nomenclature; the Kν are sometimes referred to as Whittaker
functions. Following AMS-55 (see Additional Readings for reference), we here deﬁne a
second solution in terms of the Hankel function H(1)
ν (x) as
Kν(x) ≡π
2 iν+1H(1)
ν (ix) = π
2 iν+1
Jν(ix) + iYν(ix)

.
(14.106)
5Discussion and comparison of notations will be found in Math. Tables Aids Comput. 1: 207–308 (1944) and in AMS-55
(see Additional Readings).

14.5 Modiﬁed Bessel Functions, Iν(x) and Kν(x)
683
The factor iν+1 makes Kν(x) real when x is real.6 Using Eqs. (14.57) and (14.99), we may
transform Eq. (14.106) to7
Kν(x) = π
2
I−ν(x) −Iν(x)
sinνπ
,
(14.107)
somewhat analogous to Eq. (14.57) for Yν(x). The choice of Eq. (14.106) as a deﬁnition
is somewhat unfortunate in that the function Kν(x) does not satisfy the same recurrence
relations as Iν(x). The recurrence formulas for the Kν are
Kν−1(x) −Kν+1(x) = −2ν
x Kν(x),
(14.108)
Kν−1(x) + Kν+1(x) = −2K ′
ν(x).
(14.109)
To avoid this discrepancy in the recurrence relations, some authors8 have included an
additional factor of cosνπ in the deﬁnition of Kν. This would permit Kν to satisfy the
same recurrence relations as Iν (see Exercise 14.5.8), but it has the disadvantage of making
Kν = 0 for ν = 1
2, 3
2, 5
2,....
The series expansion of Kν(x) follows directly from the series form of H(1)
ν (ix), pro-
viding that we choose the branch of lnix appropriately (see Exercise 14.5.9). Using
Eqs. (14.79) and (14.80), the lowest-order terms are then found to be
K0(x) = −ln x −γ + ln2 + ··· ,
(14.110)
Kν(x) = 2ν−10(ν)x−ν + ··· .
(14.111)
Because the modiﬁed Bessel function Iν is related to the Bessel function Jν, much as sinh
is related to sine, the modiﬁed Bessel functions Iν and Kν are sometimes referred to as
hyperbolic Bessel functions. K0 and K1 are shown in Fig. 14.12.
Integral Representations
I0(x) and K0(x) have the integral representations
I0(x) = 1
π
π
Z
0
cosh(x cosθ)dθ,
(14.112)
K0(x) =
∞
Z
0
cos(x sinht)dt =
∞
Z
0
cos(xt)dt
(t2 + 1)1/2 ,
x > 0.
(14.113)
Equation (14.112) may be derived from Eq. (14.20) for J0(x) or may be taken as a special
case of Exercise 14.5.14. The integral representation of K0, Eq. (14.113), is derived in
Section 14.6. A variety of other forms of integral representations (including ν ̸= 0) appear
6If ν is not an integer, Kν(z) has a branch point at z = 0 due to the presence of a fractional power; if ν = n, an integer, Kn(z)
has a branch point at z = 0 due to the term ln z. We normally identify Kn(z) as the branch that is real for real z.
7For integral index n we take the limit as ν →n.
8For example, Whittaker and Watson (see Additional Readings).

684
Chapter 14 Bessel Functions
in the exercises. These integral representations are useful in developing asymptotic forms
(Section 14.6) and in connection with Fourier transforms (Chapter 19).
Example 14.5.1
A GREEN’S FUNCTION
We wish to develop an expansion for the fundamental Green’s function for the Laplace
equation in cylindrical coordinates (ρ,ϕ, z). The deﬁning equation is
"
∂2
∂ρ2
1
+ 1
ρ1
∂
∂ρ1
+ 1
ρ2
1
∂2
∂ϕ2
1
+ ∂2
∂z2
1
#
G(r1,r2) = δ(ρ1 −ρ2) 1
ρ2
1
δ(ϕ1 −ϕ2)δ(z1 −z2).
(14.114)
We now write the Dirac delta function for the ϕ coordinate in the form corresponding to
Eq. (5.27):
δ(ϕ1 −ϕ2) = 1
2π
∞
X
m=−∞
eim(ϕ1−ϕ2).
For the z coordinate, we use the continuum limit of the above formula, or, equivalently,
the large-n limit of Eq. (1.155),
δ(z1 −z2) = 1
2π
∞
Z
−∞
eik(z1−z2)dk = 1
π
∞
Z
0
cosk(z1 −z2)dk.
We use the last form of the above equation so that k will never be negative.
We now expand G(r1,r2) as
G(r1,r2) =
1
2π2
X
m
∞
Z
0
dkgm(k,ρ1,ρ2)eim(ϕ1−ϕ2) cosk(z1 −z2).
(14.115)
For ϕ1 and ϕ2, this is simply an expansion in orthogonal functions; the dependence on
z1, z2, and k is actually an integral transform that will be more completely justiﬁed in
Chapter 20. For our present purposes, what is signiﬁcant is that we can apply the orthog-
onality properties of the expansion to ﬁnd that Eq. (14.114) will be satisﬁed if (for all
relevant values of k and m)
"
∂2
∂ρ2
1
+ 1
ρ1
∂
∂ρ1
−m2
ρ2
1
−k2
#
gm(k,ρ1,ρ2) = δ(ρ1 −ρ2).
(14.116)
We now have a one-dimensional (1-D) Green’s function problem for which the homoge-
neous equation can be identiﬁed as the modiﬁed Bessel equation, with solutions Im(kρ)
and Km(kρ). Keeping in mind that Im is regular at the origin, that Km is regular at inﬁnity,
and that the Green’s function we seek must be regular at both these limits, we write our
1-D axial Green’s function in the more explicit form
gm(kρ1,kρ2) = −Im(kρ<)Km(kρ>),
(14.117)

14.5 Modiﬁed Bessel Functions, Iν(x) and Kν(x)
685
where ρ< and ρ> are, respectively, the smaller and larger of ρ1 and ρ2. The coefﬁcient in
the above equation, −1, is evaluated according to Eq. (10.19), from

p(kρ)

K ′
m(kρ)Im(kρ) −I ′
m(kρ)Km(kρ)
 −1
.
The coefﬁcient p is from the differential equation, and has here the value kρ; the form
involving modiﬁed Bessel functions is their Wronskian, and has the value −1/kρ; that is
the topic of Exercise 14.5.11.
Given our explicit formula for gm, Eq. (14.115) assumes the ﬁnal form
G(r1,r2) =
1
2π2
X
m
∞
Z
0
dkgm(kρ1,kρ2)eim(ϕ1−ϕ2) cosk(z1 −z2).
(14.118)
This is the form quoted in Section 10.2.
■
Summary
To put the modiﬁed Bessel functions Iν(x) and Kν(x) in proper perspective, note that we
have introduced them here because:
•
These functions are solutions of the frequently encountered modiﬁed Bessel equation,
which arises in a variety of physically important problems,
•
Kν(x) will be found useful in determining the asymptotic behavior of all the Bessel
and modiﬁed Bessel functions (Section 14.6), and
•
Iν(x) and Kν(x) arise in our discussion of Green’s functions (Example 14.5.1).
Exercises
14.5.1
Show that e(x/2)(t+1/t)=
∞
X
n=−∞
In(x)tn, thus generating modiﬁed Bessel functions, In(x).
14.5.2
Verify the following identities
(a)
1 = I0(x) + 2P∞
n=1(−1)n I2n(x),
(b)
ex = I0(x) + 2P∞
n=1 In(x),
(c)
e−x = I0(x) + 2P∞
n=1(−1)n In(x),
(d)
cosh x = I0(x) + 2P∞
n=1 I2n(x),
(e)
sinh x = 2P∞
n=1 I2n−1(x).
14.5.3
(a)
From the generating function of Exercise 14.5.1 show that
In(x) =
1
2πi
I
e(x/2)(t+1/t) dt
tn+1 .

686
Chapter 14 Bessel Functions
(b)
For n = ν, not an integer, show that the preceding integral representation may be
generalized to
Iν(x) =
1
2πi
Z
C
e(x/2)(t+1/t) dt
tν+1 .
The contour C is the same as that for Jν(x) (Fig. 14.5).
14.5.4
For ν > −1
2 show that Iν(z) may be represented by
Iν(z) =
1
π1/20(ν + 1
2)
 z
2
ν
π
Z
0
e±z cosθ sin2ν θ dθ
=
1
π1/20(ν + 1
2)
 z
2
ν
1
Z
−1
e±zp(1 −p2)ν−1/2 dp
=
2
π1/20(ν + 1
2)
 z
2
ν
π/2
Z
0
cosh(z cosθ)sin2ν θ dθ.
14.5.5
The cylindrical cavity depicted in Fig. 14.4 has radius a and height h. For this exercise,
the end caps z = 0 and h are at zero potential, while the cylindrical wall ρ = a has a
potential of functional form V = V (ϕ, z).
(a)
Show that the electrostatic potential 8(ρ,ϕ, z) has the functional form
8(ρ,ϕ, z) =
∞
X
m=0
∞
X
n=1
Im(knρ)(amn sinmϕ + bmn cosmϕ)sinknz,
where kn = nπ/h.
(b)
Show that the coefﬁcients amn and bmn are given by
amn
bmn

=
2 −δm0
πlIm(kna)
2π
Z
0
lZ
0
V (ϕ, z)
sinmϕ
cosmϕ

sinknzdzdϕ.
Hint. Expand V (ϕ, z) as a double series and use the orthogonality of the trigonometric
functions.
14.5.6
Verify that Kν(x) as deﬁned in Eq. (14.106) is equivalent to
Kν(x) = π
2
I−ν(x) −Iν(x)
sinνπ
and from this show that
Kν(x) = K−ν(x).

14.5 Modiﬁed Bessel Functions, Iν(x) and Kν(x)
687
14.5.7
Show that Kν(x) satisﬁes the following recurrence relations:
Kν−1(x) −Kν+1(x) = −2ν
x Kν(x),
Kν−1(x) + Kν+1(x) = −2K ′
ν(x).
Note. These differ from the recurrence relations for Iν.
14.5.8
If Kν = eνπi Kν, show that Kν satisﬁes the same recurrence relations as Iν.
14.5.9
Show that when K0 is evaluated from its series expansion about x = 0, the formula
given as Eq. (14.110) only follows if a speciﬁc branch of its logarithmic term is chosen.
14.5.10
For ν > −1
2 show that Kν(z) may be represented by
Kν(z) =
π1/2
0(ν + 1
2)
 z
2
ν
∞
Z
0
e−z cosht sinh2ν t dt,
−π
2 < arg z < π
2
=
π1/2
0(ν + 1
2)
 z
2
ν
∞
Z
1
e−zp(p2 −1)ν−1/2dp.
14.5.11
Show that Iν(x) and Kν(x) satisfy the Wronskian relation
Iν(x)K ′
ν(x) −I ′
ν(x)Kν(x) = −1
x .
14.5.12
Verify that the coefﬁcient in the axial Green’s function of Eq. (14.117) is −1.
14.5.13
If r = (x2 + y2)1/2, prove that
1
r = 2
π
∞
Z
0
cos(xt)K0(yt)dt.
This is a Fourier cosine transform of K0.
14.5.14
Derive the integral representation
In(x) = 1
π
π
Z
0
ex cosθ cos(nθ)dθ.
Hint. Start with the corresponding integral representation of Jn(x). Equation (14.112)
is a special case of this representation.
14.5.15
Show that
K0(z) =
∞
Z
0
e−z coshtdt
satisﬁes the modiﬁed Bessel equation. How can you establish that this form is linearly
independent of I0(z)?

688
Chapter 14 Bessel Functions
14.5.16
The cylindrical cavity of Exercise 14.5.5 has along the cylinder walls the potential walls:
V (z) =



100 z
h ,
100

1 −z
h

,
0 ≤z
h ≤1/2,
1/2 ≤z
h ≤1.
With the radius-height ratio a/h = 0.5, calculate the potential for z/h = 0.1(0.1)0.5
and ρ/a = 0.0(0.2)1.0.
Check value. For z/h = 0.3 and ρ/a = 0.8, V = 26.396.
14.6
ASYMPTOTIC EXPANSIONS
Frequently in physical problems there is a need to know how a given Bessel or modiﬁed
Bessel function behaves for large values of the argument, that is, its asymptotic behavior.
This is one occasion when computers are not very helpful. One possible approach is to
develop a power-series solution of the differential equation, but now using negative pow-
ers. This is Stokes’ method, illustrated in Exercise 14.6.10. The limitation is that starting
from some positive value of the argument (for convergence of the series), we do not know
what mixture of solutions or multiple of a given solution we have. The problem is to relate
the asymptotic series (useful for large values of the variable) to the power-series or related
deﬁnition (useful for small values of the variable). This relationship can be established
is various ways, one of which is to introduce a suitable integral representation whose
asymptotic behavior can be studied by application of the method of steepest descents,
Section 12.7.
We start this process with a study of the Hankel functions, for which a contour integral
representation was introduced in Section 14.4.
Asymptotic Forms of Hankel Functions
In Section 14.4 it was shown that the Hankel functions, which satisfy Bessel’s equation,
may be deﬁned by the contour integrals
H(1)
ν (t) = 1
πi
Z
C1
e(t/2)(z−1/z) dz
zν+1 ,
(14.119)
H(2)
ν (t) = 1
πi
Z
C2
e(t/2)(z−1/z) dz
zν+1 ,
(14.120)
where C1 and C2 are the contours shown in Fig. 14.10. We desire formulas based on these
representations for the asymptotic behavior of the Hankel functions at large positive t.
The direct and exact evaluation of these integrals appears to be nearly impossible, but
the situation does have features permitting us to use the method of steepest descents to
make an asymptotic evaluation. Referring to the exposition of that method in Section 12.7,

14.6 Asymptotic Expansions
689
we have the approximate evaluation
Z
C
g(z,t)ew(z,t)dz ≈g(z0,t)ew(z0,t)eiθ
s
2π
|w′′(z0,t)|,
(14.121)
where the contour C passes through a saddle point at z = z0 and
θ = −arg(w′′(z0,t))
2
+
π
2 or 3π
2

(14.122)
is a phase arising from the direction of passage through the saddle point.
We regard the common integrand of Eqs. (14.119) and (14.120) as possessing a slowly
varying factor g(z) = z−ν−1 and an exponential ew with w = (t/2)(z −z−1), and seek
saddle points by ﬁnding the zeros of
w′ = t
2

1 + 1
z2

.
(14.123)
Solving the above equation, we identify the two saddle points z0 = +i and z0 = −i.
Limiting attention to H(1)
ν (t), we see that we can deform the contour C1 so that it passes
through the saddle point at z0 = i; there is neither the need nor the possibility to deform
this contour to pass through z0 = −i. Thus, at the saddle point, we have
w(+i) = it,
w′′(+i) = −t
z3
0

z0=i
= −it.
(14.124)
The argument of w′′(z0) is −π/2, so the possible values of the phase θ (the direction of
descent from the saddle point) are 3π/4 and 7π/4. We must choose θ = 3π/4 since we
cannot get into position to cross the saddle point in the direction θ = 7π/4 = −π/4 without
ﬁrst crossing a region where the integrand is larger in absolute value than its value at the
saddle point.
We now have all the information needed to use Eq. (14.121) to estimate the integral.
The result is
H(1)
ν (t) ≈1
πi e(iπ/2)(−ν−1)e3iπ/4eit
r
2π
t
≈
r
2
πt ei(t−νπ/2−π/4).
(14.125)
This is the leading term of the asymptotic expansion of the Hankel function H(1)
ν (t) for
large t. The other Hankel function can be treated similarly, but using the saddle point at
z = −i, with result
H(2)
ν (t) ≈
r
2
πt e−i(t−νπ/2−π/4).
(14.126)
Equations (14.125) and (14.126) permit us to obtain the leading terms in the asymp-
totic behavior of all the Bessel and modiﬁed Bessel functions. In particular, inserting the

690
Chapter 14 Bessel Functions
asymptotic form for H(1)(ix) into Eq. (14.106), which deﬁnes Kν(x), we ﬁnd
Kν(x) ∼π
2 iν+1
r
2
iπx e−xet−νπ/2−π/4),
∼
r π
2x e−x.
(14.127)
Another solution to the modiﬁed Bessel equation can be obtained from H(2)(ix); its
asymptotic behavior will be proportional to e+x. Combining the present observations with
Eqs. (14.100), (14.110), and (14.111), we can conclude that:
1.
The modiﬁed Bessel function Kν(x) will be irregular at x = 0 as given by Eqs. (14.110)
or (14.111), and will decay exponentially at large x;
2.
The modiﬁed Bessel function Iν(x) will (for ν ≥0) be ﬁnite at the origin, as given by
Eq. (14.100), and will increase exponentially at large x.
Rather than developing additional asymptotic forms from Eq. (14.127), we ﬁnd it more
interesting to obtain more complete asymptotic expansions by use of a particular integral
representation of Kν.
Expansion of an Integral Representation for Kν
Here we start from the integral representation
Kν(z) =
π1/2
0(ν + 1
2)
 z
2
ν
∞
Z
1
e−zx(x2 −1)ν−1/2dx,
ν > −1
2.
(14.128)
For the present let us take z to be real, although Eq. (14.128) may be established for
−π/2 < arg z < π/2 (i.e., for ℜe(z) > 0).
Before using Eq. (14.128) we need to verify that (1) the form claimed to be Kν(z)
satisﬁes the modiﬁed Bessel equation, (2) that it has the small-z behavior required for
Kν, and (3) that it has the required exponentially decaying asymptotic value. These three
features sufﬁce to establish the validity of Eq. (14.128).
The fact that Eq. (14.128) is a solution of the modiﬁed Bessel equation may be veriﬁed
by direct substitution into Eq. (14.97). After some manipulation, we obtain
zν+1
∞
Z
1
d
dx

e−zx(x2 −1)ν+1/2
dx = 0,
which transforms the combined integrand into the derivative of a function that vanishes at
both endpoints.

14.6 Asymptotic Expansions
691
We next consider how Eq. (14.128) behaves for small z. We proceed by substituting
x = 1 + t/z:
π1/2
0(ν + 1
2)
 z
2
ν
∞
Z
1
e−zx(x2 −1)ν−1/2dx
=
π1/2
0(ν + 1
2)
 z
2
ν
e−z
∞
Z
0
e−t
 t2
z2 + 2t
z
ν−1/2 dt
z
=
π1/2
0(ν + 1
2)
e−z
2νzν
∞
Z
0
e−tt2ν−1

1 + 2z
t
ν−1/2
dt.
(14.129)
This substitution has changed the limits of integration to a more convenient range and has
isolated the negative exponential dependence e−z. The integral in Eq. (14.129) may now
(for ν > 0) be evaluated for z = 0 to yield 0(2ν). Then, using the duplication formula,
Eq. (13.27), we have
lim
z→0 Kν(z) = 0(ν)2ν−1
zν
,
ν > 0.
(14.130)
Equation (14.130) agrees with Eq. (14.111), showing that Eq. (14.128) has the proper
small-z behavior to represent Kν. Note that for ν = 0, Eq. (14.128) diverges logarithmi-
cally at z = 0 and the veriﬁcation of its scale requires a different approach, which is the
topic of Exercise 14.6.4.
Finally, to complete the identiﬁcation of Eq. (14.128) with Kν, we need to verify that it
decays exponentially at large z. That feature will be a by-product of our main interest here,
which is to develop an asymptotic series for Kν(z). We do so by rewriting Eq. (14.129) as
Kν(z) =
r π
2z
e−z
0(ν + 1
2)
∞
Z
0
e−ttν−1/2

1 + t
2z
ν−1/2
dt.
(14.131)
We next expand (1 + t/2z)ν−1/2 by the binomial theorem and interchange the summation
and integration (valid for the asymptotic series we plan to obtain), reaching
Kν(z) =
r π
2z
e−z
0(ν + 1
2)
∞
X
r=0
ν −1
2
r

(2z)−r
∞
Z
0
e−ttν+r−1/2dt
=
r π
2z e−z
∞
X
r=0
0(ν + r + 1
2)
r!0(ν −r + 1
2)
(2z)−r.
(14.132)
Equation (14.132) can now be rearranged to
Kν(z) ∼
r π
2z e−z

1 + (4ν2 −12)
1!8z
+ (4ν2 −12)(4ν2 −32)
2!(8z)2
+ ···

.
(14.133)

692
Chapter 14 Bessel Functions
Equation (14.133) yields the anticipated exponential dependence, conﬁrming that
Eq. (14.128) actually represents Kν.
Although the integral of Eq. (14.128), integrating along the real axis, was convergent only
for −π/2 < arg z < π/2, Eq. (14.133) may be extended to −3π/2 < arg z < 3π/2. Consid-
ered as an inﬁnite series, Eq. (14.133) is actually divergent. However, this series is asymp-
totic, in the sense that for large enough z, Kν(z) may be approximated to any ﬁxed degree of
accuracy with a small number of terms. Compare Section 12.6 for a deﬁnition and discus-
sion of asymptotic series. The asymptotic character arises because our binomial expansion
was valid only for t < 2z but we integrated t out to inﬁnity. The exponential decrease of the
integrand has prevented a disaster, but the series is only asymptotic and not convergent. By
Table 7.1, z = ∞is an essential singularity of the Bessel (and modiﬁed Bessel) equations.
Fuchs’ theorem does not guarantee a convergent series and we did not get one.
It is convenient to rewrite Eq. (14.133) as
Kν(z) =
r π
2z e−z
Pν(iz) + i Qν(iz)

,
(14.134)
where
Pν(z) ∼1 −(µ −1)(µ −9)
2!(8z)2
+ (µ −1)(µ −9)(µ −25)(µ −49)
4!(8z)4
−···,
(14.135)
Qν(z) ∼µ −1
1!(8z) −(µ −1)(µ −9)(µ −25)
3!(8z)3
+ ···,
(14.136)
and µ = 4ν2. It should be noted that although Pν(z) of Eq. (14.135) and Qν(z) of
Eq. (14.136) have alternating signs, the series for Pν(iz) and Qν(iz) in Eq. (14.134) have
all positive signs. Finally, note that for z large, Pν dominates.
Additional Asymptotic Forms
We started our detailed study of asymptotic behavior with Kν because, with its properties
in hand, we can deduce the asymptotic expansions of the other members of the family of
Bessel-related functions.
1.
Rearranging the deﬁnition of Kν to
H(1)
ν (x) = 2
π e−(iπ/2)(ν+1)Kν(−ix),
(14.137)
we have
H(1)
ν (z) =
r
2
πz exp

i

z −

ν + 1
2
 π
2

Pν(z) + i Qν(z)

,
(14.138)
which although originally derived for real values of −ix, can be analytically continued
into the larger range −π < arg z < 2π.
2.
The second Hankel function is just (for real arguments) the complex conjugate of the
ﬁrst, and therefore
H(2)
ν (z) =
r
2
πz exp

−i

z −

ν + 1
2
 π
2

Pν(z) −i Qν(z)

,
(14.139)
valid for −2π < arg z < π.

14.6 Asymptotic Expansions
693
3.
Since Jν(z) is the real part of H(1)
ν (z) for real z,
Jν(z) =
r
2
πz

Pν(z)cos

z −

ν + 1
2
 π
2

−Qν(z)sin

z −

ν + 1
2
 π
2

,
(14.140)
valid for −π < arg z < π.
4.
The Neumann function is the imaginary part of H(1)
ν (z) for real z, or
Yν(z) =
r
2
πz

Pν(z)sin

z −

ν + 1
2
 π
2

+ Qν(z)cos

z −

ν + 1
2
 π
2

,
(14.141)
also valid for −π < arg z < π.
5.
Finally, the modiﬁed Bessel function Iν(z) is given by
Iν(z) = i−ν Jν(iz),
(14.142)
so
Iν(z) =
ez
√2πz

Pν(iz) −i Qν(iz)

,
(14.143)
valid for −π/2 < arg z < π/2.
Properties of the Asymptotic Forms
Having derived the asymptotic forms of the various Bessel functions, it is opportune to
note their essential characteristics. Remembering that in the limit of large z, Pν approaches
unity while Qν ∼1/z, we see that at large z, all the Bessel functions have leading terms
with a 1/z1/2 dependence, multiplied by either a real or complex exponential. The modiﬁed
functions Kν and Iν, respectively, contain decreasing and increasing exponentials, while
the ordinary Bessel functions Jν and Yν have leading terms with sinusoidal oscillation
(damped by the z−1/2 factor). When multiplied by a time factor e±iωt, the Hankel functions
can describe incoming and outgoing traveling waves.
Looking at the oscillatory functions Jν, Yν, H(i)
ν
in more detail, we see that exact sinu-
soidal behavior is only reached in the limit of large z, as for ﬁnite z the terms involving Qν
will to some extent alter the periodicity. The reader may wish to compare the positions of
the zeros of Jn in Table 14.1 with those predicted by its leading term, namely the zeros of
cos

z −

n + 1
2
 π
2

.
We see that Jn behaves asymptotically like a phase-shifted cosine function, with the phase
shift a function of n. The asymptotic form of Yn will be that of a sine function, with (for
the same n) the same phase shift. This causes the zeros of Jn and Yn for large z to alternate,
as we saw for J0 and Y0 in Fig. 14.8.
The asymptotic behavior of the two solutions to a problem described by ordinary or
modiﬁed Bessel functions may be sufﬁcient to eliminate immediately one of these func-
tions as a solution for a physical problem. This observation may enable us to use the behav-
ior at z = ∞as well as that at z = 0 to restrict the functional forms we need to consider.

694
Chapter 14 Bessel Functions
5.60
4.80
4.00
3.20
2.40
1.60
0.80
0.00
−1.20
−0.80
J0(x)
x
−0.40
0.00
0.40
0.80
1.20
2 cos (x −
πx
4
π )
FIGURE 14.13
Asymptotic approximation of J0(x).
Finally, we note that the asymptotic series Pν(z) and Qν(z), Eqs. (14.135) and (14.136),
terminate for ν = ±1/2, ±3/2,... and become polynomials (in negative powers of z). For
these special values of ν the asymptotic approximations become exact solutions.
It is of some interest to consider the accuracy of the asymptotic forms, taking for exam-
ple just the ﬁrst term
Jn(x) ≈
r
2
πx cos

x −

n + 1
2
π
2

.
(14.144)
Clearly, the condition for Eq. (14.144) to be accurate is that the sine term of Eq. (14.140)
be negligible; that is,
8x ≫4n2 −1.
(14.145)
In Fig. 14.13 we plot J0(x) and the leading term of its asymptotic approximation. The
agreement is nearly quantitative for x > 5. However, for n or ν > 1 the asymptotic region
may be far out.
Another use of the asymptotic formulas is to establish the constants in Wronskian for-
mulas, where we know the Wronskian of any two Bessel functions of argument x has a
1/x functional dependence but with a premultiplying constant that depends on the Bessel
functions involved.
Example 14.6.1
CYLINDRICAL TRAVELING WAVES
As an illustration of a problem in which we have chosen a speciﬁc Bessel function because
of its asymptotic properties, consider a two-dimensional (2-D) wave problem similar to the
vibrating circular membrane of Exercise 14.1.24. Now imagine that the waves are gener-
ated at r = 0 and move outward to inﬁnity. We replace our standing waves by traveling

14.6 Asymptotic Expansions
695
ones. The differential equation remains the same, but the boundary conditions change. We
now demand that for large r the wave behave like
U ∼ei(kr−ωt),
(14.146)
to describe an outgoing wave with wavelength 2π/k. We assume, for simplicity, that there
is no azimuthal dependence, so we have circular symmetry, implying m = 0. The Bessel
function of order zero with this asymptotic dependence is H(1)
0 (kr), as can be seen from
Eq. (14.138). This boundary condition at inﬁnity then determines our wave solution as
U(r,t) = H(1)
0 (kr)e−iωt.
(14.147)
This solution diverges as r →0, which is the behavior to be expected with a source at the
origin.
■
Exercises
14.6.1
Determine the asymptotic dependence of the modiﬁed Bessel functions Iν(x), given
Iν(x) =
1
2πi
Z
C
e(x/2)(t+1/t) dt
tν+1 .
The contour starts and ends at t = −∞, encircling the origin in a positive sense. There
are two saddle points. Only the one at z = +1 contributes signiﬁcantly to the asymptotic
form.
14.6.2
Determine the asymptotic dependence of the modiﬁed Bessel function of the second
kind, Kν(x), by using
Kν(x) = 1
2
∞
Z
0
e(−x/2)(s+1/s) ds
s1−ν .
14.6.3
Verify that the integral representations
In(z) = 1
π
π
Z
0
ez cost cos(nt)dt,
Kν(z) =
∞
Z
0
e−z cosht cosh(νt)dt,
ℜe(z) > 0,
satisfy the modiﬁed Bessel equation by direct substitution into that equation. How can
you check the normalization?
14.6.4
(a)
Show that when Kν is deﬁned by Eq. (14.128),
dK0(z)
dz
= −K1(z).

696
Chapter 14 Bessel Functions
t plane
(2)
(1)
−1
1
FIGURE 14.14
Modiﬁed Bessel function contours.
(b)
Show that the indeﬁnite integral of −K1(x) as deﬁned by Eq. (14.128) has in
the limit of small z the value −ln z + C, and therefore, by comparison with
Eq. (14.110), that K0 as deﬁned by Eq. (14.128) has the correct normalization.
14.6.5
Verify that Eq. (14.132) can be rearranged to the form given as Eq. (14.133).
14.6.6
(a)
Show that
y(z) = zν
Z
e−zt(t2 −1)ν−1/2dt
satisﬁes the modiﬁed Bessel equation, provided the contour is chosen so that
e−zt(t2 −1)ν+1/2
has the same value at the initial and ﬁnal points of the contour.
(b)
Verify that the contours shown in Fig. 14.14 are suitable for this problem.
14.6.7
Use the asymptotic expansions to verify the following Wronskian formulas:
(a)
Jν(x)J−ν−1(x) + J−ν(x)Jν+1(x) = −2sinνπ/πx,
(b)
Jν(x)Nν+1(x) −Jν+1(x)Nν(x) = −2/πx,
(c)
Jν(x)H(2)
ν−1(x) −Jν−1(x)H(2)
ν (x) = 2/iπx,
(d)
Iν(x)K ′
ν(x) −I ′
ν(x)Kν(x) = −1/x,
(e)
Iν(x)Kν+1(x) + Iν+1(x)Kν(x) = 1/x.
14.6.8
Verify that the Green’s function for the 2-D Helmholtz equation (operator ∇2 + k2)
with outgoing-wave boundary conditions is
G(ρ1,ρ2) = i
4 H(1)
0 (k|ρ1 −ρ2|).
Hint. H(1)
0 (kρ) is known to be an outgoing-wave solution to the homogeneous
Helmholtz equation.
14.6.9
From the asymptotic form of Kν(z), Eq. (14.134), derive the asymptotic form of
H(1)
ν (z), Eq. (14.138). Note particularly the phase, (ν + 1
2)π/2.
14.6.10
Apply Stokes’ method for obtaining an asymptotic expansion for the Hankel function
H(1)
ν
as follows:

14.6 Asymptotic Expansions
697
(a)
Replace the Bessel function in Bessel’s equation by x−1/2y(x) and show that y(x)
satisﬁes
y′′(x) +
 
1 −ν2 −1
4
x2
!
y(x) = 0.
(b)
Develop a power-series solution with negative powers of x starting with the
assumed form
y(x) = eix
∞
X
n=0
anx−n.
Obtain the recurrence relation giving an+1 in terms of an. Check your result
against the asymptotic series, Eq. (14.138).
(c)
From Eq. (14.125), determine the initial coefﬁcient, a0.
14.6.11
Using the method of steepest descents, evaluate the second Hankel function given by
H(2)
ν (t) = 1
πi
Z
C2
e(t/2)(z−1/z) dz
zν+1 ,
with contour C2 as shown in Fig. 14.10.
ANS.
H(2)
ν (t) ≈
r
2
πt e−i(t−π/4−νπ/2).
14.6.12
(a)
In applying the method of steepest descents to the Hankel function H(1)
ν (t), show
that w(z,t), which appears in Eq. (14.121), satisﬁes
ℜe[w(z,t)] < ℜe[w(z0,t)] = 0
for z on the contour C1 (Fig. 14.10) but away from the point z = z0 = i.
(b)
For general values of z = reiθ, show that
ℜe[w(z,t)] > 0
for
0 < r < 1,



π
2 < θ ≤π
−π ≤θ < π
2
and
ℜe[w(z,t)] < 0
for
r > 1,
−π
2 < θ < π
2 .
Your demonstration veriﬁes that the distribution of the sign of w is as shown
schematically in Fig. 14.15.
(c)
Explain why the contour C1 (Fig. 14.10) cannot be deformed to go through both
saddle points, and why it may not go through the saddle point at −i if it is to end
at z = −∞with argument +π.
14.6.13
Calculate the ﬁrst 15 partial sums of P0(x) and Q0(x), Eqs. (14.135) and (14.136).
Let x vary from 4 to 10 in unit steps. Determine the number of terms to be retained
for maximum accuracy and the accuracy achieved as a function of x. Speciﬁcally, how
small may x be without raising the error above 3 × 10−6?
ANS.
xmin = 6.

698
Chapter 14 Bessel Functions
+
+
+
+
−
−
−
−
y
x
FIGURE 14.15
Sign of w(z,t), occurring in Eq. (14.121), for integral representation of
Hankel functions.
14.7
SPHERICAL BESSEL FUNCTIONS
In Section 9.4 we discussed the separation of the Helmholtz equation in spherical coordi-
nates. We showed there that in the oft-occurring case that the boundary conditions of the
problem have spherical symmetry, the radial equation has the form given in Eq. (9.80),
namely,
r2 d2R
dr2 + 2r d R
dr +

k2r2 −l(l + 1)

R = 0.
(14.148)
We remind the reader that the parameter k is that from the original Helmholtz equation,
while l(l + 1) is the separation constant associated with solutions of the angular equations
identiﬁed by the index l (which is required by the boundary conditions to be an integer).
In Section 9.4 we went on to discuss the fact that the substitution
R(kr) = Z(kr)
(kr)1/2
(14.149)
permits us to rewrite Eq. (14.148) as
r2 d2Z
dr2 + r dZ
dr +
"
k2r2 −

l + 1
2
2#
Z = 0,
(14.150)
which we identiﬁed in Eq. (9.84) as Bessel’s equation of order l + 1
2.
We can now identify the general solution Z(kr) as a linear combination of Jl+1/2(kr)
and Yl+1/2(kr), which in turn means that we can write R(kr) in terms of these Bessel
functions of half-integral order, illustrated (for Jl+1/2) by
R(kr) =
C
√
kr
Jl+1/2(kr).

14.7 Spherical Bessel Functions
699
Since the R(kr) describe radial functions in spherical coordinates, they are termed spheri-
cal Bessel functions. Note also that since Eq. (14.148) is homogeneous, we are free to
deﬁne our spherical Bessel functions at any scale; the scale ordinarily used is that intro-
duced in the next subsection.
Deﬁnitions
We deﬁne our spherical Bessel functions by the following equations. It is not ordinarily
useful to introduce spherical Bessel functions with indices that are not integers, so we
assume the index n to be integral (but not necessarily nonnegative).
jn(x) =
r π
2x Jn+1/2(x),
yn(x) =
r π
2x Yn+1/2(x),
(14.151)
h(1)
n (x) =
r π
2x H(1)
n+1/2(x) = jn(x) + iyn(x),
h(2)
n (x) =
r π
2x H(2)
n+1/2(x) = jn(x) −iyn(x).
Referring to the deﬁnition of Yn+1/2, we see that
Yn+1/2(x) = cos(n + 1
2)π Jn+1/2(x) −J−n−1/2(x)
sin(n + 1
2)π
= (−1)n+1J−n−1
2
(x),
which means that
yn(x) = (−1)n+1 j−n−1(x).
(14.152)
These spherical Bessel functions (Figs. 14.16 and 14.17) can be expressed in series form.
Using Eq. (14.6), we have initially
jn(x) =
r π
2x
∞
X
s=0
(−1)s
s!0(s + n + 3
2)
x
2
2s+n+1/2
.
(14.153)
Writing
0(s + n + 3
2) = 0(n + 3
2)(n + 3
2)s,
(14.154)
where (..)s is a Pochhammer symbol, deﬁned in Eq. (1.72), we can bring Eq. (14.153) to
the form
jn(x) =
r π
2x
x
2
n+1/2
1
0(n + 3
2)
∞
X
s=0
(−1)s
s!(n + 3
2)s
x
2
2s
=
xn
(2n + 1)!!
∞
X
s=0
(−1)s
s!(n + 3
2)s
x
2
2s
.
(14.155)

700
Chapter 14 Bessel Functions
x
0.6
0.5
0.4
0.3
0.2
0.1
0
2
4
6
8
10
12
14
−0.1
−0.2
−0.3
1.0
j0(x)
j1(x)
j2(x)
FIGURE 14.16
Spherical Bessel functions.
x
0.3
0.2
0.1
0
1
3
5
7
11
13
−0.1
−0.2
−0.3
−0.4
y1(x)
y0(x)
y2(x)
9
FIGURE 14.17
Spherical Neumann functions.

14.7 Spherical Bessel Functions
701
We reached the last line of Eq. (14.155) by writing 0(n + 3
2) using the double factorial
notation (compare with Exercise 13.1.14).
If we now develop a series expansion for yn(x) by the same method that was used for
jn(x), but starting from Eq. (14.152), we get
yn(x) = −(2n −1)!!
xn+1
∞
X
s=0
(−1)s
s!( 1
2 −n)s
x
2
2s
.
(14.156)
The spherical Bessel functions are oscillatory, as can be seen from the graphs in
Figs. 14.16 and 14.17. Note that jn(x) are regular at x = 0, with limiting behavior there
proportional to xn. The yn are all irregular at x = 0, approaching that point as x−n−1.
The inﬁnite series in Eqs. (14.155) and (14.156) can be evaluated in closed form (but
with increasing difﬁculty as n increases). For the special case n = 0, we can substitute into
Eq. (14.155) s! = 2−s(2s)!! and (3/2)s = 2−s(2s + 1)!!, reaching
j0(x) =
∞
X
s=0
(−1)s22s
(2s)!!(2s + 1)!!
x
2
2s
=
∞
X
s=0
(−1)s
(2s + 1)!x2s
= sin x
x
.
(14.157)
A similar treatment of the expansion for y0 yields
y0(x) = −cos x
x
.
(14.158)
From the deﬁnition of the spherical Hankel functions, Eq. (14.151), we also have
h(1)
0 (x) = 1
x (sin x −i cos x) = −i
x eix,
(14.159)
h(2)
0 (x) = 1
x (sin x + i cos x) = i
x e−ix.
(14.160)
Since we anticipate the availability of recurrence formulas for the spherical Bessel func-
tions, and since y0 is just −j−1, we expect all the jn and yn to be linear combinations of
sines and cosines. In fact, the recurrence formulas are good ways of getting these functions
for small n. However, we identify here an alternate approach, which depends on the fact,
noted in Section 14.6, that the asymptotic expansion for the Hankel functions actually ter-
minates when the order is a half-integer, thereby yielding exact, closed expressions. We
start from
h(1)
n (x) =
r π
2x H(1)
n+1/2(x)
= (−i)n+1 eix
x

Pn+1/2(x) + i Qn+1/2(x)

,
(14.161)

702
Chapter 14 Bessel Functions
where Pν and Qν are given by Eqs. (14.135) and (14.136). Now, Pn+1/2 and Qn+1/2 are
polynomials, and we can bring Eq. (14.161) to the form
h(1)
n (x) = (−i)n+1 eix
x
n
X
s=0
is
s!(8x)s
(2n + 2s)!!
(2n −2s)!!
= (−i)n+1 eix
x
n
X
s=0
is
s!(2x)s
(n + s)!
(n −s)!.
(14.162)
For real x, jn(x) is the real part of this, yn(x) the imaginary part, and h(2)
n (x) the complex
conjugate. Speciﬁcally,
h(1)
1 (x) = eix

−1
x −i
x2

,
(14.163)
h(1)
2 (x) = eix
 i
x −3
x2 −3i
x3

,
(14.164)
j1(x) = sin x
x2
−cos x
x
,
(14.165)
j2(x) =
 3
x3 −1
x

sin x −3
x2 cos x,
(14.166)
y1(x) = −cos x
x2
−sin x
x
,
(14.167)
y2(x) = −
 3
x3 −1
x

cos x −3
x2 sin x.
(14.168)
Recurrence Relations
The recurrence relations to which we now turn provide a convenient way of developing
the higher-order spherical Bessel functions. These recurrence relations may be derived
from the power-series expansions, but it is easier to substitute into the known recurrence
relations, Eqs. (14.8) and (14.9). This gives
fn−1(x) + fn+1(x) = 2n + 1
x
fn(x),
(14.169)
nfn−1(x) −(n + 1) fn+1(x) = (2n + 1) f ′
n(x).
(14.170)
Rearranging these relations, or substituting into Eqs. (14.10) and (14.11), we obtain
d
dx [xn+1 fn(x)] = xn+1 fn−1(x),
(14.171)
d
dx [x−n fn(x)] = −x−n fn+1(x).
(14.172)
In these equations fn may represent jn, yn, h(1)
n , or h(2)
n .

14.7 Spherical Bessel Functions
703
By mathematical induction (Section 1.4) we may establish the Rayleigh formulas:
jn(x) = (−1)nxn
 1
x
d
dx
n sin x
x

,
(14.173)
yn(x) = −(−1)nxn
1
x
d
dx
n cos x
x

,
(14.174)
h(1)
n (x) = −i(−1)nxn
 1
x
d
dx
n eix
x

,
(14.175)
h(2)
n (x) = i(−1)nxn
1
x
d
dx
n e−ix
x

.
(14.176)
Limiting Values
For x ≪1,9 Eqs. (14.155) and (14.156) yield
jn(x) ≈
xn
(2n + 1)!!,
(14.177)
yn(x) ≈−(2n −1)!!
xn+1
.
(14.178)
The limiting values of the spherical Hankel functions for small x go as ±iyn(x).
The asymptotic values of jn, yn, h(1)
n , and h(2)
n
may be obtained from the asymptotic
forms of the corresponding Bessel functions, as given in Section 14.6. We ﬁnd
jn(x) ∼1
x sin

x −nπ
2

,
(14.179)
yn(x) ∼−1
x cos

x −nπ
2

,
(14.180)
h(1)
n (x) ∼(−i)n+1 eix
x = −i ei(x−nπ/2)
x
,
(14.181)
h(2)
n (x) ∼in+1 e−ix
x
= i e−i(x−nπ/2)
x
.
(14.182)
The condition for these spherical Bessel forms is that x ≫n(n + 1)/2. From these asymp-
totic values we see that jn(x) and yn(x) are appropriate for a description of standing
spherical waves; h(1)
n (x) and h(2)
n (x) correspond to traveling spherical waves. If the time
dependence for the traveling waves is taken to be e−iωt, then h(1)
n (x) yields an outgoing
traveling spherical wave, and h(2)
n (x) an incoming wave. Radiation theory in electromag-
netism and scattering theory in quantum mechanics provide many applications.
9The condition that the second term in the series be negligible compared to the ﬁrst is actually x ≪2[(2n + 2)(2n + 3)/
(n + 1)]1/2 for jn(x).

704
Chapter 14 Bessel Functions
Orthogonality and Zeros
We may take the orthogonality integral for the ordinary Bessel functions, Eqs. (11.49) and
(11.50),
a
Z
0
Jν

ανp
ρ
a

Jν

ανq
ρ
a

ρ dρ = a2
2

Jν+1(ανp)
 2δpq,
and rewrite it in terms of jn to obtain
a
Z
0
jn

αnp
r
a

jn

αnq
r
a

r2 dr = a3
2

jn+1(αnp)
 2δpq.
(14.183)
Here αnp is the p-th positive zero of jn.
Note that in contrast to the formula for the orthogonality of the Jν, Eq. (14.183) has the
weight factor r2, not r. This of course comes from the factors x−1/2 in the deﬁnition of
jn(x), but also has the effect that if the integration is construed as being over a spherical
volume rather than a linear interval, it is the factor corresponding to uniform weight of
all volume elements. (Remember that the weight ρ for the Jν integral produces uniform
weight if we construe the integration in that case as over the area within a circle.)
As for the ordinary Bessel functions, the functions that are orthogonal on (0,a) all sat-
isfy a Dirichlet boundary condition, with zeros at r = a. We therefore ﬁnd it useful to know
the values of the zeros of the jn. The ﬁrst few zeros for small n, and also the locations of
the zeros of j′
n, are listed in Table 14.2.
The following example illustrates a problem in which the zeros of the jn play an essential
role.
Table 14.2
Zeros of the Spherical Bessel Functions and Their First
Derivatives
Number
of zero
j0(x)
j1(x)
j2(x)
j3(x)
j4(x)
j5(x)
1
3.1416
4.4934
5.7635
6.9879
8.1826
9.3558
2
6.2832
7.7253
9.0950
10.4171
11.7049
12.9665
3
9.4248
10.9041
12.3229
13.6980
15.0397
16.3547
4
12.5664
14.0662
15.5146
16.9236
18.3013
19.6532
5
15.7080
17.2208
18.6890
20.1218
21.5254
22.9046
j′
0(x)
j′
1(x)
j′
2(x)
j′
3(x)
j′
4(x)
j′
5(x)
1
4.4934
2.0816
3.3421
4.5141
5.6467
6.7565
2
7.7253
5.9404
7.2899
8.5838
9.8404
11.0702
3
10.9041
9.2058
10.6139
11.9727
13.2956
14.5906
4
14.0662
12.4044
13.8461
15.2445
16.6093
17.9472
5
17.2208
15.5792
17.0429
18.4681
19.8624
21.2311

14.7 Spherical Bessel Functions
705
Example 14.7.1
PARTICLE IN A SPHERE
An illustration of the use of the spherical Bessel functions is provided by the problem of a
quantum mechanical particle of mass m in a sphere of radius a. Quantum theory requires
that the wave function ψ, describing our particle, satisfy the Schrödinger equation
−¯h2
2m ∇2ψ = Eψ,
(14.184)
subject to the conditions that (1) ψ(r) is ﬁnite for all 0 ≤r ≤a, and (2) ψ(a) = 0. This
corresponds to a square-well potential V = 0 for r ≤a, V = ∞for r > a. Here ¯h is
Planck’s constant divided by 2π. Equation (14.184) with its boundary conditions is an
eigenvalue equation; its eigenvalues E are the possible values of the particle’s energy.
Let us determine the minimum value of the energy for which our wave equation has an
acceptable solution. Equation (14.184) is Helmholtz’s equation, which after separation of
variables leads to the radial equation previously presented as Eq. (14.148):
d2R
dr2 + 2
r
d R
dr +

k2 −l(l + 1)
r2

R = 0,
(14.185)
with
k2 = 2mE/¯h2
(14.186)
and l (determined from the angular equation) a nonnegative integer. Comparing with
Eq. (14.150) and the deﬁnitions of the spherical Bessel functions, Eq. (14.151), we see
that the general solution to Eq. (14.185) is
R = Ajl(kr) + Byl(kr).
(14.187)
To satisfy the boundary conditions of the present problem, we must reject the solution yl
because it is singular at r = 0, and we must choose k such that jl(ka) = 0. This boundary
condition at r = a can be satisﬁed if
k ≡kli = αli
a ,
(14.188)
where αli is the ith positive zero of jl. From Eq. (14.186) we see that the smallest E
will correspond to the smallest acceptable k, which in turn corresponds to the smallest αli.
Thus, scanning Table 14.2, we identify the smallest αli as the ﬁrst zero of j0, a result which
we would expect after we have learned that the value l = 0 is associated with an angular
function with no kinetic energy.
We conclude this example by solving Eq. (14.186) for E with k assigned the value
α01/a = π/a10:
Emin = π2¯h2
2ma2 =
h2
8ma2 .
(14.189)
10Most of the entries in Table 14.2 are only accessible numerically, but the zeros of j0 are readily identiﬁed due to their simple
form, α0m = mπ.

706
Chapter 14 Bessel Functions
This example illustrates several features common to bound-state problems in quantum
mechanics. First, we see that for any ﬁnite sphere the particle will have a positive minimum
or zero-point energy. Second, we note that the particle cannot have a continuous range of
energy values; the energy is restricted to discrete values corresponding to the eigenvalues
of the Schrödinger equation. Third, the possible energies in this spherically symmetric
problem depend on l; as is evident from the table of zeros of jl, the minimum energy for a
given l increases with l. Finally, note that the orthogonality of the jl under the conditions
of this problem shows us that the eigenfunctions corresponding to the same l but different
i are orthogonal (with the weight factor corresponding to spherical polar coordinates). ■
We close this subsection with the observation that, in addition to orthogonality with
respect to the scaling (to bring zeros to a speciﬁed r value), the spherical Bessel functions
also possess orthogonality with respect to the indices:
∞
Z
−∞
jm(x) jn(x)dx = 0,
m ̸= n, m, n ≥0.
(14.190)
The proof is left as Exercise 14.7.12. If m = n (compare Exercise 14.7.13), we have
∞
Z
−∞
[ jn(x)]2 dx =
π
2n + 1.
(14.191)
The spherical Bessel functions will enter again in connection with spherical waves, but
further consideration is postponed until the corresponding angular functions, the Legendre
functions, have been more thoroughly discussed.
Modifed Spherical Bessel Functions
Problems involving the radial equation
r2 d2R
dr2 + 2r d R
dr −

k2r2 + l(l + 1)

R = 0,
(14.192)
which differs from Eq. (14.148) only in the sign of k2, also arise frequently in physics. The
solutions to this equation are spherical Bessel functions with imaginary arguments, leading
us to deﬁne modiﬁed spherical Bessel functions (Fig. 14.18) as follows:
in(x) =
r π
2x In+1/2(x),
(14.193)
kn(x) =
r
2
πx Kn+1/2(x).
(14.194)
Note that the scale factor in the deﬁnition of kn differs from that of the other spherical
Bessel functions.

14.7 Spherical Bessel Functions
707
x
6
5
4
3
2
1
0
1
2
3
4
5
k0(x)
k1(x)
i1(x)
i0(x)
FIGURE 14.18
Modiﬁed spherical Bessel functions.
With the above deﬁnitions, these functions have the following recurrence relations:
in−1(x) −in+1(x) = 2n + 1
x
in(x),
nin−1(x) + (n + 1)in+1(x) = (2n + 1)i′
n(x),
(14.195)
kn−1(x) −kn+1(x) = −2n + 1
x
kn(x),
nkn−1(x) + (n + 1)kn+1(x) = −(2n + 1)k′
n(x).
The ﬁrst few of these functions are
i0(x) = sinh x
x
,
k0(x) = e−x
x ,
i1(x) = cosh x
x
−sinh x
x2
,
k1(x) = e−x
1
x + 1
x2

,
(14.196)
i2(x) = sinh x
1
x + 3
x3

−3cosh x
x2
,
k2(x) = e−x
1
x + 3
x2 + 3
x3

.

708
Chapter 14 Bessel Functions
Limiting values of the modiﬁed spherical Bessel functions are, for small x,
in(x) ≈
xn
(2n + 1)!!,
kn(x) ≈(2n −1)!!
xn+1
.
(14.197)
For large z, the asymptotic behavior of these functions is
in(x) ∼ex
2x ,
kn(x) ∼e−x
x .
(14.198)
Example 14.7.2
PARTICLE IN A FINITE SPHERICAL WELL
As a ﬁnal example, we return to the problem of a particle trapped in a spherical potential
well of radius a (Example 14.7.1), but instead of conﬁning the particle by a wall at potential
V = ∞(equivalent to requiring that its wave function ψ vanish at r = a), we now consider
a well of ﬁnite depth, corresponding to
V (r) =
V0 < 0,
0 ≤r ≤a,
0,
r > a.
If the particle can have an energy E < 0, it will be localized in and near the potential well,
with a wave function that decays to zero as r increases to values greater than a. A simple
case of this problem was one of our examples of an eigenvalue problem (Example 8.3.3),
but in that case we did not proceed with enough generality to identify its solutions as Bessel
functions.
This problem is governed by the Schrödinger equation, which now has the form
−¯h2
2m ∇2ψ + V (r)ψ = Eψ.
This is an eigenvalue equation, to be solved for ψ and E over the full three-dimensional
space, subject to the condition that ψ be continuous and differentiable for all r, and that it
be normalizable (thus approaching zero asymptotically at large r). Here m is the mass of
the particle and ¯h is Planck’s constant divided by 2π.
While this problem is more difﬁcult than that of Example 14.7.1, it becomes manageable
if we realize that it is equivalent to two separate problems for the respective regions 0 ≤
r ≤a and r > a, within each of which the potential has a constant value, but constrained to
(1) have the same eigenvalue E, and (2) connect smoothly (so the r derivative will exist)
at r = a.
When our Schrödinger equation is processed by the method of separation of variables,
we obtain as its radial component
d2R
dr2 + 2
r
d R
dr +
 
2m
¯h2

E −V (r)

−l(l + 1)
r2
!
R = 0,
which is either the spherical Bessel equation, Eq. (14.150), or the modiﬁed spherical Bessel
equation, Eq. (14.192), depending on the sign of E −V (r). We see that if V0 < E < 0, then

14.7 Spherical Bessel Functions
709
for r ≤a we will have E −V (r) > 0, yielding a Bessel ODE with an acceptable solution
involving jl, while for r > a we have E −V (r) < 0, leading to a modiﬁed Bessel ODE
for which we can choose the kl solution to obtain the necessary asymptotic behavior.
Summarizing the above, we have, for the two regions:
Rin(r) = Ajl(kr),
k2 = 2m
¯h2 (E −V0)
r ≤a,
Rout(r) = Bkl(k′r), k′2 = −2m
¯h2 E
r > a.
Smooth connection at r = a then corresponds to the equations
Rin(a) = Rout(a)
−→
Ajl(ka) = Bkl(k′r),
(14.199)
d Rin
dr

r=a = d Rout
dr

r=a −→
k Aj′
l (ka) = k′Bk′
l(k′a).
(14.200)
For l = 0 this problem reduces to that considered in Example 8.3.3, where we indicate
a numerical procedure of solving it, but we are now in a position to obtain solutions for
all l.
■
Exercises
14.7.1
Show how one can obtain Eq. (14.162) starting from Eq. (14.161).
14.7.2
Show that if
yn(x) =
r π
2x Yn+1/2(x),
it automatically equals
(−1)n+1
r π
2x J−n−1/2(x).
14.7.3
Derive the trigonometric-polynomial forms of jn(z) and yn(z)11:
jn(z) = 1
z sin

z −nπ
2
 [n/2]
X
s=0
(−1)s(n + 2s)!
(2s)!(2z)2s(n −2s)!
+ 1
z cos

z −nπ
2
 [(n−1)/2]
X
s=0
(−1)s(n + 2s + 1)!
(2s + 1)!(2z)2s(n −2s −1)!,
11The upper summation limit [n/2] means the largest integer that does not exceed n/2.

710
Chapter 14 Bessel Functions
yn(z) = (−1)n+1
z
cos

z + nπ
2
 [n/2]
X
s=0
(−1)s(n + 2s)!
(2s)!(2z)2s(n −2s)!
+ (−1)n+1
z
sin

z + nπ
2
 [(n−1)/2]
X
s=0
(−1)s(n + 2s + 1)!
(2s + 1)!(2z)2s+1(n −2s −1)! .
14.7.4
Use the integral representation of Jν(x),
Jν(x) =
1
π1/20(ν + 1
2)
x
2
ν
1
Z
−1
e±ixp(1 −p2)ν−1/2dp,
to show that the spherical Bessel functions jn(x) are expressible in terms of trigono-
metric functions; that is, for example,
j0(x) = sin x
x
,
j1(x) = sin x
x2
−cos x
x
.
14.7.5
(a)
Derive the recurrence relations
fn−1(x) + fn+1(x) = 2n + 1
x
fn(x),
nfn−1(x) −(n + 1) fn+1(x) = (2n + 1) f ′
n(x)
satisﬁed by the spherical Bessel functions jn(x), yn(x),h(1)
n (x), and h(2)
n (x).
(b)
Show, from these two recurrence relations, that the spherical Bessel function
fn(x) satisﬁes the differential equation
x2 f ′′
n (x) + 2x f ′
n(x) +

x2 −n(n + 1)

fn(x) = 0.
14.7.6
Prove by mathematical induction (Section 1.4) that
jn(x) = (−1)nxn
1
x
d
dx
n sin x
x

for n, an arbitrary nonnegative integer.
14.7.7
From the discussion of orthogonality of the spherical Bessel functions, show that a
Wronskian relation for jn(x) and nn(x) is
jn(x)y′
n(x) −j′
n(x)yn(x) = 1
x2 .
14.7.8
Verify
h(1)
n (x)h(2)′
n
(x) −h(1)′
n
(x)h(2)
n (x) = −2i
x2 .
14.7.9
Verify Poisson’s integral representation of the spherical Bessel function,
jn(z) =
zn
2n+1n!
π
Z
0
cos(z cosθ)sin2n+1 θ dθ.

14.7 Spherical Bessel Functions
711
14.7.10
A well-known integral representation for Kν(x) has the form
Kν(x) = 2ν0(ν + 1
2)
√πxν
∞
Z
0
cos xt
(t2 + 1)ν+1/2 dt.
Starting from this formula, show that
kn(x) = 2n+2(n + 1)!
πxn+1
∞
Z
0
k2 j0(kx)
(k2 + 1)n+2 dk.
14.7.11
Show that
∞
Z
0
Jµ(x)Jν(x)dx
x = 2
π
sin[(µ −ν)π/2]
µ2 −ν2
,
µ + ν > 0.
14.7.12
Derive Eq. (14.190):
∞
Z
−∞
jm(x) jn(x)dx = 0,
m ̸= n,
m,n ≥0.
14.7.13
Derive Eq. (14.191):
∞
Z
−∞

jn(x)
 2dx =
π
2n + 1.
14.7.14
The Fresnel integrals (Fig. 14.19 and Exercise 12.7.2) occurring in diffraction theory
are given by
x(t) =
rπ
2 C
rπ
2 t

=
tZ
0
cos(v2)dv,
y(t) =
rπ
2 s
rπ
2 t

=
tZ
0
sin(v2)dv.
Show that these integrals may be expanded in series of spherical Bessel functions as
follows:
x(s) = 1
2
s
Z
0
j−1(u)u1/2du = s1/2
∞
X
n=0
j2n(s),
y(s) = 1
2
s
Z
0
j0(u)u1/2xdu = s1/2
∞
X
n=0
j2n+1(s).
Hint. To establish the equality of the integral and the sum, you may wish to work
with their derivatives. The spherical Bessel analogs of Eqs. (14.8) and (14.12) may
be helpful.

712
Chapter 14 Bessel Functions
x
x(t)
y(t)
y
x
π
2
1
2
⋅
1.0
0.5
1.0
2
3
4
FIGURE 14.19
Fresnel integrals.
14.7.15
A hollow sphere of radius a (Helmholtz resonator) contains standing sound waves.
Find the minimum frequency of oscillation in terms of the radius a and the velocity of
sound v. The sound waves satisfy the wave equation
∇2ψ = 1
v2
∂2ψ
∂t2
and the boundary condition ∂ψ
∂r = 0,
r = a.
The spatial part of this PDE is the same as the PDE discussed in Example 14.7.1, but
here we have a Neumann boundary condition, in contrast to the Dirichlet boundary
condition of that example.
ANS.
νmin = 0.3313v/a,
λmax = 3.018a.
14.7.16
(a)
Show that the parity of in(x) (the behavior under x →−x) is (−1)n.
(b)
Show that kn(x) has no deﬁnite parity.
14.7.17
Show that the Wronskian of the spherical modiﬁed Bessel functions is given by
in(x)k′
n(x) −i′
n(x)kn(x) = −1
x2 .

14.7 Spherical Bessel Functions
713
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974).
Jackson, J. D., Classical Electrodynamics, 3rd ed. New York: Wiley (1999).
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics, 2 vols. New York: McGraw-Hill (1953). This
work presents the mathematics of much of theoretical physics in detail but at a rather advanced level.
Watson, G. N., A Treatise on the Theory of Bessel Functions, 1st ed. Cambridge: Cambridge University Press
(1922).
Watson, G. N., A Treatise on the Theory of Bessel Functions, 2nd ed. Cambridge: Cambridge University Press
(1952). This is the deﬁnitive text on Bessel functions and their properties. Although difﬁcult reading, it is
invaluable as the ultimate reference.
Whittaker, E. T., and G. N. Watson, A Course of Modern Analysis, 4th ed. Cambridge: Cambridge University
Press (1962), paperback.

CHAPTER 15
LEGENDRE FUNCTIONS
Legendre functions are important in physics because they arise when the Laplace or
Helmholtz equations (or their generalizations) for central force problems are separated
in spherical coordinates. They therefore appear in the descriptions of wave functions for
atoms, in a variety of electrostatics problems, and in many other contexts. In addition,
the Legendre polynomials provide a convenient set of functions that is orthogonal (with
unit weight) on the interval (−1,+1) that is the range of the sine and cosine functions. And
from a pedagogical viewpoint, they provide a set of functions that are easy to work with and
form an excellent illustration of the general properties of orthogonal polynomials. Several
of these properties were discussed in a general way in Chapter 12. We collect here those
results, expanding them with additional material that is of great utility and importance.
As indicated above, Legendre functions are encountered when an equation written in
spherical polar coordinates (r,θ,ϕ), such as
−∇2ψ + V (r)ψ = λψ,
is solved by the method of separation of variables. Note that we are assuming that this
equation is to be solved for a spherically symmetric region and that V (r) is a func-
tion of the distance from the origin of the coordinate system (and therefore not a func-
tion of the three-component position vector r). As in Eqs. (9.77) and (9.78), we write
ψ = R(r)2(θ)8(ϕ) and decompose our original partial differential equation (PDE) into
the three one-dimensional ordinary differential equations (ODEs):
d28
dϕ2 = −m28,
(15.1)
1
sinθ
d
dθ

sinθ d2
dθ

−m22
sin2 θ
+ l(l + 1)2 = 0,
(15.2)
1
r2
d
dr

r2 d R
dr

+
h
λ −V (r)
i
R −l(l + 1)R
r2
= 0.
(15.3)
715
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

716
Chapter 15 Legendre Functions
The quantities m2 and l(l + 1) are constants that occur when the variables are separated;
the ODE in ϕ is easy to solve and has natural boundary conditions (cf. Section 9.4), which
dictate that m must be an integer and that the functions 8 can be written as e±imϕ or as
sin(mϕ), cos(mϕ).
The 2 equation can now be transformed by the substitution x = cosθ, cf. Eq. (9.79),
reaching
(1 −x2)P′′(x) −2x P′(x) −
m2
1 −x2 P(x) + l(l + 1)P(x) = 0.
(15.4)
This is the associated Legendre equation; the special case with m = 0, which we will
treat ﬁrst, is the Legendre ODE.
15.1
LEGENDRE POLYNOMIALS
The Legendre equation,
(1 −x2)P′′(x) −2x P′(x) + λP(x) = 0,
(15.5)
has regular singular points at x = ±1 and x = ∞(see Table 7.1), and therefore has a series
solution about x = 0 that has a unit radius of convergence, i.e., the series solution will
(for all values of the parameter λ) converge for |x| < 1. In Section 8.3 we found that for
most values of λ, the series solutions will diverge at x = ±1 (corresponding to θ = 0 and
θ = π), making the solutions inappropriate for use in central force problems. However, if
λ has the value l(l + 1), with l an integer, the series become truncated after xl, leaving a
polynomial of degree l.
Now that we have identiﬁed the desired solutions to the Legendre equations as polyno-
mials of successive degrees, called Legendre polynomials and designated Pl, let us use
the machinery of Chapter 12 to develop them from a generating-function approach. This
course of action will set a scale for the Pl and provide a good starting point for deriving
recurrence relations and related formulas.
We found in Example 12.1.3 that the generating function for the polynomial solutions
of the Legendre ODE is given by Eq. (12.27):
g(x,t) =
1
√
1 −2xt + t2 =
∞
X
n=0
Pn(x)tn.
(15.6)
To identify the scale that is given to Pn by Eq. (15.6), we simply set x = 1 in that equation,
bringing its left-hand side to the form
g(1,t) =
1
√
1 −2t + t2 =
1
1 −t =
∞
X
n=0
tn,
(15.7)
where the last step in Eq. (15.7) was to expand 1/(1 −t) using the binomial theorem.
Comparing with Eq. (15.6), we see that the scaling it predicts is Pn(1) = 1.

15.1 Legendre Polynomials
717
Next, consider what happens if we replace x by −x and t by −t. The value of g(x,t) in
Eq. (15.6) is unaffected by this substitution, but the right-hand side takes a different form:
∞
X
n=0
Pn(x)tn = g(x,t) = g(−x,−t) =
∞
X
n=0
Pn(−x)(−t)n,
(15.8)
showing that
Pn(−x) = (−1)n Pn(x).
(15.9)
From this result it is obvious that Pn(−1) = (−1)n, and that Pn(x) will have the same
parity as xn.
Another useful special value is Pn(0). Writing P2n and P2n+1 to distinguish even and
odd index values, we note ﬁrst that because P2n+1 is odd under parity, i.e., x →−x, we
must have P2n+1(0) = 0. To obtain P2n(0), we again resort to the binomial expansion:
g(0,t) = (1 + t2)−1/2 =
∞
X
n=0
−1/2
n

t2n =
∞
X
n=0
P2n(0)t2n.
(15.10)
Then, using Eq. (1.74) to evaluate the binomial coefﬁcient, we get
P2n(0) = (−1)n (2n −1)!!
(2n)!!
.
(15.11)
It is also useful to characterize the leading terms of the Legendre polynomials. Applying
the binomial theorem to the generating function,
(1 −2xt + t2)−1/2 =
∞
X
n=0
−1/2
n

(−2xt + t2)n,
(15.12)
from which we see that the maximum power of x that can multiply tn will be xn, and is
obtained from the term (−2xt)n in the expansion of the ﬁnal factor. Thus, the
coefﬁcient of xn in Pn(x) is
−1/2
n

(−2)n = (2n −1)!!
n!
.
(15.13)
These results are important, so we summarize:
Pn(x) has sign and scaling such that Pn(1) = 1 and Pn(−1) = (−1)n.
P2n(x) is an even function of x; P2n+1(x) is odd. P2n+1(0) = 0, and P2n(0)
is given by Eq. (15.11). Pn(x) is a polynomial of degree n in x, with the
coefﬁcient of xn given by Eq. (15.13); Pn(x) contains alternate powers of
x: xn, xn−2,··· ,(x0 or x1).
From the fact that Pn is of degree n with alternate powers, it is clear that P0(x) =
constant and that P1(x) = (constant) x. From the scaling requirements these must reduce to
P0(x) = 1 and P1(x) = x.
Returning to Eq. (15.12), we can get explicit closed expressions for the Legendre poly-
nomials. All we need to do is expand the quantity (−2xt + t2)n and rearrange the summa-
tions to identify the x dependence associated with each power of t. The result, which is in

718
Chapter 15 Legendre Functions
general less useful than the recurrence formulas to be developed in the next subsection, is
Pn(x) =
[n/2]
X
k=0
(−1)k
(2n −2k)!
2nk!(n −k)!(n −2k)! xn−2k.
(15.14)
Here [n/2] stands for the largest integer ≤n/2. This formula is consistent with the
requirement that for n even, Pn(x) has only even powers of x and even parity, while
for n odd, it has only odd powers of x and odd parity. Proof of Eq. (15.14) is the topic of
Exercise 15.1.2.
Recurrence Formulas
From the generating function equation we can generate recurrence formulas by differenti-
ating g(x,t) with respect to x or t. We start from
∂g(x,t)
∂t
=
x −t
(1 −2xt + t2)3/2 =
∞
X
n=0
nPn(x)tn−1,
(15.15)
which we rearrange to
(1 −2xt + t2)
∞
X
n=0
nPn(x)tn−1 + (t −x)
∞
X
n=0
Pn(x)tn = 0,
(15.16)
and then expand, reaching
∞
X
n=0
nPn(x)tn−1 −2
∞
X
n=0
nx Pn(x)tn +
∞
X
n=0
nPn(x)tn+1
+
∞
X
n=0
Pn(x)tn+1 −
∞
X
n=0
x Pn(x)tn = 0.
(15.17)
Collecting the coefﬁcients of tn from the various terms and setting the result to zero,
Eq. (15.17) is seen to be equivalent to
(2n + 1)x Pn(x) = (n + 1)Pn+1(x) + nPn−1(x),
n = 1, 2, 3, ....
(15.18)
Equation (15.18) permits us to generate successive Pn from the starting values P0 and P1
that we have previously identiﬁed. For example,
2P2(x) = 3x P1(x) −P0(x)
−→
P2(x) = 1
2

3x2 −1

.
(15.19)
Continuing this process, we can build the list of Legendre polynomials given in Table 15.1.
We can also obtain a recurrence formula involving P′
n by differentiating g(x,t) with
respect to x. This gives
∂g(x,t)
∂x
=
t
(1 −2xt + t2)3/2 =
∞
X
n=0
P′
n(x)tn,

15.1 Legendre Polynomials
719
Table 15.1
Legendre Polynomials
P0(x) = 1
P1(x) = x
P2(x) = 1
2 (3x2 −1)
P3(x) = 1
2 (5x3 −3x)
P4(x) = 1
8 (35x4 −30x2 + 3)
P5(x) = 1
8 (63x5 −70x3 + 15x)
P6(x) = 1
16 (231x6 −315x4 + 105x2 −5)
P7(x) = 1
16 (429x7 −693x5 + 315x3 −35x)
P8(x) =
1
128 (6435x8 −12012x6 + 6930x4 −1260x2 + 35)
or
(1 −2xt + t2)
∞
X
n=0
P′
n(x)tn −t
∞
X
n=0
Pn(x)tn = 0.
(15.20)
As before, the coefﬁcient of each power of t is set to zero and we obtain
P′
n+1(x) + P′
n−1(x) = 2x P′
n(x) + Pn(x).
(15.21)
A more useful relation may be found by differentiating Eq. (15.18) with respect to x and
multiplying by 2. To this we add (2n + 1) times Eq. (15.21), canceling the P′
n term. The
result is
P′
n+1(x) −P′
n−1(x) = (2n + 1)Pn(x).
(15.22)
Starting from Eqs. (15.21) and (15.22), numerous additional relations can be developed,1
including
P′
n+1(x) = (n + 1)Pn(x) + x P′
n(x),
(15.23)
P′
n−1(x) = −nPn(x) + x P′
n(x),
(15.24)
(1 −x2)P′
n(x) = nPn−1(x) −nx Pn(x),
(15.25)
(1 −x2)P′
n(x) = (n + 1)x Pn(x) −(n + 1)Pn+1(x).
(15.26)
Because we derived the generating function g(x,t) from the Legendre ODE and then
obtained the recurrence formulas using g(x,t), that ODE will automatically be consis-
tent with these recurrence relations. It is nevertheless of interest to verify this consistency,
because then we can conclude that any set of functions satisfying the recurrence formulas
will be a set of solutions to the Legendre ODE, and that observation will be relevant to
1Using the equation numbers in parentheses to indicate how they are to be combined, we may obtain some of these derivative
formulas as follows:
2 · d
dx (15.18) + (2n + 1) · (15.21) ⇒(15.22),
1
2 {(15.21) + (15.22)} ⇒(15.23),
1
2 {(15.21) −(15.22)} ⇒(15.24),
(15.23)n→n−1 + x (15.24) ⇒(15.25).

720
Chapter 15 Legendre Functions
the Legendre functions of the second kind (solutions linearly independent of the polyno-
mials Pl). A demonstration that functions satisfying the recurrence formulas also satisfy
the Legendre ODE is the topic of Exercise 15.1.1.
Upper and Lower Bounds for Pn(cosθ)
Our generating function can be used to set an upper limit on |Pn(cosθ)|. We have
(1 −2t cosθ + t2)−1/2 = (1 −teiθ)−1/2(1 −te−iθ)−1/2
=

1 + 1
2teiθ + 3
8t2e2iθ + ···

1 + 1
2te−iθ + 3
8t2e−2iθ + ···

.
(15.27)
We may make two immediate observations from Eq. (15.27). First, when any term within
the ﬁrst set of parentheses is multiplied by any term from the second set of parentheses,
the power of t in the product will be even if and only if m in the net exponential eimθ is
even. Second, for every term of the form tneimθ, there will be another term of the form
tne−imθ, and the two terms will occur with the same coefﬁcient, which must be positive
(since all the terms in both summations are individually positive). These two observations
mean that:
(1)
Taking the terms of the expansion two at a time, we can write the coefﬁcient of tn as
a linear combination of forms
1
2 anm(eimθ + e−imθ) = anm cosmθ
with all the anm positive, and
(2)
The parity of n and m must be the same (either they are both even, or both odd).
This, in turn, means that
Pn(cosθ) =
n
X
m=0 or 1
anm cosmθ.
(15.28)
This expression is clearly a maximum when θ = 0, where we already know, from the Sum-
mary following Eq. (15.11), that Pn(1) = 1. Thus,
The Legendre polynomial Pn(x) has a global maximum on the inter-
val (−1,+1) at x = 1, with value Pn(1) = 1, and if n is even, also at
x = −1. If n is odd, x = −1 will be a global minimum on this interval
with Pn(−1) = −1.
The maxima and minima of the Legendre polynomials can be seen from the graphs of
P2 through P5, in which are plotted in Fig. 15.1.
Rodrigues Formula
In Section 12.1 we showed that orthogonal polynomials could be described by
Rodrigues formulas, and that the repeated differentiations occurring therein were good

15.1 Legendre Polynomials
721
x
1
Pn(x)
0.5
P2
P3
P4
P5
1
0
−0.5
−1
FIGURE 15.1
Legendre polynomials P2(x) through P5(x).
starting points for developing properties of these functions. Applying Eq. (12.9), we ﬁnd
that the Rodrigues formula for the Legendre polynomials must be proportional to
 d
dx
n
(1 −x2)n.
(15.29)
Equation (12.9) is not sufﬁcient to set the scale of the orthogonal polynomials, and to
bring Eq. (15.29) to the scaling already adopted via Eq. (15.6) we multiply Eq. (15.29) by
(−1)n/2n n!, so
Pn(x) =
1
2n n!
 d
dx
n
(x2 −1)n.
(15.30)
To establish that Eq. (15.30) has a scaling in agreement with our earlier analyses, it
sufﬁces to check the coefﬁcient of a single power of x; we choose xn. From the Rodrigues
formula, this power of x can only arise from the term x2n in the expansion of (x2 −1)n,
and the
coefﬁcient of xn in Pn(x) (Rodrigues) is
1
2n n!
(2n)!
n!
= (2n −1)!!
n!
,
in agreement with Eq. (15.13). This conﬁrms the scale of Eq. (15.30).

722
Chapter 15 Legendre Functions
Exercises
15.1.1
Derive the Legendre ODE by manipulation of the Legendre polynomial recurrence
relations. Suggested starting point: Eqs. (15.24) and (15.25).
15.1.2
Derive the following closed formula for the Legendre polynomials Pn(x).
Pn(x) =
[n/2]
X
k=0
(−1)k
(2n −2k)!
2nk!(n −k)!(n −2k)! xn−2k,
where [n/2] stands for the integer part of n/2.
Hint. Further expand Eq. (15.12) and rearrange the resulting double sum.
15.1.3
By differentiation and direct substitution of the series form given in Exercise 15.1.2,
show that Pn(x) satisﬁes the Legendre ODE. Note that there is no restriction on x. We
may have any x, −∞< x < ∞, and indeed any z in the entire ﬁnite complex plane.
15.1.4
The shifted Legendre polynomials, designated by the symbol P∗
n (x) (where the as-
terisk does not mean complex conjugate) are orthogonal with unit weight on [0,1],
with normalization integral ⟨P∗
n |P∗
n ⟩= 1/(2n + 1). The P∗
n through n = 6 are shown in
Table 15.2.
(a)
Find the recurrence relation satisﬁed by the P∗
n.
(b)
Show that all the coefﬁcients of the P∗
n are integers.
Hint. Look at the closed formula in Exercise 15.1.2.
15.1.5
Given the series
α0 + α2 cos2 θ + α4 cos4 θ + α6 cos6 θ = a0P0 + a2P2 + a4P4 + a6P6,
where the arguments of the Pn are cosθ, express the coefﬁcients αi as a column vector
α and the coefﬁcients ai as a column vector a and determine the matrices A and B such
that
Aα = a
and
Ba = α.
Table 15.2
Shifted Legendre Polynomials
P∗
0 (x) = 1
P∗
1 (x) = 2x −1
P∗
2 (x) = 6x2 −6x + 1
P∗
3 (x) = 20x3 −30x2 + 12x −1
P∗
4 (x) = 70x4 −140x3 + 90x2 −20x + 1
P∗
5 (x) = 252x5 −630x4 + 560x3 −210x2 + 30x −1
P∗
6 (x) = 924x6 −2772x5 + 3150x4 −1680x3 + 420x2 −42x + 1

15.1 Legendre Polynomials
723
Check your computation by showing that AB = 1 (unit matrix). Repeat for the odd case
α1 cosθ + α3 cos3 θ + α5 cos5 θ + α7 cos7 θ = a1P1 + a3P3 + a5P5 + a7P7.
Note. Pn(cosθ) and cosn θ are tabulated in terms of each other in AMS-55 (see Addi-
tional Readings for the complete reference).
15.1.6
By differentiating the generating function g(x,t) with respect to t, multiplying by 2t,
and then adding g(x,t), show that
1 −t2
(1 −2tx + t2)3/2 =
∞
X
n=0
(2n + 1)Pn(x)tn.
This result is useful in calculating the charge induced on a grounded metal sphere by a
nearby point charge.
15.1.7
(a)
Derive Eq. (15.26),
(1 −x2)P′
n(x) = (n + 1)x Pn(x) −(n + 1)Pn+1(x).
(b)
Write out the relation of Eq. (15.26) to preceding equations in symbolic form
analogous to the symbolic forms for Eqs. (15.22) to (15.25).
15.1.8
Prove that
P′
n(1) = d
dx Pn(x) |x=1= 1
2 n(n + 1).
15.1.9
Show that Pn(cosθ) = (−1)n Pn(−cosθ) by use of the recurrence relation relating Pn,
Pn+1, and Pn−1 and your knowledge of P0 and P1.
15.1.10
From Eq. (15.27) write out the coefﬁcient of t2 in terms of cosnθ, n ≤2. This coefﬁ-
cient is P2(cosθ).
15.1.11
Derive the recurrence relation
(1 −x2)P′
n(x) = nPn−1(x) −nx Pn(x)
from the Legendre polynomial generating function.
15.1.12
Evaluate
1
Z
0
Pn(x) dx.
ANS.
n = 2s,
1 for s = 0, 0 for s > 0;
n = 2s + 1,
P2s(0)/(2s + 2) = (−1)s(2s −1)!!/1(2s + 2)!!.
Hint. Use a recurrence relation to replace Pn(x) by derivatives and then integrate by
inspection. Alternatively, you can integrate the generating function.
15.1.13
Show that each term in the summation
n
X
r=[n/2]+1
 d
dx
n (−1)rn!
r!(n −r)!x2n−2r
vanishes (r and n integral). Here [n/2] is the largest integer ≤n/2.

724
Chapter 15 Legendre Functions
15.1.14
Show that
R 1
−1 xm Pn(x) dx = 0 when m < n.
Hint. Use Rodrigues formula or expand xm in Legendre polynomials.
15.1.15
Show that
1
Z
−1
xn Pn(x)dx =
2 n!
(2n + 1)!!.
Note. You are expected to use the Rodrigues formula and integrate by parts, but also
see if you can get the result from Eq. (15.14) by inspection.
15.1.16
Show that
1
Z
−1
x2r P2n(x)dx =
22n+1(2r)!(r + n)!
(2r + 2n + 1)!(r −n)!,
r ≥n.
15.1.17
As a generalization of Exercises 15.1.15 and 15.1.16, show that the Legendre expan-
sions of xs are
(a)
x2r =
r
X
n=0
22n(4n + 1)(2r)!(r + n)!
(2r + 2n + 1)!(r −n)!
P2n(x),
s = 2r,
(b)
x2r+1 =
r
X
n=0
22n+1(4n + 3)(2r + 1)!(r + n + 1)!
(2r + 2n + 3)!(r −n)!
P2n+1(x), s = 2r +1.
15.1.18
In numerical work (for e.g., the Gauss-Legendre quadrature), it is useful to establish
that Pn(x) has n real zeros in the interior of [−1,1]. Show that this is so.
Hint. Rolle’s theorem shows that the ﬁrst derivative of (x2 −1)2n has one zero in the
interior of [−1,1]. Extend this argument to the second, third, and ultimately the nth
derivative.
15.2
ORTHOGONALITY
Because the Legendre ODE is self-adjoint and the coefﬁcient of P′′(x), namely (1 −x2),
vanishes at x = ±1, its solutions of different n will automatically be orthogonal with unit
weight on the interval (−1,1),
1
Z
−1
Pn(x)Pm(x)dx = 0,
(n ̸= m).
(15.31)
Because the Pn are real, no complex conjugation needs to be indicated in the orthogonality
integral. Since Pn is often used with argument cosθ, we note that Eq. (15.31) is equivalent

15.2 Orthogonality
725
to
π
Z
0
Pn(cosθ)Pm(cosθ)sinθ dθ = 0,
(n ̸= m).
(15.32)
The deﬁnition of the Pn does not guarantee that they are normalized, and in fact they
are not. One way to establish the normalization starts by squaring the generating-function
formula, yielding initially
(1 −2xt + t2)−1 =
" ∞
X
n=0
Pn(x)tn
#2
.
(15.33)
Integrating from x = −1 to x = 1 and dropping the cross terms because they vanish due to
orthogonality, Eq. (15.31), we have
1
Z
−1
dx
1 −2tx + t2 =
∞
X
n=0
t2n
1
Z
−1
h
Pn(x)
i 2
dx.
(15.34)
Making now the substitution y = 1 −2tx + t2, with dy = −2t dx, we obtain
1
Z
−1
dx
1 −2tx + t2 = 1
2t
(1+t)2
Z
(1−t)2
dy
y = 1
t ln
1 + t
1 −t

.
(15.35)
Expanding this result in a power series (Exercise 1.6.1),
1
t ln
1 + t
1 −t

= 2
∞
X
n=0
t2n
2n + 1,
(15.36)
and equating the coefﬁcients of powers of t in Eqs. (15.34) and (15.36), we must have
1
Z
−1
h
Pn(x)
i 2
dx =
2
2n + 1.
(15.37)
Combining Eqs. (15.31) and (15.37), we have the orthonormality condition
1
Z
−1
Pn(x)Pm(x)dx = 2δnm
2n + 1.
(15.38)
This result can also be obtained using the Rodrigues formulas for Pn and Pm. See Exer-
cise 15.2.1.

726
Chapter 15 Legendre Functions
Legendre Series
The orthogonality of the Legendre polynomials makes it natural to use them as a basis for
expansions. Given a function f (x) deﬁned on the range (−1,1), the coefﬁcients in the
expansion
f (x) =
∞
X
n=0
an Pn(x)
(15.39)
are given by the formula
an = 2n + 1
2
1
Z
−1
f (x)Pn(x)dx.
(15.40)
The orthogonality property guarantees that this expansion is unique. Since we can (but
perhaps will not wish to) convert our expansion into a power series by inserting the expan-
sion of Eq. (15.14) and collecting the coefﬁcients of each power of x, we can also obtain a
power series, which we thereby know must be unique.
An important application of Legendre series is to solutions of the Laplace equation. We
saw in Section 9.4 that when the Laplace equation is separated in spherical polar coordi-
nates, its general solution (for spherical symmetry) takes the form
ψ(r,θ,ϕ) =
X
l,m
(Almrl + Blmr−l−1)Pm
l (cosθ)(A′
lm sinmϕ + B′
lm cosmϕ),
(15.41)
with l required to be an integer to avoid a solution that diverges in the polar directions.
Here we consider solutions with no azimuthal dependence (i.e., with m = 0), so Eq. (15.41)
reduces to
ψ(r,θ) =
∞
X
l=0
(alrl + blr−l−1)Pl(cosθ).
(15.42)
Often our problem is further restricted to a region either within or external to a boundary
sphere, and if the problem is such that ψ must remain ﬁnite, the solution will have one of
the two following forms:
ψ(r,θ) =
∞
X
l=0
al rl Pl(cosθ)
(r ≤r0),
(15.43)
ψ(r,θ) =
∞
X
l=0
al r−l−1Pl(cosθ)
(r ≥r0).
(15.44)
Note that this simpliﬁcation is not always appropriate; see Example 15.2.2. Sometimes
the coefﬁcients (al) are determined from the boundary conditions of a problem rather than
from the expansion of a known function. See the examples to follow.

15.2 Orthogonality
727
Example 15.2.1
EARTH’S GRAVITATIONAL FIELD
An example of a Legendre series is provided by the description of the Earth’s gravitational
potential U at points exterior to the Earth’s surface. Because gravitation is an inverse-
square force, its potential in mass-free regions satisﬁes the Laplace equation, and therefore
(if we neglect azimuthal effects, i.e., those dependent on longitude) it has the form given
in Eq. (15.44).
To specialize to the current example, we deﬁne R to be the Earth’s radius at the equator,
and take as the expansion variable the dimensionless quantity R/r. In terms of the total
mass of the Earth M and the gravitational constant G, we have
R = 6378.1 ± 0.1km,
GM
R
= 62.494 ± 0.001km2/s2,
and we write
U(r,θ) = GM
R
"
R
r −
∞
X
l=2
al
 R
r
l+1
Pl(cosθ)
#
.
(15.45)
The leading term of this expansion describes the result that would be obtained if the Earth
were spherically symmetric; the higher terms describe distortions. The P1 term is absent
because the origin from which r is measured is the Earth’s center of mass.
Artiﬁcial satellite motions have shown that
a2 = (1,082,635 ± 11) × 10−9,
a3 = (−2,531 ± 7) × 10−9,
a4 = (−1,600 ± 12) × 10−9.
This is the famous pear-shaped deformation of the Earth. Other coefﬁcients have been
computed through a20.
More recent satellite data permit a determination of the longitudinal dependence of the
Earth’s gravitational ﬁeld. Such dependence may be described by a Laplace series (see
Section 15.5).
■
Example 15.2.2
SPHERE IN A UNIFORM FIELD
Another illustration of the use of a Legendre series is provided by the problem of a neutral
conducting sphere (radius r0) placed in a (previously) uniform electric ﬁeld of magnitude
E0 (Fig. 15.2). The problem is to ﬁnd the new, perturbed electrostatic potential ψ that
satisﬁes Laplace’s equation,
∇2ψ = 0.
We select spherical polar coordinates with origin at the center of the conducting sphere and
the polar (z) axis oriented parallel to the original uniform ﬁeld, a choice that will simplify

728
Chapter 15 Legendre Functions
E
V=0
z
FIGURE 15.2
Conducting sphere in a uniform ﬁeld.
the application of the boundary condition at the surface of the conductor. Separating vari-
ables, we note that because we require a solution to Laplace’s equation, the potential for
r ≥r0 will be of the form of Eq. (15.42). Our solution will be independent of ϕ because of
the axial symmetry of the problem.
Because the insertion of the conducting sphere will have an effect that is local, the
asymptotic behavior of ψ must be of the form
ψ(r →∞) = −E0 z = −E0 r cosθ = −E0 r P1(cosθ),
(15.46)
equivalent to
an = 0,
n > 1,
a1 = −E0.
(15.47)
Note that if an ̸= 0 for any n > 1, that term would dominate at large r and the boundary
condition, Eq. (15.46), could not be satisﬁed. In addition, the neutrality of the conducting
sphere requires that ψ not contain a contribution proportional to 1/r, so we also must have
b0 = 0.
As a second boundary condition, the conducting sphere must be an equipotential, and
without loss of generality we can set its potential to zero. Then, on the sphere r = r0 we
have
ψ(r0,θ) = a0 +
 
b1
r2
0
−E0r0
!
P1(cosθ) +
∞
X
n=2
bn
Pn(cosθ)
rn+1
0
= 0.
(15.48)
In order that Eq. (15.48) may hold for all values of θ, we set
a0 = 0,
b1 = E0 r3
0
bn = 0,
n ≥2.
(15.49)

15.2 Orthogonality
729
The electrostatic potential (outside the sphere) is then completely determined:
ψ(r,θ) = −E0 r P1(cosθ) + E0 r3
0
r2
P1(cosθ)
= −E0 r P1(cosθ)
 
1 −r3
0
r3
!
= −E0 z
 
1 −r3
0
r3
!
.
(15.50)
In Section 9.5 we showed that Laplace’s equation with Dirichlet boundary conditions on a
closed boundary (parts of which may be at inﬁnity) had a unique solution. Since we have
now found a solution to our current problem, it must (apart from an additive constant) be
the only solution.
It may further be shown that there is an induced surface charge density
σ = −ε0
∂ψ
∂r

r=r0
= 3ε0E0 cosθ
(15.51)
on the surface of the sphere and an induced electric dipole moment of magnitude
P = 4πr3
0ε0E0.
(15.52)
See Exercise 15.2.11.
■
Example 15.2.3
ELECTROSTATIC POTENTIAL FOR A RING OF CHARGE
As a further example, consider the electrostatic potential produced by a thin conducting
ring of radius a placed symmetrically in the equatorial plane of a spherical polar coordinate
system and carrying a total electric charge q (Fig. 15.3). Again we rely on the fact that
the potential ψ satisﬁes Laplace’s equation. Separating the variables and recognizing that
a solution for the region r > a must go to zero as r →∞, we use the form given by
(r, θ)
y
x
q
θ
r
a
z
FIGURE 15.3
Charged, conducting ring.

730
Chapter 15 Legendre Functions
Eq. (15.44), obtaining
ψ(r,θ) =
∞
X
n=0
cn
an
rn+1 Pn(cosθ),
r > a.
(15.53)
There is no ϕ (azimuthal) dependence because of the cylindrical symmetry of the system.
Note also that by including an explicit factor an we cause all the coefﬁcients cn to have the
same dimensionality; this choice simply modiﬁes the deﬁnition of cn and was, of course,
not required.
Our problem is to determine the coefﬁcients cn in Eq. (15.53). This may be done by
evaluating ψ(r,θ) at θ = 0, r = z, and comparing with an independent calculation of the
potential from Coulomb’s law. In effect, we are using a boundary condition along the
z-axis. From Coulomb’s law (using the fact that all the charge is equidistant from any
point on the z axis),
ψ(z,0) =
q
4πε0
1
(z2 + a2)1/2 =
q
4πε0 z
∞
X
s=0
−1/2
s
a2
z2
s
=
q
4πε0z
∞
X
s=0
(−1)s (2s −1)!!
(2s)!!
a
z
2s
,
z > a,
(15.54)
where we have evaluated the binomial coefﬁcient using Eq. (1.74).
Now, evaluating ψ(z,0) from Eq. (15.53), remembering that Pn(1) = 1 for all n,
we have
ψ(z,0) =
∞
X
n=0
cn
an
zn+1 .
(15.55)
Since the power series expansion in z is unique, we may equate the coefﬁcients of corre-
sponding powers of z from Eqs. (15.54) and (15.55), reaching the conclusion that cn = 0
for n odd, while for n even and equal to 2s,
c2s =
q
4πε0z (−1)s (2s −1)!!
(2s)!!
,
(15.56)
and our electrostatic potential ψ(r,θ) is given by
ψ(r,θ) =
q
4πε0r
∞
X
s=0
(−1)s (2s −1)!!
(2s)!!
a
r
2s
P2s(cosθ),
r > a.
(15.57)
The magnetic analog of this problem appears in Example 15.4.2.
■
Exercises
15.2.1
Using a Rodrigues formula, show that the Pn(x) are orthogonal and that
1
Z
−1
[Pn(x)]2dx =
2
2n + 1.
Hint. Integrate by parts.

15.2 Orthogonality
731
15.2.2
You have constructed a set of orthogonal functions by the Gram-Schmidt process
(Section 5.2), taking un(x) = xn, n = 0,1,2,... , in increasing order with w(x) = 1
and an interval −1 ≤x ≤1. Prove that the nth such function constructed in this way is
proportional to Pn(x).
Hint. Use mathematical induction (Section 1.4).
15.2.3
Expand the Dirac delta function δ(x) in a series of Legendre polynomials using the
interval −1 ≤x ≤1.
15.2.4
Verify the Dirac delta function expansions
δ(1 −x) =
∞
X
n=0
2n + 1
2
Pn(x),
δ(1 + x) =
∞
X
n=0
(−1)n 2n + 1
2
Pn(x).
These expressions appear in a resolution of the Rayleigh plane wave expansion
(Exercise 15.2.24) into incoming and outgoing spherical waves.
Note. Assume that the entire Dirac delta function is covered when integrating over
[−1,1].
15.2.5
Neutrons (mass 1) are being scattered by a nucleus of mass A (A > 1). In the center-
of-mass system the scattering is isotropic. Then, in the laboratory system the average of
the cosine of the angle of deﬂection of the neutron is
⟨cosψ⟩= 1
2
π
Z
0
A cosθ + 1
(A2 + 2A cosθ + 1)1/2 sinθ dθ.
Show, by expansion of the denominator, that ⟨cosψ⟩= 2/(3A).
15.2.6
A particular function f (x) deﬁned over the interval [−1,1] is expanded in a Legendre
series over this same interval. Show that the expansion is unique.
15.2.7
A function f (x) is expanded in a Legendre series f (x) = P∞
n=0 an Pn(x). Show that
1
Z
−1
[ f (x)]2 dx =
∞
X
n=0
2a2
n
2n + 1.
This is a statement that the Legendre polynomials form a complete set.
15.2.8
(a)
For
f (x) =
+1,
0 < x < 1,
−1, −1 < x < 0,

732
Chapter 15 Legendre Functions
show that
1
Z
−1
h
f (x)
i 2
dx = 2
∞
X
n=0
(4n + 3)
(2n −1)!!
(2n + 2)!!
2
.
(b)
By testing the series, prove that it is convergent.
(c)
The value of the integral in part (a) is 2. Check the rate at which the series con-
verges by summing its ﬁrst 10 terms.
15.2.9
Prove that
1
Z
−1
x(1 −x2)P′
n P′
m dx = 2n(n2 −1)
4n2 −1
δm,n−1 + 2n(n + 2)(n + 1)
(2n + 1)(2n + 3) δm,n+1.
15.2.10
The coincidence counting rate, W(θ), in a gamma-gamma angular correlation experi-
ment has the form
W(θ) =
∞
X
n=0
a2n P2n(cosθ).
Show that data in the range π/2 ≤θ ≤π can, in principle, deﬁne the function W(θ)
(and permit a determination of the coefﬁcients a2n). This means that although data in
the range 0 ≤θ < π/2 may be useful as a check, they are not essential.
15.2.11
A conducting sphere of radius r0 is placed in an initially uniform electric ﬁeld, E0.
Show the following:
(a)
The induced surface charge density is σ = 3ε0E0 cosθ,
(b)
The induced electric dipole moment is P = 4πr3
0ε0E0.
Note. The induced electric dipole moment can be calculated either from the surface
charge [part (a)] or by noting that the ﬁnal electric ﬁeld E is the result of superimposing
a dipole ﬁeld on the original uniform ﬁeld.
15.2.12
Obtain as a Legendre expansion the electrostatic potential of the circular ring of
Example 15.2.3, for points (r,θ) with r < a.
15.2.13
Calculate the electric ﬁeld produced by the charged conducting ring of Example 15.2.3
for
(a)
r > a,
(b) r < a.
15.2.14
As an extension of Example 15.2.3, ﬁnd the potential ψ(r,θ) produced by a charged
conducting disk, Fig. 15.4, for r > a, where a is the radius of the disk. The charge
density σ (on each side of the disk) is
σ(ρ) =
q
4πa(a2 −ρ2)1/2 ,
ρ2 = x2 + y2.

15.2 Orthogonality
733
y
z
x
a
FIGURE 15.4
Charged conducting disk.
Hint. The deﬁnite integral you get can be evaluated as a beta function, Section 13.3. For
more details see section 5.03 of Smythe in Additional Readings.
ANS.
ψ(r,θ) =
q
4πε0r
∞
X
l=0
(−1)l
1
2l + 1
a
r
2l
P2l(cosθ).
15.2.15
The hemisphere deﬁned by r = a, 0 ≤θ < π/2, has an electrostatic potential +V0.
The hemisphere r = a, π/2 < θ ≤π has an electrostatic potential −V0. Show that the
potential at interior points is
V = V0
∞
X
n=0
4n + 3
2n + 2
r
a
2n+1
P2n(0)P2n+1(cosθ)
= V0
∞
X
n=0
(−1)n (4n + 3)(2n −1)!!
(2n + 2)!!
r
a
2n+1
P2n+1(cosθ).
Hint. You need Exercise 15.1.12.
15.2.16
A conducting sphere of radius a is divided into two electrically separate hemispheres by
a thin insulating barrier at its equator. The top hemisphere is maintained at a potential
V0, and the bottom hemisphere at −V0.
(a)
Show that the electrostatic potential exterior to the two hemispheres is
V (r,θ) = V0
∞
X
s=0
(−1)s(4s + 3)(2s −1)!!
(2s + 2)!!
a
r
2s+2
P2s+1(cosθ).
(b)
Calculate the electric charge density σ on the outside surface. Note that your series
diverges at cosθ = ±1, as you expect from the inﬁnite capacitance of this system
(zero thickness for the insulating barrier).
ANS.
(b) σ = ε0En = −ε0
∂V
∂r

r=a
= ε0V0
∞
X
s=0
(−1)s(4s + 3)(2s −1)!!
(2s)!!
P2s+1(cosθ).

734
Chapter 15 Legendre Functions
15.2.17
By writing ϕs(x) = √(2s + 1)/2 Ps(x), a Legendre polynomial is renormalized to
unity. Explain how |ϕs⟩⟨ϕs| acts as a projection operator. In particular, show that if
| f ⟩= P
n a′
n|ϕn⟩, then
|ϕs⟩⟨ϕs| f ⟩= a′
s|ϕs⟩.
15.2.18
Expand x8 as a Legendre series. Determine the Legendre coefﬁcients from Eq. (15.40),
am = 2m + 1
2
1
Z
−1
x8Pm(x)dx.
Check your values against AMS-55, Table 22.9. (For the complete reference, see Addi-
tional Readings.) This illustrates the expansion of a simple function f (x).
Hint. Gaussian quadrature can be used to evaluate the integral.
15.2.19
Calculate and tabulate the electrostatic potential created by a ring of charge,
Example 15.2.3, for r/a = 1.5(0.5)5.0 and θ = 0◦(15◦)90◦. Carry terms through
P22(cosθ).
Note. The convergence of your series will be slow for r/a = 1.5. Truncating the series
at P22 limits you to about four-signiﬁcant-ﬁgure accuracy.
Check value. For r/a = 2.5 and θ = 60◦, ψ = 0.40272(q/4πε0r).
15.2.20
Calculate and tabulate the electrostatic potential created by a charged disk
(Exercise 15.2.14), for r/a = 1.5(0.5)5.0 and θ = 0◦(15◦)90◦. Carry terms through
P22(cosθ).
Check value. For r/a = 2.0 and θ = 15◦, ψ = 0.46638(q/4πε0r).
15.2.21
Calculate the ﬁrst ﬁve (nonvanishing) coefﬁcients in the Legendre series expansion
of f (x) = 1 −|x|, evaluating the coefﬁcients in the series by numerical integration.
Actually these coefﬁcients can be obtained in closed form. Compare your coefﬁcients
with those listed in Exercise 18.4.26.
ANS.
a0 = 0.5000, a2 = −0.6250, a4 = 0.1875, a6 = −0.1016, a8 = 0.0664.
15.2.22
Calculate and tabulate the exterior electrostatic potential created by the two charged
hemispheres of Exercise 15.2.16, for r/a = 1.5(0.5)5.0 and θ = 0◦(15◦)90◦. Carry
terms through P23(cosθ).
Check value. For r/a = 2.0 and θ = 45◦, V = 0.27066V0.
15.2.23
(a)
Given f (x) = 2.0, |x| < 0.5 and
f (x) = 0, 0.5 < |x| < 1.0, expand f (x) in a
Legendre series and calculate the coefﬁcients an through a80 (analytically).
(b)
Evaluate P80
n=0 an Pn(x) for x = 0.400(0.005)0.600. Plot your results.
Note. This illustrates the Gibbs phenomenon of Section 19.3 and the danger of trying to
calculate with a series expansion in the vicinity of a discontinuity.

15.2 Orthogonality
735
15.2.24
A plane wave may be expanded in a series of spherical waves by the Rayleigh equation,
eikr cosγ =
∞
X
n=0
an jn(kr)Pn(cosγ ).
Show that an = in(2n + 1).
Hint.
1.
Use the orthogonality of the Pn to solve for an jn(kr).
2.
Differentiate n times with respect to (kr) and set r = 0 to eliminate the
r-dependence.
3.
Evaluate the remaining integral by Exercise 15.1.15.
Note. This problem may also be treated by noting that both sides of the equation satisfy
the Helmholtz equation. The equality can be established by showing that the solutions
have the same behavior at the origin and also behave alike at large distances.
15.2.25
Verify the Rayleigh equation of Exercise 15.2.24 by starting with the following steps:
(a)
Differentiate with respect to (kr) to establish
X
n
an j′
n(kr)Pn(cosγ ) = i
X
n
an jn(kr)cosγ Pn(cosγ ).
(b)
Use a recurrence relation to replace cosγ Pn(cosγ ) by a linear combination of
Pn−1 and Pn+1.
(c)
Use a recurrence relation to replace j′
n by a linear combination of jn−1 and jn+1.
15.2.26
From Exercise 15.2.24 show that
jn(kr) = 1
2in
1
Z
−1
eikrµPn(µ)dµ.
This means that (apart from a constant factor) the spherical Bessel function jn(kr) is an
integral transform of the Legendre polynomial Pn(µ).
15.2.27
Rewriting the formula of Exercise 15.2.26 as
jn(z) = 1
2 (−i)n
π
Z
0
eiz cosθ Pn(cosθ)sinθ dθ,
n = 0,1,2,...,
verify it by transforming the right-hand side into
zn
2n+1n!
π
Z
0
cos(z cosθ)sin2n+1 θ dθ
and using Exercise 14.7.9.

736
Chapter 15 Legendre Functions
15.3
PHYSICAL INTERPRETATION OF GENERATING
FUNCTION
The generating function for the Legendre polynomials has an interesting and important
interpretation. If we introduce spherical polar coordinates (r,θ,ϕ) and place a charge q
at the point a on the positive z axis (see Fig. 15.5), the potential at a point (r,θ) (it is
independent of ϕ) can be calculated, using the law of cosines, as
ψ(r,θ) =
q
4πε0
1
r1
=
q
4πε0
(r2 + a2 −2ar cosθ)−1/2.
(15.58)
The expression in Eq. (15.58) is essentially that appearing in the generating function; to
identify the correspondence we rewrite that equation as
ψ(r,θ) =
q
4πε0 r

1 −2 a
r cosθ + a2
r2
−1/2
=
q
4πε0 r g

cosθ, a
r

(15.59)
=
q
4πε0 r
∞
X
n=0
Pn(cosθ)
a
r
n
,
(15.60)
where we reached Eq. (15.60) by inserting the generating-function expansion.
The series in Eq. (15.60) only converges for r > a, with a rate of convergence that
improves as r/a increases. If, on the other hand, we desire an expression for ψ(r,θ) when
r < a, we can perform a different rearrangement of Eq. (15.58), to
ψ(r,θ) =
q
4πε0a

1 −2 r
a cosθ + r2
a2
−1/2
,
(15.61)
which we again recognize as the generating-function expansion, but this time with the
result
ψ(r,θ) =
q
4πε0a
∞
X
n=0
Pn(cosθ)
r
a
n
,
(15.62)
valid when r < a.
r1
q
q
4πε0r1
z= a
r
θ
ϕ =
z
FIGURE 15.5
Electrostatic potential, charge q displaced from origin.

15.3 Physical Interpretation of Generating Function
737
Expansion of 1/|r1 −r2|
Equations (15.60) and (15.62) describe the interaction of a charge q at position a = aˆez
with a unit charge at position r. Dropping the factors needed for an electrostatics calcula-
tion, these equations yield formulas for 1/|r −a|. The fact that a is aligned with the z-axis
is actually of no importance for the computation of 1/|r −a|; the relevant quantities are r,
a, and the angle θ between r and a. Thus, we may rewrite either Eq. (15.60) or (15.62) in a
more neutral notation, to give the value of 1/|r1 −r2| in terms of the magnitudes r1, r2 and
the angle between r1 and r2, which we now call χ. If we deﬁne r> and r< to be respec-
tively the larger and the smaller of r1 and r2, Eqs. (15.60) and (15.62) can be combined
into the single equation
1
|r1 −r2| = 1
r>
∞
X
n=0
r<
r>
n
Pn(cosχ),
(15.63)
which will converge everywhere except when r1 = r2.
Electric Multipoles
Returning to Eq. (15.60) and restricting consideration to r > a, we may note that its initial
term (with n = 0) gives the potential we would get if the charge q were at the origin, and
that further terms must describe corrections arising from the actual position of the charge.
One way to obtain further understanding of the second and later terms in the expansion is
to consider what would happen if we added a second charge, −q, at z = −a, as shown in
Fig. 15.6. The potential due to the second charge will be given by an expression similar to
that in Eq. (15.58), except that the signs of q and cosθ must be reversed (the angle opposite
r2 in the ﬁgure is π −θ). We now have
ψ =
q
4πε0
 1
r1
−1
r2

r1
r2
z = a
z = −a
−q
q
r
θ
z
q
4πε0
ϕ =
1
r1
1
r2
−
FIGURE 15.6
Electric dipole.

738
Chapter 15 Legendre Functions
=
q
4πε0 r
" 
1 −2a
r cosθ + a2
r2
−1/2
−

1 + 2a
r cosθ + a2
r2
−1/2 #
=
q
4πε0 r
" ∞
X
n=0
Pn(cosθ)
a
r
n
−
∞
X
n=0
Pn(cosθ)

−a
r
n
#
.
(15.64)
If we combine the two summations in Eq. (15.64), alternate terms cancel, and we get
ψ =
2q
4πε0 r
a
r P1(cosθ) + a3
r3 P3(cosθ) + ···

.
(15.65)
This conﬁguration of charges is called an electric dipole, and we note that its leading
dependence on r goes as r−2. The strength of the dipole (called the dipole moment) can
be identiﬁed as 2qa, equal to the magnitude of each charge multiplied by their separation
(2a). If we let a →0 while keeping the product 2qa constant at a value µ, all but the ﬁrst
term becomes negligible, and we have
ψ =
µ
4πε0
P1(cosθ)
r2
,
(15.66)
the potential of a point dipole of dipole moment µ, located at the origin of the coordinate
system (at r = 0). Note that because we have limited the discussion to situations of cylin-
drical symmetry, our dipole is oriented in the polar direction; more general orientations can
be considered after we have developed formulas for solutions of the associated Legendre
equation (cases where the parameter m in Eq. (15.4) is nonzero).
We can extend the above analysis by combining a pair of dipoles of opposite orienta-
tion, for example, in the conﬁguration shown in Fig. 15.7, thereby causing cancellation of
their leading terms, leaving a potential whose leading contribution will be proportional to
r−3P2(cosθ). A charge conﬁguration of this sort is called an electric quadrupole, and
the P2 term of the generating function expansion can be identiﬁed as the contribution of
a point quadrupole, also located at r = 0. Further extensions, to 2n-poles, with contri-
butions proportional to Pn(cosθ)/rn+1, permit us to identify each term of the generating
expansion with the potential of a point multipole. We thus have a multipole expansion.
Again we observe that because we have limited discussion to situations with cylindrical
symmetry our multipoles are presently required to be linear; that restriction will be elimi-
nated when this topic is revisited in Chapter 16.
We look next at more general charge distributions, for simplicity limiting consideration
to charges qi placed at respective positions ai on the polar axis of our coordinate system.
z = a
z = −a
q
q
−2q
z
FIGURE 15.7
Linear electric quadrupole.

15.3 Physical Interpretation of Generating Function
739
Adding together the generating-function expansions of the individual charges, our com-
bined expansion takes the form
ψ =
1
4πε0 r
"X
i
qi +
X
i
qiai
r
P1(cosθ) +
X
i
qia2
i
r2 P2(cosθ) + ···
#
=
1
4πε0 r
h
µ0 + µ1
r P1(cosθ) + µ2
r2 P2(cosθ) + ···
i
,
(15.67)
where the µi are called the multipole moments of the charge distribution; µ0 is the
20-pole, or monopole moment, with a value equal to the total net charge of the distribution;
µ1 is the 21-pole, or dipole moment, equal to P
i qiai; µ2 is the 22-pole, or quadrupole
moment, given as P
i qia2
i , etc. Our general (linear) multipole expansion will converge for
values of r that are larger than all the ai values of the individual charges. Put another way,
the expansion will converge at points further from the coordinate origin than all parts of
the charge distribution.
We next ask: What happens if we move the origin of our coordinate system? Or, equiv-
alently, consider replacing r by |r −rp|. For r > rp, the binomial expansion of 1/|r −rp|n
will have the generic form
1
|r −rp|n = 1
rn + C rp
rn+1 + ··· ,
with the result that only the leading nonzero term of Eq. (15.67) will be unaffected by
the change of expansion center. Translated into everyday language, this means that the
lowest nonzero moment of the expansion will be independent of the choice of origin, but
all higher moments will change when the expansion center is moved. Speciﬁcally, the total
net charge (monopole moment) will always be independent of the choice of expansion
center. The dipole moment will be independent of the expansion point only when the net
charge is zero; the quadrupole moment will have such independence only if both the net
charge and dipole moments vanish, etc.
We close this section with three observations.
•
First, while we have illustrated our discussion with discrete arrays of point charges, we
could have reached the same conclusions using continuous charge distributions, with
the result that the summations over charges would become integrals over the charge
density.
•
Second, if we remove our restriction to linear arrays, our expansion would involve
components of the multipole moments in different directions. In three-dimensional
space, the dipole moment would have three components: a generalizes to (ax,ay,az),
while the higher-order multipoles will have larger numbers of components (a2 →
axax, axay,...). The details of that analysis will be taken up when the necessary back-
ground is in place.
•
Third, the multipole expansion is not restricted to electrical phenomena, but applies
anywhere we have an inverse-square force. For example, planetary conﬁgurations are
described in terms of mass multipoles. And gravitational radiation depends on the time
behavior of mass quadrupoles.

740
Chapter 15 Legendre Functions
Exercises
15.3.1
Develop the electrostatic potential for the array of charges shown in Fig. 15.7. This is a
linear electric quadrupole.
15.3.2
Calculate the electrostatic potential of the array of charges shown in Fig. 15.8. Here is
an example of two equal but oppositely directed quadrupoles. The quadrupole contri-
butions cancel. The octopole terms do not cancel.
15.3.3
Show that the electrostatic potential produced by a charge q at z = a for r < a is
ϕ(r) =
q
4πε0a
∞
X
n=0
r
a
n
Pn(cosθ).
15.3.4
Using E = −∇ϕ, determine the components of the electric ﬁeld corresponding to the
(pure) electric dipole potential,
ϕ(r) = 2aq P1(cosθ)
4πε0r2
.
Here it is assumed that r ≫a.
ANS.
Er = +4aq cosθ
4πε0r3 ,
Eθ = +2aq sinθ
4πε0r3 ,
Eϕ = 0.
15.3.5
Operating in spherical polar coordinates, show that
∂
∂z
 Pl(cosθ)
rl+1

= −(l + 1) Pl+1(cosθ)
rl+2
.
This is the key step in the mathematical argument that the derivative of one multipole
leads to the next higher multipole.
Hint. Compare with Exercise 3.10.28.
15.3.6
A point electric dipole of strength p(1) is placed at z = a; a second point electric dipole
of equal but opposite strength is at the origin. Keeping the product p(1)a constant, let
a →0. Show that this results in a point electric quadrupole.
Hint. Exercise 15.3.5 (when proved) will be helpful.
15.3.7
A point electric octupole may be constructed by placing a point electric quadrupole
(pole strength p(2) in the z-direction) at z = a and an equal but opposite point elec-
tric quadrupole at z = 0 and then letting a →0, subject to p(2)a = constant. Find the
electrostatic potential corresponding to a point electric octupole. Show from the con-
struction of the point electric octupole that the corresponding potential may be obtained
by differentiating the point quadrupole potential.
q
−q
−2q
+2q
z = −2a
−a
a
2a
z
FIGURE 15.8
Linear electric octopole.

15.4 Associated Legendre Equation
741
q
q′
a′
a
z
FIGURE 15.9
Image charges for Exercise 15.3.8.
15.3.8
A point charge q is in the interior of a hollow conducting sphere of radius r0. The
charge q is displaced a distance a from the center of the sphere. If the conducting
sphere is grounded, show that the potential in the interior produced by q and the dis-
tributed induced charge is the same as that produced by q and its image charge q′. The
image charge is at a distance a′ = r2
0/a from the center, collinear with q and the origin
(Fig. 15.9).
Hint. Calculate the electrostatic potential for a < r0 < a′. Show that the potential vani-
shes for r = r0 if we take q′ = −qr0/a.
15.4
ASSOCIATED LEGENDRE EQUATION
We need to extend our analysis to the associated Legendre equation because it is important
to be able to remove the restriction to azimuthal symmetry that pervaded the discussion
of the previous sections of this chapter. We therefore return to Eq. (15.4), which, before
determining what its eigenvalue should be, assumed the form
(1 −x2)P′′(x) −2x P′(x) +

λ −
m2
1 −x2

P(x) = 0.
(15.68)
Trial and error (or great insight) suggests that the troublesome factor 1 −x2 in the
denominator of this equation can be eliminated by making a substitution of the form
P = (1−x2)p P, and further experimentation shows that a suitable choice for the exponent
p is m/2. By straightforward differentiation, we ﬁnd
P = (1 −x2)m/2P,
(15.69)
P′ = (1 −x2)m/2P′ −mx(1 −x2)m/2−1P,
(15.70)
P′′ = (1 −x2)m/2P′′ −2mx(1 −x2)m/2−1P′
+
h
−m(1 −x2)m/2−1 + (m2 −2m)x2(1 −x2)m/2−2i
P.
(15.71)
Substitution of Eqs. (15.69)–(15.71) into Eq. (15.68), we obtain an equation that is poten-
tially easier to solve, namely,
(1 −x2)P′′ −2x(m + 1)P′ +
h
λ −m(m + 1)
i
P = 0.
(15.72)
We continue by seeking to solve Eq. (15.72) by the method of Frobenius, assuming a
solution in the series form P
j a j xk+ j. The indicial equation for this ODE has solutions

742
Chapter 15 Legendre Functions
k = 0 and k = 1. For k = 0, substitution into the series solution leads to the recurrence
formula
a j+2 = a j
 j2 + (2m + 1) j −λ + m(m + 1)
( j + 1)( j + 2)

.
(15.73)
Just as for the original Legendre equation, we need solutions P(cosθ) that are nonsingular
for the range −1 ≤cosθ ≤+1, but the recurrence formula leads to a power series that in
general is divergent at ±1.2
To avoid the divergence, we must cause the numerator of the fraction in Eq. (15.73) to
become zero for some nonnegative even integer j, thereby causing P to be a polynomial.
By direct substitution into Eq. (15.73), we can verify that a zero numerator is obtained for
j = l −m when λ is assigned the value l(l + 1), a condition that can only be met if l is an
integer at least as large as m and of the same parity. Further analysis for the other indicial
equation solution, k = 1, extends our present result to values of l that are larger than m and
of opposite parity.
Summarizing our results to this point, we have found that the regular solutions to the
associated Legendre equation depend on integer indices l and m. Letting Pm
l , called an
associated Legendre function, denote such a solution (note that the superscript m is not
an exponent), we deﬁne
Pm
l (x) = (1 −x2)m/2Pm
l (x),
(15.74)
where Pm
l
is a polynomial of degree l −m (consistent with our earlier observation that
l must be at least as large as m), and with an explicit form and scale that we will now
address.
A convenient explicit formula for Pm
l can be obtained by repeated differentiation of the
regular Legendre equation. Admittedly, this strategy would have been difﬁcult to devise
without prior knowledge of the solution, but there are certain advantages to using the
experience of those who have gone before. So, without apology, we apply Leibniz’s for-
mula for the mth derivative of a product (proved in Exercise 1.4.2),
dm
dxm
h
A(x)B(x)
i
=
m
X
s=0
m
s
dm−s A(x)
dxm−s
ds B(x)
dxs
,
(15.75)
to the Legendre equation,
(1 −x2)P′′
l −2x P′
l + l(l + 1)Pl = 0,
reaching
(1 −x2)u′′ −2x(m + 1)u′ +
h
l(l + 1) −m(m + 1)
i
u = 0,
(15.76)
where
u ≡dm
dxm Pl(x).
(15.77)
2The solution to the associated Legendre equation is (1 −x2)m/2P(x), suggesting the possibility that the (1 −x2)m/2 factor
might compensate the divergence in P(x), yielding a convergent limit. It can be shown that such a compensation does not occur.

15.4 Associated Legendre Equation
743
Comparing Eq. (15.76) with Eq. (15.72), we see that when λ = l(l + 1) they are identical,
meaning that the polynomial solutions P of Eq. (15.72) for given l can be identiﬁed with
the corresponding u. Speciﬁcally,
Pm
l = (−1)m dm
dxm Pl(x),
(15.78)
where the factor (−1)m is inserted to maintain agreement with AMS-55 (see Additional
Readings), which has become the most widely accepted notational standard.3
We can now write a complete, explicit form for the associated Legendre functions:
Pm
l (x) = (−1)m(1 −x2)m/2 dm
dxm Pl(x).
(15.79)
Since the Pm
l
with m = 0 are just the original Legendre functions, it is customary to omit
the upper index when it is zero, so, for example, P0
l ≡Pl.
Note that the condition on l and m can be stated in two ways:
(1)
For each m, there are an inﬁnite number of acceptable solutions to the associated
Legendre ODE with l values ranging from m to inﬁnity, or
(2)
For each l, there are acceptable solutions with m values ranging from l = 0 to l = m.
Because m enters the associated Legendre equation only as m2, we have up to this point
tacitly considered only values m ≥0. However, if we insert the Rodrigues formula for Pl
into Eq. (15.73), we get the formula
Pm
l (x) = (−1)m
2l l!
(1 −x2)m/2 dl+m
dxl+m (x2 −1)l,
(15.80)
which gives results for −m that do not appear similar to those for +m. However, it can be
shown that if we apply Eq. (15.75) for m values between zero and −l, we get
P−m
l
(x) = (−1)m (l −m)!
(l + m)! Pm
l (x).
(15.81)
Equation (15.81) shows that Pm
l
and P−m
l
are proportional; its proof is the topic of
Exercise 15.4.3. The main reason for discussing both is that recurrence formulas we will
develop for Pm
l
with contiguous values of m will give results for m < 0 that can best be
understood if we remember the relative scaling of Pm
l
and P−m
l
.
Associated Legendre Polynomials
For further development of properties of the Pm
l , it is useful to develop a generating func-
tion for the polynomials Pm
l (x), which we can do by differentiating the Legendre generat-
ing function with respect to x. The result is
gm(x,t) ≡
(−1)m(2m −1)!!
(1 −2xt + t2)m+1/2 =
∞
X
s=0
Pm
s+m(x)ts.
(15.82)
3However, we note that the popular text, Jackson’s Electrodynamics (see Additional Readings), does not include this phase
factor. The factor is introduced to cause the deﬁnition of spherical harmonics (Section 15.5) to have the usual phase convention.

744
Chapter 15 Legendre Functions
The factors t that result from differentiating the generating function have been used to
change the powers of t that multiply the P on the right-hand side.
If we now differentiate Eq. (15.82) with respect to t, we obtain initially
(1 −2tx + t2)∂gm
∂t
= (2m + 1)(x −t)gm(x,t),
which we can use together with Eq. (15.82) in a now-familiar way to obtain the recurrence
formula,
(s + 1)Pm
s+m+1(x) −(2m + 1 + 2s)xPm
s+m(x) + (s + 2m)Pm
s+m−1 = 0.
(15.83)
Making the substitution l = s + m, we bring Eq. (15.83) to the more useful form,
(l −m + 1)Pm
l+1 −(2l + 1)xPm
l + (l + m)Pm
l−1 = 0.
(15.84)
For m = 0 this relation agrees with Eq. (15.18).
From the form of gm(x,t), it is also clear that
(1 −2xt + t2)gm+1(x,t) = −(2m + 1)gm(x,t).
(15.85)
From Eqs. (15.85) and (15.82) we may extract the recursion formula
Pm+1
s+m+1(x) −2xPm+1
s+m (x) + Pm+1
s+m−1(x) = −(2m + 1)Pm
s+m(x),
which relates the associated Legendre polynomials with upper index m + 1 to those with
upper index m. Again we may simplify by making the substitution l = s + m:
Pm+1
l+1 (x) −2xPm+1
l
(x) + Pm+1
l−1 (x) = −(2m + 1)Pm
l (x).
(15.86)
Associated Legendre Functions
The recurrence relations for the associated Legendre polynomials or alternatively, differ-
entiation of formulas for the original Legendre polynomials, enable the construction of
recurrence formulas for the associated Legendre functions. The number of such formulas
is extensive because these functions have two indices, and there exists a wide variety of
formulas with different index combinations. Results of importance include the following:
Pm+1
l
(x) +
2mx
(1 −x2)1/2 Pm
l (x) + (l + m)(l −m + 1)Pm−1
l
(x) = 0,
(15.87)
(2l + 1)x Pm
l (x) = (l + m)Pm
l−1(x) + (l −m + 1)Pm
l+1(x),
(15.88)
(2l + 1)(1 −x2)1/2Pm
l (x) = Pm+1
l−1 (x) −Pm+1
l+1 (x)
(15.89)
= (l −m + 1)(l −m + 2)Pm−1
l+1 (x)
−(l + m)(l + m −1)Pm−1
l−1 (x),
(15.90)
(1 −x2)1/2
Pm
l (x)
 ′
= 1
2 (l + m)(l −m + 1)Pm−1
l
(x) −1
2 Pm+1
l
(x),
(15.91)
= (l + m)(l −m + 1)Pm−1
l
(x) +
mx
(1 −x2)1/2 Pm
l (x).
(15.92)

15.4 Associated Legendre Equation
745
Table 15.3
Associated Legendre Functions
P1
1 (x) = −(1 −x2)1/2 = −sinθ
P1
2 (x) = −3x(1 −x2)1/2 = −3cosθ sinθ
P2
2 (x) = 3(1 −x2) = 3sin2 θ
P1
3 (x) = −3
2 (5x2 −1)(1 −x2)1/2 = −3
2(5cos2 θ −1)sinθ
P2
3 (x) = 15x(1 −x2) = 15cosθ sin2 θ
P3
3 (x) = −15(1 −x2)3/2 = −15sin3 θ
P1
4 (x) = −5
2 (7x3 −3x)(1 −x2)1/2 = −5
2(7cos3 θ −3cosθ)sinθ
P2
4 (x) = 15
2 (7x2 −1)(1 −x2) = 15
2 (7cos2 θ −1)sin2 θ
P3
4 (x) = −105x(1 −x2)3/2 = −105cosθ sin3 θ
P4
4 (x) = 105(1 −x2)2 = 105sin4 θ
It is obvious that, using Eq. (15.90), all the Pm
l
with m > 0 can be generated from those
with m = 0 (the Legendre polynomials), and that these, in turn, can be built recursively
from P0(x) = 1 and P1(x) = x. In this fashion (or in other ways as suggested below), we
can build a table of associated Legendre functions, the ﬁrst members of which are listed in
Table 15.3. The table shows the Pm
l (x) both as functions of x and as functions of θ, where
x = cosθ.
It is often easier to use recurrence formulas other than Eq. (15.90) to obtain the Pm
l ,
keeping in mind that when a formula contains Pm
m−1 for m > 0, that quantity can be set to
zero. It is also easy to obtain explicit formulas for certain values of l and m which can then
be alternate starting points for recursion. See the example that follows.
Example 15.4.1
RECURRENCE STARTING FROM Pm
m
The associated Legendre function Pm
m (x) is easily evaluated:
Pm
m (x) = (−1)m
2m m! (1 −x2)m/2 d2m
dx2m (x2 −1)m = (−1)m
2m m! (2m)! (1 −x2)m/2
= (−1)m(2m −1)!!(1 −x2)m/2.
(15.93)
We can now use Eq. (15.88) with l = m to obtain Pm
m+1, dropping the term containing
Pm
m−1 because it is zero. We get
Pm
m+1(x) = (2m + 1)x Pm
m (x) = (−1)m(2m + 1)!! x(1 −x2)m/2.
(15.94)
Further increases in l can now be obtained by straightforward application of Eq. (15.88).
Illustrating for a series of Pm
l
with m = 2: P2
2 (x) = (−1)2(3!!)(1 −x2) = 3(1 −x2),
in agreement with the table value. P2
3 can be computed from Eq. (15.94) as P2
3 (x) =
(−1)2(5!!)x(1 −x2), which simpliﬁes to the tabulated result. Finally, P2
4 is obtained from

746
Chapter 15 Legendre Functions
the following case of Eq. (15.88):
7x P2
3 (x) = 5P2
2 (x) + 2P2
4 (x),
the solution of which for P2
4 (x) is again in agreement with the tabulated value.
■
Parity and Special Values
We have already established that Pl has even parity if l is even and odd parity if l is odd.
Since we can form Pm
l
by differentiating Pl m times, with each differentiation changing
the parity, and thereafter multiplying by (1−x2)m/2, which has even parity, Pm
l
must have
a parity that depends on l + m, namely,
Pm
l (−x) = (−1)l+m Pm
l (x).
(15.95)
We occasionally encounter a need for the value of Pm
l (x) at x = ±1 or at x = 0. At
x = ±1 the result is simple: The factor (1 −x2)m/2 causes Pm
l (±1) to vanish unless m =
0, in which case we recover the values Pl(1) = 1, Pl(−1) = (−1)l. At x = 0, the value
of Pm
l
depends on whether l + m is even or odd. The result, proof of which is left to
Exercises 15.4.4 and 15.4.5, is
Pm
l (0) =



(−1)(l+m)/2 (l + m −1)!!
(l −m)!!
,
l + m even,
0,
l + m odd.
(15.96)
Orthogonality
For each m, the Pm
l
of different l can be proved orthogonal by identifying them as
eigenfunctions of a Sturm-Liouville system. However, it is instructive to demonstrate the
orthogonality explicitly, and to do so by a method that also yields their normalization.
We start by writing the orthgonality integral, with the Pm
l
given by the Rodrigues for-
mula in Eq. (15.80). For compactness and clarity, we introduce the abbreviated notation
R = x2 −1, thereby getting
1
Z
−1
Pm
p (x)Pm
q (x)dx =
(−1)m
2p+q p!q!
1
Z
−1
Rm
d p+m R p
dx p+m
dq+m Rq
dxq+m

dx.
(15.97)
We consider ﬁrst the case p < q, for which we plan to prove the integral in Eq. (15.97)
vanishes. We proceed by carrying out repeated integrations by parts, in which we differ-
entiate
u = Rm
d p+m R p
dx p+m

(15.98)
p + m + 1 times while integrating a like number of times the remainder of the integrand,
dv =
dq+m Rq
dxq+m

dx.
(15.99)

15.4 Associated Legendre Equation
747
For each of these p + m + 1 ≤q + m partial integrations the integrated (uv) terms will
vanish because there will be at least one factor R that is not differentiated and will therefore
vanish at x = ±1. After the repeated differentiation, we will have
d p+m+1
dx p+m+1 u = d p+m+1
dx p+m+1

Rm
d p+m R p
dx p+m

,
(15.100)
in which a quantity whose largest power of x is x2p+2m contains also a (2p +2m +1)-fold
differentiation. There is no way these components can yield a nonzero result. Since both
the integrated terms and the transformed integral vanish, we get an overall vanishing result,
conﬁrming the orthogonality. Note that the orthogonality is with unit weight, independent
of the value of m.
We now examine Eq. (15.97) for p = q, repeating the process we just carried out, but
this time performing p + m partial integrations. Again all the integrated terms vanish,
but now there is a nonvanishing contribution from the repeated differentiation of u, see
Eq. (15.98). Since the overall power of x is still x2p+2m and the total number of differ-
entiations is also 2p + 2m, the only contributing terms are those in which the factor Rm
is differentiated 2m times and the factor R p is differentiated 2p times. Thus, applying
Leibniz’s formula, Eq. (15.75), to the p + m-fold differentiation of u, but keeping only the
contributing term, we have
d p+m
dx p+m

Rm
d p+m R p
dx p+m

=
p + m
2m
d2m Rm
dx2m
d2p R p
dx2p

=
(p + m)!
(2m)!(p −m)! (2m)!(2p)! = (p + m)!
(p −m)! (2p)!. (15.101)
Inserting this result into the integration by parts, remembering that the transformed
integration is accompanied by the sign factor (−1)p+m, and recognizing that the repeated
integration of dv, Eq. (15.99) with q = p, just yields R p, we have, returning to Eq. (15.97),
1
Z
−1
h
Pm
p (x)
i2
dx = (−1)2m+p
22p p ! p !
(p + m)!
(p −m)! (2p)!
1
Z
−1
R p dx.
(15.102)
To complete the evaluation, we identify the integral of R p as a beta function, with an
evaluation given in Exercise 13.3.3 as
1
Z
−1
R p dx = (−1)p
2(2p)!!
(2p + 1)!! = (−1)p 22p+1 p ! p !
(2p + 1)! .
(15.103)
Inserting this result, and combining with the previously established orthogonality relation,
we have
1
Z
−1
Pm
p (x)Pm
q (x)dx =
2
2p + 1
(p + m)!
(p −m)! δpq.
(15.104)

748
Chapter 15 Legendre Functions
Making the substitution x = cosθ, we obtain this formula in spherical polar coordinates:
π
Z
0
Pm
p (cosθ)Pm
q (cosθ)sinθ dθ =
2
2p + 1
(p + m)!
(p −m)! δpq.
(15.105)
Another way to look at the orthogonality of the associated Legendre functions is to
rewrite Eq. (15.104) in terms of the associated Legendre polynomials Pm
l . Invoking
Eq. (15.74), Eq. (15.104) becomes
1
Z
−1
Pm
p Pm
q (1 −x2)mdx =
2
2p + 1
(p + m)!
(p −m)! δpq,
(15.106)
showing that these polynomials are, for each m, orthogonal with the weight factor (1 −
x2)m. From that viewpoint, we can observe that each value of m corresponds to a set of
polynomials that are orthogonal with a different weight. However, since our main interest
is in the functions that are in general not polynomials but are solutions of the associated
Legendre equation, it is usually more relevant to us to note that these functions, which
include the factor (1 −x2)m/2, are orthogonal with unit weight.
It is possible, but not particularly useful, to note that we can also have orthogonality of
the Pm
l
with respect to the upper index when the lower index is held constant:
1
Z
−1
Pm
l (x)Pn
l (x)(1 −x2)−1dx = (l + m)!
m(l −m)! δmn.
(15.107)
This equation is not very useful because in spherical polar coordinates the boundary con-
dition on the azimuthal coordinate ϕ causes there already to be orthogonality with respect
to m, and we are not usually concerned with orthogonality of the Pm
l
with respect to m.
Example 15.4.2
CURRENT LOOP—MAGNETIC DIPOLE
An important problem in which we encounter associated Legendre functions is in the mag-
netic ﬁeld of a circular current loop, a situation that may at ﬁrst seem surprising since this
problem has azimuthal symmetry.
Our starting point is the formula relating a current element Ids to the vector potential
A that it produces (this is discussed in the chapter on Green’s functions, and also in texts
such as Jackson’s Classical Electrodynamics; see Additional Readings). This formula is
dA(r) = µ0
4π
Ids
|r −rs|,
(15.108)
where r is the point at which A is to be evaluated and rs is the position of element ds of the
current loop. We place our current loop, of radius a, in the equatorial plane of a spherical
polar coordinate system, as shown in Fig. 15.10. Our task is to determine A as a function
of position, and therefrom to obtain the components of the magnetic induction ﬁeld B.

15.4 Associated Legendre Equation
749
y
x
z
r
ds
rs
a
FIGURE 15.10
Circular current loop.
It is in principle possible to ﬁgure out the geometry and integrate Eq. (15.108) for the
present problem, but a more practical approach will be to determine from general consid-
erations the functional form of an expansion describing the solution, and then to determine
the coefﬁcients in the expansion by requiring correct results for points of high symmetry,
where the calculation is not too difﬁcult. This is an approach similar to that employed in
Example 15.2.3, where we ﬁrst identiﬁed the functional form of an expansion giving the
potential generated by a circular ring of charge, after which we found the coefﬁcients in
the expansion from the easily computed potential on the axis of the ring.
From the form of Eq. (15.108) and the symmetry of the problem, we see immediately
that for all r, A must lie in a plane of constant z, and in fact it must be in the ˆeϕ direction,
with Aϕ independent of ϕ, i.e.,
A = Aϕ(r,θ) ˆeϕ.
(15.109)
If A had a component other than Aϕ, it would have a nonzero divergence, as then A would
have a nonzero inward or outward ﬂux, resulting in a singularity on the axis of the loop.
Since everywhere except on the current loop itself there is no current, Maxwell’s equa-
tion for the curl of B reduces to
∇× B = ∇× (∇× A) = 0,
and, since A has only a ϕ component, it further reduces to
∇×
h
∇× Aϕ(r,θ) ˆeϕ
i
= 0.
(15.110)
The left-hand side of Eq. (15.110) was the subject of Example 3.10.4, and its evaluation
was presented as Eq. (3.165). Setting that result to zero gives the equation that must be
satisﬁed by Aϕ(r,θ):
∂2Aϕ
∂r2 + 2
r
∂Aϕ
∂r
+
1
r2 sinθ
∂
∂θ

sinθ ∂Aϕ
∂θ

−
1
r2 sin2 θ
Aϕ = 0.
(15.111)

750
Chapter 15 Legendre Functions
Equation (15.111) may now be solved by the method of separation of variables; setting
Aϕ(r,θ) = R(r)2(θ), we have
r2 d2R
dr2 + 2d R
dr −l(l + 1)R = 0,
(15.112)
1
sinθ
d
dθ

sinθ d2
dθ

+ l(l + 1)2 −
2
sin2 θ
= 0.
(15.113)
Because the second of these equations can be recognized as the associated Legendre equa-
tion, in the form given as Eq. (15.2), we have set the separation constant to the value it must
have, namely l(l + 1), with l integral. The ﬁrst equation is also familiar, with solutions for
a given l being rl and r−l−1. The second equation has solutions P1
l (cosθ), i.e., its speciﬁc
form dictates that the associated Legendre functions which solve it must have upper index
m = 1. Since our main interest is in the pattern of B at r values larger than a, the radius of
the current loop, we retain only the radial solution r−l−1, and write
Aϕ(r,θ) =
∞
X
l=1
cl
a
r
l+1
P1
l (cosθ).
(15.114)
When we obtain a more detailed solution, we will ﬁnd that it converges only for r > a,
so Eq. (15.114) and the value of B derived therefrom will only be valid outside a sphere
containing the current loop. If we were also interested in solving this problem for r < a,
we would need to construct a series solution using only the powers rl.
From Eq. (15.114) we can compute the components of B. Clearly, Bϕ = 0. And, using
Eq. (3.159), we have
Br(r,θ) = ∇× Aϕ ˆeϕ

r = cotθ
r
Aϕ + 1
r
∂Aϕ
∂θ ,
(15.115)
Bθ(r,θ) = ∇× Aϕ ˆeϕ

θ = −1
r
∂(r Aϕ)
∂r
.
(15.116)
To evaluate the θ derivative in Eq. (15.115), we need
d P1
l (cosθ)
dθ
= −sinθ d P1
l (cosθ)
d cosθ
= −l(l + 1)Pl(cosθ) −cotθ P1
l (cosθ),
(15.117)
a special case of Eq. (15.92) with m = 1 and x = cosθ. It is now straightforward to insert
the expansion for Aϕ into Eqs. (15.115) and (15.116); because of Eq. (15.117) the cotθ
term of Eq. (15.115) cancels, and we reach
Br(r,θ) = −1
r
∞
X
l=1
l(l + 1)cl
a
r
l+1
Pl(cosθ),
(15.118)
Bθ(r,θ) = 1
r
∞
X
l=1
l cl
a
r
l+1
P1
l (cosθ).
(15.119)
To complete our analysis, we must determine the values of the cl, which we do by using
the Biot-Savart law to calculate Br at points along the polar axis, where Br is synonymous

15.4 Associated Legendre Equation
751
with Bz. Since θ = 0 on the positive polar axis and Pl(cosθ) = 1, Eq. (15.118) reduces to
Br(z,0) = −1
z
∞
X
l=1
l(l + 1)cl
a
z
l+1
= −a2
z3
∞
X
s=0
(s + 1)(s + 2)cs+1
a
z
s
.
(15.120)
The symmetry of the problem permits one more simpliﬁcation; the value of Bz must be the
same at −z as at z, from which we conclude that the coefﬁcients c2, c4,...must all vanish,
and we can rewrite Eq. (15.120) as
Br(z,0) = −a2
z3
∞
X
s=0
2(s + 1)(2s + 1)c2s+1
a
z
2s
.
(15.121)
The Biot-Savart law (in SI units) gives the contribution from the current element I ds to
B at a point whose displacement from the current element is rs as
dB = µ0
4π I ds × ˆrs
r2s
.
(15.122)
We now compute B by integration of ds around the current loop. The geometry is shown
in Fig. 15.11. Note that d Bz, which will be the same for all current elements I ds, has the
value
d Bz = µ0I
4πr2s
sinχ ds,
z
a
I
ˆr
dB
ds
→
rs=
a2+z2
FIGURE 15.11
Biot-Savart law applied to a circular loop.

752
Chapter 15 Legendre Functions
where χ is the labeled angle in Fig. 15.11 and rs has the value indicated in the ﬁgure. The
integration over s simply yields a factor 2πa, and we see that sinχ = a/(a2 + z2)1/2, so
Bz = µ0Ia2
2
(a2 + z2)−3/2 = µ0Ia2
2z3

1 + a2
z2
−3/2
= µ0Ia2
2z3
∞
X
s=0
(−1)s (2s + 1)!!
(2s)!!
a
z
2s
.
(15.123)
The binomial expansion in the second line of Eq. (15.123) is convergent for z > a.
We are now ready to reconcile Eqs. (15.121) and (15.123), ﬁnding that
−2(s + 1)(2s + 1)c2s+1 = µ0I
2 (−1)s (2s + 1)!!
(2s)!!
,
which reduces to
c2s+1 = µ0I
2 (−1)s+1 (2s −1)!!
(2s + 2)!!.
(15.124)
We write ﬁnal formulas for A and B in a form that recognizes that c2s = 0, applicable
for r > a:
Aϕ(r,θ) = a2
r2
∞
X
s=0
c2s+1
a
r
2s
P1
2s+1(cosθ),
(15.125)
Br(r,θ) = −a2
r2
∞
X
s=0
(2s + 1)(2s + 2)c2s+1
a
r
2s
P2s+1(cosθ),
(15.126)
Bθ(r,θ) = a2
r3
∞
X
s=0
(2s + 1)c2s+1
a
r
2s
P1
2s+1(cosθ).
(15.127)
These formulas can also be written in terms of complete elliptic integrals. See Smythe
(Additional Readings) and Section 18.8 of this book.
A comparison of magnetic current loop and ﬁnite electric dipole ﬁelds may be of interest.
For the magnetic loop dipole, the preceding analysis gives
Br(r,θ) = µ0Ia2
2r3

P1 −3
2
a
r
2
P3 + ···

,
(15.128)
Bθ(r,θ) = µ0Ia2
4r3

−P1
1 + 3
4
a
r
2
P1
3 + ···

.
(15.129)
From the ﬁnite electric dipole potential, Eq. (15.65), one can ﬁnd
Er(r,θ) =
qa
πε0r3

P1 + 2
a
r
2
P3 + ···

,
(15.130)
Eθ(r,θ) =
qa
2πε0r3

−P1
1 −
a
r
2
P1
3 + ···

.
(15.131)

15.4 Associated Legendre Equation
753
The leading terms of both ﬁelds agree, and this is the basis for identifying both as dipole
ﬁelds.
As with electric multipoles, it is sometimes convenient to discuss point magnetic mul-
tipoles. A point dipole can be formed by taking the limit a →0, I →∞, with Ia2 held
constant. The magnetic moment m is taken to be Iπa2n, where n is a unit vector perpen-
dicular to the plane of the current loop and in the sense given by the right-hand rule.
■
Exercises
15.4.1
Apply the Frobenius method to Eq. (15.72) to obtain Eq. (15.73) and verify that the
numerator of that equation becomes zero if λ = l(l + 1) and j = l −m.
15.4.2
Starting from the entries for P2
2 and P1
2 in Table 15.3, apply a recurrence formula to
obtain P0
2 (which is P2), P−1
2
, and P−2
2 . Compare your results with the value of P2 from
Table 15.1 and with values of P−1
2
and P−2
2
obtained by applying Eq. (15.81) to entries
from Table 15.3.
15.4.3
Prove that
P−m
l
(x) = (−1)m (l −m)!
(l + m)! Pm
l (x),
where Pm
l (x) is deﬁned by
Pm
l (x) = (−1)m
2ll! (1 −x2)m/2 dl+m
dxl+m (x2 −1)l.
Hint. One approach is to apply Leibniz’s formula to (x + 1)l(x −1)l.
15.4.4
Show that
P1
2l(0) = 0,
P1
2l+1(0) = (−1)l+1 (2l + 1)!!
(2l)!!
,
by each of these three methods:
(a)
Use of recurrence relations,
(b)
Expansion of the generating function,
(c)
Rodrigues formula.
15.4.5
Evaluate Pm
l (0) for m > 0.
ANS.
Pm
l (0) =



(−1)(l+m)/2 (l + m −1)!!
(l −m)!!
,
l + m even,
0,
l + m odd.
15.4.6
Starting from the potential of a ﬁnite dipole, Eq. (15.65), verify the formulas for the
electric ﬁeld components given as Eqs. (15.130) and (15.131).

754
Chapter 15 Legendre Functions
15.4.7
Show that
Pl
l (cosθ) = (−1)l(2l −1)!! sinl θ,
l = 0,1,2,....
15.4.8
Derive the associated Legendre recurrence relation,
Pm+1
l
(x) +
2mx
(1 −x2)1/2 Pm
l (x) +
h
l(l + 1) −m(m −1)
i
Pm−1
l
(x) = 0.
15.4.9
Develop a recurrence relation that will yield P1
l (x) as
P1
l (x) = f1(x,l)Pl(x) + f2(x,l)Pl−1(x).
Follow either of the procedures (a) or (b):
(a)
Derive a recurrence relation of the preceding form. Give f1(x,l) and f2(x,l)
explicitly.
(b)
Find the appropriate recurrence relation in print.
(1)
Give the source.
(2)
Verify the recurrence relation.
ANS.
(a) P1
l (x) =
lx
(1 −x2)1/2 Pl −
l
(1 −x2)1/2 Pl−1.
15.4.10
Show that sinθ
d
d cosθ Pn(cosθ) = P1
n (cosθ).
15.4.11
Show that
(a)
π
Z
0
 
d Pm
l
dθ
d Pm
l′
dθ
+ m2Pm
l Pm
l′
sin2 θ
!
sinθ dθ = 2l(l + 1)
2l + 1
(l + m)!
(l −m)! δl l′,
(b)
π
Z
0
 
P1
l
sinθ
d P1
l′
dθ + P1
l′
sinθ
d P1
l
dθ
!
sinθ dθ = 0.
These integrals occur in the theory of scattering of electromagnetic waves by spheres.
15.4.12
As a repeat of Exercise 15.2.9, show, using associated Legendre functions, that
1
Z
−1
x(1 −x2)P′
n(x)P′
m(x)dx = n + 1
2n + 1
2
2n −1
n!
(n −2)! δm,n−1
+
n
2n + 1
2
2n + 3
(n + 2)!
n!
δm,n+1.
15.4.13
Evaluate
π
Z
0
sin2 θ P1
n (cosθ)dθ.

15.4 Associated Legendre Equation
755
15.4.14
The associated Legendre function Pm
l (x) satisﬁes the self-adjoint ODE
(1 −x2)d2Pm
l (x)
dx2
−2x d Pm
l (x)
dx
+

l(l + 1) −
m2
1 −x2

Pm
l (x) = 0.
From the differential equations for Pm
l (x) and Pk
l (x) show that for k ̸= m,
1
Z
−1
Pm
l (x)Pk
l (x)
dx
1 −x2 = 0.
15.4.15
Determine the vector potential and the magnetic induction ﬁeld of a magnetic
quadrupole by differentiating the magnetic dipole potential.
ANS.
AM Q = −µ0
2 (Ia2)(dz) P1
2 (cosθ)
r3
ˆeϕ+ higher-order terms,
BM Q = µ0(Ia2)(dz)
"
3P2(cosθ)
r4
ˆer −P1
2 (cosθ)
r4
ˆeθ
#
+ ···.
This corresponds to placing a current loop of radius a at z →dz and an oppositely
directed current loop at z →−dz. The vector potential and magnetic induction ﬁeld of
a point dipole are given by the leading terms in these expansions if we take the limit
dz →0, a →0, and I →∞subject to Ia2 dz = constant.
15.4.16
A single circular wire loop of radius a carries a constant current I.
(a)
Find the magnetic induction B for r < a, θ = π/2.
(b)
Calculate the integral of the magnetic ﬂux (B · dσ) over the area of the current
loop, that is,
a
Z
0
r dr
2π
Z
0
dϕ Bz

r,θ = π
2

.
ANS.
∞.
The Earth is within such a ring current, in which I approximates millions of amperes
arising from the drift of charged particles in the Van Allen belt.
15.4.17
The vector potential A of a magnetic dipole, dipole moment m, is given by A(r) =
(µ0/4π)(m × r/r3). Show by direct computation that the magnetic induction B = ∇×
A is given by
B = µ0
4π
3ˆr
 ˆr · m

−m
r3
.

756
Chapter 15 Legendre Functions
15.4.18
(a)
Show that in the point dipole limit the magnetic induction ﬁeld of the current loop
becomes
Br(r,θ) = µ0
2π
m
r3 P1(cosθ),
Bθ(r,θ) = −µ0
2π
m
r3 P1
1 (cosθ),
with m = Iπa2.
(b)
Compare these results with the magnetic induction of the point magnetic dipole of
Exercise 15.4.17. Take m = ˆzm.
15.4.19
A uniformly charged spherical shell is rotating with constant angular velocity.
(a)
Calculate the magnetic induction B along the axis of rotation outside the sphere.
(b)
Using the vector potential series of Example 15.4.2, ﬁnd A and then B for all
points outside the sphere.
15.4.20
In the liquid-drop model of the nucleus, a spherical nucleus is subjected to small de-
formations. Consider a sphere of radius r0 that is deformed so that its new surface is
given by
r = r0
h
1 + α2P2(cosθ)
i
.
Find the area of the deformed sphere through terms of order α2
2.
Hint.
dA =
"
r2 +
 dr
dθ
2#1/2
r sin θd θdϕ.
ANS.
A = 4πr2
0

1 + 4
5α2
2 + O
 α3
2

.
Note. The area element dA follows from noting that the line element ds for ﬁxed ϕ is
given by
ds = (r2 dθ2 + dr2)1/2 =
"
r2 +
 dr
dθ
2#1/2
dθ.
15.5
SPHERICAL HARMONICS
Our earlier discussion of separated-variable methods for solving the Laplace, Helmholtz,
or Schrödinger equations in spherical polar coordinates showed that the possible angular
solutions 2(θ)8(ϕ) are always the same in spherically symmetric problems; in particular
we found that the solutions for 8 depended on the single integer index m, and can be
written in the form
8m(ϕ) =
1
√
2π
eimϕ,
m = ...,−2,−1, 0, 1, 2,...,
(15.132)

15.5 Spherical Harmonics
757
or, equivalently,
8m(ϕ) =



1
√
2π
,
m = 0,
1
√π cosmϕ,
m > 0,
1
√π sin|m|ϕ,
m < 0.
(15.133)
The above equations contain the constant factors needed to make 8m normalized, and
those of different m2 are automatically orthogonal because they are eigenfunctions of
a Sturm-Liouville problem. It is straightforward to verify that in either Eq. (15.132) or
Eq. (15.133) our choices of the functions for +m and −m make 8m and 8−m orthogonal.
Formally, our deﬁnitions are such that
2π
Z
0
h
8m(ϕ)
i ∗
8m′(ϕ)dϕ = δmm′.
(15.134)
In Section 15.4 we found that the solutions 2(θ) could be identiﬁed as associated Leg-
endre functions that can be labeled by the two integer indices l and m, with −l ≤m ≤l.
From the orthonormality integral for these functions, Eq. (15.105), we can deﬁne the nor-
malized solutions
2lm(cosθ) =
s
2l + 1
2
(l −m)!
(l + m)! Pm
l (cosθ),
(15.135)
satisfying the relation
π
Z
0
h
2lm(cosθ)
i ∗
2l′m(cosθ)sinθ dθ = δll′.
(15.136)
We have previously noted that an orthonormality condition of this type only applies if both
functions 2 have the same value of the index m. The complex conjugate is not really nec-
essary in Eq. (15.136) because the 2 are real, but we write it anyway to maintain consistent
notation. Note also that when the argument of Pm
l
is x = cosθ, then (1 −x2)1/2 = sinθ,
so the Pm
l
are polynomials of overall degree l in cosθ and sinθ.
The product 2lm8m is called a spherical harmonic, with that name usually implying
that 8m is taken with the deﬁnition as a complex exponential; see Eq. (15.132). Therefore
we deﬁne
Y m
l (θ,ϕ) ≡
s
2l + 1
4π
(l −m)!
(l + m)! Pm
l (cosθ)eimϕ.
(15.137)
These functions, being normalized solutions of a Sturm-Liouville problem, are orthonor-
mal over the spherical surface, with
2π
Z
0
dϕ
π
Z
0
sinθ dθ
h
Y m1
l1 (θ,ϕ)
i∗
Y m2
l2 (θ,ϕ) = δl1l2δm1m2.
(15.138)

758
Chapter 15 Legendre Functions
The deﬁnition we introduced for the associated Legendre functions leads to speciﬁc signs
for the Y m
l
that are sometimes identiﬁed as the Condon-Shortley phase, after the authors
of a classic text on atomic spectroscopy. This sign convention has been found to simplify
various calculations, particularly in the quantum theory of angular momentum. One of
the effects of this phase factor is to introduce an alternation of sign with m among the
positive-m spherical harmonics. The word “harmonic” enters the name of Y m
l
because
solutions of Laplace’s equation are sometimes called harmonic functions.
The squares of the real parts of the ﬁrst few spherical harmonics are sketched in
Figure 15.12; their functional forms are given in Table 15.4.
Cartesian Representations
For some purposes it is useful to express the spherical harmonics using Cartesian coordi-
nates, which can be done by writing exp(±iϕ) as cosϕ ± i sinϕ and using the formulas
for x, y, z in spherical polar coordinates (retaining, however, an overall dependence on r,
necessary because the angular quantities must be independent of scale). For example,
cosθ = z/r,
sinθ exp(±iϕ) = sinθ cosϕ ± i sinθ sinϕ = x
r ± i y
r ;
(15.139)
these quantities are all homogeneous (of degree zero) in the coordinates.
Continuing to higher values of l, we obtain fractions in which the numerators are homo-
geneous products of x, y, z of overall degree l, divided by a common factor rl. Table 15.4
includes the Cartesian expression for each of its entries.
Overall Solutions
As we have already seen in Section 9.4, the separation of a Laplace, Helmholtz, or even a
Schrödinger equation in spherical polar coordinates can be written in terms of equations of
the generic form
R′′ + 2
r R′ +
h
f (r) −l(l + 1)
i
R = 0,
(15.140)
 1
sinθ
d
dθ

sinθ d
dθ

+
1
sin2 θ
d2
dϕ2 + l(l + 1)

Y m
l (θ,ϕ) = 0.
(15.141)
The function f (r) in Eq. (15.140) is zero for the Laplace equation, k2 for the Helmholtz
equation, and E −V (r) (V = potential energy, E = total energy, an eigenvalue) for the
Schrödinger equation. We have combined the θ and ϕ equations into Eq. (15.141) and
identiﬁed one of its solutions as Y m
l . What is important to note right now is that the com-
bined angular equation (and its boundary conditions and therefore its solutions) will be
the same for all spherically symmetric problems, and that the angular solution affects the
radial equation only through the separation constant l(l + 1). Thus, the radial equation will
have solutions that depend on l but are independent of the index m.
In Section 9.4 we solved the radial equation for the Laplace and Helmholtz equations,
with the results given in Table 9.2. For the Laplace equation ∇2ψ = 0, the general solution

15.5 Spherical Harmonics
759
m= 0, l =1
m= 1, l= 1
m= 0, l=2
m =1, l= 2
m= 2, l = 2
m= 0, l=3
m =1, l= 3
m= 2, l = 3
m = 3, l = 3
m= 0, l=0
FIGURE 15.12
Shapes of |ReY m
l (θ,ϕ)|2 for 0 ≤l ≤3, m = 0...l.
in spherical polar coordinates is a sum, with arbitrary coefﬁcients, of the solutions for the
various possible values of l and m:
ψ(r,θ,ϕ) =
∞
X
l=0
lX
m=−l

almrl + blmr−l−1
Y m
l (θ,ϕ);
(15.142)

760
Chapter 15 Legendre Functions
Table 15.4
Spherical Harmonics (Condon-Shortley Phase)
Y 0
0 (θ,ϕ) =
1
√
4π
Y 1
1 (θ,ϕ) = −
q
3
8π sinθ eiϕ = −
q
3
8π (x + iy)/r
Y 0
1 (θ,ϕ) =
q
3
4π cosθ =
q
3
4π z/r
Y −1
1
(θ,ϕ) = +
q
3
8π sinθ e−iϕ =
q
3
8π (x −iy)/r
Y 2
2 (θ,ϕ) =
q
5
96π 3sin2 θ e2iϕ = 3
q
5
96π (x2 −y2 + 2ixy)/r2
Y 1
2 (θ,ϕ) = −
q
5
24π 3sinθ cosθ eiϕ = −
q
5
24π 3z(x + iy)/r2
Y 0
2 (θ,ϕ) =
q
5
4π

3
2 cos2 θ −1
2

=
q
5
4π

3
2 z2 −1
2r2
/r2
Y −1
2
(θ,ϕ) =
q
5
24π 3sinθ cosθ e−iϕ = +
q
5
24π 3z(x −iy)/r2
Y −2
2
(θ,ϕ) =
q
5
96π 3sin2 θ e−2iϕ = 3
q
5
96π (x2 −y2 −2ixy)/r2
Y 3
3 (θ,ϕ) = −
q
7
2880π 15sin3 θ e3iϕ =−
q
7
2880π 15[x3−3xy2 + i(3x2y −y3)]/r3
Y 2
3 (θ,ϕ) =
q
7
480π 15cosθ sin2 θ e2iϕ =
q
7
480π 15z(x2 −y2 + 2ixy)/r3
Y 1
3 (θ,ϕ) = −
q
7
48π

15
2 cos2θ −3
2

sinθ eiϕ =−
q
7
48π

15
2 z2 −3
2r2
(x + iy)/r3
Y 0
3 (θ,ϕ) =
q
7
4π

5
2 cos3 θ −3
2 cosθ

=
q
7
4π z

5
2 z2 −3
2r2
/r3
Y −1
3
(θ,ϕ) = +
q
7
48π

15
2 cos2θ −3
2

sinθ e−iϕ =
q
7
48π

15
2 z2 −3
2r2
(x −iy)/r3
Y −2
3
(θ,ϕ) =
q
7
480π 15cosθ sin2 θ e−2iϕ =
q
7
480π 15z(x2 −y2 −2ixy)/r3
Y −3
3
(θ,ϕ) = +
q
7
2880π 15sin3 θ e−3iϕ =
q
7
2880π 15[x3−3xy2 −i(3x2y −y3)]/r3
for the Helmholtz equation (∇2 + k2)ψ = 0, the radial equation has the form given in
Eq. (14.148), so the general solution assumes the form
ψ(r,θ,ϕ) =
∞
X
l=0
lX
m=−l

alm jl(kr) + blm yl(kr)

Y m
l (θ,ϕ).
(15.143)
Laplace Expansion
Part of the importance of spherical harmonics lies in the completeness property, a conse-
quence of the Sturm-Liouville form of Laplace’s equation. Here this property means that
any function f (θ,ϕ) (with sufﬁcient continuity properties) evaluated over the surface of a
sphere can be expanded in a uniformly convergent double series of spherical harmonics.4
4For a proof of this fundamental theorem, see E. W. Hobson (Additional Readings), chapter VII.

15.5 Spherical Harmonics
761
This expansion, known as a Laplace series, takes the form
f (θ,ϕ) =
∞
X
l=0
lX
m=−l
clmY m
l (θ,ϕ),
(15.144)
with
clm =
D
Y m
l
 f (θ,ϕ)
E
=
2π
Z
0
dϕ
π
Z
0
sinθ dθ Y m
l (θ,ϕ)∗f (θ,ϕ).
(15.145)
A frequent use of the Laplace expansion is in specializing the general solution of the
Laplace equation to satisfy boundary conditions on a spherical surface. This situation is
illustrated in the following example.
Example 15.5.1
SPHERICAL HARMONIC EXPANSION
Consider the problem of determining the electrostatic potential within a charge-free spheri-
cal region of radius r0, with the potential on the spherical bounding surface speciﬁed as an
arbitrary function V (r0,θ,ϕ) of the angular coordinates θ and ϕ. The potential V (r,θ,ϕ) is
the solution of the Laplace equation satisfying the boundary condition at r = r0 and regular
for all r ≤r0. This means it must be of the form of Eq. (15.142), with the coefﬁcients blm
set to zero to ensure a solution that is nonsingular at r = 0.
We proceed by obtaining the spherical harmonic expansion of V (r0,θ,ϕ), namely
Eq. (15.144), with coefﬁcients
clm =
D
Y m
l (θ,ϕ)
V (r0,θ,ϕ)
E
.
Then, comparing Eq. (15.142), evaluated for r = r0,
V (r0,θ,ϕ) =
∞
X
l=0
lX
m=−l
alm rl
0 Y m
l (θ,ϕ),
with the expression from Eq. (15.144),
V (r0,θ,ϕ) =
∞
X
l=0
lX
m=−l
clmY m
l (θ,ϕ),
we see that alm = clm/rl
0, so
V (r,θ,ϕ) =
∞
X
l=0
lX
m=−l
clm
 r
r0
l
Y m
l (θ,ϕ).
■

762
Chapter 15 Legendre Functions
Example 15.5.2
LAPLACE SERIES—GRAVITY FIELDS
This example illustrates the notion that sometimes it is appropriate to replace the spherical
harmonics by their real counterparts (in terms of sine and cosine functions). The gravity
ﬁelds of the Earth, the Moon, and Mars have been described by a Laplace series of the
form
U(r,θ,ϕ) = GM
R
"
R
r −
∞
X
l=2
lX
m=0
 R
r
l+1 
ClmY e
ml(θ,ϕ) + SlmY o
ml(θ,ϕ)

#
.
(15.146)
Here M is the mass of the body, R is its equatorial radius, and G is the gravitational con-
stant. The real functions Y e
ml and Y o
ml are deﬁned by Morse and Feshbach (see Additional
Readings) as the unnormalized forms
Y e
ml(θ,ϕ) = Pm
l (cosθ)cosmϕ,
Y o
ml(θ,ϕ) = Pm
l (cosθ)sinmϕ.
Note that Morse and Feshbach place the m index before l. The normalization integrals for
Y e and Y o are the topic of Exercise 15.5.6.
Satellite measurements have led to the numerical values for C20, C22, and S22 shown in
Table 15.5.
Table 15.5
Gravity Field Coefﬁcients, Eq. (15.145).
Coefﬁcienta
Earth
Moon
Mars
C20
1.083 × 10−3
(0.200 ± 0.002) × 10−3
(1.96 ± 0.01) × 10−3
C22
0.16 × 10−5
(2.4 ± 0.5) × 10−5
(−5 ± 1) × 10−5
S22
−0.09 × 10−5
(0.5 ± 0.6) × 10−5
(3 ± 1) × 10−5
aC20 represents an equatorial bulge, whereas C22 and S22 represent an azimuthal
dependence of the gravitational ﬁeld.
■
Symmetry of Solutions
The angular solutions of given l but different m are closely related in that they lead to the
same solution for the radial equation. Except when l = 0, the individual solutions Y m
l
are
not spherically symmetric, and we must recognize that a spherically symmetric problem
can have solutions with less than the full spherical symmetry. A classical example of this
phenomenon is provided by the Earth-Sun system, which has a spherically symmetric grav-
itational potential. However, the actual orbit of the Earth is planar. This apparent dilemma
is resolved by noting that a solution exists for any orientation of the Earth’s orbital plane;
that actually occurring was determined by “initial conditions.”
Returning now to the Laplace equation, we see that a radial solution for given l, i.e., rl
or r−l−1, is associated with 2l + 1 different angular solutions Y m
l
(−l ≤m ≤l), no one
of which (for l ̸= 0) has spherical symmetry. The most general solution for this l must
be a linear combination of these 2l + 1 mutually orthogonal functions. Put another way,

15.5 Spherical Harmonics
763
the solution space of the angular solution of the Laplace equation for given l is a Hilbert
space containing the 2l +1 members Y −l
l
(θ,ϕ),...,Y l
l (θ,ϕ). Now, if we write the Laplace
equation in a coordinate system (θ′,ϕ′) oriented differently than the original coordinates,
we must still have the same angular solution set, meaning that Y m
l (θ′,ϕ′) must be a linear
combination of the original Y m
l . Thus, we may write
Y m
l (θ′,ϕ′) =
lX
m′=−l
Dl
m′mY m′
l (θ,ϕ),
(15.147)
where the coefﬁcients D depend on the coordinate rotation involved. Note that a coordi-
nate rotation cannot change the r dependence of our solution to the Laplace equation, so
Eq. (15.147) does not need to include a sum over all values of l. As a speciﬁc example, we
see (Fig. 15.12) that for l = 1 we have three solutions that appear similar, but with differ-
ent orientations. Alternatively, from Table 15.4 we see that the angular solutions Y m
1 have
forms proportional to z/r, (x + iy)/r, and (x −iy)/r, meaning that they can be combined
to form arbitrary combinations of x/r, y/r, and z/r. Since a rotation of the coordinate
axes converts x, y, and z into linear combinations of each other, we can understand why
the set of three functions Y m
1 (m = 0,1,−1) is closed under coordinate rotations.
For l = 2, there are ﬁve possible m values, so the angular functions of this l value form
a closed space containing ﬁve independent members. A fuller discussion of these spaces
spanned by angular functions is part of what will be considered in Chapter 16.
Applying the preceding analysis to solutions of the Schrödinger equation, the eigenval-
ues of which are determined by solving its radial ODE for various values of the separation
constant l(l +1), we see that all solutions for the same l but different m will have the same
eigenvalues E and radial functions, but will differ in the orientation of their angular parts.
States of the same energy are called degenerate, and the independence of E with respect
to m will cause a (2l + 1)-fold degeneracy of the eigenstates of given l.
Example 15.5.3
SOLUTIONS FOR l = 1 AT ARBITRARY ORIENTATION
Let’s do this problem in Cartesian coordinates. The angular solution Y 0
1 to Laplace’s equa-
tion is shown in Table 15.4 to be proportional to z/r, which for our present purposes we
write (r · ˆez)/r, where ˆez is a unit vector in the z direction. We seek a similar solution,
with ˆez replaced by an arbitrary unit vector ˆeu = cosα ˆex + cosβ ˆey + cosγ ˆez, where
cosα, cosβ, and cosγ are the direction cosines of ˆeu. We get immediately
(r · ˆeu)
r
= x
r cosα + y
r cosβ + z
r cosγ.
Consulting the Cartesian-coordinate expressions for the spherical harmonics in Table 15.4,
we see that the above expression can be written
(r · ˆu)
r
=
r
8π
3
 
Y −1
1
−Y 1
1
2
!
cosα +
r
8π
3
 
−Y −1
1
−Y 1
1
2i
!
cosβ +
r
4π
3 Y 0
1 cosγ.
This shows that all three Y m
1 are needed to reproduce Y 0
1 at an arbitrary orientation. Similar
manipulations can be carried out for other l and m values.
■

764
Chapter 15 Legendre Functions
Further Properties
The main properties of the spherical harmonics follow directly from those of the functions
2lm and 8m. We summarize brieﬂy:
Special values. At θ = 0, the polar direction in the spherical coordinates, the value of ϕ
becomes immaterial, and all Y m
l
that have ϕ dependence must vanish. Using also the fact
that Pl(1) = 1, we ﬁnd in general
Y m
l (0,ϕ) =
r
2l + 1
4π
δm0.
(15.148)
A similar argument for θ = π leads to
Y m
l (π,ϕ) = (−1)l
r
2l + 1
4π
δm0.
(15.149)
Recurrence formulas. Using the recurrence formulas developed for the associated Leg-
endre functions, we get for the spherical harmonics with arguments (θ,ϕ):
cosθ Y m
l =
(l −m + 1)(l + m + 1)
(2l + 1)(2l + 3)
1/2
Y m
l+1
+
 (l −m)(l + m)
(2l −1)(2l + 1)
1/2
Y m
l−1,
(15.150)
e±iϕ sinθ Y m
l = ∓
(l ± m + 1)(l ± m + 2)
(2l + 1)(2l + 3)
1/2
Y m±1
l+1
±
(l ∓m)(l ∓m −1)
(2l −1)(2l + 1)
1/2
Y m±1
l−1 .
(15.151)
Some integrals. These recurrence relations permit the ready evaluation of some inte-
grals of practical importance. Our starting point is the orthonormalization condition,
Eq. (15.138). For example, the matrix elements describing the dominant (electric dipole)
mode of interaction of an electromagnetic ﬁeld with a charged system in a spherical har-
monic state are proportional to
Z h
Y m′
l′
i ∗
cosθ Y m
l d.
Using Eq. (15.150) and invoking the orthonormality of the Y m
l , we ﬁnd
Z h
Y m′
l′
i ∗
cosθ Y m
l d =
(l −m + 1)(l + m + 1)
(2l + 1)(2l + 3)
1/2
δm′m δl′,l+1
+
 (l −m)(l + m)
(2l −1)(2l + 1)
1/2
δm′m δl′,l−1.
(15.152)
Equation (15.152) provides a basis for the well-known selection rule for dipole radiation.

15.5 Spherical Harmonics
765
Additional formulas involving products of three spherical harmonics and the detailed
behavior of these quantities under coordinate rotations are more appropriately discussed in
connection with a study of angular momentum and are therefore deferred to Chapter 16.
Exercises
15.5.1
Show that the parity of Y m
l (θ,ϕ) is (−1)l. Note the disappearance of any m dependence.
Hint. For the parity operation in spherical polar coordinates, see Exercise 3.10.25.
15.5.2
Prove that Y m
l (0,ϕ) =
2l + 1
4π
1/2
δm0.
15.5.3
In the theory of Coulomb excitation of nuclei we encounter Y m
l (π/2,0). Show that
Y m
l
π
2 ,0

=
2l + 1
4π
1/2 [(l −m)!(l + m)!]1/2
(l −m)!! (l + m)!! (−1)(l−m)/2,
l + m even,
= 0,
l + m odd.
15.5.4
The orthogonal azimuthal functions yield a useful representation of the Dirac delta
function. Show that
δ(ϕ1 −ϕ2) = 1
2π
∞
X
m=−∞
eim(ϕ1−ϕ2).
Note. This formula assumes that ϕ1 and ϕ2 are restricted to 0 ≤ϕ < 2π. Without this
restriction there will be additional delta-function contributions at intervals of 2π in
ϕ1 −ϕ2.
15.5.5
Derive the spherical harmonic closure relation
∞
X
l=0
+l
X
m=−l
h
Y m
l (θ1,ϕ1)
i ∗
Y m
l (θ2,ϕ2) =
1
sinθ1
δ(θ1 −θ2)δ(ϕ1 −ϕ2)
= δ(cosθ1 −cosθ2)δ(ϕ1 −ϕ2).
15.5.6
In some circumstances it is desirable to replace the imaginary exponential of our spher-
ical harmonic by sine or cosine. Morse and Feshbach (see Additional Readings) deﬁne
Y e
ml = Pm
l (cosθ)cosmϕ,
m ≥0,
Y o
ml = Pm
l (cosθ)sinmϕ,
m > 0,
and their normalization integrals are
2π
Z
0
π
Z
0
[Y e or o
mn
(θ,ϕ)]2 sinθ dθ dϕ =
4π
2(2n + 1)
(n + m)!
(n −m)!, n = 1,2,...
= 4π,
n = 0.

766
Chapter 15 Legendre Functions
These spherical harmonics are often named according to the patterns of their positive
and negative regions on the surface of a sphere: zonal harmonics for m = 0, sectoral har-
monics for m = n, and tesseral harmonics for 0 < m < n. For Y e
mn, n = 4, m = 0, 2, 4,
indicate on a diagram of a hemisphere (one diagram for each spherical harmonic) the
regions in which the spherical harmonic is positive.
15.5.7
A function f (r,θ,ϕ) may be expressed as a Laplace series
f (r,θ,ϕ) =
X
l,m
almrlY m
l (θ,ϕ).
Letting ⟨···⟩sphere denote the average over a sphere centered on the origin, show that
D
f (r,θ,ϕ)
E
sphere = f (0,0,0).
15.6
LEGENDRE FUNCTIONS OF THE SECOND KIND
The Legendre equation, a linear second-order ODE, has two independent solutions.
Writing this equation in the form
y′′ −
2x
1 −x2 y′ −l(l + 1)
1 −x2 y = 0,
(15.153)
and restricting consideration to integer l ≥0, our objective is to ﬁnd a second solution
that is linearly independent from the Legendre polynomials Pl(x). Using the procedure of
Section 7.6, and denoting the second solution Ql(x), we have
Ql(x) = Pl(x)
x
Z exp


x
Z
2x/(1 −x2)dx


[Pl(x)]2
dx
= Pl(x)
x
Z
dx
(1 −x2)[Pl(x)]2 dx.
(15.154)
Since any linear combination of Pl and the right-hand side of Eq. (15.154) is equally valid
as a second solution of the Legendre ODE, we note that Eq. (15.154) deﬁnes both the scale
and the speciﬁc functional form of Ql.
Using Eq. (15.154), we can obtain explicit formulas for the Ql. We ﬁnd (remembering
that P0 = 1 and expanding the denominator in partial fractions):
Q0(x) =
x
Z
1
1 −x2 dx = 1
2
Z 
1
1 + x +
1
1 −x

dx = 1
2 ln
1 + x
1 −x

.
(15.155)

15.6 Legendre Functions of the Second Kind
767
Continuing to Q1, the partial fraction expansion is a bit more involved, but leads to a
simple result. Noting that P1(x) = x, we have
Q1(z) = x
x
Z
dx
(1 −x2)x2 dx = x
2 ln
1 + x
1 −x

−1.
(15.156)
With signiﬁcantly more work, we can obtain Q2:
Q2(x) = 1
2 P2(x)ln
1 + x
1 −x

−3x
2 .
(15.157)
This process can in principle be repeated for larger l, but it is easier and more instructive to
verify that the forms of Q0, Q1, and Q2 are consistent with the Legendre-function recur-
rence relations,5 and then to obtain Ql of larger l by recurrence. The recurrence formulas,
originally written for Pl in Eq. (15.18), are
(l + 1)Ql+1(x) −(2l + 1)x Ql(x) + lQl−1(x) = 0,
(15.158)
(2l + 1)Ql(x) = Q′
l+1(x) −Q′
n−1(x).
(15.159)
Veriﬁcation that Q0, Q1, and Q2 satisfy these recurrence formulas is straightforward and
is left as a exercise. Extension to higher l leads to the formula
Ql(x) = 1
2 Pl(x)ln
1 + x
1 −x

−2l −1
1 · l
Pl−1(x) −2l −5
3(l −1) Pl−3(x) −··· .
(15.160)
Many applications using the functions Ql(x) involve values of x outside the range
−1 < x < 1. If Eq. (15.160) is extended, say, beyond +1, then 1 −x will become neg-
ative and make a contribution ±iπ to the logarithm, thereby making a contribution ±iπ Pl
to Ql. Our solution will still remain a solution if this contribution is removed, and it is
therefore convenient to deﬁne the second solution for x outside the range (−1,+1) with
ln
1 + x
1 −x

replaced by
ln
x + 1
x −1

.
From a complex-variable perspective, the logarithmic term in the solutions Ql is related to
the singularity in the ODE at z = ±1, reﬂecting the fact that to make the solutions single-
valued it will be necessary to make a branch cut, traditionally taken on the real axis from
−1 to +1. Then the Ql with the (1 + x)/(1 −x) logarithm are recovered on −1 < x < 1
if we average the results from the (z + 1)/(z −1) form on the two sides of the branch cut.
The behavior of the Ql is illustrated by plots for x < 1 in Fig. 15.13 and for x > 1 in
Fig. 15.14. Note that there is no singularity at x = 0 but all the Ql exhibit a logarithmic
singularity at x = 1.
5In Section 15.1 we showed that any set of functions that satisﬁes the recurrence relations reproduced here also satisﬁes the
Legendre ODE.

768
Chapter 15 Legendre Functions
1.5
1.0
0.5
0.2
0.4
0.6
0.8
1.0
0
−0.5
−1.0
Q0(x)
Q2(x)
Q1(x)
x
FIGURE 15.13
Legendre functions Ql(x), 0 ≤x < 1.
100
10−1
10−2
10−3
10−4
x
2
4
6
8
10
Q0(x)
Q1(x)
Q2(x)
FIGURE 15.14
Legendre functions Ql(x), x > 1.

15.6 Legendre Functions of the Second Kind
769
Properties
1.
An examination of the formulas for Ql(x) reveals that if l is even, then Ql(x) is an
odd function of x, while Ql(x) of odd l are even functions of x. More succinctly,
Ql(−x) = (−1)l+1Ql(x).
2.
The presence of the logarithmic term causes Ql(1) = ∞for all l.
3.
Because x = 0 is a regular point of the Legendre ODE, Ql(0) must for all l be ﬁnite.
The symmetry of Ql causes Ql(0) to vanish for even l; it is shown in the next subsec-
tion that for odd l,
Q2s+1(0) = (−1)s+1
(2s)!!
(2s + 1)!!.
(15.161)
4.
From the result of Exercise 15.6.3, it can be shown that Ql(∞) = 0.
Alternate Formulations
Because the singular points of the Legendre ODE nearest to the origin are at the points ±1,
it should be possible to describe Ql(x) as a power series about the origin, with convergence
for |x| < 1. Moreover, since the only other singular point of the Legendre equation is a
regular singular point at inﬁnity, it should also be possible to express one of its solutions
as a power series in 1/x, i.e., a series about the point at inﬁnity, which must converge for
|x| > 1.
To obtain a power series about x = 0, we return to the discussion of the Legendre ODE
presented in Section 8.3, where we saw that an expansion of the form
y(x) =
∞
X
j=0
a j xs+ j
(15.162)
led to an indicial equation with solutions s = 0 and s = 1, and with the a j satisfying the
recurrence formula, for eigenvalue l(l + 1),
a j+2 = a j
(s + j)(s + j + 1) −l(l + 1)
(s + j + 2)(s + j + 1)
,
j = 0, 2,....
(15.163)
When l is even, we found that Pl(x) was obtained as the solution y(x) from the indicial-
equation solution s = 0, and we did not make use (for even l) of the solution from s = 1
because that solution was not a polynomial and did not converge at x = 1. However, we are
now seeking a second solution and are no longer restricting attention to those that converge
at x = ±1. Thus, a second solution linearly independent of Pl must be that produced (again,
for even l) as the series obtained when s = 1. This second solution will have odd parity,
and therefore must be proportional to Ql(x).
Continuing, for even l, with s = 1, Eq. (15.163) becomes
a j+2 = a j
(l + j + 2)(l −j −1)
( j + 2)( j + 3)
,

770
Chapter 15 Legendre Functions
corresponding to
Ql(x) = bl

x −(l −1)(l + 2)
3!
x3 + (l −3)(l −1)(l + 2)(l + 4)
5!
x5 −···

.
(15.164)
Here bl is the value of the coefﬁcient of the expansion needed to give the formula for Ql
the proper scaling. For odd l, the corresponding formula, with s = 0, is an even function
of x, and must therefore be proportional to Ql:
Ql(x) = bl

1 −l(l + 1)
2!
x2 + (l −2)l(l + 1)(l + 3)
4!
x4 ···

.
(15.165)
To ﬁnd the values of the scale factors bl, we turn now to the explicit forms for Q0 and
Q1, Eqs. (15.155) and (15.156). Expanding the logarithm, we ﬁnd (again keeping only the
lowest-order terms)
Q0(x) = x + ··· ,
Q1(x) = −1 + ··· .
From the recurrence formula, Eq. (15.158), keeping only the lowest-order contributions,
we ﬁnd
2Q2 = 3x Q1 −Q0 −→Q2 = −2x + ···
3Q3 = 5x Q2 −2Q1 −→Q3 = 2/3 + ···
4Q4 = 7x Q3 −3Q2 −→Q4 = 8x/3 + ···
··· = ··· .
These results generalize to
bl =



(−1)p
(2p)!!
(2p −1)!!
l even, l = 2p,
(−1)p+1
(2p)!!
(2p + 1)!!
l odd, l = 2p + 1.
(15.166)
One may now combine the values of the coefﬁcients bl with the expansions in Eqs. (15.164)
and (15.165) to obtain entirely explicit series expansions of Ql(x) about x = 0. This is the
topic of Exercise 15.6.2.
As mentioned earlier, the point x = ∞is a regular singular point, and expansion about
this point yields an expansion of Ql(x) in inverse powers of x. That expansion is consid-
ered in Exercise 15.6.3.
Exercises
15.6.1
Show that if l is even, Ql(−x) = −Ql(x), and that if l is odd, Ql(−x) = Ql(x).
15.6.2
Show that

Additional Readings
771
(a)
Q2p(x) = (−1)p 22p
p
X
s=0
(−1)s
(p + s)!(p −s)!
(2s + 1)!(2p −2s)! x2s+1
+ 22p
∞
X
s=p+1
(p + s)!(2s −2p)!
(2s + 1)!(s −p)! x2s+1,
|x| < 1,
(b)
Q2p+1(x) = (−1)p+122p
p
X
s=0
(−1)s
(p + s)!(p −s)!
(2s)!(2p −2s + 1)! x2s
+ 22p+1
∞
X
s=p+1
(p + s)!(2s −2p −2)!
(2s)!(s −p −1)!
x2s,
|x| < 1.
15.6.3
(a)
Starting with the assumed form
Ql(x) =
∞
X
j=0
blj xk−j,
show that
Ql(x) = bl0x−l−1
∞
X
s=0
(l + s)!(l + 2s)!(2l + 1)!
s!(l!)2 (2l + 2s + 1)!
x−2s.
(b)
The standard choice of bl0 is
bl0 = 2l(l!)2
(2l + 1)!,
leading to the ﬁnal result
Ql(x) = x−l−1
∞
X
s=0
(l + 2s)!
(2s)!!(2l + 2s + 1)!! x−2s.
Show that this choice of bl0 brings this negative power-series form of Qn(x) into
agreement with the closed-form solutions.
15.6.4
(a)
Using the recurrence relations, prove (independent of the Wronskian relation) that
n
h
Pn(x)Qn−1(x) −Pn−1(x)Qn(x)
i
= P1(x)Q0(x) −P0(x)Q1(x).
(b)
By direct substitution show that the right-hand side of this equation equals 1.
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974).
Hobson, E. W., The Theory of Spherical and Ellipsoidal Harmonics. New York: Chelsea (1955). This is a very
complete reference, which is the classic text on Legendre polynomials and all related functions.

772
Chapter 15 Legendre Functions
Jackson, J. D., Classical Electrodynamics, 3rd ed. New York: Wiley (1999).
Margenau, H., and G. M. Murphy, The Mathematics of Physics and Chemistry, 2nd ed. Princeton, NJ: Van
Nostrand (1956).
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics, 2 vols. New York: McGraw-Hill (1953). This
work is detailed but at a rather advanced level.
Smythe, W. R., Static and Dynamic Electricity, 3rd ed. New York: McGraw-Hill (1968), reprinted, Taylor &
Francis (1989), paperback. Advanced, detailed, and difﬁcult. Includes use of elliptic integrals to obtain closed
formulas.
Whittaker, E. T., and G. N. Watson, A Course of Modern Analysis, 4th ed. Cambridge, UK: Cambridge University
Press (1962), paperback.

CHAPTER 16
ANGULAR MOMENTUM
The traditional quantum mechanical treatment of central force problems starts from
solutions to the time-independent Schrödinger equation, which, for a single particle of
mass m moving subject to a potential V (r), is an eigenvalue problem of the general form
−¯h2
2m ∇2ψ(r) + V (r)ψ(r) = Eψ(r).
(16.1)
Here ¯h is Planck’s constant divided by 2π, in SI units approximately 1.05 × 10−34 J-s
(joule-seconds); the very small value of this constant causes quantum behavior to be per-
ceptible under most circumstances only at small distances and for particles of small mass;
the relevant ranges are typically at atomic scales of mass and length.
The basic interpretation of the Schrödinger equation is that if the energy E of the particle
is measured, the result will be one of the eigenvalues of Eq. (16.1), and (subsequent to the
measurement) the location of the particle will be described by a probability distribution
P(r)d3r = |ψ(r)|2d3r,
where ψ(r) is an eigenfunction corresponding to E. As we have seen in Chapters 9 and 15,
ψ will in general have angular as well as radial dependence, and its angular part can be
written in terms of the spherical harmonics Y m
l (θ,ϕ).
A more detailed interpretation of Eq. (16.1) is to identify it as an operator equation in
which the momentum p is identiﬁed with the operator −i ¯h∇, while functions of position,
such as the potential energy V (r), are identiﬁed as multiplicative operators. Viewed in this
way, the operator −(¯h2/2m)∇2 is seen to represent p2/2m (i.e., the kinetic energy T ), and
Eq. (16.1) then becomes equivalent to
Hψ ≡(T + V )ψ = Eψ,
(16.2)
where H, the Hamiltonian, is an operator whose eigenvalues are the possible values of the
total energy.
The Hamiltonian H is a special operator in quantum mechanics because its eigenfunc-
tions yield stationary probability distributions (they do not evolve into different distribu-
tions over time). However, H is just like any other quantum operator K representing a
773
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

774
Chapter 16 Angular Momentum
dynamical quantity (with eigenvalues k that can be the result of measurement of K). If
ψ is simultaneously an eigenfunction of H and of K, then we can have deﬁnite values of
both E and k that will not evolve as a function of time, and the measuring of either will not
disturb the deﬁnite value of the other. This state of affairs can only be achieved if H and
K commute, because (see Section 6.4) [H, K] = 0 is a necessary and sufﬁcient condition
that H and K have a set of simultaneous eigenfunctions.
In earlier chapters, we examined commutators such as [x, px] = i (here and except when
noted, we use a unit system with ¯h set to unity to avoid unnecessary notational complex-
ity). The nonzero commutator of x and px tells us that we cannot simultaneously obtain
unambiguous measurement of both these quantities (i.e., we do not have a complete set of
states that are simultaneously eigenfunctions of x and px). This is the mathematical basis
of the Heisenberg uncertainty principle in quantum mechanics.
The notion of simultaneous eigenfunctions and therefore commutation plays a key role
in the study of angular momentum in quantum mechanics. Angular momentum is con-
served in the classical central force problem, and one of the focal points of the present chap-
ter is to understand the properties of angular momentum operators in quantum mechanics.
16.1
ANGULAR MOMENTUM OPERATORS
In classical physics, the kinetic energy of a particle of mass µ can be written in terms
of its momentum p as Tclass = p2/2µ. Note that we are using µ for the particle mass
to avoid confusion with the usual notation of the azimuthal wave functions ϕm. Most of
the literature uses m for both quantities. Introducing spherical polar coordinates, Tclass
can be divided into radial and angular parts, with the angular kinetic energy of the form
L2
class/2µr2. Here Lclass is the angular momentum, deﬁned as Lclass = r×p. Following the
usual Schrödinger representation of quantum mechanics, the classical linear momentum p
is replaced (in a unit system with ¯h = 1) by the operator −i∇. The quantum-mechanical
kinetic energy operator is TQM = −∇2/2µ, which in spherical polar coordinates can be
written
TQM = −1
2µ
 ∂2
∂r2 + 2
r
∂
∂r

−
1
2µr2
 1
sinθ
∂
∂θ

sinθ ∂
∂θ

+
1
sin2 θ
∂2
∂ϕ2

.
(16.3)
Like the classical kinetic energy, TQM can also be divided into radial and angular parts,
with the angular part identiﬁed in terms of the angular momentum:
TQM = Tradial,QM +
1
2µr2 L2
QM,
(16.4)
Tradial,QM = −1
2µ
 ∂2
∂r2 + 2
r
∂
∂r

,
(16.5)
L2
QM = −1
sinθ
∂
∂θ

sinθ ∂
∂θ

−
1
sin2 θ
∂2
∂ϕ2 .
(16.6)
Since our focus here is on the quantum-mechanical operators, we drop the notation “QM”
from now on.

16.1 Angular Momentum Operators
775
The notation L2 in Eq. (16.6) is only really appropriate if it is consistent with the deﬁni-
tion of the quantum-mechanical angular momentum operator, which must have the form
L = r × p = −ir × ∇.
(16.7)
One way to conﬁrm Eq. (16.6) is to start from the expression for L in spherical polar coor-
dinates, which can be deduced by applying the operator r × p to an arbitrary function ψ:
Lψ = −ir × ∇ψ = −ir ˆer ×

ˆer
∂ψ
∂r + ˆeθ
1
r
∂ψ
∂θ + ˆeϕ
1
r sinθ
∂ψ
∂ϕ

,
from which we extract the formula
L = i

ˆeθ
1
sinθ
∂
∂ϕ −ˆeϕ
∂
∂θ

.
(16.8)
We then rewrite L in Cartesian components Lx, L y, Lz (but still expressed in polar coor-
dinates) and evaluate
L2 = L · L = L2
x + L2
y + L2
z.
(16.9)
This process is the topic of Exercise 3.10.32, and leads, as expected, to Eq. (16.6).
In Section 15.5 we identiﬁed the solutions of the angular part of the Laplace and
Schrödinger equations for central force problems as the spherical harmonics, denoted
Y m
l (θ,ϕ). Now that we have also written the angular part of these equations in terms of
L2, we see that the Y m
l
can be identiﬁed as eigenfunctions of L2, i.e., that they are angular
momentum eigenfunctions, satisfying an eigenvalue equation of the form
L2Y m
l (θ,ϕ) = l(l + 1)Y m
l (θ,ϕ).
(16.10)
Summarizing the discussion to this point, and drawing on previously established properties
of the spherical harmonics:
The spherical harmonics Y m
l
are eigenfunctions of L2 with eigenvalue l(l +1). The eigen-
functions for a given l are (2l + 1)-fold degenerate and can be indexed by their m values,
which range in unit steps from −l to l.
We now strive for a deeper understanding of the role of angular momentum. The solu-
tions to the time-independent Schrödinger equation are the eigenfunctions of its total
energy operator, the Hamiltonian H. We have just observed that for central force prob-
lems the angular solutions are eigenfunctions of the angular momentum operator L2. In
order for these two statements to be mutually consistent, it is necessary that H and L2
commute. For the systems under consideration here, this is clearly true, since we have
assumed that H is of the form T + V (r), so
H = Tradial(r) +
1
2µr2 L2(θ,ϕ) + V (r).
Since the only angle-dependent quantity in H is the operator L2, and since L2 obviously
commutes with itself and is independent of r, we have
[H,L2] = 0.

776
Chapter 16 Angular Momentum
The fact that H and L2 have simultaneous eigenfunctions in central force problems
means that the stationary states of such systems can be characterized by deﬁnite values
of both the energy and the angular momentum quantum number l. States of different
l were ultimately identiﬁed with series of lines in the emission and absorption spectra
of the hydrogen atom that had previously been labeled “sharp,” “diffuse,” “principal,”
and “fundamental.” This identiﬁcation caused physicists to use the initial letters of these
names as synonyms for l values; hence it has become essential to know that the code
letters for l = 0,1,2, and 3 are respectively s, p,d, and f . For l > 3, the code letters run
alphabetically: g,h ... .
Turning now to the components of L, we have (cf. Exercise 3.10.31)
[Lj, Lk] = iε jknLn and [L2, Lj] = 0,
(16.11)
where j,k,n are different members of the set (1,2,3) and ε jkn is a Levi-Civita symbol.
Although the Lj do not commute with each other, all commute with L2 and hence also
with H, so H, L2, and any one component of L mutually commute. We conclude that there
exists a set of simultaneous eigenfunctions of H, L2, and any one component of L. For
this purpose we usually pick Lz, motivated by the fact that, in spherical polar coordinates,
it is, as found in Exercise 3.10.29,
Lz = −i ∂
∂ϕ .
(16.12)
For reference, we copy here the far more complicated results for Lx and L y, obtained from
Exercise 3.10.30:
Lx = i sinϕ ∂
∂θ + i cotθ cos ϕ ∂
∂ϕ ,
L y = −i cosϕ ∂
∂θ + i cotθ sinϕ ∂
∂ϕ .
(16.13)
The spherical harmonics are, in fact, eigenfunctions of Lz. Since
Lzeimϕ = −i ∂
∂ϕ eimϕ = meimϕ,
(16.14)
we see that Y m
l
is an eigenfunction of Lz with eigenvalue m. This is one of the reasons
why the complex exponentials, rather than the trigonometric functions, were chosen in the
deﬁnitions of the spherical harmonics. It is obvious that cosmϕ is not an eigenfunction of
Lz: −i(∂/∂ϕ)cosmϕ = im sinmϕ. Note, however, that exp(±imϕ), cosmϕ, and sinmϕ
are all eigenfunctions of the operator L2
z = −∂2/∂ϕ2 with eigenvalue m2.
Ladder Operators
The commutators of the angular momentum components permit the development of some
useful algebraic relationships. While these relationships can be found from the speciﬁc
forms of the operators (cf. Exercise 3.10.30), more general and valuable results are

16.1 Angular Momentum Operators
777
obtained by derivations based only on the commutators given in Eq. (16.11). We deﬁne
the operators
L+ = Lx + i L y,
L−= Lx −i L y,
(16.15)
and consider the commutators
[Lz, L+] = [Lz, Lx] + i[Lz, L y] = i L y + i(−i Lx) = L+,
(16.16)
[Lz, L−] = [Lz, Lx] −i[Lz, L y] = i L y −i(−i Lx) = −L−.
(16.17)
We start by applying Eq. (16.16) to a function ψm
l , which is assumed to be a normalized
simultaneous eigenfunction of L2, with eigenvalue λl, and of Lz, with eigenvalue m; the
form of ψm
l
(and even the space within which it resides) need not be speciﬁed to carry
out the present discussion. Moreover, at this point we introduce no information about the
possible values of λl and m. However, to visualize what we are doing, the reader can keep
in mind that one possible interpretation of ψm
l is the spherical harmonic Y m
l . We have
[Lz, L+]ψm
l = LzL+ψm
l −L+Lzψm
l = L+ψm
l .
Since Lzψm
l
= mψm
l , we can rewrite the central and right-hand members of the above
equation as
Lz(L+ψm
l ) −m(L+ψm
l ) = (L+ψm
l ),
which rearranges to
Lz(L+ψm
l ) = (m + 1)(L+ψm
l ).
(16.18)
This tells us that if L+ψm
l
is nonzero, it is an eigenfunction of Lz with eigenvalue m + 1;
for that reason L+ can be called a raising operator. By itself, this analysis tells us nothing
about the value(s) of m, but only that L+ increases m in unit steps. A similar development
shows that L−is a lowering operator, corresponding to the equation
Lz(L−ψm
l ) = (m −1)(L−ψm
l ).
(16.19)
Raising and lowering operators are collectively referred to as ladder operators.
Next, we recall that [L2, Li] = 0 for all components Li. This means also that
[L2, L+] = 0, so
L2(L+ψm
l ) = L+L2ψm
l = λl(L+ψm
l ),
showing that (L+ψm
l ) is still an eigenfunction of L2 with the same eigenvalue, λl, as ψm
l .
Note that we did not need to know the value of λl to draw this conclusion. Summarizing,
the operators L± convert ψm
l
into quantities proportional to ψm±1
l
, with the conversion
failing only if L±ψm
l = 0.
While Eqs. (16.18) and (16.19) tell us that L± are ladder operators, they do not tell
us whether the quantities L±ψm
l
are normalized. To address this problem, we write the
normalization expression for L+ψm
l in the form
⟨L+ψm
l |L+ψm
l ⟩= ⟨ψm
l |L−L+|ψm
l ⟩,
where we have used the fact that, because Lx and L y are Hermitian, (L+)† = L−.

778
Chapter 16 Angular Momentum
To obtain more information about L−L+, we rearrange L2 as follows:
L2 = 1
2(L+L−+ L−L+) + L2
z = L−L+ + 1
2[L+, L−] + L2
z,
(16.20)
a result that can be easily veriﬁed by expanding L+, L−, and the commutator. Then we
introduce
[L+, L−] = [Lx + i L y, Lx −i L y] = −i[Lx, L y] + i[L y, Lx] = 2Lz,
(16.21)
and solve Eq. (16.20) for L−L+, obtaining
L−L+ = L2 −L2
z −Lz.
(16.22)
Using the fact that ψm
l
is a normalized eigenfunction of both L2 and Lz, we can now
perform the evaluation
⟨L+ψm
l |L+ψm
l ⟩= ⟨ψm
l |L−L+|ψm
l ⟩= ⟨ψm
l |L2 −L2
z −Lz|ψm
l ⟩
= λl −m2 −m.
(16.23)
A parallel analysis leads to the companion result
⟨L−ψm
l |L−ψm
l ⟩= ⟨ψm
l |L+L−|ψm
l ⟩= λl −m2 + m.
(16.24)
If we use the expressions in Eqs. (16.23) and (16.24) to account for the scale factors gen-
erated by the ladder operators, we can summarize their action as
L+ψm
l =
p
λl −m(m + 1)ψm+1
l
,
(16.25)
L−ψm
l =
p
λl −m(m −1)ψm−1
l
,
where, the reader may recall, λl is the eigenvalue of L2 corresponding to quantum number
l; the current analysis has not yet determined its value. The expressions in Eq. (16.25) have
also incorporated the assumption that the signs of the ψm
l
are related as shown. That is a
matter of deﬁnition, and when the ψm
l
are taken to be the spherical harmonics Y m
l , the
Condon-Shortley phase assignment was deliberately designed to make Eq. (16.25) consis-
tent with the signs given the Y m
l
in Table 15.4.
Next, we return to Eq. (16.23) and note that since it describes a normalization integral,
it is inherently nonnegative, and can be zero only if L+ψm
l
is identically zero. The right-
hand side of Eq. (16.23), however, will be become negative if m is permitted to get too
large, so for any ﬁxed l (and therefore a ﬁxed λl), there must be some largest m, which we
call mmax, for which there exists a ψmmax
l
. But if we use Eq. (16.23) to evaluate L+ψmmax
l
,
we will, unless λl −mmax(mmax + 1) = 0, generate a function with m = mmax + 1, thereby
creating an inconsistency. Giving mmax the name l (permitted because within the current
derivation we have not yet assigned a meaning to l), what we have found so far is that
λl = l(l + 1) and that the maximum value of m is m = l. Remember that we still know
nothing about possible values for l.
Turning now to Eq. (16.24), and inserting l(l + 1) for λl, we note that if m is permitted
to become too negative we will again have an inconsistent situation, and that it is necessary
that for some mmin the right-hand side of Eq. (16.24) must vanish. Thus, we require l(l +
1) −mmin(mmin −1) = 0, an equation that is satisﬁed for mmin = l + 1 (which is clearly
irrelevant), and for mmin = −l (the solution we want).

16.1 Angular Momentum Operators
779
Finally, we observe that, starting from some ψm
l with m = l, we have the severe limita-
tion that application of the lowering operator L−will decrease the m value in unit steps,
but must ultimately reach m = −l to avoid the generation of an inconsistency. This state
of affairs is possible if l is a nonnegative integer, in which case there are 2l + 1 possible
m values, ranging in unit steps from l to −l. However, it is also possible to assign l a
half-integer value, as m = l and m = −l are then still connected by a series of unit steps. In
this case, also, there will be 2l + 1 different m values. This quantity, 2l + 1, is sometimes
called the multiplicity of the angular momentum states.
The fact that it is mathematically possible to have a series (multiplet) of states corre-
sponding to either integral or half-integral l and satisfying the angular momentum commu-
tation rules does not prove that such states are realizable in a particular algebraic system
(such as that describing ordinary three-dimensional [3-D] space), or that such states are
indeed relevant for physics. However, by solving Laplace’s equation, we have already
found that the angular momentum states of integral l can be described in ordinary space
and that they can be identiﬁed as states of ordinary (so-called orbital) angular momentum.
It is not possible to describe states of half-integral l as ordinary functions in 3-D space, so
orbital angular momentum will only involve integral l.
Example 16.1.1
SPHERICAL HARMONICS LADDER
From Exercise 3.10.30, or alternatively by combining the formulas for Lx and L y from
Eq. (16.13), the orbital angular momentum ladder operator L+ is found to be
L+ = eiϕ
 ∂
∂θ + i cotθ ∂
∂ϕ

.
Starting from Y 0
1 (θ,ϕ) = √3/4π cosθ, we can apply Eq. (16.25):
L+Y 0
1 (θ,ϕ) =
r
3
4π eiϕ ∂cosθ
∂θ
=
r
3
4π eiϕ(−sinθ) =
√
2Y 1
1 (θ,ϕ),
(16.26)
which when solved for Y 1
1 gives (with proper scale and sign) the value tabulated in
Table 15.4. The reader can verify that the application of L+ to Y 1
1 gives zero.
■
Spinors
It turns out that half-integral angular momentum states are needed to describe the intrinsic
angular momentum of the electron and many other particles. Since these particles also have
magnetic moments, an intuitive interpretation is that their charge distributions are spinning
about some axis; hence the term spin. It is now understood that the spin phenomena can-
not be explained consistently by describing these particles as ordinary charge distributions
undergoing rotational motion, but are better treated by assigning these particles to states in
an abstract space that, for the electron, has the l value 1
2 (but in this context, we normally
use s and write s = 1
2), which means that the possible m values (often written ms) are
ms = + 1
2 and ms = −1
2. It is not productive to try to think of this situation in terms of

780
Chapter 16 Angular Momentum
ordinary functions, but to accept an abstract formulation in which spin states are repre-
sented by symbols; popular choices are α or | ↑⟩for the state m = + 1
2 and β or | ↓⟩for
that with m = −1
2. These spin states can also be represented by two-component column
vectors, with the angular momentum operators given in terms of the Pauli matrices as 1
2σ i.
The quantities forming a basis for the multiplets for half-integer angular momentum
are called spinors. In addition to their manipulation using ladder operators, they have
rotational properties that are discussed in more detail in Chapter 17.
Example 16.1.2
SPINOR LADDER
Calling the angular momentum operator S, we write Sx, Sy, Sz as the 2 × 2 matrices 1
2σ i,
where σ i are deﬁned in Eq. (2.28):
Sx = 1
2
 
0
1
1
0
!
,
Sy = 1
2
 
0 −i
i
0
!
,
Sz = 1
2
 
1
0
0 −1
!
.
(16.27)
By carrying out matrix operations we can verify that these matrices satisfy the angular
momentum commutation rules. For example,
Sx Sy −SySx = 1
4
 
0
1
1
0
! 
0 −i
i
0
!
−1
4
 
0 −i
i
0
! 
0
1
1
0
!
= i
2
 
1
0
0 −1
!
= iSz.
We also ﬁnd that
S2
x = S2
y = S2
z = (1/4)1,
and we therefore have
S2 = S2
x + S2
y + S2
z = 1
4
h
1 + 1 + 1
i
= 3
41.
Note that 3/4 is S(S + 1) for S = 1/2.
The interpretation of these matrix relationships is that we have an abstract space spanned
by the two functions
ψ1/2
1/2 ≡α ≡| ↑⟩=
 
1
0
!
,
ψ−1/2
1/2
≡β ≡| ↓⟩=
 
0
1
!
,

16.1 Angular Momentum Operators
781
and that the operators S2 and Sz operate on these functions as follows:
S2α = S2ψ1/2
1/2 = 3
4
 
1
0
0
1
! 
1
0
!
= 3
4
 
1
0
!
= 3
4ψ1/2
1/2 = 3
4α,
Szα = Szψ1/2
1/2 = 1
2
 
1
0
0
−1
! 
1
0
!
= 1
2
 
1
0
!
= 1
2ψ1/2
1/2 = 1
2α,
S2β = S2ψ−1/2
1/2
= 3
4
 
1
0
0
1
! 
0
1
!
= 3
4
 
0
1
!
= 3
4ψ−1/2
1/2
= 3
4β,
Szβ = Szψ−1/2
1/2
= 1
2
 
1
0
0
−1
! 
0
1
!
= −1
2
 
0
1
!
= −1
2ψ−1/2
1/2
= −1
2β.
The above formulas show that ψ±1/2
1/2
(also denoted α and β) are simultaneous eigen-
functions of S2 and Sz. To illustrate that they are not also eigenfunctions of Sx or Sy, we
compute
Sxα = Sxψ1/2
1/2 = 1
2
 
0
1
1
0
! 
1
0
!
= 1
2
 
0
1
!
= 1
2ψ−1/2
1/2
= 1
2β.
To make ladders, we now form
S+ = Sx + iSy =
 
0
1
0
0
!
,
S−= Sx −iSy =
 
0
0
1
0
!
.
Applying these operators to α = ψ1/2
1/2,
S+α =
 
0
1
0
0
! 
1
0
!
= 0,
S−α =
 
0
0
1
0
! 
1
0
!
=
 
0
1
!
= β.
These results are in agreement with Eq. (16.25), for which, with the current parameters
λ = 3/4, m = 1/2, its coefﬁcients are
p
λ −m(m + 1) = 0,
p
λ −m(m −1) = 1.
■
Summary, Angular Momentum Formulas
The analysis of the preceding subsection applies to any system of operators satisfying the
angular momentum commutation rules. Possible areas of application include orbital angu-
lar momentum (for which the eigenfunctions are the spherical harmonics), the intrinsic
(spin) angular momentum we now know is associated with most fundamental particles,
and even the overall angular momenta that result either from considering both the orbital
and spin angular momenta of the same particle, or the total angular momentum of a collec-
tion of particles (as in a many-electron atom or even a nucleus).

782
Chapter 16 Angular Momentum
It is useful to summarize the key results; we do so giving the operators the name J,
to emphasize the fact that the results are not restricted to orbital angular momentum (for
which the symbol L is nearly universally used), or to spin angular momentum (traditionally
denoted S). Thus:
1.
We assume that there exists a Hermitian operator J with components Jx, Jy, Jz such
that J 2
x + J 2
y + J 2
z = J2 and that these quantities satisfy the commutation relations
[Jk, Jl] = iεkln Jn,
[J2, Jk] = 0,
(16.28)
where k,l,n are x, y, z in any order and εkln is a Levi-Civita symbol. Other than the
requirement of Eq. (16.28), J is arbitrary.
2.
Because the operators J2 and Jz commute, there can exist functions (in some abstract
space), generically denoted ψ M
J , with ψ M
J simultaneously a normalized eigenfunction
of Jz with eigenvalue M and an eigenfunction of J2 with eigenvalue J(J + 1):
Jzψ M
J = Mψ M
J ,
J2ψ M
J = J(J + 1)ψ M
J ,
D
ψ M
J |ψ M
J
E
= 1.
(16.29)
3.
Operators satisfying the above conditions can be called angular momentum opera-
tors; those which were used as examples of angular momentum in ordinary space
(orbital angular momentum) are clearly relevant for physics; similar operators in
more abstract spaces are relevant only to the extent that they can be identiﬁed with
physical phenomena.
We have already seen that these assumptions are sufﬁcient to enable the introduction of
ladder operators, and to reach the following conclusions:
1.
The possible values of J are integral and half-integral; in ordinary 3-D space only
functions of integral J can be realized.
2.
For a given J, the possible values of M range in unit steps from M = J to M = −J;
this produces 2J + 1 different M values.
3.
Given any one ψ M
J , we can generate others by use of the operators
J+ = Jx + i Jy,
J−= Jx −i Jy.
The result of applying these operators to ψ M
J is, see Eq. (16.25),
J+ψ M
J =
p
(J −M)(J + M + 1)ψ M+1
J
,
(16.30)
J−ψ M
J =
p
(J + M)(J −M + 1)ψ M−1
J
.
(16.31)
These formulas give zero results when J+ is applied to ψ J
J and when J−is applied
to ψ−J
J .
Exercises
16.1.1
The quantum mechanical angular momentum operators Lx ±i L y in 3-D physical space
are given by

16.1 Angular Momentum Operators
783
Lx + i L y = eiϕ
 ∂
∂θ + i cotθ ∂
∂ϕ

,
Lx −i L y = −e−iϕ
 ∂
∂θ −i cotθ ∂
∂ϕ

.
Show that
(a)
(Lx + i L y)Y M
L (θ,ϕ) =
p
(L −M)(L + M + 1)Y M+1
L
(θ,ϕ),
(b)
(Lx −i L y)Y M
L (θ,ϕ) =
p
(L + M)(L −M + 1)Y M−1
L
(θ,ϕ).
16.1.2
With L± given by
L± = Lx ± i L y = ±e±iϕ
 ∂
∂θ ± i cotθ ∂
∂ϕ

,
show that
(a)
Y m
l =
s
(l + m)!
(2l)!(l −m)!(L−)l−mY l
l ,
(b)
Y m
l =
s
(l −m)!
(2l)!(l + m)!(L+)l+mY −l
l
.
16.1.3
Using the known forms of L+ and L−(Exercise 16.1.2), show that
Z
[Y M
L ]∗L−(L+Y M
L )d =
Z
(L+Y M
L )∗(L+Y M
L )d.
Here d is the element of solid angle (sinθdθdϕ), and the integration is over the entire
angular space.
16.1.4
(a)
Show that J2 = 1
2
h
J+J−+ J−J+
i
+ J 2
z.
(b)
Use the result from part (a) and the explicit formulas for L+ and L−from Exer-
cise 16.1.2 to verify that all the spherical harmonics with l = 2 are eigenfunctions
of L2 with eigenvalue l(l + 1) = 6.
16.1.5
Derive the following relations without assuming anything about ψ M
L
other than that
they are angular momentum eigenfunctions:
(a)
ψ M
L (θ,ϕ) =
s
(L + M)!
(2L)!(L −M)!(L−)L−Mψ L
L (θ,ϕ),
(b)
ψ M
L (θ,ϕ) =
s
(L −M)!
(2L)!(L + M)!(L+)L+Mψ−L
L (θ,ϕ).

784
Chapter 16 Angular Momentum
16.1.6
Derive the operator equations
(L+)nY M
L (θ,ϕ) = (−1)neinϕsinn+M θ dn sin−M θY M
L (θ,ϕ)
(d cosθ)n
,
(L−)nY M
L (θ,ϕ) = e−inϕsinn−M θ dn sinM θY M
L (θ,ϕ)
(d cosθ)n
.
Hint. Try mathematical induction (Section 1.4).
16.1.7
Show, using (L−)n, that
Y −M
L
(θ,ϕ) = (−1)Mh
Y M
L (θ,ϕ)
i ∗
.
16.1.8
Verify by explicit calculation that
(a)
L+Y 0
1 (θ,ϕ) = −
r
3
4π sinθeiϕ =
√
2Y 1
1 (θ,ϕ),
(b)
L−Y 0
1 (θ,ϕ) = +
r
3
4π sinθe−iϕ =
√
2Y −1
1 (θ,ϕ).
The signs have the indicated values because the spherical harmonics were deﬁned to
be consistent with the results obtained using the ladder operators L+ and L−(Condon-
Shortley phase).
16.2
ANGULAR MOMENTUM COUPLING
An important application of ladder operators is to systems in which a resultant angular
momentum is the sum of two individual angular momenta. Because the angular momenta
have directional properties, we anticipate a result that has some properties in common
with vector addition, but because these are quantum mechanical quantities involving non-
commuting operators, we need to study the problem in more detail.
If j1 and j2 are two individual angular momentum operators that act on different coor-
dinate sets (as, e.g., the coordinates of two different particles), then they are unrelated
and all components of each must commute with every component of the other. This will
enable us to carry out a detailed analysis of operators of the combined system, for which
the total angular momentum operator is J = j1 + j2, with components Jx = j1x + j2x,
Jy = j1y + j2y, Jz = j1z + j2z, with the overall operator J2 = J 2
x + J 2
y + J 2
z.
To discuss the problem, we will need the commutators
[ j1k, j1l] = iεkln j1n,
[ j2k, j2l] = iεkln j2n,
[ j1k, j2l] = 0.
(16.32)
For the ﬁrst two commutators, k,l,n are x, y, z in any order; the third commutator vanishes
for all k,l including k = l. From the commutators in Eq. (16.32), it is easily established
that the overall angular momentum components obey the commutation rules
[Jx, Jy] = i Jz,
[Jy, Jz] = i Jx,
[Jz, Jx] = i Jy,
(16.33)

16.2 Angular Momentum Coupling
785
so these overall components satisfy the generic angular momentum commutation relations,
meaning also that
[J2, Ji] = 0.
(16.34)
In addition,
[J2,j2
1] = [J2,j2
2] = 0.
(16.35)
However, it is not true that the components of j1 or j2, namely j1i or j2i, commute with
J2, even though J2 and the sum j1i + j2i do commute.
Example 16.2.1
COMMUTATION RULES FOR J COMPONENTS
To ﬁnd the commutator [J2, j1z], write
J2 = (j1 + j2)2 = j2
1 + j2
2 + 2j1 · j2
= j2
1 + j2
2 + 2
 j1x j2x + j1y j2y + j1z j2z

,
so we have
[J2, j1z] = [j2
1, j1z] + [j2
2, j1z] + 2

[ j1x j2x, j1z] + [ j1y j2y, j1z] + [ j1z j2z, j1z]

= 2

j2x[ j1x, j1z] + j2y[ j1y, j1z]

= 2i( j1x j2y −j1y j2x),
(16.36)
where we have dropped terms in which the commutators involve different particles and
those, e.g., [j2
1, j1z], which vanish because the individual-particle operators are angular
momenta.
Equation (16.36) clearly shows that [J2, j1z] is nonzero. However, its contributions are
equal and opposite to those of [J2, j2z], explaining why [J2, Jz] does vanish.
Consider next [J2,j2
1]. Again expanding J2, we get
[J2,j2
1] = [j2
1,j2
1] + [j2
2,j2
1] + 2

[ j1x j2x,j2
1] + [ j1y j2y,j2
1] + [ j1z j2z,j2
1]

.
Every term of this equation vanishes, so J2 and j2
1 commute.
■
We have noted that j2
1, j2
2, and Jz all commute with each other and with J2, j1z, and
j2z, but that the last three of these operators do not all commute with each other. There
are therefore different ways of selecting maximal sets of mutually commuting operators
for which we can construct simultaneous eigenfunctions. One possibility is to select j2
1,
j2
2, j1z, j2z, and Jz, which has the advantage that the simultaneous eigenfunctions are just
products of the eigenstates for individual ji, but has the disadvantage that we will not
have states of deﬁnite total angular momentum J2. This is a big disadvantage, because in
reality different angular momenta in the same system actually interact to some extent. If
we add to the Hamiltonian of our problem a small term (a perturbation) that causes the
individual angular momenta not quite to be independent, our system will still strictly have
conservation of J2 (i.e., H and J2 will still commute), but the perturbation added to the
Hamiltonian will not commute with j1 and j2.

786
Chapter 16 Angular Momentum
Alternatively, and for most purposes better, we could choose the mutually commut-
ing operator set J2, j2
1, j2
2, and Jz, which would describe states of deﬁnite total angular
momentum, but these states would be mixtures of the individual angular-momentum states
and would not have deﬁnite values of j1z or j2z. It is the purpose of this section to relate
these two descriptions by ﬁnding the equations that connect (i.e., couple) the individual
angular momenta to form states of deﬁnite J2.
To simplify future discussion, we can refer to the product basis of the preceding para-
graph as the m1,m2 basis, and call the alternative basis of deﬁnite J the J, M basis. The
m1,m2 basis members also have deﬁnite values of M, but not J; most members of the
J, M basis will not have deﬁnite values of either m1 or m2. If we stick with problems in
which j1 and j2 are ﬁxed, all members of both bases will have the same deﬁnite values of
these quantum numbers.
Before getting into the details, let’s make two observations. First, since we have raising
and lowering operators that we can apply to the J, M basis, the functions in this basis must
include all the M values for any J that is present at all. Second, since both bases have
deﬁnite values of M, the transition from one basis to the other cannot mix functions of
different M.
Vector Model
We begin with some qualitative observations. Since Jz = j1z + j2z (with eigenvalues we
call M) is part of both our commuting operator sets, we can conclude, from looking at
the m1,m2 basis, that the maximum eigenvalue Mmax of Jz will occur when m1 = j1 and
m2 = j2, so Mmax = j1 + j2. Moving now to the J, M basis, which of course spans the
same function space, we see that because Mmax is the maximum M value, it must be a
member of a multiplet with J = Mmax, and this must be the largest possible J. Thus,
Jmax = j1 + j2.
To establish the minimum value possible for the quantum number J is a little trickier,
and we will come back to that shortly. The result, which is simple, is that Jmin = | j1 −j2|.
These maximum and minimum values of J correspond to the notion that the classical vec-
tor sum j1 +j2 has a maximum length equal to the sum of the lengths of these vectors and a
minimum length equal to the absolute value of their difference; the quantum analog of this
notion is not quantitatively exact because the magnitude of each j is actually √j( j + 1).
Further qualitative observations follow if we tabulate the various possible m1,m2 func-
tions of various M values. The concept can be understood from a simple example. Suppose
j1 = 2, j2 = 1. Then the members of the m1,m2 basis can be grouped as shown here. The
kets in the table are labeled in more detail than usual to avoid potential confusion; those
labeled m1 have j value j1, those labeled m2 have j = j2.
M = +3 |m1 = +2⟩|m2 = +1⟩
M = +2 |m1 = +2⟩|m2 = 0 ⟩
|m1 = +1⟩|m2 = +1⟩
M = +1 |m1 = +2⟩|m2 = −1⟩
|m1 = +1⟩|m2 = 0 ⟩
|m1 = 0 ⟩|m2 = +1⟩
M = 0 |m1 = +1⟩|m2 = −1⟩
|m1 = 0 ⟩|m2 = 0 ⟩
|m1 = −1⟩|m2 = +1⟩
M = −1 |m1 = −2⟩|m2 = +1⟩
|m1 = −1⟩|m2 = 0 ⟩
|m1 = 0 ⟩|m2 = −1⟩
M = −2 |m1 = −2⟩|m2 = 0 ⟩
|m1 = −1⟩|m2 = −1⟩
M = −3 |m1 = −2⟩|m2 = −1⟩

16.2 Angular Momentum Coupling
787
Because the basis transformations we are discussing only mix basis functions of the
same M, a transition to the J, M basis will have the same number of functions of each M
as are in the row of our table for that M. If there is only one function in the row, it must
(without change) be a member of the J, M basis, and we may use it as a starting point for
getting all the other members of the multiplet for the same J by application of the ladder
operators. So in our current example, we can start from |m1 = +2⟩|m2 = +1⟩, and make
one member of the multiplet for each M value in the table.
Once this has been done, we will have constructed as many J, M functions as there
are entries in the ﬁrst column of the table (but remember that in most cases they will not
be the speciﬁc functions sitting in that column). But that observation does tell us that the
numbers of functions that are still unused (but not their exact forms) will correspond with
the numbers of functions in the remainder of the table. In particular, we see that there will
in our example be one function left over with M = 2. Because it cannot have a J = 3
component, it must be a |J = 2, M = 2⟩eigenfunction and therefore must be orthogonal
to the |J = 3, M = 2⟩function we have already found. That means that we can obtain it by
Gram-Schmidt orthogonalization within the function space for M = +2.
From the |J = 2, M = 2⟩function, we can apply a ladder operator to ﬁnd |J = 2, M⟩
basis members with other M values, the number of which will correspond to the number
of entries in the second column of our table. To continue to a third column, we would need
to ﬁnd a M = +1 function orthogonal to both the |J = 3, M = +1⟩and |J = 2, M = +1⟩
functions. This process can be continued until the m1,m2 basis has been exhausted.
Taking now a further look at our table, we see that the number of columns with entries
increases as we decrease M from its maximum value until M has reached | j1 −j2|; for
smaller |M| than that, the number of columns in use stays constant, because of limitations
in the way the individual m values can be chosen to add up to M. That gives us a graphical
indication that the smallest J value will be | j1 −j2|.
A more algebraic way of determining the smallest resultant J is based on a computa-
tion of the total number of J, M states generated if the possible J values run from an as
yet undetermined value Jmin to our previously determined maximum value Jmax. Since
the number of states for each J is 2J + 1, the total number of J, M states we will have
produced is
J=Jmax
X
J=Jmin
(2J + 1) = (Jmax −Jmin + 1)(Jmax + Jmin + 1)
= (2 j1 + 1)(2 j2 + 1),
(16.37)
where the second line of this equation reﬂects the fact that the total number of states is
readily counted in the m1,m2 basis. Inserting the value Jmax = j1 + j2 and solving for
Jmin, we ﬁnd
Jmin = | j1 −j2|.
Another way of stating this result is to observe that the possible values of J satisfy a
triangle rule, meaning that they occur in unit steps from a maximum of j1 + j2 to a
minimum of | j1 −j2|.

788
Chapter 16 Angular Momentum
Ladder Operator Construction
To develop a quantitative description of angular momentum coupling, we consider the
case of general j1 and j2, and start from the lone member of the m1,m2 basis with
M = j1 + j2. In line with our earlier discussion, this m1,m2 basis member must also be a
function of the deﬁnite J value Jmax = j1 + j2. Using a notation in which the lower entry
in each ket is its J value and the upper entry gives the value of M, we indicate this by
writing

Jmax
Jmax

=

j1
j1

j2
j2

.
(16.38)
We now generate additional states of the same J but with different M by applying the
lowering operator J−to Eq. (16.38); when we apply it to the right-hand side, we do so in
the form J−= j1−+ j2−. The result, for the left side of Eq. (16.38), is
J−

Jmax
Jmax

=
p
2Jmax

Jmax −1
Jmax

.
(16.39)
The coefﬁcient √2Jmax is that given by Eq. (16.31) for J = M = Jmax. For the right side
of Eq. (16.38), we get
( j1−+ j2−)



j1
j1

j2
j2

=

j1−

j1
j1



j2
j2

+

j1
j1

j2−

j2
j2


=
p
2 j1

j1 −1
j1

j2
j2

+
p
2 j2

j1
j1

j2 −1
j2

,
(16.40)
where we have again obtained the coefﬁcients from Eq. (16.31), but now evaluating them
for the ﬁrst term with (J, M) = ( j1, j1) and for the second term with (J, M) = ( j2, j2).
Combining these results, and solving for

Jmax −1
Jmax

,

Jmax −1
Jmax

=
s
j1
Jmax

j1 −1
j1

j2
j2

+
s
j2
Jmax

j1
j1

j2 −1
j2

.
(16.41)
With escalating complexity, we could continue this process to smaller values of M.
As indicated in our earlier, more qualitative discussion, we can reach functions with
J = Jmax −1 by starting from the unused member of the set of two functions

j1 −1
j1

j2
j2

,

j1
j1

j2 −1
j2

.
The quantity we seek,

Jmax −1
Jmax −1

,

16.2 Angular Momentum Coupling
789
will be the function in the above-deﬁned subspace that is orthogonal to

Jmax −1
Jmax

as given in Eq. (16.41), and therefore will be

Jmax −1
Jmax −1

= −
s
j2
Jmax

j1 −1
j1

j2
j2

+
s
j1
Jmax

j1
j1

j2 −1
j2

.
(16.42)
At this point we note that the function produced by Eq. (16.42) could have been written
with all its signs changed, as the orthogonalization process does not determine the sign of
the orthogonal function. This only matters if we wish to correlate the signs of our J, M
constructions with work by others. Irrespective of our choice of signs, we can apply J−to
reach the full set of M values, and then continue to states of smaller J until the m1,m2
space is exhausted.
The general result of the above-described processes is to obtain each J, M eigenstate
as a linear combination of m1,m2 states of the same M, in a fashion summarized by the
following equation (written in a less cumbersome notation now that the need for detail has
disappeared):
|J, M⟩=
X
m1,m2
C( j1, j2, J|m1,m2, M)| j1,m1; j2,m2⟩.
(16.43)
Here | j1,m1; j2,m2⟩stands for | j1,m1⟩| j2,m2⟩and we have given over to the coefﬁ-
cient C( j1, j2, J|m1,m2, M) the responsibility to vanish when m1 + m2 ̸= M. Thus, the
apparent double summation in Eq. (16.43) is actually a single sum. The coefﬁcients in
Eq. (16.43) are called Clebsch-Gordan coefﬁcients. To resolve the sign ambiguity result-
ing from the orthogonalization processes, they are deﬁned to have signs speciﬁed by the
Condon-Shortley phase convention.
It is important to realize that all the results of this section remain valid irrespective of
whether j1, j2, or both are integral or half-integral. For example, if j1 = 1 and j2 = 1
2
(corresponding to the coupling of the orbital and spin angular momenta of an electron),
the possible J, M states will be a quartet for J = 3/2 (with M values +3/2, +1/2, −1/2,
−3/2), and a doublet for J = 1/2 (with M values +1/2 and −1/2).
A second way to look at the Clebsch-Gordan coefﬁcients is to identify them as the scalar
products
C( j1, j2, J|m1,m2, M) = ⟨J, M| j1,m1; j2,m2⟩.
(16.44)
Because of the method used for the construction of the |J, M⟩, we can make one additional
observation: The Clebsch-Gordan coefﬁcients will all be real, even if the | j1,m1⟩and
| j2,m2⟩used for their construction are not.
The Clebsch-Gordan expansion can be interpreted in yet another way. We can view the
Clebsch-Gordan coefﬁcients as the elements of a transformation matrix converting func-
tions of the m1,m2 basis into those of the J, M basis; since both basis sets are orthonormal,
the transformation must be unitary (and because it is real, orthogonal). This means that the
inverse transformation, (J, M) →(m1,m2), must have a transformation matrix that is the

790
Chapter 16 Angular Momentum
transpose of that for the forward transformation (m1,m2) →(J, M). That means that we
also have the equation
| j1,m1; j2,m2⟩=
X
J M
C( j1, j2, J|m1,m2, M)|J, M⟩.
(16.45)
This equation is correct and corresponds to our discussion. Note that instead of reversing
the index order of the transformation matrix we have interchanged the index sets identify-
ing the functions.
In passing, we make one further comment. While the Clebsch-Gordan coefﬁcients can be
identiﬁed as forming a transformation matrix, note that their row/column indexing differs
from the pattern to which we are accustomed, since, instead of labels running from 1 to n
(the dimension of the transformation), we are using in one dimension the compound index
(m1,m2), and in the other dimension the compound quantity (J, M). This Clebsch-Gordan
matrix will be somewhat sparse (containing many zero elements). The zeros occur because
the coefﬁcients vanish unless M = m1 + m2.
There is a signiﬁcant literature on the practical computation of Clebsch-Gordan coefﬁ-
cients,1 but to make the present discussion complete we simply give here a closed general
formula:
C( j1, j2, J|m1,m2, M) = F1F2F3,
(16.46)
where
F1 =
s
( j1 + j2 −J)!(J + j1 −j2)!(J + j2 −j1)!(2J + 1)
( j1 + j2 + J + 1)!
F2 =
q
(J + M)!(J −M)!( j1 + m1)!( j1 −m1)!( j2 + m2)!( j2 −m2)!,
F3 =
X
s
(−1)s
( j1 −m1 −s)!( j2 + m2 −s)!(J −j2 + m1 + s)!
×
1
(J −j1 −m2 + s)!( j1 + j2 −J −s)!s!.
The F3 summation is over all integer values of s for which the factorials all have non-
negative arguments (which will be integral). The sum is therefore ﬁnite in extent and F3
is a closed form. Equation (16.46) is only to be used for parameter values that satisfy the
angular momentum and coupling conditions: j1, j2, J must satisfy the triangle condition,
mi is to be from the sequence li, li −1,...,−li (i = 1,2), M to be from J, J −1,...,−J,
and M = m1 + m2.
Finally, we call attention to the fact that Clebsch-Gordan coefﬁcients have symmetries
that are not obvious from the foregoing development. To expose the symmetries, it is
convenient to convert them to the Wigner 3 j-symbols, deﬁned as
 j1 j2 j3
m1 m2 m3

= (−1) j1−j2−m3
(2 j3 + 1)1/2 C( j1, j2, j3|m1,m2,−m3).
(16.47)
1See Biedenharn and Louck, Brink and Satchler, Edmonds, Rose, and Wigner in Additional Readings. Clebsch-Gordan coefﬁ-
cients are also tabulated in many places, and can easily be found online by a Web search.

16.2 Angular Momentum Coupling
791
Extensive discussion of 3 j-symbols and related quantities is beyond the scope of this book.
This important, but advanced, topic is presented in most of the sources listed under Addi-
tional Readings.
The 3 j-symbols are invariant under even permutations of the indices (1,2,3), but under
odd permutations (1,2,3) →(k,l,n) transform as follows:
 j1
j2
j3
m1 m2 m3

= (−1) j1+ j2+ j3
 jk
jl
jn
mk ml mn

.
(16.48)
They also have the following symmetry under change of sign of their lower indices:
 j1
j2
j3
m1 m2 m3

= (−1) j1+ j2+ j3

j1
j2
j3
−m1 −m2 −m3

.
(16.49)
Even though some of the ji may be half-integral, remember that j3 must be equal to j1 + j2
or differ therefrom by an integer. This fact causes the powers of −1 in Eqs. (16.47) through
(16.49) to be integral, so these factors are not multiple-valued and the sign assignments of
the 3 j-symbols are unambiguous. These symmetry relations make a table of 3 j-symbols
more compact than one of Clebsch-Gordan coefﬁcients; such tables can be found in the
literature,2 and a short list is included here, as Table 16.1.
We close this section with two examples.
Table 16.1
Wigner 3 j-Symbols


1
2
1
2
1
1
2
−1
2
0

= 1
√
6


1
2
1
2
1
1
2
1
2 −1

= −1
√
3


1
2
1
2
0
1
2 −1
2
0

= 1
√
2


1
1
2
−3
2
1
1
2
−3
2

= 1
2


1
1
2
−3
2
1 −1
2
−1
2

= −
1
√
12


1
1
2
−3
2
0
1
2
−1
2

= −1
√
6
 1
1
0
0
0
0
!
= −1
√
3
 1
1
0
1 −1
0
!
= 1
√
3
 1
1
1
0
0
0
!
= 0
 1
1
1
1 −1
0
!
= 1
√
6
 1
1
2
0
0
0
!
=
r
2
15
 1
1
2
1 −1
0
!
=
1
√
30
 1
1
2
1
0 −1
!
= −
1
√
10
 1
1
2
1
1 −2
!
= 1
√
5
 1
2
2
1 −1
0
!
= −
1
√
10
 1
2
2
1 −2
1
!
=
1
√
15
 1
2
2
0
1 −1
!
= −
1
√
30
 1
2
2
0
2 −2
!
=
r
2
15
 1
2
2
0
0
0
!
= 0
 1
2
3
0
0
0
!
= −
r
3
35
 1
2
3
1 −1
0
!
= −
1
√
35
 1
2
3
1
0 −1
!
=
r
2
35
 1
2
3
1 −2
1
!
=
1
√
105
 1
2
3
1
1 −2
!
= −
r
2
21
 1
2
3
1
2 −3
!
= 1
√
7
 1
2
3
0
1 −1
!
=
r
8
105
 1
2
3
0
2 −2
!
= −
1
√
21
2See, for example, M. Rotenberg, R. Bivins, N. Metropolis, and J. K. Wooten, Jr., The 3j- and 6j-Symbols. Cambridge, MA:
Massachusetts Institute of Technology Press (1959).

792
Chapter 16 Angular Momentum
Example 16.2.2
TWO SPINORS
This example describes a problem that exists entirely in a abstract space, namely the cou-
pling of two spin- 1
2 particles (e.g., electrons) to form combined states of deﬁnite J. Letting
α stand for a normalized single-particle state with j = 1
2, m = + 1
2, with β a normalized
state with j = 1
2, m = −1
2, we have the following four states in the m1,m2 basis:
M = 1 :
αα
M = 0 :
αβ
βα
M = −1 :
ββ
For all these two-particle states, the ﬁrst symbol refers to particle 1, the second to particle
2. From Eq. (16.31) and Example 16.1.2, we have j−α = β, j−β = 0, and we can use
the following rearrangement of Eq. (16.31) to deal with the |J, M⟩states. Again we use a
notation in which the lower entry in the ket is J; the upper entry is M:

M −1
J

=
1
√(J + M)(J −M + 1) J−

M
J

.
(16.50)
The maximum M value in this system is M = +1, so the one state of this M value must
have J = 1. showing that

1
1

= αα. Starting from it, we lower M:

1
1

= αα,

0
1

= 1
√
2
J−

1
1

= 1
√
2
βα + 1
√
2
αβ,

−1
1

= 1
√
2
J−

0
1

= 1
√
2
 1
√
2
ββ + 1
√
2
ββ

= ββ.
These are the well-known members of the S = 1 spin multiplet, which is known as a
triplet. At M = 0, where there were two m1,m2 states, the state orthogonal to

0
1

must
be the

0
0

state.
Even though we do not have an entirely explicit representation of the states α and β, we
do know that they are normalized eigenstates of a Hermitian operator (Jz) with different
eigenvalues, and therefore they must be orthogonal. Thus, we can apply the Gram-Schmidt
process to the M = 0 subspace, using the relations
⟨α|α⟩= ⟨β|β⟩= 1,
⟨α|β⟩= 0.
We easily ﬁnd that the normalized function orthogonal to (αβ +βα)/
√
2 is (αβ −βα)/
√
2.
It is the only member of the S = 0 multiplet, and is therefore known as a singlet. Note that
we didn’t have to know anything speciﬁc about spin operators to carry out this analysis.

16.2 Angular Momentum Coupling
793
Our tableau of states can now be written in the J, M basis:
J = 1
J = 0
M = 1
αα
M = 0
(αβ + βα)/
√
2
(αβ −βα)/
√
2
M = −1
ββ
From the J, M tableau, we can read out the Clebsch-Gordan coefﬁcients:
C
  1
2, 1
2,1
 1
2, 1
2,1

= 1
C
  1
2, 1
2,1
 1
2,−1
2,0

= C
  1
2, 1
2,1
 −1
2, 1
2,0

= 1
√
2
C
  1
2, 1
2,0
 1
2,−1
2,0

= −C
  1
2, 1
2,1
 −1
2, 1
2,0

= 1
√
2
C
  1
2, 1
2,1
 −1
2,−1
2,−1

= 1
These coefﬁcients can also be obtained from our table of 3 j-symbols. Using Eq. (16.47),
we ﬁnd the coefﬁcients for |J = 1, M = 0⟩to be
C
  1
2, 1
2,1
 1
2,−1
2,0

=
√
3
 1
2
1
2
1
1
2 −1
2
0
!
,
C
  1
2, 1
2,1
 −1
2, 1
2,0

=
√
3
 
1
2
1
2
1
−1
2
1
2
0
!
.
Both these 3 j-symbols correspond to the same entry in Table 16.1, and the symmetry rules
give each the value +1/
√
6. Therefore, both these Clebsch-Gordan coefﬁcients evaluate
to
√
3/
√
6, or, as expected, 1/
√
2.
For |J = 0, M = 0⟩, we have, again calling on Eq. (16.47),
C
  1
2, 1
2,0
 1
2,−1
2,0

=
 1
2
1
2
0
1
2 −1
2
0
!
,
C
  1
2, 1
2,0
 −1
2, 1
2,0

=
 
1
2
1
2
0
−1
2
1
2
0
!
.
Again these 3 j-symbols both correspond to the same tabulated entry (with value 1/
√
2),
but this time the symmetry rules cause them to have the respective values +1/
√
2 and
−1/
√
2, in agreement with our explicit evaluation.
■
Example 16.2.3
COUPLING OF p AND d ELECTRONS
As most physics students know, a p state is an angular momentum eigenstate with l = 1 (so
m can be 1, 0, or −1). The three normalized functions constituting its multiplet are often
denoted p+, p0, and p−. A d state has l = 2; we denote the ﬁve normalized members of its

794
Chapter 16 Angular Momentum
multiplet d+2, d+, d0, d−, and d−2. The m1,m2 basis has 15 members; grouped according
to their M values, they consist of
M = +3
p+d+2
M = +2
p+d+
p0d+2
M = +1
p+d0
p0d+
p−d+2
M =
0
p+d−
p0d0
p−d+
M = −1
p+d−2
p0d−
p−d0
M = −2
p0d−2
p−d−
M = −3
p−d−2
This is the same coupling of angular momenta j = 1 and j = 2 that was introduced at
the beginning of the subsection entitled Vector Model, but we are now illustrating how to
carry out the coupling computations using Clebsch-Gordan coefﬁcients and 3 j-symbols.
From this diagram, we expect one multiplet with J = 3, which in atomic spectroscopy is
denoted F (multiparticle orbital angular momentum states are designated using upper-case
letters); one with J = 2 (called D), and one with J = 1 (called P). Our plan is to construct
these using the 3 j-symbols given in Table 16.1.
We start by writing, in the notation |J, M⟩, the members of the F multiplet with M ≥1
in terms of Clebsch-Gordan coefﬁcients (those for M < 1 do not raise important new
points):
|3,3⟩= C(1,2,3|1,2,3)p+d+2,
|3,2⟩= C(1,2,3|1,1,2)p+d+ + C(1,2,3|0,2,2)p0d+2,
|3,1⟩= C(1,2,3|1,0,1)p+d0 + C(1,2,3|0,1,1)p0d+ + C(1,2,3|−1,2,1)p−d+2.
The D and P multiplet members for M ≥1 are
|2,2⟩= C(1,2,2|1,1,2)p+d+ + C(1,2,2|0,2,2)p0d+2,
|2,1⟩= C(1,2,2|1,0,1)p+d0 + C(1,2,2|0,1,1)p0d+ + C(1,2,2|−1,2,1)p−d+2,
|1,1⟩= C(1,2,1|1,0,1)p+d0 + C(1,2,1|0,1,1)p0d+ + C(1,2,1|−1,2,1)p−d+2.
We then express the Clebsch-Gordan coefﬁcients in terms of 3 j-symbols. Doing just a rep-
resentative few, using Eq. (16.47) and then the symmetry rules, Eqs. (16.48) and (16.49),
C(1,2,3|1,2,3) = +
√
7
 
1
2
3
1
2 −3
!
= 1,
C(1,2,2|1,1,2) = −
√
5
 
1
2
2
1
1 −2
!
= +
√
5
 
1
2
2
1 −2
1
!
=
r
1
3,
C(1,2,1|−1,2,1) =
√
3
 
1
2
1
−1
2 −1
!
=
√
3
 
1
1
2
1
1 −2
!
=
r
3
5.

16.2 Angular Momentum Coupling
795
Substituting these and other Clebsch-Gordan coefﬁcients into the formulas for |J, M⟩, we
obtain the ﬁnal results:
|3,3⟩= p+d+2,
|3,2⟩=
r
1
3 p0d+2 +
r
2
3 p+d+,
|3,1⟩=
r
1
15 p−d+2 +
r
8
15 p0d+ +
r
2
5 p+d0,
|2,2⟩= −
r
2
3 p0d+2 +
r
1
3 p+d+.
|2,1⟩= −
r
1
3 p−d+2 −
r
1
6 p0d+ +
r
1
2 p+d0,
|1,1⟩=
r
3
5 p−d+2 −
r
3
10 p0d+ +
r
1
10 p+d0.
The reader may verify that states of the same M but different J have the required orthog-
onality. It is also easy to check that all these |J, M⟩states are normalized.
■
Exercises
16.2.1
Derive recursion relations for Clebsch-Gordan coefﬁcients. Use them to calculate
C(11J | m1m2M) for J = 0,1,2.
Hint. Use the known matrix elements of J+ = J1+ + J2+, Ji+, and J2 = (J1 +J2)2, etc.
16.2.2
Deﬁning (Ylχ)M
J by the formula
(Ylχ)M
J =
X
C(l 1
2 J | mlms M)Ylmlχms,
where χ±1/2 are the spin up and down eigenfunctions of σ3 = σz, show that (Ylχ)M
J is
a J, M eigenfunction.
16.2.3
Find the ( j,m) states of a p electron (l = 1), in which the orbital angular momen-
tum of the electron is coupled to its spin angular momentum (s = 1/2) to form states
whose conventional labelings are 2 p1/2 and 2 p3/2. The notation is of the general form
2s+1(symbol)j, where “symbol” is that indicating the l value (i.e., s, p,...).
16.2.4
Repeat Exercise 16.2.3 for l = 1, s = 3/2. Apply the conventional labels to the j,m
states.
16.2.5
A deuterium atom consists of a proton, a neutron, and an electron. Each of these par-
ticles has spin 1/2. The coupling of these three spins can produce J values of 3/2 and
1/2. We consider here only states with no orbital angular momentum.
(a)
Show that these J, M states consist of one quartet (J = 3/2) and two linearly
independent doublets (J = 1/2).

796
Chapter 16 Angular Momentum
Hint. Make a vector-model diagram.
(b)
One way to analyze this problem is to couple the spins of the proton and neutron
to form a nuclear triplet or singlet, and then to couple the resultant nuclear spin
to the electron spin. Find the states that are obtained in this way (designate the
single-particle states pα, pβ, nα, nβ, eα, eβ).
(c)
Another way to analyze this problem is to couple the spins of the proton and elec-
tron to form an atomic triplet or singlet, and then to couple that resultant to the
neutron spin. Find the states that result from this coupling scheme.
(d)
Show that the coupling schemes of parts (b) and (c) span the same Hilbert space.
Note. The actual interaction energies among these angular momenta cause the scheme
of part (b) to be the better way of treating this problem (the triplet nuclear state is
substantially the more stable), and the system actually looks like a spin-1 deuterium
nucleus plus an electron.
16.3
SPHERICAL TENSORS
We have already seen that the set of spherical harmonics of given l transforms within itself
under rotations. We now pursue this idea more formally. In Chapter 3 we saw that rota-
tions could be characterized by the 3 × 3 unitary transformation matrices that transform a
set of coordinates (their basis) into the new set corresponding to the rotation. These matri-
ces could be viewed as second-rank tensors, but because they are restricted to rotational
transformations, they are also known as spherical tensors.
We now wish to consider spherical tensors that transform more general sets of objects
under rotation, and in particular those spherical tensors that have spherical harmonics as
bases. Our new spherical tensors will then have dimensions other than 3 × 3; in fact, they
must exist at all the sizes that correspond to sets of angular momentum eigenfunctions.
Because we have already observed that a set of angular momentum eigenfunctions of a
given J cannot be decomposed into subsets that transform only among themselves under
rotation, we go one step further and call our spherical tensors irreducible.
Continuing for general angular momentum eigenfunctions |L, M⟩, which we assume are
representable in 3-D space as spherical harmonics or objects built from them by angular
momentum coupling, we write the following deﬁning equation for the spherical tensor
describing the effect of a coordinate rotation R on |L, M⟩:
R|L, M⟩=
X
M′
DL
M′M(R)|L, M′⟩.
(16.51)
If the |L, M⟩are actually spherical harmonics (and not more complicated objects that
resulted from angular momentum coupling), Eq. (16.51) can also be written as
Y m
l (R) =
X
m′
Dl
m′m(R)Y m′
l
().
(16.52)
Because we do not need to become embroiled in the details of the action of R on the
coordinates, we have simply replaced (θ,ϕ) by the generic symbol  and have written
R to indicate the coordinates (θ′,ϕ′) that describe the point that was labeled (θ,ϕ) in
the unrotated system. For any given l, Dl
m′m(R) can be regarded as an element of a square

16.3 Spherical Tensors
797
matrix of dimension 2l + 1 with rows and columns labeled by indices m′ and m whose
ranges are (−l,...,+l), not the more customary sequence starting from 1. The Dl
m′m(R)
are unitary, since they describe a transformation between two orthonormal sets. Because
of their early exploitation by Eugene Wigner, they are sometimes called Wigner matrices.
There is an extensive literature (see Additional Readings) on relationships satisﬁed by the
Dl
m′m(R) and on formulas for their evaluation. A related topic included in this book is the
formula, Eq. (3.37), giving the transformation of the basis x, y, z by a rotation through
Euler angles α, β, γ .
Addition Theorem
Equation (16.52) can be used to establish important rotational invariance properties. For
example, consider a quantity A deﬁned as
A =
X
m
Y m
l (1)∗Y m
l (2),
(16.53)
where 1 and 2 are two unrelated sets of angular coordinates. We apply a rotation R to
the coordinate system, denoting the result RA, and evaluating the right-hand side using
Eq. (16.52):
RA =
X
m
 X
µ
Dl
µm(R)Y µ
l (1)
!∗ X
ν
Dl
νm(R)Y ν
l (2)
!
.
(16.54)
We now reorder the summations in Eq. (16.54), and, in the second line of Eq. (16.55),
use the fact that D is unitary to change D∗to the transpose of D−1, thereby leading to the
simpliﬁcation in the third line. We have
RA =
X
µν
 X
m
Dl
µm(R)∗Dl
νm(R)
!
Y µ
l (1)∗Y ν
l (2)
=
X
µν
 X
m
h
Dl(R)−1i
mµ
h
Dl(R)
i
νm
!
Y µ
l (1)∗Y ν
l (2)
=
X
µν
δµνY µ
l (1)∗Y ν
l (2) =
X
µ
Y µ
l (1)∗Y µ
l (2) = A.
(16.55)
This shows that A is rotationally invariant, and is the starting point for an explanation of
why a totally occupied atomic subshell (particles occupying all m values for a given l)
leads to a spherically symmetric overall distribution.
The rotational invariance of A makes it easier for us to actually evaluate it, because
we can choose to do so at a coordinate orientation for which the computation is relatively
simple. Let’s rotate the coordinates to place 1 in the polar direction (so now θ1 = 0),
and the θ value of 2 in the rotated coordinates will be equal to the angle χ between the
1 and 2 directions, which is not affected by a coordinate rotation. In this new set of
coordinates, Y m
l (1) is Y m
l (0,ϕ) and is given, according to Eq. (15.148), as
Y m
l (1) =
r
2l + 1
4π
δm0.

798
Chapter 16 Angular Momentum
The summation in Eq. (16.53) therefore reduces to its m = 0 term, and the only 2 contri-
bution we need is Y 0
l (χ,ϕ2). But because m = 0, this Y does not actually depend on ϕ2,
and has the unambiguous value, from Eq. (15.137),
Y 0
l (χ,ϕ2) =
r
2l + 1
4π
Pl(cosχ).
These results enable us to obtain
A = 2l + 1
4π
Pl(cosχ),
(16.56)
which, because of the rotational invariance, remains true whether or not the coordinate sys-
tem was rotated. Inserting the original formula for A, and solving Eq. (16.56) for Pl(cosχ),
we obtain the spherical harmonic addition theorem,
Pl(cosχ) =
4π
2l + 1
X
m
Y m
l (1)∗Y m
l (2),
(16.57)
where χ is the angle between the directions 1 and 2.
Example 16.3.1
ANGLE BETWEEN TWO VECTORS
A useful special case of the addition theorem is for l = 1, for which P1(cosχ) = cosχ.
Then, writing i ≡θi,ϕi, and evaluating all the spherical harmonics on the right-hand side
of Eq. (16.57), we have
cosχ = 1
2

sinθ1e−iϕ1
∗
sinθ2e−iϕ2

+ cosθ1 cosθ2
+ 1
2

−sinθ1eiϕ1
∗
−sinθ2eiϕ2

= cosθ1 cosθ2 + 1
2 sinθ1 sinθ2

ei(ϕ1−ϕ2) + ei(ϕ2−ϕ1)
.
(16.58)
This reduces to the standard formula for the angle χ between directions (θ1,ϕ1) and
(θ2,ϕ2):
cosχ = cosθ1 cosθ2 + sinθ1 sinθ2 cos(ϕ2 −ϕ1).
(16.59)
■
Spherical Wave Expansion
An important application of the addition theorem is the spherical wave expansion, which
states
eik·r = 4π
∞
X
l=0
lX
m=−l
il jl(kr)Y m
l (k)∗Y m
l (r)
(16.60)
= 4π
∞
X
l=0
lX
m=−l
il jl(kr)Y m
l (k)Y m
l (r)∗.
(16.61)

16.3 Spherical Tensors
799
Here k and r are the magnitudes of k and r, and k, r denote their respective angu-
lar coordinates. The two forms shown are equivalent because a change in the sign of m
changes each harmonic to its complex conjugate (possibly with both harmonics under-
going a sign change). The quantity jl(kr) is a spherical Bessel function. This formula is
particularly useful because it expresses the plane wave on its left-hand side as a series
of spherical waves. This conversion is useful in scattering problems in which a plane
wave, incident upon a scattering center, produces outgoing spherical waves with different
spherical-harmonic (called partial-wave) components.
To establish Eq. (16.61), we write k · r as kr cosχ, where χ is the angle between k and
r, and then expand exp(ikr cosχ) as a series of Legendre polynomials:
eikr cosχ =
∞
X
l=0
cl Pl(cosχ),
(16.62)
with the coefﬁcients cl given by
cl = 2l + 1
2
1
Z
−1
eikrt Pl(t)dt.
(16.63)
We now recognize the integral in Eq. (16.63) as proportional to an integral representation
of jl that was the topic of Exercise 15.2.26 and which we repeat here:
jl(x) = i−l
2
1
Z
−1
eixt Pl(t)dt.
(16.64)
This permits us to evaluate cl, obtaining
cl = (2l + 1)il jl(kr).
Inserting this expression for cl into Eq. (16.62) and replacing Pl(cosχ) in that equation by
its equivalent as given by the addition theorem, Eq. (16.57), we have the desired veriﬁca-
tion of Eq. (16.61).
Laplace Spherical Harmonic Expansion
Another application of the addition theorem is to the Laplace expansion, where in
Chapter 15 we found that the inverse distance between points r1 and r2 could be expanded
in Legendre polynomials:
1
|r1 −r2| =
∞
X
l=0
rl
<
rl+1
>
Pl(cosχ).
(16.65)
Here r1 and r2 are measured from a common origin, with respective magnitudes r1 and
r2; χ is the angle between r1 and r2. We deﬁne r> and r< as, respectively, the larger and

800
Chapter 16 Angular Momentum
the smaller of r1 and r2. If we now insert the addition theorem, we bring this expansion to
the form
1
|r1 −r2| =
∞
X
l=0
4π
2l + 1
rl
<
rl+1
>
lX
m=−l
Y m
l (1)∗Y m
l (2),
(16.66)
where 1 and 2 are the angular coordinates of r1 and r2 in a coordinate system of arbi-
trary orientation.
Example 16.3.2
SPHERICAL GREEN’S FUNCTION
An explicit expansion of the Green’s function for the 3-D Laplace equation may be
obtained by considering its deﬁning equation
∇2
1G(r1,r2) = δ(r1 −r2)δ(1 −2)
r2
1
,
(16.67)
where we have written ∇1 to remind the reader that it acts only on r1. Also, note that on the
right-hand side the factor 1/r2
1 is inserted to adjust the angular delta function to unit scale;
it could equally well have been written 1/r2
2 because of the presence also of δ(r1 −r2).
We now insert into Eq. (16.67), the following general expansion for G(r1,r2):
G(r1,r2) =
X
lm
X
l′m′
gll′mm′(r1,r2)Y m′
l′ (1)Y m
l (2)∗,
and the expansion of Exercise 16.3.9 for the angular delta function:
δ(1 −2) =
X
lm
Y m
l (1)Y m
l (2)∗.
We also write the Laplacian in the form
∇2
1 = ∂2
∂r2
1
+ 2
r1
∂
∂r1
−L2
1
r2
1
,
where L1 operates only on functions of 1.
We next take scalar products of the resulting expanded equation with all possible spher-
ical harmonics of both 1 and 2, in addition taking note that Y m
l (1) is an eigenfunction
of L2
1 with eigenvalue l(l +1). We ﬁnd that many terms cancel, so the scalar products lead,
for each l and m, to the following result:
"
d2
dr2
1
+ 2
r1
d
dr1
−l(l + 1)
#
gl(r1,r2) = δ(r1 −r2).
(16.68)
We have collapsed the original four indices of gll′mm′(r1,r2) into the single index l
because all instances of Eq. (16.68) with l ̸= l′ or m ̸= m′ vanish, and g has the same value
for all m.
Equation (16.68) is for each l an ODE which, with boundary conditions g = 0 at r = 0
and r = ∞, deﬁnes the spherical Green’s functions we identiﬁed in Section 10.2. Since

16.3 Spherical Tensors
801
the homogeneous equation corresponding to Eq. (16.68) has solutions rl and r−l−1, its
Green’s function must have the form
g(r1,r2) = Al
rl
<
rl+1
>
,
(16.69)
with Al = −1/(2l + 1), a result that can be obtained by application of Eq. (10.19).
Comparing Eq. (16.66) with the result for G(r1,r2) obtained by using Eq. (16.69), we
now have yet another way of verifying the result that is familiar from Coulomb’s law:
G(r1,r2) = −
∞
X
l=0
1
2l + 1
rl
<
rl+1
>
lX
m=−l
Y m
l (1)∗Y m
l (2)
(16.70)
= −1
4π
1
|r1 −r2|.
(16.71)
■
General Multipoles
We are now ready to return to the multipole expansion. Given a set of charges qi at respec-
tive points ri, all located within a sphere of radius a centered at the origin of a spherical
polar coordinate system, we now consider the calculation of the electrostatic potential
ψ(r) at points outside the sphere, i.e., at points r such that r > a. Our starting point is
the Laplace expansion of 1/|r1 −r2| in the form presented as Eq. (16.66). Since for all
ri we have ri < r, we can write
ψ(r) =
1
4πϵ0
X
i
qi
∞
X
l=0
4π
2l + 1
rl
i
rl+1
lX
m=−l
Y m
l (θi,ϕi)∗Y m
l (θ,ϕ)
=
1
4πε0
∞
X
l=0
lX
m=−l
4π
2l + 1
"X
i
qirl
i Y m
l (θi,ϕi)∗
#
Y m
l (θ,ϕ)
rl+1
.
(16.72)
We see that this substitution has caused the entire effect of the charges qi to be localized
into the expressions
Mm
l =
4π
2l + 1
X
i
qirl
i Y m
l (θi,ϕi)∗,
(16.73)
so that the potential due to the qi, for points farther from r = 0 than all the charges, assumes
the compact form,
ψ(r) =
1
4πε0
∞
X
l=0
lX
m=−l
Mm
l
Y m
l (θ,ϕ)
rl+1
.
(16.74)
Equation (16.74) is called the multipole expansion, and the Mm
l are known as the multi-
pole moments of the charge distribution. At this point we note that different authors deﬁne
the multipole moments with different scalings, making up the difference by the inclusion
of an appropriate factor in their formulas correponding to Eq. (16.74). One reason for the

802
Chapter 16 Angular Momentum
variety of notations is that Mm
l
as deﬁned in Eq. (16.73), which leads to the simplest for-
mulas, does not yield the low-order moments at their “traditional” scalings. For example,
the monopole moment, M0
0, evaluates to (4π)1/2 times the total charge, while M0
1, the
z-component of the dipole moment, comes out as (4π/3)1/2 P
i qizi.
Of more fundamental interest is the relation between the multipole moments and the
Cartesian forms that can represent them. We proceed by considering the Mm
l
that result
from a unit charge placed at (x, y, z). Using the Cartesian representations of the spherical
harmonics given in Table 15.4, the ﬁrst few Mm
l have the forms given here:
M2
2 =
3π
10
1/2
(x2 −y2 + 2ixy)
M1
1 = −
2π
3
1/2
(x + iy)
M1
2 = −
3π
40
1/2
z(x + iy)
M0
0 = (4π)1/2
M0
1 =
4π
3
1/2
z
M0
2 =
4π
5
1/2
2z2 −x2 −y2
2

M−1
1
=
2π
3
1/2
(x −iy)
M−1
2
=
3π
40
1/2
z(x −iy)
M−2
2
=
3π
10
1/2
(x2 −y2 −2ixy)
The ﬁrst point to note is that for any l value, the Cartesian representation of each Mm
l
involves a homogeneous polynomial of combined degree l in x, y, and z. It is obviously
necessary that the Mm
l of different m be linearly independent, and we see that for l = 0 and
l = 1, the number of independent monomials is equal to 2l + 1, the number of m values.
Speciﬁcally, for l = 0 we have only the monomial 1, while for l = 1 we have x, y, and z.
But for l = 2, there are six independent monomials (x2, y2, z2, xy, xz, yz), but only ﬁve
values of m. The discrepancy is resolved by observing that one linear combination of these
monomials, namely r2 = x2 + y2 + z2, remains invariant under all rotations of the coor-
dinates, and it therefore has different symmetry properties than the ﬁve-dimensional space
orthogonal to r2. In fact, r2 has the same symmetry as M0
0, but has the wrong r depen-
dence to contribute to a solution to the Laplace equation (and therefore to the potential of
a charge distribution).
If we were to continue to l = 3, we would ﬁnd that there are 10 linearly independent
monomials of degree 3, but they divide into a group of seven functions (the space spanned
by Mm
3 ) with an orthogonal complement (functions orthogonal to the ﬁrst seven) of dimen-
sion 3. These three remaining functions have a rotational symmetry similar to Mm
1 , but
again with the wrong r dependence to contribute to the potential. This type of pattern con-
tinues to higher l, making logical the observation that a multipole moment of degree l (a
“2l-moment”) has only 2l + 1 components, despite the fact that in general the space of
homogeneous polynomials of degree l has a larger dimension.
The multipole expansion is useful for continuous distributions of charge in addition
to the discrete charge sets we have considered up to this point. The generalization of
Eq. (16.73) is
Mm
l =
4π
2l + 1
Z
ρ(r′)(r′)l+2Y m
l (θ′,ϕ′)∗sinθ′dr′dθ′dϕ′,
(16.75)

16.3 Spherical Tensors
803
where ρ(r) is the charge density. This expression will yield valid results when ψ(r) is
computed via Eq. (16.74) for r values greater than the largest r′ for which ρ(r) is nonzero.
Integrals of Three Spherical Harmonics
Our ﬁnal spherical tensor application is to the integrals of three spherical harmonics (all
of the same argument). These integrals arise in the evaluation of matrix elements of angle-
dependent operators which themselves can be written in terms of spherical harmonics.
While it is possible to evaluate some such integrals using the techniques illustrated in
Eq. (15.152), a more general result is available. Note that this is not an angular momentum
coupling problem of the type we considered in Section 16.2, because that section treated
angular momenta with independent arguments that depended on different variables. Here
we have a different and more specialized situation in which all three angular momentum
functions have the same argument.
The formula we seek is most easily derived if we have access to values of some of the
rotation coefﬁcients Dl
m′m (a.k.a. Wigner matrices) deﬁned in Eq. (16.52). The coefﬁcients
we need can be easily deduced with the aid of the spherical harmonic addition theorem, so
we start by establishing the following lemma (a lemma is a mathematical result needed to
prove something else):
Lemma: Evaluation of Dl
m0(R):
Writing ﬁrst the spherical harmonic addition theorem, Eq. (16.57),
Pl(cosχ) =
4π
2l + 1
X
m
Y m
l (1)∗Y m
l (2),
where χ is the angle between the directions 1 and 2, we replace its left-hand side by
the equivalent form
Pl(cosχ) =
r
4π
2l + 1Y 0
l (χ,0),
thereby reaching
Y 0
l (χ,0) =
r
4π
2l + 1
X
m
Y m
l (1)∗Y m
l (2).
(16.76)
We now compare this expression with Eq. (16.52), which we write here in a notation
designed to make the comparison more obvious:
Y 0
l (R2) =
X
m
Dl
m0(R)Y m
l (2).
(16.77)
If we select R to be a rotation that converts 1 to the polar direction, then R2 will be
(χ,0); note that Y 0
l (R2) is independent of ϕ so we can set its ϕ coordinate to zero. Thus,
the comparison of Eqs. (16.76) and (16.77) yields
Dl
m0(R) =
r
4π
2l + 1Y m
l (1)∗.
(16.78)

804
Chapter 16 Angular Momentum
We remind the reader that R is a rotation that converts 1 to the polar direction.
Equation (16.78) has been derived under the assumption that the quantities being
rotationally transformed are spherical harmonics (and not more complicated angular-
momentum functions such as might be obtained via angular-momentum coupling). How-
ever, it is possible to show that the result generalizes, without change, to any angular
momentum functions of integer l.
■
We now continue toward the goal of this subsection, namely the evaluation of integrals
involving three spherical harmonics. The result we seek involves products of spherical
harmonics with the same argument, but our method of obtaining that result proceeds by
considering the rotational behavior of an angular momentum coupling formula (i.e., a prod-
uct involving spherical harmonics of different arguments). So we now look at a special case
of Eq. (16.45),
Y 0
l1(1)Y 0
l2(2) =
X
L
C(l1,l2, L|0,0,0)|L,0⟩,
(16.79)
where | j1,m1; j2,m2⟩of Eq. (16.45) is the product of spherical harmonics with m1 =
m2 = 0 shown on the left-hand side of Eq. (16.79); the |J, M⟩state of Eq. (16.45) is now
|L,0⟩. We next apply a rotation R to Eq. (16.79), using Eqs. (16.51) and (16.52) to get
X
m1m2
Dl1
m10(R)Dl2
m20(R)Y m1
l1 (1)Y m2
l2 (2) =
X
L,σ
C(l1,l2, L|0,0,0)DL
σ0(R)|L,σ⟩.
(16.80)
Finally, we convert |L,σ⟩back to the m1,m2 basis, using Eq. (16.43):
X
m1m2
Dl1
m10(R)Dl2
m20(R)Y m1
l1 (1)Y m2
l2 (2) =
X
L,σ
C(l1,l2, L|0,0,0)DL
σ0(R)
×
X
m1m2
C(l1,l2, L|m1,m2,σ)Y m1
l1 (1)Y m2
l2 (2).
(16.81)
This relatively complicated equation must be satisﬁed for all values of 1 and 2, which
will only be possible if its two sides are equal for each set of m1,m2 values. We therefore
have the set of simpler equations,
Dl1
m10(R)Dl2
m20(R) =
X
Lσ
C(l1,l2, L|0,0,0)C(l1,l2, L|m1,m2,σ)DL
σ0(R),
(16.82)
satisﬁed separately for all values of the free parameters.
We are now ready to replace all the Dl in Eq. (16.82) by the result obtained in our
lemma, Eq. (16.78). Since the rotation R is arbitrary, both in the lemma and in the present
work, our use of Eq. (16.78) will produce some angular coordinates  that have nothing
to do with the i we were previously using; the point that is important here is that because
the same R occurs throughout Eq. (16.82), the application of Eq. (16.78) will everywhere

16.3 Spherical Tensors
805
produce the same . Substitution of the lemma result yields
4π
√(2l1 + 1)(2l2 + 1)Y m1
l1 ()∗Y m2
l2 ()∗=
X
Lσ
C(l1,l2, L|0,0,0)C(l1,l2, L|m1,m2,σ)
r
4π
2L + 1Y σ
L ()∗.
Since the Y m
l
are the only potentially complex quantities appearing here, we may remove
the complex conjugate signs by complex conjugating the entire equation. After other
minor rearrangements and recognition of the fact that the only contributing σ value is
σ =m1 + m2, we reach the ﬁnal form
Y m1
l1 ()Y m2
l2 () =
X
L
s
(2l1 + 1)(2l2 + 1)
4π(2L + 1)
× C(l1,l2, L|0,0,0)C(l1,l2, L|m1,m2,m1 +m2)Y m1 +m2
L
().
(16.83)
At last we can meet the objective of this subsection. Multiplying both sides of
Eq. (16.83) by some Y m3
l3 ()∗and integrating in  over the angular space, we get
D
Y m3
l3
Y m1
l1
Y m2
l2
E
=
2π
Z
0
dϕ
π
Z
0
sinθdθY m3
l3 (θ,ϕ)∗Y m1
l1 (θ,ϕ)Y m2
l2 (θ,ϕ)
=
s
(2l1 + 1)(2l2 + 1)
4π(2L + 1)
C(l1,l2,l3|0,0,0)C(l1,l2,l3|m1,m2,m3).
(16.84)
We do not have to include a Kronecker delta because the condition m3 = m1 + m2 is taken
care of by the fact that the Clebsch-Gordan coefﬁcients vanish in the absence of this or any
other condition needed for a nonzero result.
Some further insight can be obtained by considering the special case m1 = m2 = m3 = 0
and writing the spherical harmonics in terms of Legendre polynomials. This brings us (after
the substitution t = cosθ) to
1
Z
−1
Pl3(t)Pl1(t)Pl2(t)dt =
2
2l3 + 1C(l1,l2,l3|0,0,0)2.
(16.85)
Since we know that the Legendre polynomial Pl(t) of even l is an even function of t, while
that of odd l is odd in t, we see from Eq. (16.85) that unless l1 +l2 +l3 is even, the integral
will vanish, telling us that C(l1,l2,l3|0,0,0) will only be nonzero if l1 + l2 + l3 is even.
In addition, if the product of any two of the Pl(t) does not contain a power of t as large as
the index of the third Pl, the integral will vanish due to the orthogonality of the Legendre
functions. This observation translates into a triangle condition, namely that the integral
will vanish unless |l1 −l2| ≤l3 ≤l1 +l2. Since these are conditions on the Clebsch-Gordan
coefﬁcient C(l1,l2,l3|0,0,0), they apply also to the general integral formula, Eq. (16.84).
Summarizing, integrals of products of three spherical harmonics, evaluated in
Eq. (16.84), will only be nonzero if the three following conditions are satisﬁed:

806
Chapter 16 Angular Momentum
1.
The l values satisfy the triangle condition |l1 −l2| ≤l3 ≤l1 + l2,
2.
The m values satisfy the condition m3 = m1 + m2,
3.
The sum of the l values, l1 + l2 + l3, is even.
Exercises
16.3.1
For l = 1, Eq. (16.52) becomes
Y m
1 (θ′,ϕ′) =
1
X
m′=−1
D1
m′m(α,β,γ )Y m′
1 (θ,ϕ).
Rewrite these spherical harmonics in Cartesian form. Show that the resulting Cartesian
coordinate equations are equivalent to the Euler rotation matrix A(α,β,γ ), Eq. (3.37).
16.3.2
In proving the addition theorem, we assumed that Y k
l (θ1,ϕ1) could be expanded in a
series of Y m
l (θ2,ϕ2), in which m varied from −l to +l but l was held ﬁxed. What argu-
ments can you develop to justify summing only over the upper index, m, and not over
the lower index, l?
Hints. One possibility is to examine the homogeneity of the Y m
l , that is, Y m
l
may be
expressed entirely in terms of the form cosl−p θ sinp θ, or xl−p−s y pzs/rl. Another
possibility is to examine the behavior of the Legendre equation under rotation of the
coordinate system.
16.3.3
An atomic electron with angular momentum l and magnetic quantum number m has a
wave function
ψ(r,θ,ϕ) = f (r)Y m
l (θ,ϕ).
Show that the sum of the electron densities in a given complete shell is spherically
symmetric; that is, Pl
m=−l ψ∗(r,θ,ϕ)ψ(r,θ,ϕ) is independent of θ and ϕ.
16.3.4
The potential of an electron at point re in the ﬁeld of Z protons at points rp is
8 = −e2
4πε0
Z
X
p=1
1
|re −rp|.
Show that for re larger than all rp, this may be written as
8 = −
e2
4πε0re
Z
X
p=1
X
L,M
rp
re
L
4π
2L + 1Y M
L (θp,ϕp)∗Y M
L (θe,ϕe).
How should 8 be written for re < rp?
16.3.5
Two protons are uniformly distributed within the same spherical volume. If the coor-
dinates of one element of charge are (r1,θ1,ϕ1) and the coordinates of the other are
(r2,θ2,ϕ2) and r12 is the distance between them, the element of repulsion energy will
be given by
dψ = ρ2 dτ1 dτ2
r12
= ρ2r2
1 dr1 sinθ1dθ1dϕ1r2
2 dr2 sinθ2 dθ2 dϕ2
r12
,

16.3 Spherical Tensors
807
where
ρ = charge
volume =
3e
4π R3
and
r2
12 = r2
1 + r2
2 −2r1r2 cosγ.
Here ρ is the charge density and γ is the angle between r1 and r2. Calculate the
total electrostatic energy (of repulsion) of the two protons. This calculation is used in
accounting for the mass difference in “mirror” nuclei, such as O15 and N15.
ANS.
6
5
e2
R .
16.3.6
Each of the two 1s electrons in helium may be described by a hydrogenic wave function
ψ(r) =
 
Z3
πa3
0
!1/2
e−Zr/a0
in the absence of the other electron. Here Z, the atomic number, is 2. The symbol a0
is the Bohr radius, ¯h2/me2. Find the mutual potential energy of the two electrons,
given by
Z
ψ∗(r1)ψ∗(r2)
e2
|r1 −r2|ψ(r1)ψ(r2)d3r1 d3r2.
ANS.
5e2Z
8a0
.
16.3.7
The probability of ﬁnding a 1s hydrogen electron in a volume element r2dr sinθ dθ dϕ is
1
πa3
0
e−2r/a0r2 dr sinθ dθ dϕ,
where r is the distance of the electron from the nucleus. Find the electrostatic potential
of this charge distribution at points r1, where you may not assume that r1 is on the polar
axis of your coordinate system. Calculate the potential from
V (r1) = −
e
4πε0
Z ρ(r2)
r12
d3r2,
where r12 = |r1 −r2|. Expand r12. Apply the Legendre polynomial addition theorem
and show that the angular dependence of V (r1) drops out.
ANS.
V (r1) = −
e
4πε0
 1
2r1
γ

3, 2r1
a0

+ 1
a0
0

2, 2r1
a0

, where
γ and 0 are incomplete gamma functions, Eq. (13.73).
16.3.8
A hydrogen electron in a 2p orbital has a charge distribution
ρ = −
e
64πa5
0
r2e−r/a0 sin2 θ,
where a0 = ¯h2/me2 is the Bohr radius, and r is the distance between the electron and
the nucleus. Find the electrostatic potential energy for this atomic state.

808
Chapter 16 Angular Momentum
16.3.9
(a)
As a Laplace series and as an example of Eq. (5.27), show that
δ(1 −2) =
∞
X
l=0
lX
m=−l
Y m
l (θ2,ϕ2)∗Y m
l (θ1,ϕ1).
(b)
Show also that this same representation of the Dirac delta function may be
written as
δ(1 −2) =
∞
X
l=0
2l + 1
4π
Pl(cosγ ),
and identify γ . Now, if you can justify equating the summations over l term by term,
you have an alternate derivation of the spherical harmonic addition theorem.
16.3.10
Verify
(a)
Z
Y M
L (θ,ϕ)Y 0
0 (θ,ϕ)Y M∗
L
(θ,ϕ)d =
1
√
4π
,
(b)
Z
Y M
L Y 0
1 Y M∗
L+1d =
r
3
4π
s
(L + M + 1)(L −M + 1)
(2L + 1)(2L + 3)
,
(c)
Z
Y M
L Y 1
1 Y M+1∗
L+1
d =
r
3
8π
s
(L + M + 1)(L + M + 2)
(2L + 1)(2L + 3)
,
(d)
Z
Y M
L Y 1
1 Y M+1∗
L−1
d = −
r
3
8π
s
(L −M)(L −M −1)
(2L −1)(2L + 1)
.
These integrals were used in an investigation of the angular correlation of internal
conversion electrons.
16.3.11
Show that
(a)
1
Z
−1
x PL(x)PN(x)dx =



2(L + 1)
(2L + 1)(2L + 3), N = L + 1,
2L
(2L −1)(2L + 1), N = L −1,
(b)
1
Z
−1
x2PL(x)PN(x)dx =



2(L + 1)(L + 2)
(2L + 1)(2L + 3)(2L + 5), N = L + 2,
2(2L2 + 2L −1)
(2L −1)(2L + 1)(2L + 3), N = L,
2L(L −1)
(2L −3)(2L −1)(2L + 1), N = L −2.

16.4 Vector Spherical Harmonics
809
16.3.12
Since x Pn(x) is a polynomial (of degree n + 1), it may be represented by the Legendre
series
x Pn(x) =
∞
X
s=0
as Ps(x).
(a)
Show that as = 0 for s < n −1 and s > n + 1.
(b)
Calculate an−1, an, and an+1 and show that you have reproduced the recurrence
relation, Eq. (15.18).
Note. This argument may be put in a general form to demonstrate the existence of a
three-term recurrence relation for any of our complete sets of orthogonal polynomials:
xϕn = an+1ϕn+1 + anϕn + an−1ϕn−1.
16.4
VECTOR SPHERICAL HARMONICS
Maxwell’s equations lead naturally to applications involving a vector Helmholtz equation
for the vector potential A, and various classical and quantum-mechanical problems in this
area are usefully attacked by introducing vector spherical harmonics. Our ﬁrst step in
this direction will be to recognize that a set of unit vectors can be thought of as a spherical
tensor of rank 1 and can be discussed in terms of the angular momentum formalism. We
will later (in Chapter 17) pursue rotational symmetry in greater depth; for our present
purposes it sufﬁces to conﬁrm the relationship between rotations in 3-D space and angular
momentum operators.
A Spherical Tensor
We consider here vectors in 3-D space, of the form u = ux ˆex + uyˆey + uzˆez, but, unlike
our practice in Chapter 3, we will permit the u j to be complex, and use the complex scalar
product ⟨u|u⟩1/2 as a measure of the magnitude of u. If we restrict the vectors u to be of
unit length, they satisfy the conditions necessary to be identiﬁed as spherical tensors of
rank 1.
We now introduce operators Ki deﬁned by the following matrices:
K1 =


0
0
0
0
0
−i
0
i
0

,
K2 =


0
0
i
0
0
0
−i
0
0

,
K3 =


0
−i
0
i
0
0
0
0
0

.
(16.86)
The reader can easily verify that these matrices satisfy the angular momentum commu-
tation rules, and in fact describe the result of applying the angular momentum operator
L = r × p, where p = −i∇, to the basis x, y, z. We next calculate
K2 = K 2
1 + K 2
2 + K 2
3 = 2


1
0
0
0
1
0
0
0
1

,

810
Chapter 16 Angular Momentum
showing that all members of the basis are eigenvectors of K2, with eigenvalue 2, which is
k(k +1) with k = 1. All members of our basis therefore have one unit of some abstract sort
of angular momentum (often referred to as spin), and we can obtain a set of eigenvectors
with values of an index m that can have values +1, 0, and −1. By diagonalizing the matrix
K3, we ﬁnd its eigenvectors to be
k1 =


−1
√
2
−i/
√
2
0

,
k0 =


0
0
1

,
k−1 =


1
√
2
−i/
√
2
0

.
(16.87)
While in principle the signs of these eigenvectors are arbitrary, they have been chosen here
to agree with the Condon-Shortley phase convention.
Vector Coupling
The vector spherical harmonics are now deﬁned as the quantities that result from the cou-
pling of ordinary spherical harmonics and the vectors em to form states of deﬁnite J (the
resultant of the orbital angular momentum of the spherical harmonic and the one unit pos-
sessed by the em). It is customary to label the vector spherical harmonics to show both
the L value from the ordinary (scalar) harmonic and the M value (the eigenvalue of Jz).
Thus, the vector spherical harmonic will have three indices: J, L, and M. From the general
formula for angular-momentum coupling, Eq. (16.43), we have
YJ LM(θ,ϕ) =
X
mm′
C(L,1, J|mm′M)Y m
L (θ,ϕ)ˆem′.
(16.88)
Remember that M is MJ , not the m value of Y m
L , and that ˆem′ are the angular momentum
eigenfunctions given in Eq. (16.87).
Because Eq. (16.88) couples an angular momentum L with one of magnitude k = 1, the
L values in a vector spherical harmonic of given J are restricted to J + 1, J, and J −1,
a condition enforced by the values of the Clebsch-Gordan coefﬁcients. Moreover, because
the Clebsch-Gordan coefﬁcients describe a unitary transformation, the obvious orthogo-
nality of the states in the m,m′ basis (Y m
l ˆem′) will cause the vector spherical harmonics
also to be orthonormal:
Z
YJ LM(θ,ϕ) · YJ ′L′M′(θ,ϕ)d = δJ J ′δLL′δM M′.
(16.89)
In addition, we can invert Eq. (16.89) using Eq. (16.45), reaching
Y m
L (θ,ϕ)ˆem′ =
X
J M
C(L,1, J|mm′M)YJ LM.
(16.90)
The manipulation of expressions involving the vector spherical harmonics depends
crucially on a few identities, of which perhaps the most important is the formula
ˆrY M
L (θ,ϕ) = −
 L + 1
2L + 1
1/2
YL,L+1,M +

L
2L + 1
1/2
YL,L−1,M.
(16.91)

16.4 Vector Spherical Harmonics
811
To establish this formula, and at the same time to make its meaning more obvious, we start
by noting that ˆr has a form that depends on the angular coordinates; speciﬁcally, it is
ˆr =
r
sinθ cosϕˆex + sinθ sinϕˆey + cosθ ˆez
.
For our present purposes, it is more convenient to rearrange this to the form
ˆr = sinθ
eiϕˆe−1 −e−iϕˆe+1
√
2

+ cosθ ˆe0.
(16.92)
It is now clear that in order to prove Eq. (16.91) we must show that each ˆem has the same
coefﬁcient on both sides of the equation. Taking ﬁrst the coefﬁcient of ˆe0, the left-hand
side of Eq. (16.91) yields, after use of Eq. (16.92),
cosθY M
L (θ,ϕ) =
(l −m + 1)(l + m + 1)
(2l + 1)(2l + 3)
1/2
Y m
l+1
+
 (l −m)(l + m)
(2l −1)(2l + 1)
1/2
Y m
l−1,
(16.93)
a result previously exhibited as Eq. (15.150). The ˆe0 terms from the right-hand side of
Eq. (16.91) consist of
−
 L + 1
2L + 1
1/2
C(L + 1,1, L|M,0, M)Y M
L+1ˆe0
+

L
2L + 1
1/2
C(L −1,1, L|M,0, M)Y M
L−1ˆe0.
The Clebsch-Gordan coefﬁcients appearing here have the values
C(L + 1,1, L|M,0, M) = −
(L + M + 1)(L −M + 1)
(L + 1)(2L + 3)
1/2
,
C(L −1,1, L|M,0, M) =
 L2 −M2
L(2L −1)
1/2
.
These data permit conﬁrmation of the e0 terms of Eq. (16.91). The terms in e+1 and e−1
can also be shown consistent; the formulas needed for that purpose are Eqs. (15.151) and
(15.152).
Another useful formula, which can be obtained by using Eq. (16.91) to simplify the
radial component when the gradient operator is applied to the form f (r)Y M
L (θ,ϕ), is
∇
h
f (r)Y M
K (θ,ϕ)
i
= −
 L + 1
2L + 1
1/2  ∂
∂r −L
r

f (r)YL,L+1,M(θ,ϕ)
+

L
2L + 1
1/2  ∂
∂r + L + 1
r

f (r)YL,L−1,M(θ,ϕ).
(16.94)

812
Chapter 16 Angular Momentum
Under coordinate inversion the vector spherical harmonics transform as
YL,L+1,M(θ′,ϕ′) = (−1)L+1YL,L+1,M(θ,ϕ),
YL,L−1,M(θ′,ϕ′) = (−1)L+1YL,L−1,M(θ,ϕ),
(16.95)
YLLM(θ′,ϕ′) = (−1)LYLLM(θ,ϕ),
where
θ′ = π −θ
ϕ′ = π + ϕ.
Starting from Eqs. (16.91) and (16.94), a number of formulas can be derived for the
divergence and curl of vector spherical harmonics. These formulas include the following:
∇·
h
f (r)YL,L+1,M(θ,ϕ)
i
= −
 L + 1
2L + 1
1/2 d f (r)
dr
+ L + 2
r
f (r)

Y M
L (θ,ϕ),
(16.96)
∇·
h
f (r)YL,L−1,M(θ,ϕ)
i
=

L
2L + 1
1/2 d f (r)
dr
−L −1
r
f (r)

Y M
L (θ,ϕ), (16.97)
∇·
h
f (r)YLLM(θ,ϕ)
i
= 0,
(16.98)
∇×
h
f (r)YL,L+1,M(θ,ϕ)
i
= i

L
2L + 1
1/2 d f (r)
dr
+ L + 2
r
f (r)

YLLM,
(16.99)
∇×
h
f (r)YLLM(θ,ϕ
i
= i

L
2L + 1
1/2 d f (r)
dr
−L
r f (r)

YL,L+1,M(θ,ϕ),
+ i
 L + 1
2L + 1
1/2 d f (r)
dr
+ L + 1
r
f (r)

YL,L−1,M,
(16.100)
∇×
h
f (r)YL,L−1,M(θ,ϕ)
i
= i
 L + 1
2L + 1
1/2 d f (r)
dr
−L −1
r
f (r)

YLLM(θ,ϕ).
(16.101)
For a complete derivation of Eqs. (16.96) to (16.101) we refer to the literature.3 These
relations play an important role in the partial wave expansion of classical and quantum
electrodynamics.
The deﬁnitions of the vector spherical harmonics given here are dictated by convenience,
primarily in quantum mechanical calculations, in which the angular momentum is a sig-
niﬁcant parameter. Further examples of the usefulness and power of the vector spherical
harmonics will be found in Blatt and Weisskopf, in Morse and Feshbach, and in Jackson
(all in Additional Readings).
In closing, we note that
3E. H. Hill, Theory of vector spherical harmonics, Am. J. Phys. 22: 211 (1954). Note that Hill assigns phases in accordance with
the Condon-Shortley phase convention. In Hill’s notation, XLM = YLLM, VLM = YL,L+1,M, WLM = YL,L−1,M.

16.4 Vector Spherical Harmonics
813
•
Vector spherical harmonics are developed from coupling L units of orbital angular
momentum and one unit of spin angular momentum.
•
An extension, coupling L units of orbital angular momentum and two units of spin
angular momentum to form tensor spherical harmonics, is presented by Mathews.4
•
The major application of tensor spherical harmonics is in the investigation of gravita-
tional radiation.
Exercises
16.4.1
Construct the l = 0,m = 0 and l = 1,m = 0 vector spherical harmonics.
ANS.
Y010 = −ˆr(4π)−1/2
Y000 = 0
Y120 = −ˆr(2π)−1/2 cosθ −ˆθ(8π)−1/2 sinθ
Y110 = ˆϕi(3/8π)1/2 sinθ
Y100 = ˆr(4π)−1/2 cosθ −ˆθ(4π)−1/2 sinθ.
16.4.2
Verify that the parity of YLL+1M is (−1)L+1, that of YLLM is (−1)L, and that of
YLL−1M is (−1)L+1. What happened to the M-dependence of the parity?
Hint. ˆr and ˆϕ have odd parity; ˆθ has even parity (compare with Exercise 3.10.25).
16.4.3
Verify the orthonormality of the vector spherical harmonics YJ LMJ .
16.4.4
Jackson’s Classical Electrodynamics (see Additional Readings) deﬁnes YLLM by the
equation
YLLM(θ,ϕ) =
1
√L(L + 1)LY M
L (θ,ϕ),
in which the angular momentum operator L is given by
L = −i(r × ∇).
Show that this deﬁnition agrees with Eq. (16.88).
16.4.5
Show that
L
X
M=−L
Y∗
LLM(θ,ϕ) · YLLM(θ,ϕ) = 2L + 1
4π
.
Hint. One way is to use Exercise 16.4.4 with L expanded in Cartesian coordinates and
to apply raising and lowering operators.
4J. Mathews, Gravitational multipole radiation, J. Soc. Ind. Appl. Math. 10: 768 (1963).

814
Chapter 16 Angular Momentum
16.4.6
Show that
Z
YLLM · (ˆr × YLLM)d = 0.
The integrand represents an interference term in electromagnetic radiation that contributes
to angular distributions but not to total intensity.
Additional Readings
Biedenharn, L. C., and J. D. Louck, Angular Momentum in Quantum Physics: Theory and Application. Ency-
clopedia of Mathematics and Its Applications, vol. 8. Reading, MA: Addison-Wesley (1981). An extremely
detailed account, containing much material not easily found elsewhere.
Blatt, J. M., and V. Weisskopf, Theoretical Nuclear Physics. New York: Wiley (1952). Treats vector spherical
harmonics.
Brink, D. M., and G. R. Satchler, Angular Momentum. New York: Oxford (1993). Contains a good presentation
of graphical methods for the manipulation of 3j, 6j, and even 9j symbols. The 6j and 9j symbols are useful
in dealing with the coupling of more than two angular momenta.
Condon, E. U., and G. H. Shortley, Theory of Atomic Spectra. Cambridge: Cambridge University Press (1935).
This is the original and standard work on spin-orbit coupling in atomic states. It is extremely thorough and not
for the beginner.
Edmonds, A. R., Angular Momentum in Quantum Mechanics. Princeton, NJ: Princeton University Press (1957).
A good introductory text, with detailed discussion of the symmetries of 3 j, 6 j, and 9 j symbols.
Jackson, J. D., Classical Electrodynamics, 3rd ed. New York: Wiley (1999). Applies vector spherical harmonics
to multipole radiation and related problems.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics, 2 vols. New York: McGraw-Hill (1953).
Includes material on vector spherical harmonics.
Rose, M. E., Elementary Theory of Angular Momentum. New York: Wiley (1957), reprinted, Dover (1995). As
part of the development of the quantum theory of angular momentum, Rose includes a detailed and readable
account of the rotation group.
Wigner, E. P., Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra (translated by
J. J. Grifﬁn). New York: Academic Press (1959). This is the classic reference on group theory for the physi-
cist. The rotation group is treated in considerable detail. There is a wealth of applications to atomic physics.
The translation from the original German edition included a conversion from a left-handed to a right-handed
coordinate system. This conversion introduced a few errors that can be resolved by comparison with the
untranslated book.

CHAPTER 17
GROUP THEORY
Disciplined judgment, about what is neat
and symmetrical and elegant, has time and
time again proved an excellent guide to
how nature works.
MURRAY GELL-MANN
17.1
INTRODUCTION TO GROUP THEORY
Symmetry has long been important in the study of physical systems. Connections between
the geometric symmetry of crystalline systems and their x-ray diffraction spectra were
found to be crucial to the interpretation of the diffraction patterns and the extraction
therefrom of information locating the atoms in the crystal. The geometric symmetries of
molecules determine which vibrational modes will be active in absorbing or emitting radi-
ation; the symmetries of periodic systems have implications as to their energy bands, their
ability to conduct electricity, and even their superconductivity. The invariance of physi-
cal laws with respect to position or orientation (i.e., the symmetry of space) gives rise to
conservation laws for linear and angular momentum. Sometimes the implications of sym-
metry invariance are far more complicated or sophisticated than might at ﬁrst be supposed;
the invariance of the forces predicted by electromagnetic theory when measurements are
made in observation frames moving uniformly at different speeds (inertial frames) was
an important clue leading Einstein to the discovery of special relativity. With the advent of
quantum mechanics, considerations of angular momentum and spin introduced new sym-
metry concepts into physics. These ideas have since catalyzed the modern development of
particle theory.
Central to all these symmetry notions is the fact that complete sets of symmetry oper-
ations form what in mathematics are known as groups. The elements of a group may be
ﬁnite in number, in which case the group is then termed ﬁnite or discrete, as for example
the symmetry operations shown for the object depicted in Fig. 17.2. But alternatively, the
815
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

816
Chapter 17 Group Theory
symmetry operations may be inﬁnite in number and described by continuously variable
parameter(s); such groups are termed continuous. An example of a continuous group is
the set of possible rotational displacements of a circular object about its axis (in which
case the parameter is the rotation angle).
Deﬁnition of a Group
A group G is deﬁned as a set of objects or operations (e.g., rotations or other transfor-
mations), called the elements of G, that may be combined, by a procedure to be called
multiplication and denoted by *, to form a well-deﬁned product, subject to the following
four conditions:
1.
If a and b are any two elements of G, then the product a ∗b is also an element of
G; more formally, a ∗b associates an element of G with the ordered pair (a,b) of
elements of G. In other words, G is closed under multiplication of its own elements.
2.
This multiplication is associative: (a ∗b) ∗c = a ∗(b ∗c).
3.
There is a unique identity element1 I in G, such that I ∗a = a ∗I = a for every
element a in G.
4.
Each element a of G has an inverse, denoted a−1, such that a ∗a−1 = a−1 ∗a = I.
The above simple rules have a number of direct consequences, including the following:
•
It can be shown that the inverse of any element a is unique: If a−1 and ˆa−1 are both
inverses of a, then ˆa−1 = ˆa−1 ∗(a ∗a−1) = (ˆa−1 ∗a) ∗a−1 = a−1.
•
The products g ∗a, where a is ﬁxed and g ranges over all elements of the group, consist
(in some order) of all the elements of the group. If g and g′ produce the same element,
then g∗a = g′ ∗a. Multiplying on the right by a−1, we get (g∗a)∗a−1 = (g′ ∗a)∗a−1,
which reduces to g = g′.
Here are some useful conventions and further deﬁnitions:
•
The * for multiplication is tedious to write; when no ambiguity will result it is custom-
ary to drop it, and instead of a ∗b we write ab.
•
When a and b are operations, and ab is to be applied to an object appearing to their
right, b is deemed to act ﬁrst, with a then applied to the result of operation with b.
•
If a discrete group possesses n elements (including I), its order is n; a continuous
group of order n has elements that are deﬁned by n parameters.
•
If ab = ba for all a, b of G, the multiplication is commutative, and the group is called
abelian.
•
If a group possesses an element a such that the sequence I, a, a2(= aa), a3, ···
includes all elements of the group, it is termed cyclic. If a group is cyclic, it must
also be abelian. However, not all abelian groups are cyclic.
1Following E. Wigner, the identity element of a group is often labeled E, from the German Einheit, that is, unit; some other
authors just write 1.

17.1 Introduction to Group Theory
817
•
Two groups {I,a,b,···} and {I ′,a′,b′,···} are isomorphic if their elements can be
put into one-to-one correspondence such that for all a and b, ab = c ⇐⇒a′b′ = c′. If
the correspondence is many-to-one, the groups are homomorphic.
•
If a subset G′ of G is closed under the multiplication deﬁned for G, it is also a group
and called a subgroup of G. The identity I of G always forms a subgroup of G.
Examples of Groups
Example 17.1.1
D3, SYMMETRY OF AN EQUILATERAL TRIANGLE
The symmetry operations of an equilateral triangle form a ﬁnite group with six elements;
our triangle can be placed either side up, and with any vertex in the top position. The six
operations that convert the initial orientation into symmetry equivalents are I (the identity
operation that makes no orientation change), C3, an operation which rotates the triangle
counterclockwise by 1/3 of a revolution, C2
3 (two successive C3 operations), C2, rotation
by 1/2 a revolution (for this group the rotation is about an axis in the plane of the trian-
gle), and C′
2 and C′′
2 (180◦rotations about additional axes in the plane of the triangle).
Figure 17.1 is a schematic diagram indicating these symmetry operations, and Fig. 17.2
shows their result, with the vertices of the triangle numbered to show the effect of each
operation. The multiplication table for the group is shown in Table 17.1, where the prod-
uct ab (which describes the result of ﬁrst applying operation b, and then operation a) is
(0, 1)
y
C″2
C′2
x
C3
1
C2
2
3
)
)
(
(
2
3
−
, −2
1
2
3 , −2
1
FIGURE 17.1
Diagram identifying symmetry operations of an equilateral triangle. I is
the identity operation (the diagram as shown here). C3 and C2
3 are counterclockwise
rotations, by, respectively, 120◦and 240◦; C2, C′
2, C′′
2 are operations that turn the triangle
over by rotation about the indicated axes.

818
Chapter 17 Group Theory
1
1
1
1
1
3
3
3
I
C3
C2
C2
3
C′2
C″2
3
3
1
3
2
2
2
2
2
2
FIGURE 17.2
Result of applying the symmetry operations identiﬁed in Fig. 17.1 to
an equilateral triangle. One side of the triangle is shaded to make it obvious when that
side is up.
Table 17.1
Multiplication Table for Group D3
I
C3
C2
3
C2
C′
2
C′′
2
I
I
C3
C2
3
C2
C′
2
C′′
2
C3
C3
C2
3
I
C′′
2
C2
C′
2
C2
3
C2
3
I
C3
C′
2
C′′
2
C2
C2
C2
C′
2
C′′
2
I
C3
C2
3
C′
2
C′
2
C′′
2
C2
C2
3
I
C3
C′′
2
C′′
2
C2
C′
2
C3
C2
3
I
Operations are pictured in Fig. 17.2. The table entry for row a and column b
is the product element ab. For example, C2C3 = C′
2.
the group element listed in row a and column b of the table. This group has several names,
of which one is D3 (“D” for dihedral, referring to a 180◦rotation axis lying in a plane
perpendicular to the main symmetry axis). From the multiplication table or by examination
of the symmetry operations themselves, we can see that the inverse of I is I, the inverse
of C3 is C2
3 (so the inverse of C2
3 is C3), and each C2 is its own inverse. This group is not
abelian; C3C2 ̸= C2C3 (C3C2 = C′′
2 , while C2C3 = C′
2).
■
Example 17.1.2
ROTATION OF A CIRCULAR DISK
The rotations of a circular disk about its symmetry axis form a continuous group of order 1
whose elements consist of rotations through angles ϕ. The group elements R(ϕ) are inﬁnite
in number, with ϕ any angle in the range (0,2π). The identity element is clearly R(0);
the inverse of R(ϕ) is R(2π −ϕ). The multiplication rule for this group is R(ϕ)R(θ) =
R(ϕ +θ) (reduced to a value between 0 and 2π), so R(ϕ)R(θ) = R(θ)R(ϕ), and this group
is abelian. It will be useful to ﬁgure out what happens to a point on the disk that before the
rotation was at (x, y). The rotation is by an angle ϕ about the z axis, clockwise, looking
down from positive z, a choice made to be consistent with the counterclockwise rotations
of the coordinate axes used elsewhere in this book. The ﬁnal location of this point, (x′, y′),

17.1 Introduction to Group Theory
819
is given by the matrix equation
x′
y′

=
 
cosϕ
sinϕ
−sinϕ
cosϕ
!x
y

.
(17.1)
■
Example 17.1.3
AN ABSTRACT GROUP
Groups do not need to represent geometric operations. Consider a set of four quantities
(elements) I, A, B, C, with our knowledge about them only that when any two are multi-
plied, the result is an element of the set. The multiplication table of this four-element set
is shown in Table 17.2. These elements form a group, because each has an inverse (itself),
there is an identity element (I), and the set is closed under multiplication.
Table 17.2
Multiplication Table
for the Vierergruppe
I
A
B
C
I
I
A
B
C
A
A
I
C
B
B
B
C
I
A
C
C
B
A
I
The table entry for row a and column b is the
product element ab.
■
Example 17.1.4
ISOMORPHISM AND HOMOMORPHISM: C4 GROUP
The symmetry operations of a square that cannot be turned over form a four-membered
group sometimes called C4 whose elements can be named I, C4 (90◦rotation), C2 (180◦
rotation), C′
4 (270◦rotation). The four complex numbers 1, i, −1, −i also form a group
when the group operation is ordinary multiplication. These groups are isomorphic, and can
be put into correspondence in two different ways:
I ↔1, C4 ↔i, C2 ↔−1, C′
4 ↔−i
or
I ↔1, C4 ↔−i, C2 ↔−1, C′
4 ↔i.
This group is also cyclic, as C2
4 = C2, C3
4 = C′
4, or equivalently i2 = −1, i3 = −i.
The group C4 has a two-to-one correspondence with the ordinary multiplicative group
containing only 1 and −1: I and C2 ↔1, while C4 and C′
4 ↔−1. This is a homomor-
phism. A more trivial homomorphism, possessed by all groups, is obtained when every
element is assigned to correspond to the identity.
■

820
Chapter 17 Group Theory
Exercises
17.1.1
The Vierergruppe (German: four-membered group) is a group different from the C4
group introduced in Example 17.1.4. The Vierergruppe has the multiplication table
shown in Table 17.2. Determine whether this group is cyclic and whether it is abelian.
17.1.2
(a)
Show that the permutations of n distinct objects satisfy the group postulates.
(b)
Construct the multiplication table for the permutations of three objects, giving
each permutation a name of some sort. (Suggestion: Use I for the permutation
that leaves the order unchanged.)
(c)
Show that this permutation group (named S3) is isomorphic with D3 and identify
corresponding operations. Is your identiﬁcation unique?
17.1.3
Rearrangement theorem: Given a group of distinct elements (I,a,b,...,n), show
that the set of products (aI,a2,ab,ac,...,an) reproduces all the group elements in a
new order.
17.1.4
A group G has a subgroup H with elements hi. Let x be a ﬁxed element of the original
group G and not a member of H. The transform
xhi x−1,
i = 1,2,...
generates a conjugate subgroup x Hx−1. Show that this conjugate subgroup satisﬁes
each of the four group postulates and therefore is a group.
17.1.5
(a)
A particular group is abelian. A second group is created by replacing gi by g−1
i
for each element in the original group. Show that the two groups are isomorphic.
Note. This means showing that if ab = c, then a−1b−1 = c−1.
(b)
Continuing part (a), show that the second group is also abelian.
17.1.6
Consider a cubic crystal consisting of identical atoms at r = (la,ma,na), with l,m,
and n taking on all integral values.
(a)
Show that each Cartesian axis is a fourfold symmetry axis.
(b)
The cubic point group will consist of all operations (rotations, reﬂections, inver-
sion) that leave the simple cubic crystal invariant and that do not move the atom at
l = m = n = 0. From a consideration of the permutation of the positive and nega-
tive coordinate axes, predict how many elements this cubic group will contain.
17.1.7
A plane is covered with regular hexagons, as shown in Fig. 17.3.
(a)
Determine the rotational symmetry of an axis perpendicular to the plane through
the common vertex of three hexagons (A). That is, if the axis has n-fold symmetry,
show (with careful explanation) what n is.
(b)
Repeat part (a) for an axis perpendicular to the plane through the geometric center
of one hexagon (B).
(c)
Find all the different kinds of axes within the plane of hexagons about which a
180◦rotation is a symmetry element (this corresponds to turning the plane over by
rotation about that axis).

17.2 Representation of Groups
821
A
B
FIGURE 17.3
Plane covered by hexagons.
17.2
REPRESENTATION OF GROUPS
All discrete groups and the continuous groups we study here can be represented by square
matrices. By this we mean that to each element of the group we can associate a matrix,
and that if U(a) is the matrix associated with a and U(b) the matrix associated with b,
then the matrix product U(a)U(b) will be the matrix associated with ab. In other words,
the matrices have the same multiplication table as the group. We call these matrices U
because they can be chosen to be unitary. It is not necessary that U have a dimension equal
to the order of the group.
Sometimes we need to identify representations with a label. For speciﬁc representations
we can use their generally adopted names; when we need a generic label, we will use K or
K ′. Thus, we can refer to representation K, consisting of matrices UK(a).
Example 17.2.1
A UNITARY REPRESENTATION
Here is a unitary representation of the group D3 illustrated in Fig. 17.2:
U(I) =
 
1
0
0
1
!
,
U(C3) =
 
−1
2
1
2
√
3
−1
2
√
3
−1
2
!
,
U(C2
3) =
 
−1
2
−1
2
√
3
1
2
√
3
−1
2
!
,
U(C2) =
 
1
0
0
−1
!
,
U(C′
2) =
 
−1
2
1
2
√
3
1
2
√
3
1
2
!
,
U(C′′
2) =
 
−1
2
−1
2
√
3
−1
2
√
3
1
2
!
.
(17.2)
Several features of this representation are apparent:
•
The unit operation is represented by a unit matrix.
•
The inverse of an operation is represented by the inverse of its matrix.

822
Chapter 17 Group Theory
We can check that the U form a representation: From the multiplication table, we have
C2 C3 = C′
2. Now we evaluate
U(C2)U(C3) =
 
1
0
0
−1
! 
−1
2
1
2
√
3
−1
2
√
3
−1
2
!
=
 
−1
2
1
2
√
3
1
2
√
3
1
2
!
,
which is indeed U(C′
2). The reader can easily verify that other products of group elements
correspond to the products of the representation matrices. Matrix multiplication is in gen-
eral not commutative, and gives results that are consistent with the lack of commutativity
of the group operations.
The 2×2 representation shown above is faithful, meaning that each group element cor-
responds to a different matrix. In other words, our 2 × 2 representation is isomorphic with
the original group. Not all representations are faithful; consider the relatively trivial repre-
sentation in which every group element is represented by the 1×1 matrix (1). Every group
will possess this representation. A somewhat less trivial, but still unfaithful, representation
of D3 is one in which
U(I) = U(C3) = U(C2
3) = 1,
U(C2) = U(C′
2) = U(C′′
2) = −1.
(17.3)
This representation distinguishes elements according to whether they involve turning the
triangle over. Not all groups will possess this 1 × 1 representation; if we had not permit-
ted the triangle to be turned over, this representation would have been excluded. These
unfaithful representations are homomorphic with the original group.
■
An important feature of a representation of a group G is that its essential fea-
tures are invariant if we make the same unitary transformation on the matrices repre-
senting all the group elements. To see this, consider what happens when we replace
each U(g) by VU(g)V−1. Then the product U(g)U(g′), which is some U(g′′), becomes
(VU(g)V−1)(VU(g′)V−1) = VU(g)U(g′)V−1 = VU(g′′)V−1, so the transformed matrices
still form a representation of G. Representations that can be transformed into each other
by application of a unitary transformation are termed equivalent.
The possibility of unitary transformation also enables us to consider whether a repre-
sentation of G is reducible. An irreducible representation of G is deﬁned as one that
cannot be broken into a direct sum of representations of smaller dimension by application
of the same unitary transformation to all members of the representation. What we mean by
a direct sum of representations is that each matrix will be block diagonal (all with the same
sequence of blocks). Since different blocks will not mix under matrix multiplication, cor-
responding blocks of the representation members will themselves deﬁne representations
(see Fig. 17.4). If a representation named K is a direct sum of smaller representations K1
and K2, that fact can be indicated by the notation
K = K1 ⊕K2.
It is not always obvious whether a representation is reducible. We will shortly encounter
theorems that provide (for discrete groups) ways of determining what irreducible repre-
sentations are present in a representation that may be reducible. Moreover, if a group is
abelian, then the fact that all its elements commute means that the matrices represent-
ing them can all be diagonalized simultaneously. From that fact we can conclude that all
irreducible representations of abelian groups are 1 × 1.

17.2 Representation of Groups
823
UK(a)=
UK′(a)
UK″(a)
UK′″(a)
FIGURE 17.4
A member of a reducible representation in direct-sum form. All members
will have the same block structure, so individual blocks deﬁne representations of smaller
dimension.
It is important to understand that reducibility implies the existence of a unitary trans-
formation that brings all members of a representation to the same block-diagonal form; a
reducible representation may not exhibit the block-diagonal form if it has not been sub-
jected to a suitable unitary transformation. Here is an example illustrating that point.
Example 17.2.2
A REDUCIBLE REPRESENTATION
Here is a reducible representation for our equilateral triangle:
U(I) =


1
0
0
0
1
0
0
0
1

,
U(C3) =


0
1
0
0
0
1
1
0
0

,
U(C2
3) =


0
0
1
1
0
0
0
1
0

,
U(C2) =


0
0
1
0
1
0
1
0
0

,
U(C′
2) =


1
0
0
0
0
1
0
1
0

,
U(C′′
2) =


0
1
0
1
0
0
0
0
1

.
(17.4)
Note that some of these matrices are not in any direct-sum form. To show that the repre-
sentation of Eq. (17.4) is reducible, we transform all the U to U′ = VUV−1, using
V =


1/
√
3
1/
√
3
1/
√
3
1/
√
6
−√2/3
1/
√
6
1/
√
2
0
−1/
√
2

,

824
Chapter 17 Group Theory
which brings us to
U′(I) =


1
0
0
0
1
0
0
0
1

,
U′(C3) =


1
0
0
0
−1
2
1
2
√
3
0
−1
2
√
3
−1
2

,
U′(C2
3) =


1
0
0
0
−1
2
−1
2
√
3
0
1
2
√
3
−1
2

,
U′(C2) =


1
0
0
0
1
0
0
0
−1

,
U′(C′
2) =


1
0
0
0
−1
2
1
2
√
3
0
1
2
√
3
1
2

,
U′(C′′
2) =


1
0
0
0
−1
2
−1
2
√
3
0
−1
2
√
3
1
2

.
(17.5)
All the matrices of this representation are block diagonal, and are direct sums that consist
of an upper 1 × 1 block that is the trivial representation, all of whose elements are (1),
and a lower 2 × 2 block that is exactly the 2 × 2 representation illustrated in Eq. (17.2).
There exists no unitary transformation that will simultaneously reduce the 2 × 2 blocks of
all members of the representation to direct sums of 1 × 1 blocks, so we have reduced the
representation of Eq. (17.4) to its irreducible components.2
■
Example 17.2.3
REPRESENTATIONS OF A CONTINUOUS GROUP
Example 17.1.2 presented a continuous group of order 1 whose elements are rotations R(ϕ)
about the symmetry axis of a circular disk. These rotations were taken to be deﬁned by
the matrix equation presented as Eq. (17.1). The 2 × 2 matrix in that equation can also be
viewed as a representation of R(ϕ):
U(ϕ) =
 cosϕ
sinϕ
−sinϕ
cosϕ
!
.
Because this group is abelian (two successive rotations yield the same result if applied
in either order), we know that this representation is reducible. If we apply the unitary
transformation
U′(ϕ) = VU(ϕ)V−1,
with
V =


1/
√
2
−i/
√
2
1/
√
2
i/
√
2

,
the result is
U′(ϕ) =


cosϕ + i sinϕ
0
0
cosϕ −i sinϕ

=
 eiϕ
0
0
e−iϕ
!
.
(17.6)
2We know this because some of these 2 × 2 matrices do not commute with each other and therefore cannot be diagonalized
simultaneously.

17.2 Representation of Groups
825
Equation (17.6) applies to every element of our rotation group after transforming with V,
and we see that every rotation now has a diagonal representation. In other words, U(ϕ) has
been transformed into a direct sum of two one-dimensional (1-D) representations, U′ =
U1 ⊕U(−1), with U1(ϕ) = eiϕ and U(−1)(ϕ) = e−iϕ. In fact, these are only two of an
inﬁnite number of irreducible representations, all of dimension 1:
Un(ϕ) = einϕ,
where n can have any positive or negative integer value, including zero. The reason n is
limited to integer values is to assure that U(2π) = U(0). Note that only the n values ±1
lead to faithful representations.
■
Exercises
17.2.1
For any representation K of a group, and for any group element a, show that
h
UK(a)
i −1
= UK (a−1).
17.2.2
Show that these four matrices form a representation of the Vierergruppe, whose multi-
plication table is in Table 17.2.
I =
1
0
0
1

,
A =
−1
0
0 −1

,
B =
0
1
1
0

,
C =

0 −1
−1
0

.
17.2.3
Show that the matrices 1,A,B, and C of Exercise 17.2.2 are reducible. Reduce them.
Note. This means transforming B and C to diagonal form (by the same unitary transfor-
mation).
17.2.4
(a)
Once you have a matrix representation of any group, a 1-D representation can be
obtained by taking the determinants of the matrices. Show that the multiplicative
relations are preserved in this determinant representation.
(b)
Use determinants to obtain a 1-D representation of D3 from the 2 × 2 representa-
tion in Eq. (17.2).
17.2.5
Show that the cyclic group of n objects, Cn, may be represented by rm, m =
0,1,2,...,n −1. Here r is a generator given by
r = exp(2πis/n).
The parameter s takes on the values s = 1,2,3,...,n, with each value of s yielding a
different 1-D (irreducible) representation of Cn.
17.2.6
Develop the irreducible 2 × 2 matrix representation of the group of rotations (including
those that turn it over) that transform a square into itself. Give the group multiplication
table.
Note. This group has the name D4 (see Fig. 17.5).

826
Chapter 17 Group Theory
C4
C2
C2
z
x
C2′
C2′
y
FIGURE 17.5
D4 symmetry group.
17.3
SYMMETRY AND PHYSICS
Representations of groups provide a key connection between group theory and the sym-
metry properties of physical systems. Our discussion will be directed mainly at quantum
systems, but much of it will also apply to systems that can be described using classical
physics.
Consider a quantum system whose Hamiltonian H possesses certain geometric sym-
metries. If we write H = T + V , the symmetries will be those of the potential energy
V , since the kinetic energy operator T is invariant with respect to rotations and displace-
ments of the coordinate axes. A concrete example that illustrates the concept would be the
determination of the wave function of an electron in the presence of nuclei in some ﬁxed
conﬁguration possessing symmetry, such as the equilibrium locations of the nuclei in a
symmetric molecule.
The symmetry of H corresponds to a requirement that H be invariant with respect to
the application of any element of its symmetry group. Letting R denote such a symmetry
element, the invariance of H means that if ϕ is a solution of the Schrödinger equation with
energy E, then Rϕ must also be a solution with the same energy eigenvalue:
H(Rϕ) = E(Rϕ).
By successively applying the elements of our symmetry group to ϕ, we can generate a
set of eigenfunctions, all with the same eigenvalue. If ϕ happened to have the full sym-
metry of H, this set would contain only one member and the situation would be easy to
understand. But if ϕ had less symmetry,3 our eigenfunction set would have more than one
member, with its maximum possible size being the number of elements in our symmetry
group. When the eigenfunction set has more than one member, the eigenfunctions do not
individually have the complete symmetry of the Hamiltonian, but they form a closed set
that permits the partial symmetry to be expressed in all symmetry-equivalent ways. For
example, the hydrogenic eigenfunctions known as p states form a three-membered set;
3This is possible; an example is a hydrogen-atom p state.

17.3 Symmetry and Physics
827
none has the full spherical symmetry of the hydrogen atom Hamiltonian, but linear com-
binations of the three p states can describe a p orbital at an arbitrary orientation (obvious
because a vector in an arbitrary direction can be written as a linear combination of vectors
in the coordinate directions).
So let’s assume that, starting from some chosen ϕ, we have found a full set of symmetry-
related eigenfunctions, have eliminated from them any linear dependence, and have formed
an orthonormal eigenfunction set, denoted ϕi, i = 1... N.
Because of the way in which the ϕi were constructed, they will transform linearly among
themselves if we apply to them any operation R from our symmetry group, so we may write
Rϕi =
X
j
Uji(R)ϕ j.
(17.7)
If we apply two symmetry operations (R followed by S), the transformation rule for the
result will be
SRϕi =
X
jk
Ukj(S) Uji(R)ϕk.
(17.8)
Equations (17.7) and (17.8) show that the transformation for the group element SR is the
matrix product of those for S and R, so the matrices U(S) and U(R) have properties that
make them members of a representation of our symmetry group. What is new here is that
we have identiﬁed U as a representation associated with the basis {ϕi}.
At this point we do not know whether the representation formed from our {ϕi} basis is
reducible; its reducibility depends on the quantum system under study and the particular
choice made for the initial function ϕ. If our U are reducible, let’s assume we now apply
a transformation that will convert them into the direct-sum form. The transformation to
obtain the direct-sum separation corresponds to a division of the basis into smaller sets of
functions that transform only among themselves. Our overall conclusion from the above
analysis is:
If a Hamiltonian H is fully symmetric under the operations of a symmetry group,
all its eigenfunctions can be classiﬁed into sets, with each set forming a basis for
an irreducible representation of the symmetry group. The members of a symmetry-
related set of eigenfunctions will be degenerate and are referred to as a multi-
plet. Ordinarily different multiplets will correspond to different eigenvalues; any
degeneracy between eigenfunctions of different irreducible representations arises
from sources other than the symmetry under study.
Because the eigenfunctions of a Hamiltonian possessing geometric symmetry can be
identiﬁed with irreducible representations of its symmetry group, it is natural to use
approximate eigenfunctions with similar symmetry restrictions.
Example 17.3.1
AN EVEN HAMILTONIAN
Consider a Hamiltonian H(x), which is even in x, meaning that H(−x) = H(x), but has
no other symmetry. Letting σ stand for the reﬂection operator x →−x (σ is the usual

828
Chapter 17 Group Theory
notation for a reﬂection operation), our symmetry group, called Cs, consists only of the
two operations I and σ, and its multiplication table is
I I = σ σ = I,
Iσ = σ I = σ.
This group is abelian, and has two irreducible representations of dimension 1: one (A1) that
is completely symmetric, U(I) = U(σ) = 1, and one (A2) with sign alternation, U(I) = 1,
U(σ) = −1. The eigenfunctions of H will therefore be even or odd, and there is no inherent
symmetry requirement that even and odd states be degenerate with each other.
If we start with a function ϕ(x) that is even, we will have Iϕ = σϕ = ϕ, so our basis
will consist only of ϕ, and U(I) = U(σ) = 1, indicating that the representation constructed
using this basis will be the fully symmetric A1.
On the other hand, if our starting function ϕ(x) was odd, then Iϕ = ϕ but σϕ = −ϕ;
again our basis will consist only of ϕ(x), but now the representation constructed from it
will consist of U(I) = 1, U(σ) = −1, and will be the alternating-sign representation A2.
But if we start with a function ϕ(x) that is neither even nor odd, then Iϕ(x) = ϕ(x),
but σϕ(x) = ϕ(−x). Our assumption that ϕ(x) is neither even nor odd means that ϕ(x)
and ϕ(−x) are linearly independent, so our basis will consist of two members (and there-
fore be of dimension 2). Since the symmetry group has only A1 and A2 as irreducible
representations, the representation built from our two-membered basis will be reducible,
and will reduce to A1 ⊕A2. The basis will separate into the two members ϕ(x) + ϕ(−x)
(a 1-D A1 basis) and ϕ(x) −ϕ(−x) (an A2 basis).
Given a problem with an even Hamiltonian, one may use the above-identiﬁed symmetry
analysis to seach for solutions that are restricted to have either even or odd symmetry. This
strategy may greatly simplify the process of ﬁnding solutions. The notion can be extended
to problems with different or greater degrees of symmetry.
■
It is important to note that all geometric symmetry groups (other than the trivial group,
which has only the element I) will possess representations other than A1, which means
that they will have bases of less symmetry than the original group. In Example 17.3.1, our
Hamiltonian was even, but could have eigenfunctions that are either even (A1) or odd (A2).
A Hamiltonian with D3 symmetry (which we have already seen has irreducible represen-
tations of dimensions 1 and 2) can have A1 eigenfunctions of the full three-dimensional
(3-D) symmetry or A2 eigenfunctions with alternating-sign symmetry. It can also have sets
of two degenerate eigenfunctions corresponding to the representation in Eq. (17.2), where
(as indicated by the 2 × 2 matrices) the symmetry operations can convert either of the
basis members into linear combinations of both. The irreducibility means that there exists
no single function built from this two-member basis that will remain the same (except for
a possible sign or phase factor) under all the group operations. The existence of an irre-
ducible basis with more than one member is a consequence of the fact that the symmetry
group is not abelian.
Although the elements of a symmetry group may not all commute with each other, they
all commute with a Hamiltonian (or other operator) having the full group symmetry. To
show this, note that for any eigenfunction ψ and any group element R,
Hψ = Eψ −→H(Rψ) = E(Rψ) = R(Eψ) = RHψ −→HR = RH.
The last step follows because the previous steps are valid for all members of a complete
set of eigenfunctions ψ.

17.3 Symmetry and Physics
829
Sometimes, especially for continuous groups, we will know in advance how to construct
bases for irreducible representations. For example, the spherical harmonics of a given l
value form a basis for representation of the 3-D rotation group. From Chapter 16, we know
that these spherical harmonics form a closed set under rotation, but only if the set includes
all m values. This information, together with the orthonormality of the Y m
l , tells us that Y m
l ,
m = −l,...,l is an orthonormal basis of dimension 2l +1 for an irreducible representation
of the 3-D rotation group, which is named SO(3). In contrast to the situation for discrete
groups, continuous groups (even of low order) may possess an inﬁnite number of ﬁnite-
dimensional irreducible representations.
An experienced investigator can often ﬁnd bases for irreducible representations by
inspection or educated insight. However, if simple methods for ﬁnding a basis prove insuf-
ﬁcient, general methods can be used to construct basis functions if the matrices deﬁning
the relevant irreducible representation are available. Details of the process can be found in
the works by Falicov, Hamermesh, and Tinkham (see Additional Readings).
Example 17.3.2
QUANTUM MECHANICS, TRIANGULAR SYMMETRY
Let’s consider a Hamiltonian that has the D3 symmetry of an equilateral triangle that can
be turned over, and our problem is such that its solution can be approximated as a wave
function that is distributed over orbitals centered at the three vertices Ri of the triangle, of
the form ψ(r) = a1ϕ(r1) + a2ϕ(r2) + a3ϕ(r3), where ri is the distance |r −Ri|, and ϕ is
a spherically symmetric orbital. The function
ψ0 = ϕ(r1) + ϕ(r2) + ϕ(r3)
is a basis for the trivial (A1) representation of the D3 group. But because we have three
orbitals, there will be two other linear combinations of them that are linearly independent
of ψ0, and one way to choose them is
ψ1 = 1
√
2

ϕ(r1) −ϕ(r3)

,
ψ2 = 1
√
6

−ϕ(r1) + 2ϕ(r2) −ϕ(r3)

.
Neither of these functions (nor any linear combination of them) has enough symmetry to
be either A1 or A2 basis functions, and they therefore must (together) form a basis for a
2 × 2 irreducible representation of the D3 symmetry group that is called E. Knowing that
this would be the case, we chose these functions in a way that makes them orthogonal and
at a consistent normalization, and they are in fact a basis for the irreducible representation
given in Eq. (17.2).
We can check this by applying group operations to ψ1 and ψ2, verifying that the result
corresponds to the appropriate column of the matrix for the operation. We make one such
check here: Applying C3 to ψ1, we get C3ψ1 = [ϕ(r3)−ϕ(r2)]/
√
2, while the ﬁrst column
of U(C3) in Eq. (17.2) yields
C3ψ1 = −1
2 ψ1 −
√
3
2 ψ2 = −1
2
ϕ(r1) −ϕ(r3)
√
2

−
√
3
2
−ϕ(r1) + 2ϕ(r2) −ϕ(r3)
√
6

.
The reader can verify that these two expressions for C3ψ1 are equal, and can make further
checks if desired.

830
Chapter 17 Group Theory
One might think that because of the triangular symmetry there would be an irreducible
representation of dimension 3. But mathematics is not that simple; all D3 representations
of dimension 3 are reducible!
■
The symmetries required of solutions to Schrödinger equations have implications
beyond their role in causing or explaining degeneracy. The dominant interaction between
an electromagnetic ﬁeld and a molecule can occur only if the molecule has an electric
dipole moment, and the presence of a dipole moment depends on the symmetry of the
electronic wave function. Another context in which symmetry is important is in the evalu-
ation of the expectation values of quantum operators. These expectation values will vanish
unless the integrals that deﬁne them have integrands with a fully symmetric part. In addi-
tion, it is worth mentioning that many quantum calculations are simpliﬁed by limiting them
to contributions that do not vanish by reason of symmetry. All these issues can be framed
in terms of the irreducible representations for which our wave functions are bases.
In the next sections, we develop some key results of group representation theory, ﬁrst
for discrete groups because the analysis is simpler, and then (in less detail) for continuous
groups that have become important in particle theory and relativity.
Exercises
17.3.1
Consider a quantum mechanics problem with D3 symmetry, with the threefold symme-
try axis taken as the z direction, and with orbitals ϕ(r −R j) located at the vertices of
an equilateral triangle. This is the same system geometry as in Example 17.3.2, but in
the present problem ϕ will no longer be chosen to have spherical symmetry.
Given that ϕ(r) = (z/r) f (r) (so ϕ has the symmetry of a p orbital oriented along the
symmetry axis), construct linear combinations of the ϕ that are bases for irreducible
representations of D3, for each basis indicating its representation.
17.4
DISCRETE GROUPS
Classes
It has been found useful to divide the elements of a ﬁnite group G into sets called classes.
Starting from a group element a1, one can apply similarity transformations of the form
ga1g−1, where g can be any member of G. If we let a1 be transformed in this way, using
all the elements g of G, the result will be a set of elements that we can denote a1,...,ak,
where k may or may not be larger than 1. Certainly this set will include a1 itself, as that
result is obtained when g = I and also when g = a1 or g = a−1
1 . The set of elements
obtained in this way is called a class of G, and can be identiﬁed by specifying one of its
members. If we choose a1 = I, we ﬁnd that I is in a class all by itself; often classes will
have larger numbers of members.
A class will have the same members no matter which of its elements is assigned the role
of a1. This is clear, since if ai = ga1g−1 then also a1 = g−1aig, showing that we can get
a1 from any other element of the class, and therefrom all the elements reachable from a1.

17.4 Discrete Groups
831
Example 17.4.1
CLASSES OF THE TRIANGULAR GROUP D3
As observed already in general, one class of D3 will consist solely of I. The class including
C3 contains also C2
3 (the result of C2C3C−1
2 ). Finally, C2, C′
2, and C′′
2 constitute a third
class.
■
Classes are important because:
•
For a given representation (whether or not reducible), all matrices of the same class will
have the same value of their trace—obvious because trace(gag−1) = trace(ag−1g) =
trace(a). In the group theory world, the trace is also known as the character, custom-
arily identiﬁed with the symbol 0.
•
It can be shown that the number of inequivalent irreducible representations of a ﬁnite
group is equal to its number of classes. (For proof and fuller discussion, see Additional
Readings at the end of this chapter.)
It can be shown (again, see Additional Readings) that the set of characters for all elements
and irreducible representations of a ﬁnite group deﬁnes an orthogonal ﬁnite-dimensional
vector space. Writing 0K(g) as the character of group element g in irreducible representa-
tion K, we have the key relations, for a group of order n:
ng
X
K
0K(g)0K(g′) = n δgg′,
X
g
0K(g)0K ′(g) = n δK K ′.
(17.9)
Here ng is the number of elements in the class containing g. These relations enable any
reducible representation to be decomposed into a direct sum of irreducible representations,
and can also be of aid in ﬁnding the characters of irreducible representations if they were
not already known.
Another theorem of great importance in the theory of ﬁnite groups, sometimes called
the dimensionality theorem, is that the sum of the squares of the dimensions nK of the
inequivalent irreducible representations is equal to the order, n, of the group:
X
K
n2
K = n.
(17.10)
This theorem, together with the theorem that the number of irreducible K equals the num-
ber of classes, imposes stringent limits on the number and size of the irreducible represen-
tations of a group. These two requirements are often enough to determine completely the
inventory of irreducible representations.
Since the ﬁnite groups of interest in physics have been well studied, the most frequent
use of these orthogonality relations is to extract from a basis that may be reducible (i.e.,
a basis for a possibly reducible representation) the irreducible bases that may be included
therein. This task is usually carried out with a table of irreducible representations at hand.
Example 17.4.2
ORTHOGONALITY RELATIONS, GROUP D3
The usual scheme for tabulating discrete group characters is called a character table; that
for our triangle group D3 is shown in Table 17.3. The rows of the table are labeled with

832
Chapter 17 Group Theory
Table 17.3
Character Table for Group D3
I
2C3
3C2
A1
1
1
1
A2
1
1
−1
E
2
−1
0
9
3
0
1
Each row corresponds to an irreducible representation, and each
column corresponds to a class. The table entry is the character for
each element of that irreducible representation and class. The row
below the boxed table (labeled 9) is not part of the table but is used
in connection with Example 17.4.4.
the usual names assigned the irreducible representations: The labels A and B (the latter not
used for this group) are reserved for 1 × 1 representations. Representations of dimension
2 are normally assigned a label E, and those of dimension 3 (also not occurring here) are
called T . Each column of the character table is labeled with a typical member of the class,
preceded by a number indicating the number of group elements in the class. This number
is omitted if the class contains only one element.
Because the representation of group element I is a unit matrix, the characters (traces)
in column I directly indicate the dimensions of the representations. We see that A1 is a
1 × 1 representation, so each A1 matrix contains a single number equal to the character
shown, meaning that A1 is the trivial totally symmetric representation. We see that A2 is
also 1 × 1, but the three group elements for which the triangle was turned over are now
represented by −1. Finally, representation E is seen to be 2 × 2, and is the representation
we found long ago in Eq. (17.2).
Checking the ﬁrst orthogonality relation for g = g′ = I, for which ng = 1, we have
1(12 +12 +22) = 6, as expected. For g = I, g′ = C3, we have 1[1(1)+1(1)+2(−1)] = 0,
and for g = g′ = C3, we note that ng = 2 and we have 2[12 + 12 + (−1)2] = 6. The reader
can check other cases of this orthogonality relation.
Moving to the second orthogonality relation, we take K = K ′ = E, ﬁnding 1(22) +
2(−1)2 + 3(02) = 6; the 1, 2, and 3 multiplying individual terms allow for the fact that the
sum is over all elements, not just over classes. Other cases follow similarly.
■
Example 17.4.3
COUNTING IRREDUCIBLE REPRESENTATIONS
We consider two cases, ﬁrst the group C4, which was the subject of Example 17.1.4. This
group is cyclic, with elements I, a, a2, a3; those are all the elements, because a4 = I. As
already indicated, a faithful representation of this group consists of 1, i, −1, −i, with the
group operation being ordinary multiplication. Another realization of C4 is an object that is
symmetric under 90◦rotation about a single axis. This group is abelian, as a paq = aqa p.
Then gag−1 = a for any group elements a and g, so each element is in a class by itself. So
we have four classes, and hence four irreducible representations. We also have, from the

17.4 Discrete Groups
833
dimension theorem,
4
X
K=1
n2
K = 4.
The only way to satisfy this equation is to have four irreducible representations, each of
dimension 1. This result should have been expected, since C4 is abelian. Our irreducible
representations can be built from the four following choices of U(a): 1, i, −1, −i, leading
to the following character table.
I
a
a2
a3
A1
1
1
1
1
A2
1
i
−1
−i
A3
1
−1
1
−1
A4
1
−i
−1
i
Our second case is D3, which has six elements and the three classes identiﬁed in
Example 17.4.1. This means that it has three irreducible representations with dimensions
whose squares add to six. The only set of dimensions satisfying this requirements is 1,
1, and 2.
■
Example 17.4.2 can be generalized to deal with reducible representations; any repre-
sentation whose characters do not match any row of the character table must be reducible
(unless just wrong!). If we were to transform a reducible representation to direct-sum form,
it would then be obvious that its trace will be the sum of the traces of its blocks, and that
property will hold even if we do not know how to make the block-diagonalizing trans-
formation. In group-theory lingo we would say that the characters of a reducible repre-
sentation will be the sum of the characters of the irreducible representations it contains.
Note that if a given irreducible representation occurs more than once, its characters must
be added a corresponding number of times.
Now suppose that we have a reducible representation 9 of a group of order n. Even
if we do not yet know its decomposition into irreducible components, we can write its
characters for group elements g in the form
09(g) =
X
K
cK 0K(g),
(17.11)
where cK is the number of times irreducible representation K is contained in 9. If we
multiply both sides of this equation by 0K ′(g) and sum over g, the orthogonality kicks
in, and
X
g
0K ′(g)09(g) =
X
g
X
K
cK 0K ′(g)0K(g) = ncK ′.
(17.12)
Evaluating the left-hand side of Eq. (17.12), we easily solve for cK ′. We can repeat this
sequence of steps with different K ′ until all the irreducible representations in 9 have been
found.

834
Chapter 17 Group Theory
Example 17.4.4
DECOMPOSING A REDUCIBLE REPRESENTATION
Suppose we start from the following set of three basis functions for the triangular
group D34:
ψ1 = x2,
ψ2 = y2,
ψ3 =
√
2 xy,
(17.13)
where x, y, z are Cartesian coordinates with origin at the center of the triangle, and the axes
are in the directions shown in Fig. 17.1. Since C3 x = −1
2 x + 1
2
√
3 y, C3 y = −1
2
√
3 x −
1
2 y, we can (somewhat tediously) determine that
C3 x2 = 1
4 x2 + 3
4 y2 −
r
3
8(
√
2 xy),
C3 y2 = 3
4 x2 + 1
4 y2 +
r
3
8(
√
2 xy),
C3 (
√
2 xy) =
r
3
8 x2 −
r
3
8 y2 −1
2(
√
2 xy),
so in the ψ basis,
U9(C3) =


1
4
3
4
q
3
8
3
4
1
4
−
q
3
8
−
q
3
8
q
3
8
−1
2

.
(17.14)
Similar analysis can be used to obtain the matrix of C2, which is easier because the opera-
tion involved is just x −→−x, with y remaining unchanged. We get
U9(C2) =


1 0
0
0 1
0
0 0 −1

.
(17.15)
The representation of I is, of course, just the 3×3 unit matrix. Since the only data we need
right now are the traces of one representative of each class, we are ready to proceed, and
we see that
09(I) = 3,
09(C3) = 0,
09(C2) = 1.
We are labeling the characters with superscript 9 as a reminder that the representation is
that associated with the ψi. These characters have been appended below their respective
columns in Table 17.3.
Now we use the fact that the 9 representation must decompose into
9 = c1A1 ⊕c2A2 ⊕c3E,
(17.16)
4These basis functions have been chosen in a way that makes the reducible representation unitary. The factor
√
2 in ψ3 is needed
to make all the ψi at the same scale.

17.4 Discrete Groups
835
and we ﬁnd the ci by applying Eq. (17.12). Using the data in Table 17.3, and taking K ′ to
be in turn A1, A2, and E,
A1 : (1)(3) + 2(1)(0) + 3(1)(1) = 6 = 6c1, so c1 = 1,
A2 : (1)(3) + 2(1)(0) + 3(−1)1) = 0 = 6c2, so c2 = 0,
E : (2)(3) + 2(−1)(0) + 3(0)(1) = 6 = 6c3, so c3 = 1.
Thus, 9 = A1 ⊕E. We can check our work by summing the A1 and E entries from the
character table. As they must, they add to give the entries for 9.
■
For some purposes it is insufﬁcient just to know which irreducible representations are
included in a reducible basis for a group G. We may also need to know how to transform
the basis so that each basis member will be associated with a speciﬁc irreducible represen-
tation of G. Sometimes it is easy to see how to do this. For the above example, the basis
function for A1 must have the full group symmetry, while the E basis functions must be
orthogonal to the A1 basis. These considerations lead us to
A1 : ϕ = ψ1 + ψ2 = x2 + y2,
(17.17)
E : ϕ1 = ψ1 −ψ2 = x2 −y2,
ϕ2 =
√
2 ψ3 = 2xy.
(17.18)
However, if ﬁnding the irreducible basis functions by inspection proves difﬁcult, there are
formulas that can be used to ﬁnd them. See Additional Readings.
Other Discrete Groups
Most of the examples we have used have been for one group, D3, in which we have
considered symmetry operations that involve rotations about axes through the center of
the system. Groups keeping a central point ﬁxed are called point groups, and they arise,
among other places, when studying phenomena that depend on the geometric symmetries
of molecules. Some point groups have additional symmetries associated with inversion or
reﬂection. It is possible for a point group to have a single n-fold axis for any positive inte-
ger n (meaning that a symmetry element is a rotation through an angle 2π/n). However,
the number of point groups having multiple symmetry axes with n ≥3 is very limited;
they correspond to the Platonic regular polyhedra, and therefore can only be tetrahedral,
cubic/octahedral, and dodecahedral/icosahedral.
Other discrete groups arise when we consider permutational symmetry; the symmetric
group is important in many-body physics and is the subject of a separate section of this
chapter.
Exercises
17.4.1
The Vierergruppe has the multiplication table shown in Table 17.2.
(a)
Divide its elements into classes.
(b)
Using the class information, determine for the Vierergruppe its number of inequiv-
alent irreducible representations and their dimensions.
(c)
Construct a character table for the Vierergruppe.

836
Chapter 17 Group Theory
17.4.2
The group D3 may be discussed as a permutation group of three objects. Operation
C3, for instance, moves vertex 1 to the position formerly occupied by vertex 2; like-
wise vertex 2 moves to the original position of vertex 3 and vertex 3 moves to the
original position of vertex 1. So this shufﬂing could be described as the permutation of
(1,2,3) to (2,3,1). Using now letters a,b,c to avoid notational confusion, this permuta-
tion (abc) →(bca) corresponds to the matrix equation
C3


a
b
c

=


0
1
0
0
0
1
1
0
0




a
b
c

=


b
c
a

,
thereby identifying a 3 × 3 representation of the operation C3.
(a)
Develop analogous 3 × 3 representations for the other elements of D3.
(b)
Reduce your 3 × 3 representation to the direct sum of a 1 × 1 and a 2 × 2 repre-
sentation. Note: This 3 × 3 representation must be reducible or Eq. (17.10) would
be violated.
17.4.3
The group named D4 has a fourfold axis of symmetry, and twofold axes in four direc-
tions perpendicular to the fourfold axis. See Fig. 17.5. D4 has the following classes (the
numbers preceding the class descriptors indicate the number of elements in the class):
I, 2C4, C2, 2C′
2, 2C′′
2 . The twofold axes marked with primes are in the plane of fourfold
symmetry.
(a)
Find the number and dimensions of the irreducible representations.
(b)
Given that all the characters of the representations of dimension 1 are ±1 and that
C2 = C2
4, use the orthogonality conditions to construct a complete character table
for D4.
17.4.4
The eight functions ±x3, ±x2y, ±xy2, ±y3 form a reducible basis for D4, with C4
a 90◦counterclockwise rotation in the xy plane, C2 = C2
4, C′
2 = (x →−x, y →y),
C′′
2 = (x →y, y →x), and the remaining members of D4 are additional members of
the classes containing the above operations. Find the characters of the reducible repre-
sentation for which these functions form a basis, and ﬁnd the direct sum of irreducible
representations of which it consists.
17.4.5
The group C4v has a fourfold symmetry axis in the z direction, reﬂection symmetries
(σv) about the xz and yz planes, and additional reﬂection symmetries (σd, d = dihedral)
about planes that contain the z axis but are 45◦from the x and y axes. See Fig. 17.6.
The character table for C4v follows.
I
2C4
C2
2σv
2σd
A1
1
1
1
1
1
A2
1
1
1
−1
−1
B1
1
−1
1
1
−1
B2
1
−1
1
−1
1
E
2
0
−2
0
0

17.5 Direct Products
837
σv′
σd′
σd
σv
y
x
C4
FIGURE 17.6
C4v symmetry group. At left, a molecule with this symmetry.
At right, a diagram identifying the reﬂection planes, which are perpendicular to the
plane of the diagram.
(a)
Construct the matrix representing one member of each class of C4v using as a
basis a pz orbital at each of the points (x, y) = (a,0), (0,a), (−a,0), (0,−a), and
therefrom extract the characters of the reducible representation for which these pz
orbitals form a basis. A pz orbital has functional form (z/r) f (r).
(b)
Determine the irreducible representations contained in our reducible pz represen-
tation.
(c)
Form those linear combinations of our pz functions that are bases for each of the
irreducible representations found in part (a).
17.4.6
Using the notation and geometry of Exercise 17.4.5, repeat that exercise for the eight-
member basis consisting of a px and a py orbital at each of the points (x, y) = (a,0),
(0,a), (−a,0), (0,−a).
17.5
DIRECT PRODUCTS
Many multiparticle quantum-mechanical systems are described using wave functions that
are products of individual-particle states. This approach is that of an independent-particle
model, which at a higher degree of approximation can include interparticle interactions.
The single-particle states can then be chosen to reﬂect the symmetry of the system, mean-
ing that each one-particle state will be a basis member of some irreducible representation
of the system’s symmetry group. This idea is obvious, for example, in atomic structure,
where we encounter notations such as 1s22s22p3 (the ground-state electron conﬁguration
of the N atom).

838
Chapter 17 Group Theory
When a multiparticle system with symmetry group G is subjected to one of its sym-
metry operations, each single-particle factor in its wave function transforms according to
its individual irreducible representation of G, so the overall wave function may contain
products of arbitrary components of each particle’s representation. Thus, the multiparticle
basis consists of all the products that can be formed by taking one member of each single-
particle basis. This is what is termed a direct product. This multiparticle basis will also
constitute a representation of G. The notation
K = K1 ⊗K2
indicates that the representation K of G is the direct product of the representations K1 and
K2. This means also that the representation matrix UK(a) of any element a of G can be
formed as the direct product (see Eq. 2.55) of the matrices UK1(a) and UK2(a).
The representation of a group G formed as a direct product of two (or more) of its irre-
ducible representations may or may not be irreducible. For ﬁnite groups, a useful theorem
is that the characters for a direct product of representations are, for each class, the product
of the individual characters for that class. Once the characters for the direct product have
been constructed, the methods of the previous section can be used to ﬁnd the irreducible
components of the product states.
Example 17.5.1
EVEN-ODD SYMMETRY
Sometimes the analysis of a direct product is simple. Consider a system of n independent
particles subject to a potential whose only symmetry element (other than I) is inversion
(denoted i) through the origin of the coordinate system, so V (−r) = V (r). In this case,
G (conventionally named Ci) has the two elements I and i, with the following character
table.
I
i
Ag
1
1
Au
1
−1
Individual particles with A1 wave functions, which remain unchanged under inversion,
are conventionally labeled g (from the German word gerade). Particles with A2 wave
functions, which change sign on inversion, are labeled u, for ungerade. In fact, the usual
notation for the character table of the Ci group writes Ag and Au in place of A1 and
A2, thereby conveying more information about the symmetries of the corresponding basis
functions.
Now suppose that this system is in a state with j of the particles in u states and n −j of
the particles in g states. Intuitively, we know that if j is an odd number, the overall wave
function will change sign on inversion, but will not change sign if j is even. Formally, we
examine the direct product representation K:
K = u(1) ⊗u(2) ⊗··· ⊗u( j) ⊗g( j + 1) ⊗··· ⊗g(n).

17.5 Direct Products
839
Using the theorem that the characters of representation K can be obtained by multiplying
those of its constituent factors, we ﬁnd 0K(I) = 1, 0K(i) = (−1) j. Irrespective of the
value of j, K will be irreducible: It is Ag if j is even, and Au if j is odd.
■
Example 17.5.2
TWO QUANTUM PARTICLES IN D3 SYMMETRY
This case is not as simple. Suppose both particles are in states of E symmetry, a situa-
tion spectroscopists would identify with the notation e2; they use lower-case symbols to
identify individual-particle states, reserving capital letters for the overall symmetry desig-
nation. For deﬁniteness, let’s further suppose5 that each particle has a wave function of the
form found in Eq. (17.18), so particle i will have the two-member basis
ϕa(i) =

x2
i −y2
i

,
ϕb(i) = 2xi yi,
and the product basis will therefore have the four members
8aa = ϕa(1)ϕa(2),
8ab = ϕa(1)ϕb(2),
(17.19)
8ba = ϕb(1)ϕa(2),
8bb = ϕb(1)ϕb(2).
The matters at issue are (1) to ﬁnd the overall symmetries this system can exhibit, and (2)
to identify the basis functions for each symmetry.
Consulting Table 17.3, we compute the products for e ⊗e:
I
2C3
3C2
e ⊗e : 4
1
0.
Since this representation has dimension 4 while the largest irreducible representation has
dimension 2, it must be reducible. Applying the technique of Example 17.4.4, we can ﬁnd
that it decomposes into e ⊗e = A1 ⊕A2 ⊕E, a result that is easily checked by adding
entries in the D3 character table.
A set of basis functions corresponding to the decomposition into irreducible representa-
tions are
ψ A1 = (x2
1 −y2
1)(x2
2 −y2
2) + 4x1y1x2y2,
(17.20)
ψ A2 = 2
h
(x2
1 −y2
1)x2y2 −x1y1(x2
2 −y2
2)
i
,
(17.21)
ψ E
1 = (x2
1 −y2
1)(x2
2 −y2
2) −4x1x1y1x2y2,
(17.22)
ψ E
2 = 2
h
(x2
1 −y2
1)x2y2 + x1y1(x2
2 −y2
2)
i
.
Finding these could be challenging; verifying them is less so.
■
For continuous groups, it is usually simpler to decompose direct-product representations
in other ways. For example, in Chapter 16 we used ladder operators to identify overall
5An actual problem will have a wave function that, in addition to the functional dependence shown here, will have a completely
symmetric additional factor that is not relevant for the present group-theoretic discussion.

840
Chapter 17 Group Theory
Table 17.4
Character Table, Group C4v
I
2C4
C2
2σ v
2σ d
A1
1
1
1
1
1
A2
1
1
1
−1
−1
B1
1
−1
1
1
−1
B2
1
−1
1
−1
1
E
2
0
−2
0
0
angular-momentum states (irreducible representations) formed from products of individual
angular momenta. The resulting multiplets correspond to the irreducible representations,
and the angular momentum functions that we found are their bases.
Exercises
17.5.1
The group C4v has eight elements, corresponding to the rotational and reﬂection sym-
metries of a square that cannot be turned over. See Fig. 17.6. Symmetry rotations about
the z axis are denoted C4, C2, C′
4. Reﬂections relative to the xz and yz planes are
named σ v and σ ′
v; those at 45◦relative to the xz and yz planes are called σ d and σ ′
d (d
indicates “dihedral”). The character table for C4v is in Table 17.4.
(a)
Find the direct sum of irreducible representations of C4v corresponding to the
direct product E ⊗E.
(b)
A basis for E (in the context of Fig. 17.5) consists of the two functions ϕ1 = x,
ϕ2 = y. Apply a few of the group operations to this basis and verify the entries for
E in the character table.
(c)
Assume now that we have two sets of variables, x1, y1 and x2, y2, and we form
the direct-product basis x1x2, x1y2, y1x2, y1y2. Determine how the direct-product
basis functions can be combined to form bases for each of the irreducible repre-
sentations in the direct sum corresponding to E ⊗E.
17.6
SYMMETRIC GROUP
The symmetric group Sn is the group of permutations of n distinguishable objects, and
is therefore of order n!. To see this, note that to make a permutation, we may choose the
ﬁrst object in n different ways, then the second in n −1 ways, etc., until we reach the nth
object, which can be chosen in only one way. The total number of possible permutations is
therefore n(n −1)...(1) = n!. This group is important in the physics of identical-particle
systems, whose wave functions must be either symmetric with respect to particle inter-
changes (particles with this symmetry are called bosons), or antisymmetric under pairwise
particle interchanges (these particles are called fermions). This means that an n-boson
wave function 9B(1,2,...,n) must satisfy
P9B(1,...,n) = 9B(1,...,n),
(17.23)

17.6 Symmetric Group
841
where P is any permutation of the particle numbers. From a group-theoretical viewpoint
this means that 9B is a sole basis function for the trivial A1 representation of Sn: 1×1, with
all members of the representation equal to (1). Many-fermion wave functions 9F(1,...,n)
satisfy
P9F(1,...,n) = ϵP9F(1,...,n),
(17.24)
where ϵP is the n-particle Levi-Civita symbol with an index string corresponding to P; in
simple language this means ϵP = 1 if P is an even permutation of the particle numbers
(one requiring an even number of pairwise interchanges), and ϵP = −1 if P is odd. This
means that 9F is the sole basis function for the 1 × 1 totally antisymmetric representation
of Sn with members (ϵP), which we will call A2.
Since the representations needed for either bosons or fermions are simple and of
dimension 1 × 1, it might seem that sophisticated group-theoretic considerations would be
unnecessary. But that is an oversimpliﬁcation, because many-fermion systems (and some
boson systems) consist of direct products of spatial and spin functions, and the spin func-
tions may form a basis of Sn of dimension larger than one.
Example 17.6.1
TWO AND THREE IDENTICAL FERMIONS
In elementary quantum mechanics, the ground state of a two-fermion system such as the
two electrons of the He atom can be treated using a simple wave function of the form
9F =

f (1)g(2) + g(1) f (2)

α(1)β(2) −β(1)α(2)

.
Here f and g are single-particle spatial functions, and α, β describe single-particle spin
states. We continue, using a streamlined notation in which the particle numbers are
suppressed, understanding that they always occur in ascending numerical order, so 9F
will henceforth be written ( f g + g f )(αβ −βα). It is obvious that 9F has the fermion
(anti)symmetry; we note that it is an A2 basis function, which is the product of a symmet-
ric A1 spatial function and an antisymmetric A2 spin function. The physics of this problem
demands that the overall ground-state wave function 9F contain spin function αβ −βα
because it is a two-particle spin eigenstate. The two-particle example shows that the A2
overall representation was obtained as A1 ⊗A2.
For three particles, things are different. To treat the ground state of the Li atom, we
cannot form a completely antisymmetric spin function using only the two single-particle
spin functions α and β. The actual spin functions relevant for the ground state form a 2×2
representation of Sn, which we will call E:
θ1 = 1
√
6
(2ααβ −αβα −βαα),
θ2 = 1
√
2
(βαα −αβα).
(17.25)
Since permutations mix θ1 and θ2, the overall wave function for this three-particle system
must be of the form
9F = χ1θ1 + χ2θ2,

842
Chapter 17 Group Theory
where χ1 and χ2 are three-body spatial functions such that 9F has the required A2 sym-
metry. If the χi are built from spatial orbitals f , g, and h, one possible set of χi are
χ1 = 1
2 (gh f −h f g −hg f + f hg),
(17.26)
χ2 = 1
√
3

f gh + g f h −1
2gh f −1
2h f g −1
2hg f −1
2 f hg

,
a result that it is difﬁcult to ﬁnd by trial and error. Since the spin functions, and therefore
also the spatial functions, become more complicated as the system size increases, the value
of a group-theoretic description clearly becomes more urgent.
■
We consider now, from a formal viewpoint, only the many-fermion case. As illustrated
in Example 17.6.1, we deal with space-spin functions in which the spin function has, for
reasons we will not discuss here, been chosen to be built from an irreducible representation
K of the symmetric group, whose member for permutation P is a unitary matrix designated
UK(P), and whose basis is a set of spin functions θi, i = 1,...,nK , where nK is the
dimension of the spin representation. This means that
P θi =
nK
X
j=1
U K
ji(P)θ j.
(17.27)
We shall now show that an antisymmetric overall space-spin function can result if we form
9F =
nK
X
i=1
χi θi,
(17.28)
where the χi are basis functions for a representation K ′, of the same dimension as K,
meaning that
P χi =
nK
X
k=1
U K ′
ki (P)χk.
(17.29)
The representation K ′ is assumed to have members that satisfy
UK ′(P) = ϵPUK(P)∗.
(17.30)
The representation K ′ must exist, since it is (apart from a complex conjugate) the direct
product of representations K and A2. Because A2 only imparts sign changes to various UK,
the representation K ′ will be irreducible because representation K is. The representation
K ′ is termed dual to representation K.

17.6 Symmetric Group
843
To verify that the assumed form of 9F has the required A2 symmetry, we take it, as
given in Eq. (17.28), and apply to it an arbitrary permutation P:
P 9F =
nK
X
i=1
(Pχi)(Pθi) =
X
i
 X
k
U K ′
ki (P)χk
!
X
j
U K
ji(P)θ j


=
X
jk
 X
i
U K ′
ki (P)U K
ji(P)
!
χk θ j
=
X
jk
 X
i
ϵPU K
ki(P)∗U K
ji(P)
!
χk θ j.
(17.31)
The steps taken in the processing of Eq. (17.31) are substitutions of Eqs. (17.27) and
(17.29) for Pχi and Pθi, followed by a conversion from UK ′ to UK through the use
of Eq. (17.30). We complete our analysis by recognizing that because U is unitary,
Uki(P)∗= (U−1)ik(P), so
X
i
ϵPU K
ki(P)∗U K
ji(P) = ϵPδ jk,
leading to the ﬁnal result
P9F =
X
jk
ϵP δ jkχk θ j = ϵP
X
k
χk θk = ϵP9F.
(17.32)
Equation (17.32) shows that the overall wave function 9F has the required fermion anti-
symmetry.
Our only remaining problem is to construct spatial functions χk, which are bases for rep-
resentation K ′. We state without proof (see Additional Readings) that this can be accom-
plished using the formula
χ j
i =
X
P
U K ′
i j (P)∗Pχ0,
(17.33)
where χ0 is a single spatial function whose permutations will be used to construct the χi.
The index j identiﬁes an entire set of χi; if χ0 has no permutational symmetry, we can
create sets of χi in nK ′ in different ways, each corresponding to a different value of j.
Example 17.6.2
CONSTRUCTION OF MANY-BODY SPATIAL FUNCTIONS
We consider a three-electron problem in which the spin states are given by Eq. (17.25). We
need the representation of S3 for which these θi are a basis. We are fortunate to already
have this representation, as S3 is isomorphic (in 1–1 correspondence) with D3, so we can
use the set of 2×2 representation matrices given in Eq. (17.2), if we make the identiﬁcation
C2 ↔P(12), C′
2 ↔P(13), C′′
2 ↔P(23), where P(i j) denotes the permutation that inter-
changes the ith and jth items in the ordered list to which the permutation is applied. The
permutation P(123 →312) corresponds to C3, and P(123 →231) corresponds to C2
3.

844
Chapter 17 Group Theory
We now apply Eq. (17.33); an easy way to do this is to start by generating the matrix T
that results from keeping all i and j. In the present case, that means forming the matrix sum
T = U(I)χ0 −U(C2)P(12)χ0 −U(C′
2)P(13)χ0 −U(C′′
2)P(23)χ0
+ U(C3)P(123 →312)χ0 + U(C2
3)P(123 →231)χ0.
The minus signs for the U(C2) terms arise from the ϵP which is needed to convert UK
into UK ′.
Taking χ0 as the product f (1)g(2)h(3), hereafter written f gh, and inserting numerical
values for the U, we reach
T =


f gh −g f h −1
2(gh f
1
2
√
3(gh f −h f g
+h f g −hg f −f hg)
−hg f + f hg)
1
2
√
3(−gh f + h f g
f gh + g f h −1
2(gh f
−hg f + f hg)
+h f g + hg f + f hg)


.
(17.34)
Each column of Eq. (17.34) deﬁnes a set of χi, in a form that is not guaranteed to be
normalized. From the second column, dividing through by
√
3 for normalization, we obtain
the χi that were listed as a possible wave function in Example 17.6.1 at Eq. (17.26). The
ﬁrst column of Eq. (17.34) shows that there is a second possibility for an antisymmetric
wave function built from the spatial product f gh, namely one that can be written
9′
F = χ′
1θ1 + χ′
2θ2,
with the normalized spatial functions
χ′
1 = 1
√
3

f gh −g f h −1
2gh f −1
2h f g + 1
2hg f + 1
2 f hg

,
χ′
2 = 1
2 (−gh f + h f g −hg f + f hg).
■
Exercises
17.6.1
(a)
The objects (abcd) are permuted to (dacb). Write out a 4 × 4 matrix representa-
tion of this one permutation.
Hint: Compare with Exercise 17.4.2.
(b)
Is the permutation (abdc) →(dacb) odd or even?
(c)
Is this permutation a possible member of the D4 group, which was the subject of
Exercise 17.4.3? Why or why not?
17.6.2
(a)
The permutation group of four objects, S4, has 4! = 24 elements. Treating the
four elements of the cyclic group, C4, as permutations, set up a 4 × 4 matrix
representation of C4. Note that C4 is a subgroup of P4.
(b)
How do you know that this 4 × 4 matrix representation of C4 must be reducible?

17.7 Continuous Groups
845
17.6.3
The permutation group of four objects, S4, has ﬁve classes.
(a)
Determine the number of elements in each class of S4 and identify one element of
each class as a product of cycles.
(b)
Two of the irreducible representations of S4 are of dimension 1 (and are usually
denoted A1 and A2). Noting that permutations can be classiﬁed as even or odd,
ﬁnd the characters of A1 and A2.
Hint. Set up a character table and ﬁll in the A1 and A2 lines.
(c)
One irreducible representation of S4 (usually denoted E) is of dimension 2. Deter-
mine the dimensions of all the irreducible representations of S4 other than A1, A2,
and E.
(d)
Complete the character table of S4.
Hint. Only the even permutations have nonzero characters in the E representation.
17.7
CONTINUOUS GROUPS
Several continuous groups whose importance in physics was recognized long ago corre-
spond to rotational symmetry in two- or three-dimensional space. Here the group elements
are the rotations, the angles of which can vary continuously and thereby assume an inﬁnite
number of values. For rotations, the group multiplication rule corresponds to the applica-
tion of successive rotations, which we have seen can be described by matrix multiplication.
Rotations clearly form a group since they contain an identity element (no rotation), suc-
cessive rotations are equivalent to a single rotation, and every rotation has an inverse (its
reverse).
Rotations in two-dimensional (2-D) space can be described by 2×2 orthogonal matrices
with determinant +1; the group consisting of these rotations is named SO(2) (SO stands
for “special orthogonal”). If we also include reﬂections, so that the determinant can be
±1, the group is named O(2). Since a 2-D rotation is completely speciﬁed by a single
angle, SO(2) is a one-parameter group. A matrix representation of SO(2) was introduced
in Eq. (17.1); the group parameter is the rotation angle ϕ.
Rotations in 3-D space are described by 3×3 orthogonal matrices. The resulting groups
are designated O(3) and SO(3); for SO(3), three angles (e.g., the Euler angles) are group
parameters. Generalizing to n × n matrices, the groups are named O(n) and SO(n); the
number of parameters needed to specify fully an n×n real orthogonal matrix is n(n−1)/2,
and that is the number of independent parameters (generalizations of the Euler angles)
needed in SO(n). If we further generalize to unitary matrices, we have the groups SU(n)
and U(n). Proof that these sets of unitary matrices form groups is left as an exercise.
Let’s introduce some nomenclature. The n ×n matrices referred to above can be thought
of as the deﬁning, or fundamental, representations of the groups involved. The order of
a continuous group is deﬁned as the number of independent parameters needed to specify
its fundamental representation, so the order of SO(n) is the previously stated n(n −1)/2;
the order of the group SU(n) is n2 −1.
In addition to their use for the treatment of rotational symmetry, continuous groups
are also relevant to the classiﬁcation of elementary (and not so elementary) particles. It has
been experimentally observed that regularities in the masses and charges of sets of particles

846
Chapter 17 Group Theory
can be explained if their wave functions are identiﬁed as basis members of an irreducible
representation of an appropriate group. Note that now the group does not describe rotations
in ordinary space, but refers to a more abstract space relevant to an understanding of the
physics involved. The earliest example of this idea was electron spin; spin wave functions
are objects in an abstract SU(2) space, together with rules to unravel their observational
properties. A further abstraction began with the notion that the proton and neutron might
form a basis for an abstract SU(2) representation, and has since blossomed with the intro-
duction of SU(3) and other continuous groups into particle physics. A brief survey of these
ideas is presented in our speciﬁc discussion of SU(3).
Lie Groups and Their Generators
It is extremely useful to manage groups such as SO(n) or SU(n) in ways that do not
explicitly involve an inﬁnite number of elements; a formalism for doing so was devised
by the Norwegian mathematician Sophus Lie. Groups for which Lie’s analysis is appli-
cable, called Lie groups, have elements that depend continuously on parameters that vary
over closed intervals (meaning that the parameter set includes the limit of any converging
sequence of parameters). The groups SO(n) and SU(n) are Lie groups.
Lie’s essential idea was to describe a group in terms of its generators, a minimal set
of quantities that could be used in a speciﬁc way (multiplied by parameters) to produce
any element of the group. Our starting point is, for each parameter ϕ controlling a group
operation, to introduce a generator S with the property that when ϕ is inﬁnitesimal (and
therefore written δϕ) the group element with parameter δϕ (which must be close to the
identity element of the group) can be represented by
U(δϕ) = 1 + i δϕ S.
(17.35)
The factor i in Eq. (17.35) could have been included in S but it is more convenient not to
do so. Group operations corresponding to larger values of ϕ can now be generated from
repeated operation (N times) by ϕ/N, where ϕ/N is small. We therefore identify U(ϕ) as
the limit
U(ϕ) = lim
N→∞

1 + i ϕ S
N
N
;
This large-N limit deﬁnes the exponential, so we have the general result
U(ϕ) = exp(iϕ S).
(17.36)
Given any representation U of our continuous group, we can ﬁnd the generator S cor-
responding to the parameter ϕ for that representation by differentiation of Eq. (17.36),
evaluated at the identity element of our group. In particular,
−i
dU(ϕ)
dϕ

ϕ =0
= S,
(17.37)
revealing that the entire behavior of a representation U can be deduced from its behavior
in an inﬁnitesimal parameter-space neighborhood of the identity operation. However, to
obtain complete knowledge of the structure of a Lie group we need to study the behavior

17.7 Continuous Groups
847
of its generators for a representation that is faithful; for that purpose it is desirable to use
the fundamental representation.
Example 17.7.1
SO(2) GENERATOR
SO(2) involves rotational symmetry about a single axis, and its operations are counter-
clockwise rotations of the coordinate axes through angles ϕ. Working with the 2 × 2
fundamental representation of SO(2), an inﬁnitesimal rotation δϕ causes (to ﬁrst order)
(x′, y′) = (x + y δϕ, y −x δϕ), or
 
x′
y′
!
=
 
1
δϕ
−δϕ
1
! 
x
y
!
=
" 
1
0
0
1
!
+ δϕ
 
0
1
−1
0
!# 
x
y
!
= 1 + iδϕS,
with
iS =
 
0
1
−1
0
!
,
or
S =
 
0
−i
i
0
!
= σ 2,
(17.38)
where σ 2 is a Pauli matrix. A general rotation is then represented by Eq. (17.36) as
U(ϕ) = eiϕS = 12 cosϕ + iσ 2 sinϕ =
 cosϕ
sinϕ
−sinϕ
cosϕ
!
,
(17.39)
where we have evaluated the exponential of the matrix in Eq. (17.39) using the Euler
identity, Eq. (2.80). This equation can be recognized as the transformation law for a 2-D
coordinate rotation, Eq. (3.23), verifying that the generator formalism works as expected.
If we had started from the ﬁnal expression for U(ϕ) given in Eq. (17.39), we could have
generated S from it by applying the differentiation formula, Eq. (17.37).
■
The generator form, Eq. (17.36), has some nice features:
1.
For the groups SO(n) and SU(n), any U will be unitary (remember, “orthogonal” is
a special case of “unitary”). This means that
U−1 = exp(−iϕS) = U† = exp(−iϕS†),
(17.40)
so S = S†, showing that S is Hermitian. That is the proximate reason for inclusion
of i in the deﬁning equation for S.
2.
Because for both SO(n) and SU(n), det(U) = 1, we also have, invoking the trace
formula, Eq. (2.84),
det(U) = exp
 trace(lnU)

= exp
 iϕ trace(S)

= 1.
(17.41)
This condition is satisﬁed for general ϕ only if trace(S) = 0. So S is not only Hermi-
tian, but traceless.
3.
It can be shown (but is not proved here) that the number of independent generators of
a Lie group is equal to the order of the group.

848
Chapter 17 Group Theory
One of Lie’s key observations was that by focusing on inﬁnitesimal group elements,
various properties of the generators could be deduced. We have already seen that if the
form of U in terms of its parameters is known, the generators S can be obtained by differ-
entiation of Eq. (17.36) in the limit corresponding to the identity group element.
Second, relations between the generators can be developed, as follows: Let us consider
two operations U j(ϵ j) and Uk(ϵk) of a group G, that respectively correspond to the gen-
erators S j and Sk. The values of ϵ j and ϵk are assumed small, so the resulting U j and
Uk differ, but only slightly, from the identity element. Expanding the exponentials and
keeping terms through second order in ϵ,
U j = exp(iϵ jS j) = 1 + iϵ jS j −1
2ϵ2
jS2
j + ···,
Uk = exp(iϵkSk) = 1 + iϵkSk −1
2ϵ2
kS2
k + ···,
we evaluate the leading term (in ϵ) of the matrix product U−1
k U−1
j UkU j. The linear terms
all cancel, as do several of the quadratic terms. The remaining quadratic terms can be
grouped so as to reach the result
U−1
k U−1
j UkU j = 1 + ϵ jϵk[S j,Sk] + ···
= 1 + iϵ jϵk
X
l
f jklSl + ···.
(17.42)
The last line of Eq. (17.42) reﬂects the fact that the left-hand side of the equation must
correspond to some group element, and that element must, to ﬁrst order in the generators,
be of the form shown. Note that the premultipliers iϵ jϵk are not a form restriction, as their
presence simply changes the value of f jkl.
Comparing the two lines of Eq. (17.42), we obtain the important closure relation among
the generators of the group G:
[S j,Sk] = i
X
l
f jklSl.
(17.43)
The coefﬁcients f jkl are called the structure constants of G. It can be shown that f jkl is
antisymmetric with respect to index permutations, so f jkl = fklj = fljk =−fkjl =−flkj =
−f jlkj. The structure constants provide a representation-independent characterization of a
Lie group, but as already mentioned, to determine them we will need to work with a faithful
representation, such as the group’s fundamental representation. We will shortly do so for
the groups we study in detail.
As is obvious from the foregoing analysis, Lie group generators will not in general com-
mute. In 3-D, rotations about different axes do not commute, and therefore their generators
cannot commute either. An additional indicator for group classiﬁcation is the maximum
number of independent generators that all mutually commute. This number is called the
rank of the group; it is signiﬁcant because the generators can be subjected to unitary trans-
formations without changing the ultimate group structure, and the mutually commuting
generators can therefore be brought simultaneously to diagonal form. Once this is done,
the basis members of the generator set can be labeled using the diagonal elements (the

17.7 Continuous Groups
849
eigenvalues) of the commuting generators. The values of the labels (and the physical phe-
nomena related thereto) depend on the representation in use.
For the orthogonal groups SO(n) and unitary groups SU(n) the commutation relations,
Eq. (17.43), can be developed along the lines of angular momentum, leading to generalized
ladder operators (and selection rules) in conjunction with the mutually commuting opera-
tors. For these central aspects of (the so-called classical) Lie groups we refer to the work
by Greiner and Mueller (see Additional Readings).
Summarizing, the rank of a group indicates the number of indices needed to label the
basis. In applications to quantum mechanics, these indices are often referred to as quantum
numbers. For example, in SO(3), which is of rank 1, the index is usually taken to be ML,
usually identiﬁed physically as the z component of an angular momentum; when SU(2),
also of rank 1, is used for the description of electron spin, the index is usually called MS.
The possible values of ML or MS depend on the representation, and we saw in Chapter 16
that the values range, in unit steps, between +L and −L (or +S and −S), so that diagrams
identifying these basis members can be plotted on a line. In contrast, we will see that
SU(3) is of rank 2, so its basis members are labeled with two quantum numbers. Diagrams
identifying the label assignments will in that case need to be 2-D.
It is also possible to label entire representations. One way to label them is to use the
eigenvalues of operators that commute with all the generators of the group; such operators
are called Casimir operators; the number of independent Casimir operators is equal to
the rank of the group. SO(3) has therefore one Casimir operator; it is the operator usually
known in angular-momentum applications as L2 or J 2.
Groups SO(2) and SO(3)
SO(2) and SO(3) are rotation groups; SO(2) corresponds to rotational symmetry about
one axis, which we will take to be the z axis when the symmetry is for a 3-D system. SO(2)
will therefore have only one generator, that already found in Eq. (17.38):
Sz = σ 2 =
 
0
−i
i
0
!
.
(17.44)
To use Sz as one of the generators of SO(3), we extend to a 3 × 3 basis, calling the
generator S3, obtaining
S3 =


0
−i
0
i
0
0
0
0
0

.
(17.45)
SO(3) has two other generators, S1 and S2. To obtain S1, the generator corresponding to
Ux(ψ) =


1
0
0
0
cosψ
sinψ
0
−sinψ
cosψ

,
(17.46)

850
Chapter 17 Group Theory
we apply Eq. (17.37):
S1 = −i
d Rx(φ)
dψ

ψ=0
= −i


0
0
0
0 −sinψ
cosψ
0 −cosψ −sinψ


ψ=0
=


0
0
0
0
0
−i
0
i
0

.
(17.47)
In a similar fashion, starting from
Uy(θ) =


cosθ
0
−sinθ
0
1
0
sinθ
0
cosθ

,
(17.48)
we ﬁnd
S2 =


0
0
i
0
0
0
−i
0
0

.
(17.49)
Summarizing, the structure of SO(2) is trivial, as it has only a single generator, and has
order 1 and rank 1. However, the structure of SO(3) is not entirely trivial. Because no two
of S1, S2, and S3 commute, SO(3) will have order 3, but rank 1. By matrix multiplication,
we may compute its structure constants. It is easily veriﬁed that
[S j,Sk] = iϵ jklSl,
(17.50)
where ϵ jkl is a Levi-Civita symbol. Thus, the Levi-Civita symbols are the structure con-
stants for SO(3). Note also that the Sj obey the angular momentum commutation rules.
In fact, these are the same matrices that were called Ki in Eq. (16.86) in Chapter 16, and
they were identiﬁed there as matrices describing the components of angular momentum in
a basis consisting of x, y, and z. This observation can be generalized to reach the conclu-
sion that for any representation of SO(3), the generators can be taken to be the angular
momentum components L j ( j = 1,2,3) as expressed in any basis for that representation.
Example 17.7.2
GENERATORS DEPEND ON BASIS
To show that the generators indeed have a form that depends on the choice of basis, con-
sider a basis for SO(3) proportional to the spherical harmonics for l = 1 with standard
phases,
ψ1 = −1
√
2
(x + iy),
ψ2 = z,
ψ3 = 1
√
2
(x −iy).
(17.51)
We now apply Lx = −i[y∂/∂z −z∂/∂y] to the basis members, getting the result Lxψ1 =
z/
√
2 = ψ2/
√
2, Lxψ2 = −iy = (ψ1 +ψ3)/
√
2, Lxψ3 = z/
√
2 = ψ2/
√
2, meaning that
the matrix representation of Lx, and therefore of a generator we will call Sx, is
Sx = 1
√
2


0
1
0
1
0
1
0
1
0

.
(17.52)

17.7 Continuous Groups
851
Applying L y and Lz to the spherical harmonic basis, we obtain generators Sy and Sz:
Sy = 1
√
2


0 −i
0
i
0 −i
0
i
0

,
Sz =


1 0
0
0 0
0
0 0 −1

.
(17.53)
These generators, though different from those given in Eqs. (17.45), (17.47), and (17.49),
are equivalent to them in the sense that they deﬁne the same irreducible representation
of SO3.
■
Group SU(2) and SU(2)–SO(3) Homomorphism
A complete set of generators for the fundamental representation of SU(2) must span the
space of traceless 2 × 2 Hermitian matrices; since there is only one off-diagonal element
above the diagonal that can have an arbitrary complex value, it can, if nonzero, be assigned
in two linearly independent ways (such as 1 and −i). The below-diagonal element is then
completely determined by Hermiticity. There is only one independent way to assign the
diagonal elements, as there are two and they must be real and sum to zero. Thus, a simple
set of matrices satisfying the necessary conditions consists of the three Pauli matrices σ j,
j = 1,2,3. Noting also that there would be advantages to having the generators scaled
so that they would satisfy the angular momentum commutation relations, we choose the
deﬁnition
S j = 1
2 σ j,
j = 1,2,3.
(17.54)
Then, based on our many previous encounters or by performing the matrix multiplications,
we can conﬁrm
[S j,Sk] = iϵ jklSl.
(17.55)
In addition, for rotation parameters denoted as α j in connection with generators S j, we
have, calling the corresponding SU(2) members U j,
U j(α j) = exp(iα jσ j/2),
j = 1,2,3.
(17.56)
Invoking the Euler identity, Eq. (2.80), we can rewrite Eq. (17.56) as
U j(α j) = 12 cos
α j
2

+ iσ j sin
α j
2

.
(17.57)
The group SU(2) was ﬁrst recognized as relevant for physics when it was observed
that spin states of the electron form a basis for its fundamental representation. We already
know, from Chapter 16, that orbital angular momentum multiplets come in sets with odd
numbers of members (2L + 1, with L integral). But we also observed that abstract quanti-
ties that obey the angular momentum commutation rules with half-integer L values come
in multiplets with even numbers of members. The multiplet with two members is the fun-
damental basis for the group SU(2). These basis functions are conventionally written |↑⟩
and |↓⟩, (or just α and β), and in matrix notation are
|↑⟩=
1
0

,
|↓⟩=
0
1

.
(17.58)

852
Chapter 17 Group Theory
Since the structure constants for SU(2) show that its generators satisfy the angular momen-
tum commutation rules, we may conclude that all angular momentum multiplets deﬁne
representations of SU(2); in Chapter 16 we found that the multiplets of odd dimension
(2L + 1 with L integral) can be chosen to be the spherical harmonics of angular momen-
tum L and are therefore also a basis for a representation of SO(3). Angular momentum
multiplets of even dimension do not have a 3-D spatial representation and cannot corre-
spond to a representation of SO(3). They are the more abstract quantities we call spinors,
have half-integer angular-momentum quantum numbers, and are bases only for represen-
tations of SU(2).
Further understanding of the situation can be obtained by applying Ux(ϕ), a synonym
for U1(ϕ), to the spin function |↑⟩. Taking ϕ = π, this corresponds to a 180◦rotation about
the x axis, which we might expect would convert |↑⟩into |↓⟩. Applying Eq. (17.57), which
for the current case assumes the form Ux = iσ 1, we have
Ux |↑⟩= i
0
1
1
0
1
0

= i
0
1

= i |↓⟩.
(17.59)
So far, so good. But let’s now try a similar rotation with ϕ = 2π. We then have Ux = −12,
meaning that a complete 360◦rotation does not restore |↑⟩, but gives instead −|↑⟩, namely
the expected state, but with a change of sign. To recover |↑⟩with its original (+) sign would
require a rotation ϕ = 4π, i.e., two revolutions. Each rotation between ϕ = 2π and ϕ = 4π
is, with opposite sign, equivalent to one in the (0,2π) range.
We now see the essential difference between SU(2) and SO(3): The angular range of
the rotation parameters in SU(2) is twice that in SO(3), so each SO(3) element is gen-
erated twice in each dimension (with different signs) in SU(2). Thus the correspondence
between the two groups is not one-to-one (an isomorphism), but is two-to-one, a homo-
morphism. The existence of this homomorphism is not important for irreducible represen-
tations of odd dimension (corresponding to integer L or, in more general contexts, J), since
then U(2π) = U(0) and the range (2π,4π) simply duplicates (0,2π). But the homomor-
phism remains important for even-dimension representations of SU(2), which correspond
to half-integer J and are not representations of SO(3). However, the fact that all rep-
resentations of SO(3) are also representations of SU(2) means that we can form within
SU(2) direct products that include representations of both even and odd dimension. This
observation validates our analysis of states with both orbital and spin angular momentum.
In summary, we observe that half-integer angular momentum basis functions, which
in earlier discussion we have already labeled as spinors, not only are objects that cannot
be represented as functions in ordinary 3-D space, but are also objects whose rotational
properties are unusual in that their angular periodicity is 4π, not the value 2π that would
ordinarily be expected. They are thus somewhat abstract quantities whose relevance to
physics rests on their ability to explain the “spin” properties of electrons and other fermions.
Group SU(3)
Starting in the 1930s, physicists began to give considerable attention to the symmetries of
baryons, particles that, as the preﬁx “bary” implies, are heavy in comparison to electrons,
and that interact subject to a force called the strong interaction. The earliest conjecture,

17.7 Continuous Groups
853
by Heisenberg, was to the effect that the approximate charge independence of the nuclear
forces involving protons and neutrons suggested that they could be viewed as different
quantum states of the same particle (called the nucleon), with the nucleon having a sym-
metry appropriate to the existence of a doublet of states. The nucleon was postulated to
have the same symmetry as electron spin, namely that of the continuous group SU(2).
Although the nucleon symmetry has nothing to do with spin, it is referred to as isospin,
with the isospin symmetry described by the matrices τ i, i = 1,2,3 (equal to the corre-
sponding Pauli spin matrices σ i), and the isospin states can be classiﬁed by the eigenvalue
of τ3 (designated I3), with I3 = +1/2 corresponding to the proton, I3 = −1/2 correspond-
ing to the neutron.
By the early 1960s, a large number of additional baryons with strong interactions had
been identiﬁed, of which eight (proton, neutron, and six others) were rather similar in mass.
The masses of the baryons discussed in this section are listed in Table 17.5.
In 1961, Gell-Mann, and independently Ne’eman, suggested that these eight baryons
might be symmetry-related, and proposed that they be identiﬁed with an irreducible rep-
resentation of the group SU(3), with the relatively small mass differences attributed to
forces weaker than the strong interaction and with different symmetry. The states describ-
ing these eight particles would be a basis for the generators of an SU3 representation of
dimension 8. Subsequently, it was proposed that all eight of these particles were actually
formed from combinations of three smaller, and presumably more fundamental, particles
called quarks, and the three types of quarks initially postulated, given the names up (u),
down (d), and strange (s), were ultimately identiﬁed as forming a basis for the generators
of SU(3). This original insight then led to the identiﬁcation of a set of mesons involved
with strong interaction as species consisting of one quark and one antiquark, thereby also
corresponding to basis members of representations of SU(3).
The situation described in the preceding paragraph can be more fully understood by pro-
ceeding to a somewhat detailed discussion of the group SU(3). This group is deﬁned by its
generators, of which there are eight. The maximum number that commute with each other
is two, so the group is of order 32−1 = 8 and rank 2. The simplest useful way to specify the
Table 17.5
Baryon Octet
Mass
Y
I3
4 :
4−
1321.32
−1
−1
2
40
1314.9
−1
+ 1
2
6 :
6−
1197.43
0
−1
60
1192.55
0
0
6+
1189.37
0
+1
3 :
3
1115.63
0
0
N :
n
939.566
1
−1
2
p
938.272
1
+ 1
2
Masses are given as rest-mass energies, in MeV (1 MeV =
106 eV).

854
Chapter 17 Group Theory
generators is to write them as 3×3 matrices in the SU(3) fundamental representation. Like
other continuous groups, SU(3) has an inﬁnite number of other irreducible representations
of various sizes, but the key properties of the generators (speciﬁcally, their commutation
rules) will be the same as those of the fundamental representation. We accordingly write
the eight SU(3) generators in terms of zero-trace Hermitian matrices λ1 through λ8, with
Si = 1
2 λi,
(17.60)
where the λi, known as the Gell-Mann matrices, are
λ1 =


0
1
0
1
0
0
0
0
0

,
λ2 =


0
−i
0
i
0
0
0
0
0

,
λ3 =


1
0
0
0
−1
0
0
0
0

,
λ4 =


0
0
1
0
0
0
1
0
0

,
(17.61)
λ5 =


0
0
−i
0
0
0
i
0
0

,
λ6 =


0
0
0
0
0
1
0
1
0

,
λ7 =


0
0
0
0
0
−i
0
i
0

,
λ8 = 1
√
3


1
0
0
0
1
0
0
0
−2

.
In our use of SU(3), we will associate the rows and columns of this representation (in
order) to the quarks u, d, and s. Note that λ1, λ2, and λ3 are block diagonal with the upper
block being the SU(2) isospin matrices, signaling the presence of an SU(2) subgroup with
generators λ1/2, λ2/2, and λ3/2. If we combine λ3 and λ8 so as to choose the generators
in different ways, we can replace λ3 with one of the following:
λ′
3 =
√
3 λ8 −λ3 =


0
0
0
0
1
0
0
0
−1

,
(17.62)
λ′′
3 =
√
3 λ8 + λ3 =


1
0
0
0
0
0
0
0
−1

,
(17.63)
indicating the existence of another SU(2) subgroup with generators S′
1 = λ6/2, S′
2 =
λ7/2, S′
3 = λ′
3/2, and a third SU(2) subgroup, with generators S′′
1 = λ4/2, S′′
2 = λ5/2,
S′′
3 = λ′′
3/2. These observations support the notion that isospin multiplets can exist within
an SU(3) basis.
Because SU(3) is of rank 2, the members of its representations can be labeled according
to the eigenvalues of two commuting generators, in contrast to the single label, Sz or Iz,
that we employed to label SU(2) members. It is customary to use for this purpose the two
generators (λ3 and λ8) already in diagonal form. Continuing with the notation introduced
for the nucleon, the eigenvalue of the SU(3) generator S3 is identiﬁed as I3, while S8 is
used to construct the identiﬁer Y (known as hypercharge), deﬁned as the eigenvalue of
2S8/
√
3. An oft-used alternative to Y is the strangeness S ≡Y −1.

17.7 Continuous Groups
855
Example 17.7.3
QUANTUM NUMBERS OF QUARKS
From
S3 = 1
2


1
0
0
0
−1
0
0
0
0

,
we can read out the quark I3 values + 1
2 for u, −1
2 for d, and 0 for s. From
2S8/
√
3 = λ8/
√
3 = 1
3


1
0
0
0
1
0
0
0
−2

,
we ﬁnd the Y values 1
3 for u and d, and −2
3 for s.
■
From the deﬁnitions of the Si in Eq. (17.60), one can readily carry out the matrix opera-
tions needed to establish their commutation rules. Note that even though the commutation
rules will be obtained by examining the speciﬁc representation introduced in Eq. (17.60),
they apply to all representations of the SU(3) generators.
We will use the commutation rules in a ladder-operator approach to the analysis of the
symmetry properties of the three-quark multiplets. It is helpful to systematize the work by
temporarily renaming S1, S2 as I1, I2; S6, S7 as U1, U2; and S4, S5 as V1, V2. Then we
introduce
I+ = I1 + iI2,
I−= I1 −iI2,
U+ = U1 + iU2,
U−= U1 −iU2,
(17.64)
V+ = V1 + iV2,
V−= V1 −iV2,
and write some relevant commutators as
[S3,I±] = ±I±,
[S3,U±] = ∓1
2 U±,
[S3,V±] = ±1
2 V±,
(17.65)
[S8,I±] = 0,
[S8,U±] = ±1
2
√
3 U±,
[S8,V±] = ±1
2
√
3 V±.
Using the logic of ladder operators (described in detail for applications to angular momen-
tum operators in Section 16.1), the above commutators can be used to show that, starting
from a basis function ψ(I3,Y), we can apply I±, U±, or V± to obtain basis functions with
other label sets. For example,
[S8,U+]ψ(I3,Y) = S8U+ψ(I3,Y) −U+S8ψ(I3,Y) = 1
2
√
3 U+ψ(I3,Y).
Replacing S8ψ(I3,Y) by 1
2
√
3 Yψ(I3,Y), this equation can be rearranged to
S8

U+ψ(I3,Y)

= 1
2
√
3 (Y + 1)

U+ψ(I3,Y)

,
which shows that if it does not vanish, U+ψ(I3,Y) is an eigenvector of S8 with an
eigenvalue corresponding to an increase of one unit in Y . Similarly, from the relation
[S3,U+]ψ(I3,Y) = −1
2U+ψ(I3,Y), we ﬁnd that U+ψ(I3,Y), if nonvanishing, is an

856
Chapter 17 Group Theory
eigenvector of S3 with an eigenvalue less by 1/2 than that of ψ(I3,Y). These observa-
tions correspond to the equation U+ψ(I3,Y) = Cψ(I3 −1
2,Y + 1). This and other ladder
identities are summarized in the following equations:
I±ψ(I3,Y) = CI ψ(I3 ± 1,Y),
U±ψ(I3,Y) = CU ψ(I3 ∓1
2,Y ± 1),
(17.66)
V±ψ(I3,Y) = CV ψ(I3 ± 1
2,Y ± 1).
The constants C will depend on the representation under study and on the values of I3 and
Y ; if the result of an operation according to any of these equations leads to an (I3,Y) set
that is not part of the representation’s basis, the C associated with that equation will vanish
and the ladder construction will terminate.
It is important to stress that the operators in Eq. (17.66) only move within the represen-
tation under study, so if we start with a basis member of an irreducible representation, all
the functions we will be able to reach will also be members of the same representation.
Example 17.7.4
QUARK LADDERS
As a preliminary to our study of baryon and meson symmetries, let’s see how the ladder
operators work, with the quarks, symbolically ψ(I3,Y), represented by
u = ψ
1
2, 1
3

=


1
0
0

,
d = ψ

−1
2, 1
3

=


0
1
0

,
s = ψ

0,−2
3

=


0
0
1

.
As explained in Example 17.7.3, the values of I3 and Y are obtained from the diagonal ele-
ments (the eigenvalues) of S3 and S8. The 3×3 matrices representing the ladder operators
in this example are
I+ =


0
1
0
0
0
0
0
0
0

,
U+ =


0
0
1
0
0
0
0
0
0

,
V+ =


0
0
0
0
0
1
0
0
0

,
(17.67)
I−=


0
0
0
1
0
0
0
0
0

,
U−=


0
0
0
0
0
0
1
0
0

,
V−=


0
0
0
0
0
0
0
1
0

.
By straightforward matrix multiplication, we ﬁnd I−u = d, I+d = u, U−d = s, U+s = d,
V−u = s, V+s = u; all other operations yield vanishing results. These relationships can
be represented in the 2-D graph shown as Fig. 17.7 with Y in the vertical direction and I3
horizontal. The arrows in the graph are labeled to indicate the results of application of the
ladder operators.
■
Continuing now to the baryons, we consider representations appropriate to three quarks,
which we can form as the direct product of three single-quark representations. Using the
notation 3 as shorthand for the fundamental representation (which is of dimension 3), the

17.7 Continuous Groups
857
+
−
+
−
+
−
I
Y
I
u
v
s (0, −    )
2
3
d (−    ,    )
1
2
1
3
u (   ,     )
1
2
1
3
FIGURE 17.7
Conversions between u, d, and s quarks by application of ladder operators
I±, U±, and V±. The coordinates of each particle are its (I,Y).
I−
I
Y
I +
v+
u−
v −
u +
(−1, 0)
(+1, 0)
(−    , +1)
1
2
(−    , −1)
1
2
(+    , −1)
1
2
(+    , +1)
1
2
FIGURE 17.8
Root diagram of SU(3). Each operator is labeled by the changes
it causes: (1I, 1Y ).
direct product we need is 3⊗3⊗3. This direct product is a reducible representation, which
decomposes into the direct sum
3 ⊗3 ⊗3 = 10 ⊕8 ⊕8 ⊕1,
(17.68)
where 10, 8, and 1 refer to irreducible representations of the indicated dimensions.
A standard way to decompose product representations such as we have here uses dia-
grams known as Young tableaux. Because development of the rules for construction and
use of Young tableaux would take us beyond the scope of this text, we pursue here an alter-
nate route that uses the ladder operators of Eq. (17.66). Use of the ladder operators also
has the advantage that it yields explicit expressions for the I3,Y eigenfunctions. Since the
direction in which ladder operators connect states in an I3,Y diagram is general, we can
draw a picture that summarizes their properties. Such a picture is called a root diagram;
that for SU(3) is shown in Fig. 17.8.
Example 17.7.5
GENERATORS FOR DIRECT PRODUCTS
If we apply an operation R depending on a parameter ϕ to a product of basis functions for
different particles, each function will transform according to its representation, which we

858
Chapter 17 Group Theory
presently assume to be the fundamental representation:
R

ψi(1)ψ j(2)

=

U(R)ψi(1)

U(R)ψ j(2)

=

eiϕS(1)ψi(1)

eiϕS(2)ψ j(2)

= eiϕ[S(1)+S(2)]ψi(1)ψ j(2),
where the notation is supposed to indicate that S(1) acts only on particle 1 and S(2) acts
only on particle 2 (this can be arranged by an appropriate deﬁnition of the direct-product
matrices and the operators to which they correspond). The important point here is that
because generators appear in an exponent, a product of single-particle operations can be
obtained using a sum of single-particle generators. This observation is a generalization
of our earlier writing of resultant multiparticle angular momenta as sums of individual
contributions, and enables us to write, for three-quark products, expressions such as
I± = I±(1) + I±(2) + I±(3);
so, for example (dropping the proportionality constant CI ),
I−u(1)u(2)u(3) = d(1)u(2)u(3) + u(1)d(2)u(3) + u(1)u(2)d(3).
Suppressing the explicit particle numbers, this can be shortened to I−uuu = duu + udu +
uud. Corresponding results apply to all the other ladder operators and to all three-quark
products, and to the application of the diagonal generators, such as
S3 u(1)u(2)u(3) =

S3(1)u(1)

u(2)u(3) + u(1)

S3(2)u(2)

u(3)
+ u(1)u(2)

S3(3)u(3)

= 3
2 u(1)u(2)u(3),
or S3 uuu = 3
2 uuu, equivalent to assigning I3 = 3
2 to uuu. Similar analysis can yield
results such as I3 = 1
2 for uud, or (2S8/
√
3)dss = −dss, showing that dss has Y = −1. ■
We are now ready to return to the veriﬁcation of Eq. (17.68).
Example 17.7.6
DECOMPOSITION OF BARYON MULTIPLETS
There are 27 three-quark products, which, using the analysis of Example 17.7.5, have the
(I3,Y) values shown here.
 + 3
2,1

uuu
 + 1
2,1

uud,udu,duu
 −1
2,1

udd,dud,ddu
 −3
2,1

ddd
(+1,0)
uus,usu,suu
(0,0)
uds,dus,usd,dsu,sud,sdu
(−1,0)
dds,dsd,sdd
 + 1
2,−1

uss,sus,ssu
 −1
2,−1

dss,sds,ssd
(0,−2)
sss

17.7 Continuous Groups
859
We can ﬁnd the irreducible representations in our direct product in a relatively mechani-
cal way. We start by placing the 27 quark products at their coordinate positions in an I3,Y
diagram. We note that the point ( 3
2,1) is occupied by only one product, uuu, so it must, by
itself, be a member of some irreducible representation of SU3. Starting there, we may take
steps in any of the directions indicated in the root diagram, providing there is a function
at each point to which we move. Since all we are doing is identifying possible states, we
need not make any sophisticated computations as we proceed. Since uuu is completely
symmetric under permutations, the basis function at each point will be a symmetric sum
of the products at each point reached. When we have reached all the points, we will have
identiﬁed a total of 10 basis functions, all members of the same irreducible representation,
the one we called 10. This set of 10 basis functions is called a decuplet. The graph for
these basis functions, called a weight diagram, is shown in Fig. 17.9.
At the points where there was more than one quark product, there will be products left
over after accounting for 10; if we want to be quantitative, they will be linear combinations
that are orthogonal to the symmetric forms used in 10. Continuing with either of the two
leftover functions at ( 1
2,1), we may construct another set of basis functions from the left-
overs; these sets will contain eight members, with the weight diagram shown in Fig. 17.10.
(There are only seven points still occupied in the diagram, but the one at (0,0) yields two
different functions when approached from different directions; the function obtained when
(0,0) is reached horizontally can, via a subgroup analysis, be related to the members of its
representation at (±1,0). These points are elaborated in Exercise 17.7.4.) After account-
ing for these two octets, corresponding to representations 8, there will be one completely
antisymmetric function left at (0,0); it is a basis for 1.
■
Both the representations 8 and 10 are relevant for particle physics. The rationalization
of the similar-mass baryon octet was based on assignment of those particles to members
of 8, with the small mass differences associated with the breaking of the strong-interaction
symmetry by a weaker force which retained some of the SU(2) subgroup symmetries, and
by the (weaker still) electromagnetic forces that also broke the SU(2) symmetries. The
identiﬁcation of the octet members with the basis functions of 8 is included in Fig. 17.10,
and the energetics of the overall situation is indicated schematically in Fig. 17.11.
−1
0
1
−2
−1
0
1
I3
Ω −
Ξ∗−
Ξ∗0
Σ∗+
Σ∗−
Δ−
Δ+
Δ++
Δ0
Σ∗0
Y
FIGURE 17.9
Weight diagram, baryon decuplet. The symbols at the various points are
the names of particles assigned to the basis.

860
Chapter 17 Group Theory
−1/2
1/2
−1
1
−1
0
1
I3
Ξ0
Ξ −
Σ −
Σ +
Σ0, Λ
Y
n
p
FIGURE 17.10
Weight diagram, baryon octet.
Mass
N
Ξ−
Λ
Σ
Ξ
Hstrong
Hstrong
+ Hmedium
Hstrong
+ Hmedium
+ Helectromagnetic
Λ
n
p
Σ−
Σ+
Σ0
Ξ0
FIGURE 17.11
Baryon mass splitting.
The representation 10 provides an explanation for the set of 10 excited-state baryons
whose weight diagram is shown in Fig. 17.9. When Gell-Mann ﬁtted the then existent
data to the decuplet representation, the −particle had not yet been discovered, and its
prediction and subsequent detection provided a strong indication of the relevance of SU(3)

17.7 Continuous Groups
861
to physics. Yet another instance of the importance of SU(3) is provided by the existence
of a meson octet (displaced by one unit in Y relative to the primary baryon octet).
Finally, we caution the reader that the foregoing discussion is by no means complete.
It does not take full account of fermion antisymmetry requirements, the consideration of
which led to the SU(3)-color gauge theory of the strong interaction called quantum chro-
modynamics (QCD). QCD also, at a minimum, involves the group SU(3). We have also
left much unsaid about subgroup decompositions of the overall symmetry group, qualita-
tively alluded to in the discussion supporting Fig. 17.9.
To keep group theory and its very real value in proper perspective, we should empha-
size that group theory identiﬁes and formalizes symmetries. It classiﬁes (and sometimes
predicts) particles. But apart from saying, e.g., that one part of the Hamiltonian has SU(2)
symmetry and another part has SU(3) symmetry, group theory says nothing about the par-
ticle interaction. Likewise, a spherically symmetric Hamiltonian has (in ordinary space)
SO(3) symmetry, but this fact tells us nothing about the radial dependence of either the
potential or the wave function.
Exercises
17.7.1
Determine three SU(2) subgroups of SU(3).
17.7.2
Prove that the matrices U(n) (unitary matrices of order n) form a group, and that SU(n)
(those with determinant unity) form a subgroup of U(n).
17.7.3
Using Eq. (17.56) for the matrix elements of SU(2) corresponding to rotations about
the coordinate axes, ﬁnd the matrix corresponding to a rotation deﬁned by Euler angles
(α,β,γ ). The Euler angles are deﬁned in Section 3.4.
17.7.4
For a product of three quarks, the member of SU(3) representation 10 with (I3,Y) =
(+ 3
2,1) is uuu.
(a)
Apply operators in the root diagram for SU(3), Fig. 17.8, to obtain all the remain-
ing members of the decuplet comprising the representation 10.
(b)
The two representations 8 can be chosen to have for I3 = 1
2, Y = 1 the respective
members ψ1
  1
2,1

= (ud −du)u and ψ2
  1
2,1

= 2uud −udu −duu. Brieﬂy
explain why this choice is possible.
(c)
Using the operators in the root diagram and the above ψ1
  1
2,1

, ﬁnd expressions
for ψ1
 −1
2,1

, ψ1(−1,0), ψ1(1,0), ψ1
 −1
2,−1

, and ψ1
  1
2,−1

.
(d)
Taking each of the six ψ1 functions you now have, apply an operator that will
convert it into ψ1(0,0). Show that you obtain exactly two linearly independent
ψ1(0,0), thereby justifying the claim that the ψ1 are an octet at the points shown
in Fig. 17.10.
(e)
Show that the octet built starting from ψ2( 1
2,1) is linearly independent from that
built from ψ1.
(f)
Find the wave function ψ(0,0) that is linearly independent of all the ψ(0,0) func-
tions found in parts (a)–(e). It is the sole member of the representation 1.

862
Chapter 17 Group Theory
17.8
LORENTZ GROUP
It has long been accepted that the laws of physics should be covariant, meaning that they
should have forms that are (1) independent of the origin of the coordinates used to describe
them (leading from an isolated system to the law of conservation of linear momentum);
(2) independent of the orientation of our coordinates (leading to a conservation law for
angular momentum); and (3) independent of the zero from which time is measured. Most
of our experience suggests that velocities should add like ordinary vectors; for example,
a person walking toward the front of a moving train would, as viewed by a stationary
observer, have a net velocity equal to the sum of that of the train and the walker’s veloc-
ity relative to the train. This rule for velocity addition is identiﬁed as Galilean, and is
correct in the limit of small velocities. However, it is now known that transformations
between coordinate systems with a constant nonzero relative velocity must lead to a non-
intuitive velocity addition law that causes the velocity of light to be the same as measured
by observers in all coordinate systems (reference frames). As Einstein showed in 1905,
the necessary velocity addition law could be obtained if coordinate-system changes were
described by Lorentz transformations. Einstein’s theory, now known as special relativ-
ity (its extension to curved space-time to describe gravitation is called general relativity),
also helped to complete an understanding of the way in which electric and magnetic phe-
nomena become interconverted when charges at rest in one coordinate system are viewed
as moving in another.
The transformations that are consistent with the symmetry of space-time form a group
known as the inhomogeneous Lorentz group or the Poincaré group. The Poincaré group
consists of space and time displacements and all Lorentz transformations; here we shall
only discuss the Lorentz transformations, which by themselves form the Lorentz group,
sometimes for clarity referred to as the homogeneous Lorentz group.
Homogeneous Lorentz Group
Lorentz transformations can be likened to rotations that affect both the spatial and the time
coordinates. An ordinary spatial rotation about the origin, in which (x1, x2) →(x′
1, x′
2), has
the property that the length of the associated vector is unchanged by the rotation, so that
x2
1 + x2
2 = x′2
1 + x′2
2 . But we now consider transformations involving a spatial coordinate
(let’s choose z) and a time coordinate t, but with z2 −c2t2 = z′2 −c2t′2, so that the
velocity of light, c, computed for travel from the origin (0,0) to (z,t) will be the same as
that for travel from the origin to (z′,t′). We are therefore abandoning the notion that the
time variable is universal, assuming instead that it changes together with changes in the
spatial variable(s) in a way that keeps the velocity of light constant. We also see that it is
natural to rescale the t coordinate to x0 = ct, so that the invariant of the transformation
becomes z2 −x2
0.
Let’s now examine a situation in which the coordinate system is moving in the +z
direction at an inﬁnitesimal velocity c δρ (so that a Galilean transformation applies to z):
z′ = z −c(δρ)t = z −(δρ)x0.
But we assume that t also changes, to
t′ = t −a(δρ)z,
or
x′
0 = x0 −ac(δρ)z,

17.8 Lorentz Group
863
with a chosen to keep z2 −x2
0 constant to ﬁrst order in δρ. The value of a that satisﬁes this
requirement is a = +1/c, so our inﬁnitesimal Lorentz transformation is
 x′
0
z′
!
=
 1
−δρ
−δρ
1
! x0
z
!
=
"
12 −δρ
 0
1
1
0
!# x0
z
!
.
To identify this equation in terms of a generator, we note that
−δρ
 
0
1
1
0
!
= i(δρ)S,
or
S = i
 
0
1
1
0
!
= iσ 1,
(17.69)
where σ 1 is a Pauli matrix. Extending now to a ﬁnite velocity just as we did for ordinary
rotations in the passage from Eq. (17.35) to Eq. (17.36), we have an expression that is
similar to Eq. (17.39), except that we now have σ 1 instead of σ 2, while in place of ϕ we
now have iρ. The result is
U(ρ) = exp(iρ[iσ 1]) = cos(iρ) + iσ 1 sin(iρ) = cosh(ρ) −σ 1 sinh(ρ)
=
 
coshρ
−sinhρ
−sinhρ
coshρ
!
.
(17.70)
While δρ was an inﬁnitesimal velocity (in units of c), it does not follow that ρ, the result
of repeated δρ transformations, is proportional to the resultant velocity in the ﬁnal trans-
formed coordinates. However, from the equation z′ = z coshρ −x0 sinhρ, we identify the
resultant velocity as v = c sinhρ/coshρ = c tanhρ.
Summarizing, and introducing the symbols usually used in relativistic mechanics, we
identify
β ≡v
c ,
tanhρ = β,
coshρ =
1
p
1 −β2 ≡γ,
sinhρ = βγ.
(17.71)
The range of ρ (sometimes called the rapidity) is unlimited, but tanhρ < 1, thereby show-
ing that c is an upper limit to v (which cannot be reached for ﬁnite ρ).
A Lorentz transformation that does not also involve a spatial rotation is known as a
boost or a pure Lorentz transformation. Successive boosts can be analyzed using the
group property of the Lorentz transformations: A boost of rapidity ρ followed by another,
of rapidity ρ′, both in the z direction, must have transformation matrix
U(ρ′)U(ρ) =
 coshρ′
−sinhρ′
−sinhρ′
coshρ′
! coshρ
−sinhρ
−sinhρ
coshρ
!
=
 
coshρ′ coshρ + sinhρ′ sinhρ
−coshρ′ sinhρ −sinhρ′ coshρ
sinhρ′ coshρ −coshρ′ sinhρ
sinhρ′ sinhρ + coshρ′ coshρ
!
=
 
cosh(ρ + ρ′)
−sinh(ρ + ρ′)
−sinh(ρ + ρ′)
cosh(ρ + ρ′)
!
= U(ρ + ρ′),

864
Chapter 17 Group Theory
showing that the rapidity (not the velocity) is the additive parameter for successive boosts
in the same direction. The result we have just obtained is obvious if we write it in the
generator notation; it is
U(ρ′)U(ρ) = exp(−ρ′σ 1)exp(−ρσ 1) = exp(−(ρ′ + ρ)σ 1) = U(ρ′ + ρ).
(17.72)
Because of the group property, successive boosts in different spatial directions must
yield a resultant Lorentz transformation, but the result is not equivalent to any single boost,
and corresponds to a boost plus a spatial rotation. This rotation is the origin of the Thomas
precession that arises in the treatment of spin-orbit coupling terms in atomic and nuclear
physics. A good discussion of the Thomas precession frequency is in the work by Goldstein
(Additional Readings).
Example 17.8.1
ADDITION OF COLLINEAR VELOCITIES
Let’s now apply Eq. (17.72) to two successive boosts in the z direction, identifying each by
its individual velocity (v′ for the ﬁrst boost, v′′ for the second), or equivalently β′ = v′/c,
β′′ = v′′/c. The corresponding rapidities will be denoted ρ′ and ρ′′, so
tanhρ′ = β′ = v′
c ,
tanhρ′′ = β′′ = v′′
c .
The resultant of the two successive boosts will have rapidity ρ = ρ′+ρ′′, and will therefore
be associated with a resultant velocity v satisfying tanh(ρ′ + ρ′′) = v/c = β. From the
summation formula for the hyperbolic tangent, we have
v
c = β = tanh(ρ′ + ρ′′) = tanhρ′ + tanhρ′′
1 + tanhρ′ tanhρ′′ =
v′
c + v′′
c
1 + v′v′′
c2
= β′ + β′′
1 + β′β′′ .
(17.73)
Equation (17.73) shows that when v′ and v′′ are both small compared to c, the velocity
addition is approximately Galilean, becoming exactly Galilean in the small-velocity limit.
But as the individual velocities increase, their resultant decreases relative to their arithmetic
sum, and never exceeds c. This behavior is to be expected, since (for real arguments) the
hyperbolic tangent cannot exceed unity.
■
Minkowski Space
If we make the deﬁnition x4 = ict, the formulas we have just obtained, and many others as
well, can be written in a systematic form that does not have minus signs explicitly present
for the time coordinate. Then Lorentz transformations act like rotations in a space with
basis (x1, x2, x3, x4), and the conserved quantity is x2
1 + x2
2 + x2
3 + x2
4. This approach is
appealing and is widely used.
An alternative way to proceed, which has the disadvantage of being a bit more cum-
bersome, but with the advantage of providing a framework suitable for the extension to
general relativity, is to use real coordinates (as was done in the preceding subsection), but
to handle the difference in behavior of the spatial and time coordinates by introducing a
suitably deﬁned metric tensor. One possibility (for basis x0 = ct, x1, x2, x3), where xi

17.8 Lorentz Group
865
(i = 1, 2, 3) are Cartesian spatial coordinates, is to use the Minkowski metric tensor, ﬁrst
introduced in Example 4.5.2,
(gµν) = (gµν) =


1
0
0
0
0
−1
0
0
0
0
−1
0
0
0
0
−1

,
(17.74)
where it is understood that Greek indices run over the four-index set 0 to 3, and that
displacements are rendered as scalar products of the form xµgµνx′ν or xµgµνx′
ν, where
the repeated indices are understood to be summed (the Einstein summation convention).
Note that because all the analysis in this section is in Cartesian coordinates, the distinction
between contravariant and covariant indices is limited to the insertion of minus signs in
some elements of products that involve the metric tensor.
As was pointed out in Example 4.6.2, this metric tensor sometimes appears with the
signs of all its diagonal elements reversed. Either choice of signs is valid and yields proper
results for problems of physics if used consistently, but trouble can arise if material from
inconsistent sources is combined. The cited example also indicates how Maxwell’s equa-
tions can be written in a manifestly covariant form.
Note that the transformation matrices S and U must be mixed tensors, since they convert
a vector (whether covariant or contravariant) into another vector of the same variance
status. Since for a pure boost these matrices are symmetric, either index can be deemed to
be covariant (the other then being contravariant).
Exercises
17.8.1
Show that in 3 + 1 dimensions (this means three spatial dimension plus time), a boost
in the xy plane at an angle θ from the x direction has, in coordinates (x0, x1, x2, x3),
the generator
S = i


0
cosθ
sinθ
0
cosθ
0
0
0
sinθ
0
0
0
0
0
0
0

.
17.8.2
(a)
Show that the generator in Exercise 17.8.1 produces a Lorentz transformation
matrix for rapidity ρ given by
U(ρ;θ) =


coshρ
−cosθ sinhρ
−sinθ sinhρ
0
−cosθ sinhρ
sin2 θ + cos2 θ coshρ
cosθ sinθ(coshρ −1)
0
−sinθ sinhρ
cosθ sinθ(coshρ −1)
cos2 θ + sin2 θ coshρ
0
0
0
0
1


.
Note. This transformation matrix is symmetric. All single boosts (in any spatial
direction) have symmetric transformation matrices.
(b)
Verify that the transformation matrix of part (a) is consistent with (1) rotating
the spatial coordinates to align the boost direction with a coordinate axis, (2)
performing a boost in the direction of that axis using Eq. (17.70), and (3) rotat-
ing back to the original coordinate system.

866
Chapter 17 Group Theory
17.8.3
Obtain the Lorentz transformation matrix for a boost of ﬁnite amount ρ′ in the x direc-
tion followed by a ﬁnite boost ρ′′ in the y direction. Show that there are no values of ρ
and θ that can bring this transformation to the form given in Exercise 17.8.2.
17.9
LORENTZ COVARIANCE OF MAXWELL’S EQUATIONS
We start our discussion of Lorentz covariance by recalling how the magnetic and electric
ﬁelds B and E depend on the vector and scalar potentials A and ϕ:
B = ∇× A,
(17.75)
E = −∂A
∂t −∇ϕ.
Restricting consideration to situations where ε and µ have their free-space values ε0 and
µ0 (with ε0µ0 = 1/c2), it can be shown that A and ϕ form a four-vector whose components
Aµ (in contravariant form) are
Ai = cε0Ai,
i = 1, 2, 3,
(17.76)
A0 = ε0ϕ.
We now form the tensor Fµλ with elements
Fµλ = ∂Aλ
∂xµ
−∂Aµ
∂xλ
,
(17.77)
which we evaluate (consistently with our choice of Minkowski metric) using
∂
∂x0
= ∂
c∂t ,
∂
∂x1
= −∂
∂x ,
∂
∂x2
= −∂
∂y ,
∂
∂x3
= −∂
∂z .
(17.78)
The resulting form for Fµλ, known as the electromagnetic ﬁeld tensor, is
Fµλ = ε0


0
−Ex
−Ey
−Ez
Ex
0
−cBz
cBy
Ey
cBz
0
−cBx
Ez
−cBy
cBx
0


.
(17.79)
The quantity Fµλ is, as its name implies, a second-order tensor that must have the
transformation properties associated with the Lorentz group. We know this to be the case
because we constructed Fµλ as a linear combination of terms, each of which was the
derivative of a four-vector; differentiation of a vector (in a Cartesian system) generates a
second-order tensor.
An interesting aside to the above analysis is provided by the discussion of Maxwell’s
equations in the language of differential forms. In Example 4.6.2 we showed that the dif-
ferential form
F = −Ex dt ∧dx −Ey dt ∧dy −Ez dt ∧dz + Bx dy ∧dz + By dz ∧dx + Bz dx ∧dy

17.9 Lorentz Covariance of Maxwell’s Equations
867
was a starting point from which Maxwell’s equations could be derived; we now observe
that the individual terms of this differential form correspond to the elements of the tensor
under discussion here.
Lorentz Transformation of E and B
Returning to the main matter of present concern, we now apply a Lorentz transformation
to Fµλ. For simplicity we take a pure boost in the z direction, which will have matrix
elements similar to those of Eq. (17.70); using the notations introduced in Eq. (17.71), our
transformation matrix can be written
U =


γ
0
0
−βγ
0
1
0
0
0
0
1
0
−βγ
0
0
γ


.
(17.80)
Noting that we must apply our Lorentz transformation to both indices of Fµλ, and keeping
in mind that U is symmetric and, as pointed out in Section 17.8, a mixed tensor, we can
write
F′ = UFU,
(17.81)
where F and F′ are both contravariant matrices. If we now compare the individual elements
of F′ with those of F, we obtain formulas for the components of E′ and B′ in terms of the
components of E and B. For the transformation at issue here, the results are (where v is the
velocity of the transformed coordinate system, in the z direction, relative to the original
coordinates):
E′
x = γ
 Ex −βcBy

= γ
 Ex −vBy

,
E′
y = γ
 Ey + βcBx

= γ
 Ey + vBx

,
(17.82)
E′
z = Ez,
B′
x = γ

Bx + β
c Ey

= γ

Bx + v
c2 Ey

,
B′
y = γ

By −β
c Ex

= γ

By −v
c2 Ex

,
(17.83)
B′
z = Bz.
We can generalize the above to a boost v in an arbitrary direction:
E′ = γ (E + v × B) + (1 −γ )Ev,
B′ = γ

B −v × E
c2

+ (1 −γ )Bv,
(17.84)

868
Chapter 17 Group Theory
where Ev = (E · ˆv)ˆv and Bv = (B · ˆv)ˆv are the projections of E and B in the direction of v.
In the limit v ≪c, these equations reduce to
E′ = E + v × B,
B′ = B −v × E
c2
.
(17.85)
Note that the coordinate transformation changes the velocity with which charges move
and therefore changes the magnetic force. It is now clear that the Lorentz transformation
explains how the total force (electric plus magnetic) can be independent of the reference
frame (i.e., the relative velocities of the coordinate systems). In fact, the need to make the
total electromagnetic force independent of the reference frame was ﬁrst noted by Lorentz
and Poincaré. This was where Lorentz transformations were ﬁrst recognized as relevant for
physics, and that may have provided Einstein with a clue as he developed his formulation
of special relativity.
Example 17.9.1
TRANSFORMATION TO BRING CHARGE TO REST
Consider a charge q moving at a velocity v, with v ≪c. By giving the coordinate system
a boost v, we transform to a frame in which the charge is at rest and experiences only an
electric force qE′. But since the total force is independent of the reference frame, it is also
given, according to Eq. (17.86), as
F = q(E + v × B),
(17.86)
which is just the classical Lorentz force.
■
The ability to write Maxwell’s equations in a tensor form that gives the experimentally
observed results under Lorentz transformation is an important achievement because it guar-
antees that the formulation is consistent with special relativity. This is one of the reasons
that modern theories of quantum electrodynamics and elementary particles are often writ-
ten in this manifestly covariant form. Conversely, the insistence on such a tensor form
has been a useful guide in the construction of these theories.
We close with the following general observations:
The Lorentz group is the symmetry group of electrodynamics, of the electroweak
gauge theory, and of the strong interactions described by quantum chromo-
dynamics. It appears necessary that mechanics in general have the symmetry of
the Lorentz group, and that requirement corresponds to the general applicability of
special relativity. With respect to electrodynamics, the Lorentz symmetry explains
the fact that the velocity of light is the same in all inertial frames, and it explains
how electric and magnetic forces are interrelated and yield physical results that
are frame-independent. While a detailed study of relativistic mechanics is beyond
the scope of this book, the extension to special relativity of Newton’s equations of
motion is straightforward and leads to a variety of results, some of which challenge
human intuition.

17.10 Space Groups
869
Exercises
17.9.1
Apply the Lorentz transformation of Eq. (17.80) to Fµλ as given in Eq. (17.79). Verify
that the result is a matrix F′ whose elements conﬁrm the results given in Eqs. (17.82)
and (17.83).
17.9.2
Conﬁrm that the generalization of Eqs. (17.82) and (17.83) to a boost corresponding to
an arbitrary velocity v is properly given by Eq. (17.84).
17.10
SPACE GROUPS
Perfect crystals exhibit translational symmetry, meaning that they can be considered as a
space-ﬁlling array of parallelepipeds stacked end-to-end and side-to-side, with each con-
taining an identical set of identically placed atoms. A single parallelepiped is referred to as
the unit cell of the crystal; a unit cell can be speciﬁed by giving the vectors that deﬁne its
edges. Calling these vectors h1, h2, h3, equivalent points in any two unit cells are separated
from each other by vectors
b = n1h1 + n2h2 + n3h3,
where n1, n2, n3 can be any integers (positive, negative, or zero). The set of these equiva-
lent points is called the Bravais lattice of the crystal.
A Bravais lattice will have a symmetry that depends on the angles and relative lengths
of the lattice vectors; in three dimensions there are 14 different symmetries possible for
Bravais lattices. There are 32 3-D point groups that are symmetry-compatible with at least
one Bravais lattice; these are called crystallographic point groups to distinguish them
from the inﬁnite number of point groups that can exist in the absence of any compatibility
requirement.
Example 17.10.1
TILING A FLOOR
To understand the notion of crystallographic point group, consider what would happen
(in two dimensions) if we try to tile a ﬂoor with identical tiles in the shape of a regular
polygon. We will have success with squares and triangles, and even with hexagons. These
work because an integer number of tiles can be placed so that they have vertices at the
same point. A triangle has an internal angle of 60◦, so six of them can meet at a point;
similarly, four squares can meet at a point, as can three hexagons (internal angle 120◦).
But we cannot tile with regular pentagons (internal angle 108◦) or any regular polygon
with more than six sides.
■
Combining Bravais lattices and compatible point groups, there is a total of 230 different
groups in 3-D that exhibit translational symmetry and some sort of point-group symmetry.
These 230 groups are called space groups. Their study and use in crystallography (e.g., to
determine the detailed structure of a crystal from its x-ray scattering) is the topic of several
of the larger books in the Additional Readings.
Systems with periodicity in only one or two dimensions also exist in nature; some lin-
ear polymers are 1-D periodic systems; surface systems and single-layer arrays such as
graphene (a macroscopic hexagonal array of carbon atoms) exhibit periodicity in two

870
Chapter 17 Group Theory
dimensions. There is even a kind of translational symmetry that involves elements that
form helical structures. The recognition of this type of symmetry in crystallographic stud-
ies of DNA was the key contribution leading to the discovery that DNA existed as a dou-
ble helix.
Additional Readings
Buerger, M. J., Elementary Crystallography. New York: Wiley (1956). A comprehensive discussion of crystal
symmetries. Buerger develops all 32 point groups and all 230 space groups. Related books by this author
include Contemporary Crystallography. New York: McGraw-Hill (1970); Crystal Structure Analysis. New
York: Krieger (1979) (reprint, 1960); and Introduction to Crystal Geometry. New York: Krieger (1977)
(reprint, 1971).
Burns, G., and A. M. Glazer, Space Groups for Solid-State Scientists. New York: Academic Press (1978). A
well-organized, readable treatment of groups and their application to the solid state.
de-Shalit, A., and I. Talmi, Nuclear Shell Model. New York: Academic Press (1963). We adopt the Condon-
Shortley phase conventions of this text.
Falicov, L. M., Group Theory and Its Physical Applications. Notes compiled by A. Luehrmann. Chicago: Uni-
versity of Chicago Press (1966). Group theory, with an emphasis on applications to crystal symmetries and
solid-state physics.
Gell-Mann, M., and Y. Ne’eman, The Eightfold Way. New York: Benjamin (1965). A collection of reprints of
signiﬁcant papers on SU(3) and the particles of high-energy physics. Several introductory sections by Gell-
Mann and Ne’eman are especially helpful.
Goldstein, H., Classical Mechanics, 2nd ed. Reading, MA: Addison-Wesley (1980). Chapter 7 contains a short
but readable introduction to relativity from a viewpoint consonant with that presented here.
Greiner, W., and B. Müller, Quantum Mechanics Symmetries. Berlin: Springer (1989). We refer to this textbook
for more details and numerous exercises that are worked out in detail.
Hamermesh, M., Group Theory and Its Application to Physical Problems. Reading, MA: Addison-Wesley (1962).
A detailed, rigorous account of both ﬁnite and continuous groups. The 32 point groups are developed. The
continuous groups are treated, with Lie algebra included. A wealth of applications to atomic and nuclear
physics.
Hassani, S., Foundations of Mathematical Physics. Boston: Allyn and Bacon (1991).
Heitler, W., The Quantum Theory of Radiation, 2nd ed. Oxford: Oxford University Press (1947), reprinting,
Dover (1983).
Higman, B., Applied Group-Theoretic and Matrix Methods. Oxford: Clarendon Press (1955). A rather complete
and unusually intelligible development of matrix analysis and group theory.
Jackson, J. D., Classical Electrodynamics, 3rd ed. New York: Wiley (1998).
Messiah, A., Quantum Mechanics, vol. II. Amsterdam: North-Holland (1961).
Panofsky, W. K. H., and M. Phillips, Classical Electricity and Magnetism, 2nd ed. Reading, MA: Addison-
Wesley (1962). The Lorentz covariance of Maxwell’s equations is developed for both vacuum and material
media. Panofsky and Phillips use contravariant and covariant tensors.
Park, D., Resource letter SP-1 on symmetry in physics. Am. J. Phys. 36: 577–584 (1968). Includes a large selec-
tion of basic references on group theory and its applications to physics: atoms, molecules, nuclei, solids, and
elementary particles.
Ram, B., Physics of the SU(3) symmetry model. Am. J. Phys. 35: 16 (1967). An excellent discussion of the
applications of SU(3) to the strongly interacting particles (baryons). For a sequel to this see R. D. Young,
Physics of the quark model. Am. J. Phys. 41: 472 (1973).
Tinkham, M., Group Theory and Quantum Mechanics. New York: McGraw-Hill (1964), reprinting, Dover
(2003). Clear and readable.
Wigner, E. P., Group Theory and Its Application to the Quantum Mechanics of Atomic Spectra (translated by J.
J. Grifﬁn). New York: Academic Press (1959). This is the classic reference on group theory for the physicist.
The rotation group is treated in considerable detail. There is a wealth of applications to atomic physics.

CHAPTER 18
MORE SPECIAL FUNCTIONS
In this chapter we shall study four sets of orthogonal polynomials: Hermite, Laguerre, and
Chebyshev1 of the ﬁrst and second kinds. Although these four sets are of less importance
in mathematical physics than are the Bessel and Legendre functions of Chapters 14 and 15,
they are used and therefore deserve attention. For example, Hermite polynomials occur in
solutions of the simple harmonic oscillator of quantum mechanics and Laguerre polynomi-
als in wave functions of the hydrogen atom. Because the general mathematical techniques
duplicate those used for Bessel and Legendre functions, the development of these functions
is only outlined. Detailed proofs are for the most part left to the reader.
The sets of polynomials treated in this chapter can be related to the more general quan-
tities known as hypergeometric and conﬂuent hypergeometric functions (solutions of
the hypergeometric ODE). For practical reasons we defer most discussion of these rela-
tionships until we have had an opportunity to deﬁne the hypergeometric functions and
the associated nomenclature. The beneﬁt accruing from the connection to hypergeomet-
ric functions is that the hypergeometic recurrence formulas and other general properties
translate into useful relationships for the polynomial sets that we are presently studying.
We conclude the chapter with a short section on elliptic integrals. Although the impor-
tance of this subject has declined as the power of computers has increased, there are some
physical problems for which they are useful and it is not yet time to eliminate them from
this text.
18.1
HERMITE FUNCTIONS
We start by identifying Hermite functions as solutions of the Hermite ODE,
H′′
n (x) −2x H′
n(x) + 2nHn(x) = 0.
(18.1)
1This is the spelling choice of AMS-55 (for the complete reference, see Abramowitz in Additional Readings). However, various
names, such as Tschebyscheff, are encountered in the literature.
871
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

872
Chapter 18 More Special Functions
Here n is a parameter. When n ≥0 is integral, this ODE will have a solution Hn(x) which
is a polynomial of degree n; these solutions are known as Hermite polynomials.
In the presence of appropriate boundary conditions, the Hermite ODE is a Sturm-
Liouville system; polynomial solutions to such ODEs was the topic of Section 12.1. We
showed there, in Example 12.1.1, that the Hermite polynomials could be generated from
their Rodrigues formula, Eq. (12.17), and that, in turn, a Rodrigues formula can be
obtained from the underlying ODE. We also showed in that same section how we can
go from the Rodrigues formula to a generating function for a given polynomial set, pre-
senting in Table 12.1 a list of generating functions that could be found in this way. That
list included the following generating function for the Hermite polynomials:
g(x,t) = e−t2+2tx =
∞
X
n=0
Hn(x)tn
n!.
(18.2)
Here we elect not to depend on the analysis of Section 12.1 but rather to take the view-
point that Eq. (18.2) can be regarded as a deﬁnition of the Hermite polynomials, thereby
making the present analysis completely self-contained. Accordingly, we proceed by verify-
ing that these polynomials satisfy the Hermite ODE, have the expected Rodrigues formula,
and exhibit the other properties that can be developed starting from the generating function.
Recurrence Relations
Note the absence of a superscript, which distinguishes Hermite polynomials from the unre-
lated Hankel functions. From the generating function we ﬁnd that the Hermite polynomials
satisfy the recurrence relations
Hn+1(x) = 2x Hn(x) −2nHn−1(x)
(18.3)
and
H′
n(x) = 2nHn−1(x).
(18.4)
The Hermite polynomials were used in Example 12.1.2 as a detailed illustration of the
method for obtaining recurrence formulas from generating functions; we summarize the
process here. By differentiating the generating function formula with respect to t we obtain
∂g
∂t = (−2t + 2x)e−t2+2tx =
∞
X
n=0
Hn+1(x)tn
n!,
or
−2
∞
X
n=0
Hn(x)tn+1
n!
+ 2x
∞
X
n=0
Hn(x)tn
n! =
∞
X
n=0
Hn+1(x)tn
n!.
Because this equation must be satisﬁed separately for each power of t, we arrive at
Eq. (18.3). Similarly, differentiation with respect to x leads to
∂g
∂x = 2te−t2+2tx =
∞
X
n=0
H′
n(x)tn
n! = 2
∞
X
n=0
Hn(x)tn+1
n! ,
from which we can obtain Eq. (18.4).

18.1 Hermite Functions
873
The Maclaurin expansion of the generating function
e−t2+2tx =
∞
X
n=0
(2tx −t2)n
n!
= 1 + (2tx −t2) + ···
(18.5)
gives H0(x) = 1 and H1(x) = 2x, and then the recursion formula, Eq. (18.3), permits
the construction of any Hn(x) desired. For convenient reference the ﬁrst several Hermite
polynomials are listed in Table 18.1 and presented graphically in Fig. 18.1.
Special Values
Special values of the Hermite polynomials follow from the generating function for x = 0:
e−t2 =
∞
X
n=0
(−t2)n
n!
=
∞
X
n=0
Hn(0)tn
n!,
Table 18.1
Hermite Polynomials
H0(x) = 1
H1(x) = 2x
H2(x) = 4x2 −2
H3(x) = 8x3 −12x
H4(x) = 16x4 −48x2 + 12
H5(x) = 32x5 −160x3 + 120x
H6(x) = 64x6 −480x4 + 720x2 −120
x
10
8
6
4
2
1
2
0
−2
H2(x)
H1(x)
H0(x)
FIGURE 18.1
Hermite polynomials.

874
Chapter 18 More Special Functions
that is,
H2n(0) = (−1)n (2n)!
n! ,
H2n+1(0) = 0,
n = 0,1,···.
(18.6)
We also obtain from the generating function the important parity relation
Hn(x) = (−1)n Hn(−x)
(18.7)
by noting that Eq. (18.3) yields
g(−x,−t) =
∞
X
n=0
Hn(−x)(−t)n
n!
= g(x,t) =
∞
X
n=0
Hn(x)tn
n!.
Hermite ODE
If we substitute the recursion formula Eq. (18.4) into Eq. (18.3), we can eliminate the index
n −1, obtaining
Hn+1(x) = 2x Hn(x) −H′
n(x).
If we differentiate this recurrence relation and substitute Eq. (18.4) for the index n + 1, we
ﬁnd
H′
n+1(x) = 2(n + 1)Hn(x) = 2Hn(x) + 2x H′
n(x) −H′′
n (x),
which can be rearranged to the second-order Hermite ODE, Eq. (18.1). This completes
the process of establishing the identiﬁcation of the Hermite polynomials obtained from the
generating function as solutions of the Hermite ODE.
Rodrigues Formula
A simple way to generate the Rodrigues formula for the Hermite polynomials starts from
the observations that
g(x,t) = e−t2+2tx = ex2 e−(t−x)2
and
∂
∂t e−(t−x)2 = −∂
∂x e−(t−x)2.
We note that n-fold differentiation of the generating function formula, Eq. (18.2), followed
by setting t = 0, yields
∂n
∂tn g(x,t)

t=0
= Hn(x),
and we can therefore obtain the Rodrigues formula as
Hn(x) = ∂n
∂tn g(x,t)

t=0
= ex2 ∂n
∂tn e−(t−x)2
t=0
= (−1)nex2 ∂n
∂xn e−(t−x)2
t=0
= (−1)nex2 ∂n
∂xn e−x2.
(18.8)

18.1 Hermite Functions
875
Series Expansion
Starting from the Maclaurin expansion, Eq. (18.5), we can derive our Hermite polynomial
Hn(x) in series form: Using the binomial expansion of (2x −t)ν, we initially get
e−t2+2tx =
∞
X
ν=0
tν
ν!(2x −t)ν =
∞
X
ν=0
tν
ν!
ν
X
s=0
ν
s

(2x)ν−s(−t)s
=
∞
X
ν=0
ν
X
s=0
tν+s
(ν + s)!
(−1)s(ν + s)!(2x)ν−s
(ν −s)! s!
.
Changing the ﬁrst summation index from ν to n = ν +s, and noting that this change causes
the s summation to range from zero to [n/2], the largest integer less than or equal to n/2,
our expansion takes the form
e−t2+2tx =
∞
X
n=0
tn
n!
[n/2]
X
s=0
(−1)sn!
(n −2s)!s!(2x)n−2s,
from which we can read out the formula for Hn:
Hn(x) =
[n/2]
X
s=0
(−1)sn!
(n −2s)! s!(2x)n−2s.
(18.9)
Finally, we note that Hn(x) can be written as a Schlaeﬂi integral. Comparing with
Eq. (12.18),
Hn(x) = n!
2πi
I
t−n−1e−t2+2tx dt.
(18.10)
Orthogonality and Normalization
The orthogonality of the Hermite polynomials is demonstrated by identifying them as aris-
ing in a Sturm-Liouville system. The Hermite ODE, however, is clearly not self-adjoint,
but can be made so by multiplying it by exp(−x2) (see Exercise 8.2.2). With exp(−x2) as
a weighting factor, we obtain the orthogonality integral
∞
Z
−∞
Hm(x)Hn(x)e−x2dx = 0,
m ̸= n.
(18.11)
The interval (−∞,∞) is chosen to obtain the Hermitian operator boundary conditions (see
Section 8.2).
It is sometimes convenient to absorb the weighting function into the Hermite polynomi-
als. We may deﬁne
ϕn(x) = e−x2/2Hn(x),
(18.12)

876
Chapter 18 More Special Functions
with ϕn(x) no longer a polynomial. Substitution into Eq. (18.1) yields the differential equa-
tion for ϕn(x),
ϕ′′
n(x) + (2n + 1 −x2)ϕn(x) = 0.
(18.13)
Equation (18.13) is self-adjoint, and the solutions ϕn(x) are orthogonal on the interval
−∞< x < ∞with a unit weighting function.
We still need to normalize these functions. One approach is to combine two instances of
the generating function formula (using variables s and t), after which we multiply by e−x2
and integrate over x from −∞to ∞. These steps yield
∞
Z
−∞
e−x2e−s2+2sxe−t2+2txdx =
∞
X
m,n=0
smtn
m!n!
∞
Z
−∞
e−x2 Hm(x)Hn(x)dx.
(18.14)
We next note that the exponentials on the left-hand side of Eq. (18.14) can be combined
into e2ste−(x−s−t)2, after which the integral can be evaluated:
∞
Z
−∞
e−x2e−s2+2sxe−t2+2txdx = e2st
∞
Z
−∞
e−(x−s−t)2dx = π1/2e2st.
Inserting this result into Eq. (18.14) after expanding it in a power series, we get
π1/2e2st = π1/2
∞
X
n=0
2nsntn
n!
=
∞
X
m,n=0
smtn
m!n!
∞
Z
−∞
e−x2 Hm(x)Hn(x)dx.
By equating coefﬁcients of equal powers of s and t, we both conﬁrm the orthogonality and
obtain the normalization integral
∞
Z
−∞
e−x2h
Hn(x)
i 2
dx = 2nπ1/2n!.
(18.15)
Exercises
18.1.1
Assume that the Hermite polynomials are known to be solutions of the Hermite ODE,
Eq. (18.1). Assume further that the recurrence relation, Eq. (18.3), and the values of
Hn(0) are also known. Given the existence of a generating function
g(x,t) =
∞
X
n=0
Hn(x)tn
n!,
(a)
Differentiate g(x,t) with respect to x and using the recurrence relation develop a
ﬁrst-order PDE for g(x,t).

18.1 Hermite Functions
877
(b)
Integrate with respect to x, holding t ﬁxed.
(c)
Evaluate g(0,t) using the known values of Hn(0).
(d)
Finally, show that g(x,t) = exp(−t2 + 2tx).
18.1.2
In developing the properties of the Hermite polynomials, start at a number of different
points, such as:
1.
Hermite’s ODE, Eq. (18.1),
2.
Rodrigues’s formula, Eq. (18.8),
3.
Integral representation, Eq. (18.10),
4.
Generating function, Eq. (18.2),
5.
Gram-Schmidt construction of a complete set of orthogonal polynomials over
(−∞,∞) with a weighting factor of exp(−x2) (Section 5.2).
Outline how you can go from any one of these starting points to all the other points.
18.1.3
Prove that |Hn(x)| ≤|Hn(ix)|.
18.1.4
Rewrite the series form of Hn(x), Eq. (18.9), as an ascending power series.
ANS.
H2n(x) = (−1)n
n
X
s=0
(−1)2s(2x)2s
(2n)!
(2s)!(n −s)!,
H2n+1(x) = (−1)n
n
X
s=0
(−1)s(2x)2s+1
(2n + 1)!
(2s + 1)!(n −s)!.
18.1.5
(a)
Expand x2r in a series of even-order Hermite polynomials.
(b)
Expand x2r+1 in a series of odd-order Hermite polynomials.
ANS.
(a) x2r = (2r)!
22r
r
X
n=0
H2n(x)
(2n)!(r −n)!
(b) x2r+1 = (2r + 1)!
22r+1
r
X
n=0
H2n+1(x)
(2n + 1)!(r −n)!,
r = 0,1,2,... .
Hint. Use a Rodrigues representation and integrate by parts.
18.1.6
Show that
(a)
∞
Z
−∞
Hn(x)exp

−x2
2

dx =
(2πn!/(n/2)!, n even
0, n odd.
(b)
∞
Z
−∞
x Hn(x)exp

−x2
2

dx =



0, n even
2π
(n + 1)!
((n + 1)/2)!, n odd.

878
Chapter 18 More Special Functions
18.1.7
(a)
Using the Cauchy integral formula, develop an integral representation of Hn(x)
based on Eq. (18.2) with the contour enclosing the point z = −x.
ANS.
Hn(x) = n!
2πi ex2 I
e−z2
(z + x)n+1 dz.
(b)
Show by direct substitution that this result satisﬁes the Hermite equation.
18.2
APPLICATIONS OF HERMITE FUNCTIONS
One of the most important applications of Hermite functions in physics arises from the
fact that the functions ϕn(x) of Eq. (18.12) are the eigenstates of the quantum-mechanical
simple harmonic oscillator, which describes motion subject to a quadratic (also known as
a harmonic or a Hooke’s-law) potential. This fact causes Hermite polynomials not only
to appear in elementary quantum-mechanics problems, but also in analyses of the vibra-
tional states of molecules, where the lowest-order description of the interatomic potential
is harmonic. In view of the importance of these topics, we now proceed to examine them
in some detail.
Simple Harmonic Oscillator
The quantum mechanical simple harmonic oscillator is governed by a Schrödinger equa-
tion of the form
−¯h2
2m
d2ψ(z)
dz2
+ k
2 z2ψ(z) = Eψ(z),
(18.16)
where m is the mass of the oscillator, k is the force constant for its Hooke’s law force
directed toward z = 0, ¯h is Planck’s constant divided by 2π, and E is an eigenvalue giv-
ing the energy of the oscillator. Equation (18.16) is to be solved subject to the boundary
condition that ψ(z) vanish at z = ±∞. It is convenient to make a change of variable that
eliminates the various constants from the equation, and we therefore make the substitutions
z =
¯h1/2 x
(km)1/4 ,
k
2 z2 = ¯h
2
r
k
m x2,
¯h2
2m
d2
dz2 = ¯h
2
r
k
m
d2
dx2 ,
which converts Eq. (18.16) into
−1
2
d2ϕ(x)
dx2
+ x2
2 ϕ(x) = λϕ(x),
(18.17)
with boundary conditions at x = ±∞. The eigenvalue λ in this equation is related to E by
E = ¯hλ
r
k
m .
(18.18)
The solutions of Eq. (18.17) that satisfy the boundary conditions can now be identiﬁed as
given by Eq. (18.13), and we can identify λn, the eigenvalue of Eq. (18.17) corresponding
to ϕn(x), as having the value n + 1
2. Turning to Eq. (18.12), and expressing x in terms

18.2 Applications of Hermite Functions
879
of the original variable z, the eigenstates of Eq. (18.16) can be characterized (including a
normalization constant Nn) as
ψn(z) = Nne−(αz)2/2Hn(αz),
En = (n + 1
2)¯h
r
k
m ,
α =
¯h1/2
(km)1/4 ,
(18.19)
with n restricted to the integer values 0, 1, 2, ···. The normalization constant can be
deduced from Eq. (18.15). Noting that the normalization integral is to be over the vari-
able z, we ﬁnd it to be
Nn =

α
2nπ1/2n!
1/2
.
(18.20)
It is of interest to examine a few of the eigenstates of this oscillator problem. For refer-
ence, a classical oscillator of mass m and force constant k will have the angular oscillation
frequency
ωclass =
r
k
m ,
and can have an arbitrary energy of oscillation, while our quantum oscillator is restricted to
oscillation energies (n + 1
2)¯hωclass, with n a nonnegative integer. We note that the quantum
oscillator must have at least the total energy 1
2 ¯hωclass; this is usually referred to as its
zero-point energy and is a consequence of the fact that its spatial distribution must be
described by a wave function of ﬁnite extent.
The three lowest-energy eigenfunctions of the quantum oscillator are shown in Fig. 18.2.
We note that these wave functions predict a position distribution that extends to ±∞,
albeit with exponentially decaying amplitude for larger |z|. The corresponding classical
oscillator will have excursions in z that are strictly bounded by kz2
max/2 = E, where E
can be assigned any value greater than or equal to zero. We have marked in Fig. 18.2
the excursion range of a classical oscillator with an energy equal to the eigenvalue of the
quantum oscillator; note that the exponential decay of the quantum wave function begins
at the ends of the classical range.
Operator Approach
While the analysis of the preceding subsection is straightforward and provides a complete
set of eigenstates for the simple quantum oscillator, additional insight can be obtained
by an alternative approach that uses the commutation and other algebraic properties of
the quantum-mechanical operators. Our starting point for this development is the recogni-
tion that the differential operator −d2/dx2 of Eq. (18.17) arose as a representation of the
dynamical quantity p2, where (in units with ¯h = 1) p ←→−i d/dx. Then our Schrödinger
equation of Eq. (18.17) can be written
Hϕ = p2 + x2
2
ϕ = λϕ,
(18.21)
where H is the Hamiltonian operator, with eigenvalues λ.

880
Chapter 18 More Special Functions
0.5
5
x
ψ2(x)
0.5
5
5
ψ1(x)
ψ0(x)
x
x
FIGURE 18.2
Quantum mechanical oscillator wave functions. The heavy bar on the
x-axis indicates the allowed range of the classical oscillator with the same total energy.
The key to an approach starting from Eq. (18.21) is that x and p satisfy the basic com-
mutation relation
[x, p] = xp −px = i,
(18.22)
a result discussed in detail in the analysis leading to Eq. (5.43). In fact, if we proceed
under the assumption that Eq. (18.22) is all that we know about x and p, there is the
additional advantage that any results we obtain will be more general than those from our
original oscillator problem in ordinary space. This observation underlies much recent work
in which physical theory has evolved in more abstract directions.
With a knowledge of the way in which angular momentum theory was developed in
terms of raising and lowering operators, one can easily motivate a somewhat similar

18.2 Applications of Hermite Functions
881
procedure here, by deﬁning the two operators
a = 1
√
2
(x + ip),
a† = 1
√
2
(x −ip).
(18.23)
Since we typically use a to denote a constant, we remind the reader that in the present
development it is an operator (involving x and d/dx). With suitable Sturm-Liouville
boundary conditions, x and p are both Hermitian. But the presence of the imaginary unit
i causes a not to be Hermitian, and (as indicated by the notation) changing the sign of the
term ip converts a into its adjoint, a†.
Our ﬁrst use of Eq. (18.23) is to form a†a and aa†:
a†a = 1
2 (x −ip)(x + ip) = 1
2 (x2 + p2) + i
2 (xp −px) = H + i
2[x, p] = H −1
2,
aa† = 1
2 (x + ip)(x −ip) = 1
2 (x2 + p2) −i
2 (xp −px) = H −i
2[x, p] = H + 1
2.
From these equations we obtain the useful formulas
H = a†a + 1
2,
(18.24)
[a,a†] = aa† −a†a = 1,
(18.25)
and therefrom
[H,a] = [a†a + 1
2,a] = [a†a,a] = a†aa −aa†a = (a†a −aa†)a = −a.
(18.26)
Applying [H,a] to an eigenfunction ϕn with eigenvalue λn (assumed not yet known), we
write
[H,a]ϕn = H(aϕn) −aHϕn = H(aϕn) −λn(aϕn) = −(aϕn),
which we easily rearrange to the form
H(aϕn) = (λn −1)(aϕn).
(18.27)
Equation (18.27) shows that we can interpret a as a lowering operator that converts an
eigenfunction with eigenvalue λn into another eigenfunction that has eigenvalue λn −1.
A similar analysis, left to the reader, shows that from the commutator [H,a†] we ﬁnd a†
to be a raising operator, according to
H(a†ϕn) = (λn + 1)(a†ϕn).
(18.28)
These formulas show that, given any eigenfunction ϕn, we can construct a ladder of eigen-
states whose eigenvalues differ by unit steps. The only limitation that would terminate the
construction of an inﬁnite ladder would be the possibility that for some ϕn, either aϕn or
a†ϕn might be zero.
To investigate the circumstances under which aϕn might vanish, let’s form the scalar
product ⟨aϕn|aϕn⟩. We ﬁnd
⟨aϕn|aϕn⟩= ⟨ϕn|a†a|ϕn⟩= ⟨ϕn|H −1
2|ϕn⟩= ⟨ϕn|λn −1
2|ϕn⟩.
(18.29)

882
Chapter 18 More Special Functions
Equation (18.29) shows that only if λn = 1
2 will we have aϕn = 0. That equation also shows
that if λn < 1
2 we have the mathematical inconsistency that the norm of aϕn is predicted
to be negative. These observations together imply that the only possible values of λn are
positive half-integers, as otherwise by repeated application of the lowering operator a we
can move to a λ value prohibited by Eq. (18.29). We leave to the reader the veriﬁcation
that the application of the raising operator a† to any valid ϕn produces a new eigenfunction
a†ϕn with a positive norm.
Our overall conclusion is that any system with a Hamiltonian of the form given by
Eq. (18.21), whether or not represented by an ODE in ordinary space, will have eigenstates
whose eigenvalues form a ladder of unit spacing, with the smallest eigenvalue equal to 1
2.
This makes it natural to label the states ϕn by integers n ≥0, and therefore to write
Hϕn = λnϕn,
λn = n + 1
2,
n = 0, 1, 2 ···,
(18.30)
in agreement with what we found from our original approach; compare Eq. (18.19).
Before leaving this exercise in operator algebra, it may be worth noting that the notion
of raising and lowering operators also arises in contexts where the states thereby reached
can be interpreted as those containing different numbers of particles (or quasiparticles, a
physics jargon that refers to objects, such as photons, whose population is easily changed
by interaction with their surroundings). In such contexts, a raising operator is then often
referred to as a creation operator, with a lowering operator then called an annihilation
(or sometimes a destruction) operator. Obviously these terms have to be interpreted with
an understanding of the underlying physics.
Returning to the description of p as a differential operator, the equation aϕ0 = 0 can
be identiﬁed as a differential equation satisﬁed by the ground (lowest-energy) state of our
oscillator. More speciﬁcally,
√
2 aϕ0 = (x + ip)ϕ0 =

x + i

−i d
dx

ϕ0 =

x + d
dx

ϕ0 = 0,
(18.31)
which has the advantage of being a ﬁrst-order ODE. This ODE is separable, and can be
integrated:
dϕ0
ϕ0
= −x dx,
lnϕ0 = −x2
2 + lnc0,
ϕ0 = c0 e−x2/2,
in agreement with our previous analysis.
Eigenstates for arbitrary n can now be generated by repeated application of a† to ϕ0.
Doing so is left as an exercise.
Molecular Vibrations
In the dynamics and spectroscopy of molecules in the Born-Oppenheimer approximation,
the motion of a molecule is separated into electronic, vibrational, and rotational motion. In

18.2 Applications of Hermite Functions
883
treating the vibrational motion, the departure of nuclei from their equilibrium positions is to
lowest order described by a quadratic potential, and the resulting oscillations are identiﬁed
as harmonic. These harmonic motions can be treated as coupled simple harmonic oscil-
lators, and we can decouple the individual nuclear motions by making a transformation to
normal coordinates, as was illustrated in Example 6.5.2. In this harmonic oscillation limit,
the vibrational wave functions have the form given in the preceding subsection, and the
computation of properties associated with these wave functions then involve integrals in
which products of Hermite functions appear.
The simplest integrals occurring in vibrational problems are of the form
∞
Z
−∞
xre−x2 Hn(x)Hm(x)dx.
Examples for r = 1 and r = 2 (with n = m) are included in the exercises at the end of this
section. A large number of other examples can be found in the work by Wilson, Decius, and
Cross.2 Some of the vibrational properties of molecules require the evaluation of integrals
containing as many as four Hermite functions. In the remainder of this subsection we
illustrate some of the possibilities and the associated mathematical procedures.
Example 18.2.1
THREEFOLD HERMITE FORMULA
Consider the following integral involving three Hermite polynomials
I3 ≡
∞
Z
−∞
e−x2 Hm1(x)Hm2(x)Hm3(x)dx,
(18.32)
where Ni ≥0 are integers. The formula (due to E. C. Titchmarsh, J. Lond. Math. Soc. 23:
15 (1948); see Gradshteyn and Ryzhik, p. 804, in Additional Readings) generalizes the I2
case needed for the orthogonality and normalization of Hermite polynomials. To start, we
note that the integrand of I3 will be even if the index sum m1 + m2 + m3 is even, and odd
if that index sum is odd, so I3 will vanish unless m1 + m2 + m3 is even. In addition, we
see that if the product Hm1 Hm2 is expanded and written as a sum of Hermite polynomials,
the resulting polynomial of largest index will be Hm1+m2, so I3 will vanish due to orthog-
onality unless m1 + m2 is at least as large as m3. This condition must continue to hold
if the roles of the mi are permuted; a convenient way of summarizing these observations
is to state that the mi must satisfy a triangle condition. Both the even index sum and the
triangle condition parallel similar conditions on integrals of Legendre polynomials which
we encountered in Section 16.3 and discussed in detail at Eq. (16.85).
2E. B. Wilson, Jr., J. C. Decius, and P. C. Cross, Molecular Vibrations, New York: McGraw-Hill (1955), reprinted, Dover
(1980).

884
Chapter 18 More Special Functions
To derive I3, we start with the product of three generating functions of Hermite polyno-
mials, multiply by e−x2, and integrate over x:
Z3 ≡
∞
Z
−∞
e−x2
3
Y
j=1
e2xt j−t2
j dx =
∞
Z
−∞
e−(t1+t2+t3−x)2+2(t1t2+t1t3+t2t3)dx
= √π e2(t1t2+t1t3+t2t3) = √π
∞
X
N=0
2N
N!
X
n1,n2,n3≥0
n1+n2+n3=N
N!
n1!n2!n3! tn2+n3
1
tn1+n3
2
tn1+n2
3
.
(18.33)
In reaching Eq. (18.33), we recognized the x integration as an error integral, Eq. (1.148),
and then expanded the resulting exponential, ﬁrst as a power series in w = 2(t1t2 + t1t3 +
t2t3), and then expanding the powers of w by the generalization of the binomial theorem
given as Eq. (1.80). Note that the index for the power of tit j in the polynomial expansion
was designated nk, where i, j, k are (in some order) 1, 2, 3.
We next expand the generating functions in terms of Hermite polynomials and set the
result equal to a slightly simpliﬁed version of the expression just obtained for Z3:
Z3 =
∞
X
m1,m2,m3=0
tm1
1 tm2
2 tm3
3
m1!m2!m3!
∞
Z
−∞
e−x2 Hm1(x)Hm2(x)Hm3(x)dx
= √π
∞
X
n1,n2,n3=0
2N tn2+n3
1
tn1+n3
2
tn1+n2
3
n1!n2!n3!
,
(18.34)
with N = n1 + n2 + n3. In Eq. (18.34) we now equate the coefﬁcients of equal powers of
the t j, ﬁnding that m1 = n2 + n3, m2 = n1 + n3, m3 = n1 + n2, that
N = m1 + m2 + m3
2
,
and that n1 = N −m1, n2 = N −m2, n3 = N −m3. From the coefﬁcients of tm1
1 tm2
2 tm3
3 ,
we obtain the ﬁnal result
I3 =
√π 2N m1!m2!m3!
(N −m1)!(N −m2)!(N −m3)!.
(18.35)
Equation (18.35) explicitly reﬂects the necessity of the triangle condition. If it is not satis-
ﬁed but the sum of the mi is even, at least one of the factorials in the denominator of
Eq. (18.35) will have a negative integer argument, thereby causing I3 to be zero. The
requirement that the sum of the mi be even is not explicit in the form of Eq. (18.35), but
the formula for I3 is restricted to that case because the right-hand side of Eq. (18.34) only
contains terms in which the sum of the powers of the ti is even.
■
Hermite Product Formula
The integrals Im with m > 3 can be obtained in closed form, but as ﬁnite sums. The starting
point for that analysis is a formula for the product of two Hermite polynomials due to

18.2 Applications of Hermite Functions
885
E. Feldheim, J. Lond. Math. Soc. 13: 22 (1938). To derive Feldheim’s formula, we can
start from a product of two generating functions, written as
e2x(t1+t2)−t2
1 −t2
2 =
∞
X
m1,m2=0
Hm1(x)Hm2(x) tm1
1
m1!
tm2
2
m2!
= e2x(t1+t2)−(t1+t2)2 e2t1t2 =
∞
X
n=0
Hn(x)(t1 + t2)n
n!
∞
X
ν=0
(2t1t2)ν
ν!
.
Applying the binomial expansion to (t1 + t2)n and then comparing like powers of t1 and
t2 in the two lines of the above equation, we ﬁnd
Hm1(x)Hm2(x) =
min(m1,m2)
X
ν=0
Hm1+m2−2ν(x)
m1!m2!2ν
ν!(m1 + m2 −2ν)!
m1 + m2 −2ν
m1 −ν

=
min(m1,m2)
X
ν=0
Hm1+m2−2ν(x)2νν!
m1
ν
m2
ν

.
(18.36)
For ν = 0 the coefﬁcient of HN1+N2 is obviously unity. Special cases, such as
H2
1 = H2 + 2, H1H2 = H3 + 4H1, H2
2 = H4 + 8H2 + 8, H1H3 = H4 + 6H2
can be derived from Table 13.1 and agree with the general twofold product formula.
The product formula has been generalized to products of m > 2 Hermite polynomials,
thereby providing a new way of evaluating the integrals Im. For details we refer the reader
to work by Liang, Weber, Hayashi, and Lin.3
Example 18.2.2
FOURFOLD HERMITE FORMULA
An important application of the Hermite product formula is a newly reported evaluation of
the integral I4 containing a product of four Hermite polynomials. The analysis is that of
one of the present authors and his colleagues.3
The integral we are about to study is of the form
I4 =
∞
Z
−∞
e−x2 Hm1(x)Hm2(x)Hm3(x)Hm4(x)dx.
(18.37)
It is convenient to order the indices of the Hermite polynomials so that m1 ≥m2 ≥m3 ≥
m4. Our approach will be to apply the product formula to Hm1 Hm2 and to Hm3 Hm4, thereby
3K. K. Liang, H. J. Weber, M. Hayashi, and S. H. Lin, Computational aspects of Franck-Condon overlap intervals. In Pandalai,
S. G., ed., Recent Research Developments in Physical Chemistry, Vol. 8, Transworld Research Network (2005).

886
Chapter 18 More Special Functions
initially obtaining
I4 =
min(m1,m2)
X
µ=0
2µµ!
m1
µ
m2
µ
 min(m3,m4)
X
ν=0
2νν!
m3
ν
m4
ν

×
∞
Z
−∞
e−x2 Hm1+m2−2µ(x)Hm3+m4−2ν(x)dx.
(18.38)
Invoking the orthogonality of the Hm with the weighting factor shown, the integral in
Eq. (18.38) can be evaluated, yielding
∞
Z
−∞
e−x2 Hm1+m2−2µ(x)Hm3+m4−2ν(x)dx
= √π 2m3+m4−2ν(m3 + m4 −2ν)!δm1+m2−2µ,m3+m4−2ν.
(18.39)
The Kronecker delta in Eq. (18.39) limits the value of µ to the single value, if any, that
satisﬁes
µ = m1 + m2 −m3 −m4
2
+ ν,
(18.40)
so the double summation collapses to a single sum over ν. Moreover, when the powers of
2 in Eqs. (18.38) and (18.39) are combined, their resultant is 2M, where
M = m1 + m2 + m3 + m4
2
.
(18.41)
We now rewrite Eq. (18.38), removing the µ summation and assigning µ the value from
Eq. (18.40), writing the binomial coefﬁcients in terms of their constituent factorials, and
introducing M wherever it will result in simpliﬁcation. We reach
I4 =
X
ν
√π 2M (m3 + m4 −2ν)!m1!m2!m3!m4!
(M −m3 −m4 + ν)!(M −m1 −ν)!(M −m2 −ν)!(m3 −ν)!(m4 −ν)!ν!.
(18.42)
This formula for I4 will only be valid when the sum of the mi is even, equivalent to the
requirement that M (and therefore also µ) be integral. If the sum of the mi is odd, then I4
will have an odd integrand and will vanish by symmetry. The summation in Eq. (18.42)
will be over the nonnegative integral values of ν for which none of the factorials in the
denominator of that summation has a negative argument. Note that there will be no value of
ν that satisﬁes this condition if m1 > m2 +m3 +m4, because M −m1 will then be negative,
and then I4 = 0. Thus we have a generalization of the triangle condition that applied to the
threefold Hermite formula: If the largest of the mi is greater than the sum of the others, the
Hm of smaller m cannot combine to yield a Hermite polynomial of sufﬁciently large index
to avoid an orthogonality zero.
Further examination of the factorials in the denominator of Eq. (18.42) reveals that the
lower limit of the summation will (if m1 ≤m2 + m3 + m4) always be ν = 0; note that

18.2 Applications of Hermite Functions
887
M −m3 −m4 will always be nonnegative. The upper limit of the summation will be the
smaller of m4 and M −m1.
■
The Hermite polynomial product formula can also be applied to products of Hermite
polynomials with a different exponential weighting function than in the examples we have
presented. To evaluate such integrals we use the generalized product formula in conjunc-
tion with the integral (see Gradshteyn and Ryzhik, p. 803, in the Additional Readings),
∞
Z
−∞
e−a2x2 Hm(x)Hn(x)dx =
2m+n
am+n+1 (1 −a2)(m+n)/20
m + n + 1
2

×
min(m,n)
X
ν=0
(−m)ν(−n)ν
ν!
1 −m −n
2

ν

a2
2(a2 −1)
ν
,
(18.43)
instead of the standard orthogonality integral for the product of two Hermite polynomials.
The quantity (−m)ν is a Pochhammer symbol, and causes the ν summation in Eq. (18.43)
to be a ﬁnite sum. The summation can also be identiﬁed as a hypergeometric function; see
Exercise 18.5.11. The process we have sketched yields a result that is similar to Im but
somewhat more complicated. We omit details.
The oscillator potential has also been employed extensively in calculations of nuclear
structure (nuclear shell model), as well as in quark models of hadrons and the nuclear
force.
Exercises
18.2.1
Prove that

2x −d
dx
n
1 = Hn(x).
Hint. Check out the cases n = 0 and n = 1 and then use mathematical induction
(Section 1.4).
18.2.2
Show that
Z ∞
−∞
xme−x2 Hn(x)dx = 0
for m an integer,
0 ≤m ≤n −1.
18.2.3
The transition probability between two oscillator states m and n depends on
∞
Z
−∞
xe−x2 Hn(x)Hm(x)dx.
Show that this integral equals π1/22n−1n! δm,n−1 + π1/22n(n + 1)! δm,n+1. This result
shows that such transitions can occur only between states of adjacent energy levels,
m = n ± 1.
Hint. Multiply the generating function, Eq. (18.2), by itself using two different sets
of variables (x,s) and (x,t). Alternatively, the factor x may be eliminated by the
recurrence relation, Eq. (18.3).

888
Chapter 18 More Special Functions
18.2.4
Show that
Z ∞
−∞
x2e−x2 Hn(x)Hn(x)dx = π1/22nn!

n + 1
2

.
This integral occurs in the calculation of the mean-square displacement of our quantum
oscillator.
Hint. Use the recurrence relation, Eq. (18.3), and the orthogonality integral.
18.2.5
Evaluate
∞
Z
−∞
x2e−x2 Hn(x)Hm(x)dx
in terms of n and m and appropriate Kronecker delta functions.
ANS.
2n−1π1/2(2n + 1)n! δnm + 2nπ1/2(n + 2)! δn+2,m + 2n−2π1/2n! δn−2,m.
18.2.6
Show that
Z ∞
−∞
xre−x2 Hn(x)Hn+p(x)dx =
(0,
p > r
2nπ1/2(n + r)!,
p = r,
with n, p, and r nonnegative integers.
Hint. Use the recurrence relation, Eq. (18.3), p times.
18.2.7
With ψn(x) = e−x2/2
Hn(x)
(2nn! π1/2)1/2 , verify that
aψn(x) = x −ip
√
2
= 1
√
2

x + d
dx

ψn(x) = n1/2ψn−1(x),
a†ψn(x) = x + ip
√
2
= 1
√
2

x −d
dx

ψn(x) = (n + 1)1/2ψn+1(x).
Note. The usual quantum mechanical operator approach establishes these raising and
lowering properties before the form of ψn(x) is known.
18.2.8
(a)
Verify the operator identity
x + ip = x −d
dx = −exp
x2
2
 d
dx exp

−x2
2

.
(b)
The normalized simple harmonic oscillator wave function is
ψn(x) = (π1/22nn!)−1/2 exp

−x2
2

Hn(x).
Show that this may be written as
ψn(x) = (π1/22nn!)−1/2

x −d
dx
n
exp

−x2
2

.

18.3 Laguerre Functions
889
Note. This corresponds to an n-fold application of the raising operator of Exer-
cise 18.2.7.
18.3
LAGUERRE FUNCTIONS
Rodrigues Formula and Generating Function
Let’s start from the Laguerre ODE,
xy′′(x) + (1 −x)y′(x) + ny(x) = 0.
(18.44)
This ODE is not self-adjoint, but the weighting factor needed to make it self-adjoint can
be computed from the usual formula,
w(x) = 1
x exp
Z 1 −x
x
dx

= 1
x exp(ln x −x) = e−x.
(18.45)
Given w(x), we may now use the method developed in Section 12.1 to obtain a Rodrigues
formula and generating function for the Laguerre polynomials. Letting Ln(x) denote the
nth Laguerre polynomial, the Rodrigues formula is (apart from a scale factor) given by
Eq. (12.9):
Ln(x) =
1
w(x)
 d
dx
n  w(x)p(x)n
,
where p(x) is the coefﬁcient of y′′ in the ODE. Inserting the expressions for w(x) and
p(x), and inserting a factor 1/n! to bring the Laguerre polynomials to their conventional
scaling, the Rodrigues formula takes the more complete and explicit form,
Ln(x) = ex
n!
 d
dx
n  xne−x
.
(18.46)
A generating function can now be written as a sum of contour integrals of the Schlaeﬂi
type, as in Eq. (12.25):
g(x,t) =
∞
X
t=0
Ln(x)tn =
1
w(x)
∞
X
n=0
cn tn n!
2πi
I
C
w(z)[p(z)]n
(z −x)n+1 dz,
where the contour surrounds the point x and no other singularities. Specializing to our
current problem, and noting that the coefﬁcient cn has the value 1/n!, this formula becomes
g(x,t) = ex
2πi
∞
X
n=0
I
C
e−z (tz)n
(z −x)n+1 dz = ex
2πi
I
C
e−z dz
(z −x)
∞
X
n=0

tz
z −x
n
.
(18.47)
We now recognize the n summation as a geometric series, so our generating function
becomes
g(x,t) = ex
2πi
I
C
e−z dz
z −x −tz .
(18.48)

890
Chapter 18 More Special Functions
Our integrand has a simple pole at z = x/(1−t), with residue e−x/(1−t)/(1−t), and g(x,t)
reduces to
g(x,t) = ex e−x/(1−t)
1 −t
= e−xt/(1−t)
1 −t
=
∞
X
n=0
Ln(x)tn,
(18.49)
the form given in Table 12.1.
Not all workers deﬁne Laguerre polynomials at the scale chosen here and represented by
the speciﬁc formulas in Eq. (18.46) and (18.49). However, our choice is probably the most
common, and is consistent with that in AMS-55 (see Abramowitz in Additional Readings).
Properties of Laguerre Polynomials
By differentiating the generating function in Eq. (18.45) with respect to x and t, we obtain
recurrence relations for the Laguerre polynomials as follows. Using the product rule for
differentiation we verify the identities
(1 −t)2 ∂g
∂t = (1 −x −t)g(x,t),
(t −1)∂g
∂x = tg(x,t).
(18.50)
Writing the left-hand and right-hand sides of the ﬁrst identity in terms of Laguerre polyno-
mials using the expansion given in Eq. (18.49), we obtain
X
n

(n + 1)Ln+1(x) −2nLn(x) + (n −1)Ln−1(x)

tn
=
X
n

(1 −x)Ln(x) −Ln−1(x)

tn.
Equating coefﬁcients of zn yields
(n + 1)Ln+1(x) = (2n + 1 −x)Ln(x) −nLn−1(x).
(18.51)
To get the second recursion relation we use both identities of Eqs. (18.50) to verify a third
identity,
x ∂g
∂x = t ∂g
∂t −t ∂(tg)
∂t
,
which, when written similarly in terms of Laguerre polynomials, is seen to be equivalent to
xL′
n(x) = nLn(x) −nLn−1(x).
(18.52)
To use these recurrence formulas we need starting values. From the Rodrigues formula,
we easily ﬁnd L0(x) = 1 and L1(x) = 1 −x. Applying Eq. (18.51) we continue to Ln(x)
with n > 1, obtaining the results given in Table 18.2. The ﬁrst three Laguerre polynomials
are plotted in Fig. 18.3.

18.3 Laguerre Functions
891
Table 18.2
Laguerre Polynomials
L0(x) = 1
L1(x) = −x + 1
2! L2(x) = x2 −4x + 2
3! L3(x) = −x3 + 9x2 −18x + 6
4! L4(x) = x4 −16x3 + 72x2 −96x + 24
5! L5(x) = −x5 + 25x4 −200x3 + 600x2 −600x + 120
6! L6(x) = x6 −36x5 + 450x4 −2400x3 + 5400x2 −4320x + 720
1
−1
−2
−3
L0(x)
1
2
4
L2(x)
L1(x)
x
3
FIGURE 18.3
Laguerre polynomials.
From the recurrence relations or the Rodrigues formula, we ﬁnd the the power series
expansion of Ln(x):
Ln(x) = (−1)n
n!

xn −n2
1! xn−1 + n2(n −1)2
2!
xn−2 −··· + (−1)nn!

=
n
X
m=0
(−1)mn! xm
(n −m)!m!m! =
n
X
s=0
(−1)n−sn! xn−s
(n −s)!(n −s)!s!.
(18.53)
Also, from Eq. (18.49) we ﬁnd
g(0,t) =
1
1 −t =
∞
X
n=0
tn =
∞
X
n=0
Ln(0)tn,
which shows that at x = 0 the Laguerre polynomials have the special value
Ln(0) = 1.
(18.54)

892
Chapter 18 More Special Functions
The form of the generating function, that of Laguerre’s ODE, and Table 18.2 all show that
the Laguerre polynomials have neither odd nor even symmetry under the parity transfor-
mation x →−x.
As we already observed at the beginning of this section, the Laguerre ODE is not self-
adjoint but can be made so by appending the weighting factor e−x. Noting also that with
this weighting factor, the Laguerre polynomials satisfy Sturm-Liouville boundary condi-
tions at x = 0 and x = ∞, we see that the Ln(x) must satisfy an orthogonality condition
of the form
∞
Z
0
e−x Lm(x)Ln(x)dx = δmn.
(18.55)
Equation (18.55) indicates that for this interval and weighting factor the Laguerre polyno-
mials are normalized. Proof is the topic of Exercise 18.3.3.
It is sometimes convenient to deﬁne orthogonalized Laguerre functions (with unit
weighting factor) by
ϕn(x) = e−x/2Ln(x).
(18.56)
Our new orthonormal functions, ϕn(x), satisfy the self-adjoint ODE
xϕ′′
n(x) + ϕ′
n(x) +

n + 1
2 −x
4

ϕn(x) = 0,
(18.57)
and are eigenfunctions of a Sturm-Liouville system on the range (0 ≤x < ∞).
Associated Laguerre Polynomials
In many applications, particularly in quantum mechanics, we need the associated Laguerre
polynomials deﬁned by4
Lk
n(x) = (−1)k dk
dxk Ln+k(x).
(18.58)
By differentiating the power series for Ln(x) given in Eq. (18.53) (compare Table 18.2),
we can get the explicit forms shown in Table 18.3. In general,
Lk
n(x) =
n
X
m=0
(−1)m
(n + k)!
(n −m)!(k + m)!m! xm,
k ≥0.
(18.59)
One of the present authors5 has recently found a new generating function for the associ-
ated Laguerre polynomials with the remarkably simple form
gl(x,t) = e−tx(1 + t)l =
∞
X
n=0
Ll−n
n
(x)tn.
(18.60)
4Some authors use Lk
n+k(x) = (dk/dxk)[Ln+k(x)]. Hence our Lkn(x) = (−1)kLk
n+k(x).
5H. J. Weber, Connections between real polynomial solutions of hypergeometric-type differential equations with Rodrigues
formula, Cent. Eur. J. Math. 5: 415–427 (2007).

18.3 Laguerre Functions
893
Table 18.3
Associated Laguerre Polynomials
Lk
0 = 1
1! Lk
1 = −x + (k + 1)
2! Lk
2 = x2 −2(k + 2)x + (k + 1)2
3! Lk
3 = −x3 + 3(k + 3)x2 −3(k + 2)2x + (k + 1)3
4! Lk
4 = x4 −4(k + 4)x3 + 6(k + 3)2 −4(k + 2)3 + (k + 1)4
5! Lk
5 = −x5 + 5(k + 5)x4 −10(k + 4)2x3 + 10(k + 3)3x2 −5(k + 2)4x + (k + 1)5
6! Lk
6 = x6 −6(k + 6)x5 + 15(k + 5)2x4 −20(k + 4)3x3 + 15(k + 3)4x2
−6(k + 2)5x + (k + 1)6
7! Lk
7 = −x7 + 7(k + 7)x6 −21(k + 6)2x5 + 35(k + 5)3x4 −35(k + 4)4x3
+21(k + 3)5x2 −7(k + 2)6x + (k + 1)7
The notations (k + n)m are Pochhammer symbols, deﬁned in Eq. (1.72).
Rather than deriving this formula, we verify it by showing that it produces the deﬁning
relation for the Lk
n, Eq. (18.58), and is consistent with the previously presented formulas
for the ordinary Laguerre polynomials (i.e., the Lk
n with k = 0).
If we multiply both members of Eq. (18.60) by 1 −t, the coefﬁcients of tn yield the
recurrence formula
Ll−n
n
+ Ll−n+1
n−1
= Ll−n+1
n
,
or
Lk
n −Lk+1
n
= −Lk+1
n−1.
(18.61)
On the other hand, differentiation of Eq. (18.60) with respect to x and writing
∂gl(x,t)
∂x
=
X
n
dLl−n
n
(x)
dx
tn = −te−tx(1 + t)l = e−tx(1 + t)l −e−tx(1 + t)l+1,
the coefﬁcients of tn yield a formula for dLl−n
n
(x)/dx, namely (with k = l −n)
dLk
n(x)
dx
= Lk
n(x) −Lk+1
n
(x),
(18.62)
and substituting the result from Eq. (18.61), we reach
dLk
n(x)
dx
= −Lk+1
n−1,
(18.63)
thereby conﬁrming that our generating function yields Eq. (18.58).
The veriﬁcation that our generating function is correct is now completed by using it to
ﬁnd L0
n(x), which is the coefﬁcient of tn in e−tx(1 + t)n. Using the binomial expansion of
(1 + t)n and the Maclaurin series for the exponential, we get
L0
n(x) =
n
X
m=0

n
n −m
 (−x)m
m!
=
n
X
m=0
(−1)mn!
(n −m)!m!m! xm,
in agreement with Eq. (18.53).

894
Chapter 18 More Special Functions
We can also conﬁrm the series expansion given as Eq. (18.59) for Lk
n. It is the coefﬁcient
of tn in e−tx(1 + t)k+n, obtained in a manner similar to the procedure we just carried out
for L0
n.
The generating function provides convenient routes to other properties of the associated
Laguerre polynomials. Special values for x = 0 can be obtained from
X
n
Ll−n
n
(0)tn = (1 + t)l =
lX
n=0
l
n

tn.
We therefore have
Lk
n(0) =
n + k
n

.
(18.64)
A formula for recurrence in the index n of Lk
n(x) can be obtained by differentiating the
generating function formula with respect to t. Doing so, from the coefﬁcient of tn and
setting l = k + n,
(n + 1)Lk−1
n+1(x) = (k + n)Lk−1
n
(x) −xLk
n(x).
(18.65)
Using Eq. (18.61) to raise the upper index in the two terms for which it is k −1, we ﬁnd
after collecting similar terms,
(n + 1)Lk
n+1(x) −(2n + k + 1 −x)Lk
n(x) + (n + k)Lk
n−1(x) = 0,
(18.66)
a lower-index recurrence formula.
Finally, returning to Eq. (18.65), differentiating it once with respect to x, and identifying
h
Lk−1
n+1
i′
= −Lk
n, we get
(n + k)
h
Lk−1
n
i′
= x
h
Lk
n
i′
+ Lk
n −(n + 1)Lk
n = x
h
Lk
n
i′
−nLk
n.
(18.67)
A second differentiation brings us to
x
h
Lk
n
i′′
+ (1 −n)
h
Lk
n
i′
= (n + k)
h
Lk−1
n
i′′
= (n + k)
h
Lk−1
n
i′
−(n −k)
h
Lk
n
i′
,
(18.68)
where the ﬁnal member of Eq. (18.68) was the result of substituting the derivative of
Eq. (18.62) with k →k −1. Using Eq. (18.67) to replace (n + k)

Lk−1
n
′ by a form in
which the upper index is k, we reach an ODE for Lk
n:
x d2Lk
n(x)
dx2
+ (k + 1 −x)dLk
n(x)
dx
+ nLk
n(x) = 0.
(18.69)
This ODE is known as the associated Laguerre equation. When associated Laguerre
polynomials appear in a physical problem it is usually because that physical problem
involves Eq. (18.69). The most important application is their use to describe the bound
states of the hydrogen atom, which are derived in upcoming Example 18.3.1.

18.3 Laguerre Functions
895
The associated Laguerre equation, Eq. (18.69), is not self-adjoint, but the weighting
function needed to bring it to self-adjoint form (for upper index k) can be found in the
usual way:
wk(x) = 1
x exp
Z k + 1 −x
x
dx

= xk e−x.
(18.70)
When we also note that Sturm-Liouville boundary conditions are satisﬁed at x = 0 and
x = ∞, we see that the associated Laguerre polynomials are orthogonal according to the
equation
∞
Z
0
e−xxk Lk
n(x)Lk
m(x)dx = (n + k)!
n!
δmn.
(18.71)
The value of the integral in Eq. (18.71) for m = n can be established using the generating
function, Eq. (18.58). Doing so is left as an exercise.
Equation (18.71) shows the same orthogonality interval (0,∞) as that for the Laguerre
polynomials, but with a different weighting function for each k. We see that for each k the
associated Laguerre polynomials deﬁne a new set of orthogonal polynomials.
A Rodrigues representation of the associated Laguerre polynomials is useful and can be
found in various ways. A fairly direct approach is simply to use Eq. (12.9) with p(x) = x,
the coefﬁcient of the second-derivative term in Eq. (18.69) and the value of wk(x) given
in Eq. (18.70). The result is
Lk
n(x) = exx−k
n!
dn
dxn (e−xxn+k).
(18.72)
Note that this and all our earlier formulas involving the Lk
n(x) reduce properly to corre-
sponding expressions involving Ln(x) when k = 0.
By letting ψk
n(x) = e−x/2xk/2Lk
n(x), we ﬁnd that ψk
n(x) satisﬁes the self-adjoint ODE,
x d2ψk
n(x)
dx2
+ dψk
n(x)
dx
+

−x
4 + 2n + k + 1
2
−k2
4x

ψk
n(x) = 0.
(18.73)
The ψk
n(x) are sometimes called Laguerre functions. Equation (18.57) is the special case
k = 0 of Eq. (18.73).
A further useful form is given by deﬁning6
8k
n(x) = e−x/2x(k+1)/2Lk
n(x).
(18.74)
Substitution into the associated Laguerre equation yields
d28k
n(x)
dx2
+

−1
4 + 2n + k + 1
2x
−k2 −1
4x2

8k
n(x) = 0.
(18.75)
The 8k
n(x) are orthogonal with weighting function x−1.
The associated Laguerre ODE, Eq. (18.69), has solutions even if n is not an integer, but
they are then not polynomials and diverge proportionally to xkex as x →∞. This fact is
useful in the following example.
6This corresponds to modifying the function ψ in Eq. (18.73) to eliminate the ﬁrst derivative.

896
Chapter 18 More Special Functions
Example 18.3.1
THE HYDROGEN ATOM
The most important application of the Laguerre polynomials is in the solution of the
Schrödinger equation for the hydrogen-like atom (H, He+, Li2+, etc.). For a system con-
sisting of a nucleus of charge Ze ﬁxed at the origin and one electron whose distribution is
described by a wave function ψ, this equation is
−¯h2
2m ∇2ψ −
Ze2
4πϵ0r ψ = Eψ,
(18.76)
in which Z = 1 for hydrogen, Z = 2 for He+, and so on. Separating variables in spherical
polar coordinates and recognizing that the angular part of the solution to this equation must
be a spherical harmonic, we set ψ(r) = R(r)Y M
L (θ,ϕ) with R(r) satisfying the ODE
−¯h2
2m
1
r2
d
dr

r2 d R
dr

−
Ze2
4πϵ0r R + ¯h2
2m
L(L + 1)
r2
R = E.
(18.77)
For bound states, R →0 as r →∞, and it can be shown that these conditions can only
be met if E < 0. In addition, R must be ﬁnite at r = 0. We do not consider unbound
(continuum) states with positive energy. Only when the latter are included do hydrogenic
wave functions form a complete set.
By use of the abbreviations (resulting from rescaling r to the dimensionless radial vari-
able ρ)
α =

−8mE
¯h2
1/2
,
ρ = αr,
λ =
mZe2
2πϵ0α¯h2 ,
χ(ρ) = R(r),
(18.78)
Eq. (13.85) becomes
1
ρ2
d
dρ

ρ2 dχ(ρ)
dρ

+
λ
ρ −1
4 −L(L + 1)
ρ2

χ(ρ) = 0.
(18.79)
For our present purposes, it is useful to rewrite the ﬁrst term of Eq. (18.79) using the
identity
1
ρ2
d
dρ

ρ2 dχ
dρ

= 1
ρ
d2
dρ2 (ρχ)
and then multiply the resulting equation by ρ, reaching
d
dρ2 (ρχ) +
λ
ρ −1
4 −L(L + 1)
ρ2

(ρχ) = 0.
(18.80)
A comparison with Eq. (18.75) for 8k
n(x) shows that Eq. (18.80) is satisﬁed by
ρχ(ρ) = e−ρ/2ρL+1L2L+1
λ−L−1(ρ),
(18.81)
where k and n of Eq. (18.75) have been, respectively, replaced by 2L + 1 and λ −L −1.
The parameter λ must be restricted to values such that λ −L −1 is both integral and
nonnegative. If this requirement is violated, L2L+1
λ−L−1 will diverge too rapidly to permit
ρχ(ρ) to go to zero at large r, which is required for a bound-state electron distribution.

18.3 Laguerre Functions
897
Since we already know that L, a spherical harmonic index, must be integral and nonnega-
tive, we see that the possible values of λ are integers n at least as large as L + 1.7
This restriction on λ, imposed by our boundary condition, has the effect of quantizing
the energy. Inserting λ = n, the deﬁnitions in Eqs. (18.78) lead to
En = −Z2m
2n2¯h2
 e2
4πϵ0
2
.
(18.82)
Since our Schrödinger equation implicitly set the potential energy to zero when the electron
is at an inﬁnite separation from the nucleus, the negative sign reﬂects the fact that we are
dealing here with bound states in which the electron cannot escape to inﬁnity. The other
quantities introduced in Eq. (18.78) can also be expressed in terms of n:
α =
me2
2πϵ0¯h2
Z
n = 2Z
na0
,
ρ = 2Z
na0
r,
with a0 = 4πϵ0 ¯h2
me2
.
(18.83)
The quantity a0, of dimension length, is known as the Bohr radius, and its appearance as
a scale factor causes the potential energy (for n = 1, the smallest possible value) to have
an average value corresponding to this electron-nuclear separation.
Summarizing, the ﬁnal normalized hydrogen wave function is
ψnLM(r,θ,ϕ) =
" 2Z
na0
3 (n −L −1)!
2n(n + L)!
#1/2
e−αr/2(αr)L L2L+1
n−L−1(αr)Y M
L (θ,ϕ).
(18.84)
Note that the energy corresponding to ψnLM depends only on n, which is called the
principal quantum number of this system. Note also that if n is assigned a speciﬁc inte-
gral value, the condition on λ requires that L ≤n −1, thereby explaining the well-known
pattern of possible hydrogenic energy states: If n = 1, L can only be zero; for n = 2, we
can have L = 0 or L = 1, etc.
■
Exercises
18.3.1
Show with the aid of the Leibniz formula that the series expansion of Ln(x), Eq. (18.53),
follows from the Rodrigues representation, Eq. (18.72).
18.3.2
(a)
Using the explicit series form, Eq. (18.53), show that
L′
n(0) = −n,
L′′
n(0) = 1
2n(n −1).
(b)
Repeat without using the explicit series form of Ln(x).
18.3.3
Derive the normalization relation, Eq. (18.71) for the associated Laguerre polynomials,
thereby also conﬁrming Eq. (18.55) for the Ln.
7This is the conventional notation for λ. It is not the same n as the index n in 8kn(x).

898
Chapter 18 More Special Functions
18.3.4
Expand xr in a series of associated Laguerre polynomials Lk
n(x), with k ﬁxed and n
ranging from 0 to r (or to ∞if r is not an integer).
Hint. The Rodrigues form of Lk
n(x) will be useful.
ANS.
xr = (r + k)!r!
r
X
n=0
(−1)nLk
n(x)
(n + k)!(r −n)!,
0 ≤x < ∞.
18.3.5
Expand e−ax in a series of associated Laguerre polynomials Lk
n(x), with k ﬁxed and n
ranging from 0 to ∞.
(a)
Evaluate directly the coefﬁcients in your assumed expansion.
(b)
Develop the desired expansion from the generating function.
ANS.
e−ax =
1
(1 + a)1+k
∞
X
n=0

a
1 + a
n
Lk
n(x),
0 ≤x < ∞.
18.3.6
Show that
Z ∞
0
e−xxk+1Lk
n(x)Lk
n(x)dx = (n + k)!
n!
(2n + k + 1).
Hint. Note that xLk
n = (2n + k + 1)Lk
n −(n + k)Lk
n−1 −(n + 1)Lk
n+1.
18.3.7
Assume that a particular problem in quantum mechanics has led to the ODE
d2y
dx2 −
k2 −1
4x2
−2n + k + 1
2x
+ 1
4

y = 0
for nonnegative integers n,k. Write y(x) as y(x) = A(x)B(x)C(x), with the require-
ment that
(a)
A(x) be a negative exponential giving the required asymptotic behavior of y(x),
and
(b)
B(x) be a positive power of x giving the behavior of y(x) for 0 ≤x ≪1.
Determine A(x) and B(x). Find the relation between C(x) and the associated Laguerre
polynomial.
ANS.
A(x) = e−x/2,
B(x) = x(k+1)/2,
C(x) = Lk
n(x).
18.3.8
From Eq. (18.84) the normalized radial part of the hydrogenic wave function is
RnL(r) =

α3 (n −L −1)!
2n(n + L)!
1/2
e−αr(αr)L L2L+1
n−L−1(αr),
in which α = 2Z/na0 = 2Zme2/4πϵ0 ¯h2. Evaluate
(a) ⟨r⟩=
∞
Z
0
r RnL(αr)RnL(αr)r2 dr,

18.4 Chebyshev Polynomials
899
(b) ⟨r−1⟩=
∞
Z
0
r−1RnL(αr)RnL(αr)r2 dr.
The quantity ⟨r⟩is the average displacement of the electron from the nucleus, whereas
⟨r−1⟩is the average of the reciprocal displacement.
ANS.
⟨r⟩= a0
2
h
3n2 −L(L + 1)
i
,
⟨r−1⟩=
1
n2a0
.
18.3.9
Derive a recurrence formula for the hydrogen wave function expectation values:
s + 2
n2 ⟨rs+1⟩−(2s + 3)a0⟨rs⟩+ s + 1
4
h
(2L + 1)2 −(s + 1)2i
a2
0⟨rs−1⟩= 0,
with s ≥−2L −1.
Hint. Transform Eq. (18.80) into a form analogous to Eq. (18.73). Multiply by
ρs+2u′ −cρs+1u, with u = ρ8. Adjust c to cancel terms that do not yield expecta-
tion values.
18.3.10
Show that
∞
Z
−∞
xne−x2 Hn(xy)dx = √π n! Pn(y), where Pn is a Legendre polynomial.
18.4
CHEBYSHEV POLYNOMIALS
The generating function for the Legendre polynomials can be generalized to the following
form:
1
(1 −2xt + t2)α =
∞
X
n=0
C(α)
n (x)tn.
(18.85)
The coefﬁcients C(α)
n (x) are known as the ultraspherical polynomials (also called
Gegenbauer polynomials). For α = 1/2, we recover the Legendre polynomials; the
special cases α = 0 and α = 1 yield two types of Chebyshev polynomials that are the sub-
ject of this section. The primary importance of the Chebyshev polynomials is in numerical
analysis.
Type II Polynomials
With α = 1 and C(1)
n (x) written as Un(x), Eq. (18.85) gives
1
1 −2xt + t2 =
∞
X
n=0
Un(x)tn,
|x| < 1,
|t| < 1.
(18.86)
These functions are called type II Chebyshev polynomials. Although these polynomials
have few applications in mathematical physics, one unusual application is in the develop-
ment of four-dimensional spherical harmonics used in angular momentum theory.

900
Chapter 18 More Special Functions
Type I Polynomials
With α = 0 there is a difﬁculty. Indeed, our generating function reduces to the constant
1. We may avoid this problem by ﬁrst differentiating Eq. (18.85) with respect to t. This
yields
−α(−2x + 2t)
(1 −2xt + t2)α+1 =
∞
X
n=1
nC(α)
n (x)tn−1,
or
x −t
(1 −2xt + t2)α+1 =
∞
X
n=1
n
2
"
C(α)
n (x)
α
#
tn−1.
(18.87)
We deﬁne C(0)
n (x) as
C(0)
n (x) = lim
α→0
C(α)
n (x)
α
.
(18.88)
The purpose of differentiating with respect to t was to get α in the denominator and to
create an indeterminate form. Now multiplying Eq. (18.87) by 2t and adding 1 in the form
(1 −2xt + t2)/(1 −2xt + t2), we obtain
1 −t2
1 −2xt + t2 = 1 + 2
∞
X
n=1
n
2 C(0)
n (x)tn.
(18.89)
We deﬁne Tn(x) as
Tn(x) =



1,
n = 0,
n
2 C(0)
n (x),
n > 0.
(18.90)
Note the special treatment for n = 0. We will encounter a similar treatment of the n = 0
term when we study Fourier series in Chapter 19. Also, note that C(0)
n
is the limit indicated
in Eq. (18.88) and not a literal substitution of α = 0 into the generating function series.
With these new labels,
1 −t2
1 −2xt + t2 = T0(x) + 2
∞
X
n=1
Tn(x)tn,
|x| ≤1,
|t| < 1.
(18.91)
We call Tn(x) the type I Chebyshev polynomials. Note that the notation and spelling of
the name for these functions differ from reference to reference. Here we follow the usage
of AMS-55 (Additional Readings).

18.4 Chebyshev Polynomials
901
Recurrence Relations
Differentiating the generating function, Eq. (18.91), with respect to t and multiplying by
the denominator, 1 −2xt + t2, we obtain
−t −(t −x)
"
T0(x) + 2
∞
X
n=1
Tn(x)tn
#
= (1 −2xt + t2)
∞
X
n=1
nTn(x)tn−1
=
∞
X
n=1
h
nTntn−1 −2xnTntn + nTntn+1i
,
from which after several simpliﬁcation steps we reach the recurrence relation
Tn+1(x) −2xTn(x) + Tn−1(x) = 0,
n > 0.
(18.92)
A similar treatment of Eq. (18.86) yields the corresponding recursion relation for Un:
Un+1(x) −2xUn(x) + Un−1(x) = 0,
n > 0.
(18.93)
Using the generating functions directly for n = 0 and 1, and then applying these recur-
rence relations for the higher-order polynomials, we get Table 18.4. Plots of the Tn and Un
are presented in Figs. 18.4 and 18.5.
Differentiation of the generating functions for Tn(x) and Un(x) with respect to the vari-
able x leads to a variety of recurrence relations involving derivatives. For example, from
Eq. (18.89) we thus obtain
(1 −2xt + t2)2
∞
X
n=1
T ′
n(x)tn = 2t
"
T0(x) + 2
∞
X
n=1
Tn(x)tn
#
,
from which we extract the recursion formula
2Tn(x) = T ′
n+1(x) −2xT ′
n(x) + T ′
n−1(x).
(18.94)
Other useful recurrence formulas we can ﬁnd in this way are
(1 −x2)T ′
n(x) = −nxTn(x) + nTn−1(x)
(18.95)
Table 18.4
Chebyshev Polynomials: Type I (Left), Type II
(Right)
T0 = 1
U0 = 1
T1 = x
U1 = 2x
T2 = 2x2 −1
U2 = 4x2 −1
T3 = 4x3 −3x
U3 = 8x3 −4x
T4 = 8x4 −8x2 + 1
U4 = 16x4 −12x2 + 1
T5 = 16x5 −20x3 + 5x
U5 = 32x5 −32x3 + 6x
T6 = 32x6 −48x4 + 18x2 −1
U6 = 64x6 −80x4 + 24x2 −1

902
Chapter 18 More Special Functions
T1(x)
1
−1
−1
1
T3(x)
T2(x)
x
FIGURE 18.4
The Chebyshev polynomials T1, T2, and T3.
U1(x)
5
4
3
2
1
−1
1
−1
−2
−3
x
U2(x)
U3(x)
FIGURE 18.5
The Chebyshev polynomials U1, U2, and U3.
and
(1 −x2)U′
n(x) = −nxUn(x) + (n + 1)Un−1(x).
(18.96)
Manipulating a variety of these formulas as in Section 15.1 for Legendre polynomials
one can eliminate the index n −1 in favor of T ′′
n and establish that Tn(x), the Chebyshev

18.4 Chebyshev Polynomials
903
polynomial type I, satisﬁes the ODE
(1 −x2)T ′′
n (x) −xT ′
n(x) + n2Tn(x) = 0.
(18.97)
The Chebyshev polynomial of type II, Un(x), satisﬁes
(1 −x2)U′′
n (x) −3xU ′
n(x) + n(n + 2)Un(x) = 0.
(18.98)
We could have deﬁned the Chebyshev polynomials starting from these ODEs, but we chose
instead a development based on generating functions.
Processes similar to those used for the Chebyshev polynomials can be applied to the
general ultraspherical polynomials; the result is the ultraspherical ODE
(1 −x2) d2
dx2 C(α)
n (x) −(2α + 1)x d
dx C(α)
n (x) + n(n + 2α)C(α)
n (x) = 0.
(18.99)
Special Values
Again, from the generating functions, we can obtain the special values of various polyno-
mials:
Tn(1) = 1,
Tn(−1) = (−1)n,
T2n(0) = (−1)n,
T2n+1(0) = 0;
Un(1) = n + 1,
Un(−1) = (−1)n(n + 1),
U2n(0) = (−1)n,
U2n+1(0) = 0.
(18.100)
Veriﬁcation of Eq. (18.100) is left to the exercises.
The polynomials Tn and Un satisfy parity relations that follow from their generating
functions with the substitutions t →−t, x →−x, which leave them invariant; these are
Tn(x) = (−1)nTn(−x),
Un(x) = (−1)nUn(−x).
(18.101)
Rodrigues representations of Tn(x) and Un(x) are
Tn(x) = (−1)nπ1/2(1 −x2)1/2
2n0(n + 1
2)
dn
dxn
h
(1 −x2)n−1/2i
(18.102)
and
Un(x) =
(−1)n(n + 1)π1/2
2n+10(n + 3
2)(1 −x2)1/2
dn
dxn
h
(1 −x2)n+1/2i
.
(18.103)

904
Chapter 18 More Special Functions
Trigonometric Form
At this point in the development of the properties of the Chebyshev polynomials it
is beneﬁcial to change variables, replacing x by cosθ. With x = cosθ and d/dx =
(−1/sinθ)(d/dθ), we verify that
(1 −x2)d2Tn
dx2 = d2Tn
dθ2 −cot θ dTn
dθ ,
xT ′
n = −cot θ dTn
dθ .
Adding these terms, Eq. (18.97) becomes
d2Tn
dθ2 + n2Tn = 0,
(18.104)
the simple harmonic oscillator equation with solutions cosnθ and sinnθ. The special val-
ues (boundary conditions at x = 0 and 1) identify
Tn = cosnθ = cos(n arccos x).
(18.105)
For n ̸= 0 a second linearly independent solution of Eq. (18.104) is labeled
Vn = sinnθ = sin(n arccos x).
(18.106)
The corresponding solutions of the type II Chebyshev equation, Eq. (18.98), become
Un = sin(n + 1)θ
sinθ
,
(18.107)
Wn = cos(n + 1)θ
sinθ
.
(18.108)
The two sets of solutions, type I and type II, are related by
Vn(x) = (1 −x2)1/2Un−1(x),
(18.109)
Wn(x) = (1 −x2)−1/2Tn+1(x).
(18.110)
As already seen from the generating functions, Tn(x) and Un(x) are polynomials. Clearly,
Vn(x) and Wn(x) are not polynomials. From
Tn(x) + iVn(x) = cosnθ + i sinnθ
= (cosθ + i sinθ)n =
h
x + i(1 −x2)1/2in
,
|x| ≤1
(18.111)
we can apply the binomial theorem to obtain expansions
Tn(x) = xn −
n
2

xn−2(1 −x2) +
n
4

xn−4(1 −x2)2 −···
(18.112)
and, for n > 0
Vn(x) =
p
1 −x2
n
1

xn−1 −
n
3

xn−3(1 −x2) + ···

.
(18.113)

18.4 Chebyshev Polynomials
905
From the generating functions, or from the ODEs, power-series representations are
Tn(x) = n
2
[n/2]
X
m=0
(−1)m (n −m −1)!
m!(n −2m)! (2x)n−2m
(18.114)
for n ≥1, with [n/2] the integer part of n/2 and
Un(x) =
[n/2]
X
m=0
(−1)m
(n −m)!
m!(n −2m)! (2x)n−2m.
(18.115)
Application to Numerical Analysis
An important feature of the Chebyshev polynomials Tn(x) with n > 0 is that as x is varied,
they oscillate between the extreme values Tn = +1 and Tn = −1. This behavior is readily
seen from Eq. (18.105) and is illustrated for T12 in Fig. 18.6. If a function is expanded in
the Tn and the expansion is extended sufﬁciently that the contributions of successive Tn are
decreasing rapidly, a good approximation to the truncation error will be proportional to the
ﬁrst Tn not included in the expansion. In this approximation, there will be negligible error
at the n values of x where Tn is zero, and there will be maximum errors (all of the same
magnitude but alternating in sign) at the extrema of Tn that fall between the zeros. In that
sense, the errors satisfy a minimax principle, meaning that the maximum of the error has
been minimized by distributing it evenly into the regions between the points of negligible
error.
−1
−0.5
0.5
1
1
5
.
0
5
.
0
−
1
−
FIGURE 18.6
The Chebyshev polynomial T12.

906
Chapter 18 More Special Functions
Example 18.4.1
MINIMIZING THE MAXIMUM ERROR
Figure 18.7 shows the errors in four-term expansions of ex on the range [−1,1] carried
out in various ways: (a) Maclaurin series, (b) Legendre expansion, and (c) Chebyshev
expansion. The power series is optimum at the point x = 0 and the error increases with
increasing values of |x|. The orthogonal expansions produce a ﬁt over the region [−1,1],
with the maximum errors occurring at x = ±1 and three intermediate values of x. How-
ever, the Legendre expansion has larger errors at ±1 than it has at the interior points, while
the Chebyshev expansion yields smaller errors at ±1 (with a concomitant increase in the
error at the other maxima) with the result that all the error maxima are comparable. This
choice approximately minimizes the maximum error.
0.006
(a)
(c)
(b)
−0.006
−1
1
FIGURE 18.7
Error in four-term approximations to ex: (a) Power series; (b) Legendre
expansion; and (c) Chebyshev expansion.
■
Orthogonality
If Eq. (18.97) is put into self-adjoint form (Section 8.2), we obtain w(x) = (1 −x2)−1/2
as a weighting factor. For Eq. (18.98) the corresponding weighting factor is (1 −x2)+1/2.

18.4 Chebyshev Polynomials
907
The resulting orthogonality integrals,
1
Z
−1
Tm(x)Tn(x)(1 −x2)−1/2 dx =



0, m ̸= n,
π
2 , m = n ̸= 0,
π, m = n = 0,
(18.116)
1
Z
−1
Vm(x)Vn(x)(1 −x2)−1/2 dx =



0, m ̸= n,
π
2 , m = n ̸= 0,
0, m = n = 0,
(18.117)
1
Z
−1
Um(x)Un(x)(1 −x2)1/2 dx = π
2 δmn,
(18.118)
and
1
Z
−1
Wm(x)Wn(x)(1 −x2)1/2 dx = π
2 δmn,
(18.119)
are a direct consequence of the Sturm-Liouville theory. The normalization values may best
be obtained by making the substitution x = cosθ.
Exercises
18.4.1
By evaluating the generating function for special values of x, verify the special values
Tn(1) = 1,
Tn(−1) = (−1)n,
T2n(0) = (−1)n,
T2n+1(0) = 0.
18.4.2
By evaluating the generating function for special values of x, verify the special values
Un(1) = n + 1,
Un(−1) = (−1)n(n + 1),
U2n(0) = (−1)n,
U2n+1(0) = 0.
18.4.3
Another Chebyshev generating function is
1 −xt
1 −2xt + t2 =
∞
X
n=0
Xn(x)tn,
|t| < 1.
How is Xn(x) related to Tn(x) and Un(x)?
18.4.4
Given
(1 −x2)U′′
n (x) −3xU′
n(x) + n(n + 2)Un(x) = 0,
show that Vn(x), Eq. (18.106), satisﬁes
(1 −x2)V ′′
n (x) −xV ′
n(x) + n2Vn(x) = 0,
which is Chebyshev’s equation.

908
Chapter 18 More Special Functions
18.4.5
Show that the Wronskian of Tn(x) and Vn(x) is given by
Tn(x)V ′
n(x) −T ′
n(x)Vn(x) = −
n
(1 −x2)1/2 .
This veriﬁes that Tn and Vn (n ̸= 0) are independent solutions of Eq. (18.97). Con-
versely, for n = 0, we do not have linear independence. What happens at n = 0? Where
is the “second” solution?
18.4.6
Show that Wn(x) = (1 −x2)−1/2Tn+1(x) is a solution of
(1 −x2)W ′′
n (x) −3xW ′
n(x) + n(n + 2)Wn(x) = 0.
18.4.7
Evaluate the Wronskian of Un(x) and Wn(x) = (1 −x2)−1/2Tn+1(x).
18.4.8
Vn(x) = (1 −x2)1/2Un−1(x) is not deﬁned for n = 0. Show that a second and inde-
pendent solution of the Chebyshev differential equation for Tn(x) (n = 0) is V0(x) =
arccos x (or arcsin x).
18.4.9
Show that Vn(x) satisﬁes the same three-term recurrence relation as Tn(x), Eq. (18.92).
18.4.10
Verify the series solutions for Tn(x) and Un(x), Eqs. (18.114) and (18.115).
18.4.11
Transform the series form of Tn(x), Eq. (18.114), into an ascending power series.
ANS. T2n(x) = (−1)nn
n
X
m=0
(−1)m (n + m −1)!
(n −m)!(2m)!(2x)2m, n ≥1,
T2n+1(x) = 2n + 1
2
n
X
m=0
(−1)m+n(n + m)!
(n −m)!(2m + 1)!(2x)2m+1.
18.4.12
Rewrite the series form of Un(x), Eq. (18.115), as an ascending power series.
ANS.
U2n(x) = (−1)n
n
X
m=0
(−1)m
(n + m)!
(n −m)!(2m)!(2x)2m,
U2n+1(x) = (−1)n
n
X
m=0
(−1)m
(n + m + 1)!
(n −m)!(2m + 1)!(2x)2m+1.
18.4.13
(a)
From the differential equation for Tn (in self-adjoint form) show that
1
Z
−1
dTm(x)
dx
dTn(x)
dx
(1 −x2)1/2dx = 0,
m ̸= n.
(b)
Conﬁrm the preceding result by showing that
dTn(x)
dx
= nUn−1(x).
18.4.14
The substitution x = 2x′ −1 converts Tn(x) into the shifted Chebyshev polynomials
T ∗
n (x′). Verify that this produces the shifted polynomials shown in Table 18.5 and that

18.4 Chebyshev Polynomials
909
Table 18.5
Shifted Type I Chebyshev Polynomials
T ∗
0 = 1
T ∗
1 = 2x −1
T ∗
2 = 8x2 −8x + 1
T ∗
3 = 32x3 −48x2 + 18x −1
T ∗
4 = 128x4 −256x3 + 160x2 −32x + 1
T ∗
5 = 512x5 −1280x4 + 120x3 −400x2 + 50x −1
T ∗
6 = 2048x6 −6144x5 + 6912x4 −3584x3 + 840x2 −72x + 1
they satisfy the orthonormality condition
1
Z
0
T ∗
n (x′)Tm(x′)[x(1 −x)]−1/2 dx = δmnπ
2 −δn0
.
18.4.15
The expansion of a power of x in a Chebyshev series leads to the integral
Imn =
1
Z
−1
xmTn(x)
dx
√
1 −x2 .
(a)
Show that this integral vanishes for m < n.
(b)
Show that this integral vanishes for m + n odd.
18.4.16
Evaluate the integral
Imn =
1
Z
−1
xmTn(x)
dx
√
1 −x2
for m ≥n and m + n even by each of two methods:
(a)
Replacing Tn(x) by its Rodrigues representation.
(b)
Using x = cosθ to transform the integral to a form with θ as the variable.
ANS.
Imn = π
m!
(m −n)!
(m −n −1)!!
(m + n)!!
,
m ≥n, m + n even.
18.4.17
Establish the following bounds, −1 ≤x ≤1:
(a) |Un(x)| ≤n + 1,
(b)

d
dx Tn(x)
 ≤n2.
18.4.18
(a)
Show that for −1 ≤x ≤1,
|Vn(x)| ≤1.
(b)
Show that Wn(x) is unbounded in −1 ≤x ≤1.

910
Chapter 18 More Special Functions
18.4.19
Verify the orthogonality-normalization integrals for
(a)
Tm(x), Tn(x),
(b)
Vm(x), Vn(x),
(c)
Um(x), Un(x),
(d)
Wm(x), Wn(x).
Hint. All these can be converted to trigonometric integrals.
18.4.20
Show whether
(a)
Tm(x) and Vn(x) are or are not orthogonal over the interval [−1,1] with respect
to the weighting factor (1 −x2)−1/2.
(b)
Um(x) and Wn(x) are or are not orthogonal over the interval [−1,1] with respect
to the weighting factor (1 −x2)1/2.
18.4.21
Derive
(a)
Tn+1(x) + Tn−1(x) = 2xTn(x),
(b)
Tm+n(x) + Tm−n(x) = 2Tm(x)Tn(x), from the “corresponding” cosine identities.
18.4.22
A number of equations relate the two types of Chebyshev polynomials. As examples
show that
Tn(x) = Un(x) −xUn−1(x)
and
(1 −x2)Un(x) = xTn+1(x) −Tn+2(x).
18.4.23
Show that
dVn(x)
dx
= −n Tn(x)
√
1 −x2
(a)
using the trigonometric forms of Vn and Tn,
(b)
using the Rodrigues representation.
18.4.24
Starting with x = cosθ and Tn(cosθ) = cosnθ, expand
xk =
eiθ + e−iθ
2
k
and show that
xk =
1
2k−1

Tk(x) +
k
1

Tk−2(x) +
k
2

Tk−4 + ···

,
the series in brackets terminating after the term containing T1 or T0.

18.5 Hypergeometric Functions
911
18.4.25
Develop the following Chebyshev expansions (for [−1,1]):
(a)
(1 −x2)1/2 = 2
π
"
1 −2
∞
X
s=1
(4s2 −1)−1T2s(x)
#
,
(b)
+1,
0 < x ≤1
−1, −1 ≤x < 0
)
= 4
π
∞
X
s=0
(−1)s(2s + 1)−1T2s+1(x).
18.4.26
(a)
For the interval [−1,1] show that
|x| = 1
2 +
∞
X
s=1
(−1)s+1 (2s −3)!!
(2s + 2)!!(4s + 1)P2s(x)
= 2
π + 4
π
∞
X
s=1
(−1)s+1
1
4s2 −1T2s(x).
(b)
Show that the ratio of the coefﬁcient of T2s(x) to that of P2s(x) approaches (πs)−1
as s →∞. This illustrates the relatively rapid convergence of the Chebyshev
series.
Hint. With the Legendre recurrence relations, rewrite x Pn(x) as a linear combination
of derivatives. The trigonometric substitution x = cosθ, Tn(x) = cosnθ is most helpful
for the Chebyshev part.
18.4.27
Show that
π2
8 = 1 + 2
∞
X
s=1
(4s2 −1)−2.
Hint. Apply Parseval’s identity (or the completeness relation) to the results of
Exercise 18.4.26.
18.4.28
Show that
(a)
cos−1 x = π
2 −4
π
∞
X
n=0
1
(2n + 1)2 T2n+1(x).
(b)
sin−1 x = 4
π
∞
X
n=0
1
(2n + 1)2 T2n+1(x).
18.5
HYPERGEOMETRIC FUNCTIONS
In Chapter 7 the hypergeometric equation8
x(1 −x)y′′(x) + [c −(a + b + 1)x]y′(x) −ab y(x) = 0
(18.120)
8This is sometimes called Gauss’ ODE. The solutions are then referred to as Gauss functions.

912
Chapter 18 More Special Functions
was introduced as a canonical form of a linear second-order ODE with regular singularities
at x = 0,1, and ∞. One solution, designated 2F1, is
y(x) = 2F1(a,b;c; x)
= 1 + a b
c
x
1! + a(a + 1)b(b + 1)
c(c + 1)
x2
2! + ···,
c ̸= 0,−1,−2,−3,...,
which is known as the hypergeometric function or hypergeometric series. For real a, b,
and c (the only case considered here), the range of convergence for c > a + b is −1 ≤x ≤
1, while for a +b −1 < c ≤a +b the convergence range is −1 ≤x < 1. For c ≤a +b −1
the hypergeometric series diverges.
The terms of the hypergeometric series are conveniently written in terms of the
Pochhammer symbol, introduced at Eq. (1.72); we repeat the deﬁnition here:
(a)n = a(a + 1)(a + 2)···(a + n −1),
(a)0 = 1.
Using this notation, the hypergeometric function becomes
2F1(a,b;c; x) =
∞
X
n=0
(a)n(b)n
(c)n
xn
n! .
(18.121)
In this form the signiﬁcance of the subscripts 2 and 1 becomes clear. The leading sub-
script 2 indicates that two Pochhammer symbols appear in the numerator and the trail-
ing subscript 1 indicates one Pochhammer symbol in the denominator. The subscripts 2
and 1 are only useful if one intends to discuss analogs of the “standard” hypergeometric
function that involve different numbers of Pochhammer symbols. We retain the subscripts
because we will shortly identify conﬂuent hypergeometric functions with forms similar
to Eq. (18.121) but with only one Pochhammer symbol in the numerator, therefore of the
form 1F1(a;c; z). Note also that the numerator and denominator parameters are set off by
semicolons (actually making the subscripts unnecessary). We retain them to conform to
the most widely used notations for these functions.
Looking further at Eq. (18.121), we note that the series will reduce to zero (for all x)
if c is either zero or a negative integer (unless the denominator is fortuitously cancelled
by a particular choice of a or b). On the other hand, if a or b equals 0 or a negative inte-
ger, the series terminates and the hypergeometric function becomes a polynomial. Many
more or less elementary functions can be represented by the hypergeometric function.9 For
example,
ln(1 + x) = x 2F1(1,1;2;−x).
(18.122)
The hypergeometric equation as a second-order linear ODE has a second independent
solution. The usual form is
y(x) = x1−c 2F1(a + 1 −c,b + 1 −c;2 −c; x),
c ̸= 2,3,4,....
(18.123)
If c is an integer either the two solutions coincide or (barring a rescue by integral a or
integral b) one of the solutions will blow up (see Exercise 18.5.1). In such a case the
second solution is expected to include a logarithmic term.
9With three parameters, a,b, and c, we can represent almost anything.

18.5 Hypergeometric Functions
913
Alternate forms of the hypergeometric ODE include
(1 −z2) d2
dz2
1 −z
2

y

−
h
(a + b + 1)z −(a + b + 1 −2c)
i d
dz
1 −z
2

y

−ab
1 −z
2

y

= 0,
(18.124)
(1 −z2) d2
dz2 y(z2) −

(2a + 2b + 1)z + 1 −2c
z
 d
dz y(z2) −4ab y(z2) = 0.
(18.125)
Contiguous Function Relations
The parameters a,b, and c enter in the same way as the parameter n of Bessel, Legen-
dre, and other special functions. As we found with these functions, we expect recurrence
relations involving unit changes in the parameters a,b, and c. Hypergeometric functions
that differ by ±1 in a parameter are referred to as contiguous functions. Generalizing
this term to include simultaneous unit changes in more than one parameter, we ﬁnd 26
functions contiguous to 2F1(a,b;c; x). Taking them two at a time, we can develop the
formidable total of 325 equations among the contiguous functions. Two typical examples
are
(a −b)
n
c(a + b −1) + 1 −a2 −b2 + [(a −b)2 −1](1 −x)
o
2F1(a,b;c; x)
= (c −a)(a −b + 1)b 2F1(a −1,b + 1;c; x)
+ (c −b)(a −b −1)a 2F1(a + 1,b −1;c; x),
(18.126)
[2a −c + (b −a)x] 2F1(a,b;c; x) = a(1 −x) 2F1(a + 1,b;c; x)
−(c −a) 2F1(a −1,b;c; x).
(18.127)
Many more contiguous relations can be found in AMS-55 or in Olver et al. (Additional
Readings).
Hypergeometric Representations
A number of the special functions introduced in this book can be expressed in terms of
hypergeometric functions. The identiﬁcation can usually be made by noting that these
functions are solutions of ODEs that are special cases of the hypergeometric ODE. It is
also necessary to determine the factors needed to express the functions at the agreed-upon
scale. We cite several examples.
1.
The ultraspherical functions C(α)
n (x) satisfy the ODE given as Eq. (18.99), and since
that equation is a special case of the hypergeometric equation, Eq. (18.120), we see that
ultraspherical functions (and Legendre and Chebyshev functions) may be expressed as

914
Chapter 18 More Special Functions
hypergeometric functions. For the ultraspherical function we obtain
C(α)
n (x) =
(n + 2α)!
2αn!0(α + 1) 2F1

−n,n + 2α + 1;1 + α; 1 −x
2

,
(18.128)
with the factor preceding the 2F1 function determined by requiring C(α)
n
to have the
proper scale.
2.
For Legendre and associated Legendre functions we ﬁnd
Pn(x) = 2F1

−n,n + 1;1; 1 −x
2

,
(18.129)
Pm
n (x) = (n + m)!
(n −m)!
(1 −x2)m/2
2mm!
2F1

m −n,m + n + 1;m + 1; 1 −x
2

.
(18.130)
Alternate forms for the Legendre functions are
P2n(x) = (−1)n (2n)!
22nn!n! 2F1

−n,n + 1
2; 1
2; x2

= (−1)n (2n −1)!!
(2n)!!
2F1

−n,n + 1
2; 1
2; x2

,
(18.131)
P2n+1(x) = (−1)n (2n + 1)!
22nn!n! x 2F1

−n,n + 3
2; 3
2; x2

= (−1)n (2n + 1)!!
(2n)!!
x 2F1

−n,n + 3
2; 3
2; x2

.
(18.132)
3.
The Chebyshev functions have representations
Tn(x) = 2F1

−n,n; 1
2; 1 −x
2

,
(18.133)
Un(x) = (n + 1) 2F1

−n,n + 2; 3
2; 1 −x
2

,
(18.134)
Vn(x) = n
p
1 −x2 2F1

−n + 1,n + 1; 3
2; 1 −x
2

.
(18.135)
The leading factors are determined by direct comparison of complete power series,
comparison of coefﬁcients of particular powers of the variable, or evaluation at
x = 0 or 1.
The hypergeometric series may be used to deﬁne functions with nonintegral indices. The
physical applications are minimal.

18.5 Hypergeometric Functions
915
Exercises
18.5.1
(a)
For c, an integer, and a and b nonintegral, show that
2F1(a,b;c; x)
and
x1−c 2F1(a + 1 −c,b + 1 −c;2 −c; x)
yield only one solution to the hypergeometric equation.
(b)
What happens if a is an integer, say, a = −1, and c = −2?
18.5.2
Find the Legendre, Chebyshev I, and Chebyshev II recurrence relations corresponding
to the hypergeometric contiguous function relation given as Eq. (18.126).
18.5.3
Transform the following polynomials into hypergeometric functions of argument x2:
(a)
T2n(x);
(b)
x−1T2n+1(x);
(c)
U2n(x);
(d)
x−1U2n+1(x).
ANS.
(a) T2n(x) = (−1)n 2F1(−n,n; 1
2; x2).
(b) x−1T2n+1(x) = (−1)n(2n + 1) 2F1
 −n,n + 1; 3
2; x2
.
(c) U2n(x) = (−1)n 2F1
 −n,n + 1; 1
2; x2
.
(d) x−1U2n+1(x) = (−1)n(2n + 2) 2F1
 −n,n + 2; 3
2; x2
.
18.5.4
Derive or verify the leading factor in the hypergeometric representations of the
Chebyshev functions.
18.5.5
Verify that the Legendre function of the second kind, Qν(z), is given by
Qν(z) =
π1/2ν!
0(ν + 3
2)(2z)ν+1 2F1
ν
2 + 1
2, ν
2 + 1; ν
2 + 3
2; z−2

,
where |z| > 1, |arg z| < π, and ν ̸= −1,−2,−3,···.
18.5.6
The incomplete beta function was deﬁned in Eq. (13.78) as
Bx(p,q) =
x
Z
0
t p−1(1 −t)q−1 dt.
Show that
Bx(p,q) = p−1x p 2F1(p,1 −q; p + 1; x).
18.5.7
Verify the integral representation
2F1(a,b;c; z) =
0(c)
0(b)0(c −b)
1
Z
0
tb−1(1 −t)c−b−1(1 −tz)−a dt.
What restrictions must be placed on the parameters b and c?

916
Chapter 18 More Special Functions
Note. Although the power series used to establish this integral representation is only
valid for |z| < 1, the representation is valid for general z, as can be established by
analytic continuation. For nonintegral a the real axis in the z-plane from 1 to ∞is a cut
line.
Hint. The integral is suspiciously like a beta function and can be expanded into a series
of beta functions.
ANS.
c > b > 0.
18.5.8
Prove that
2F1(a,b;c;1) = 0(c)0(c −a −b)
0(c −a)0(c −b),
c ̸= 0,−1,−2,...,
c > a + b.
Hint. Here is a chance to use the integral representation in Exercise 18.5.7.
18.5.9
Prove that
2F1(a,b;c; x) = (1 −x)−a 2F1

a,c −b;c; −x
1 −x

.
Hint. Try an integral representation.
Note. This relation is useful in developing a Rodrigues representation of Tn(x) (see
Exercise 18.5.10).
18.5.10
Derive the Rodrigues representation of Tn(x),
Tn(x) = (−1)nπ1/2(1 −x2)1/2
2n(n −1
2)!
dn
dxn
h
(1 −x2)n−1/2i
.
Hint. One possibility is to use the hypergeometric function relation
2F1(a,b;c; z) = (1 −z)−a 2F1

a,c −b;c; −z
1 −z

,
with z = (1−x)/2. An alternate approach is to develop a ﬁrst-order differential equation
for y = (1 −x2)n−1/2. Repeated differentiation of this equation leads to the Chebyshev
equation.
18.5.11
Show that the summation in Eq. (18.43),
min(m,n)
X
ν=0
(−m)ν(−n)ν
ν!
1 −m −n
2

ν

a2
2(a2 −1)
ν
,
can be written as a hypergeometric function.
18.5.12
Verify that
2F1(−n,b;c;1) = (c −b)n
(c)n
.
Hint. Here is a chance to use the contiguous function relation Eq. (18.127) and math-
ematical induction (Section 1.4). Alternatively, use the integral representation and the
beta function.

18.6 Conﬂuent Hypergeometric Functions
917
18.6
CONFLUENT HYPERGEOMETRIC FUNCTIONS
The conﬂuent hypergeometric equation,10
xy′′(x) + (c −x)y′(x) −ay(x) = 0,
(18.136)
has a regular singularity at x = 0 and an irregular one at x = ∞. It is obtained from the
hypergeometric equation of Section 18.5 in the limit that one of the singularities at ﬁnite x
is merged with that at inﬁnity, causing that singularity to become irregular. One solution
of the conﬂuent hypergeometric equation is
y(x) = 1F1(a;c; x) = M(a,c, x)
= 1 + a
c
x
1! + a(a + 1)
c(c + 1)
x2
2! + ···,
c ̸= 0,−1,−2,···.
(18.137)
The notation M(a,c, x) (with commas, not semicolons) has become standard for this solu-
tion. It is convergent for all ﬁnite x (or complex z). In terms of the Pochhammer symbols,
we have
M(a,c, x) =
∞
X
n=0
(a)n
(c)n
xn
n! .
(18.138)
Clearly, M(a,c, x) becomes a polynomial if the parameter a is 0 or a negative integer.
Numerous more or less elementary functions may be represented by the conﬂuent hyper-
geometric function. Examples are the error function and the incomplete gamma function:
erf(x) =
2
π1/2
x
Z
0
e−t2dt =
2
π1/2 x M
1
2, 3
2,−x2

,
(18.139)
γ (a, x) =
x
Z
0
e−tta−1dt = a−1xa M(a,a + 1,−x),
ℜe(a) > 0.
(18.140)
A second solution of Eq. (18.136) is given by
y(x) = x1−cM(a + 1 −c,2 −c, x),
c ̸= 2,3,4,···.
(18.141)
Clearly, this coincides with the ﬁrst solution for c = 1.
The standard form of the second solution of Eq. (18.136) is a linear combination of
Eqs. (18.137) and (18.141):
U(a,c, x) =
π
sinπc

M(a,c, x)
0(a −c + 1)0(c) −x1−cM(a + 1 −c,2 −c, x)
0(a)0(−c)

.
(18.142)
Note the resemblance to our deﬁnition of the Neumann function, Eq. (14.57). As with the
Neumann function, this deﬁnition of U(a,c, x) becomes indeterminate for certain param-
eter values, namely when c is an integer.
10This is often called Kummer’s equation. The solutions, then, are Kummer functions.

918
Chapter 18 More Special Functions
An alternate form of the conﬂuent hypergeometric equation is obtained by changing the
independent variable from x to x2:
d2
dx2 y(x2) +
2c −1
x
−2x
 d
dx y(x2) −4ay(x2) = 0.
(18.143)
As with the hypergeometric functions, contiguous functions exist in which the param-
eters a and c are changed by ±1. Including the cases of simultaneous changes in the two
parameters, we have eight possibilities. Taking the original function and pairs of the con-
tiguous functions, we can develop a total of 28 equations. The recurrence relations for
Bessel, Hermite, and Laguerre functions are special cases of these equations.
Integral Representations
It is frequently convenient to have the conﬂuent hypergeometric functions in integral form.
We ﬁnd (Exercise 18.6.10)
M(a,c, x) =
0(c)
0(a)0(c −a)
1
Z
0
extta−1(1 −t)c−a−1dt,
c > a > 0,
(18.144)
U(a,c, x) =
1
0(a)
∞
Z
0
e−xtta−1(1 + t)c−a−1dt,
ℜe(x) > 0,a > 0.
(18.145)
Three important techniques for deriving or verifying integral representations are as fol-
lows:
1.
Transformation of generating function expansions and Rodrigues representations: The
Bessel and Legendre functions provide examples of this approach.
2.
Direct integration to yield a series: This direct technique is useful for a Bessel function
representation (Exercise 14.1.17) and a hypergeometric integral (Exercise 18.5.7).
3.
(a) Veriﬁcation that the integral representation satisﬁes the ODE. (b) Exclusion of
the other solution. (c) Veriﬁcation of normalization. This is the method used in Sec-
tion 14.6 to establish an integral representation of the modiﬁed Bessel function Kν(z).
It will work here to establish Eqs. (18.144) and (18.145).
Conﬂuent Hypergeometric Representations
Special functions that can be represented in terms of conﬂuent hypergeometric functions
include the following:
1.
Bessel functions:
Jν(x) =
e−ix
0(ν + 1)
x
2
ν
M

ν + 1
2,2ν + 1,2ix

,
(18.146)

18.6 Conﬂuent Hypergeometric Functions
919
whereas for the modiﬁed Bessel functions of the ﬁrst kind,
Iν(x) =
e−x
0(ν + 1)
x
2
ν
M

ν + 1
2,2ν + 1,2x

.
(18.147)
2.
Hermite functions:
H2n(x) = (−1)n (2n)!
n!
M

−n, 1
2, x2

.
(18.148)
H2n+1(x) = (−1)n 2(2n + 1)!
n!
x M

−n, 3
2, x2

,
(18.149)
using Eq. (13.150).
3.
Laguerre functions:
Ln(x) = M(−n,1, x).
(18.150)
The constant is ﬁxed as unity by noting Eq. (18.54) for x = 0. For the associated
Laguerre functions,
Lm
n (x) = (−1)m dm
dxm Ln+m(x) = (n + m)!
n!m!
M(−n,m + 1, x).
(18.151)
Alternate veriﬁcation is obtained by comparing Eq. (18.151) with the power-series solu-
tion, Eq. (18.59). Note that in the hypergeometric form, as distinct from a Rodrigues rep-
resentation, the indices n and m need not be integers, but if they are not integers, Lm
n (x)
will not be a polynomial.
Further Observations
There are certain advantages in expressing our special functions in terms of hypergeo-
metric and conﬂuent hypergeometric functions. If the general behavior of the latter func-
tions is known, the behavior of the special functions we have investigated follows as a
series of special cases. This may be useful in determining asymptotic behavior or evalu-
ating normalization integrals. The asymptotic behavior of M(a,c, x) and U(a,c, x) may
be conveniently obtained from integral representations of these functions, Eqs. (18.144)
and (18.145). The further advantage is that the relations between the special functions are
clariﬁed. For instance, an examination of Eqs. (18.148), (18.149), and (18.151) suggests
that the Laguerre and Hermite functions are related.
The conﬂuent hypergeometric equation, Eq. (18.136), is clearly not self-adjoint. For this
and other reasons it is convenient to deﬁne
Mkµ(x) = e−x/2xµ+1/2M(µ −k + 1
2,2µ + 1, x).
(18.152)
This new function, Mkµ(x), is called a Whittaker function; it satisﬁes the self-adjoint equa-
tion
M′′
kµ(x) +
 
−1
4 + k
x +
1
4 −µ2
x2
!
Mkµ(x) = 0.
(18.153)

920
Chapter 18 More Special Functions
The corresponding second solution is
Wkµ(x) = e−x/2xµ+1/2U(µ −k + 1
2,2µ + 1, x).
(18.154)
Exercises
18.6.1
Verify the conﬂuent hypergeometric representation of the error function
erf(x) = 2x
π1/2 M
1
2, 3
2,−x2

.
18.6.2
Show that the Fresnel integrals C(x) and s(x) of Exercise 12.6.1 may be expressed in
terms of the conﬂuent hypergeometric function as
C(x) + is(x) = x M
1
2, 3
2, iπx2
2

.
18.6.3
By direct differentiation and substitution verify that
y = ax−a
x
Z
0
e−tta−1 dt = ax−aγ (a, x)
satisﬁes
xy′′ + (a + 1 + x)y′ + ay = 0.
18.6.4
Show that the modiﬁed Bessel function of the second kind, Kν(x), is given by
Kν(x) = π1/2e−x(2x)νU(ν + 1
2,2ν + 1,2x).
18.6.5
Show that the cosine and sine integrals of Section 13.6 may be expressed in terms of
conﬂuent hypergeometric functions as
Ci(x) + i si(x) = −eixU(1,1,−ix).
This relation is useful in numerical computation of Ci(x) and si(x) for large values of x.
18.6.6
Verify the conﬂuent hypergeometric form of the Hermite polynomial H2n+1(x),
Eq. (18.149), by showing that
(a)
H2n+1(x)/x satisﬁes the conﬂuent hypergeometric equation with a = −n, c = 3/2
and argument x2,
(b)
lim
x→0
H2n+1(x)
x
= (−1)n 2(2n + 1)!
n!
.
18.6.7
Show that the contiguous conﬂuent hypergeometric function equation
(c −a)M(a −1,c, x) + (2a −c + x)M(a,c, x) −aM(a + 1,c, x) = 0
leads to the associated Laguerre function recurrence relation, Eq. (18.66).

18.6 Conﬂuent Hypergeometric Functions
921
18.6.8
Verify the Kummer transformations:
(a)
M(a,c, x) = ex M(c −a,c,−x),
(b)
U(a,c, x) = x1−cU(a −c + 1,2 −c, x).
18.6.9
Prove that
(a)
dn
dxn M(a,c, x) = (a)n
(b)n
M(a + n,b + n, x),
(b)
dn
dxn U(a,c, x) = (−1)n(a)nU(a + n,c + n, x).
18.6.10
Verify the following integral representations:
(a)
M(a,c, x) =
0(c)
0(a)0(c −a)
Z 1
0
extta−1(1 −t)c−a−1dt, c > a > 0,
(b)
U(a,c, x) =
1
0(a)
Z ∞
0
e−xtta−1(1 + t)c−a−1dt,
ℜe(x) > 0, a > 0.
Under what conditions can you accept ℜe(x) = 0 in part (b)?
18.6.11
From the integral representation of M(a,c, x), Exercise 18.6.10(a), show that
M(a,c, x) = ex M(c −a,c,−x).
Hint. Replace the variable of integration t by 1 −s to release a factor ex from the
integral.
18.6.12
From the integral representation of U(a,c, x) in Exercise 18.6.10(b), show that the
exponential integral is given by
E1(x) = e−xU(1,1, x).
Hint. Replace the variable of integration t in E1(x) by x(1 + s).
18.6.13
From the integral representations of M(a,c, x) and U(a,c, x) in Exercise 18.6.10,
develop asymptotic expansions of
(a) M(a,c, x),
(b) U(a,c, x).
Hint. You can use the technique that was employed with Kν(z) in Section 14.6.
ANS.
(a) 0(c)
0(a)
ex
xc−a

1 + (1 −a)(c −a)
1! x
+ (1 −a)(2 −a)(c −a)(c −a + 1)
2! x2
+ ···

,
(b) 1
xa

1 + a(1 + a −c)
1!(−x)
+ a(a + 1)(1 + a −c)(2 + a −c)
2!(−x)2
+ ···

.

922
Chapter 18 More Special Functions
18.6.14
Show that the Wronskian of the two conﬂuent hypergeometric functions M(a,c, x) and
U(a,c, x) is given by
MU′ −M′U = −(c −1)!
(a −1)!
ex
xc .
What happens if a is 0 or a negative integer?
18.6.15
The Coulomb wave equation (radial part of the Schrödinger equation with Coulomb
potential) is
d2y
dr2 +

1 −2η
r −L(L + 1)
r2

y = 0.
Show that a regular solution y = FL(η,r) is given by
FL(η,r) = CL(η)r L+1e−ir M(L + 1 −iη,2L + 2,2ir).
18.6.16
(a)
Show that the radial part of the hydrogen-atom wave function, Eq. (18.81), may
be written as
e−αr/2(αr)L L2L+1
n−L−1(αr) =
(n + L)!
(n −L −1)!(2L + 1)!e−αr/2(αr)L M(L + 1 −n,2L + 2,αr).
(b)
It was assumed previously that the total (kinetic + potential) energy E of the
electron was negative. Rewrite the (unnormalized) radial wave function for an
unbound hydrogenic electron, E > 0.
ANS.
eiαr/2(αr)L M(L + 1 −in,2L + 2,−iαr), outgoing wave. This repre-
sentation provides a powerful alternative technique for the calculation of
photoionization and recombination coefﬁcients.
18.6.17
Evaluate
(a)
Z ∞
0
[Mkµ(x)]2dx,
(b)
Z ∞
0
[Mkµ(x)]2 dx
x ,
(c)
Z ∞
0
[Mkµ(x)]2 dx
x1−a ,
where 2µ = 0,1,2,...,
k −µ−1
2 = 0,1,2,...,
a > −2µ−1.
ANS.
(a) 2k(2µ)!, (b) (2µ)!, (c) (2k)a(2µ)!.

18.7 Dilogarithm
923
18.7
DILOGARITHM
The dilogarithm, deﬁned as
Li2(z) = −
z
Z
0
ln(1 −t)
t
dt
(18.155)
and its analytic continuation beyond the range of convergence of the above integral, arises
in the evaluation of matrix elements in few-body problems of atomic physics and in vari-
ous perturbation-theoretic contributions to quantum electrodynamics. Because of a historic
lack of familiarity with this special function among physicists, many places of its occur-
rence have only been recognized in recent years.
Expansion and Analytic Properties
Expanding the logarithm in Eq. (18.155), using the series in Eq. (1.97), we directly obtain
the series expansion
Li2(z) =
∞
X
n=1
zn
n2 .
(18.156)
Note that we have inserted the logarithm without an additional multiple of 2πi, thereby
obtaining the branch of Li2 that is nonsingular at z = 0.
Further applications of the operator that converts −ln(1 −z) into Li2(z) produce poly-
logarithms, which also occur in physics, albeit less frequently:
Lip(z) =
z
Z
0
Lip−1(t)dt
t =
∞
X
n=1
zn
n p
p = 3, 4,....
(18.157)
However, in this text we limit consideration to the ﬁrst member of this sequence, Li2.
The series expansion of Li2, Eq. (18.156), has circle of convergence |z| = 1, with con-
vergence for all z on this circle. The singularity limiting the radius of convergence is not
apparent from the form of the expansion, but, looking at Eq. (18.155), we identify it as a
branch point located at z = 1. It is customary to draw a branch cut from z = 1 to z = ∞
along, and just below the positive real axis, and to deﬁne the principal value of Li2 as that
which corresponds to Eq. (18.156) and its analytic continuation.
From the form of Eq. (18.156), it is apparent that for real z in the interval −1 ≤z ≤+1,
Li2(z) will also be real. For z > 1, we see from Eq. (18.155) that for part of the range of
integration, the factor ln(1−t) will necessarily be complex, with the result that Li2(z) will
no longer be real, even for real z. However, there is no similar problem for negative real z,
as the principal value of ln(1 −t) remains real for all negative real values of t.
Analyzing further the behavior of the integral in Eq. (18.155), we note that if we reach
a point z by carrying out the integral, along a path (in t) that goes ﬁrst from t = 0 to just
above the branch point at t = 1, and then in a straight line to z, we will for the last segment
of the path alter the argument of 1−t by some amount θ in the clockwise direction, thereby
adding an amount −iθ to the numerator of the integrand. See Fig. 18.8.

924
Chapter 18 More Special Functions
0
1
z
z
.
2π −θ
0
1
.
θ
FIGURE 18.8
Contours for integral representation of dilogarithm.
This addition to the numerator means that the evaluation of Li2(z) will have the form
Li2(z) = −
1
Z
0
ln(1 −t)
t
dt −
z
Z
1
ln(|1 −t|)
t
dt + iθ
z
Z
1
dt
t
= Li2(1) −
z
Z
1
ln(|1 −t|)
t
dt + iθ ln z
(path above z = 1).
(18.158)
If we repeat the above analysis to reach the same point z by a path (in t) that passes around
z = 1 below the real axis, the argument of 1 −t will be changed by an amount 2π −θ in
the counterclockwise direction, and
Li2(z) = Li2(1) −
z
Z
1
ln(|1 −t|)
t
dt −i(2π −θ)ln z
(path below z = 1).
(18.159)
Comparing Eqs. (18.158) and (18.159), we see that the values of Li2(z), for the same z,
but on these two different branches the values, will differ by an amount 2πi ln z. If z is
complex, the difference will affect both the real and imaginary parts of Li2(z), in ways
more complicated than either changing the phase or adding a multiple of π to the imagi-
nary part. When working with the dilogarithm, it is therefore essential to make a careful
determination of the branch on which it is to be evaluated. In fact, whenever possible
formulas involving the dilogarithm and (because of the context) known to be real-valued
should be manipulated (using formulas such as those in the next subsection) to cause each
dilogarithm in the formula to be for a value of z that is real and with z < 1.
Properties and Special Values
From Eq. (18.156), we see that Li2(0) = 0. Setting z = 1, we note that we get the series
for ζ(2), so Li2(1) = ζ(2) = π2/6. We also have Li2(−1) = −η(2), where η(2) is the
Dirichlet series in Eq. (12.62), so Li2(−1) = −π2/12.

18.7 Dilogarithm
925
The dilogarithm has a derivative that follows directly from Eq. (18.155),
d Li2(z)
dz
= −ln(1 −z)
z
,
(18.160)
and possesses several functional relations enabling an easy analytic continuation beyond
the convergence range of Eq. (18.156). Some of these are the following:
Li2(z) + Li2(1 −z) = π2
6 −ln z ln(1 −z)
(18.161)
Li2(z) + Li2(z−1) = −π2
6 −1
2 ln2(−z)
(18.162)
Li2(z) + Li2

z
z −1

= −1
2 ln2(1 −z).
(18.163)
These relationships are most easily established by showing that the derivatives of both
sides of the equations are equal and that the values of the two sides correspond for some
convenient value of z. These functional relations enable the determination of Li2(z) for all
real z from values on the real line in the range |z| ≤1
2, for which the series in Eq. (18.155)
converges rapidly.
From the functional relations it is possible to identify a few more speciﬁc values of z
for which the principal value of Li2(z) can be expressed in terms of elementary functions.
For example, Li2(1/2) = −1
2 ln2(2) + π2/12. But for most z, closed expressions are not
available.
Example 18.7.1
CHECK USEFULNESS OF FORMULA
The integral
I =
1
8π2
Z Z
d3r1d3r2
e−αr1−βr2−γr12
r2
1r2
2r12
arises in computations of the electronic structure of the He atom. Here ri are the positions
of two electrons relative to the nucleus (which is at the origin of our coordinate system),
the integration is over the full three-dimensional spaces of r1 and r2, ri = |ri|, and r12 =
|r1 −r2|.
This integral is found to have the value
I = 1
γ
π2
6 + Li2
γ −β
α + γ

+ Li2
γ −α
β + γ

+ 1
2 ln2
α + γ
β + γ

.
We now ask: Are its individual terms real?
We note from the deﬁnition of I that it will be convergent only if α+β, α+γ , and β +γ
are all positive. If that is not the case, in the portion of the space in which some particle
is far from the other two, the overall exponential will increase without limit. Looking now

926
Chapter 18 More Special Functions
at the formula for the integral, we see immediately that the ln2 term will be real, as its
argument is the quotient of two positive numbers. The ﬁrst Li2 term can be written
Li2
γ −β
α + γ

= Li2

1 −α + β
α + γ

,
showing that the argument of Li2 is real and less than +1, meaning that this Li2 will
evaluate to a real result. Similar observations apply to the second instance of Li2. We
conclude that our formula is in a proper form for unambiguous computation using principal
values of its multivalued functions.
■
Exercises
18.7.1
Prove that the expansion of Li2(z), Eq. (18.156), converges everywhere on the circle
|z| = 1.
18.7.2
Use the functional relations, Eqs. (18.161) to (18.163), to ﬁnd the principal value of
Li2(1/2).
18.7.3
Find all the multiple values of Li2(1/2).
18.7.4
Explain why Eq. (18.161) gives the expected result for z = 0 when on the principal
branch of the dilogarithm.
18.7.5
Show that
Li2
1 + z−1
2

= −Li2
1 + z
1 −z

−1
2 ln2
1 −z−1
2

.
18.7.6
The following integral arises in the computation of the electronic energy of the Li atom
using a correlated wave function (one that explicitly includes the electron-electron dis-
tances as well as the distances of electrons from the nucleus):
I =
Z Z Z
d3r1d3r2d3r3
e−α1r1−α2r2−α3r3
r1r2r3r12r13r23
,
where ri = |ri|, ri j = |ri −r j|, and the integrations are over the entire three-dimensional
space of each ri. For convergence of I, we require all α j > 0, but there are no restric-
tions on their relative magnitudes.
In terms of the auxiliary quantities
ζ1 =
α1
α2 + α3
,
ζ2 =
α2
α1 + α3
,
ζ3 =
α3
α1 + α2
,
this integral has the value
I = 32π3
α1α2α3

−π2
2 +
3
X
j=1

Li2(ζ j) −Li2(−ζ j) + lnζ j ln
1 −ζ j
1 + ζ j



18.8 Elliptic Integrals
927
Rearrange I to a form (ﬁrst found by Remiddi11), in which all terms in the ﬁnal expres-
sion are guaranteed to evaluate to real quantities and can be evaluated as principal val-
ues.
18.8
ELLIPTIC INTEGRALS
Elliptic integrals occasionally arise in physical problems and therefore it is worthwhile to
summarize their deﬁnitions and properties. Before the advent of computers, it was also
important for physicists and engineers to be familiar with methods for hand computation
of elliptic integrals, but that need has diminished with time and expansion methods for
these functions will not be emphasized here. We do, however, illustrate problems in which
elliptic integrals arise; the following example is a case in point.
Example 18.8.1
PERIOD OF A SIMPLE PENDULUM
For small-amplitude oscillations, a pendulum (Fig. 18.9) has simple harmonic motion with
a period T = 2π(l/g)1/2. But for a maximum amplitude θM large enough that sinθM can-
not be approximated by θM, a direct application of Newton’s second law of motion and
solution of the resulting ODE becomes difﬁcult. In that situation a good way to proceed
is to write the equation for conservation of energy. Setting the zero of potential energy at
the point from which the pendulum is suspended, the potential energy of a pendulum of
mass m and length l at angle θ is −mgl cosθ, and its total energy (the potential energy
at angle θM) is −mgl cosθM. The pendulum has kinetic energy ml2(dθ/dt)2/2, so energy
conservation requires
1
2ml2
dθ
dt
2
−mgl cos θ = −mgl cos θM.
(18.164)
Solving for dθ/dt we obtain
dθ
dt = ±
2g
l
1/2
(cos θ −cos θM)1/2,
(18.165)
θ
m
FIGURE 18.9
Simple pendulum.
11E. Remiddi, Analytic value of the atomic three-electron correlation integral with Slater wave functions. Phys. Rev. A 44:
5492 (1991).

928
Chapter 18 More Special Functions
with the mass m canceling out. At t = 0 we choose as initial conditions θ = 0 and dθ/dt >
0. An integration from θ = 0 to θ = θM yields
θM
Z
0
(cosθ −cosθM)−1/2dθ =
2g
l
1/2
tZ
0
dt =
2g
l
1/2
t.
(18.166)
This is 1
4 of a cycle, and therefore the time t is 1
4 of the period T . We note that θ ≤θM,
and with a bit of clairvoyance we try the half-angle substitution
sin
θ
2

= sin
θM
2

sinϕ.
(18.167)
With this, Eq. (18.166) becomes
T = 4
 l
g
1/2 π/2
Z
0

1 −sin2
θM
2

sin2 ϕ
−1/2
dϕ.
(18.168)
The integral in Eq. (18.168) does not reduce to an elementary function; in fact, it is an ellip-
tic integral of a standard type. Further examples of elliptic integrals in physical problems
can be found in the exercises.
■
Deﬁnitions
The elliptic integral of the ﬁrst kind is deﬁned as
F(ϕ\α) =
ϕ
Z
0
(1 −sin2 α sin2 θ)−1/2dθ,
(18.169)
or
F(x|m) =
x
Z
0
h
(1 −t2)(1 −mt2)
i −1/2
dt,
0 ≤m < 1.
(18.170)
This is the notation of AMS-55 (Additional Readings). Note the use of the separators \ and
| to identify the speciﬁc functional forms. When the upper limit in these integrals is set to
ϕ = π/2 or x = 1, we have the complete elliptic integral of the ﬁrst kind,
K(m) =
π/2
Z
0
(1 −m sin2 θ)−1/2dθ
=
1
Z
0
h
(1 −t2)(1 −mt2)
i −1/2
dt,
(18.171)
with m = sin2 α, 0 ≤m < 1.

18.8 Elliptic Integrals
929
The elliptic integral of the second kind is deﬁned by
E(ϕ\α) =
ϕ
Z
0
(1 −sin2 α sin2 θ)1/2dθ
(18.172)
or
E(x|m) =
x
Z
0
1 −mt2
1 −t2
1/2
dt,
0 ≤m ≤1.
(18.173)
Again, for the case ϕ = π/2, x = 1, we have the complete elliptic integral of the second
kind:
E(m) =
π/2
Z
0
(1 −m sin2 θ)1/2dθ
=
1
Z
0
1 −mt2
1 −t2
1/2
dt,
0 ≤m ≤1.
(18.174)
Series Expansions
For our range 0 ≤m < 1, the denominator of K(m) may be expanded by the binomial
series in Eq. (1.74):
(1 −m sin2 θ)−1/2 =
∞
X
n=0
(2n −1)!!
(2n)!!
mn sin2n θ,
after which the resulting series is then integrated term by term. The integrals of the indi-
vidual terms are beta functions (see Exercise 13.3.8), and we get
K(m) = π
2
(
1 +
∞
X
n=1
(2n −1)!!
(2n)!!
2
mn
)
.
(18.175)
Similarly (see Exercise 18.8.2),
E(m) = π
2
(
1 −
∞
X
n=1
(2n −1)!!
(2n)!!
2
mn
2n −1
)
.
(18.176)
These series can be identiﬁed as hypergeometric functions. Comparing with the general
deﬁnitions in Section 18.5, we have
K(m) = π
2 2F1( 1
2, 1
2;1;m),
E(m) = π
2 2F1(−1
2, 1
2;1;m).
(18.177)
The complete elliptic integrals are plotted in Fig. 18.10.

930
Chapter 18 More Special Functions
3.0
2.0
1.0
0
π/2
1.0
1.0
0.5
m
E(m)
K(m)
FIGURE 18.10
Complete elliptic integrals, K(m) and E(m).
Limiting Values
From the series Eqs. (18.175) and (18.176), or from the deﬁning integrals,
lim
m→0 K(m) = π
2 ,
lim
m→0 E(m) = π
2 .
(18.178)
For m →1 the series expansions are of little use. However, the integrals yield
lim
m→1 K(m) = ∞,
lim
m→1 E(m) = 1.
(18.179)
The divergence in K(m) is logarithmic.
Elliptic integrals have been used extensively in the past for evaluating integrals. For
instance, general integrals of the form
I =
x
Z
0
R

t,
p
a4t4 + a3t3 + a2t2 + a1t1 + a0

dt,
where R is a rational function of its arguments, may be expressed in terms of elliptic
integrals. Jahnke and Emde (Additional Readings) give pages of such transformations.
With computers available for direct numerical evaluation, interest in these elliptic integral
techniques has declined. A more extensive account of elliptic functions, integrals, and
the related Jacobi theta functions can be found in Whittaker and Watson’s treatise. Many

18.8 Elliptic Integrals
931
formulas and tables of elliptic integrals are in AMS-55 and even more formulas are in
Olver et al. (all of these sources are in the Additional Readings).
Exercises
18.8.1
The ellipse x2/a2 + y2/b2 = 1 may be represented parametrically by x = a sinθ, y =
b cosθ. Show that the length of arc within the ﬁrst quadrant is
a
π/2
Z
0
(1 −m sin2 θ)1/2dθ = aE(m).
Here 0 ≤m = (a2 −b2)/a2 ≤1.
18.8.2
Derive the series expansion
E(m) = π
2
(
1 −
1
2
2 m
1 −
1 · 3
2 · 4
2 m2
3 −···
)
.
18.8.3
Show that
lim
m→0
(K −E)
m
= π
4 .
18.8.4
A circular loop of wire in the xy-plane, as shown in Fig. 18.11, carries a current I.
Given that the vector potential is
Aϕ(ρ,ϕ, z) = aµ0I
2π
π
Z
0
cosα dα
(a2 + ρ2 + z2 −2aρ cosα)1/2 ,
x
a
y
ϕ
(ρ, ϕ, z)
(ρ, ϕ, 0)
ρ
z
FIGURE 18.11
Circular wire loop.

932
Chapter 18 More Special Functions
show that
Aϕ(ρ,ϕ, z) = µ0I
πk
a
ρ
1/2 
1 −k2
2

K(k2) −E(k2)

,
where
k2 =
4aρ
(a + ρ)2 + z2 .
Note. For extension of this exercise to B, see Smythe.12
18.8.5
An analysis of the magnetic vector potential of a circular current loop leads to the
expression
f (k2) = k−2h
(2 −k2)K(k2) −2E(k2)
i
,
where K(k2) and E(k2) are the complete elliptic integrals of the ﬁrst and second kinds.
Show that for k2 ≪1(r ≫radius of loop)
f (k2) ≈πk2
16 .
18.8.6
Show that
(a)
dE(k2)
dk
= 1
k (E −K),
(b)
dK(k2)
dk
=
E
k(1 −k2) −K
k .
Hint. For part (b) show that
E(k2) = (1 −k2)
π/2
Z
0
(1 −k sin2 θ)−3/2dθ
by comparing series expansions.
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions, Applied Mathematics Series-55
(AMS-55). Washington, DC: National Bureau of Standards (1964), paperback edition, Dover (1974). Chapter
22 is a detailed summary of the properties and representations of orthogonal polynomials. Other chapters
summarize properties of Bessel, Legendre, hypergeometric, and conﬂuent hypergeometric functions and much
more. See also Olver et al., below.
Buchholz, H., The Conﬂuent Hypergeometric Function. New York: Springer Verlag (1953), translated (1969).
Buchholz strongly emphasizes the Whittaker rather than the Kummer forms. Applications to a variety of other
transcendental functions.
12W. R. Smythe, Static and Dynamic Electricity, 3rd ed. New York: McGraw-Hill (1969), p. 270.

Additional Readings
933
Erdelyi, A., W. Magnus, F. Oberhettinger, and F. G. Tricomi, Higher Transcendental Functions, 3 vols. New
York: McGraw-Hill (1953), reprinted, Krieger (1981). A detailed, almost exhaustive listing of the properties
of the special functions of mathematical physics.
Fox, L., and I. B. Parker, Chebyshev Polynomials in Numerical Analysis. Oxford: Oxford University Press (1968).
A detailed, thorough, but very readable account of Chebyshev polynomials and their applications in numerical
analysis.
Gradshteyn, I. S., and I. M. Ryzhik, Table of Integrals, Series and Products (A. Jeffrey and D. Zwillinger, eds.),
7th ed. New York: Academic Press (2007).
Jahnke, E., and F. Emde, Tables of Functions with Formulae and Curves. Leipzig: Teubner (1933), Dover (1945).
Jahnke, E., F. Emde, and F. Lösch, Tables of Higher Functions, 6th ed. New York: McGraw-Hill (1960). An
enlarged update of the work by Jahnke and Emde.
Lebedev, N. N., Special Functions and Their Applications (translated by R. A. Silverman). Englewood Cliffs,
NJ: Prentice-Hall (1965), paperback, Dover (1972).
Luke, Y. L., The Special Functions and Their Approximations, 2 vols. New York: Academic Press (1969).
Volume 1 is a thorough theoretical treatment of gamma functions, hypergeometric functions, conﬂuent hyper-
geometric functions, and related functions. Volume 2 develops approximations and other techniques for
numerical work.
Luke, Y. L., Mathematical Functions and Their Approximations. New York: Academic Press (1975). This is
an updated supplement to Handbook of Mathematical Functions with Formulas, Graphs and Mathematical
Tables (AMS-55).
Magnus, W., F. Oberhettinger, and R. P. Soni, Formulas and Theorems for the Special Functions of Mathematical
Physics. New York: Springer (1966). An excellent summary of just what the title says.
Olver, F. W. J., D. W. Lozier, R. F. Boisvert, and C. W. Clark, eds., NIST Handbook of Mathematical Functions.
Cambridge: Cambridge University Press (2010). Update of AMS-55 (Abramowitz and Stegun, above), but
links to computer programs are provided instead of tables of data.
Rainville, E. D., Special Functions. New York: Macmillan (1960), reprinted, Chelsea (1971). This book is a
coherent, comprehensive account of almost all the special functions of mathematical physics that the reader is
likely to encounter.
Sansone, G., Orthogonal Functions (translated by A. H. Diamond). New York: Interscience (1959), reprinted,
Dover (1991).
Slater, L. J., Conﬂuent Hypergeometric Functions. Cambridge: Cambridge University Press (1960). This is a
clear and detailed development of the properties of the conﬂuent hypergeometric functions and of relations of
the conﬂuent hypergeometric equation to other ODEs of mathematical physics.
Sneddon, I. N., Special Functions of Mathematical Physics and Chemistry, 3rd ed. New York: Longman (1980).
Whittaker, E. T., and G. N. Watson, A Course of Modern Analysis. Cambridge: Cambridge University Press,
reprinted (1997). The classic text on special functions and real and complex analysis.

CHAPTER 19
FOURIER SERIES
Periodic phenomena involving waves, rotating machines (harmonic motion), or other
repetitive driving forces are described by periodic functions. Fourier series are a basic
tool for solving ordinary differential equations (ODEs) and partial differential equations
(PDEs) with periodic boundary conditions. Fourier integrals for nonperiodic phenomena
are developed in Chapter 20. The common name for the ﬁeld is Fourier analysis.
19.1
GENERAL PROPERTIES
A Fourier series is deﬁned as an expansion of a function or representation of a function in
a series of sines and cosines, such as
f (x) = a0
2 +
∞
X
n=1
an cosnx +
∞
X
n=1
bn sinnx.
(19.1)
The coefﬁcients a0,an, and bn are related to f (x) by deﬁnite integrals:
an = 1
π
2π
Z
0
f (s)cosns ds,
n = 0,1,2,...,
(19.2)
bn = 1
π
2π
Z
0
f (s)sinns ds,
n = 1,2,...,
(19.3)
which are subject to the requirement that the integrals exist. Note that a0 is singled out for
special treatment by the inclusion of the factor 1
2. This is done so that Eq. (19.2) will apply
to all an, n = 0 as well as n > 0.
The conditions imposed on f (x) to make Eq. (19.1) valid are that f (x) have only a
ﬁnite number of ﬁnite discontinuities and only a ﬁnite number of extreme values (max-
ima and minima) in the interval [0,2π].1 Functions satisfying these conditions may be
1These conditions are sufﬁcient but not necessary.
935
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

936
Chapter 19 Fourier Series
called piecewise regular. The conditions themselves are known as the Dirichlet condi-
tions. Although there are some functions that do not obey these conditions, they can be
considered pathological for purposes of Fourier expansions. In the vast majority of physi-
cal problems involving a Fourier series, the Dirichlet conditions will be satisﬁed.
Expressing cosnx and sinnx in exponential form, we may rewrite Eq. (19.1) as
f (x) =
∞
X
n=−∞
cneinx,
(19.4)
in which
cn = 1
2 (an −ibn),
c−n = 1
2 (an + ibn),
n > 0,
(19.5)
and
c0 = 1
2 a0.
(19.6)
Sturm-Liouville Theory
The ODE
−y′′(x) = λy(x)
on the interval [0,2π] with boundary conditions y(0) = y(2π), y′(0) = y′(2π) is a
Sturm-Liouville problem, and these boundary conditions make it Hermitian. Therefore
its eigenfunctions, either cosnx (n = 0, 1,...) and sinnx (n = 1, 2,...), or exp(inx)
(n = ...,−1, 0, 1,...), form a complete set, with eigenfunctions of different eigenval-
ues orthogonal. Since the eigenfunctions have respective values n2, those of different |n|
will automatically be orthogonal, while those of the same |n| can be orthogonalized if
necessary. Deﬁning the scalar product for this problem as
⟨f |g⟩=
2π
Z
0
f ∗(x)g(x)dx,
it is easy to check that ⟨einx|e−inx⟩= 0 for n ̸= 0, and if we write cosnx and sinnx
as complex exponentials, it is also easy to see that ⟨sinnx| cosnx⟩= 0. To make the
eigenfunctions normalized, a simple approach is to note that the average value of sin2 nx
or cos2 nx over an integer number of oscillations is 1/2 (again for n ̸= 0), so
2π
Z
0
sin2 nx dx =
2π
Z
0
cos2 nx dx = π
(n ̸= 0),
and ⟨einx|einx⟩= 2π.
The relationships identiﬁed above indicate that the eigenfunctions ϕn = einx/
√
2π, (n =
...,−1, 0, 1,...) form an orthonormal set, as do
ϕ0 =
1
√
2π
, ϕn = cosnx
√π , ϕ−n = sinnx
√π ,
(n = 1, 2,...),

19.1 General Properties
937
so expansions in these functions have the forms given in Eqs. (19.1) to (19.3) or Eqs. (19.4)
to (19.6). Since we know that the eigenfunctions of a Sturm-Liouville operator form a
complete set, we know that our Fourier-series expansions of L2 functions will at least
converge in the mean.
Discontinuous Functions
There are signiﬁcant differences between the behavior of Fourier- and power-series
expansions. A power series is essentially an expansion about a point, using only infor-
mation from that point about the function to be expanded (including, of course, the values
of its derivatives). We already know that such expansions only converge within a radius of
convergence deﬁned by the position of the nearest singularity. However, a Fourier series
(or any expansion in orthogonal functions) uses information from the entire expansion
interval, and therefore can describe functions that have “nonpathological” singularities
within that interval. However, we also know that the representation of a function by an
orthogonal expansion is only guaranteed to converge in the mean. This feature comes into
play for the expansion of functions with discontinuities, where there is no unique value to
which the expansion must converge. However, for Fourier series, it can be shown that if a
function f (x) satisfying the Dirichlet conditions is discontinuous at a point x0, its Fourier
series evaluated at that point will be the arithmetic average of the limits of the left and right
approaches:
fFourier series(x0) = lim
ε→0
 f (x0 + ε) + f (x0 −ε)
2

.
(19.7)
For proof of Eq. (19.7), see Jeffreys and Jeffreys or Carslaw (Additional Readings). It
can also be shown that if the function to be expanded is continuous but has a ﬁnite dis-
continuity in its ﬁrst derivative, its Fourier series will then exhibit uniform convergence
(see Churchill, Additional Readings). These features make Fourier expansions useful for
functions with a variety of types of discontinuities.
Example 19.1.1
SAWTOOTH WAVE
An idea of the convergence of a Fourier series and the error in using only a ﬁnite number
of terms in the series may be obtained by considering the expansion of
f (x) =
(
x,
0 ≤x < π,
x −2π,
π < x ≤2π.
(19.8)
This is a sawtooth wave form, as shown in Fig. 19.1. Using Eqs. (19.2) and (19.3), we ﬁnd
the expansion to be
f (x) = 2

sin x −sin2x
2
+ sin3x
3
−··· + (−1)n+1 sinnx
n
+ ···

.
(19.9)
Figure 19.2 shows f (x) for 0 ≤x < 2π for the sum of 4, 6, and 10 terms of the series.
Three features deserve comment.

938
Chapter 19 Fourier Series
2π
π
−π
π
FIGURE 19.1
Sawtooth wave form.
2
4
6
10 terms
0
−2
π
FIGURE 19.2
Expansion of sawtooth wave form, range [0,2π].
1.
There is a steady increase in the accuracy of the representation as the number of terms
included is increased.
2.
At x = π, where f (x) changes discontinuously from +π to −π, all the curves pass
through the average of these two values, namely f (π) = 0.
3.
In the vicinity of the discontinuity at x = π, there is an overshoot that persists and
shows no sign of diminishing.
As a matter of incidental interest, setting x = π/2 in Eq. (19.9) leads to
f
π
2

= π
2 = 2

1 −0 −1
3 −0 + 1
5 −0 −1
7 + ···

,

19.1 General Properties
939
thereby yielding an alternate derivation of Leibniz’s formula for π/4, which was obtained
by another method in Exercise 1.3.2.
■
Periodic Functions
Fourier series are used extensively to represent periodic functions, especially wave forms
for signal processing. The form of the series is inherently periodic; the expansions in
Eqs. (19.1) and (19.4) are periodic with period 2π, with sinnx, cosnx, and exp(inx),
each completing n cycles of oscillation in that interval. Thus, while the coefﬁcients in a
Fourier expansion are determined from an interval of length 2π, the expansion itself (if
the function involved is actually periodic) applies for an indeﬁnite range of x. The period-
icity also means that the interval used for determining the coefﬁcients need not be [0,2π]
but may be any other interval of that length. Often one encounters situations in which the
formulas in Eqs. (19.2) and (19.3) are changed so that their integrations run between −π
and π. In fact, it would have been natural to have restated Example 19.1.1 as dealing with
f (x) = x, for −π < x < π. This of course does not remove the discontinuity or change
the form of the Fourier series. The discontinuity has simply been moved to the ends of the
interval in x.
In actual situations, the natural interval for a Fourier expansion will be the wavelength
of our wave form, so it may make sense to redeﬁne our Fourier series so that Eq. (19.1)
becomes
f (x) = a0
2 +
∞
X
n=1
an cos nπx
L
+
∞
X
n=1
bn sin nπx
L ,
(19.10)
with
an = 1
L
L
Z
−L
f (s)cos nπs
L
ds,
n = 0,1,2,...,
(19.11)
bn = 1
L
L
Z
−L
f (s)sin nπs
L
ds,
n = 1,2,....
(19.12)
In many problems the x dependence of a Fourier expansion describes the spatial depen-
dence of a wave distribution that is moving (say, toward +x) with phase velocity v. This
means that in place of x we need to write x −vt, and this substitution carries the implicit
assumption that the wave form retains the same shape as it moves forward.2 The individual
terms of the Fourier expansion can now be given an interesting interpretation. Taking as
an example the term
cos
hnπ
L (x −vt)
i
,
2For waves in physical media, this assumption is by no means always true, as it depends on the time-dependent response
properties of the medium.

940
Chapter 19 Fourier Series
we note that it describes a contribution of wavelength 2L/n (when x increases this much
at constant t, the argument of the cosine function increases by 2π). We also note that
the period of the oscillation (the change in t at constant x for one cycle of the cosine
function) is T = 2L/nv, corresponding to the oscillation frequency ν = nv/2L. If we call
the frequency for n = 1 the fundamental frequency and denote it ν0 = v/2L, we identify
the terms for each n > 1 in the Fourier series as describing overtones, or harmonics of the
fundamental frequency, with individual frequencies nν0.
A typical problem for which Fourier analysis is suitable is one in which a particle under-
going oscillatory motion is subject to a periodic driving force. If the problem is described
by a linear ODE, we may make a Fourier expansion of the driving force and solve for each
harmonic individually. This makes the Fourier expansion a practical tool as well as a nice
analytical device. We stress, however, that its utility depends crucially on the linearity of
our problem; in nonlinear problems an overall solution is not a superposition of component
solutions.
As suggested earlier, we have proceeded on the assumption that v, the phase velocity, is
the same for all terms of the Fourier series. We now see that this assumption corresponds to
the notion that the medium supporting the wave motion can respond equally well to forces
at all frequencies. If, for example, the medium consists of particles too massive to respond
quickly at high frequency, those components of the wave form will become attenuated and
damped out of a propagating wave. Conversely, if the system contains components that
resonate at certain frequencies, the response at those frequencies will be enhanced. Fourier
expansions give physicists (and engineers) a powerful tool for analyzing wave forms and
for designing media (e.g., circuits) that yield desired behaviors.
One question that is sometimes raised is: “Were the harmonics there all along, or were
they created by our Fourier analysis?” One answer compares the functional resolution into
harmonics with the resolution of a vector into rectangular components. The components
may have been present, in the sense that they may be isolated and observed, but the reso-
lution is certainly not unique. Hence many authors prefer to say that the harmonics were
created by our choice of expansion. Other expansions in other sets of orthogonal functions
would produce a different decomposition. For further discussion, we refer to a series of
notes and letters in the American Journal of Physics.3
What if a function is not periodic? We can still obtain its Fourier expansion, but (a) the
results will of course depend on how the expansion interval is chosen (both as to posi-
tion and length), and (b) because no information outside the expansion interval was used
in obtaining the expansion, we can have no realistic expectation that the expansion will
produce there a reasonable approximation to our function.
Symmetry
Suppose we have a function f (x) that is either an even or an odd function of x. If it is
even, then its Fourier expansion cannot contain any odd terms (since all terms are linearly
independent, no odd term can be removed by retaining others). Our expansion, developed
3B. L. Robinson, Concerning frequencies resulting from distortion. Am. J. Phys. 21: 391 (1953); F. W. Van Name, Jr., Concern-
ing frequencies resulting from distortion. Am J. Phys. 22: 94 (1954).

19.1 General Properties
941
for the interval [−π,π], then must take the form
f (x) = a0
2 +
∞
X
n=1
an cosnx,
f (x) even.
(19.13)
On the other hand, if f (x) is odd, we must have
f (x) =
∞
X
n=1
bn sinnx,
f (x) odd.
(19.14)
In both cases, when determining the coefﬁcients we only need consider the interval [0,π],
referring to Eqs. (19.2) and (19.3), as the adjoining interval of length π will make a con-
tribution identical to that considered. The series in Eqs. (19.13) and (19.14) are sometimes
called Fourier cosine and Fourier sine series.
If we have a function deﬁned on the interval [0,π], we can represent it either as a Fourier
sine series or as a Fourier cosine series (or, if it has no interfering singularities, as a power
series), with similar results on the interval of deﬁnition. However, the results outside that
interval may differ markedly because these expansions carry different assumptions as to
symmetry and periodicity.
Example 19.1.2
DIFFERENT EXPANSIONS OF f (x) = x
We consider three possible ways to expand f (x) = x based on its values on the range
[0,π]:
•
Its power-series expansion will (obviously) have the power-series expansion f (x) = x.
•
Comparing with Example 19.1.1, its Fourier sine series will have the form given in
Eq. (19.9).
•
Its Fourier cosine series will have coefﬁcients determined from
an = 2
π
π
Z
0
x cosnx dx =



π,
n = 0,
−4
n2π ,
n = 1, 3, 5,...,
0,
n = 2, 4, 6,...,
corresponding to the expansion
f (x) = π
2 −
∞
X
n=0
4
π
cos(2n + 1)x
(2n + 1)2
.
All three of these expansions represent f (x) well in the range of deﬁnition, [0,π], but their
behavior becomes strikingly different outside that range. We compare the three expansions
for a range larger than [0,π] in Fig. 19.3.
■

942
Chapter 19 Fourier Series
(b)
(a)
(c)
(a)
(c)
(b)
0
(abc)
(ab)
−π
π
2π
FIGURE 19.3
Expansions of f (x) = x on [0,π]: (a) power series, (b) Fourier sine series,
(c) Fourier cosine series.
Operations on Fourier Series
Term-by-term integration of the series
f (x) = a0
2 +
∞
X
n=1
an cosnx +
∞
X
n=1
bn sinnx
(19.15)
yields
x
Z
x0
f (x) dx = a0x
2

x
x0
+
∞
X
n=1
an
n sinnx

x
x0
−
∞
X
n=1
bn
n cosnx

x
x0
.
(19.16)
Clearly, the effect of integration is to place an additional power of n in the denomina-
tor of each coefﬁcient. This results in more rapid convergence than before. Consequently,
a convergent Fourier series may always be integrated term by term, the resulting series
converging uniformly to the integral of the original function. Indeed, term-by-term inte-
gration may be valid even if the original series, Eq. (19.15), is not itself convergent. The
function f (x) need only be integrable. A discussion will be found in Jeffreys and Jeffreys
(Additional Readings).
Strictly speaking, Eq. (19.16) may not be a Fourier series; that is, if a0 ̸= 0, there will
be a term 1
2a0x. However,
x
Z
x0
f (x)dx −1
2a0x
(19.17)
will still be a Fourier series.

19.1 General Properties
943
The situation regarding differentiation is quite different from that of integration. Here
the word is caution. Consider the series for
f (x) = x,
−π < x < π.
(19.18)
We readily found (in Example 19.1.1) that the Fourier series is
x = 2
∞
X
n=1
(−1)n+1 sinnx
n
,
−π < x < π.
(19.19)
Differentiating term by term, we obtain
1 = 2
∞
X
n=1
(−1)n+1 cosnx,
(19.20)
which is not convergent. Warning: Check your derivative for convergence.
For the triangular wave shown in Fig. 19.4 (and treated in Exercise 19.2.9), the Fourier
expansion is
f (x) = π
2 −4
π
∞
X
n=1,odd
cosnx
n2
,
(19.21)
which converges more rapidly than the expansion of Eq. (19.19); in fact, it exhibits uniform
convergence. Differentiating term by term we get
f ′(x) = 4
π
∞
X
n=1,odd
sinnx
n
,
(19.22)
which is the Fourier expansion of a square wave,
f ′(x) =
(
1,
0 < x < π,
−1,
−π < x < 0.
(19.23)
Inspection of Fig. 19.3 veriﬁes that this is indeed the derivative of our triangular wave.
f(x)
x
π
2π
3π
4π
π
−4π
−3π
−2π
−π
FIGURE 19.4
Triangular wave.

944
Chapter 19 Fourier Series
•
As the inverse of integration, the operation of differentiation has placed an additional
factor n in the numerator of each term. This reduces the rate of convergence and may,
as in the ﬁrst case mentioned, render the differentiated series divergent.
•
In general, term-by-term differentiation is permissible if the series to be differentiated
is uniformly convergent.
Summing Fourier Series
Often the most efﬁcient way to identify the function represented by a Fourier series is
simply to identify the expansion in a table. But if it is our desire to sum the series ourselves,
a useful approach is to replace the trigonometric functions by their complex exponential
forms, and then identifying the Fourier series as one or more power series in e±ix.
Example 19.1.3
SUMMATION OF A FOURIER SERIES
Consider the series P∞
n=1(1/n)cosnx, x ∈(0,2π). Since this series is only conditionally
convergent (and diverges at x = 0), we take
∞
X
n=1
cosnx
n
= lim
r→1
∞
X
n=1
rn cosnx
n
,
absolutely convergent for |r| < 1. Our procedure is to try forming power series by trans-
forming the trigonometric functions into exponential form:
∞
X
n=1
rn cosnx
n
= 1
2
∞
X
n=1
rneinx
n
+ 1
2
∞
X
n=1
rne−inx
n
.
Now, these power series may be identiﬁed as Maclaurin expansions of −ln(1 −z), with
z = reix or re−ix. From Eq. (1.97),
∞
X
n=1
rn cosnx
n
= −1
2[ln(1 −reix) + ln(1 −re−ix)]
= −ln[(1 + r2) −2r cos x]1/2.
Setting r = 1, we see that
∞
X
n=1
cosnx
n
= −ln(2 −2cos x)1/2
= −ln

2sin x
2

,
(0 < x < 2π).
(19.24)
Both sides of this expression diverge as x →0 and as x →2π.4
■
4Note that the range of validity of Eq. (19.24) may be shifted to [−π,π] (excluding x = 0) if we replace x by |x| on the
right-hand side.

19.1 General Properties
945
Exercises
19.1.1
A function f (x) (quadratically integrable) is to be represented by a ﬁnite Fourier series.
A convenient measure of the accuracy of the series is given by the integrated square of
the deviation,
1p =
2π
Z
0
"
f (x) −a0
2 −
p
X
n=1
(an cosnx + bn sinnx)
#2
dx.
Show that the requirement that 1p be minimized, that is,
∂1p
∂an
= 0,
∂1p
∂bn
= 0,
for all n, leads to choosing an and bn as given in Eqs. (19.2) and (19.3).
Note. Your coefﬁcients an and bn are independent of p. This independence is a conse-
quence of orthogonality and would not hold if we expanded f (x) in a power series.
19.1.2
In the analysis of a complex waveform (ocean tides, earthquakes, musical tones, etc.),
it might be more convenient to have the Fourier series written as
f (x) = a0
2 +
∞
X
n=1
αn cos(nx −θn).
Show that this is equivalent to Eq. (19.1) with
an = αn cosθn,
α2
n = a2
n + b2
n,
bn = αn sinθn,
tanθn = bn/an.
Note. The coefﬁcients α2
n as a function of n deﬁne what is called the power spectrum.
The importance of α2
n lies in their invariance under a shift in the phase θn.
19.1.3
A function f (x) is expanded in an exponential Fourier series
f (x) =
∞
X
n=−∞
cneinx.
If f (x) is real, f (x) = f ∗(x), what restriction is imposed on the coefﬁcients cn?
19.1.4
Assuming that
R π
−π[ f (x)]2dx is ﬁnite, show that
lim
m→∞am = 0,
lim
m→∞bm = 0.
Hint. Integrate [ f (x) −sn(x)]2, where sn(x) is the nth partial sum, and use Bessel’s
inequality (Section 5.1). For our ﬁnite interval the assumption that f (x) is square inte-
grable (
R π
−π | f (x)|2 dx is ﬁnite) implies that
R π
−π | f (x)| dx is also ﬁnite. The converse
does not hold.

946
Chapter 19 Fourier Series
3π
π
−π
2
π
2
π
−
FIGURE 19.5
Reverse sawtooth wave.
19.1.5
Apply the summation technique of this section to show that
∞
X
n=1
sinnx
n
=
(
1
2(π −x),
0 < x ≤π,
−1
2(π + x),
−π ≤x < 0.
This is the reverse sawtooth wave shown in Fig. 19.5.
19.1.6
Sum the series P∞
n=1(−1)n+1 sinnx
n
and show that it equals x/2.
19.1.7
Sum the trigonometric series P∞
n=0
sin(2n+1)x
2n+1
and show that it equals
(
π/4,
0 < x < π ,
−π/4,
−π < x < 0.
19.1.8
Let f (z) = ln(1 + z) = P∞
n=1
(−1)n+1zn
n
. This series converges to ln(1 + z) for |z| ≤1,
except at the point z = −1.
(a)
From the real parts show that
ln

2cos θ
2

=
∞
X
n=1
(−1)n+1 cosnθ
n
,
−π < θ < π.
(b)
Using a change of variable, transform part (a) into
−ln

2sin θ
2

=
∞
X
n=1
cosnθ
n
,
0 < θ < 2π.
19.1.9
(a)
Expand f (x) = x in the interval (0,2L). Sketch the series you have found (right-
hand side of ANS.) over (−2L,2L).
ANS.
x = L −2L
π
∞
X
n=1
1
n sin
nπx
L

.

19.1 General Properties
947
(b)
Expand f (x) = x as a sine series in the half interval (0, L). Sketch the series you
have found (right-hand side of Ans.) over (−2L,2L).
ANS.
x = 4L
π
∞
X
n=0
1
2n + 1 sin
(2n + 1)πx
L

.
19.1.10
In some problems it is convenient to approximate sinπx over the interval [0,1] by a
parabola ax(1 −x), where a is a constant. To get a feeling for the accuracy of this
approximation, expand 4x(1 −x) in a Fourier sine series (−1 ≤x ≤1):
f (x) =
(4x(1 −x),
0 ≤x ≤1
4x(1 + x), −1 ≤x ≤0
)
=
∞
X
n=1
bn sinnπx.
ANS.
bn = 32
π3
1
n3 ,
n odd,
bn = 0,
n even.
This approximation is shown in Fig. 19.6.
19.1.11
Verify that δ(ϕ1 −ϕ2) =
1
2π
P∞
m=−∞eim(ϕ1−ϕ2) is a Dirac delta function by showing
that it satisﬁes the deﬁnition,
π
Z
−π
f (ϕ1) 1
2π
∞
X
m=−∞
eim(ϕ1−ϕ2)dϕ1 = f (ϕ2).
Hint. Represent f (ϕ1) by an exponential Fourier series.
19.1.12
Show that integration of the Fourier expansion of f (x) = x, −π < x < π, leads to
π2
12 =
∞
X
n=1
(−1)n+1
n2
= 1 −1
4 + 1
9 −1
16 + ··· .
Note. The series for f (x) = x was the subject of Example 19.1.1. Conﬁrm that the
change in the deﬁned range from [0,2π] to [−π,π] has no effect on the expansion.
f(x)
x
1
−1
FIGURE 19.6
Parabolic approximation to sine wave.

948
Chapter 19 Fourier Series
19.1.13
(a)
Assuming that the Fourier expansion of f (x) is uniformly convergent, show that
1
π
π
Z
−π

f (x)
 2 dx = a2
0
2 +
∞
X
n=1
(a2
n + b2
n).
This is Parseval’s identity. Note that it is a completeness relation for the Fourier
expansion.
(b)
Given x2 = π2
3 + 4
∞
X
n=1
(−1)n cosnx
n2
,
−π ≤x ≤π,
apply Parseval’s identity to obtain ζ(4) in closed form.
(c)
The condition of uniform convergence is not necessary. Show this by applying the
Parseval identity to the square wave
f (x) =
(
−1,
−π < x < 0
1,
0 < x < π
= 4
π
∞
X
n=1
sin(2n −1)x
2n −1
.
19.1.14
Given
ϕ1(x) ≡
∞
X
n=1
sinnx
n
=



−1
2(π + x),
−π ≤x < 0,
1
2(π −x),
0 < x ≤π,
show by integrating that
ϕ2(x) ≡
∞
X
n=1
cosnx
n2
=



1
4(π + x)2 −π2
12 ,
−π ≤x ≤0,
1
4(π −x)2 −π2
12 ,
0 ≤x ≤π.
19.1.15
Given
ψ2s(x) =
∞
X
n=1
sinnx
n2s , ψ2s+1(x) =
∞
X
n=1
cosnx
n2s+1 ,
develop the following recurrence relations:
(a)
ψ2s(x) =
x
Z
0
ψ2s−1(x)dx,
(b)
ψ2s+1(x) = ζ(2s + 1) −
x
Z
0
ψ2s(x)dx.

19.2 Applications of Fourier Series
949
Note. The functions ψs(x) and ϕs(x) of this and the preceding exercise are known as
Clausen functions. In theory they may be used to improve the rate of convergence of
a Fourier series. As is often the case, there is the question of how much analytical work
we do and how much arithmetic work we demand that a computer do. As computers
become steadily more powerful, the balance progressively shifts so that we are doing
less and demanding that computers do more.
19.1.16
Show that f (x) = P∞
n=1
cosnx
n+1 may be written as
f (x) = ψ1(x) −ϕ2(x) +
∞
X
n=1
cosnx
n2(n + 1),
where ψ1(x) and ϕ2(x) are the Clausen functions deﬁned in Exercises 19.1.14 and
19.1.15.
19.2
APPLICATIONS OF FOURIER SERIES
We present in this section two typical problems and a short table of useful Fourier series,
followed by a substantial number of exercises that illustrate some of the techniques that
arise in applications.
Example 19.2.1
SQUARE WAVE
One application of Fourier series, the analysis of a “square” wave (Fig. 19.7) in terms of its
Fourier components, occurs in electronic circuits designed to handle sharply rising pulses.
Suppose that our wave is deﬁned by
f (x) = 0,
−π < x < 0,
(19.25)
f (x) = h,
0 < x < π.
f (x)
x
h
π
−3π
3π
−2π
2π
−π
FIGURE 19.7
Square wave.

950
Chapter 19 Fourier Series
From Eqs. (19.2) and (19.3), we ﬁnd
a0 = 1
π
π
Z
0
h dt = h,
an = 1
π
π
Z
0
h cosnt dt = 0,
n = 1,2,3,...,
bn = 1
π
π
Z
0
h sinnt dt = h
nπ (1 −cosnπ)
=



2h
nπ ,
n odd,
0,
n even.
The resulting series is
f (x) = h
2 + 2h
π
sin x
1
+ sin3x
3
+ sin5x
5
+ ···

.
(19.26)
Except for the ﬁrst term, which represents an average of f (x) over the interval [−π,π],
all the cosine terms have vanished. Since f (x) −h/2 is odd, we have a Fourier sine series.
Although only the odd terms in the sine series occur, they fall only as n−1. This conditional
convergence is like that of the alternating harmonic series. Physically this means that our
square wave contains a lot of high-frequency components. If the electronic apparatus will
not pass these components, our square-wave input will emerge more or less rounded off,
perhaps as an amorphous blob.
■
Example 19.2.2
FULL-WAVE RECTIFIER
As a second example, let us ask how well the output of a full-wave rectiﬁer approaches
pure direct current. Our rectiﬁer may be thought of as passing the positive peaks of an
incoming sine wave and inverting the negative peaks, as shown in Fig. 19.8. This yields
f (t) =
(
sinωt,
0 < ωt < π,
−sinωt,
−π < ωt < 0.
(19.27)
Since f (t) as deﬁned here is even, no terms of the form sinnωt will appear. Again, from
Eqs. (14.2) and (14.3), we have
a0 = −1
π
0
Z
−π
sinωt d(ωt) + 1
π
π
Z
0
sinωt d(ωt)
= 2
π
π
Z
0
sinωt d(ωt) = 4
π ,

19.2 Applications of Fourier Series
951
f (t)
ωt
π
2π
−2π
−π
FIGURE 19.8
Full wave rectiﬁer.
an = 2
π
π
Z
0
sinωt cosnωt d(ωt)
=



−2
π
2
n2 −1,
n even,
0,
n odd.
Note that [0,π] is not an orthogonality interval for both sines and cosines together and we
do not get zero when n is even. The resulting series is
f (t) = 2
π −4
π
∞
X
n=2,4,6,...
cosnωt
n2 −1 .
(19.28)
The original frequency, ω, has been eliminated; in fact, all its odd harmonics are also
absent. The lowest-frequency oscillation is 2ω. The high-frequency components fall off as
n−2, showing that the full-wave rectiﬁer does a fairly good job of approximating direct
current. Whether this good approximation is adequate depends on the particular applica-
tion. If the remaining alternating current components are objectionable, they may be further
suppressed by appropriate ﬁlter circuits.
■
These examples bring out two features characteristic of Fourier expansions:5
•
If f (x) has discontinuities, as in the square wave in Example 19.2.1, we can expect the
nth coefﬁcient to be decreasing as O(1/n). Convergence is conditional only.
•
If f (x) is continuous (although possibly with discontinuous derivatives as in the full-
wave rectiﬁer of Example 19.2.2), we can expect the nth coefﬁcient to be decreasing
as 1/n2, that is, absolute convergence.
We close this section by providing, in Table 19.1, a list of Fourier series that have been
introduced either as examples or in the exercises of this chapter. More extensive lists can
be found in the Additional Readings, particularly in the work by Oberhettinger, but also in
the texts by Carslaw, Churchill, and Zygmund.
5G. Raisbeek, Order of magnitude of Fourier coefﬁcients. Am. Math. Mon. 62: 149 (1955).

952
Chapter 19 Fourier Series
Table 19.1
Some Fourier Series Used in This Text
Fourier Series
Reference
1.
∞
X
n=1
sinnx
n
=
(
−1
2(π + x), −π ≤x < 0
1
2(π −x),
0 ≤x < π
Exercise 19.1.5
Exercise 19.2.8
2.
∞
X
n=1
(−1)n+1 sinnx
n
= x
2 ,
−π < x < π
Exercise 19.1.6
Exercise 19.2.7
3.
∞
X
n=0
sin(2n + 1)x
2n + 1
=
(
−π/4, −π < x < 0
+π/4,
0 < x < π
Exercise 19.1.7
Eq. (19.26)
4.
∞
X
n=1
cosnx
n
= −ln

2sin
|x|
2

,
−π < x < π
Exercise 19.1.8(b)
Eq. (19.24)
5.
∞
X
n=1
(−1)n cosnx
n
= −ln
h
2cos
 x
2
i
,
−π < x < π
Exercise 19.1.8(a)
6.
∞
X
n=0
cos(2n + 1)x
2n + 1
= 1
2 ln

cot |x|
2

,
−π < x < π
Exercise 19.2.5
Exercises
19.2.1
Transform the Fourier expansion of a square wave, Eq. (19.26), into a power series.
Show that the coefﬁcients of x1 form a divergent series. Repeat for the coefﬁcients
of x3.
Note. A power series cannot handle a discontinuity. These inﬁnite coefﬁcients are the
result of attempting to beat this basic limitation on power series.
19.2.2
Derive the Fourier series expansion of the Dirac delta function δ(x) in the interval
−π < x < π.
(a)
What signiﬁcance can be attached to the constant term?
(b)
In what region is this representation valid?
(c)
With the identity
N
X
n=1
cosnx = sin(Nx/2)
sin(x/2) cos

N + 1
2
 x
2

,
show that your Fourier representation of δ(x) is consistent with Eq. (5.27).
19.2.3
Expand δ(x −t) in a Fourier series. Compare your result with the bilinear form of
Eq. (5.27).

19.2 Applications of Fourier Series
953
ANS.
δ(x −t) = 1
2π + 1
π
∞
X
n=1
(cosnx cosnt + sinnx sinnt)
= 1
2π + 1
π
∞
X
n=1
cosn(x −t).
19.2.4
Show that integrating the Fourier expansion of the Dirac delta function (Exercise 19.2.2)
leads to the Fourier representation of the square wave, Eq. (19.26), with h = 1.
Note. Integrating the constant term (1/2π) leads to a term x/2π. What are you going
to do with this?
19.2.5
Starting from the Fourier series given as lines 4 and 5 of Table 19.1, show that:
∞
X
n=0
cos(2n + 1)x
2n + 1
= 1
2 ln

cot |x|
2

.
19.2.6
Develop the Fourier series representation of
f (t) =
(
0,
−π ≤ωt ≤0,
sinωt,
0 ≤ωt ≤π.
This is the output of a simple half-wave rectiﬁer. It is also an approximation of the solar
thermal effect that produces “tides” in the atmosphere.
ANS.
f (t) = 1
π + 1
2 sinωt −2
π
∞
X
n=2,4,6,...
cosnωt
n2 −1 .
19.2.7
A sawtooth wave is given by
f (x) = x,
−π < x < π.
Show that
f (x) = 2
∞
X
n=1
(−1)n+1
n
sinnx.
19.2.8
A different sawtooth wave is described by
f (x) =
(−1
2(π + x),
−π ≤x < 0
+ 1
2(π −x),
0 < x ≤π.
Show that f (x) =
∞
X
n=1
(sinnx/n).
19.2.9
A triangular wave (Fig. 19.4) is represented by
f (x) =
(
x,
0 < x < π
−x,
−π < x < 0.

954
Chapter 19 Fourier Series
Represent f (x) by a Fourier series.
ANS.
f (x) = π
2 −4
π
X
n=1,3,5,...
cosnx
n2
.
19.2.10
Expand
f (x) =
(
1,
x2 < x2
0
0,
x2 > x2
0
in the interval [−π,π].
Note. This variable-width square wave is of some importance in electronic music.
19.2.11
A metal cylindrical tube of radius a is split lengthwise into two nontouching halves.
The top half is maintained at a potential +V , the bottom half at a potential −V . See
Fig. 19.9. Separate the variables in Laplace’s equation and solve for the electrostatic
potential for r ≤a. Observe the resemblance between your solution for r = a and the
Fourier series for a square wave.
19.2.12
A metal cylinder is placed in a (previously) uniform electric ﬁeld, E0, with the axis of
the cylinder perpendicular to that of the original ﬁeld.
(a)
Find the perturbed electrostatic potential.
(b)
Find the induced surface charge on the cylinder as a function of angular position.
19.2.13
(a)
Find the Fourier series representation of
f (x) =
(
0,
−π < x ≤0
x,
0 ≤x < π.
(b)
From the Fourier expansion show that
π2
8 = 1 + 1
32 + 1
52 + ··· .
+V
−V
FIGURE 19.9
Cross section of split tube.

19.2 Applications of Fourier Series
955
δn(x)
n
−π
π
−1
2n
1
2n
x
FIGURE 19.10
Rectangular pulse.
19.2.14
Integrate the Fourier expansion of the unit step function
f (x) =
(
0,
−π < x < 0
1,
0 < x < π.
Show that your integrated series agrees with Exercise 19.2.13.
19.2.15
In the interval (−π,π),
δn(x) =
(
n,
|x| < 1/2n,
0,
|x| > 1/2n.
This wave form is the pulse shown in Fig. 19.10.
(a)
Expand δn(x) as a Fourier cosine series.
(b)
Show that your Fourier series agrees with a Fourier expansion of δ(x) in the limit
as n →∞.
19.2.16
Conﬁrm the delta function nature of your Fourier series of Exercise 19.2.15 by showing
that for any f (x) that is ﬁnite in the interval [−π,π] and continuous at x = 0,
π
Z
−π
f (x) [Fourier expansion of δ∞(x)] dx = f (0).
19.2.17
(a)
Show that the Dirac delta function δ(x −a), expanded in a Fourier sine series in
the half-interval (0, L) (0 < a < L) is given by
δ(x −a) = 2
L
∞
X
n=1
sin
nπa
L

sin
nπx
L

.
Note that this series actually describes −δ(x + a) + δ(x −a) in the interval
(−L, L).
(b)
By integrating both sides of the preceding equation from 0 to x, show that the
cosine expansion of the square wave
f (x) =
(0,
0 ≤x < a
1,
a < x < L,

956
Chapter 19 Fourier Series
is
f (x) = 2
π
∞
X
n=1
1
n sin
nπa
L

−2
π
∞
X
n=1
1
n sin
nπa
L

cos
nπx
L

,
for 0 ≤x < L.
(c)
Show that the term 2
π
∞
X
n=1
1
n sin
nπa
L

is the average of f (x) on (0, L).
19.2.18
Verify the Fourier cosine expansion of the square wave, Exercise 19.2.17(b), by direct
calculation of the Fourier coefﬁcients.
19.2.19
(a)
A string is clamped at both ends x = 0 and x = L. Assuming small-amplitude
vibrations, we ﬁnd that the amplitude y(x,t) satisﬁes the wave equation
∂2y
∂x2 = 1
v2
∂2y
∂t2 .
Here v is the wave velocity. The string is set in vibration by a sharp blow at x = a.
Hence we have
y(x,0) = 0,
∂y(x,t)
∂t
= Lv0δ(x −a) at t = 0.
The constant L is included to compensate for the dimensions (inverse length) of
δ(x −a). With δ(x −a) given by Exercise 19.2.17(a), solve the wave equation
subject to these initial conditions.
ANS.
y(x,t) = 2v0L
πv
∞
X
n=1
1
n sin nπa
L
sin nπx
L
sin nπvt
L
.
(b)
Show that the transverse velocity of the string ∂y(x,t)/∂t is given by
∂y(x,t)
∂t
= 2v0
∞
X
n=1
sin nπa
L
sin nπx
L
cos nπvt
L
.
19.2.20
A string, clamped at x = 0 and at x = L, is vibrating freely. Its motion is described by
the wave equation
∂2u(x,t)
∂t2
= v2 ∂2u(x,t)
∂x2
.
Assume a Fourier expansion of the form
u(x,t) =
∞
X
n=1
bn(t)sin nπx
L
and determine the coefﬁcients bn(t). The initial conditions are
u(x,0) = f (x)
and
∂
∂t u(x,0) = g(x).

19.3 Gibbs Phenomenon
957
Note. This is only half the conventional Fourier orthogonality integral interval. How-
ever, as long as only the sines are included here, the Sturm-Liouville boundary condi-
tions are still satisﬁed and the functions are orthogonal.
ANS. bn(t) = An cos nπvt
L
+ Bn sin nπvt
L
,
An = 2
L
L
Z
0
f (x)sin nπx
L
dx,
Bn =
2
nπv
L
Z
0
g(x)sin nπx
L
dx.
19.2.21
(a)
Let us continue the vibrating string problem in Exercise 19.2.20. We assume now
that the presence of a resisting medium will damp the vibrations according to the
equation
∂2u(x,t)
∂t2
= v2 ∂2u(x,t)
∂x2
−k ∂u(x,t)
∂t
.
Introduce a Fourier expansion
u(x,t) =
∞
X
n=1
bn(t)sin nπx
L
and again determine the coefﬁcients bn(t). Take the initial and boundary condi-
tions to be the same as in Exercise 19.2.20. Assume the damping to be small.
(b)
Repeat, but assume the damping to be large.
ANS.
(a)
bn(t) = e−kt/2[An cosωnt + Bn sinωnt], ω2
n =
nπv
L

−
k
2
2
> 0,
An = 2
L
L
Z
0
f (x)sin nπx
L
dx, Bn =
2
ωnL
L
Z
0
g(x)sin nπx
L dx +
k
2ωn
An.
(b)
bn(t) = e−kt/2[An coshσnt + Bn sinhσnt], σ 2
n =
k
2
2
−
nπv
L
2
> 0,
An = 2
L
L
Z
0
f (x)sin nπx
L
dx, Bn =
2
σnL
L
Z
0
g(x)sin nπx
L
dx +
k
2σn
An.
19.3
GIBBS PHENOMENON
The Gibbs phenomenon is an overshoot, a peculiarity of the Fourier series and other eigen-
function series at a simple discontinuity. An example is seen in Fig. 19.2.
Partial Summation of Fourier Series
To better understand the Gibbs phenomenon we examine methods for the partial summa-
tion of Fourier series. This procedure is unlikely to lead to convenient solutions of practical

958
Chapter 19 Fourier Series
problems for which Fourier series are ideal, but it may provide insight that is needed for
our present study.
We start from the Fourier series of a function f (x) in exponential form, truncating it to
retain terms only for n ≤|r| and labeling the truncated expansion fr(x):
fr(x) =
r
X
n=−r
cneinx,
cn = 1
2π
π
Z
−π
f (t)e−int dt.
Combining these equations in a way useful for the present discussion, we have
fr(x) = 1
2π
π
Z
−π
f (t)
r
X
n=−r
ei(x−t)dt.
(19.29)
The summation in Eq. (19.29) is a geometric series. Using a result easily obtained from
Eq. (1.96),
r
X
n=−r
yn = y−r −yr+1
1 −y
= yr+ 1
2 −y−(r+ 1
2 )
y1/2 −y−1/2
,
we set y = ei(x−t), after which we can identify the resulting expression as a quotient of
sine functions:6
r
X
n=−r
ein(x−t) = ei(r+ 1
2 )(x−t) −e−i(r+ 1
2 )(x−t)
ei(x−t)/2 −e−i(x−t)/2
= sin[(r + 1
2)(x −t)]
sin 1
2(x −t)
.
(19.30)
Inserting Eq. (19.30) into Eq. (19.29), we reach
fr(x) = 1
2π
π
Z
−π
f (t)sin[(r + 1
2)(x −t)]
sin 1
2(x −t)
dt.
(19.31)
This is convergent at all points, including t = x. Equation (19.31) shows that the quantity
1
2π
sin[(r + 1
2)(x −t)]
sin 1
2(x −t)
is in the large-r limit a Dirac delta distribution.
Square Wave
For convenience of numerical calculation we consider the behavior of the Fourier series
that represents the periodic square wave
f (x) =



h
2,
0 < x < π,
−h
2,
−π < x < 0.
(19.32)
6This series also occurs in the analysis of a diffraction grating consisting of r slits.

19.3 Gibbs Phenomenon
959
This is essentially the square wave used in Example 19.2.1, and we immediately see that
its Fourier expansion is
f (x) = 2h
π
sin x
1
+ sin3x
3
+ sin5x
5
+ ···

.
(19.33)
Applying Eq. (19.31) to our square wave, we have
fr(x) = h
4π
π
Z
0
sin[(r + 1
2)(x −t)]
sin 1
2(x −t)
dt −h
4π
0
Z
−π
sin[(r + 1
2)(x −t)]
sin 1
2(x −t)
dt.
Making the substitution x −t = s in the ﬁrst integral and x −t = −s in the second, we
obtain
fr(x) = h
4π
x
Z
−π+x
sin(r + 1
2)s
sin 1
2s
ds −h
4π
−x
Z
−π−x
sin(r + 1
2)s
sin 1
2s
ds.
(19.34)
It is important to note that both the integrals in Eq. (19.34) have the same integrand, and
therefore have the same indeﬁnite integral, which we denote 8(t). We may therefore write
f (r) = h
4π
h
8(x) −8(−π + x)
i
−h
4π
h
8(−x) −8(−π −x)
i
= h
4π
h
8(x) −8(−x)
i
−h
4π
h
8(−π + x) −8(−π −x)
i
,
(19.35)
where the second line of Eq. (19.35) is an obvious rearrangement of the ﬁrst. However,
this second line is useful because it shows that we can also write fr(x) as
fr(x) = h
4π
x
Z
−x
sin(r + 1
2)s
sin 1
2s
ds −h
4π
−π+x
Z
−π−x
sin(r + 1
2)s
sin 1
2s
ds.
(19.36)
We are now ready to consider the partial sums in the vicinity of the discontinuity, x = 0.
For small x, the denominator of the second integrand approaches −1, and the second inte-
gral therefore becomes negligible in the limit x →0. On the other hand, the ﬁrst integrand
becomes large near s = 0, and the value of the ﬁrst integral depends on the magnitudes of
r and x. If we now introduce the new variables p = r + 1
2 and ξ = ps, we have (noting
that the integrand is an even function of s)
fr(x) ≈h
2π
px
Z
0
sinξ
sin(ξ/2p)
dξ
p .
(19.37)
Calculation of Overshoot
We are now prepared to make a computation of the Fourier series overshoot. From
Eq. (19.37), we see that for any ﬁnite r, fr(0) will be zero, giving at x = 0 the average
of the two square-wave values (+h/2 and −h/2). However (keeping r ﬁxed), Eq. (19.37)
also tells us that fr(x) will increase as px becomes nonzero, reaching a maximum when

960
Chapter 19 Fourier Series
px = π. This maximum, which we will shortly show constitutes an overshoot, will there-
fore occur at x = π/p, which is approximately x = π/r. We thus see that the location
of the overshoot maximum will differ from x = 0 in a manner approximately inversely
proportional to the number of terms taken in the Fourier expansion.
To estimate the maximum value of fr(x), we substitute px = π into Eq. (19.37), which
we then simplify by making the good approximation sin(ξ/2p) ≈ξ/2p:
fr(xmax) = h
2π
π
Z
0
sinξ dξ
p sin(ξ/2p) ≈h
π
π
Z
0
sinξ
ξ
dξ.
(19.38)
If the upper limit of the ﬁnal integral of Eq. (19.38) were replaced by inﬁnity, we would
have
∞
Z
0
sinξ
ξ
dξ = π
2 ,
(19.39)
a result found in Example 11.8.5. Note that this replacement would cause fr(x) to have
the value h/2, which is the exact value of f (x) for x > 0.
The integral we would have to add to that of Eq. (19.38) to obtain the inﬁnite range is
∞
Z
π
sinξ
ξ
dξ = −si(π) ;
(19.40)
we have identiﬁed this integral as the sine integral function si(x) introduced in Table 1.2
and plotted in Fig. 13.6. Thus,
π
Z
0
sinξ
ξ
dξ = π
2 + si(π).
(19.41)
The graph of si(x) shows that si(π) > 0, indicating an overshoot. A direct demonstration
that our integral is larger than π/2 can also be deduced by writing


∞
Z
0
−
3π
Z
π
−
5π
Z
3π
−···

sinξ
ξ
dξ =
π
Z
0
sinξ
ξ
dξ.
(19.42)
The ﬁrst integral on the left-hand side has value π/2, while each of those to be subtracted
is negative (and therefore makes a further positive contribution).
A Gaussian quadrature or a power-series expansion and term-by-term integration yields
2
π
π
Z
0
sinξ
ξ
dξ = 1.1789797...,
(19.43)
which means that the Fourier series tends to overshoot the positive corner of the square
wave by some 18% and to undershoot the negative corner by the same amount. This behav-
ior is illustrated in Fig. 19.11. The inclusion of more terms (increasing r) does nothing to

19.3 Gibbs Phenomenon
961
x
0.04
0.10
1.2
1.0
0.8
0.6
0.4
0.2
0.1
20 terms
100 terms
80
0.02
0.06
0.08
40
60
FIGURE 19.11
Square wave: Gibbs phenomenon.
remove this overshoot but merely moves it closer to the point of discontinuity. The over-
shoot is the Gibbs phenomenon, and because of it the Fourier series representation may be
highly unreliable for precise numerical work, especially in the vicinity of a discontinuity.
The Gibbs phenomenon is not limited to the Fourier series. It occurs with other eigen-
function expansions. For more details, see W. J. Thompson, Fourier series and the Gibbs
phenomenon, Am. J. Phys. 60: 425 (1992).
Exercises
19.3.1
With the partial-sum summation techniques of this section, show that at a discontinuity
in f (x) the Fourier series for f (x) takes on the arithmetic mean of the right- and left-
hand limits:
f (x0) = 1
2[ f (x0 + 0) + f (x0 −0)].
In evaluating lim
r→∞sr(x0), you may ﬁnd it convenient to identify part of the integrand
as a Dirac delta function.
19.3.2
Determine the partial sum, sn, of the series in Eq. (19.33) by using
(a)
sinmx
m
=
x
Z
0
cosmy dy,
(b)
n
X
p=1
cos(2p −1)y = sin2ny
2sin y .
Do you agree with the result given in Eq. (19.40)?
19.3.3
(a)
Calculate the value of the Gibbs phenomenon integral
I = 2
π
π
Z
0
sint
t
dt
by numerical quadrature accurate to 12 signiﬁcant ﬁgures.

962
Chapter 19 Fourier Series
(b)
Check your result by (1) expanding the integrand as a series, (2) integrating term
by term, and (3) evaluating the integrated series. This calls for double-precision
calculation.
ANS.
I = 1.178979744472.
Additional Readings
Carslaw, H. S., Introduction to the Theory of Fourier’s Series and Integrals, 2nd ed. London: Macmillan (1921);
3rd ed., paperback, Dover (1952). This is a detailed and classic work; includes considerable discussion of
Gibbs phenomenon in chapter IX.
Churchill, R. V., Fourier Series and Boundary Value Problems, 5th ed., New York: McGraw-Hill (1993).
Discusses uniform convergence in Section 38.
Jeffreys, H., and B. S. Jeffreys, Methods of Mathematical Physics, 3rd ed. Cambridge: Cambridge University
Press (1972). Termwise integration of Fourier series is treated in section 14.06.
Kufner, A., and J. Kadlec, Fourier Series. London: Iliffe (1971). This book is a clear account of Fourier series in
the context of Hilbert space.
Lanczos, C., Applied Analysis. Englewood Cliffs, NJ: Prentice-Hall (1956), reprinted, Dover (1988). The book
gives a well-written presentation of the Lanczos convergence technique (which suppresses the Gibbs phe-
nomenon oscillations). This and several other topics are presented from the point of view of a mathematician
who wants useful numerical results and not just abstract existence theorems.
Oberhettinger, F., Fourier Expansions; A Collection of Formulas. New York: Academic Press (1973).
Zygmund, A., Trigonometric Series. Cambridge: Cambridge University Press (1988). The volume contains an
extremely complete exposition, including relatively recent results in the realm of pure mathematics.

CHAPTER 20
INTEGRAL TRANSFORMS
20.1
INTRODUCTION
Frequently in mathematical physics we encounter pairs of functions related by an expres-
sion of the form
g(x) =
b
Z
a
f (t)K(x,t)dt,
(20.1)
where it is understood that a, b, and K(x,t) (called the kernel) will be the same for all
function pairs f and g. We can write the relationship expressed in Eq. (20.1) in the more
symbolic form
g(x) = L f (t),
(20.2)
thereby emphasizing the fact that Eq. (20.1) can be interpreted as an operator equation. The
function g(x) is called the integral transform of f (t) by the operator L, with the speciﬁc
transform determined by the choice of a, b, and K(x,t). The operator deﬁned by Eq. (20.1)
will be linear:
b
Z
a
[ f1(t) + f2(t)]K(x,t)dt =
b
Z
a
f1(t)K(x,t)dt +
b
Z
a
f2(t)K(x,t)dt,
(20.3)
b
Z
a
cf (t)K(α,t)dt = c
b
Z
a
f (t)K(α,t)dt.
(20.4)
In order for transforms to be useful, we will shortly see that we need to be able to
“undo” their effect. From a practical viewpoint, this means that not only must there exist
963
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

964
Chapter 20 Integral Transforms
an operator L−1, but also that we have a reasonably convenient and powerful method of
evaluating
L−1g(x) = f (t)
(20.5)
for an acceptably broad range of g(x). The procedure for inverting a transform takes a
wide variety of forms that depend on the speciﬁc properties of K(x,t), so we cannot write
a formula that is as general as that for L in Eq. (20.1).
Not all superﬁcially reasonable choices for the kernel K(x,t) will lead to operators L
that have inverses, and even for strategically chosen kernels it may be the case that L and
L−1 will only exist for substantially restricted classes of functions. Thus, the entire devel-
opment of the present chapter is restricted (for any given integral transform) to functions
for which the indicated operations can be carried out.
Before embarking on a study of integral transforms, we may well ask, “Why are integral
transforms useful?” Their most common applications are in situations illustrated schemat-
ically in Fig. 20.1, where we have a problem that can be solved only with difﬁculty, if
at all, in its original formulation (usually in ordinary space, sometimes called direct or
physical space). However, it may happen that the transform of the problem can be solved
relatively easily. Our strategy, then, will be to formulate and solve our problem in the trans-
form space, after which we transform the solution back to direct space. This strategy often
works because the most popular integral transforms are changed in simple ways by differ-
entiation and integration operators, with the result that differential and integral equations
assume relatively simple forms. This feature will be discussed and illustrated at length later
in this chapter.
Another frequent use of integral transforms is to use one, together with its inverse, to
form an integral representation of a function that we originally had in an explicit form.
This move (which appears to be in the direction of generating greater complexity) has
value that arises from the relatively simple behavior of the transforms of differentiation
and integration operators. Procedures involving integral representations are also presented
in later sections of this chapter.
Problem in
transform space
Solution in
transform space
Solution of
original problem
Inverse
transform
Original
problem
Integral
transform
Difficult solution
Relatively easy solution
FIGURE 20.1
Schematic: use of integral transforms.

20.1 Introduction
965
Some Important Transforms
The integral transform that has seen the widest use is the Fourier transform, deﬁned as
g(ω) =
1
√
2π
∞
Z
−∞
f (t)eiωtdt.
(20.6)
The notation for this transform is not entirely universal; some writers omit the prefactor
1/
√
2π; we keep it because it causes the transform and its inverse to have formulas that are
more symmetrical. In applications involving periodic systems, one occasionally encounters
a deﬁnition with kernel exp(2πiωt/a0), where a0 is a lattice constant. These differences
in notation do not change the mathematics, but cause formulas to differ by powers of 2π
or a0. Caution is therefore advised when combining material from different sources.
We have deﬁned the Fourier transform in a notation that assigns the symbol ω to the
transform variable. We did so because, in studying signal processing (an important use
of Fourier transforms), the function f (t) usually represents the time behavior of a signal
(typically a wave distribution of some kind). Its Fourier transform, g(ω), can then be iden-
tiﬁed as the corresponding frequency distribution. However, it is worth pointing out that
Fourier transforms turn up in contexts far removed from signal-processing problems; they
can be used to advantage in evaluating integrals, in alternative formulations of quantum
mechanics, and in a wide range of other mathematical procedures.
A second transform that has historically been of great importance is the Laplace
transform,
F(s) =
∞
Z
0
e−ts f (t)dt.
(20.7)
One of its useful features is the fact that under transformation, differential equations
become algebraic equations (as we shall see in detail in Section 20.8). Since algebraic
equations are usually easier to solve than differential equations, this feature lends itself to
the strategy illustrated in Fig. 20.1. A disadvantage of the Laplace transform is that the for-
mula for its inverse is relatively difﬁcult to use. Historically, this difﬁculty was dealt with
by developing tables of Laplace transforms (which can be used to identify inverses). As
digital computers have become more powerful, the use of Laplace transforms has declined,
but they remain sufﬁciently useful that we treat them in some detail in the present chapter.
Among other transforms that have seen signiﬁcant use, we mention here two:
1.
The Hankel transform,
g(α) =
∞
Z
0
f (t)t Jn(αt)dt.
(20.8)
This transform represents the continuum limit of the Bessel series we studied in
Eqs. (14.47) and (14.48).

966
Chapter 20 Integral Transforms
2.
The Mellin transform,
g(α) =
∞
Z
0
f (t)tα−1 dt.
(20.9)
We have actually used the Mellin transform without calling it by name; for example,
g(α) = 0(α) is the Mellin transform of f (t) = e−t. Many Mellin transforms are given
in a text by Titchmarsh (see Additional Readings).
20.2
FOURIER TRANSFORM
We proceed now to a more detailed discussion of the Fourier transform,
g(ω) =
1
√
2π
∞
Z
−∞
f (t)eiωtdt.
(20.10)
If we rewrite the exponential in Eq. (20.10) in terms of the sine and cosine, and then
restrict consideration to functions that are assumed to be either even or odd functions of x,
we obtain variants of the original form that are also useful integral transforms:
gc(ω) =
r
2
π
∞
Z
0
f (t)cosωt dt,
(20.11)
gs(ω) =
r
2
π
∞
Z
0
f (t)sinωt dt.
(20.12)
These formulas deﬁne the Fourier cosine and Fourier sine transforms. Their kernels,
which are real, are natural for use in studies of wave motion and for extracting informa-
tion from waves, particularly when phase information is involved. The output of a stellar
interferometer, for instance, involves a Fourier transform of the brightness across a stellar
disk. The electron distribution in an atom may be obtained from a Fourier transform of the
amplitude of scattered x-rays.
Example 20.2.1
SOME FOURIER TRANSFORMS
1.
f (t) = e−α|t|, with α > 0. To deal with the absolute value, we break the transform
integral into two regions:
g(ω) =
r
1
2π
0
Z
−∞
eαt+iωtdt +
r
1
2π
∞
Z
0
e−αt+iωtdt
=
r
1
2π

1
α + iω +
1
α −iω

=
r
1
2π
2α
α2 + ω2 .
(20.13)

20.2 Fourier Transform
967
We note two features of this result: (1) It is real; from the form of the transform, we
can see that if f (t) is even, its transform will be real. (2) The more localized is f (t),
the less localized will be g(ω). The transform will have an appreciable value until
ω ≫α; larger α corresponds to greater localization of f (t).
2.
f (t) = δ(t). We easily ﬁnd
g(ω) =
r
1
2π
∞
Z
−∞
δ(t)eiωt dt =
r
1
2π .
(20.14)
This is the ultimately localized f (t), and we see that g(ω) is completely delocalized;
it has the same value for all ω.
3.
f (t) = 2α√1/2π /(α2 + t2), with α > 0. One way to evaluate this transform is by
contour integration. It is convenient to start by writing initially
g(ω) = 1
2π
∞
Z
−∞
2α eiωt
(t −iα)(t + iα) dt.
The integrand has two poles: one at t = iα with residue e−αω/i and one at t = −iα
with residue e+αω/(−i). If ω > 0, our integrand will become negligible on a large
semicircle in the upper half-plane, so an integral over the contour shown in Fig. 20.2(a)
will be that needed for g(ω). This contour encloses only the pole at t = iα, so we get
g(ω) = 1
2π (2πi)e−αω
i
(ω > 0).
(20.15)
However, if ω < 0, we must close the contour in the lower half-plane, as in
Fig. 20.2(b), circling the pole at t = −iα in a clockwise sense (thereby generating
a minus sign). This procedure yields
g(ω) = 1
2π (−2πi)e+αω
−i
(ω < 0).
(20.16)
If ω = 0, we cannot perform a contour integration on either of the paths shown in
Fig. 20.2, but we then do not need this sophisticated an approach, as we have the
iα
(a)
(b)
−i α
i α
−iα
FIGURE 20.2
Contours for third transform in Example 20.2.1.

968
Chapter 20 Integral Transforms
elementary integral
g(0) = 1
2π
∞
Z
−∞
2α
t2 + α2 dt = 1.
(20.17)
Combining Eqs. (20.15)–(20.17) and simplifying, we have
g(ω) = e−α|ω|.
Here we Fourier transformed the transform from our ﬁrst example, recovering the
original untransformed function. This provides an interesting clue as to the form to
be expected for the inverse Fourier transform. It is only a clue, because our example
involved a transform that was real (i.e., not complex).
■
An important Fourier transform follows.
Example 20.2.2
FOURIER TRANSFORM OF GAUSSIAN
The Fourier transform of a Gaussian function e−at2, with a > 0,
g(ω) =
1
√
2π
∞
Z
−∞
e−at2eiωt dt,
can be evaluated analytically by completing the square in the exponent,
−at2 + iωt = −a

t −iω
2a
2
−ω2
4a ,
which we can check by evaluating the square. Substituting this identity and changing the
integration variable from t to s = t −iω/2a, we obtain (in the limit of large T )
g(ω) =
1
√
2π
e−ω2/4a
T −iω/2a
Z
−T −iω/2a
e−as2ds.
(20.18)
The s integration, shown in Fig. 20.3, is on a path parallel to, but below the real axis by
an amount iω/2a. But because connections from that path to the real axis at ±T make
negligible contributions to a contour integral and since the contours in Fig. 20.3 enclose no
−T
T
−T −iω/2a
T −iω/ 2a
O
FIGURE 20.3
Contour for transform of Gaussian in Example 20.2.2.

20.2 Fourier Transform
969
singularities, the integral in Eq. (20.18) is equivalent to one along the real axis. Changing
the integration limits to ±∞and rescaling to the new variable ξ = s/√a, we reach
∞
Z
−∞
e−as2dt = 1
√a
∞
Z
−∞
e−ξ2dξ =
rπ
a ,
where we have used Eq. (1.148) to evaluate the error-function integral. Substituting these
results we ﬁnd
g(ω) =
1
√
2a
exp

−ω2
4a

,
(20.19)
again a Gaussian, but in ω-space. An increase in a makes the original Gaussian e−at2
narrower, while making wider its Fourier transform, the behavior of which is dominated
by the exponential e−ω2/4a.
■
Fourier Integral
When we ﬁrst encountered the delta function, its representation which is the large-n limit of
δn(t) = 1
2π
n
Z
−n
eiωtdω,
(20.20)
was identiﬁed as particularly useful in Fourier analysis. We now use that representation
to obtain an important result known as the Fourier integral. We write the fairly obvious
equation,
f (x) = lim
n→∞
∞
Z
−∞
f (t)δn(t −x)dt
= lim
n→∞
1
2π
∞
Z
−∞
f (t)


n
Z
−n
eiω(t−x)dω

dt.
(20.21)
We now interchange the order of integration and take the limit n →∞, reaching
f (x) = 1
2π
∞
Z
−∞
dω
∞
Z
−∞
dt f (t)eiω(t−x).
Finally, we rearrange this equation to the form
f (x) = 1
2π
∞
Z
−∞
e−iωxdω
∞
Z
−∞
f (t)eiωtdt.
(20.22)
Equation (20.22), the Fourier integral, is an integral representation of f (x), and will be
more obviously recognized as such if the inner integration (over t) is performed, leaving

970
Chapter 20 Integral Transforms
unevaluated that over ω. In fact, if we identify the inner integration as (apart from a factor
√1/2π) the Fourier transform of f (t), and label it g(ω) as in Eq. (20.10), then Eq. (20.22)
can be rewritten
f (t) =
r
1
2π
∞
Z
−∞
g(ω)e−iωtdω,
(20.23)
showing that whenever we have the Fourier transform of a function f (t) we can use it to
make a Fourier integral representation of that function.
The Fourier integral formula, written as in Eq. (20.23), illustrates the value of Fourier
analysis in signal processing. If f (t) is an arbitrary signal, Eq. (20.23) describes the signal
as composed of a superposition of waves e−iωt at angular frequencies1 ω, with respec-
tive amplitudes g(ω). Thus, the Fourier integral is the underlying justiﬁcation that one can
express a signal either by its time dependence f (t) or by its (angular) frequency distribu-
tion g(ω).
Before leaving the Fourier integral, we should remark that our derivation of it did not
provide a rigorous justiﬁcation for the reversal of the order of integration and the passage
to the inﬁnite-n limit. The interested reader can ﬁnd a more rigorous treatment in, for
example, the work Fourier Transforms by I. N. Sneddon (Additional Readings).
Example 20.2.3
FOURIER INTEGRAL REPRESENTATION
From the ﬁrst transform of Example 20.2.1, we found that f (t) = e−α|t| has Fourier trans-
form g(ω) = √1/2π 2α/(α2 + ω2). If we substitute these data into Eq. (20.23), we obtain
e−α|t| = f (t) = 1
2π
∞
Z
−∞
2αe−iωt
α2 + ω2 dω = α
π
∞
Z
−∞
e−iωt
α2 + ω2 dω.
(20.24)
Equation (20.24) provides an integral representation for exp(−α|t|) that contains no
absolute value signs and may constitute a useful starting point for various analytical mani-
pulations. We will shortly encounter some more substantive examples with immediate
applications for physics.
■
Inverse Fourier Transform
As the reader may have noticed, Eq. (20.23) is a formula for the inverse Fourier trans-
form. Note that the regular (“direct”) and inverse Fourier transforms are given by very
similar (but not quite identical) formulas. The only difference is in the sign of the complex
exponential. This change of sign causes two successive applications of the Fourier trans-
form not to be identical with applying the transform and then its inverse, and the difference
shows up when g(ω) is not real.2
1The wave e−iωt has period 2π/ω, thus frequency ν = ω/2π. Its angular frequency (radians per unit time rather than cycles) is
2πν = ω.
2Even functions have real Fourier transforms; the transforms of odd functions are imaginary. A function that is neither even nor
odd will have a Fourier transform that is complex.

20.2 Fourier Transform
971
The analysis of the preceding subsection can also be applied to the Fourier cosine and
sine transforms. For convenience, we summarize the formulas for all three varieties of the
Fourier transform and their respective inverses.
g(ω) =
1
√
2π
∞
Z
−∞
f (t)eiωtdt,
(20.25)
f (t) =
1
√
2π
∞
Z
−∞
g(ω)e−iωtdω,
(20.26)
gc(ω) =
r
2
π
∞
Z
0
f (t)cosωt dt,
(20.27)
fc(t) =
r
2
π
∞
Z
0
g(ω)cosωt dω,
(20.28)
gs(ω) =
r
2
π
∞
Z
0
f (t)sinωt dt,
(20.29)
fs(t) =
r
2
π
∞
Z
0
g(ω)sinωt dω.
(20.30)
Note that the Fourier sine and cosine transforms only use data for 0 ≤t < ∞. Therefore,
even though it is possible to evaluate the corresponding inverse transforms for negative t,
the results may be irrelevant to the actual situation at those t values. But if our function
f (t) is even, then the cosine transform will reproduce it faithfully for negative t; odd
functions will be properly described for negative t by the sine transform.
Example 20.2.4
FINITE WAVE TRAIN
An important application of the Fourier transform is the resolution of a ﬁnite pulse into
sinusoidal waves. Imagine that an inﬁnite wave train sinω0t is clipped by Kerr cell or
saturable dye cell shutters so that we have
f (t) =



sinω0t,
|t| < Nπ
ω0
,
0,
|t| > Nπ
ω0
.
(20.31)

972
Chapter 20 Integral Transforms
This corresponds to N cycles of our original wave train (Fig. 20.4). Since f (t) is odd, we
use the Fourier sine transform, Eq. (20.28), to obtain
gs(ω) =
r
2
π
Nπ/ω0
Z
0
sinω0t sinωt dt.
(20.32)
Integrating, we ﬁnd our amplitude function:
gs(ω) =
r
2
π
sin[(ω0 −ω)(Nπ/ω0)]
2(ω0 −ω)
−sin[(ω0 + ω)(Nπ/ω0)]
2(ω0 + ω)

.
(20.33)
It is of considerable interest to see how gs(ω) depends on frequency. For large ω0 and
ω ≈ω0, only the ﬁrst term will be of any importance because of the denominators. It is
plotted in Fig. 20.5. This is the amplitude curve for the single-slit diffraction pattern. It has
zeros at
ω0 −ω
ω0
= 1ω
ω0
= ± 1
N ,± 2
N ,
and so on.
(20.34)
For large N, gs(ω) may also be interpreted as proportional to a Dirac delta distribution.
f(t)
t
t = 5π
ω 0
FIGURE 20.4
Finite wave train.
ω = ω0
ω
gs(ω)
1
2π
ω0
ω0
Nπ
FIGURE 20.5
Fourier transform of ﬁnite wave train.

20.2 Fourier Transform
973
Since a large fraction of the frequency distribution falls within its central maximum, the
half-width of that maximum,
1ω = ω0
N ,
(20.35)
is a good measure of the spread in angular frequency of our wave pulse. Clearly, if N is
large (a long pulse), the frequency spread will be small. On the other hand, if our pulse is
clipped short, N small, the frequency distribution will be wider.
The inverse relationship between frequency spread and pulse length is a fundamental
property of ﬁnite wave distributions; the precision with which a signal can be identiﬁed as a
speciﬁc frequency depends on the pulse length. This same principle ﬁnds expression as the
Heisenberg uncertainty principle of quantum mechanics, in which position uncertainty
(the quantum variable corresponding to pulse length) is inversely related to momentum
uncertainty (quantum analog of frequency). It is worth noting that the uncertainty principle
in quantum mechanics is a consequence of the wave nature of matter and does not depend
on additional ad hoc postulates.
■
Transforms in 3-D Space
Applying the Fourier transform operator in each dimension of a three-dimensional (3-D)
space, we obtain the extremely useful formulas
g(k) =
1
(2π)3/2
Z
f (r)eik·r d3r,
(20.36)
f (r) =
1
(2π)3/2
Z
g(k)e−ik·r d3k.
(20.37)
These integrals are over all space. Veriﬁcation, if desired, follows immediately by sub-
stituting the left-hand side of one equation into the integrand of the other equation and
choosing the integration order that permits the complex exponentials to be identiﬁed as
delta functions in each of the three dimensions. Equation (20.37) may be interpreted as
an expansion of a function f (r) in a continuum of plane waves; g(k) then becomes the
amplitude of the wave exp(−ik · r).
Example 20.2.5
SOME 3-D TRANSFORMS
1.
Let’s ﬁnd the Fourier transform of the Yukawa potential, e−αr/r. Using the notation
[···]T to denote the Fourier transform of the included object, we seek
e−αr
r
T
(k) =
1
(2π)3/2
Z e−αr
r
eik·r d3r.
(20.38)

974
Chapter 20 Integral Transforms
Perhaps the simplest way to proceed is to introduce the spherical wave expansion for
exp(ik · r), Eq. (16.61). Equation (20.38), written in spherical polar coordinates, then
assumes the form
e−αr
r
T
(k) =
4π
(2π)3/2
∞
Z
0
r dr
Z
dr
X
lm
ile−αr jl(kr)Y m
l (k)∗Y m
l (r).
(20.39)
All terms of the angular integration vanish except that with l = m = 0. For that term,
each Y 0
0 has the constant value 1/
√
4π, and Eq. (20.39) reduces to
e−αr
r
T
(k) =
4π
(2π)3/2
∞
Z
0
r e−αr j0(kr)dr.
(20.40)
Inserting j0(kr) = sinkr/kr, the r integration becomes elementary, and we reach
e−αr
r
T
(k) =
1
(2π)3/2
4π
k2 + α2 .
(20.41)
We wrote Eq. (20.41) as we did to make obvious that if the transform were scaled
without the factor 1/(2π)3/2, we would have the well-known result 4π/(k2 + α2).
2.
Even more important than the Fourier transform of the Yukawa potential is that of
the Coulomb potential, 1/r. An attempt to evaluate this transform directly leads to
convergence problems, but it is easy to evaluate it as the limiting case of the Yukawa
potential with α = 0. Thus, we have the extremely important result,
1
r
T
(k) =
1
(2π)3/2
4π
k2 .
(20.42)
3.
From the relation between the Fourier transform and its inverse, Eq. (20.42) can effec-
tively be inverted to yield
 1
r2
T
(k) =
π
2
1/2 1
k .
(20.43)
4.
Another useful Fourier transform is that of the hydrogenic 1s orbital, which (in unnor-
malized form) is exp(−Zr). A simple way to evaluate this transform is to differentiate
the transform for the Yukawa potential with respect to its parameter, α in Eq. (20.41).
Noting that differentiation with respect to this parameter commutes with the transform
operator (which involves integration with respect to other variables), we have
−∂
∂Z
e−Zr
r
T
(k) =
h
e−Zri T
(k) =
1
(2π)3/2
8π Z
(k2 + Z2)2 .
(20.44)

20.2 Fourier Transform
975
5.
Consider next an arbitrary function whose angular dependence is a spherical harmonic
(i.e., an angular momentum eigenfunction). Using spherical polar coordinates, we
look at
h
f (r)Y m
l (r)
i T
(k) =
1
(2π)3/2
∞
Z
0
f (r)r2 dr
Z
drY m
l (r)eik·r
=
4π
(2π)3/2
∞
Z
0
f (r)r2 dr
Z
drY m
l (r)
×
X
l′m′
il′ jl′(kr)Y m′
l′ (k)Y m′
l′ (r)∗,
where we have inserted the spherical wave expansion, Eq. (16.61), for exp(ik · r).
Because the Y m
l
are orthonormal, the summation reduces to a single term, and we
have
h
f (r)Y m
l (r)
i T
(k) =
4πil
(2π)3/2 Y m
l (k)
∞
Z
0
f (r) jl(kr)r2dr.
(20.45)
Equation (20.45) shows that a function with spherical harmonic angular dependence
has a transform containing the same spherical harmonic and that the radial dependence
of the transform is essentially a Hankel transform. Compare with Eq. (20.8).
6.
As a ﬁnal example, consider the Fourier transform of a 3-D Gaussian. Again using
spherical polar coordinates and the spherical wave expansion (a procedure generally
applicable for transforms of spherically symmetric functions), we get
h
e−ar2i T
(k) =
4π
(2π)3/2
∞
Z
0
r2 e−ar2 j0(kr)dr.
(20.46)
Using methods similar to those of Example 20.2.2, we ﬁnd
h
e−ar2i T
(k) =
1
(2a)3/2 e−k2/4a.
(20.47)
This result could also be obtained using Cartesian coordinates and using the result of
Example 20.2.2 in each of the three dimensions.
■
Exercises
20.2.1
(a)
Show that g(−ω) = g∗(ω) is a necessary and sufﬁcient condition for f (x) to be
real.
(b)
Show that g(−ω) = −g∗(ω) is a necessary and sufﬁcient condition for f (x) to be
pure imaginary.
Note. The condition of part (a) is used in the development of the dispersion relations of
Section 12.8.

976
Chapter 20 Integral Transforms
20.2.2
The function
f (x) =
(
1,
|x| < 1
0,
|x| > 1
is a symmetrical ﬁnite step function.
(a)
Find gc(ω), Fourier cosine transform of f (x).
(b)
Taking the inverse cosine transform, show that
f (x) = 2
π
∞
Z
0
sinω cosωx
ω
dω.
(c)
From part (b) show that
∞
Z
0
sinω cosωx
ω
dω =



0,
|x| > 1,
π
4 ,
|x| = 1,
π
2 ,
|x| < 1.
20.2.3
(a)
Show that the Fourier sine and cosine transforms of e−at are
gs(ω) =
r
2
π
ω
ω2 + a2 ,
gc(ω) =
r
2
π
a
ω2 + a2 .
Hint. Each of the transforms can be related to the other by integration by parts.
(b)
Show that
∞
Z
0
ω sinωx
ω2 + a2 dω = π
2 e−ax,
x > 0,
∞
Z
0
cosωx
ω2 + a2 dω = π
2a e−ax,
x > 0.
These results can also be obtained by contour integration (Exercise 11.8.12).
20.2.4
Find the Fourier transform of the triangular pulse (Fig. 20.6),
f (x) =
(h(1 −a|x|),
|x| < 1/a,
0,
|x| > 1/a.
Note. This function provides another delta sequence with h = a and a →∞.
20.2.5
Consider the sequence
δn(x) =
(
n,
|x| < 1/2n,
0,
|x| > 1/2n.

20.2 Fourier Transform
977
f(x)
h
−1/a
1/a
x
FIGURE 20.6
Triangular pulse.
This is Eq. (1.152). Express δn(x) as a Fourier integral (via the Fourier integral theorem,
inverse transform, etc.). Finally, show that we may write
δ(x) = lim
n→∞δn(x) = 1
2π
∞
Z
−∞
e−ikx dk.
20.2.6
Using the sequence
δn(x) =
n
√π exp(−n2x2),
show that
δ(x) = 1
2π
∞
Z
−∞
e−ikx dk.
Hint. Remember that δ(x) is deﬁned in terms of its behavior as part of an integrand.
20.2.7
The formula
δ(t −x) = 1
2π
∞
Z
−∞
eiω(t−x)dω = 1
2π
∞
Z
−∞
eiωte−iωxdω
can be identiﬁed as the continuum limit of an eigenfunction expansion. Derive sine and
cosine representations of δ(t −x) that are comparable to the exponential representation
just given.
ANS.
2
π
∞
Z
0
sinωt sinωx dω,
2
π
∞
Z
0
cosωt cosωx dω.
20.2.8
In a resonant cavity, an electromagnetic oscillation of frequency ω0 dies out as
A(t) =
(
A0 e−ω0t/2Q e−iω0t,
t > 0,
0,
t < 0.
The parameter Q is a measure of the ratio of stored energy to energy loss per cycle.
Calculate the frequency distribution of the oscillation, a∗(ω)a(ω), where a(ω) is the
Fourier transform of A(t).

978
Chapter 20 Integral Transforms
Note. The larger Q is, the sharper your resonance line will be.
ANS.
a∗(ω)a(ω) = A2
0
2π
1
(ω −ω0)2 + (ω0/2Q)2 .
20.2.9
Prove that
¯h
2πi
∞
Z
−∞
e−iωtdω
E0 −i0/2 −¯hω =



exp

−0t
2¯h

exp

−i E0t
¯h

,
t > 0,
0,
t < 0.
This Fourier integral appears in a variety of problems in quantum mechanics: barrier
penetration, scattering, time-dependent perturbation theory, and so on.
Hint. Try contour integration.
20.2.10
Verify that the following are Fourier integral transforms of one another:
(a)



r
2
π ·
1
√
a2 −x2 ,
|x| < a,
0,
|x| > a,


and J0(ay),
(b)



0,
|x| < a,
−
r
2
π
1
√
x2 + a2 ,
|x| > a,



and Y0(a|y|),
(c)
rπ
2
1
√
x2 + a2
and
K0(a|y|).
(d)
Can you suggest why I0(ay) is not included in this list?
Hint. J0, Y0, and K0 may be transformed most easily by using an exponential represen-
tation, reversing the order of integration, and employing the Dirac delta function expo-
nential representation, Eq. (20.20). These cases can be treated equally well as Fourier
cosine transforms.
20.2.11
Show that the following are Fourier transforms of each other:
in Jn(t)
and



r
2
π Tn(x)(1 −x2)−1/2,
|x| < 1,
0,
|x| > 1.
Tn(x) is the nth-order Chebyshev polynomial.
Hint. With Tn(cosθ) = cosnθ, the transform of Tn(x)(1 −x2)−1/2 leads to an integral
representation of Jn(t).

20.2 Fourier Transform
979
20.2.12
Show that the Fourier exponential transform of
f (µ) =
( Pn(µ),
|µ| ≤1
0,
|µ| > 1
is (2in/2π) jn(kr). Here Pn(µ) is a Legendre polynomial and jn(kr) is a spherical
Bessel function.
20.2.13
(a)
Show that f (x) = x−1/2 is a self-reciprocal under both Fourier cosine and sine
transforms; that is,
r
2
π
∞
Z
0
x−1/2 cos xt dx = t−1/2,
r
2
π
∞
Z
0
x−1/2 sin xt ds = t−1/2.
(b)
Use the preceding results to evaluate the Fresnel integrals
∞
Z
0
cos(y2)dy
and
∞
Z
0
sin(y2)dy.
20.2.14
Show that
 1
r2
T
(k) =
π
2
1/2 1
k .
20.2.15
The Fourier transform formulas for a function of two variables are
F(u,v) = 1
2π
Z Z
f (x, y)ei(ux+vy) dx dy,
f (x, y) = 1
2π
Z Z
F(u,v)e−i(ux+vy) du dv,
where the integrations are over the entire xy or uv plane. For f (x, y) = f ([x2 +
y2]1/2) = f (r), show that the zero-order Hankel transforms
F(ρ) =
∞
Z
0
r f (r)J0(ρr)dr,
f (r) =
∞
Z
0
ρF(ρ)J0(ρr)dρ,
are a special case of the Fourier transforms.
Note. This technique may be generalized to derive the Hankel transforms of order ν = 0,
1
2, 1, 3
2,.... See the two texts by Sneddon (Additional Readings). It might also be noted
that the Hankel transforms of half-integral orders ν = ± 1
2 reduce to Fourier sine and
cosine transforms.

980
Chapter 20 Integral Transforms
20.2.16
Show that the 3-D Fourier exponential transform of a radially symmetric function may
be rewritten as a Fourier sine transform:
1
(2π)3/2
∞
Z
−∞
f (r)eik·r d3x = 1
k
r
2
π
∞
Z
0
r f (r) sinkr dr.
20.3
PROPERTIES OF FOURIER TRANSFORMS
Fourier transforms have a number of useful properties, many of which follow directly from
the transform deﬁnition. Using the 3-D transform as an illustration, and letting g(k) be the
Fourier transform of f (r):
h
f (r −R)
i T
(k) = eik·Rg(k),
(translation),
(20.48)
h
f (αr)
i T
(k) = 1
α3 g(α−1k),
(change of scale),
(20.49)
h
f (−r)
i T
(k) = g(−k),
(sign change),
(20.50)
h
f ∗(−r)
i T
(k) = g∗(k),
(complex conjugation),
(20.51)
h
∇f (r)
i T
(k) = −ik g(k),
(gradient),
(20.52)
h
∇2 f (r)
i T
(k) = −k2 g(k),
(Laplacian).
(20.53)
The ﬁrst four of the above formulas can be obtained by carrying out appropriate operations
on the deﬁning equation of the transform; details are left to the exercises. Equations (20.52)
and (20.53) are easily established from the inverse transform formula. For example, from
Eq. (20.37),
∇f (r) =
1
(2π)3/2
Z
g(k)
h
∇re−ik·ri
dk
=
1
(2π)3/2
Z
g(k)
h
(−ik)e−ik·ri
dk
=
1
(2π)3/2
Z h
−ik g(k)
i
e−ik·r dk,
(20.54)
showing that −ik g(k) is indeed the Fourier transform of ∇f (r). It should be noted that
this demonstration requires the existence of the integrals involved.
The translation formula is of considerable practical value, as it enables a function that
is most conveniently described relative to an origin at R to have a transform whose nat-
ural representation is about the origin in the k space, albeit with a complex phase factor,
exp(ik·R). This feature will become important, for example, in problems involving atoms
centered at different spatial points, because the transforms of atomic orbitals on such atoms
can all be written as centered at a single point in the transform space. Thus, the translation

20.3 Properties of Fourier Transforms
981
formula can convert a spatially complex problem into a single-center problem (though now
with oscillatory character due to the phase factors).
The formulas for the gradient and Laplacian, as well as their one-dimensional (1-D)
variants,
h
f ′(t)
i T
(ω) = −iω g(ω),
(ﬁrst derivative),
(20.55)
h dn
dtn f (t)
i T
(ω) = (−iω)n g(ω),
(nth derivative),
(20.56)
make the application of these differential operators have simple forms in the transform
space. As we see from Eq. (20.55), the operation of differentiation corresponds in the
transform space to multiplication by −iω.
Example 20.3.1
WAVE EQUATION
Fourier transform techniques may be used to advantage in handling partial differential
equations (PDEs). To illustrate the technique, let us derive a familiar expression from
elementary physics. An inﬁnitely long string is vibrating freely. The amplitude y of the
(small) vibrations satisﬁes the wave equation
∂2y
∂x2 = 1
v2
∂2y
∂t2 ,
(20.57)
where v is the phase velocity of the wave propagation. We take as initial conditions
y(x,0) = f (x),
∂y(x,t)
∂t

t=0
= 0,
(20.58)
where f is assumed localized, meaning that limx=±∞f (x) = 0.
Our method for solving the PDE of Eq. (20.57) will be to take the Fourier transforms (in
x) of its two members, using α as the transform variable. This is equivalent to multiplying
Eq. (20.57) by eiαx and integrating over x. Before simplifying, we have
∞
Z
−∞
∂2y(x,t)
∂x2
eiαxdx = 1
v2
∞
Z
−∞
∂2y(x,t)
∂t2
eiαxdx.
(20.59)
If we recognize
Y(α,t) =
1
√
2π
∞
Z
−∞
y(x,t)eiαxdx
(20.60)
as the transform (from our initial variable x to our transform variable α) of the solution
y(x,t) of our PDE, we can rewrite Eq. (20.59) as
(−iα)2Y(α,t) = 1
v2
∂2Y(α,t)
∂t2
.
(20.61)

982
Chapter 20 Integral Transforms
Here we have used Eq. (20.56) for the transform of ∂2y/∂x2 and moved the operator
∂2/∂t2, which is irrelevant to the transform operator, outside the integral, leaving behind
just Y(α,t).
Our original problem has now been converted into Eq. (20.61), but this new equation
has the important simplifying feature that the only derivative appearing in it is that with
respect to t; we have therefore succeeded in replacing our original PDE (in x and t) with
an ordinary differential equation (ODE) (in t only). The dependence of our problem on α
(the variable to which x was converted) is only algebraic.
This transformation, from a PDE to an ODE, is a signiﬁcant achievement. We are now
ready to solve Eq. (20.61), subject to the initial conditions, which we need to express in
terms of Y. Taking transforms of the quantities in Eq. (20.58), we have



Y(α,0) =
1
√
2π
∞
Z
−∞
f (x)eiαxdx = F(α),
∂Y(α,t)
∂t

t=0
= 0.
(20.62)
It is important to recognize that F(α) is (in principle) known; it is the Fourier transform of
the known initial amplitude f (x).
Solving Eq. (20.61) subject to the initial conditions on Y given in Eq. (20.62), we obtain
Y(α,t) = F(α) eiαvt + e−iαvt
2
.
(20.63)
We could have written the t dependence as cos(αvt), but the exponential form is better
suited to what we will do next.
Since we really want our solution in terms of x rather than α, our ﬁnal step will be to
apply inverse Fourier transforms to both sides of Eq. (20.63):
1
√
2π
∞
Z
−∞
Y(α,t)e−iαxdα =
1
√
2π
∞
Z
−∞
F(α) eiαvt−iαx + e−iαvt−iαx
2
dα.
(20.64)
The left-hand side of Eq. (20.64) is clearly y(x,t); each term on the right-hand side is an
inverse transform of F (and is therefore f ), but the ﬁrst exponential, if written e−iα(x−vt),
can be seen to lead to an inverse transform of argument x −vt, while the second expo-
nential leads to an inverse transform of argument x + vt. Thus, our ﬁnal simpliﬁcation of
Eq. (20.64) takes the form
y(x,t) = 1
2
h
f (x −vt) + f (x + vt)
i
.
(20.65)
Our solution thus consists of a superposition in which half the amplitude of the original
wave form is moving toward +x (at velocity v) while the other half of the original wave
form is moving (also at velocity v) in the −x direction.
■

20.3 Properties of Fourier Transforms
983
Example 20.3.2
HEAT FLOW PDE
To illustrate another transformation of a PDE into an ODE, let us Fourier transform the
1-D heat-ﬂow PDE,
∂ψ
∂t = a2 ∂2ψ
∂x2 ,
where the solution ψ(x,t) is the temperature at position x and time t.
We transform the x dependence, with the transform variable denoted y, writing the
transform of ψ(x,t) as 9(y,t), and identifying the transform of ∂2ψ(x,t)/∂x2 as
−y29(y,t). Our heat ﬂow equation then takes the form
∂9(y,t)
∂t
= −a2y2 9(y,t),
with general solution
ln9(y,t) = −a2y2t + lnC(y),
or
9 = C(y)e−a2y2t.
The physical signiﬁcance of C(y) is that it is the initial spatial distribution of 9 or, in other
words, the Fourier transform of the initial temperature proﬁle ψ(x,0). Thus, if we assume
the initial temperature distribution is known, then so also is C(y), and our PDE solution,
the inverse transform of 9, assumes the form
ψ(x,t) = 1
2π
∞
Z
−∞
C(y)e−a2y2te−iyxdy.
(20.66)
Further progress depends on the speciﬁc form of C(y). If we assume the initial tem-
perature to be a delta-function spike at x = 0, corresponding to an instantaneous pulse of
thermal energy at x = t = 0, we then have as its Fourier transform C(y) = constant, see
Eq. (20.14). We can now evaluate the integral in Eq. (20.66) to obtain an explicit form for
ψ(x,t). With C constant, the functional form of Eq. (20.66) is (apart from the sign of i)
just that encountered in Example 20.2.2 for the Fourier transform of a Gaussian, and we
can evaluate the integral to obtain
ψ(x,t) =
C
a
√
2t
exp

−x2
4a2t

.
This form for ψ was obtained in Section 9.7, but it arose there as a clever guess that was
ultimately justiﬁed because it led to a solution of the diffusion PDE.
■
Example 20.3.3
COULOMB GREEN’S FUNCTION
The Green’s function associated with the Poisson equation satisﬁes the PDE
∇2
r G(r,r′) = δ(r −r′).
(20.67)
We take the Fourier transform of both sides of this equation with respect to r, desig-
nating g(k,r′) as the transform of G. Note that r′ is unaffected by the transformation.

984
Chapter 20 Integral Transforms
Using Eq. (20.53), the left-hand side of Eq. (20.67) becomes −k2g(k,r′), while the right-
hand side, in which the delta function has been translated an amount r′, has according to
Eq. (20.48) the transform eik·r′δT (k). Thus, Eq. (20.67) transforms into
−k2g(k,r′) =
1
(2π)3/2 eik·r′,
where the transform of the delta function has been evaluated as the 3-D equivalent of
Eq. (20.14). We may now solve for g:
g(k,r′) = −
1
(2π)3/2
eik·r′
k2 ,
and recover G by taking the inverse transform,
G(r,r′) = −
1
(2π)3
Z eik·r′
k2
e−ik·rd3k = −
1
(2π)3
Z d3k
k2 e−ik·(r−r′).
We see that the evaluation is proportional to that of the inverse transform of 1/k2, but for
argument r −r′. Using Eq. (20.43) (which applies also for the inverse transform because
it is real), we reach
G(r,r′) = −
1
(2π)3/2
π
2
1/2
1
|r −r′| = −1
4π
1
|r −r′|,
a result we have previously obtained by other methods (cf. Section 10.2). Note that we did
not assume G to be a function of r −r′; we found it to have that form.
■
Successes and Limitations
Some of the above examples illustrate an important role played by the Fourier transform:
•
Use of the Fourier transform can convert a PDE into an ODE, thereby reducing the
“degree of transcendence” of the problem.
All the examples also illustrate the procedure sketched schematically in Fig. 20.1:
•
Fourier transformation can often convert a difﬁcult problem into one which we are
able to solve. A useful form for our solution can then be obtained by transforming it
back to physical space.
Despite these successes, it is worth noting that not all problems posed as differential
equations are amenable to Fourier-transform solution methods. Some of the limitations
arise from the implicit requirement that the necessary transforms and their inverses exist.
We can also expect Fourier methods to work only when the solution is unique, as the
process of taking a transform and then solving an algebraic equation produces a single
result, and not a set of two or more linearly independent solutions.
Usually the boundary conditions are the proximate reason that a differential equa-
tion solution is unique, and the requirement that an (exponential) Fourier transform exist

20.4 Fourier Convolution Theorem
985
imposes Dirichlet boundary conditions at inﬁnity. For 1-D systems on the semi-inﬁnite
range 0 ≤x < ∞, use of the Fourier sine transform imposes a Dirichlet condition at the
ﬁnite boundary x = 0, while use of the cosine transform corresponds to a Neumann bound-
ary condition there.
Additional opportunities for solving differential equations by transform methods are
provided by use of the Laplace transform, for which it is more natural to introduce bound-
ary data. See the later sections of this chapter.
Exercises
20.3.1
Write the 1-D equivalents of the equations for translation, scale change, sign change,
and complex conjugation that were given for 3-D transforms in Eqs. (20.48) to (20.51).
20.3.2
(a)
Show that by replacement of r by r −R in the formula for the Fourier transform
of f (r), one can derive the translation formula, Eq. (20.48).
(b)
Using methods similar to those for part (a), establish the formulas for scale change,
sign change, and complex conjugation, Eqs. (20.49) to (20.51).
20.3.3
Derive Eq. (20.53), the formula for the Fourier transform of ∇2 f (r).
20.3.4
Verify Eqs. (20.55) and (20.56), the formulas for the derivatives of 1-D Fourier trans-
forms.
20.3.5
Derive the inverse of Eq. (20.56), namely that

tn f (t)
T (ω) = i−n dn
dωn g(ω).
20.3.6
The 1-D neutron diffusion equation with a (plane) source is
−D d2ϕ(x)
dx2
+ K 2Dϕ(x) = Q δ(x),
where ϕ(x) is the neutron ﬂux, Q δ(x) is the (plane) source at x = 0, and D and K 2 are
constants. Apply a Fourier transform. Solve the equation in transform space. Transform
your solution back into x-space.
ANS.
ϕ(x) =
Q
2K D e−|K x|.
20.4
FOURIER CONVOLUTION THEOREM
An important relationship satisﬁed by Fourier transforms is that known as the convolu-
tion theorem. As we shall soon see, this theorem is useful in the solution of differential
equations, in establishing the normalization of momentum wave functions, in the evalua-
tion of integrals arising in many branches of physics, and in a variety of signal-processing
applications.

986
Chapter 20 Integral Transforms
We deﬁne the convolution of two functions f (x) and g(x), understood here to be over
the interval (−∞,∞), as the following operation designated f ∗g:
( f ∗g)(x) ≡
1
√
2π
∞
Z
−∞
g(y) f (x −y)dy.
(20.68)
The corresponding deﬁnition in three dimensions is
( f ∗g)(r) ≡
1
(2π)3/2
Z
g(r′) f (r −r′)d3r′,
(20.69)
where the integral is over the full 3-D space.
This operation is sometimes referred to as Faltung, the German term for “folding.”
To better understand the origin of this name, look at Fig. 20.7, where we have plotted
f (y) = e−y and f (x −y) = e−(x−y). Clearly, f (y) and f (x −y) are related by reﬂection
relative to the vertical line y = x/2; that is, we could generate f (x −y) by folding over
f (y) on the line y = x/2.
Our interest here is not primarily in the nomenclature, but rather to understand what hap-
pens if we take the Fourier transform of a convolution. Letting F(t) and G(t), respectively,
be the Fourier transforms of f and g, we ﬁnd
( f ∗g)T (t) =
1
√
2π
∞
Z
−∞
dx


1
√
2π
∞
Z
−∞
dy g(y) f (x −y)

eitx
=


1
√
2π
∞
Z
−∞
dy g(y)eity




1
√
2π
∞
Z
−∞
dx f (x −y)eit(x−y)


=


1
√
2π
∞
Z
−∞
dy g(y)eity




1
√
2π
∞
Z
−∞
dz f (z)eitz


= G(t)F(t).
(20.70)
e−y
e−(x−y)
x
y
FIGURE 20.7
Factors in a Faltung.

20.4 Fourier Convolution Theorem
987
In the second line of the above equation set we simply divided eitx into the two factors eity
and eit(x−y); the third line was reached by changing the integration variable of the second
integral from x to z = x −y. After this change, y only appears in the ﬁrst set of square
brackets and z only appears in the second bracket set. We are then able to continue to the
fourth line where we identify the integrals as Fourier transforms.
We often encounter integrals that have the form of a convolution f ∗g. The convolution
theorem then enables the construction of the Fourier transform of the integral, and the
integral itself will then be given by taking the inverse transform of ( f ∗g)T . This process
corresponds to
∞
Z
−∞
g(y) f (x −y)dy =
√
2π( f ∗g)(x) =
√
2π
1
√
2π
∞
Z
−∞
( f ∗g)T (t)e−ixt dt
=
∞
Z
−∞
G(t)F(t)e−ixtdt.
(20.71)
Once again we see an appealing feature inherent to Fourier analysis. While the two func-
tions in our original integral, g(y) and f (x −y), had different arguments, their transforms,
G(t) and F(t), have the same argument. We still have an integral to evaluate after using
the convolution theorem, but (as just observed) the integrand consists of a product of quan-
tities both of which are evaluated at the same point. The cost of the transformation is the
presence of a complex exponential, which imparts oscillatory character to the integral. We
have thus traded geometric complexity for oscillational complexity. Often this will be an
advantageous trade-off.
For the record, here is the 3-D equivalent of Eq. (20.71):
Z
g(r′) f (r −r′)d3r′ =
Z
F(k)G(k)e−ik·rd3k.
(20.72)
Parseval Relation
If we specialize Eq. (20.71) to x = 0, we get the relatively simple result
∞
Z
−∞
f (−y)g(y)dy =
∞
Z
−∞
F(t)G(t)dt.
(20.73)
This equation becomes more easily interpreted if we change f (y) to f ∗(−y). Then we
must replace f (−y) in Eq. (20.73) by f ∗(y), while F(t) becomes [ f ∗(−y)]T , which,
invoking Eq. (20.51), can be written F∗(t). With these changes, we have
∞
Z
−∞
f ∗(y)g(y)dy =
∞
Z
−∞
F∗(t)G(t)dt.
(20.74)
This equation is known as the Parseval relation; some authors prefer to call it Rayleigh’s
theorem.

988
Chapter 20 Integral Transforms
The integrals in Eq. (20.74) are of the form of scalar products, and will exist if f and g
(and therefore also F and G) are quadratically integrable (i.e., members of an L2 space).
Letting F denote the Fourier transform operator, we can rewrite Eq. (20.74) in the compact
form
⟨f |g⟩= ⟨F f |Fg⟩.
(20.75)
If we now move the F out of the left half-bracket, writing instead its adjoint in the right
half-bracket, we reach
⟨f |g⟩= ⟨f |F†Fg⟩.
(20.76)
Since this equation must hold for all f and g in our Hilbert space, it is necessary that F†F
reduce to the identity operator, meaning that
F† = F−1.
(20.77)
Our conclusion is that the Fourier transform operator is unitary.
If, next, we consider the special case g = f , Eq. (20.75) takes the form
⟨f | f ⟩= ⟨F|F⟩,
(20.78)
showing that f and its transform, F, have the same norm, a result that is hardly surprising
since we already know that transforming f twice brings us back to at worst f multiplied
by a complex phase factor.
An interesting consequence of the unitarity property is illustrated by the formulas gov-
erning Fraunhofer diffraction optics. The amplitude of the diffraction pattern appears as
the Fourier transform of the function describing the aperture (compare Exercise 20.4.3).
With intensity proportional to the square of the amplitude, the Parseval relation implies
that the energy passing through the aperture (the integral of | f |2) is equal to that in the
diffraction pattern, whose total energy is the integral of |F|2. In this problem the Parseval
relation corresponds to energy conservation.
We close this topic with two observations. First, note how the clarity and simplicity of
our discussion of the Parseval relation was greatly enhanced by introducing appropriate
notation. Much of our insight and intuition regarding mathematical concepts ﬂows directly
from the use of good notations for their description. Secondly, we call attention to the fact
that Parseval’s relation can be developed independently of the inverse Fourier transform
and then used rigorously to derive the inverse transform. Details can be found in the text
by Morse and Feshbach (Additional Readings).
Here are some examples illustrating use of the convolution theorem.
Example 20.4.1
POTENTIAL OF CHARGE DISTRIBUTION
We require the potential at all points r produced by a charge distribution ρ(r′). From
Coulomb’s law, or equivalently from the Green’s function for Poisson’s equation, we have
ψ(r) = 1
4π
Z
ρ(r′)
|r −r′| d3r.
(20.79)

20.4 Fourier Convolution Theorem
989
The integral for ψ is of the convolution form, and its presence in this problem suggests
that convolutions will arise in a wide variety of problems in which there is a distributed
source of almost any kind and an effect therefrom that depends on relative position.
Taking f (r) = 1/r, so that f (r −r′) = 1/|r −r′|, and g(r) = ρ(r), application of the
convolution formula Eq. (20.72) yields
ψ(r) = 1
4π
Z
f T (k)gT (k)e−ik·rd3k.
Since
f T (k) =
1
(2π)3/2
4π
k2
and
gT(k) = ρT(k),
we have
ψ(r) =
1
(2π)3/2
Z ρT(k)
k2
e−ik·rd3k.
(20.80)
Depending on the functional form of ρ, Eq. (20.80) may or may not be easier to evaluate
than the original equation for ψ, Eq. (20.79).
■
Example 20.4.2
TWO-CENTER OVERLAP INTEGRAL
In quantum mechanics problems involving molecules, one often encounters the so-called
overlap integral, which is the scalar product of two atomic orbitals, one, ϕa, centered at
a point A, and another, ϕb, centered at a different point B. This overlap integral, denoted
Sab, can be written
Sab =
Z
ϕ∗
a(r −A)ϕb(r −B)d3r.
(20.81)
The integral is over the full 3-D space. One way to evaluate Sab starts by changing to
coordinates in which the origin is at A; this amounts to the substitution r′ = r −A, in
terms of which r −B = r′ −(B −A), so
Sab =
Z
ϕ∗
a(r′)ϕb(r′ −R)d3r′,
where R = B −A. We note the physically expected feature that the value of Sab does not
depend on A and B separately but only on the vector R describing their relative position.
This integral for Sab is almost in the standard form for a convolution (it differs there-
from by having r′ −R instead of R −r′). This discrepancy can be handled by invoking
Eq. (20.50); the net effect is to change the sign of the transform variable k when we eval-
uate ϕT
b.
Again using Eq. (20.72), we write
Sab =
Z h
ϕ∗
a
i T
(k)ϕT
b (−k)e−ik·R d3k.

990
Chapter 20 Integral Transforms
We continue with the speciﬁc case that ϕa and ϕb are Slater-type orbitals (STOs), both
with the same screening parameter ζ. These STOs, and their Fourier transforms (which
can be obtained by differentiating Eq. (20.41) with respect to its parameter α), are
ϕ = ϕ∗= e−ζr,
ϕT =
1
(2π)3/2
8πζ
(k2 + ζ 2)2.
Inserting the formula for ϕT into the integral for Sab, we get
Sab = (8πζ)2
(2π)3
Z
e−ik·R
(k2 + ζ 2)4 d3k.
At this point we already see an advantage of the convolution-based procedure. This integral
(whether or not we can easily evaluate it) has assumed a single-center character, with the
interorbital spacing relegated to the complex exponential factor.
To complete the evaluation, we now insert the spherical wave expansion for
exp(−ik · R), Eq. (16.61), and we note the further simpliﬁcation that the only term sur-
viving the integration over the angular coordinates of k is the l = 0 term of the expansion.
Keeping in mind that Y 0
0 = 1/
√
4π, that term is seen to be just j0(kR), so our formula for
Sab becomes
Sab = (8πζ)2
(2π)3
∞
Z
0
j0(kR)
(k2 + ζ 2)4 4πk2 dk.
We now have a known 1-D integral, which in fact we encountered in Exercise 14.7.10:
kn(x) = 2n+2 (n + 1)!
π xn+1
∞
Z
0
k2 j0(kx)
(k2 + 1)n+2 dk.
Changing x in this formula to ζ R and replacing k by k/ζ, we reach
Sab = π R3
3
k2(ζ R) = π e−ζ R
3ζ 3

ζ 2R2 + 3ζ R + 3

.
(20.82)
Note that when we insert the explicit form for k2, we obtain a relatively simple ﬁnal result.
There are other ways to obtain this formula (one of which is to use prolate ellip-
soidal coordinates with A and B as foci), but the method we have chosen here provides
a good illustration of the issues and formulas that arise when the convolution method is
applicable.
■
Multiple Convolutions
Some important problems take the form of multiple convolutions, which we illustrate in
one dimension by the convolution of a function h ﬁrst with a function g followed by the

20.4 Fourier Convolution Theorem
991
convolution of that result with f , i.e., f ∗(g ∗h). Thus,
h
f ∗(g ∗h)
i
(x) =
1
√
2π
∞
Z
−∞
dy f (y)(g ∗h)(x −y)
= 1
2π
∞
Z
−∞
dy
∞
Z
−∞
dt f (y) g(t)h(x −y −t),
which after making the substitution t = z −y (and therefore x −y −t = x −z) becomes
h
f ∗(g ∗h)
i
(x) = 1
2π
∞
Z
−∞
dy
∞
Z
−∞
dz f (y) g(z −y)h(x −z).
(20.83)
Letting F, G, and H be the Fourier transforms of f , g, and h, this case of the convolution
theorem is
h
f ∗g ∗h
i T
(ω) = F(ω)G(ω)H(ω).
(20.84)
We have now omitted the parentheses surrounding g ∗h since we would have gotten the
same result if we convoluted f , g, and h in any order. Then, taking the inverse transform,
we have
∞
Z
−∞
dy
∞
Z
−∞
dz f (y) g(z −y)h(x −z) = (2π)1/2
∞
Z
−∞
F(ω)G(ω)H(ω)e−iωxdω.
(20.85)
In three dimensions, the corresponding formulas are
h
f ∗(g ∗h)
i
(r) =
1
(2π)3
Z
d3r′
Z
d3r′′ f (r′) g(r′′ −r′)h(r −r′′),
(20.86)
h
f ∗(g ∗h)
i T
(k) = F(k)G(k)H(k),
(20.87)
Z
d3r′
Z
d3r′′ f (r′) g(r′′ −r′)h(r −r′′) = (2π)3/2
Z
F(k)G(k)H(k)e−ik·r d3k.
(20.88)
Example 20.4.3
INTERACTION OF TWO CHARGE DISTRIBUTIONS
The electrostatic interaction of two charge distributions ρ1(r) and ρ2(r) is given by the
integral
V =
Z
d3r′
Z
d3r′′ ρ1(r′)ρ2(r′′)
|r′′ −r′|
,
(20.89)
which is a double convolution, as in Eq. (20.88), but with the free argument r set to zero
and with a sign discrepancy in the argument of h (which is ρ2 of the present example).

992
Chapter 20 Integral Transforms
Taking the above into account and applying Eq. (20.88), we have
V = (2π)3/2
Z
d3k ρT
1(k)
1
r
T
(k)ρT
2 (−k)
= 4π
Z d3k
k2 ρT
1 (k)ρT
2 (−k),
(20.90)
where we have inserted the value of (1/r)T from Eq. (20.42). This expression has the
obvious advantage that it is a 3-D integral in place of the original six-fold integration in
Eq. (20.89). The price we have to pay for this simpliﬁcation is the cost of taking the Fourier
transforms of ρ1 and ρ2.
■
Transform of a Product
The similarity between the formulas for the direct and inverse Fourier transforms sug-
gest that we may be able to identify the Fourier transform of a product as a convolution.
Accordingly, we rewrite Eq. (20.71) with x replaced by −x and also change the variable
of integration in that equation from y to −y. We then have (multiplying the equation by
1/
√
2π)
1
√
2π
∞
Z
−∞
g(−y) f (y −x)dy =
1
√
2π
∞
Z
−∞
G(t)F(t)eixt dt =
h
G(t) F(t)
i T
(x).
(20.91)
If we now make the further identiﬁcations
h
G(t)
i T
(y) = g(−y)
and
h
F(t)
i T
(x −y) = f (y −x),
we have
(FT ∗GT )(x) =
h
G(t) F(t)
i T
(x).
(20.92)
Rewriting Eq. (20.92) with the functions renamed f and g and their respective transforms
denoted F and G, we have our desired ﬁnal result:
h
f g
i T
= F ∗G.
(20.93)
Equation (20.93) will be useful only if f and g individually have Fourier transforms.
It is possible that this condition is not satisﬁed despite the fact that f g possesses a trans-
form. We therefore proceed to consider the case that f not have a transform, but instead
possesses a Maclaurin expansion, and therefore can be represented by a series in positive
integer powers of x. Then, starting from the relation
h
xng(x)
i T
(t) = i−n dn
dtn G(t),
the topic of Exercise 20.3.5, we can write
h
f g
i T
(t) = f

−i d
dt

G(t),
(20.94)

20.4 Fourier Convolution Theorem
993
where the expression −i(d/dt) is the argument of f (and not a multiplicative factor).
Unless f is quite simple, this expression may be of limited practical value.
Momentum Space
Hamilton’s equations of classical mechanics formalize a symmetry between position vari-
ables q and the corresponding (conjugate) momentum variables p. This same corre-
spondence carries over into quantum mechanics, where (in one dimension, in units with
¯h = 1), the fundamental relationship is the commutator [x, p] = i. The time-independent
Schrödinger equation (for a particle of mass m) is
Hψ ≡
h 1
2m p2 + V (x)
i
ψ = E ψ,
and it is usually made more explicit by taking p = −i(d/dx), in which case the wave
function ψ is a function of x: ψ = ψ(x). In principle we could have chosen p as the
fundamental variable, in which case the proper value of the commutator is recovered if
we take x = +i(d/dp), and ψ (which we will now give the name ϕ) will be a function
of p: ϕ = ϕ(p). These two representations of the Schrödinger equation in one dimension
correspond, respectively, to the two ODEs:
−1
2m
d2
dx2 ψ(x) + V (x)ψ(x) = Eψ(x),
(20.95)
p2
2m ϕ(p) + V

i d
dp

ϕ(p) = Eϕ(p).
(20.96)
Note that in the second of these two equations, the argument of V is a differential operator,
and unless the form of V is relatively simple, the momentum-space ODE will be quite
complicated and correspondingly difﬁcult to solve.
In the coordinate representation (x,−id/dx), a wave function exp(ikx) is an eigen-
function of momentum with eigenvalue k:
p eikx = −i d
dx eikx = −i(ik)eikx = k eikx,
and this fact suggests that momentum wave functions will be Fourier transforms of their
coordinate counterparts. We therefore seek to verify the consistency of Eqs. (20.95) and
(20.96) by Fourier transforming the ﬁrst of these two equations, letting g(t) represent the
transform of ψ and using Eq. (20.56) to take the transform of the second derivative.3 In
the case that V has a Maclaurin expansion, we then use Eq. (20.94), obtaining
t2
2m g(t) + V

−i d
dt

g(t) = Eg(t).
This equation can be brought into agreement with Eq. (20.96) if we take its complex con-
jugate (assuming V to be real), so we can make the identiﬁcation ϕ(p) ←→g∗(t).
3Here t is the transform variable; in the present context it has nothing to do with time.

994
Chapter 20 Integral Transforms
On the other hand, if V has a transform we can use the convolution formula, Eq. (20.93),
thereby converting Eq. (20.95) into an integral equation:
p2
2m ϕ(p) +
1
√
2π
∞
Z
−∞
V T (p −p′)ϕ(p′)dp′ = E ϕ(p).
(20.97)
Example 20.4.4
MOMENTUM-SPACE SCHRÖDINGER EQUATION
The time-independent Schrödinger equation for the hydrogen atom has (in hartree atomic
units ¯h = m = e = 1) the coordinate representation
−1
2 ∇2ψ(r) −1
r ψ(r) = E ψ(r).
Taking the Fourier transform of this equation, we get for the momentum-space wave
function ϕ(k)
k2
2 ϕ(k) −
1
(2π)3
Z
4π
|k −k′|2 ϕ(k′)d3k′ = E ϕ(k).
(20.98)
In reaching Eq. (20.98), we have used the 3-D version of Eq. (20.97), inserting for the
transform of V the result from Eq. (20.42).
In principle one can solve Eq. (20.98) for ϕ(k) and the corresponding eigenvalues E,
and the results should be equivalent to the original equation. That is a more difﬁcult task
than we will undertake now, but it is straightforward to verify that the Fourier transform of
the known solution for the hydrogen ground state is a solution to Eq. (20.98).
From Eq. (20.44), the hydrogen 1s wave function e−r is seen to have Fourier transform
ϕ(k) =
C
(k2 + 1)2 ,
where C is independent of k and has a value that is irrelevant here. Inserting this result into
Eq. (20.98), we ﬁnd
1
2
Ck2
(k2 + 1)2 −C
2π2
Z
d3k′
|k −k′|2(k′2 + 1)2 = E
C
(k2 + 1)2 .
(20.99)
Writing |k −k′|2 = k2 + 2kk′ cosθ + k′2, the integral, though a bit tedious, is found to be
elementary. Inserting its value, Eq. (20.99) becomes (canceling the common factor C),
1
2
k2
(k2 + 1)2 −1
2
1
k2 + 1 = E
1
(k2 + 1)2 .
This equation is satisﬁed if E = −1/2, the correct energy (in hartree atomic units) for the
hydrogen 1s state.
■

20.4 Fourier Convolution Theorem
995
Exercises
20.4.1
Work out the convolution equation corresponding to Eq. (20.71) for
(a)
Fourier sine transforms
1
2
∞
Z
0
g(y)
h
f (y + x) + f (y −x)
i
dy =
∞
Z
0
Fs(s)Gs(s)cossx ds,
where f and g are odd functions.
(b)
Fourier cosine transforms
1
2
∞
Z
0
g(y)
h
f (y + x) + f (x −y)
i
dy =
∞
Z
0
Fc(s)Gc(s)cossx ds,
where f and g are even functions.
20.4.2
Show that for both Fourier sine and Fourier cosine transforms Parseval’s relation has
the form
∞
Z
0
F(t)G(t)dt =
∞
Z
0
f (y)g(y)dy.
20.4.3
(a)
A rectangular pulse is described by
f (x) =
(
1,
|x| < a,
0,
|x| > a.
Show that the Fourier exponential transform is
F(t) =
r
2
π
sinat
t
.
This is the single-slit diffraction problem of physical optics. The slit is described
by f (x). The diffraction pattern amplitude is given by the Fourier transform F(t).
(b)
Use the Parseval relation to evaluate
∞
Z
−∞
sin2 t
t2
dt.
This integral may also be evaluated by using the calculus of residues
(Exercise 11.8.9).
ANS.
(b) π.

996
Chapter 20 Integral Transforms
20.4.4
Solve Poisson’s equation, ∇2ψ(r) = −ρ(r)/ε0, by the following sequence of opera-
tions:
(a)
Take the Fourier transform of both sides of this equation. Solve for the Fourier
transform of ψ(r).
(b)
Carry out the Fourier inverse transform.
20.4.5
(a)
Given f (x) = 1 −|x/2| for −2 ≤x ≤2, with f (x) = 0 elsewhere, show that the
Fourier transform of f (x) is
F(t) =
r
2
π
sint
t
2
.
(b)
Using the Parseval relation, evaluate
∞
Z
−∞
sint
t
4
dt.
ANS.
(b) 2π
3 .
20.4.6
With F(t) and G(t) the Fourier transforms of f (x) and g(x), respectively, show that
∞
Z
−∞
 f (x) −g(x)

2
dx =
∞
Z
−∞
 F(t) −G(t)

2
dt.
If g(x) is an approximation to f (x), the preceding relation indicates that the mean
square deviation in t-space is equal to the mean square deviation in x-space.
20.4.7
Use the Parseval relation to evaluate
(a)
∞
Z
−∞
dω
(ω2 + a2)2 ,
(b)
∞
Z
−∞
ω2 dω
(ω2 + a2)2 .
Hint. Compare Exercise 20.2.3.
ANS.
(a) π
2a3 ,
(b)
π
2a .
20.4.8
The nuclear form factor F(k) and the charge distribution ρ(r) are 3-D Fourier trans-
forms of each other:
F(k) =
1
(2π)3/2
Z
ρ(r)eik·rd3r.
If the measured form factor is
F(k) = (2π)−3/2

1 + k2
a2
−1
,
ﬁnd the corresponding charge distribution.
ANS.
ρ(r) = a2
4π
e−ar
r
.

20.5 Signal-Processing Applications
997
20.4.9
Using convolution methods, ﬁnd an integral whose value is the electrostatic interaction
energy between a charge distribution ρ(r −A) and a unit point charge at C.
20.4.10
With ψ(r) a wave function in ordinary space and ϕ(p) the corresponding momentum
function, show that
(a)
1
(2π ¯h)3/2
Z
rψ(r)e−ir·p/¯h d3r = i ¯h∇p ϕ(p),
(b)
1
(2π ¯h)3/2
Z
r2ψ(r)e−r·p/¯h d3r = (i ¯h∇p)2ϕ(p).
Note. ∇p is the gradient in momentum space:
ˆex
∂
∂px
+ ˆey
∂
∂py
+ ˆez
∂
∂pz
.
These results may be extended to any positive integer power of r and therefore to any
(analytic) function that may be expanded as a Maclaurin series in r.
20.4.11
The ordinary space wave function ψ(r,t) satisﬁes the time-dependent Schrödinger
equation,
i ¯h ∂ψ(r,t)
∂t
= −¯h2
2m ∇2ψ + V (r)ψ.
Show that the corresponding time-dependent momentum wave function satisﬁes the
analogous equation
i ¯h ∂ϕ(p,t)
∂t
= p2
2m ϕ + V (i ¯h ∇p)ϕ.
Note. Assume that V (r) may be expressed by a Maclaurin series and use Exer-
cise 20.4.10. V (i ¯h∇p) is the same function of the variable i ¯h ∇p that V (r) is of the
variable r.
20.5
SIGNAL-PROCESSING APPLICATIONS
A time-dependent electrical pulse f (t) may be regarded as a superposition of waves of
many frequencies. For angular frequency ω, we have a contribution
F(ω)eiωt.
Then the complete pulse may be written as
f (t) = 1
2π
∞
Z
−∞
F(ω)eiωtdω.
(20.100)
Because the angular frequency ω is related to the linear frequency ν by
ν = ω
2π ,

998
Chapter 20 Integral Transforms
most physicists associate the entire 1/2π factor with this integral, so this formula differs
by a factor (2π)−1/2 from the deﬁnition we have adopted for the Fourier transform.
But if ω is a frequency, what about the negative frequencies? The negative ω may be
looked on as a mathematical device to avoid dealing with two functions (cosωt and sinωt)
separately.
Because Eq. (20.100) has the form of a Fourier transform, we may solve for F(ω) by
taking the inverse transform. Keeping in mind the scale at which we wrote Eq. (20.100),
we get
F(ω) =
∞
Z
−∞
f (t)e−iωtdt.
(20.101)
Equation (20.101) represents a resolution of the pulse f (t) into its angular frequency
components. Equation (20.100) is a synthesis of the pulse from its components.
Now consider some device, such as a servomechanism or a stereo ampliﬁer, with an
input f (t) and an output g(t). For an input of a single frequency fω with input fω(t) =
F(ω)eiωt, the device will alter the amplitude and may also change the phase. For the
situations we discuss here, we assume a linear response, which means that we are assuming
that gω (the output corresponding to fω) will be a signal at the same frequency as fω, will
scale linearly with fω, and be independent of the simultaneous presence of signals at other
frequencies. However, the responses of interesting devices will depend on the frequency.
Hence, our assumption is that gω and fω are related by an equation of the form
gω(t) = ϕ(ω) fω(t).
(20.102)
This amplitude- and phase-modifying function, ϕ(ω), is called a transfer function. When
making schematic diagrams of electronic circuits, it is customary to designate a device
characterized by a transfer function by a suitably labeled box with input and output con-
ductors, as shown in (Fig. 20.8).
Because we have assumed the operation corresponding to the transfer function to be
linear, the total output from a pulse containing many frequencies may be obtained by inte-
grating over the entire input, as modiﬁed by the transfer function,
g(t) = 1
2π
∞
Z
−∞
ϕ(ω)F(ω)eiωtdω.
(20.103)
The transfer function is characteristic of the device to which it applies. Once it is
known (either by calculation or measurement), the output g(t) can be calculated for any
input f (t).
ϕ(ω)
Input
Output
f(t)
g(t)
FIGURE 20.8
Schematic for device described by transfer function.

20.5 Signal-Processing Applications
999
Equation (20.103) can be brought to a convenient form if we recognize that it is simply
the formula for the Fourier transform of the product ϕ(ω)F(ω). We already know that
F(ω) has transform f (t). Letting 8(t) be (at the scaling of this section) the transform
of ϕ(ω), we may then use Eq. (20.93) to rewrite Eq. (20.103) as the convolution of the
transforms f and 8:
g(t) =
∞
Z
−∞
f (t′)8(t −t′)dt′.
(20.104)
Interpreting Eq. (20.104), we have an input (a “cause”), namely f (t′), modiﬁed by
8(t −t′), producing an output (an “effect”), namely g(t). Adopting the concept of causal-
ity (that the cause precedes the effect), we must obtain contributions to g(t) only from
times t′ such that t′ < t. We do this by requiring
8(t −t′) = 0,
t′ > t.
(20.105)
Then Eq. (20.104) becomes
g(t) =
tZ
−∞
f (t′)8(t −t′)dt′.
(20.106)
Since Eq. (20.106) must yield real output g(t) for arbitrary real input f (t), we see that in
addition to the requirement in Eq. (20.105), we also know that 8(t) must be real.
The adoption of Eq. (20.106) and the reality of 8 have profound consequences here and
equivalently in dispersion theory (Section 12.8).
Example 20.5.1
TRANSFER FUNCTION: HIGH-PASS FILTER
A high-pass ﬁlter permits almost complete transmission of high-frequency electrical sig-
nals but strongly attenuates those at lower frequencies. A very simple high-pass ﬁlter is
shown in Fig. 20.9. Its transfer function describes the steady-state behavior of the ﬁlter in
the absence of loading (meaning that the output terminals are not connected to anything),
so we can assume that, for a signal at frequency ω, the input, output, and current are the
real parts of the respective quantities Vineiωt, Vouteiωt, Ieiωt. Possible phase differences in
these quantities are allowed for by permitting Vin, Vout, and I to be complex.
I
I
R
C
Vin
Vout
I
FIGURE 20.9
Simple high-pass ﬁlter.

1000
Chapter 20 Integral Transforms
Following the usual procedure for electrical circuit analysis, we solve Kirchhoff’s equa-
tion (the condition that the net change in potential around any loop of the circuit vanishes):
Vineiωt =
tZ
I
C eiωtdt + R I eiωt.
(20.107)
Differentiating with respect to t (to eliminate the integral), we have
Vin
d
dt eiωt = I
C eiωt + R I d
dt eiωt,
which, evaluating the derivatives, reduces to
iωVin = I
C + iωRI,
with solution
I =
iωCVin
1 + iωRC .
(20.108)
Since Vout = I R, we easily ﬁnd the transfer function
ϕ(ω) = Vout
Vin
=
iωRC
1 + iωRC .
(20.109)
To conﬁrm the behavior of the ﬁlter, note that in the limit of large ω, ϕ(ω) →1, while at
small ω, ϕ(ω) →iωRC, which vanishes in the limit of small ω. The transition between
these two limiting behaviors is a function of the product RC.
■
Limitations on Transfer Functions
Let us write the transfer function ϕ(ω) as the inverse Fourier transform of 8(t) (still using
the scaling of this section), keeping in mind that 8(t) vanishes for t < 0,
ϕ(ω) =
∞
Z
0
8(t)e−iωtdt.
(20.110)
Now, separating ϕ into its real and imaginary parts: ϕ(ω) = u(ω) + iv(ω), and making the
same separation for the right-hand side of Eq. (20.110), we have
u(ω) =
∞
Z
0
8(t)cosωt dt,
v(ω) = −
∞
Z
0
8(t)sinωt dt.
(20.111)
These formulas tell us that u(ω) is even, and that v(ω) is odd.
Since Eqs. (20.111) are cosine and sine transforms, they can be inverted to give two
alternative formulas for 8(t) in the range of applicability of these transforms, namely for

20.5 Signal-Processing Applications
1001
t > 0. Continuing to use the transform scaling of this section,
8(t) = 2
π
∞
Z
0
u(ω)cosωt dω,
(t > 0)
(20.112)
= −2
π
∞
Z
0
v(ω)sinωt dω.
The present signiﬁcance of these results is that
∞
Z
0
u(ω)cosωt dω = −
∞
Z
0
v(ω)sinωt dω,
(t > 0).
(20.113)
The imposition of causality has led to a mutual interdependence of the real and imagi-
nary parts of the transfer function. The present result is similar to those involving causality
that were discussed in Section 12.8.
We close this subsection by verifying that the conditions on u and v are consistent with
the properties required of 8. Writing
8(t) = 1
2π
∞
Z
−∞
ϕ(ω)eiωt dt,
then inserting eiωt = cosωt + i sinωt and ϕ = u + iv, we have
8(t) = 1
2π
∞
Z
−∞
h
u(ω)cosωt −v(ω)sinωt
i
dω
+ i
2π
∞
Z
−∞
h
u(ω)sinωt + v(ω)cosωt
i
dω.
(20.114)
The imaginary part of Eq. (20.114) vanishes because its integrand is an odd function of ω.
If t > 0, we know from Eq. (20.113) that the two terms of the real part of Eq. (20.114) are
equal, and we get the expected nonzero result. But if t < 0, the sign of the second term of
the real part is changed and they then add to zero.
Exercises
20.5.1
Find the transfer function ϕ(ω) for the circuit shown in the left panel of Fig. 20.10. Is
this a high-pass, a low-pass, or a more complicated ﬁlter?
20.5.2
Find the transfer function ϕ(ω) for the circuit shown in the right panel of Fig. 20.11.
Hint. The potential difference across an inductor is given by L dI/dt.

1002
Chapter 20 Integral Transforms
Vin
Vout
Vin
Vout
R
R
L
C
FIGURE 20.10
Circuits for Exercise 20.5.1 (left) and Exercise 20.5.2 (right).
Vin
Vout
C2
I2
I2
R2
R1
I1
C1
I1 + I2
I1 + I2
FIGURE 20.11
Circuit for Exercise 20.5.3.
Vin
(Fig. 20.9)
(Fig. 20.10)
left
Vout
FIGURE 20.12
Representation of the circuit in Fig. 20.11 in terms of successive transfer
functions.
20.5.3
Find the transfer function ϕ(ω) for the circuit shown in Fig. 20.11. This is a band-pass
ﬁlter.
Hint. Assume the currents in the various parts of the circuit to have the values shown in
the ﬁgure.
20.5.4
The circuit elements for Exercise 20.5.3 correspond to the successive transfer functions
shown in Fig. 20.12. Explain why the transfer function for this exercise is only the
product of the individual transfer functions in the limit R2 ≫R1.
20.6
DISCRETE FOURIER TRANSFORM
For many physicists the Fourier transform is automatically the continuous Fourier trans-
form whose analytical properties we have been discussing in previous sections of this
chapter. The use of digital computers, however, presents an opportunity to work with
numerically determined Fourier transforms, which consist of values given at a discrete set
of points. Integrations are therefore converted into ﬁnite summations. Transforms deﬁned
on discrete point sets have properties worth pursuing, and analysis in that area is the topic
of this section.

20.6 Discrete Fourier Transform
1003
Orthogonality on Discrete Point Sets
Throughout the earlier chapters of this book we have introduced and made use of the prop-
erties of orthogonal functions, where orthogonality has been deﬁned as the vanishing of an
integral whose integrand contains a product of the functions under study. The alternative,
to be discussed here, is to deﬁne orthogonality over a discrete point set as the vanishing of
a sum of products computed at the individual points. It turns out that sines, cosines, and
imaginary exponentials have the remarkable property that they are also orthogonal over a
series of discrete, equally spaced points on an orthogonality interval.
To analyze this situation, we take a set of N equally spaced points xk, on the interval
(0,2π):
xk = 2πk
N ,
k = 0, 1, 2,..., N −1,
(20.115)
and we consider functions ϕp(x), deﬁned only on the points xk and for integer p, as
ϕp(x) = eipx.
(20.116)
In line with our introductory discussion, we deﬁne the scalar products of these functions as
⟨ϕp |ϕq⟩=
N−1
X
k=0
ϕ∗
p(xk)ϕq(xk).
(20.117)
Inserting Eq. (20.115) for the xk, the scalar product takes the form
⟨ϕp |ϕq⟩=
N−1
X
k=0
e2πik(q−p)/N =
N−1
X
k=0
rk,
(20.118)
where r = e2πi(q−p)/N. This is a ﬁnite geometric series; if r = 1 its sum has the value N;
otherwise the sum evaluates to (1 −r N)/(1 −r). But r N = e2πi(q−p), and because p and
q were restricted to integer values, we have r N = 1, so the sum vanishes. To complete our
understanding of the situation, we need to determine the conditions under which r = 1. We
clearly have r = 1 when q = p. Note that we also have r = 1 when q −p is any integer
multiple of N. Thus, a formal statement relative to this scalar product is
⟨ϕp |ϕq⟩= N
∞
X
n=−∞
δq−p,nN.
(20.119)
Note that at most only one of the inﬁnite sum of Kronecker deltas will be nonzero, and all
will be zero unless q −p is a multiple of N (one of which is q −p = 0).
Equation (20.119) is more complicated than necessary. Because the functions ϕp are
deﬁned by their values at N points, only N of them are linearly independent. In fact,
ϕp+N(xk) = e2πi(p+N)k/N = e2πipk/N = ϕp(xk).

1004
Chapter 20 Integral Transforms
We can therefore restrict p and q in Eq. (20.119) to the range (0, N −1), and our orthog-
onality relation then becomes
⟨ϕp|ϕq⟩= Nδpq,
0 ≤p,q ≤N −1.
(20.120)
Obviously, if function values on a discrete point set are to be used to represent a contin-
uous function, the amount of detail that is retained in the analysis will depend on the size
of the point set. We will come back to this issue in a later part of the present section.
Discrete Fourier Transform
By analogy with the deﬁnitions introduced for the conventional Fourier transform, we
deﬁne the discrete transform gp (p = 0,..., N −1) of a function f deﬁned only on the
points xk by the formula
gp = N −1/2
N−1
X
k=0
e2πikp/N fk.
(20.121)
We are now writing fk as a shorthand for f (xk), and that substitution has pretty much
decoupled the problem from the original interval of deﬁnition 0 ≤x ≤2π. In essence, we
are now discussing transformations between two N-member sets of function values.
The transformation inverse to Eq. (20.121) is
f j = N −1/2
N−1
X
p=0
e−2πi jp/N gp;
(20.122)
Eq. (20.122) can be veriﬁed by substituting into it the formula for gp, yielding
f j = N −1
N−1
X
p=0
N−1
X
k=0
e2πi(k−j)p/N fk =
N−1
X
k=0
δkj fk = f j,
as required.
These discrete transforms have properties similar to those of their continuous cousins.
For example, the transform of fk−j, where j is an integer, corresponding to translation by
j steps in the f array, is
[ fk−j]T
p = N −1/2
N−1
X
k=0
e2πikp/N fk−j = e2πi jp/N N −1/2
N−1
X
k=0
e2πi(k−j)p/N fk−j.
Because of the periodicity of the fk, we note that
N −1/2
N−1
X
k=0
e2πi(k−j)p/N fk−j = N −1/2
N−j−1
X
k′=−j
e2πik′ p/N fk′ = N −1/2
N−1
X
k′=0
e2πik′ p/N fk′,

20.6 Discrete Fourier Transform
1005
which is the formula for the p coefﬁcient in the transform of f. We therefore have the
translation formula
[ fk−j]T
p = e2πi jp/N gp.
(20.123)
We examine next the convolution theorem, where the discrete convolution of two point
sets f and g is deﬁned as
[ f ∗g]k = N −1/2
N−1
X
j=0
f j gk−j.
(20.124)
Taking the transform of this convolution, we have
N −1
N−1
X
k=0
e2πikp/N
N−1
X
j=0
f j gk−j
=

N −1/2
N−1
X
j=0
e2πi jp/N f j



N −1/2
N−1
X
k=0
e2πi(k−j)p/N gk−j

.
As in the continuous case, we have split the complex exponential into two factors. We now
redeﬁne the index of the second summation from k to l = k −j, thereby making the two
square brackets completely independent. Each can then be recognized as a transform (for
the second, we need to use the fact that the gk are periodic). The ﬁnal result is
[ f ∗g]T
p = Fp G p,
(20.125)
where F and G are the respective discrete transforms of f and g. This result is completely
analogous with the convolution theorem for the continuous transform.
We close this discussion with the observation that the discrete transform and its inverse
are linear transformations on coefﬁcient arrays (vectors) of ﬁnite dimension N. Therefore,
each transform operator can be represented as an N × N matrix whose rows and columns
correspond to the points k or p. The fact that the transform and its inverse are complex
conjugates means that the transformation matrices are unitary. Moreover, from the forms of
the transform and its inverse, we see that all the elements of these matrices are proportional
to complex exponentials.
Limitations
As mentioned earlier, the ability of discrete transforms to reproduce phenomena that are
actually based on continuous functions will depend on the size of the point set in use. A
large amount of detail on errors and limitations in the use of the discrete Fourier transform
is provided by Hamming (see Additional Readings). We illustrate the potential problems
in the following example.

1006
Chapter 20 Integral Transforms
Example 20.6.1
DISCRETE FOURIER TRANSFORM: ALIASING
Let’s consider the simple case f (x) = cos3x on the interval 0 ≤x ≤2π, which we
(ill-advisedly) attempt to treat by the discrete Fourier transform method with N = 4. Our
four points are at x = 0, π/2, π, and 3π/2, and the four corresponding values of fk
are (1, 0, −1, 0). The problem is that these same four values would be produced from
g(x) = cos x, so neither our discrete transform nor any information derived therefrom can
properly reﬂect any difference in behavior between f (x) and g(x). If all that we are given
are the four values (1, 0, −1, 0), the most straightforward thing to do is take the discrete
transform, yielding (0,1,0,1), which (from the formula for the inverse transform) corre-
sponds to
1
2(0,1,0,1) −→eiπx/2 + e3iπx/2
2
.
If evaluated only at the chosen points, this expression is correct, but if used as an approx-
imation over the continuous range (0,2π) it cannot distinguish between cos x, cos3x, or
any linear combination of the two with unit overall weight.
Situations in which the behavior at one wavelength or frequency is mistaken for that
at another is called aliasing. The best way to avoid aliasing errors is to use point sets
of sufﬁcient size to accommodate the expected extent of oscillatory character in our
problem.
■
Fast Fourier Transform
The fast Fourier transform (FFT) is a particular way of factoring and rearranging the terms
in the sums of the discrete Fourier transform. Brought to the attention of the scientiﬁc com-
munity by Cooley and Tukey,4 its importance lies in the drastic reduction in the number of
numerical operations required. The reduction is possible because the transformation matrix
contains large numbers of duplicate entries, and the FFT procedure organizes the compu-
tation in a way permitting identical sets of coefﬁcients to be computed only once. Because
of the tremendous increase in speed achieved (and reduction in cost), the fast Fourier trans-
form has been hailed as one of the few really signiﬁcant advances in numerical analysis in
the past few decades.
For N data points, a direct calculation of a discrete Fourier transform would require
about N 2 multiplications. For N a power of 2, the fast Fourier transform technique of
Cooley and Tukey cuts the number of multiplications required to (N/2)log2 N. If N =
1024 (210), the fast Fourier transform achieves a computational reduction by a factor of
over 200. This is why the fast Fourier transform is called fast and why it has revolutionized
4J. W. Cooley and J. W. Tukey, Math. Comput. 19: 297 (1965).

20.6 Discrete Fourier Transform
1007
the digital processing of waveforms. Details on its internal operation will be found in the
paper by Cooley and Tukey and in other sources.5
Exercises
20.6.1
Derive the trigonometric forms of discrete orthogonality corresponding to Eq. (20.120):
N−1
X
k=0
cos(2πpk/N)sin(2πqk/N) = 0
N−1
X
k=0
cos(2πpk/N)cos(2πqk/N) =



0,
p ̸= q
N/2,
p = q ̸= 0, N/2
N,
p = q = 0, N/2
N−1
X
k=0
sin(2πpk/N)sin(2πqk/N) =



0,
p ̸= q
N/2,
p = q ̸= 0, N/2
0,
p = q = 0, N/2.
Note. If N is odd, p and q will never have the value N/2.
Hint. Consider the use of trigonometric identities such as
sin A cos B = 1
2
h
sin(A + B) + sin(A −B)
i
.
20.6.2
Show in detail how to go from
Fp =
1
N 1/2
N−1
X
k=0
fke2πipk
to
fk =
1
N 1/2
N−1
X
p=0
Fpe−2πipk.
20.6.3
The N-membered point sets fk and Fp are discrete Fourier transforms of each other.
Derive the following symmetry relations:
(a)
If fk is real, Fp is Hermitian symmetric; that is, Fp = F∗
N−p.
(b)
If fk is pure imaginary, then Fp = −F∗
N−p.
Note. The symmetry of part (a) is an illustration of aliasing. If Fp describes an ampli-
tude at a frequency proportional to p, we necessarily predict an equal amplitude at the
frequency proportional to N −p.
5See, for example, G. D. Bergland, A guided tour of the fast Fourier transform, IEEE Spectrum 6: 41 (1969). A good discussion
can also be found in W. H. Press, B. P. Flannery, S. A. Teukolsky, and W. T. Vetterling, Numerical Recipes, 2nd ed., Cambridge:
Cambridge University Press (1996), section 12.3.

1008
Chapter 20 Integral Transforms
20.7
LAPLACE TRANSFORMS
Deﬁnition
The Laplace transform f (s) of a function F(t) is deﬁned by6
f (s) = L{F(t)} =
∞
Z
0
e−st F(t)dt.
(20.126)
A few comments on the existence of the integral are in order. The inﬁnite integral of F(t),
∞
Z
0
F(t)dt,
need not exist. For instance, F(t) may diverge exponentially for large t. However, if there
are some constants s0, M, and t0 ≥0 such that for all t > t0
|e−s0t F(t)| ≤M,
(20.127)
the Laplace transform will exist for s > s0; F(t) is then said to be of exponential order.
As a counterexample, F(t) = et2 does not satisfy the condition given by Eq. (20.127) and
is not of exponential order. Thus, L
n
et2o
does not exist.
The Laplace transform may also fail to exist because of a sufﬁciently strong singularity
in the function F(t) as t →0. For example,
∞
Z
0
e−sttn dt
diverges at the origin for n ≤−1. The Laplace transform L{tn} does not exist for n ≤−1.
Since, for two functions F(t) and G(t) for which the integrals exist,
L
n
aF(t) + bG(t)
o
= aL{F(t)} + bL{G(t)},
(20.128)
the operation denoted by L is linear.
Elementary Functions
To introduce the Laplace transform, let us apply the operation to some of the elementary
functions. In all cases we assume that F(t) = 0 for t < 0. If
F(t) = 1,
t > 0,
6This is sometimes called a one-sided Laplace transform; the integral from −∞to +∞is referred to as a two-sided Laplace
transform. Some authors introduce an additional factor of s. This extra s appears to have little advantage and continually gets
in the way; for further comments, see section 14.13 in the text by Jeffreys and Jeffreys (Additional Readings). Generally, we
take s to be real and positive. It is possible to let s become complex, provided ℜe(s) > 0.

20.7 Laplace Transforms
1009
then
L{1} =
∞
Z
0
e−st dt = 1
s ,
for
s > 0.
(20.129)
Next, let
F(t) = ekt,
t > 0.
The Laplace transform becomes
L
n
ekto
=
∞
Z
0
e−st ekt dt =
1
s −k ,
for
s > k.
(20.130)
Using this relation, we obtain the Laplace transform of certain other functions. Since
coshkt = 1
2(ekt + e−kt),
sinhkt = 1
2(ekt −e−kt),
(20.131)
we have
L{coshkt} = 1
2

1
s −k +
1
s + k

=
s
s2 −k2 ,
(20.132)
L{sinhkt} = 1
2

1
s −k −
1
s + k

=
k
s2 −k2 ,
(20.133)
both valid for s > k.
From the relations
coskt = coshikt,
sinkt = −i sinhikt,
it is evident that we can obtain transforms of the sine and cosine if k is replaced by ik in
Eqs. (20.132) and (20.133):
L{coskt} =
s
s2 + k2 ,
(20.134)
L{sinkt} =
k
s2 + k2 ,
(20.135)
both valid for s > 0. Another derivation of this last transform is given in Example 20.8.1.
It is a curious fact that lims→0 L{sinkt} = 1/k despite the fact that
R ∞
0 sinkt dt does not
exist.
Finally, for F(t) = tn, we have
L

tn	
=
∞
Z
0
e−sttn dt,
which is just a gamma function. Hence
L

tn	
= 0(n + 1)
sn+1
,
s > 0, n > −1.
(20.136)

1010
Chapter 20 Integral Transforms
Note that in all these transforms we have the variable s in the denominator, so that it
occurs as a negative power. From the deﬁnition of the transform, Eq. (20.126) and the
existence condition, Eq. (20.127), it is clear that if f (s) is a Laplace transform, then
lims→∞f (s) = 0. The signiﬁcance of this point is that if f (s) behaves asymptotically
for large s as a positive power of s, then no inverse transform can exist.
Heaviside Step Function
In Exercise 1.11.9 we encountered the Heaviside step function u(t). Because of its utility
in describing discontinuous signal pulses, its Laplace transform occurs frequently. We
therefore remind the reader of the deﬁnition
u(t −k) =
(
0,
t < k,
1,
t > k.
(20.137)
Taking the transform, we have
L{u(t −k)} =
∞
Z
k
e−st dt = 1
s e−ks.
(20.138)
Example 20.7.1
TRANSFORM OF SQUARE PULSE
Let’s compute the transform of a square pulse F(t) of height A that is on from t = 0 to
t = t0; see Fig. 20.13. Using the Heaviside step function, the pulse can be represented as
F(t) = A
h
u(t) −u(t −t0)
i
.
Its transform is therefore
L{F(t)} = 1
s (1 −e−t0s).
■
Dirac Delta Function
For use with differential equations one further transform is helpful, namely that of the
Dirac delta function. From the properties of the delta function, we have
L{δ(t −t0)} =
∞
Z
0
e−stδ(t −t0)dt = e−st0,
for
t0 > 0.
(20.139)
t =0
t= t0
A−
FIGURE 20.13
Square pulse.

20.7 Laplace Transforms
1011
For t0 = 0 we must be a bit more careful, as the sequences we have used for deﬁning the
delta function involve contributions symmetrically distributed about t0, and the integration
deﬁning the Laplace transform is restricted to t ≥0. Consistent results when using Laplace
transforms, however, are obtained if we consider delta sequences that are entirely within
the range t ≥t0, which is equivalent to
L{δ(t)} = 1.
(20.140)
This delta function is frequently called the impulse function because it is so useful in
describing impulsive forces, that is, forces lasting only a short time.
Inverse Transform
As we have already seen in our discussion of the Fourier transform, the taking of an integral
transform will ordinarily have little value unless we can carry out the inverse transform.
That is, with
L{F(t)} = f (s),
then it is desirable to be able to compute
L−1 { f (s)} = F(t).
(20.141)
However, this inverse transform is not entirely unique. Two functions F1(t) and F2(t) can
have the same transform, f (s), if their difference, N(t) = F1(t)−F2(t), is a null function,
meaning that for all t0 > 0 it satisﬁes
t0
Z
0
N(t)dt = 0.
This result is known as Lerch’s theorem, and is not quite equivalent to F1 = F2, because
it permits F1 and F2 to differ at isolated points. However, in most problems studied by
physicists or engineers this ambiguity is not important and we will not consider it further.
The inverse transform can be determined in various ways.
1.
A table of transforms can be built up and used to identify inverse transformations,
exactly as a table of logarithms can be used to look up antilogarithms. The preceding
transforms constitute the beginnings of such a table. More complete sets of Laplace
transforms are in several of the Additional Readings, and a relatively short table of
transforms appears in the present text as Table 20.1. Many functional forms not in
Table 20.1 can be reduced to tabular entries using a partial fraction expansion or other
properties of the Laplace transform presented later in this chapter. Of particular value
in this regard are the translation and derivative formulas. There is some justiﬁcation for
suspecting that these tables are probably of more value in solving textbook exercises
than in solving real-world problems.
2.
A general technique for L−1 will be developed in Section 20.10 by using the calculus
of residues.
3.
Transforms and their inverses can be represented numerically. See the work by Krylov
and Skoblya in Additional Readings.

1012
Chapter 20 Integral Transforms
Table 20.1
Laplace Transformsa
f (s)
F(t)
Limitation
Equation
1.
1
δ(t)
Singularity at +0
(20.140)
2.
1
s
1
s > 0
(20.129)
3.
0(n + 1)
sn+1
tn
s > 0, n > −1
(20.136)
4.
1
s −k
ekt
s > k
(20.130)
5.
1
(s −k)2
tekt
s > k
(20.176)
6.
s
s2 −k2
coshkt
s > k
(20.132)
7.
k
s2 −k2
sinhkt
s > k
(20.133)
8.
s
s2 + k2
coskt
s > 0
(20.134)
9.
k
s2 + k2
sinkt
s > 0
(20.135)
10.
s −a
(s −a)2 + k2
eat coskt
s > a
(20.159)
11.
k
(s −a)2 + k2
eat sinkt
s > a
(20.158)
12.
s2 −k2
(s2 + k2)2
t coskt
s > 0
(20.177)
13.
2ks
(s2 + k2)2
t sinkt
s > 0
(20.178)
14.
(s2 + a2)−1/2
J0(at)
s > 0
(20.182)
15.
(s2 −a2)−1/2
I0(at)
s > a
Exercise 20.8.13
16.
1
a cot−1  s
a

j0(at)
s > 0
Exercise 20.8.14
17.
1
2a ln s + a
s −a
1
a coth−1  s
a




i0(at)
s > a
Exercise 20.8.14
18.
(s −a)n
sn+1
Ln(at)
s > 0
Exercise 20.8.16
19.
1
s ln(s + 1)
E1(x)
s > 0
Exercise 20.8.17
20.
lns
s
−lnt −γ
s > 0
Exercise 20.10.9
a γ is the Euler-Mascheroni constant.

20.7 Laplace Transforms
1013
Example 20.7.2
PARTIAL FRACTION EXPANSION
The function f (s) = k2/s(s2 + k2) does not appear as a transform listed in Table 20.1, but
we may obtain it from the tabulated transforms by observing that it has the partial fraction
expansion
f (s) =
k2
s(s2 + k2) = 1
s −
s
s2 + k2 .
The partial fraction technique was discussed in Section 1.5, and the present example was
the subject of Example 1.5.3.
Each of the two partial fractions corresponds to an entry in Table 20.1, and we can
therefore take the inverse transform of f (s) term by term:
L−1 { f (s)} = 1 −coskt.
Remember that the range of the inverse transform is restricted to t ≥0.
■
Example 20.7.3
A STEP FUNCTION
This example shows how Laplace transforms can be used to evaluate a deﬁnite integral.
Consider
F(t) =
∞
Z
0
sintx
x
dx.
(20.142)
Suppose we take the Laplace transform of this deﬁnite (and improper) integral, naming
it f (s):
f (s) = L



∞
Z
0
sintx
x
dx


=
∞
Z
0
e−st
∞
Z
0
sintx
x
dx dt.
Now, interchanging the order of integration (which is justiﬁed),7 we get
f (s) =
∞
Z
0
1
x


∞
Z
0
e−st sintx dt

dx =
∞
Z
0
dx
s2 + x2 ,
(20.143)
since the factor in square brackets is just the Laplace transform of sintx. The integral on
the right-hand side is elementary, with evaluation
f (s) =
∞
Z
0
dx
s2 + x2 = 1
s tan−1 x
s

∞
0
= π
2s .
(20.144)
Using entry #2 in Table 20.1, we carry out the inverse transformation to obtain
F(t) = π
2 ,
t > 0,
(20.145)
7See Chapter 1 in Jeffreys and Jeffreys (Additional Readings) for a discussion of uniform convergence of integrals.

1014
Chapter 20 Integral Transforms
π
2
t
F(t)
π
2
−
FIGURE 20.14
F(t) =
R ∞
0
sintx
x
dx, a step function.
in agreement with an evaluation by the calculus of residues, Eq. (11.107). It has been
assumed that t > 0 in F(t). For F(−t) we need note only that sin(−tx) = −sintx, giving
F(−t) = −F(t). Finally, if t = 0, F(0) is clearly zero. Therefore,
∞
Z
0
sintx
x
dx = π
2 [2u(t) −1] =



π
2 ,
t > 0
0,
t = 0
−π
2 ,
t < 0.
(20.146)
Here u(t) is the Heaviside unit step function, Eq. (20.137). Thus,
∞
R
0
(sintx/x)dx, taken as
a function of t, describes a step function (Fig. 20.14), with a step of height π at t = 0.
■
The technique in the preceding example was to (1) introduce a second integration,
namely the Laplace transform, (2) reverse the order of integration and integrate once,
and (3) take the inverse Laplace transform. This is a technique that will apply to many
problems.
Exercises
20.7.1
Prove that
lim
s→∞s f (s) = lim
t→+0 F(t).
Hint. Assume that F(t) can be expressed as F(t) = P∞
n=0 antn.
20.7.2
Show that
1
π lim
s→0L{cos xt} = δ(x).
20.7.3
Verify that
L
cosat −cosbt
b2 −a2

=
s
(s2 + a2)(s2 + b2),
a2 ̸= b2.

20.7 Laplace Transforms
1015
20.7.4
Using partial fraction expansions, show that
(a)
L−1

1
(s + a)(s + b)

= e−at −e−bt
b −a
,
a ̸= b.
(b)
L−1

s
(s + a)(s + b)

= ae−at −be−bt
a −b
,
a ̸= b.
20.7.5
Using partial fraction expansions, show that for a2 ̸= b2,
(a)
L−1

1
(s2 + a2)(s2 + b2)

= −
1
a2 −b2
sinat
a
−sinbt
b

.
(b)
L−1

s2
(s2 + a2)(s2 + b2)

=
1
a2 −b2 {a sinat −b sinbt}.
20.7.6
Show that
(a)
∞
Z
0
coss
sν ds =
π
2(ν −1)!cos(νπ/2),
0 < ν < 1.
(b)
∞
Z
0
sins
sν ds =
π
2(ν −1)!sin(νπ/2),
0 < ν < 2.
Why is ν restricted to (0, 1) for (a), to (0, 2) for (b)? These integrals may be interpreted
as Fourier transforms of s−ν and as Mellin transforms of sins and coss.
Hint. Replace s−ν by a Laplace transform integral: L

tν−1	
/0(ν). Then integrate with
respect to s. The resulting integral can be treated as a beta function (Section 13.3).
20.7.7
A function F(t) can be expanded in a Maclaurin series,
F(t) =
∞
X
n=0
antn.
Then
L{F(t)} =
∞
Z
0
e−st
∞
X
n=0
antn dt =
∞
X
n=0
an
∞
Z
0
e−sttn dt.
Show that f (s), the Laplace transform of F(t), contains no powers of s greater than
s−1. Check your result by calculating L{δ(t)}, and comment on this ﬁasco.
20.7.8
Show that the Laplace transform of the conﬂuent hypergeometric function M(a,c; x) is
L{M(a,c; x)} = 1
s 2F1

a,1;c; 1
s

.

1016
Chapter 20 Integral Transforms
20.8
PROPERTIES OF LAPLACE TRANSFORMS
Transforms of Derivatives
Perhaps the main application of Laplace transforms is in converting differential equations
into simpler forms that may be solved more easily. It will be seen, for instance, that coupled
differential equations with constant coefﬁcients transform to simultaneous linear algebraic
equations. For the study of differential equations we need formulas for the Laplace trans-
forms of the derivatives of a function.
Let us transform the ﬁrst derivative of F(t):
L

F′(t)
	
=
∞
Z
0
e−st dF(t)
dt
dt.
Integrating by parts, we obtain
L

F′(t)
	
= e−st F(t)

∞
0
+ s
∞
Z
0
e−st F(t)dt
= sL{F(t)} −F(0).
(20.147)
Strictly speaking, F(0) = F(+0),8 and dF/dt is required to be at least piecewise
continuous for 0 ≤t < ∞. Naturally, both F(t) and its derivative must be such that the
integrals do not diverge. An extension to higher derivatives gives
L
n
F(2)(t)
o
= s2L{F(t)} −sF(+0) −F′(+0),
(20.148)
L{F(n)(t)} = snL{F(t)} −sn−1F(+0) −··· −F(n−1)(+0).
(20.149)
The Laplace transform, like the Fourier transform, replaces differentiation with multi-
plication. In the following examples ODEs become algebraic equations. Here is the power
and the utility of the Laplace transform. But see Example 20.8.7 for what may happen if
the coefﬁcients are not constant.
Note how the initial conditions, F(+0), F′(+0), and so on, are incorporated into the
transform. This situation is different than for the Fourier transform, and arises from
the ﬁnite lower limit (t = 0) of the integral deﬁning the transform. This property makes
the Laplace transform more powerful for obtaining solutions to differential equations sub-
ject to initial conditions.
8This notation means that zero is approached from the positive side.

20.8 Properties of Laplace Transforms
1017
Example 20.8.1
USE OF DERIVATIVE FORMULA
Here is an example showing how the derivative formula has uses even in contexts not
involving the solution to a differential equation. Starting from the identity
−k2 sinkt = d2
dt2 sinkt,
(20.150)
we apply on both sides of the equation the Laplace transform operation, reaching
−k2L{sinkt} = L
 d2
dt2 sinkt

= s2L{sinkt} −s sin(0) −d
dt sinkt

t=0
.
Since sin(0) = 0 and d/dt sinkt |t =0= k, the above equation has solution
L{sinkt} =
k
s2 + k2 .
This result conﬁrms Eq. (20.135).
■
Examples involving the solutions of differential equations follow.
Example 20.8.2
SIMPLE HARMONIC OSCILLATOR
As a physical example, consider a mass m oscillating under the inﬂuence of an ideal spring,
spring constant k. As usual, friction is neglected. Then Newton’s second law becomes
m d2X(t)
dt2
+ kX(t) = 0.
(20.151)
We take as initial conditions
X(0) = X0,
X′(0) = 0.
Applying the Laplace transform, we obtain
mL
d2X
dt2

+ kL{X(t)} = 0.
(20.152)
Letting x(s) denote the presently unknown transform L{X(t)} and using Eq. (20.148), we
convert Eq. (20.152) to the form
ms2x(s) −msX0 + kx(s) = 0,
which has solution
x(s) = X0
s
s2 + ω2
0
,
with ω2
0 ≡k
m .
From Table 20.1 this is seen to be the transform of cosω0t, which gives the expected result:
X(t) = X0 cosω0t.
(20.153)
■

1018
Chapter 20 Integral Transforms
Example 20.8.3
EARTH’S NUTATION
A somewhat more involved example is the nutation of the Earth’s poles (force-free pre-
cession). We treat the Earth as a rigid (oblate) spheroid, with z-axis through its direction
of symmetry. We assume the spheroid to have moments of inertia Iz and Ix = Iy and to
be rotating about its x, y, and z axes at the respective angular velocities X(t) = ωx(t),
Y(t) ≡ωy(t), ωz = constant. The Euler equations of motion for X and Y reduce to
d X
dt = −aY,
dY
dt = +aX,
(20.154)
where a ≡[(Iz −Ix)/Iz]ωz. For the Earth, the initial values of X and Y are not both
zero, so the axis of rotation is not aligned with the symmetry axis (see Fig. 20.15), and
because of this lack of alignment, the axis of rotation precesses about the axis of symmetry.
For the Earth, the deviation between the rotation and symmetry axes is small, only about
15 meters (measured at the Earth’s surface at the poles).
Our ﬁrst step in solving these coupled ODEs is to take their Laplace transforms,
obtaining
sx(s) −X(0) = −ay(s),
sy(s) −Y(0) = ax(s).
Combining to eliminate y(s), we have
s2x(s) −sX(0) + aY(0) = −a2x(s),
or
x(s) = X(0)
s
s2 + a2 −Y(0)
a
s2 + a2 .
(20.155)
Recognizing these functions of s as transforms listed in Table 20.1,
X(t) = X(0)cosat −Y(0)sinat.
Y
X
ωz
FIGURE 20.15
Earth’s rotation axis and its components.

20.8 Properties of Laplace Transforms
1019
Similarly,
Y(t) = X(0)sinat + Y(0)cosat.
This is seen to be a rotation of the vector (X,Y) counterclockwise (for a > 0) about the
z-axis with angle θ = at and angular velocity a.
A direct interpretation may be found by choosing the x and y axes so that Y(0) = 0.
Then
X(t) = X(0)cosat,
Y(t) = X(0)sinat,
which are the parametric equations for rotation of (X,Y) in a circular orbit of radius X(0),
with angular velocity a in the counterclockwise sense.
For the Earth, a as deﬁned here corresponds to a period (2π/a) of some 300 days.
Actually, because of departures from the idealized rigid body assumed in setting up Euler’s
equations, the period is about 427 days.9
These same equations arise in electromagnetic theory. If in Eq. (20.154) we set
X(t) = Lx,
Y(t) = L y,
where Lx and L y are the x- and y-components of the angular momentum L of a charged
particle moving in a uniform magnetic ﬁeld Bzez, and then assign a the value a = −gL Bz,
where gL is the gyromagnetic ratio of the particle, then Eq. (20.148) determines its
Larmor precession in the magnetic ﬁeld.
■
Example 20.8.4
IMPULSIVE FORCE
For an impulsive force acting on a particle of mass m, Newton’s second law takes the form
m d2X
dt2 = Pδ(t),
where P is a constant. Transforming, we obtain
ms2x(s) −msX(0) −mX′(0) = P.
For a particle starting from rest, X′(0) = 0. We shall also take X(0) = 0. Then
x(s) =
P
ms2 ,
and, taking the inverse transform,
X(t) = P
m t,
d X(t)
dt
= P
m ,
a constant.
The effect of the impulse P δ(t) is to transfer (instantaneously) P units of linear momen-
tum to the particle.
9D. Menzel, ed., Fundamental Formulas of Physics, Englewood Cliffs, NJ: Prentice-Hall (1955), reprinted, Dover (1960),
p. 695.

1020
Chapter 20 Integral Transforms
A similar analysis applies to the ballistic galvanometer. The torque on the galvanometer
is given initially by kι, in which ι is a pulse of current and k is a proportionality constant.
Since ι is of short duration, we set
kι = kq δ(t),
where q is the total charge carried by the current ι. Then, with I the moment of inertia,
I d2θ
dt2 = kq δ(t),
and transforming, as before, we ﬁnd that the effect of the current pulse is a transfer of kq
units of angular momentum to the galvanometer.
■
Change of Scale
If we replace t by at in the deﬁning formula for the Laplace transform, we readily obtain
L{F(at)} =
∞
Z
0
e−st F(at)dt = 1
a
∞
Z
0
e−(s/a)(at)F(at)d(at)
= 1
a f
 s
a

.
(20.156)
Substitution
If we replace the parameter s by s −a in the deﬁnition of the Laplace transform,
Eq. (20.126), we have
f (s −a) =
∞
Z
0
e−(s−a)t F(t)dt =
∞
Z
0
e−steat F(t)dt
= L

eat F(t)
	
.
(20.157)
Hence the replacement of s with s −a corresponds to multiplying F(t) by eat, and
conversely. This result can used to check some entries in our table of transforms. From
Eq. (20.157) we ﬁnd immediately that
L

eat sinkt
	
=
k
(s −a)2 + k2 ,
(s > a),
(20.158)
and
L

eat coskt
	
=
s −a
(s −a)2 + k2 ,
s > a.
(20.159)
These are entries 10 and 11 of Table 20.1.

20.8 Properties of Laplace Transforms
1021
Example 20.8.5
DAMPED OSCILLATOR
Equations (20.158) and (20.159) are useful when we consider an oscillating mass with
damping proportional to the velocity. Equation (20.151), with such damping added,
becomes
mX′′(t) + bX′(t) + kX(t) = 0,
(20.160)
in which b is a proportionality constant. Let us assume that the particle starts from rest at
X(0) = X0, so X′(0) = 0. The transformed equation is
m[s2x(s) −sX0] + b[sx(s) −X0] + kx(s) = 0,
with solution
x(s) = X0
ms + b
ms2 + bs + k .
This transform does not appear in our table, but may be handled by completing the square
of the denominator:
s2 + b
m s + k
m =

s + b
2m
2
+
 k
m −b2
4m2

.
Considering further only the case that the damping is small enough that b2 < 4km, then
the last term is positive and will be denoted by ω2
1. We then rearrange x(s) to the form
x(s) = X0
s + b/m
(s + b/2m)2 + ω2
1
= X0
s + b/2m
(s + b/2m)2 + ω2
1
+ X0
ω1(b/2mω1)
(s + b/2m)2 + ω2
1
.
These are the same transforms we encountered in Eqs. (20.158) and (20.159), so we may
take the inverse transform of our formula for x(s), reaching
X(t) = X0 e−(b/2m)t

cosω1t +
b
2mω1
sinω1t

= X0
ω0
ω1
e−(b/2m)t cos(ω1t −ϕ).
(20.161)
Here we have made the substitutions
tanϕ =
b
2mω1
,
ω2
0 = k
m .
Of course, as b →0, this solution goes over to the undamped solution, given in Example
20.8.2.
■

1022
Chapter 20 Integral Transforms
C
R
L
FIGURE 20.16
RLC circuit.
RLC Analog
It is worth noting the similarity between the damped simple harmonic oscillation of a
mass (Example 20.8.5) and an RLC circuit (resistance, inductance, and capacitance). See
Fig. 20.16. At any instant, the sum of the potential differences around the loop must be
zero (Kirchhoff’s law, conservation of energy). This gives
L dI
dt + RI + 1
C
tZ
I dt = 0.
(20.162)
Differentiating Eq. (20.162) with respect to time (to eliminate the integral), we have
L d2I
dt2 + R dI
dt + 1
C I = 0.
(20.163)
If we replace I (t) with X(t), L with m, R with b, and C−1 with k, then Eq. (20.163) is
identical with the mechanical problem. It is but one example of the uniﬁcation of diverse
branches of physics by mathematics. A more complete discussion will be found in a book
by Olson.10
Translation
This time let f (s) be multiplied by e−bs, with b > 0:
e−bs f (s) = e−bs
∞
Z
0
e−st F(t)dt
=
∞
Z
0
e−s(t+b)F(t)dt.
(20.164)
10H. F. Olson, Dynamical Analogies, New York: Van Nostrand (1943).

20.8 Properties of Laplace Transforms
1023
Now let t + b = τ. Equation (20.164) becomes
e−bs f (s) =
∞
Z
b
e−sτ F(τ −b)dτ.
(20.165)
Since F(t) is assumed to be equal to zero for t < 0, so that F(τ −b) = 0 for 0 ≤τ < b,
we can change the lower limit in Eq. (20.165) to zero without changing the value of the
integral. Then renaming τ as our standard Laplace transform variable t, we have
e−bs f (s) = L{F(t −b)}.
(20.166)
If instead of relying on the assumption that F(t) = 0 for negative t we insert a Heaviside
unit step function u(τ −b) to restrict the contributions from F to positive arguments,
Eq. (20.165) takes the form
e−bs f (s) =
∞
Z
0
e−sτ F(τ −b)u(τ −b)dτ.
For this reason the translation formula, Eq. (20.166), is often called the Heaviside shifting
theorem.
Example 20.8.6
ELECTROMAGNETIC WAVES
The electromagnetic wave equation with E = Ey or Ez, a transverse wave propagating
along the x-axis, is
∂2E(x,t)
∂x2
−1
v2
∂2E(x,t)
∂t2
= 0.
(20.167)
We want to solve this PDE for the situation that a source at x = 0 generates a time-
dependent signal E(0,t) starting at time t = 0 and propagating only toward positive x,
with initial conditions that for x > 0,
E(x,0) = 0,
∂E(x,t)
∂t

t=0
= 0.
Transforming Eq. (20.167) with respect to t, we get
∂2
∂x2 L{E(x,t)} −s2
v2 L{E(x,t)} + s
v2 E(x,0) + 1
v2
∂E(x,t)
∂t

t=0
= 0,
which due to the initial conditions simpliﬁes to
∂2
∂x2 L{E(x,t)} = s2
v2 L{E(x,t)}.
(20.168)

1024
Chapter 20 Integral Transforms
The general solution of Eq. (20.168) (which is an ODE in x) is
L{E(x,t)} = f1(s)e−(s/v)x + f2(s)e+(s/v)x.
(20.169)
To understand more fully this result consider ﬁrst the case f2(s) = 0. Then Eq. (20.169)
becomes
L{E(x,t)} = e−(x/v)s f1(s),
(20.170)
which we recognize as of the same form as Eq. (20.166), meaning that
E(x,t) = F

t −x
v

,
where F is the function whose Laplace transform is f1, namely E(0,t).11 Since F is
assumed to vanish when its argument is negative, this formula can be written in the more
explicit form
E(x,t) =



F

t −x
v

= E

0,t −x
v

,
t ≥x
v ,
0,
t < x
v .
(20.171)
This solution represents a wave (or pulse) moving in the positive x-direction with velocity
v. Note that for x > vt the region remains undisturbed; the pulse has not had time to get
there. If we had decided to take the solution of Eq. (20.169) with f1(s) = 0, we would have
obtained
E(x,t) =



F

t + x
v

= E

0,t + x
v

,
t ≥−x
v ,
0,
t < −x
v ,
(20.172)
which we must reject because (for propagation toward positive x) it violates causality.
Our solution to this problem, Eq. (20.171), can be veriﬁed by differentiation and substi-
tution into the original PDE, Eq. (20.167).
■
Derivative of a Transform
When F(t), which is at least piecewise continuous, and s are chosen so that e−st F(t)
converges exponentially for large s, the integral
∞
Z
0
e−st F(t)dt
11Consider Eq. (20.170) with x set to zero.

20.8 Properties of Laplace Transforms
1025
is uniformly convergent and may be differentiated (under the integral sign) with respect
to s. Then
f ′(s) =
∞
Z
0
(−t)e−st F(t)dt = L{−t F(t)}.
(20.173)
Continuing this process, we obtain
f (n)(s) = L

(−t)n F(t)
	
.
(20.174)
All the integrals so obtained will be uniformly convergent because of the decreasing expo-
nential behavior of e−st F(t).
This technique may be applied to generate more transforms. For example,
L
n
ekto
=
∞
Z
0
e−stekt dt =
1
s −k ,
s > k.
(20.175)
Differentiating with respect to s (or with respect to k), we obtain
L
n
tekto
=
1
(s −k)2 ,
s > k.
(20.176)
If we replace k by ik and separate Eq. (20.176) into its real and imaginary parts, we get
L{t coskt} =
s2 −k2
(s2 + k2)2 ,
(20.177)
L{t sinkt} =
2ks
(s2 + k2)2 .
(20.178)
These expressions are valid for s > 0.
Example 20.8.7
BESSEL’S EQUATION
An interesting application of a differentiated Laplace transform appears in the solution of
Bessel’s equation with n = 0. From Chapter 14 we have
x2y′′(x) + xy′(x) + x2y(x) = 0.
This ODE cannot be solved by the method illustrated in Example 20.8.2 because the deriva-
tives are multiplied by functions of the independent variable x. However, an alternate
approach depending on Eq. (20.174) is available. Dividing by x and substituting t = x and
F(t) = y(x) to agree with the present notation, we see that the Bessel equation becomes
t F′′(t) + F′(t) + t F(t) = 0.
(20.179)

1026
Chapter 20 Integral Transforms
We need a regular solution, and it appears possible for F(0) to be nonzero, so we scale the
solution by setting F(0) = 1. Then, setting t = 0 in Eq. (20.179), we ﬁnd that F′(+0) = 0.
In addition, we assume that our unknown F(t) has a transform. Transforming Eq. (20.179),
using Eqs. (20.147) and (20.148) for the derivatives and Eq. (20.173) to append factors of
t, we have
−d
ds
h
s2 f (s) −s
i
+ s f (s) −1 −d
ds f (s) = 0.
(20.180)
Rearranging and simplifying, we obtain
(s2 + 1) f ′(s) + s f (s) = 0,
or
d f
f = −s ds
s2 + 1,
a ﬁrst-order ODE. By integration,
ln f (s) = −1
2 ln(s2 + 1) + lnC,
which may be rewritten as
f (s) =
C
√
s2 + 1
.
(20.181)
To conﬁrm that our transform yields the power-series expansion of J0, we expand f (s)
as given in Eq. (20.181) in a series of negative powers of s, convergent for s > 1:
f (s) = C
s

1 + 1
s2
−1/2
= C
s

1 −1
2s2 +
1 · 3
22 · 2!s4 −··· + (−1)n(2n)!
(2nn!)2s2n + ···

.
Inverting, term by term, we obtain
F(t) = C
∞
X
n=0
(−1)nt2n
(2nn!)2 .
When C is set equal to 1, as required by the initial condition F(0) = 1, we recover J0(t),
our familiar Bessel function of order zero. Hence,
L{J0(t)} =
1
√
s2 + 1
.
(20.182)
This simple, closed form is the Laplace transform of J0(t). After making a scale change to
form J0(at) using Eq. (20.156), we conﬁrm entry 14 of Table 20.1.
Note that in our derivation of Eq. (20.182) we assumed s > 1. The proof for s > 0 is the
topic of Exercise 20.8.10.
■

20.8 Properties of Laplace Transforms
1027
It is worth noting that this application was successful and relatively easy because we took
n = 0 in Bessel’s equation. This made it possible to divide out a factor of x (or t). If this
had not been done, the terms of the form t2F(t) would have introduced a second derivative
of f (s). The resulting equation would have been no easier to solve than the original one.
This observation illustrates the point that when we go beyond linear ODEs with constant
coefﬁcients, the Laplace transform may still be applied, but there is no guarantee that it
will be helpful.
The application to Bessel’s equation, n ̸= 0, will be found in the Additional Readings.
Alternatively, given the result
L{Jn(at)} = a−n(
√
s2 + a2 −s)n
√
s2 + a2
,
(20.183)
we can conﬁrm its validity by expressing Jn(t) as an inﬁnite series and transforming term
by term.
Integration of Transforms
Again, with F(t) at least piecewise continuous and x large enough so that e−xt F(t)
decreases exponentially (as x →∞), the integral
f (x) =
∞
Z
0
e−xt F(t)dt
is uniformly convergent with respect to x. This justiﬁes reversing the order of integration
in the following equation:
∞
Z
s
f (x)dx =
∞
Z
s
dx
∞
Z
0
dt e−xt F(t) =
∞
Z
0
e−st F(t)
t
dt,
= L
 F(t)
t

,
(20.184)
where the last member of the ﬁrst line is obtained by integrating with respect to x. The
lower limit s must be chosen large enough so that f (s) is within the region of uniform
convergence. Equation (20.184) is valid when F(t)/t is ﬁnite at t = 0 or diverges less
strongly than t−1 (so that L{F(t)/t} will exist).
For convenience we summarize the deﬁnition and properties of the Laplace transform
in Table 20.2. Included in the table are formulas for convolution and inversion that will be
discussed in Sections 20.9 and Sections 20.10.

1028
Chapter 20 Integral Transforms
Table 20.2
Laplace Transform Operations
Operation
Equation
1.
Laplace transform
f (s) = L{F(t)} =
∞
Z
0
e−st F(t)dt
(15.99)
2.
Transform of derivative
s f (s) −F(+0) = L

F′(t)
	
(15.123)
s2 f (s) −sF(+0) −F′(+0) = L

F′′(t)
	
(15.124)
3.
Transform of integral
1
s f (s) = L



t
Z
0
F(x)dx



Exercise 20.9.1
4.
Change of scale
1
a f
 s
a

= L{F(at)}
(20.156)
5.
Substitution
f (s −a) = L

eat F(t)
	
(15.152)
6.
Translation
e−bs f (s) = L{F(t −b)}
(15.164)
7.
Derivative of transform
f (n)(s) = L

(−t)n F(t)
	
(15.173)
8.
Integral of transform
∞
Z
s
f (x)dx = L
 F(t)
t

(15.189)
9.
Convolution
f1(s) f2(s) = L



t
Z
0
F1(t −z)F2(z)dz



(15.193)
10.
Inverse transform,
1
2πi
β+i∞
Z
β−i∞
est f (s)ds = F(t)
(15.212)
Bromwich integrala
a β must be large enough that e−βt F(t) vanishes as t →+∞.
Exercises
20.8.1
Use the expression for the transform of a second derivative to obtain the transform of
coskt.
20.8.2
A mass m is attached to one end of an unstretched spring, spring constant k (Fig. 20.17).
Starting at time t = 0, the free end of the spring experiences a constant acceleration a,
away from the mass. Using Laplace transforms,
(a)
ﬁnd the position x of m as a function of time.
(b)
determine the limiting form of x(t) for small t.
ANS.
(a)
x = 1
2at2 −a
ω2 (1 −cosωt),
ω2 = k
m ,
(b)
x = aω2
4! t4,
ωt ≪1.

20.8 Properties of Laplace Transforms
1029
m
x
x1
FIGURE 20.17
Spring, Exercise 20.8.2.
20.8.3
Radioactive nuclei decay according to the law
dN
dt = −λN,
with N the concentration of a given nuclide and λ its particular decay constant. This
equation may be interpreted as stating that the rate of decay is proportional to the num-
ber of these radioactive nuclei present. They all decay independently.
Consider now a radioactive series of n different nuclides, with Nuclide 1 decaying into
Nuclide 2, Nuclide 2 into Nuclide 3, etc., until reaching Nuclide n, which is stable. The
concentrations of the various nuclides satisfy the system of ODEs
dN1
dt
= −λ1N1,
dN2
dt
= λ1N1 −λ2N2,
···,
dNn
dt
= λn−1Nn−1.
(a)
For the case n = 3 ﬁnd N1(t), N2(t), and N3(t), with N1(0) = N0 and N2(0) =
N3(0) = 0.
(b)
Find an approximate expression for N2 and N3, valid for small t when λ1 ≈λ2.
(c)
Find approximate expressions for N2 and N3, valid for large t, when (1) λ1 ≫λ2,
(2) λ1 ≪λ2.
ANS.
(a)
N1(t) = N0e−λ1t,
N2(t) = N0
λ1
λ2 −λ1
(e−λ1t −e−λ2t),
N3(t) = N0

1 −
λ2
λ2 −λ1
e−λ1t +
λ1
λ2 −λ1
e−λ2t

.
(b)
N2 ≈N0λ1t,
N3 ≈N0
2 λ1λ2t2.
(c)
(1) N2 ≈N0eλ2t
N3 ≈N0(1 −e−λ2t),
λ1t ≫1.
(2) N2 ≈N0(λ1/λ2)e−λ1t,
N3 ≈N0(1 −e−λ1t),
λ2t ≫1.

1030
Chapter 20 Integral Transforms
20.8.4
The rate of formation of an isotope in a nuclear reactor is given by
dN2
dt
= ϕ
h
σ1N1(0) −σ2N2(t)
i
−λ2N2(t).
Here N1(0) is the concentration of the original isotope (assumed constant), and N2 is
that of the newly formed isotope. The ﬁrst two terms on the right-hand side describe the
production and destruction of the new isotope via neutron absorption; ϕ is the neutron
ﬂux (units cm−2s−1); σ1 and σ2 (units cm2) are neutron absorption cross sections. The
ﬁnal term describes the radioactive decay of the new isotope, with decay constant λ2.
(a)
Find the concentration N2 of the new isotope as a function of time.
(b)
For original isotope 153Eu, σ1 = 400 barns = 400 × 10−24 cm2, σ2 = 1000 barns
= 1000 × 10−24 cm2, and λ2 = 1.4 × 10−9 s−1. If N1(0) = 1020 and ϕ =
109 cm−2s−1, ﬁnd N2, the concentration of 154Eu, after 1 year of continuous irra-
diation. Is the assumption that N1 is constant justiﬁed?
20.8.5
In a nuclear reactor 135Xe is formed as both a direct ﬁssion product of 235U and by
decay of 135I (another ﬁssion product), half-life 6.7 hours. The half-life of 135Xe is
9.2 hours. Because 135Xe strongly absorbs thermal neutrons, thereby “poisoning” the
nuclear reactor, its concentration is a matter of great interest. The relevant equations are
dNI
dt = ϕγI(σ f NU) −λINI,
dNXe
dt
= ϕ
h
γXe(σ f NU) −σXeNXe
i
+ λINI −λXeNXe.
Here NI, NXe, NU are the concentrations of 135I, 135Xe, 235U, with NU assumed to be
constant. The neutron ﬂux ϕ in the reactor causes ﬁssion of 235U with cross section σ f
and removes 135Xe by neutron absorption with cross section σXe = 3.5 × 106 barns =
3.5 × 10−18 cm2. Neutron absorption by 135I is negligible. The yield of 135I and 135Xe
per ﬁssion are, respectively, γI = 0.060 and γXe = 0.003.
(a)
Find NXe(t) in terms of neutron ﬂux ϕ and the product σ f NU.
(b)
Find NXe(t →∞).
(c)
After NXe has reached equilibrium, the reactor is shut down: ϕ = 0. Find NXe(t)
following shutdown. Note the short-term increase in NXe, which may for a few
hours interfere with starting the reactor up again.
Hint. The half-life t1/2 of a radioactive isotope is the time required for decay of half of
the nuclides in a sample. For a decay rate dN/dt = −λN, the half-life has the value
t1/2 = ln2/λ, so λ can be computed as λ = ln2/t1/2 = 0.693/t1/2.
20.8.6
Solve Eq. (20.160), which describes a damped simple harmonic oscillator, for X(0) =
X0, X′(0) = 0, and
(a)
b2 = 4mk (critically damped),
(b)
b2 > 4mk (overdamped).
ANS.
(a) X(t) = X0e−(b/2m)t

1 + b
2m t

.

20.8 Properties of Laplace Transforms
1031
20.8.7
Again solve Eq. (20.160), which describes a damped simple harmonic oscillator, but
this time for X(0) = 0, X′(0) = v0, and
(a)
b2 < 4mk (underdamped),
(b)
b2 = 4mk (critically damped),
(c)
b2 > 4mk (overdamped).
ANS.
(a) X(t) = v0
ω1
e−(b/2m)t sinω1t,
(b) X(t) = v0te−(b/2m)t.
20.8.8
The motion of a body falling in a resisting medium may be described by
m d2X(t)
dt2
= mg −b d X(t)
dt
when the retarding force is proportional to the velocity. Find X(t) and d X(t)/dt for the
initial conditions
X(0) = d X
dt

t=0
= 0.
20.8.9
Ringing circuit. In certain electronic devices, resistance, inductance, and capacitance
are placed in a circuit as shown in Fig. 20.18. A constant voltage is maintained across
the capacitance, keeping it charged. At time t = 0 the circuit is disconnected from the
voltage source. Find the voltages across each of the elements R, L, and C as a function
of time. Assume R to be small.
Hint. By Kirchhoff ’s laws
IRL + IC = 0
and
ER + EL = EC,
where
ER = IRL R,
EL = L dIRL
dt
,
EC = q0
C + 1
C
tZ
0
IC dt,
q0 = initial charge of capacitor.
20.8.10
With J0(t) expressed as a contour integral, apply the Laplace transform operation,
reverse the order of integration, and thus show that
L{J0(t)} = (s2 + 1)−1/2,
for s > 0.
L
R
C
FIGURE 20.18
Ringing circuit.

1032
Chapter 20 Integral Transforms
20.8.11
Develop the Laplace transform of Jn(t) from L{J0(t)} by using the Bessel function
recurrence relations.
Hint. Here is a chance to use mathematical induction (Section 1.4).
20.8.12
A calculation of the magnetic ﬁeld of a circular current loop in circular cylindrical
coordinates leads to the integral
∞
Z
0
e−kzk J1(ka)dk,
ℜe z ≥0.
Show that this integral is equal to a/(z2 + a2)3/2.
20.8.13
Show that
L{I0(at)} = (s2 −a2)−1/2,
s > a.
20.8.14
Verify the following Laplace transforms:
(a)
L{ j0(at)} = L
sinat
at

= 1
a cot−1  s
a

,
(b)
L{y0(at)} does not exist.
(c)
L{i0(at)} = L
sinhat
at

= 1
2a ln s + a
s −a = 1
a coth−1  s
a

,
(d)
L{k0(at)} does not exist.
20.8.15
Develop a Laplace transform solution of Laguerre’s equation,
t F′′(t) + (1 −t)F′(t) + nF(t) = 0.
Note that you need a derivative of a transform and a transform of derivatives. Go as far
as you can with a general value of n; then (and only then) set n = 0.
20.8.16
Show that the Laplace transform of the Laguerre polynomial Ln(at) is given by
L{Ln(at)} = (s −a)n
sn+1
,
s > 0.
20.8.17
Show that
L{E1(t)} = 1
s ln(s + 1),
s > 0,
where
E1(t) =
∞
Z
t
e−τ
τ
dτ =
∞
Z
1
e−xt
x
dx.
E1(t) is the exponential integral function, ﬁrst encountered in this book in Table 1.2.

20.8 Properties of Laplace Transforms
1033
20.8.18
(a)
From Eq. (20.184) show that
∞
Z
0
f (x)dx =
∞
Z
0
F(t)
t
dt,
provided the integrals exist.
(b)
From the preceding result show that
∞
Z
0
sint
t
dt = π
2 ,
in agreement with Eqs. (20.146) and (11.107).
20.8.19
(a)
Show that
L
sinkt
t

= cot−1 s
k

.
(b)
Using this result (with k = 1), prove that
L{si(t)} = −1
s tan−1 s,
where
si(t) = −
∞
Z
t
sin x
x
dx,
the sine integral.
20.8.20
If F(t) is periodic (Fig. 20.19) with a period a so that F(t + a) = F(t) for all t ≥0,
show that
L{F(t)} =
1
1 −e−as
a
Z
0
e−st F(t)dt.
Note that the integration is now over only the ﬁrst period of F(t).
a
2a
3a
t
f(t)
FIGURE 20.19
Periodic function.

1034
Chapter 20 Integral Transforms
20.8.21
Find the Laplace transform of the square wave (period a) deﬁned by
F(t) =
(1,
0 < t < a/2,
0,
a/2 < t < a.
ANS.
f (s) = 1
s
1 −e−as/2
1 −e−as .
20.8.22
Show that
(a)
L{coshat cosat} =
s3
s4 + 4a4 ,
(b)
L{coshat sinat} = as2 + 2a3
s4 + 4a4 ,
(c)
L{sinhat cosat} = as2 −2a3
s4 + 4a4 ,
(d)
L{sinhat sinat} =
2a2s
s4 + 4a4 .
20.8.23
Show that
(a)
L−1 n
(s2 + a2)−2o
=
1
2a3 sinat −
t
2a2 cosat,
(b)
L−1 n
s(s2 + a2)−2o
= t
2a sinat,
(c)
L−1 n
s2(s2 + a2)−2o
= 1
2a sinat + t
2 cosat,
(d)
L−1 n
s3(s2 + a2)−2o
= cosat −at
2 sinat.
20.8.24
Show that
L{(t2 −k2)−1/2u(t −k)} = K0(ks).
Hint. Try transforming an integral representation of K0(ks) into the Laplace transform
integral.
20.9
LAPLACE CONVOLUTION THEOREM
One of the most important properties of the Laplace transform is that given by the convo-
lution, or Faltung, theorem. We take two transforms,
f1(s) = L{F1(t)}
and
f2(s) = L{F2(t)},
and multiply them together:
f1(s) f2(s) =
∞
Z
0
e−sx F1(x)dx
∞
Z
0
e−sy F2(y)dy.
(20.185)

20.9 Laplace Convolution Theorem
1035
If we introduce the new variable t = x + y and integrate over t and y instead of x and
y, the limits of integration become (0 ≤t ≤∞), (0 ≤y ≤t). Noting that the Jacobian of
the transformation from (x, y) to (t, y) is unity, we have
f1(s) f2(s) =
∞
Z
0
e−st dt
tZ
0
F1(t −y)F2(y)dy
= L



tZ
0
F1(t −y)F2(y)dy



= L{F1 ∗F2},
(20.186)
where, similarly to the Fourier transform, we use the notation
tZ
0
F1(t −z)F2(z)dz ≡F1 ∗F2 ,
(20.187)
and call this operation the convolution of F1 and F2. It can be shown that convolution is
symmetric:
F1 ∗F2 = F2 ∗F1.
(20.188)
Carrying out the inverse transform, we also ﬁnd
L−1 { f1(s) f2(s)} =
tZ
0
F1(t −z)F2(z)dz = F1 ∗F2.
(20.189)
Convolution formulas are useful for ﬁnding new transforms or, in some cases, as an alterna-
tive to a partial fraction expansion. They also ﬁnd use in the solution of integral equations,
as is illustrated in Chapter 21.
Example 20.9.1
DRIVEN OSCILLATOR WITH DAMPING
As one illustration of the use of the convolution theorem, let us return to the mass m on a
spring, with damping and a driving force F(t). The equation of motion, Eq. (20.160), now
becomes
mX′′(t) + bX′(t) + kX(t) = F(t).
(20.190)
Initial conditions X(0) = 0, X′(0) = 0 are used to simplify this illustration, and the trans-
formed equation is
ms2x(s) + bs x(s) + k x(s) = f (s),
with solution
x(s) = f (s)
m
1
(s + b/2m)2 + ω2
1
,
(20.191)

1036
Chapter 20 Integral Transforms
where, as in Example 20.8.5,
ω2
0 ≡k
m ,
ω2
1 ≡ω2
0 −b2
4m2 .
(20.192)
We identify the right-hand side of Eq. (20.191) as the product of two known transforms:
f (s)
m
= 1
m L{F(t)},
1
(s + b/2m)2 + ω2
1
= 1
ω1
L{e−(b/2m)t sinω1t},
where the second of these is a case of Eq. (20.158).
Now applying the convolution theorem, Eq. (20.189), we obtain the solution to our
original problem as an integral:
X(t) = L−1{x(s)} =
1
mω1
tZ
0
F(t −z)e−(b/2m)z sinω1z dz.
(20.193)
We go on to consider two speciﬁc choices for the driving force F(t). We ﬁrst take the
impulsive force F(t) = Pδ(t). Then
X(t) =
P
mω1
e−(b/2m)t sinω1t.
(20.194)
Here P represents the momentum transferred by the impulse, and the constant P/m takes
the place of an initial velocity X′(0).
As a second case, let F(t) = F0 sinωt. We could again use Eq. (20.193), but a partial
fraction expansion is perhaps more convenient. With
f (s) =
F0ω
s2 + ω2
Eq. (20.191) can be written in the partial fraction form,
x(s) = F0ω
m
1
s2 + ω2
1
(s + b/2m)2 + ω2
1
= F0ω
m
"
a′s + b′
s2 + ω2 +
c′s + d′
(s + b/2m)2 + ω2
1
#
,
(20.195)
with coefﬁcients a′, b′, c′, and d′ (independent of s) to be determined. Direct calculation
shows for a′ and b′
−1
a′ = b
m ω2 + m
b (ω2
0 −ω2)2,
−1
b′ = −m
b (ω2
0 −ω2)
 b
m ω2 + m
b (ω2
0 −ω2)2

.
The terms of x(s) containing a′ and b′ lead upon inversion of the Laplace transform to
the steady-state component of the solution:
X(t) =
F0
[b2ω2 + m2(ω2
0 −ω2)2]1/2 sin(ωt −ϕ),
(20.196)

20.9 Laplace Convolution Theorem
1037
where
tanϕ =
bω
m(ω2
0 −ω2).
Differentiating the denominator, we ﬁnd that the amplitude has a maximum when ω = ω2,
with
ω2
2 = ω2
0 −b2
2m2 = ω2
1 −b2
4m2 .
(20.197)
This is the resonance condition.12 At resonance the amplitude becomes F0/bω1, showing
that the mass m goes into inﬁnite oscillation at resonance if damping is neglected (b = 0).
This calculation differs from those used for the determination of transfer functions (com-
pare Example 20.5.1) in that a steady-state solution at a ﬁxed frequency is not assumed.
Use of the Laplace transform (rather than the Fourier transform) permits solution for tran-
sient as well as steady-state components of the solution. The transients, which we will not
work out in detail, arise from the terms of Eq. (20.195) involving c′ and d′. These terms
contain the quantity (s + b/2m)2 in the denominator, and its presence will generate terms
of the inverse transform that contain the exponential factor e−bt/2m. In other words, these
terms describe exponentially decaying transients.
It is worth noting that we have had three different characteristic frequencies:
Resonance for forced oscillations with damping:
ω2
2 = ω2
0 −b2
2m2 ,
Free oscillation frequency, with damping:
ω2
1 = ω2
0 −b2
4m2 ,
Free oscillation frequency, no damping:
ω2
0 = k
m .
These frequencies coincide only if the damping is zero.
■
Recall that Eq. (20.190) is our ODE for the response of a dynamical system to an arbi-
trary driving force. The ﬁnal response clearly depends on both the driving force and the
characteristics of our system. This dual dependence is separated in the transform space. In
Eq. (20.191) the transform of the response (output) appears as the product of two factors,
one describing the driving force (input) and the other describing the dynamical system.
This is a factorization similar to that we found when discussing the use of Fourier trans-
forms in signal-processing applications in Section 20.5.
Exercises
20.9.1
From the convolution theorem show that
1
s f (s) = L



tZ
0
F(x)dx


,
where f (s) = L{F(t)}.
12The amplitude (squared) has the typical resonance denominator (the Lorentz line shape), found in Exercise 20.2.8.

1038
Chapter 20 Integral Transforms
20.9.2
If F(t) = ta and G(t) = tb, a > −1, b > −1,
(a)
Show that the convolution F ∗G is given by
F ∗G = ta+b+1
1
Z
0
ya(1 −y)b dy.
(b)
By using the convolution theorem, show that
1
Z
0
ya(1 −y)b dy =
a!b!
(a + b + 1)! = B(a + 1,b + 1),
where B is the beta function.
20.9.3
Using the convolution integral, calculate
L−1

s
(s2 + a2)(s2 + b2)

,
a2 ̸= b2.
20.9.4
An undamped oscillator is driven by a force F0 sinωt. Find the displacement X(t) as a
function of time, subject to initial conditions X(0) = X′(0) = 0. Note that the solution
is a linear combination of two simple harmonic motions, one with the frequency of the
driving force and one with the frequency ω0 of the free oscillator.
ANS.
X(t) =
F0/m
ω2 −ω2
0
 ω
ω0
sinω0t −sinωt

.
20.10
INVERSE LAPLACE TRANSFORM
Bromwich Integral
We now develop an expression for the inverse Laplace transform L−1 appearing in the
equation
F(t) = L−1 { f (s)}.
(20.198)
One approach lies in the Fourier transform, for which we know the inverse relation. There
is a difﬁculty, however. Our Fourier transformable function had to satisfy the Dirichlet
conditions. In particular, we required that in order for g(ω) to be a valid Fourier transform,
lim
ω→∞g(ω) = 0,
(20.199)
so that the inﬁnite integral would be well deﬁned.13 Now we wish to treat functions F(t)
that may diverge exponentially. To surmount this difﬁculty, we extract an exponential
factor, eβt, from our (possibly) divergent F(t) and write
F(t) = eβtG(t).
(20.200)
13We made an exception to deal with the delta function, but even in that case g(ω) was bounded for all ω.

20.10 Inverse Laplace Transform
1039
If F(t) diverges as eαt, we require β to be greater than α so that G(t) will be convergent.
Now, with G(t) = 0 for t < 0 and otherwise suitably restricted so that it may be represented
by a Fourier integral, as in Eq. (20.22), we have
G(t) = 1
2π
∞
Z
−∞
eiut du
∞
Z
0
G(v)e−iuv dv.
(20.201)
Inserting Eq. (20.201) into Eq. (20.200), we have
F(t) = eβt
2π
∞
Z
−∞
eiut du
∞
Z
0
F(v)e−βve−iuv dv.
(20.202)
We now make a change of variable to s = β + iu, causing the integral over v in
Eq. (20.202) to assume the form of a Laplace transform:
∞
Z
0
F(v)e−sv dv = f (s).
The variable s is now complex, but must be restricted to ℜe(s) ≥β in order to guarantee
convergence. Note that the Laplace transform has extended a function speciﬁed on the
positive real axis onto the complex plane, ℜe s ≥β.14
We now need to rewrite Eq. (20.202) using the variable s in place of u. The range
−∞< u < ∞corresponds to a contour in the complex plane of s, which is a vertical line
from β −i∞to β + i∞; we also need to substitute du = ds/i. Making these changes,
Eq. (20.202) becomes
F(t) =
1
2πi
β+i∞
Z
β−i∞
est f (s)ds.
(20.203)
Here is our inverse transform. The path has become an inﬁnite vertical line in the complex
plane. Note that the constant β was chosen so that f (s) would be nonsingular for s ≥β. It
can be shown that the nonsingularity of f (s) extends to complex s provided that ℜe s ≥β,
so the integrand of Eq. (20.203) can have singularities only to the left of the integration
path. See Fig. 20.20.
The inverse transformation given by Eq. (20.203) is known as the Bromwich integral,
although sometimes it is referred to as the Fourier-Mellin theorem or Fourier-Mellin
integral. This integral may now be evaluated by the regular methods of contour integration
(Chapter 11). If t > 0 and f (s) is analytic except for isolated singularities (and no branch
points), and is also small at large |s|, the contour may be closed by an inﬁnite semicircle
in the left half-plane that does not contribute to the integral. Then by the residue theorem
(Section 11.8),
F(t) =
X
(residues included for ℜe s < β).
(20.204)
14For a derivation of the inverse Laplace transform using only real variables, see C. L. Bohn and R. W. Flynn, Real variable
inversion of Laplace transforms: An application in plasma physics, Am. J. Phys. 46: 1250 (1978).

1040
Chapter 20 Integral Transforms
Possible singularities
of est f(s)
s-plane
γ
FIGURE 20.20
Possible singularities of est f (s).
It is worth mentioning that in many cases of interest f (s) may become large in the left half-
plane or have branch points, and evaluation of the Bromwich integral may then present
signiﬁcant challenges.
Possibly this means of evaluation with ℜe s ranging through negative values seems
paradoxical in view of our previous requirement that ℜe s ≥β. The paradox disappears
when we recall that the requirement ℜe s ≥β was imposed to guarantee convergence
of the Laplace transform integral that deﬁned f (s). Once f (s) is obtained, we may then
proceed to exploit its properties as an analytic function in the complex plane wherever we
choose.
Perhaps a pair of examples may clarify the evaluation of Eq. (20.203).
Example 20.10.1
INVERSION VIA CALCULUS OF RESIDUES
If f (s) = a/(s2 −a2), then the integrand for the Bromwich integral will be
est f (s) =
aest
s2 −a2 =
aest
(s + a)(s −a).
(20.205)
From the form of Eq. (20.205), we see that this integrand has poles at s = ±a, and the
value of β for the integral must be larger than |a|. Since these are simple poles, it is easy to
verify that the residue at s = a must be eat/2, while the residue at s = −a will be −e−at/2.
The form of the integrand also permits us to close the contour in the left half-plane. We
ﬁnd, in accord with Eq. (20.204),
Residues =
1
2

(eat −e−at) = sinhat = F(t).
(20.206)
Equation (20.206) is in agreement with entry #7 of our table of Laplace transforms, Table
20.1.
■

20.10 Inverse Laplace Transform
1041
Example 20.10.2
MULTIREGION INVERSION
If f (s) = (1 −e−as)/s, the Bromwich integral then has integrand
est f (s) = est
1 −e−as
s

,
(20.207)
and the possibilities for closing the contour depend on the relative magnitudes of t and a.
Considering ﬁrst t > a, we may close the contour for the Bromwich integral in the left
half-plane without changing its value. Our integrand is an entire function (analytic every-
where in the ﬁnite s-plane; note that the s in the denominator cancels when the numerator
is expanded in a Maclaurin series). Since no singularities are enclosed, we conclude that
for t > a, F(t) = 0.
For t in the range 0 < t < a, a different situation is encountered. Expanding the inte-
grand into the two terms
est
s −es(t−a)
s
,
we see that the ﬁrst becomes small in the left half-plane (but large in the right half-plane),
while the second terms behaves in an opposite fashion (large in the left half-plane, small
in the right). The obvious solution is to use different contours for the two terms, each of
which is individually singular, with a pole at s = 0. We therefore close the contour for the
ﬁrst term in the left half-plane, but close that for the second term in the right half-plane.
Since the vertical portion of the contour is at ℜe s = β > 0, we see that the integral of the
ﬁrst term encloses the singularity, while the integral of the second term does not. Therefore
the ﬁrst integral will have a value equal to the residue of the integrand at the singularity
(this residue is 1), while the second integral will vanish. These contours are illustrated in
Fig. 20.21.
Finally, for t < 0, the entire integrand becomes small in the right half-plane, the contour
(for the entire integrand) surrounds no singularities, and the integral is zero. Summarizing
these three cases,
F(t) = u(t) −u(t −a) =



0,
t < 0,
1,
0 < t < a,
0,
t > a,
(20.208)
a step function of unit height and length a (Fig. 20.22).
■
FIGURE 20.21
Contours for Example 20.10.2.

1042
Chapter 20 Integral Transforms
t= a
F(t)
t
1
FIGURE 20.22
Finite-length step function u(t) −u(t −a).
Two general comments may be in order. First, these two examples hardly begin to show
the usefulness and power of the Bromwich integral. It is always available for inverting a
complicated transform when the tables prove inadequate.
Second, this derivation is not presented as a rigorous one. Rather, it is given more as a
plausibility argument, although it can be made rigorous. The determination of the inverse
transform is somewhat similar to the solution of a differential equation. It makes little
difference how you get the inverse transform. Guess at it if you want. It can always be
checked by verifying that
L{F(t)} = f (s).
Two alternate derivations of the Bromwich integral are the subjects of Exercises 20.10.1
and (20.10.2).
Exercises
20.10.1
Derive the Bromwich integral from Cauchy’s integral formula.
Hint. Apply the inverse transform L−1 to
f (s) =
1
2πi lim
α→∞
β+iα
Z
β−iα
f (z)
s −z dz,
where f (z) is analytic for ℜe z ≥β.
20.10.2
Starting with
1
2πi
β+i∞
Z
β−i∞
est f (s)ds,
show that by introducing
f (s) =
∞
Z
0
e−sz F(z)dz

20.10 Inverse Laplace Transform
1043
we can convert our integral into the Fourier representation of a Dirac delta function.
From this derive the inverse Laplace transform.
20.10.3
Derive the Laplace transformation convolution theorem by use of the Bromwich inte-
gral.
20.10.4
Find
L−1

s
s2 −k2

(a)
by a partial fraction expansion.
(b)
Repeat, using the Bromwich integral.
20.10.5
Find
L−1

k2
s(s2 + k2)

(a)
by using a partial fraction expansion.
(b)
Repeat using the convolution theorem.
(c)
Repeat using the Bromwich integral.
ANS.
F(t) = 1 −coskt.
20.10.6
Use the Bromwich integral to ﬁnd the function whose transform is f (s) = s−1/2. Note
that f (s) has a branch point at s = 0. The negative x-axis may be taken as a cut line.
See Fig. 20.23.
FIGURE 20.23
Contour for Exercise 20.10.6.

1044
Chapter 20 Integral Transforms
Hint. A portion of the path needed to close the contour will yield nonzero contributions
to the contour integral. These will need to be taken into account to get the proper value
for the Bromwich integral.
ANS.
F(t) = (πt)−1/2.
20.10.7
Show that
L−1 n
(s2 + 1)−1/2o
= J0(t)
by evaluation of the Bromwich integral.
Hint. Convert your Bromwich integral into an integral representation of J0(t). Fig-
ure 20.24 shows a possible contour.
20.10.8
Evaluate the inverse Laplace transform
L−1 n
(s2 −a2)−1/2o
by each of the following methods:
(a)
Expansion in a series and term-by-term inversion.
(b)
Direct evaluation of the Bromwich integral.
(c)
Change of variable in the Bromwich integral: s = (a/2)(z + z−1).
20.10.9
Show that
L−1
lns
s

= −lnt −γ,
where γ = 0.5772... is the Euler-Mascheroni constant.
s= i
s= −i
FIGURE 20.24
A possible contour for the inversion of J0(t).

Additional Readings
1045
20.10.10 Evaluate the Bromwich integral for
f (s) =
s
(s2 + a2)2 .
20.10.11 Heaviside expansion theorem. If the transform f (s) may be written as a ratio
f (s) = g(s)
h(s),
where g(s) and h(s) are analytic functions, with h(s) having simple, isolated zeros at
s = si, show that
F(t) = L−1
g(s)
h(s)

=
X
i
g(si)
h′(si) esit.
Hint. See Exercise 11.6.3.
20.10.12 Using the Bromwich integral, invert f (s) = s−2e−ks. Express F(t) = L−1 { f (s)} in
terms of the (shifted) unit step function u(t −k).
ANS.
F(t) = (t −k)u(t −k).
20.10.13 You have a Laplace transform:
f (s) =
1
(s + a)(s + b),
a ̸= b.
Invert this transform by each of three methods:
(a)
Partial fractions and use of tables,
(b)
Convolution theorem,
(c)
Bromwich integral.
ANS.
F(t) = e−bt −e−at
a −b
, a ̸= b.
Additional Readings
Abramowitz, M., and I. A. Stegun, eds., Handbook of Mathematical Functions with Formulas, Graphs, and
Mathematical Tables (AMS-55). Washington, DC: National Bureau of Standards (1972), reprinted, Dover
(1974). Chapter 29 contains tables of Laplace transforms.
Champeney, D. C., Fourier Transforms and Their Physical Applications. New York: Academic Press (1973).
Fourier transforms are developed in a careful, easy-to-follow manner. Approximately 60% of the book is
devoted to applications of interest in physics and engineering.
Erdelyi, A., W. Magnus, F. Oberhettinger, and F. G. Tricomi, Tables of Integral Transforms, 2 vols. New York:
McGraw-Hill (1954). This text contains extensive tables of Fourier sine, cosine, and exponential transforms,
Laplace and inverse Laplace transforms, Mellin and inverse Mellin transforms, Hankel transforms, and other
more specialized integral transforms.
Hamming, R. W., Numerical Methods for Scientists and Engineers, 2nd ed. New York: McGraw-Hill (1973),
reprinted, Dover (1987). Chapter 33 provides an excellent description of the fast Fourier transform.
Hanna, J. R., Fourier Series and Integrals of Boundary Value Problems. Somerset, NJ: Wiley (1990). This book
is a broad treatment of the Fourier solution of boundary value problems. The concepts of convergence and
completeness are given careful attention.

1046
Chapter 20 Integral Transforms
Jeffreys, H., and B. S. Jeffreys, Methods of Mathematical Physics, 3rd ed. Cambridge: Cambridge University
Press (1972).
Krylov, V. I., and N. S. Skoblya, Handbook of Numerical Inversion of Laplace Transform (translated by
D. Louvish). Jerusalem: Israel Program for Scientiﬁc Translations (1969).
Lepage, W. R., Complex Variables and the Laplace Transform for Engineers. New York: McGraw-Hill (1961);
Dover (1980). A complex variable analysis that is carefully developed and then applied to Fourier and Laplace
transforms. It is written to be read by students, but intended for the serious student.
McCollum, P. A., and B. F. Brown, Laplace Transform Tables and Theorems. New York: Holt, Rinehart and
Winston (1965).
Miles, J. W., Integral Transforms in Applied Mathematics. Cambridge: Cambridge University Press (1971). This
is a brief but interesting and useful treatment for the advanced undergraduate. It emphasizes applications rather
than abstract mathematical theory.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Parseval’s
relations are derived independently of the inverse Fourier transform in Section 4.8 of this comprehensive, but
difﬁcult text.
Papoulis, A., The Fourier Integral and Its Applications. New York: McGraw-Hill (1962). This is a rigorous
development of Fourier and Laplace transforms and includes extensive applications in science and
engineering.
Roberts, G. E., and H. Kaufman, Table of Laplace Transforms. Philadelphia: Saunders (1966).
Sneddon, I. N., Fourier Transforms. New York: McGraw-HiII (1951), reprinted, Dover (1995). A detailed com-
prehensive treatment, this book is loaded with applications to a wide variety of ﬁelds of modern and classical
physics.
Sneddon, I. N., The Use of Integral Transforms. New York: McGraw-Hill (1974). Written for students in science
and engineering in terms they can understand, this book covers all the integral transforms mentioned in this
chapter as well as in several others. Many applications are included.
Titchmarsh, E. C., Introduction to the Theory of Fourier Integrals, 2nd ed. New York: Oxford University Press
(1937).
Van der Pol, B., and H. Bremmer, Operational Calculus Based on the Two-sided Laplace Integral, 3rd ed.
Cambridge, UK: Cambridge University Press (1987). Here is a development based on the integral range −∞
to +∞, rather than the useful 0 to ∞. Chapter V contains a detailed study of the Dirac delta function (impulse
function).
Wolf, K. B., Integral Transforms in Science and Engineering. New York: Plenum Press (1979). This book is a
very comprehensive treatment of integral transforms and their applications.

CHAPTER 21
INTEGRAL EQUATIONS
21.1
INTRODUCTION
With the exception of the integral transforms of Chapter 20, we have for the most part been
considering relations between an unknown function ϕ(x) and one or more of its deriva-
tives. We now proceed to investigate equations containing the unknown function within an
integral. As with differential equations, we shall conﬁne our attention to linear relations,
which are called linear integral equations. These integral equations are classiﬁed in two
ways:
•
If the limits of integration are ﬁxed, we call the equation a Fredholm equation; if
one limit is variable, it is a Volterra equation.
•
If the unknown function appears only under the integral sign, we label it ﬁrst kind.
If it appears both inside and outside the integral, it is labeled second kind.
Here are some examples of these deﬁnitions. In each of the following equations, ϕ(t) is
an unknown function whose value we seek. K(x,t), which we call the kernel, and f (x)
are assumed to be known. When f (x) = 0, the equation is said to be homogeneous.
This is a Fredholm equation of the ﬁrst kind,
f (x) =
b
Z
a
K(x,t)ϕ(t)dt.
(21.1)
Next we have a Fredholm equation of the second kind, which is an eigenvalue equation
with λ the eigenvalue,
ϕ(x) = f (x) + λ
b
Z
a
K(x,t)ϕ(t)dt.
(21.2)
1047
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

1048
Chapter 21 Integral Equations
Here we have a Volterra equation of the ﬁrst kind,
f (x) =
x
Z
a
K(x,t)ϕ(t)dt;
(21.3)
and a Volterra equation of the second kind,
ϕ(x) = f (x) +
x
Z
a
K(x,t)ϕ(t)dt.
(21.4)
Why do we bother about integral equations? After all, the differential equations have
done a rather good job of describing our physical world so far. However, there are several
reasons for introducing integral equations.
First, we have placed considerable emphasis on the solution of differential equations
subject to particular boundary conditions. For instance, the boundary condition at r =
0 determines whether the Neumann function Yn(r) is present when Bessel’s equation is
solved. The boundary condition for r →∞determines whether In(r) is present in our
solution of the modiﬁed Bessel equation. To the contrary, an integral equation relates the
unknown function not only to its values at neighboring points (derivatives) but also to
its values throughout a region, including the boundary. In a very real sense the boundary
conditions are built into the integral equation rather than imposed at the ﬁnal stage of the
solution. It will be seen later in this section that if we construct an integral equation that is
equivalent to a differential equation with its boundary conditions, the form of that integral
equation depends on the boundary conditions.
A second feature of integral equations is that their compact and completely self-
contained form may turn out to be a more convenient or powerful formulation of a problem
than a differential equation and its boundary conditions. Mathematical problems such as
existence, uniqueness, and completeness may often be handled more easily and elegantly
in integral form. And ﬁnally, whether or not we like it, there are problems, such as some
diffusion and transport phenomena, that cannot be represented by differential equations. If
we wish to solve such problems, we are forced to handle integral equations.
Example 21.1.1
MOMENTUM REPRESENTATION IN QUANTUM MECHANICS
The Schrödinger equation (in ordinary space representation) for a particle of mass m sub-
ject to a potential V(r) is
−¯h2
2m ∇2ψ(r) + V (r)ψ(r) = Eψ(r),
(21.5)
and we previously found, extending the 1-D result from Eq. (20.97), that in momentum
space the equivalent equation (for the Coulomb potential in hartree atomic units) is
k2
2m ϕ(k) +
1
(2π)3/2
Z
4π
|k −k′|2 ϕ(k′)d3k′ = Eϕ(k).
(21.6)
This is an integral-equation eigenvalue problem. Note that the kernel of Eq. (21.6) is a
function of k−k′; this functional dependence, which arises from the convolution theorem,

21.1 Introduction
1049
is typical of an ordinary potential in which the direct-space wave function is multiplied by
a function that depends only on position.
■
Transformation of a Differential Equation into an
Integral Equation
Often we ﬁnd that we have a choice. The physical problem may be represented by a dif-
ferential or an integral equation. Let us assume that we have the differential equation and
wish to transform it into an integral equation. Starting with a linear second-order ordinary
differential equation (ODE),
y′′ + A(x)y′ + B(x)y = g(x),
(21.7)
with initial conditions
y(a) = y0,
y′(a) = y′
0,
we integrate to obtain
y′(x) = −
x
Z
a
A(t)y′(t)dt −
x
Z
a
B(t)y(t)dt +
x
Z
a
g(t)dt + y′
0.
Integrating the ﬁrst integral on the right by parts yields
y′(x) = −A(x)y(x) −
x
Z
a
h
B(t) −A′(t)
i
y(t)dt +
x
Z
a
g(t)dt + A(a)y0 + y′
0.
Integrating a second time, we obtain
y(x) = −
x
Z
a
A(t)y(t)dt −
x
Z
a
du
u
Z
a
h
B(t) −A′(t)
i
y(t)dt
+
x
Z
a
du
u
Z
a
g(t)dt +
h
A(a)y0 + y′
0
i
(x −a) + y0.
(21.8)
To transform this equation into a neater form, we use the relation
x
Z
a
du
u
Z
a
f (t)dt =
x
Z
a
f (t)dt
x
Z
t
du =
x
Z
a
(x −t) f (t)dt.
(21.9)

1050
Chapter 21 Integral Equations
Applying this result to Eq. (21.8), we obtain
y(x) = −
x
Z
a

A(t) + (x −t)
h
B(t) −A′(t)
i
y(t)dt
+
x
Z
a
(x −t)g(t)dt +
h
A(a)y0 + y′
0
i
(x −a) + y0.
(21.10)
If we now introduce the abbreviations
K(x,t) = (t −x)
h
B(t) −A′(t)
i
−A(t),
f (x) =
x
Z
a
(x −t)g(t)dt +
h
A(a)y0 + y′
0
i
(x −a) + y0,
Eq. (21.10) becomes
y(x) = f (x) +
x
Z
a
K(x,t)y(t)dt,
(21.11)
which is a Volterra equation of the second kind. Note that f (x) in Eq. (21.11) has a form
that includes the initial conditions from the original differential equation.
Another method for obtaining an integral equation equivalent to a differential equa-
tion plus its boundary conditions was presented in Section 10.1, where we found that the
Green’s function for a differential equation appeared as the kernel of the equivalent integral
equation.
Example 21.1.2
LINEAR OSCILLATOR EQUATION
Let’s ﬁnd an integral equation equivalent to the linear oscillator equation
y′′ + ω2y = 0
(21.12)
with boundary conditions
y(0) = 0,
y′(0) = 1.
This corresponds to Eq. (21.7) with
A(x) = 0,
B(x) = ω2,
g(x) = 0.
Substituting into Eq. (21.10), we ﬁnd that the integral equation becomes
y(x) = x + ω2
x
Z
0
(t −x)y(t)dt.
(21.13)

21.1 Introduction
1051
This integral equation, Eq. (21.13), is equivalent to the original differential equation
plus the initial conditions. A check shows that each form is indeed satisﬁed by y(x) =
(1/ω)sinωx.
Let us reconsider the linear oscillator equation, Eq. (21.12), but now with the boundary
conditions
y(0) = 0,
y(b) = 0.
Since y′(0) is not given, we must modify the procedure. The ﬁrst integration gives
y′ = −ω2
x
Z
0
y dx + y′(0).
Integrating a second time and again using Eq. (21.9), we have
y = −ω2
x
Z
0
(x −t)y(t)dt + xy′(0).
(21.14)
To eliminate the unknown y′(0), we now impose the condition y(b) = 0. This gives
ω2
b
Z
0
(b −t)y(t)dt = by′(0).
Substituting this back into Eq. (21.14), we obtain
y(x) = −ω2
x
Z
0
(x −t)y(t)dt + ω2 x
b
b
Z
0
(b −t)y(t)dt.
Now let us break the interval [0,b] into two intervals, [0, x] and [x,b]. Since
x
b (b −t) −(x −t) = t
b(b −x),
we ﬁnd
y(x) = ω2
x
Z
0
t
b(b −x)y(t)dt + ω2
b
Z
x
x
b (b −t)y(t)dt.
(21.15)
Finally, if we deﬁne the kernel
K(x,t) =



t
b(b −x),
t < x,
x
b (b −t),
t > x,
(21.16)

1052
Chapter 21 Integral Equations
we have
y(x) = ω2
b
Z
0
K(x,t)y(t)dt,
(21.17)
a homogeneous Fredholm equation of the second kind.
Our new kernel, K(x,t), illustrated in Fig. 21.1, has some interesting properties.
1.
It is symmetric, K(x,t) = K(t, x).
2.
It is continuous, in the sense that
t
b(b −x)

t=x
= x
b (b −t)

t=x
.
3.
Its derivative with respect to t is discontinuous. As t increases through the point
t = x, there is a discontinuity of −1 in ∂K(x,t)/∂t.
Comparing with the discussion in Section 10.1, we identify K(x,t) as the Green’s func-
tion for this ODE with the speciﬁed boundary conditions. Note in particular Eq. (10.30),
which corresponds exactly to what was found here.
■
The above example shows how the initial or boundary conditions play a decisive role in
the conversion of a linear second-order ODE into an integral equation. Summarizing,
If we have initial conditions (only one end of our interval), the differen-
tial equation transforms into a Volterra integral equation. But if we have a
boundary value problem (boundary conditions at both ends of our inter-
val), the differential equation leads to a Fredholm-type integral equation
with a kernel that will be the Green’s function appropriate to the given
boundary conditions.
In closing, we call attention to the fact that the reverse transformation (integral equation
to differential equation) is not always possible. There exist integral equations for which no
corresponding differential equation is known.
x
b
t
K(x, t)
FIGURE 21.1
Kernel, Eq. (21.16), for linear oscillator boundary-value problem.

21.2 Some Special Methods
1053
Exercises
21.1.1
Starting with the ODE, integrate twice and derive the Volterra integral equation corre-
sponding to
(a)
y′′(x) −y(x) = 0;
y(0) = 0,
y′(0) = 1.
ANS.
y =
x
Z
0
(x −t)y(t)dt + x.
(b)
y′′(x) −y(x) = 0;
y(0) = 1,
y′(0) = −1.
ANS.
y =
x
Z
0
(x −t)y(t)dt −x + 1.
Check your results with Eq. (21.11).
21.1.2
Starting with the given answers of Exercise 21.1.1, differentiate and recover the original
ODEs and the boundary conditions.
21.1.3
Given ϕ(x) = x −
x
Z
0
(t −x)ϕ(t)dt,
solve this integral equation by converting it to an ODE (plus boundary conditions) and
solving the ODE (by inspection).
21.1.4
Show that the homogeneous Volterra equation of the second kind
ψ(x) = λ
x
Z
0
K(x,t)ψ(t)dt
has no solution (apart from the trivial solution ψ = 0).
Hint. Develop a Maclaurin expansion of ψ(x). Assume that ψ(x) and K(x,t) are dif-
ferentiable with respect to x as needed.
21.2
SOME SPECIAL METHODS
It is well known that general methods are available both for differentiating functions and
(compare Chapters 7 and 9) for solving linear differential equations, while there is no
general direct method for evaluating integrals. Integrations are carried out using a variety
of tools of limited applicability, and the process is ultimately one of pattern recognition
and the application of experience. Similar observations apply to the solution of integral
equations. We consider here some special methods that work when the integral equation
under study has suitable characteristics.

1054
Chapter 21 Integral Equations
Integral-Transform Methods
When the kernel of an integral equation (and its integration limits) match the speciﬁcation
of an integral transform for which we have an inversion formula, we can use that identiﬁ-
cation to solve the integral equation. Formulas based on four integral transforms are listed
here for reference, in each case with f (x) a known function and ϕ(x) to be determined.
If our integral equation is f (x) =
1
√
2π
R ∞
−∞eixtϕ(t)dt, then its solution is
ϕ(x) =
1
√
2π
∞
Z
−∞
e−ixt f (t)dt
(Fourier transform).
(21.18)
If our integral equation is f (x) =
R ∞
0 e−xtϕ(t)dt, then its solution is
ϕ(x) =
1
2πi
γ +i∞
Z
γ −i∞
ext f (t)dt
(Laplace transform).
(21.19)
If our integral equation is f (x) =
R ∞
0 tx−1ϕ(t)dt, then its solution is
ϕ(x) =
1
2πi
γ +i∞
Z
γ −i∞
x−t f (t)dt
(Mellin transform).
(21.20)
If our integral equation is f (x) =
R ∞
0 tϕ(t)Jν(xt)dt, then its solution is
ϕ(x) =
∞
Z
0
t f (t)Jν(xt)dt
(Hankel transform).
(21.21)
Note that these formulas can also be applied “in reverse,” i.e., with ϕ(x) known and f (x)
to be determined. This observation, however, is of somewhat limited utility since nothing
signiﬁcantly new appears for the inverse Fourier and Hankel transforms, while the integra-
tion limits for the inverse Laplace and Mellin transforms make them unlikely to appear in
an integral equation.
Actually the usefulness of the integral transform technique extends a bit beyond these
four rather specialized forms. We illustrate with two examples.
Example 21.2.1
FOURIER TRANSFORM SOLUTION
Let’s consider a Fredholm equation of the ﬁrst kind with a kernel of the general type
k(x −t), where k is a function (not a constant),
f (x) =
∞
Z
−∞
k(x −t)ϕ(t)dt,
(21.22)

21.2 Some Special Methods
1055
in which ϕ(t) is our unknown function. Assuming that the needed transforms exist, we
apply the Fourier convolution theorem, Eq. (20.71), to obtain
f (x) =
∞
Z
−∞
K(ω)8(ω)e−iωxdω.
(21.23)
The functions K(ω) and 8(ω) are, respectively, the Fourier transforms of k(x) and ϕ(x).
Taking the Fourier transform of both sides of Eq. (21.23), the formula for which is
Eq. (21.18), we ﬁnd
K(ω)8(ω) = 1
2π
∞
Z
−∞
f (x)eiωxdx = F(ω)
√
2π
,
(21.24)
where F(ω) is the Fourier transform of f (x). Since 8(ω) is the only unknown in
Eq. (21.24), we may solve for it, obtaining
8(ω) =
1
√
2π
F(ω)
K(ω),
(21.25)
and, using the inverse Fourier transform, we have the solution to Eq. (21.22):
ϕ(x) = 1
2π
∞
Z
−∞
F(ω)
K(ω)e−iωxdω.
(21.26)
A rigorous justiﬁcation of this result is presented by Morse and Feshbach (see Additional
Readings). An extension of this transformation solution appears as Exercise 21.2.1.
■
Example 21.2.2
GENERALIZED ABEL EQUATION
The generalized Abel equation is a Volterra equation of the ﬁrst kind:
f (x) =
x
Z
0
ϕ(t)
(x −t)α dt,
0 < α < 1,
with
 f (x)
known,
ϕ(t)
unknown.
(21.27)
Taking the Laplace transform of both sides of this equation, we obtain
L{ f (x)} = L



x
Z
0
ϕ(t)
(x −t)α dt


= L{x−α}L{ϕ(x)},

1056
Chapter 21 Integral Equations
the last step following by the Laplace convolution theorem, Eq. (20.186). Then, evaluating
L{x−α} from entry 3 of Table 20.1,
L{ϕ(x)} = s1−αL{ f (x)}
0(1 −α)
.
(21.28)
In principle, our integral equation is solved, since all that remains is to take the inverse
transform of Eq. (21.28). A clever way of obtaining the inverse transform proceeds as
follows, with its initial step being to divide Eq. (21.28) by s.1 We get
1
s L{ϕ(x)} = s−αL{ f (x)}
0(1 −α)
= L{xα−1}L{ f (x)}
0(α)0(1 −α) .
Combining the gamma functions according to Eq. (13.23) and applying the Laplace con-
volution theorem again, we discover that
1
s L{ϕ(x)} = sinπα
π
L



x
Z
0
f (t)
(x −t)1−α dt


.
Inverting with the aid of Entry 3 of Table 20.2, we get
x
Z
0
ϕ(t)dt = sinπα
π
x
Z
0
f (t)
(x −t)1−α dt,
and ﬁnally, by differentiating, we have the solution to our generalized Abel equation:
ϕ(x) = sinπα
π
d
dx
x
Z
0
f (t)
(x −t)1−α dt.
(21.29)
■
Generating-Function Method
Occasionally, the reader may encounter integral equations that involve generating func-
tions. Suppose we have the admittedly special case,
f (x) =
1
Z
−1
ϕ(t)
(1 −2xt + x2)1/2 dt,
−1 ≤x ≤1,
(21.30)
where f (x) is known and ϕ(t) is to be determined.
We note two important features:
1.
(1 −2xt + x2)−1/2 generates the Legendre polynomials.
2.
[−1,1] is the orthogonality interval for the Legendre polynomials.
1This division converts s1−α, which cannot be inverted when 0 < α < 1, into s−α, which is the transform of xα−1/0(α).

21.2 Some Special Methods
1057
These features make it possible to expand the denominator in Legendre polynomials, sug-
gesting that it may be useful also to represent ϕ(t) as an expansion in these same functions.
Thus, we introduce the expansions
1
(1 −2xt + x2)1/2 =
∞
X
n=0
Pn(t)xn,
ϕ(t) =
∞
X
m=0
am Pm(t).
Substituting these expansions into our integral equation, Eq. (21.30),
f (x) =
∞
X
n=0
∞
X
m=0
amxn
1
Z
−1
Pn(t)Pm(t)dt =
∞
X
n=0
∞
X
m=0
amxn 2δnm
2n + 1
=
∞
X
n=0
2an
2n + 1xn.
(21.31)
If we now insert into Eq. (21.31) the Maclaurin series expansion for f (x),
f (x) =
∞
X
n=0
f (n)(0)
n!
,
we may equate powers of x, reaching, for each n,
f (n)(0)
n!
=
2an
2n + 1,
so the solution to our integral equation is
ϕ(t) =
∞
X
n=0
2n + 1
2
f (n)(0)
n!
Pn(t).
(21.32)
Similar results may be obtained with other generating functions (see the list in Table 12.1).
This technique of expanding in a series of special functions is always avail-
able. It is worth a try whenever the expansion is possible (and convenient)
and the interval is appropriate.
Separable Kernel
We consider here the special case that the kernel of our integral equation is separable, in
the sense that
K(x,t) =
n
X
j=1
M j(x)N j(t),
(21.33)
where n, the upper limit of the sum, is ﬁnite. Such kernels are sometimes called degener-
ate. Our class of separable kernels includes all polynomials and many of the elementary
transcendental functions. For example, K(x,t) = cos(t −x) is separable:
cos(t −x) = cost cos x + sint sin x.

1058
Chapter 21 Integral Equations
Integral equations with separable kernels have the desirable property that they can be
related to eigenvalue equations and permit the application of methods of linear algebra.
Let’s consider a Fredholm equation of the second kind, Eq. (21.2), with a separable
kernel of the form given in Eq. (21.33). Inserting this formula for K(x,t) and bringing the
summation outside the integral, we have
ϕ(x) = f (x) + λ
n
X
j=1
M j(x)
b
Z
a
N j(t)ϕ(t)dt.
(21.34)
We now see that the integral with respect to t will for each j be a constant (with values
that are currently not known):
b
Z
a
N j(t)ϕ(t)dt = c j.
(21.35)
Hence Eq. (16.71) becomes
ϕ(x) = f (x) + λ
n
X
j=1
c j M j(x).
(21.36)
Once the constants ci have been determined, Eq. (21.36) will give us ϕ(x), the solution to
our integral equation. Equation (21.36) further tells us that the form of ϕ(x) will consist of
f (x) plus a linear combination of the x-dependent factors in the separable kernel.
We may ﬁnd the ci by multiplying Eq. (21.36) by Ni(x) and integrating to eliminate the
x-dependence. Use of Eq. (21.35) yields
ci = bi + λ
n
X
j=1
ai jc j,
(21.37)
where
bi =
b
Z
a
Ni(x) f (x)dx,
ai j =
b
Z
a
Ni(x)M j(x)dx.
(21.38)
It is perhaps helpful to write Eq. (21.37) in matrix form , with A = (ai j):
b = c −λAc = (1 −λA)c,
(21.39)
or
c = (1 −λA)−1b.
(21.40)
Equation (21.39) is equivalent to a set of simultaneous linear algebraic equations
(1 −λa11)c1 −λa12c2 −λa13c3 −··· = b1,
−λa21c1 + (1 −λa22)c2 −λa23c3 −··· = b2,
(21.41)
−λa31c1 −λa32c2 + (1 −λa33)c3 −··· = b3,
and so on.

21.2 Some Special Methods
1059
If our integral equation is homogeneous, so f (x) = 0, then b = 0. To get a solution in that
case, we set the determinant of the coefﬁcients of ci equal to zero:
|1 −λA| = 0,
(21.42)
exactly as for any matrix eigenvalue problem. The roots of Eq. (21.42) yield our eigen-
values. Substituting into (1 −λA)c = 0, we ﬁnd the ci and then Eq. (21.36) gives our
solution.
Example 21.2.3
HOMOGENEOUS FREDHOLM EQUATION
To illustrate this technique for determining eigenvalues and eigenfunctions of the homo-
geneous Fredholm equation of the second kind, we consider
ϕ(x) = λ
1
Z
−1
(t + x)ϕ(t)dt.
(21.43)
Writing the kernel of this equation as M1(x)N1(t) + M2(x)N2(t), we have
M1(x) = 1,
M2(x) = x,
N1(t) = t,
N2(t) = 1.
Using the notation of Eqs. (21.33) to (21.42), we ﬁnd from Eq. (21.38):
a11 = a22 = 0,
a12 = 2
3,
a21 = 2;
b1 = b2 = 0.
Equation (21.42), our secular equation, becomes2

1 −2λ
3
−2λ
1
 = 0.
(21.44)
Expanding, we obtain
1 −4λ2
3
= 0,
λ = ±
√
3
2 .
(21.45)
Substituting the eigenvalues λ = ±
√
3/2 into Eq. (21.39), we have
c1 ∓c2
√
3
= 0.
(21.46)
Finally, with the choice c1 = 1, Eq. (21.36) gives the two solutions
ϕ1(x) =
√
3
2 (1 +
√
3x),
λ =
√
3
2 ,
(21.47)
2This equation would look more like our usual secular equations if each row of the determinant were divided by λ. Then we
would have the secular equation in a familiar form, but with 1/λ identiﬁed as the eigenvalue.

1060
Chapter 21 Integral Equations
ϕ2(x) = −
√
3
2 (1 −
√
3x),
λ = −
√
3
2 .
(21.48)
Since our equation is homogeneous, the normalization of ϕ(x) is arbitrary.
■
If the kernel of an integral equation is not separable in the sense of Eq. (21.33), there is
still the possibility that it may be approximated by a kernel that is separable. Then we can
get the exact solution of an approximate equation, which we can treat as an approximation
to the solution of the original equation.
Exercises
21.2.1
The kernel of a Fredholm equation of the second kind,
ϕ(x) = f (x) + λ
∞
Z
−∞
K(x,t)ϕ(t)dt,
is of the form k(x −t).3 Assuming that the required transforms exist, show that
ϕ(x) =
1
√
2π
∞
Z
−∞
F(t)e−ixt dt
1 −
√
2πλK(t)
.
F(t) and K(t) are the Fourier transforms of f (x) and k(x), respectively.
21.2.2
(a)
The kernel of a Volterra equation of the ﬁrst kind,
f (x) =
x
Z
0
K(x,t)ϕ(t)dt,
has the form k(x −t). Assuming that the required transforms exist, show that
ϕ(x) =
1
2πi
γ +i∞
Z
γ −i∞
F(s)
K(s)exsds,
where F(s) and K(s) are, respectively, the Laplace transforms of f (x) and k(x).
(b)
In terms of the notation of part (a), show that the Volterra equation of the second
kind,
ϕ(x) = f (x) + λ
x
Z
0
K(x,t)ϕ(t)dt,
3This kernel and a range 0 ≤x < ∞are the characteristics of integral equations of the Wiener-Hopf type. Details will be found
in Chapter 8 of Morse and Feshbach (1953); see the Additional Readings.

21.2 Some Special Methods
1061
has solution
ϕ(x) =
1
2πi
γ +i∞
Z
γ −i∞
F(s)
1 −λK(s)exsds.
21.2.3
Using the Laplace transform solution (Exercise 21.2.2), solve
(a)
ϕ(x) = x +
x
Z
0
(t −x)ϕ(t)dt.
ANS.
ϕ(x) = sin x.
(b)
ϕ(x) = x −
x
Z
0
(t −x)ϕ(t)dt.
ANS.
ϕ(x) = sinh x.
Check your results by substituting back into the original integral equations.
21.2.4
Reformulate the equations of Example 21.2.1 for integrals on the range (0,∞) using
Fourier cosine transforms.
21.2.5
Given the Fredholm integral equation,
e−x2 =
∞
Z
−∞
e−(x−t)2ϕ(t)dt,
apply the Fourier convolution technique of Example 21.2.1 to solve for ϕ(t).
21.2.6
Solve Abel’s equation,
f (x) =
x
Z
0
ϕ(t)
(x −t)α dt,
0 < α < 1,
by the following method:
(a)
Multiply both sides by (z −x)α−1 and integrate with respect to x over the range
0 ≤x ≤z.
(b)
Reverse the order of integration and evaluate the integral on the right-hand side
(with respect to x) by recognizing it as a beta function.
Note.
z
Z
t
dx
(z −x)1−α(x −t)α = B(1 −α,α) = 0(α)0(1 −α) =
π
sinπα .

1062
Chapter 21 Integral Equations
21.2.7
Given the generalized Abel equation with f (x) = 1,
1 =
x
Z
0
ϕ(t)
(x −t)α dt,
0 < α < 1,
solve for ϕ(t) and verify that ϕ(t) is a solution of the given equation.
ANS.
ϕ(t) = sinπα
π
tα−1.
21.2.8
A Fredholm equation of the ﬁrst kind has a kernel e−(x−t)2:
f (x) =
∞
Z
−∞
e−(x−t)2ϕ(t)dt.
Show that the solution is
ϕ(x) =
1
√π
∞
X
n=0
f (n)(0)
2nn! Hn(x),
in which Hn(x) is an nth-order Hermite polynomial.
21.2.9
Solve the integral equation
f (x) =
1
Z
−1
ϕ(t)
(1 −2xt + x2)1/2 dt,
−1 ≤x ≤1,
for the unknown function ϕ(t), if
(a) f (x) = x2s,
(b) f (x) = x2s+1.
ANS.
(a) ϕ(t) = 4s + 1
2
P2s(t),
(b) ϕ(t) = 4s + 3
2
P2s+1(t).
21.2.10
Find the eigenvalues and eigenfunctions of
ϕ(x) = λ
1
Z
−1
(t −x)ϕ(t)dt.
21.2.11
Find the eigenvalues and eigenfunctions of
ϕ(x) = λ
2π
Z
0
cos(x −t)ϕ(t)dt.
ANS.
λ1 = λ2 = 1
π ,
ϕ(x) = A cos x + B sin x.

21.2 Some Special Methods
1063
21.2.12
Find the eigenvalues and eigenfunctions of
y(x) = λ
1
Z
−1
(x −t)2y(t)dt.
Hint. This problem may be treated by the separable-kernel method or by a Legendre
expansion.
21.2.13
Use the separable-kernel technique to show that
ψ(x) = λ
π
Z
0
cos x sintψ(t)dt
has no solution (apart from ψ = 0). Explain this result in terms of separability and
symmetry.
21.2.14
Given ϕ(x) = λ
1
Z
0
(1 + xt)ϕ(t)dt,
solve for the eigenvalues and the eigenfunctions by the separable-kernel technique.
21.2.15
Knowing the form of the solutions of an integral equation can be a great advantage. For
ϕ(x) = λ
1
Z
0
(1 + xt)ϕ(t)dt,
assume ϕ(x) to have the form 1 + bx. Substitute into the integral equation. Integrate
and solve for b and λ.
21.2.16
The equation
f (x) =
b
Z
a
K(x,t)ϕ(t)dt
has a degenerate kernel K(x,t) = Pn
i=1 Mi(x)Ni(t).
(a)
Show that this integral equation has no solution unless f (x) can be written as
f (x) =
n
X
i=1
fi Mi(x),
where the fi are constants.
(b)
Show that to any solution ϕ(x) we may add ψ(x), provided that ψ(x) is orthogo-
nal to all Ni(x):
b
Z
a
Ni(x)ψ(x)dx = 0
for all i.

1064
Chapter 21 Integral Equations
21.2.17
A Kirchhoff diffraction theory analysis of a laser leads to the integral equation
v(r2) = γ
Z Z
K(r1,r2)v(r1)d A.
The unknown, v(r1), gives the geometric distribution of the radiation ﬁeld over one
mirror surface; the range of integration is over the surface of that mirror. For square
confocal spherical mirrors, the integral equation becomes
v(x2, y2) = −iγ eikb
λb
a
Z
−a
a
Z
−a
e−(ik/b)(x1x2+y1y2)v(x1, y1)dx1dy1,
in which b is the centerline distance between the laser mirrors. This can be put in a
somewhat simpler form by the substitutions
kx2
i
b
= ξ2
i ,
ky2
i
b
= η2
i ,
and
ka2
b
= 2πa2
λb
= α2.
(a)
Show that the variables separate and we get two integral equations.
(b)
Show that the new limits, ±α, may be approximated by ±∞for a mirror dimen-
sion a ≫λ.
(c)
Solve the resulting integral equations.
21.3
NEUMANN SERIES
Many and probably most integral equations cannot be solved by the specialized techniques
of the preceding section. Here we develop a rather general technique for solving integral
equations. The method, due largely to Neumann, Liouville, and Volterra, develops the
unknown function ϕ(x) as a power series in λ, where λ is a given constant. The method is
applicable whenever the series converges.
We solve a linear integral equation of the second kind by successive approximations;
let’s take as an example the Fredholm equation
ϕ(x) = f (x) + λ
b
Z
a
K(x,t)ϕ(t)dt,
(21.49)
in which f (x) ̸= 0. If the upper limit of the integral is a variable (Volterra equation),
the following development will still hold, but with minor modiﬁcations. Let us make the
following initial approximation to our unknown function:
ϕ(x) ≈ϕ0(x) = f (x).
(21.50)
This choice is not mandatory. If you can make a better guess, go ahead and guess. The
choice here is equivalent to saying that the term of the equation containing the integral is

21.3 Neumann Series
1065
small relative to f (x). To improve this ﬁrst crude approximation, we feed ϕ0(x) back into
the integral in Eq. (21.49), getting
ϕ1(x) = f (x) + λ
b
Z
a
K(x,t) f (t)dt.
(21.51)
Substituting the new ϕ1(x) back into Eq. (21.49), we obtain a second approximation to
ϕ(x):
ϕ2(x) = f (x) + λ
b
Z
a
K(x,t1) f (t1)dt1
+ λ2
b
Z
a
b
Z
a
K(x,t1)K(t1,t2) f (t2)dt2 dt1.
This process can be repeated indeﬁnitely, deﬁning after n steps the nth order approximation
ϕn(x) =
n
X
i=0
λiui(x),
(21.52)
where
u0(x) = f (x)
u1(x) =
b
Z
a
K(x,t1) f (t1)dt1
(21.53)
u2(x) =
b
Z
a
b
Z
a
K(x,t1)K(t1,t2) f (t2)dt2 dt1
un(x) =
b
Z
a
b
Z
a
···
b
Z
a
K(x,t1)K(t1,t2)··· K(tn−1,tn) f (tn)dtn ···dt1.
We expect that our solution ϕ(x) will be
ϕ(x) = lim
n→∞ϕn(x) = lim
n→∞
n
X
i=0
λiui(x),
(21.54)
provided that our inﬁnite series converges.
We may conveniently check the convergence by the Cauchy ratio test, Section 1.1, not-
ing that
|λnun(x)| ≤|λn| · | f |max · |K|n
max · |b −a|n,

1066
Chapter 21 Integral Equations
using | f |max to represent the maximum value of | f (x)| in the interval [a,b] and |K|max
to represent the maximum value of |K(x,t)| in its domain in the xt-plane. A sufﬁcient
condition for convergence is
|λ| · |K|max · |b −a| < 1.
(21.55)
Note that λ|un(max)| is being used as a comparison series. If it converges, our actual series
must converge. If this condition is not satisﬁed, we may or may not have convergence, and
a more sensitive test would be required to determine the convergence. Of course, even if the
Neumann series diverges, there still may be a solution to our integral equation obtainable
by another method.
To gain more understanding of our iterative manipulation, we may ﬁnd it helpful to
rewrite the Neumann series solution, Eq. (21.54), in operator form. We start by rewriting
Eq. (21.49) as
ϕ = λKϕ + f,
where K represents the integral operator
R b
a K(x,t)[ ]dt. Solving symbolically for ϕ, we
obtain
ϕ = (1 −λK)−1 f.
Binomial expansion leads to Eq. (21.54). The convergence of the Neumann series is a
demonstration that the inverse operator (1 −λK)−1 exists.
Example 21.3.1
NEUMANN SERIES SOLUTION
To illustrate the Neumann method, we consider the integral equation
ϕ(x) = x + 1
2
1
Z
−1
(t −x)ϕ(t)dt.
(21.56)
To start the Neumann series, we take
ϕ0(x) = x.
Then
ϕ1(x) = x + 1
2
1
Z
−1
(t −x)tdt = x + 1
2
1
3t3 −1
2t2x

1
−1
= x + 1
3.
Substituting ϕ1(x) back into Eq. (21.56),we get
ϕ2(x) = x + 1
2
1
Z
−1
(t −x)tdt + 1
2
1
Z
−1
(t −x)1
3dt = x + 1
3 −x
3 .

21.3 Neumann Series
1067
Continuing this process of substituting back into Eq. (21.56), we obtain
ϕ3(x) = x + 1
3 −x
3 −1
32 ,
and by mathematical induction (Section 1.4),
ϕ2n(x) = x +
n
X
s=1
(−1)s−13−s −x
n
X
s=1
(−1)s−13−s.
(21.57)
Letting n →∞, we get
ϕ(x) = 3
4x + 1
4.
(21.58)
This solution can (and should) be checked by substituting back into the original equation,
Eq. (21.56).
It is interesting to note that our series converged easily even though Eq. (21.55) is not
satisﬁed in this particular case. Actually Eq. (21.55) is a rather crude upper bound on
λ. It can be shown that a necessary and sufﬁcient condition for the convergence of our
series solution is that |λ| < |λe|, where λe is the eigenvalue of smallest magnitude of the
corresponding homogeneous equation (that with f (x) = 0). For this particular example,
λe =
√
3/2. Clearly, λ = 1
2 < λe.
■
The technique illustrated by the Neumann series occurs in a number of contexts in quan-
tum mechanics. For example, one approach to the calculation of time-dependent perturba-
tions in quantum mechanics starts with the integral equation for the evolution operator
U(t,t0) = 1 −i
¯h
tZ
t0
dt1V (t1)U(t1,t0).
(21.59)
Iteration leads to
U(t,t0) = 1 −i
¯h
tZ
t0
dt1V (t1) +
 i
¯h
2
tZ
t0
dt1
t1
Z
t0
dt2V (t1)V (t2) + ··· .
(21.60)
The evolution operator is obtained as a series of multiple integrals of the perturbing poten-
tial V (t), closely analogous to the Neumann series, Eq. (21.52).
A second and similar relationship between the Neumann series and quantum mechanics
appears when the Schrödinger wave equation for scattering is reformulated as an integral
equation. See Example 10.2.2. The ﬁrst term in a Neumann series solution is the incident
(unperturbed) wave. The second term is the ﬁrst-order Born approximation, Eq. (10.51).
The Neumann method may also be applied to Volterra integral equations of the second
kind, corresponding to replacing the ﬁxed upper limit b in Eq. (21.49) by a variable, x. In
the Volterra case the Neumann series converges for all λ as long as the kernel is square
integrable.

1068
Chapter 21 Integral Equations
Exercises
21.3.1
Using the Neumann series, solve
(a)
ϕ(x) = 1 −2
x
Z
0
tϕ(t)dt,
ANS.
(a) ϕ(x) = e−x2.
(b)
ϕ(x) = x +
x
Z
0
(t −x)ϕ(t)dt,
(c)
ϕ(x) = x −
x
Z
0
(t −x)ϕ(t)dt.
21.3.2
Solve
ψ(x) = x +
1
Z
0
(1 + xt)ψ(t)dt
by each of the following methods:
(a)
The Neumann series technique,
(b)
The separable-kernel technique,
(c)
Educated guessing.
21.3.3
Solve
ϕ(x) = 1 + λ2
x
Z
0
(x −t)ϕ(t)dt
by each of the following methods:
(a)
Reduction to an ODE (ﬁnd the boundary conditions),
(b)
The Neumann series,
(c)
The use of Laplace transforms.
ANS.
ϕ(x) = coshλx.
21.3.4
(a)
In Eq. (21.59), take V = V0, independent of t. Without using Eq. (21.60), show
that Eq. (21.59) leads directly to
U(t −t0) = exp

−i
¯h (t −t0)V0

.
(b)
Repeat for Eq. (21.60) without using Eq. (21.59).

21.4 Hilbert-Schmidt Theory
1069
21.4
HILBERT-SCHMIDT THEORY
Symmetrization of Kernels
The Hilbert-Schmidt theory deals with linear integral equations of the Fredholm type with
symmetric kernels:
K(x,t) = K(t, x).
(21.61)
The symmetry is of great importance, both because we will ﬁnd it leads to results parallel
to those found for the Sturm-Liouville theory of differential equations, and also because
many problems of physical relevance can be written as Fredholm integral equations with
symmetric kernels.
Before plunging into the theory, we note that some important nonsymmetric kernels can
be symmetrized. If we have the equation
ϕ(x) = f (x) + λ
b
Z
a
K(x,t)ρ(t)ϕ(t)dt,
(21.62)
the total kernel is actually K(x,t)ρ(t), clearly not symmetric if K(x,t) alone is symmetric.
However, if we multiply Eq. (21.62) by √ρ(x) and substitute
p
ρ(x)ϕ(x) = ψ(x),
we obtain
ψ(x) =
p
ρ(x) f (x) + λ
b
Z
a
h
K(x,t)
p
ρ(x)ρ(t)
i
ψ(t)dt,
(21.63)
with a symmetric total kernel K(x,t)√ρ(x)ρ(t).
Orthogonal Eigenfunctions
We now focus on the homogeneous Fredholm equation of the second kind:
ϕ(x) = λ
b
Z
a
K(x,t)ϕ(t)dt.
(21.64)

1070
Chapter 21 Integral Equations
We assume that the kernel K(x,t) is symmetric and real. Perhaps one of the ﬁrst questions
we might ask about the equation is: “Does it make sense?” or more precisely, “Does an
eigenvalue λ satisfying this equation exist?” This question can be answered in the afﬁr-
mative. Courant and Hilbert (in their work cited in the Additional Readings, chapter III,
section 4) show that if K(x,t) is continuous, there is at least one such eigenvalue and
possibly an inﬁnite number of them.
It is useful to recognize that Eq. (21.64) represents a linear-operator eigenvalue problem:
The integral on its right-hand side converts ϕ into (in general) some other function, which
we can indicate symbolically by the equation
ψ(x) =
b
Z
a
K(x,t)ϕ(t)dt ≡Kϕ(x),
(21.65)
so our eigenvalue problem is
Kϕ(x) = 1
λϕ(x).
(21.66)
We do not have to worry about the possibility that λ = 0, since we can read directly from
Eq. (21.64) that in that case the solution to our integral equation will be uniquely ϕ(x) = 0.
The integral operator K is linear, since it is obviously true that
K

aϕ1(x) + bϕ2(x)

= aKϕ1(x) + bKϕ2(x).
In addition, if we deﬁne the scalar product as an integral on the range (a,b):
⟨ψ|ϕ⟩≡
b
Z
a
ψ∗(x)ϕ(x)dx,
(21.67)
we then see that our requirement that the kernel K(x,t) be real and symmetric will make
K a self-adjoint operator:
⟨ψ|Kϕ⟩=
b
Z
a
ψ∗(x)

b
Z
a
K(x,t)ϕ(t)dt

dx =
b
Z
a
dt

b
Z
a
dxK(t, x)ψ(x)
 ∗
ϕ(t)
= ⟨Kψ|ϕ⟩.
(21.68)
The linearity and self-adjointness indicate that we can expect to conﬁrm that K has the key
properties of self-adjoint operators, namely that its eigenvalues are real and (except in the
case of degeneracy) its eigenvectors are orthogonal.
While the above constitutes a complete demonstration of the orthogonality of our
solutions to the homogeneous Fredholm equation, let’s conﬁrm these properties more
explicitly.

21.4 Hilbert-Schmidt Theory
1071
We can start from the two equations,
1
λi
ϕi(x) =
b
Z
a
K(x,t)ϕi(t)dt,
(21.69)
1
λ j
ϕ j(x) =
b
Z
a
K(x,t)ϕ j(t)dt.
(21.70)
If we multiply Eq. (21.69) by ϕ∗
j (x) and Eq. (21.70) by ϕ∗
i (x) and then integrate with
respect to x, the two equations become4
1
λi
b
Z
a
ϕ∗
j (x)ϕi(x)dx =
b
Z
a
b
Z
a
K(x,t)ϕ∗
j (t)ϕi(x)dtdx,
(21.71)
1
λ j
b
Z
a
ϕ∗
i (x)ϕ j(x)dx =
b
Z
a
b
Z
a
K(x,t)ϕ∗
i (t)ϕ j(x)dtdx.
(21.72)
Since we have demanded that K(x,t) be real and symmetric, we may take the com-
plex conjugate of Eq. (21.72) and then interchange the roles of x and t in the integral,
reaching
1
λ∗
j
b
Z
a
ϕi(x)ϕ∗
j (x)dx =
b
Z
a
b
Z
a
K(x,t)ϕi(x)ϕ∗
j (t)dtdx.
(21.73)
Subtracting Eq. (21.73) from Eq. (21.71), we obtain
 
1
λi
−1
λ∗
j
!
b
Z
a
ϕ∗
j (x)ϕi(x)dx = 0.
(21.74)
Just as in our earlier derivation from Sturm-Liouville theory, we conclude that if i = j the
integral in Eq. (21.74) is necessarily nonzero; so 1/λi = 1/λ∗
i , meaning that λi must be
real. But if λi ̸= λ j,
b
Z
a
ϕ∗
i (x)ϕ j(x)dx = 0,
λi ̸= λ j,
(21.75)
proving orthogonality. The derivation can also be completed if K(x,t) is Hermitian, mean-
ing that K(t, x) = K ∗(x,t). See Exercise 21.4.1. Since we are mostly concerned with real
4We assume that the necessary integrals exist. For an example of a simple pathological case, see Exercise 21.4.4.

1072
Chapter 21 Integral Equations
K, it is appropriate to assume also that ϕ is real, and for the remainder of this chapter we
will often omit the complex conjugate asterisks that occur, for example, in Eq. (21.75).
If the eigenvalue λi is degenerate,5 the eigenfunctions for that particular eigenvalue
may be orthogonalized by the Gram-Schmidt method (Section 5.2). Our orthogonal eigen-
functions may, of course, be normalized, and we assume that this has been done. The
result is
b
Z
a
ϕ∗
i (x)ϕ j(x)dx = δi j.
(21.76)
It can be shown that the eigenfunctions of our integral equations form a complete set,6
in the sense that if a function g(x) can be generated by the integral
g(x) =
Z
K(x,t)h(t)dt,
with h(t) a piecewise continuous function, then g(x) can be represented by a series of
eigenfunctions,
g(x) =
∞
X
n=1
anϕn(x).
(21.77)
The series in Eq. (21.77) can be shown to converge uniformly and absolutely.
Let us extend this to the kernel K(x,t) by asserting that
K(x,t) =
∞
X
n=1
anϕn(t),
(21.78)
and an = an(x). Substituting into the original integral equation, Eq. (21.64), and using the
orthogonality integral, we obtain
ϕi(x) = λiai(x).
(21.79)
Therefore, for our homogeneous Fredholm equation of the second kind, the kernel may be
expressed in terms of the eigenfunctions and eigenvalues as
K(x,t) =
∞
X
n=1
ϕn(x)ϕn(t)
λn
.
(21.80)
Equation (21.80) is not actually a new result. In the Green’s function chapter, Section
10.1, we identiﬁed K(x,t), there called G(x,t), as the Green’s function appearing in
Eq. (10.30), with the expansion given in Eq. (10.14). However, it is possible that the
5As for differential operators, if more than one distinct eigenfunction of Eq. (21.64) corresponds to the same eigenvalue, that
eigenvalue is said to be degenerate.
6For a proof of this statement, see Courant and Hilbert (1953), chapter III, section 5, in the Additional Readings.

21.4 Hilbert-Schmidt Theory
1073
expansion given by Eq. (21.80) may not exist. As an illustration of the sort of pathological
behavior that may occur, you are invited to apply this analysis to
ϕ(x) = λ
∞
Z
0
e−xtϕ(t)dt.
Compare Exercise 21.4.4.
It should be emphasized that this Hilbert-Schmidt theory is concerned with the establish-
ment of properties of the eigenvalues (real) and eigenfunctions (orthogonality, complete-
ness), properties that may be of great interest and value. The Hilbert-Schmidt theory does
not solve the homogeneous integral equation for us any more than the Sturm-Liouville
theory for differential equations solved the ODEs. The solutions of the integral equation
come by the application of techniques such as were introduced in Sections 21.2 and 21.3,
or perhaps even by numerical methods.
Inhomogeneous Integral Equation
We now continue with the Hilbert-Schmidt theory by seeking solutions of the inhomoge-
neous equation
ϕ(x) = f (x) + λ
b
Z
a
K(x,t)ϕ(t)dt.
(21.81)
We assume that the solutions of the corresponding homogeneous integral equation are
already known:
ϕn(x) = λn
b
Z
a
K(x,t)ϕn(t)dt,
(21.82)
the solution ϕn(x) corresponding to the eigenvalue λn. Note that at this point we are assum-
ing nothing about λ; it is a constant that has no speciﬁc relationship to the eigenvalues λn
of the homogeneous integral equation.
We expand both ϕ(x) and f (x) in terms of this set of eigenfunctions:
ϕ(x) =
∞
X
n=1
anϕn(x)
(an unknown),
(21.83)
f (x) =
∞
X
n=1
bnϕn(x)
(bn known).
(21.84)
Substituting into Eq. (21.81), we obtain
∞
X
n=1
anϕn(x) =
∞
X
n=1
bnϕn(x) + λ
b
Z
a
K(x,t)
∞
X
n=1
anϕn(t)dt.
(21.85)

1074
Chapter 21 Integral Equations
By interchanging the order of integration and summation, we may evaluate the integral by
Eq. (21.82), and we get
∞
X
n=1
anϕn(x) =
∞
X
n=1
bnϕn(x) + λ
∞
X
n=1
anϕn(x)
λn
.
(21.86)
If we multiply by ϕi(x) and integrate from x = a to x = b, the orthogonality of our eigen-
functions leads to
ai = bi + λai
λi
.
(21.87)
This can be rewritten as
ai = bi +
λ
λi −λbi.
(21.88)
We now multiply Eq. (21.88) by ϕi(x) and sum over i, giving
ϕ(x) = f (x) + λ
∞
X
i=1
ϕi(x)
λi −λbi
= f (x) + λ
∞
X
i=1
ϕi(x)
λi −λ
b
Z
a
f (t)ϕi(t)dt.
(21.89)
Here it is assumed that the eigenfunctions ϕi(x) are normalized to unity. Note that if
f (x) = 0, there is no solution unless λ is equal to one of the λi, thereby conﬁrming that
the homogeneous integral equation has only the solutions ϕi(x).
In the event that λ for the inhomogeneous equation, Eq. (21.81), is equal to one of
the eigenvalues λp of the homogeneous equation, our solution, Eq. (21.89), blows up. It
can be shown that the inhomogeneous equation then has no solution unless the coefﬁcient
bp vanishes, meaning that there is no solution unless the inhomogeneous term f (x) is
orthogonal to the eigenfunction ϕp. If the eigenvalue λp is degenerate, there will be no
solution unless f (x) is orthogonal to all the degenerate eigenfunctions.
For the case that bp = 0, we can return to Eq. (21.87), which then reduces for ap to
ap = bp + ap = ap,
(21.90)
which gives no information about ap. Note that if bp ̸= 0 this equation cannot be satisﬁed,
a signal that a solution cannot be obtained.
Under the assumption that bp = 0 we now can rewrite Eq. (21.86), identifying its ﬁrst
two summations, respectively, as ϕ(x) and f (x), separating the ﬁnal summation into the
single term apϕp(x) plus a sum over all n other than p, thereby reaching
ϕ(x) = f (x) + apϕp + λp
∞
X
i=1
′ ϕi(x)
λi −λp
b
Z
a
f (t)ϕi(t)dt.
(21.91)

21.4 Hilbert-Schmidt Theory
1075
In this solution the ap remains as an undetermined constant,7 and the prime indicates that
i = p is to be omitted from the sum.
It is of interest to relate Eq. (21.89) to what might be expected if we tried to develop a
similar equation by Green’s-function methods. To do this, we start by rewriting Eq. (21.81)
as an operator equation of the form
Kϕ(x) −1
λϕ(x) = −f (x)
λ
,
(21.92)
where K is the operator that we introduced in Eq. (21.65). Next, we note that, from
Eq. (21.82), the ϕn are eigenfunctions of K with eigenvalues 1/λn:
Kϕn(x) = ϕn(x)
λn
.
(21.93)
Then, applying Eq. (10.39), the Green’s function of the entire left-hand side of Eq. (21.92)
will be (assuming ϕ is real):
G(x,t) =
X
n
ϕn(x)ϕn(t)
λ−1
n
−λ−1 = λ
X
n
λn
λ −λn
ϕn(x)ϕn(t)
= −λ
X
n
ϕn(x)ϕn(t) + λ2 X
n
ϕn(x)ϕn(t)
λ −λn
= −λδ(t −x) −λ2 X
n
ϕn(x)ϕn(t)
λn −λ
.
(21.94)
To reach the last line of Eq. (21.94) we used the eigenfunction expansion of the delta
function, Eq. (5.27). Applying this Green’s function to the right-hand side of Eq. (21.92),
we get
ϕ(x) = −1
λ
b
Z
a
G(x,t) f (t)dt
= −1
λ
b
Z
a
"
−λδ(t −x) −λ2 X
n
ϕn(x)ϕn(t)
λn −λ
#
f (t)dt
= f (x) + λ
X
n
ϕn(x)
λn −λ
b
Z
a
ϕn(t) f (t)dt,
(21.95)
which agrees with Eq. (21.89).
7This is like the inhomogeneous linear ODE. We may add to its solution any constant times a solution of the corresponding
homogeneous ODE.

1076
Chapter 21 Integral Equations
Example 21.4.1
INHOMOGENEOUS FREDHOLM EQUATION
Let’s seek solutions to the inhomogeneous Fredholm equation
ϕ(x) = x3 + λ
1
Z
−1
(t + x)ϕ(t)dt,
(21.96)
for the two λ values λ = 1 and λ =
√
3/2. The corresponding homogeneous equation,
treated in Example 21.2.3, has solutions only for the two eigenvalues ±
√
3/2. In norma-
lized form, they are:
λ1 =
√
3
2 , ϕ1 =
√
3
2

x + 1
√
3

;
λ2 = −
√
3
2 , ϕ2 =
√
3
2

x −1
√
3

.
Taking ﬁrst λ = 1, which is not an eigenvalue of the homogeneous equation, we have
ϕ(x) = x3 +
2
X
i=1
ϕi(x)
λi −1
1
Z
−1
t3ϕi(t)dt
= x3 +
√
3
2

x + 1
√
3

√
3
2 −1
√
3
2
1
Z
−1
t3

t + 1
√
3

dt
+
√
3
2

x −1
√
3

−
√
3
2 −1
√
3
2
1
Z
−1
t3

t −1
√
3

dt
= x3 −6
5(2x + 1).
(21.97)
Continuing now to λ =
√
3/2, we note that it is the eigenvalue λ1 of the homogeneous
integral equation. That means the integral equation will have no solution unless ⟨ϕ1| f ⟩= 0.
For the present problem,
⟨ϕ1| f ⟩=
√
3
2
1
Z
−1

x + 1
√
3

x3dx ̸= 0,
so our integral equation will have no solution for λ =
√
3/2. If in spite of this observation
we attempted to generate a solution using Eq. (21.91), the function ϕ(x) we obtained would
not satisfy the integral equation, irrespective of the value we might choose to assign to ap.
The immediate reason we cannot obtain a solution is that the integral
1
Z
−1
(t + x) f (t)dt =
1
Z
−1
(t + x)t3 dt = 2
5

21.4 Hilbert-Schmidt Theory
1077
evaluates to a quantity that cannot be represented as a linear combination of the eigenfunc-
tions ϕi other than ϕp (in the present case, this means that 2/5 is not proportional to ϕ2).
There is therefore no way to add an additional component to f (x) to obtain a cancellation
of the 2/5.
■
Exercises
21.4.1
In the Fredholm equation
ϕ(x) = λ
b
Z
a
K(x,t)ϕ(t)dt,
assume that the kernel K(x,t) is self-adjoint or Hermitian:
K(x,t) = K ∗(t, x).
Extend the analysis of the present section to show that
(a)
the eigenfunctions are orthogonal, in the sense that
b
Z
a
ϕ∗
m(x)ϕn(x)dx = 0,
m ̸= n(λm ̸= λn).
(b)
the eigenvalues are real.
21.4.2
(a)
Show that the eigenfunctions of Exercise 21.2.12 are orthogonal.
(b)
Show that the eigenfunctions of Exercise 21.2.14 are orthogonal.
21.4.3
Use the Hilbert-Schmidt method to solve the inhomogeneous integral equation
ϕ(x) = x + 1
2
1
Z
−1
(t + x)ϕ(t)dt.
The corresponding homogeneous integral equation was treated in Example 21.2.3.
Note. The application of the Hilbert-Schmidt technique here is somewhat like using
a shotgun to kill a mosquito, especially when the equation can be solved quickly by
expanding in Legendre polynomials.
21.4.4
The Fredholm integral equation
ϕ(x) = λ
∞
Z
0
e−xtϕ(t)dt
has an inﬁnite number of solutions, of which one is
ϕ(x) = x−1/2,
λ = π−1/2.
Verify that this is a solution and that it is not normalizable.

1078
Chapter 21 Integral Equations
Note. A basic reason for this anomalous behavior is that the range of integration is
inﬁnite, making this a “singular” integral equation. Note also that a series expansion of
the kernel e−xt would permit a solution by the separable-kernel method (Section 21.2),
except that the series is inﬁnite. This observation is consistent with the fact that this
integral equation has an inﬁnite number of eigenvalues and eigenfunctions.
21.4.5
Given
y(x) = x + λ
1
Z
0
xt y(t)dt:
(a)
Determine y(x) as a Neumann series.
(b)
Find the range of λ for which your Neumann series solution is convergent. Com-
pare with the value obtained from
|λ| · |K|max < 1.
(c)
Find the eigenvalue and the eigenfunction of the corresponding homogeneous inte-
gral equation.
(d)
By the separable-kernel method show that the solution is
y(x) =
3x
3 −λ.
(e)
Find y(x) by the Hilbert-Schmidt method.
21.4.6
In Exercise 21.2.11 it was found that the integral equation
ϕ(x) = λ
2π
Z
0
cos(x −t)ϕ(t)dt
had (unnormalized) eigenfunctions cos x and sin x, both with eigenvalue λi = 1/π.
Show that the kernel of this integral equation has an expansion of the form
K(x,t) =
2
X
n=1
ϕn(x)ϕn(t)
λn
.
21.4.7
The integral equation ϕ(x) = λ
1
Z
0
(1 + xt)ϕ(t)dt
has eigenvalues λ1 = 0.7889 and λ2 = 15.211. The corresponding eigenfunctions are
ϕ1 = 1 + 0.5352x and ϕ2 = 1 −1.8685x.
(a)
Show that these eigenfunctions are orthogonal over the interval [0,1].
(b)
Normalize the eigenfunctions to unity.

21.4 Hilbert-Schmidt Theory
1079
(c)
Show that
K(x,t) = ϕ1(x)ϕ1(t)
λ1
+ ϕ2(x)ϕ2(t)
λ2
.
ANS.
(b)
ϕ1(x) = 0.7831 + 0.4191x,
ϕ2(x) = 1.8403 −3.4386x.
21.4.8
An alternate form of the solution to the inhomogeneous integral equation, Eq. (21.81), is
ϕ(x) =
∞
X
i=1
biλi
λi −λϕi(x).
(a)
Derive this form without using Eq. (21.89).
(b)
Show that this form and Eq. (21.89) are equivalent.
Additional Readings
Bocher, M., An Introduction to the Study of Integral Equations, Cambridge Tracts in Mathematics and Mathe-
matical Physics, No. 10. New York: Hafner (1960). This is a helpful introduction to integral equations.
Byron, F. W., Jr., and R. W. Fuller, Mathematics of Classical and Quantum Physics. Reading, MA: Addison-
Wesley (1969), reprinted, Dover (1992). The treatment of integral equations is rather advanced.
Cochran, J. A., The Analysis of Linear Integral Equations. New York: McGraw-Hill (1972). This is a comprehen-
sive treatment of linear integral equations intended for applied mathematicians and mathematical physicists.
It assumes a moderate to high level of mathematical competence on the part of the reader.
Courant, R., and D. Hilbert, Methods of Mathematical Physics, Vol. 1 (English edition). New York: Interscience
(1953). This is one of the classic works of mathematical physics. Originally published in German in 1924,
the revised English edition is an excellent reference for a rigorous treatment of integral equations, Green’s
functions, and a wide variety of other topics on mathematical physics.
Golberg, M. A., ed., Solution Methods of Integral Equations. New York: Plenum Press (1979). This is a set of
papers from a conference on integral equations. The initial chapter is excellent for up-to-date orientation and
a wealth of references.
Kanval, R. P., Linear Integral Equations. New York: Academic Press (1971), reprinted, Birkhäuser (1996). This
book is a detailed but readable treatment of a variety of techniques for solving linear integral equations.
Morse, P. M., and H. Feshbach, Methods of Theoretical Physics. New York: McGraw-Hill (1953). Detailed,
rigorous, and difﬁcult.
Muskhelishvili, N. I., Singular Integral Equations, 2nd ed. New York: Dover (1992).
Stakgold, I., Green’s Functions and Boundary Value Problems. New York: Wiley (1979).

CHAPTER 22
CALCULUS OF VARIATIONS
The calculus of variations deals with problems where we search for a function or curve,
rather than a value of some variable, that makes a given quantity stationary, usually an
energy or action integral. Because a function is varied, these problems are called varia-
tional. Variational principles, such as those of D’Alembert, Lagrange, and Hamilton, have
been developed in classical mechanics; Fermat’s principle (that of the shortest optical path)
ﬁnds use in electrodynamics. Lagrangian variational techniques also occur in quantum me-
chanics and ﬁeld theory. Before plunging into this rather different branch of mathematical
physics, let us summarize some of its uses in both physics and mathematics.
1.
In existing physical theories:
a.
Uniﬁcation of diverse areas of physics using energy as a key concept
b.
Convenience in analysis: Lagrange equations, Section 22.2
c.
Elegant treatment of constraints, Section 22.4
2.
Starting point for new, complex areas of physics and engineering. In general rela-
tivity, the geodesic is taken as the minimum path of a light pulse or the free-fall path
of a particle in curved Riemannian space. Variational principles appear in quantum
ﬁeld theory. Variational principles have been applied extensively in control theory.
3.
Mathematical uniﬁcation. Variational analysis provides a proof of the complete-
ness of the Sturm-Liouville eigenfunctions, and can be used to establish bounds for
the eigenvalues. Similar results follow for the eigenvalues and eigenfunctions in the
Hilbert-Schmidt theory of integral equations.
22.1
EULER EQUATION
The calculus of variations typically involves problems in which a quantity to be minimized
(or maximized) appears as a functional, meaning that it is a quantity whose argument(s)
are themselves function(s), not just variable(s). As a simple, yet fairly general case, let J
1081
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

1082
Chapter 22 Calculus of Variations
be a functional of y, deﬁned as
J[y] =
x2
Z
x1
f

y(x), dy(x)
dx
, x

dx.
(22.1)
Here f is a ﬁxed function of the three variables y, dy/dx, and x, while J will have a value
dependent on the choice of y. The square-bracket notation is frequently used to remind the
reader that J is a functional. Because J is given as an integral, its value depends on the
behavior of y(x) throughout the entire range of x (here x1 ≤x ≤x2). A typical problem
in the calculus of variations is to ﬁnd (usually subject to some constraints) a continuous
and differentiable function y(x) that makes J stationary relative to small changes in y
anywhere (or everywhere) in its range of deﬁnition. These stationary values of J will in
many problems be minima or maxima, but they can also be saddle points. The conditions
of physical problems will normally require that variations in y be restricted to those that
preserve its continuity and differentiability
It is convenient to introduce a notation that makes our discussions less cumbersome; we
usually rewrite Eq. (22.1) in a notation with dy/dx denoted yx and with the arguments
x and [y] suppressed, and we indicate the variation in J produced by a (small) variation
in y as
δJ = δ
x2
Z
x1
f (y, yx, x)dx.
(22.2)
Note that we wrote δ rather than d or ∂; this distinction reminds us that the variation is that
of a function (here y) rather than that of a variable.
In visualizing the situation described by Eq. (22.2), it is helpful to think of y(x) as a path
or curve connecting the values y(x1) and y(x2); in fact, a common problem in the calculus
of variations will be to determine y(x) subject to the constraint that y(x1) and y(x2) have
speciﬁed values (and often subject to further constraints that may also be integrals). To
illustrate the class of problems represented by Eq. (22.2), here are two simple examples:
•
Determination of the minimum-energy conﬁguration of a rope or chain of given length
attached to ﬁxed points at both ends, in the presence of a uniform gravitational ﬁeld.
•
Determination of the track between two points at different heights that will minimize
the travel time of an object that, starting from rest, slides without friction along the track
subject only to a uniform gravitational ﬁeld (this is known as the brachistochrone
problem).
The problems here under consideration are much more difﬁcult than typical minimiza-
tions in differential calculus, where the minimum in a function can be found by comparing
its values, say y(x), at neighboring points (by looking at dy/dx). What we can do, instead,
is to start by assuming the existence of an optimum path, i.e., a function y(x) for which J
is stationary, and then compare J for our (unknown) optimum path with that obtained from
neighboring paths, of which there are an inﬁnite number. See Fig. 22.1. Even this strategy
may sometimes fail, as there exist functionals J for which there is no optimum path.

22.1 Euler Equation
1083
y
x2, y2
x1, y1
δy
x
FIGURE 22.1
Neighboring paths.
Restricting attention to functions y(x) for which the endpoints y(x1) and y(x2) are ﬁxed,
we consider a deformation of y(x), called the variation of y and denoted δy. We describe
δy by introducing a new function, η(x), and a scale factor α that controls the magnitude of
the variation. The function η(x) is arbitrary except for being continuous and differentiable,
and, to keep the endpoints ﬁxed, with
η(x1) = η(x2) = 0.
(22.3)
With these deﬁnitions, our path, now a function of α, is
y(x,α) = y(x,0) + αη(x),
(22.4)
and we choose y(x,0) as the (unknown) path that will minimize J. Relative to y(x,0), the
variation δy is then
δy = αη(x).
(22.5)
Using Eq. (22.4), our formula for J can now be written
J(α) =
x2
Z
x1
f

y(x,α), yx(x,α), x

dx,
(22.6)
and we see that we have reached a simpler formulation in which J is now a function of α
rather than a functional of y. This means that we now know how to optimize it.1
We proceed now to obtain a stationary value of J by imposing the condition
∂J(α)
∂α

α=0
= 0,
(22.7)
analogous to the vanishing of the derivative dy/dx in differential calculus.
1The arbitrary nature of the dependence of J(α) on η(x) will come into play later.

1084
Chapter 22 Calculus of Variations
Now, the α dependence of the integral is contained in y(x,α) and yx(x,α) =
(∂/∂x)y(x,α). Therefore,2
∂J(α)
∂α
=
x2
Z
x1
∂f
∂y
∂y
∂α + ∂f
∂yx
∂yx
∂α

dx = 0.
(22.8)
From Eq. (22.4),
∂y(x,α)
∂α
= η(x)
and
∂yx(x,α)
∂α
= dη(x)
dx
,
(22.9)
so Eq. (22.8) becomes
∂J(α)
∂α
=
x2
Z
x1
∂f
∂y η(x) + ∂f
∂yx
dη(x)
dx

dx = 0.
(22.10)
Integrating the second term by parts to get η(x) as a common factor, we convert it to
x2
Z
x1
dη(x)
dx
∂f
∂yx
dx = η(x) ∂f
∂yx

x2
x1
−
x2
Z
x1
η(x) d
dx
∂f
∂yx
dx.
(22.11)
The integrated part vanishes by Eq. (22.3), and Eq. (22.10) becomes
∂J(α)
∂α
=
x2
Z
x1
∂f
∂y −d
dx
∂f
∂yx

η(x)dx = 0.
(22.12)
Equation (22.12), which must be satisﬁed for arbitrary η(x), is to be understood as a con-
dition on y(x). Occasionally we will see Eq. (22.12) multiplied by δα, which gives, upon
using η(x)δα = δy,
δJ =
x2
Z
x1
∂f
∂y −d
dx
∂f
∂yx

δy dx = 0.
(22.13)
Equation (22.13) is to be solved for arbitrary δy with δy(x1) = δy(x2) = 0.
We now take up the solution of Eq. (22.12). That equation can be satisﬁed for arbitrary
η(x) only if the bracketed expression forming the remainder of its integrand vanishes “al-
most everywhere,” meaning everywhere except possibly at isolated points.3 The condition
for our stationary value is thus formally a partial differential equation (PDE),
∂f
∂y −d
dx
∂f
∂yx
= 0,
(22.14)
known as the Euler equation. Since the form of f is known, it will actually reduce (be-
cause there is really only one independent variable, x) to an ordinary differential equation
(ODE) for y with boundary conditions at x1 and x2. In that connection, it is important
2Note that y and yx are being treated as independent variables because they occur as different arguments of f.
3Compare the discussion of convergence in the mean, at Eq. (5.22).

22.1 Euler Equation
1085
to note that the derivative d/dx occurs in the Euler equation, and that it has a meaning
distinct from the partial derivative ∂/∂x. In particular, if f = f (y(x), yx, x), then d f/dx,
which stands for the change in f (from all sources) due to a change in x, has the evaluation
d f
dx = ∂f
∂x + ∂f
∂y
dy
dx + ∂f
∂yx
d2y
dx2 ,
where the last term has the form given because dyx/dx = d2y/dx2. Note that the ﬁrst
term on the right gives the explicit x-dependence of f ; the second and third terms give its
implicit x-dependence via y and yx.
The Euler equation, Eq. (22.14), is a necessary, but by no means sufﬁcient condition
that there be a function y(x) that is continuous and differentiable on the range (x1, x2) and
yields a stationary value of J.4 A nice example of a lack of sufﬁciency is provided by the
problem of determining stationary paths between points on the surface of a sphere (this
example was provided by Courant and Robbins; see Additional Readings). The minimum-
distance path from point A to point B on a spherical surface is the arc of a great circle,
shown as Path 1 in Fig. 22.2. But Path 2 also satisﬁes the Euler equation. Path 2 is a
maximum, but only if we demand that it be a great circle and then only if we make less
than one circuit (as Path 2 plus n complete revolutions is also a solution). If the path is
not required to be a great circle, any deviation from Path 2 will increase the length. This
is hardly the property of a local maximum, and that illustrates why it is important to check
solutions of the Euler equation to see if they satisfy the physical conditions of the given
problem.
Sometimes a problem admits a discontinuous solution that has physical relevance and
will not be found by straightforward application of the Euler equation. An example is
provided by the soap ﬁlm of Example 22.1.3, where such a solution describes what happens
if the ﬁlm becomes unstable and breaks.
Following are examples of the use of the Euler equation.
2
A
1
B
FIGURE 22.2
Stationary paths over a sphere.
4For a discussion of sufﬁciency conditions and the development of the calculus of variations as a part of mathematics, see the
works by Ewing and Sagan in Additional Readings.

1086
Chapter 22 Calculus of Variations
Example 22.1.1
STRAIGHT LINE
Perhaps the simplest application of the Euler equation is in the determination of the shortest
distance between two points in the Euclidean xy-plane. Since the element of distance is
ds = [(dx)2 + (dy)2]1/2 = [1 + y2
x]1/2 dx,
the distance J may be written as
J =
x2,y2
Z
x1,y1
ds =
x2
Z
x1
[1 + y2
x]1/2 dx.
(22.15)
Comparison with Eq. (22.2) shows that
f (y, yx, x) = (1 + y2
x)1/2.
Substituting into Eq. (22.14) and noting that ∂f/∂y vanishes, we obtain
−d
dx

1
(1 + y2x)1/2

= 0,
or
1
(1 + y2x)1/2 = C,
a constant.
This equation is satisﬁed if
yx = a,
a second constant.
Integrating this expression for yx, we get
y = ax + b,
(22.16)
which is the familiar equation for a straight line. The constants a and b are now chosen so
that the line passes through the two points (x1, y1) and (x2, y2). Hence the Euler equation
predicts that the shortest5 distance between two ﬁxed points in Euclidean space is a straight
line.
■
The generalization of this to curved four-dimensional space-time leads to the impor-
tant concept of the geodesic in general relativity. A further discussion of geodesics is in
Section 22.2.
5Technically, we have only found a y(x) of stationary J. By inspection of the solution, we easily determine the distance to be a
minimum.

22.1 Euler Equation
1087
Example 22.1.2
OPTICAL PATH NEAR A BLACK HOLE
We now wish to determine the optical path in an atmosphere where the velocity of light
increases in proportion to the height y according to v(y) = y/b, with b > 0 some parameter
describing the light speed. So v = 0 at y = 0, which simulates the conditions at the surface
of a black hole, called its event horizon, where the gravitational force is so strong that the
velocity of light goes to zero, thus trapping light.
Our variational principle (Fermat’s principle) is that light will take the path of shortest
travel time from (x1, y1) to (x2, y2), namely
1t =
Z
dt =
x2,y2
Z
x1,y1
ds
v =
x2,y2
Z
x1,y1
b
y ds = b
x2,y2
Z
x1,y1
p
dx2 + dy2
y
= minimum.
(22.17)
The path is along a line deﬁned by the relation between y and x. While we have in previous
equations taken x to be the independent variable, there is no inherent requirement to do so,
and our work on the present problem will be simpliﬁed if we choose y as the independent
variable, and we write Eq. (22.17) in the form
1t =
y2
Z
y1
q
x2y + 1
y
dy,
(22.18)
where xy stands for dx/dy. Then our Euler equation will be
∂f
∂x −d
dy
∂f
∂xy
= 0,
with
f (x, xy, y) =
q
x2y + 1
y
.
Noting that ∂f/∂x = 0 and differentiating ∂f/∂xy, we have
−d
dy
xy
y
q
x2y + 1
= 0.
This equation can be integrated, giving
xy
y
q
x2y + 1
= C1 = constant,
or
xy =
C1y
q
1 −C2
1 y2
.
Writing xy = dx/dy and separating dx and dy in this ﬁrst-order ODE, we ﬁnd the integral
x
Z
dx =
y
Z
C1y dy
q
1 −C2
1 y2
,
which yields
x + C2 = −
q
1 −C2
1 y2
C1
,
or
(x + C2)2 + y2 = 1
C2
1
.

1088
Chapter 22 Calculus of Variations
1
2
y
x
FIGURE 22.3
Circular optical path in medium.
Irrespective of the values of C1 and C2, this light path is the arc of a circle whose center is
on the line y = 0, namely the event horizon. The actual path of light passing from (x1, y1)
to (x2, y2) will be on the circle through those points centered on y = 0; the construction
of the path can be performed geometrically as shown in Fig. 22.3. Note that light will not
escape completely from the black hole with this model for v(y) unless x1 = x2 (a path
perpendicular to the event horizon).
This example may be adapted to a mirage (Fata Morgana) in a desert with hot air near
the ground and cooler air aloft (the index of refraction changes with height in cool vs. hot
air). For the mirage problem, the relevant velocity law is v(y) = v0 −y/b. In that case, the
circular light path is no longer convex with center on the x-axis, but becomes concave. ■
Alternate Forms of Euler Equations
Another form of the Euler equation, which is often useful (Exercise 22.1.1), is
∂f
∂x −d
dx

f −yx
∂f
∂yx

= 0.
(22.19)
In problems in which f = f (y, yx), i.e., in which x does not appear explicitly,
Eq. (22.19) reduces to
d
dx

f −yx
∂f
∂yx

= 0,
(22.20)
or
f −yx
∂f
∂yx
= constant.
(22.21)
Example 22.1.3
SOAP FILM
As our next illustrative example, consider two parallel coaxial wire circles to be connected
by a surface of minimum area that is generated by revolving a curve y(x) about the x-axis.

22.1 Euler Equation
1089
(x1, y1)
(x2, y2)
y
ds
y
x
FIGURE 22.4
Surface of rotation, soap-ﬁlm problem.
See Fig. 22.4. The curve is required to pass through ﬁxed endpoints (x1, y1) and (x2, y2).
The variational problem is to choose the curve y(x) so that the area of the resulting surface
will be a minimum. A physical situation corresponding to this problem is that of a soap
ﬁlm suspended between the wire circles.
For the element of area shown in Fig. 22.4,
d A = 2πy ds = 2πy(1 + y2
x)1/2 dx.
The variational equation is then
J =
x2
Z
x1
2πy(1 + y2
x)1/2 dx.
Neglecting the 2π, we identify
f (y, yx, x) = y(1 + y2
x)1/2.
Since ∂f/∂x = 0, we may apply Eq. (22.20) and get
y(1 + y2
x)1/2 −
y y2
x
(1 + y2x)1/2 = c1,
which simpliﬁes to
y
(1 + y2x)1/2 = c1.
(22.22)
Squaring, we get
y2
1 + y2x
= c2
1,

1090
Chapter 22 Calculus of Variations
which rearranges to
(yx)−1 = dx
dy =
c1
q
y2 −c2
1
.
(22.23)
We note in passing that c1 had better have a value that causes dy/dx to be real. Equa-
tion (22.23) may be integrated to give
x = c1 cosh−1 y
c1
+ c2,
and, solving for y, we have
y = c1 cosh
x −c2
c1

.
(22.24)
Finally, c1 and c2 are determined by requiring the solution to pass through the points
(x1, y1) and (x2, y2). Our “minimum”-area surface is a special case of a catenary of revo-
lution, or a catenoid.
■
Soap Film: Minimum Area
This calculus of variations contains many pitfalls for the unwary. Remember, the Euler
equation is a necessary condition, and assumes a differentiable solution. The sufﬁciency
conditions are quite involved. Again, see the Additional Readings for details. Respect for
some of these hazards may be developed by further considering the soap-ﬁlm problem in
Example 22.1.3, with (x1, y1) = (−x0,1), (x2, y2) = (+x0,1). We are therefore consider-
ing a soap ﬁlm stretched between two rings of unit radius at x = ±x0. The problem is to
predict the curve y(x) assumed by the soap ﬁlm.
By referring to Eq. (22.24), we ﬁnd that c2 = 0 because our problem is symmetric about
x = 0. Then
y = c1 cosh
 x
c1

,
(22.25)
and our endpoint conditions become
c1 cosh
x0
c1

= 1.
(22.26)
If we take x0 = 1
2 we obtain the following transcendental equation for c1:
1 = c1 cosh
 1
2c1

.
(22.27)
We ﬁnd that this equation has two solutions: c1 = 0.2350, leading to a “deep” curve, and
c1 = 0.8483, leading to a “shallow” curve. Which curve is assumed by the soap ﬁlm?

22.1 Euler Equation
1091
Before answering this question, consider the physical situation with the rings moved apart
so that x0 = 1. Then Eq. (22.26) becomes
1 = c1 cosh
 1
c1

,
(22.28)
which has no real solutions. The physical signiﬁcance is that as the unit-radius rings were
moved out from the origin, a point was reached at which the soap ﬁlm could no longer
maintain the same horizontal force over each vertical section. Stable equilibrium was no
longer possible. The soap ﬁlm broke (irreversible process) and formed a circular ﬁlm over
each ring (with a total area of 2π = 6.2832...). This is known as the Goldschmidt discon-
tinuous solution to the soap-ﬁlm problem.
The next question is: How large may x0 be and still give a real solution for Eq. (22.26)?
Solving Eq. (22.26) for x0,
x0 = c1 cosh−1(1/c1),
(22.29)
we ﬁnd that x0 will be real only for c1 ≤1 and that its maximum value is attained when
dx0/dc1 = 0. A plot of x0 vs. c1 is shown in Fig. 22.5; it helps to explain the behavior we
observed at x0 = 1
2. We see from the plot (and more precisely from Exercise 22.1.6) that
the Euler equation has no solutions for x0 > xmax, where xmax ≈0.6627, and that this x0
value occurs when c1 ≈0.5524. For values of x0 smaller than xmax, there are solutions for
two different values of c1, corresponding to the “deep” and “shallow” curves found earlier
for x0 = 1
2.
Returning to the question as to which solution of Eq. (22.26) describes the soap ﬁlm,
let us calculate the area corresponding to each solution. Using Eq. (22.22) to reach the last
0.25
0.5
Deep curve
Shallow curve
x0
0.2
0.4
0.6
0.8
1
c1
FIGURE 22.5
Solutions of Eq. (22.26) for unit-radius rings at x = ±x0.

1092
Chapter 22 Calculus of Variations
member of the ﬁrst line below, we have
A = 4π
x0
Z
0
y(1 + y2
x)1/2 dx = 4π
c1
x0
Z
0
y2 dx
= 4πc1
x0
Z
0

cosh x
c1
2
dx = πc2
1

sinh
2x0
c1

+ 2x0
c1

.
(22.30)
For x0 = 1
2, Eq. (22.30) leads to
c1 = 0.2350
→
A = 6.8456,
c1 = 0.8483
→
A = 5.9917,
showing that the former can at most be only a local minimum. A more detailed investiga-
tion (compare Bliss, Additional Readings, chapter IV) shows that this surface is not even
a local minimum. For x0 = 1
2, the soap ﬁlm will be described by the shallow curve
y = 0.8483cosh

x
0.8483

.
This shallow catenoid (catenary of revolution) will be an absolute minimum for 0 ≤x0 <
0.528. However, for 0.528 < x < 0.6627, its area is greater than that of the Goldschmidt
discontinuous solution (6.2832) and it is only a relative minimum. See Fig. 22.6.
8.0
7.0
6.0
5.0
4.0
Area
3.0
Shallow curve
Deep curve
Goldschmidt
discontinuous
solution
2.0
1.0
0
0.1
0.2
0.3
0.4
x0
0.5
0.6
0.7
0.8
FIGURE 22.6
Catenoid area and that of the discontinuous solution of the soap-ﬁlm
problem (unit-radius rings at x = ±x0).

22.1 Euler Equation
1093
For an excellent discussion of both the mathematical problems and experiments with
soap ﬁlms, we refer to Courant and Robbins in Additional Readings. The larger message
of this subsection is the extent to which one must use caution in accepting solutions of the
Euler equations.
Exercises
22.1.1
For dy/dx ≡yx ̸= 0, show the equivalence of the two forms of Euler’s equation:
∂f
∂x −d
dx
∂f
∂yx
= 0
and
∂f
∂y −d
dx

f −yx
∂f
∂yx

= 0.
22.1.2
Derive Euler’s equation by expanding the integrand of
J(α) =
x2
Z
x1
f

y(x,α), yx(x,α), x

dx
in powers of α.
Note. The stationary condition is ∂J(α)/∂α = 0, evaluated at α = 0. The terms
quadratic in α may be useful in establishing the nature of the stationary solution (maxi-
mum, minimum, or saddle point).
22.1.3
Find the Euler equation corresponding to Eq. (22.14) if f = f (yxx, yx, y, x), assuming
that y and yx have ﬁxed values at the endpoints of their interval of deﬁnition.
ANS.
d2
dx2
 ∂f
∂yxx

−d
dx
 ∂f
∂yx

+ ∂f
∂y = 0.
22.1.4
The integrand f (y, yx, x) of Eq. (22.2) has the form
f (y, yx, x) = f1(x, y) + f2(x, y)yx.
(a)
Show that the Euler equation leads to
∂f1
∂y −∂f2
∂x = 0.
(b)
What does this imply for the dependence of the integral J on the choice of path?
22.1.5
Show that the condition that J =
Z
f (x, y)dx has a stationary value
(a)
leads to f (x, y) independent of y and
(b)
yields no information about any x-dependence.
We get no (continuous, differentiable) solution. To be a meaningful variational problem,
dependence on y or higher derivatives is essential.

1094
Chapter 22 Calculus of Variations
Note. The situation will change when constraints are introduced (compare to Exer-
cise 22.4.6).
22.1.6
A soap ﬁlm stretched between two rings of unit radius centered at ±x0 will have its
closest approach to the x-axis at x = 0, with the distance from the axis given by c1,
with x0 and c1 related by Eq. (22.26) or Eq. (22.29).
(a)
Show that dc1/dx0 becomes inﬁnite when x0 sinh(x0/c1) = 1, indicating that the
soap ﬁlm becomes unstable if x0 is increased beyond the value satisfying this
condition.
(b)
Show that the condition of part (a) is equivalent to
x0
c1
= coth
x0
c1

.
(c)
Solve the transcendental equation of part (b) to obtain the critical value of x0/c1
and show that the separate values of x0 and c1 are then approximately x0 ≈0.6627
and c1 ≈0.5524.
22.1.7
A soap ﬁlm is stretched across the space between two rings of unit radius centered
at ±x0 on the x-axis and perpendicular to the x-axis. Using the solution developed in
Example 22.1.3, set up the transcendental equations for the condition that x0 is such that
the area of the curved surface of rotation equals the area of the two rings (Goldschmidt
discontinuous solution). Solve for x0.
22.1.8
In Example 22.1.1, expand J[y(x,α)] −J[y(x,0)] in powers of α. The term linear in
α leads to the Euler equation and to the straight-line solution, Eq. (22.16). Investigate
the α2 term and show that the stationary value of J, the straight-line distance, is a
minimum.
22.1.9
(a)
Show that the integral
J =
x2
Z
x1
f (y, yx, x)dx,
with
f = y(x),
has no extreme values.
(b)
If f (y, yx, x) = y2(x), ﬁnd a discontinuous solution similar to the Goldschmidt
solution for the soap-ﬁlm problem.
22.1.10
Fermat’s principle of optics states that a light ray in a medium for which n is the
(position-dependent) index of refraction will follow the path y(x) for which
x2,y2
Z
x1,y1
n(y, x)ds
is a minimum. For y2 = y1 = 1, −x1 = x2 = 1, ﬁnd the ray path if
(a) n = ey,
(b) n = a(y −y0),
y > y0.
22.1.11
A particle moves, starting at rest, from point A on the surface of the Earth to point B
(also on the surface) by sliding frictionlessly through a tunnel. Find the differential

22.1 Euler Equation
1095
equation satisﬁed by the path if the transit time is to be a minimum. Assume the Earth
to be a nonrotating sphere of uniform density.
Hint. The potential energy of a particle of mass m a distance r < R from the center of
the Earth, with R the Earth’s radius, is 1
2mg(R2 −r2)/R, where g is the gravitational
acceleration at the Earth’s surface. It is convenient to describe the path of the particle (in
the plane through A, B, and the center of the Earth) by plane polar coordinates (r,θ),
with A at (R,−ϕ) and B at (R,ϕ).
ANS.
Letting r0 be the minimum value of r (reached at θ = 0),
Eq. (22.21) yields r2
θ = r2R2(r2 −r2
0)
r2
0(R2 −r2)
(the constant in
that equation has the value such that rθ = 0 at θ = 0).
The solution for the path is a hypocycloid, generated by a circle of radius 1
2(R −r0)
rolling inside the circle of radius R. You might like to show that the transit time is
t = π (R2 −r2
0)1/2
(Rg)1/2
.
For details see P. W. Cooper, Am. J. Phys. 34: 68 (1966); G. Veneziano, et al., 34: 701
(1966).
22.1.12
A ray of light follows a straight-line path in a ﬁrst homogeneous medium, is refracted
at an interface, and then follows a new straight-line path in the second medium. See
Fig. 22.7. Use Fermat’s principle of optics to derive Snell’s law of refraction:
n1 sinθ1 = n2 sinθ2.
Hint. Keep the points (x1, y1) and (x2, y2) ﬁxed and vary x0 to satisfy Fermat’s
principle.
Note. This is not an Euler equation problem, because the light path is not differentiable
at x0.
n1
θ1
θ2
(x0, 0)
(x2, y2)
(x1, y1)
n2
x
FIGURE 22.7
Snell’s law.

1096
Chapter 22 Calculus of Variations
22.1.13
A second soap-ﬁlm conﬁguration for the unit-radius rings at x = ±x0 consists of a
circular disk, radius a, in the x = 0 plane and two catenoids of revolution, one joining
the disk and each ring. One catenoid may be described by
y = c1 cosh
 x
c1
+ c3

.
(a)
Impose boundary conditions at x = 0 and x = x0.
(b)
Although not necessary, it is convenient to require that the catenoids form an angle
of 120◦where they join the central disk. Express this third boundary condition in
mathematical terms.
(c)
Show that the total area of catenoids plus central disk is then
A = c2
1

sinh
2x0
c1
+ 2c3

+ 2x0
c1

.
Note. Although this soap-ﬁlm conﬁguration is physically realizable and stable, the area
is larger than that of the simple catenoid for all ring separations for which both ﬁlms
exist.
ANS.
(a)



1 = c1 cosh
x0
c1
+ c3

a = c1 coshc3
(b) dy
dx = tan30◦= sinhc3.
22.1.14
For the soap ﬁlm described in Exercise 22.1.13, ﬁnd (numerically) the maximum value
of x0.
Note. This calls for a calculator with hyperbolic functions or a table of hyperbolic cotan-
gents.
ANS.
x0max = 0.4078.
22.1.15
Find the curve of quickest descent from (0,0) to (x0, y0) for a particle that, starting
from rest, slides under gravity and without friction. Show that the ratio of times taken
by the particle along a straight line joining the two points compared to along the curve
of quickest descent is (1 + 4/π2)1/2.
Hint. Take y to increase downwards. Apply Eq. (22.21) to obtain y2
x = (1 −c2y)/c2y,
where c is an integration constant. It is helpful to make the substitution c2y = sin2 ϕ/2
and take (x0, y0) = (π/2c2,1/c2).
22.2
MORE GENERAL VARIATIONS
Several Dependent Variables
To apply variational methods to classical mechanics, we need to generalize the Euler equa-
tion to situations in which there is more than one dependent variable in roles like y in

22.2 More General Variations
1097
Eq. (22.2). The generalization corresponds to functionals J of the form
J =
x2
Z
x1
f

u1(x),u2(x),...,un(x),u1x(x),u2x(x),...,unx(x), x

dx.
(22.31)
We are now calling the dependent variables ui to be consistent with notations we will
shortly introduce, and as before we use the subscript x to denote differentiation with respect
to x, so that uix = dui/dx and (later) ηix = dηi/dx. As in Section 22.1, we determine
stationary values of J by comparing neighboring paths for each ui. Let
ui(x,α) = ui(x,0) + αηi(x),
i = 1,2,...,n,
(22.32)
with the ηi independent of one other but subject to the continuity and endpoint restrictions
discussed in Section 22.1. By differentiating J from Eq. (22.31) with respect to α and
setting α = 0 (the condition that J be stationary), we obtain
x2
Z
x1
X
i
 ∂f
∂ui
ηi + ∂f
∂uix
ηix

dx = 0.
(22.33)
Again, each of the terms (∂f/∂uix)ηix is integrated by parts. The integrated part vanishes
and Eq. (22.33) becomes
x2
Z
x1
X
i
 ∂f
∂ui
−d
dx
∂f
∂uix

ηi dx = 0.
(22.34)
Since the ηi are arbitrary and independent of one another,6 each of the terms in the sum
must vanish independently. We have
∂f
∂ui
−d
dx
∂f
∂uix
= 0,
i = 1,2,...,n,
(22.35)
a whole set of Euler equations, each of which must be satisﬁed for a stationary value of J.
Hamilton’s Principle
The most important application of Eq. (22.31) occurs when the integrand f is taken to
be a Lagrangian L. The Langrangian (for nonrelativistic systems; see Exercise 22.2.5 for
a relativistic particle) is deﬁned as the difference of kinetic and potential energies of a
system:
L ≡T −V.
(22.36)
Using time as an independent variable instead of x and xi(t) as the dependent variables,
our conversion of Eq. (22.31) involves the replacements
x →t,
yi →xi(t),
yix →˙xi(t);
6For example, we could set η2 = η3 = η4 ··· = 0, eliminating all but one term of the sum, and then treat η1 exactly as in
Section 22.1.

1098
Chapter 22 Calculus of Variations
xi(t) is the position and ˙xi = dxi/dt is the velocity of particle i as a function of time.
The equation δJ = 0 is then a mathematical statement of Hamilton’s principle of classical
mechanics,
δ
t2
Z
t1
L(x1, x2,..., xn, ˙x1, ˙x2,..., ˙xn;t) dt = 0.
(22.37)
In words, Hamilton’s principle asserts that the motion of the system from time t1 to t2
is such that the time integral of the Lagrangian L, or action, has a stationary value. The
resulting Euler equations are usually called the Lagrangian equations of motion,
d
dt
∂L
∂˙xi
−∂L
∂xi
= 0
(each i).
(22.38)
These Lagrangian equations can be derived from Newton’s equations of motion, and New-
ton’s equations can be derived from Lagrange’s. The two sets of equations are equally
“fundamental.”
The Lagrangian formulation has advantages over the conventional Newtonian laws.
Whereas Newton’s equations are vector equations, we see that Lagrange’s equations
involve only scalar quantities. The coordinates x1, x2,... need not be a standard set of
coordinates or lengths. They can be selected to match the conditions of the physical prob-
lem. The Lagrange equations are invariant with respect to the choice of coordinate system.
Newton’s equations (in component form) are not manifestly invariant. For example, Exer-
cise 3.10.27 shows what happens when F = ma is resolved in spherical polar coordinates.
Exploiting the concept of energy, we may easily extend the Lagrangian formulation from
mechanics to diverse ﬁelds, such as electrical networks and acoustical systems. Extensions
to electromagnetism appear in the exercises. The result is a uniﬁcation of otherwise sep-
arate areas of physics. In the development of new areas, the quantization of Lagrangian
particle mechanics provided a model for the quantization of electromagnetic ﬁelds and led
to the gauge theory of quantum electrodynamics.
One of the most valuable advantages of Hamilton’s principle (the Lagrange equation
formulation) is the ease in seeing a relation between a symmetry and a conservation law. As
an example, let xi = ϕ, an azimuthal angle. If our Lagrangian is independent of ϕ (that is,
ϕ is said to be an ignorable coordinate), there are two consequences: (1) the conservation
or invariance of the component of angular momentum associated with (conjugate to) ϕ,
and (2) from Eq. (22.38), ∂L/∂˙ϕ = constant. Similarly, invariance under translation leads
to conservation of linear momentum.
Example 22.2.1
MOVING PARTICLE, CARTESIAN COORDINATES
A particle of mass m moves in one dimension with its position described by a Cartesian
coordinate x, subject to a potential V (x). Its kinetic energy is given by T = m ˙x2/2, so its
Lagrangian L has the form
L = T −V = 1
2m ˙x2 −V (x).

22.2 More General Variations
1099
We will need
∂L
∂˙x = m ˙x,
∂L
∂x = −dV (x)
dx
= F(x).
(22.39)
We have identiﬁed the force F as the negative gradient of the potential. Inserting the results
from Eq. (22.39) into the Lagrangian equation of motion, Eq. (22.38), we get
d
dt (m ˙x) −F(x) = 0,
which is Newton’s second law of motion.
■
Example 22.2.2
MOVING PARTICLE, CIRCULAR CYLINDRICAL COORDINATES
Now let us consider a particle of mass m moving in the xy-plane, that is, z = 0. We use
cylindrical coordinates ρ,ϕ. The kinetic energy is
T = 1
2m(˙x2 + ˙y2) = 1
2m( ˙ρ2 + ρ2 ˙ϕ2),
(22.40)
and we take V = 0 for simplicity.
We could have converted ˙x2 + ˙y2 into circular cylindrical coordinates by taking
x(ρ,ϕ) = ρ cosϕ, y(ρ,ϕ) = ρ sinϕ, and then differentiating with respect to time and
squaring. What we actually did was to recognize that the cylindrical coordinates are an
orthogonal system with scale factors hρ = 1, hϕ = ρ, so the velocity v has in the cylindri-
cal system components vρ = ˙ρ and vϕ = ρ ˙ϕ.
We now apply the Lagrangian equations of motion ﬁrst to the ρ coordinate and then
to ϕ:
d
dt (m ˙ρ) −mρ ˙ϕ2 = 0,
d
dt (mρ2 ˙ϕ) = 0.
The second equation is a statement of conservation of angular momentum. The ﬁrst may be
interpreted as radial acceleration7 equated to centrifugal force. In this sense the centrifugal
force is a real force. It is of some interest that this interpretation of centrifugal force as a
real force is supported by the general theory of relativity.
■
Hamilton’s Equations
Hamilton was the ﬁrst to show that Euler’s equation for the Lagrangian enabled the equa-
tions of motion to be reduced to the set of coupled ﬁrst-order PDEs called Hamilton’s
equations. A starting point for this analysis is the deﬁnition of the canonical momentum
pi conjugate to the coordinate qi, deﬁned as
pi = ∂L
∂˙qi
.
(22.41)
7Here is a second method of attacking Exercise 3.10.13.

1100
Chapter 22 Calculus of Variations
This deﬁnition is consistent with the elementary deﬁnition of momentum in Cartesian
coordinates, where (in one dimension) T = m ˙q2/2, p = m ˙q. From Eq. (22.41) and the
Lagrangian equations of motion, Eq. (22.38), we have by direct substitution
˙pi = ∂L
∂qi
,
(22.42)
and this permits us to write the variation of L in the form
dL =
X
i
 ∂L
∂qi
dqi + ∂L
∂˙qi
d ˙qi

+ ∂L
∂t dt =
X
i
( ˙pi dqi + pi d ˙qi) + ∂L
∂t dt.
(22.43)
We now deﬁne the Hamiltonian as
H =
X
i
pi ˙qi −L,
(22.44)
and compute
d H =
X
i
(pid ˙qi + ˙qidpi)−
 X
i
( ˙pidqi + pid ˙qi) + ∂L
∂t dt
!
=
X
i
(˙qidpi −˙pidqi)−∂L
∂t dt.
(22.45)
But from the chain rule for differentiation, we also have
d H =
X
i
∂H
∂pi
dpi + ∂H
∂qi
dqi

+ ∂H
∂t dt.
(22.46)
Equating the coefﬁcients of dpi, dqi, and dt in Eqs. (22.45) and (22.46), we obtain Hamil-
ton’s equations:
∂H
∂pi
= ˙qi,
∂H
∂qi
= −˙pi,
∂H
∂t = −∂L
∂t .
(22.47)
In conservative systems, ∂H/∂t = 0, and H has a constant value equal to the total energy
of the system.
Several Independent Variables
Sometimes the integrand f in an equation analogous to Eq. (22.2) will contain an unknown
function, u, that is a function of several independent variables, u = u(x, y, z). In the three-
dimensional case, for example, that equation becomes
J =
Z Z Z
f
 u,ux,uy,uz, x, y, z

dx dy dz,
(22.48)
where ux = ∂u/∂x, uy = ∂u/∂y, uz = ∂u/∂z, and u is assumed to have speciﬁed values
on the boundary of the region of integration.
Generalizing the analysis of Section 22.1, we represent the variation of u as
u(x, y, z,α) = u(x, y, z,0) + αη(x, y, z),

22.2 More General Variations
1101
where η is arbitrary except that it must vanish on the boundary. Our integral J is now, as
in Section 22.1, a function of α, and our variational problem is to make J stationary with
respect to α.
Differentiating the integral Eq. (22.48) with respect to the parameter α and then setting
α = 0, we obtain
∂J
∂α

α=0
=
Z Z Z ∂f
∂u η + ∂f
∂ux
ηx + ∂f
∂uy
ηy + ∂f
∂uz
ηz

dx dy dz = 0.
We continue to use a notation similar to that used previously: ηx is shorthand for
∂η/∂x, etc.
Again, we integrate each of the terms (∂f/∂ui)ηi by parts. The integrated part vanishes
at the boundary (because the deviation η is required to go to zero there) and we get
Z Z Z ∂f
∂u −∂
∂x
∂f
∂ux
−∂
∂y
∂f
∂uy
−∂
∂z
∂f
∂uz

η(x, y, z)dx dy dz = 0.
(22.49)
We must now digress to clarify the notation in Eq. (22.49). The derivative ∂/∂x enters
that equation as a result of the integration by parts, and it therefore must act on all the x
dependence of ∂f/∂ux, not just on the explicit appearance of x in f . The reader may recall
that this derivative was written d/dx when it arose in Section 22.1, but that notation is not
entirely appropriate here as the functions involved also depend on y and z.
We conclude our analysis with the now-familiar observation that since the variation
η(x, y, z) is arbitrary, the term in large parentheses is set equal to zero. This yields the
Euler equation for (three) independent variables,
∂f
∂u −∂
∂x
∂f
∂ux
−∂
∂y
∂f
∂uy
−∂
∂z
∂f
∂uz
= 0.
(22.50)
Remember that the derivative ∂/∂x operates on both the explicit and implicit x dependence
of ∂f/∂ux; similar remarks apply to ∂/∂y and ∂/∂z.
Example 22.2.3
LAPLACE’S EQUATION
A variational problem with several independent variables is provided by electrostatics. An
electrostatic ﬁeld has
energy density = 1
2εE2,
where E is the electric ﬁeld. In terms of the static potential ϕ,
energy density = 1
2ε(∇ϕ)2.
Now let us impose the requirement that the electrostatic energy (associated with the ﬁeld)
in a given charge-free volume be a minimum subject to speciﬁc conditions on ϕ at the
boundary. The assumption that the volume is charge-free makes ϕ continuous and dif-
ferentiable throughout the volume, and we therefore have a situation to which an Euler
equation applies. We have the volume integral
J =
Z Z Z
(∇ϕ)2 dx dy dz =
Z Z Z
(ϕ2
x + ϕ2
y + ϕ2
z )dx dy dz,

1102
Chapter 22 Calculus of Variations
where ϕx stands for ∂ϕ/∂x. Thus,
f (ϕ,ϕx,ϕy,ϕz, x, y, z) = ϕ2
x + ϕ2
y + ϕ2
z ,
so Euler’s equation, Eq. (22.50), yields (with u in that equation replaced by ϕ)
−2(ϕxx + ϕyy + ϕzz) = 0,
which in the usual vector notation is equivalent to
∇2ϕ(x, y, z) = 0.
This is Laplace’s equation of electrostatics.
Closer investigation shows that this stationary value is indeed a minimum. Thus the
demand that the ﬁeld energy be minimized leads to Laplace’s PDE.
■
Several Dependent and Independent Variables
In some cases our integrand f contains more than one dependent variable and more than
one independent variable. Consider
f = f

p(x, y, z), px, py, pz,q(x, y, z),qx,qy,qz,r(x, y, z),rx,ry,rz, x, y, z

.
(22.51)
We proceed as before with
p(x, y, z,α) = p(x, y, z,0) + αξ(x, y, z),
q(x, y, z,α) = q(x, y, z,0) + αη(x, y, z),
r(x, y, z,α) = r(x, y, z,0) + αζ(x, y, z),
and so on.
Keeping in mind that ξ,η, and ζ are independent of one another, as were the ηi in
Eq. (22.32), the same differentiation and then integration by parts will lead to
∂f
∂p −∂
∂x
∂f
∂px
−∂
∂y
∂f
∂py
−∂
∂z
∂f
∂pz
= 0,
(22.52)
with similar equations for functions q and r. Replacing p, q, r, ... with yi and x, y, z, ...
with xi, we can put Eq. (22.52) in a more compact form:
∂f
∂yi
−
X
j
∂
∂xj
 ∂f
∂yi j

= 0,
i = 1,2,...,
(22.53)
in which
yi j ≡∂yi
∂xj
.
An application of Eq. (22.53) appears in Exercise 22.2.10.

22.2 More General Variations
1103
Geodesics
Particularly in general relativity, it is of interest to identify the shortest path between two
points in a “curved space,” i.e., a space characterized by a metric tensor more general than
that of Euclidean or even Minkoswki space. A path that is a “local minimum” (calculated
using the relevant metric), meaning that it is shorter than other paths that can be reached
from it by small deformations, is referred to as a geodesic. This deﬁnition causes both the
two great-circle paths of Fig. 22.2 to be identiﬁed as geodesics, because even the longer
path is of minimum length relative to small deformations. In practice, it is usually easy to
identify which of several geodesics in fact corresponds to the shortest path.
The calculus of variations is the natural tool for identifying geodesics, and in fact it was
used in Example 22.1.1 to verify that a straight line is the geodesic connecting given points
in Euclidean space. To extend the analysis to more general metric spaces, we start by relat-
ing the distance between two neighboring points, ds, with the changes in their coordinates,
dqi, i = 1,2,.... Note that we distinguish between covariant and contravariant quantities,
using superscripts for the latter (coordinate displacements are contravariant; compare with
Section 4.3). The distance ds is a scalar, given by
ds2 = gi j dqi dq j.
(22.54)
Here gi j is the metric tensor, which is symmetric but in many cases of interest not diagonal.
Note that we are using the Einstein summation convention, so i and j in Eq. (22.54) are
summed, causing ds2 to be a scalar. This formula is an obvious generalization of that for
Euclidean space,
ds2 = dx2 + dy2 + dz2,
but differs therefrom in that the coordinates qi are not assumed to be mutually orthogonal,
so ds2 contains cross terms dqidq j with i ̸= j.
A path in our curved space can be described parametrically by giving the qi as functions
of an independent variable that we will call u, and the distance between two points A and
B can then be represented as
J =
B
Z
A
ds
du du =
B
Z
A
q
gi j dqi dq j
du
du =
B
Z
A
s
gi j
dqi
du
dq j
du du
=
B
Z
A
q
gi j ˙qi ˙q jdu,
(22.55)
where we are borrowing the dot notation, ˙qi ≡dqi/du.
One could now proceed to ﬁnd the qi(u) that minimize J, but this is a relatively difﬁcult
problem. Instead we rely on the Lagrangian formulation of relativistic mechanics, where,
for a particle not subject to a potential (other than a gravitational force whose effect is
described by the metric), the Lagrangian reduces to
L = m
2 gi j ˙qi ˙q j.
(22.56)

1104
Chapter 22 Calculus of Variations
Here the dot notation refers to derivatives with respect to the proper time τ (or to any other
variable related thereto by an afﬁne transformation (meaning the new variable, e.g., u, is
related to τ by a transformation of the form u = aτ + b). This means that we can replace
the minimization of J by that of the action:
δ
B
Z
A
gi j ˙qi ˙q j du = 0,
(22.57)
in effect simplifying our problem by eliminating the radical that was present in Eq. (22.55).
The minimization in Eq. (22.57) is a relatively simple standard problem in the calculus
of variations; for solving it we note that each gi j is in general a function of all the qk (but
not the derivatives ˙qk). There will be an Euler equation for each k; before simpliﬁcation
they take the form
∂gi j ˙qi ˙q j
∂qk
−d
du
∂gi j ˙qi ˙q j
∂˙qk
= 0.
(22.58)
Starting to evaluate Eq. (22.58), we get
∂gi j
∂qk ˙qi ˙q j −d
du gi j
∂
∂˙qk

˙qi ˙q j
= ∂gi j
∂qk ˙qi ˙q j −d
du

gkj ˙q j + gik ˙qi
= 0.
(22.59)
Some simpliﬁcation is achieved by using the relations
d ˙q j
du = ¨q j
and
dgkj
du = ∂gkj
∂qi ˙qi
(remember that the Einstein summation convention is still in use). Equation (22.59)
reduces to
1
2 ˙qi ˙q j
∂gi j
∂qk −∂gkj
∂qi −∂gik
∂q j

−gik ¨qi = 0.
(22.60)
As a ﬁnal simpliﬁcation, we multiply Eq. (22.60) by gkl and use the identity gklgik = δl
i,
reaching (in a more expanded notation) the geodesic equation
d2ql
du2 + dqi
du
dq j
du
1
2 gklh ∂gkj
∂qi + ∂gik
∂q j −∂gi j
∂qk
i
= 0.
(22.61)
Comparing with the formula for the Christoffel symbol, Eq. (4.63), we can rewrite
Eq. (22.61) as
d2ql
du2 + dqi
du
dq j
du 0l
i j = 0.
(22.62)
Note that although Eq. (22.62) gives the differential equation describing geodesics in
curved space, it is a long way from that equation to its explicit solution for signiﬁcant
problems in general relativity. The exploration of such solutions is a topic of current re-
search and beyond the scope of the present text.

22.2 More General Variations
1105
Relation to Physics
The calculus of variations as developed so far provides an elegant description of a wide
variety of physical phenomena. The physics includes classical mechanics, as in Ex-
amples 22.2.1 and 22.2.2; relativistic mechanics, Exercise 22.2.5; electrostatics, Exam-
ple 22.2.3; and electromagnetic theory in Exercise 22.2.10. The convenience should not
be minimized, but at the same time we should be aware that in these cases the calculus
of variations has only provided an alternate description of what was already known. The
situation does change with incomplete theories.
If the basic physics is not yet known, a postulated variational principle can be a useful
starting point.
Exercises
22.2.1
(a)
Develop the equations of motion corresponding to L = 1
2m(˙x2 + ˙y2).
(b)
In what sense do your solutions minimize the integral
Z t2
t1
L dt?
Compare the result for your solution with x = constant, y = constant.
22.2.2
From the Lagrangian equations of motion, Eq. (22.38), show that a system in stable
equilibrium has a minimum potential energy.
22.2.3
Write out the Lagrangian equations of motion of a particle in spherical coordinates for
potential V equal to a constant. Identify the terms corresponding to (a) centrifugal force
and (b) Coriolis force.
22.2.4
The spherical pendulum consists of a mass on a wire of length l, free to move in polar
angle θ and azimuth angle ϕ (Fig. 22.8).
(a)
Set up the Lagrangian for this physical system.
(b)
Develop the Lagrangian equations of motion.
θ
ϕ
l
y
x
FIGURE 22.8
Spherical pendulum.

1106
Chapter 22 Calculus of Variations
22.2.5
Show that the Lagrangian
L = m0c2

1 −
s
1 −v2
c2

−V (r)
leads to a relativistic form of Newton’s second law of motion,
d
dt
 
m0vi
p
1 −v2/c2
!
= Fi,
in which the force components are Fi = −∂V/∂xi.
22.2.6
The Lagrangian for a particle with charge q in an electromagnetic ﬁeld described by
scalar potential ϕ and vector potential A is
L = 1
2mv2 −qϕ + qA · v.
Find the equation of motion of the charged particle.
Hint. (d/dt)A j = ∂A j/∂t +P
i(∂A j/∂xi)˙xi. The dependence of the force ﬁelds E and
B on the potentials ϕ and A is developed in Section 3.9; see in particular Eq. (3.108).
ANS.
m ¨xi = q[E + v × B]i.
22.2.7
Consider a system in which the Lagrangian is given by
L(qi, ˙qi) = T (qi, ˙qi) −V (qi),
where qi and ˙qi represent sets of variables. The potential energy V is independent of
velocity and neither T nor V has any explicit time dependence.
(a)
Show that
d
dt

X
j
˙q j
∂L
∂˙q j
−L

= 0.
(b)
The constant quantity
X
j
˙q j
∂L
∂˙q j
−L
deﬁnes the Hamiltonian H. Show that under the preceding assumed conditions, H
satisﬁes H = T + V , and is therefore the total energy.
Note. The kinetic energy T is a quadratic function of the ˙qi.
22.2.8
The Lagrangian for a vibrating string (small-amplitude vibrations) is
L =
Z 1
2ρu2
t −1
2τu2
x

dx,
where ρ is the (constant) linear mass density and τ is the (constant) tension. The
x-integration is over the length of the string. Show that application of Hamilton’s

22.3 Constrained Minima/Maxima
1107
principle to the Lagrangian density (the integrand), now with two independent vari-
ables, leads to the classical wave equation,
∂2u
∂x2 = ρ
τ
∂2u
∂t2 .
22.2.9
Show that the stationary value of the total energy of the electrostatic ﬁeld of
Example 22.2.3 is a minimum.
Hint. Investigate the α2 terms of J.
22.2.10
The Lagrangian (per unit volume) of an electromagnetic ﬁeld with a charge density ρ
and current density J is given by
L = 1
2

ε0E2 −1
µ0
B2

−ρϕ + J · A.
Show that Lagrange’s equations lead to two of Maxwell’s equations. (The remaining
two are a consequence of the deﬁnition of E and B in terms of A and ϕ.)
Hint. Take ϕ and the components of A as dependent variables; and x, y, z, and t as
independent variables. E and B are given in terms of A and ϕ by Eq. (3.108).
22.3
CONSTRAINED MINIMA/MAXIMA
In preparation for dealing with problems in the calculus of variations in which an inte-
gral is to be minimized subject to constraints (which may either be algebraic equations or
ﬁxed values of other integrals), we look now at situations in which we seek a constrained
extremum of an ordinary function.
A typical constrained problem of the type now under consideration is the minimization
of a function of several variables, here illustrated as f (x, y, z), subject to the constraint that
g(x, y, z) be kept constant. Since the equation g(x, y, z) = C deﬁnes a surface, our con-
strained problem is that of minimizing f (x, y, z) on a surface of constant g. The presence
of the constraint means that only two of the three variables x, y, z are actually independent,
and in principle one could solve the constraint equation to obtain z as a function of x and
y: z = z(x, y), after which one could obtain the desired minimum by setting to zero the
derivatives
∂
∂x f

x, y, z(x, y)

and
∂
∂y f

x, y, z(x, y)

.
However, it may be cumbersome, or in some cases nearly impossible to solve the constraint
equation, and in any case this approach does not treat the variables x, y, z on an explicitly
equivalent basis. For these reasons it is useful to employ an alternate procedure, known as
the method of Lagrangian multipliers.

1108
Chapter 22 Calculus of Variations
Lagrangian Multipliers
Continuing with our three-dimensional illustration in which we seek to minimize f (x, y, z)
subject to the constraint g(x, y, z) = C, our starting point is that the constraint equation
implies
dg =
∂g
∂x

yz
dx +
∂g
∂y

xz
dy +
∂g
∂z

xy
dz = 0,
where (as indicated here explicitly) the partial derivatives of g are taken viewing x, y, and
z as independent. Proceeding as for the derivation of Eq. (1.144), we have
 ∂z
∂x

y
= −
∂g
∂x

yz
∂g
∂z

xy
and
 ∂z
∂y

x
= −
∂g
∂y

xz
∂g
∂z

xy
.
(22.63)
Now setting (∂f/∂x)y to zero, we have (imposing the constraint dg = 0)
∂f
∂x

y
=
∂f
∂x

yz
+
∂f
∂z

xy
 ∂z
∂x

y
=
∂f
∂x

yz
−
∂f
∂z

xy
∂g
∂z

xy
∂g
∂x

yz
=
∂f
∂x

yz
−λ
∂g
∂x

yz
= 0,
(22.64)
where
λ =
∂f
∂z

xy
∂g
∂z

xy
.
(22.65)
The quantity λ is called a Lagrangian multiplier.
Now taking Eq. (22.64), its equivalent with y replacing x, and a rearranged form of
Eq. (22.65), we have the symmetrical set of formulas
∂f
∂x

yz
−λ
∂g
∂x

yz
= 0,
∂f
∂y

xz
−λ
∂g
∂y

xz
= 0,
∂f
∂z

xy
−λ
∂g
∂z

xy
= 0.
(22.66)

22.3 Constrained Minima/Maxima
1109
The generalization of Eqs. (22.66) to n variables and k constraints is
∂f
∂xi
−
k
X
j=1
λ j
∂g j
∂xi
= 0,
i = 1,2,...,n.
(22.67)
The n equations, Eqs. (22.67), contain n + k unknowns (the n xi and the k λ j), and they
are to be solved subject also to the k constraint equations. In some problems it is never
necessary to evaluate explicitly the Lagrangian multipliers, and for this reason the method
is sometimes referred to as that of (Lagrange’s) undetermined multiplier(s).
Note that the formulation provided above does not only identify minima; the same equa-
tions will locate maxima and saddle points. It is necessary to determine the nature of the
stationary points from the speciﬁc problem at hand.
While the derivation of Eq. (22.66) was asymmetric in that λ was obtained considering
z to be a dependent variable, we could have carried out the analysis with x or y in place
of z. This gives us an alternate route to the ﬁnal formulas in the special case that (∂g/∂z)
vanishes, in which case Eq. (22.65) becomes undeﬁned. The method only fails if all the
derivatives of a constraint function vanish at the stationary point.
Example 22.3.1
MINIMIZING SURFACE-TO-VOLUME RATIO
Consider a right circular cylinder of radius r and height h. We wish to ﬁnd the ratio h/r
that will minimize the surface area for a ﬁxed enclosed volume. The relevant formulas are:
surface area S = 2π(rh + r2), volume V = πr2h.
Applying Eqs. (22.67) for the case of one constraint and two independent variables,
we have
∂S
∂r −λ∂V
∂r = 2π(h + 2r) −λ(2πrh) = 0,
∂S
∂h −λ∂V
∂h = 2πr −λπr2 = 0.
Eliminating λ from these equations, we ﬁnd h/r = 2. Because we have not also used
the constraint equation, we get only the ratio of the two variables h and r (which is the
information that is relevant for the present problem). However, if we specify the volume
V (i.e., use the constraint equation), we then get individual values of h and r.
We close with two more observations: (1) Our solution obviously provides a minimum
S/V ratio, but in principle this has to be determined by closer study of the problem. In
the present case, there is no maximum, as S/V increases without limit as h/r approaches
zero. (2) We note that minimizing S for ﬁxed V is the same thing as maximizing V for
ﬁxed S, and leads to equivalent Lagrangian multiplier equations.
■

1110
Chapter 22 Calculus of Variations
Exercises
22.3.0
The following problems are to be solved by using Lagrangian multipliers.
22.3.1
The ground-state energy of a quantum particle of mass m in a pillbox (right-circular
cylinder) is given by
E = ¯h2
2m
(2.4048)2
R2
+ π2
H2

,
in which R is the radius and H is the height of the pillbox. Find the ratio of R to H that
will minimize the energy for a ﬁxed volume.
22.3.2
The U.S. Post Ofﬁce limits ﬁrst-class mail to Canada to a total of 36 inches, length plus
girth. Using Lagrange multipliers, ﬁnd the dimensions of the rectangular parallelepiped
of maximum volume subject to this constraint.
22.3.3
A thermal nuclear reactor is subject to the constraint
ϕ(a,b,c) =
π
a
2
+
π
b
2
+
π
c
2
= B2,
a constant,
where the reactor is a rectangular parallelepiped of sides a, b, and c. Find the ratios of
a, b, and c that maximize the reactor volume.
ANS.
a = b = c,cube.
22.3.4
For a lens of focal length f, the object distance p and the image distance q are related
by 1/p + 1/q = 1/f . Find the minimum object-image distance (p + q) for ﬁxed f .
Assume real object and image (p and q both positive).
22.3.5
You have an ellipse (x/a)2 + (y/b)2 = 1. Find the inscribed rectangle of maximum
area. Show that the ratio of the area of the maximum-area rectangle to the area of the
ellipse is 2/π = 0.6366.
22.3.6
A rectangular parallelepiped is inscribed in an ellipsoid of semiaxes a,b, and c. Maxi-
mize the volume of the inscribed rectangular parallelepiped. Show that the ratio of the
maximum volume to the volume of the ellipsoid is 2/π
√
3 ≈0.367.
22.3.7
Find the maximum value of the directional derivative of ϕ(x, y, z),
dϕ
ds = ∂ϕ
∂x cosα + ∂ϕ
∂y cosβ + ∂ϕ
∂z cosγ,
subject to the constraint,
cos2 α + cos2 β + cos2 γ = 1.

22.4 Variation with Constraints
1111
22.4
VARIATION WITH CONSTRAINTS
As in earlier sections, we seek the path that will make the integral
J =
Z
f

yi, ∂yi
∂xj
, xj

dxj
(22.68)
stationary. This is the general case in which xj represents a set of independent variables
and yi a set of dependent variables. Now, however, we introduce one or more constraints.
This means that the yi are no longer independent of each other. Then, if we vary the yi
by writing yi(α) = yi(0) + αηi, not all the ηi may then be varied arbitrarily, and the Euler
equations would not apply.
Our approach will be to use Lagrange’s method of undetermined multipliers. We con-
sider ﬁrst the possibility that the kth constraint takes the form of an equation:
ϕk

yi, ∂yi
∂xj
, xj

= 0.
(22.69)
This will ordinarily not be meaningful unless there is more than one dependent or indepen-
dent variable, so that Eq. (22.69) restricts, but does not fully determine yi. Remember that
yi and xj are here used to denote sets of variables. To introduce an undetermined multi-
plier and remain in harmony with our study of the calculus of variations, we note that the
constraint, Eq. (22.69), can be stated in the form
Z
λk(xj)ϕk

yi, ∂yi
∂xj
, xj

dxj = 0,
(22.70)
with λk(xj) an arbitrary function of the xj. Equation (22.70) is clearly satisﬁed if
δ
Z
λk(xj)ϕk

yi, ∂yi
∂xj
, xj

dxj = 0.
(22.71)
Alternatively, we may have a constraint in the form of an integral (now dependent on both
the yi and their derivatives throughout the interval on which the problem is deﬁned):
Z
ϕk

yi, ∂yi
∂xj
, xj

dxj = constant.
(22.72)
The effect of this constraint can be brought to a form consistent with Eq. (22.71) by writing
δ
Z
λk ϕk

yi, ∂yi
∂xj
, xj

dxj = 0.
(22.73)
Note that in this equation λk does not depend on the xj but is simply a constant, as it is
only the integral of ϕk that is required to be stationary.
At this point, our constraints have been written as integrals that are dependent on the
undetermined multipliers λk, where λk means either λk(xj) or just λk, depending on
whether the constraint was from Eq. (22.71) or (22.73). We therefore have our problem
in a form suitable for applying the method of Lagrangian multipliers as developed in

1112
Chapter 22 Calculus of Variations
Section 22.3, and may use a formula analogous to Eq. (22.67). In our present notation,
we obtain
δ
Z "
f

yi, ∂yi
∂xj
, xj

+
X
k
λkϕk

yi, ∂yi
∂xj
, xj
#
dxj = 0.
(22.74)
Remember that the Lagrangian multiplier λk may depend on the xj when ϕ(yi, xj) is given
in the form of Eq. (22.69).
We now continue by treating the entire integrand as a new function whose integral is to
be made stationary:
g

yi, ∂yi
∂xj
, xj

= f +
X
k
λkϕk.
(22.75)
If we have N dependent variables yi (i = 1,2,..., N) and m constraints (k = 1, 2,...,m),
then N −m of the ηi may be taken as arbitrary. In place of arbitrary variation of the m
remaining ηi, we may instead set the m multipliers λk to the (presently unknown) values
that permit the Euler equations to be satisﬁed. The overall result is that we may require
satisfaction of an Euler equation for each of the dependent variables yi, but the m quantities
λk that appear in the solution of the Euler equations must be assigned values consistent
with the constraints that have been imposed. In other words, it will be necessary to solve
simultaneously the Euler equations and the equations of constraint to ﬁnd the function g
(and hence f ) yielding a stationary value.
Lagrangian Formulation with Constraints
In the absence of constraints, Lagrange’s equations of motion Eq. (17.52) were found to be8
d
dt
∂L
∂˙qi
−∂L
∂qi
= 0,
with t (time) the one independent variable and qi(t) (the particle positions) a set of depen-
dent variables. Usually the generalized coordinates qi are chosen to eliminate the forces
of constraint, but this is not necessary and not always desirable. In the presence of holo-
nomic constraints (those that can be expressed via mathematical expressions, e.g., ϕk = 0),
Hamilton’s principle is
δ
Z "
L(qi, ˙qi,t) +
X
k
λk(t)ϕk(qi,t)
#
dt = 0,
(22.76)
and the constrained Lagrangian equations of motion are
d
dt
∂L
∂˙qi
−∂L
∂qi
=
X
k
aikλk.
(22.77)
8The symbol q is customary in classical mechanics. It serves to emphasize that the variable is not necessarily a Cartesian variable
(and not necessarily a length).

22.4 Variation with Constraints
1113
Usually the constraint is of the form ϕk = ϕk(qi,t), independent of the generalized veloci-
ties ˙qi. In this case the coefﬁcient aik is given by
aik = ∂ϕk
∂qi
.
(22.78)
Then aikλk (no summation) represents the force of the kth constraint in the ˆqi-direction,
appearing in Eq. (22.77) in exactly the same way as −∂V/∂qi.
Example 22.4.1
SIMPLE PENDULUM
To illustrate, consider the simple pendulum, a mass m, constrained by a wire of length
l to swing in an arc (Fig. 22.9) under a gravitational force characterized by a constant
acceleration g. In the absence of the one constraint,
ϕ1 = r −l = 0,
(22.79)
there are two generalized coordinates r and θ (assuming the motion to be restricted to a
vertical plane). The Lagrangian is
L = T −V = 1
2m(˙r2 + r2 ˙θ2) + mgr cosθ,
(22.80)
taking the potential V to be zero when the pendulum is horizontal, at θ = π/2. Noting that
ar1 = ∂ϕ1
∂r = 1,
aθ1 = ∂ϕ1
∂θ = 0,
the equations of motion obtained from Eq. (22.77) are
d
dt
∂L
∂˙r −∂L
∂r = λ1,
d
dt
∂L
∂˙θ −∂L
∂θ = 0,
(22.81)
or
d
dt (m˙r) −mr ˙θ2 −mg cosθ = λ1,
d
dt (mr2 ˙θ) + mgr sinθ = 0.
θ
m
FIGURE 22.9
Simple pendulum.

1114
Chapter 22 Calculus of Variations
Substituting from the equation of constraint (r = l, ˙r = 0), these equations become
ml ˙θ2 + mg cosθ = −λ1,
ml2 ¨θ + mgl sinθ = 0.
(22.82)
The second equation may be solved for θ(t) to yield simple harmonic motion if the am-
plitude is small (sinθ ≈θ), whereas the ﬁrst equation expresses the tension in the wire in
terms of θ and ˙θ. Note that since the equation of constraint, Eq. (22.79), is in the form of
Eq. (22.69), the Lagrange multiplier λ1 will be a function of t. Since the second equation
sufﬁces to determine θ(t) (assuming a choice of initial conditions), the left-hand side of
the ﬁrst equation can be evaluated if an explicit form for λ1 is desired.
■
Example 22.4.2
SLIDING OFF A LOG
Another example from mechanics is the problem of a particle sliding on a cylindrical sur-
face, as shown in Fig. 22.10. The object is to ﬁnd the critical angle θc at which the particle
ﬂies off from the surface. This critical angle is the angle at which the radial force of con-
straint goes to zero, and it will depend on the initial velocity with which the particle departs
from a position atop the cylinder. To make the problem well-deﬁned, we seek the maxi-
mum value that can be attained by θc, corresponding to its limit at low initial velocity.
To illustrate the present constrained-minimization method, we take
L = T −V = 1
2m(˙r2 + r2 ˙θ2) −mgr cosθ
(22.83)
and the one equation of constraint,
ϕ1 = r −l = 0.
(22.84)
Proceeding as in Example 22.4.1, with
ar1 = ∂ϕ1
∂r = 1,
aθ1 = ∂ϕ1
∂θ = 0,
we reach
m¨r −mr ˙θ2 + mg cosθ = λ1(θ),
mr2 ¨θ + 2mr ˙r ˙θ −mgr sinθ = 0.
θ
r
FIGURE 22.10
A particle sliding on a cylindrical surface.

22.4 Variation with Constraints
1115
We have chosen to identify the constraining force λ1 as a function of the angle θ, a valid
choice since θ is a single-valued function of the independent variable t.
Inserting the constrained values r = l, ¨r = ˙r = 0, these equations reduce to
−ml ˙θ2 + mg cosθ = λ1(θ),
(22.85)
ml2 ¨θ −mgl sinθ = 0.
(22.86)
Differentiating Eq. (22.85) with respect to time and remembering that
d f (θ)
dt
= d f (θ)
dθ
˙θ,
we obtain
−2ml ¨θ −mg sinθ = dλ1(θ)
dθ
.
(22.87)
Combining Eqs. (22.86) and (22.87) to eliminate the ¨θ term, we have
dλ1
dθ = −3mg sinθ,
which integrates to
λ1(θ) = 3mg cosθ + C.
(22.88)
To ﬁx the constant C, we evaluate Eq. (22.88) for θ = 0:
−ml ˙θ2
θ=0 + mg = 3mg + C,
which shows that C ≤−2mg, with C = −2mg when the initial velocity ˙θ(0) is zero. Using
this value of C (which leads to the largest critical angle), we have
λ1(θ) = mg(3cosθ −2).
(22.89)
The particle will stay on the surface as long as the force of constraint is nonnegative, that
is, as long as the surface has to push outward on the particle, corresponding to λ1(θ) > 0.
From Eq. (22.89) we ﬁnd that the critical angle, at which λ1(θc) = 0, satisﬁes
cosθc = 2
3,
or
θc = 48◦11′
from the vertical. At or before this angle (neglecting all friction) our particle takes off.
It must be admitted that this result can be obtained more easily by considering a varying
centripetal force furnished by the radial component of the gravitational force. The example
was chosen to illustrate the use of Lagrange’s undetermined multiplier without confusing
the reader with a complicated physical system.
■

1116
Chapter 22 Calculus of Variations
Example 22.4.3
SCHRÖDINGER WAVE EQUATION
As a ﬁnal illustration of a constrained minimum, let us ﬁnd the Euler equations for the
quantum mechanical problem of a particle of mass m subject to a potential V,
δJ =
Z
ψ∗(r)Hψ(r)d3r,
(22.90)
with the constraint that ψ is the normalized wave function of a bound state:
Z
ψ∗(r)ψ(r)d3r = 1.
(22.91)
Equation (22.90) is a statement that the energy of the system is stationary, with H its
Hamiltonian operator
H = −¯h2
2m ∇2 + V (r).
(22.92)
In Eq. (22.90) ψ and ψ∗are dependent variables; since they are in principle complex we
can treat each as a separate variable; this point was discussed in Chapter 5, footnote 3.
The integrand in Eq. (17.121) involves second derivatives, but it is convenient to convert
them to ﬁrst derivatives using Green’s theorem, Eq. (3.86):
Z
ψ∗(r)∇2ψ(r)d3r =
Z
S
ψ∗∇ψ · dσ −
Z
∇ψ∗· ∇ψd3r.
We now observe that the surface terms vanish due to the requirement that ψ be continuous,
and our variational principle becomes
δ
Z "
¯h2
2m ∇ψ∗· ∇ψ + V ψ∗ψ
#
d3r = 0.
(22.93)
The function g for our constrained variation is therefore
g = ¯h2
2m ∇ψ∗· ∇ψ + V ψ∗ψ −λψ∗ψ
= ¯h2
2m (ψ∗
x ψx + ψ∗
yψy + ψ∗
z ψz) + V ψ∗ψ −λψ∗ψ,
(22.94)
again using the subscript x to denote ∂/∂x. For yi = ψ∗, our Euler equation becomes
∂g
∂ψ∗−∂
∂x
∂g
∂ψ∗x
−∂
∂y
∂g
∂ψ∗y
−∂
∂z
∂g
∂ψ∗z
= 0.
This yields
V ψ −λψ −¯h2
2m (ψxx + ψyy + ψzz) = 0,
or
−¯h2
2m ∇2ψ + V ψ = λψ.
(22.95)

22.4 Variation with Constraints
1117
The Euler equation for yi = ψ gives the complex conjugate of Eq. (22.95), and therefore
provides no further information. Reference to Eq. (22.92) enables us to identify λ physi-
cally as the energy of the quantum mechanical system. With this interpretation, Eq. (22.95)
is the celebrated Schrödinger wave equation.
■
Rayleigh-Ritz Technique
A number of physically important problems can be related to variational principles of the
general form
δJ = δ
b
Z
a

p(x)y2
x + q(x)y2
dx = 0,
(22.96)
where y(a) and y(b) have ﬁxed values, and the variation is subject to the constraint
b
Z
a
y2w(x)dx = constant.
(22.97)
Treating Eqs. (22.96) and (22.97) as a constrained minimization, its Euler equation takes
the form
d
dx

p(x) dy
dx

−q(x)y + λwy = 0,
(22.98)
where λ is a Lagrange multiplier. This situation usually arises in contexts such that w(x)
is a nonnegative weight function and y(a) and y(b) satisfy Sturm-Liouville boundary con-
ditions, meaning that
p(x)yx y

b
a = 0.
(22.99)
From the above we conclude that although originally introduced as a Lagrange multiplier,
λ must also be an eigenvalue of the Sturm-Liouville system described by Eqs. (22.98) and
(22.99). This identiﬁcation was already noted in Example 22.4.3.
Often problems of the type now under discussion are presented as unconstrained mini-
mizations of the form
δJ = δ


b
Z
a

p(x)y2
x + q(x)y2
dx
b
Z
a
y2w(x)dx


= 0.
(22.100)
Equation (22.100) is equivalent to the earlier formulation because py2
x + qy2 is homoge-
neous in y and the denominator normalizes y without otherwise changing its functional
form. The J satisfying Eq. (22.100) evaluates to the eigenvalue λ.

1118
Chapter 22 Calculus of Variations
In the frequently occurring case that p(x) is actually independent of x, we can manipu-
late the y2
x term in the integrand of J, causing Eqs. (22.96), (22.97), and (22.100) to assume
the useful forms,
δJ = δ
b
Z
a
 −p yxx + q(x)y2
dx = 0,
(constrained minimum),
(22.101)
p d2y
dx2 −q(x)y + λwy = 0,
(22.102)
δJ = δ


b
Z
a
 −p y yxx + q(x)y2
dx
b
Z
a
y2w(x)dx


= 0,
(unconstrained, J = λ).
(22.103)
The Rayleigh-Ritz technique uses the direct evaluation of any one of the above forms for
δJ = 0 as a means of obtaining solutions to the eigenvalue problem shown as Eq. (22.98)
or (22.102). Application of the technique can be as simple as guessing a form for y and
evaluating J, but more accurate results are obtained by taking a form for y(x) that contains
adjustable parameters, and then varying the parameters to minimize J within the param-
eter space. The quality of the results obtained obviously depends on whether the actual
minimum form for y has been well approximated.
Ground State Eigenfunction
Suppose that we seek to compute the ground-state eigenfunction y0 and eigenvalue λ0
of some complicated atomic or nuclear system.9 A classic example, for which no exact
analytical solution has been found, is the helium atom problem. The eigenfunction y0 is
unknown, but we shall assume that we can make a pretty good guess at an approximation
to it, which we will call y. Although we do not know either y0 or any other eigenfunctions
yi (i = 1, 2 ...), or the corresponding eigenvalues λi, we do know, because the eigenfunc-
tions can be chosen to form a complete orthogonal set, that we can write the expansion
y = c0y0 +
∞
X
i=1
ci yi.
(22.104)
We shall assume that we picked y sensibly enough that it is not orthogonal to the ground
state, so c0 ̸= 0. Invoking the orthogonality property, Ey, the expectation value of the
9This means that λ0 is the smallest eigenvalue.

22.4 Variation with Constraints
1119
energy for wave function y, is
Ey = ⟨y|H|y⟩
⟨y|y⟩
=
∞
X
i=0
|ci|2λi
∞
X
i=0
|ci|2
,
(22.105)
where H, the operator deﬁning the Schrödinger equation, typically has the form
H = −¯h2
2m
d2
dx2 + V (x).
The Schrödinger equation and its approximate solution Ey are then seen to correspond to
Eqs. (22.102) and (22.103). The ﬁnal member of Eq. (22.105) results from the substitution
of Eq. (22.104). This substitution is similar to that carried out in Eq. (6.30), but note that
in that equation the function (there called ψ) was assumed normalized. As we already
observed in Section 6.4, the expression for Ey is a weighted average of the eigenvalues
(with all the weights |ci|2 ≥0), so Ey must be at least as large as y0, and in fact must be
larger if y contains any admixture of eigenfunctions whose λi are larger than λ0.
It is useful to scale y so c0 = 1 and rearrange Eq. (22.105) to
Ey = λ0 +
∞
X
i=1
c2
i λi
1 +
∞
X
i=1
c2
i
,
(22.106)
a form that makes clear that the error in Ey will be quadratic in the ci, even though the
difference between y and y0 is linear in the ci.
Our analysis therefore contains two important results.
(1)
Whereas the error in the eigenfunction y was O(ci), the error in λ is only O(c2
i ).
Even a poor approximation of the eigenfunctions may yield an accurate calcula-
tion of the eigenvalue.
(2)
If λ0 is the lowest eigenvalue (ground state), then Ey > λ0, so our approximation
is always on the high side, but converges to λ0 as our approximate eigenfunction
y improves (ci →0).
In practical problems in quantum mechanics, y often depends on parameters that may be
varied to minimize Ey and thereby improve the estimate of the ground-state energy λ0.
This is the “variational method” discussed in quantum mechanics texts. It was illustrated
in Example 8.4.1.
Example 22.4.4
QUANTUM OSCILLATOR
The ground state of a quantum-mechanical particle of mass m constrained to the region
0 ≤x < ∞and subject also to a potential V = kx2/2 is described (in a unit system with

1120
Chapter 22 Calculus of Variations
¯h = 1) by the lowest-eigenvalue eigenstate of the Schrödinger equation,
−1
2m
d2ψ
dx2 + kx2
2 ψ = Eψ,
(22.107)
subject to the boundary conditions ψ(0) = ψ(∞) = 0. A guessed wave function consis-
tent with the boundary conditions is y(x) = xe−αx. Let’s ﬁnd the value of α making the
approximate eigenvalue a minimum.
Our Schrödinger equation is of the type represented by Eq. (22.102), so we can use
Eq. (22.103) and ﬁnd the unconstrained value of J as given there with p = 1/2m, q =
kx2/2, w = 1, and integration range (0,∞). Noting that yxx = α(αx −2)e−αx, we have
J =
∞
Z
0

−αx
2m (αx −2) + kx4
2

e−2αxdx
∞
Z
0
x2e−2αxdx
=
1
8mα + 3k
8α5
1
4α3
= α2
2m + 3k
2α2 .
(22.108)
Differentiating Eq. (22.108) with respect to α2 and setting the result to zero, we get
1
2m −3k
2α4 = 0,
or
α = (3mk)1/4.
Inserting this α value into the expression for J, Eq. (22.108), we ﬁnd
J = (3mk)1/2
2m
+
3k
2(3mk)1/2 =
r
3k
m ≈1.732
r
k
m .
(22.109)
This value of J is an upper bound to the ground-state energy, the exact value of which is
1.5√k/m.
Taking a somewhat more complicated (and ﬂexible) wave function, of the form y = (x +
cx2)e−αx, and optimizing both α and c, the approximate energy improves to 1.542√k/m .
The approximate wave functions of this example are compared with the exact wave func-
tion in Fig. 22.11. Note that the second approximation yields an eigenvalue that is in error
2
Exact
Exact
Approx
Approx
4
6
2
4
6
FIGURE 22.11
Exact and approximate ground-state wave functions for quantum
oscillator, Example 22.4.4, plotted for k/m = 1. Left: Single-term approximation
y = xe−αx. Right: Two-term approximation y = (x + cx2)e−αx.

22.4 Variation with Constraints
1121
by less than 3%, even though the approximate wave function exhibits considerably larger
relative errors.
■
Example 22.4.5
VARIATION OF LINEAR PARAMETERS
A frequent use of the Rayleigh-Ritz technique is the approximation of an eigenfunction of
a Schrödinger equation,
Hψ(x) = Eψ(x),
as a truncated expansion in a ﬁxed orthonormal set of functions. The advantage of this pro-
cedure is that the parameters in the wave function all occur linearly, and the optimization
reduces to a matrix eigenvalue problem.
Given an approximate function (often called a trial function) of the form
y(x) =
N
X
i=1
ciϕi(x),
(22.110)
we seek to minimize
J = ⟨y|H|y⟩
subject to
⟨y|y⟩= 1.
(22.111)
Again we emphasize that the ϕi have no speciﬁc relation to the eigenfunction we seek;
they are simply members of an orthonormal set that has two desirable features: (1) they are
such that a few of them can provide a reasonable representation of the eigenfunction, and
(2) they are tractable in the sense that it is convenient to evaluate the matrix elements we
are about to deﬁne.
Deﬁning a matrix H of elements Hi j = ⟨ϕi|H|ϕ j⟩and a column vector c with compo-
nents ci, we can restate Eq. (22.111) as the minimization of
J = c†Hc
subject to
c†c = 1.
(22.112)
This formulation, in turn, can be reduced using Lagrangian multipliers to the unconstrained
matrix eigenvalue problem
Hc = λc,
(22.113)
which we can solve (using matrix methods) for λ (the approximate value of J). This ap-
plication of the Rayleigh-Ritz technique is therefore seen to be equivalent to the approxi-
mation of an operator equation by a ﬁnite matrix equation.
■
Exercises
22.4.1
A particle of mass m is on a frictionless horizontal surface. In terms of plane polar
coordinates (r,θ), it is constrained to move so that θ = ωt (accomplished by pushing
it with a rotating radial arm against which it can slide frictionlessly). With the initial
conditions
t = 0,
r = r0,
˙r = 0:

1122
Chapter 22 Calculus of Variations
(a)
Find the radial position as a function of time.
ANS.
r(t) = r0 coshωt.
(b)
Find the force exerted on the particle by the constraint.
ANS.
F(c) = 2m˙rω = 2mr0ω2 sinhωt.
22.4.2
A point mass m is moving on a ﬂat, horizontal, frictionless plane. The mass is con-
strained by a string to move radially inward at a constant rate. Using plane polar coor-
dinates (r,θ), r = r0 −kt:
(a)
Set up the Lagrangian.
(b)
Obtain the constrained Lagrange equations.
(c)
Solve the θ-dependent Lagrange equation to obtain ω(t), the angular velocity.
What is the physical signiﬁcance of the constant of integration that you get from
your “free” integration?
(d)
Using the ω(t) from part (b), solve the r-dependent (constrained) Lagrange equa-
tion to obtain λ(t). In other words, explain what is happening to the force of
constraint as r →0.
22.4.3
A ﬂexible cable is suspended from two ﬁxed points. The length of the cable is ﬁxed.
Find the curve that will minimize the total gravitational potential energy of the cable.
ANS.
Hyperbolic cosine.
22.4.4
A ﬁxed volume of water is rotating in a cylinder with constant angular velocity ω. Find
the curve of the water surface that will minimize the total potential energy of the water
in the combined gravitational-centrifugal force ﬁeld.
ANS.
Parabola.
22.4.5
(a)
Show that for a ﬁxed-length perimeter the plane ﬁgure with maximum area is a
circle.
(b)
Show that for a ﬁxed planar area the boundary with minimum perimeter is a circle.
Hint. The radius of curvature R is given by
R =
(r2 + r2
θ )3/2
rrθθ −2r2
θ −r2 .
Note. The problems of this section, variation subject to constraints, are often called
isoperimetric. The term arose from problems of maximizing area subject to a ﬁxed
perimeter, as in part (a) of this problem.
22.4.6
Show that requiring J, given by
J =
b
Z
a
b
Z
a
K(x,t)ϕ(x)ϕ(t)dx dt,

22.4 Variation with Constraints
1123
to have a stationary value subject to the normalizing condition
b
Z
a
ϕ2(x)dx = 1
leads to a Hilbert-Schmidt integral equation of the form given in Eq. (20.64).
Note. The kernel K(x,t) is symmetric.
22.4.7
An unknown function satisﬁes the differential equation
y′′ +
π
2
2
y = 0
and the boundary conditions y(0) = 1, y(1) = 0.
(a)
Calculate the approximation λ = F[ytrial] for ytrial = 1 −x2.
(b)
Compare with the exact eigenvalue.
ANS.
(a) λ = 2.5.
(b) λ/λexact = 1.013.
22.4.8
In Exercise 22.4.7 use a trial function y = 1 −xn.
(a)
Find the value of n that will minimize F[ytrial].
(b)
Show that the optimum value of n drives the ratio λ/λexact down to 1.003.
ANS.
(a) n = 1.7247.
22.4.9
A quantum mechanical particle in a sphere (Example 14.7.1) satisﬁes
∇2ψ + k2ψ = 0,
with k2 = 2mE/¯h2. The boundary condition is that ψ = 0 at r = a, where a is the
radius of the sphere. For the ground state [where ψ = ψ(r)], try an approximate wave
function,
ψa(r) = 1 −
r
a
2
,
and calculate an approximate eigenvalue k2
a.
Hint. To determine p(r) and w(r), put your equation in self-adjoint form (in spherical
polar coordinates).
ANS.
k2
a = 10.5
a2 ,
k2
exact = π2
a2 .
22.4.10
The wave equation for a quantum mechanical oscillator may be written as
d2ψ(x)
dx2
+ (λ −x2)ψ(x) = 0,

1124
Chapter 22 Calculus of Variations
with λ = 1 for the ground state; see Eq. (18.17). Take
ψtrial =



1 −x2
a2 , x2 ≤a2
0,
x2 > a2
for the ground-state wave function (with a2 an adjustable parameter) and calculate the
corresponding ground-state energy. How much error do you have?
Note. Your parabola is really not a very good approximation to a Gaussian exponential.
What improvements can you suggest?
22.4.11
The Schrödinger equation for a central potential may be written as
Lu(r) + ¯h2l(l + 1)
2Mr2
u(r) = Eu(r).
The l(l + 1) term, the angular momentum barrier, comes from splitting off the angu-
lar dependence. Compare Eq. (9.80) (divide that equation through by −r2). Use the
Rayleigh-Ritz technique to show that E > E0, where E0 is the energy eigenvalue of
Lu0 = E0u0 corresponding to l = 0. This means that the ground state will have l = 0,
zero angular momentum.
Hint. You can expand u(r) as u0(r) + P∞
i=1 ciui, where Lui = Eiui, Ei > E0.
Additional Readings
Bliss, G. A., Calculus of Variations. The Mathematical Association of America. LaSalle, IL: Open Court Pub-
lishing Co. (1925). As one of the older texts, this is still a valuable reference for details of problems such as
minimum-area problems.
Courant, R., and H. Robbins, What Is Mathematics? 2nd ed. New York: Oxford University Press (1996). Chapter
VII contains a ﬁne discussion of the calculus of variations, including soap-ﬁlm solutions to minimum-area
problems.
Ewing, G. M., Calculus of Variations with Applications. New York: Norton (1969). Includes a discussion of
sufﬁciency conditions for solutions of variational problems.
Lanczos, C., The Variational Principles of Mechanics, 4th ed. Toronto: University of Toronto Press (1970),
reprinted, Dover (1986). This book is a very complete treatment of variational principles and their applications
to the development of classical mechanics.
Sagan, H., Boundary and Eigenvalue Problems in Mathematical Physics. New York: Wiley (1961), reprinted,
Dover (1989). This delightful text could also be listed as a reference for Sturm-Liouville theory, Legendre and
Bessel functions, and Fourier series. Chapter 1 is an introduction to the calculus of variations, with applications
to mechanics. Chapter 7 picks up the calculus of variations again and applies it to eigenvalue problems.
Sagan, H., Introduction to the Calculus of Variations. New York: McGraw-Hill (1969), reprinted, Dover (1983).
This is an excellent introduction to the modern theory of the calculus of variations, which is more sophisticated
and complete than his 1961 text. Sagan covers sufﬁciency conditions and relates the calculus of variations to
problems of space technology.
Weinstock, R., Calculus of Variations. New York: McGraw-Hill (1952), New York: Dover (1974). A detailed,
systematic development of the calculus of variations and applications to Sturm-Liouville theory and physical
problems in elasticity, electrostatics, and quantum mechanics.
Yourgrau, W., and S. Mandelstam, Variational Principles in Dynamics and Quantum Theory, 3rd ed. Philadel-
phia: Saunders (1968), New York: Dover (1979). This is a comprehensive, authoritative treatment of vari-
ational principles. The discussions of the historical development and the many metaphysical pitfalls are of
particular interest.

CHAPTER 23
PROBABILITY AND
STATISTICS
Probabilities arise in many problems dealing with random events or large numbers of parti-
cles deﬁning random variables. An event is called random if it is practically impossible to
predict from the initial state. This includes cases where we have merely incomplete infor-
mation about initial states and/or the dynamics. For example, in statistical mechanics we
deal with systems containing large numbers of particles, but our knowledge is ordinarily
limited to a few average or macroscopic quantities such as the total energy, the volume,
the pressure, or the temperature. Because the values of these macroscopic variables are
consistent with very large numbers of different microscopic conﬁgurations of our system,
we are prevented from predicting the behavior of individual atoms or molecules. Often the
average properties of many similar events are predictable, as in quantum theory. This is
why probability theory can be and has been developed.
Random variables are also involved when data depend on chance, such as weather re-
ports and stock prices. The theory of probability describes mathematical models of chance
processes in terms of probability distributions of random variables that describe how some
“random events” are more likely than others. In this sense, probability is a measure of our
ignorance, giving quantitative meaning to qualitative statements, such as “It will probably
rain tomorrow” and “I’m unlikely to draw the queen of hearts.” Probabilities are of fun-
damental importance in quantum mechanics and statistical mechanics and are applied in
meteorology, economics, games, and many other areas of daily life.
Because experiments in the sciences are always subject to measurement errors, theories
of errors and their propagation involve probabilities. Statistics is the area of mathematics
that connects observations on data samples to inferences about the probable content of
the entire population from which the sample(s) came. It is an extensive and sophisticated
branch of mathematics, and in the present text only a few of the most basic concepts can
be presented. The material found here may be adequate to provide a conceptual basis for
statistical mechanics, but can at best be an elementary introduction to the ideas needed to
1125
Mathematical Methods for Physicists.
© 2013 Elsevier Inc. All rights reserved.

1126
Chapter 23 Probability and Statistics
gain maximum information from data-intensive experimental studies such as those arising
from the study of cosmic rays or the data from high-energy particle accelerators. A more
complete picture of the role of statistics in physics and engineering can be obtained from a
number of the texts in the Additional Readings.
23.1
PROBABILITY: DEFINITIONS, SIMPLE
PROPERTIES
All possible mutually exclusive1 outcomes of an experiment that is subject to chance
represent the events (or points) of a sample space S. Suppose we toss a coin, and record
that it lands either “heads” or “tails.” These are mutually exclusive events, so our sample
space for a single coin toss can be deemed to be spanned by a discrete random variable x,
with possible values xi, which (based on our experiment, called a trial), will have one of
the two values x1 (for heads) or x2 (for tails). Now suppose, with the same sample space,
we carry out larger numbers of trials. Some will have the result x1 (heads), others x2 (tails).
It is of interest to deﬁne the probability of an outcome in our sample space by the ratio
P(xi) ≡number of times event xi occurs
total number of trials
,
(23.1)
where it is assumed that the number of trials is large enough that P(xi) approaches a
constant limiting value. In the event that we are able to enumerate all the possible events
that produce outcomes in our sample space and can also assume that each event is equally
likely, we may then deﬁne the theoretical probability of an outcome xi as
P(xi) ≡number of outcomes xi
total number of all events.
(23.2)
An example of the use of this theoretical probability can be illustrated using coin tosses.
For example, suppose that we toss a coin twice and take our random variable x to be the
number of heads obtained in a two-toss trial. Our sample S now contains three possible
values of x, which we designate x0, x1, x2, where we are now letting xi stand for the
occurrence of i heads in the two tosses. Obviously, the only possible values of x are 0, 1,
and 2. But we also know that the four possible results of two successive tosses are (heads,
then heads), (heads, then tails), (tails, then heads), (tails, then tails); these possibilities are
mutually exclusive and it is reasonable to assume that they are equally likely. Then, using
Eq. (23.2), we conclude that the probabilities of x2 (two heads) and x0 (no heads) will each
be 1/4, while the probability of x1 (one heads) will be 1/2.
The experimental deﬁnition, Eq. (23.1), is the more appropriate when the total number
of events is not well deﬁned (or is difﬁcult to obtain) or we cannot identify equally likely
outcomes. A large, thoroughly mixed pile of black and white sand grains of the same size
and in equal proportions is a relevant example, because it is impractical to count them all.
But we can count the grains in a small sample volume that we pick. This way we can check
that white and black grains turn up with roughly equal probability 1/2, provided that we
put back each sample and mix the pile again. It is found that the larger the sample volume,
1This means that given that one particular event did occur, the others could not have occurred.

23.1 Probability: Deﬁnitions, Simple Properties
1127
the smaller the spread in probability about 1/2 will be. Moreover, the more trials we run,
the closer the average of all the individual trial probabilities will be to 1/2. We could even
pick single grains and check if the probability 1/4 of picking two black grains in a row
equals that of two white grains, etc. There are lots of statistics questions we can pursue.
Thus, piles of colored sand provide for instructive experiments.
The following axioms are self-evident.
•
Probabilities satisfy 0 ≤P ≤1. Probability 1 means certainty; probability 0 means
impossibility.
•
The entire sample has probability 1. For example, drawing an arbitrary card from a
deck of cards has probability 1.
•
The probabilities for mutually exclusive events add. The probability for getting exactly
one head in two coin tosses is 1/4 + 1/4 = 1/2 because it is 1/4 for head ﬁrst and then
tail, plus 1/4 for tail ﬁrst and then head.
Example 23.1.1
PROBABILITY FOR A OR B
Before proceeding with this example, we must clarify the deﬁnition of “or.” In probability
theory, “A or B” means A, B, or both A and B. The speciﬁcation “A or B but not both”
is referred to as the exclusive or of A and B (sometimes abbreviated xor).
What is the probability for drawing a club or a jack from a shufﬂed deck of cards?2 To
answer this question we need to identify equally probable mutually exclusive events. We
note that because there are 52 cards in a deck, the drawing of each being equally likely
(with 13 cards for each suit and 4 jacks), there are 13 clubs including the club jack, and 3
other jacks; that is, there are 16 mutually exclusive draws that meet our speciﬁcation out
of the total of 52, giving the probability (13 + 3)/52 = 16/52 = 4/13.
■
Sets, Unions, and Intersections
If we represent a sample space by a set S of points, then events meeting certain speciﬁca-
tions can be identiﬁed as subsets A, B,... of S, denoted as A ⊂S, etc. Two sets A, B are
equal if A is contained in B, denoted A ⊂B, and B is contained in A, denoted B ⊂A.
The union A ∪B consists of all points (events) that are in A or B or both (see Fig. 23.1).
The intersection A ∩B consists of all points that are in both A and B. If A and B have no
common points, their intersection is the empty set (which has no elements), and we write
A ∩B = ∅. The set of points in A that are not in the intersection of A and B is denoted
by A −A ∩B, thereby deﬁning a subtraction of sets. If we take the club suit in Exam-
ple 23.1.1 as set A and the four jacks as set B, then their union comprises all clubs and
jacks, and their intersection is the club jack only.
Each subset A has its probability P(A) ≥0. In terms of these set-theory concepts and
notations, the probability laws we just discussed become
0 ≤P(A) ≤1.
2Note that these events are not mutually exclusive.

1128
Chapter 23 Probability and Statistics
A
B
FIGURE 23.1
The shaded area gives the intersection A ∩B, corresponding to the A and
B event sets; the dashed line encloses A ∪B, corresponding to the event set A or B.
The entire sample space has P(S) = 1. The probability of the union A ∪B of mutually
exclusive events is the sum
P(A ∪B) = P(A) + P(B),
where
A ∩B = ∅.
(23.3)
The addition rule for probabilities of arbitrary sets is given by the following theorem:
Addition rule:
P(A ∪B) = P(A) + P(B) −P(A ∩B).
(23.4)
To prove Eq. (23.4), we write the union as two mutually exclusive sets: A ∪B = A ∪
(B −B ∩A), where we have subtracted the intersection of A and B from B before joining
them. The respective probabilities of these mutually exclusive sets are P(A) and P(B) −
P(B ∩A), which we add. We could also have written A ∪B = (A −A ∩B) ∪B, from
which our theorem follows similarly by adding these probabilities: P(A ∪B) = [P(A) −
P(A ∩B)] + P(B). Note that A ∩B = B ∩A. The relationships among these sets can be
checked by referring to Fig. 23.1.
Sometimes the rules and deﬁnitions of probabilities that we have discussed so far are
not sufﬁcient, and we need to introduce the notion of conditional probability. Let A and B
denote sets of events in our sample space. The conditional probability P(B|A) is deﬁned
to be the probability that an event which is a member of A is also a member of B. To
understand the need for this somewhat formal deﬁnition, consider the following example.
Example 23.1.2
CONDITIONAL PROBABILITY
Consider a box of 10 identical red and 20 identical blue pens, from which we remove pens
successively in a random order without putting them back. Suppose we draw a red pen ﬁrst,
event R, followed by the draw of a blue pen, event B. One way to compute P(R, B) is
to note that our sample space consists of 30 × 29 mutually exclusive and equally probable
points (each a two-event ordered sequence), of which 10 × 20 meet our speciﬁcations,
leading to the computation P(R, B) = (10 × 20)/(30 × 29) = 20/87. Note that in this
example, P(R, B) refers to ordered events.
Another way of making the same computation is to start by noting that the initial drawing
of a red pen will occur with probability P(R) = 10/30. But now the probability of drawing
a blue pen in the next round, event B, however, will depend on the fact that we drew a red
pen in the ﬁrst round, and is given by the conditional probability P(B|R). Since there are

23.1 Probability: Deﬁnitions, Simple Properties
1129
now 29 pens of which 20 are blue, we easily compute P(B|R) = 20/29, and the probability
of the sequence “red, then blue” is
P(R, B) = 10
30
20
29 = 20
87,
(23.5)
equal to the result we obtained previously.
■
The generalization of the result in Eq. (23.5) is the very useful formula
P(A, B) = P(A)P(B|A),
(23.6)
which has the obvious interpretation that the probability that A and B both occur can be
written as the probability of A, multiplied by the conditional probability P(B|A) that B
occurs, given the occurrence of A.
Two observations relative of Eq. (23.6) are in order. First, it can be rearranged to reach
an explicit formula for P(B|A):
P(B|A) = P(A, B)
P(A) .
(23.7)
Second, if the conditional probability P(B|A) = P(B) is independent of A, then the events
A and B are called independent, and the combined probability is simply the product of
both probabilities, or
P(A, B) = P(A)P(B),
(A and B independent).
(23.8)
If A and B are deﬁned in a way that neither depends on the other (a condition not
satisﬁed in Example 23.1.2), we can rewrite Eq. (23.7) as
P(B|A) = P(A ∩B)
P(A)
.
(23.9)
Example 23.1.3
SCHOLASTIC APTITUDE TESTS
Colleges and universities rely on the verbal and mathematics SAT scores, among others, as
predictors of a student’s success in passing courses and graduating. A research university
is known to admit mostly students with a combined verbal and mathematics score of 1400
points or more. The graduation rate is 95%; that is, 5% drop out or transfer elsewhere. Of
those who graduate, 97% have an SAT score of at least 1400 points, while 80% of those
who drop out have an SAT score below 1400. Suppose a student has an SAT score below
1400. What is his/her probability of graduating?
Let A represent all students with an SAT test score below 1400, and let B represent
those with scores ≥1400. These are mutually exclusive events with P(A) + P(B) = 1.
Let C represent those students who graduate, and let ˜C represent those who do not. Our
problem here is to determine the conditional probabilities P(C|A) and P(C|B). To apply
Eq. (23.9) we need the four probabilities P(A), P(B), P(A ∩C), and P(B ∩C).
Among the 95% of students who graduate, 3% are in set A and 97% are in set B, so
P(A ∩C) = (0.95)(0.03) = 0.0285,
P(B ∩C) = (0.95)(0.97) = 0.9215.

1130
Chapter 23 Probability and Statistics
Among the 5% of students who do not graduate, 80% are in set A and 20% are in set B, so
P(A ∩˜C) = (0.05)(0.80) = 0.0400,
P(B ∩˜C) = (0.05)(0.20) = 0.0100.
Since P(A) = P(A ∩C) + P(A ∩˜C), and likewise for P(B), we have
P(A) = 0.0285 + 0.0400 = 0.0685,
P(B) = 0.9215 + 0.0100 = 0.9315.
Now, applying Eq. (23.9), we obtain the ﬁnal results
P(C|A) = P(A ∩C)
P(A)
= 0.0285
0.0685 ≈41.6%,
P(C|B) = P(B ∩C)
P(B)
= 0.9215
0.9315 ≈98.9%;
that is, a little less than 42% is the probability for a student with a score below 1400 to
graduate at this particular university.
■
As a corollary to the equation for conditional probability, Eq. (23.9), we now compare
P(A|B) = P(A ∩B)/P(B) and P(B|A) = P(A ∩B)/P(A), obtaining a result known as
Bayes’ theorem:
P(A|B) = P(A)
P(B) P(B|A).
(23.10)
Bayes’ theorem is a special case of the following more general theorem:
If the random events Ai with probabilities P(Ai) > 0 are mutually exclusive and their
union represents the entire sample S, then an arbitrary random event B ⊂S has the
probability
P(B) =
n
X
i=1
P(Ai)P(B|Ai).
(23.11)
The decomposition law given by Eq. (23.11) resembles the expansion of a vector into a ba-
sis of unit vectors deﬁning its components. This relation follows from the obvious decom-
position B = ∪i(B ∩Ai) (this notation indicates the union of all the quantities B ∩Ai, see
Fig. 23.2), which implies P(B) = P
i P(B ∩Ai) because the components B ∩Ai are mu-
tually exclusive. For each i, we know from Eq. (23.9) that P(B ∩Ai) = P(Ai)P(B|Ai),
which proves the theorem.
Counting Permutations and Combinations
Counting the events in samples can help us ﬁnd probabilities; this procedure is found to be
of great importance in statistical mechanics.
If we have n different molecules, let us ask in how many ways we can arrange them in
a row, that is, permute them. This number is deﬁned as the number of their permutations.
Thus, by deﬁnition, the order matters in permutations. There are n choices of picking
the ﬁrst molecule, n −1 for the second, etc. Altogether there are n! permutations of n
different molecules or objects.

23.1 Probability: Deﬁnitions, Simple Properties
1131
B
A1
A2
A3
FIGURE 23.2
The shaded area B is composed of mutually exclusive subsets of B
belonging also to A1, A2, A3, where the Ai are mutually exclusive.
Generalizing this, suppose there are n people but only k < n chairs to seat them. In how
many ways can we seat k people in the chairs? Counting as before, we get
n(n −1)···(n −k + 1) =
n!
(n −k)!
(23.12)
for the number of permutations of k objects which can be formed by selection from a set
originally containing n objects.
We now consider the counting of combinations of objects, where the term combination
is deﬁned to refer to sets in which the object order is irrelevant. For example, three letters
a,b,c can be combined, two letters at a time, in three ways: ab, ac, bc. If letters can be
repeated, then we also have the pairs aa, bb, cc and have a total of six combinations.
These examples illustrate the fact that a combination of different particles differs from a
permutation in that the particles’ order does not matter. Combinations may occur with
repetition or without; the essential point is that no two combinations contain the same
particles.
The number of different combinations of n numbered (and thereby distinguishable) par-
ticles, k at a time and without repetitions, is given by the binomial coefﬁcient
n(n −1)···(n −k + 1)
k!
=
n
k

.
(23.13)
To prove Eq. (23.13), we start from the number n!/(n −k)! of permutations in which k
particles were chosen from n, and divide out the number k! of permutations of the group
of k particles because their order does not matter in a combination.
A generalization of the above is a situation in which we have a total of n distinguishable
(numbered) objects, and we place n1 of these into Box 1, n2 into Box 2, etc. We wish to
know how many different ways this can be done (this is a combination problem because
the objects in each box do not form ordered sets). A simple way to solve this problem is to
identify each permutation of the n objects with an assignment into boxes; the ﬁrst n1 of the
permuted objects is placed in Box 1, the next n2 in Box 2, etc. However, permutations
that differ only in the ordering of objects destined for the same box do not constitute
different distributions, so the total number of distributions will be n! (the overall number

1132
Chapter 23 Probability and Statistics
of permutations) divided by n1!, n2!, etc. Thus, our overall formula is
B(n1,n2,...) =
n!
n1!n2!....
(23.14)
This quantity is sometimes referred to as a multinomial coefﬁcient; if there were only two
boxes it reduces to the binomial coefﬁcient.
For a related problem with repetition, suppose that we have an unlimited supply of
particles bearing each number from 1 through k. Then the number of distinct ways in
which n particles can be chosen can be shown to be
n + k −1
n

=
n + k −1
k −1

.
(23.15)
The following example provides a proof of Eq. (23.15).
Example 23.1.4
COMBINATIONS WITH REPETITION
The physical relevance of the situation giving rise to Eq. (23.15) is that it is mathematically
equivalent to the number of ways that n identical, indistinguishable particles can be placed
in k boxes. To see that these problems are equivalent, note that the number on each particle
of Eq. (23.15) can be used to identify the box in which that particle will be placed.
A simple way to count the possible assignments is to consider the distinguishable ways
that n indistinguishable particles and k −1 indistinguishable partitions can be placed in a
line containing n + k −1 items. The particles (if any) that occur in the line earlier than
the ﬁrst partition are assigned to Box 1; those between the ﬁrst and second partitions are
assigned to Box 2, etc., with the particles (if any) occurring later than the (k −1)th (the
last) partition are assigned to Box k. Each different placement of the partitions yields a
unique assignment of particles to boxes, and the number of different partition placements
is the number of combinations given by the binomial coefﬁcient in Eq. (23.15).
■
In statistical mechanics, we frequently need to know the number of ways in which it
is possible to put n particles in k boxes subject to various additional speciﬁcations. If we
are working in classical theory, our more complete speciﬁcation includes the notion that
the particles are distinguishable, and we refer to the probability computation as that given
by Maxwell-Boltzmann statistics. In the quantum domain, it is assumed that identical
particles are inherently indistinguishable; in fact, we cannot even identify them by their
trajectories, as the notion of path is blurred by the Heisenberg uncertainty principle. This
indistinguishability leads to the requirement that many-particle states must have symmetry
under the interchange of identical particles, and in nature we ﬁnd two cases: Either the
wave function is symmetric under interchange of the coordinates of a pair of identical
particles (such particles are said to exhibit Bose-Einstein statistics), or the coordinate
interchange causes a reversal in the sign of the wave function (the case called Fermi-
Dirac statistics). The symmetry (or antisymmetry) under particle interchange inﬂuences
the way in which particles can be assigned to states (boxes): In Bose-Einstein statistics

23.1 Probability: Deﬁnitions, Simple Properties
1133
any number of indistinguishable particles may be placed in the same box; in Fermi-Dirac
statistics no box may contain more than one indistinguishable particle.
Application of the various kinds of statistics in general problems is outside the scope of
this text; however, the basic case in which we simply count the number of assignments that
are possible is easily approached. If we have n particles and k available states:
•
In Maxwell-Boltzmann (classical) statistics, the number of possible assignments of
particles to states is kn (each particle can independently be assigned to any state).
•
In Bose-Einstein statistics, the number of possible assignments is given by Eq. (23.15).
•
In Fermi-Dirac statistics, the number of possible assignments is
 k
n

. This formula gives
the number of ways that n of the k states can be selected for occupancy. Note that the
number of assignments is zero if n > k, indicating that we cannot make any assignment
(with a maximum of one particle per state) unless there are at least as many states as
there are particles.
Exercises
23.1.1
A card is drawn from a shufﬂed deck. (a) What is the probability that it is black, (b) a red
nine, (c) or a queen of spades?
23.1.2
Find the probability of drawing two kings from a shufﬂed deck of cards (a) if the ﬁrst
card is put back before the second is drawn, and (b) if the ﬁrst card is not put back after
being drawn.
23.1.3
When two fair dice are thrown, what is the probability of (a) observing a number less
than 4, or (b) a number greater than or equal to 4 but less than 6?
23.1.4
Rolling three fair dice, what is the probability of obtaining six points?
23.1.5
Determine the probability P(A ∩B ∩C) in terms of P(A), P(B), P(C), P(A ∪B),
P(A ∪C), P(B ∪C), and P(A ∪B ∪C).
23.1.6
Determine directly or by mathematical induction (Section 1.4) the probability of a dis-
tribution of N (Maxwell-Boltzmann) particles in k boxes with N1 in Box 1, N2 in
Box 2,..., Nk in the kth box for any numbers N j ≥1 with N1 + N2 + ··· + Nk =
N, k < N. Repeat this for Fermi-Dirac and Bose-Einstein particles.
23.1.7
Show that P(A ∪B ∪C) = P(A) + P(B) + P(C) −P(A ∩B) −P(A ∩C) −
P(B ∩C) + P(A ∩B ∩C).
23.1.8
Determine the probability that a positive integer n ≤100 is divisible by a prime number
p ≤100. Verify your result for p = 3,5,7.
23.1.9
Put two particles obeying Maxwell-Boltzmann (Fermi-Dirac, or Bose-Einstein) statis-
tics in three boxes. How many ways of doing so are there in each case?

1134
Chapter 23 Probability and Statistics
23.2
RANDOM VARIABLES
In this section we deﬁne properties that characterize the probability distributions of random
variables, by which we mean variables that will assume various numerical values with
individual probabilities. Thus, the name of a color (e.g., “black” or “white”) cannot be
the value assigned a random variable, but we can deﬁne a random variable to have one
numerical value for “black” and another for “white”; the usefulness of our deﬁnition may
depend on the problem we are attempting to solve.
Having deﬁned a random variable and given its distribution, we are interested in partic-
ular in its mean or average value, and in measures of the width or spread of its values.
The width is of particular importance when the random variable represents repeated mea-
surements of the same quantity but subject to experimental error. In addition, we introduce
properties that characterize the extent to which the value of one random variable depends
on (i.e., is correlated with) those of another.
Random variables can be discrete, as for example those introduced in the previous sec-
tion to describe the outcomes of coin tosses, or they may be continuous, either inherently
so (as, for example, the wave function in a quantum mechanical system) or because they
consist of so many closely spaced discrete points that it is impractical to work with them
individually.
Example 23.2.1
DISCRETE RANDOM VARIABLE
The possible outcomes of the tossing of a die deﬁne a random variable X with values
x1, x2,..., x6, each with probability 1/6; we can denote this by writing P(xi) = 1/6, i =
1...6.
If we toss two dice and record the sum of the points shown in each trial, then this sum
is also a discrete random variable, which takes on the value 2 when both dice show 1 with
probability (1/6)2; the value 3 in either of the two cases in which one die has 1 and the
other 2, hence with probability (1/6)2 +(1/6)2 = 1/18. Continuing, the value 4 is reached
in three equally probable ways: 2 + 2, 3 + 1, and 1 + 3 with total probability 3(1/6)2 =
1/12; the values 5 and 6 are reached with the respective probabilities 4(1/6)2 = 1/9 and
5(1/6)2 = 5/36; and the value 7 occurs with the maximum probability, 6(1/6)2 = 1/6.
The value 8 is reached in ﬁve ways (6 + 2, 5 + 3, 4 + 4, 3 + 5, 2 + 6), with probability
5(1/6)2 = 5/36, and further increases in x lead to smaller probabilities, ﬁnally at x = 12
reaching probability (1/6)2 = 1/36. This probability distribution is symmetric about
x = 7, and can be represented graphically as in Fig. 23.3 or algebraically as
P(x) = x −1
36
= 6 −(7 −x)
36
,
x = 2, 3,...,7,
P(x) = 13 −x
36
= 6 + (7 −x)
36
,
x = 7, 8,...,12.
■

23.2 Random Variables
1135
P(x)
x
6
36
5
36
4
36
3
36
2
36
1
36
0
1
2
3
4
5
6
7
8
9
10
11
12
FIGURE 23.3
Probability distribution P(x) of the sum of points when
two dice are tossed.
In summary, then,
•
If a discrete random variable X can assume the values xi, each value occurs by chance
with a probability P(X = xi) = pi ≥0 that is a discrete-valued function of the random
variable X, and the probabilities satisfy P
i pi = 1.
•
We deﬁne the probability density f (x) of a continuous random variable X as
P(x ≤X ≤x + dx) = f (x)dx;
(23.16)
that is, f (x)dx is the probability that X lies in the interval x ≤X ≤x + dx. For f (x)
to be a probability density, it has to satisfy f (x) ≥0 and
R
f (x)dx = 1.
•
The generalization to probability distributions depending on several random variables
is straightforward. Quantum physics abounds in examples.
Example 23.2.2
CONTINUOUS RANDOM VARIABLE: HYDROGEN ATOM
Quantum mechanics gives the probability |ψ|2 d3r of ﬁnding a 1s electron in a hydrogen
atom in volume3 d3r, where ψ = Ne−r/a is the 1s wave function, a is the Bohr radius,
and N = (πa3)−1/2 is a normalization constant such that
Z
|ψ|2 d3r = 4πN 2
∞
Z
0
e−2r/ar2dr = πa3N 2 = 1.
3Note that |ψ|24πr2dr gives the probability for the electron to be found between r and r + dr, at any angle.

1136
Chapter 23 Probability and Statistics
The value of this integral can be checked by identifying it as a gamma function:
∞
Z
0
e−2r/ar2dr =
a
2
3
∞
Z
0
e−xx2dx = a3
8 0(3) = a3
4 .
■
Computing Discrete Probability Distributions
In Example 23.2.1 the overall probability of a particular value of a discrete random variable
was computed as a product in which one factor was the number of equally likely ways
in which that value could be obtained, and the other factor was the probability of each
mutually exclusive occurrence. This type of computation arises sufﬁciently frequently that
we should learn how to deal with it in general. Therefore, consider a situation in which
N independent events take place (examples of such events include tosses of an individual
die, selection of a card from a deck, energy state occupied by a molecule, orientation of the
magnetic moment of a particle), and that each such event has one of a set of m mutually
exclusive outcomes (e.g., number showing on the die, identity of the card, energy state, or
magnetic moment orientation).
We assume that the outcomes x1, x2,..., xm of an individual event will have the respec-
tive probabilities p1, p2,..., pm, with p1 + p2 + ··· + pm = 1 (so that we have included
all the possible outcomes). Then, we compute the probability that any n1 of the events have
outcome x1, any n2 events have outcome x2, etc.:
P(n1,n2,...,nm) = B(n1,n2,...,nm)(p1)n1(p2)n2 ...(pm)nm,
(23.17)
where n1 +n2 +···+nm = N, and B(n1,n2,...,nm) is the number of ways that, for each
i, ni of the events have outcome xi.
Now B(n1,n2,...,nm) is just the multinomial coefﬁcient encountered earlier; in the
present context the numbered objects correspond to events numbered from 1 to N and each
box corresponds to an individual-event outcome. Thus, our ﬁnal formula for the probability
of a distribution deﬁned by n1, n2, etc., is
P(n1,n2,...,nm) =
N!
n1!n2!...nm!(p1)n1(p2)n2 ...(pm)nm.
(23.18)
Mean and Variance
When we make n measurements of a quantity x, obtaining the values x j, we deﬁne the
average value
¯x = 1
n
n
X
j=1
x j
(23.19)
of the trials, also called the mean or expectation value, where this formula assumes that
every observed value xi is equally likely and occurs with probability 1/n. This connection

23.2 Random Variables
1137
is the key link of experimental data with probability theory. This observation and practical
experience suggest deﬁning the mean value for a discrete random variable X as
⟨X⟩≡
X
i
xi pi,
(23.20)
while deﬁning the mean value for a continuous random variable x characterized by prob-
ability density f (x) as
⟨X⟩=
Z
x f (x)dx.
(23.21)
Other notations for the mean in the literature are ¯X and E(X).
The use of the arithmetic mean ¯x of n measurements as the average value is suggested
by simplicity and plain experience, again assuming equal probability for each xi. But why
do we not consider the geometric mean
xg = (x1 x2 ... xn)1/n
or the harmonic mean xh determined by the relation
1
xh
= 1
n
 1
x1
+ 1
x1
+ ··· + 1
xn

or the value ˜x that minimizes the sum of absolute deviations |xi −˜x|? Here the xi are taken
to increase monotonically. When we plot O(x) = P2n+1
i=1 |xi −x|, as in Fig. 23.4(a), for
an odd number of points, we realize that it has a minimum at its central value i = n, while
for an even number of points E(x) = P2n
i=1 |xi −x| is ﬂat in its central region, as shown in
Fig. 23.4(b). These properties make these functions unacceptable for determining average
values. Instead, when we minimize (with respect to x) the sum of quadratic deviations,
n
X
i=1
(x −xi)2 = minimum,
(23.22)
(a)
(b)
x1
x2
x3
x1
x2 x3 x4
FIGURE 23.4
(a) P3
i=1 |xi −x| for an odd number of points. (b) P4
i=1 |xi −x| for an
even number of points.

1138
Chapter 23 Probability and Statistics
setting the derivative equal to zero yields 2P
i(x −xi) = 0, or
x = 1
n
X
i
xi ≡¯x,
that is, the arithmetic mean. The arithmetic mean has another important property: If we
denote by vi = xi −¯x the deviations, then P
i vi = 0, that is, the sum of positive deviations
equals the sum of negative deviations. This principle of minimizing the quadratic sum of
deviations, called the method of least squares, is due to C. F. Gauss, among others.
The ability of a mean value to represent a set of data points depends on the spread of the
individual measurements from this mean. Again, we reject the average sum of deviations
Pn
i=1 |xi −¯x|/n as a measure of the spread because it selects the central measurement as
the best value for no good reason. A more appropriate deﬁnition of the spread is based on
the average of the squares of the deviations from the mean. This quantity, known as the
standard deviation, is deﬁned as
σ =
v
u
u
t1
n
n
X
i=1
(xi −¯x)2,
(23.23)
where the square root is motivated by dimensional analysis.
If we square Eq. (23.23) and expand (xi −¯x)2, written as (xi −⟨x⟩)2, we get
nσ 2 =
n
X
i=1
x2
i −2⟨x⟩
n
X
i=1
xi + n⟨x⟩2
= n

⟨x2⟩−⟨x⟩2
.
Dividing through by n, we obtain the very useful formula,
σ 2 = ⟨x2⟩−⟨x⟩2.
(23.24)
Note that these two expectation values are equal only if all the xi have the same value;
for example, if we have two xi, equal, respectively, to ⟨x⟩+ δ and ⟨x⟩−δ, then ⟨x2⟩=
⟨x⟩2 + δ2, so the spread in the xi has caused ⟨x2
i ⟩to increase.
Example 23.2.3
STANDARD DEVIATION OF MEASUREMENTS
From the measurements x1 = 7, x2 = 9, x3 = 10, x4 = 11, x5 = 13, we extract ¯x = 10 for
the mean value and, using Eq. (23.23),
σ =
s
(−3)2 + (−1)2 + 02 + 12 + 32
5
= 2.2361
for the standard deviation, or spread.
■

23.2 Random Variables
1139
There is yet another interpretation of the standard deviation, in terms of the sum of
squares of measurement differences:
X
i<k
(xi −xk)2 = 1
2
n
X
i=1
n
X
k=1

x2
i + x2
k −2xi xk

= 1
2
h
2n2⟨x2⟩−2n2⟨x⟩2i
= n2σ 2.
(23.25)
The last step in the above equation made use of Eq. (23.24).
Now we are ready to generalize the spread in a set of n measurements with equal prob-
ability 1/n to the variance of an arbitrary probability distribution. For a discrete random
variable X with probabilities pi at X = xi, we deﬁne the variance
σ 2 =
X
j
 x j −⟨X⟩
2 p j;
(23.26)
for a continuous probability distribution the deﬁnition becomes
σ 2 =
∞
Z
−∞
(x −⟨X⟩)2 f (x)dx.
(23.27)
We now develop some relationships satisﬁed by random variables:
1.
The variance σ 2 of a random variable X has the property
σ 2 = ⟨X2⟩−⟨X⟩2.
(23.28)
This formula, previously derived as Eq. (23.24) only for a discrete random variable
with all xi equally probable, is true in general. The proof is left as Exercise 23.2.3.
2.
If random variables X and Y are related by the linear equation Y = aX + b, then Y
has mean value ⟨Y⟩= a⟨X⟩+ b and variance σ 2(Y) = a2σ 2(X).
We prove this theorem only for a continuous distribution, leaving the case of a
discrete random variable as an exercise for the reader. Directly from the deﬁnitions,
we have
⟨Y⟩=
∞
Z
−∞
(ax + b) f (x)dx = a⟨X⟩+ b,
where the integral multiplying b simpliﬁes because
R
f (x)dx = 1. For the variance we
similarly obtain
σ 2(Y) =
∞
Z
−∞
(ax + b −a⟨X⟩−b)2 f (x)dx =
∞
Z
−∞
a2(x −⟨X⟩)2 f (x)dx
= a2σ 2(X).

1140
Chapter 23 Probability and Statistics
3.
Probabilities of random variables satisfy the Chebyshev inequality,
P(|x −⟨X⟩| ≥kσ) ≤1
k2 ,
(23.29)
which demonstrates why the standard deviation serves as a measure of the spread of an
arbitrary probability distribution from its mean value ⟨X⟩. We ﬁrst derive the simpler
inequality
P(Y ≥K) ≤⟨Y⟩
K
for a continuous random variable Y with values y restricted to y ≥0. (The proof for a
discrete random variable follows along similar lines.) This inequality follows from
⟨Y⟩=
∞
Z
0
y f (y)dy =
K
Z
0
y f (y)dy +
∞
Z
K
y f (y)dy
≥
∞
Z
K
y f (y)dy ≥K
∞
Z
K
f (y)dy = KP(Y ≥K).
Next we apply the same method to the positive variance integral,
σ 2 =
Z
(x −⟨X⟩)2 f (x)dx ≥
Z
|x−⟨X⟩|≥kσ
(x −⟨X⟩)2 f (x)dx
≥k2σ 2
Z
|x−⟨X⟩|≥kσ
f (x)dx = k2σ 2P(|x −⟨X⟩| ≥kσ),
where we have ﬁrst decreased the right-hand side by omitting the part of the positive
integral with |x −⟨X⟩| ≤kσ and then decreased it further by replacing (x −⟨X⟩)2
in the remaining integral by its minimum value, k2σ 2. We now divide the ﬁrst and
last members of this sequence of inequalities by the positive quantity k2σ 2, thereby
proving the Chebyshev inequality. For k = 3 we have the conventional three-standard-
deviation estimate,
P(|x −⟨X⟩| ≥3σ) ≤1
32 = 1
9.
(23.30)

23.2 Random Variables
1141
Moments of Probability Distributions
It is straightforward to generalize the mean value to higher moments of probability distri-
butions relative to the mean value ⟨X⟩:
D
(X −⟨X⟩)kE
=
X
j
 x j −⟨X⟩
k p j,
discrete distribution,
D
(X −⟨X⟩)kE
=
∞
Z
−∞
(x −⟨X⟩)k f (x)dx,
continuous distribution.
(23.31)
The moment-generating function
⟨et X⟩=
Z
etx f (x)dx = 1 + t⟨X⟩+ t2
2!⟨X2⟩+ ···
(23.32)
is a weighted sum of the moments of the continuous random variable X, which is obtained
by substituting the Taylor expansion of the exponential functions. Therefore,
⟨X⟩= d⟨et X⟩
dt

t=0
,
⟨X2⟩= d2⟨et X⟩
dt2

t=0
,...,
⟨Xn⟩= dn⟨et X⟩
dtn

t=0
.
(23.33)
Note that the moments here are not relative to the expectation value, but are relative to
x = 0; they are called central moments.
Example 23.2.4
MOMENT-GENERATING FUNCTION
Suppose we have four cards, numbered from 1 through 4, from which we draw two at
random and add their numbers. Letting this sum of the drawn numbers be values of a
random variable X, we ﬁnd that X has the following values and respective probabilities
P(x):
P(3) = 1/6,
P(4) = 1/6,
P(5) = 1/3,
P(6) = 1/6,
P(7) = 1/6.
Verifying these probabilities is the topic of Exercise 23.2.1.
The moment-generating function for this system has the form
M = 1
6

e3t + e4t + 2e5t + e6t + e7t
,
and its ﬁrst two derivatives are
M′ = 1
6

3e3t + 4e4t + 10e5t + 6e6t + 7e7t
,
M′′ = 1
6

9e3t + 16e4t + 50e5t + 36e6t + 49e7t
.

1142
Chapter 23 Probability and Statistics
Setting t = 0, we get
⟨X⟩= M′(0) = 5,
⟨X2⟩= M′′(0) = 80
3 .
Thus, the mean of X is found to be 5, and its variance is given by
σ 2 = ⟨X2⟩−⟨X⟩2 = 80
3 −25 = 5
3.
In this example we see that the moment-generating function does (in a systematic way) the
same thing as direct formation of the moments; in a later example, Example 23.3.2, we see
a situation in which the use of the moment-generating function provides an opportunity to
compute moments with rather little computational work.
■
Mean values, central moments, and variance can be deﬁned analogously for probability
distributions that depend on several random variables. We illustrate for the case of two
random variables X and Y , for which the mean values and the variance of each variable
take the forms
⟨X⟩=
∞
Z
−∞
∞
Z
−∞
x f (x, y)dx dy,
⟨Y⟩=
∞
Z
−∞
∞
Z
−∞
y f (x, y)dx dy,
(23.34)
σ 2(X) =
∞
Z
−∞
∞
Z
−∞
(x −⟨X⟩)2 f (x, y)dx dy,
σ 2(Y) =
∞
Z
−∞
∞
Z
−∞
(y −⟨Y⟩)2 f (x, y)dx dy.
(23.35)
Covariance and Correlation
Two random variables are said to be independent if the probability density f (x, y) fac-
torizes into a product f (x)g(y) of probability distributions of one random variable each.
The covariance, deﬁned as
cov(X,Y) = ⟨(X −⟨X⟩)(Y −⟨Y⟩)⟩,
(23.36)

23.2 Random Variables
1143
is a measure of how much the random variables X and Y are correlated (or related): It is
zero for independent random variables because
cov(X,Y) =
Z
(x −⟨X⟩)(y −⟨Y⟩) f (x, y)dx dy
=
Z
(x −⟨X⟩) f (x)dx
Z
(y −⟨Y⟩) g(y)dy
= (⟨X⟩−⟨X⟩)(⟨Y⟩−⟨Y⟩) = 0.
The normalized covariance cov(X,Y)/σ(X)σ(Y), which has values between −1 and +1,
is often called correlation.
In order to demonstrate that the correlation is bounded by
−1 ≤cov(X,Y)
σ(X)σ(Y) ≤1,
we analyze the positive mean value
Q = ⟨[a(X −⟨X⟩) + c(Y −⟨Y⟩)]2⟩
= a2⟨[X −⟨X⟩]2⟩+ 2ac⟨[X −⟨X⟩][Y −⟨Y⟩]⟩+ c2⟨[Y −⟨Y⟩]2⟩
= a2σ(X)2 + 2ac cov(X,Y) + c2σ(Y)2 ≥0.
(23.37)
For this quadratic form to be nonnegative for all values of the constants a and c, its discrim-
inant must satisfy cov(X,Y)2 −σ(X)2σ(Y)2 ≤0, which proves the desired inequality.
The usefulness of the correlation as a quantitative measure is emphasized by the follow-
ing theorem:
The probability P(Y = aX + b) will be unity if, and only if, the correlation
cov(X,Y)/σ(X)σ(Y) is equal to ±1.
This theorem states that a ±100% correlation between X and Y implies not only some
functional relation between both random variables but that the relation between them is
linear.
Our ﬁrst step in proving this theorem is to show that P(Y = aX + b) = 1 (meaning
that Y = aX + b) implies that cov(X,Y)/σ(X)σ(Y) = ±1. For the mean ⟨Y⟩, we simply
compute
⟨Y⟩= ⟨aX + b⟩= a⟨X⟩+ b.
For the variance,
σ(Y)2 = ⟨Y 2⟩−⟨Y⟩2 = ⟨(aX + b)2⟩−(a⟨X⟩+ b)2
= a2⟨X2⟩+ 2ab⟨X⟩+ b2 −

a2⟨X⟩2 + 2ab⟨X⟩+ b2
= a2 
⟨X2⟩−⟨X⟩2
= a2σ(X)2,

1144
Chapter 23 Probability and Statistics
which is equivalent to σ(Y) = ±aσ(X). We also need cov(X,Y), which is
cov(X,Y) = ⟨(X −⟨X⟩)((aX + b) −(a⟨X⟩+ b))⟩
= a

⟨X2⟩−⟨X⟩2
= aσ 2(X) = ±σ(X)σ(Y),
where the last equality was obtained by identifying aσ(X) as ±σ(Y). This result completes
the ﬁrst step in our proof of the theorem.
To complete the proof, we must establish the converse of the relation we have just
proved, namely that cov(X,Y)/σ(X)σ(Y) = ±1 implies P(Y = aX +b) = 1 for some set
of values (a,b). We proceed by forming the quadratic expectation value
*h
(σ(Y)X ∓σ(X)Y) −
D
σ(Y)X ∓σ(X)Y
Ei 2+
,
where the symbol ∓indicates that we choose a sign opposite to that of the correlation
cov(X,Y)/σ(X)σ(Y). Our plan is to show that this expectation value is zero. Since the
expectation value is that of an inherently nonnegative quantity, we may then conclude that
σ(Y)X ∓σ(X)Y is (almost) everywhere equal to its expectation value, the value of which
is some constant C. We therefore have
σ(Y)X ∓σ(X)Y = C,
equivalent to
Y = ±σ(Y)X −C)
σ(X)
,
the linear relation we seek.
It remains to conﬁrm that the quadratic expectation value vanishes. Rearranging it ﬁrst
to the form
*h
σ(Y)(X −⟨X⟩) ∓σ(X)(Y −⟨Y⟩)
i 2+
and then expanding the square, we reach
D
σ(Y)2(X −⟨X⟩)2 + σ(X)2(Y −⟨Y⟩)2 ∓2σ(X)σ(Y)(X −⟨X⟩)(Y −⟨Y⟩)
E
.
Making now the substitutions (X −⟨X⟩)2 = σ(X)2, (Y −⟨Y⟩)2 = σ(Y)2, and

(X −⟨X⟩)
(Y −⟨Y⟩)

= ±σ(X)σ(Y), our quadratic expectation value reduces to zero.
Marginal Probability Distributions
It is sometimes useful to integrate out (i.e., average over) one of the random variables in a
multivariable distribution. When we do so, we are left with the probability distribution of
the other random variables. For a two-variable distribution, we can eliminate either of the
two variables:
F(x) =
Z
f (x, y)dy,
or
G(y) =
Z
f (x, y)dx,
(23.38)
and analogously for discrete probability distributions. When one or more random variables
are integrated out, the remaining probability distribution is called marginal, the name

23.2 Random Variables
1145
motivated by the geometric aspects of projection. It is straightforward to show that these
marginal distributions satisfy all the requirements of properly normalized probability dis-
tributions.
Here is a comprehensive example that illustrates the computation of probability distri-
butions and their mean values, variances, covariance, and correlation.
Example 23.2.5
REPEATED DRAWS OF CARDS
This example deals with independent repeated draws from a deck of playing cards. To
make sure that these events stay independent, we draw the ﬁrst card at random from a
bridge deck containing 52 cards and then put it back at a random place and reshufﬂe the
deck. Now we repeat the process for a second card. Let’s deﬁne the random variables:
•
X = number of so-called honors, that is, tens, jacks, queens, kings, or aces;
•
Y = number of twos or threes.
In a single draw the probability of Event a (drawing an honor) is pa = 20/52 = 5/13,
while the probability of Event b (drawing a two or three) is pb = 2(4/52) = 2/13. The
probability of Event c (drawing anything else) is pc = (13 −5 −2)/13 = 6/13. Since that
exhausts all the mutually exclusive possibilities, we have a + b + c = 1.
In two drawings, it is possible to draw zero, one, or two honors (i.e., x = 0, 1, or 2).
Likewise, we may draw zero, one, or two cards of value 2 or 3 (i.e., y = 0, 1, or 2). But
because we are only drawing two cards, we have the additional condition 0 ≤x + y ≤2.
The probability function P(X = x,Y = y), which we will write in the simpler form
P(x, y), is given by a formula of the type presented in Eq. (23.18), with N (the number
of events) equal to 2 and with the three individual-event probabilities pa, pb, and pc. The
number of events a is x, the number of events b is y, and therefore the number of events c
is 2 −x −y, and, by Eq. (23.18),
P(x, y) =
2!
x! y!(2 −x −y)!(pa)x(pb)y(pc)2−x−y
=
2!
x! y!(2 −x −y)!
 5
13
x  2
13
y  6
13
2−x−y
,
(23.39)
with 0 ≤x + y ≤2. More explicitly, P(x, y) has the following values:
P(0,0) =
 6
13
2
,
P(1,0) = 2 · 5
13 · 6
13 = 60
132 ,
P(2,0) =
 5
13
2
,
P(0,1) = 2 · 2
13 · 6
13 = 24
132 ,
P(0,2) =
 2
13
2
,
P(1,1) = 2 · 5
13 · 2
13 = 20
132 .

1146
Chapter 23 Probability and Statistics
The probability distribution is properly normalized. Its expectation values are given by
⟨X⟩=
X
0≤x+y≤2
x P(x, y) = P(1,0) + P(1,1) + 2P(2,0)
= 60
132 + 20
132 + 2
 5
13
2
= 130
132 = 10
13 = 2pa,
and
⟨Y⟩=
X
0≤x+y≤2
yP(x, y) = P(0,1) + P(1,1) + 2P(0,2)
= 24
132 + 20
132 + 2
 2
13
2
= 52
132 = 4
13 = 2pb.
The values 2pa and 2pb are expected because we are drawing a card two times. The vari-
ances are
σ 2(X) =
X
0≤x+y≤2

x −10
13
2
P(x, y)
=

−10
13
2
[P(0,0) + P(0,1) + P(0,2)] +
 3
13
2
[P(1,0) + P(1,1)] +
16
13
2
P(2,0)
= 102 · 64 + 32 · 80 + 162 · 52
134
= 42 · 5 · 169
134
= 80
132 ,
σ 2(Y) =
X
0≤x+y≤2

y −4
13
2
P(x, y)
=

−4
13
2
[P(0,0) + P(1,0) + P(2,0)] +
 9
13
2
[P(0,1) + P(1,1)] +
22
13
2
P(0,2)
= 42 · 112 + 92 · 44 + 222 · 22
134
= 11 · 4 · 169
134
= 44
132 .
The covariance is given by
cov(X,Y) =
X
0≤x+y≤2

x −10
13

y −4
13

P(x, y) = 10 · 4
132 · 62
132 −10 · 9
132 · 24
132
−10 · 22
132
· 4
132 −3 · 4
132 · 60
132 + 3 · 9
132 · 20
132 −16 · 4
132 · 52
132 = −20
132 .
Therefore, the correlation of the random variables X,Y is given by
cov(X,Y)
σ(X)σ(Y) = −
20
8
√
5 · 11
= −1
2
r
5
11 = −0.3371,

23.2 Random Variables
1147
which means that there is a small (negative) correlation between these random variables,
because if an honor is drawn, that drawing is not available to yield a 2 or a 3, and vice
versa.
Finally, let us determine the marginal distribution,
P(X = x) =
2
X
y=0
P(x, y),
or explicitly,
P(X = 0) = P(0,0) + P(0,1) + P(0,2) =
 6
13
2
+ 24
132 +
 2
13
2
=
 8
13
2
,
P(X = 1) = P(1,0) + P(1,1) = 60
132 + 20
132 = 80
132 ,
P(X = 2) = P(2,0) =
 5
13
2
,
which is properly normalized because
P(x = 0) + P(X = 1) + P(X = 2) = 64 + 80 + 25
132
= 169
132 = 1.
The mean value and variance of X can be computed from the marginal probabilities:
⟨X⟩=
2
X
x=0
x P(X = x) = P(X = 1) + 2P(X = 2) = 80 + 2 · 25
132
= 130
132 = 10
13,
σF =
2
X
x=0

x −10
13
2
P(X = x) =

−10
13
2  8
13
2
+
 3
13
2 80
132 +
16
13
2  5
13
2
= 80 · 169
134
= 80
132 .
These data agree with our earlier computations of the same quantities.
■
Conditional Probability Distributions
If we are interested in the distribution of a random variable X for a deﬁnite value y = y0
of another random variable, then we deal with a conditional probability distribution
P(X = x|Y = y0). The corresponding continuous probability density is f (x, y0).
Exercises
23.2.1
Verify the probabilities for the outcomes of the two-card draws in Example 23.2.4, and
by direct computation of the mean and variance check the results given in that example.

1148
Chapter 23 Probability and Statistics
23.2.2
Show that adding a constant c to a random variable X changes the expectation value
⟨X⟩by that same constant but not the variance. Show also that multiplying a random
variable by a constant multiplies both the mean and variance by that constant. Show
that the random variable X −⟨X⟩has mean value zero.
23.2.3
Using the deﬁnition given in Eq. (23.27) for the variance σ 2 of a continuous random
variable, show that
σ 2 = ⟨X2⟩−⟨X⟩2.
23.2.4
A velocity v j = x j/t j is measured by recording the distances x j at the corresponding
times t j. Show that ¯x/¯t is a good approximation for the average velocity v, provided
that all the errors are small: |x j −¯x| ≪|¯x| and |t j −¯t| ≪|¯t|.
23.2.5
Redeﬁne the random variable Y in Example 23.2.5 as the number of fours through nines.
Then determine the correlation of the X and Y random variables for the drawing of two
cards (with replacement, as in the example).
23.2.6
The probability that a particle of an ideal gas travels a distance x between collisions
is proportional to e−x/f dx, where f is the constant mean free path. Verify that f is
the average distance between collisions, and determine the probability of a free path of
length l ≥3 f .
23.2.7
Determine the probability density for a particle in simple harmonic motion in the
interval −A ≤x ≤A.
Hint. The probability that the particle is between x and x + dx is proportional to the
time it takes to travel across the interval.
23.3
BINOMIAL DISTRIBUTION
In this and the next two sections, we explore speciﬁc random variable distributions that are
of importance both in physics and in the mathematical theories of probability and statistics.
The topic of the present section is the binomial distribution, which typically occurs in the
study of repeated independent trials of random events.
Example 23.3.1
REPEATED TOSSES OF DICE
What is the probability of three sixes in four tosses, all trials being independent? Getting
one six in a single toss of a fair die has probability a = 1/6, and getting anything else has
probability b = 5/6 with a + b = 1. Let the random variable S = s be the number of sixes.
In four tosses, 0 ≤s ≤4. The probability distribution P(S = s) is given by the product of
the two possibilities, as and b4−s, times the number of ways that s sixes can be obtained
from four tosses. This number is given by Eq. (23.18), and our probability is
P(S = s) =
4!
s!(4 −s)!asb4−s =
4
s

asb4−s.
(23.40)

23.3 Binomial Distribution
1149
We can now check that our probability is properly normalized by verifying that the sum of
P(S = s) for all s adds to unity. From properties of the binomial coefﬁcients, we ﬁnd
4
X
s=0
4
s

asb4−s = (a + b)4 =
1
6 + 5
6
4
= 1.
(23.41)
Writing out the cases of Eq. (23.40) explicitly, we have
f (0) = b4,
f (1) = 4ab3,
f (2) = 6a2b2,
f (3) = 4a3b,
f (4) = a4,
so we can answer our original question: The probability of three sixes in four tosses is
4a3b = 4
1
6
3 5
6 =
5
4 · 34 ,
which is fairly small.
■
This case dealt with repeated independent trials, each with two possible outcomes of
constant probability p for a hit and q = 1 −p for a miss, and it is typical of many ap-
plications, including practical issues such as the random instances of defective products.
The generalization to S = s successes in n trials is given by the binomial probability
distribution:
P(S = s) =
n!
s!(n −s)! psqn−s =
n
s

psqn−s.
(23.42)
Figure 23.5 shows histograms for cases with 20 trials and various hit probabilities p.
Example 23.3.2
USE OF MOMENT-GENERATING FUNCTION
If we view our probability distribution as the result of adding together n random variables
Si, each having the value si = 1 with probability p and the value si = 0 with probability
q, we can use the moment-generating function of Eq. (23.32) to obtain more information
about the binomial distribution. We write
⟨etS⟩= ⟨et(S1+S2+···+Sn)⟩= ⟨etS1⟩⟨etS2⟩···⟨etSn⟩=
h
⟨etS1⟩
in
,
(23.43)
where we have used the fact that the trials are independent to write ⟨etS⟩as a product of
single-trial expectation values, all of which are identical.
We continue by evaluating ⟨etS1⟩, which is an average for the two values s1 = 1, with
probability p, and s1 = 0, with probability q. We get
⟨etS1⟩= pet + qe0 = pet + q,
(23.44)
so Eq. (23.43) reduces to
⟨etS⟩= (pet + q)n.
(23.45)

1150
Chapter 23 Probability and Statistics
f(x= n)
p=0.1
p = 0.3
p = 0.5
0.30
0.25
0.20
0.15
0.10
0.05
0
2
4
6
8
10
12
14
16
18
20
n
FIGURE 23.5
Binomial probability distributions for n = 20 and p = 0.1, 0.3, 0.5.
Note that the fact the trials were independent enabled us to obtain the moment-distribution
function without enumerating all the many-trial possibilities.
Now that we have ⟨etS⟩we can differentiate it, as in Eq. (23.33), to obtain moments of
our distribution. Using
∂⟨etS⟩
∂t
= npet(pet + q)n−1,
⟨S⟩=
X
i
si f (si) = ∂⟨etS⟩
∂t

t=0
= np,
∂2⟨etS⟩
∂t2
= npet(pet + q)n−1 + n(n −1)p2e2t(pet + q)n−2,
⟨S2⟩=
X
i
s2
i f (si) = ∂2⟨etS⟩
∂t2

t=0
= np + n(n −1)p2,
we obtain, applying Eq. (23.28),
σ 2(S) = ⟨S2⟩−⟨S⟩2 = np + n(n −1)p2 −n2 p2
= np(1 −p) = npq.
For a given n, we see that the variance is largest when p = q = 1/2. This behavior is
apparent in Fig. 23.5, where we see that the distribution broadens as p is increased from
0.1 to 0.3 to 0.5.
■

23.4 Poisson Distribution
1151
Exercises
23.3.1
Show that the variable X = x, deﬁned as the number of heads in n coin tosses, is a
random variable and determine its probability distribution. Describe the sample space.
What are its mean value, the variance, and the standard deviation? Plot the probability
function P(x) = [n!/x!(n −x)!]2−n for n = 10, 20, and 30 using graphics software.
23.3.2
Plot the binomial probability function for the probabilities p = 1/6, q = 5/6, and n = 6
throws of a die.
23.3.3
A hardware company knows that the probability of mass-producing nails includes a
small probability p = 0.03 of defective nails (usually without a sharp tip). What is the
probability of ﬁnding more than two defective nails in its commercial box of 100 nails?
23.3.4
Four cards are drawn from a shufﬂed bridge deck. What is the probability that they
are all red? That they are all hearts? That they are honors? Compare the probabilities
when each card is put back at a random place before drawing the next card, with the
probabilities when the cards are not replaced in the deck.
23.3.5
Show that for the binomial distribution of Eq. (23.42), the most probable value of x
is np.
23.4
POISSON DISTRIBUTION
The Poisson distribution is often used to describe situations in which an event occurs
repeatedly at a constant rate of probability. Typical applications involve the decay of
radioactive samples, but only in the approximation that the decay rate is slow enough that
depletion in the population of the decaying species can be neglected. Other applications
of interest include so-called Poisson noise, where ﬂuctuations in a low rate of arrival of
particles at a detector cause statistically predictable ﬂuctuations in the detector signal.
The Poisson distribution can be developed by considering the probabilities that varying
numbers of events are detected over an interval during which events occur at a constant
rate of probability. The essential features of the development are that it assumes that (1) the
event rate is small enough that there will be observationally accessible intervals in which
at most one event occurs (i.e., one can consider intervals containing either zero or one
event), and (2) the total number of events is small enough that it is useful to model their
occurrence by a discrete probability distribution.
Let’s proceed by deﬁning the probability Pn(t) that exactly n events occur in a time
t, and that the probability of one event occurring in a short time interval dt will be µdt,
where µ is a constant such that µdt ≪1. This time interval dt is therefore short enough
that we can neglect the possibility that more than one event occurs within it. Based on this
hypothesis, we can set up a recursion relation for Pn(t) by considering the two following
mutually exclusive possibilities for the occurrence of n events in a time d + dt: (1) that
n events occur during a time t and no events occur in a subsequent time interval dt, and
(2) that n −1 events occur during the time t and one event occurs during the subsequent
interval dt. We therefore write
Pn(t + dt) = Pn(t)P0(dt) + Pn−1(t)P1(dt).

1152
Chapter 23 Probability and Statistics
Then, inserting P1(dt) = µdt and P0(dt) = 1 −P1(dt) and dividing through by dt, we
get, after minor rearrangement,
d Pn(t)
dt
= Pn(t + dt) −Pn(t)
dt
= µPn−1(t) −µPn(t).
(23.46)
As a ﬁrst step in solving this recursion relation, we note that for n = 0 it simpliﬁes
(because the possibility involving Pn−1 does not exist) to
d P0(t)
dt
= −µP0(t).
(23.47)
This equation, with initial condition P0(0) = 1 (meaning that it is certain that no events are
observed in an interval of zero length), has solution P0(t) = e−µt. Our solution informs us
that the probability that no events have occurred before time t decays exponentially with
t, at a rate dependent on the magnitude of µ. From this starting point and the further initial
conditions Pn(0) = 0 for n ≥1 (again, no detection of events occurs during an interval of
zero length), the recursion relation can be solved to yield
Pn(t) = (µt)n
n!
e−µt.
(23.48)
Equation (23.48) can be checked by substituting it into the recursion formula, Eq. (23.46),
and by verifying that it satisﬁes the initial conditions Pn(0) = δn0.
Equation (23.48) is taken as the deﬁnition of the Poisson distribution, regarded as a func-
tion of the quantity µt. Replacing µt by µ, we write the Poisson-distribution probabilities
given for a discrete random variable X in the standard form,
p(n) = µn
n! e−µ,
X = n = 0,1,2,....
(23.49)
We can check that the probabilities in Eq. (23.49) are properly normalized by noting that
P
n µn/n! evaluates to eµ. An example of a Poisson distribution is given in Fig. 23.6.
The mean value and variance of a Poisson distribution are easily calculated:
⟨X⟩=
∞
X
n=1
n µn
n! e−µ = e−µ
∞
X
n=1
µn
(n −1)! = µ,
(23.50)
⟨X2⟩=
∞
X
n=1
n2 µn
n! e−µ = e−µ
∞
X
n=1

µn
(n −2)! +
µn
(n −1)!

= µ2 + µ,
(23.51)
σ 2 = ⟨X2⟩−⟨X⟩2 = µ(µ + 1) −µ2 = µ.
(23.52)
The moments can also be calculated from the moment-generating function
D
et XE
=
∞
X
n=0
µn
n! e−µetn = e−µ
∞
X
n=0
(µet)n
n!
= eµ(et−1).
Recall that the procedure for obtaining moments is to differentiate with respect to t and
read out the derivatives evaluated at t = 0.

23.4 Poisson Distribution
1153
0
2
4
6
8
10
12
14
16
18
20
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
FIGURE 23.6
Poisson distribution, µ = 5.
Relation to Binomial Distribution
A Poisson distribution becomes a good approximation of the binomial distribution for a
large number n of trials and small probability p ∼µ/n, with µ held constant.
Theorem: In the limit n →∞and p →0 so that the mean value np →µ stays ﬁnite,
the binomial distribution becomes a Poisson distribution.
To prove this theorem, we need to ﬁnd the large-n limit of the binomial distribution
formula, Eq. (23.42). To do so, we apply Stirling’s formula, in the form n! ∼
√
2πn(n/e)n
for large n. See Eq. (12.110). For the quotient of the two n-dependent factorials occurring
in Eq. (23.42), we have (keeping s ﬁnite while letting n →∞):
n!
(n −s)! ∼
n
e
n 
e
n −s
n−s
∼
n
e
s 
n
n −s
n−s
∼
n
e
s 
1 +
s
n −s
n−s
.
The factor in the ﬁnal expression raised to the power n −s is, in the limit of large n,
an expression of value es (in fact, it is, with n −s changed to n, one of the often-used
deﬁnitions of the exponential). The ﬁnal result is
n!
(n −s)! ∼ns.
(23.53)
We use a similar deﬁning expression for the exponential to evaluate the factor qn−s in
Eq. (23.42). Writing qn−s = (1 −p)n−s and replacing p by its limiting value p = µ/n,

1154
Chapter 23 Probability and Statistics
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
2
4
6
8
10
12
14
16
18
20
FIGURE 23.7
Comparison of binomial distribution (N = 80, p = 0.1), wide bars, and
Poisson distribution (µ = 8), narrow bars.
we have
qn−s = (1 −p)n−s ∼

1 −µ
n
n 
1 −µ
n
−s
∼e−µ (1) ∼e−µ.
(23.54)
Inserting the large-n limiting values from Eqs. (23.53) and (23.54) into the formula for the
binomial distribution, we reach
P(S = s) =
n!
s!(n −s)! psqn−s ∼ns
s! pse−µ ∼µs
s! e−µ,
(23.55)
where in the last step we have combined ns and ps into µs.
Equation (23.55) establishes our theorem, and thereby completes the connection be-
tween the Poisson and binomial distributions. This result, which becomes valid in the limit
of a large number of trials, each of small probability, is sometimes referred to as an exam-
ple of the laws of large numbers. A comparison of the binomial and Poisson distributions
is presented as Fig. 23.7.
Exercises
23.4.1
Radioactive decays for long-lived isotopes are governed by the Poisson distribution. In
a Rutherford-Geiger experiment, the numbers of emitted α particles are counted in each
of n = 2608 time intervals of 7.5 seconds each. In Table 23.1 ni is the number of time
intervals in which i particles were emitted. Determine the average number λ of particles
emitted per time interval, and compare the ni of Table 23.1 with npi computed from
the Poisson distribution with mean value λ.

23.5 Gauss’ Normal Distribution
1155
Table 23.1
Data for Exercise 23.4.1
i
→
0
1
2
3
4
5
6
7
8
9
10
ni
→
57
203
383
525
532
408
273
139
45
27
16
23.4.2
Derive the standard deviation of a Poisson distribution of mean value µ.
23.4.3
The number of α-particles emitted by the decay of a radium sample is counted per
minute for 40 hours. The total number is 5000. How many 1-minute intervals are ex-
pected with (a) 2, and (b) 5 α-particles?
23.4.4
For a radioactive sample, 10 decays are counted on average in 100 seconds. Use the
Poisson distribution to estimate the probability of counting 3 decays in 10 seconds.
23.4.5
238U has a half-life of 4.51×109 years. Its decay series ends with the stable lead isotope
206Pb. The ratio of the number of 206Pb to 238U atoms in a rock sample is measured as
0.0058. Estimate the age of the rock assuming that all the lead in the rock is from the
initial decay of the 238U, which determines the rate of the entire decay process, because
the subsequent steps take place far more rapidly.
Hint. This is not a Poisson distribution problem, but is an application of the decay law
N(t) = Ne−λt, where λ, the decay constant, is related to the half-life T by T = ln2/λ.
ANS.
3.8 × 107 years.
23.4.6
The probability of hitting a target in one shot is known to be 20%. If ﬁve shots are ﬁred
independently, what is the probability of striking the target at least once?
23.5
GAUSS’ NORMAL DISTRIBUTION
The bell-shaped Gauss distribution is deﬁned by the probability density
f (x) =
1
σ
√
2π
exp

−[x −µ]2
2σ 2

,
−∞< x < ∞,
(23.56)
with mean value µ and variance σ 2. In part because it represents continuous limits of
both the binomial and Poisson distributions, it is by far the most important continuous
probability distribution and is displayed in Fig. 23.8.

1156
Chapter 23 Probability and Statistics
h =3
h =2
h =1
f
1
0
1
x
FIGURE 23.8
Gauss normal distribution for mean value zero and various standard
deviations (marked by circles). Curves are labeled by h = 1/σ
√
2.
It is properly normalized because, substituting y = (x −µ)/σ
√
2, we obtain
1
σ
√
2π
∞
Z
−∞
e−(x−µ)2/2σ 2dx =
1
√π
∞
Z
−∞
e−y2dy =
2
√π
∞
Z
0
e−y2dy = 1.
To check the mean value, we can make the substitution y = x −µ, and ﬁnd that
⟨X⟩−µ =
∞
Z
−∞
x −µ
σ
√
2π
e−(x−µ)2/2σ 2dx =
∞
Z
−∞
y
σ
√
2π
e−y2/2σ 2dy = 0,
(23.57)
showing that ⟨X⟩= µ. The zero result in Eq. (23.57) occurs because the integrand is odd
in y, so the integral over y > 0 cancels that over y < 0. A check that the variance of this
normal distribution is indeed σ 2 is the topic of Exercise 23.5.1.
We can compute conditional probabilities for the normal distribution. In particular,
making for convenience the substitution y = (x −⟨X⟩)/σ,
P(|X −⟨X⟩| > kσ) = P
|X −⟨X⟩|
σ
> k

= P(|Y| > k)
=
r
2
π
∞
Z
k
e−y2/2dy =
r
4
π
∞
Z
k/
√
2
e−z2dz = erfc(k/
√
2),
we can evaluate the integral for k = 1,2,3, and thus extract the following numerical rela-
tions for a normally distributed random variable:
P(|X −⟨X⟩| ≥σ) ≈0.3173,
P(|X −⟨X⟩| ≥2σ) ≈0.0455,
P(|X −⟨X⟩| ≥3σ) ≈0.0027.
(23.58)

23.5 Gauss’ Normal Distribution
1157
It is interesting to compare the last of these quantities with Chebyshev’s inequality, which
gives 1/9 for the probability that an event falls further than 3σ from the mean. The 1/9
applies to an arbitrary probability distribution, and is in strong contrast to the much
smaller 0.0027 given by the 3σ-rule for the normal distribution.
Limits of Poisson and Binomial Distributions
In a special limit, the discrete Poisson probability distribution is closely related to the
continuous Gauss distribution. This limit theorem is another example of the laws of large
numbers, which are often dominated by the bell-shaped normal distribution.
Theorem: For large n and mean value µ, the Poisson distribution approaches a Gauss
distribution.
To prove this theorem, in the limit n →∞, we approximate for large n the factorial
in the Poisson probability p(n) by Stirling’s asymptotic formula, n! ∼
√
2nπ(n/e)n, and
choose the deviation v = n −µ from the mean value as the new variable. We let the mean
value µ approach ∞and treat v/µ as small, but assume v2/µ to be ﬁnite. Substituting
n = µ + v, we obtain
ln p(n) = ln
µne−µ
n!

= n lnµ −µ −ln
√
2nπ −n lnn + n
= (µ + v)ln

µ
µ + v

+ v −ln
p
2(µ + v)π
= (µ + v)ln

1 −
v
µ + v

+ v −ln
p
2π(µ + v).
We next expand the ﬁrst logarithmic term in powers of v/(µ + v), reaching
ln p(n) = −
∞
X
t=1
vt
t(µ + v)t−1 + v −ln
p
2π(µ + v).
(23.59)
The ﬁrst two terms of the t summation yield nonvanishing contributions in the large-µ
limit; further terms vanish because the power of v in the numerator is less than twice that
of µ in the denominator. Replacing µ + v by µ, Eq. (23.59) reduces to
ln p(n) ∼−v2
2µ −ln
p
2πµ,
equivalent to
p(n) ∼
1
√2πµe−v2/2µ.
(23.60)
This is a Gauss distribution of the continuous variable v with mean value 0 and standard
deviation σ = √µ.
In another special limit, the discrete binomial probability distribution is also closely
related to the continuous Gauss distribution. This limit theorem is yet another example of
the laws of large numbers.

1158
Chapter 23 Probability and Statistics
Theorem: In the limit n →∞, with p a ﬁnite trial probability such that the mean value
np →∞, the binomial distribution becomes a Gauss normal distribution. Recall from
Section 23.4 that, when np →µ < ∞, the binomial distribution becomes a Poisson
distribution.
Instead of the large number s of successes in n trials, we use the deviation v = s −
pn from the (large) mean value pn as our new continuous random variable, under the
condition that as n →∞, |v| ≪pn (so v/n →0) but v2/n is ﬁnite. Thus, we replace s by
pn + v and n −s by qn −v in the factorials of the formula for the binomial distribution,
Eq. (23.42). Writing now W(v) as our probability distribution in the large-pn limit, we
apply Stirling’s formula as we have done several times before, obtaining initially
W(v) =
psqn−snn+1/2e−n+s+(n−s)
√
2π(pn + v)s+1/2(qn −v)n−s+1/2 .
(23.61)
Next we factor out the dominant powers of n and cancel powers of p and q to ﬁnd
W(v) =
1
√2πpqn

1 + v
pn
−(pn+v+1/2) 
1 −v
qn
−(qn−v+1/2)
.
(23.62)
Taking the logarithm of W(v) and expanding in powers of v, we retain only the terms
through v2, yielding
ln W(v) = −ln
p
2πpqn
−
v
n
 1
2p −1
2q

−v2
n2
 1
4p2 +
1
4q2

+ v2
n
 1
2p + 1
2q

+ ···

.
(23.63)
Setting v/n to zero, noting that
 1
2p + 1
2q

= p + q
2pq =
1
2pq ,
and dropping all terms vt with t > 2, we obtain our large-n limit
W(v) =
1
√2πpqn e−v2/2pqn,
(23.64)
which is a Gauss distribution in the deviations s −pn, with mean value 0 and standard
deviation σ = √npq. The large values assumed for both pn and qn (and the discarded
terms) restrict the validity of the theorem to the central part of the Gaussian bell shape, and
exclude the tails.
Exercises
23.5.1
Show that the variance of the normal distribution given by Eq. (23.56) is σ 2, the symbol
in that equation.
23.5.2
Show that Eq. (23.62) can be obtained by manipulation of the formula Eq. (23.61) for
W(v).
23.5.3
With W(v) the expression in Eq. (23.62), show that the expansion of ln W(v) in powers
of v leads to Eq. (23.63).

23.6 Transformations of Random Variables
1159
23.5.4
What is the probability for a normally distributed random variable to differ by more
than 4σ from its mean value? Compare your result with the corresponding one from
Chebyshev’s inequality. Explain the difference in your own words.
23.5.5
An instructor grades a ﬁnal exam of a large undergraduate class, obtaining the mean
value of points M and the variance σ 2. Assuming a normal distribution for the number
M of points, he deﬁnes a grade F when M < m −3σ/2, D when m −3σ/2 < M <
m −σ/2, C when m −σ/2 < M < m +σ/2, B when m +σ/2 < M < m +3σ/2, and
A when M > m + 3σ/2. What is the percentage of As, Fs; Bs, Ds; and Cs? Redesign
the cutoffs so that there are equal percentages of As and Fs (5%), 25% Bs and Ds, and
40% Cs.
23.6
TRANSFORMATIONS OF RANDOM VARIABLES
We have already encountered some elementary transformations involving random vari-
ables: In Section 23.2 we observed that a random variable Y = aX + b will have mean
value ⟨Y⟩= a⟨X⟩+ b and variance σ 2(Y) = a2σ 2(X). Here we consider more general
transformations, with particular focus on continuous probability distributions.
First, consider a simple change of random variable from X to Y, where y = y(x). If
the probability distribution of X is f (x)dx, then the contribution at x to some quantity
M(y) is
P{M[y(x)]}dx = M[y(x)] f (x)dx.
(23.65)
But we may wish to express the probability in terms of the distribution of Y , writing
P[M(y)]dy = M(y)g(y)dy,
(23.66)
for y evaluated at the point corresponding to x, i.e., y = y(x). To make these equations
consistent, it is necessary that
g(y)dy = f (x)dx,
or
g(y) = f [x(y)]dx
dy .
(23.67)
For example, if y = x2, then dx/dy = 1/(2x) = y−1/2/2 and g(y) = f (√y)y−1/2/2.
Let’s now address the transformation of two random variables X, Y into U(X,Y),
V (X,Y). Again we treat the continuous case. If
u = u(x, y),
v = v(x, y),
x = x(u,v),
y = y(u,v)
(23.68)
describe the transformation and its inverse; integrals of the probability density will trans-
form by formulas that include the Jacobian of the transformation (see Section 4.4). The
transformed probability density becomes
g(u,v) = f (x(u,v), y(u,v))|J|,
(23.69)

1160
Chapter 23 Probability and Statistics
where the Jacobian is
J = ∂(x, y)
∂(u,v) =

∂x
∂u
∂x
∂v
∂y
∂u
∂y
∂v

.
(23.70)
This is a generalization of Eq. (23.67).
Addition of Random Variables
Let’s apply this analysis to a situation in which Z is the sum of two random variables X
and Y , or Z = X + Y. We transform to new variables X and Z, so the transformation is
x = x, z = x + y; or x = x, y = z −x. The Jacobian for this transformation is
J =

∂x
∂x
∂x
∂z
∂(z −x)
∂x
∂(z −x)
∂z

=

1 0
−1 1
 = 1.
If our original probability distribution was f (x, y), it therefore transforms into g(x, z) =
f (x, z −x). We are usually interested in the marginal distribution in Z, obtained by inte-
grating over x, and
P(Z = z) ≡g(z) =
∞
Z
−∞
f (x, z −x)dx.
(23.71)
In the oft-occurring case that X and Y are independent random variables, so f (x, y) =
f1(x) f2(y), Eq. (23.71) assumes the form
g(z) =
∞
Z
−∞
f1(x) f2(z −x)dx,
(23.72)
which we recognize as a Fourier convolution, see Eq. (20.68):
g(z) =
∞
Z
−∞
f1(x) f2(z −x)dx =
√
2π( f1 ∗f2)(z).
(23.73)
Equation (23.72) gives us a general formula whereby we can obtain the distribution of
Z = X + Y from the distributions of independent variables X and Y, while Eq. (23.73)
shows that it may be useful to consider the use of Fourier transforms for evaluating the
integral. In fact, the moment-generating function, Eq. (23.32), is (if t is replaced by it)
proportional to the Fourier transform of the probability density, and
⟨eitX⟩=
Z
eitx f (x)dx =
√
2π f T (t)
(23.74)
is known as the characteristic function in probability theory.

23.6 Transformations of Random Variables
1161
Applying the Fourier convolution theorem, Eq. (20.70), we therefore write
[P(Z = z)]T (t) ≡gT (t) =
√
2π f T
1 (t) f T
2 (t),
(23.75)
showing that we can obtain g(z) as the inverse Fourier transform
g(z) =
Z
e−izt f T
1 (t) f T
2 (t)dt.
(23.76)
Connection with statistics texts will be improved by restating Eqs. (23.75) and (23.76)
using the characteristic function notation. Equation (23.75) is equivalent to
⟨eit Z⟩= ⟨eit(X+Y)⟩= ⟨eit X⟩⟨eitY ⟩.
(23.77)
Equation (23.76) states that g(z) is the distribution that corresponds to ⟨eit Z⟩; since Fourier
transforms have inverses it can be assured that such a distribution exists.
Example 23.6.1
ADDITION THEOREM, NORMAL DISTRIBUTION
A good example of the analysis for a random variable Z = X + Y is provided when X and
Y are taken to be Gauss normal distributions with zero mean value and the same variance.
This situation corresponds to a relationship known as the addition theorem for normal
distributions.
Theorem: If the independent random variables X,Y have identical normal distribu-
tions, that is, the same mean value and variance, then Z = X + Y has normal distribu-
tion with twice the mean value and twice the variance of X and Y .
To prove this theorem, we assume without loss of generality that the normal distributions
each have variance σ = 1, so, from Eq. (23.56), each has the form
f (x) =
1
√
2π
e−(x−µ)2/2.
From Eq. (20.18) and the translation formula, Eq. (20.67), we ﬁnd the Fourier transform
of f (x) to be
f T (t) =
1
√
2π
eitµe−t2/2.
Now, applying Eq. (23.75), we have
gT (t) =
1
√
2π
e2itµe−t2.
Taking the inverse transform, noting that the complex exponential shifts the origin by an
amount 2µ, we get
g(z) =
1
2√π e−(z−2µ)2/4,
which shows that the mean and variance of Z are twice those of X and Y, so the theorem
is satisﬁed.
■

1162
Chapter 23 Probability and Statistics
Multiplication or Division of Random Variables
Consider now the product Z = XY, taking X, Z as the new variables. This corresponds to
the transformation x = x, y = z/x, with Jacobian
J =

∂x
∂x
∂x
∂z
∂(z/x)
∂x
∂(z/x)
∂z

=

1
0
−z/x2 1/x

= 1
x ,
so the marginal distribution of Z is given by
g(z) =
∞
Z
−∞
f

x, z
x
dx
|x|.
(23.78)
If the random variables X, Y are independent with densities f1, f2, then
g(z) =
∞
Z
−∞
f1(x) f2
 z
x
dx
|x|.
(23.79)
Finally, let Z = X/Y , taking Y, Z as the new variables, corresponding to x = yz, y = y,
with Jacobian
J =

∂(yz)
∂y
∂(yz)
∂z
∂y
∂y
∂y
∂z

=

z y
1 0
 = −y,
and the probability distribution of Z is given by
g(z) =
∞
Z
−∞
f (yz, y)|y|dy.
(23.80)
If the random variables X, Y are independent with densities f1, f2, then
g(z) =
∞
Z
−∞
f1(yz) f2(y)|y|dy.
(23.81)
Gamma Distribution
Up to this point the only speciﬁc continuous probability distribution we have introduced
is the Gauss normal distribution. However, if we make a change in the random variable of
that distribution from X to Y = X2, there will result a different distribution of signiﬁcant
utility, known as a gamma distribution. Let’s start the present discussion with the now

23.6 Transformations of Random Variables
1163
quite familiar normal distribution of mean value zero and variance σ 2. It has probability
distribution
f (x) =
1
σ
√
2π
ex2/2σ 2.
As indicated just after Eq. (23.67), a transformation to write the distribution in terms of
y = x2 leads us to
g(y) = e−y/2σ 2
σ
√
2π
y−1/2
2
.
However, this equation does not take into account the fact that y must be restricted to
nonnegative values, and that the same value of y will be encountered for two different
values of x, namely x = +√y and x = −√y. These considerations make a more proper
and complete formula for g(y) the following:
g(y) =



0,
y ≤0,
y−1/2e−y/2σ 2
(2σ 2)1/2√π ,
y > 0.
(23.82)
This expression for g(y) is normalized (it must be, due to the way in which it was
obtained). However, it is instructive to check, which is best done by changing to a new
variable z = y/2σ 2, in terms of which we have
∞
Z
0
g(z)dz =
1
√π
∞
Z
0
z−1/2e−z dz = 0( 1
2)
√π = 1,
where we have identiﬁed the integral as 0
  1
2

and also noted that 0
  1
2

= √π.
Because the functional form of g(y) is essentially that of the integrand of the integral
representation of the gamma function, the distribution given by g(y) is called a gamma
distribution, and in particular, a gamma distribution with parameters p = 1/2 (the argu-
ment of the gamma function) and σ 2 (the variance of the underlying normal distribution).
We generalize to gamma distributions of general p and σ:
g(p,σ; y) ≡



0,
y ≤0,
y p−1e−y/2σ 2
(2σ 2)p0(p) ,
y > 0.
(23.83)
The gamma distribution often appears in contexts where the random variables involved
need to be added together. It is therefore useful to take note of the Fourier transform of
g(p,σ; y):
[g(p,σ)]T (t) =
1
√
2π
1
(1 −2iσ 2t)p .
(23.84)
Using the characteristic-function notation as introduced at Eq. (23.74), and deﬁning X to
be a (p,σ) gamma-distributed random variable, Eq. (23.84) takes the alternative form
⟨eit X⟩=
1
(1 −2iσ 2t)p .
(23.85)

1164
Chapter 23 Probability and Statistics
Example 23.6.2
ADDITION OF GAMMA-DISTRIBUTION RANDOM VARIABLES
Let’s compute the distribution of a random variable Y = X1 + X2, where X1 has gamma
distribution g(p1,σ; x1) and X2 has gamma distribution g(p2,σ; x2). Note that both X1
and X2 have the same variance.
Using Eq. (23.77) for the characteristic function of X1 + X2 and Eq. (23.85) to evaluate
⟨eit Xj ⟩, we get
⟨eitY ⟩=
1
(1 −2iσ 2t)p1+p2 .
Recognizing this result as the characteristic function for a gamma distribution of parameter
p = p1 + p2, we see that
g(y) = g(p1 + p2,σ; y).
Generalizing this result to an arbitrary number of Xj:
The probability distribution for a sum of gamma-distributed random variables Xj of
parameters p j but all of the same σ is a gamma distribution for that σ and with p =
P
j p j.
A corollary to the above is obtained if we consider the probability distribution of a sum of
the form
Z =
n
X
j=1
X2
j ,
(23.86)
where the Xj are Gauss normal distributions, all with the same variance σ 2. Because the
quantities being summed are squares of random variables, it is useful ﬁrst to make the
substitutions Y j = X2
j , changing each distribution of Xj to that of a Y j gamma distribution
with p = 1/2, and ﬁnally combining the n gamma distributions to form the distribution
of Z; the result will be a gamma distribution with p = n/2 and the common value of σ.
Summarizing,
The probability distribution for the sum of the squares of n Gauss normal random
variables with a common variance σ 2, as in Eq. (23.86), will be a gamma distribution
with parameters p = n/2 and the common value of σ.
■
Exercises
23.6.1
Let X1, X2,..., Xn be independent normal random variables with the same mean ¯x and
variance σ 2. Show that
P
i Xi/n −¯x
σ√n
is normal with mean zero and variance 1.
23.6.2
If the random variable X is normal with mean value 29 and standard deviation 3, what
can you say about the distributions of 2X −1 and 3X + 2?

23.7 Statistics
1165
23.6.3
For a normal distribution of mean value m and variance σ 2, ﬁnd the distance r such
that half the area under the bell shape is between m −r and m + r.
23.6.4
If ⟨X⟩,⟨Y⟩are the average values of two independent random variables X, Y, what is
the expectation value of the product XY?
23.6.5
If X and Y are two independent random variables with different probability densities
and the function f (x, y) has derivatives of any order, express ⟨f (X,Y)⟩in terms of
⟨X⟩and ⟨Y⟩. Develop similarly the covariance and correlation.
23.6.6
Let f (x, y) be the joint probability density of two random variables X, Y. Find the
variance σ 2(aX + bY), where a,b are constants. What happens when X, Y are
independent?
23.6.7
Obtain an addition theorem for the distribution of a random variable Y = X1 + X2
where X1 and X2 are Gauss normal distributions with different mean values µ j and
variances σ 2
j .
ANS.
Y is normal with mean µ1 + µ2 and variance σ 2
1 + σ 2
2 .
23.6.8
Show that the Fourier transform of the gamma-distribution probability density,
Eq. (23.83), has the functional form given in Eq. (23.84).
23.7
STATISTICS
In statistics, probability theory is applied to the evaluation of data from random experi-
ments or to samples to test some hypothesis because the data have random ﬂuctuations
due to lack of complete control over the experimental conditions. Typically one attempts
to estimate the mean value and variance of the distributions from which the samples
derive, and to generalize properties valid for a sample to the rest of the events at a pre-
scribed conﬁdence level. Any assumption about an unknown probability distribution is
called a statistical hypothesis. The concepts of tests and conﬁdence intervals are among
the most important developments of statistics.
Error Propagation
When we measure a quantity x repeatedly, obtaining the values x j, or select a sample for
testing, we can compute
¯x = 1
n
n
X
j=1
x j,
σ 2 = 1
n
n
X
j=1
(x j −¯x)2,
where ¯x is the mean value and σ 2 is the variance, a measure of the spread of the points
about the mean value. We can write x j = ¯x + ej, where ej is the deviation from the mean
value, and we know that P
j ej = 0.
Now suppose we want to estimate the value of a known function f (x) based on these
measurements x j; that is, we want to assign a value of f given the set f j = f (x j).

1166
Chapter 23 Probability and Statistics
Substituting x j = ¯x + ej and forming the mean value
¯f = 1
n
X
j
f (x j) = 1
n
X
j
f (¯x + ej)
= f (¯x) + 1
n f ′(¯x)
X
j
ej + 1
2n f ′′(¯x)
X
j
e2
j + ···
= f (¯x) + 1
2σ 2 f ′′(¯x) + ··· ,
(23.87)
we obtain the average value ¯f as f (¯x) in lowest order, as expected. But in second order
there is a correction given by the variance with a scale factor f ′′(¯x)/2.
It is also of interest to determine the spread predicted for the values of f (x j). To lowest
order, this is given by the average of the sum of squares of the deviations. Approximating
f j as ¯f + f ′(¯x)ej, we get
σ 2( f ) ≡1
n
X
j
( f j −¯f )2 ≈[ f ′(¯x)]2 1
n
X
j
e2
j = [ f ′(¯x)]2σ 2.
(23.88)
In summary, we may formulate somewhat symbolically
f (¯x ± σ) = f (¯x) ± f ′(¯x)σ
as the simplest form of error propagation for a function of one measured variable.
For a function f (x j, yk) ≡f jk of two quantities x j = ¯x + u j, yk = ¯y + vk, where the x j
and yk are measured independently of each other and we have r values of j and s values
of k, we obtain similarly
¯f = 1
rs
r
X
j=1
s
X
k=1
f jk = f (¯x, ¯y) + 1
r fx
X
j
u j + 1
s fy
X
k
vk + ···
≈f (¯x, ¯y).
(23.89)
The error in ¯f is seen to be second-order in the u j and vk. In writing Eq. (23.89) we have
used the relations P
j u j = P
k vk = 0 and have introduced the deﬁnitions
fx = ∂f
∂x

¯x, ¯y,
fy = ∂f
∂y

¯x, ¯y.
(23.90)
The variance of f is (to ﬁrst order)
σ 2( f ) = 1
rs
r
X
j=1
s
X
k=1
( f jk −¯f )2 = 1
rs
X
j,k
(u j fx + vk fy)2 = f 2
x
r
X
j
u2
j +
f 2
y
s
X
k
v2
k,

23.7 Statistics
1167
where we have dropped the zero cross term P
j,k u jvk = P
j u j
P
k vk. Noting that
P
j u2
j = rσ 2
x and P
k v2
k = sσ 2
y , we reach the ﬁnal result
σ 2( f ) = 1
rs
X
j,k
( f jk −¯f )2 = f 2
x σ 2
x + f 2
y σ 2
y .
(23.91)
Symbolically, the error propagation for a function of two measured variables may be
summarized as
f (¯x ± σx, ¯y ± σy) = f (¯x, ¯y) ±
q
f 2x σ 2x + f 2y σ 2y .
Example 23.7.1
REPEATED MEASUREMENTS
As an application and generalization of the result given in Eq. (23.91), let’s consider what
happens when we regard the mean of n measurements x j as a function
¯x = f (x1, x2,... + xn) = (x1 + x2 + ··· + xn)/n
of the variables x1,..., xn, each with variance σ 2. Then we have fx j = 1/n for each j,
and, according to Eq. (23.91),
σ 2(¯x) =
n
X
j=1
f 2
x j σ 2 =
n
X
j=1
σ 2
n2 = σ 2
n .
(23.92)
This result indicates that the standard deviation of the mean value, σ(¯x), will decrease
with the number of repeated measurements, approaching zero as σ/√n. It is important to
recognize the distinction between the variance of the mean value, denoted σ 2(¯x), and the
corresponding quantity for the individual measurements (denoted σ 2).
If we refer now to our earlier result that the sum of n identically distributed, Gauss
normal random variables is also a normal random variable with a variance equal to n times
that of each variable (see Example 23.6.1), and note also that division of the sum by n (to
form the mean) causes division of the variance by n2, as discussed following Eq. (23.28),
we ﬁnd a result identical to that developed in the present example, but with the additional
feature that the mean is also normally distributed.
■
The arithmetic mean ¯x will, because of the distribution in the x j, differ from the true
(but unknown) value µ, with µ and ¯x differing by some amount α, or ¯x = µ+α. However,
as the number n of measurements increases, we expect that the error α will tend to zero,
and that, according to Example 23.7.1, we can estimate α to fall in the range (−σ/√n <
α < σ/√n). We can reﬁne this estimate by considering the spread of the x j measured with
respect to the true value µ, meaning that we compute the variance using the average of v2
j,
where v j = x j −µ, instead of that of e2
j , where, as before, ej = x j −¯x. Calling this version
of the variance s2, we write
s2 = 1
n
n
X
j=1
v2
j = 1
n
n
X
j=1
(ej + α)2 = 1
n
n
X
j=1
e2
j + α2,
(23.93)

1168
Chapter 23 Probability and Statistics
where the term linear in ej vanishes because P
j ej = 0. Inserting now an estimate of α, in
the form α2 ≈s2/n (this approximation good to ﬁrst order), Eq. (23.93) rearranges to
s2

1 −1
n

= 1
n
n
X
j=1
e2
j ,
(23.94)
equivalent to
s =
sP
j(x j −¯x)2
n −1
.
(23.95)
The quantity s is referred to as the sample standard deviation. Equation (23.95) is not
well deﬁned when n = 1, but that is not an issue because a single data point is insufﬁcient
to determine a spread. The presence of n −1, in contrast to the factor n in Eq. (23.23),
allows for the probable error in ¯x, and is known as Bessel’s correction to the standard
deviation formula.
Fitting Curves to Data
Suppose we have a sample of measurements yj taken at times t j, where the time is known
precisely but the yj are subject to experimental error. An example would be snapshots
of the position of a particle in uniform motion at the times t j. Our statistical hypothesis,
motivated by Newton’s ﬁrst law and the initial condition that y = 0 when t = 0, is that y(t)
satisﬁes an equation of the form y = at, where the constant a is to be determined from the
measurements.
To ﬁt our equation to the data, we ﬁrst minimize the sum of the squares of deviations
S = P
j(at j −yj)2 to determine the slope parameter a, also called regression coefﬁcient,
using the method of least squares. Differentiating S with respect to a we obtain
2
X
j
(at j −yj)t j = 0,
which we can solve for a:
a =
P
j t j yj
P
j t2
j
.
(23.96)
Note that the numerator is built like a sample covariance, the scalar product of the variables
t, y of the sample. As shown in Fig. 23.9, the measured values yj do not as a rule lie on the
line. They have the sample standard deviation, computed from Eq. (23.95),
s =
sP
j(yj −at j)2
n −1
.
Alternatively, suppose that the yj values are known precisely while the t j are measure-
ments subject to experimental error. As suggested by Fig. 23.10, in this case we need to

23.7 Statistics
1169
y
t
0
FIGURE 23.9
Straight line ﬁt to data points (t j, yj) with t j known, yj measured.
y
t
0
FIGURE 23.10
Straight line ﬁt to data points (t j, yj) with yj known, t j measured.
interchange the roles of t and y and to ﬁt the line t = by to the data points. We minimize
S = P
j(byj −t j)2, setting dS/db = 0, and ﬁnd similarly the slope parameter
b =
P
j t j yj
P
j y2
j
.
(23.97)
In case both t j and yj have errors (we take t and y to have the same measurement
precision), we have to minimize the sum of squares of the deviations of both variables. It is
convenient to ﬁt to a parameterization t sinα −y cosα = 0, so y/t = sinα/cosα = tanα,
meaning that α is the angle the ﬁtting line makes with the t-axis (see Fig. 23.11). Our task
will therefore be to determine α. We also see from Fig. 23.11 that the ﬁtting line has to
be drawn so that the sum of the squares of the distances d j of the points (t j, yj) from the
line becomes a minimum. To ﬁnd d j, we rotate our coordinate system the angle α, which
moves
 t j, yj

to
 t′
j, y′
j

according to
 t′
j
y′
j
!
=
 
cosα sinα
−sinα cosα
! t j
yj
!
,

1170
Chapter 23 Probability and Statistics
y
t
tj
uj
vj
dj
yj
t
Pj
0
y
t
•
0
(a)
(b)
α
FIGURE 23.11
(a) Straight line ﬁt to data points (t j, yj). (b) Geometry of
deviations u j,v j,d j.
which yields d j = y′
j = −t j sinα + yj cosα, the (signed) distance to the line at angle α.
The minimum of the square of the distances to the line is found from
d
dα
X
j
d2
j = 2
X
j
(−t j sinα + yj cosα)(−t j cosα −yj sinα)
= sinα cosα
X
j
 t2
j −y2
j

−
 cos2 α −sin2 α
X
j
t j yj = 0,
which can be reduced to
tan2α =
2P
j t j yj
P
j
 t2
j −y2
j
.
(23.98)
This least-squares ﬁtting is appropriate when the measurement errors are unknown, as it
gives equal weight to the deviation of each point from the ﬁtting line.
Finally, if we have information that permits the assignment of different probable errors
to different points, we have the alternative of making a “weighted” least-squares ﬁt called
a chi square ﬁt, which we discuss in the next subsection.
The χ2 Distribution
Given a set of u j corresponding to values t j of an independent variable (which is not nec-
essarily a time), we seek to ﬁt these data to a function u(t,a1,a2,...), where the ai are
parameters that are adjusted to optimize the ﬁt. The optimization is carried out by mini-
mizing a weighted sum of the squares of the deviations, where the weights are controlled
by the assumed standard deviations σj of the respective measurements u j. The quantity to
be minimized is traditionally labeled χ2 and called chi-square, and its precise deﬁnition is
χ2 =
n
X
j=1
u j −u(t j,a,...)
σj
2
,
(23.99)

23.7 Statistics
1171
where n is the number of data points. This quadratic merit function gives more weight to
points with small measurement uncertainties σj.
The key assumptions adopted to analyze the probability distribution corresponding to the
chi-square ﬁt are (1) that each data point is an independent Gauss normal random variable
Xj with zero mean and unit variance, with the unit variance assured by the presence of the
σj in each term, and (2) that the distribution χ2 is related to the Xj by
χ2 =
n
X
j=1
X2
j .
(23.100)
Making a chi-square ﬁt requires no knowledge of statistics; we simply apply standard
analytical or numerical methods to minimize χ2 for our set of data points. On the other
hand, a knowledge of the chi-square probability distribution will be needed to determine
whether we are getting the expected quality from our chi-square ﬁt. In particular, if we
wish to determine the probability of the occurrence of our data set based on the chi-square
distribution (and possibly assess the adequacy of our assumptions regarding the individual-
point variances σ 2
j ), we must undertake further analysis.
Our earlier discussion of transformations of random variables included the analysis of
sums of normally distributed X2
j of the form given in Eq. (23.100), with the result de-
veloped in Example 23.6.2. Specializing to the case at hand, we note that χ2 will have a
gamma probability distribution with parameters p = n/2 and σ = 1, so
g(χ2 = y) = y(n/2)−1e−y/2
2n/20(n/2) .
(23.101)
Plots of g(y) for several values of n are given in Fig. 23.12.
0.5
0.45
n=2
n= 3
n= 4
n=5
0.35
0.25
0.15
0.05
0.4
Chi-square densities, n = 2, 3, 4, and 5
0.3
0.2
0.1
00
2
12
10
8
6
4
FIGURE 23.12
χ2 probability density gn(y).

1172
Chapter 23 Probability and Statistics
Table 23.2
χ2 Distribution
n
v = 0.8
v = 0.7
v = 0.5
v = 0.4
v = 0.3
v = 0.2
v = 0.1
1
0.064
0.148
0.455
0.708
1.074
1.642
2.706
2
0.446
0.713
1.386
1.833
2.408
3.219
4.605
3
1.005
1.424
2.366
2.946
3.665
4.642
6.251
4
1.649
2.195
3.357
4.045
4.878
5.989
7.779
5
2.343
3.000
4.351
5.132
6.064
7.289
9.236
6
3.070
3.828
5.348
6.211
7.231
8.558
10.645
Note: A data set with n degrees of freedom will have probability v that its value of χ2 exceeds
the tabulated value.
It is also useful to note the moment-generating function for this distribution:
⟨etχ2⟩=
1
(1 −2t)n/2 ,
(23.102)
a result that follows directly from Eq. (23.85). Differentiating Eq. (23.102), we ﬁnd
⟨χ2⟩= d(1 −2t)−n/2
dt

t=0 = n,
⟨(χ2)2⟩= d2(1 −2t)−n/2
dt2

t=0 = n(n + 2), (23.103)
and therefore
σ 2(χ2) = ⟨(χ2)2⟩−⟨χ2⟩2 = n(n + 2) −n2 = 2n.
(23.104)
These results suggest that typical data with realistically assigned individual-measurement
variances would yield a value of χ2 comparable to the number of data points. However,
by calculating
P(χ2 > y0) =
∞
Z
y0
g(y)dy,
(23.105)
where g(y) is the distribution in Eq. (23.101), we can obtain for any y0 the probability
that a data set would have a larger spread than that corresponding to χ2 = y0. Because
it is somewhat laborious to compute the integral in Eq. (23.105), its values are generally
obtained by table lookup. A short table of these chi-square data are given in Table 23.2.
Before closing this subsection, we need to deal with the fact that our random variables
Xj do not really have zero mean values if the function u(t j,...) was chosen based on
the available data and therefore was not exact. By reasoning similar to that involved in
the discussion leading to Eq. (23.95), it can be shown that if a chi-square ﬁt involves
n data points and the determination of r parameters, the effective number of degrees of
freedom is n −r, with the implication that the inexactness of the ﬁt in r degrees of freedom
corresponds to a chi-square distribution with n replaced by n −r.
Finally, it is worth pointing out that the χ2 analysis does not really test the assumptions
that the data points are independent normal random variables. If these assumptions are not
approximately valid, it is unlikely that good chi-square ﬁts can be achieved.

23.7 Statistics
1173
Example 23.7.2
CHI-SQUARE FIT
Let us apply the χ2 function to a straight-line ﬁt of the type shown in Fig. 23.9, with the
three measured points and their individual standard deviations, written as (t j,u j ± σj),
having the values
(1,0.8 ± 0.1), (2,1.5 ± 0.05), (3,2.7 ± 0.2).
Before proceeding to the chi-square ﬁt, we ﬁrst ﬁt a line assuming the points to be equally
weighted, corresponding to using Eq. (23.96) for the slope a. We ﬁnd
a = 1(0.8) + 2(1.5) + 3(2.7)
12 + 22 + 32
= 11.9
14 = 0.850.
The sample variance of the points from the line is
σ 2 = 1
2

[0.8 −1(0.850)]2 + [1.5 −2(0.850)]2 + (2.7 −3(0.850)]2
= 0.0325,
and the variance of a is
σ 2(a) =
X
j
 ∂a
∂u j
2
σ 2 =
X
j
 
t j
P
k t2
k
!2
σ 2 =
σ 2
P
k t2
k
= 0.0325
14
= 0.00232.
Thus, the unweighted ﬁt yields a = 0.850 ±
√
0.00232 = 0.850 ± 0.048.
Turning now to the chi-square ﬁt, we next minimize
χ2 =
X
j
u j −at j
σj
2
with respect to a. This process yields
∂χ2
∂a = −2
X
j
t j(u j −at j)
σ 2
j
= 0,
or
a =
X
j
t ju j
σ 2
j
,X
j
t2
j
σ 2
j
.
In our case
a =
1(0.8)
0.12 + 2(1.5)
0.052 + 3(2.7)
0.22
12
0.12 +
22
0.052 + 32
0.22
= 1482.5
1925 = 0.770.
The value we obtained for a is dominated by the middle point, the smallest σj; if that point
were the only one used, we would have gotten a = 1.5/2 = 0.75. The variance of a, σ 2(a),
is now
σ 2(a) =
X
j
 ∂a
∂u j
2
σ 2
j =
X
j
 
t j/σ 2
j
P
k t2
k /σ 2
k
!2
σ 2
j =
1
P
k t2
k /σ 2
k
=
1
1925 = 0.000519.
The chi-square estimate of a is therefore a = 0.770 ±
√
0.000519 = 0.770 ± 0.023.

1174
Chapter 23 Probability and Statistics
Our ﬁt has for χ2 the value
χ2 = [0.8 −1(0.770)]2
0.12
+ [1.5 −2(0.770)]2
0.052
+ [2.7 −3(0.770)]2
0.22
= 4.533.
Our problem involves three points and one parameter, and therefore its chi-square distri-
bution has two degrees of freedom and, according to Eqs. (23.103) and (23.104), has a
mean value of 2 and a variance of 4. Our value of χ2, 4.533, is signiﬁcantly larger than
the mean value of the distribution and therefore describes a data set with more spread than
would normally be expected for the stated values of σj. We can obtain a more quantitative
measure of the probability that χ2 would be at least as large as our value by comparing
with the entries in Table 23.2. Using the row of the table for n = 2, we see that the prob-
ability of getting a spread larger than that of our present data is quite small, only slightly
above 0.1.
■
Student t Distribution
The Student t distribution (sometimes just called the t distribution) is designed for use
with small data sets for which the variance is unknown. This distribution was ﬁrst de-
scribed by W. S. Gosset, who published his work under the pen name “Student” because
his employer, the Guinness brewery, would not permit him to publish it under his own
name.
Gosset considered the probability distribution of a random variable T , of the form
T = Y√n
√S/n .
(23.106)
For the applications under consideration here,
Y = 1
n
n
X
j=1
Xj −µ = ¯X −µ,
(23.107)
S =
n
X
j=1
X2
j .
(23.108)
Here the Xj are a set of n independent Gauss normal random variables, each of the same
unknown variance σ 2. The quantity µ is the (unknown) value of the mean of X. An
important feature of Gosset’s choice for T is that (as we shall shortly show) its proba-
bility distribution fn(t) is independent of the variance of the Xj.
The procedure for obtaining the probability distribution of T depends on the fact that
Y and S are independent random variables. That is so, but proof is beyond the scope of
the present abbreviated discussion. We start by noting that S is a gamma distribution,
with probability distribution g(n,σ;s), as given in Eq. (23.85). Next, we proceed to the
distribution of U = √S/n, which we denote h(u). Making a change of variable from s to
nu2, and observing that ds = (ds/du)du, we ﬁnd
h(u) = g(n,σ;nu2)(2nu).
(23.109)

23.7 Statistics
1175
This is the probability distribution of the denominator of T. To get the distribution of the
numerator, we note that Y is a normal distribution with variance σ 2/n (see Eq. (23.92)),
and mean zero. It, therefore (see Eq. (23.56)), has the distribution we denote r(y), of the
form
r(y) =
r
n
2πσ 2 e−ny2/2σ 2.
(23.110)
The numerator, Z = Y√n, will therefore have a distribution k(z), where z = y√n, so
k(z) = r(z/√n)(dy/dz) =
1
√
2πσ 2 ez2/2σ 2;
(23.111)
the presence of the factor √n causes the numerator to have variance σ 2. Finally, we use
the formula for the ratio of two independent distributions, Eq. (23.81), to obtain
f (t) =
∞
Z
0
k(ut)h(u)u du.
(23.112)
The integration only extends from zero to inﬁnity because the gamma distribution in h(u)
is only nonzero for positive u. Inserting expressions for the quantities in Eq. (23.112),
fn(t) =
1
√
2πσ 2
∞
Z
0
e−u2t2/2σ 2g(n,σ;nu2)(2nu2)du
=
1
√
2πσ 2
1
2n/2σ n0(n/2)
∞
Z
0
e−u2t2/2σ 2(nu2)(n/2)−1e−nu2/2σ 2(2nu2)du
=
2
σ n+1√πn
n
2
(n+1)/2
1
0(n/2)
∞
Z
0
e−u2(t2+n)/2σ 2undu.
(23.113)
To complete the evaluation, we change variables in the integral to z = u2(t2 + n)/2σ 2,
thereby making the integral identiﬁable as a gamma function, so
∞
Z
0
e−u2(t2+n)/2σ 2undu = 1
2
 2σ 2
t2 + n
(n+1)/2 ∞
Z
0
z(n−1)/2e−zdz
= 1
2
 2σ 2
t2 + n
(n+1)/2
0
n + 1
2

.
(23.114)
Inserting this result into Eq. (23.113) and simplifying, we note that the instances of σ
entirely cancel, and we are left with
fn(t) =
0
n + 1
2

√πn0
n
2


1 + t2
n
−(n+1)/2
.
(23.115)

1176
Chapter 23 Probability and Statistics
0.4
T density for n =2, 10, 20, and 30
0.35
0.3
0.25
0.2
0.15
0.05
−4
−3
−2
−1
0.1
0
1
2
3
4
n =2
n =10
n =20
n= 30
FIGURE 23.13
Student t probability density fn(t) for n = 2, 10, 20, and 30.
Equation (23.115) is the probability density for the t distribution with n degrees of free-
dom. This equation shows that we have achieved the desired result, namely that the dis-
tribution T is independent of the variance of the input random variables Xj. Since it is
our intention to use the t distribution for the reduction of experimental data of unknown
variance, we have achieved our current objective. Figure 23.13 shows densities fn(t) for
several n; an important feature of these curves is that they depend very weakly on n.
Conﬁdence Intervals
A conﬁdence interval for a random variable X is the range within which x will fall, not
with certainty but with a high probability, the conﬁdence level, which we can choose. If
X has a probability distribution f (x), the conﬁdence interval for probability p will be
the range of x, usually symmetrically centered about some value x0, that contains the
fraction p of its probability distribution. If this range is bounded by x0 −dx and x0 + dx,
it is customary to write that x = x0 ± dx with (100p)% conﬁdence. If, for example, a
computed value of x is 0.50 and 90% of its probability distribution falls between x = 0.40
and x = 0.60, we say that x has the value x = 0.50 ± 0.10 with 90% conﬁdence.
Conﬁdence intervals are usually found by what is called the pivotal method, which
involves relating the variable for which we desire a conﬁdence interval to a known proba-
bility distribution. The identiﬁcation and selection of pivotal quantities is in general outside
the scope of this text, but for a Gauss normal random variable with zero mean, a suitable
pivot is its t distribution. Referring to Eq. (23.106), this means we can estimate a conﬁ-
dence interval for Y (the deviations of the observed mean ¯X from its true value) from the
equation
Y = ¯X −µ = T √S/n
√n
,
(23.116)

23.7 Statistics
1177
where T is the random variable corresponding to the t distribution and S is the single value
obtained by inserting the observed values of the Xi into Eq. (23.108). We use Eq. (23.116)
by inserting into it the range of T that corresponds to a total probability p, calculating
therefrom the corresponding range of Y. Note that we do not insert a probability distribution
for S; we use the value of S arising from our data.
The distribution of T is an even function of t with a maximum at t = 0, as is obvious
from Eq. (23.115) and Fig. 23.13, and our conﬁdence interval for T will naturally be cen-
tered about zero. Therefore, a conﬁdence interval of probability p will correspond to a
symmetric range of t, (−C p < t < +C p), such that
P(−C p < t < +C p) = p.
Because f (t) is even, we also have
P(−∞< t < +C p) = 1
2 + (−C p < t < C p)/2,
which is equivalent to
P(−∞< t < +C p) = 1 + p
2
≡ˆp.
(23.117)
Because of the frequent need to use values of C corresponding to various values of ˆp and
degrees of freedom n, these C values have been tabulated and appear in many statistics
texts. A short table is included here (Table 23.3).
Given a conﬁdence interval for T , we may insert it into Eq. (23.116), which when solved
for µ becomes
µ = ¯X −T
√S/n
√n
= ¯x −T σ
√n .
(23.118)
From the limiting values for T, we get the corresponding range for µ, which is valid with
the probability of the T range. Note that except for the range of T, all the quantities on
the right-hand side of Eq. (23.118) are to be computed from our sample data. In particular,
we need the mean value ¯X for our sample and the standard deviation of our data points,
σ = √S/n. Note further that, as with the chi-square distribution, when measured data
are used to generate the sample mean and sample standard deviation, the appropriate t
distribution to use for n data points is that with n −1 degrees of freedom, and in using
Table 23.3
Student t Distribution
ˆp
n = 1
n = 2
n = 3
n = 4
n = 5
0.8
1.38
1.06
0.98
0.94
0.92
0.9
3.08
1.89
1.64
1.53
1.48
0.95
6.31
2.92
2.35
2.13
2.02
0.975
12.7
4.30
3.18
2.78
2.57
0.99
31.8
6.96
4.54
3.75
3.36
0.999
318.3
22.3
10.2
7.17
5.89
Note: Entries are the values C in
R C
−∞fn(t)dt = ˆp, where fn(t) is
given in Eq. (23.115), with n the number of degrees of freedom.

1178
Chapter 23 Probability and Statistics
Eq. (23.118) it is customary to take σ as the sample standard deviation, as deﬁned in
Eq. (23.95). These points are explained more fully in several of the Additional Readings.
Example 23.7.3
CONFIDENCE INTERVAL
Suppose we have the following random data from a population that can be assumed to have
a Gauss normal distribution:
7.12
4.95
6.18
5.69
2.90
8.47,
and we wish to determine 90% and 95% conﬁdence intervals for the population mean.
Since we have neither the population mean nor variance, but have assumed the popula-
tion distribution to be normal, we can use the t distribution as just outlined. As a prelimi-
nary to doing so, we need to calculate the sample mean and standard deviation. Since we
have six data points, the number of degrees of freedom will be n = 5. We have
¯X = (7.12 + 4.95 + 6.18 + 5.60 + 2.90 + 8.47)/6 = 5.885,
σ =
1
5

(7.12 −5.885)2 + ··· + (8.47 −5.885)21/2
= 1.9035.
Considering ﬁrst the 90% conﬁdence interval that corresponds to the range (−C90 < t <
C90) with ˆp = (1 + p)/2 = 0.95, we read from Table 23.3 the value C90 = 2.02. Thus,
µ = 5.885 ± (2.02)(1.9035)
√
5
= 5.885 ± 1.720
(90% conﬁdence).
For 95% conﬁdence, we need C95, again for n = 5. This time, ˆp = 0.975, so C95 = 2.57,
and
µ = 5.885 ± (2.57)(1.9035)
√
5
= 5.885 ± 2.188
(95% conﬁdence).
A few ﬁnal observations are in order. First, we see that by demanding an increase in
the conﬁdence level, the interval probably containing the true mean becomes wider. Note
that at high conﬁdence levels the probable width can become much larger than the sample
standard deviation. Finally, note that even the conﬁdence intervals are sample-dependent.
Other data from the same population could generate intervals of different widths. Perhaps
oversimplifying, these analyses show that there is no way of converting probability data
into signiﬁcant statements that have complete certainty.
■
Exercises
23.7.1
Let 1A be the error of a measurement of A, etc. Use error propagation to show that
σ(C)
C
2
=
σ(A)
A
2
+
σ(B)
B
2
holds for the product C = AB and the ratio C = A/B.

23.7 Statistics
1179
23.7.2
Find the mean value and standard deviation of the sample of measurements x1 = 6.0,
x2 = 6.5, x3 = 5.9, x4 = 6.1, x5 = 6.2. If the point x6 = 6.1 is added to the sample, how
does the change affect the mean value and standard deviation?
23.7.3
Carry out a χ2 analysis of the ﬁt corresponding to Fig. 23.10 using the same points as
in Example 23.7.2, but with the errors now associated with the ti rather than the yi.
23.7.4
Using the data from Exercise 23.7.2 (including the point x6), ﬁnd the 90% and 95%
conﬁdence intervals for the mean of the xi.
Additional Readings
Bevington, P. R., and D. K. Robinson, Data Reduction and Error Analysis for the Physical Sciences, 3rd ed.
New York: McGraw-Hill (2003).
Chung, K. L., A Course in Probability Theory Revised, 3rd ed. New York: Academic Press (2000).
DeGroot, M. H., Probability and Statistics, 2nd ed. Reading, MA: Addison-Wesley (1986).
Devore, J. L., Probability and Statistics for Engineering and the Sciences, 5th ed. New York: Duxbury Press
(1999).
Freund, J. E., and R. E. Walpole, Mathematical Statistics, 4th ed. Englewood Cliffs, NJ: Prentice Hall (1987).
This well-regarded text is at a level comparable to the exposition in this chapter. Clear and with many statistical
tables.
Kreyszig, E., Introductory Mathematical Statistics: Principles and Methods. New York: Wiley (1970).
Montgomery, D. C., and G. C. Runger, Applied Statistics and Probability for Engineers, 2nd ed. New York:
Wiley (1998).
Papoulis, A., Probability, Random Variables, and Stochastic Processes, 3rd ed. New York: McGraw-Hill (1991).
Ramachandran, K. M., and C. P. Tsokos, Mathematical Statistics with Applications. New York: Academic Press
(2009). Relatively detailed but readable and self-contained.
Ross, S. M., First Course in Probability, 5th ed., vol. A. New York: Prentice Hall (1997).
Ross, S. M., Introduction to Probability and Statistics for Engineers and Scientists, 2nd ed. New York: Academic
Press (1999).
Ross, S. M., Introduction to Probability Models, 7th ed. New York: Academic Press (2000).
Suhir, E., Applied Probability for Engineers and Scientists. New York: McGraw-Hill (1997).

INDEX
Page numbers followed by ‘ f ’ and ‘t’ indicate ﬁgures and tables, respectively.
Numbers
0-forms, 233
1-D axial Green’s function, 684
1-forms, 233
2-D integration, 74f
region, 70, 71f
2-forms, 233
3-forms, 233
A
Abel equation, generalized, 1055–1056
Abel’s test, 23
abelian group, 816, 822
absolute convergence, 13, 23, 29
abstract group, 819, 819t
addition
of gamma-distribution random variables, 1164
of tensors, 208
of random variables, 1160–1161
addition by scalar, 255
addition rule, for probabilities, 1128
addition theorem
application
Laplace expansion, 799–801
spherical wave expansion, 798–799
for spherical harmonics, 797–798
for normal distributions, 1161
adjoint matrix, 105
adjoint operator, 277, 297
and scalar product, 278
basis expansion of, 281–282
ﬁnding, 278
afﬁne transformation, 1104
algebraic formula, 51
aliasing, 1006
alternating series
absolute convergence, 13
Leibniz criterion, 11–12
angular momentum, 126, 126f , 299
angular momentum operators, 774–776
angular momentum formulas, 781–782
exercises, 782–784
ladder operators, 776–779
spinor, 779–781
coupling, 784–786
Clebsch-Gordan coefﬁcients, 789–791
exercises, 795–796
ladder operators construction, 788–795
of p and d electrons, 793–795
spinors, 792–793
vector model, 786–788
angular momentum formulas, 781–782
angular momentum operators, 774–776
angular momentum formulas, 781–782
exercises, 782–784
ladder operators, 776–779
spinor, 779–781
annihilation operator, 882
anti-Hermitian, 277
anti-Hermitian matrices, 108, 319
antiderivation, 239
antisymmetric stretching mode, 323
antisymmetric tensor, 208, 216
arbitrary probability distribution, 1157
arbitrary-vector technique, 167
Argand diagram, 56, 56f , 57, 470, 492
arithmetic mean, 1137
associated Laguerre equation, 894
associated Laguerre polynomials
generating function, 892–895
associated Legendre equation, 425, 716, 741–743
exercises, 753–756
magnetic ﬁeld of current loop, 748–753
orthogonality, 746–748
parity and special values, 746
associated Legendre functions, 425, 744–745,
745t
associated Legendre polynomials, 743–744
associative, 96, 97
asymptotic expansions, 581
asymptotic forms, 692–693
of Hankel functions, 688–690
properties of, 693–695
exercises, 695–698
of an integral representation, 690–692
Stokes’ method, 688
1181

1182
Index
asymptotic series, 577
Bessel functions, 691
cosine and sine integrals, 580–582
deﬁnition of, 582–583
exercises, 583–584
exponential integral, 578–580
integral representation expansion, 690–692
overview, 577
asymptotic values, Bessel functions, 690, 703
atomic interaction integral, 72
average value, 1136
axial Green’s function, 464
axial vectors, 136, 215
B
Baker-Hausdorff formula, 114
band-pass ﬁlter, 1002f
baryons, 852, 853t
multiplets, decomposition of, 858–861, 859f ,
860f
basis expansion, adjoint, 281–282
basis functions, 252, 253
Bayes’ theorem, 1130
Bernoulli equation, 330, 378
Bernoulli numbers, 556, 562t
contour of integration for, 563f
exercises, 566–567
generating-function, 560
overview, 560–565
polynomials, 565–566
Riemann zeta function, 564
Bernoulli polynomials, 565–566
Euler-Maclaurin integration formula, 567
Bessel functions, 67
asymptotic expansions
asymptotic forms, 692–695
exercises, 695–698
Hankel functions, asymptotic forms of,
688–690
of an integral representation, 690–692
asymptotic values, 690
of ﬁrst kind
Bessel’s differential equation, 646–647
conﬂuent hypergeometric representation, 919
cylindrical resonant cavity, 650–653
exercises, 654–661
Fraunhofer diffraction, circular aperture,
648–650
Frobenius method, 643
generating function for integral order,
644–645
integral representation, 647–648
modiﬁed, 681
orthogonality, 661
recurrence relations, 645–646
second kind, 644
Wronskian, 670–671
Hankel functions
contour integral representation of, 676–678
deﬁnitions, 674–675
exercises, 678–680
Helmholtz equation, 680, 698, 705
hyperbolic, 683
Laplace equation, 651
modiﬁed, 428, 643, 678, 682f
asymptotic expansion, 688
contours, 696f
exercises, 688
Fourier transforms, 684
Green’s function, 684–685
Hankel function, 682
hyperbolic Bessel functions, 683
integral representation, 683–684, 690–692
Laplace equations, 680
recurrence relations, 681–682
series expansion, 681
Whittaker functions, 682
Neumann functions, Bessel functions of second
kind
coaxial wave guides, 672
deﬁnition and series form, 667–669
exercises, 674
integral representations, 669
recurrence relations, 669–670
uses of, 671
Wronskian formulas, 670–671
orthogonality
Bessel series, 663
electrostatic potential in a hollow cylinder,
663–664
exercises, 665–667
Neumann boundary condition, 662
normalization, 662
Sturm-Liouville theory, 661
PDEs, 643
recurrence relations, 645–646
of second kind, 667
Schlaeﬂi integral, 653–654
spherical, 643
asymptotic values, 703
deﬁnitions, 702
exercises, 709–712
Helmholtz equation, 698
limiting values, 703
modiﬁed, 709
orthogonality and zeros, 703
particle in a sphere, 704–706
recurrence relations, 702
waves, 703

Index
1183
of third kind, 675
in wave guides, 671–672
zeros, 648–653
Bessel series, 663
Bessel’s correction, 1168
Bessel’s differential equation, 646–647
Bessel’s equation, 344–345, 366–367, 1025–1027
limitations of series approach, 351–353
Bessel’s inequality, 262
beta function, 617
deﬁnite integrals, alternate forms, 618
derivation of Legendre duplication formula,
618–619
exercises, 619–622
binomial coefﬁcients, 34
binomial distribution, 1148–1151
limits of, 1157–1158
and Poisson distribution, 1153–1154, 1154f
binomial expansion, application of, 41–42
binomial probability distribution, 1149, 1150f
binomial theorem, 33–36, 493–494, 581, 716
exercise, 36–40
Biot-Savart law, 750, 751, 751f
black hole, optical path near event horizon of,
1087–1088, 1088f
Bohr radius, 897, 1135
Born approximation, quantum mechanical
scattering, 465–466
Bose-Einstein statistics, 1132, 1133
bosons, 840
boundary conditions, 381, 405
Cauchy, 412
Dirichlet, 412, 985
Green’s function, 452–454
hollow cylinder, 664
homogeneous, 448
Neumann, 412
ring of charge, 730
speciﬁc, 438–439
sphere in uniform electric ﬁeld, 728
sphere with, 428–430
waveguide, coaxial cable, 671
boundary curve, 406
boundary value problem, 1052
brachistochrone problem, 1082
branch cut (cut line), 500, 508f
exploiting, 534–537
using, 534–535
branch points, 499–503, 499f , 500f , 502f , 503f ,
503t, 536–537
avoidance of, 532–534
of order 2, 500
Bravais lattice, 869
Bromwich integral, 1038–1040, 1040f
brute-force approach, 32
C
calculus of residues
Cauchy principal value, 512–515, 512f , 515f
computing residues, 510–511
counting poles and zeros, 518–519
exercises, 520–522
pole expansion of meromorphic functions,
515–518
product expansion of entire functions, 519–520
residue theorem, 509–510, 509f
calculus of variations
Euler equation, 1081–1085
alternate forms of, 1088
exercises, 1093–1096
optical path near event horizon of a black
hole, 1087–1088, 1088f
soap ﬁlm, 1088–1090, 1089f
soap ﬁlm–minimum area, 1090–1093, 1092f
straight line, 1086
Lagrangian multipliers, 1107–1109
Rayleigh-Ritz variational technique, 1117–1118
ground state eigenfunction, 1118–1119
several dependent variables, 1096–1097, 1102
exercises, 1105–1107
Hamilton’s principle, 1097–1098
Laplace’s equation, 1101–1102
moving particle–Cartesian coordinates,
1098–1099
moving particle–circular cylindrical
coordinates, 1099
several independent variables, 1100–1102
variation with constraints, 1111–1112
exercises, 1121–1124
Lagrangian equations, 1112–1113
Schrödinger wave equation, 1116–1117
simple pendulum, 1113–1114, 1113f
sliding off a log, 1114–1115, 1114f
canonical momentum, 1099
Cartesian coordinate system, 47
Cartesian coordinates, 415–420
spherical harmonics using, 758
Casimir operators, 849
Catalan’s constant, 13, 572, 613
catenoid, catenary of revolution, 1090
Cauchy (Maclaurin) integral test, 5–8
Cauchy boundary conditions, 412
Cauchy criterion, 2
Cauchy inequality, 490
Cauchy principal value, 512–515, 512f , 515f ,
Cauchy ratio test, 1065
Cauchy root test, 4
Cauchy’s integral formula, 486–487, 554, 591
applications of, 490
derivatives, 488
exercises, 491–492

1184
Index
Cauchy’s integral formula (continued)
Morera’s theorem, 489–490
Cauchy’s integral theorem
contour integrals, 477–478, 478f
exercises, 485, 486f
Goursat proof, 481–482, 481f
Laurent expansion, 492–497
multiply connected regions, 483–484, 483f
statement of, 478–481, 480f
Cauchy-Riemann conditions, 471–477
analytic functions, 472–474
derivatives of, 474–475
exercises, 476–477
overview, 471–472
point at inﬁnity, 475
Cauchy-Riemann differential equations, 591
causality, 591
cavities, cylindrical, 650–653
central ﬁeld potential, Laplacian of, 154
central force, 192
central force problems, 426
central moments, 1141
chain rule, 63
chaotic behaviour, 377
character, 831, 832t
characteristic curves, 405
characteristic equation, 303
characteristic function in probability theory, 1160
characteristic polynomial, 303
characteristics of PDEs, 404–406
charge density, 739
Chebyshev differential equation, 388
Chebyshev inequality, 1140
Chebyshev polynomials
exercises, 907–911
generating functions, 899
hypergeometric representations, 914
numerical analysis, 905–906
orthogonality, 906–907
recurrence relations, 901–903
shifted, 908
trigonometric form, 904–905
type I, 900
type II, 899
ultraspherical polynomials, 899
chi square ﬁt, 1170, 1173–1174
chi-square (χ2) distribution, 1170–1174
Christoffel symbols, 222
evaluating, 223–224
circle of convergence, 493
circular contour
zn on, 479
circular cylindrical coordinates, 187–190, 188f ,
421, 431
cylindrical eigenvalue problem, 422–424
circular disk, rotations of, 818
circular functions, 58–59
circular membrane, Bessel functions, 659
circular optical path, 1088f
circular wave guide, 672
circular wire loop, 931f
classes, 830–835
Clausen functions, 949
Clebsch-Gordan coefﬁcients, 789–791
Clifford algebra, 112
closed loop, 499–501, 499f , 500f
closure relation, 264
coaxial wave guides, 671–672
coefﬁcient vector, 261
colatitude, 72
collinear velocities, addition of, 864
column vector, 95, 123, 125
extraction of, 108
combinations, counting of, 1130–1133
commutation rules, 785–786
commutative, 96, 816
commutative operation, 47
commutator, 97, 276
comparison tests, 3–4
completeness, 255, 262
of Hilbert-Schmidt of integral equations, 1073
complex conjugation, 54, 470
complex exponentials, integrals with, 527–531,
529f
complex numbers
and functions, 53
Cartesian components, 53
circular and hyperbolic functions, 58–59
complex domain, 55–56
exercises, 60–61
imaginary numbers, 54
logarithm, 60
polar representation, 56–58
powers and roots, 59
multiplication of, 54
of unit magnitude, 57
complex plane, 56
complex variable theory, 53
complex variables, see also Cauchy-Riemann
conditions; mapping; singularities
algebra using, permanence of algebraic form, 55
Cauchy’s integral formula, 591
causality, 591
dispersion relations

Index
1185
exercises, 596–597
optical dispersion, 594–595
overview, 591–593
Parseval relation, 595–596
symmetry relations, 593
functions of, 470
computing residues, 510–511
conditional convergence, 13
conditional probability, 1128
conditional probability distributions, 1147
Condon-Shortley phase, 758, 760t
conﬁdence interval, 1176–1178
conﬂuent hypergeometric functions, 912
asymptotic expansions, 919
Bessel and modiﬁed Bessel functions, 918–919
exercises, 920–922
Hermite functions, 919
Laguerre functions, 919
Whittaker function, 919
Wronskian, 922
conformal mapping, 549
conjugate subgroup, 820
conjugation, complex, 56, 105
connected, simply, 164
conservation laws, 815
conservative force, 171, 244
constant B ﬁeld, vector potentials of, 172
constant coefﬁcients, with ODEs, 342–343
constrained minima/maxima, 1107–1109
exercises, 1110
contiguous function relations, 913
continuous deformation, 484
continuous groups, 816, 845–846
exercises, 861
homomorphism SU(2)–SO(3), 851–852
Lie groups and their generators, 846–849
of representation, 824–825
SO(2) and SO(3), 849–851
SU(3), 852
continuous random variable, 1135–1137
contour integral, 477–478, 478f
contour integral representation, 676–678
contour integration, 967, 967f
singularity on, 530–531
methods, 572, 603
contraction, 209–210
contravariant basis vectors, 220–221
contravariant metric tensor, 219
contravariant tensors, 206–207
contravariant vectors, 206, 219
convergence
inﬁnite products, 575
inﬁnite series, partial sum approximation, 579
of Neumann series, 1066
convergence in the mean, 262
convergence of inﬁnite series
absolute, 13, 23
of power series, 29
rate, 16
tests, see also Cauchy (Maclaurin) integral test
comparison, 3–4
Gauss’, 9
improvement of, 16–17
Kummer’s, 8–10
uniform and nonuniform, 21–22
convergence, rate of, 16
convolution (Faltungs) theorem
driven oscillator with damping, 1035–1037
Parseval relation, 987–990
coordinate transformations
exercises, 138–139
of orthogonal, 135
of reﬂections, 136–137, 137f
of rotations, 133–135
of successive operations, 137–138
coordinates, see also Cartesian coordinates;
circular cylindrical coordinates; orthogonal
coordinates; spherical polar coordinates
curvilinear, 182
correlation, 1142–1144
cosines
asymptotic expansion, 581, 582
conﬂuent hypergeometric representation, 920
inﬁnite products, 575
integral of in denominator, 523–524
integrals cos in asymptotic series, 580–582
Coulomb’s law, 447, 730
counting poles and zeros, 518–519
coupling, angular momentum, see angular
momentum
covariance, 1142–1144
covariance of Maxwell’s equations, Lorentz, see
Lorentz covariance of Maxwell’s equations
covariant, 862
covariant basis vectors, 218, 220–221
covariant derivatives, 222–223
covariant metric tensor, 219
covariant tensors, 206–207
covariant vector, 206
Cramer’s rule, 84
creation operator, 882
criterion, Leibniz, 11–12
cross derivatives, 62
cross product, 126–128, 126f , 127f , see also triple
vector product
crossing conditions, 593
crystallographic point groups, 869
curl, ∇×, 149–153
circular cylindrical coordinates, 193
in curvilinear coordinates, 186–187, 186f

1186
Index
curvilinear coordinates, 182
differential operators in, 185–187, 185f , 186f
exercises, 196–203
integrals in, 184–185
cut line (branch cut), 500, 508f
exploiting, 534
using, 534–535
cylindrical symmetry, 443
cylindrical traveling waves, 694–695
D
d’Alembert ratio test, 4–5, 55, 578
d’Alembert’s solution, of wave equation, 436
damped oscillator, 1021
de Moivre’s Theorem, 59
decuplet, 859, 859f
defective matrices, 324
deﬁnite integral (Euler), 600–601
deﬁnite integrals, 580
evaluation of, 522
exercises, 538–544
degeneracy, 307–308
degenerate, 1057, 1072
delta function, Dirac, 263–265, 1010–1011
δ-sequence function, 76f
Dirichlet kernel, 77
exercise, 80–81
Fourier series, 77
Kronecker delta, 79–80
properties of, 78–79
sequence, 76
spherical polar coordinates, 79
denominator, integral of cos in, 523–524
dependent variable, 329
derivative operators, tensor, see tensor derivative
operators
derivatives, see also exterior derivatives
chain rule, 63
cross derivatives, 62
exercises, 64
mixed derivatives, 401
partial derivatives, 62, 401
stationary points, 63–64
determinants, 295
and linear dependence, 89–90
derivatives of, 102
exercises, 93–94
homogeneous linear equations, 83–84
inhomogeneous linear equations, 84
product theorem, 103–104
properties of, 87
deuteron, 391–393
diagonal matrices, 99
eigenvalues, 313
eigenvector, 312
diagonalization
matrices, 311–314
simultaneous, 314–315
differentiable manifolds, 233
differential equations
ﬁrst-order differential equations, 331–342
exact differential equations, 333
exercises, 339–342
homogeneous equations, 334–335
isobaric equations, 335
linear ﬁrst-order ODEs, 336–339
nonseparable ODEs, 333–334
parachutist, 331–332
RL circuit, 338–339
separable equations, 331
Fuchs’ theorem, 355
linear independence of solutions, 358–360
second solution, 362–363
series form of the second solution, 364–366
nonlinear, 377–380
number of solutions, 361
partial differential equations, 329
particular solution, 337
second solution
exercises, 370–374
ﬁnding, 362–363
for linear oscillator equation, 363
logarithmic term, 668
Neumann functions, 368–369
of Bessel’s equation, 366–367
series solutions, Frobenius method, 346–350,
350f
exercises, 355–358
expansion about, 350
Fuchs’ theorem, 355
limitations of series approach, Bessel’s
equation, 351–353
regular and irregular singularities, 353–354
symmetry of solutions, 350–351
singular points, 343–345, 345t
differential forms, 232
0-forms, 233
1-forms, 233
2-forms, 233
3-forms, 233
complementary, 235–236
exercises, 238, 243, 248
exterior algebra, 233–235
exterior derivatives, 238–243
Hodge operator, 235
in Minkowski space, 236–237
integration of, 243–248
Maxwell’s equations, 241–243
miscellaneous, 237–238
simplifying, 234

Index
1187
Stokes’ theorem on, 245
three-dimensional (3-D), 407
differential operators, 275
differential vector operators, 143
gradient, 143
properties, 153–157
exercises, 157–159
differentiate parameter, 67
differentiation
of forms, 238–243
power series, 30
diffraction, 648–650
diffusion partial differential equations, 437–444
digamma and polygamma functions, 610
digamma functions, 610–611
exercises, 614–616
Maclaurin expansion, computation, 613
polygamma function, 612
series summation, 613
dihedral, 818
dilogarithm
exercises, 926
expansion and analytic properties, 923–924
properties and special values, 924–926
dimensionality theorem, 831
dipole moment, 738
Dirac braket notation, 265
Dirac delta distribution, 972
Dirac delta function, see delta function, Dirac
Dirac gamma matrices, 112
Dirac half-braket notation, 265
Dirac matrices, 111
Dirac notation, 265–266
Dirac’s relativistic theory, 38
direct product, 108–112, 837–839
exercises, 837f , 840, 840t
generators for, 857–858
of tensors, 210–211
direct space, 964
Dirichlet boundary conditions, 385, 412, 704
Dirichlet conditions, 936, 985
Dirichlet kernel, 77
Dirichlet series
exercises, 573–574
overview, 571–572
discontinuous functions, 937–939
expansions in, 262–263
discrete Fourier transform, 1002–1007
aliasing, 1006
exercises, 1007
fast Fourier transform, 1006–1007
limitations, 1005
orthogonality over discrete points, 1002–1004
discrete groups, 815
classes, 830–835
exercises, 835–837, 836t, 837f
other, 835
discrete probability distributions, computing, 1136
discrete random variables, 1134–1135
discrete spectrum, 420
dispersion integral contour for, 592f
dispersion relations
causality, 591
crossing conditions, 593
exercises, 596–597
Hilbert transforms, 593, 595
optical dispersion, 594–595
overview, 591–593
Parseval relation, 595–596
sum rules, 596
symmetry relations, 593–594
divergence, ∇, 146–149, 149f
curvilinear coordinates, 185–186, 185f
divergent series, 4
division, of random variables, 1162
Doppler shift, 37
dot products, 49–50
gradient of, 143, 157
double factorial notation, 35
double series, rearrangement of, 18–19
driven oscillator with damping, 1035–1037
dual tensors, 216–217
E
Earth’s gravitational ﬁeld, 727
Earth’s nutation, 1018–1019, 1018f
eigenfunction, 299
eigenfunction completeness of Hilbert-Schmidt
of integral equations, 1073
orthogonal, 1069–1073
eigenfunction expansion of Green’s function,
460–461
eigenvalue problem, 422–424
eigenvalues
equations, 299–300
basic expansions, 300
equivalence of operator and matrix form, 300
of Hermitian matrices, 310
of Hilbert-Schmidt theory, 1073
eigenvectors, 300
normalizing, 304
of Hermitian matrices, 310–311
Einstein convention, 207
electric dipole, 738, 737f
electric multipoles, 737–739
electric quadrupole, 738
electromagnetic ﬁeld tensor, 866
electromagnetic wave equation, 156
electromagnetic waves, 1023–1024
electromagnetism, potentials in, 174

1188
Index
electron spin, 846
electrostatic potential
for ring of charge, 729–730
in hollow cylinder, 663–664
elementary functions, 1008–1010
elliptic integrals
deﬁnitions of, 928–929
exercises, 931–932
of ﬁrst kind, 928
limiting values, 930
period of simple pendulum, 927–928
of second kind, 928
series expansion, 929–930
elliptic partial differential equations (PDEs), 410
empty set, 1127
energy, relativistic, 35–36
entire function, 519–520
equality of matrices, 96
equation of continuity, 148
equations, see also Maxwell’s equations
motion and ﬁeld, 213
equilateral triangle, symmetry of, 817, 817f , 818f
error function, 637
error propagation, 1165–1168
essential (irregular) singular point, 344
essential singularities, 344, 498
Euclidean space, 237
Euler angles, 140, 140f
Euler equation, 1081–1085
alternate forms of, 1088
exercises, 1093–1096
soap ﬁlm, 1088–1090, 1089f
soap ﬁlm–minimum area, 1090–1093, 1092f
straight line, 1086
Euler identity, 113
Euler transformation, 43, 44
Euler-Maclaurin integration formula, 566
Bernoulli polynomials, 567
example, 569–570
exercises, 570–571
overview, 567–569
Euler-Mascheroni constant, 7, 33, 367, 675
event horizon, 1087
evolution operator, 1067
exact ODEs, 333–334
expansion, 736, see also Taylor’s expansion
Laplace expansion, 760–762, 799–801
pole, of meromorphic functions, 498, 515–518
product, of entire function, 519–520
spherical harmonic, 761
spherical wave, 798–799
expectation value, 283, 285, 297, 1136
in transformation basis, 295
exponential function, of Maclaurin theorem,
27–28
exponential integral, 578–580, 634–637
exterior algebra, 233
exterior derivatives, 238–243
exterior products, 233
extrema, 62–64
F
factorial function, asymptotic form of, 588
factorial notation, 606
faithful group, 822
Faraday’s law, 168
fast Fourier transform (FFT), 1006–1007
Feldheim’s formula, 885
Fermi-Dirac statistics, 1132, 1133
fermions, 840
FFT, see fast Fourier transform
ﬁeld equations, 213
ﬁnite wave train, 971–973, 972f
ﬁrst-order Born approximation, 1067
ﬁrst-order differential equations, 331–342
exact differential equations, 333
exercises, 339–342
homogeneous equations, 334–335
isobaric equations, 335
linear ﬁrst-order ODEs, 336–339
nonseparable ODEs, 333–334
parachutist, 331–332
RL circuit, 338–339
separable equations, 331
ﬁrst-order partial differential equations, 403
characteristics of, 404–406
exercises, 408–409
general, 406–407
ﬁxed and movable singularities, special solutions,
378–379
ﬂux, 148
Fourier convolution theorem, 1055
exercises, 994–997
multiple convolutions, 990–992
Fourier cosine series, 941
Fourier cosine, sine transforms, 966
Fourier expansions, characteristic of, 951
Fourier integral representation, 969–970
Fourier series, 77
applications of, 949–957
exercises, 952–957
full-wave rectiﬁer, 950–951, 951f , 952t
square wave, high frequencies, 949–950,
949f
deﬁnition of, 935
general properties, 935–949
discontinuous functions, 937–939
exercises, 945–949
periodic functions, 939–940
sawtooth wave, 937–939, 938f

Index
1189
Sturm-Liouville theory, 936–937
summation of a Fourier series, 944
symmetry, 940–941, 942f
Gibbs phenomenon
calculation of overshoot, 959–961
exercises, 961–962
square wave, 958–959
summation of series, 957–958
operations on, 942–944
Fourier sine series, 941
Fourier transform, 965–968
aliasing, 1006
convolution theorem, 985–987
of derivatives
heat ﬂow PDE, 983
wave equation, 981–982
discrete, see discrete Fourier transforms
exercises, 975–980, 985
fast, 1006–1007
of Gaussian, 969, 968f
inverse, 970–973
limitations on transfer functions, 1000–1001
momentum space representation, 993–994
of product, 992
properties of, 980–984
solution, 1054–1055
successes and limitations of, 984–985
in 3-D space, 973–975
unitary operator, 988
Fourier transforms–inversion theorem, ﬁnite wave
train, 971–973, 972f
Fourier-Mellin integral, 1039
Fraunhofer diffraction, Bessel function, 648–650
Fredholm equation, 1047, 1052, 1054, 1058, 1064
homogeneous, 1059–1060, 1069
inhomogeneous, 1076–1077
Fresnel integrals, 712f
Frobenius method, 643, 645
series solutions, 346–350, 350f
Fuchs’ theorem, 355, 692
full-wave rectiﬁer, 950–951, 951f , 952t
functions, 143
Chebyshev polynomials
exercises, 907–911
generating functions, 899
numerical analysis, 905–906
orthogonality, 906–907
recurrence relations, 901–903
trigonometric form, 904–905
type I, 900
type II, 899
ultraspherical polynomials, 899
conﬂuent hypergeometric functions
Bessel and modiﬁed Bessel functions,
918–919
exercises, 920–922
Hermite functions, 919
Laguerre functions, 919
Whittaker function, 919
dilogarithm
exercises, 926
expansion and analytic properties, 923–924
properties and special values, 924–926
Dirac delta, 263–265
discontinuous, 262–263
entire, 498
exponential, of Maclaurin theorem, 27–28
Hermite functions
applications of the product formulas,
885–887
direct expansion of products of Hermite
polynomials, 884–887
exercises, 876–878, 887–888
Hermite product formula, 884–887
molecular vibration, 882–883
orthogonality and normalization, 875–876
quantum mechanical simple harmonic
oscillator, 878–879
recurrence relations, 872–873
Rodrigues formula, 874
threefold Hermite formula, 883–884
values of, 873–874
hypergeometric functions, 911
conﬂuent, 912
contiguous function relations, 913
exercises, 915–916
hypergeometric representations, 913–914
Pochhammer symbol, 912
Laguerre functions
associated Laguerre polynomials, 892–895
differential equation–Laguerre polynomials,
890–892
exercises, 897–899
hydrogen atom, 896–897
Rodrigues formula and generating function,
889–890
of complex variables, 470
of operators, 282
orthonormal, 269–271
series expansions, 41–44
excercise, 44–45
series of
Abel’s test, 23
exercises, 24–25, 32–33
uniform and nonuniform convergence, 21–22
Weierstrass M test, 22–23
square-wave, 263
G
Galilean, 862
gamma distribution, 607, 1162–1164

1190
Index
gamma function, see also factorial function
analytic properties, 604
asymptotic form of, 588–589
beta function
deﬁnite integrals, alternate forms, 618
derivation of Legendre duplication formula,
618–619
exercises, 619–622
deﬁnitions, simple properties, 599
deﬁnite integral (Euler), 600–601
factorial notation, 606
inﬁnite limit (Euler), 599–600
inﬁnite product (Weierstrass), 602
incomplete beta function, 634
incomplete gamma functions and related
functions, 633–634
error function, 637
exercises, 638–641
exponential integral, 634–637
Riemann zeta function, 626–631
Stirling’s series, 622
derivation from Euler-Maclaurin integration
formula, 623–624
gamma function contour, 605f
gamma functional relation, 506
gauge condition, 174
gauge transformations, 174
Gauss elimination, 91–93
Gauss technique, 91
Gauss’ fundamental theorem of algebra, 490
Gauss’ law, 175–176, 175f
Gauss’ normal distribution, 1155–1159
Gauss’ test, 9
Legendre series, 9–10
Gauss’ theorem, 164–165, 165f , 176, 248
Green’s theorem, 165–166
Gegenbauer polynomials, see ultraspherical
polynomials
Gell-Mann matrices, 854
general coordinates, tensor in
covariant derivatives, 222–223
exercises, 226
metric tensor in, 218–219
general relativity, 862
generalized Abel equation, 1055–1056
generating function, 555, 1056
associated Laguerre polynomials, 892–895
Bernoulli numbers, 560
Bessel functions, modiﬁed, 919
Chebyshev polynomials, 899
electric multipoles, 737–739
exercises, 740–741
expansion, 736–737
Hermite polynomials, 555–556, 872
for integral order, 644–645
Laguerre polynomials, 889–890
Legendre polynomials, 557–558
physical interpretation of, 735
Taylor expansion of, 565
generators of continuous groups, 846–849
geodesics, 1103–1104
geometric properties, 47
geometric series, 2–3
Gibbs phenomenon
calculation of overshoot, 959–961
exercises, 961–962
square wave, 958–959
summation of series, 957–958
Goldschmidt discontinuous solution, 1091, 1092f
Goursat proof of Cauchy’s integral, 481–482,
481f
gradient, ∇
as differential vector operator, 143–146
in curvilinear coordinates, 185
of dot product, 157
Gram-Schmidt orthogonalization
example, 270–272
exercises, 273–275
orthonormalizing physical vectors, 273
overview, 269–270
physical vectors, 272
vectors by, 269–275
Gram-Schmidt process, 308
Gram-Schmidt transformation, 293
graphene, 869
Grassmann algebra, see exterior algebra
gravitational potential, 172
Green’s function, 447–467, 684–685, 983–984,
1050, 1052, 1072, 1075
advantage of, 452
axial, 464
boundary conditions, 452–454
accomodating, 464
at inﬁnity, 454
initial value problem, 453–454
differential vs. integral formulation, 456
eigenfunction expansion of, 460–461
exercises, 456–459, 466–467
features of, 448, 459–460
form of, 450–452, 461–466
fundamental, 462, 463t
general properties of, 449–450
Helmholtz equation, 463
Laplace’s equation, 462, 464
one-dimensional, 448–459
relation to integral equation, 454–456
self-adjoint problems, 460
spherical, 463, 800–801
two and three dimension problems, 459–467
Green’s theorem, 165–166, 246–247

Index
1191
Gregory series, 39
ground state, 391
ground state eigenfunction, 1118–1119
group theory, see also generators of continuous
groups; homogeneous Lorentz group
deﬁnition of, 816–817, 817, 818f , 818t
discrete
classes, 830–835
other, 835
exercises, 820, 821f
faithfulness, 822
homomorphic, 817
homomorphism and isomorphism, 819
isomorphic, 817
Lorentz covariance of Maxwell’s equations,
866–868
vierergruppe, 820
H
Hamilton’s equations, 1099–1100
Hamilton’s principle, 1097, 1098
Hankel functions, 682
asymptotic forms, 692, 693
contour integral representation of, 676–678
deﬁnition, 674–675
integral representation of, 698f
series expansion, 675
spherical, 701
Wronskian formulas, 675
Hankel transforms, 965, 1054
harmonic functions, 473
harmonic numbers, 3
harmonic oscillator, 878–879, 1017
harmonic series, 3
harmonics, 799, see also spherical harmonics;
vector spherical harmonics
Hartree atomic units, 396
heat ﬂow partial differential equations, 437–444,
983
Heaviside shifting theorem, 1023
Heaviside step function, 1010
Heisenberg uncertainty principle, 973
Helmholtz equation, 415, 422, 439, 705
Bessel functions, 680, 698
Green’s function, 463
spherical coordinates, 698
Helmholtz’s theorem, 177–180
Hermite equation, 390–391
Hermite functions
applications of the product formulas, 885–887
conﬂuent hypergeometric functions, 919
direct expansion of products of Hermite
polynomials, 884–887
exercises, 876–878, 887–888
Hermite polynomials, 872
Hermite product formula, 884–887
molecular vibration, 882–883
orthogonality and normalization, 875–876
quantum mechanical simple harmonic
oscillator, 878–879
recurrence relations, 872–873
Rodrigues formula, 874
threefold Hermite formula, 883–884
values of, 873–874
Hermite polynomial, see also Legendre
polynomials
Hermite polynomials, 280, 391, 554, 873f
direct expansion of products of, 884–887
example, 554–556
generating function, 555–556, 872
orthogonality integral, 875
recurrence relations, 872–873
Rodrigues representation, 874
Hermitian matrices, 108, 301
anti-, 319
diagonalization, 311–313
example, 313
exercises, 317–318
expectation values, 316
ﬁnding diagonalizing transformation,
313–314
positive deﬁnite and singular operators, 317
simultaneous, 314–315
spectral decomposition, 315–316
of eigenvalues, 310
unitary transformation, 313
Hermitian operator, 277, 284
expectation value, 316
self-adjoint ODEs, 384
Hilbert space, 255–256, 278, 279, 289
Hilbert transforms, 593, 595
Hilbert-Schmidt theory
homogeneous Fredholm equation, 1069
inhomogeneous Fredholm equation, 1076–1077
inhomogeneous integral equation, 1073–1077
orthogonal eigenfunctions, 1069–1073
symmetrization of kernels, 1069
Hodge operator, 235
homogeneous boundary condition, 448
homogeneous equations, 334–335
homogeneous Fredholm equation, 1059–1060,
1069
homogeneous linear equations, 83–84
ODEs, 330
homogeneous Lorentz group, 862–864
homogeneous ODEs, 335, 338, 351
second-order, 344
homomorphic group, 817
homomorphism, 819
SU(2) and SU(2)–SO(3), 851–852

1192
Index
Hooke’s law spring, 342–343
Hubble’s law, 52
hydrogen atom, 896–897
Schrödinger’s wave equation, 896
hyperbolic functions, 58–59
hyperbolic partial differential equations (PDEs),
410
hypercharge, 854
hypergeometric equation
alternate forms, 918
singularities, 345, 912, 917
hypergeometric functions, 911
conﬂuent, 912
contiguous function relations, 913
exercises, 915–916
hypergeometric representations, 913–914
Pochhammer symbol, 912
hypergeometric series, see hypergeometric
function
I
identity operator, 277
imaginary axis, 56
imaginary numbers, 54
imaginary part, 56
improper rotations, of coordinate system, 215
impulse function, 1011
impulsive force, 1020
incomplete beta function, 634
incomplete gamma functions, 633–634
of ﬁrst kind conﬂuent hypergeometric
representation, 917
indeﬁnite integral of f (z), 489
independence, linear, 671
independent variables, 329, 407–408, 411
indeterminate forms, 31
indicial equation, 348
indistinguishable particles, 1133
inertial frames, 815
inﬁnite limit (Euler), 599–600
inﬁnite product (Weierstrass), 602
inﬁnite products
convergence, 575
evaluate, 575
exercises, 576–577
overview, 574–575
sine and cosines, 575
inﬁnite series, 1, see also Taylor’s expansion;
power series
algebra of
alternating series, 11–13
convergence, 13
convergence: absolute, 13
convergence: Cauchy integral, 5–8
convergence: Cauchy root, 4
convergence: comparison, 3–4
convergence: conditional, Leibniz criterion,
15–16
convergence: d’Alembert ratio, 4–5
convergence: Gauss’, 9
convergence: Kummer’s, 8–10
convergence: Maclaurin integral, 5–8
convergence: test of, 3–11
convergence: uniform, 21–22, 29
divergence of squares, 15–16
double series, 18–19
exercises, 20–21
rearrangement of double, 18–19
exercises, 10–11, 13–14
fundamental concepts
geometric series, 2–3
harmonic, 3
of functions
Abel’s test, 23
exercises, 24–25
uniform and nonuniform convergence, 21–22
Weierstrass M test, 22–23
power series, 29–30
inﬁnity, boundary conditions at, 454
inhomogeneous Fredholm equation, 1076–1077
inhomogeneous integral equation, 1073–1077
inhomogeneous linear equations, 84
inhomogeneous linear ODEs, 375–377
exercises, 377
inhomogeneous Lorentz group, 862
inner product and matrix multiplication, 97–98
integer powers, 59
integers, sum of, 40–41, 41
integral equations
boundary condition, 1048
exercises, 1060–1064
feature of, 1048
Fredholm equation, 1047, 1052, 1054,
1058–1060, 1069
generating-function, 1056–1057
Green’s function, 454–456
Hilbert-Schmidt theory
exercises, 1077–1079
homogeneous Fredholm equation,
1059–1060
orthogonal eigenfunctions, 1069–1073
symmetrization of kernels, 1069
integral-transforms
Fourier transform solution, 1054–1055
generalized Abel equation, 1055–1056
introduction, 1047–1048
deﬁnition, 1047
exercises, 1053
linear oscillator equation, 1050–1052

Index
1193
momentum representation in quantum
mechanics, 1048–1049
transformation of differential equation into
integral equation, 1049–1050
linear, 1047
Neumann series
exercises, 1068
overview, 1064–1066
solution, 1066–1067
separable kernel, 1057–1059
Volterra equation, 1047, 1048, 1050, 1055
integral form, Neumann functions, 671
integral operator, 275, 1066
linear, 1070
integral representations, 647–648, 964
of dilogarithm, 924f
expansion of, 690–692
of Hankel functions, 698f
modiﬁed Bessel functions, 684
integral test, Cauchy, see Cauchy (Maclaurin)
integral test
integral theorems
exercises, 169–170
Gauss’ theorem, 164–165, 165f
Green’s theorem, 165–166
Stokes’ theorem, 167–168, 167f , 168f
integral transforms, 1054
convolution (Faltungs) theorem, driven
oscillator with damping, 1035–1037
convolution theorem, 985–987
Parseval relation, 987–990
Fourier transform of derivatives
heat ﬂow PDE, 983
wave equation, 981–982
Fourier transform of Gaussian, 968–969, 968f
Fourier transform solution, 1054–1055
generalized Abel equation, 1055–1056
inverse Laplace transform
Bromwich integral, 1038–1040, 1040f
exercises, 1042–1045
inversion via calculus of residues, 1040
multiregion inversion, 1041–1042, 1041f ,
1042f
Laplace transform of derivatives, 1016–1020
Earth’s nutation, 1018–1019, 1018f
impulsive force, 1019–1020
simple harmonic oscillator, 1017
use of derivative formula, 1017
Laplace transforms, 1008–1034
deﬁnition, 1008
Dirac delta function, 1010–1011
elementary functions, 1008–1010
exercises, 1014–1015
Heaviside step function, 1010
inverse transform, 1012t, 1011–1014, 1014f
partial fraction expansion, 1013
properties of, 1016–1034
step function, 1013–1014, 1014f
Laplace, Mellin, and Hankel transforms,
965–966
use of, 964f
integrals, 67, 764, 927, see also Cauchy
(Maclaurin) integral test; deﬁnite integrals;
elliptic integrals
containing logarithm, 532–534, 533f
contour, 581, 592, 581f
cosine, 580–582
deﬁnite, 580
evaluation of, 65
1-D integral, 66
differentiate parameter, 67
exercises, 74–75
integration by parts, 65
integration variables, 72–74
multiple integrals, 70–72
recursion, 69
trigonometric integral, 69
exponential, 578–580
of meromorphic function, 526–527, 527f
of three spherical harmonics, 803–805
oscillatory, 529–530
range, 525–527, 525f
sine, 580–582
trigonometric, 522–524
with complex exponentials, 527–531, 529f
integrating factors, 334
integration
by parts, 65, 568, 578
by parts of volume integrals, 163
contour of, 530–531
of power series, 30, 583
order, reversing, 70–71
technique, 531–532
variables, 72–74
intersections, 1127–1130, 1128f
invariants
example, 295
exercises, 296
overview, 294–295
inverse Fourier transform, 1055
inverse Laplace transform
Bromwich integral, 1038–1040, 1040f
exercises, 1042–1045
inversion via calculus of residues, 1040
multiregion inversion, 1041–1042, 1041f ,
1042f
inverse matrix, 99–102
inverse transform, 211, 1011–1014, 1012t
inversion
multiregion, 1041–1042, 1041f , 1042f

1194
Index
inversion (continued)
of power series, 32
via calculus of residues, 1040
inversion operation, 136
irreducible representations, 822
irreducible spherical tensors, 796
irregular (essential) singular point, 344
irregular sign changes, series with, 12–13
irregular singularities, 353–354
irregular solution, 369
irrotational, 152, 154–155
isobaric ODEs, 335
isomorphic group, 817
isomorphism, 819
isospin, SU(2), 852–861
isotropic tensors, 209
J
Jacobi method, 314
Jacobi-Anger expansion, 655
Jacobian, 73
2-D and 3-D, 229–230
deﬁniton, 227
direct approaches to, 231
exercises, 231–232
inverse of, 230–231
Jacobian determinant, 229
Jacobian matrix, 229
Jensen’s theorem, 585
Jordan’s lemma, 528
K
Kepler’s laws of planetary motion, 189–190
kernel, 963
of integral equation, 455
kernel equation, 1047, 1052f
separable, 1057–1059
kernel function, 447
Kirchoff diffraction theory, 166
Kirchoff’s law, 338
Korteweg-deVries equation, 413
Kronecker delta, 79–80, 209, 258, 805
Kronig-Kramers optical dispersion relations, 591,
594
Kummer’s test, 8–10
L
L’Hôpital’s rule, 31, 517, 516, 576, 662
ladder operators, 776–779
construction, 788–795
Lagrangian equations, 1112–1113
of motion, 1098
Lagrangian mechanics, 63
Lagrangian multipliers, 1107–1109
Laguerre functions
associated Laguerre polynomials, 892–895
differential equation–Laguerre polynomials,
890–892
exercises, 897–899
hydrogen atom, 896–897
Rodrigues formula and generating function,
889–890
Laguerre polynomials
associated
conﬂuent hypergeometric representation, 919
generating function, 892–895
integral representation, 895
orthogonality, 895
recurrence relations, 893
Rodrigues’ representation, 895
Schrödinger’s wave equation, 896
conﬂuent hypergeometric representation, 919
differential equation, 890–892
generating function, 889–890
recurrence relations, 890, 893
Rodrigues’ formula, 889–890
self-adjoint form, 894
Laplace convolution theorem, 1034–1038
exercises, 1037–1038
Laplace equation, 154, 433–434, 726, 1101–1102
Bessel functions, 651
for parallelepiped, 417–419
Green’s function, 462, 464
Laplace expansion, 760–761
Laplace series
expansion theorem, 762
gravity ﬁelds, 762
Laplace spherical harmonic expansion, 799–801
Laplace transforms, 965, 1008–1034, 1054
convolution theorem, 1056
of derivatives, 1016–1020
Earth’s nutation, 1018–1019, 1018f
impulsive force, 1019–1020
simple harmonic oscillator, 1017
use of derivative formula, 1017
deﬁnition, 1008
Dirac delta function, 1010–1011
elementary functions, 1008–1010
exercises, 1014–1015
Heaviside step function, 1010
inverse transform, 1011–1014, 1012t, 1014f
one-sided, 1008
operations, 1028t
other properties
Bessel’s equation, 1025–1027
change of scale, 1020
damped oscillator, 1021
derivative of a transform, 1024–1025
electromagnetic waves, 1023–1024
exercises, 1028–1034

Index
1195
integration of transforms, 1027
RLC analog, 1022, 1022f
substitution, 1020
translation, 1022–1023
partial fraction expansion, 1013
properties of, 1016–1034
step function, 1013–1014, 1014f
two-sided, 1008
Laplacian, 154
development by minors, 88
in circular cylindrical coordinates, 192
of vector, 155–156
Laurent expansion
exercises, 496–497
Laurent series, 494–496
Taylor expansion, 492–494, 493f
Laurent series, 33, 494–496, 644
least squares, method of, 1138
Legendre duplication formula, derivation of,
618–619
Legendre equation, 425, 716
Legendre functions, 425, 715, 768f
associated, 744–745, 745t
hypergeometric representation, 914
recurrence formulas for, 745–746, 764
associated Legendre equation, 741–743
exercises, 753–756
magnetic ﬁeld of current loop, 748–753
orthogonality, 748
parity and special values, 746
generating function
electric multipoles, 737–739
exercises, 740–741
expansion, 736–737
physical interpretation of, 735
Legendre polynomials, 716
associated, 743–744
exercises, 722–724
recurrence formulas, 718–720
Rodrigues formulas, 720–721
upper and lower bounds for Pn(cosθ), 720
of second kind, 766
alternate formulations, 769–770
exercises, 770–771
properties, 769
orthogonality, 724
Earth’s gravitational ﬁeld, 727
electrostatic potential for ring of charge,
729–730
exercises, 730–735
Legendre series, 726–730
sphere in uniform ﬁeld, 727–729, 728f
spherical harmonics, 756
Cartesian representations, 758
exercises, 765–766
Laplace expansion, 760–762
properties of, 764–765
solutions, 758–760
symmetry of solutions, 762–763
Legendre ordinary differential equations (ODEs),
716
Legendre polynomials, 270–271, 425, 557–558
719t
associated, 743–744
exercises, 722–724
generating function, 557–558, 716, 735
orthogonality of, 726
recurrence formulas, 718–720
Rodrigues formulas, 720–721
Schlaeﬂi integral, 557
upper and lower bounds for Pn(cosθ), 720
Legendre series, 9–10, 726–730
Legendre’s differential equation, 276, 388
Legendre’s duplication formula, 604
Legendre’s equation, 389–390
Leibniz criterion, 11–12
Leibniz’s formula, 553, 742
Lerch’s theorem, 1011
level lines, 586
Levi-Civita symbol, 85, 87, 216, 841, 850
line integrals, 159–160, 160f
linear electric quadrupole, 738f
linear equation, 88–89
linear equation system, 102–103
linear ﬁrst-order ODEs, 336–339
linear Hermitian operator, 311
linear independence of solutions, 358–360
linear integral equations, 1047
linear integral operator, 1070
linear operation, 329
linear operators, 275, 329, 401
linear oscillator, 347–350
linear oscillator equation, 347, 363, 1050–1052
linear parameters, variation of, 1121
linear vector space, 252
linearly dependent equations, 90–91
Liouville’s theorem, 490
Lippmann-Schwinger equation, 466
logarithm, 60
Lommel integrals, 665
Lorentz covariance of Maxwell’s equations,
866–868
exercises, 868–869
Lorentz gauge, 174
Lorentz group, see homogeneous Lorentz group
exercises, 865–866
Lorentz transformation, 862
of E and B, 867–868
lowering operator, 777

1196
Index
M
Maclaurin expansion, 64
computation, 613
Maclaurin integral test, 5–8
Riemann Zeta function, 7
Maclaurin series, 27, 44, 253
Maclaurin theorem, 27
exponential function, 27–28
logarithm, 28–29
magnetic dipole, 748–753
magnetic ﬁeld of current loop, 748–753
magnetic moment, 753
magnetic vector potential, 173–174, 193
manifestly covariant form, 868
mapping, 57
complex variables, 547–549, 548f
exercises, 549–550
conformal, 549
matching conditions, 391
mathematical induction, 40–41
excercise, 41
matrices, 95
addition and subtraction, 96
adjoint matrix, 105
defective, 324
deﬁnitions, 95–96
diagonalization, 311–314
Dirac notation in, 266
direct product, 108–112
equality, 96
functions of, 113–114
Hermitian matrices, 108, 315
multiplication, 97, 279
inner product, 97–98
by scalar, 96
normal
exercises, 324–326
normal modes of vibration, 322–324
overview, 319–320
null matrix, 96
numerical inversion of, 100
orthogonal matrices, 107
product theorem, 103–104
rank of, 104
symmetric, 105
trace matrix, 105
transpose matrix, 104
unitary matrices, 107, 314
matrix algebra, 95
matrix eigenvalue equation, 300
matrix eigenvalue problems, 301
example, 301–303
2-D ellipsoidal basin, 303–305
block-diagonal matrix, 305–307
exercises, 308–310
matrix elements, 279
of operator, 280–281
matrix invariant, 295
matrix products, operations on, 106
Maxwell’s equations, 155, 241–243, 594
Gauss’ law, 176
Lorentz covariance of, 866–868
Maxwell-Boltzmann distribution, 606–607
Maxwell-Boltzmann statistics, 1132, 1133
mean value, 1136–1140
mean value theorem, 26, 62
measurement errors, 1125, 1170
Mellin transforms, 966, 1054
meromorphic, 498, 515
meromorphic functions
integral of, 526–527, 527f
pole expansion of, 515–518
metric spaces, 218
metric tensor, 218–219
Christoffel symbols as derivatives of, 223
metric, curvilinear coordinates, 184
Milne’s model, 37
Minkowski space, 236–237, 864
miscellaneous vector identities, 156–157
Mittag-Lefﬂer theorem, 515–516
mixed derivatives, 401
mixed tensor, 209, 210
modiﬁed Bessel functions, 678, 680, 682f
asymptotic expansion, 688
contours, 696f
exercises, 688
Fourier transforms, 684
Green’s function, 684–685
Hankel function, 682
hyperbolic Bessel functions, 683
integral representation, 684, 690–692
Laplace equations, 680
recurrence relations, 681–682
series expansion, 681
Whittaker functions, 682
modiﬁed spherical Bessel functions, 428
modulus, 56, 470
molecular vibration, 882–883
moment-generating function, 1141–1142,
1149–1150
momentum, see angular momentum
momentum representation
in quantum mechanics, 1048–1049
Schrödinger wave equation, 994
monopole moment of charge distribution, 739
monotonic decreasing function, 5
movable singularities, 378–379
moving particle, Cartesian coordinates,
1098–1099
multinomial coefﬁcient, 1132

Index
1197
multiple integrals, 70–72
multiplet, 827, 851
multiplication
by scalar, 255
of matrices, inner product, 97–98
operator, 275
of random variables, 1162
multiply connected regions, 483–484, 483f
multipole expansion, 738, 739, 801–803
multipole moments of charge distribution, 739,
801
multivalued function, 500
mutually commuting operator, 785
mutually exclusive events, 1126
N
Navier-Stokes equations, 190, 377
NDEs, see nonlinear differential equations
negative deﬁnite operators, 317
neighboring paths, 1082, 1083f
Neumann boundary conditions, 385, 412, 428, 662
Neumann functions, 367–369, 693, 917
Bessel functions of second kind
coaxial wave guides, 672
deﬁnition and series form, 667–669
exercises, 674
integral representations, 669
recurrence relations, 669–670
uses of, 671
Wronskian formulas, 670–671
integral form, 671
recurrence relations, 669–670
spherical, 700f
Wronskian formulas, 670–671
Neumann series, 1064–1066
exercises, 1068
Newton’s equations of motion, 213
Newton’s law, 331, 342
Newton’s second law of motion, 322, 1106
nodes of standing wave, 435
nonlinear differential equations (NDEs), 377–380
Bernoulli and Riccati equations, 378
exercises, 379–380
ﬁxed and movable singularities, special
solutions, 378–379
nonlinear dispersive equation, 413
nonlinear methods and chaos
nonlinear differential equations (NDEs)
Bernoulli and Riccati equations, 378
exercises, 379–380
ﬁxed and movable singularities, special
solutions, 378–379
nonlinear ODEs, 377–380
nonnormal matrices, 322–324
nonuniform convergence, 21–22
nonunitary transformations, 293
normal distributions, addition theorem for, 1161
normal eigensystem, 320–321
normal matrices
defective, 324
example, 320–321
exercises, 324–328
normal modes of vibration, 322–324
overview, 319–320
normalization, 662
normalization constant, 606
nucleon, 853
null matrix, 96
numerical evaluation, 91–93
O
ODEs, see ordinary differential equations
Oersted’s law, 168
Olbers’ paradox, 11
one-dimensional problems, Green’s function,
448–459
one-sided Laplace transform, 1008
operators
adjoint, 277
basis expansions of, 279–280
commutation of, 276–277
example, 277, 278, 280–282
exercises, 282–283
expression, 285–286
functions of, 282
identity, inverse, adjoint, 277–278
matrix elements, 280–281
overview, 275–276
self-adjoint, 277, 284–285
example, 284–286
overview, 283–284
transformations of, 291
exercises, 294
nonunitary transformations, 293
unitary
successive transformations, 290
unitary transformations, 287–288
operators, differential vector, see differential
vector operators
optical dispersion, 594–595
optical path near event horizon of black hole,
1087–1088, 1088f
orbital angular momentum, 782
order 2 branch points, 500
ordinary differential equations (ODEs), 329, 330,
381, 644, 715, 982, 1084
exact, 333–334
Hermite, 554
homogeneous linear, 330
homogenous, 335

1198
Index
ordinary differential equations (continued)
inhomogeneous linear, 375–377
exercises, 377
isobaric, 335
Legendre, 557, 716
linear ﬁrst-order, 336–339
linear second-order, 1049
initial/boundary conditions in, 1052
nonlinear, 413
nonseparable exact, 333–334
Rodrigues formulas, 551, 552
second order, 452
second-order linear, 343–346
second-order Sturm-Liouville, 551
separable, 331–332
singularities of, 345t
with constant coefﬁcients, 342–343
ordinary points of the ODE, 344
orthogonal, 124
transformations, 135
orthogonal coordinates, R3, 182–184, 182f , 183f
orthogonal eigenfunctions, 1069–1073
orthogonal functions, expansions in, 258–259
orthogonal matrices, 107, 135
orthogonal polynomials, 272f
exercises, 558–560
generating function, 555, 556
Hermite, 555–556
Rodrigues formula, 551–554
Schlaeﬂi integral, 554
orthogonal unitary, 277
orthogonality, 51, 703, 724, 906–907
associated Legendre equation, 746–748
Bessel series, 663
Earth’s gravitational ﬁeld, 727
electrostatic potential for ring of charge,
729–730
electrostatic potential in a hollow cylinder,
663–664
exercises, 665–667, 730–735
integral, Hermite polynomials, 875
Legendre series, 726–730
Neumann boundary condition, 662
normalization, 662
over discrete points, 1002–1004
sphere in uniform ﬁeld, 727–729, 728f
Sturm-Liouville differential equations, 1073
Sturm-Liouville theory, 661
orthogonalization
Gram-Schmidt
overview, 269–270
example, 270–272
exercises, 273–275
orthonormalizing physical vectors, 272–273
orthogonalized Laguerre functions, 892
orthonormal set, 258
orthonormalization, physical vectors, 272–273
oscillator
damping, 1035–1037
driven, 1035–1037
harmonic, 878–879
oscillatory integral, 529–530
oscillatory series, 2
outward ﬂow, 148
overlap integral, 989–990
overlap matrix, 317
overshoot, calculation of, 959–961
P
parabolic partial differential equations (PDEs),
410
parallelepiped, Laplace equation for, 417–419
parity
and special values, 746
Bessel functions, 655
Parseval relation, 595–596, 987–990
partial derivatives, 62, 401
partial differential equations (PDEs), 329, 643,
981–982
boundary conditions, 405, 411–413
characteristics of, 404–406
classes of, 409–411
elliptic, 410
examples of, 402–403
exercises, 408–409
ﬁrst-order, 403–408
heat ﬂow, or diffusion, 983
alternate solutions, 439–441
exercises, 444
special boundary condition again, 441–442
speciﬁc boundary condition, 437
spherically symmetric heat ﬂow, 442–444
homogeneous, 402
hyperbolic, 410
nonlinear, 413–414
parabolic, 410
second-order, 409–411
separation of variables, 414, 430–432
Cartesian coordinates, 415–420
circular cylindrical coordinates, 421–424,
431
exercises, 432–433
spherical polar coordinates, 424–430
types of, 402
partial fraction expansion, 42, 43, 767, 1013
partial sum approximation, 579
partial-wave components, 799
particle, in a sphere, 704–706
passive rotations, of coordinate system, 215
Pauli matrices, 112

Index
1199
PDEs, see partial differential equations
periodic functions, 939–940
permutation group, 845
permutations, counting of, 1130–1133
physical space, 964
piecewise regular, 936
pivotal method, 1176
plane triangle, 131, 131f
Pochhammer symbol, 35, 699, 912, 917
Poincaré group, 862
Poincaré’s lemma, 239, 240
point groups, 820, 835
point quadrupole, 738
Poisson distribution, 1151, 1153f
exercises, 1154–1155
limits of, 1157–1158
relation to binomial distribution, 1153–1154,
1154f
Poisson noise, 1151
Poisson’s equation, 176–177, 433–434
polar coordinates, 442, see also spherical polar
coordinates
evaluation, 72
polar vectors, 136
polarization matrix, 212
pole expansion of meromorphic functions, 498,
515–518
poles, 497–498
polygamma function, 612
polylogarithms, 923
polynomials, 701, 748
Bernoulli, 565–566
Hermite, 280, 554
example, 554–555, 556
Legendre, 270–272, 557–558
orthogonal, 272f
exercises, 558–560
generating function, 555
Hermite, 555–556
Rodrigues formulas, 551–554
Schlaeﬂi integral, 554
positive deﬁnite operators, 317
potential theory
exercises, 180–182
Gauss’ law, 175–176, 175f
Helmholtz’s theorem, 177–180
Poisson’s equation, 176–177
scalar potential, 171–172
vector potential, 172–175
potential, of charge distribution, 988–989
power series, convergence, uniform and absolute,
29
differentiation and integration, 30
inversion of, 32
uniqueness theorem, L’Hôpital’s rule, 31
power spectrum, 945
power-series expansion, 670
principal axes, 300, 305
principal quantum number, 897
principal value, 501
probability
binomial distribution
exercises, 1151
limits of, 1157–1158
moment-generating function, 1149–1150
repeated tosses of dice, 1148–1149
deﬁnitions, simple properties, 1126
conditional probability, 1128–1129
counting permutations and combinations,
1130–1133
exercises, 1133
probability for A or B, 1127
scholastic aptitude tests, 1129–1130
Gauss’ normal distribution, 1155–1159
Poisson distribution, 1151, 1153f
exercises, 1154–1155
limits of, 1157–1158
relation to binomial distribution, 1153–1154,
1154f
random variables
addition of, 1160–1161
computing discrete probability distributions,
1136
continuous random variable: hydrogen atom,
1135–1136
discrete, 1134–1135, 1137
exercises, 1147–1148
mean and variance, 1136–1140
multiplication or division of, 1162
repeated draws of cards, 1145–1147
standard deviation of measurements,
1138–1140
transformations of, 1159–1160
statistics
chi-square (χ2) distribution, 1170–1174
conﬁdence interval, 1176–1178
error propagation, 1165–1168
exercises, 1178–1179
ﬁtting curves to data, 1168–1170
student t distribution, 1174–1176
theory of, 1125
probability density, student t, 1176f
probability distributions
arbitrary, 1157
conditional, 1147
marginal, 1144–1147
moments of, 1141–1142
product expansion of entire functions, 519–520
products, see cross product; direct product
expansion of entire functions, 519–520

1200
Index
products (continued)
inﬁnite
convergence of, 575
exercises, 576–577
overview, 574–575
sine, cosine functions, 575
pseudoscalar, 137
pseudotensors, 215–216
and dual tensors, 216–217
exercises, 217–218
Levi-Civita symbol, 216
pseudovectors, 136, 137f , 215
Q
QCD, see quantum chromodynamics
quantum chromodynamics (QCD), 861
quantum mechanical oscillator wave functions,
880f
quantum mechanical scattering, Born
approximation, 465–466
quantum mechanical simple harmonic oscillator,
878–879
quantum mechanics
momentum representation in, 1048–1049
of triangular symmetry, 829–830
Schrödinger equation of, 330, 1048
sum rules, 594, 596
time-dependent perturbations, 1067
quantum number, 776
quantum oscillator, 1120f , 1119–1121
quantum particle, 419–420
quantum theory, 704
quarks, 853
ladders, 856–857, 857f
quantum numbers of, 855–856
quotient rule, 211–213
R
radius of convergence, 494
radius vector, 48
raising operator, 777
random variables, 1125
addition of, 1160–1161
gamma-distribution, 1164
computing discrete probability distributions,
1136
continuous random variable: hydrogen atom,
1135–1136
discrete, 1134–1135, 1137
exercises, 1147–1148
mean and variance, 1136–1140
multiplication or division of, 1162
repeated draws of cards, 1145–1147
standard deviation of measurements,
1138–1140
transformations of, 1159–1160
exercises, 1164–1165
rank, 848
tensor of, 205, 207–208
rapidity, 863
ratio test, Cauchy, d’Alembert, 4–5
Rayleigh formulas, 702
Rayleigh’s theorem, see Parseval relation
Rayleigh-Ritz variational technique, 1117–1118,
1121
ground state eigenfunction, 1118–1119
real axis, 56
real part, 56
rearrangement of double series, 18–19
rearrangement theorem, 820
reciprocal lattice, 129
recurrence formulas, 556, 718–720, 764
recurrence relations, 348
Bessel functions, 645–646
spherical, 702
Chebyshev polynomials, 901–903
conﬂuent hypergeometric functions, 918
Hankel functions, 675
Hermite polynomials, 872–873
Laguerre polynomials, associated, 893
modiﬁed Bessel functions, 681–682
Neumann functions, 669–670
spherical Bessel functions, 702
recursion, 69
reducible representation, 823–824
reference frame, 868
reﬂection formula, 603
reﬂections
in spherical coordinates, 196
of coordinate transformations, 136–137, 137f
regression coefﬁcient, 1168
regular singularities, 353–354
regular solution, 369
relativistic energy, 35–36
representation
counting irreducible, 832–833, 833t
decomposing a reducible, 834–835
exercises, 825, 826f
of continuous groups, 824–825
of group, 821
reducible, 823–824
unitary, 821–823, 823f
residue theorem, 509–510, 509f
resolution of identity, 266, 297
resonant cavity, 650–653
Riccati equations, 378
Riemann Zeta function, 7, 16–17, 571, 626–631
exercises, 631–633
Riemann’s theorem, 15
Riemannian spaces, see metric spaces

Index
1201
RL circuit, 338–339
RLC analog, 1022, 1022f
Rodrigues formula, 551–554, 720–721
for Hermite ODE, 554
Laguerre polynomials, 889–890
associated, 895
Rodrigues representation, Hermite polynomials,
874
root diagram, 857
root test, Cauchy, 4
rotations
groups SO(2) and SO(3), 849–851
in R3, 139–142, 140f
exercises, 142–143
in spherical coordinates, 194–196, 195f
of circular disk, 818
of coordinate system, 215
of coordinate transformations, 133–135
Rouché’s theorem, 518–519
row vectors, 95, 123, 125
extraction of, 108
S
saddle points, 63, 433
argument, 586
asymptotic forms
factorial function, 588
of gamma function, 588–589
for avoiding oscillations, 589
method, 587–588
overview, 585–587
sample space, 1126
sample standard deviation, 1168
sawtooth wave, 937–939, 938f ,
scalar, 205
scalar ﬁeld, 143
scalar potential, 171–172
scalar product, 51, 254–255, 271, 285, 295, 297
and adjoint operator, 278
in spin space, 259
triple, 128–130, 129f ,
scalar quantities, 46
scattering cross section, 465
Schlaeﬂi integral, 554, 604, 653f , 676
Legendre polynomials, 557
scholastic aptitude tests, 1129–1130
Schrödinger equation, 708, 1048, 1116–1117
hydrogen atom, 896
momentum space representation, 994
of quantum mechanics, 330
Schwarz inequality, 51, 257
Schwarz reﬂection principle, 547
exercises, 549–550
second-order linear ODEs, 343–346
second-order partial differential equations (PDEs)
boundary conditions, 411–413
classes of, 409–411
exercises, 414
nonlinear, 413–414
second-order Sturm-Liouville ordinary differential
equations (ODEs), 551
second-rank tensor, 207–208
secular determinant, 302
secular equation, 302, 306
self-adjoint matrices, 108
self-adjoint ODEs
boundary conditions, 381
deuteron, 391–393
eigenvalues, 389
exercises, 393–395
Hermitian operators, 384
Legendre’s equation, 389–390
self-adjoint operators, 277, 284–286, 1070
example, 284–286
exercises, 286–287
overview, 283–284
self-adjoint poblems, Green’s function, 460
self-adjoint theory, 384
semi-convergent series, 579
separable kernel, 1057–1059
homogeneous Fredholm equation, 1059–1060
separable ODEs, 331–332
separation of variables, 403, 414, 430–432
Cartesian coordinates, 415–420
circular cylindrical coordinates, 421–424, 431
exercises, 432–433
spherical polar coordinates, 424–430
series approach
Bessel’s equation, limitations of, 351–353
Chebyshev, 25
hypergeometric, 912
Legendre, 9–10
shifted polynomials, Chebyshev, 908
ultraspherical, 25
series expansion, 681
series solutions, Frobenius method, 346–350, 350f
exercises, 355–358
expansion about, 350
Fuchs’ theorem, 355
limitations of series approach, Bessel’s
equation, 351–353
regular and irregular singularities, 353–354
symmetry of solutions, 350–351
sets, 1127–1130
several dependent and independent variables,
relation to physics, 1105
sign changes, series with alternating, 12–13
signal-processing applications, 997–1001
exercises, 1001–1002
similarity transformations, 208, 293

1202
Index
simple pendulum, 927–928, 1113–1114, 1113f
simple pole, 498
simultaneous diagonalization, 314–315
sine
inﬁnite products, 575
integrals in asymptotic series, 580–582
single-electron wave function, 396
single-slit diffraction pattern, 972
singular points, 343–345, 345t
essential (irregular), 344
irregular (essential), 344
isolated, 497
singularities
analytic continuation, 503–507, 504f , 505f
exercises, 507–508
ﬁxed, 378–379
movable, 378–379
on contour of integration, 530–531
poles, 497–498
Slater-type orbitals (STOs), 990
Snell’s law, 1095, 1095f
SO(2) rotation groups, 849–851
SO(3) rotation groups, 849–851
soap ﬁlm, 1088–1090, 1089f
soap ﬁlm–minimum area, 1090–1093, 1092f
solar products, 256–257
solenoidal, 149, 154–155
soliton, 413
source term, 447
space groups, 869
special relativity, 862
special unitary groups, SU(3), Gell-Mann
matrices, 852–861
special values, 764
parity and, 746
spectral decomposition, 315–316
sphere in uniform ﬁeld, 727–729, 728f
sphere with boundary conditions, 428–430
spherical Bessel functions, 427, 428
asymptotic values, 703
deﬁnitions, 702
exercises, 709–712
Helmholtz equation, 698
limiting values, 703
modiﬁed, 709
orthogonality and zeros, 703
particle in a sphere, 704–706
recurrence relations, 702
spherical coordinates, Helmholtz equation, 698
spherical Green’s functions, 463, 800–801
spherical harmonics, 445, 473, 756
addition theorem for, 797–798
Cartesian representations, 758
Condon-Shortley phase, 758, 760t
exercises, 765–766
integrals of three, 803–805
ladder, 779
Laplace expansion, 760–761, 799–801
Laplace series–gravity ﬁelds, 762
properties of, 764–765
symmetry of solutions, 762–763
vector, 809–813
spherical pendulum, 1105, 1105f
spherical polar coordinates, 79, 183, 190–194,
194f , 424–430
spherical tensors, 796
addition theorem, 797–798
Laplace expansion, 799–801
spherical wave expansion, 798–799
exercises, 806–809
integrals of three spherical harmonics, 803–805
spherical volume, 704
spherical waves
Bessel functions, 703
expansion, 798–799
spin operator, adjoint of, 282
spin space, 259–260
of electron, 253
spinor ladder, 780–781
spinors, 213, 779–780, 852
square integrable, 595
square integration contour, zn on, 479–481, 480f ,
square pulse, transform of, 1010, 1010f
square wave, 949–950, 949f , 958–959
expansion of, 264f
squares of random variables, 1164
squares of series, divergent, 15–16
standard deviation, 1138
of measurements, 1138–1140
sample, 1168
standing waves, 382–384, 435
star operator, see Hodge operator
stationary, 63
stationary paths, 1085, 1085f
stationary points, 433
statistical hypothesis, 1165
statistics, 1125
chi-square (χ2) distribution, 1170–1174
conﬁdence interval, 1176–1178
error propagation, 1165–1168
exercises, 1178–1179
ﬁtting curves to data, 1168–1170
student t distribution, 1174–1176
steepest descent
method of, 585
asymptotic form of gamma function,
588–589
exercises, 590–591
factorial function, 588
saddle points, 585–588

Index
1203
step function, 1013–1014, 1014f
Stirling’s expansion, 589
Stirling’s series
derivation from Euler-Maclaurin integration
formula, 623–624
exercises, 625–626
Stirling’s series, 624
Stirlings formula, 567
Stokes’ theorem, 167–168, 167f , 168f , 193–194
on differential forms, 245–248
STOs, see Slater-type orbitals
stream lines, 149
strong interaction, 852
structure constants, 848
student t distribution, 1174–1176, 1177t
student t probability density, 1176f
Sturm-Liouville boundary conditions, 892
Sturm-Liouville equation, 1117
Sturm-Liouville system, 746, 892
Sturm-Liouville theory, 384, 661, 936–937, 1071,
1073
SU(2)
and SO(3) homomorphism, 851–852
isospin and SU(3) symmetry, 852
SU(3) symmetry, 852–861
substitution, 1020
subtraction
of sets, 1127
of tensors, 208
successive applications of ∇, 153–154
successive operations, of coordinate
transformations, 137–138
successive transfer functions, 1002f
successive unitary transformations, 290
sum
evaluation of, 544–546, 546t
exercises, 546–547
sum rules, 596
summation of series, 957–958
superposition principle, 402
for homogenous ODEs, PDEs, 330
surface integrals, 161–162, 161f , 162f
symmetric group, 835, 840–844
exercises, 844–845
symmetric matrix, 105
symmetric stretching mode, 323
symmetric tensor, 208
symmetrization of kernels, 1069
symmetry, 815, 940–941, 942f
and physics, 826–830
exercises, 830
of equilateral triangle, 817f , 817, 818f
of solutions, 762–763
relations, 593–594
T
Taylor expansion, 492–494, 493f
Taylor series, 560
Taylor’s expansion, 653
binomial theorem, relativistic energy, 35–36
Maclaurin theorem
exponential function, 27–28
logarithm, 28–29
tensor analysis, 205–213
addition and subtraction of, 208
covariant and contravariant, 206–207
exercises, 213–215
isotropic, 209
symmetric and antisymmetric, 208
tensor derivative operators
curl, 225
divergence, 224–225
gradient, 224
Laplacian, 225
tensors, see also direct product; quotient rule;
spinors; pseudotensors
direct product of, 210–211
in general coordinates
covariant derivatives, 222–223
exercises, 226
metric tensor, 218–219
second-rank, 207–208
tensors of rank 0, 205
tensors of rank 1, 205
tensors of rank 2, 207–208
three-dimensional (3-D) differential forms, 407
threefold Hermite formula, 883–884
time-independent Schrödinger equation, 300
TM, see transverse magnetic
trace matrix, 105, 210
transfer function, 998–999, 998f
high-pass ﬁlter, 999–1000, 999f
limitations on, 1000–1001
transform, derivative of, 965, 966, see also
Hankel; Laplace; Mellin
transformations
Gram-Schmidt, 293
of differential equation into integral equation,
1049–1050
of operators, 291
nonunitary transformations, 293
of random variables, 1159–1165
unitary, 287–290
translation, 1022–1023
transpose matrix, 104
transverse magnetic (TM), 651
traveling waves, 435
triangle rule, 788
triangular pulse, Fourier transform of, 976, 977f

1204
Index
triangular symmetry, quantum mechanics of,
829–830
trigonometric form, 904–905
trigonometric functions
exploiting periodicity of, 537–538
trigonometric integrals, 69, 522–524
triple scalar product, 128–130, 129f
triple vector product, 130
triplet state, 259
Two and three dimension problems, Green’s
function, 459–467
two-sided Laplace transforms, 1008
U
ultraspherical polynomials, 388, 899
equation, 903
self-adjoint form, 906
undetermined multipliers, see Lagrangian
multipliers
uniform convergence, 21–22, 29, 262
uniformly convergent series, properties of, 24
unions, 1127–1130
unique expansion, 494
uniqueness theorem
L’Hôpital’s rule, 31
of power series, 30–31
unit cell, 869
unit matrix, 99
unit vectors, 47
unitary matrices, 107
unitary operators
example, 289–290
exercises, 290–291
successive transformations, 290
unitary transformations, 287–288
unitary representation, 821–823, 823f
unitary transformation, 297
V
variables
dependent, 1096–1097
Hamilton’s Principle, 1097–1098
Laplace’s equation, 1101–1102
moving particle–Cartesian coordinates,
1098–1099
moving particle–circular cylindrical
coordinates, 1099
independent, 407–408, 411
separation of, 403
variance, 1136–1140
variation, 1081
with constraints, 1111–1112
exercises, 1121–1124
Lagrangian equations, 1112–1113
Schrödinger wave equation, 1116–1117
simple pendulum, 1113–1114, 1113f
sliding off a log, 1114–1115, 1114f
of linear parameters, 1121
of constant, 338
of parameters, 338, 375–376
variation method, 395–397
exercises, 397
vector analysis
reciprocal lattice, 130
rotation of coordinate transformations, 133–135
vector ﬁelds, 46, 143
vector integration
exercises, 163–164
line integrals, 159–160, 160f
surface integrals, 161–162, 161f , 162f
volume integrals, 162–163
vector Laplacian, 155–156
vector model, 786–788
vector potential, 172–175, 175
vector spaces, 253–254, 295
completeness, 255, 262
linear space, 252
vector spherical harmonics
coupling, 810–813
exercises, 813
spherical tensor, 809–810
vector triple product, 130
vectors, 123, 205, see also rotations; gradient,
∇; tensors; Stokes’ theorem
addition of, 47f
angle between two, 798
basic properties of, 124–125
by Gram-Schmidt orthogonalization, 269–275
coefﬁcient, 261
contravariant, 206, 219
contravariant basis, 220–221
covariant, 206
covariant basis, 218, 220–221
cross product, 126–128, 126f , 127f
differential vector operators, 143
gradient, 143
direct product of, 210–211
dot products, 49–50
exercises, 52–53, 131–133
ﬁelds, 123
Gauss’ theorem, 164–165, 165f
Green’s theorem, 165–166
Helmholtz’s theorem, 177–180
in function spaces
Dirac notation, 265–266
example, 253–254, 256–263, 265
exercises, 266–269
expansions, 261
Hilbert space, 255–256
orthogonal expansions, 257–258

Index
1205
overview, 251–253
scalar product, 254–255, 260–261
Schwarz inequality, 257
irrotational, 154–155
matrix representation of, 106–107
multiplication of, 252
orthogonality, 51
physical, 272–273
radius vector, 48
Stokes’ theorem, 167–168, 167f , 168f
successive applications of ∇, 153–154
triple product, 130
triple scalar product, 128–130, 129f
unit vectors, 47
vibrating string, 382–384
vibration, normal modes of, 322–324
vierergruppe, 820
Volterra equation, 1047, 1048, 1050, 1055, 1067
volume integrals, 162–163
vorticity, 151
W
wave equation, 435, 981–982
d’Alembert’s solution of, 436
exercises, 437
wave guides, coaxial, Bessel functions, 671–672
wedge operator, 233
wedge products, 233
Weierstrass, 504
Weierstrass M test, 22–23
Weierstrass inﬁnite-product form of, 602
weight diagram, 859f , 859, 860f
Weyl representation, 121
Whittaker functions, 682, 919
Wigner matrices, 797
WKB expansion, 577
Wronskian determinant, 359–360
Wronskian formulas
Bessel functions, 670–671, 694
conﬂuent hypergeometric functions, 922
linear dependence/independence of functions,
360, 671
solutions of self-adjoint differential equation,
670
Z
zero matrix, 96
zero-point energy, 705, 879
zeros, Bessel function, 648–653

