Lecture Notes in Networks and Systems 855
Pandian Vasant · 
Mohammad Shamsul Arefin · 
Vladimir Panchenko · J. Joshua Thomas · 
Elias Munapo · Gerhard-Wilhelm Weber · 
Roman Rodriguez-Aguilar   Editors
Intelligent 
Computing and 
Optimization
Proceedings of the 6th International 
Conference on Intelligent Computing 
and Optimization 2023 (ICO2023), 
Volume 5

Lecture Notes in Networks and Systems
855
Series Editor
Janusz Kacprzyk
, Systems Research Institute, Polish Academy of Sciences, Warsaw,
Poland
Advisory Editors
Fernando Gomide, Department of Computer Engineering and Automation—DCA,
School of Electrical and Computer Engineering—FEEC, University of
Campinas—UNICAMP, São Paulo, Brazil
Okyay Kaynak, Department of Electrical and Electronic Engineering, Bogazici
University, Istanbul, Türkiye
Derong Liu, Department of Electrical and Computer Engineering, University of
Illinois at Chicago, Chicago, USA
Institute of Automation, Chinese Academy of Sciences, Beijing, China
Witold Pedrycz, Department of Electrical and Computer Engineering, University of
Alberta, Alberta, Canada
Systems Research Institute, Polish Academy of Sciences, Warsaw, Poland
Marios M. Polycarpou, Department of Electrical and Computer Engineering,
KIOS Research Center for Intelligent Systems and Networks, University of Cyprus,
Nicosia, Cyprus
Imre J. Rudas, Óbuda University, Budapest, Hungary
Jun Wang, Department of Computer Science, City University of Hong Kong,
Kowloon, Hong Kong

The series “Lecture Notes in Networks and Systems” publishes the latest developments
in Networks and Systems—quickly, informally and with high quality. Original research
reported in proceedings and post-proceedings represents the core of LNNS.
Volumes published in LNNS embrace all aspects and subﬁelds of, as well as new
challenges in, Networks and Systems.
The series contains proceedings and edited volumes in systems and networks, span-
ning the areas of Cyber-Physical Systems, Autonomous Systems, Sensor Networks,
Control Systems, Energy Systems, Automotive Systems, Biological Systems, Vehicular
Networking and Connected Vehicles, Aerospace Systems, Automation, Manufacturing,
Smart Grids, Nonlinear Systems, Power Systems, Robotics, Social Systems, Economic
Systems and other. Of particular value to both the contributors and the readership are the
short publication timeframe and the world-wide distribution and exposure which enable
both a wide and rapid dissemination of research output.
The series covers the theory, applications, and perspectives on the state of the art
and future developments relevant to systems and networks, decision making, control,
complex processes and related areas, as embedded in the ﬁelds of interdisciplinary and
applied sciences, engineering, computer science, physics, economics, social, and life
sciences, as well as the paradigms and methodologies behind them.
Indexed by SCOPUS, INSPEC, WTI Frankfurt eG, zbMATH, SCImago.
All books published in the series are submitted for consideration in Web of Science.
For proposals from Asia please contact Aninda Bose (aninda.bose@springer.com).

Pandian Vasant · Mohammad Shamsul Areﬁn ·
Vladimir Panchenko · J. Joshua Thomas ·
Elias Munapo · Gerhard-Wilhelm Weber ·
Roman Rodriguez-Aguilar
Editors
Intelligent Computing
and Optimization
Proceedings of the 6th International Conference
on Intelligent Computing and Optimization
2023 (ICO2023), Volume 5

Editors
Pandian Vasant
Faculty of Electrical and Electronics
Engineering, Modeling Evolutionary
Algorithms Simulation and Artiﬁcial
Intelligence
Ton Duc Thang University
Ho Chi Minh City, Vietnam
Vladimir Panchenko
Laboratory of Non-traditional Energy
Systems, Department of Theoretical
and Applied Mechanics, Federal Scientiﬁc
Agroengineering Center VIM
Russian University of Transport
Moscow, Russia
Elias Munapo
School of Economics and Decision Sciences
North West University
Mmabatho, South Africa
Roman Rodriguez-Aguilar
Facultad de Ciencias Económicas y
Empresariales, School of Economic
and Business Sciences
Universidad Panamericana
Mexico City, Mexico
Mohammad Shamsul Areﬁn
Department of Computer Science
Chittagong University of Engineering
and Technology
Chittagong, Bangladesh
J. Joshua Thomas
Department of Computer Science
UOW Malaysia KDU Penang University
College
George Town, Malaysia
Gerhard-Wilhelm Weber
Faculty of Engineering Management
Pozna´n University of Technology
Pozna´n, Poland
ISSN 2367-3370
ISSN 2367-3389 (electronic)
Lecture Notes in Networks and Systems
ISBN 978-3-031-50157-9
ISBN 978-3-031-50158-6 (eBook)
https://doi.org/10.1007/978-3-031-50158-6
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2023
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations,recitation,broadcasting,reproductiononmicroﬁlmsorinanyotherphysicalway,andtransmission
or information storage and retrieval, electronic adaptation, computer software, or by similar or dissimilar
methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant
protective laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the
editors give a warranty, expressed or implied, with respect to the material contained herein or for any errors
or omissions that may have been made. The publisher remains neutral with regard to jurisdictional claims in
published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland
Paper in this product is recyclable.

Preface
The sixth edition of the International Conference on Intelligent Computing and Opti-
mization (ICO’2023) was held during April 27–28, 2023, at G Hua Hin Resort and Mall,
Hua Hin, Thailand. The objective of the international conference is to bring the global
research scholars, experts and scientists in the research areas of intelligent computing
and optimization from all over the world to share their knowledge and experiences on the
current research achievements in these ﬁelds. This conference provides a golden oppor-
tunity for global research community to interact and share their novel research results,
ﬁndings and innovative discoveries among their colleagues and friends. The proceedings
of ICO’2023 is published by SPRINGER (in the book series Lecture Notes in Networks
and Systems) and indexed by SCOPUS.
Almost 70 authors submitted their full papers for the 6th ICO’2023. They repre-
sent more than 30 countries, such as Australia, Bangladesh, Bhutan, Botswana, Brazil,
Canada, China, Germany, Ghana, Hong Kong, India, Indonesia, Japan, Malaysia, Mau-
ritius, Mexico, Nepal, the Philippines, Russia, Saudi Arabia, South Africa, Sri Lanka,
Thailand, Turkey, Ukraine, UK, USA, Vietnam, Zimbabwe and others. This worldwide
representation clearly demonstrates the growing interest of the global research commu-
nity in our conference series. The organizing committee would like to sincerely thank all
the authors and the reviewers for their wonderful contribution for this conference. The
best and high-quality papers will be selected and reviewed by International Program
Committee in order to publish the extended version of the paper in the international
indexed journals by SCOPUS and ISI WoS.
This conference could not have been organized without the strong support and help
from LNNS SPRINGER NATURE, Easy Chair, IFORS and the Committee of ICO’2023.
We would like to sincerely thank Prof. Roman Rodriguez-Aguiler (Universidad Panamer-
icana, Mexico) and Prof. Mohammad Shamsul Areﬁn (Daffodil International University,
Bangladesh), Prof. Elias Munapo (North West University, South Africa) and Prof. José
Antonio Marmolejo Saucedo (National Autonomous University of Mexico, Mexico) for
their great help and support for this conference.
We also appreciate the wonderful guidance and support from Dr. Sinan Melih Nigdeli
(Istanbul University—Cerrahpa¸sa, Turkey), Dr. Marife Rosales (Polytechnic Univer-
sity of the Philippines, Philippines), Prof. Rustem Popa (Dunarea de Jos University,
Romania), Prof. Igor Litvinchev (Nuevo Leon State University, Mexico), Dr. Alexan-
der Setiawan (Petra Christian University, Indonesia), Dr. Kreangkri Ratchagit (Maejo
University, Thailand), Dr. Ravindra Boojhawon (University of Mauritius, Mauritius),
Prof. Mohammed Moshiul Hoque (CUET, Bangladesh), Er. Aditya Singh (Lovely Pro-
fessional University, India), Dr. Dmitry Budnikov (Federal Scientiﬁc Agroengineering
Center VIM, Russia), Dr. Deepanjal Shrestha (Pokhara University, Nepal), Dr. Nguyen
Tan Cam (University of Information Technology, Vietnam) and Dr. Thanh Dang Trung
(Thu Dau Mot University, Vietnam). The ICO’2023 committee would like to sincerely
thank all the authors, reviewers, keynote speakers (Prof. Roman Rodriguez-Aguiler,

vi
Preface
Prof. Kaushik Deb, Prof. Rolly Intan, Prof. Francis Miranda, Dr. Deepanjal Shrestha,
Prof. Sunarin Chanta), plenary speakers (Prof. Celso C. Ribeiro, Prof. José Antonio
Marmolejo, Dr. Tien Anh Tran), session chairs and participants for their outstanding
contribution to the success of the 6th ICO’2023 in Hua Hin, Thailand.
Finally, we would like to sincerely thank Prof. Dr. Janusz Kacprzyk, Dr. Thomas
Ditzinger, Dr. Holger Schaepe and Ms. Varsha Prabakaran of LNNS SPRINGER
NATURE for their great support, motivation and encouragement in making this event
successful in the global stage.
April 2023
Dr. Pandian Vasant (Chair)
Prof. Dr. Gerhard-Wilhelm Weber
Prof. Dr. Mohammad Shamsul Areﬁn
Prof. Dr. Roman Rodriguez-Aguiler
Dr. Vladimir Panchenko
Prof. Dr. Elias Munapo
Dr. J. Joshua Thomas

Contents
Business, Economics, Finance, Management, Social and Smart
Technology
Validating the Measurement Scale Items on Readiness to Adopt Human
Resource Analytics in the Organizations of Nepal . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Shanti Devi Chhetri, Devesh Kumar, Deepesh Ranabhat,
and Pradeep Sapkota
Optimizing Fire Response Unit Location for Urban-Rural Area . . . . . . . . . . . . . .
14
Sunarin Chanta and Ornurai Sangsawang
Effects of Financial Literacy on Financial Inclusion: Evidence
from Nepal’s Gandaki Province . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
Deepesh Ranabhat, Narinder Verma, Pradeep Sapkota,
and Shanti Devi Chhetri
Business and Information Technology Strategy Impact on Organizational
Performance: A Case Study of Nepal Telecom . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
Sudip Poudel, Neesha Rajkarnikar, Deepanjal Shrestha,
Deepmala Shrestha, and Seung Ryul Jeong
A Systematic Literature Review on Factors Affecting Rural Tourism . . . . . . . . . .
45
Pradeep Sapkota, Kamal Kant Vashisth, and Deepesh Ranabhat
K-Modes with Binary Logistic Regression: An Application in Marketing
Research . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
Jonathan Rebolledo and Roman Rodriguez-Aguilar
Optimizing Headways Using Evolutionary Algorithms for Improved Light
Rail Transit System Efﬁciency and Passenger Service . . . . . . . . . . . . . . . . . . . . . . .
65
Oomesh Gukhool, Nooswaibah Binti Nooroodeen Soosor,
Ravindra Boojhawon, and Mohammad Zaheer Doomah
Improving the Ergonomics of the Master Data Management System Using
Annotated Metagraph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
76
D. R. Nikolsky, A. A. Sukhobokov, and Goryachkin B.S.
Patent Classiﬁcation for Business Strategy with BERT . . . . . . . . . . . . . . . . . . . . . .
84
Masaki Higashi, Yoshimasa Utsumi, and Kazuhide Nakata

viii
Contents
GIS Based Flood Hazard and Risk Assessment Using Multi Criteria
Decision Making Approach in Rapti River Watershed, India . . . . . . . . . . . . . . . . .
95
Raashid Khan, Jawed Anwar, Saif said, Sarfarazali Ansari,
Azazkhan Ibrahimkhan Pathan, and Lariyah Mohd Sidek
Optimizing Laser Drilling of Kenaf/HDPE Composites: A Novel
CRITIC-MABAC Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
104
Sellamuthu Prabhukumar, Jasgurpeet Singh Chohan, and Kanak Kalita
Education, Healthcare, Industry, and Advanced Engineering
Determination of the Optimal Speed of Movement of the Conveyor Belt
of the Prototype Weighing Belt Batcher . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
Denis Shilin, Dmitry Shestov, Alexey Vasiliev, and Valery Moskvin
Spatial Analysis: Cases of Acute Bloody Diarrhea in Baguio City,
Philippines from 2015 to 2018 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
125
Guinness G. Maza, Kendrick Jules G. Zante,
Clarence Kyle L. Pagunsan, Angela Ronice A. Doctolero,
Rostum Paolo B. Alanas, Criselda P. Libatique, and Rizavel C. Addawe
The Economic Dimensions of the Non-communicable Diseases: A Panel
Data Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134
Sergio Arturo Domínguez-Miranda and Roman Rodriguez-Aguilar
Re-strengthening of Real Sized RC Beams Subjected to Corrosion Using
Glass Fiber Reinforced Polymer Sheets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
Sunil Garhwal, Shruti Sharma, Sandeep Kumar Sharma, Anil Garhwal,
and Anirban Banik
Optimization of the Lubricating and Cooling Fluid Composition . . . . . . . . . . . . .
153
I. Yu. Ignatkin, P. Kazantsev Sergey, D. M. Skorokhodov, N. V. Serov,
T. Kildeev, A. V. Serov, and A. Anisimov Alexander
Research of the Dosing Process with the Installation of Magnetic
Stimulation of Seeds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
V. Syrkin, S. Mashkov, P. Ishkin, S. Vasilev, and Yu. Daus
Investigation of Hydrodynamic Behaviour in Rectangular Sheet Shaped
Membrane Using Computational Fluid Dynamics (CFD) . . . . . . . . . . . . . . . . . . . .
170
Anirban Banik, Sushant Kumar Biswal, Tarun Kanti Bandyopadhyay,
Vladimir Panchenko, Sunil Garhwal, and Anil Garhwal

Contents
ix
A Review on the Impacts of Social Media on the Mental Health . . . . . . . . . . . . . .
181
Md. Abu Bakar Siddiq Tapu, Rashik Shahriar Akash, Haﬁz Al Fahim,
Tanin Mohammad Jarin, Touhid Bhuiyan, Ahmed Wasif Reza,
and Mohammad Shamsul Areﬁn
Factor Inﬂuencing Online Purchase Intention Among University Students
in Nepal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
Deepesh Ranabhat, Sujita Adhikari, and Narinder Verma
Effect of Thin Polymer Interlayers in the Spindle-Bearing Joint
on the Stiffness and Durability of Spindle Bearing Assemblies of Mills . . . . . . . .
207
A. S. Kononenko, T. A. Kildeev, Ignatkin I. Yu, and N. A. Sergeeva
The Use of a Nutrient Solution Containing Chelated Forms of Various
Trace Elements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
K. Pishchaeva, S. Muradyan, E. Nikulina, S. Buleeva, and A. Saproshina
Designing an Inventory Control System in Food and Beverage Industry . . . . . . .
223
Tiovitus Flomando Tunga and Tanti Octavia
Evaluating Research Impact: A Comprehensive Overview of Metrics
and Online Databases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236
Seema Ukidve, Ramsagar Yadav, Mukhdeep Singh Manshahia,
and Jasleen Randhawa
Hazard Identiﬁcation, Risk Assessment and Control (HIRAC) at the Wood
Processing Industry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244
Herry Christian Palit and Alexander
Location Selection of Rail Transportation Hub Using TOPSIS Method . . . . . . . .
253
Kanokporn Sripathomswat, Nattawat Tipchareon,
Worapat Aruntippaitoon, Itiphong Trirattanasarana, and Sunarin Chanta
Developing a Transaction System in Blockchain . . . . . . . . . . . . . . . . . . . . . . . . . . .
262
Afsana Nur Meem, Lamya Ishrat Nodi, Efte Kharul Islam,
Minhazul Amin Tomal, Ahmed Wasif Reza,
and Mohammad Shamsul Areﬁn
Using the Phi-Function Technique for the Optimized Virtual Localization
Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
Sergiy Plankovskyy, Yevgen Tsegelnyk, Tetyana Romanova,
Oleksandr Pankratov, Igor Litvinchev, and Volodymyr Kombarov

x
Contents
COVID-19 Detection from Chest X-Ray Images Using CNN Models
and Deep Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
288
Naﬁsha Binte Moin, Shamima Sultana, Abdullah Al Munem,
Omar Tawhid Imam, Ahmed Wasif Reza, and Mohammad Shamsul Areﬁn
A Note on Solving the Transportation Model by the Hungarian Method
of Assignment: Uniﬁcation of the Transportation and Assignment Models . . . . .
301
Santosh Kumar, Trust Tawanda, Elias Munapo, and Philimon Nyamugure
Automatic Crack Detection Approach for the Offshore Flare System
Inspection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
314
Teepakorn Tosawadi, Pakcheera Choppradit, Satida Sookpong,
Sasin Phimsiri,Vasin Suttichaya,Chaitat Utintu,andEk Thamwiwatthana
The Recent Trend of Artiﬁcial Neural Network in the Field of Civil
Engineering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
324
Aditya Singh
Analyzing Price Forecasting of Grocery Products in Bangladesh:
A Comparison of Time Series Modeling Approaches . . . . . . . . . . . . . . . . . . . . . . .
334
Md Mahmudul Hoque, Ikbal Ahmed, Nayan Banik,
and Mohammed Moshiul Hoque
Big Data: Identiﬁcation of Critical Success Factors . . . . . . . . . . . . . . . . . . . . . . . . .
342
Leo Willyanto Santoso
Effect of Parameter Value of a Hybrid Algorithm for Optimization of Truss
Structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
350
Melda Yücel, Sinan Melih Nigdeli, and Gebrail Bekda¸s
Brain MRI Classiﬁcation for Alzheimer’s Disease Based on Convolutional
Neural Network . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
359
Md. Saiful, Arpita Saha, Faria Tabassum Mim, Naﬁsa Tasnim,
Ahmed Wasif Reza, and Mohammad Shamsul Areﬁn
Drivers and Barriers for Going Paperless in Tertiary Educational Institute . . . . . .
368
Raﬁd Mahmud Haque, Lamyea Tasneem Maha, Oshin Nusrat Rahman,
Noor Fabi Shah Safa, Rashedul Amin Tuhin, Ahmed Wasif Reza,
and Mohammad Shamsul Arein
Impact of Lifestyle on Career: A Review . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
379
Md. Jabed Hosen, Md. Injamul Haque, Saiful Islam,
Mohammed Nadir Bin Ali, Touhid Bhuiyan, Ahmed Wasif Reza,
and Mohammad Shamsul Areﬁn

Contents
xi
Deep Learning Approach for COVID-19 Detection: A Diagnostic Tool
Based on VGG16 and VGG19 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
394
Fardin Rahman Akash, Ajmiri Afrin Priniya, Jahani Shabnam Chadni,
Jobaida Ahmed Shuha, Ismot Ara Emu, Ahmed Wasif Reza,
and Mohammad Shamsul Areﬁn
A Survey of Modeling the Healthcare Inventory for Emerging Infectious
Diseases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
404
Tatitayakorn Limsakul and Sompoap Taladgaew
MEREC-MABAC Based-Parametric Optimization of Chemical Vapour
Deposition Process for Diamond-Like Carbon Coatings . . . . . . . . . . . . . . . . . . . . .
414
Sellamuthu Prabhukumar, Jasgurpeet Singh Chohan, and Kanak Kalita
Author Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
423

About the Editors
Pandian Vasant is Research Associate at Modeling Evo-
lutionary Algorithms Simulation and Artiﬁcial Intelligence,
Faculty of Electrical and Electronics Engineering, Ton Duc
Thang University, Ho Chi Minh City, Vietnam, and Editor
in Chief of International Journal of Energy Optimization
and Engineering (IJEOE). He holds Ph.D. in Computa-
tional Intelligence (UNEM, Costa Rica), M.Sc. (Univer-
sity Malaysia Sabah, Malaysia, Engineering Mathematics)
and B.Sc. (Hons, Second Class Upper) in Mathematics
(University of Malaya, Malaysia). His research interests
include soft computing, hybrid optimization, innovative
computing and applications. He has co-authored research
articles in journals, conference proceedings, presentations,
special issues Guest Editor, chapters (312 publications
indexed in Research-Gate) and General Chair of EAI Inter-
national Conference on Computer Science and Engineering
in Penang, Malaysia (2016) and Bangkok, Thailand (2018).
In the years 2009 and 2015, he was awarded top reviewer
and outstanding reviewer for the journal Applied Soft Com-
puting (Elsevier). He has 35 years of working experience
at the universities. Currently, Dr. Pandian Vasant is Gen-
eral Chair of the International Conference on Intelligent
Computing and Optimization (https://www.icico.info/) and
Research Associate at Modeling Evolutionary Algorithms
Simulation and Artiﬁcial Intelligence, Faculty of Electrical
and Electronics Engineering, Ton Duc Thang University,
HCMC, Vietnam.
Professor Dr. Mohammad Shamsul Areﬁn is
in
lien
from Chittagong University of Engineering and Technol-
ogy (CUET), Bangladesh and currently afﬁliated with
the Department of Computer Science and Engineering
(CSE), Daffodil International University (DIU), Dhaka,
Bangladesh. Earlier he was the head of CSE Department,
CUET. Prof. Areﬁn received his Doctor of Engineering
Degree in Information Engineering from Hiroshima Uni-
versity, Japan with support of the scholarship of MEXT,
Japan. As a part of his doctoral research, Dr. Areﬁn was
with IBM Yamato Software Laboratory, Japan. His research
includes data privacy and mining, big data management,

xiv
About the Editors
IoT, Cloud Computing, Natural Language processing,
Image Information Processing, Social Networks Analysis
and Recommendation Systems and IT for agriculture,
education and environment. Prof. Areﬁn is the Editor in
Chief of Computer Science and Engineering Research
Journal (ISSN: 1990-4010) and was the Associate Editor
of BCS Journal of Computer and Information Technology
(ISSN: 2664-4592) and a reviewer as well as TPC member
of many international journals and conferences. Dr. Areﬁn
has more than 120 referred publications in international
journals, book series and conference proceedings. He
delivered more than 30 keynote speeches/invited talks.
He also received a good number of research grants/funds
from home and abroad. Dr. Areﬁn is a senior member
of IEEE, Member of ACM, Fellow of IEB and BCS.
Prof. Areﬁn involves/earlier involved in many professional
activities such as Chairman of Bangladesh Computer
Society (BCS) Chittagong Branch; Vice-President (Aca-
demic) of BCS National Committee; Executive Committee
Member of IEB Computer Engineering Division; Advisor,
Bangladesh Robotic Foundation. He was also a member of
pre-feasibility study team of CUET IT Business Incubator,
ﬁrst campus based IT Business Incubator in Bangladesh.
Prof. Areﬁn is an Principle Editor of the Lecture Notes on
Data Engineering and Communications Technologies
book series (LNDECT, Volume 95) published by Springer
and an editor of the books on Applied Informatics for
Industry 4.0, Applied Intelligence for Industry 4.0 and
Computer Vision and Image Analysis for Industry 4.0 to be
published Tailor and Francis. Prof. Areﬁn is the Vice-Chair
(Technical) of IEEE CS BDC for the year 2022. He was
the Vice-Chair (Activity) of IEEE CS BDC for the year
2021 and the Conference Co-Coordinator of IEEE CS
BDC for two consecutive years, 2018 and 2019. He is
acting as a TPC Chair of MIET 2022 and the TPC Chair of
IEEE Summer Symposium 2022. He was the Organizing
Chair of International Conference on Big Data, IoT and
Machine Learning (BIM 2021) and National Workshop
on Big Data and Machine Learning (BDML 2020). He
served as the TPC Chair, International Conference on
Electrical, Computer and Communication Engineering
(ECCE 2017); Organizing Co-chair, ECCE 2019, Techni-
cal Co-chair, IEEE CS BDC Winter Symposium 2020 and
Technical Secretary, IEEE CS BDC Winter Symposium
2021. Dr. Areﬁn helped different international conferences

About the Editors
xv
in the form of track chair, TPC member, reviewer and/or
secession chair etc. He is a reviewer of many reputed
journals including IEEE Access, Computing Informatics,
ICT Express, Cognitive Computation etc. Dr. Areﬁn visited
Japan, Indonesia, Malaysia, Bhutan, Singapore, South
Korea, Egypt, India, Saudi Arabia and China for different
professional and social activities.
Vladimir Panchenko is an Associate Professor of the
“Department of Theoretical and Applied Mechanics” of the
“Russian University of Transport”, Senior Researcher of
the “Laboratory of Non-traditional Energy Systems” of the
“Federal Scientiﬁc Agroengineering Center VIM” and the
Teacher of additional education. Graduated from the “Bau-
man Moscow State Technical University” in 2009 with the
qualiﬁcation of an engineer. Ph.D. thesis of the specialty
“Power plants based on renewable energy” was defended
in 2013. From 2014 to 2016 Chairman of the Council of
Young Scientists and the Member of the Academic Council
of the All-Russian Institute for Electriﬁcation of Agricul-
ture, Member of the Council of Young Scientists of the
Russian University of Transport, Member of the Interna-
tional Solar Energy Society, Individual supporter of Green-
peace and the World Wildlife Fund, Member of the Rus-
sian Geographical Society, Member of the Youth section
of the Council “Science and Innovations of the Caspian
Sea”, Member of the Committee on the use of renewable
energy sources of the Russian Union of Scientiﬁc and Engi-
neering Public Associations. Diplomas of the winner of
the competition of works of young scientists of the All-
Russian Scientiﬁc Youth School with international partic-
ipation “Renewable Energy Sources”, Moscow State Uni-
versity M.V. Lomonosov in 2012, 2014, 2018 and 2020,
Diploma with a bronze medal of the 15th Russian agro-
industrial exhibition “Golden Autumn—2013”, Diploma
withagoldmedalofthe18thRussianagro-industrialexhibi-
tion “Golden Autumn—2016”, Diploma with a silver medal
of the XIX Moscow International Salon of Inventions and
Innovative technologies “Archimedes—2016”, Diploma for
the winning the schoolchildren who have achieved high
results in signiﬁcant events of the Department of Education
and Science of the City of Moscow (2020–2021, School
No. 2045). Scientiﬁc adviser of schoolchildren-winners
and prize-winners of the Project and Research Competi-
tion “Engineers of the Future” at NUST MISiS 2021 and

xvi
About the Editors
RTU MIREA 2022. Invited expert of the projects of the
ﬁnal stages of the “Engineers of the Future” (2021, 2022)
and the projects of the “Transport of the Future” (2022,
Russian University of Transport). Grant “Young teacher of
MIIT” after competitive selection in accordance with the
Regulations on grants for young teachers of MIIT (2016–
2019). Scholarship of the President of the Russian Federa-
tion for 2018–2020 for young scientists and graduate stu-
dents carrying out promising research and development in
priority areas of modernization of the Russian economy.
Grant of the Russian Science Foundation 2021 “Conduct-
ing fundamental scientiﬁc research and exploratory scien-
tiﬁc research by international research teams”. Reviewer
of articles, chapters and books IGI, Elsevier, Institute of
Physics Publishing, International Journal of Energy Opti-
mization and Engineering, Advances in Intelligent Systems
and Computing, Journal of the Operations Research Soci-
ety of China, Applied Sciences, Energies, Sustainability,
AgriEngineering, Ain Shams Engineering Journal, Concur-
rency and Computation: Practice and Experience. Presen-
ter of the sections of the Innovations in Agriculture con-
ference, keynote speaker of the ICO 2019 conference ses-
sion, keyspeaker of the special session of the ICO 2020
conference. Assistant Editor since 2019 of the “Interna-
tional Journal of Energy Optimization and Engineering”,
Guest Editor since 2019 of the Special Issues of the jour-
nal MDPI (Switzerland) “Applied Sciences”, Editor of the
book of the “IGI GLOBAL” (USA), as well as book of
the “Nova Science Publisher” (USA). Participated in more
than 100 exhibitions and conferences of various levels.
Published more than 250 scientiﬁc papers, including 14
patents, 1 international patent, 6 educational publications,
4 monographs.
J. Joshua Thomas is an Associate Professor at UOW
Malaysia KDU Penang University College, Malaysia since
2008. He obtained his Ph.D. (Intelligent Systems Tech-
niques) in 2015 from University Sains Malaysia, Penang,
and Master’s degree in 1999 from Madurai Kamaraj Uni-
versity, India. From July to September 2005, he worked
as a research assistant at the Artiﬁcial Intelligence Lab in
University Sains Malaysia. From March 2008 to March
2010, he worked as a research associate at the same Univer-
sity. Currently, he is working with Machine Learning, Big
Data, Data Analytics, Deep Learning, specially targeting on

About the Editors
xvii
Convolutional Neural Networks (CNN) and Bi-directional
Recurrent Neural Networks (RNN) for image tagging with
embedded natural language processing, End to end steering
learning systems and GAN. His work involves experimental
research with software prototypes and mathematical mod-
elling and design He is an editorial board member for the
Journal of Energy Optimization and Engineering (IJEOE),
and invited guest editor for Journal of Visual Languages
Communication (JVLC-Elsevier). Recently with Computer
Methods and Programs in Biomedicine (Elsevier). He has
published more than 40 papers in leading international
conference proceedings and peer reviewed journals.
Elias Munapo has a Ph.D. obtained in 2010 from the
National University of Science and Technology (Zim-
babwe) and is a Professor of Operations Research at the
North West University, Maﬁkeng Campus in South Africa.
He is a Guest Editor of the Applied Sciences Journal and has
co-published two books. The ﬁrst book is titled Some Inno-
vations in OR Methodology: Linear Optimization and was
published by Lambert Academic publishers in 2018. The
second book is titled Linear Integer Programming: The-
ory, Applications, and Recent Developments and was pub-
lished by De Gruyter publishers in 2021. Professor Munapo
has co-edited a number of books, is currently a reviewer
of a number of journals, and has published over 100 jour-
nal articles and book chapters. In addition, Prof. Munapo
is a recipient of the North West University Institutional
Research Excellence award and is a member of the Opera-
tions Research Society of South Africa (ORSSA), EURO,
and IFORS. He has presented at both local and international
conferences and has supervised more than 10 doctoral stu-
dents to completion. His research interests are in the broad
area of operations research.

xviii
About the Editors
Gerhard-Wilhelm Weber is a Professor at Poznan Uni-
versity of Technology, Poznan, Poland, at Faculty of Engi-
neering Management. His research is on mathematics,
statistics, operational research, data science, machine learn-
ing, ﬁnance, economics, optimization, optimal control,
management, neuro-, bio- and earth-sciences, medicine,
logistics, development, cosmology and generalized space-
time research. He is involved in the organization of sci-
entiﬁc life internationally. He received Diploma and Doc-
torate in Mathematics, and Economics/Business Adminis-
tration, at RWTH Aachen, and Habilitation at TU Darm-
stadt (Germany). He replaced Professorships at University
of Cologne, and TU Chemnitz, Germany. At Institute of
Applied Mathematics, Middle East Technical University,
Ankara, Turkey, he was a Professor in Financial Math-
ematics and Scientiﬁc Computing, and Assistant to the
Director, and has been a member of ﬁve further gradu-
ate schools, institutes and departments of METU. G.-W.
Weber has afﬁliations at Universities of Siegen (Germany),
Federation University (Ballarat, Australia), University of
Aveiro (Portugal), University of North Sumatra (Medan,
Indonesia), Malaysia University of Technology, Chinese
University of Hong Kong, KTO Karatay University (Konya,
Turkey), Vidyasagar University (Midnapore, India), Mazan-
daran University of Science and Technology (Babol, Iran),
Istinye University (Istanbul, Turkey), Georgian Interna-
tional Academy of Sciences, at EURO (Association of Euro-
pean OR Societies) where he is “Advisor to EURO Confer-
ences” and IFORS (International Federation of OR Soci-
eties), where he is member in many national OR societies,
honorary chair of some EURO working groups, subeditor
of IFORS Newsletter, member of IFORS Developing Coun-
tries Committee, of Paciﬁc Optimization Research Activity
Group, etc. G.-W. Weber has supervised many M.Sc. and
Ph.D. students, authored and edited numerous books and
articles, and given many presentations from a diversity of
areas, in theory, methods and practice. He has been a mem-
ber of many international editorial, special issue and award
boards; he participated at numerous research projects; he
received various recognitions by students, universities, con-
ferences and scientiﬁc organizations. G.-W. Weber is an
IFORS Fellow.

About the Editors
xix
Roman Rodriguez-Aguilar is a professor in the School
of Economic and Business Sciences of the “Universidad
Panamericana” in Mexico. His research is on large-scale
mathematical optimization, evolutionary computation, data
science, statistical modeling, health economics, energy,
competition, and market regulation. He is particularly inter-
ested in topics related to artiﬁcial intelligence, digital trans-
formation, and Industry 4.0. He received his Ph.D. at the
School of Economics at the National Polytechnic Institute,
Mexico. He also has a master’s degree in Engineering from
the School of Engineering at the National University of
Mexico (UNAM), a master’s degree in Administration and
Public Policy in the School of Government and Public Pol-
icy at Monterrey Institute of Technology and Higher Edu-
cation, a postgraduate in applied statistics at the Research
Institute in Applied Mathematics and Systems of the UNAM
and his degree in Economics at the UNAM. Prior to joining
Panamericana University, he has worked as a specialist in
economics, statistics, simulation, ﬁnance, and optimization,
occupying different management positions in various public
entities such as the Ministry of Energy, Ministry of Finance,
and Ministry of Health. At present, he has the second-
highest country-wide distinction granted by the Mexican
National System of Research Scientists for scientiﬁc merit
(SNI Fellow, Level 2). He has co-authored research articles
in science citation index journals, conference proceedings,
presentations, and book chapters.

Business, Economics, Finance,
Management, Social and Smart
Technology

Validating the Measurement Scale Items
on Readiness to Adopt Human Resource
Analytics in the Organizations of Nepal
Shanti Devi Chhetri1(B)
, Devesh Kumar1
, Deepesh Ranabhat2
,
and Pradeep Sapkota2
1 Faculty of Management Sciences, Shoolini University, Bajhol, Himachal Pradesh 173229,
India
schhetri635@gmail.com
2 Faculty of Management Studies, Pokhara University, Pokhara, Nepal
Abstract. The key purpose of this study is to validate the measurement scale
items on readiness to adopt Human Resource Analytics via pilot survey. Data were
gathered using a questionnaire following the distribution of (30) questionnaires
among the research sample. The data were examined with the use of the SPSS
and SmartPLS programs. The total of 78 items were established out of 104 items
for the testing reliability and validity of all the constructs. The study found that
composite reliability, Cronbach’s Alpha, AVE, the heterotrait-monotrait (HTMT)
ratio, Cross-loadings, and the Fornell and Larcker criterion, fulﬁll the required
threshold for reliability and validity. The study concluded that further study can
be done to validate the scale in the developing nations like Nepal.
Keywords: Human resource analytics · Composite reliability · Discriminant
validity · SmartPLS · Nepal
1
Introduction
The most common organizational skill gap is still in HR analytics. The ideal environment
for using HR data to make business and HR choices is being created by rapidly advanc-
ing technology capacity and increased accessibility to “people data” [1]. Real-time data
analysis enables businesses to observe the past and predict the future. This is the beneﬁt
of streaming analytics, which can describe what happened, diagnose why it happened,
predict what might happen in the future, and, ultimately, decide how to change the course
of events (prescriptive). The corporate world is experiencing a widespread emergence
of data analysis. Data is used widely in the sales, customer service, and ﬁnancial sec-
tors, and businesses are now looking to analyze data more in the human resources area
as well [2]. Although analytics have long been used to inform choices about human
capital management (HCM), far too many businesses still rely on these programs to
present straightforward correlations and descriptive statistics. It still needs a lot of work
before advanced analytics is completely employed for HCM decisions, although it has
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 3–13, 2024.
https://doi.org/10.1007/978-3-031-50158-6_1

4
S. D. Chhetri et al.
been accepted by other corporate areas like ﬁnance and marketing [3]. Over the past ten
years, data analytics has grown and changed by a variety of themes and discourses. The
development and use of numerous academic models helped HR professionals adopt HR
Analytics. In most companies, HR analytics are largely unheard of, therefore managers
frequently rely on their gut instincts when assessing employee data. By explaining how
HR practices and procedures affect an organization’s performance as a whole, HR ana-
lytics bring value to the company. Utilizing statistical tools and methods that assist in
establishing a connection with HR practices and HR indicators constitutes HR analytics
[4]. With the effective use of analytics in the human resources department, there may
be a lot more seamless ﬂow of communication between employers and employees, and
the efﬁciency of employees can be measured. The main focus of this paper is to validate
the measurement scale items on readiness to adopt HR analytics in the organizations of
Nepal.
2
Literature Review
2.1
Importance of HR Analytics
Analytics is the term used to describe the use of quantitative methodologies, decision-
making, and computer science to the organization, analysis, and explication of the grow-
ing volume of data produced by contemporary civilization. The inclusion of the word
“HR” denotes that these analyses are focused on internal company personnel [5]. How-
ever, [6] revealed that the majority of the surveyed organizations are at the basic level
when it comes to human resource analytics, which means they use analytical tools that
can measure HR based on descriptive factors, like the number of specialists who have
received training and those who need it instead of ﬁguring out a return on investment.
Again, [7] explained that analytics data that may monitor employee attendance, work-
place accidents, injuries, turnover costs, recruiting decisions, and engagement are used
to forecast the future HR elements in the organization. It offers the organization a sus-
tainable economic beneﬁt. Also, [8] suggested that the productivity and proﬁtability of
an organization are likely to increase if it combines efﬁcient HR management proce-
dures with efﬁcient HR technology. Akhmetova and Nevskaya [9] concluded that an
organization’s operational effectiveness is improved via HR analytics. The effectiveness
of data in decision-making, rather than the volume of data that is gathered and processed,
determines the success of HR analytics. The HR division should not be the only one to
consider HR analytics to be relevant. Analytics boosts a business’s overall performance.
van den Heuvel and Bondarouk [10] discovered that by 2025, HR analytics will be a
well-established ﬁeld, have a measurable impact on business results, and play a key
role in managerial and strategic decision-making. The integration will also be key to the
development of HR analytics, linking data and IT infrastructure across disciplines and
even corporate boundaries. The HR analytics position may also be a part of a central ana-
lytics role that crosses ﬁelds like marketing, ﬁnance, and HRM. Wandhe [11] stated that
not only can HR analytics help ﬁrms make the best HR decisions possible with reliable
data, but they also encourage them to keep appropriate quality data on hand to support
their HR investment decisions. Although HR Analytics is a new ﬁeld with considerable
interest, the creation of an integrated, strategic framework for its implementation and

Validating the Measurement Scale Items on Readiness
5
operation will lessen the uncertainty that early adopters currently experience. Bakre [12]
suggested that to give the best solutions for managing human capital, HR profession-
als must adapt to the changing circumstances and embrace an integrated management
model. By determining the appropriate methods for gauging its inﬂuence on the suc-
cess drivers, a business needs to comprehend and be able to measure the entire strategic
impact of HR.
2.2
Factors Affecting Adoption of HR Analytics
The researcher [13] used the UTAUT (the uniﬁed theory of technology adoption and use
of technology) approach to evaluate construction organizations’ intentions to adopt Big
Data Analytics (BDA). The study found that an organization’s decision to implement
BDA is inﬂuenced by factors related to conducive conditions, performance expectations,
and social inﬂuence. Likewise, [14] based on the Technology-Organization-Environment
framework complexity, compatibility, governmental backing, size of the organization,
level of competition, and environmental uncertainty were discovered to be important
factors in determining the adoption of BDAs. Similarly, [15] investigated the factors
affecting the intention to adopt BDA and found that competitive pressure, organizational
data environment, relative advantage, complexity, compatibility, support from top-level
management, technology readiness, and data availability in the organization signiﬁcantly
impact adoption. Also, [16] suggested that expectations of performance, effort, social
inﬂuence, and facilitating circumstances inﬂuence the behavioral intention to adopt HR
analytics. Lastly, [17] revealed that technological considerations, data infrastructure, and
data quality control are the most crucial.
3
Research Methodology
This paper focus on pilot study survey and adopts quantitative approach to analyze
reliability and validity of the measurement items for the adoption of HR analytics in
the organization of Nepal. The targeted sample for ﬁnal study is 300 HR managers
from different types of organization. Therefore, researcher distributed self-administered
questionnaire to 30 HR managers working in different sectors through google forms and
physical forms for pilot survey. Since, 10% of the total targeted sample is acceptable for
the pilot study [18, 19]. The questionnaire is divided into three sections, the ﬁrst section
comprises personal details of the respondents, the second section is related to readiness
and the third section includes 104 items on a 5-point Likert scale to analyze the constructs
used in this study. Table 1 shows the details of the constructs, items and indicators used
for the study. Data were analyzed using SPSS and Smart-PLS4. The personal details of
respondents and their readiness to adopt HR analytics were analyzed in SPSS. Whereas
the validity and reliability of the items were analyzed in Smart-PLS. The reliability
and validity of the scale’s items were examined using Cronbach’s alpha coefﬁcient,
composite reliability, convergent and discriminant validity, and cross-loadings.

6
S. D. Chhetri et al.
Table 1. List of constructs, items and indicators
Constructs
No. of items
Name of items
Indicators
Information technology
infrastructure
8
ITI1, ITI2, ITI3, ITI4,
ITI5, ITI6, ITI7, ITI8
Internet connection,
technical
infrastructure,
advanced software
Relative advantage
6
RA1, RA2, RA3, RA4,
RA5, RA6
Cost, quality of work,
time, effort
Complexity
7
C1, C2, C3, C4, C5, C6,
C7
Easy to use, data
maintenance,
implementation,
analysis of data
Compatibility
7
CO1, CO2, CO3, CO4,
CO5, CO6, CO7
Suitable, technical
changes, match with
present data, ﬁts with
organization
Top management support
6
TMS1, TMS2, TMS3,
TMS4, TMS5, TMS6
Understand, support,
involvement,
encouragement
Learning organization
8
LO1, LO2, LO3, LO4,
LO5, LO6, LO7, LO8
Advancements,
acquisition of new
skills, innovation
Financial readiness
6
FR1, FR2, FR3, FR4,
FR5, FR6
Fund,
budget allocation, loan
External pressure
6
EP1, EP2, EP3, EP4,
EP5, EP6
Competition, response,
competitive advantage
Employee orientation
5
EO1, EO2, EO3, EO4,
EO5
Opinion, requirement,
concern
Trading partner pressure
6
TP1, TP2, TP3, TP4,
TP5, TP6
Recommendation,
demand, request
Awareness
7
AW1, AW2, AW3,
AW4, AW5, AW6, AW7
Information, data
identiﬁcation,
advantage
Information technology
expertise/analytical skills
8
ITA1, ITA2, ITA3,
ITA4, ITA5, ITA6,
ITA7, ITA8
Technical knowledge,
abilities, IT literate,
proﬁciency
Innovation
9
IN1, IN2, IN3, IN4,
IN5, IN6, IN7, IN8, IN9
Learning, easy
adoption,
advancements
(continued)

Validating the Measurement Scale Items on Readiness
7
Table 1. (continued)
Constructs
No. of items
Name of items
Indicators
Readiness
7
RS1, RS2, RS3, RS4,
RS5, RS6, RS7
Ready to use,
interested, learning,
understanding
Level of adoption
8
HRA1, HRA2, HRA3,
HRA4, HRA5, HRA6,
HRA7, HRA8
Planning, initial phase,
adopted, interested
4
Data Analysis and Results
4.1
Personal Proﬁle and Readiness to Adopt HR Analytics
The personal proﬁle of respondents comprised of gender, type of organization they
are employed in, qualiﬁcation, and total experience as a human resource professional.
Whereas, readiness to adopt HR analytics include no. of employees in the organization,
the organization having a separate HR department, technology savvy human resource,
collection of employee-related data, analysis of employee recorded data, separate team
for HR analytics, beneﬁts experience by using HR analytics and vision and mission
for HR analytics. SPSS is used for analyzing the frequency distribution of respondent
proﬁles and the status of the organization.
In this paper, the majority of the respondents (66.7%) are male and only 33.3% of
the respondents are females. However, the majority of the respondents (80%) are from
service sectors which include banks, hospitals, cooperatives, and automobiles and only
20% of the respondents are from manufacturing sectors. In the case of education qual-
iﬁcation, most of the respondents are having masters and above (83.3%), respondents
hold a bachelor’s degree 13.3%, and only 3.3% hold a diploma certiﬁcate. Similarly, in
terms of experience as an HR professional, most of the respondents (63.3%) fall under
the category of 5 years and above. Likewise, the majority of the organization (73.3%)
have employees more than 101. In terms of the organization having separate HR depart-
ments, most of them (76.7%) have separate departments. Also, most of the organizations
(53.3%) are having 50% of the employees as technology savvy. Furthermore, the major-
ity of the organizations (96.7%) collect employee-related data and most of them (90%)
analyze employee-recorded data. Additionally, 76.7% of the organization have their team
for the analysis of data. Moreover, most organizations (53.3%) can manage employee
performance and productivity through HR analytics. Lastly, most of the organization
(50%) have not yet planned for the vision and mission for HR analytics.
4.2
Reliability and Validity
The validity and reliability is examined in the current study utilizing the partial least
squares (PLS) method. The current study focused on measurement model of SmartPLS
to validate the items in the measurement scale.

8
S. D. Chhetri et al.
Measurement Model
The measurement model evaluates the connection between a construct and its observed
elements. Indicator loading, construct reliability, convergent validity, and discriminant
validity are examined during the assessment of a measurement model. To assess mea-
surement model, study used 104 items in an initial phase (shown in Table 1). After
removing 26 items (AW3, EO1, EO2, EO5, EP1, EP2, EP3, EP4, HRA3, HRA4, HRA5,
IN6, IN7, IN8, IN9, ITI1, ITI2, ITI5, LO1, LO2, RS4, RS7, TP2, TP3, TP5, TP6), 78
total items were established for the testing reliability and validity of all the items. In the
current study, information technology (IT) infrastructure, relative advantage, complexity,
compatibility, support from top management, learning organization, ﬁnancial readiness,
external pressure, employees orientation, trading partner pressure, awareness, informa-
tion technology expertise/analytical skills, innovation, readiness are considered as latent
variables of HR analytics.
Validating the Constructs
Cronbach Alpha and Composite Reliability (CR) were used to examine the reliability
of the study’s constructs. According to [20] both of these measures should have a value
above 0.70 for the constructs to be reliable. The results in Table 2 show that both measures
meet the requirements for acceptance, establishing reliability.
Table 2. Composite reliability and convergent validity with acceptable values (Cronbach’s alpha,
composite reliability ≥0.70 and AVE > 0.5)
Constructs
Cronbach’s alpha
Composite reliability
(AVE)
Awareness
0.951
0.961
0.804
Compatibility
0.950
0.955
0.755
Complexity
0.912
0.927
0.647
Employee orientation
0.735
0.876
0.781
External pressure
0.793
0.884
0.794
Financial readiness
0.935
0.948
0.753
HRA adoption
0.874
0.909
0.666
IT infrastructure
0.962
0.970
0.865
IT expertise
0.957
0.963
0.765
Innovation
0.875
0.906
0.661
Learning organization
0.920
0.937
0.715
Readiness
0.897
0.923
0.706
Relative advantage
0.953
0.960
0.800
Top management support
0.956
0.964
0.818
Trading partner
0.909
0.957
0.917

Validating the Measurement Scale Items on Readiness
9
Construct Validity
The degree to which a construct used in the study accurately measures what it is intended
to assess is known as construct validity. Convergent validity and discriminant validity
are two types of validity that are evaluated using construct validity.
Convergent Validity
Convergent validity is the measurement of the degree of agreement between several
indicators of the same construct. The indicator’s cross-loadings, composite reliability
(CR), and average variance extracted (AVE) all need to be analyzed to prove convergent
validity. The value is between 0 and 1. To be sufﬁcient for convergent validity, the AVE
value should be more than 0.50 [21]. In the current study, all the measured constructs’
AVE value is above 0.50. Thus, convergent validity is determined.
Discriminant Validity
Discriminant validity describes how much an empirically determined construct differs
from one another. It also evaluates the degree to which the overlapping constructions
diverge from one another. Cross-loadings of indicators, the Fornell and Larcker crite-
rion, and the heterotrait-monotrait (HTMT) ratio of correlation are used to evaluate the
discriminant validity [20].
Cross Loadings
When compared to other constructs in the study, an item should have larger loadings
on its parent construct, according to cross-loadings [20]. The ﬁnding reveals that the
cross-loadings of the items in their parent construct are higher than the loadings on other
constructs. As a result, discriminant validity is determined.
Fornell and Larcker
In this approach, the correlation of latent constructs is contrasted with the average
extracted square root of the variance (AVE). A latent construct should be able to account
for variation in its indicator than for variance in other latent constructs. Accordingly,
correlations with other latent constructs must be lower than the square root of each con-
struct’s AVE [21]. According to Table 3, the square root of the AVE (top value in each
column) was higher than its correlation with other components. The ﬁndings of the study
provide strong support for discriminant validity.
Heterotrait-Monotrait Ratio (HTMT) Ratio
Correlations with an acceptable “Heterotrait-Monotrait ratio (HTMT)” value of less
than 0.85 should be used to assess the discriminant validity [22]. The results in Table 4
demonstrate that all values are acceptable, hence the discriminant validity is established.
5
Conclusion
The digital transformation of HR is a trend these days, emerging themes include Human
Resource Analytics (HRA), artiﬁcial intelligence (AI), and cloud-based HR technology.
However, gaps between research and practice are once again felt strongly. Developing

10
S. D. Chhetri et al.
Table 3. Discriminant validity-Fornell and Larcker criteria
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Awareness
0.897
Compatibility
0.081
0.869
Complexity
0.073
0.467
0.804
Employee
orientation
0.069
0.561
0.287
0.884
External
pressure
0.541
0.293
0.418
0.427
0.891
Financial
readiness
0.499
0.229
0.146
0.447
0.616
0.867
HRA adoption
0.476
0.294
0.368
0.520
0.457
0.683
0.816
IT
infrastructure
−0.006
0.253
0.059
0.052
0.165
0.125
0.245
0.930
IT expertise
0.732
0.154
−0.082
0.154
0.546
0.492
0.413
0.161
0.875
Innovation
0.472
0.397
0.245
0.380
0.272
0.353
0.559
0.353
0.411
0.813
Learning
organization
0.462
0.184
0.136
0.291
0.487
0.546
0.381
0.108
0.529
0.662
0.846
Readiness
0.676
0.385
0.167
0.190
0.487
0.481
0.481
0.332
0.624
0.674
0.660
0.840
Relative
advantage
−0.105
0.517
0.523
0.417
0.109
0.107
0.338
0.083
−0.025
0.162
0.171
−0.054
0.895
Top
management
support
0.259
0.324
0.287
0.261
0.569
0.555
0.271
0.257
0.320
0.386
0.758
0.547
0.326
0.905
Trading partner
0.540
0.151
0.381
0.526
0.583
0.515
0.641
−0.094
0.343
0.408
0.361
0.413
0.084
0.210
0.958
The bold values are square root of AVE which is higher than its correlation with other components which shows the establishment of discriminant validity

Validating the Measurement Scale Items on Readiness
11
Table 4. Discriminant validity-HTMT ratio
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
Awareness
Compatibility
0.141
Complexity
0.177
0.473
Employee orientation
0.128
0.686
0.350
External pressure
0.568
0.350
0.383
0.558
Financial readiness
0.520
0.211
0.225
0.563
0.677
HRA adoption
0.508
0.260
0.381
0.612
0.460
0.734
IT infrastructure
0.132
0.251
0.183
0.085
0.195
0.168
0.256
IT expertise
0.760
0.170
0.189
0.166
0.588
0.501
0.456
0.181
Innovation
0.517
0.445
0.278
0.435
0.327
0.374
0.593
0.381
0.460
Learning organization
0.477
0.263
0.219
0.382
0.498
0.578
0.402
0.169
0.562
0.755
Readiness
0.721
0.362
0.210
0.255
0.464
0.505
0.527
0.321
0.670
0.776
0.723
Relative advantage
0.176
0.554
0.542
0.517
0.145
0.179
0.318
0.122
0.118
0.228
0.186
0.154
Top management support
0.267
0.336
0.316
0.395
0.614
0.593
0.309
0.258
0.336
0.432
0.793
0.592
0.302
Trading partner
0.585
0.153
0.381
0.631
0.637
0.555
0.711
0.110
0.355
0.424
0.377
0.421
0.115
0.214

12
S. D. Chhetri et al.
nations are still on initial phase of adoption of analytics. Hence, this study focus on
validating the scale to measure readiness to adopt HR analytics in developing nations
like Nepal. Reliability and Discriminant Validity was examined to check the reliability
and validity. Out of 104 items, 78 items were established for the test.
The study concludes that using this scale can further be validated and use to analyze
the adoption level of HR analytics in the nations like Nepal. Awareness of HR analytics
and its usage is still to be known by many organizations. There are numerous activi-
ties performed in HR department. Through research we can identify for what purpose
analytics are used.
6
Research Limitations and Future Directions
Although the study presents the reliability and validity test of the items for readiness
to adopt HR analytics in the organizations of Nepal. There are number of shortcomings
that should be considered when analysing the results. First, the survey is done to a small
size and is limited to organizations of Nepal. However, more research must be done
with larger size and in other nations. Second, the study has not considered moderating
factors for adoption of HR analytics. Understanding whether other moderating factors
can affect or impact the intention to adopt HRA for the purpose of changing adoption
intention is necessary. Lastly, the study is a pilot survey done in Pokhara city of Nepal.
Further research need to be carried out taking different cities of Nepal for validating the
scale.
References
1. McCartney, S., Fu, N.: Bridging the gap: why, how and when HR analytics can impact
organizational performance. Manag. Decis. 60(13), 25–47 (2021). https://doi.org/10.1108/
MD-12-2020-1581
2. Klimoski, R., et al.: Use of Workforce Analytics for Competitive Advantage, pp. 1–38. SHRM
Report (2016)
3. Sesil, J.C.: Applying Advanced Analytics to HR Management Decisions: Methods for Selec-
tion, Developing Incentives, and Improving Collaboration (2013). [Online]. Available: https://
books.google.com/books?id=8R37AAAAQBAJ&pgis=1
4. Israrul Haque, M.: Human Resource Analytics: A Strategic Approach
5. Karma´nska, A.: The beneﬁts of HR analytics. Pr. Nauk. Uniw. Ekon. Wroc. 64(8), 30–39
(2020). https://doi.org/10.15611/pn.2020.8.03
6. Kapoor, B., Kabra, Y.: Current and future trends in human resources analytics adoption. J.
Cases Inf. Technol. 16(1), 50–59 (2014). https://doi.org/10.4018/jcit.2014010105
7. Alamelu, R., Nalini, R., Cresenta Shakila Motha, L., Amudha, R., Bowiya, S.: Adoption
factors impacting human resource analytics among employees (2017). [Online]. Available:
https://nsuworks.nova.edu/hsbe_etd
8. Johnson, R.D., Gueutal, H.G.: SHRM Foundation’s Effective Practice Guidelines Series the
Use of E-HR and HRIS in Organizations Transforming HR Through Technology (2017).
[Online]. Available: www.shrm.org/foundation
9. Akhmetova, S.G., Nevskaya, L.V.: HR Analytics: Challenges and Opportunities in Russian
Companies. New Silk Road: Business …. atlantis-press.com (2020). https://doi.org/10.2991/
aebmr.k.200324.011

Validating the Measurement Scale Items on Readiness
13
10. van den Heuvel, S., Bondarouk, T.: The rise (and fall?) of HR analytics: a study into the future
application, value, structure, and system support. J. Organ. Eff. 4(2), 157–178 (2017). https://
doi.org/10.1108/JOEPP-03-2017-0022
11. Wandhe, P.: HR analytics: a tool for strategic approach to HR productivity. SSRN Electron.
J. (2020). https://doi.org/10.2139/ssrn.3700502
12. Bakre, D.M.P.: The role of HR analytics in the global village. Int. J. Trend Sci. Res. Dev.
3(2), 210.212.169.38 (2019)
13. Aghimien, D.O., Ikuabe, M., Aigbavboa, C., Oke, A., Shirinda, W.: Unravelling the factors
inﬂuencing construction organisations’ intention to adopt big data analytics in South Africa.
Constr. Econ. Build. 21(3), 262–281 (2021). https://doi.org/10.5130/AJCEB.V21I3.7634
14. Agrawal, K.P.: Investigating the determinants of big data analytics (BDA) adoption in Asian
emerging economies. In: 2015 America’s Conference on Information Systems AMCIS 2015,
pp. 1–18 (2015).https://doi.org/10.5465/ambpp.2015.11290abstract
15. Verma, S., Chaurasia, S.: Understanding the determinants of big data analytics adoption. Inf.
Resour. Manag. J. 32(3), 1–26 (2019). https://doi.org/10.4018/IRMJ.2019070101
16. Ekka, S., Singh, P.: Predicting HR professionals’ adoption of HR analytics: an extension of
UTAUT model. Organizacija 55(1), 77–93 (2022). https://doi.org/10.2478/orga-2022-0006
17. Nam, D., Lee, J., Lee, H.: Business analytics adoption process: an innovation diffusion per-
spective. Int. J. Inf. Manage. 49, 411–423 (2019). https://doi.org/10.1016/j.ijinfomgt.2019.
07.017
18. Ã, S.M.F., El-Masri, M.M.: Focus on research methods handling missing data in self-report
measures, pp. 488–495 (2005). https://doi.org/10.1002/nur
19. Pilot Studies - Document - Gale Academic OneFile. [Online]. Available: https://go.gale.
com/ps/i.do?p=AONE&u=googlescholar&id=GALE%7CA192589717&v=2.1&it=r&sid=
AONE&asid=87470d30
20. Hair, J.F., Hult, G.T.M., Ringle, C.M., Sarstedt, M.: A Primer on Partial Least Squares
Structural Equation Modeling (PLS-SEM), p. 165. Thousand Oaks, Sage (2017)
21. Ab Hamid, M.R., Sami, W., Mohmad Sidek, M.H.: Discriminant validity assessment: use of
Fornell & Larcker criterion versus HTMT criterion. J. Phys. Conf. Ser. 890(1) (2017). https://
doi.org/10.1088/1742-6596/890/1/012163
22. Al-Skaf, S., Youssef, E., Habes, M., Alhumaid, K., Salloum, S.A.: The acceptance of social
media sites: an empirical study using PLS-SEM and ML approaches. Adv. Intell. Syst.
Comput. 1339, 548–558 (2021). https://doi.org/10.1007/978-3-030-69717-4_52

Optimizing Fire Response Unit Location
for Urban-Rural Area
Sunarin Chanta(B) and Ornurai Sangsawang
King Mongkut’s University of Technology North Bangkok, 129 M.21 Noenhom, Muang
Prachinburi 25230, Thailand
{sunarin.s,ornurai.s}@itm.kmutnb.ac.th
Abstract. Fire safety is very important. Efﬁcient facility planning is required to
protect against destruction and reduce the risk of damage to facilities, and loss of
lives, In this study, we propose an urban-rural maximal covering location model
with the conﬁguration of different types of areas for optimizing ﬁre response units.
Since the basic covering location model tends to cover more in a high population
density area, with a limitation of resources a low population density area may
leave as an unserved area. The objective is to maximize the demand that can be
covered in standard response time. The GIS is utilized for managing the data and
classifying the service area. A real-world case study is presented. The results show
that solving problems with the proposed model can achieve full coverage with less
number of facilities.
Keywords: Facility location · Optimization · Rural · GIS · Fire department
1
Introduction
The Royal Thai Government has launched the 20-Year Strategy for Thailand to achieve
high-income status by 2036, namely Thailand 4.0. The government initiates the Eastern
Economic Corridor (EEC) project aiming to revitalize the well-known eastern seaboard.
The EEC is a special economic zone that consists of 3 provinces in the east of Thailand,
namely Rayong, Chonburi, and Chachoengsao. The EEC development plan envisages a
signiﬁcant transformation of both physical and social development and plays an impor-
tant role as a regulatory sandbox uplifting the country’s competitiveness. The objective
of EEC development is to support the investment in the supercluster industry and the
country’s target industries to be a mechanism to drive the economy in the next 20 years.
The government has invested in the transportation infrastructure to support the industrial
sector for enhancing the quality and connectivity within the country and the region more
by land, sea and air, as well as to support the transportation and logistics hub of Asia
in the future. The transportation infrastructure includes the construction of high-speed
trains, dual-track trains, Laem Chabang Port, Sattahip Port, and U-Tapao Airport.
Chonburi, one of the 3 provinces under the EEC project, is the province that has
expanded the most representing 3.80% per year, followed by Rayong representing 3.17%
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 14–21, 2024.
https://doi.org/10.1007/978-3-031-50158-6_2

Optimizing Fire Response Unit Location for Urban-Rural Area
15
per year, and Chachoengsao representing 1.57% per year, respectively. This data indi-
cates the potential of the area where entrepreneurs have been setting up establishments
continuously in the past 3 years and are still likely to expand more in the future. The
information shows that Chonburi province tends to increase in terms of establishments,
which is possible that accidents or risks in terms of ﬁre may have a high chance of
occurring. So in this case protection and prevention against damage from emergency
incidents are essential. The Department of Disaster Prevention and Mitigation has a
duty to prevent and extinguish the ﬁre and various public disasters, including natural
disasters and other emergency incidents.
In this study, we aim to determine the optimal location of ﬁre response units, which
include ﬁre trucks, staff, and necessary equipment for ﬁre response units in the area of
Chonburi province, Thailand. Since the area consists of both urban and rural land, so we
proposed a mathematical model to solve the problem. The objective is to maximize the
coverage in the whole area under the limitation of resources.
2
Literature Review
The ﬁrst mathematical model for facility location was the Maximal Covering Location
Model (MCLP), which was ﬁrst introduced by Church and ReVelle [1], with the objective
of maximizing demands that can be covered subject to the limitation of the number of
facilities. Demand can be covered when there exists a facility located within the standard
response time. The MCLP model is well known and was applied for locating public
facilities such as ﬁre stations, ambulance stations, relief centers, etc. [2–6]. In the MCLP
model, the standard response time is assumed to be the same for all the demands in the
whole service area. However, in reality, there are some cases in which we cannot provide
the same service standard for the whole area.
In this study, we focus on the service area that has different population densities:
urbanandrural.Urbanregionscanbedescribedascityareaswithhighpopulationdensity,
number of housing units, number of buildings, business areas, etc. On the opposite, rural
regions are countryside areas with less population density [7]. More factors to classify
areas such as age structure, registered resident structure, income level, etc. [8]. Therefore,
with a limited number of facilities, public service departments tend to manage their
facilities to be located in urban areas to save more people. The previous works related
to facility location planning are brieﬂy reviewed here.
Chanta et al. [9] improved the quality of EMS in rural areas by providing a bi-
objective covering location model to locate ambulances at appropriate stations in the
urban-rural area. The ﬁrst objective was to maximize the expected number of requested
calls that can be covered, while the second objective was to reduce disparities in service
between different demographics. Three alternatives for the second objective had been
proposed, which were minimizing the maximum distance between uncovered demand
zones and their closest opened stations, minimizing the number of uncovered rural
demand zones, and minimizing the number of uncovered demand zones.
Karim et al. [10] integrated Geographic Information System (GIS)-based network
analysis and multicriteria decision analysis (MCDA) technique for locating public ser-
vicefacilitiesinthedistrictsofBuraidahthecityintheKingdomofSaudiArabia.TheGIS

16
S. Chanta and O. Sangsawang
Network Analysis tools were applied to analyze the service area. Different coverages
were set for different services including universities, hospitals, government services,
etc. The Analytic Hierarchy Process (AHP) technique was used to determine the crite-
ria weights, and the location-allocation model was used for suggestions of new service
locations.
Luo et al. [11] proposed a multi-objective optimization model to reduce urban-rural
inequalities in EMS accessibility and coverage in Wuhan, China. The ﬁrst objective was
to minimize the total weighted distance of the uncovered demand from the nearest open
EMS station in a rural area, which virtually maximized the accessibility of uncovered
rural demand. The other two objectives were maximizing the service coverage and
minimizing urban-rural inequality in service coverage.
Liu et al. [12] proposed a location model for selecting the location of Park-and-Ride
(P&R) facilities in the outer suburbs and in the urban areas. They considered the coverage
demand characteristics, with the goal of maximizing the mileage of truncated private
vehicles (for outer suburbs) and maximizing the demand for P&R facility coverage (for
urban areas). Their P&R facility location model was constructed based on the travel
choice behavior model and the progressive cooperation coverage model.
For more review on service facility location, see Farahani et al. [13].
3
The Proposed Mathematical Model
The proposed mathematical model is developed based on the formulations of the MCLP.
Instead of using the same coverage, in this model the coverage is deﬁned according to
the types of areas. The urban areas require a short standard response time than the rural
areas. The proposed mathematical model is described as follows.
Index and Notations
i = set of zones = 1, 2, …, n
j = set of facility locations = 1, 2, …, m
n = number of zones
m = number of facility locations
P = number of facilities to be located
Parameters
hi = number of calls in zone i
ri = 1 if zone i is rural
= 0 otherwise
dij = distance from zone i to facility location at j
Du = coverage distance for urban area (the maximum distance that facility can
respond to a call within the urban standard time)
Dr = coverage distance for rural area (the maximum distance that facility can respond
to a call within the rural standard time)
aij = 1 if a call in zone i can serve by facility j within urban standard response time
Du (dij ≤Du)
= 0 otherwise

Optimizing Fire Response Unit Location for Urban-Rural Area
17
bij = 1 if a call in zone i can serve by facility j within rural standard response time
Dr (dij ≤Dr)
= 0 otherwise
Decision Variables
Yiu = 1 if urban zone i is covered by a facility
= 0 otherwise
Yir = 1 if rural zone i is covered by a facility
= 0 otherwise
Xj = 1 if there is a facility located at location j
= 0 otherwise
Optimization Model
Maximize
n

i=1
hiriY r
i +
n

i=1
hi(1 −ri)Y u
i
(1)
Subject to
Y u
i ≤
n

i=1
m

j=1
aijXj, ∀i
(2)
Y r
i ≤
n

i=1
m

j=1
bijXj, ∀i
(3)
m

j=1
Xj ≤P
(4)
Xj ∈{0, 1}, ∀j
(5)
Y u
i , Y r
i ∈{0, 1}, ∀i
(6)
The objective in Eq. (1) is to maximize the total expected calls that can be covered in
a standard time. Note that in this study, there are two types of service areas, namely urban
(with high a volume of calls) and rural (with a low volume of calls). So, the coverage
conditions in the urban and rural areas will be different with respect to the area type.
Constraints in Eqs. (2)–(3) force that a call can be covered if there exists a facility at
a facility location. Note that a call in an urban zone can be covered within the urban
standard response time, while a call in a rural zone may be covered within the rural or
urban standard response time, respectively. Constraint in Eq. (4) limits the number of
facilities to be located. Constraints in Eqs. (5)–(6) assign the domain of the decision
variables.

18
S. Chanta and O. Sangsawang
4
A Case Study of Fire Department, Chonburi, Thailand
Chonburi province has an area of approximately 4,363 km2. The administrative area is
divided into 11 districts and 92 sub-districts. The districts in Chonburi province are as
follows: (1) Mueang, (2) Ban Bueng, (3) Nong Yai, (4) Bang Lamung, (5) Phan Thong,
(6) Phanat Nikhom, (7) Sri Racha, (8) Koh Si Chang, (9) Sattahip, (10) Bo Thong,
(11) Koh Jan. The total population is 1,566,875 people (as of 2020), with the density of
359.13 people per square kilometer. From the data of emergency notiﬁcation via number
1669 in 2017, there were a total of 14,274 emergency calls, divided into 1,704 critical
calls, 9,718 emergency calls, 2,539 non-urgent calls, 282 general calls, and 31 other
calls. The data are analyzed based on the GIS software. The distribution of emergency
calls is classiﬁed by type of emergency or severity shown in Fig. 1. As you can see from
Fig. 1 Chonburi consists of both urban and rural areas. The high volume of emergency
requested calls appeared more on side of the region.
To manage the area and the calls, we divided the area of the whole province of
Chonburi into 3 × 3 km2 grid size zones. A total of 609 zones were considered to be
cut off since the zones did not have the incidents. Therefore, the total service area is 423
zones. The density of emergency incidents in each zone is shown in Fig. 2.
Fig. 1. Distribution of emergency calls in the area of Chonburi province, Thailand.

Optimizing Fire Response Unit Location for Urban-Rural Area
19
Fig. 2. Density of emergency calls in the area of Chonburi province, Thailand.
5
Computational Results
To determine the proper location of the ﬁre response units, we used the data of emergency
calls via number 1669 in the year 2017 obtained from the National Institute of Emergency
Medicine (NIEM). In this case, we have n = number of zones = 423, m = number of
facility locations = 423. The area types are classiﬁed as rural (with a number of calls
than or equal to 30 calls) and urban (with a number of calls more than 30 calls). The
urban standard response time is set at 10 min, while the rural standard response time is
set at 20 min. The number of facilities varies from 5 to 20. To compare the results of
MCLP and the proposed model, we conduct 2 computational experiments. At ﬁrst, we
solve the problem with the MCLP model. The results of MCLP are shown in Table 1.
Then, we solve the problem with the proposed model, where urban and rural areas are
treated differently. The results of the proposed model are shown in Table 2. As you can,
see that by using the proposed model, less number of facilities require for the same
coverage. In this case, instead of leaving the rural areas uncovered due to the limitation
of the number of facilities, we can serve the rural area with longer coverage.

20
S. Chanta and O. Sangsawang
Table 1. The results of MCLP model
Number of units
Optimal location
Coverage
5
{68 81 103 179 270}
84.09
6
{68 81 103 179 270 295}
88.04
7
{34 69 81 103 179 270 295}
91.69
8
{34 69 76 110 180 186 251 313}
94.07
9
{34 69 76 110 180 186 251 313 334}
95.61
10
{34 69 76 110 180 186 251 295 334 364}
96.83
15
{26 38 43 60 110 146 151 180 244 251 260 312 334 375 389}
99.47
20
{10 26 29 34 84 96 125 154 182 186 232 246 279 289 333 343
383 396 403 418}
100.00
Table 2. The results of MCLP with urban-rural area model
Number of units
Optimal location
Coverage
5
{69 104 111 198 251}
86.61
6
{68 81 103 179 270 295}
90.50
7
{34 69 81 103 179 270 295}
94.14
8
{34 69 76 110 180 186 251 313}
96.55
9
{34 69 76 110 180 186 251 313 334}
97.99
10
{34 69 76 110 180 186 251 295 334 364}
98.91
14
{13 18 25 99 111 153 162 189 232 277 287 346 353 407}
100.00
6
Conclusion and Future Research
In this study, we purpose a mathematical model for determining the optimal ﬁre response
units for ﬁre departments. A case study of Chonburi province, Thailand is presented.
The objective is to maximize the number of incidents that can be responded to within the
standard time. Since the case study area consists of urban and rural parts, so 2 different
standard times are deﬁned for both urban and rural areas. The GIS is applied to partition
the service zone and classify the incidents over the response area. The results show that
by using the proposed mathematical model, we efﬁciently optimal locate ﬁre response
units by maximizing the coverage under a low number of facilities.
In this present study, we classify the service area based on incident density, and all
incidents are treated the same. However, the severity of the incidents may different and
requires different equipment and different types of rescue trucks. For future research, we
would like to develop a model that considers different types of incidents corresponding
to different types of facilities.

Optimizing Fire Response Unit Location for Urban-Rural Area
21
References
1. Church, R., ReVelle, C.: The maximal covering location problem. Pap. Reg. Sci. Assoc. 32,
101–118 (1974)
2. Pirkkul, H., Schilling, D.: The maximal covering location problem with capacities on total
workload. Manage. Sci. 37(2), 233–248 (1991)
3. Black, B., Meamon, B.: Facility location in humanitarian relief. Int. J. Log. Res. Appl. 11(2),
101–121 (2008)
4. Chanta, S., Sangsawang, O.: Shelter-site selection during ﬂood disaster. Lect. Notes Manag.
Sci. 4, 282–288 (2012)
5. Alzahrani, A., Hanbali, A.: Maximum coverage location model for ﬁre stations with top
corporate risk locations. Int. J. Ind. Eng. Oper. Manag. 3(2), 58–74 (2021)
6. Srianan, T., Sangsawang, O.: Path-relinking for ﬁre station location. In: Vasant, P., Zelinka, I.,
Weber, G.W. (eds.) Intelligent Computing & Optimization. ICO 2018. Advances in Intelligent
Systems and Computing, vol. 866. Springer, Cham (2019)
7. U.S. Department of Agriculture, Economic Research Service, Rural Classiﬁcation. Accessible
at https://www.ers.usda.gov
8. Wang, J., Zhou, J.: Spatial evaluation of the accessibility of public service facilities in
Shanghai: a community differentiation perspective. PLoS ONE 17(5), e0268862 (2022)
9. Chanta, S., Mayorga, M., McLay, L.: Improving emergency service in rural areas: a bi-
objective covering location model for EMS systems. Ann. Oper. Res. 221, 133–159 (2014)
10. Karim, A., Awawdeh, M.: Integrating GIS accessibility and location-allocation models
with multicriteria decision analysis for evaluating quality of life in Buraidah City, KSA.
Sustainability 12, 1412 (2020)
11. Luo, W., Yao, J., Mitchell, R., Zhang, X., Li, W.: Locating emergency services to reduce
urban-rural inequalities. Socio-Econ. Plann. Sci. 84, 101416 (2022)
12. Liu, H., Li, Y., Hou, B., Zhao, S.: Optimizing the location of park-and-ride facilities in subur-
ban and urban areas considering the characteristics of coverage requirements. Sustainability
14, 1502 (2022)
13. Farahani, R., Fallah, S., Ruiz, R., Hosseini, S., Asgari, N.: OR models in urban service facility
location: a critical review of applications and future developments. Eur. J. Oper. Res. 276,
1–27 (2019)

Effects of Financial Literacy on Financial
Inclusion: Evidence from Nepal’s Gandaki
Province
Deepesh Ranabhat1(B)
, Narinder Verma1
, Pradeep Sapkota2
,
and Shanti Devi Chhetri2
1 Faculty of Management Sciences, Shoolini University, Bajhol, (HP) 173229, India
deepeshrana2000@gmail.com
2 Faculty of Management Studies, Pokhara University, Pokhara-30, Kaski, Nepal
Abstract. Financial inclusion is regarded as an essential instrument for socioe-
conomic development. It has become a big issue in developing nations, as many
people are still economically excluded. Different factors including ﬁnancial liter-
acy signiﬁcantly inﬂuence ﬁnancial inclusion. This study aims to determine how
ﬁnancialliteracyimpactsﬁnancialinclusion.ThestudyareaistheprovinceofGan-
daki in Nepal, from which a sample of one thousand respondents is selected for
data collecting. For the study of independent and dependent variables, the Partial
Least Squares Structural Equation Modelling method is applied. The examina-
tion of the data indicates that ﬁnancial literacy considerably improves ﬁnancial
inclusion in the study region. This study concludes that ﬁnancial inclusion can
be improved by educating individuals on the signiﬁcance of ﬁnancial products
and services. These ﬁndings aid policymakers in implementing ﬁnancial literacy
programs to improve ﬁnancial inclusion.
Keywords: Financial inclusion · Financial literacy · Development · Structural
equation modelling · Nepal
1
Introduction
Financial inclusion refers to the delivery of various ﬁnancial services including deposits,
loans, insurance, and remittances to everyone in the country easily and at a lower price.
It is broadly considered an important tool for socio-economic development; however,
ﬁnancial inclusion is hampered by a lack of ﬁnancial literacy. People will not demand
ﬁnancial products and services when they are unware about them. Functioning ﬁnancial
markets need knowledgeable customers, i.e. customers who are more ﬁnancially liter-
ate to make better ﬁnancial decisions. Literate customers demand more sophisticated
ﬁnancial services which enhance ﬁnancial inclusion. Financial inclusion is affected by
a person’s ﬁnancial literacy level [1]. Financial literacy also improves the overall well-
being of people by giving them the fundamental tools for budgeting, motivating them to
save, and ensuring that they can live a respectable life after retirement [2].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 22–32, 2024.
https://doi.org/10.1007/978-3-031-50158-6_3

Effects of Financial Literacy on Financial Inclusion
23
Financial literacy is a key concept in the context of ﬁnancial inclusion. It is an ability
to handle one’s ﬁnancial resources successfully to achieve long-term ﬁnancial security
[3]. It is deﬁned as an understanding of fundamental economic principles, along with the
capacity to apply that knowledge and skill for the efﬁcient use of money for long-term
economic well-being [4].
According to the ﬁnancial literacy-based approach of ﬁnancial inclusion, increas-
ing individuals’ ﬁnancial literacy through education is necessary to achieve ﬁnancial
inclusion. If people are ﬁnancially educated, they will be more likely to participate in
the formal ﬁnancial sphere. Financial literacy helps to inform people about the ﬁnancial
services and products available to them and their importance, which increases their par-
ticipation in the formal banking services by having a bank account. Likewise, by having
better ﬁnancial literacy, people can beneﬁt from additional advantages offered by the
formal ﬁnancial system, like investment and mortgage products. Financial literacy also
helps individuals be independent and stable in their personal ﬁnances by educating indi-
viduals to discriminate between needs and wants, make and maintain a budget, save for
timely payment of bills, and make retirement plans [5].
Financial inclusion has emerged as a signiﬁcant concern in developing countries
like Nepal, where a large proportion of the population (approximately 55%) still lacks
accessibility of formal ﬁnancial services [6]. People’s ability to access formal banking
services is hampered by their low literacy level. In Nepal, 40% of adults are illiterate
and the majority of the literate population have basic and secondary-level education only
[7]. Because of low literacy levels, many individuals are not conscious of the ﬁnancial
services provided by the ﬁnancial institution and their function. Due to signiﬁcance of
ﬁnancial literacy, the central bank of Nepal, Nepal Rastra Bank (NRB), with an empha-
sis on the Government of Nepal (GoN), has set an objective to increase ﬁnancial access
through enhancing ﬁnancial literacy and ﬁnancial education. Different policies and pro-
grams including ﬁnancial literacy programs have been conducted by the central bank of
Nepal and other organizations to increase ﬁnancial inclusion in Nepal. However, to the
best of the researchers’ understanding, no study has been done in Nepal to determine
how ﬁnancial literacy impacts ﬁnancial inclusion. Determining how ﬁnancial literacy
affects ﬁnancial inclusion is crucial. Thus, this study seeks to quantify how ﬁnancial
inclusion is inﬂuenced by ﬁnancial literacy in Gandaki province, Nepal.
2
Literature Review
2.1
Concept of Financial Literacy and Financial Inclusion
Financial literacy is deﬁned as understanding and use of ﬁnancial knowledge, skills,
and theories to help people make wise ﬁnancial decisions for the betterment of their life
[8]. It includes ﬁnancial knowledge, awareness, attitude, skill, and behavior required
to make wise economic decisions and attain the economic well-being of an individual
[9]. Both deﬁnitions focus on understanding and use of ﬁnancial concepts to obtain
economic well-being. In this research, ﬁnancial literacy refers to the combination of
ﬁnancial knowledge, skill, attitude, and behavior that help an individual to select and
use various ﬁnancial products and services for better economic life.

24
D. Ranabhat et al.
Financial inclusion as deﬁned by Centre for Financial Inclusion is the state to which
each person who is eligible to use ﬁnancial services has accessibility of full range of
superior services that are provided conveniently, at reasonable prices, and with consider-
ation for the customers. CRISIL deﬁne ﬁnancial inclusion as the degree of accessibility
of formal ﬁnancial service, comprising of services such as deposits, credits, remittance,
insurance, and pensions, available to all sectors of society. [10] used three dimensions
to construct the ﬁnancial inclusion index. These dimensions comprise of accessibility,
availability, and usage, all of which contribute towards the overall measurement of ﬁnan-
cial inclusion. Different researchers [11, 12], and [13] have used similar dimensions used
by [10]. The term ﬁnancial inclusion in this research pertains to the degree of access,
availability, and utilization of ﬁnancial services across all segments of the community.
2.2
Financial Literacy and Financial Inclusion Relationship
Having ﬁnancial literacy is a fundamental requirement for achieving ﬁnancial inclusion
[14]. Raising understanding of various ﬁnancial services, saving, capital management,
and credit management improves ﬁnancial inclusion by increasing the demand for ﬁnan-
cial products [15]. Numerous studies have proven the connection between ﬁnancial lit-
eracy and ﬁnancial inclusion. A study by [16] discovered that ﬁnancial literacy had a
strong favorable inﬂuence on ﬁnancial inclusion among 400 low-income rural Ugan-
dans. The study also revealed that ﬁnancial literacy enables low-income individuals to
comprehend various banking services and make prudent judgments in order to maxi-
mize their use. [1] and [17] also discovered a statistically signiﬁcant positive association
between ﬁnancial literacy and all metrics of ﬁnancial inclusion. Similarly, according
to [18], ﬁnancial literacy positively promotes both savings and ﬁnancial inclusion. The
following research model (Fig. 1) and hypothesis (H1) have been created based on past
research.
Fig. 1. Research model
Research Hypothesis (H1): Financial literacy signiﬁcantly enhances ﬁnancial
inclusion.

Effects of Financial Literacy on Financial Inclusion
25
3
Research Methodology
This study follows the positivism philosophy and is cross-sectional. It was conducted
in Gandaki province, Nepal. The total households of 579,942 of Gandaki province [19]
were used as population units and out of which 1000 households were chosen as a sample.
A researcher-administered questionnaire with a ﬁxed set of questions was developed to
collect the necessary data which comprised of a socio-demographic proﬁle along with 48
items (taken from various sources) on a ﬁve-point Likert scale to assess ﬁnancial literacy
and ﬁnancial inclusion across four and three dimensions respectively. Both ﬁnancial
literacy and ﬁnancial inclusion were regarded as higher-order constructs comprising
various dimensions. The higher-order constructs with their dimensions are given in
Table 1. In this study, the researchers followed multistage sampling for the collection of
data.First,threedistrictswereselectedrandomlyoutof11districtsinthestudyarea.Then
the number of households to be taken from each district was calculated proportionately.
Finally, the households from each district were selected using the snowball sampling
(through referrals) and the required data were collected from the chief decision maker of
selected household. The researchers visited the respondents and requested them to ﬁll up
the questionnaire. However, for illiterate respondents, the researchers took an interview
and ﬁlled up the questionnaire. Data were collected for six months starting from April
2022. The researchers used SPSS and Smart-PLS for data analysis. Descriptive statistics
such as frequency distribution in SPSS were used to assess the characteristics of the
respondents, and Partial Least Squares Structural Equation Modelling (PLS-SEM) in
Smart-PLS was applied to quantify the effect of ﬁnancial literacy on ﬁnancial inclusion.
4
Results and Analysis
4.1
Socio-demographic Summary
This includes socio-demographic characteristics such as respondents’ district, gender,
age, marital status, family type, caste, educational status, income per month, and expen-
diture per month. These variables were described through percentage frequency distri-
bution using SPSS. For this study, participants were selected in proportion from three
different districts. More than half (55.7%) of respondents are from the Kaski district. It
is followed by 29% from the Syangja district and 15.3% from the Parbat district. The
majority of households (64.5%) have a male as the primary decision maker, while only
35.5% of households have a female as the primary decision maker. Likewise, most of
the respondents (87.5%) are married, majority of them are 41–50 years old (35.0%), live
in a nuclear family (68.5%), and are Brahmin (53.4%). Similarly, the majority of them
(44.3%) have a secondary education, have income of NRs. 10,001 to Rs. 40,000 per
month (49.2%), and have expenditure of NRs. 20,001 to Rs. 40,000 per month (40.2%).
4.2
Structural Equation Modelling
In this research, PLS-SEM was applied, which involves the measurement model and the
structural model.

26
D. Ranabhat et al.
Table 1. Constructs and their Indicators
Higher order
construct
Lower order construct
Items
Indicators
Financial
literacy
Financial knowledge
FinKw1, FinKw2,
FinKw3, FinKw4,
FinKw5, FinKw6,
FinKw7, FinKw8,
FinKw9
Knowledge about
savings, loans,
mortgages, cost of
ﬁnancial services, ATMs,
type of insurance,
remittance process,
digital ﬁnancial services,
ﬁnancial risk
Financial skill
FinS1, FinS2,
FinS3, FinS4,
FinS5, FinS6,
FinS7
Ability to choose an
appropriate account, ﬁll
up account opening form,
determine cost and
beneﬁts from ﬁnancial
dealing, complete loan
form, compute interest,
ﬁll remittance form, use
ATM
Financial attitude
FinA1, FinA2,
FinA3, FinA4,
FinA5, FinA6
Readiness towards saving
money, ﬁnancial news,
insurance, spending, loan
from banks, using digital
ﬁnancial services
Financial behaviour
FinB1, FinB2,
FinB3, FinB4
Habit of saving money,
less spending, balancing
income and expenses,
maintaining ﬁnancial
records
Financial
inclusion
Accessibility
FinAcc1, FinAcc2,
FinAcc3, FinAcc4,
FinAcc5, FinAcc6,
FinAcc7
Convenient location, ease
of access of employees,
closeness of ATMs,
convenient transaction
time, access of
information, no. of
branches, fair account
opening charges
(continued)
Measurement Model
It applies reliability and validity tests to check the quality of the constructs. The
researchers used a disjoint two-stage method to conﬁrm reliability and validity of both
higher-order constructs as well as their dimensions (lower-order constructs). First, the
reliability and validity of dimensions were assessed and then of higher-order constructs.

Effects of Financial Literacy on Financial Inclusion
27
Table 1. (continued)
Higher order
construct
Lower order construct
Items
Indicators
Availability
FinAva1, FinAva2,
FinAva3, FinAva4,
FinAva5, FinAva6,
FinAva7, FinAva8
Availability of various
saving products, loan
products, bank locker,
ATM services, zero
balance accounts,
mobile/internet banking,
fast service, assistance of
banking staff
Usage
FinUse1, FinUse2,
FinUse3, FinUse4,
FinUse5, FinUse6,
FinUse7
Frequency of deposit,
frequency of withdraw,
frequency of bank visit,
frequency of using bank
loan, payment of utilities,
stock trading using bank,
frequency of using locker
facilities
Initially, the study used all 48 items of all dimensions (lower-order constructs) as
given in Table 1. After deletion of 12 items (FinAcc5, FinAva1, FinAva2, FinAva5,
FinUse2, FinUse4, FinK8, FinK9, FinS1, FinS7, FinA4, and FinA5), the reliability and
validity of all dimensions were realized with 36 items. Then, the researchers checked
the reliability and validity of higher-order constructs. For this, the researcher considered
ﬁnancial knowledge, skill, attitude, and behavior to be latent variables of ﬁnancial lit-
eracy and usage, availability, and accessibility were treated as latent factors of ﬁnancial
inclusion, and further calculation were made using their scores.
Validating Lower Order Constructs
The researchers used Cronbach’s Alpha and Composite Reliability (CR) to determine
reliability. Similarly, the construct validity includes convergent and discriminant validity.
The researchers used Average Variance Extracted (AVE) for establishing convergent
validity, and Heterotrait-Monotrait Ratio (HTMT), and Fornell and Larcker criteria for
discriminant validity.
Reliability and Convergent Validity
The result of reliability test and AVE of lower-order constructs is presented in Table 2. All
the values Cronbach’s alpha and CR tests (except Cronbach Alpha of ﬁnancial attitude)
are higher than the required minimum value of 0.70 [20] and the Cronbach Alpha of
ﬁnancial attitude is also more than 0.60, which is acceptable [21]. Hence, reliability
is conﬁrmed. Similarly, convergent validity is established when a latent unobserved
construct has an AVE value of 0.50 or more [22]. The authors also mentioned that an
AVE value of more than 0.40 is acceptable if the composite reliability of that construct is
above 0.70. In the current study, all constructs (except ﬁnancial attitude has AVE close to

28
D. Ranabhat et al.
0.50) are having an AVE value greater than the required limit of 0.50. Hence, convergent
validity is established for all the constructs.
Table 2. Reliability tests and AVE
Construct
Cronbach’s alpha
CR
AVE
Access
0.863
0.898
0.594
Availability
0.833
0.882
0.601
Usage
0.763
0.84
0.513
Financial knowledge
0.888
0.912
0.597
Financial skill
0.881
0.913
0.678
Financial attitude
0.642
0.781
0.473
Financial behavior
0.754
0.837
0.563
Fornell and Larcker Criteria for Discriminant Validity
Table 3 illustrates the square root of AVE for each construct (italicized diagonal values)
and their corresponding correlation values. All of the square root of AVE values are
higher than their respective correlation values, indicating strong evidence in favor of
discriminant validity [22].
Table 3. Fornell and Larcker criteria for discriminant validity
Acc
Avail
FA
FB
FK
FS
Use
Access (Acc)
0.771
Availability (Avail)
0.505
0.775
Fin attitude (FA)
0.315
0.418
0.688
Fin behavior (FB)
0.267
0.308
0.531
0.751
Fin knowledge (FK)
0.327
0.304
0.424
0.378
0.773
Fin skill (FS)
0.309
0.335
0.442
0.331
0.71
0.824
Usage (Use)
0.373
0.227
0.369
0.343
0.474
0.443
0.716
Discriminant Validity - Heterotrait-Monotrait Ratio (HTMT) Ratio
Another broadly used technique to establish discriminant validity is HTMT. It measures
the average correlation of indicators between the constructs. As shown in Table 4, all
values are lower than the maximum threshold of 0.85, which conﬁrms the existence of
discriminant validity [23].

Effects of Financial Literacy on Financial Inclusion
29
Table 4. Discriminant validity-HTMT ratio
Acc
Avail
FA
FB
FK
FS
Use
Access (Acc)
Availability (Avail)
0.586
Fin attitude (FA)
0.395
0.545
Fin behavior (FB)
0.303
0.381
0.772
Fin knowledge (FK)
0.37
0.348
0.515
0.425
Fin skill (FS)
0.352
0.386
0.536
0.363
0.802
Usage (Use)
0.463
0.285
0.484
0.388
0.57
0.536
Higher Order Construct Validation
After validating the lower-order construct, the validity and reliability of higher-order
construct were examined. Table 5 shows that reliability was established using both
Cronbach’s Alpha and CR. The Cronbach’s Alpha and CR values were found to be higher
than 0.60 and 0.70, respectively, indicating that reliability was successfully established.
Furthermore, an AVE greater than 0.50 indicates convergent validity (See Table 5).
Likewise, Table 6 displays the outcomes of Fornell and Larcker criteria, indicating
that the square root of AVE of ﬁnancial inclusion and ﬁnancial literacy (italicized diag-
onal values) is greater than their correlation. Moreover, Table 7 shows that the HTMT
value is less than 0.85. Hence, it conﬁrms the discriminant validity of ﬁnancial literacy
and ﬁnancial inclusion.
Table 5. Reliability tests and AVE—higher order constructs
Cronbach’s alpha
CR
AVE
Financial inclusion
0.636
0.803
0.576
Financial literacy
0.779
0.858
0.603
Table 6. Discriminant validity of higher order constructs—fornell and larcker criteria
Financial inclusion
Financial literacy
Financial inclusion
0.759
Financial literacy
0.608
0.777
Structural Model
Path analysis is used to investigate how ﬁnancial literacy affects ﬁnancial inclusion when
the constructs’ validity and reliability are established. The outcomes of the path analysis
are given in Table 8.

30
D. Ranabhat et al.
Table 7. Discriminant validity of higher order constructs—HTMT method
Financial inclusion
Financial inclusion
Financial literacy
0.844
Table 8. Result of path analysis
Association
Path coeff. (β)
T-stat
P-value
R2
Financial literacy →Financial inclusion
0.608
27.746
0
0.369
Table 8 reveals that ﬁnancial literacy has statistically signiﬁcant positive impact on
ﬁnancial inclusion (beta = 0.608, P-value < 0.001). And the coefﬁcient of determination
(R2) value is 0.369, which shows a moderate effect [20] in the established relationship.
4.3
Discussion
This study was conducted to measure the effect of ﬁnancial literacy on ﬁnancial inclu-
sion in Gandaki province, Nepal using PLS-SEM to quantify the effect. The measure-
ment model in PLS-SEM found the constructs are reliable and valid and appropriate
for conducting structural model. The ﬁnding shows that ﬁnancial inclusion is signiﬁ-
cantly enhanced by ﬁnancial literacy. The result is similar to those of [1, 16–18], indi-
cating that ﬁnancial education helps to increase ﬁnancial inclusion. Financial literacy
programs conducted by different organization enables individuals to understand vari-
ous ﬁnancial products and services and helps them to choose appropriate ﬁnancial ser-
vices. It also motivates individuals to use various ﬁnancial services for better economic
life. Thereby, ﬁnancial literacy programs enhance ﬁnancial inclusion. Thus, ﬁnancial
inclusion is highly inﬂuenced by ﬁnancial literacy.
5
Conclusion
In developing nations like Nepal, where many people remain ﬁnancially excluded, ﬁnan-
cial inclusion has emerged as a key problem. Various policies and strategies, including
ﬁnancial literacy programs, have been devised and implemented to improve ﬁnancial
inclusion. This study discovered that ﬁnancial literacy is a crucial factor in promoting
ﬁnancial inclusion. This study concludes that ﬁnancial inclusion in Gandaki province can
be enhanced by increasing ﬁnancial literacy. Financial awareness programs raise under-
standing of different ﬁnancial products and their signiﬁcance and inspires individual to
use them which promotes ﬁnancial inclusion. The research’s ﬁndings complement the
ﬁnancial inclusion theory based on ﬁnancial literacy and earlier literatures that describe
the signiﬁcance of ﬁnancial literacy on ﬁnancial inclusion. This research advises to the
policymakers to implement ﬁnancial literacy programs for individuals in order to expand
ﬁnancial inclusion.

Effects of Financial Literacy on Financial Inclusion
31
6
Research Limitations and Opportunities for Future Research
In this study, Gandaki province was taken as a study area. In Nepal, due to diverse
geographic areas and unequal access to ﬁnancial services, the ﬁnancial literacy level as
well as ﬁnancial inclusion is not same in all provinces. So, the results may not apply in
other provinces of Nepal. In the future, a comparative study of different provinces can
be conducted. Likewise, an experimental and comparative study can be conducted by
providing ﬁnancial literacy to one group and controlling another group and making a
comparative study between two groups.
References
1. Grohmann, A., Klühs, T., Menkhoff, L.: Does ﬁnancial literacy improve ﬁnancial inclusion?
cross country evidence. World Dev. 111(95), 84–96 (2018). https://doi.org/10.1016/j.wor
lddev.2018.06.020
2. Ramachandran R.: Financial literacy-the demand side of ﬁnancial inclusion. In: 26th SKOCH
Summit 2011 Swabhiman-Inclusive Growth and Beyond 2 nd & 3 rd June, Mumbai India,
no. ii, pp. 1–16 (2011)
3. Stolper, O.A., Walter, A.: Financial literacy, ﬁnancial advice, and ﬁnancial behavior. J. Bus.
Econ. 87(5), 581–643 (2017). https://doi.org/10.1007/s11573-017-0853-9
4. Hung, A.A., Parker, A.M., Yoong, J.K.: Deﬁning and measuring ﬁnancial literacy
(2009). [Online]. Available: https://www.rand.org/content/dam/rand/pubs/working_papers/
2009/RAND_WR708.pdf
5. Ozili, P.K.: Munich personal RePEc archive theories of ﬁnancial inclusion theories of ﬁnancial
inclusion. Munich Pers. RePEc Arch. (104257) (2020)
6. Demirgüç-kunt, A., Klapper, L., Singer, D., Ansar, S., Hess, J.: The global ﬁndex database.
The World Bank, Washington (2017)
7. Nepal Rastra Bank.: Financial Inclusion Roadmap 2017–2022 (2020)
8. OCDE.: PISA 2012 Results: Students and Money: Financial Literacy Skills for the 21st
Century, vol. VI (2014)
9. Nepal Rastra Bank. Financial literacy framework (2020)
10. Sarma, M.:Index of Financial Inclusion (2008)
11. Yadav, P., Sharma, A.K.: Financial inclusion in India: an application of TOPSIS. Humanomics
32(3), 328–351 (2016). https://doi.org/10.1108/H-09-2015-0061
12. Hanivan, H., Nasrudin, N.: A ﬁnancial inclusion index for Indonesia. Bul. Ekon. Monet. dan
Perbank. 22(3), 351–366 (2019). https://doi.org/10.21098/bemp.v22i3.1056
13. Ranabhat, D. Verma, N., Sapkota, P., Chhetri, S.D.: Impact of ﬁnancial inclusion on social
and economic well-being of households: a case of Kaski District, Nepal. In: BT—Intelligent
Computing & Optimization, pp. 1027–1037 (2023)
14. Vishvesh, R., Venkatraman, B.: Financial inclusion through ﬁnancial education. Int. J. Appl.
Bus. Econ. Res. 13(1), 19–35 (2015)
15. Verma, S., Oum Kumari, R.: Role of ﬁnancial literacy in achieving ﬁnancial inclusion. Int. J.
Appl. Bus. Econ. Res. 14(6), 4607–4613 (2016)
16. Bongomin, G.O.C., Munene, J.C., Ntayi, J.M., Malinga, C.A.: Nexus between ﬁnancial lit-
eracy and ﬁnancial inclusion: examining the moderating role of cognition from a developing
country perspective. Int. J. Bank Mark. 36(7), 1190–1212 (2018). https://doi.org/10.1108/
IJBM-08-2017-0175
17. Ghosh, S.: Financial literacy and ﬁnancial inclusion unbundling the nexus. Econ. Polit. Wkly.
54(13), 75–82 (2019)

32
D. Ranabhat et al.
18. Morgan, P.J., Long, T.Q.: Financial literacy, ﬁnancial inclusion, and savings behavior in Laos.
J. Asian Econ. 68(7) (2020). https://doi.org/10.1016/j.asieco.2020.101197
19. CBS Nepal.: Nepal Census 2021, pp. 1–96 (2022) [Online]. Available: https://census
nepal.cbs.gov.np/Home/Details?tpid=5&dcid=3479c092-7749-4ba6-9369-45486cd67f30&
tfsid=17
20. Hair, J.F., Ringle, C.M., Sarstedt, M.: PLS-SEM: Indeed a silver bullet. J. Mark. Theory Pract.
19(2), 139–151 (2011). https://doi.org/10.2753/MTP
21. Ursachi, G., Horodnic, I.A., Zait, A.: How reliable are measurement scales? external factors
with indirect inﬂuence on reliability estimators. Procedia Econ. Financ. 20(15), 679–686
(2015). https://doi.org/10.1016/s2212-5671(15)00123-9
22. Fornell, C., Larcker, D.F.: Evaluating structural equation models with unobservable variables
and measurement error. J. Mark. Res. 18(1), 39 (1981). https://doi.org/10.2307/3151312
23. Henseler, J., Ringle, C.M., Sarstedt, M.: A new criterion for assessing discriminant validity
in variance-based structural equation modeling. J. Acad. Mark. Sci. 43(1), 115–135 (2014).
https://doi.org/10.1007/s11747-014-0403-8

Business and Information Technology Strategy
Impact on Organizational Performance: A Case
Study of Nepal Telecom
Sudip Poudel1, Neesha Rajkarnikar1, Deepanjal Shrestha1,2(B)
,
Deepmala Shrestha1, and Seung Ryul Jeong3
1 School of Business, Pokhara University, Pokhara, Nepal
spredcloud29@gmail.com, neesha_rk@hotmail.com,
deepanjal@hotmail.com, deepmala@pusob.edu.np
2 Nanjing University of Aeronautics and Astronautics, Nanjing, China
3 Graduate School of Business IT, Kookmin University, Seoul, South Korea
srjeong@kookmin.ac.kr
Abstract. An effective business and information technology (IT) strategy has a
signiﬁcant impact on organizational performance. This study examines the impact
of Nepal Telecom’s business and IT strategy on organizational performance. The
study is based on primary data and employs descriptive and analytical research
design to examine the impact of independent variables (business strategies, IT
strategies, individual’s decision-making capacity, IT implementations, technolog-
ical deployment and budget allocations) on the dependent variable (organiza-
tional performance of Nepal telecommunication. The respondents are employees
of Nepal Telecommunications working in technical as well as managerial posi-
tions. The ﬁndings of this study are signiﬁcant to various telecom companies to
understand and identify business and IT strategies that are required to take deci-
sions for improving organizational performance. The research provides informa-
tion on the latest technologies and approaches that the company can use to improve
its operations and service delivery and increase customer satisfaction. This study
helps to understand how appropriate business and IT strategies can help to gain
a competitive advantage and drive the performance of a business. This research
can also help the company to make wise decisions on investments and resource
optimization for better growth and better organizational performance.
Keywords: IT strategy · Business strategy · Telecommunications · Services ·
Organizational performance · Nepal telecom
1
Introduction
The telecommunications sector is rapidly evolving day by day with the help of the
latest technological developments. The technological developments in the world require
businesses to use information systems more than they used to. Establishing relationships
between the various resources is important to meet the organizational goals and attain
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 33–44, 2024.
https://doi.org/10.1007/978-3-031-50158-6_4

34
S. Poudel et al.
the fully desired organizational performance. According to Kaplan, for a competitive
advantage to be created in an organization, its business and information technology
strategies and processes must be aligned well. Although the strategic relationship has a
history of being rated as a top issue facing business and IT executives, no single solution
has been established to address this issue [1]. Organizations today operate in dynamic and
constantlychangingenvironments,requiringcontinuousassessmentofstrategicrelations
to ensure that they operate at an optimal level. A key success factor for a successful
company to meet its performance requirements in a dynamic environment is effective
and efﬁcient information technology supporting business strategies and processes [2].
CombiningthebusinessstrategywiththeInformationsystemsstrategyhasbecomeavital
concern in the planning process of organizations [3]. Today, Information technology (IT)
is a very important part of an organization’s daily operations and business strategy. IT is
viewed as a technological resource that would help organizations does better things [4].
The consequences of not adopting effective strategic planning in an organization could
lead to the inability of the organization to deal with the rapidly changing environment.
When the pace of doing business is slow, managers could operate on the assumption
that the future, would be substantially like the past and thus could establish plans/goals
simply by extrapolating from past experiences. Unfortunately, today events are moving
too rapidly for experience to be a reliable guide to a manager. Another effect of not
adopting an effective strategy in an organization is the inability to have clearly deﬁned
objectives and methods for achieving these objectives. Thus, such organizations may not
have a clear purpose and direction. It is important to study this area so that business and
information technology strategies must align in such a way that organizations perform
better in all aspects.
1.1
History of Nepal Telecom
The start of telecommunication service in Nepal can be traced back to 1973, with the
establishment of Mohan Akashwani in B.S. 2005 [5]. The Telecommunication Depart-
ment was established in 2012–2017 (BS), and the Telecommunications Development
Board was converted into Telecommunications Development Board in 2026 (BS). Nepal
Telecommunications Corporation was established in 2032 (BS) and transformed into
Nepal Doorsanchar Company Limited (NDCL) in 2061. Nepal Telecom has adopted
and implemented changes in its business model in response to rapid advancements in
technology, high expectations of customers, and ever-changing market situations. Today,
Nepal Telecom has no monopoly on the market due to competition from other telecom
players such as Ncell, UTL, Smart Telecom, Nepal Satellite Telecom Company Ltd,
and around 50 Internet Service Providers (ISP). After Ncell was acquired by Telia Son-
era group and Celcom Axiata group, the state-owned company has been facing ﬁerce
completion and many challenges and threats in the market in voice telephony as well
as internet service [6]. The current market share of around 92% belongs to Nepal Tele-
com and Ncell, making it a duopoly irrespective of the presence of the other 3 telecom
operators.

Business and Information Technology Strategy Impact
35
1.2
Problem Statement
IT is an important part of an organization’s daily operations and business strategy. It is
observed over the past three decades that there has been a rapid change in technology.
Not adopting an effective IT-based strategy in an organization could lead to the inability
to deal with the rapidly changing environment. Further, there is a proper need for IT strat-
egy alignment with the business strategies so that business objectives can be achieved.
Thus, it is important to study this area and ﬁll in the gap by deﬁning the variables that
tend to inﬂuence IT strategies and business strategies in an organizational setup. This
study describes Nepal Telecom’s business and IT strategy and its impact on organiza-
tional performance. The study has considered multiple types of factors that inﬂuence
organizational performance that is decisive on various levels of organizational perfor-
mance. These factors include business strategies, IT strategies, technological deploy-
ment, IT implementation, the impact of IT on individuals’ decision-making capacity,
budget allocation, and organizational performance.
1.3
Hypothesis
• H1: There is a signiﬁcant impact of business strategies on the organizational
performance of Nepal telecom.
• H2: There is a signiﬁcant impact of IT strategies on the organizational performance
of Nepal telecom.
• H3: There is a signiﬁcant impact of individual decision-making capacity on the
organizational performance of Nepal telecom.
• H4: There is a signiﬁcant impact of IT implementations on the organizational
performance of Nepal telecom.
• H5: There is a signiﬁcant impact of technology deployment on the organizational
performance of Nepal telecom.
• H6: There is a signiﬁcant impact of budget allocations on the organizational
performance of Nepal telecom.
• H7: Educational level of employees plays a moderating role in the research model
concerning the impact of business and IT strategies on the organizational performance
of Nepal telecom.
• H8: Work experience of employees plays a moderating role in the research model
concerning the impact of business and IT strategies on the organizational performance
of Nepal Telecom
1.4
Signiﬁcance of the Study
In Nepal, there is a lack of prior research on how business and information technology
strategies impact organizational performance. NTC is the biggest telecom company, and
taking it as a case study will help the company be more competitive and demanding. This
research helps NTC in identifying business and IT strategies to develop more effective
and efﬁcient decisions, as well as providing information on the latest technologies and
approaches to improve operations and service delivery. The research is designed to help
the company drive its performance and anticipate and respond to changes in the market
and industry.

36
S. Poudel et al.
2
Literature Review
The literature for both the global context and local context was considered to trace the
earlier studies and ﬁnd the gap in the literature. It was observed that a good number
of studies are carried out at the global level in this sector and many aspects related to
the basic theme are studied. However, there is a huge gap in the study of this aspect in
the Nepalese context. It was required that the gap in this area in the context of Nepal
must be brought to light which has helped this study to come up. Further, the growth
of telecommunication and IT-based industries in Nepal is witnessing a sharp rise with
many technology-based solutions and services offered by them [8]. The need to align the
IT and business strategies is important to guarantee good organizational performance
and meet the very objectives of the business (Table 1).
3
Research Methodology
3.1
Conceptual Framework
The conceptual framework deﬁnes the independent variables, dependent variables and
moderating variables of the study as shown in ﬁgure the business strategy, IT strategy,
individual decision-making capacity, IT implementation, technological deployment and
budget allocation are the independent variables that are studied for dependent variable
organizational performance. The study also considers educational qualiﬁcation and work
experience as moderating variables (Fig. 1).
3.2
Data Collection, Analysis Method and Reliability
The study is based on primary data and some secondary data to understand the underly-
ing concepts and build a strong foundation for the study. Primary data is used to test the
hypothesis and test the evidence of the claim. A survey questionnaire method is used
for 40 respondents and the study has two parts. The ﬁrst part includes the personal and
general information of the respondents. The second part investigated the factors affect-
ing the various independent variables that have impact on the overall organizational
performance. The respondents of the study include business IT and business managers
working in Nepal Telecom, head ofﬁce in Pokhara and other cities of Nepal. Quantitative
data analysis is used in the study to meet the speciﬁed research objectives. Both descrip-
tive and inferential statistical methods of data analysis are used. Descriptive statistics
is used to compute mean, standard deviation, frequency distributions of the data, which
is presented in graphs, charts, and cross-tabulations etc. Also, different inferential tests
are performed for the analysis like ANOVA test, T-test and correlation. The reliability
test is performed using Cronbach’s α where all the variables are found to be greater than
0.7, indicating that the measurement data is reliable. Further, the overall reliability of
the scale in the study is 0.758, conferring to the overall reliability of the questionnaire.

Business and Information Technology Strategy Impact
37
Table 1. Literature review with key theories and ﬁndings
Ref
Year, Author
Findings
[9]
2021, Dairo M, Adekola J,
Apostolopoulos C, Tsaramirsis G
Contribute to the practical understanding of
strategic alignment and demonstrates the
applicability and robustness of the Strategic
Alignment Model (SAM). It also analyzes
potential opportunities and risks associated
with the IT strategy alignment with business
[10]
2003, Grover, V., & Teng, J. T. C
The study shows the functional integration as
a horizontal relationship, and the extension
of the concept of strategic ﬁt to the functional
domain of the strategic alignment framework
[11]
2017, Al-hamadi
This study discusses emerging trends in the
telecommunications industry, including the
growth of mobile data, the emergence of 5G
technology, and the increasing use of
artiﬁcial intelligence in the industry. It also
explores the potential impact of these trends
on the industry and its future outlook
[12]
2020, D. Shrestha, T. Wenan, SR Jeong
Understanding the attitude of tourist and
tourism SMEs for ICT and digital
technologies in the Nepalese context. The
aspects are studied under TAM model
[13]
2003, Mintzberg
The study brings out the strategy process,
concepts, contexts and cases in business. It
highlights the various authors and managers
and their use of concepts differently that
might include other related terms such as
goals and objectives as part of the strategy,
whereas others make ﬁrm distinctions
between them
[14]
2020, Deepanjal Shrestha, Tan Wenan,
Adesh Khadka, and Seung Ryul Jeong
This study has examined the ICT and digital
technology implementation at the Nepal
government operational level. It provides
insight on how the government is shaping
policies for ICT and digital technologies. It
also provides the different technologies in
existence in Nepal and how these services
are used in the current context
(continued)

38
S. Poudel et al.
Table 1. (continued)
Ref
Year, Author
Findings
[15]
1999, Wetherbe J, E. Turban, and E.
Mclean
The study examines Information Technology
for management and how making
connections for strategic advantages are
important in a technology-oriented business
environment
[16]
2017, Sibanda, Mabutho and
Ramrathan, Durrel
The study portrays the regulatory challenges
in the telecommunications Industry. The
study has built a comprehensive landscape of
regulatory policies and how they impose
challenges in its implementations
Independent Variables
Business Strategy
IT Strategy
Individuals Decision Making Capacity
IT Implementations
Technological Deployment
Budget Allocations
Dependent Variable
Organizational Performance
Moderating variables
Educational Qualification
Work Experience
Fig. 1. Conceptual framework of the study
4
Data Analysis and Results
4.1
Respondents’ Proﬁle
The demography data of the respondents is depicted in Table 2 and work experience is
shown in Table 3 followed by qualiﬁcation in Table 4 and employee proﬁle in Table 5.
Table 2. Socio-demographic data
Gender
Frequency
Percent
Male
15
37.5
Female
25
62.5
Total
40
100

Business and Information Technology Strategy Impact
39
Table 3. Distribution by work experience in NTC
Work experience period
Frequency
Percent
Less than 2 years
3
7.5
2–5 years
16
40.0
5–10 years
11
27.5
Above 10 years
10
25.0
Total
40
100.0
Table 4. Qualiﬁcation of employees
Educational level
Frequency
Percent
Plus 2/diploma
6
15.0
Bachelors
19
47.5
Masters and above
15
37.5
Total
40
100.0
Table 5. Employee job proﬁle
Work departments
Frequency
Percent
Ofﬁce of COO (Chief Operating Ofﬁcer)
9
22.5
Ofﬁce of CTO (Chief Technical Ofﬁcer)
6
15.0
Ofﬁce of CCO (Chief Commercial Ofﬁcer)
4
10.0
Ofﬁce of CFO (Chief Finance Ofﬁcer)
6
15.0
Ofﬁce of CHRO (Chief Human Resource Ofﬁcer)
4
10.0
Wireline and CSD(Customer service department)
5
12.5
Wireless service directorate
4
10.0
Others
2
5.0
Total
40
100.0
4.2
Analysis of Independent Variables
The data analysis depicts that the mean value of business strategy dimensions is more
than 3 with a 0.918 SD, which indicates that overall respondents are pleased with the
performance offered by Nepal Telecom. Similarly, the IT strategy dimensions also show
a mean value of 3.48 with a 0.829 SD, which again conﬁrms that there is a positive
impact of IT strategy on the overall organizational performance. The mean value of 3.75
with a SD of 0.825 shows that the impact of IT on individual decision-making capacity
has an inﬂuential role on overall organizational performance. Further, the mean value of

40
S. Poudel et al.
3.51 with an SD of 0.859 shows that technological deployment dimensions have a great
contributiontoorganizationalperformance.Inaddition,thebudgetallocationdimensions
also depict the mean value to be 3.25 with SD of 0.927, indicating satisfaction with the
budget allocation and the impact of the budget on organizational performance.
4.3
One-Way ANOVA
Table 6. Impact of business and information technology strategies on the organizational perfor-
mance on the basis of the educational level of respondents.
Educational level
Mean
Std. deviation
Minimum
Maximum
Fvalue
Pvalue
Plus 2/Diploma
3.160
.907
1.7
4.25
0.504
0.608
Bachelors
3.293
.743
2.0
4.25
Masters
3.486
.677
1.7
4.25
Source: SPSS Output, 2022, data survey and interview
The impact of business and information technology was studied with respect to the
education level, and a one-way ANOVA test was carried out on the collected data. It
was observed that the educational level of respondents’ mean value was in the range of
1.7–4.25, with an average of 3.160 and a standard deviation (SD) of 0.907. Similarly,
the bachelor’s degree holders had a value in the range of 2.0–4.25, with an average of
3.293 and a standard deviation of 0.743. Further, respondents with a master’s degree
had a value ranging from 1.7 to 4.25, with an average of 3.486 and an SD of 0.677.
Since p-value = 0.608 > level of signiﬁcance (α) = 0.05, hypothesis H7 was rejected
(Table 5). The same results can be drawn for Table 6, and it is seen that hypothesis 8 is
also rejected (Table 7).
Table 7. Impact of business and information technology strategies on organizational performance
based on years of service.
Years of working
Mean
Std. deviation
Minimum
Maximum
F value
P value
Less than 2 years
3.541
.921
2.50
4.25
0.109
0.954
2–5 years
3.294
.786
1.75
4.25
5–10 years
3.392
.812
1.71
4.25
Above 10 years
3.316
.603
2.25
4.00
Source: SPSS Output, 2022, data survey and interview
4.4
Inferential Statistical Analysis
Correlation Analysis

Business and Information Technology Strategy Impact
41
Table 8. Correlation between business and technology strategies dimensional factors and
Organizational performance of Nepal telecom
OP
BS
ITS
IDM
ITM
TD
BA
OP
1
.340*
.410**
−.085
.209
−.188
.253
.032
.009
.604
.195
.245
.116
BS
1
.170
−.076
.177
.165
−.338
.293
.640
.274
.310
.033
ITS
1
.012
.250
.254
.065
.941
.120
.113
.688
IDM
1
.265
.000
−.087
.098
1.000
.592
ITM
1
.051
-.253
.752
.115
TD
1
−.197
.224
BA
1
Source: SPSS Output, 2022, data survey and interview
Table 8 shows the correlation matrix between the independent variable and depen-
dent variables. The level of signiﬁcance (p = 0.032, p < 0.05) indicates that there is a
signiﬁcant impact of business strategy on organizational performance. The level of sig-
niﬁcance (p = 0.009 p < 0.05) indicates that there is a signiﬁcant impact of IT strategies
on organizational performance. The level of signiﬁcance (p = 0.604, p > 0.05) indicates
that there is no signiﬁcant impact of an individual’s decision-making capacity on orga-
nizational performance inferring that there is no correlation between them. Further, the
level of signiﬁcance (p = 0.195, p > 0.05) indicates that there is no signiﬁcant impact
of IT on organizational performance. The Pearson coefﬁcient value of 0.209 indicates
that the two variables have a weak correlation and are positive. The level of signiﬁcance
(p = 0.245, p > 0.05) indicates technological deployment has no that there is no signif-
icant impact on organizational performance. The Pearson coefﬁcient value of −0.188
indicates that these two variables are not correlated and have negative correlation. The
level of signiﬁcance (p = 0.116, p > 0.05) indicates that there is no signiﬁcant impact
of budget allocations on organizational performance. ANOVA analysis was conducted
to determine the signiﬁcance of the regression model. It can be seen that the p-value of
the model is 0.001 which is less than the signiﬁcance level of the study (i.e. p < 0.05)
so, the hypothesis was accepted. This means there is a signiﬁcant impact between the
dependent variables and independent variables as shown in Table 9. In this study, the
regression model was derived for reaching the point of conclusion as follows:
OP = β0 + β1BS + β2ITS + β3IDM + β4ITM + β5TD + β6BA
(1)
where,

42
S. Poudel et al.
OP = Organizational performance
BS = Business Strategy
ITS = IT strategy
IDM = Individuals decision making
ITM = IT implementations
TD = Technological deployment
BA = Budget allocation
Table 9. ANOVA for multiple regression
Model
Sum of squares
df
Mean square
F
Sig
1
Regression
9.787
6
1.63
4.781
.001b
Residual
11.260
33
.341
Total
21.048
39
Source: SPSS Output, 2022, data survey and interview
From Table 10, we can determine the regression model as follows:
OP = −0.257 + 0.564BS + 0.415ITS −0.82IDM + 0.167ITM −0.321TD + 0.337BA
(2)
The hypothesis testing shows that H1 and H2 were accepted with 0.006 and 0.019 val-
ues, whereas H3 and H4 were rejected with 0.589 and 0.235 values. Further, hypotheses
H5 and H6 were accepted with values 0.044 and 0.021.
Table 10. Regression coefﬁcient analysis of variables
Coefﬁcient analysis in Table
Beta
T-value
P value
Alternative hypothesis
(Constant)
−.257
−.203
0.84
Business strategy
.564
2.926
.006
Accepted
IT Strategy
.415
2.470
.019
Accepted
Individual decision making
−0.82
−0.545
.589
Rejected
IT implementations
0.167
1.211
.235
Rejected
Technological deployment
−0.321
−2.091
.044
Accepted
Budget allocations
0.337
2.427
.021
Accepted
R–square
.465
F
4.781
P value
0.001
Source: SPSS Output, 2022, data survey and interview

Business and Information Technology Strategy Impact
43
5
Conclusion
The study on the impact of business and information technology strategies on organi-
zational performance at Nepal Telecom concluded that business and IT strategies had
a signiﬁcant inﬂuence on organizational performance. The study found that individual
decision-making capacity and technological deployment did not have a signiﬁcant inﬂu-
ence on organizational performance. Additionally, the moderating effect of educational
level and work experience among different respondents of different departments and age
groups and responsibilities was found to have no signiﬁcant effect. The study concluded
that a strong business strategy is an important component and must align with the com-
pany’s overall goals and objectives to increase organizational performance. Further, IT
strategies are essential for the success of a telecom company, as they improve efﬁciency,
reduce costs, and increase customer satisfaction. The IT strategies were also important
to gain a competitive advantage in business and introduce new products and services.
References
1. Farida, I., Setiawan, D.: Business strategies and competitive advantage: the role of perfor-
mance and innovation. J. Open Innov. Technol. Mark. Complex. 8, 163 (2022). https://doi.
org/10.3390/joitmc8030163
2. Klein, J.T., Sorra, J.S.: The challenge of innovation implementation. Acad. Manag. Rev. 21(4),
1055–1080 (1996)
3. Hofstede, G.: A study of business and IT alignment in the 21st century. J. Manag. Inf. Syst.
20(2), 193–212 (2003)
4. Jonas, H., Thomas, K.: The business model concept: theoretical underpinnings and empirical
illustrations. Eur. J. Inf. Syst. 12(1), 49–59 (2010). https://doi.org/10.1057/palgrave.ejis.300
0446
5. Nisha, M.: Financial analysis of public comany and its contribution to Nepalese economy: a
case study of Nepal Telecom, Faculty of Management Tribhuvan University, (2020). https://
elibrary.tucl.edu.np/bitstream/123456789/9769/2/Proposal%20-NTC.pdf
6. Shrestha, D., Devkota, B., Jeong, S.R.: Challenges and factors affecting E-governance prac-
tices in Nepal. In: 9th International conference on software, knowledge, information manage-
ment and applications (SKIMA), vol. 9. Kathmandu, Nepal 15–17 December (2015). https://
doi.org/10.1109/SKIMA.2015.7399981
7. Masume, P., Neda, A., Saeedeh, R.H.: The impact of IT resources and strategic alignment
on organizational performance: the moderating role of environmental uncertainty. Digi-
tal Business 2(2), 100026 ISSN 2666–9544 (2022). https://doi.org/10.1016/j.digbus.2022.
100026
8. Wenan, T., Shrestha, D., Shrestha, D., Rajkarnikar, N., Jeong, S.R.: The role of emerging
technologies in digital tourism business ecosystem model for Nepal. In: Vasant, P., Weber,
G.W., Marmolejo-Saucedo, J.A., Munapo, E., Thomas, J.J. (eds.) Intelligent Computing &
Optimization. ICO 2022. Lecture Notes in Networks and Systems, vol 569. Springer, Cham.
https://doi.org/10.1007/978-3-031-19958-5_105
9. Dairo, M., Adekola, J., Apostolopoulos, C., Tsaramirsis, G.: Benchmarking strategic align-
ment of business and IT strategies: opportunities, risks, challenges and solutions. Int. J. Inf.
Technol. 13(6), 2191–2197 (2021). https://doi.org/10.1007/s41870-021-00815-7
10. Grover, V., Teng, J.T.C.: Business/IT alignment: Achieving a durable partnership. J. Manag.
Inf. Syst. 20(1), 191–206 (2003)

44
S. Poudel et al.
11. Al-Hamadi, H.M.: Emerging trends in the telecommunications industry: a review. J. Netw.
Comput. Appl. 88, 1–10 (2017)
12. Wenan,T.,Shrestha,D.,Shrestha,D.,Jeong,S.R.:AttitudeofinternationaltouristtowardsICT
and digital services in tourism industry of Nepal. In: Association for Computing Machinery
(AISS 2019), pp. 1–7. USA, Article 10 (2019). https://doi.org/10.1145/3373477.3373487
13. Mintzberg, H., Lampel, J., Quinn, J.B., Ghoshal, S.: The strategy process: concepts, contexts,
cases. Fourth Edition, Pearson Education Limited (2003)
14. Shrestha, D., Wenan, T., Khadka, A., Jeong, S.R.: Digital tourism security system for Nepal.
KSII Trans. Internet Inf. Syst. 14(11), 4331–4354 (2020). https://doi.org/10.3837/tiis.2020.
11.005
15. Wetherbe, J., Turban, E., Mclean, E.: Information Technology for management: Making
connections for strategic advantages, 2nd edn. NY, John Wiley and Sons Inc, New York
(1999)
16. Tannenbaum, R.S.: Regulatory challenges in the telecommunications industry: a literature
review. J. Manag. 36(2), 307–327 (2010)

A Systematic Literature Review on Factors
Affecting Rural Tourism
Pradeep Sapkota1(B)
, Kamal Kant Vashisth1
, and Deepesh Ranabhat2
1 Faculty of Management Sciences, Shoolini University, Bajhol, Himachal Pradesh 173229,
India
ursparu061@gmail.com
2 Faculty of Management Studies, Pokhara University, Pokhara-30, Kaski, Nepal
Abstract. Studies related to rural tourism have been done from a different per-
spective, but lack of proper systematic reviews has been observed on factors affect-
ing rural tourism. To overcome the problems, this paper has been conducted. This
investigation reviewed articles related to factors affecting rural tourism found in
the Scopus database from 2001 to 2021. The research subject, purpose, techniques,
and data source of 61 articles from various journals were selected and listed in
a complete table, showing the study topic, purpose, methods, and data source
of articles. This article focuses on general characteristics, citation analysis, and
keyword co-occurrence analysis by using the VOS viewer program. The results
expose the trend and impact of literature, journals, and studied area on the related
topic together. Hence, this study can unfold and boost the knowledge of the related
topic and give an idea of a new research trend.
Keywords: Tourists · Rural tourism · Systematic literature review · Citation
analysis · Bibliometric analysis
1
Introduction
Tourism in rural areas is regarded as an activity that can help to solve rural problems
by forging strong links between tourism and rural life [1]. Tourism in rural areas in the
past was observed only as a monetary compliment for farmers but now it is an important
sector which contribute to generate social beneﬁts as well [2]. Rural tourism “includes a
wide range of leisure activities carried out in rural regions, including community-based
tourism, ecotourism, cultural tourism, adventure tourism, guest farms, backpacking,
horseback riding, homestays, and agritourism [3]. The purpose of visiting these places
is to learn about, experience, and appreciate their unique cultures and natural features
[4]. Rural tourism plays an important role in the economic development of the com-
munity and to spend better living standard. Most of such research mainly focuses on
making rural tourism more successful and sustainable [5]. Market-oriented development
has been proved in numerous studies to be a viable method for rural tourism. Similarly,
stakeholder’s views play important role for the development of rural tourism [6]. Quan-
titative methods are commonly used to identify the key variables and factors inﬂuencing
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 45–55, 2024.
https://doi.org/10.1007/978-3-031-50158-6_5

46
P. Sapkota et al.
stakeholders’ perceptions [7]. However, several research are focused on either the service
provider or the tourist’s perspective which is not enough for the success of rural tourism.
To comprehend the factors inﬂuencing rural tourism development, it is necessary to look
at both the demand and supply sides of the tourism industry.
Over the past two decades, it has been observed that there has been tremendous
growth of academic research in the ﬁeld of rural tourism. Most of the research related
to rural tourism is focused on developed countries. Only a very few studies have been
conducted in developing nations [8, 9]. As interest in the success of rural tourism,
lot of research related to rural tourism have been conducted recently but a core study
related to key success factors of rural tourism is yet to be done through systematic
literature review. Thus, the need for systematic review has emerged. A comprehensive
and systematic understanding of a certain area is very important to expand the knowledge
of a particular topic and gives an idea of new research directions [10]. This study will
help researchers to design new research topics and quickly get used to them. It also
helps on identifying new research problems and topics that has not been investigated by
analyzing the overall mode of the research that has been done to date. It will also give
proper guidance for the researcher who wants to conduct studies on the topic of rural
tourism.
Indeed, [11] has answered this need by conducting a literature review using database
in Web of Science published during 2009–2019. Their literature review is based on
“sustainable rural tourism” key words. They reviewed 76 manuscripts with those key
words. This article reviewed different papers related to factors affecting rural tourism
from the Scopus database from 2001 to 2021 A.D. This study was conducted to answer
the following questions:
1. What are the general characteristics related to rural tourism?
2. What is the pattern of use of selected articles in another research works?
3. What are the key factors affecting rural tourism?
2
Research Methodology
2.1
Search and Information Sources
This study was conducted by reviewing the articles related to factors affecting rural
tourism and homestay searched on the Scopus database. This study consists of the articles
conducted from 2001 to 2021 A.D. to ﬁnd a trend of factors affecting rural tourism and
homestay and to provide the most important information. Different keywords were used
to collect the articles related to the study. Keywords search process was:
TIT −ABS −KEY((Factor ∗OR Determinant∗)
AND (Rural tourism OR homestay))
AND (LIMIT - TO(DOCTYPE, ar))
AND (LIMIT - TO(LANGUAGE, English))
AND (LIMIT - TO(SRCTYPE, j))
From above-mentioned key word 440 articles were found on the date 10th August
2022 in Scopus database. A detailed list of articles was extracted for further analysis.

A Systematic Literature Review on Factors Affecting Rural Tourism
47
2.2
Process of Data Collection and Article Screening
In this study, only peer-reviewed journal articles were considered. Books, papers, con-
ference papers, book chapters, and other editorial materials were excluded to make the
study more systematic and scientiﬁc. Papers published in the English language only were
considered for systematic review. The forms for data extraction have been developed to
identify articles and provide an overview of research on rural tourism. Each article was
then measured in a structured data extraction format by considering the quality of the
study. From a total of 440 articles, ﬁrst-level screening was done by dividing them into
two parts, fundamentally related to factors affecting rural tourism or homestay and not
related to rural tourism and homestay (Fig. 1).
Fig. 1. PRISMA framework
The result shows that 181 articles were relevant to the above-mentioned criteria.
In the next step, the screening of data based on title and abstract was done. Finally,
61 articles covering the desired study as the main issue were selected for systematic
literature review.
3
Results
3.1
General Characteristics
To address question 1, analysis was done regarding the overall characteristics of selected
studies. The analysis covered is a year-wise study in the related subject, journal-wise
publication, methods applied, research perspective, country-wise study, citation by
documents, citation by journals, citation by country, and co-occurrence of keywords.
Year Wise Distribution
The paper related to rural tourism has been steadily increased from 2005 to 2019 but
there was a slight decrease in 2020 that may be due to Corona Pandemic (Fig. 2). In the

48
P. Sapkota et al.
1 
1 
1 
1 
3 
1 
2 
3 
3 
5 
4 
8 
3 
9 
4 
9 
0
2
4
6
8
10
2006200720082009201020112012201320142015201620172018201920202021
Yearwise Distribuon 
Fig. 2. Year-wise distribution of articles
year 2021 we can observe gradual increment in the number of papers. It can be observed
that the interest in rural tourism has been increasing tremendously over time.
Journal-Wise Articles
The systematic literature review was conducted by taking 61 articles published in differ-
ent journals around the globe All the papers are selected from the Scopus database. Arti-
cles are taken from 47 different journals. Sustainability Journal (5 papers) and Tourism
Management (3 papers) lead in the ﬁeld of rural tourism, which is followed by Asian
Academy of Management Journal, Advanced Science Letters, African Journal of Hos-
pitality, Tourism and Leisure, Asia Paciﬁc Journal of Tourism Research, DETUROPE,
Journal of Travel and Tourism Marketing, Journal of Travel Research, Tourism Manage-
ment Perspectives with 2-2 papers from each journal. The remaining journals had one
paper each.
Research Method and Tools Applied
In the study of 61 different articles related to rural tourism, it was found that three dif-
ferent methods of research were applied (Table 1). The ﬁrst method was quantitative
analysis which was used in 44 articles (72.13% of the total study). Among these 44 arti-
cles, structural equation modeling (SEM) was the primary statistics method deployed to
analysis the data collected. It was used in 16 articles. Followed by regression (including
logistic regression) in the second. There were 9 documents which used regression in
analyzing. Following descriptive statistics with or without correlation in 6 documents,
analysis of variance (ANOVA) in 5, conﬁrmatory factor analysis (CFA) in four arti-
cles, Exploratory Factor Analysis (EFA) in 3, canonical correlation in 2 articles, and
importance-performance analysis (IPA) also in 2 articles.
Therewerethreedocumentsthatcombinetwoormoreanalysistools.Twodocuments
used mixed method that combine EFA, CFA, and SEM. One article used three steps
analysis, consists of principal correspondence analysis, cluster analysis, and discriminant
analysis. The papers related to the quantitative analysis deployed questionnaire as data
collection instrument. The second method was qualitative analysis which was used in 12

A Systematic Literature Review on Factors Affecting Rural Tourism
49
articles. Among the paper where qualitative analysis was applied, 9 papers use in-depth
interview in collecting the data. Each of three remaining papers deploy focus group
discussion, participants observation and structured qualitative questionnaire. The third
was the mixed method which was applied in 5 papers.
Table 1. Research method applied
Classiﬁcation
No.of articles
%
Research methods
Quantitative analysis
Survey questionnaire
44
72.13
Qualitative analysis
In-depth interview
9
14.75
Focus group discussion
1
1.64
Participants observation
1
1.64
Qualitative questionnaire
1
1.64
Mixed analysis
Mixed methods research design
5
8.19
Total
61
100
Research Perspective
Regarding the perspective of research out of 61 selected papers, it was found that 44
articles (equal to 72.13%) were written from a tourist perspective, 8 articles of them
were from the stakeholder’s perspective, 5 papers from the resident’s perspective, and
the remaining 4 papers from host’s perspective (Table 2). It shows that most of the articles
were written from tourists’ perspective.
Table 2. A research perspective
Classiﬁcation
No. of articles
%
Research perspective
Tourists
44
72.13
Stakeholders
8
13.11
Residents
5
8.20
Hosts
4
6.55
Grand total
61
100
Region-Wise Classiﬁcation
It was found that the study related to rural tourism was conducted in various parts of the
world. Most of the studies were done in Asia (50.81%) and Europe (40.98%) as shown in

50
P. Sapkota et al.
Table 3. The remaining studies were done in Africa and North America. It was observed
that most of the studies were performed in Malaysia (11 papers) and it was followed by
Spain (8 papers) and China (7 papers).
Table 3. Region-wise classiﬁcation
Region
Number
%
Countries
Asia
31
50.81
Malaysia (11), China (7), South
Korea (1), Taiwan (2), Indonesia (2),
Thailand (3), Cambodia (1), India
(2), Iran (1), Vietnam (1)
Europe
25
40.98
Spain (8), Portugal (4), Serbia (4),
Cyprus (3), Czech Republic (1),
Germany (1), Hungary (1), Turkey
(1)
Romania (1), UK (1)
Africa
3
4.92
Ghana (1), Kenya (1), South Africa
(1)
North America
2
3.28
USA (2)
3.2
Citation and Keyword Co-occurrence Analysis
To provide insight on research question 2, the study was applied to investigate document-
wise, journal-wise, and country-wise citations from the selected 61 papers.
Document-Wise Citations
The number of citations in the papers represents the quality of documents, its inﬂuence
and popularity within a research ﬁeld. Out of 61 articles, one article entitled “Inﬂuence
of the user’s psychological factors on the online purchase intention in rural tourism:
Integrating innovativeness to the UTAUT framework” was found to have the highest
number of citation (cited 340 times). Following in the second order is the article entitled
“Factors for success in rural tourism development” (cited 326 times). The third place is
the article entitled “The role of motivation in visitor satisfaction: Empirical evidence in
rural tourism” with total citation 282 times. Articles which have citation more than 50
times are taken into consideration as shown in Table 4.
Country-Wise Citations
The study was conducted to ﬁnd out country-wise citation as well. Countries with more
than 10 citations for the selected study were considered for the study. It was observed
that that the highest number was in Spain with 1140 citations from nine papers. This is
followed by the USA with 438 citations (4 papers), Portugal 343 citations (6 papers),
Malaysia 296 citations (13 papers), Taiwan 70 citations (2 papers), UK 58 citations
(2 papers), Singapore 49 citations (1 paper), Cyprus 43 citations (3 papers), China 30

A Systematic Literature Review on Factors Affecting Rural Tourism
51
Table 4. Document-wise citations
S. No.
Documents
Citations
1
san martín h. (2012)
340
2
wilson s. (2001)
326
3
devesa m. (2010)
282
4
loureiro s.m.c. (2008)
181
5
albacete-sáez c.a. (2007)
128
6
rasoolimanesh s.m. (2017)
107
7
jamal s.a. (2011)
105
8
murray a. (2015)
96
9
chen l.c. (2013)
69
10
kastenholz e. (2005)
69
11
loureiro s.m.c. (2010)
59
12
albaladejo-pina i.p. (2009)
58
13
greaves n. (2010)
52
citations (6 papers), Germany 22 citations (1 paper), Italy 22 citations (1 paper), Ghana
15 citations (1 paper), Czech Republic 14 citations (1 paper), Iraq 12 citations (1 paper),
Romania 12 citations (1 paper), Russia 10 citations (3 papers), and Serbia 10 citations
(3 papers).
Keyword Co-occurrence
Use of keywords which were related to rural tourism and homestay was also a concern.
The keyword co-occurrence study always investigates the use of the frequent used key-
words in the articles. This analysis was conducted with the help of co-occurrence. VOS
viewer software was used to analyze the co-occurrence of keywords. The objective of
the keyword co-occurrence study was to visualize trends of the important research topics
in this ﬁeld. We concentrated on the author keywords that appear below the abstract.
This technique is used to count the papers in which two keywords comes together.
In Fig. 3 the main keywords and their connection with other keywords (when node
and keyword seem to be larger than results repetition of the keyword in more papers) can
be observed. The line drawn between the nodes depicts the frequent co-occurrence of
keywords in various papers, and the distance between nodes determines the relationship
between keywords (shorter the distance, stronger the relation). In this study of 61 articles,
considering the threshold of a minimum of ten occurrences, ﬁfty-four keywords were
found in eight main clusters. Each cluster is separated by different colors. The most
frequently used keyword used in the study leading the cluster was “rural tourism” which
represents the brown color. Other main keyword leading the clusters were: "satisfaction”
(green), “rural development” (light blue), “tourism development” and “perception” (red),

52
P. Sapkota et al.
Fig. 3. Keyword co-occurrence map.
“hospitality industry” (yellow), “tourism management” (blue), “service quality” (grey),
and “motivation” (purple).
3.3
Key Factors
Examining 61 articles we noticed 53 different factors which affect rural tourism. The
accommodation was the most important in rural tourism which was pointed 15 times
in this study. Destination image (13) and price (13) were the two other most important
factors that affect rural tourism (Table 5). Factors having more than seven frequencies
are only considered for the study.
Table 5. Key factors affecting rural tourism.
S. No.
Factors
Frequency
1
Accommodation
15
2
Destination image
13
3
Price
13
4
Cultural experience
12
5
Security
11
6
Community participation
10
7
Adventure
9
8
Marketing
9
9
Peaceful environment
9
10
Leadership
8
11
Interaction
7

A Systematic Literature Review on Factors Affecting Rural Tourism
53
4
Discussion
The aim of this paper was to examine the different studies related to factors affecting
rural tourism. So, this paper examined the issues theoretically by observing the role of
community, hosts, tourists, and stakeholders. This study analyzed the topic of factors
affecting rural tourism, observing the pace in the literature as well as, multidimensional
area that focus on the views regarding hosts, community, tourists, and other stakeholders.
This research also focuses on the search for prior analysis of rural tourism structure with
an emphasis on earlier studies. It was observed that systematic review was done on var-
ious other aspects of tourism but not on this topic. So, due to the lack of reviewed pieces
of literature and relevance of systematic literature review and bibliometric approach this
study was conducted.
The outcomes of this study investigated the need for an integrated, environmental,
social, and economic aspects of rural tourism from an interdisciplinary approach. The
study reveals that after going through different papers, the questions could be prepared
by giving priority on other and trends dimensions. It is also recommended to focus
on different methodological instruments, and more applicable and empirical study to
analyze more speciﬁc problems related to the topic.
This study analyzed 61 articles from different journals published from January 2001–
2021 through a systematic process. It was observed that the trend of studies related to
rural tourism is increasing signiﬁcantly since the last decade. However, in comparison
with other topics the literature is still scarce in this ﬁeld. This paper is new in the
framework and trends in the study of factors affecting rural tourism. Examination of
the sources indicates that related studies have been done in various places of the world.
However, most of the studies were conducted in Asia (50.81%) and Europe (40.98%).
While going through the counties we found most studies were conducted in Malaysia
(Asia). This proves that the study of rural tourism is considered as one of the major
topics in the Asian region.
The studies were conducted using qualitative, quantitative, and mixed methods of
research where quantitative analysis was applied for most of the studies (72.13%). This
shows most of the studies were conducted by researchers by preparing structured ques-
tionnaire and ﬁlling out it by respondents. While analyzing the perspective of research it
was found that studies were conducted from tourists, hosts, residents, and stakeholder’s
perspectives. Most of the studies (72.13%) were found to be done from a tourist per-
spective. This result emphasized the need for study from a tourist perspective for the
success of rural tourism.
The authors conducted different citation studies for the selected papers. From the
study it was found that “Tourism Management” from which 3 papers are included got
the highest number of citations (680). The other two journals got more than 200 citations
"Journaloftravelresearch”and“Journaloftravelandtourismmarketing”.Theincreasing
interest in rural tourism can be observed in the number of citations of papers.
Several factors have been identiﬁed as being critical to the success of rural tourism.
Accommodation, destination image, and price are the key factors affecting rural tourism.
So, concerned stakeholders need to focus on these key elements to achieve success in
ruraltourism.Itwasalsoobservedthatthekeywordsrelatedtothestudyweredividedinto
8 clusters. Rural tourism was the main keyword related to the study. Whereas tourism,

54
P. Sapkota et al.
satisfaction, service quality, perception, motivation, and hospitality industry were other
keywords leading each cluster.
5
Conclusion
This study analyzed 61 articles from 47 journals published from January 2001–2021,
related to factors affecting rural tourism through the data screening process. The study
in rural documents shows that this area is quite wide and diversiﬁed so that a broader
perspective needs to be included. This paper has indicated different dimensions of rural
tourism and interesting trends in literature. The result that was obtained will help policy
makers and stakeholders for the scientiﬁc development of rural tourism. Meanwhile,
researchers can get some diverse topics that can open new research areas. The study has
given more emphasis on the factors affecting rural tourism. It is essential to study rural
tourism from a diverse perspective such as environmental, sociocultural, economic, etc.
Some of the perspective related to factors affecting rural tourism has been studied but it
would be better to include the impact of social media and technology factor as well in
the future.
This study has some limitations though it contributes a lot in rural tourism ﬁeld.
Firstly, the articles used in this research were only collected from the Scopus database.
Thus, studies from other journals and publications which are not indexed in Scopus
database on the same topic were not considered for this study. Only 61 out of available
440 articles were selected for the ﬁnal study through the data screening process, which
may also affect the result of the study. So, this may bound the opportunity to gain broader
knowledge and information related to rural tourism.
It is recommended that researchers monitor the development of the various key-
words in literature and examine deeply some of the study clusters revealed in our
research. Newspapers, books, and other informational sources could also be consid-
ered for new methodologies outside to organize and study various literatures. Future
researchers should make a strong effort to generate greater knowledge and develop a
new framework by examining the consensus as well as the limitations of the result of
this study by doing a more scientiﬁc and systematic review in rural tourism.
References
1. Garrod, B., Wornell, R., Youell, R.: Re-conceptualising rural resources as countryside capital:
The case of rural tourism. J. Rural Stud. 22(1), 117–128 (2006). https://doi.org/10.1016/j.jru
rstud.2005.08.001
2. Tirado Ballesteros, J.G., Hernández Hernández, M.: Challenges facing rural tourism man-
agement: a supply-based perspective in Castilla-La Mancha (Spain). Tour. Hosp. Res. 21(2),
216–228 (2021). https://doi.org/10.1177/1467358420970611
3. Viljoen, J., Tlabela, K.: Rural tourism development in South Africa, trends and challenges.
Hum. Sci. Res. Counc. 1–29 (2007)
4. Nair, V., Munikrishnan, U.T., Rajaratnam, S.D., King, N.: Redeﬁning rural tourism in
malaysia: a conceptual perspective. Asia Paciﬁc J. Tour. Res. 20(3), 314–337 (2015). https://
doi.org/10.1080/10941665.2014.889026

A Systematic Literature Review on Factors Affecting Rural Tourism
55
5. Wilson, S., Fesenmaier, D.R., Fesenmaier, J., Van Es, J.C.: Factors for success in rural tourism
development. J. Travel Res. 40(2), 132–138 (2001). https://doi.org/10.1177/004728750104
000203
6. Devesa, M., Laguna, M., Palacios, A.: The role of motivation in visitor satisfaction: empirical
evidence inrural tourism. Tour. Manag. 31(4), 547–552 (2010)
7. Látková, P., Vogt, C.A.: Residents’ attitudes toward existing and future tourism development
in rural communities. J. Travel Res. 51(1), 50–67 (2012). https://doi.org/10.1177/004728751
0394193
8. Sharpley, R.: Host perceptions of tourism: a review of the research. Tour. Manag. 42, 37–49
(2014)
9. Nunkoo, R., Ramkissoon, H.: Travelers’ E-purchase intent of tourism products and services. J.
Hosp. Mark. Manag. 22(5), 505–529 (2013). https://doi.org/10.1080/19368623.2012.680240
10. Hulland, J., Houston, M.B.: Why systematic review papers and meta-analyses matter: an
introduction to the special issue on generalizations in marketing. J. Acad. Mark. Sci. 48(3),
351–359 (2020). https://doi.org/10.1007/s11747-020-00721-7
11. An, W., Alarcón, S.: How can rural tourism be sustainable? a systematic review. Sustain.
12(18) (2020). https://doi.org/10.3390/SU12187758

K-Modes with Binary Logistic Regression:
An Application in Marketing Research
Jonathan Rebolledo1 and Roman Rodriguez-Aguilar2(B)
1 Facultad de Ingeniería, Universidad Anáhuac México, Av. Universidad Anahuac 46, Lomas
Anahuac, 52786 Lomas Anahuac, Mexico
2 Facultad de Ciencias Económicas y Empresariales, Universidad Panamericana, Augusto
Rodin 498, 03920 Mexico City, Mexico
rrodrigueza@up.edu.mx
Abstract. Binary logistic regression is a statistical method used to analyze data
with binary outcome variables. It is a type of generalized linear model (GLM) and
is often used in ﬁelds such as medicine, psychology, and sociology. Using this
model with some degree of data noise might give wrong results.
Keywords: k-modes · Outliers · Logistic regression · Deviance
1
Introduction
Binary logistic regression is a statistical model commonly used in various ﬁelds, includ-
ing social sciences, medical research, and marketing, to analyze and understand the
relationships between variables. The model uses a logistic function to map the linear
combination of predictor variables to a probability value between 0 and 1. The model
estimates the parameters of the logistic function using maximum likelihood estimation,
which ﬁnds the values of the parameters that maximize the likelihood of the observed
data. The model output includes coefﬁcients for each predictor variable, which repre-
sent the strength and direction of the relationship between the variable and the outcome.
Binary logistic regression is a powerful tool for analyzing and predicting binary out-
comes, but it is important to consider its assumptions and limitations, such as linearity
of the relationship between predictor variables and the outcome and independence of
observations.
1.1
Outliers in Binary Logistic Regression
Outliers can affect binary logistic regression in the same way they affect linear regression.
Outliers are data points that are far away from the bulk of the data and can have a
disproportionate effect on the model. Outliers can impact the estimated coefﬁcients and
increase the variability of the model, leading to biased and unreliable predictions.
In binary logistic regression, outliers may lead to an incorrect classiﬁcation of the
outcome variable for some observations, which can lead to inaccurate predictions. For
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 56–64, 2024.
https://doi.org/10.1007/978-3-031-50158-6_6

K-Modes with Binary Logistic Regression: An Application
57
example, an outlier with a very high probability of the outcome variable may shift the
classiﬁcation threshold and result in misclassifying other observations.
Identifying outliers in categorical data can be challenging, as there are no clear
numerical cut-off points. However, there are several methods that can be used:
Frequency distributions: One way to identify outliers in categorical data is to examine
the frequency distributions of each category. If a category has a much lower or higher
frequency than the others, it may be an outlier.
Box plots: Another way to identify outliers in categorical data is to create box plots
for each category. Box plots show the median, quartiles, and range of the data, and can
help to identify categories that have signiﬁcantly different distributions from the others.
Chi-square test: A chi-square test can be used to compare the observed frequencies
of each category to the expected frequencies, based on the overall distribution of the data.
Categories that have a signiﬁcantly higher or lower observed frequency than expected
may be outliers.
Once outliers have been identiﬁed, there are several ways to deal with them:
• Removing outliers: This is the most common method of dealing with outliers. How-
ever, it should be done with caution, as removing too many observations may lead to
a loss of information.
• Transforming the data: In some cases, outliers can be caused by a skewed distribution.
Transforming the data, such as taking the log or square root, can help to alleviate this
problem.
• Using robust estimation methods: Some estimation methods, such as M-estimators
and maximum likelihood estimators, are less sensitive to outliers than traditional
methods. This includes the Robust Regression: a type of regression technique that
uses weighted least squares to minimize the effect of outliers in the data. It works
by giving higher weights to data points that are close to the line of best ﬁt and lower
weights to data points that are further away.
• Regularization involves adding a penalty to the cost function to reduce the effects of
outliers on the model. This penalty is determined by how far away each data point is
from the line of best ﬁt. Some examples of regularization techniques include Ridge
Regression, which adds an L2 penalty to the cost function, and Lasso Regression,
which adds an L1 penalty to the cost function.
1.2
Categorical Variables
In binary logistic regression, categorical variables can be used as predictor variables to
explain the variation in the outcome variable, which is binary (0 or 1).
There are two types of categorical variables in binary logistic regression:
Nominal variables: These are variables that have no inherent order. For example,
a variable that represents different types of fruits (apple, banana, orange) is a nominal
variable. In binary logistic regression, nominal variables are often represented by dummy
variables. A dummy variable is a binary variable that represents the presence or absence
of a speciﬁc category.
Ordinal variables: These are variables that have an inherent order. For example, a
variable that represents different levels of education (high school, college, graduate) is an
ordinal variable. In binary logistic regression, ordinal variables are often represented by

58
J. Rebolledo and R. Aguilar
ordinal dummy variables. An ordinal dummy variable is a binary variable that represents
whether an observation is in a speciﬁc category.
2
Marketing Research
In marketing research, almost all the data is categorical. Why? Because in real life,
an individual buys or not a product or service based on the properties of the product or
service, they like or not the price, they like or not the design, they like or not the color, etc.
So when evaluating a marketing campaign, 95% or more of the predictive variables are
categorical, which includes demographical variables such as gender, socioeconomical
level, age in range levels, the location of the individual, etc. And as mentioned before,
the Binary Logistic Regression model may give them incorrect results since it has not
been optimized for working with all predictive variables being categorical.
2.1
K-Modes and Binary Logistic Regression
One of the assumptions in the basic model is that there is one big population and there
are no outliers, but actually a lot of outliers are present in the data set. How can we tell
to the basic model that there are many outliers? How can we obtain robust estimations
of the Beta parameters?
We want to know the ranking and the Odds of the principal variables, without remov-
ing the “outliers” since every registry in the data set has money implications, the client
has paid a lot of cash for the data, and removing an individual is not an option. The clients
use the Odds ratio estimation because of the interpretation in terms of percentage, it is
easy to understand that the possibilities of purchasing a product or service increases “X
percentage” if an individual likes a speciﬁc characteristic of the product or service they
are selling.
There are a lot of methods to detect patterns in statistics, these are known as “unsu-
pervised learning”; these methods allow them to detect such patterns. The basic idea is
to add an additional variable to the dataset indicating the group each individual belongs
to, then run the binary logistic regression with this new variable and obtain the best esti-
mation of the other predictive variables in order to determine the path the client should
follow to increase the probability of the dependent variable.
An obvious question arises: how many groups do we need? This is obtained by
analyzing the pattern of the deviance as a goodness of ﬁt of the model; in this approach
(since all their variables in the dataset are categorical dummy), the use of K-Modes
unsupervised learning is applied, with the number of groups varying from 2 to 10.
K-modes is a clustering algorithm used for categorical data. It is similar to the k-
means algorithm, but instead of calculating the mean of the data points in a cluster,
it calculates the mode. The mode is the most frequently occurring value in a dataset.
The algorithm starts by randomly selecting k initial modes, one for each cluster, and
then assigns each data point to the cluster with the closest mode. The modes are then
recalculated for each cluster, and the process is repeated until the assignments of data
points to clusters no longer change. k-modes is used for categorical data because it can
handle categorical variables that have many different levels.

K-Modes with Binary Logistic Regression: An Application
59
To determine the optimal number of groups, we create a scatter plot of the behavior
of the deviance for 1 group (the basic model), 2 groups, 3 groups, etc. then we select
the number of groups where the deviance is minimized. Later we run the binary logistic
regression with this additional variable and compare the results.
Howdoweknowthismethodologyactuallyworks?Sincetherearemanyapproaches,
we compare the basic logistic regression model, the ridge regression, and this new
approach with K-modes; we cannot use “stepwise” o Lasso regularization because in
marketing research the variables evaluated by the individual cannot value cero, because
of the money implications; These variables were obtained by qualitative studies, which
indicate these attributes must be evaluated. Sad but true. Ultimately, we use the Lasso
regularization as validation of the path of the top 4 variables the client must follow to
increase the sales of the product or service.
3
Data Set
The data set belongs to IPSOS©, it corresponds to a study of evaluating the best fast-food
restaurant with the main question “What makes an individual choose between one place
to another?”.
The data base consists of 4,512 interviewees. The survey was carried out in a period
of 1 month (March 2022) in the 3 main Metropolitan Areas of Mexico: Mexico City,
Guadalajara and Monterrey. Men and women from 18 to 45 years of all socioeconomic
levels were considered. Each interviewee evaluates the brands they know and have
consumed, being able to evaluate only 1 brand. The ﬁlter is that none of the interviewees
works in any market research company, as well as having consumed products from any
of the following brands in the last 20 days:
• Benedetti’s Pizza
• Burger King
• Domino´s Pizza
• KFC
• Little Caesar’s
• McDonald’s
• Papa John’s
• Pizza Hut
• Subway
• Carl’s Jr
Dependent variable is purchase consideration: Would you consider buying again in
“xxx” soon? Possible responses “yes”, “no”.
The base structure and some basic demographical insigns can be seen in Tables 1
and 2.
4
Results
After executing the K-modes from 1 to 10 groups, we see this pattern graphically:

60
J. Rebolledo and R. Aguilar
Table 1. Data base structure.
Variable
Label
Measurment Scale
Possible values
Respondent_Serial Serial number
Nominal
Numerical
resp_gender
Are you male of female?
Nominal
1 Male, 2 Female
resp_age
Respondent Age
Ordinal
1 (18-24 years), 2(25-34 
years), 3(35-45 years)
Brand
Evaluated Brand
Nominal
Text
Q8
Purchase consideraon
Target, Binary
0 No, 1 Yes
Q11_1
It has the best promoons
Input, Binary
0 No, 1 Yes
Q11_2
It oﬀers a good variety of items on the menu
Input, Binary
0 No, 1 Yes
Q11_3
It has a fast service
Input, Binary
0 No, 1 Yes
Q11_4
It features quality food\ingredients
Input, Binary
0 No, 1 Yes
Q11_5
It has quality service (friendly\ courteous staﬀ)
Input, Binary
0 No, 1 Yes
Q11_6
It has the tasest food
Input, Binary
0 No, 1 Yes
Q11_7
It gives me more value for my money
Input, Binary
0 No, 1 Yes
Q11_8
It's a fast food place I like
Input, Binary
0 No, 1 Yes
Q11_9
Their poron sizes\dishes are large enough to sasfy my hunger
Input, Binary
0 No, 1 Yes
Q11_10
It's a clean place to visit
Input, Binary
0 No, 1 Yes
Q11_11
It's easy to order and get my food
Input, Binary
0 No, 1 Yes
Q11_12
It's a restaurant for people like me
Input, Binary
0 No, 1 Yes
Table 2. Demographics statistics of the data set.
Frequency
Percent
Frequency
Percent
Male
2254
49.956
1.00 18-24 years
1940
43.0
Female
2258
50.044
2.00 25-34 years
1710
37.9
Total
4512
100.0
3.00 35-45 years
862
19.1
Total
4512
100.0
Frequency
Percent
Frequency
Percent
1 Mexico City
2256
50.0
1 Benedetti's Pizza
278 6.161348
2 Guadalajara
1128
25.0
2 Burger King
517 11.45833
3 Monterrey
1128
25.0
3 Carl’s Jr
466 10.32801
Total
4512
100.0
4 Domino´s Pizza
501 11.10372
5 KFC
539 11.94592
6 Little Caesar’s
555 12.30053
7 McDonald’s
457 10.12855
8 Papa John’s
299 6.626773
9 Pizza Hut
427 9.463652
10 Subway
473 10.48316
Total
4512
100
Gender
Age
Location
Evaluated Brand
The number of groups that minimize the deviance is 5 with a deviance value of
5092.949 (Fig. 1), then the logistic regression with k = 5 is conducted.

K-Modes with Binary Logistic Regression: An Application
61
Fig. 1. Pattern of the deviance after including the K-Modes variable
We compare the Beta estimations, signiﬁcance, and Odds ratio (>1) of 4 models:
K-Modes with Logistic Regression, Basic Binary Logistic Regression, Ridge regular-
ization and additionally the LASSO regularization, this one not useful due to money
implications.
4.1
K-modes with Binary Logistic Regression
See Table 3.
4.2
Basic Binary Logistic Regression
See Table 4.
4.3
Ridge Regression
Deviance: 5316.5, Optimal lambda 0.027543 (# of folds for CV: 10, seed 1234) (Table 5).
4.4
LASSO
Deviance: 5215.25, Optimal lambda 0.003814 (#of folds for CV; 10, seed 1234)
(Table 6).
As we can see, the K-modes has the best goodness of ﬁt, the deviance is much
lower than the others, the beta estimations are better and the odds of the variables.
One thing to highlight is the signiﬁcance (α = 0.05) of the betas, in the Basic Binary
Logistic Regression only Q11_1 is statistically important and that is weird since this set
of variables were deﬁned by previous qualitative studies.

62
J. Rebolledo and R. Aguilar
Table 3. K-Modes optimization results.
Variable
Esmate
Pr(>|z|)
Odds Rao Variable Importance
Ranking
(Intercept)
0.402064 0.137195
Q11_1Yes
0.418671
1.34E-05 ***
52%
13%
1
Q11_2Yes
-1.2806
4.6E-23 ***
2%
12
Q11_3Yes
0.167329 0.095285
18%
10%
7
Q11_4Yes
0.266051 0.006775 ***
30%
12%
3
Q11_5Yes
-0.9973
2.55E-13 ***
3%
9
Q11_6Yes
0.231491 0.025803 ***
26%
11%
4
Q11_7Yes
0.219134 0.025642 ***
24%
11%
5
Q11_8Yes
0.171757 0.108669
19%
11%
6
Q11_9Yes
0.014773 0.881213
1%
9%
8
Q11_10Yes
-1.20368
1.31E-18 ***
3%
10
Q11_11Yes
0.288735
3.97E-03 ***
33%
12%
2
Q11_12Yes
-1.23185
1.02E-20 ***
3%
11
Cluster 1
-1.95935
1.05E-10 ***
Cluster 2
2.899163
1.36E-21 ***
Cluster 3
-1.08823 0.000126 ***
Cluster 4
-1.39291
1.21E-07 ***
Coeﬃcients:
Table 4. Binary logistic regression results.
Variable
Esmate
Pr(>|z|)
Odds Rao Variable Importance
Ranking
(Intercept)
-1.36636
2E-16 ***
Q11_1Yes
0.376158
5.79E-06 ***
46%
11%
1
Q11_2Yes
-0.00903
0.9044
8%
10
Q11_3Yes
0.130001
0.1105
14%
9%
4
Q11_4Yes
0.040665
0.6139
4%
8%
7
Q11_5Yes
0.015428
8.53E-01
2%
8%
9
Q11_6Yes
0.019338
0.8205
2%
8%
8
Q11_7Yes
0.139824
0.0979
15%
9%
2
Q11_8Yes
0.129904
0.1381
14%
9%
5
Q11_9Yes
-0.15639
0.0556
7%
12
Q11_10Yes 0.058095
0.4555
6%
8%
6
Q11_11Yes 0.135644
1.04E-01
15%
9%
3
Q11_12Yes
-0.01225
0.8756
8%
11
Coeﬃcients:
In this study, four different statistical methods were used to analyze the data, K-
modes analysis, basic logistic regression, ridge regularization, and LASSO regulariza-
tion. The results of each method reveal different variable importance rankings, which
are as follows:

K-Modes with Binary Logistic Regression: An Application
63
Table 5. Ridge regularization results.
Variable
Esmate Odds Rao Variable Importance
Ranking
(Intercept)
-0.52174
Q11_1Yes
-0.22532
25%
10%
1
Q11_2Yes
-0.01045
1%
8%
11
Q11_3Yes
-0.10524
11%
9%
2
Q11_4Yes
-0.03615
4%
8%
8
Q11_5Yes
-0.0332
3%
8%
9
Q11_6Yes
-0.03678
4%
8%
7
Q11_7Yes
-0.10317
11%
9%
4
Q11_8Yes
-0.09653
10%
9%
5
Q11_9Yes
0.052596
7%
12
Q11_10Yes
-0.04786
5%
8%
6
Q11_11Yes
-0.10396
11%
9%
3
Q11_12Yes
-0.01914
2%
8%
10
Coeﬃcients:
Table 6. LASSO regularization results.
Variable
Esmate Odds Rao Variable Importance
Ranking
(Intercept)
-0.55655
Q11_1Yes
-0.32161
38%
20%
1
Q11_2Yes
0
Q11_3Yes
-0.09726
10%
16%
3
Q11_4Yes
0
Q11_5Yes
0
Q11_6Yes
0
Q11_7Yes
-0.08244
9%
16%
5
Q11_8Yes
-0.09433
10%
16%
4
Q11_9Yes
0
Q11_10Yes
-0.0155
2%
15%
6
Q11_11Yes
-0.10669
11%
16%
2
Q11_12Yes
0
Coeﬃcients:
K-modesanalysis:ThemostimportantvariablesinorderareQ11_1,Q11_11,Q11_4,
and Q11_6.
Basic logistic regression: The most important variables in order are Q11_1, Q11_7,
Q11_11, and Q11_3.
Ridge regularization: The most important variables in order are Q11_1, Q11_3,
Q11_11, and Q11_7.
LASSO regularization: The most important variables in order are Q11_1, Q11_11,
Q11_3, and Q11_8.

64
J. Rebolledo and R. Aguilar
While the variable Q11_1 is consistently ranked as the most important variable across
all four methods, the other variables’ rankings differ. These ﬁndings provide valuable
insight into which variables are more inﬂuential depending on the statistical method
used, allowing for more informed decision-making and further analysis.
K-modes is the winner; the Lasso (that cannot be used to communicate results to
the client) also conﬁrms the key variables (Q11_1 and Q11_11) that must be considered
if the client wants to increase the probability of purchase consideration being “It has
the best promotions” and “It’s easy to order and get my food” as the most important
variables with a variable relative importance of 13% and 12% respectably, the odds are
also very High.
5
Conclusions
The methodology here presented is one alternative when the data set has categorical
variables and a huge number of outliers that can mislead the conclusions of the model.
Thisalternativerepresentssomethingeasytouse,easytoimplementandcomputationally
affordable. In case the database has mixed types of variables, (categorical and numerical)
then the k-Prototypes Clustering could be used, and the minimization occurs, in case of
scale variable target, using RMSE as goodness of ﬁt of the model (suggested).
References
Agresti, A.: Categorical data analysis (3rd ed.). John Wiley & Sons (2013)
Bishop, C.M. (ed.): Pattern Recognition and Machine Learning. ISS, Springer, New York (2006).
https://doi.org/10.1007/978-0-387-45528-0
DeSarbo, W.S., Ramaswamy, V., Cohen, S.H.: Market segmentation with choice-based conjoint
analysis. Mark. Lett. 6(2), 137–147 (1995)
Everitt, B.S., Hothorn, T.: An introduction to applied multivariate analysis with R. Springer (2011)
Field, A.: Discovering statistics using IBM SPSS statistics (5th ed.). SAGE Publications (2018)
Hair, J.F., Black, W.C., Babin, B.J., Anderson, R.E.: Multivariate data analysis (8th ed.). Cengage
Learning (2018)
Hastie, T., Tibshirani, R., Friedman, J.: The elements of statistical learning: Data mining, inference,
and prediction (2nd ed.). Springer (2009)
Hosmer, D. W., Lemeshow, S., & Sturdivant, R.X.: Applied logistic regression (3rd ed.). John
Wiley & Sons (2013)
Huber, P.J.: Robust Statistics. John Wiley & Sons (1981)
Huang, Z.: A fast-clustering algorithm to cluster very large categorical data sets in data mining.
Res. Issues Data Min. Knowl. Discov. (DMKD ‘97) (1997)
James, G., Witten, D., Hastie, T., Tibshirani, R.: An introduction to statistical learning: with
applications in R. Springer (2013)
Jolliffe, I.T., Cadima, J.: Principal component analysis: a review and recent developments. Philos.
Trans. Royal Soc. A: Math., Phy. Eng. Sci. 374(2065), 20150202 (2016)
Kelleher, J.D., Mac Namee, B., D’Arcy, A.: Fundamentals of Machine Learning for Predictive
Data Analytics: Algorithms, Worked Examples, and Case Studies. MIT Press (2015)
Malhotra, N.K.: Marketing research: an applied orientation (7th ed.). Pearson (2019)
McCullagh, P., Nelder, J.A.: Generalized linear models (2nd ed.). Chapman and Hall/CRC (1989)
Tibshirani, R.: Regression shrinkage and selection via the Lasso. J. Roy. Stat. Soc.: Ser. B
(Methodol.) 58(1), 267–288 (1996)

Optimizing Headways Using Evolutionary
Algorithms for Improved Light Rail Transit
System Efﬁciency and Passenger Service
Oomesh Gukhool(B)
, Nooswaibah Binti Nooroodeen Soosor, Ravindra Boojhawon,
and Mohammad Zaheer Doomah
University of Mauritius, Reduit, Mauritius
o.gukhool@uom.ac.mu
Abstract. This study compares and evaluates the effectiveness of two optimiza-
tionalgorithms,GeneticAlgorithm(GA)andDifferentialEvolution(DE),indeter-
mining the optimal headway values for Light Rail Vehicles (LRVs) at stations. The
aim is to minimize both passenger waiting time and operational costs for LRT sys-
tems. Both algorithms use a standard cost function, where the decision variables
are headway values representing the time intervals between the arrival of con-
secutive LRVs at the same station. The study assesses the performance of both
algorithms and determines which is more effective in optimizing headways for
LRVs. The results of this study will provide valuable insights to LRT operators
in terms of choosing the most efﬁcient algorithm to optimize headways, which
can lead to reduced operational costs and improved service quality for passengers.
Ultimately, optimizing LRV headways can help LRT systems to enhance their
efﬁciency and provide better service to their passengers.
Keywords: Headway · Optimization · Evolutionary Algorithms
1
Introduction
Headway optimization is a critical component of public transportation, as it pertains
to the time interval between consecutive vehicles on a given route or line. This inter-
val determines the frequency of service and the number of vehicles necessary to meet
demand. Lately, there has been an increasing interest in leveraging data-driven tech-
niques to optimize headways and enhance transit operations. One speciﬁc application
of headway optimization is in light rail transit (LRT) systems. LRT systems are electric
railway systems that typically operate on tracks in dedicated lanes or in mixed trafﬁc
and serve intermediate distances and denser urban areas. These systems frequently have
multiple lines, each with its schedule and headway.
LRT headway optimization involves utilizing real-time data and predictive analytics
to adjust the frequency of service based on passenger demand, trafﬁc conditions, and
other relevant factors. This approach can reduce wait times for passengers, improve
reliability, and increase overall system efﬁciency. Many research studies have delved
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 65–75, 2024.
https://doi.org/10.1007/978-3-031-50158-6_7

66
O. Gukhool et al.
into the utilization of data-driven techniques for optimizing train headways, and some
of these studies have employed machine learning algorithms to dynamically adjust the
headways in response to changing conditions. [1, 2]. However, it has been observed
that there is a paucity of similar studies focusing speciﬁcally on LRT systems. The
application of data-driven approaches to optimize headways in LRT systems remains
relatively unexplored, warranting further research in this area to bridge the gap and
uncover potential opportunities for enhancing the efﬁciency of LRT operations.
While there is no concrete research conducted in the ﬁeld of LRT headway optimiza-
tion, we have seen studies conducted in other transit system that supports the importance
of optimizing headway to improve operations and passenger demand. Several studies
have contributed to the ﬁeld of transit headway optimization. For instance, one study
investigated how stochastic vehicle arrivals impacted optimal stop spacing and headway
for a feeder bus route [3]. Another study proposed a multi-objective model for schedul-
ing problems in Bus Rapid Transit (BRT) systems [4]. Other studies have explored the
inﬂuence of value-of-time distributions on transit headway optimization [5], the com-
bined effects of probabilistic vehicle arrivals and passenger walking time on transfer
coordination [6], and the optimization of headways with stop-skipping control in BRT
systems [7]. Furthermore, research has been conducted on optimal control of headway
variation on transit routes [8] and multi-objective optimal formulations for bus ﬂeet size
in public transit systems under headway-based holding control [9].
Despite the lack of application of optimization algorithms such as genetic algorithms
(GA) and differential evolution (DE) to optimize LRT headway, there have been notable
studies that have successfully utilized GA and DE techniques to optimize headways in
other transit systems. For instance, a study presented in [4] introduced a novel multi-
objective model for Bus Rapid Transit (BRT) scheduling problems, which was solved
using the Fast Non-dominated Sorting Genetic Algorithm (NSGA-II), a commonly used
multi-objective optimization algorithm. Another study conducted by [10] proposed a
two-phase algorithm that combined a single-objective covariance matrix adaptation evo-
lution strategy with a multi-directional local search to optimize headways in complex
urban mass rapid transit systems. The authors of [11] introduced a novel bi-level opti-
mization model for solving the transit frequency setting problem in bi-modal networks.
The proposed model leverages a differential evolution (DE) algorithm in the upper-
level model to effectively determine the optimal headways for a given route structure,
showcasing its innovative approach to tackling this complex problem.
These studies collectively demonstrate the potential of optimization algorithms, such
as GA and DE, in optimizing headways in transit systems, and highlight the need for
further research in this area to develop more robust and efﬁcient optimization techniques
for LRT headway optimization. Despite this, there has been no direct comparison of the
two algorithms for optimizing headway in LRT systems. DE has been shown to have
faster convergence than GA for a wide range of problems, particularly those with high-
dimensional search spaces, due to its more directed exploration of the search space
through the mutation operator [12, 13]. Furthermore, DE requires less parameter tuning
compared to GA, which has several parameters that need careful calibration, including
the selection operator, crossover operator, mutation operator, and population size. In
contrast, DE only requires tuning of a few parameters, such as the mutation factor and

Optimizing Headways Using Evolutionary Algorithms
67
crossover rate [12, 13]. DE is also more robust than GA in handling changes in the cost
function or search space due to its ﬁxed mutation and crossover operator, while GA’s
more complex operators may be sensitive to such changes [12, 13]. DE is also more
effective in incorporating constraints directly into the cost function compared to GA,
which requires a more complex approach to handling constraints [12, 13].
The aim of this study is to compare and evaluate the efﬁcacy of two algorithms,
GA and DE, in optimizing headways for Light Rail Vehicles (LRVs) with the goal
of minimizing passenger waiting time and operational costs for LRT systems. Both
algorithms utilize a standard cost function, in which the decision variables are headway
values, representing the time intervals between consecutive LRV arrivals at the same
station. By comparing the performance of these two algorithms, we can determine which
algorithmismoreeffectiveatoptimizingLRVheadwaysinordertoreduceLRToperation
costs and passenger waiting times. By optimizing the headway values using a cost
function and an optimization algorithm, LRT systems can improve their efﬁciency and
reduce costs, while providing better service to passengers.
2
Methodology
The proposed optimization algorithms are used to determine the best solution to a cost
function which minimizes both the average waiting time of passengers and the service
operator costs. The decision variables are the headways, and the optimization algorithms
used are DE and GA, both implemented in MATLAB. The algorithms’ performance
is assessed by analyzing their capability to generate high-quality solutions, their rate
of convergence, and their robustness in dealing with constraints. Statistical analysis
techniques, such as t-tests, are employed in the evaluation process. The results of the
study shed light on the performance of both algorithms, which are elaborated in detail
in the result and discussion section.
2.1
Cost Function
The deﬁned mathematical model considers the movement of LRV along only one direc-
tion of the LRT line. The LRV move to terminal station S starting from origin station
1. The daily operating time of the LRT service is truncated into P equal-length periods.
In the model, stations are denoted by s, where s can take values between 1 and S-1
inclusive, and the periods are denoted by p, where p can take values between 1 and P
inclusive. Moreover, in this case study, the length of each period is taken to be 1 h, and
the start of each period p in the model occurs at multiples of 60 starting from 0th minute
in period 1, 60th minute in period 2, and so on. The ﬁrst LRV in each period depart from
origin station 1 at the start of each period.
The cost function Z incorporates two conﬂicting objectives: the passenger waiting
time–cost function and the service operator cost function. The relative importance of
each objective is determined by the weights W0 and W1. W0 and W1 are user-deﬁned
values between 0 and 1; sum to 1; and are set at the discretion of the service operator and
relevant stakeholders. The passenger waiting time model follows the assumptions that
passengers arrive randomly at stations and that they board on the ﬁrst arriving LRV at

68
O. Gukhool et al.
the respective stations they are waiting at. The mean waiting time per passenger waiting
in period p is then assumed to be half the headway value in that period in accordance
with the passenger model deﬁned by Bowman and Turnquist [14]. The cost function Z
is thus deﬁned as follows:
min Z = W0 × β
⎛
⎝
P

p=1
S−1

s=1

ωs
p × 1
2hp
⎞
⎠+
⎛
⎝W1 ×
P

p=1
λ × np
⎞
⎠
(1)
and in some of the tests below, is subject to the constraint:
np = ﬂoor
60p −start of period p at station 1
hp

< Tmax
(2)
The decision variables of the model are the headway values hp to be set in each period
p to minimize both the passenger waiting times and the total service operator costs. hp
lies in the interval [hlb … hub] where hlb is the minimum value that hp can take so as
to have a safe running distance between any two LRV and hub is an operator-decided
value which represents the maximum passenger waiting time allowed during a period
p. Moreover, between any two periods p and p + 1, the mathematical model ensures
that there is at least hlb minutes between the last departing LRV in period p and the ﬁrst
departing LRV in period p + 1, by accordingly adjusting the beginning of period p +
1, if need be. ωs
p denotes the number of passengers waiting at station s in period p; β
is the dollar equivalent per minute of passenger waiting time; np is the number of LRV
serving the direction of the LRT line under study during period p; Tmax represents the
maximum available number of LRV to serve the direction of the LRT line under study
during any period of the day; λ denotes an average ﬁxed cost of running of an LRV
along the LRT line. It encapsulates the per-LRV operating expenses, salaries of the LRT
staff, maintenance cost and so on. Two different optimization algorithms are considered
to minimize Z, namely GA and DE.
2.2
Optimization Algorithms
GA is a heuristic search method which mimics the processes involved in the biological
evolution theory. It is based on the principle that only the ﬁttest individuals survive from
one generation to the next generation. During the initial iteration step (generation) of
the algorithm, a set of parent individuals is produced by random generation. Each of
these individuals represents a potential solution to the cost function. The population
size, which is a value determined by the user, refers to the quantity of parent individuals
generated at each iteration step. The parent individuals then undergo the crossing and
mutation operations in an attempt to generate better offspring individuals, that is, better
candidate solutions to the cost function. After each iteration step, the ﬁtness of both
parents and offspring individuals is evaluated to minimize the cost function. The ﬁtter
individuals are then chosen as parents for the next generation, and this process continues
until a user-deﬁned termination criterion is met. In this study, we set the termination
criteria as a maximum number of iterations after which the algorithm stops.

Optimizing Headways Using Evolutionary Algorithms
69
DE is categorized an evolutionary algorithm [15]. DE follows a similar approach
to GA by creating a population of potential solutions, with the population size at each
iteration step determined a user-deﬁned value. While it shares commonalities with GA
in that both algorithms use genetic operators such as mutation, crossover and selection
on possible candidate solutions, DE distinguishes itself from GA in the mechanism that
it employs to do these operations. In this study, the classic DE/rand/1/bin strategy was
employed as the DE approach. During the mutation stage, mutant vectors are created
for each member of the population known target vectors. This is done by carrying out a
weighted difference using three distinct random individuals (other than the target vector)
from the current population. In the crossover stage, new individuals (trial vectors) are
created by mixing components of mutant vectors with components of their corresponding
target vectors according to some deﬁned probabilistic rule. The ﬁtter individual between
the trial vector and its corresponding target vector is chosen as a member of the next
population in each iteration. This process is repeated until the algorithm reaches the
user-deﬁned termination criterion. For the purpose of this study, we set the termination
criterion as the maximum number of iterations after which the algorithm stops.
For DE, the codes, as shown in Fig. 2, were written in reference to the steps outlined
by [15], whereas the codes used for GA (Fig. 1) have been developed by [16]. For both
algorithms, the codes were adjusted so that the generated candidate solutions at the ﬁrst
iteration were integer values from the interval [hlb … hub]. Moreover, after the mutation
stage for GA and the crossover stage for both GA and DE, the offspring candidates were
bounded between hlb and hub and rounded off to the nearest integer values.
Fig. 1. Genetic algorithm
We examined the performance of the two algorithms under two different conditions:

70
O. Gukhool et al.
Fig. 2. Differential evolution algorithm
1. Without the constraint deﬁned by Eq. (2) above.
2. With the deﬁned constraint in Eq. (2) above.
To this end, the constraint is added to the cost function, given by Eq. (1), through the
use of a penalty function deﬁned by M 	P
p=1

max

0, np −Tmax

, where the penalty
coefﬁcient M is set to 10,000 and Tmax is 4. In both conditions 1 and 2, for different values
of (population size/maximum number of iterations), the optimal values obtained by both
algorithms were recorded over 50 runs of the algorithm. The values for (maximum
number of iterations/population size) tested were: (50/100), (100/100), (100/150) and
(150/150). The two-sample one tailed t-test (assuming inequality of variances) was
then carried out on the values recorded for each combination of (maximum number
of iterations/population size) considered, in order to determine if there was statistical
evidence that one algorithm outperforms the other.
The O-D matrix information was based off data obtained from the local LRT ser-
vice provider, Metro Express Limited (MEL). The model parameter values were set
as follows: W0 = 0.5, W1 = 0.5, λ = $10, P = 13, S = 19 and β = 0.0067. The
GA parameters (gamma, mu, beta) as deﬁned in the codes by Heris [16] regulate the
crossover and mutation processes, and were set to (0.05, 0.08, 0.08). For DE, the scale
factor F, and the crossover rate CR, were set to (F, CR) = (0.2, 0.05). The choice for
the combination of the parameter values used for DE and GA has been made after the
following investigations had been carried out:
• For a particular cost function (other than the above-deﬁned one), an extensive trial
and error runs over different combinations of parameter values was carried out.

Optimizing Headways Using Evolutionary Algorithms
71
• The set of parameter values which yielded the minimum recorded cost function was
then chosen as the ﬁnal set of parameter values.
3
Results and Discussion
3.1
Quality of Solution
To provide evidence of the effectiveness of the optimization model and both optimization
algorithms in decreasing the overall costs of an LRT network, we conducted a survey of
thelocalLRTnetworkandcollecteddataonheadwayvaluesduringeachtimeperiod.The
generalized cost of the LRT network under the current operational plan was calculated
and compared with the optimal cost values obtained by both DE and GA (using the
cost function with constraints at (population size/maximum number of iterations) =
(150/150)). It was observed that both DE and GA were successful in decreasing the
generalized cost of the LRT network by 6.09% and 6.15% respectively, indicating the
efﬁcacy of the model and both optimization algorithms at reducing the generalized cost of
the LRT network. In contrast to our initial observation of a consistent headway for LRVs
during our on-site research, we discovered that both optimization algorithms produced
varying headway values for each time period based on the number of passengers waiting
at each station. Moreover, the optimized headway resulted in a decrease in overall costs.
Table 1 presents a summary of the optimal costs achieved through four tests that
employed various population sizes and iterations in both the DE and GA optimization
algorithms, using the cost function (Eq. (1)) as previously described. To examine whether
the mean value of the best solution for GA is inferior to that of DE, taking into account
the constraints in their respective cost functions, t-tests were conducted. A one-tailed test
was conducted because the alternative hypothesis was directional. Four tests were carried
out with varied iterations and population size. In all four tests, the degrees of freedom
(df) were 49, and the one-tailed t-tests were performed. The t critical one-tail values
were 1.675905025 in test 1, 1.676550893 in tests 2–4, and the alpha level was set at
0.05. The computed t-statistic exceeded the critical t-value at a signiﬁcance level of 0.05,
indicating that we can reject the null hypothesis and accept the alternative hypothesis.
This indicates that the mean value of the best solution for GA was considerably lower
than that of DE, implying that GA outperformed DE in discovering the optimal solution.
Moreover, the p-value for each test was very small (less than 0.05), indicating that the
observed differences in the means were statistically signiﬁcant. Thus, we can conclude
that there is strong evidence that GA outperformed DE in ﬁnding the optimal solution,
and this holds true for different combinations of maximum iteration and population size
when considering the cost function with constraints.
3.2
Convergence Speed
One common way to assess convergence speed is to plot the values of the cost function
To study the convergence rate of each algorithm, a plot of the minimum cost recorded
per iteration was made for both algorithms. Figs. 3 and 4 show graphs of the best
minimum cost recorded at each iteration for a random run of GA and DE at (population
size/maximum number of iterations) = (150/150). In this case, the cost function was

72
O. Gukhool et al.
Table 1. Best average cost for DE and GA (with constraint)
Test
Max iteration
Population size
Best average cost DE
Best average cost GA
1
100
50
659.31
652.11
2
100
100
653.3
651.9
3
150
100
653.3
651.9
4
150
150
652.32
651.9
considered without constraints and with the aforementioned set of iteration number and
population size since the statistical tests reveal there is no signiﬁcant difference in their
performance at these values (as is discussed in Sect. 150.3).
Fig. 3. Graph of best cost recorded v/s the iteration number for GA
Fig. 4. Graph of best cost recorded v/s the iteration number for DE
The graphs above demonstrate that both GA and DE converge to their respective
optimal solutions after around 50 algorithm runs. Table 2 indicates the average time
required by each algorithm to reach its optimal solution when utilizing the cost function
without constraints. We note that, at (population size/maximum number of iterations)
= (150/150), DE takes on average less time to reach its optimal value (0.5103 s) than
GA (1.0513 s). Despite comparing the time needed by DE to achieve its optimal solu-
tion at (150/150) with the population size and maximum number of iterations at which

Optimizing Headways Using Evolutionary Algorithms
73
GA produces its lowest recorded value, speciﬁcally at (100/100), we can still conclude
that DE required less time than GA. Hence, when considering the cost function with-
out constraint, we can conclude that, while DE may require a larger iteration number
and population size for its performance to be deemed statistically equivalent to GA, it
produces optimal results within a smaller timeframe than GA.
Table 2. Best average costs and computation time for DE and GA (without constraint)
Pop. size/max.
iteration
DE
GA
Best average cost
(std. dev)
Average time
taken(s)
Best average cost
(std. dev)
Average time
taken(s)
50/100
642.6962
(0.9987)
0.1806
641.4869
(7.2832e−04)
0.4962
100/100
641.6792
(0.3558)
0.3662
641.4868
(4.5936e−13)
0.7874
100/150
641.6283
(0.3731)
0.3500
641.4868
(4.5936e−13)
0.7839
150/150
641.5093
(0.1034)
0.5103
641.4868
(4.5936e−13)
1.0513
3.3
Robustness
To determine which algorithm is more robust, we tested both DE and GA with different
cost functions. The ﬁrst cost function has constraints, and the second one does not have
any constraints. When tested with the normal cost function (with constraints), both DE
and GA algorithms were able to ﬁnd the minimum value, but with variations in their mean
values. The mean values of the cost function for DE were higher than that of GA in all
the four tests conducted. The t-tests showed that the means were signiﬁcantly different.
However, when tested with the cost function without constraints (using results obtained
from 50 runs of both algorithms), the difference between the mean best values of DE and
GA reduced signiﬁcantly (Table 2). In this case, the mean values for GA and DE were
almost the same, with DE having a slightly higher mean value in the ﬁrst test. We utilized
a one-tailed t-test to report the statistical signiﬁcance of the variance in mean values. The
p-values for the initial three tests were below the signiﬁcance level of 0.05, indicating
that there was a notable difference between the average values of both algorithms. In
other words, at (50/100), (100/100) and (100/150), GA still generated slightly better
solutions than DE. It should be noted that when we increased the maximum iteration
number and population size to (150/150), the p-value was calculated to be 0.0647, which
is higher than the signiﬁcance level of 0.05. This outcome implies that the difference
between the average values of GA and DE was not statistically signiﬁcant.
Overall, theresults suggest that bothalgorithms arerobust andcanperformwell when
tested with different cost functions. When constraints are involved, the DE algorithm

74
O. Gukhool et al.
may have a higher cost value than the GA algorithm, and this difference is statistically
signiﬁcant. However, when constraints are removed, the performance of both algorithms
are fairly similar, and the choice between them may depend on other factors such as
computation time and ease of implementation.
4
Conclusion
In conclusion, this study compared the efﬁcacy of GA and DE algorithms in optimizing
headways for LRVs with the goal of minimizing passenger waiting time and operational
costs for LRT systems. The results of this study show that both DE and GA algorithms,
when applied to the proposed optimization model, are effective in reducing the general-
ized costs of an LRT network. The optimization algorithms were successful in achieving
a decrease of 6.09% and 6.15% in the generalized cost of the LRT network, demonstrat-
ing the efﬁcacy of the model and optimization algorithms. The results also showed that
GA outperformed DE in ﬁnding the optimal solution, as evidenced by the lower mean
value of the best solution for GA in all tests. However, DE was found to be faster than
GA in completing the optimization process. Both algorithms were found to be robust
and able to perform well when tested with different cost functions.
Our ﬁndings differ from the research described at the start of this paper, since they
focused mainly on GA as the optimization algorithm for transit systems other than LRT.
Our study compared the performance of GA and DE algorithms in optimizing LRV
headways, which is a novel approach. Additionally, our study provides evidence that
GA outperforms DE in ﬁnding the optimal solution for LRV headways. Our study also
provides additional evidence that DE is a viable alternative to GA in this optimization
problem and may have advantages in terms of ﬁnding the optimal solution.
In general, our results indicate that the GA is more efﬁcient than DE for optimizing
LRV headways, particularly when considering cost function with constraints. However,
the selection of DE or GA may depend on additional factors like computation time and
implementation difﬁculty. Further studies could explore the efﬁcacy of these algorithms
in other optimization problems related to LRT systems.
Acknowledgement. We are grateful to the Higher Education Commission (Award Number TB-
2019-08) and all our collaborators, including the University of Mauritius, project investigators,
research assistants and training project assistants, Metro Express Ltd, Ministry of Land Transport
and Light Rail, Trafﬁc Management and Road Safety Unit, and Road Development Authorities
for their invaluable support of this research project.
References
1. Niu, H., Zhou, X., Gao, R.: Train scheduling for minimizing passenger waiting time with
time-dependent demand and skip-stop patterns: Nonlinear integer programming models with
linear constraints. Trans. Res. Part B: Methodol. 76, 117–135 (2015)
2. Besinovic, N., et al.: Artiﬁcial Intelligence in railway transport: taxonomy, regulations, and
applications. IEEE Trans. Intell. Transp. Syst. 23(9), 14011–14024 (2022)

Optimizing Headways Using Evolutionary Algorithms
75
3. Zhao, L., Chien, S.I.: Investigating the impact of stochastic vehicle arrivals to optimal stop
spacing and headway for a feeder bus route. J. Adv. Transp. 49(3), 341–357 (2014)
4. Thuan, N.Q., Thang, P.N.: A novel model for BRT scheduling problems. Int. J. Mach. Learn.
Comput. 9(4), 401–406 (2019)
5. Wang, J.Y.T., Yang, H., Verhoef, E.T.: Strategic interactions of bilateral monopoly on a private
highway. Netw. Spat. Econ. 4(2), 203–235 (2004)
6. Xiao, M., Chien, S., Hu, D.: Optimizing coordinated transfer with probabilistic vehicle arrivals
and passengers’ walking time. J. Adv. Transp. 50(8), 2306–2322 (2016)
7. Chen, X., et al.: Optimization of headways with stop-skipping control: a case study of bus
rapid transit system. J. Adv. Transp. 49(3), 385–401 (2014)
8. Abkowitz, M., Eiger, A., Engelstein, I.: Optimal control of headway variation on transit routes.
J. Adv. Transp. 20(1), 73–88 (1986)
9. Liang, S., Ma, M., He, S.: Multiobjective optimal formulations for bus ﬂeet size of public
transit under headway-based holding control. J. Adv. Transp. 2019, 1–14 (2019)
10. Schmaranzer, D., Braune, R., Doerner, K.F.: Multi-objective simulation optimization for
complex Urban mass rapid transit systems. Ann. Oper. Res. 305(1–2), 449–486 (2019)
11. Mutlu, M.M., Aksoy, ˙I.C., Alver, Y.: Transit frequency optimization in bi-modal networks
using differential evolution algorithm. Teknik Dergi (2022)
12. Qian, X., He, J., Liu, Y., Wu, C.: A differential evolution algorithm for multi-objective
optimization problems. Soft. Comput. 22(8), 2649–2660 (2018)
13. Das, S., Suganthan, P.N.: Differential evolution: a survey of the state-of-the-art. IEEE Trans.
Evol. Comput. 15(1), 4–31 (2011)
14. Bowman, L.A., Turnquist, M.: A: Service frequency, schedule reliability and passenger wait
times at transit stops. Trans. Res. Part A: Gen. 15(6), 465–471 (1981)
15. Storn, R., Price, K.: Differential evolution–a simple and efﬁcient heuristic for global
optimization over continuous spaces. J. Global Optim. 11(4), 341–359 (1997)
16. Heris, M.K.: Practical genetic algorithms in python and MATLAB–video tutorial. https://yar
piz.com/632/ypga191215-practical-genetic-algorithms-in-python-and-matlab. Yarpiz (2020)

Improving the Ergonomics of the Master Data
Management System Using Annotated
Metagraph
D. R. Nikolsky1(B), A. A. Sukhobokov1,2, and Goryachkin B.S.1
1 Bauman Moscow State Technical University, Baumanskaya 2-ya 5, 105005 Moscow, Russia
nikolsky.dan@gmail.com
2 SAP CBS, Dietmar-Hopp-Allee 16, 69190 Walldorf, Germany
Abstract. The article highlights the importance of human-oriented design of
information systems. A data model is proposed and considered for the asset mas-
ter data management system in the form of a multilayer graph forest based on the
nature of the relationships between enterprise assets. The structure of this data
model is described. An equivalent replacement of this data model with an anno-
tated metagraph is proposed. A typical analysis problem solved by the user of such
a system is highlighted and ergonomic problems that need to be solved are stated.
A solution to these problems is proposed using the entity-resolution algorithm and
a decision is made about the ergonomic failure of this solution. An approach is
proposed to modernize the user interface by replacing a multilayer graph forest
with an annotated metagraph. The ergonomic improvement of the user interface
and its positive impact on usability are described.
Keywords: Ergonomics · Human-oriented design · Master data management ·
Multilayer graph forest · Annotated metagraph
1
Introduction
With the transition to comprehensive automation of production, the role of humans as the
subject of labor and management has increased. Humans are responsible for the efﬁcient
operation of the entire technical system, making human-centered design of interactive
systems especially important [1].
Ergonomics, speciﬁcally microergonomics, deals with the study and design of man-
machine systems. It solves the problem of optimizing human functioning in the control
circuit of automated information processing and control systems. This includes creating
optimalworkingconditionsthatincreaseproductivity,provideconvenience,andpreserve
the strength, health, and efﬁciency of humans.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 76–83, 2024.
https://doi.org/10.1007/978-3-031-50158-6_8

Improving the Ergonomics of the Master Data Management
77
2
Information Model of the Proposed System, Description
of the Subject Area
Master Data Management (MDM) is a set of best practices for managing data that helps
key stakeholders, participants, and business customers implement business applications,
information management practices, and data management tools. These practices, poli-
cies, procedures, services, and infrastructure support the collection, integration, and
sharing of accurate, timely, consistent, and complete master data [2, 3]. In these sys-
tems, humans directly interact with data to analyze, interpret, and reveal hidden patterns.
In this case, the level of organization of the considered ergatic system is the third - the
system provides the energy and information functions, and the person is the controlling
one.
The tasks for which master data management systems are used deﬁne the possible
mappings of objects with which businesses interact and their relationships, including
hierarchical ones [4]. All of an enterprise’s assets are reﬂected in different areas of the
enterprise’s internal operations and accounting. One such area that reﬂects a holistic
view of data from the perspective of one or more interrelated applications is called a
perspective. An example of this relationship can be observed in Fig. 1.
This means that all information about assets can be represented as a large, complex
graph, where the nodes are the enterprise’s assets (i.e., their representations in different
perspectives), and the edges are the relationships between their representations [5].
Fig. 1. Example of a lattice of inter-perspective connections

78
D. R. Nikolsky et al.
Each perspective is described by a hierarchical structure, and the structure of the
hierarchy itself can vary signiﬁcantly from one perspective to another. The graph is a set
of tree perspectives, which represent the enterprise assets in different ways. An example
hierarchy is shown in Fig. 2.
Fig. 2. Example of hierarchical relationships in the perspective
Data hierarchies are very important for MDM-systems, because they provide trans-
parency in the structure of company assets and allow you to clarify the relationship
between the master data.
Data hierarchies are crucial for MDM systems because they provide transparency
in the structure of company assets and clarify the relationship between master data.
Connecting the perspectives is necessary to create a uniﬁed, holistic view of the master
data assets. In this case, an enterprise asset is represented by a grid of nodes in different
perspectives. Therefore, the proposed graph is also a set of rings (lattices) between nodes
from different perspectives, representing the enterprise assets. An example of such links
is shown in the previously discussed Fig. 1.
A similar data structure, a multilayer graph forest, can also be represented as an
annotated metagraph [6]. Nodes of different perspectives, which are connected by lattices
and represent nodes meaning one object, can be enclosed in one meta-vertex to avoid
data redundancy in the system. An example of semantically equivalent data structures
is shown in Fig. 3.
3
Problem Statement
The volume of perception of the human visual analyzer is determined by the number
of objects a person can grasp and remember during one visual ﬁxation. When unrelated
objects are presented, the volume of visual perception is limited to 4–8 elements. Note

Improving the Ergonomics of the Master Data Management
79
Fig. 3. On the left is a graph structure of data with a lattice of inter-route connections. On the
right is an equivalent metagraph.
that the amount of information that can be reproduced is determined not by the amount
of perception, but by the amount of memory. A visual image may reﬂect a larger number
of objects, but they cannot be reproduced due to limited memory capacity. Therefore,
it is more important to consider memory capacity than perception capacity for practical
purposes. To work efﬁciently, it is necessary that no more than 6 ± 2 elements fall into
the central ﬁeld of vision, limited by an angle of 4–10° for operators [7].
In any system involving an operator, several fundamental principles must be
followed. Three of them are of interest in this situation:
1. The principle of maximum understanding. The system must provide full support to
the person, with information that does not require interpretation or recoding.
2. The principle of the minimum amount of users’ working memory. A person should
remember as little as possible.
3. The principle of optimal loading. It recommends a distribution of functions in which
the operator would not experience starvation (loss of activity) or overload (missing
signals) at the rate of incoming data.
In the proposed MDM-system, when analyzing objects on the screen, the user solves
the problem of reduction - identifying the necessary nodes in the graph related to one
object in the real world and mentally combining them to highlight the object.
Figure 4 shows the layout of the master data management system’s object view screen
interface as seen by the user. As the number of inter-view links increases, the lattice of
nodes corresponding to one real-world entity creates difﬁculties for perception.
4
Proposed Solution
To ensure high-quality software, the dialog stages must conform to the goal of suc-
cessfully executing a production task. The dialog should only include necessary stages
without which it is impossible to solve the task. Stages that include actions more appro-
priate for automatic execution by the system must be excluded [8]. To facilitate the user’s
task of analyzing objects on the screen, a union of nodes corresponding to a single real-
world object must be constructed into a special set that reﬂects the object. Figure 5 shows
the resulting screen layout. Instead of analyzing multiple nodes and graph edges, the

80
D. R. Nikolsky et al.
Fig. 4. The mockup of the data display area of the screen with highlighted inter-perspective
connections, reﬂecting the correspondence of nodes from different perspectives to the same object
in the real world.
user only needs to perceive one object displayed by an ellipse, which corresponds to one
real-world entity and includes all nodes displaying this object on different perspectives
of master data.
To search for such objects in the entire data set, the proposed algorithm works as
follows:
1. Iterate over every node in all hierarchies.
2. For each node, view and trace its inter-perspective connections and the rings of
connections formed by them during traversal.
3. Mark nodes of different perspectives enclosed in one or more rings as one entity.
4. Combine the intersecting and nested sets of nodes found in the previous step into a
single entity. The set of such objects forms the user representation shown in Fig. 5.
This algorithm allows the user to perform their tasks by creating an aggregated
representation of the data. However, due to the nature of the graph structure, the algorithm
must be executed every time the user requests the data for display. This is because the
graph structure doesn’t allow for storing pre-built indexes. As a result, the principle of
optimal loading will be violated, and the user will have to wait for the algorithm to ﬁnish
execution to get the data presentation.
To solve this problem, the data structure itself could be modernized. The intro-
duction of an annotated metagraph as a data structure storing information about assets
accomplishes several goals simultaneously:

Improving the Ergonomics of the Master Data Management
81
Fig. 5. Layout of the data display area, modiﬁed by bundling nodes.
1. The Real Object Index is built initially, which provides users with an ergonomically
correct display of asset data without the need for special processing.
2. The size and volume of the data are reduced to lower the likelihood of errors and
incorrectness. This is done by excluding individual inter-perspective links and replac-
ing them with meta-vertices. An annotated metagraph is chosen from various types
of metagraphs because of the presence of meta-vertices, which may include other
vertices and metavertices as their internal elements.
The choice of an annotated metagraph from various types of metagraphs is due to
the presence of metavertices, which may include other vertices and metavertices as their
internal elements.
If asset information is presented using an annotated metagraph, the user will see
the screen layout shown in Fig. 6. It can be seen that the inter-perspective connections
that take away the user’s focus are abolished and replaced by meta-vertices. In this
case, the semantic completeness of the data is not lost because the user can highlight
both individual objects of the real world in the system’s information ﬁeld and speciﬁc
representations of these objects in different perspectives of the enterprise’s business
activities.

82
D. R. Nikolsky et al.
Fig. 6. Layout of the data display area. The system uses a metagraph as a data structure to store
asset information.
5
Conclusion
Using an annotated metagraph-based data model to represent master data on assets does
not affect user performance because the semantic correctness of the stored and displayed
data is not changed. At the same time, this model provides the following advantages:
1. The efﬁciency of user activity is increased by reducing the time and mental resources
spent by the user to solve the problem.
2. User satisfaction increases by reducing mental overload and removing the task of
entity resolution from the user.
A change in these indicators leads to a cumulative improvement in the usability of
the system [9].
References
1. ISO 9241-210. https://www.iso.org/standard/77520.html. Last accessed 10 Feb 2023
2. Loshin, D.: Master data management. Morgan Kaufmann (2010)
3. Talburt J., Zhou Y.: Entity information life cycle for big data: Master data management and
information integration. Morgan Kaufmann (2015)
4. Ng, S.T., Xu, F.J., Yang, Y., Lu, M.: A master data management solution to unlock the value
of big infrastructure data for smart, sustainable and resilient city planning. Proc Eng 196,
939–947 (2017)
5. Sukhobokov, A.A., Strogonova, V.I.: On an approach to construct asset master data
management system. Softw Syst 30(1), 51–60 (2017)

Improving the Ergonomics of the Master Data Management
83
6. Gapanyuk, Y.: The development of the metagraph data and knowledge model. In: Selected
contributions to the 10th international conference on “integrated models and soft computing
in artiﬁcial intelligence (IMSC-2021). CEUR WORKSHOP PROCEEDINGS, vol. 2965,
pp. 1–7 (2021)
7. Goryachkin, B.S.: Ergonomic passport of an automated system for processing and displaying
information and control. Int Res J 9–2(51), 25–29 (2016)
8. Goryachkin, B.S., Umryadev, D.T.: The role of software ergonomics standards in the analysis,
design and evaluation of information systems software. Trends Develop Sci Educ 73–3, 153–
161 (2021)
9. ISO 9241–11. https://www.iso.org/standard/63500.html. Last accessed 27 Jan 2023
10. Intelligent Computing & Optimization, Conference proceedings ICO 2018, Springer, Cham,
ISBN 978-3-030-00978-6
11. Intelligent Computing and Optimization Proceedings of the 3rd International Conference on
Intelligent Computing and Optimization 2020 (ICO 2020)
12. Intelligent Computing & Optimization Proceedings of the 4th International Conference on
Intelligent Computing and Optimization 2021 (ICO2021)
13. Intelligent Computing & Optimization Proceedings of the 5th International Conference on
Intelligent Computing and Optimization 2022 (ICO2022)
14. Special Issue. https://link.springer.com/journal/40305/volumes-and-issues/10-4

Patent Classiﬁcation for Business Strategy
with BERT
Masaki Higashi1(B), Yoshimasa Utsumi2, and Kazuhide Nakata1
1 Tokyo Institute of Technology, Tokyo, Japan
higashi.m.ac@m.titech.ac.jp, remove@gmail.com
2 Rakuten Group, Inc., Tokyo, Japan
remove@gmail.com
Abstract. Recent developments in deep learning have greatly improved the accu-
racy of various natural language processing tasks. Patent classiﬁcation using patent
documents has also seen a signiﬁcant improvement in accuracy based on the inter-
nationallydeﬁnedIPCaswellastheregion-speciﬁcCPCpredictionmethods.Such
an automated patent classiﬁcation capability can reduce the burden on examiners
and improve efﬁciency. It can also be used to organize patents, such as in prior art
searches. However, such a classiﬁcation cannot be used to support the formula-
tion of management strategies, such as capturing the ﬁelds in which the predicted
patent classiﬁcation will be used in the management of a company. To this end, this
study aims to classify patents based on technological ﬁeld instead of the existing
IPC classiﬁcation such that the developed automated patent classiﬁcation is useful
for companies. To elaborate, this study investigates the differences between IPC
and the proposed classiﬁcation based on technological ﬁeld, selects documents to
be used, and proposes a classiﬁcation model using IPC information. The proposed
classiﬁcation model is comprised of two distinct components, namely a model for
document input and another for IPC input. BERT is employed for the document
input model, while skip-connections are used for the IPC input model. Finally,
the improvement in accuracy is examined compared to existing IPC using actual
data; moreover, solutions are proposed to the problems identiﬁed in this study. As
a result, our proposed model signiﬁcantly improves precision, recall, F1, and AP
over existing models.
Keywords: Deep learning · BERT · Patent classiﬁcation · Graph embedding
1
Introduction
Withthedevelopmentofdeeplearning,theaccuracyofmachinelearningusingdocument
data has been greatly improved. In particular, the Transformer model [12] has contributed
greatlytoimprovingtheaccuracyofvarioustasksregardingdocumentdata.Furthermore,
the emergence of BERT [2], which performs pre-training using the Transformer model,
has greatly improved the accuracy of natural language processing tasks, allowing real-
world applications.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 84–94, 2024.
https://doi.org/10.1007/978-3-031-50158-6_9

Patent Classiﬁcation for Business Strategy with BERT
85
Currently, the main patent classiﬁcation methods include the internationally deﬁned
IPC and the region-speciﬁc CPC. Since these are assigned manually by humans, cost and
human error are major issues. Therefore, deep learning with patent documents is used
to improve the accuracy of patent classiﬁcation. For example, Patent BERT, which uses
BERT as described earlier, has succeeded in improving the accuracy beyond aforemen-
tioned methods using deep learning. However, most of the current patent classiﬁcation
methods focus on IPC that can be used to organize patents but cannot be used for manage-
ment strategies of companies that own patents or are going to use patents. This is because
the IPC is classiﬁed into categories in which the uses and characteristics of patents are
brieﬂy described, and many speciﬁc uses are possible from a single IPC even if they
are applied to management strategies or services as they are; for example, “G08G 5/00”
mean “trafﬁc control system for aircraft”. It is difﬁcult to consider speciﬁc management
strategies and services from the IPC alone, because “trafﬁc control system for aircraft”
has various possible applications, such as “automatic piloting”, “air route prediction”,
and so on. Therefore, we currently use a search formula that combines information on
the type of IPC that a patent has, and the type of words that comprise the text of the
patent, and the type of word combinations used, to determine whether a patent is useful
for a certain business or not. However, since these are also performed manually on a
rule-based basis, cost, human error, and omissions are major issues, as in the case of
IPC. Therefore, patents must not only be classiﬁed by the IPC but also by more detailed
categories, such as “electronic money”, and “e-books”.
In this paper, a dataset of categories that can be used for corporate management
strategies is prepared using a search formula, and the dataset is applied to an existing
method, Patent BERT with “Abstract”. Moreover the “Detailed Description of Invention”
section of a patent and IPC information are incorporated into the learning model as a
more detailed description than “Abstract” and compared to Patent BERT with “Abstract”.
However, two problems emerge. First, the detailed description of the invention is long
and includes many contents that are not relevant to classiﬁcation. Second, the IPC is
assigned only as a number and has no speciﬁc features. Therefore, a rule-based method
is proposed for extracting the necessary contents from the detailed description of the
invention, and a method is developed for creating IPC features using a graph structure
that represents the IPC sharing relationship among patents. Finally, a model is proposed
that combines these methods with summary sentences, and the model is then validated
through numerical experiments.
2
About Patents
In this section, patents are explained and discussed. First, the IPC is explained, followed
by patent documents component.
2.1
IPC
When a patent is granted, it is assigned an IPC according to its ﬁeld. IPC is an inter-
nationally uniﬁed classiﬁcation, which consists of four elements: “Section”, “Class”,
“Subclass”, and “Main Group/Subgroup”. For example, about “G08G5/00”, “G” in

86
M. Higashi et al.
“Section” means “Physics” and “G08” in “Class” means “Signal”. The “subclass” of
“G08G” means “Trafﬁc control system”, and “Sub group/Main group” of “G08G5/00”
means “Trafﬁc control system for aircraft”.
2.2
Patent Document
The patent document is composed of many elements. In particular,“Abstract”, “Claim”,
“Detailed Description of Invention” are the texts containing the description of the patent;
moreover, they are important parts of the patent classiﬁcation using documents. Among
them,the“DetailedDescriptionofInvention”isatextitemthatcontainsalargeamountof
information about the patent, and is mainly composed of “Technical ﬁeld”, “Background
technology”, “Prior art document”, “Summary of invention”, and “Brief description
of ﬁgure”. The “Summary of invention" is the section that describes the patent and
comprises “Problem to be solved by the invention", “Means to solve the problem",
and “Effects of the invention". The “Problem to be solved by the invention" text item
describes the motivation and purpose for developing the patent. Moreover, the “Means
to solve the problem" text item describes the proposed approach to achieve the purpose.
Finally, the “Effects of the invention" text item describes the implications of the patent.
As described in this section, the “Detailed Description of Invention” section provides
multiple details of the patent.
3
Related Research
As described in the previous section, most of the current patent classiﬁcations predict
the categories of IPC and other patent classiﬁcation notations. Example models include
Support Vector Machines (SVM) [1, 10] and k-Nearest Neighbor (KNN) [1] models.
In addition, there exist models that utilize deep learning techniques, including those
employing long-short-term memory (LSTM) [7], convolutional neural networks (CNN)
[6], and Bidirectional Encoder Representations from Transformers (BERT) [5] models.
Therefore, in this section, BERT model is ﬁrst introduced that predict IPC, etc.; next,
a model is introduced that classiﬁes patents into categories other than IPC. Finally, the
relevance of these models to this study is described.
Lee et al. [5] proposed PatentBERT. PatentBERT uses the USPTO-2M dataset and
the USPTO-3M dataset, which contains 3,050,615 patents with 632 and 656 subclasses
for IPC and CPC, respectively. Compared to DeepPatent [6], the accuracy was improved
in precision@1 and F1@5. A comparison of the predictability of IPC and CPC using
claims shows that CPC is more predictable with regards to precision@1, recall@1,
and F1@1. The model proposed by Choi et al. [11] is different from the aforemen-
tioned study in that it does not predict IPC, but rather predicts whether the technology
under development to avoid patent infringement belongs to a category created by the
authors. The language model is Transformer, and the summary sentences are inputted
by using word2vec [8] for word embedding. Moreover, features are created using the
co-occurrence graphs of IPC, CPC, and USPG and diff2vec [9]. Finally, the output of the
Transformer and the created features are used to make predictions. The four categories
are “Marine Plant Using Augmented Reality Technology (MPUART)”, “Technology for

Patent Classiﬁcation for Business Strategy with BERT
87
1MW Dual Frequency System (1MWDFS)”, “Technology for Micro Radar Rain Gauge
(MRRG)”, and “Technology for Geostationary Orbit Complex Satellite (GOCS)”. The
datasets are retrieved from the USPTO by extracting important keywords from each
category. Compared to PatentBERT, “MPUART” and “MRRG” show a decrease in pre-
cision but an improvement in recall, F1, and AP. However, “1MWDFS” decreases in
precision, recall, F1, and AP, and “GOCS” improves in recall but not in precision, F1,
and AP.
In this study, IPC features are ﬁrst extracted with reference to Choi et al. [11] and then
compared with those obtained from PatentBERT. In addition, while previous studies used
the abstract section of a patent, such as summary sentences, this study uses the “Detailed
Description of the Invention” section to distinguish patents more clearly.
4
Proposed Method
In this section, ﬁrst, a method is proposed for extracting sentences of a length that can be
inputted into BERT on a rule basis from the “Detailed Description of Invention" section
of Japanese patent documents. Next, the IPC feature extraction method is described.
Lastly, the proposed model based on BERT and the problems of the proposed model are
described.
4.1
Extraction of Documents from Detailed Description Items
As mentioned in Sect. 3, in the conventional methods, the title, abstract, and claims
are used as features of patent documents. However, the categories used in the proposed
classiﬁcation are more detailed than the conventional IPC. As an example, consider the
application number of “Patent Application 2020-524926” that has an IPC of “G08G
5/00”, which implies “Trafﬁc control system for aircraft”. The practical applications of
the “Trafﬁc control system for aircraft” classiﬁcation may include various tasks such
as “autopilot”, and “air route prediction”. The technical ﬁeld assigned to this patent
in this research is “logistics means (drone/unmanned delivery)”, which is more practi-
cal and detailed than the IPC. This indicates that a patent must be distinguished from
other patents more clearly using sentences that include more concrete contents, rather
than abstract documents such as titles, abstracts, and claims of existing methods. There-
fore, the “Detailed Description of Invention” section of a patent is focused upon in this
study. However, note that this section does comprise contents unrelated to the present
classiﬁcation at times, and the lengths of the sentences themselves are very long.
Step 1 : Use the items “Problem to be solved by the invention” and “Means for
solving the problem” in the detailed description of the invention.
Step 2 : If none of the above items in 1 are applicable, use those summarized in
“Summary of Invention”, “Overview”, and “Invention Disclosure”.
Step 3 : If none of the items in 2 are applicable, a document in which the items after
"Brief Description of Drawings" are deleted is used.
First, the items in the detailed description of the invention that describe the character-
istics of the patent in detail are “Problem to be solved by the invention” and “Means for
solving the problem”. However, not all patents have these sections, and many patents that

88
M. Higashi et al.
do not have these items are described using the “Summary of Invention”, “Overview”,
and “Invention Disclosure” items, which are then used. Finally, for documents without
the “Summary of Invention”, “Overview”, and “Invention Disclosure” items, the docu-
ments with the items after “Brief Description of Drawings” deleted are used to excludes
sentences that do not describe the patent.
4.2
IPC Feature Generation Methods
In conventional patent classiﬁcation, IPC information cannot be used because the task
is to predict the IPC. However, in this study, since classiﬁcation is performed based
on categories other than IPC, IPC can be used. Therefore, IPC features are generated
using Node2vec [3] with reference to the co-occurrence graph proposed by Choi et al.
[11]. A co-occurrence graph is a graph in which nodes that co-occur with each other
are connected by edges. For example, if a patent A exists with IPCs G06F3/06 and
G06F13/14 as well as a patent B with IPCs G06F3/06, G06F13/14, and G08G 5/00, then
an edge of weight 1 exists between nodes G06F3/06 and G08G5/00; nodes G06F13/14
and G08G 5/00; and nodes G06F3/06 and G06F13/14. Moreover, an edge of weight
2 exists between nodes G06F3/06 and G06F13/14. Next, the features of each IPC are
obtained from the co-occurrence graph using Node2vec. Finally, the IPC features are
averaged and inputted into the model as the IPC features of the patent.
4.3
Model Structure
The model proposed in this section describes the overall picture of a language model
that inputs the abstract text of a patent and a detailed description of the invention, of
an IPC model that inputs IPC features, and of a model that combines them. This task
is considered to be a multi-label classiﬁcation because a patent may belong to more
than one category. Therefore, the loss function is Binary Cross Entropy. The language
model uses BERT. The input document is “sent”, and the number of categories is C.
Moreover, the linear transformation parameters are W ∈R768×C, and the label attached
to one-hot for each category is y ∈{0, 1}C. When yi, ˆyi is the i-th element of y, ˆy, the
model structure is represented by Eqs. (1)–(3):
x = BERT(sent)
(1)
ˆy = sigmoid(Wx)
(2)
Loss = −

i∈Z,i∈[1,C]

yi log ˆyi + (1 −yi) log(1 −ˆyi)

(3)
First, documents are inputted to BERT using Eq. (1), and the number of dimensions
of the features is adjusted to the number of categories using a linear transformation
in Eq. (2). Subsequently, the documents are passed through a sigmoid function to set
the output to (0, 1). Finally, the loss value is outputted using Binary Cross Entropy, as
represented in Eq. (3).

Patent Classiﬁcation for Business Strategy with BERT
89
The model with both the abstract and the detailed description of the invention is
expressed in Eqs. (4)–(6):
x = BERTabst(abst) + BERTdetail(detail)
(4)
ˆy = sigmoid(Wx)
(5)
Loss = −

i∈Z,i∈[1,C]

yi log ˆyi + (1 −yi) log(1 −ˆyi)

(6)
The summary text is inputted as “abst”. Additionally, the detailed descriptions are
inputted as “detail”, and the other variables are used in the same way as in Eq. (1).
First, the summary and detailed descriptions are entered into different BERTs and then
added together using Eq. (4). Next, as in Eq. (5), linear transformations are used to match
the number of dimensions of the features to the number of categories. The output is then
passed through the sigmoid function to obtain (0, 1). Finally, the loss value is outputted
using Binary Cross Entropy.
The IPC model is expressed in Eqs. (7)–(9):
xj
ipc = W j
2(ReLu(Wj
1xj−1
ipc )) + xj−1
ipc
(7)
ˆy = sigmoid(Wipcxn
ipc)
(8)
Loss = −

i∈Z,i∈[1,C]

yi log ˆyi + (1 −yi) log(1 −ˆyi)

(9)
For x0
ipc using the IPC features generated in Sect. 152.2, the linear transformation param-
eters are Wj
1 ∈Rd×d, Wj
2 ∈Rd×d, and Wipc ∈Rd×C. Moreover, n is the number of
layers in Eq. (7). The other variables are used in the same way as in Eqs. (1)–(3). Note
that d is the number of dimensions of the IPC features generated in Sect. 152.2, and j
is the number of times that Eq. (7) is repeated. Furthermore, the relationship 1 ≤j ≤n
holds true. First, the IPC features generated in Sect. 152.2 are linearly transformed with
W0
1 using Eq. (7) when j = 1. Next, they are passed through the activation function
ReLu function and then linearly transformed with W0
2. Subsequently, since no infor-
mation must be lost as the layers get deeper, skip-connection [4] is used to add the
before and after transformations using Eq. (7) when j = 1. After this step is repeated
n times, a linear transformation is used, as in Eq. (8), to match the dimensionality of
the features to the number of categories. The output is then passed through the sigmoid
function to set the output to (0, 1). Finally, the loss value is outputted using Binary
Cross Entropy. Finally, a model that combines IPC and documents is described. First,
on the document model side, when either the abstract or detailed description is used,
the document feature obtained by Eq. (1) is used; moreover, when both the abstract and
detailed description are used, the document feature obtained by Eqs. (4)–(6) is used. The
IPC features obtained by n iterations of Eq. (7) are used as the input for the IPC model.
Next, a linear transformation is applied to the concatenated documents and IPC features

90
M. Higashi et al.
to match the dimension of the features to the number of categories, and the output is
passed through a sigmoid function to set the output to (0, 1). Finally, the loss values
are outputted using Binary Cross Entropy. Figure 1 shows the combined document and
IPC model when using both the abstract and detailed descriptions. The model is used
to provide a detailed description and abstract; a comparison of the two combined; and a
comparison of the two with and without IPC.
Fig. 1. Model with IPC and patent documents
4.4
Problems in the Model and Developed Solutions
Choi et al. [11] reported that accuracy is better when IPC is combined with abstract
sentences than when abstract sentences are used alone. However, when the accuracy of
the model with IPC and abstract sentences as input is compared to the model with the
number of layers of the IPC model n = 1 as input, in Sect. 5, the former model showcases
no improvement in accuracy compared to the latter model, even though IPC is used as
an additional input. First, the major difference between the existing studies [11] and the
proposed model are considered. The existing studies used a single-layer Transformer
Encoderasthelanguagemodel,whileourmodelusesaBERT-basedmodelwith12layers
of Transformer Encoders. Therefore, the combination of the IPC and BERT models is
considered to be problematic, and the learning speed of each model is investigated.
Figure 2 shows the results of this study; note that a large discrepancy exists in the speed
of convergence of the learning by the BERT and IPC models. In this situation, BERT
model converges ﬁrst and overlearning occurs, resulting in a convergence of the entire
model without a convergence of the IPC model. This diminishes the beneﬁt of including
IPC information. However, since BERT is pre-trained on large data sets, the model itself
changes if only one layer of the transformer is used, as in existing studies, and the
pre-training becomes meaningless. Therefore, the learning speed of the IPC model is
increased to make it closer to that of BERT by deepening the layers of the IPC model.By
increasing the number of layers n in the IPC model described in Sect. 152.3, the learning
speed increases; moreover, a comparison is shown in Fig. 3. The learning speed of the
IPC model approaches that of the BERT model when the number of layers is n = 60,
thus beneﬁtting from the input of IPC information. A quantitative evaluation of accuracy
is described in Sect. 5.

Patent Classiﬁcation for Business Strategy with BERT
91
Fig. 2. Learning curve of BERT and IPC model
Fig. 3. Learning curve of IPC models
5
Experimental Results
This section compares the accuracy with and without the detailed description and abstract
sections of a patent as well as discusses the results. Next, the changes in the accuracy
with and without IPC are discussed. Finally, the changes in accuracy with and without
the solution described in Sect. 152.4 are discussed.
5.1
Experimental Setup
A total number of labels is 95 and 8160 patents are assigned multiple labels. The experi-
ments are conducted using training, validation, and test data divided in the ratio of 6:2:2,
respectively. The BERT model was ﬁne-tuned, and the parameters of the IPC model
were updated using the training data. Subsequently, the validation data were used for
inference, and the training process was terminated if the loss value did not show any
improvement for ﬁve consecutive epochs. Finally, the evaluation value was calculated
by performing inference on the test data.
For the hyperparameters of the model, Node2vec is ﬁxed as dimensions = 128, walk
length = 10, num walks = 10, p = 0.5, q = 2, and window = 3. For the number of
layers n of the IPC model, n = 60 is used for both IPC and documents, as described
in Sect. 152.4. Furthermore, a pre-trained BERT base model (number of Transformer
blocks = 12, hidden layer size = 768, and number of Multi-Head-Attention heads =
12) provided by Tohoku University is used. For the other hyperparameters, a batch size
of 32 and a learning rate of 2e-5 are used. We use precision, recall, F1, and AP as the
evaluation indices.

92
M. Higashi et al.
5.2
Comparison of the Abstract and Detailed Description
Table 1 lists the accuracy of the abstract, the detailed description, and both documents
together. Note that the accuracy of the summary is about 0.02 higher than that of the
detailed description in all evaluation indices. Moreover, the use of both the detailed
description and abstract improves the accuracy by 0.03∼0.04 compared to classiﬁcation
using either the detailed description or abstract. In other words, the abstract contains
more information than the detailed description, but adding the information from the
detailed description to the case of using only the abstract makes the distinction between
patents more accurate.
Table 1. Accuracy ratings
Precision
Recall
F1
AP
Abstract
0.620
0.612
0.616
0.630
Detailed description
0.604
0.593
0.598
0.613
Abstract & Detailed description
0.657
0.642
0.649
0.676
5.3
Comparison Based on the IPC and Number of Layers
Table 2 lists the experimental results for the IPC model with n = 1 and n = 60 layers.
Tables 1 and 2 summarize that the accuracy appears to improve with IPC (n = 1) when
the detailed description is used, but not for the other models. Tables 1 and 2 summarizes
that for abstract, the inclusion of IPC (n = 60) improves precision, recall, and F1 by 0.03
as well as AP by about 0.06. For the detailed description, IPC (n = 60) improves the four
evaluation indices by 0.05∼0.06; moreover, for the combination of abstract and detailed
description, IPC (n = 60) improves precision, recall, and F1 by about 0.01 as well as
AP by about 0.03. Thus, when the number of layers n = 1, the accuracy does not change
signiﬁcantly even when IPC is incorporated. However, by bringing the learning speeds of
BERT and IPC models closer together, the accuracy signiﬁcantly increases with regards
to precision, recall, F1, and AP whether the summary and detailed description are by
themselves or combined. In other words, when two models with different learning speeds
are combined, the models will not be able to learn well if the learning speeds are not
aligned.
6
Conclusion and Future Work
To classify patent documents based on technology for the purpose of utilizing them in
management strategies, this study improves the existing Patent BERT method from the
three viewof documents used, IPC feature generation, and model combination.
First, the accuracy of classiﬁcation is successfully improved by incorporating the
detailed description section of a patent compared to the existing method of using only

Patent Classiﬁcation for Business Strategy with BERT
93
Table 2. Accuracy evaluation of IPC model with n = 1 and n = 60 layers
n = 1
n = 60
Precision
Recall
F1
AP
Precision
Recall
F1
AP
IPC & abstract
0.625
0.619
0.622
0.633
0.655
0.644
0.649
0.690
IPC & detailed
description
0.639
0.629
0.634
0.622
0.662
0.647
0.655
0.677
IPC, abstract &
detailed
description
0.654
0.645
0.649
0.682
0.670
0.654
0.661
0.705
the abstract. Furthermore, IPC information is demonstrably extracted by creating an IPC
co-occurrence graph and generating features using Node2vec. In addition, this study
demonstrates that combining a model with a large learning convergence speed, such
as BERT, with a model having a small convergence speed does not improve accuracy
signiﬁcantly; however, the accuracy improves signiﬁcantly by bringing the convergence
speeds closer together.
Three issues need to be addressed in the future. The ﬁrst is the proportion of labels.
The data acquired are generally unbalanced, with a large number of certain labels
assigned to a large number of datasets, and a small number of labels assigned to a
small number of datasets. This may result in a good prediction for the former label, but
not for the latter. However, since labeling is done manually, expanding the dataset is
difﬁcult. Therefore, a learning method such as semi-supervised learning must be con-
sidered using this model. The second is the use of IPCs. While the IPCs used in this
experiment are narrowed down in advance by the search formula, the feature values of a
large number of IPCs must be obtained if these IPCs are to be applied to a wide range of
ﬁelds. In addition, the use of a large number of IPCs may cause noise in the information,
making it impossible to extract the information properly. Therefore, experiments must
be conducted with more data to see if the proposed model can be adapted to a large
number of IPCs, and the approach for extracting features in such scenarios must be
investigated. The third is to make better use of the graph structure of patents. Currently,
research is being conducted to extract features from the graph structure of papers using
Graph Convolution Neural Networks. By applying this method to patents as well, a more
accurate classiﬁcation can be achieved.
References
1. Benzineb, K., Guyot, J.: Automated patent classiﬁcation. Curr. Challenges Pat. Inf. Retr. 29,
239–261 (2011)
2. Devlin, J., Chang, M.-W., Lee, K., Toutanova, K.: BERT: Pre-training of deep bidirectional
transformers for language understanding. In: TProceedings of the 2019 Conference of the
North American Chapter of the Association for Computational Linguistics: Human Language
Technologies, vol. 1, pp. 4171–4186 (2019)

94
M. Higashi et al.
3. Grover, A., Leskovec, J.: node2vec: scalable feature learning for networks. In: Proceedings
of the 22nd ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining, pp. 855–864 (2016)
4. Kaiming, H., Xiangyu, Z., Shaoqing, R., Jian S.: deep residual learning for image recognition.
Comput. Vis. Patt. Recog. 770–778 (2016)
5. Lee, J.-S., Hsiang, J.: PatentBERT: Patent classiﬁcation with ﬁne-tuning a pre-trained BERT
model. World Pat. Inf. 61, 101965 (2020)
6. Li, S., Hu, J., Cui, Y., Hu, J.: DeepPatent: patent classiﬁcation with convolutional neural
networks and word embedding. Scientometrics 117(2), 721–744 (2018)
7. Marawan, S., Jan S., Matthias, S., Stephan, G.: An LSTM approach to patent classiﬁcation
based on ﬁxed hierarchy vectors. In: SIAM International Conference on Data Mining 495–503
(2018)
8. Mikolov, T., Sutskever, I., Chen, K., Corrado, G., Dean, J.: Distributed representations of
words and phrases and their compositionality. Neural Inf. Process. Syst. 26, 3111–3119 (2013)
9. Rozemberczki, B., Sarkar, R.: Fast sequence-based embedding with diffusion graphs. Int.
Workshop Complex Netw. 99–107 (2018)
10. Chih-Hung, W., Yun, K., Tao, H.: Patent classiﬁcation system using a new hybrid genetic
algorithm support vector machine. Appl. Soft Comput. 10, 1164–1177 (2010)
11. Choi, S., Lee, H., Park, E., Choi, S.: Deep learning for patent landscaping using transformer
and graph embedding. Technol. Forecast. Soc. Change 175, 121413 (2022)
12. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł.,
Polosukhin, I.: Attention is all you need. Adv. Neural Inf. Process. Syst. 30 (2017)

GIS Based Flood Hazard and Risk Assessment
Using Multi Criteria Decision
Making Approach in Rapti River Watershed,
India
Raashid Khan1, Jawed Anwar1, Saif said1, Sarfarazali Ansari1,
Azazkhan Ibrahimkhan Pathan2(B), and Lariyah Mohd Sidek3
1 Department of Civil Engineering, Zakir Husain College of Engineering and Technology,
Aligarh Muslim University, Aligarh, India
mdraashidkhan9@gmail.com
2 Indian Institute of Technology Gandhinagar, Gandhinagar, Gujarat, India
pathanazaz02@gmail.com
3 Institute of Energy Infrastructure, Civil Engineering Department, College of Engineering,
Universiti Tenaga Nasional, (The Energy University) UNITEN, Kajang, Malaysia
Abstract. The ﬂood is a catastrophic event that causes losses in life and prop-
erty. The magnitude of food-related losses has prompted researchers to place a
greater emphasis on robust and comprehensive modelling techniques for mitigat-
ing food damage. Recently developed multi-criteria decision making (MCDM)
techniques are extensively used to make decision-making processes more col-
laborative, logical, and efﬁcient. The Rapti River is among the most ﬂood-prone
rivers in North-Eastern Uttar Pradesh, where numerous villages and towns are
annually inundated by monsoon ﬂooding. The present study is undertaken with an
aim to identify and map areas of ﬂood hazard and risk in the Rapti River watershed
by employing MCDM based analytical hierarchy process (AHP) approach within
GIS interface. Flood hazard analysis was carried out by considering eight hazard
indicators namely Height above nearest drainage, distance from river, elevation,
land use land cover, slope, soil type, drainage density and rainfall. The analysis
of ﬂood risk was performed by employing ﬂood hazard layer, population density,
and land use land cover as risk indicators. A weighed overlay approach was imple-
mented to prepare ﬂood hazard and risk maps of the study area. The results of the
analysis showed that around 57.61% and 62.67% of the study region fall within
the moderate category of ﬂood hazard and risk intensity respectively. Further-
more, signiﬁcantly high i.e., 27.57% and 12.75% of the study region fall into the
category of high hazard and risk intensity respectively. The ﬁndings of this study
suggest that the integration of AHP and GIS techniques provides an effective tool
for decision making procedures in ﬂood hazard and risk mapping, as it enables a
coherent and effective use of spatial data.
Keywords: GIS · AHP · Flood hazard · Flood risk · Rapti River
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 95–103, 2024.
https://doi.org/10.1007/978-3-031-50158-6_10

96
R. Khan et al.
1
Introduction
Flooding is a recurring hazardous event that results in increasingly global losses of life
and property [1]. The Indo-Gangetic and Brahmaputra plains are among the most ﬂood-
prone areas in India and are considered the worst ﬂood-exaggerated regions in the entire
world. Every year, states like Uttar Pradesh, Bihar, and West Bengal situated in the Indo-
Gangetic basin experience severe ﬂoods due to the high magnitude of discharge during
the monsoon season and the large volume of sediment brought down by the Himalayan
Rivers and their tributaries [2]. These ﬂoods cause great loss of life and property, and
damage to existing infrastructure, such as roads, railways, bridges, and agricultural
land. The Rapti River is one of the most ﬂood-prone rivers in North-East Uttar Pradesh.
Several villages in the districts of Gorakhpur, Deoria, Maharajganj, Balrampur, and
Siddharthnagar get inundated every year due to ﬂoods during the monsoon season. The
region has experienced four major ﬂoods within two decades viz. 1998, 2001, 2007, and
2017. The severity and recurrence of these ﬂoods have been predicted to rise over time
as a result of climate change [3, 4].
The recently developed and widely accepted MCDM-based AHP method has been
employed in several ﬂood hazard and risk assessment studies, revealing fast and precise
prediction and is capable of handling inherent complexities resulting from involvement
of multiple components of ﬂood causative elements. Pathan et al. [5] employed two
decision making techniques i.e., a strategy for order of preference by similarity to ideal
solution (TOPSIS) and analytical hierarchy process (AHP) to prioritize sub-watersheds
of the Ami River Basin, Uttar Pradesh, to assess the ﬂood risk in the basin. TOPSIS
technique revealed better results as compared to AHP. The objective of present study
is to create ﬂood hazard and risk maps for the Rapti River watershed by considering
eight ﬂood hazard indicators (i.e., height from nearest drainage, distance from river,
elevation, land use and land cover, slope, soil type, drainage density and rainfall) in
order to identify areas that requires the greatest involvement in the development of risk
reduction and mitigation strategies. Population density along with ﬂood hazard spatial
layer and rainfall were considered as ﬂood risk indicators for the analysis of risk intensity
of ﬂoods in the study area. The ﬁndings of this study could be used as a decision making
tool for effective improvisation of ﬂood control plans and initiatives.
2
Study Area
The part of Rapti River watershed lying within the Indian territory is selected as the study
area. The study area is located in the Northeastern part of Uttar Pradesh, India and covers
an area of about 15,000 km2. The major land-use covers include buildup area, crops and
forests. Geographically, the study area is located between 27˚ 59′ 25′′ N to 26˚ 15′
57′′ N latitude and 81˚ 40′ 50′′ E to 83˚ 51′ 11′′ E longitude. The river emerges from
the Himalayan range in the Nepalese territory, then enters the plainlands and ﬁnally
joins Ghagra River which is one of the major tributaries of the Ganga River. The total
main stream length of the river within India is about 640 km (Fig. 1).

GIS Based Flood Hazard and Risk Assessment Using Multi Criteria
97
Fig. 1. Location of the study area
3
Methodology
The ﬂood hazard and risk analysis was carried out using Geographic Information Sys-
tems (GIS) based MCDM approach. The ﬂood hazard and risk indicators were selected
and assigned ranks based on an extensive review of the published literature and by col-
lecting the expert’s opinion working in the ﬁeld of ﬂood modelling and analysis through
questionnaires. Height above nearest drainage (HAND), elevation, distance from river,
land use (LULC), slope, drainage density, soil, and rainfall were considered as eight
important ﬂooding hazard indicators. Analysis of ﬂooding risk was based on ﬂood haz-
ard spatial layer, population density and LULC. Weighted linear combination (WLC)
method was used for overlaying different layers to obtain ﬂood hazard and ﬂood risk
maps of the study area. SRTM digital elevation model (DEM) of 30 m spatial resolution
was download from United States Geological Survey (USGS) web portal and utilized
for creating elevation, slope, drainage density and HAND layers within GIS framework.
Landsat 8 OLI/TIRS image of October 2020 with 30 m resolution was downloaded
from USGS web portal utilized for generating LULC map of the study area. The soil
data was downloaded from the National Bureau of Soil Survey and Land Use Planning
(NBSS & LUP) and transformed into soil map of the study area. Rainfall data of ten
years duration i.e., from 2010 to 2020 was procured from the World Bank Group (cli-
mate change portal) and the same were utilized for creating rainfall map of the study
area. The comprehensive database pertaining to demographic statistics i.e., population
density data was collected from the District Census Handbook (DCH) of the State, and
transformed into spatial map layers utilized for ﬂood risk analysis.
3.1
Flood Hazard Indicators
Height Above Nearest Drainage (HAND) The relative height of a location in relation to
the nearest river tributary is important in determining its ﬂood susceptibility since low-
lyinglandsadjacenttostreamsaremorevulnerabletoﬂoodingthanhigherelevationland.

98
R. Khan et al.
For creating the HAND map, SRTM DEM and the Stream ordering layer are required as
inputs. Distance from River (DR) is another important parameter in determining an
area’s ﬂood susceptibility. Floods have a greater impact on areas near rivers than on
areas farther away from them. The reclassiﬁcation of the layer was based on assigning
a value of 5 to areas farthest from the river, and a value of 1 to areas near to the river.
Elevation (E) Elevation of an area has an important role in controlling the movement of
the runoff water and the depth of the water level. The Elevation raster map was created
using the SRTM DEM and slope generation tools in ArcGIS software. The DEM reveals
the southeast region to be more susceptible to ﬂooding owing to relatively low elevation
values.
Land Use Land Cover (LULC) Urban expansion is considered one of the major
contributors to the increased frequency of ﬂoods observed in recent times. As impervious
cover increases and forest cover shrinks in urban areas, run-off increases. The change in
land use directly effects the ﬂooding and susceptibility to damages. Accordingly, water
bodies were ranked with the value of 5 as this LULC classiﬁcation is highly susceptible
to ﬂoods. Grassland/bare ground were assigned the value of 2, since both land cover
features accounts for low susceptibility to ﬂoods. Likewise, trees were assigned the
lowest rank as 1, due to low susceptibility to ﬂooding. Slope (S) The slope of a feature
is its angle of inclination with respect to the horizontal plane. Slope is a key indicator
of regions that are highly prone to ﬂooding. The rate and duration of runoff are highly
inﬂuenced by slope. The runoff accumulates at a much faster rate in ﬂat areas thereby,
making these areas more prone to ﬂooding than steeper surfaces.
Soil Type (ST) Soil properties in a watershed, such as permeability, soil layer thick-
ness, rate of inﬁltration and antecedent soil moisture may have a positive impact on
rainfall-runoff process and may eventually lead to ﬂooding of an area under extreme
circumstances. Flooding becomes more likely as soil inﬁltration capacity decreases,
resulting in an increased surface runoff. When surface runoff is generated in a quan-
tity that exceeds the soil’s inﬁltration capacity, it travels down the slope or accumulates
on a ﬂat terrain and may lead to ﬂooding. Drainage Density (DD) is the measure-
ment of sum of the drainage lengths per unit basin area and is considered as one of
the key factors that inﬂuences peak ﬂows during rainfall. Drainage density of an area
indicates the nature of soil and its geotechnical properties and therefore, drainage den-
sity depends on soil permeability and erodibility, LULC and slope. Drainage density
is an inverse function of inﬁltration. Areas with high drainage density indicates higher
runoff, and low ﬂood risk. Thus, the ranking for drainage density decreases with increas-
ing drainage density. Rainfall (RF) The worldwide frequency of severe rainfall events
has enhanced and the intensity of ﬂood is directly proportional to the intensity of rainfall.
The increase in both severity and quantum of rainfall owing to climate change has sig-
niﬁcantly brought a rise in severe ﬂooding events. Spatially distributed maps of the eight
selected ﬂood hazard indicators and a population density map as ﬂood risk indicator are
shown in Fig. 2. For the purpose of generating ﬂood indices using the spatial analyst tool
in ArcGIS, the MCDM-based AHP technique determined the relative weights of each
raster layer and the spatially distributed ﬂood hazard index (FHI) and ﬂood risk index
(FRI) maps were generated using standard expression implying sum product of relative
weight and the corresponding raster layer.

GIS Based Flood Hazard and Risk Assessment Using Multi Criteria
99
(a)
(b)
(c)                                                           (d)
(e)                              
(f)
Fig. 2. Flood hazard Indicators (a) height above the nearest drainage; (b) distance from the tribu-
taries; (c) elevation; (d) LULC; (e) slope; (f) soil classiﬁcation; (g) drainage density; (h) rainfall;
(i) population density
3.2
Analytical Hierarchy Process
Saaty [6] developed the Analytic Hierarchy Process (AHP) to handle complicated issues
with multiple criteria. This method analyses judgements with the use of mathemati-
cal procedures that take into account the preferences of decision-makers or groups of
people in a speciﬁc ﬁeld based on selected factors. Nearest neighbor comparisons are
performed in this method to establish relative priorities among the multiple factors or
criteria involved, and these pairwise comparisons are done on a nine-level standardized

100
R. Khan et al.
(g)                                                             (h)
(i)
Fig. 2. (continued)
scale [7, 8]. The relative importance of parameters is measured on a scale from 1 to
9, with 1 indicating that the two parameters are of equal importance and 9 indicating
that one parameter is signiﬁcantly more signiﬁcant than the other. The reciprocal of
Saaty’s rating scale (i.e., 1/1 to 1/9) indicates that one parameter is less important than
the other (Table 1). The parameters under consideration are ranked based on their rel-
ative importance, which is determined by expert judgement via surveys and Pearson’s
inter-correlation between the parameters. The comparison matrix is generated by com-
paring each parameter one to one, yielding a total of nC2 comparisons. If the judgement
criteria ﬁlls the upper portion of the diagonal in the comparison matrix, the reciprocals
ﬁll the lower portion of the matrix, and the comparison matrix is constructed. The pair
wise comparison matrix is then linearly normalized, where each element is split by the
sum of the elements in its corresponding column after the sum of all the values from
each column has been obtained. The average of all the elements in each row of the nor-
malized matrix is then calculated to determine the relative weights of each factor as seen
in Table 2. To check the preciseness of the relative importance of each criteria obtained
through the process, the consistency ratio (CR) is applied which is deﬁned as the ratio
of consistency index (CI) to the random index (RI). The CR value less than 10% suggest
that the judgement on relative importance among criterion/parameters is consistent.

GIS Based Flood Hazard and Risk Assessment Using Multi Criteria
101
Table 1. Saaty’s scale
Intensity
Preference degree
Explanation
1
Equally
Both parameters have equal importance
3
Moderately
One parameter is favored slightly more than another
5
Strongly
One parameter is favored strongly than another
7
Very strongly
One parameter is favored very strongly and is considered
superior to another
9
Extremely
One parameter is favored as superior to other parameter in
highest possible order of afﬁrmation
2,4,6,8 can be used for intermediate of the above values
Table 2. Normalization of the comparison matrix of the ﬂood hazard indicators
Flood
hazard
indicators
HAND
DR
LULC
S
ST
RF
DD
E
Weight
HAND
0.32
0.41
0.33
0.30
0.27
0.23
0.17
0.17
0.27
DR
0.16
0.20
0.33
0.20
0.20
0.19
0.17
0.17
0.20
LULC
0.16
0.10
0.16
0.30
0.20
0.23
0.24
0.19
0.20
S
0.11
0.10
0.05
0.10
0.20
0.14
0.17
0.14
0.13
ST
0.08
0.07
0.05
0.03
0.07
0.14
0.10
0.12
0.08
RF
0.06
0.05
0.03
0.03
0.02
0.05
0.10
0.12
0.06
DD
0.06
0.04
0.02
0.02
0.02
0.02
0.03
0.07
0.04
E
0.04
0.03
0.02
0.02
0.01
0.01
0.01
0.02
0.02
Total
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
1.00
4
Results
The consistency ratio (CR) for the ﬂood hazard was evaluated as 8.29% and the principal
eigenvalue as 7.67 on the basis of the weight scale derived from the standardized pairwise
comparison matrix. For the ﬂood risk evaluation, the CR was computed as 9.82% and the
principal eigenvalue 7.78 which shows consistency in the judgement process.
The MCDM based AHP approach calculated the weights of various contribut-
ing hazard and risk indicators that were multiplied with the spatial layer of respec-
tive indicator and overlaid using spatial analyst’s weighted linear combination
(WLC) method within the GIS interface. The ﬁnal formulated expression for FHI is writ-
ten below and the expression was used for generating the ﬂood hazard map (Fig. 3a). The
ﬂood hazard and risk maps were further categorized into 5 classes: i.e., very high, high,
moderate, low, and very low.

102
R. Khan et al.
FHI = 0.27 × HAND + 0.20 × DR
+ 0.20 × LULC + 0.13 × S + 0.08 × ST+
0.06 × RF + 0.04 × DD + 0.02 × E
Fig. 3. (a) Flood hazard and (b) ﬂood risk map of the Rapti River watershed
The population density layer, LULC layer, and the ﬂood hazard layer i.e., FHI, mul-
tiplied by considering weights in equal proportion (i.e., 0.33) were utilized to generate
the ﬂood risk map of the study area (Fig. 3b). The ﬁnal formulated FRI expression is
written below.
FRI = 0.33 × FHI
+ 0.33 × LULC
+ 0.33 × Population density
The ﬂood hazard map of Rapti River watershed shown in Fig. 3a, reveals that 0.13%
(19.76 km2), 14.34% (2149.5 km2), 57.61% (8634.8 km2), 27.57% (4131.6 km2), and
0.35% (52.5 km2) of the watershed area falls within very low, low, moderate, high, and
very high category of ﬂood hazard respectively. Results further reveal that a substan-
tial proportion of the study area lies within moderate to high risk of ﬂooding. The ﬂood
risk map for the watershed (Fig. 3b) shows that 5.67% (844.73 km2), 18.65% (2780.55
km2), 62.67% (9342.19 km2), 12.75% (1900.47 km2), and 0.26% (38.29 km2) of the
watershed’s area is at the risk of very low, low, moderate, high and very high ﬂooding
respectively. Results are indicative of the fact that a signiﬁcant proportion of the study
area i.e., 85.18% and 75.42% is located in regions with moderate to high ﬂood hazard
and risk intensities respectively.
5
Conclusion
The most frequent natural disaster that affects people and results in signiﬁcant loss
of life and property worldwide is ﬂooding which occur for numerous reasons and in
various situations. Recent reports indicate that the frequency of ﬂood have increased

GIS Based Flood Hazard and Risk Assessment Using Multi Criteria
103
many fold in recent times necessitating precise identiﬁcation of ﬂood prone regions
to ensure the long-term sustainable solutions of ﬂood risk reduction by recommend-
ing effective ﬂood control measures on priority basis. This study used MCDM based
AHP approach to create ﬂood hazard and risk maps of the Rapti River watershed within
GIS interface. A total of eight ﬂood hazard indicators including height from nearest
drainage, distance from river, elevation, LULC, slope, soil type, drainage density and
rainfall were considered for evaluating the ﬂood hazard. Population density along with
FHI spatial layer and cumulative rainfall were considered for evaluating the ﬂood risk
in the Rapti River watershed. The degree of food hazard and risk intensity in terms of
areal coverage were categorised into ﬁve classes: very low, low, moderate, high, and
very high. The results of the analysis reveal that almost 57.61% and 62.67% of the study
area falls under the moderate category of ﬂood hazard and risk intensity respectively.
Moreover, around 27.57% and 12.75% of the study area falls under the high category
of hazard and risk intensity respectively. The areas that fall into the moderate and high
hazard and risk intensity categories need to have effective ﬂood control measures, such
as levees or ﬂood walls along key river sections. It is also important to identify feasible
locations to construct ﬂood control reservoirs and rehabilitate storm water drainage sys-
tems. The results can be used by disaster management authorities to implement effective
and sustainable ﬂood management strategies in ﬂood-prone areas.
References
1. Ogato, G.S., Bantider, A., Abebe, K., Geneletti, D.: Geographic information system (GIS)-
Based multicriteria analysis of ﬂooding hazard and risk in Ambo Town and its watershed,
West shoa zone, oromia regional State, Ethiopia. J. Hydrol. Reg. Studies 27 (2020). https://
doi.org/10.1016/j.ejrh.2019.100659
2. Pathan, A.I., Agnihotri, P.G., Patel, D.: Integrated approach of AHP and TOPSIS (MCDM)
techniques with GIS for dam site suitability mapping: a case study of Navsari City, Gujarat.
India. Environ. Earth Sci. 81(18), 443 (2022)
3. Dewan, A., Islam, M.M., Kumamoto, T., Nishigaki, M.: Evaluating ﬂood hazard for land-use
planning in greater Dhaka of Bangladesh using remote sensing and GIS techniques. Water
Resour. Manage. 21, 1601–1612 (2006). https://doi.org/10.1007/s11269-006-9116-1
4. Abah, R.C.: An application of geographic information system in mapping ﬂood risk zones in a
north central city in Nigeria. African J. Environ. Sci. Technol. 7, 365–371. https://doi.org/10.
5897/AJEST12.182
5. Pathan, A., Kantamaneni, K., Agnihotri, P., Patel, D., Said, S., Singh, S.K.: Integrated ﬂood
risk management approach using mesh grid stability and hydrodynamic model. Sustainability
14(24), 16401 (2022)
6. Saaty, T.L.: A scaling method for priorities in hierarchical structures. J. Mathem. 15(3), 234–81
(1977)
7. Uddin, K., Gurung, D.R., Giriraj, A., Shrestha, B.: Application of remote sensing and gis for
ﬂood hazard management: a case study from sindh province, Pakistan. Am. J. Geogr. Inf. Syst.
2013, 1–5 (2013). https://doi.org/10.5923/j.ajgis.20130201.01
8. Pathan, A.I., Girish Agnihotri, P., Said, S., Patel, D.: AHP and TOPSIS based ﬂood risk
assessment-a case study of the Navsari City, Gujarat. India. Environ. Monit. Assess. 194(7),
509 (2022)

Optimizing Laser Drilling of Kenaf/HDPE
Composites: A Novel CRITIC-MABAC
Methodology
Sellamuthu Prabhukumar1, Jasgurpeet Singh Chohan2, and Kanak Kalita3(B)
1 Department of Mechanical Engineering, Presidency University, Bangalore, India
prabhukumar.sellamuthu@presidencyuniversity.in
2 Department of Mechanical Engineering and University Centre for Research & Development,
Chandigarh University, Mohali, India
3 Department of Mechanical Engineering, Vel Tech Rangarajan Dr. Sagunthala R&D Institute of
Science and Technology, Avadi, India
drkanakkalita@veltech.edu.in
Abstract. In this paper, a novel hybrid multi-criteria decision-making (MCDM)
methodology called the CRITIC-MABAC approach is introduced. It is applied in
optimization of laser drilling of kenaf/high-density polyethylene (HDPE) com-
posites. The objective is to simultaneously minimize the kerf taper angle (ϕ) and
surface roughness (Ra) by optimizing the two process parameters (namely laser
power and cutting speed). To establish the efﬁcacy of the CRITIC-MABAC, its
results are compared with Entropy-MABAC approach. The optimal parametric
combination is found to be at a laser power of 120W and cutting speed of 4 mm/s.
Thus, the proposed CRITIC-MABAC method demonstrates its effectiveness and
can be used as a means to achieve superior hole quality.
Keywords: Composites · Laser drilling · CRITIC · MABAC · Multi-criteria
decision-making · Optimization
1
Introduction
Composites have become widely popular in the recent years due to their eco-friendliness,
low cost and superior mechanical properties. They are widely used in numerous indus-
tries like automotive, construction, and packaging. Achieving high-quality hole charac-
teristics is crucial for the assembly and functionality of composites, making the drilling
process an essential operation in their manufacturing.
Laser drilling is a non-contact, highly precise, and efﬁcient technique for creating
holes in a variety of materials. A focused laser beam is used to remove material by vapor-
ization and melting, resulting in the formation of a hole. However, achieving the desired
hole quality characteristics in kenaf/high-density polyethylene (HDPE) composites can
be challenging due to the heterogeneous nature of the composite and the sensitivity of
the kenaf ﬁbers to thermal degradation. Therefore, it is vital to optimize the laser drilling
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 104–111, 2024.
https://doi.org/10.1007/978-3-031-50158-6_11

Optimizing Laser Drilling of Kenaf/HDPE Composites
105
process variables to realize least kerf taper angle (ϕ) and surface roughness (Ra) while
maintaining the structural integrity of the composite.
In the literature, various studies have conducted the optimization of laser drilling
processes for different materials. Singh et al. [1] conducted a comprehensive study on
the laser drilling of HDPE. They employed ANOVA and response surface methodology
for optimization. The cutting speed and laser power were considered as the process
variables for optimizing Ra, ϕ, and heat-affected zones (HAZ).
Researchers have also investigated the optimization of mechanical properties and
surface quality in laser-drilled glass ﬁber-reinforced plastic (GFRP) laminates [2].
Several studies have explored conventional drilling operations, such as the work
by Palanikumar et al. [3], who investigated the drilling of GFRP using an L9 orthog-
onal array. They employed Grey Relational Analysis (GRA), an MCDM technique, to
optimize the drilling process. Shunmugesh and Panneerselvan [4] employed an L27
orthogonal array to design experimental runs for drilling carbon ﬁber-reinforced plastic
(CFRP), using Taguchi and GRA methods for optimization and analysis.
Other studies have studied the performance of CO2 laser cutting in processing CFRP
sheets [5], laser cut quality of GFRP composites [6] etc. Takahashi et al. [7] analyzed the
HAZ and kerf in CFRP composites. The effects of gas ﬂow rate, cutting speed, and laser
power on kerf width, HAZ and taper percentage in laser cutting processes was studied
by Rao et al. [8]. The consequences of laser drilling on the mechanical and surface
properties of GFRP composites was studied by Solati et al. [9].
Despite the extensive research on the optimization of laser drilling processes for dif-
ferent materials, limited studies have focused on kenaf/HDPE composites. Most existing
studies have employed traditional single-objective optimization techniques, which may
not accurately represent the trade-offs between multiple conﬂicting objectives. Further-
more, very few studies have utilized hybrid multi-criteria decision-making (MCDM)
methodologies to optimize laser drilling processes.
The current literature lacks a comprehensive approach that simultaneously optimizes
hole quality characteristics, such as ϕ and Ra, in laser-drilled kenaf/HDPE composites
using a hybrid MCDM methodology. Additionally, the effectiveness of the CRITIC
method for weight allocation in MCDM processes has not been extensively explored in
the context of laser drilling optimization of composites.
This paper addresses the identiﬁed research gap by proposing a novel hybrid
CRITIC-MABAC methodology for optimizing the laser drilling process of kenaf/HDPE
composites. The key contributions of this paper are—
• The development of a hybrid CRITIC-MABAC methodology for optimizing the laser
drilling process of kenaf/HDPE composites to achieve minimum ϕ and Ra.
• A comparative analysis of the proposed CRITIC-MABAC methodology with the
Entropy-MABAC approach, highlighting the effectiveness of the CRITIC method.
• The identiﬁcation of the optimal parametric combination for laser drilling of
kenaf/HDPE composites.

106
S. Prabhukumar et al.
2
Problem Description
The primary objective of this paper is to examine the hole quality features in laser-
drilled kenaf/HDPE composites using a hybrid MCDM methodology. Hybrid CRITIC-
MABAC methodology is used to determine the optimal parametric combination. The
idea is to optimize the process parameters to achieve minimum ϕ and Ra. The Laser
power and cutting speed are considered as the process parameters. A Kenaf/HDPE
composite is fabricated using microwave-assisted compression molding is considered
as the workpiece. The experimental dataset is designed using central composite design.
More details of the experimentation and the decision matrix for the MCDM process can
be found in Tewari et al. [10].
3
Methodology
3.1
CRITIC Method
When decision-makers lack the ability to compare multiple standards or hold divergent
opinions regarding the relative relevance of distinct criteria under consideration [11], this
method is used to estimate impartial criteria weights. Standard deviation and correlation
with other evaluation criteria are used to estimate the weight of a given criterion in this
method. The jth criterion’s weight is calculated as,
wj =
Cj
n
j=1 Ci
(1)
Cj can be calculated as,
Cj = σj
m

i=1

1 −cij

(2)
Here σj is the standard deviation of jth criterion, and cij is the correlation coefﬁcient
between ith and jth criteria.
3.2
MABAC Method
The University of Defence in Belgrade created a system for making multi-criteria deci-
sions called Multi-Attributive Border Approximation Area Comparison (MABAC) [12].
Alternatives can be ranked and evaluated using MABAC’s predetermined criteria. The
procedure accounts for the relative relevance of each criterion. The pseudo code for the
MABAC method is as follows—
1. Develop the decision matrix (X ) with m alternatives and n criteria. xij are the elements
of X .

Optimizing Laser Drilling of Kenaf/HDPE Composites
107
2. Normalize X to form the normalized matrix R (with elements rij) using the following
rules
rij =
xij −x−
j
x+
j −x−
j
for beneﬁt criteria
(3)
rij =
xij −x+
j
x−
j −x+
j
for cost criteria
(4)
x+
j and x−
j are the maximum and minimum values of the Jth criterion.
3. Compute the weighted normalized decision matrix V (with elements vij)
vij = wj ·

rij + 1

(5)
wj is the weight of the Jth criterion.
4. Compute the border approximation area (BAA) matrix B (with elements bj)
bj =
m
i=1 vij
1/m
(6)
5. Compute the distance matrix of alternatives (Q) from the BAA. qij are the elements
of Q.
Q = V −B
(7)
6. Compute the criteria function (Si) values and ranking the alternatives:
Si =
n
j=1 qij, j = 1, 2, . . . , n, i = 1, 2, . . . , m
(8)
7. Rank the alternatives in descending order of Si values.
4
Results & Discussion
4.1
Optimization with CRITIC-MABAC
The experimental data from Tewari et al. [10] is used as the decision matrix in this
paper. To ﬁnd the effect of the weight allocation method two different weight allocation
strategies namely the entropy method and the CRITIC method are used. The weights
allocated by Entropy method is 11.27% and 88.73% respectively for Ra (C1) and ϕ
(C2) respectively. Similarly, the weights allocated by CRITIC method for C1 and C2 is
49.73% and 50.27% respectively. Thus, it is observed that the entropy method produces
skewed weights, whereas the CRITIC method produces weights that are almost equal-
valued. Table 1 shows the decision matrix and the normalized weighted matrix for both
the Entropy-MABAC method and the CRITIC-MABAC method.
Figure 1 shows the variation of values with respect to the various experiments. It is
observed that the Q-values extend in both the positive and negative directions. For both
the weight allocation methods, experiment number 4 is found to have the largest Q-value,
indicating that this is the most optimal parametric combination. Experiment number 10
is observed to be the worst parametric combination as per the MABAC method.

108
S. Prabhukumar et al.
Table 1. Decision matrix and normalized weighted matrix values
Experimental
Normalized entropy weighted
Normalized CRITIC
Weighted
C1
C2
C1
C2
C1
C2
5.25
0.314
0.1782
1.5103
0.7863
0.8556
6.61
0.69
0.1330
1.1250
0.5868
0.6373
5.52
0.672
0.1692
1.1434
0.7467
0.6478
3.83
0.056
0.2254
1.7746
0.9947
1.0053
5.25
0.68
0.1782
1.1353
0.7863
0.6431
5.25
0.68
0.1782
1.1353
0.7863
0.6431
6.83
0.736
0.1257
1.0779
0.5545
0.6106
7.22
0.744
0.1127
1.0697
0.4973
0.6060
5.25
0.68
0.1782
1.1353
0.7863
0.6431
6.53
0.922
0.1356
0.8873
0.5986
0.5027
6.12
0.28
0.1493
1.5451
0.6587
0.8753
5.25
0.68
0.1782
1.1353
0.7863
0.6431
5.25
0.68
0.1782
1.1353
0.7863
0.6431
Fig. 1. Variation of MABAC Q-values with respect to experiments
4.2
Parametric Optimization
In this section, the effect of the process parameters on the responses is observed. Figure 2
shows the effect of the power on the Ra and the ϕ. It is observed that with an increase

Optimizing Laser Drilling of Kenaf/HDPE Composites
109
in power, the Ra becomes worse. However, in the case of the ϕ, the increase in power
increases the ϕ up to a certain power value beyond which the ϕ begins to drop down.
Fig. 2. Effect of power on aggregated values of (a) Ra (b) ϕ
In the context of this study, the MABAC Q-values can be used as a substitute for the
combined metric representing the lower power and lower ϕ. Figure 3 shows the effect
of power on the aggregated Q-values of the MABAC method. In the case of both the
Entropy-MABAC method and the CRITIC-MABAC method the optimal value of power
is observed to be 120W.
Fig. 3. Effect of power on aggregated Q-values of (a) Entropy-MABAC (b) CRITIC-MABAC
Figure 4 shows the effect of speed on the Ra and the ϕ. It is noted that an increase in
speed gives lower average Ra and lower average ϕ.
Figure 5 shows the effect of the speed on the Q-values of the Entropy-MABAC and
CRITIC-MABAC methods. It is observed from Fig. 5 that the speed of 4 mm/s is optimal
for minimizing the Ra and the ϕ simultaneously.

110
S. Prabhukumar et al.
Fig. 4. Effect of speed on aggregated values of (a) Ra (b) ϕ
Fig. 5. Effect of speed on aggregated Q-values of (a) Entropy-MABAC (b) CRITIC-MABAC
5
Conclusion
In this paper, a novel hybrid CRITIC-MABAC method is proposed for optimizing the
laser drilling process parameters of kenaf/HDPE composites. The objective of this study
is to minimize the ϕ and Ra. The laser power and cutting speed are considered as
the process parameters. An experimental dataset, designed in central composite design
is used in the case study. The analysis of the case study using the CRITIC-MABAC
method reveals that a laser power of 120W and cutting speed of 4 mm/s is the most
optimal. Entropy-MABAC approach is also used to compare and validate the ﬁndings
of CRITIC-MABAC. Thus, the CRITIC-MABAC method is found to be effective in
optimizing parametric combination for laser drilling of kenaf/HDPE composites.
The ﬁndings of this study provide valuable insights for researchers and practitioners
working with kenaf/HDPE composites, as the proposed CRITIC-MABAC methodology
can be effectively applied to optimize the laser drilling process for achieving supe-
rior hole quality characteristics. In addition, the study highlights the importance of the
CRITIC method for weight allocation in MCDM processes, as it produces more balanced

Optimizing Laser Drilling of Kenaf/HDPE Composites
111
weight allocations compared to the Entropy method. Future research could explore the
application of the CRITIC-MABAC methodology to other composite materials and man-
ufacturing processes, as well as compare its performance with other MCDM methods
for process optimization.
References
1. Singh, S., Yaragatti, N., Doddamani, M., Powar, S., Zafar, S.: Drilling parameter optimization
of cenosphere/HDPE syntactic foam using CO2 laser. J. Manuf. Process. 80, 28–42 (2022)
2. Solati, A., Hamedi, M., Safarabadi, M.: Combined GA-ANN approach for prediction of HAZ
and bearing strength in laser drilling of GFRP composite. Opt. Laser Technol. 113, 104–115
(2019)
3. Palanikumar, K., Latha, B., Senthilkumar, V.S., Davim, J.P.: Analysis on drilling of glass
ﬁber-reinforced polymer (GFRP) composites using grey relational analysis. Mater. Manuf.
Process. 27, 297–305 (2012)
4. Shunmugesh, K., Panneerselvam, K.: Optimization of process parameters in micro-drilling of
carbon ﬁber reinforced polymer (CFRP) using Taguchi and grey relational analysis. Polym.
Polym. Compos. 24, 499–506 (2016)
5. Riveiro, A., Quintero, F., Lusquiños, F., et al.: Experimental study on the CO2 laser cutting of
carbon ﬁber reinforced plastic composite. Compos. Part A Appl. Sci. Manuf. 43, 1400–1409
(2012)
6. Choudhury, I.A., Chuan, P.C.: Experimental evaluation of laser cut quality of glass ﬁbre
reinforced plastic composite. Opt. Lasers Eng. 51, 1125–1132 (2013)
7. Takahashi, K., Tsukamoto, M., Masuno, S., et al.: Inﬂuence of laser scanning conditions on
CFRP processing with a pulsed ﬁber laser. J. Mater. Process. Technol. 222, 110–121 (2015)
8. Rao, S., Sethi, A., Das, A.K., et al.: Fiber laser cutting of CFRP composites and process
optimization through response surface methodology. Mater. Manuf. Process. 32, 1612–1621
(2017)
9. Solati, A., Hamedi, M., Safarabadi, M.: Comprehensive investigation of surface quality and
mechanical properties in CO2 laser drilling of GFRP composites. Int. J. Adv. Manuf. Technol.
102(1–4), 1–18 (2018)
10. Tewari, R., Singh, M.K., Zafar, S., Powar, S.: Parametric optimization of laser drilling
of microwave-processed Kenaf/HDPE composite. Polym. Polym. Compos. 29(3), 176–187
(2021)
11. Diakoulaki, D., Mavrotas, G., Papayannakis, L.: Determining objective weights in multiple
criteria problems: the CRITIC method. Comput. Oper. Res. 22(1), 763–770 (1995)
12. Pamuˇcar, D., ´Cirovi´c, G.: The selection of transport and handling resources in logistics centers
using multi-attributive border approximation area comparison (MABAC). Expert Syst. Appl.
42(6), 3016–3028 (2015)

Education, Healthcare, Industry,
and Advanced Engineering

Determination of the Optimal Speed
of Movement of the Conveyor Belt
of the Prototype Weighing Belt Batcher
Denis Shilin, Dmitry Shestov, Alexey Vasiliev(B), and Valery Moskvin
National Research University “MPEI”, Moscow, Russian Federation
lex.of@mail.ru, moskvinVG@mpei.ru
Abstract. Today, the agro-industrial complex of Russia is increasing production
volumes due to the introduction of high-performance technological lines, and
in the ﬁeld of mixture formation, the processes of production of dry combined
products in continuous installations are becoming widespread. Also, the Russian
Federation is a large center of light and heavy industry, each of which, at one level
or another, involves continuous weighing of components for the production or
packaging of bulk mixtures. The weighing accuracy in these units directly affects
the cost price and product quality. As a rule, the developers of continuous dosing
systems declare dosing accuracy in nominal modes, although in real conditions
such systems are characterized by an uneven ﬂow of bulk material, which in
turn signiﬁcantly affects the ﬁnal dosing error. In connection with the above, the
authors of the work developed a prototype of a weighing belt batcher (WBB) and
conducted research to determine the optimal range of speeds of the conveyor belt,
at which the minimum relative error is achieved.
Keywords: Weighing belt batcher · Weighing error · Electric drive · Conveyor ·
Intelligent algorithm · Fuzzy controller · Weighing mode
1
Introduction
The Russian Federation is a large center of light and heavy industry enterprises, each of
which, at one level or another of the technological cycle, uses systems for continuous
weighing of bulk materials in the manufacture or packaging of various mixtures. At the
same time, the cost price and quality of the prepared mixtures are inﬂuenced by such
indicators as: the productivity of such systems, the accuracy and energy consumption
of the weighing process. Typically, such complexes consist of weighing belt batcher,
and control systems, the execution of which is implemented both at the hardware and
software levels. The main problems of such complexes and the methods of dosing bulk
materials used in them are associated with deviations from the required proportions in
the preparation of bulk mixtures. In existing technological lines, weighing belt batcher
with a dosing error of more than 2.5% are still used. There are hundreds of designs of
weighing belt batcher, in which different methods of weight continuous dosing of bulk
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 115–124, 2024.
https://doi.org/10.1007/978-3-031-50158-6_12

116
D. Shilin et al.
mixtures are implemented [11–13]. Typical schemes of weighing belt batcher usually
differ in the place of attachment of the strain gauge [11, 12], while the analysis showed
that there are no systems on the world market with attachment of sensors from the side
of loading materials. The authors of the work have developed an intelligent hardware
and software complex for continuous weighing of bulk materials, which consists of a
weighing belt batcher with a unique attachment of two strain gauges in the loading area
of bulk material and an intelligent algorithm for continuous weighing. The proposed
kinematic scheme determines the high scientiﬁc signiﬁcance of the work done due to
the absence of such complexes on the world market, in which the kinematic scheme for
connecting the strain gauges of the hardware corresponded to the proposed one. The
developed intelligent algorithm for continuous weighing is a scientiﬁc novelty of the
complex.
The core of the unique continuous weighing technology is the intelligent fuzzy logic
controller [14–18]. The principle of operation of the unique technology for weighing
bulk mixtures is based on an automatic system for recording the weight indicators of
load cells located on the side of the loaded materials, which, thanks to the use of a fuzzy
controller, maintains the speed of the belt conveyor and recalculates telemetry in the
developed WBB control cabinet. Figure 1 shows the dependence of the readings of the
strain gauges XT and YT on time.
Thanks to the developed fuzzy weighing algorithm, the system locates the center of
the instantaneous mass of the incoming material along the width H and length L of the
conveyor belt, as shown in Fig. 2.
Readings of strain gauges 
and
, [kg]
Time, [s]
Fig. 1. Time dependence of load cell readings
As seen in Fig. 2, the resulting instantaneous mass is displaced to one side of the
conveyor belt. The exact location of the center of mass of the obtained instantaneous
mass of material at one time or another, taking into account the dynamics of the induction
motor and a unique accounting algorithm, allows you to record the shipped mass with
high accuracy [19].
The control system is based on an industrial logic controller. The intelligent hardware
and software complex makes it possible to minimize the weighing error (less than 0.5%)
by ensuring the operation of the belt conveyor in the maximum loaded mode, which

Determination of the Optimal Speed of Movement of the Conveyor
117
Fig. 2. Determination of the position of the center of mass of the incoming material on the WBB
conveyor belt
avoids the accumulation of errors when idling. As a result of the analysis, it was found
that one of the main causes of errors when using continuous weighing batchers is the
dynamic effects on strain gauges. In this regard, it is necessary to conduct an experiment
and empirically determine the permissible speed of the dispenser conveyor belt, which
ensures the required weighing accuracy.
2
Mathematical Description of the FC—AM System and Production
Line Model with an Electric Drive System
Consider the generalized functional diagram of the automated electric drive in Fig. 3,
where indicated: C—controller; FC—voltage converter; OR—the object of regulation;
FS—feedback sensor. A control signal is input to the system, which sets the value of the
output coordinate and can be represented in analog, pulse or digital form.
FS
ОR
FC
C
Fig. 3. A generalized functional diagram of an automated electric drive with the control circuit
of the output coordinate.
The object of regulation is affected by a disturbing effect Uim in the form of a
change: frequency, or voltage of the primary power source of the drive, or static load

118
D. Shilin et al.
moment. The converter can be of any type: electromechanical or static on semiconductor
switches. The object of regulation often includes the engine together with the actuator
of the actuator, therefore it can be described by various transfer functions. An analog
or frequency speed or angle sensor with a gear ratio can be used as a feedback sensor
kFS. The difference between the signals given by the output coordinate of the drive
and proportional to the actual value enters the regulator, which, in accordance with its
transfer function, generates a control signal for the voltage converter UFC, which in turn
controls the motor Um to reduce this difference to zero or, if possible, reduce it [20].
UFC = kC(U0 −UFS),
(1)
UFS = kFSUout.
(2)
In order to investigate such a system, it is necessary to have a system of differential
equations characterizing the dependence of coordinates on external inﬂuences and from
each other. In general, these equations may be non-linear, i.e. their coefﬁcients may
depend on time or coordinate values. Some coordinates of the drive can be a function of
the product of external inﬂuences and variables, as in our case with a conveyor dispenser
[21].
Based on the above generalized functional scheme, the structure of the mathematical
model of the technological line of mixing preparation has been developed (Fig. 4).
Conveyor 
belt speed
Conveyor 
dosing 
unit with 
material 
weight 
sensors
Calculator 
of mass 
and 
volume of 
material
Speed 
feedback
Feedback 
on the 
volume of 
material
Frequency 
converter
The task for 
the speed of 
the conveyor 
belt
Controller
МOTOR
Bulk 
material 
supply
Fig. 4. The structure of the production line model.

Determination of the Optimal Speed of Movement of the Conveyor
119
3
Prototype Test Bench
Within the framework of this work, tests were carried out of the experimental WBB
shown in Fig. 5. The stand includes a WBB (1), consisting of a belt conveyor (2), on the
shaft of which an asynchronous motor (3) is installed, and a control system (4) connected
with local (5) and remote (6) control systems. The main structural elements of the WBB
are: a metal body and a belt conveyor design. These systems were discussed in detail in
articles by Shilin et al. [22, 23].
4
3
5
6
2
1
Fig. 5. External view of the WBB pilot plant
WBBs include loading bulk material from a feeder onto a conveyor belt, dynamic
weighing along its entire length, and unloading bulk material at the end of its stroke
[24–27].
The control system contains the necessary functionality for testing to determine the
optimal speed of the conveyor belt, namely: adjusting the speed of the conveyor belt, ﬁx-
ing the shipped mass and other technological settings [28]. The technical characteristics
of the device are shown in Table 1.
4
Test Procedure
Experimental studies were carried out for various types of bulk materials, the density of
which varied from 800 kg/m3 to 2500 kg/m3. The average duration of the experiment
was calculated as the arithmetic mean of the duration of the 3 tests performed. Mass
ﬂow rate QM was calculated using the following formula:
QM = me
tm
.
(3)

120
D. Shilin et al.
Table 1. Technical characteristics of WBB
№
Name of metrological and technical characteristics
Value
1
Maximum performance limit (MPL), [tons/hour]
Up to 100
2
Minimum performance limit, [MPL %]
10
3
Limits of permissible error, [MPL %]
0.5
4
Power consumption, not more than, [kW]
1.5
5
Maximum bulk density of material, [tons/m3]
1.4
6
Maximum conveyor belt speed, not more than, [m/s]
1.0
7
Conveyor belt width, [m]
0.5
8
Conveyor belt length, [m]
1.0
9
Power supply parameters voltage [V], frequency [Hz]
350 10 [V], 50 1 [Hz]
10
Probability of no-failure operation after 2000 h, [%]
0.92
11
Operating temperature range, [°C]
From minus 10 to plus 40
12
Total average service life of ﬂow meter-batcher, [years]
10
13
Communication interface with the upper level automatic
control system
PROFIBUS DP, ETHERNET
(ModBus TCP), ModBus
RTU, 4–20 mA
14
Degree of protection
IP54
where tm is the average duration of the experiment, s; me is the reference weight of the
cargo, kg. The reference mass of the material to be weighed was preliminarily recorded
on a ﬂoor scale. The volumetric ﬂow rate QV is calculated using the formula:
QV = QM
ρM
.
(4)
where QM is the calculated mass ﬂow rate, kg/s; ρM—tabular value of material density,
kg/m3. The absolute measurement error  is calculated as the difference between the
reference and received masses md:
 = me −md.
(5)
The relative measurement error δ is calculated by the formula:
δ = 
me
· 100% .
(6)
The average value of the measurement error δm was obtained using the following
formula:
δm =
 n

i=1
|δi|

/N.
(7)

Determination of the Optimal Speed of Movement of the Conveyor
121
where δi is the relative error of the i-th test for one measurement, %; N is the number
of tests, pcs. Experimental studies were carried out with the fulﬁllment of the following
conditions:
(1) the inﬂuence of the feeding device on the device body is completely excluded, and,
therefore, on the readings of strain gauges;
(2) the supply of the weighed bulk material is carried out strictly on the weighing axis
(the axis passing through the centers of two strain gauges);
(3) the supply of bulk material is carried out uniformly on the entire weighing axis (over
the entire width of the ﬂow area of the feeding device).
The experiment program includes the following steps:
(1) control weighing of material using a reference balance;
(2) setting the speed of the conveyor belt;
(3) calibration of strain gauges;
(4) reset of the previously recorded readings (the accumulation of the measured mass
occurs due to uncorrected sensors);
(5) loading the material to be weighed into the feeding device;
(6) start of material supply to the conveyor belt and start of timing;
(7) when the feeding device is completely empty and the conveyor belt is released,
recording the readings of the measured weight and weighing time.
After completing all the steps of the experiment, the required number of times, the
calculation of the absolute and relative measurement error is carried out according to
formulas (5) and (6), respectively.
The study of WBB was carried out on the basis of the computer program «Program
for control and weighing of bulk material» [29].
5
Determination of the Optimal Speed Range of Movement
of the WBB Conveyor Belt
As a result of the data obtained, the dependence of the relative measurement error δ
on the speed of the conveyor belt ν at different performance of the WBB was built
(see Fig. 6). These characteristics were obtained as a result of developing an intelligent
control algorithm based on a fuzzy controller. According to the presented data, it can be
seen that with a signiﬁcant increase in the speed of the conveyor belt ν, with different
productivity of the WBB, the relative weighing error δ [30, 31].
It was also found that there is an optimal speed range of movement of the conveyor
belt ν = 0.25 ÷ 0.75 m/s, at which a relative weighing error is achieved that does not
exceed δ = ±0.5%. According to the data obtained, the dependence of the average
relative measurement error δm on the performance of the WBB QM (see Fig. 7) was
built, obtained during the development of an intelligent control algorithm based on a
fuzzy controller in a given speed range of the conveyor belt movement.
The characteristic shows that with an increase in the productivity of the WBB QM ,
the average relative weighing error δm decreases. This is due to the fact that at a low load
of strain gauge sensors, their noise signiﬁcantly affects the resulting indicators of the

122
D. Shilin et al.
Relative measurement error , [%]
Conveyor belt speed
, [m/s]
Fig. 6. Dependence of the relative measurement error δ on the speed of movement of the conveyor
belt ν at different productivity of the WBB
mass ﬂow rate of the installation. Accordingly, with increasing productivity, the effect
of noise on the mass ﬂow indication is not signiﬁcant, which leads to a decrease in the
relative weighing error.
Average relative error
, [%]
Mass flow of the weighing tape batcher
, [t/h]
Fig. 7. Dependence of the average relative measurement error δm on the mass ﬂow rate QM WBB
6
Conclusions
1. It was found that there is an optimal speed range of movement of the conveyor belt
ν = 0.25 ÷ 0.75 m/s, at which a relative weighing error is achieved that does not
exceed δ = ±0, 5% (according to GOST 30124-94 “Scales and weight dispensers of
continuous action”) over the entire possible range of productivity QM = 5.0 ÷ 50.0
tons/hour.

Determination of the Optimal Speed of Movement of the Conveyor
123
2. It has been established that the relative weighing error decreases with an increase in
the WBB productivity. This is due to the fact that at a low load of strain gauge sensors,
their noise signiﬁcantly affects the resulting indicators of mass ﬂow. Accordingly, as
the load (capacity) perceived by the sensors increases, the effect of noise on the mass
ﬂow reading is not signiﬁcant, which leads to a decrease in the relative weighing
error.
3. The experiments carried out on the prototype WBB at various intensities helped to
determine the optimal speed range of the units and the margin of error, which will
allow us to determine the calibration table of the technical characteristics of the WBB
line of various productivity and production intensity.
4. WBB can be recommended for mass production for mass introduction in such
industries as the production of polymers, household chemicals, pharmaceuticals and
medical preparations, the production of rubbers, precious metals, chemical reagents.
References
1. Alspaugh, M.: Bulk Material Handling by Conveyor Belt, Preface, Littleton, 98 p (2004)
2. Boyd, D.C., Panigrahi, S.: Methods and practices of pressure measurements in Silos. In:
Design and Selection of Bulk Material Handling Equipment and Systems: Mining, Mineral
Processing, Port, Plant and Excavation Engineering, issue 2, pp. 307–335 (2012)
3. Faber, T.E., Lumley, J.L.: Fluid Dynamics for Physicists. Cambridge University Press,
Cambridge, 440 p (1995)
4. Vislov, I.S.: A batch feeder for inhomogeneous bulk materials. In: IOP Conference Series:
Materials Science and Engineering (2016)
5. Russell, K.: The Principles of Dairy Farming. Farming Press Books and Videos, Ipswich, 370
p (1991)
6. Jackson, W.: Livestock Farming (2004)
7. Lodewijks, G.: The two-dimensional behaviour of belt conveyors. In: Proceedings of the
Beltcon 8 Conference, Pretoria, South Africa, pp. 24–26 (1995)
8. Phillips, C.J.C.: Grazing Management and Systems, pp. 188–200 (2015)
9. Shi, F., Minzu, Z., Xiaofeng, S.: The application of iterative learning control algorithm in
weighing batch-ing system. In: Mechanical and Electrical Technology, pp. 47–50 (2014)
10. Hua, Z., Zhijiong, L., Rixing, C.: New automatic rubber mixer batching and weighing control
system. J. Weighing Apparatus 1–6 (2013)
11. Vidineev, Yu.D., Yanbukhtin, I.R.: Automatic Continuous Dosing of Bulk Materials. Energy,
Moscow, 120 p (1974)
12. Pershina, S.V., Katalymov, A.V., Odnolko, V.G., Pershin, V.F., Glinkina, T.M.: Weight
Batching of Granular Materials. Mechanical Engineering, Moscow, 260 p (2009)
13. Shestov, D.A.: Analysis of the processes of dosing and weighing bulk materials. In: Pro-
ceedings of the International Scientiﬁc and Technical Conference Energy Supply and Energy
Conservation in Agriculture. GNU VIESH, vol. 3, pp. 109–115 (2012)
14. Karande, A.M., Kalbande, D.R.: Weight assignment algorithms for designing fully connected
neural network. IJISA 6, 68–76 (2018). https://doi.org/10.5815/ijisa.2018.06.08
15. Dharmajee Rao, D.T.V., Ramana, K.V.: Winograd’s inequality: effectiveness for efﬁcient
training of deep neural networks. IJISA 6, 49–58 (2018). https://doi.org/10.5815/ijisa.2018.
06.06

124
D. Shilin et al.
16. Hu, Z., Tereykovskiy, I.A., Tereykovska, L.O., Pogorelov, V.V.: Determination of structural
parameters of multilayer perceptron designed to estimate parameters of technical systems.
IJISA 10, 57–62 (2017). https://doi.org/10.5815/ijisa.2017.10.07
17. Awadalla, M.H.A.: Spiking neural network and bull genetic algorithm for active vibration
control. IJISA 10(20), 17–26 (2018). https://doi.org/10.5815/ijisa.2018.02.02
18. Abuljadayel, A., Wedyan, F.: An approach for the generation of higher order mutants using
genetic algorithms. IJISA 10(1), 34–35 (2018). https://doi.org/10.5815/ijisa.2018.01.05
19. Shilin, D.V., Shestov, D.A., Ganin, E.: Improving the accuracy of weighing bulk materials in
a dispenser on-stream ﬂow meter with two strain gauges. Vestnik MEI 3(3), 116–123 (2019).
https://doi.org/10.24160/1993-6982-2019-3-116-1233
20. Kang, I.-J., Kwon, J.H., Moon, S.M., Hong, D.: A Control System Using Butterworth Filter
for Loss-in-Weight Feeders (2014). https://doi.org/10.7736/KSPE.2014.31.10.905
21. Siva Vardhan, D.S.V., Shivraj Narayan, Y.: Development of an Automatic Monitoring and
Control System for the Objects on the Conveyor Belt (2015). https://doi.org/10.1109/MAMI.
2015.7456594
22. Shilin, D., Shestov, D., Ganin, P., Novikov, A., Moskvin, V.: Development of an experimental
model of a ﬂow meter-batcher of various intensities. In: Annals of DAAAM and Proceedings
of the International DAAAM Symposium, vol. 31, issue 1, pp. 98–103 (2020). https://doi.
org/10.2507/31st.daaam.proceedings.013
23. Vasilyev, A.N., Vasilyev, A.A., Shestov, D.A., Shilin, D.V.: Mathematical Modeling of the
Work of the Flow-Meter Flowmeter-Doser, pp. 293–299 (2019). https://doi.org/10.1007/978-
3-030-00979-3_30
24. Kuo, B.C.: Automatic Control Systems, 7th edn. Prentice-Hall Inc, Englewood Cliffs New
Jersey, 1003 p (1995)
25. Zhi-yong, Z., Shi-ming, Y., ShiJin, P.: Three-Phase Asynchronous Motor Based on Fuzzy
PI Control of Simulink Modeling and Simulation, Mechanical and Electrical Engineering,
pp. 53–57 (2012)
26. Rivkin, M.: Bulk Material Handling: Practical Guidance for Mechanical Engineers. Partridge
Publishing Singapore, Singapore (2018)
27. Ross, S.M. Simulation. Taylor & Francis Group, Amsterdam, 297 p (2006)
28. Lieberwirth, H.: Design of belt conveyors with horizontal curves. In: Bulk Solids Handling,
№# 14, pp. 283–285 (1994)
29. Shestov, D.A., Shilin, D.V., Ganin, P.E.: Computer program 2018661990 Russian Federation,
Program for Control and Weighing of Bulk Materials, Applicant and patentee PRIZMA
LLC—App. 08/27/2018; publ. 25.09.2018
30. McGlinchey, D.: Bulk Solids Handling. C.O.S. Printers Pte Ltd, Singapore, 290 p (2008)
31. Chaturvedi, D.K.: Modeling and Simulation of Systems Using MATLAB and Simulink.
Taylor & Francis, Berkeley, 734 p (2009)

Spatial Analysis: Cases of Acute Bloody
Diarrhea in Baguio City, Philippines from 2015
to 2018
Guinness G. Maza(B), Kendrick Jules G. Zante, Clarence Kyle L. Pagunsan,
Angela Ronice A. Doctolero, Rostum Paolo B. Alanas, Criselda P. Libatique,
and Rizavel C. Addawe
Department of Mathematics and Computer Science, College of Science, University of the
Philippines Baguio, 2600 Baguio City, Philippines
ggmaza@up.edu.ph
Abstract. This study analyzes the spatial autocorrelation of Acute Bloody Diar-
rhea (ABD) prevalence in Baguio City, Philippines using records of ABD cases
from 2015 to 2018. The Global Moran’s I was used to identify the spatial pattern,
while the Local Moran’s I was utilized to determine if hotspots exist within the
area. Results showed a positive spatial autocorrelation in 2015 at a 0.05 signiﬁ-
cance level, indicating a clustered pattern of ABD prevalence. For the years 2016
to 2018, no signiﬁcant spatial patterns were found. ABD hotspots were identiﬁed
for each year. Clustering was observed in the central parts of Baguio City.
Keywords: Spatial analysis · Spatial autocorrelation · Hotspots · Acute bloody
diarrhea · Moran’s I
1
Introduction
Acute Bloody Diarrhea (ABD) is characterized by abdominal pain, dehydration, and
consecutive bowel movements with a loose, watery stool containing blood. ABD is also
known as dysentery and can be caused by various enteric pathogens and noninfectious
gastrointestinal illnesses [1]. It has been linked to Escherichia coli and other Shiga-toxin-
producing E. coli, a group of bacteria found in the intestines of humans and animals that
can cause symptoms through contaminated meat or water consumption [2, 3].
Diarrhea-related diseases have become a worldwide health concern, with nearly 1.7
billion cases and are responsible for the deaths of around 525,000 children annually
[4]. The number of reported food and waterborne diseases (FWBDs) in Baguio City
signiﬁcantly increased from 410 in 2005 to 1,938 in 2010, among which ABD had
the highest incidence rate [5]. Concerns of diarrhea epidemic remain as Baguio City
continues to face problems of water supply and access to potable water [6].
Environmental factors can alter the exposure pathways of FWBDs and can be inﬂu-
enced by climatic conditions, hence impacting the transmission and reproduction of
pathogens [7]. Therefore, understanding and addressing the burden of ABD and other
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 125–133, 2024.
https://doi.org/10.1007/978-3-031-50158-6_13

126
G. G. Maza et al.
FWBDs require the study of spatial patterns of disease occurrence. Spatial autocorrela-
tion, deﬁned as the statistical relationship between the values of a variable at different
locations, can provide insights into the factors contributing to disease patterns and help
identify potential interventions for time- and cost-efﬁcient disease control [8–10].
This study aims to (i) examine the spatial patterns of ABD among the barangays—
the smallest political unit of the Philippines—of Baguio City, (ii) identify the hotspots of
ABD from 2015 to 2018, and (iii) determine whether the identiﬁed hotspots demonstrate
any signiﬁcant patterns in the spread of the disease. The results of this study will be
used to assist the local government and health authorities in implementing policies and
programs aimed at preventing and controlling the spread of ABD in Baguio City and
similar communities, aligning with the United Nations Sustainable Development Goal
3: Good Health and Well-being.
The rest of this study is organized as follows. The methodology, area of the study, data
collection, the Moran’s I index, and the research instruments are discussed in Sect. 2.
The results of the descriptive and inferential statistics are presented in Sect. 3. Lastly,
the conclusion and recommendations of this study are presented in Sect. 4.
2
Methodology
This section presents all the details regarding the methods applied in the collection,
computation, and analysis of the data. This includes the scope, formulas, and instruments
used in the study.
2.1
Area of the Study
The study area comprises the whole of Baguio City. It is composed of 20 administrative
districts and 129 barangays. The City is situated in the province of Benguet, surrounded
by different municipalities such as La Trinidad on the North, Itogon on the East, and
Tuba on the South and West borders [11].
2.2
Data Collection
The Exploratory Data Analysis (EDA) team of the University of the Philippines Baguio
(UPB) and the Baguio City Health Services Ofﬁce (BCHSO) provided the records of
ABD cases in Baguio City from 2015 to 2018. The shapeﬁles (SHP ﬁles) containing the
geographical features of the barangays of Baguio City were also provided by the EDA
team of UPB. The factors considered for the study include the stool culture results—
both positive and unknown, the age of the patient in years, and the barangay where the
patients reside. The ABD prevalence was also calculated by dividing the total number
of cases per barangay by the total population per barangay in the given year.

Spatial Analysis: Cases of Acute Bloody Diarrhea in Baguio…
127
2.3
Moran’s I Index
The Global Moran’s I Index, an indicator for global spatial autocorrelation [12], was
utilized to assess if there exists a spatial correlation of ABD prevalence among the
barangays of Baguio City. A positive value of I would suggest a positive correlation or
clustered pattern, while a negative value indicates a negative correlation or dispersed
pattern. The null hypothesis for this study is I = 0, which implies complete spatial
randomness. The formula for Global Moran’s I is
I =
n n
i=1
n
j=1 wijzizj
S0
n
i=1 z2
i
(1)
where S0 = n
i=1
n
j=1 wij and zi = xi −¯x. The variable xi is the ABD prevalence in
each barangay i, xj is the ABD prevalence of the neighbors of i, n is the total number of
barangays in the study, ¯x is the mean ABD prevalence of all barangays, and wij is the
measure of the weight of barangay i and j that demonstrates the sharing of a boundary
between the two.
This study utilized the Rook contiguity weights matrix, which assigns a value of 1
to neighboring locations, where
wij =
1, if location i and j share a common borderline, and i ̸= j;
0, otherwise.
(2)
The weights wij were then divided by the number of neighbors of i. This process is also
known as row standardization, which ensures that each neighbor j contributes equally
to the value of i. The following formula for the z-score was used to test the signiﬁcance
of Global Moran’s I under randomization. Here,
zI = I −E(I)
√V(I)
(3)
where the expected value E(I) is computed as
E(I) = −
1
n −1.
(4)
The variance V(I) of the Global Moran’s I [13] is computed as
V(I) = n

(n2 −3n + 3)S1 −nS2 + 3S2
0

−a

(n2 −n)S1 −2nS2 + 6S2
0

(n −1)(n −2)(n −3)S2
0
−E(I)2
(5)
where S1 = 1
2
n
i=1
n
j=1(wij + wji)2, S2 = n
i=1
n
i=1 wij + n
j=1 wji
2, and a =
n
i=1 z4
i
n
i=1 z2
i
2 .
The resulting values from computing the Global Moran’s I index will only iden-
tify the spatial pattern and not provide the speciﬁc location of clusters [14]. Thus, the

128
G. G. Maza et al.
Local Moran’s I of the Local Indicator of Spatial Association (LISA) principle was
implemented in this study to specify the locations of possible hotspots. In each i,
Ii =
n
j=1 wij(xi −¯x)(xj −¯x)
n
i=1(xi −¯x)2
.
(6)
The Monte-Carlo method was used to test the signiﬁcance of the Local Moran’s I. It
formulates a pseudo-p-value using the following formula,
p = R + 1
M + 1
(7)
where R is the total number of random simulations greater than or equal to the computed
Local Moran’s I, and M is the number of permutations. Pseudo p-values that are less
than or equal to 0.05 will be considered hotspots.
2.4
Research Instruments
Three free and open-source software were utilized for data visualization and statistical
computation. First, RStudio was utilized for data visualization and computing the Global
Moran’s I. The speciﬁc packages used were sf [15] for importing SHP ﬁles, spdep [16]
for the Moran’s I analysis, readxl [17] to analyze data inside excel ﬁles, and plotrix
[18] for specialized plots. Second, Quantum Geographic Information System (QGIS)
[19] was used to edit and add variables to the SHP ﬁle of Baguio City. QGIS was also
utilized to produce the geometric map of the overlapping hotspots. Lastly, GeoDa [20]
was used to compute the Local Moran’s I and to generate LISA cluster maps.
3
Results and Discussion
This section shows the computed statistics and analysis of the data. This includes graphs,
maps, and tables containing the summary of results.
3.1
Descriptive Statistics
A total of 948 cases of ABD were reported in Baguio City from 2015 to 2018. Figure 1
shows the age group distribution of ABD patients. The age grouping was derived from
the Philippine Health Statistics’ standard, which is also based on the World Health
Organization’s standard [21].
Most of the reported cases were children and young adults. The age group 0 to 4
years old has the highest number of cases at 213, while the age group 60 to 64 has the
lowest number of cases at 21. According to Wardlaw et al. [22], children are more likely
to come into contact with unsanitary environments, especially when unsupervised. They
also have weaker immune systems compared to adults, making them more susceptible
to ABD compared to older age groups.
Figure 2 shows the number of monthly ABD cases from 2015 to 2018. A total of
347 ABD cases were reported in 2015, 260 cases in 2016, 215 cases in 2017, and 126
cases in 2018. Therefore, the number of ABD cases from 2015 to 2018 has decreased.

Spatial Analysis: Cases of Acute Bloody Diarrhea in Baguio…
129
Fig. 1. Age distribution of ABD patients from 2015 to 2018.
Fig. 2. Monthly ABD cases from 2015 to 2018.
3.2
Inferential Statistics
The results of the Global Moran’s I are shown in Table 1. There is a positive spatial
correlation in 2015 with a Global Moran’s I of 0.1145 and a p-value of 0.009, indicating
an ABD prevalence in Baguio City with a clustered pattern at 0.05 signiﬁcance level.
The p-values of the Global Moran’s I from 2016 to 2018 are greater than 0.05, implying
no signiﬁcant patterns of ABD prevalence.
The Local Moran’s I was utilized to determine the clusters per year from 2015 to
2018, as shown in the LISA cluster maps in Fig. 3. A High-High relationship indi-
cates that barangays with a high ABD prevalence surround a barangay with a high
ABD prevalence. A Low-Low relationship suggests that barangays with a low ABD
prevalence surround a barangay with a low ABD prevalence. A High-Low relationship
indicates that barangays with a low ABD prevalence surround a barangay with a high
ABD prevalence. A Low-High relationship suggests that barangays with a high ABD
prevalence surround a barangay with a low ABD prevalence. The clustering pattern
in 2015 was identiﬁed around the central parts of Baguio City, speciﬁcally barangays
Harrison-Claudio Carantes, Session Road Area, Salud Mitra, and Engineer’s Hill.

130
G. G. Maza et al.
Table 1. Global Moran’s I of ABD prevalence in Baguio City
Year
Moran’s I
p-value
2015
0.1145
0.009∗
2016
0.0393
0.1486
2017
−0.0064
0.5104
2018
0.0729
0.0513
∗Signiﬁcant at 0.05
Fig. 3. LISA cluster maps of ABD prevalence for (a) 2015, (b) 2016, (c) 2017, and (d) 2018
A total of 16 barangays were identiﬁed as ABD hotspots in 2015, 14 in 2016, 16
in 2017, and 19 in 2018 at a 0.05 signiﬁcance level. The test was done in GeoDa using
Monte-Carlo simulation at 999 permutations. Figure 4 shows the overlapping of hotspots
with 4-digit labels. That is, barangays highlighted as 1111 were hotspots of ABD from
2015 to 2018, while those labeled as 0000 were not hotspots from 2015 to 2018. Barangay
Malcolm Square-Perfecto, located in the center of Baguio City, was a hotspot of ABD
from 2015 to 2018 as shown in Fig. 4. Barangays Lourdes Subdivision Proper, Camp
Allen, and Bagong Lipunan were all ABD hotspots from 2016 to 2018. Barangays

Spatial Analysis: Cases of Acute Bloody Diarrhea in Baguio…
131
identiﬁed as ABD hotspots for two or more years were mostly situated at the center of
Baguio City.
Fig. 4. Hotspots of ABD from 2015 to 2018 where 1 indicates a hotspot in a particular year;
otherwise, 0. For example, 1100 means that the barangay was only a hotspot for 2015 and 2016.
4
Conclusion and Recommendations
The descriptive statistics of monthly cases showed decreased ABD cases from 2015
to 2018. Most reported cases belonged to younger age groups, with the highest being
children aged 0 to 4 years old. The spatial analysis result suggests a positive spatial
correlation in 2015, implying a clustered pattern of ABD, while spatial randomness was
found in 2016, 2017, and 2018. The LISA cluster maps showed that the ABD hotspots
from 2015 to 2018 were around the central parts of Baguio City, with barangay Malcolm
Square-Perfecto as a hotspot for all four years. The researchers recommend that the
local government and health authorities look into the health status of the identiﬁed ABD
hotspots and surrounding areas and assess potential risks of ABD infection among the
citizens. This study also recommends further investigation into other potential variables
that contribute to ABD incidence in Baguio City, such as environmental factors and the
socio-demographic proﬁle of the patients.
Acknowledgement. This study is supported by the University of the Philippines Baguio (UPB)
through its DATIVA-Rico Foundation Funds and the Department of Mathematics and Computer
Science International Publication Awards Funds. The researchers would like to thank the BCHSO
and the EDA team of UPB for the data and SHP ﬁles used in the study.

132
G. G. Maza et al.
References
1. Holtz, L.R., Neill, M.A., Tarr, P.I.: Acute bloody diarrhea: a medical emergency for patients
of all ages. Gastroenterology, vol. 136, pp. 1887–1898. Elsevier (2009). https://doi.org/10.
1053/j.gastro.2009.02.059
2. Talan, D.A., Moran, G.J., Newdow, M., Ong, S., Mower, W.R., Nakase, J.Y., Pinner, R.W.,
Slutsker, L.: Etiology of bloody diarrhea among patients presenting to United States emer-
gency departments: prevalence of Escherichia coli O157:H7 and other enteropathogens. In:
Clinical Infectious Diseases, vol. 32, pp. 573–580. The University of Chicago Press (2001).
https://doi.org/10.1086/318718
3. Rane, S.: Street vended food in developing world: hazard analyses. Indian J. Microbiol. 51,
100–106 (Springer) (2011). https://doi.org/10.1007/s12088-011-0154-x
4. World Health Organization: Diarrhoeal Disease (2017). Last Access 9 Jan 2023. https://www.
who.int/news-room/fact-sheets/detail/diarrhoeal-disease
5. Padilla, J.R.F., Pilar, K.C.N., Bitanga, C.A.G., Bumengeg, L.N., Addawe, R.C.: Incidence
of food and water-borne diseases in Baguio City. In: The 4th Innovation and Analytics
Conference and Exhibition, vol. 2318, p. 050024–5 (2019). https://doi.org/10.1063/1.512
1129
6. Mendoza, L.C., Cruz, G.A., Ciencia, A.N. and Penalba, M.A.: Local policy and water access
in Baguio City, Philippines. Int. J. Soc. Ecol. Sustain. Dev. (IJSESD) 11(1), 1–13 (2020).
https://doi.org/10.4018/IJSESD.2020010101
7. Semenza, J.C., Herbst, S., Rechenburg, A., Suk, J.E., Höser, C., Schreiber, C., Kistemann,
T.: Climate change impact assessment of food-and waterborne diseases. In: Critical Reviews
in Environmental Science and Technology, vol. 4, pp. 857–890. Taylor & Francis (2012).
https://doi.org/10.1080/10643389.2010.534706
8. Hershey, C.L., Doocy, S., Anderson, J., Haskew, C., Spiegel, P., Moss, W.J.: Incidence and risk
factors for malaria, pneumonia and diarrhea in children under 5 in UNHCR refugee camps:
a retrospective study. Conﬂict Health 5, 7 (BioMed Central) (2011). https://doi.org/10.1186/
1752-1505-5-24
9. Kamath, A., Shetty, K., Unnikrishnan, B., Kaushik, S., Rai, S. N.: Prevalence, Patterns, and
Predictors of Diarrhea: A Spatial-Temporal Comprehensive Evaluation in India. BMC Public
Health, vol. 18, pp. 1–10. Springer, Heidelberg (2018). https://doi.org/10.1186/s12889-018-
6213-z
10. Su, T., Liu, Y., Zhao, W., Yu, Q., Xie, Y., Li, Q., Qi, S.: Epidemiological characteristics and
spatial-temporal cluster analysis of other infectious diarrhea in Hebei Province from 2015 to
2020. Chin. J. Dis. Control Prev. 26(2), 175–181 (2022). https://doi.org/10.16462/j.cnki.zhj
bkz.2022.02.009
11. City Environment and Parks Management Ofﬁce: City Report, Baguio City, Philippines. In:
Eighth Regional EST Forum in Asia, p. 1 (2014). Last Access 18 Feb 2023: Retrieved from
https://www.uncrd.or.jp/content/documents/21038EST-City-Report_Philippines-Baguio.pdf
12. Moran, P.A.P.: The interpretation of statistical maps. J. R. Stat. Soc.: Ser. B (Methodological)
10, 243–251 (JSTOR) (1948). https://doi.org/10.1111/j.2517-6161.1948.tb00012.x
13. Bivand, R.S., Wong, D.W.S.: Comparing implementations of global and local indicators of
spatial association. TEST 27, 716–748 (2018). https://doi.org/10.1007/s11749-018-0599-x
14. Anselin, L.: Local indicators of spatial association-LISA. Geogr. Anal. 27(2), 93–115 (1995).
https://doi.org/10.1111/j.1538-4632.1995.tb00338.x
15. Pebesma, E.J.: Simple features for R: standardized support for spatial vector data. R. J. 10(1),
439 (2018). Last Access 24 Dec 2022. Retrieved from https://www.pebesma.staff.ifgi.de/RJw
rapper.pdf

Spatial Analysis: Cases of Acute Bloody Diarrhea in Baguio…
133
16. Bivand, R., Altman, M., Anselin, L., Assunção, R., Berke, O., Bernat, A., Blanchet, G.:
Package ‘spdep’. The Comprehensive R Archive Network, vol. 604, p. 605 (2015). Last
Access 24 Dec 2022. Retrieved from https://www.cran.microsoft.com/snapshot/2017-09-17/
web/packages/spdep/spdep.pdf
17. Wickham, H., Bryan, J., Kalicinski, M., Valery, K., Leitienne, C., Colbert, B., Hoerl, D.,
Miller, E. and Bryan, M.J.: Package ‘readxl’. Version 1.3, vol. 1 (2019). Last Access 24 Dec
2022. Retrieved from https://www.cran.microsoft.com/snapshot/2019-03-09/web/packages/
readxl/readxl.pdf
18. Lemon, J., Bolker, B., Oom, S., Klein, E., Rowlingson, B., Wickham, H., Tyagi, A., Eterra-
dossi, O., Grothendieck, G., Toews, M. and Kane, J.: Package ‘plotrix’. Vienna: R Develop-
ment Core Team (2015). Last Access 24 Dec 2022. Retrieved from https://www.mran.revolu
tionanalytics.com/snapshot/2020-04-25/web/packages/plotrix/plotrix.pdf
19. QGIS Development Team: Quantum Geographic Information System. Last Access 21 Dec
2022. www.qgis.org
20. Anselin, L., Syabri, I., Younghin, K.: GeoDa: an introduction to spatial data analysis. Geogr.
Anal. 38(1), 5–22 (2006). https://doi.org/10.1007/978-3-642-03647-7_5
21. Epidemiology Bureau, Department of Health: The 2019 Philippine Health Statistics, vol.
1, pp. 85–89. Philippine Health Statistics (2019). Last Access 5 Feb 2022. Retrieved from
https://www.doh.gov.ph/sites/default/ﬁles/publications/2019PHS_Final_092121.pdf
22. Wardlaw, T., Salama, P., Brocklehurst, C., Chopra, M., Mason, E.: Diarrhoea: why children
are still dying and what can be done. The Lancet 375(9718), 870–872 (2010). https://doi.org/
10.1016/s0140-6736(09)61798-0

The Economic Dimensions
of the Non-communicable Diseases: A Panel
Data Study
Sergio Arturo Domínguez-Miranda(B)
and Roman Rodriguez-Aguilar
Facultad de Ciencias Económicas y Empresariales, Universidad Panamericana, Mexico
University, Augusto Rodin 498, 03920 Mexico City, Mexico
0246533@up.edu.mx
Abstract. Noncommunicable diseases (NCD) are the leading causes of death and
the main public health problem worldwide [1] and are associated with an acute
picture of dyslipidemia that is part of the main risk factors along with smoking,
sedentary activities, incorrect diet, and genetic factors that have caused diseases
such as metabolic syndrome, oncological, cardiology, neurological and respiratory
diseases. NCD are a major problem in low-income and middle-income countries,
consuming increasing proportions of health care budgets. NCD are among the
leading causes of disability and ill health and are the leading cause of preventable
and premature death, having a signiﬁcant impact on economies. NCD generate
large out-of-pocket health costs for both individuals and families, as well as huge
health outlays in national budgets. The analysis proposed can provide critical
information to monitor trends in population health outcomes, recognize the pat-
tern of diseases and injuries affecting premature mortality and disability. This
paper shows an approach for the identiﬁcation of the behavior of four main NCD
(cardiovascular diseases, respiratory diseases, diabetes mellitus, and neoplasms)
along thirteen countries selected and various economic variables related to the
health and work issues from 1961 to 2021 with unbalanced data. The primary
focus is the analysis on mortality in population within working age.
Keywords: Noncommunicable diseases · Econometric models · Panel data ·
Working age · Health economics
1
Introduction
1.1
Overview of Noncommunicable Diseases Globally and Its Economical
Dimensions
For just over 20 years, NCD have occupied the ﬁrst places as causes of general death:
heart disease, stroke, and diabetes mellitus, being in the ﬁrst, second and ninth place
respectively [2]. Researchers in conjunction with the Pan American Health Organization
[3] estimated that from 2010 to 2030 an expenditure of 47 trillion dollars is expected
on the treatment of NCD, which is equivalent to a loss of 48% of annual global GDP
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 134–144, 2024.
https://doi.org/10.1007/978-3-031-50158-6_14

The Economic Dimensions of the Non-communicable Diseases
135
and impacts 4% of annual GDP in middle- to low-income countries. Other ones have
identiﬁed that about 60% of cases reviewed out-of-pocket expenses represent 20% to
30% of their income [4] estimated that in the United States of America there is a loss
of productivity in a range of 8539 to 10,175 dollars for cardiovascular diseases and
1,962,314 dollars per year due to diabetes mellitus with a premature mortality of 49% [5].
Consideringtheeffectsofpreventionandpromotiononhealth,companiesaresusceptible
to the effects generated by NCD in the absence of strategies for employee health, causing
presenteeism, poor job performance, and loss of productivity.
Several chronic and acute diseases are increasingly important, and the medical indus-
try is changing drastically due to the need for real-time diagnosis and monitoring of
long-term health conditions. However, in the absence of data analysis, and information,
it is difﬁcult to proceed with any care program, especially in support of the working-age
population to prevent and predict the behavior of health indicators in such a way that the
main question is: What is the behavior of NCD in the world and its relation to relevant
economic variables in the population whose age is within the work period?
Some research in the past had tried to explain certain social different variables with
different economic models [6, 7], some other developed a classiﬁcation method using
social-economic data for just one year [8], or for validating public policies base on one
data base along the time [9], but there is still a gap showing how some variables along
the time could affect the one of the main social problems, the health status.
This research article proposes to review the quantitative evidence of non-
communicable diseases for selected countries trying to better understand the impact
of relevant variables, analyzing the period from 1961 to 2021 through an unbalanced
data panel for a working age. The work methodology will be mentioned, considering the
way of extracting and ﬁltering variables, the mathematical model used, and the results
obtained from the analysis based on panel data in each NCD. The objective is to under-
stand the behavior of economic indicators, lifestyles, and diet in NCD mortality to ﬁnd
patterns of behavior, understand the impact of the most signiﬁcant variables, and be
able to support both companies and policyholders of public decisions to take the perti-
nent measures to improve health indicators, and therefore have an impact on productive
indicators.
2
Methodology
Econometrics with economic models can help identify whether there is a statistical
problem so that the researcher is able to interpret, summarize, and describe the situation
coherently. In the same way, it helps us to make valid inferences for a larger population
of individuals with similar characteristics and the samples are analyzed over time in the
panel data, usually before the intervention of the program and after the intervention,
getting two dimensions: spatial or structural and temporal [10]. It allows to model indi-
vidual observable differences and comes to solve one of the weaknesses of the classic
regression model, which is presence of heteroscedasticity.
Being a study focused on the behavior of NCD within the working period, age ﬁlters
between 20 and 64 years were used, as well as thirteen countries selected non-randomly
considering variability in their GDP, being analyzed: South Korea, China, India, Canada,

136
S. A. Domínguez-Miranda and R. Rodriguez-Aguilar
Colombia, Turkey, United States of America, Argentina, Mexico, Chile, Australia, Israel,
Brazil, Japan, France, Germany, and Poland in the period from 1961 to 2021 generating
a non-balanced panel.
Databases and variables shown in Table 1 from IHME [11], OECD [12], WHO
[13] and FAO [14] were used. Prevalence, incidence, and mortality of the four main
NCD were included [11], being cardiovascular diseases (which includes stroke and
ischemic heart disease, among others), respiratory diseases (only includes NCDs such
as chronic obstructive disease), diabetes mellitus (which includes the conditions that this
disease generates such as kidney diseases) and neoplasm (where all types of tumors are
included in the population). Population means of health indicators were selected [12].
Relevant elements involving lifestyles (such as years of life lost or exposure to pollution),
economic variables (such as investment and government spending on health) were used
and variables related to work (such as productivity or economically active population)
using the ﬁlters initially considered [13]. Finally, indicators related to eating behavior
(such as consumption of meat or sugary drinks) were included [14].
An unbalanced data panel matrix was integrated, obtaining observations from 1961
to 2021 getting a matrix of dimensions 61 × 578. Several methods were used to elim-
inate multicollinearity to avoid the problem that could lead to statistically insigniﬁcant
variables and, on the other hand, to the inclusion of new variables.
Traditionally, the statistical methods that try to explain an observed phenomenon
through a series of variables have been treated by linear regressions, using the OLS
method. To evaluate this condition, the Hausman test can be used, ﬁrst making an esti-
mation by means of OLS and then making a panel of data, to ﬁnally run the Hausman
analysis [15]. Since there may be differences in the behavior of individuals, the source of
sample variation is important in the formulation and estimation of our economic model.
For the case where the endogenous variable Yi and several explanatory variables X1,
X2,…, Xk are observed for each individual and period, we could have the model of
constant coefﬁcients or data pool, where the intersection and the coefﬁcients are con-
stant with respect to time and between individuals. Therefore, the possible differences
between individuals and different moments of time are assimilated to the random term.
Using the formula (Eq. 1):
Yit = β1 + β2Xit + β3Xit + . . . + βkXit + uit
(1)
where Y is the variable focused on the outcome of NCD, in this case mortality as the main
dependent variable, also evaluating prevalence and incidence. The i value was used for
the cross-sectional variables related to the countries, and t for the time from the period
1961 to 2021. The difference, between the countries in their different variables and time
periods, was added to the model, seeking to reduce autocorrelation problems because
the variance of the disturbances can be different with respect to countries or over time,
and/or heteroscedasticity.
As all the slopes or coefﬁcients of the variables are constant, but not the intersection,
the model was adjusted looking for heterogeneity in the behavior of the teaching units

The Economic Dimensions of the Non-communicable Diseases
137
Table 1. Data bases used for analysis
Variables
Unit of measure
Incidence
Number of people
[11]
Permanence
Number of people
[11]
Deaths
Number of people
[11]
Average systolic blood pressure
MmHg
[12]
Average HDL cholesterol
Mmol/L
[12]
Prevalence of hypertension
Average years
[12]
Average rapid glucose
Mmol/L
[12]
BMI_Promedio
kg/m2
[12]
Out-of-pocket expenditure
USD per capita
[13]
Government Investment
USD per capita
[13]
Years lost to live
Years per 100,000 habitants
[13]
Government investment
% Of GDP
[13]
Economic active population
% With respect to population
[13]
Productivity
GDP/hour/person
[13]
Health expenditure
USD per capita
[13]
Insurance spending
USD per capita
[13]
Government health expenditure
USD per capita
[13]
Total out of pocket expenditure
USD per capita
[13]
Total population
Millions of people
[13]
Pollution exposure
Micrograms/mts3/population
[13]
Wheat and products Food supply quantity
Kcal/person/day
[14]
Processed rice—Food supply quantity
Kcal/person/day
[14]
Consumption of alcoholic beverages
Liters per capita
[14]
Beef Food supply quantity
Kcal/person/day
[14]
Meat consumption
Kilograms per capita
[14]
Maize and products Food supply quantity
Kcal/person/day
[14]
Food supply sweeteners
Kcal/person/day
[14]
collected through the independent terms, that is, in the case of equality of the mean
values of the explanatory variables across countries, the mean value of the dependent
variable would be different. Therefore, various variables (D) relative to the variables
from the databases were assigned to model the independent parameter for each of the
countries (i) (Eq. 2).
Yit = α1 + α2D2 + α3D3 + . . . + αNDN + β2X2it + β3Xit + uit
(2)

138
S. A. Domínguez-Miranda and R. Rodriguez-Aguilar
Variables were transformed into deviations from their temporal mean for every case.
To determine if the heterogeneity of the model comes from the differences in the inde-
pendent term, and, therefore, that the model can be conducted by means of a test based
on the F statistic, which is formulated as follows (Eq. 3):
FN−1,NT−k−N =
R2
G−R2
rest
N−1
1−R2
G
NT−k−N
(3)
where R2
G is the coefﬁcient of determination of the general model and R2
rest is the coef-
ﬁcient of determination of the restricted model called covariance analysis. Considering
that the coefﬁcients are constant, but the intersection varies according to the countries
and time, the model can be used (Eq. 4):
Yit = α1 + α2D2 + α3D3 + . . . + αNDN + γ1 + γ2 + . . . γt + β2X2it + β3Xit + uit
(4)
For the multivariate case: Yit = β1 + βkXik + uit where Y is a matrix of k explanatory
variables derived from the one reﬂected in the different databases for each one of the
countries so that:
Yit =
y1t
y2t
:
y Nt
where is the Yit vector containing the information of the countries (i) in
all t of selected time. Xkit is the matrix of observations of the explanatory variables of
the indicators used according to Table 1 (k), for the countries (i), from 1961 to 2021 (t).
Xkit =
X11t
X12t
:
X 1Nt
X21t
X22t
:
X 2Nt
..
..
..
Xk1t
Xk2t
:
X kNt
in turn βk =
β1
β2
:
β N
and Uk =
u1t
u2t
:
u Nt
vector containing the
t random perturbations of each individual variables.
The parameter vector α′i = [α1, α2,…, αN] collects the individual effects. The
assumptions made in these models are fundamentally the non-correlation between the
perturbations of each of the groups, and the temporal non-correlation, and that the vari-
ances of the perturbations are homoscedastic and not self-correlated, meaning that:
E[uit] = 0; var[uit] = σ 2; cov

uit, ujs

= 0.
The mortality of the four main NCD (Cardiovascular, diabetes mellitus, neoplas-
tic, respiratory) was selected a dependent variable (Y), in addition, it was veriﬁed the
variables of the state of health and diet that could have a greater impact on the disease
were analyzed based on the literature regarding the behavior of the disease in all the
variables those related to health expenditure, social determinants and population growth
(k). Subsequently, from the variables used, and using the pooled least squares in E-views
software application, those that were not signiﬁcant were eliminated, leaving only those
that generated statistical signiﬁcance.
Three models were generated for mortality derived from cardiovascular diseases,
three derived from diabetes mellitus, one for neoplastic diseases and two for respiratory

The Economic Dimensions of the Non-communicable Diseases
139
diseases due to different explanations found variating indicators within the models.
In addition to explain more information with the Neoplasm disease, the variable of
incidence was used due to the high impact in the main variable and explained later.
Among the estimated models, the most appropriate ones were selected based on the
overall signiﬁcance, the number of explanatory variables and R2.
3
Results
3.1
Cardiovascular
Table 2 shows in model 1 that incidence (t-value = 35.32) and investment in health (t-
value = −25.06) are the elements with the greatest signiﬁcance (both with p = 0.000),
which reinforces the concept if investments in health are reduced, the impact will be
growth in mortality. On the other hand, incidence is one of the main causes of mortality,
which agrees with the literature. Additionally, the working population is observed as
relevant ﬁndings with a negative impact on the indicator, this probably due to the activity
generated by work in people who maintain physical activity. It´s also observed that when
using a lag of value 3 (equivalent to 3 years) in the average body mass index, this impacts
on cardiovascular disease, this can be understood that the impact of not maintaining a
healthy life is not reﬂected in the impact of the disease until later. Similarly, food serves as
a relevant element in cardiovascular diseases as it can be observed with the consumption
of sweetening beverages (p = 0.000; t = 9.8) as well as meat consumption (p = 0.000;
t = 5.7).
3.2
Diabetes Mellitus
It is observed according to Table 3 in model 1 that the consumption of wheat (p = 0.071),
rice (p = 0.0 76) and sweetening beverages (p = 0.045) has a relevant impact on the
evolution of mortality due to diabetes mellitus, in the case of wheat and rice products
it is again shown, that using lags of order 5 and 3 respectively, impacts on mortality,
an additional indication that correct feeding is essential. Regarding model 2, population
indicators were used, where population growth (p = 0.000) is the most relevant.
Additionally, the working population shows a positive signiﬁcance (p = 0.026),
unlike cardiovascular diseases, it is likely that the explanation is that, although work
generates physical activation, it does not necessarily go hand in hand with food, a sit-
uation that agrees with the literature when showing that diabetes diseases, although it
is multifactorial, food is of great relevance. As relevant data for model 3 it is shown in
this case that the body mass index shows lag at one year (p = 0.008) is the element with
greater signiﬁcance along with the incidence of this disease.
3.3
Neoplasm Diseases
Table 4 shows only one construction model to explain what happens with Neoplasm
disease, the variables that explain this phenomenon seem logical at the beginning because
they depend on the incidence (p = 0.000) and the total population mainly (p = 0.000),

140
S. A. Domínguez-Miranda and R. Rodriguez-Aguilar
Table 2 PLS results—cardiovascular diseases
Dependent variable: Mortality from cardiovascular diseases
Independent variables
Model 1
Model 2
Model 3
Mean systolic pressure
673.10(0.000)
Prevalence of hypertension
−420.89(0.000)
Body mass index (With 3 lags)
1249.47(0.000)
4612.50(0.005)
Investment in health
−3.98(0.000)
−2232.81(0.084)
Total population
1154.79(0.000)
Working population
−635.31(0.000)
1798.61(0.002)
Out-of-pocket expense
15.93(0.000)
Cardiovascular disease incidence
0.03(0.000)
0.051(0.000)
Cardiovascular disease prevalence
1907.92(0.000)
−0.002(0.000)
Meat consumption (With 5 lags)
128.57(0.000)
Consumption of sweetened beverages
422(0.000)
AR(1)
1.62(0.000)
1.48(0.000)
AR(2)
−0.68(0.000)
−0.98(0.000)
AR(3)
0.5(0.000)
AR(8)
−0.028(0.031)
R2
0.9991
0.9992
0.9996
R2 Adjusted
0.9990
0.9992
0.9996
Standard regression error
668.44
2227.98
11668.75
Durbin-Watson
1.81
2.022
2.03
Remarks
73
400
322
so we proceeded to carry out a second approach to understand in detail the incidence as
a response to the understanding of mortality.
Table 5 shows the application of a semilogarithmic system in the dependent, to
understand the behavior of the growth of the incidence in Neoplasm diseases. In this
sense, it’s shown that there are elements that impact on this disease derived from food,
but the impact is not so high as it has low coefﬁcients compared to diabetes mellitus or
cardiovascular disease. In this model, per capita alcohol consumption now appears (p =
0.068; t = 1.8), although with low explanation.
3.4
Respiratory Diseases
Table 6 now shows the analysis of respiratory diseases where 2 models are used, in
the ﬁrst shows a logical element that is derived from population growth (p = 0.000),
however in a second model there are relevant elements such as exposure to pollution (p
= 0.002; t = 3.03) that is relevant in the analysis, Although the incidence also appears

The Economic Dimensions of the Non-communicable Diseases
141
Table 3 PLS results—diabetes mellitus
Dependent variable: Mortality from diseases caused by diabetes mellitus
Independent variables
Model 1
Model 2
Model 3
Wheat consumption (5 lags)
13.01(0.071)
Rice consumption (3 lags)
34.44(0.076)
Consumption of sweetened beverages
87.12(0.047)
Body mass index (1 lag)
1650(0.008)
Average fasting blood glucose
−7249.77(0.012)
Total population
63.43(0.000)
61.42(0.000)
Working population
65.54(0.026)
Productivity per hour per person
−17.91(0.014)
Incidence of diabetes mellitus diseases
0.014(0.000)
Prevalence of diabetes mellitus diseases
−0.0006(0.004)
AR(1)
1.35(0.000)
1.47(0.000)
1.41(0.000)
AR(2)
−0.35(0.000)
−0.44(0.000)
−0.40(0.000)
R2
0.9996
0.9995
0.9995
R2 Adjusted
0.9995
0.9995
0.9995
Standard regression error
699.23
250.20
743.72
Durbin-Watson
1.82
1.89
1.83
Remarks
374
340
352
as an explanatory part of this phenomenon, it is correct to say that the impact is low (p
= 0.1075; t = 1.6) as well as its probability.
4
Discussions and Conclusions
NCD have had a relevant increase with economically active population, therefore, dis-
eases are worsening in the world. With mathematical models like used in this research
helps to understand the relationship between the economic factors considered in the
health sector with chronic diseases.
The labor variables like productivity, working productivity as well as economic
variables like investment in health and the out-of-pocket budget affect are factors that
impact on the NCD and affect the mortality.
It’s notable that the impact on mortality derived from diseases by diabetes mellitus
and cardiovascular have two important effects, on the one hand, the approach of eating
habitsandphysicalactivity,whichimpactsonthedisease.Itwasobservedthatthefeeding
of products derived from wheat, corn, as well as sweetening drinks, have a signiﬁcance
in mortality. From the economic point of view, the reduction of investments in the health
sector impacts cardiovascular disease. The literature shows that arterial hypertension is

142
S. A. Domínguez-Miranda and R. Rodriguez-Aguilar
Table 4 PLS results—neoplasm mortality
Dependent variable: Mortality from diseases caused by neoplasm diseases
Independent variables
Model 1
Productivity per hour
−19.93(0.026)
Total population
144.98(0.000)
Incidence of Neoplasm diseases
0.012(0.000)
AR(1)
1.56(0.000)
AR(2)
−0.55(0.000)
R2
0.9996
R2 Adjusted
0.9995
Standard regression error
332.68
Durbin-Watson
2.03
Remarks
360
Table 5 PLS results—neoplasm incidence
Dependent variable: Increase in the incidence of diseases caused by Neoplasm diseases
Independent variables
Model 1
Alcohol consumption
0.007(0.068)
Working population
0.024(0.002)
Meat consumption
0.0005(0.041)
AR(1)
2.02(0.000)
AR(2)
−1.27(0.000)
AR(3)
0.24(0.000)
R2
0.9999
R2 Adjusted
0.9999
Standard regression error
757.95
Durbin-Watson
1.94
Remarks
238
relevant for cardiovascular disease [16], an element that is corroborated by the analyzes
performed, however an element to denote is that the prevalence in hypertension shows
a negative signiﬁcance, it is highly probable that this is because when there is control of
hypertension, mortality decreases.
In the case of Neoplasm diseases, although there are essential elements to discuss
again such as investment in health and food, they do not show high signiﬁcance, so other
variables should be considered such as genetic load, ethnological proﬁle, or exposure

The Economic Dimensions of the Non-communicable Diseases
143
Table 6 PLS results—respiratory diseases
Dependent variable: Mortality from diseases caused by respiratory diseases
Independent variables
Model 1
Model 2
Total population
111.32(0.0)
Exposure to pollution
129.94(0.002)
Incidence of respiratory diseases
0.005(0.1075)
AR(1)
2.02(0.000)
AR(2)
−1.27(0.000)
R2
0.9999
0.9996
R2 Adjusted
0.9999
0.9996
Standard regression error
757.95
998.70
Durbin-Watson
1.94
2.23
Remarks
238
136
to various elements as sun or radiation. Further research in this area is necessary to
statistically understand the behavior of the disease.
In mortality derived from respiratory diseases, only one relevant element with signiﬁ-
cance was found, which is exposure to pollution, it is relevant to ﬁnd additional indicators
that show exposure to other environments or in contact with dangerous elements in the
work environment. This indicator gives us the opportunity for further research into ele-
ments of exposure to which workers and people may have contact and can generate
problems to increase respiratory disease, and, therefore, death.
Finally, in all models is found as a variable a signiﬁcant autoregressive model (p
= 0.000), explaining that the effect of mortality is repeated in 2, 3 and up to 8 years,
this shows us the need to invest in health because the effect that occurs now by the
improvement in prevention or early diagnosis is reﬂected in 2, 3 or up to 8 years.
More research is necessary to elucidate the factors related with NCD in each of the
countries and more precisely the economic level to which they belong to be able to better
evaluate the disease, however, with the information found, it is possible to recommend
appropriate strategies for the early diagnosis and control of NCD to reduce mortality as
well as policies to motivate people and employees to improve nutrition, on the other hand
investments in the health sector, although they have been a constant recommendation,
this time it is observed in the analyses, mainly in cardiovascular disease, which represents
the leading cause of death in the world, as a crucial element to minimize the problem of
mortality.
References
1. Cordova-Villalobos, J.A., et al.: Chronic noncommunicable diseases in Mexico: epidemio-
logical overview and comprehensive prevention. Salud Pública de México 50(5), 419–427
(2008). https://doi.org/10.1590/s0036-36342008000500015
2. WHO, World Health Organization: The top 10 causes of death (2020). https://www.who.int/
news-room/fact-sheets/detail/the-top-10-causes-of-deat

144
S. A. Domínguez-Miranda and R. Rodriguez-Aguilar
3. OPS, Pan American Health Organization: Leading causes of mortality and disability (2022).
Last accessed 2023/29/03. https://www.paho.org/es/enlace/causas-principales-mortalidad-
discapacidad
4. Jaspers, L., et al.: The global impact of non-communicable diseases on households and impov-
erishment: a systematic review. Eur. J. Epidemiol. 30(3), 163–188 (2014). https://doi.org/10.
1007/s10654-014-9983-3
5. Chaker, L., et al.: The global impact of non-communicable diseases on macro-economic
productivity: a systematic review. Eur. J. Epidemiol. 30(5), 357–395 (2015). https://doi.org/
10.1007/s10654-015-0026-5
6. Rodríguez-Aguilar, R., Rivera-Peña, G., Ramírez-Pérez, H.X.: Household expenditure in
health in Mexico, 2016. In: Vasant, P., Zelinka, I., Weber, G.-W. (eds.) ICO 2019. AISC, vol.
1072, pp. 662–670. Springer, Cham (2020). https://doi.org/10.1007/978-3-030-33585-4_64
7. Rashida, M., Iffath, F., Karim, R., Areﬁn, M.S.: Trends and techniques of biomedical text
mining: a review. In: Vasant, P., Zelinka, I., Weber, G.-W. (eds.) ICO 2021. LNNS, vol. 371,
pp. 968–980. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-93247-3_92
8. Wang, Y., Wang, J.: Modelling and prediction of global non-communicable diseases. BMC
Public Health 20, 1–13 (2020). https://doi.org/10.1186/s12889-020-08890-4
9. Allen, L.N., Wigley, S., Holmer, H.: Implementation of non-communicable disease policies
from 2015 to 2020: a geopolitical analysis of 194 countries. Lancet Glob. Health 9(11),
e1528–e1538 (2021). https://doi.org/10.1016/S2214-109X(21)00359-4
10. Perazzi, J.R., Merli, G.O.: Panel data regression models and their application in the evaluation
ofimpactsofsocialprograms.Telos16(1),157–164(2014).https://www.redalyc.org/pdf/993/
99330402007.pdf
11. IHME—Institute for Health Metrics and Evaluation: Global Burden of Disease Study 2019,
Results (2020). https://vizhub.healthdata.org/gbd-results/
12. WHO, World Health Organization: The Global Health Observatory (2022). Last
accessed 2023/03/29. https://www.who.int/data/gho/data/indicators/indicator-details/GHO/
mean-bmi-(kg-m-)-(age-standardized-estimate
13. OECD: Working Age Population (Indicator) (2022). https://doi.org/10.1787/d339918b-en.
https://data.oecd.org/pop/working-age-population.htm
14. FAO: Food Balances (2022). Last accessed 2023/03/29. https://www.fao.org/faostat/en/#data/
FBSH
15. Wooldridge, J.M.: Introductory econometrics: a modern approach. Cengage Learning (2015)
16. de la Torre Díez, I., Garcia-Zapirain, B., Méndez-Zorrilla, A., López-Coronado, M.: Moni-
toring and follow-up of chronic heart failure: a literature review of eHealth applications and
systems. J. Med. Syst. 40(7), 1–9 (2016). https://doi.org/10.1007/s10916-016-0537-y

Re-strengthening of Real Sized RC Beams
Subjected to Corrosion Using Glass Fiber
Reinforced Polymer Sheets
Sunil Garhwal1,2(B), Shruti Sharma2, Sandeep Kumar Sharma3, Anil Garhwal2,
and Anirban Banik4
1 Department of Civil Engineering, MMEC, Maharishi Markandeshwar University, Mullana,
Ambala 133203, India
sunil12garhwal@gmail.com
2 Department of Civil Engineering, Thapar Institute of Engineering and Technology, Patiala,
Punjab 147004, India
shrutisharma.ced@thapar.edu
3 Department of Mechanical Engineering, Thapar Institute of Engineering and Technology,
Patiala, Punjab 147004, India
sksharma@thapar.edu
4 Department of Civil Engineering, National Institute of Technology Sikkim, Ravangla, South
Sikkim 737139, India
Abstract. Reinforced concrete (RC) structures have been an integral part of
today’s civilization. The strength and stability of a RC structure come to stake
when it is exposed to corrosion. Thus the focus of the research is to inspect real
sized Reinforced Concrete (RC) beams subjected to various level of corrosion
and re-strengthening them using Glass Fiber Reinforced Polymer (GFRP). With
increase in the corrosion level the RC beams indicated increase in brittleness and
reduced ductility as the steel area is reduced drastically. A substantially increase
in ultimate load and deﬂection characteristics is observed when corroded beams
were repaired using GFRP. This Research facilitate the use of GFRP sheets to
rehabilitate structures subjected to savior damage due to corrosion.
Keywords: Corrosion · GFRP repairing · Deﬂection · Ultimate load · Ductility
1
Introduction
The most often used building material is cement concrete that has been strengthened
with steel bars. The vulnerability of these buildings to environmental attack is a serious
issue that can signiﬁcantly lower their strength and lifespan [1]. When it’s humid, air pol-
lution seeps through the concrete cover and corrodes the steel reinforcing. The corrosion
products that arise take up space that is several times more than that of the steel [2–8].
Tensile forces imposed on by the increased volume induce cracking, delamination, and
spalling in the concrete. As a result, the corrosion is accelerated and the reinforcements
are vulnerable to immediate exposure to aggressive agents. In addition to being poor in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 145–152, 2024.
https://doi.org/10.1007/978-3-031-50158-6_15

146
S. Garhwal et al.
aesthetic, it signiﬁcantly compromises the concrete structure. Moreover, it also effects
the interfacial bond between steel and concrete which ultimately inﬂuence the ultimate
load carrying capacity of structure. Fiber reinforced polymer have emerged as an efﬁ-
cient and effective repairing material which not only strengthens the RC structure but
also act as protecting shield to aggressive agents and prevent further corrosion [9–15].
In the present study real size RC beams were cast and were subjected to various level of
corrosion and were compared with the rehabilitated beams.
We would like to draw your attention to the fact that it is not possible to modify a
paper in any way, once it has been published. This applies to both the printed book and
the online version of the publication. Every detail, including the order of the names of
the authors, should be checked before the paper is sent to the Volume Editors.
2
Experimental Program and Methodology
2.1
Specimen Details
Seven RC beams of size 127 mm × 227 mm × 4100 mm were cast for the experimental
investigation using a design mix proportion of 1:1.5:3 (cement, sand, aggregate). As
indicated in Table 1, the damaged beams advanced to various stages of corrosion before
being further repaired by GFRP wrapping. The procedure for accelerated corrosion of
beams and the repair method using GFRP have been detailed in the following parts.
Table 1. Nomenclature used for specimens
Specimen
(Corroded at constant voltage of 10 V)
C-0
Control
C-20
Corroded for 20 days
C-30
Corroded for 30 days
C-40
Corroded for 40 days
H-20
C-20 repaired with GFRP
H-30
C-30 repaired with GFRP
H-40
C-40 repaired with GFRP
2.2
Accelerated Corrosion of RC Beams
As the wet and dry processes takes much longer time, the RC beams in this study were
exposed to expedited chloride-induced corrosion utilising the impressed current method.
The centre 1.5 m of the beam had stainless steel wire mesh wrapped around it, and a
water tank system was used to continually provide 3.5% NaCl solution (Fig. 1). The
corrosion of RC beams was carried out at a constant voltage of 10 V. The negative end of
the power source was connected to the wire mess acting as cathode. The positive terminal
of the power supply was linked to the Main steel acting as anode. Each corrosion cycle
lasted 20, 30, and 40 cycles on six identical RC beams.

Re-strengthening of Real Sized RC Beams Subjected to Corrosion
147
Fig. 1. Accelerated impressed current corrosion set up
2.3
Repairing of Corroded Beams
RC beams subjected to varying level of corrosion damaged (C-20, C-30, and C-40) was
Re-strengthened using GFRP sheets. The re-strengthening was conducted in ﬁve steps.
In ﬁrst step concrete damaged due to tensile pressure exerted by corrosion product was
removed by tapping. In step two the exposed surface was cleaned and two layers of Dr.
Fixit Epoxy 211 was applied so as a proper bond between old concrete and new concrete
can be achieved. Further in step three micro-concrete placed layer by layer. In step four
the surface of harden micro concrete was grinded to make it smooth so to prevent any
air gap between concrete surface and GFRP sheets. In the last step two layers of GFRP
sheets were applied at the bottom of the beam and a U-wrap in the middle portion to
provide conﬁnement pressure on micro-concrete (Fig. 2).
3
Results
3.1
Load Deﬂection Characteristics
The corresponding loads and deﬂections were measured at mid-span on corroded beams
and their GFRP repairs after they had been loaded and tested in two-point bending
to failure. It was demonstrated that when corrosion levels increased, the load-bearing

148
S. Garhwal et al.
Fig. 2. Repairing of damaged RC beams with GFRP
capacity declined noticeably, but the GFRP-repaired corroded beams had dramatically
enhanced load-bearing capacity that was similar to the control beams. RC beam (C-0)
with no damage was subjected to ﬂexural loading and load deﬂection parameters were
recorded and kept as a base line parameter. The beam successfully undertook a load of
49.56 KN and deﬂection of 121 mm.
For beams corroded to varying degrees, a signiﬁcant decrease in load-bearing capac-
ity and deﬂection at the mid-span is observed with increasing corrosion. For the C-20
beam, maximum load is signiﬁcantly reduced to 31.44 KN and mid-span deﬂection of
52.91. For the severely corroded beam (C-30), a sudden mid-span failure with corro-
sion cracks integrated into large vertical cracks is observed at a maximum deﬂection of
47.59 mm and a load of 22 kN. Finally, a severely degraded C-40 beam with numerous
vertical corrosion cracks along with longitudinal cracks suddenly failed at a very small
load of 17.43 kN with a mid-span deﬂection of 31.48 mm. Therefore, the reduction in
ultimate load and mid-span deﬂection with increasing corrosion levels is alarming and
should be carefully addressed.
In addition, girders corroded to varying degrees and repaired with micro-concrete
and GFRP coating showed signiﬁcantly improved load-bearing capacity compared to
the original corroded beams. The H-20 beam reported an increase in ultimate load of
46.27 KN and major increase in ultimate deﬂection almost equal to health specimen that
is 101.85 mm. RC beam repaired after 30 and 40 days of corrosion that is H-30 and
H-40 reported an increase in load and deﬂection. H-30 showed an increase in ultimate
load as compared to C-30 from 22 KN to 43.15 KN and Deﬂection from 47.59 to
95.83 mm. Similarly, H-40 showed an impressive improvement in ultimate load of 32
KN and deﬂection of 77.3 mm. This increase in ultimate load and deﬂection of repaired
specimen clearly demonstrate the effectiveness of GFRP sheets in rehabilitating the
structures (Fig. 3) (Table 2; Figs. 4 and 5).

Re-strengthening of Real Sized RC Beams Subjected to Corrosion
149
Fig. 3. Load-deﬂection characteristics of corroded and repaired beams, a. Load deﬂection—
healthy beam

150
S. Garhwal et al.
Fig. 3. (continued)
Fig. 4. Load characteristics of corroded and repaired beams
4
Conclusions
This study focuses on evaluating the performance of real sized RC beams corroded to
various levels and repaired with GFRP.
Following major conclusions have been drawn from the study:
1. Visible inspections reveal that RC beam corrosion damage is accelerating, growing
the huge longitudinal fractures and causing transverse cracking of the rust-covered
beams.
2. The results of bending tests on corroded RC beams demonstrate that as corrosion
level rises, maximum load capacity and maximum deﬂection decrease.
3. Increase in ultimate load and deﬂection is observed when repaired using GFRP sheets.

Re-strengthening of Real Sized RC Beams Subjected to Corrosion
151
Fig. 5. Deﬂection characteristics of corroded and repaired beams
Table 2. Ultimate load and deﬂection of corroded & GFRP repaired specimen
Specimen
Load (KN)
Deﬂection (mm)
Healthy
49.56
121.053
C-20
31.44
52.91
C-30
22.015
47.59
C-40
17.43
31.486
H-20
46.275
101.857
H-30
43.15
95.833
H-40
32.025
77.316
References
1. Elsener, B., Angst, U.: Corrosion inhibitors for reinforced concrete. In: Science and
Technology of Concrete Admixtures (2016)
2. Garhwal, S., Sharma, S., Sharma, S.K.: Acoustic emission monitoring of RC beams corroded
to different levels under ﬂexural loading. Arab. J. Sci. Eng. 46(5), 4319–4335 (2020). https://
doi.org/10.1007/s13369-020-04930-8
3. Garhwal, S., Sharma, S., Sharma, S.K.: Monitoring the ﬂexural performance of GFRP repaired
corroded reinforced concrete beams using passive acoustic emission technique. Struct. Concr.
(2020). https://doi.org/10.1002/suco.202000247
4. Garhwal, A., Sharma, S., Danie Roy, A.B.: Performance of expanded polystyrene (EPS)
sandwiched concrete panels subjected to accelerated corrosion. In: Structures, vol. 43,
pp. 1057–1072 (2022)

152
S. Garhwal et al.
5. Sharma, S., Mukherjee, A.: Monitoring corrosion in oxide and chloride environments using
ultrasonicguidedwaves.J.Mater.Civ.Eng.(2011).https://doi.org/10.1061/(ASCE)MT.1943-
5533.0000144
6. Sharma, A., Sharma, S., Sharma, S., Mukherjee, A.: Investigation of deterioration in corroding
reinforced concrete beams using active and passive techniques. Constr. Build. Mater. (2018).
https://doi.org/10.1016/j.conbuildmat.2017.11.165
7. Shan, H., Xu, J., Wang, Z., Jiang, L., Xu, N.: Electrochemical chloride removal in reinforced
concrete structures: improvement of effectiveness by simultaneous migration of silicate ion.
Constr. Build. Mater. (2016). https://doi.org/10.1016/j.conbuildmat.2016.09.137
8. Ohtsu, M., Tomoda, Y.: Phenomenological model of corrosion process in reinforced concrete
identiﬁed by acoustic emission. ACI Mater. J. (2008). https://doi.org/10.14359/19764
9. Gadve, S., Mukherjee, A., Malhotra, S.N.: Corrosion protection of ﬁber-reinforced polymer-
wrapped reinforced concrete. ACI Mater. J. (2010). https://doi.org/10.14359/51663860
10. Siddika, A., Al Mamun, M.A., Alyousef, R., Amran, Y.H.M.: Strengthening of reinforced
concrete beams by using ﬁber-reinforced polymer composites: a review. J. Build. Eng. (2019).
https://doi.org/10.1016/j.jobe.2019.100798
11. Mukherjee, A., Boothby, T.E., Bakis, C.E., Joshi, M.V., Maitra, S.R.: Mechanical behavior
of ﬁber-reinforced polymer-wrapped concrete columns—complicating effects. J. Compos.
Constr. (2004). https://doi.org/10.1061/(ASCE)1090-0268(2004)8:2(97)
12. Mukherjee, A., Gadve, S., Malhotra, S.: Active protection of FRP wrapped reinforced concrete
structures against corrosion. In: Concrete Solutions (2009)
13. Kreit, A., Al-Mahmoud, F., Castel, A., François, R.: Repairing corroded RC beam with near-
surface mounted CFRP rods. Mater. Struct. Constr. (2011). https://doi.org/10.1617/s11527-
010-9693-6
14. Triantaﬁllou, T.C., Antonopoulos, C.P.: Design of concrete ﬂexural members strengthened in
shear with FRP. J. Compos. Constr. (2000). https://doi.org/10.1061/(ASCE)1090-0268(200
0)4:4(198)
15. Soudki, K.A., Sherwood, T., Masoud, S.: FRP repair of corrosion-damaged reinforced
concrete beams. In: Proceedings of the 3rd Fiber International Congress—2010 (2002)

Optimization of the Lubricating and Cooling
Fluid Composition
I. Yu. Ignatkin1
, P. Kazantsev Sergey1
, D. M. Skorokhodov1(B)
,
N. V. Serov1
, T. Kildeev2
, A. V. Serov1
, and A. Anisimov Alexander1
1 Russian State Agrarian University—Moscow Timiryazev Agricultural Academy,
Moscow 127550, Russia
{ignatkin,d.skorokhodov}@rgau-msha.ru
2 Bauman Moscow State Technical University, Moscow 105005, Russia
Abstract. The article deals with the issue of the durability of a metal-cutting
tool increasing. The main advantages of oil lubricants and coolants are compared.
The hypothesis of the taps durability increase due to the usage of an oil-based
coolant with the use of a metal-coating additive is being investigated in the study.
The metal-coating additive “Valena” was used to increase the taps durability in
the conditions of oil-based lubricating and cooling liquid. Industrial oil I-30 was
used as a lubricating and cooling medium for control samples. Studies of the
durability period were carried out on machine taps M10x1 made of steel grade
P6M5. Thread cutting was performed by the vertical drilling machine 2H118.
During the experiment, when the workpiece made of Steel 40X material was
processed, the appearance of a silver ﬁlm on the surface of the Valena solution
was observed. When the steel St3 was processed, the silver ﬁlm was not observed.
Keywords: Durability · Wear · Oil lubricants · Metal-cutting tool · Tap ·
Additive · Coolant · Friction
1
Introduction
The production and renovation of equipment is always accompanied by mechanical
surface treatment [1], which leads to the wear of the tool. It indicates the urgency of the
durability of metal-bearing tools increasing problem.
The solution of this problem can be carried out by technological and constructive
ways, for example, through the use of the new tool materials and wear-resistant coatings
applied to tools by surfacing or welding [2, 3], gas-dynamic or gas-thermal methods [4–
6], PVD and CVD methods, as well as replacing metal workpieces (objects of mechanical
processing) with more technological and durable products made of polymer composite
materials [7, 8].
Considering all the listed methods, it is advisable to describe the reasons of the cutting
tool wear. Concerning the process of wear, it should be noted that there are permissible
(relatively smooth change in size and shape) and unacceptable (damage) types of wear
[9].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 153–162, 2024.
https://doi.org/10.1007/978-3-031-50158-6_16

154
I. Yu. Ignatkin et al.
In terms of the cutting tool durability increasing, the permissible types of wear are
of the greatest interest. According to the physico-chemical mechanism the impact on the
tool can be divided into: abrasive (mechanical), oxidative (chemical-mechanical), wear
of ﬁlms of non-oxygen origin (mechanochemical) [10].
Oxidative wear (OW). Occurs under conditions of friction in an air or other oxygen-
containing medium. The oxides form a protective layer on the friction surface, protecting
it from setting, so that the OW speed is relatively small. However, under the inﬂuence
of plastic deformation, oxides are destroyed and, having a high hardness, can act as an
abrasive. The destroyed oxides are quickly restored, which is acceleratev by epy elevated
temperatures in the friction zone [11].
Another form of OW is formed by the oxide ﬁlms on the surface of rubbing parts with
their subsequent removal. The formed structures have signiﬁcantly greater hardness and
brittleness than the base metal. The formed oxide ﬁlms have a clear partition boundary
with the base material, and the bond strength is low. The combination of high brittleness
and low strength of the joint leads to intensive destruction and the formation of new
ﬁlms, while the surface acquires a heterogeneous structure.
The wear of the ﬁlms of non-oxygen origin involves the formation of sulﬁde,
phosphate, nitride, servovite and other ﬁlms on the surface with their subsequent
destruction.
Hydrogen wear (HW) consists in the diffusion of atomic hydrogen into the mate-
rial of friction pairs and the subsequent embrittlement of interfacial boundaries, grain
boundaries, the development of microcracks. Sources of atomic hydrogen surround the
friction zone, mechanical work, elevated temperature contribute to the active release of
atomic hydrogen from organic compounds of fuels and lubricants, plastics, and water.
At the same time, two fundamentally different types of hydrogen wear are distin-
guished: by dispersion and by destruction. In the ﬁrst case, as a result of plastic defor-
mation, the hydrogen-embrittled material is fragmented, destroyed with the formation
of a ﬁne metal powder. At the same time, the surface doesn’t show the visible signs of
wear.
In case of destruction, hydrogen penetrates into the pores of the metal and combines
into molecules, under the pressure of the friction pair, the pores close, the pressure inside
the pores increases and destruction occurs along all microcracks, a metal ﬂake is instantly
separated to form a cavity [12].
Abrasive wear (AW) occurs as a result of interaction with abrasive particles having
a signiﬁcant superiority in hardness. In this case, abrasive particles are usually divided
into ﬁxed, tangentially in contact with the surface; loose abrasive; free particles located
in the gap of the friction pair; free particles carried by the ﬂow of liquid or gas [13].
In order to avoid premature failure for the reasons described above, it is necessary
to strengthen the tool, modify the design or protect (in whole or in part) the tool from
the effects of negative factors.
All available hardening methods are divided into 6 main classes:
1. hardening with the formation of a ﬁlm on the surface;
2. with a change in the chemical composition of the surface layer;
3. with a change in the structure of the surface layer;
4. with a change in the energy reserve of the surface layer;

Optimization of the Lubricating and Cooling Fluid Composition
155
5. with the change of the microgeometry of the surface and the riveting;
6. with a change in the structure over the entire volume of the material.
The improvement of the structure by heat treatment (cold treatment, steam treatment)
is also applied.
Optimization of geometric parameters is carried out in the direction of macrogeom-
etry (angles, chip breakers, rags, reinforcing chamfers, etc.) and microgeometry (regular
microrelief, surface roughness, etc.). Improving the surface quality of the tool can be
carried out, for example, by ﬁne-tuning.
For increase of the durability of the cutting edge it should be adjusted. Sharpening
and ﬁnishing of cutting edges, especially with diamond circles, makes it possible to
increase the average durability of a number of tools, especially ﬁnishing ones, by 2–3
times or more, since it improves the surface quality of the tool, and, consequently, the
working conditions of the cutting part of the tool.
Special attention should be paid to the use of lubricating and cooling process media
(LCPM) [14].
LCPM perform cooling, lubricating and washing functions, which makes it possible
to carry out metalworking with greater productivity, achieve higher surface quality, and
wash out chips from the cutting zone [15].
LCPM are presented in the following forms:
– gaseous LCPM;
– plastic LCPM (technological lubricants);
– liquid LCPM (LCF, or lubricating and cooling ﬂuids);
– solid LCPM.
Gaseous LCPM are represented by inert or neutral gases (helium, argon, nitrogen),
activegases(oxygen,carbondioxide,air),therearealsoexamplesofadditionalactivation
of gases, for example, by ionization. Activation allows intensifying of the protective ﬁlms
formation on the surfaces of interacting parts. However, the complexity of application,
low heat capacity, lack of detergent properties limit the use of gaseous LCPM in practice
[16, 17].
Plastic LCPM are applied during manual low-productivity processing or in highly
loaded periodic types of processing. The main difﬁculties are caused by the complexity
of the supply (withdrawal) of the material to the processing zone, collection, cleaning,
cyclic use. Cooling and washing effects from the use of plastic STS are practically absent.
Mineral materials of layered structure (graphite, molybdenum disulﬁde, talc, mica),
soft metals (lead, copper, tin) or organic compounds (wax, soap, solid fats, polymers) are
used as solid LCPM. These lubricating and cooling compounds are applied as coatings
to the treated surface or tool. They are used at high loads and temperatures when the use
of other types of LCPM is impossible or difﬁcult. Under normal processing conditions,
solid LCPM is usually not used due to the low efﬁciency of the heat removal and the
complexity of the application [10, 13].
The most common LCPM currently are lubricating and cooling ﬂuids (LCF), which
are able to cope with the tasks of cooling, lubrication and washing out of cutting prod-
ucts from the processing zone. According to the composition, they are divided into the
following types:

156
I. Yu. Ignatkin et al.
• rapidly evaporating;
• oil;
• water-based.
Oil lubricants and coolants are made on the basis of mineral oils. The composition
of these LCF may also include antifriction, antiburring, anti-wear, anti-foam and anti-
fog additives; corrosion inhibitors, antioxidants. The physico-chemical properties of oil
LCF and their characteristics affecting the process of friction and wear of surfaces are
determined, ﬁrst of all, by the base oils included in the material composition.
As a base for oil LCF, well puriﬁed mineral parafﬁn or naphthenic oils, low-viscosity
extracts of selective puriﬁcation, as well as mixtures of several mineral oils can be used.
The oil content in the coolant of this class is usually 60.95% by the weight. Synthetic
oils have a fairly high cost. They can be used in oil lubricants as additives.
Oils without additives are used as coolant in light modes of soft metals cutting
(copper, brass, bronze, magnesium, carbon steels). In severe cutting conditions of hard-
to-process steels and alloys, oil lubricants without the addition of an additive package
are usually ineffective [12].
The concentration of antifriction additives in oil coolants ranges from 5 to 25%. They
are usually based on the organic or polymeric unsaturated fatty acids, their esters, veg-
etable oils and fats. Extreme pressure additives in oil coolants are substances containing
sulfur, chlorine, phosphorus. The most common among them are sulﬁdes, poly–sulﬁdes,
blackened fats, chlorinated parafﬁn. Their content in the composition of the material
varies from 0.5 to 20% and depends on the purpose and conditions of application of oil
coolants. The content of anti-wear additives (polymer fatty acids, dialkyl phosphates or
blackened fats) in coolant usually ranges from 0.5 to 5%. It depends on the purpose of
the liquid. Polyoleﬁns or atactic polypropylene are used as anti-fog additives in coolant.
These substances are introduced into lubricants in an amount of 0.5–3% to reduce the
formation of oil mist. Dimethylselicone polymers have become the most widespread
among anti-foam additives. Their concentration in oil coolants is 0.0005–0.001% [10,
11].
Under the inﬂuence of the mineral oils oxidation products, additives, their decom-
position products, corrosion can forme on the parts of the equipment and the processed
nodes. Corrosion inhibitors, which are introduced into the composition of oil coolant,
prevent its occurrence. The tendency to corrosion in different structural materials can
vary signiﬁcantly, so inhibitors are selected depending on the application of coolant.
In some cases, additives for improving lubricating properties can be quite effective
corrosion inhibitors: unsaturated fatty acids, disulﬁdes, aminophosphates.
Oil LCF have a whole range of advantages compared to other types of LCPM:
• they provide longer operation of the cutting tool;
• have excellent lubricating properties;
• better protect the processed metal and tools from corrosion;
• chips and tool wear products are washed out of the cutting zone more efﬁciently;
• can be used in centralized lubrication systems;
• subject to recycling, cleaning and reuse.
However, oil coolants also have a number of disadvantages that signiﬁcantly narrow
the scope of their application. They are ﬂammable, have high evaporation, relatively

Optimization of the Lubricating and Cooling Fluid Composition
157
low cooling properties and low thermal stability. In addition, the use of oil coolants is a
rather expensive way to lubricate and cool the cutting tool [17–19].
Sulfofresol has proven itself well—a mineral oil of medium viscosity, which includes
sulfur. When processing under the inﬂuence of high temperatures, protective sulﬁde ﬁlms
are formed on the surfaces of the workpiece and the tool, which signiﬁcantly reduce the
coefﬁcient of friction. At high processing speeds, sulfofresol actively evaporates and
smokes. Sulfur is consumed during processing, which reduces the efﬁciency of LCF,
and resinous formations are deposited on the machines. Sulfofresols are toxic.
A promising direction in processing is the use of LCF with anti-wear metal-coating
additives. The metal-coating additive “Valena” implements the effect of wear-free fric-
tion discovered by D.N. Garkunov and I.V.Kragelsky. The essence of the effect is the
formation of a protective copper ﬁlm with a spongy structure on the surface of the rubbing
parts. The ﬁlm thickness is about one micrometer.
In this paper, it is proposed to conduct the study of the hypothesis of increasing the
durability of taps due to the use of an oil-based coolant with the supplementation of a
metal-coating additive.
2
Materials and Methods
Studies of the durability period were carried out on machine taps M10x1 made of steel
grade P6M5.
To test the hypothesis of increasing the resistance of taps under the conditions of
using an oil-based lubricating and cooling liquid with the use of a metal-coating additive
“Valena”, an experimental solution was prepared: industrial oil I-30 with an additive
“Valena”, the concentration of the solution is 20%. To test the hypothesis, the concen-
tration (20%) is taken as the arithmetic mean of the manufacturer’s recommendations
(10–30%). If the hypothesis is conﬁrmed, it will be advisable to conduct research on
optimizing the composition of LCF.
Industrial oil I-30 was used as a lubricating and cooling process medium for control
samples.
The thread was cut in through holes with a height of 30 mm. The processing material
is 40X steel with low-temperature tempering (200 °C).
The tested taps cutted the holes until their blunting. According to the results of
installation tests, the reference wear with the use of coolant with the additive “Valena”
was established. Counting was stopped when comparable wear with the use of pure I-30
was achieved. The degree of wear was monitored through every 5 holes on the Supereyes
b008 microscope (Fig. 1) with the ability of the connection with the computer via the
usb interface.
The ratio of the number of holes produced, provided that the processing modes are
the same, is equal to the ratio of the durability periods.
k = T2
T1
= z2
z1
,
(1)
where T1, T2—the period of the tap durability when using the experimental and control
LCPM respectively, min.; z1, z2—the number of threaded holes produced when using
the experimental and control LCPM respectively, pieces.

158
I. Yu. Ignatkin et al.
Fig. 1. Supereyeb008 microscope.
Threading was performed with the vertical drilling machine 2H118 (Fig. 2). The
spindle speed n = 350 rpm, which corresponds to a cutting speed of 11 m/min. The feed
corresponds to the pitch of the thread being cut (1 mm).
Fig. 2. Column drilling machine 2H118.
3
Results
According to the results of the installation tests, the condition of the taps after processing
100 holes with the use of LCF t with the additive “Valena” was taken as the reference
wear (Fig. 3). The test results are shown in Table 1 (Fig. 4).
4
Discussion
In the process of drilling holes, the appearance of a silver ﬁlm on the surface of the Valena
solution was observed during the processing a workpiece made of Steel 40X material
(Fig. 5). When working with steel St3, this ﬁlm was not observed. It can be assumed
that the alloying element chromium is isolated from the material. The discovered effect
requires further research.

Optimization of the Lubricating and Cooling Fluid Composition
159
Table 1. Test results.
№
Cutting ﬂuid
20% «Valena»
Industrial oil I-30
1
100
30
2
30
3
35
Average value
32
Fig. 3. Tap cutting edge: a original, b worn
Fig. 4. Resistance taps with different cutting ﬂuid: Z1—the number of machined holes with a tap
with LCF 1; Z2—the number of machined holes tap with LCF 2

160
I. Yu. Ignatkin et al.
In further work, we plan to clarify the optimal concentration of the additive in the
composition,aswellastheseriesoftribotechnicaltestsofsolutionsonafrictionmachine.
As well as a study on increasing the cooling and washing abilities of the resulting coolant.
Fig. 5. Silver ﬁlm formation.
5
Conclusion
When threading in 40X Steel, the use of a 20% solution of the Valena metal-coating
additive in industrial oil I-30 in the quality of LCPM allows the increase of the period
of taps resistance by more than 3 times.
To obtain the maximum durability period, it is necessary to investigate the effect of
the concentration of the multifunctional additive “Valena” and the cutting speed on the
speciﬁed parameter, which is planned to be done in the future.
The research was conducted with the ﬁnancial support of the K.A. Timiryazev
Russian State Agrarian University-Moscow Agricultural Academy program “Scien-
tiﬁc internship” (the theme of the project “Research of structural materials and nano-
structured functional coatings for agricultural machinery and equipment”) as part of
the implementation of the University development program “Agroproyv-2030” of the
academic strategic leadership program “Priority-2030”.
References
1. Kononenko, A.S., Khabbatullin, R.R.: Theoretical substantiation of the conditions for the
applicability of deformationless ﬁxation by means of a polymer glue for workpieces during
their mechanical processing on a milling machine with computer numerical control. In: Poly-
mer Science. Series D, vol. 15, no. 4, pp. 523–528 (2022). https://doi.org/10.1134/S19954
21222040141
2. Latypov, R.A., Serov, A.V., Ignatkin, I.Y., Serov, N.V.: Utilization of the wastes of mechanical
engineering and metallurgy in the process of hardening and restoration of machine parts. Part
1. In: Metallurgist, vol. 65, no. 5–6, pp. 578–585 (2021). https://doi.org/10.1007/s11015-021-
01193-y
3. Latypov, R.A., Serov, A.V., Ignatkin, I.Y., Serov, N.V.: Utilization of the wastes of mechanical
engineering and metallurgy in the process of hardening and restoration of machine parts. Part
2. In: Metallurgist, vol. 65, no b/n, pp. 689–695 (2021). https://doi.org/10.1007/s11015-021-
01206-w

Optimization of the Lubricating and Cooling Fluid Composition
161
4. Zayatzev, A.N., Lukianova, A.N., Demoretsky, D.A.: Assessment of shear bond strength
of thermal spray coatings by applying prismatic samples. Solid State Phenom. 337, 35–41
(2022). https://doi.org/10.4028/p-297el3
5. Zayatzev, A.N., Alexandrova, Y.P.: Effect of elevated temperatures on tribological charac-
teristics of a plasma-sprayed Al2O3 coating under sliding conditions without lubrication. J.
Friction Wear 41(3), 242–246 (2020). https://doi.org/10.3103/S1068366620030174
6. Zayatzev, A.N., Alexandrova, Y.P.: Reduction of shear stresses in friction units with an electri-
cal insulation coating of the ITER blanket modules. J. Mach. Manuf. Reliab. 49(9), 763–769
(2020). https://doi.org/10.3103/S1052618820090137
7. Kononenko, A.S., Solovyeva, A.A., Komogortsev, V.F.: Theoretical determination of the
minimum thickness of a polymer layer providing ensured protection of a shaft–bearing joint
from fretting corrosion. In: Polymer Science. Series D, vol. 13, №1, pp. 45–49 (2020). https://
doi.org/10.1134/S1995421220010116
8. Kononenko, A.S., Ignatkin, I.Y., Drozdov, A.V.: Recovering a reducing-gear shaft neck by
reinforced-bush adhesion. In: Polymer Science, Series D, vol. 15, no 2, pp. 137–142 (2022).
https://doi.org/10.1134/S1995421222020113
9. Fanidi, O., Kostryukov, A.A., Shchedrin, A.V., Ignatkin, I.Y.: Comparison of analytical and
experimental force in cylindrical workpiece drawing process. In: Tribology in Industry, vol.
43, no 1, pp. 1–11 (2021). https://doi.org/10.24874/ti.1000.11.20.02
10. Garkunov, D.N.: Tribotehnika (konstruirovanie, izgotovlenieijekspluatacijamashin) [Tri-
botechnology (de-sign, manufacture and operation of machines)], Publ., Moscow «MSHA»
632 p (2002)
11. Sorokin, G.M.: Tribologijastalejisplavov [Tribology of steels and alloys], p. 314. Publ. Nedra,
Moscow (2000)
12. Suranov, G.I.: O mehanizmesnizhenijavodorodnogoiznashivanijadetalejmagnitnojobrabot-
koj [On the mechanism of reducing hydrogen wear of parts by magnetic treatment], Jeffekt-
bezyznosnostiitribotehnologii. [The effect of fatigue and tribotechnology.], no. 2, pp. 27–31
(1992)
13. Garkunov, D.N., Kornik, P.I.: Vidytrenijaiiznosa. Jekspluatacionnyepovrezhdenijadetalej-
mashin [Types of friction and wear. Operational damage to machine parts]. Publ., Moscow
«MSHA», 344 p (2003)
14. Skorokhodov, D., Krasnyashchikh, K., Kazantsev, S., Anisimov, A.: Theory and methods of
means and modes selection of agricultural equipment spare part quality control. In: Engi-
neering for Rural Development: 19, Jelgava, 20–22 ma 2020 goda. Jelgava, pp. 1140–1146
(2020). https://doi.org/10.22616/erdev.2020.19.tf274. EDN BKFWAI
15. Erokhin, A., Kazantsev, S., Pastukhov, A., Golubev, I.: Theoretical basis of justiﬁcation of
electromechanicalhardeningmodesofmachineparts.In:EngineeringforRuralDevelopment:
19, Jelgava, 20–22 ma 2020 goda. Jelgava, pp. 147–152 (2020). https://doi.org/10.22616/
ERDev.2020.19.TF032. EDN IDCZQE
16. Tikhomirov, D., Kuzmichev, A., Rastimeshin, S., Trunov, S., Dudin, S.: Energy-efﬁcient
pasteurizer of liquid products using IR and UV radiation. In: Vasant, P., Zelinka, I., Weber,
G.-W. (eds.) ICO 2018. AISC, vol. 866, pp. 178–186. Springer, Cham (2019). https://doi.org/
10.1007/978-3-030-00979-3_18
17. Pol’cer, G., Firkovskij, A., Lange, I.I.: i dr. Finishnajaantifrikcionnajabezabrazivnajao-
brabotka (FABO) iizbiratel’nyjperenos [Finishing anti-friction non-abrasive treatment
(FABO) and selective transfer]. Sb. Dolgovech-nost’ trushhihsjadetalejmashin [Collection.
The durability of the rubbing parts of machines.], no 4. Publ. Mashinostroenie, Moscow, 316
p (1990)
18. Dorokhov, A., Kirsanov, V., Pavkin, D., Yurochka, S., Vladimirov, F.: Recognition of cow
teats using the 3D-ToF camera when milking in the “Herringbone” milking parlor. In: Vasant,

162
I. Yu. Ignatkin et al.
P., Zelinka, I., Weber, G.-W. (eds.) ICO 2019. AISC, vol. 1072, pp. 128–137. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-33585-4_13
19. Lazar, V.V., et al.: J. Phys.: Conf. Ser. 1679, 042058 (2020). https://doi.org/10.1088/1742-
6596/1679/4/042058

Research of the Dosing Process
with the Installation of Magnetic Stimulation
of Seeds
V. Syrkin1
, S. Mashkov1
, P. Ishkin1
, S. Vasilev1
, and Yu. Daus2(B)
1 Samara State Agrarian University, Uchebnaya St., 2, 446442 Ust-Kinelskiy, Russia
2 Kuban State Agrarian University, Kalinina St. 13, 350044 Krasnodar, Russia
zirochka2505@gmail.com
Abstract. Magnetic stimulation of seeds is one of the electrophysical methods
that increases the germination and intensity of plant growth. To identify the fac-
tors affecting the magnetic ﬁeld on seeds, an experimental research method was
developed using a magnetic seed stimulation unit with a vibrating dispenser. Stud-
ies have been conducted on the inﬂuence of the frequency of the magnetic ﬁeld
on the process of seed dosing. The minimum supply of Spring Wheat and Ama-
ranth seeds, respectively, was 3.1 kg/h and 1.7 kg/h at a frequency of 10 Hz. The
maximum feed was observed at a frequency of 110 Hz—70.1 kg/h and 22.7 kg/h,
respectively.The stimulation time at a frequency from 10 Hz to 110 Hz for spring
wheat varies in the range from 90 s to 4 s, Amaranth—from 89 s to 7 s. Given
the required seed stimulation time, the necessary feed can be set by changing the
frequency.
Keywords: Magnetic stimulation · Dosing process · Electrophysical methods
1
Introduction
Crop production is one of the main directions of the agro-industrial complex, providing
the population with food, animal husbandry with feed and raw materials of a number of
industrial areas. The intensiﬁcation of crop production leads to a reduction in cost and
an increase in production [1, 2].
The main directions of crop production development are the use of new advanced
technologies and equipment, increasing soil fertility, the use of high-quality seed
material, etc. [3–5].
The use of high-quality seed provides high germination, the intensity of plant growth,
immunity to external factors such as weather conditions and diseases. Over time, the
quality of the seed decreases, which leads to a decrease in production. As a result, there
is a need to purchase new expensive seeds, while most of the quality indicators of the
old seeds remain at an acceptable level [6–8].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 163–169, 2024.
https://doi.org/10.1007/978-3-031-50158-6_17

164
V. Syrkin et al.
A decrease in germination leads to overspending of seed material, which affects the
increase in production costs. To increase the germination of seeds, various methods of
exposure are used [10–12]. One of the promising methods is the treatment of seeds in
a magnetic ﬁeld. This method is environmentally friendly and does not require large
energy costs. There are various devices for magnetic stimulation of plants. However, not
all of them provide the main task—the effect on the plant with a uniform magnetic ﬁeld.
Also, some plants operate in a cyclic type, which automates the process and increases
the processing time, while ﬂow-type plants can be used in a production line for preparing
seeds for sowing [3–8].
2
Materials and Methods
Whentreatingseedswithamagneticﬁeld,themainfactorsaffectingseedgerminationare
the parameters of the magnetic ﬁeld, the stimulation time and the holding time before
sowing [9, 10]. It was also found that the seeds are more responsive to a U-shaped
magnetic ﬁeld of different frequencies.
At the Department of “Electriﬁcation and Automation of the Agroindustrial Com-
plex” of the Samara State Agrarian University, an experimental laboratory installation
of magnetic stimulation of seeds of a ﬂow type with a vibrating dispenser was developed
(Fig. 1). The installation consists of a frame of a vibrating dispenser 2, a magnetic seed
stimulation unit 3, a hopper 4, a power supply and control unit 5. A multimeter 6 was
used to set the required frequency. When the unit was operating, a receiving box was
placed under the vibrating dispenser [5, 6].
The main feature of the installation design is the magnetic stimulation unit 3, which
is a two-circuit branched circuit with inductors on the external branches of the magnetic
circuit. In the Central branch of the magnetic circuit 1 (Fig. 2, a) an air gap, which passes
through a vertical pipe 2 connecting the hopper 4 (Fig. 1) with vibration pump 2.
Vibratorydispenserincludesamagneticcircuit3(Fig.1),aninductioncoil4,building
5, the pressure limiter seeds 6 and vibrating plates 8.
In the process of work, due to the creation of oscillatory movements of the vibrating
plates 8 (Fig. 2), the seeds begin to move, move to the edge of the plates and pour off it.
At the same time, seeds come from the hopper through the nozzle 2 in their place. In the
process of moving along the branch pipe 2, the seeds fall into the zone of the magnetic
stimulation unit, where they are exposed to a magnetic ﬁeld [6].
By changing the frequency of the magnetic ﬁeld of the dispenser, the vibration
frequency of the plates 8 (Fig. 2) changes, as a result of which the seed supply Q
changes.
The seed supply when moving along the branch pipe will be determined as follows:
Q = γ · a · b · v, kg/s,
(1)
where γ—the seed density, kg/m3;
a and b—the dimensions of the side walls of the pipe, m;
v—the ﬂow rate of seeds in the branch pipe, m/s.

Research of the Dosing Process with the Installation of Magnetic
165
Fig. 1. General view of the laboratory installation of magnetic stimulation of seeds
Fig. 2. The main components of the installation: (a) plant stimulation zone; (b) vibration dis-
penser: 1—magnetic core of the seed magnetic stimulation unit; 2—branch pipe; 3—magnetic
core of the dispenser; 4—induction coil; 5—dispenser housing; 6—limiter; 7—ﬂap; vibration
plate
The ﬂow rate will be equal to:
v = h
t , m/s,
(2)
where h—the height of the stimulation zone (the height of the electromagnet), m;
t—the time of passage of the stimulation zone by seeds, s.

166
V. Syrkin et al.
Substituting formula 2 into formula 1 we get
Q = γ · a · b · h
t
, kg/s.
(3)
The supply of the vibrating dispenser is determined as follows:
(4)
where B—width of plates, m;
H—height of the seed layer, m;
vd—the average speed of movement of seeds on the vibrating plate, m/s.
n—number of plates, pcs.
Thevalueoftheaveragespeedofmovementofseedsonvibratingplatesisdetermined
as follows
(5)
gde k—coefﬁcient depending on the physical and mechanical properties of seeds,
determined experimentally;
A—vibration amplitude, m;
f —frequency of plate vibrations, Hz.
Then
(6)
The movement of seeds along the vibrating plates is carried out due to the lateral
pressure of the seeds coming from the branch pipe.
As
, then we equate the right parts of formulas (3) and (4) we get
γ · a · b · h
t
= B · H · k · A · f · γ · n,
(7)
After performing the transformations and expressing the time, we get
t =
a · b · h
B · H · k · A · f · n.
(8)
As a result, the seed stimulation time is inversely proportional to the vibration
frequency of the plates.
Experimental studies were carried out to determine the effect of the vibration fre-
quency of the vibrating dispenser plates on the seed supply. The variable factor was the
frequency of the magnetic ﬁeld, which directly affects the vibration frequency of the
plates. The frequency gradation of the magnetic ﬁeld was 10, 30, 50, 70, 90, 110, 130
and 150 Hz. For each frequency value, the experiment was carried out with a three-fold
repetition. The seed stimulation time range was assumed to be 1 min. The experiments
were carried out on wheat and amaranth seeds.

Research of the Dosing Process with the Installation of Magnetic
167
Table 1. Results of studies of the inﬂuence of the magnetic f frequency on the supply of the
vibrating dispenser
Frequency f, Hz
Seed supply Q, kg/h. for Spring wheat
Seed supply Q, kg/h. for Amaranth
10
3.1
1.7
30
16.3
4.1
50
31.1
7.7
70
47.8
12.7
90
61.7
18.3
110
70.1
22.7
130
61.5
20.1
150
33.2
13.6
3
Results and Discussion
The results of the inﬂuence of the frequency of vibration of the dispenser plates on its
performance are presented in Table 1.
When analyzing the results of the experiment, the following data were revealed. That
with an increase in frequency in the range from 10 to 110 Hz, the supply of processed
seeds increases. The low vibration frequency of the plates had a low incentive for seeds
to pour out. At the same time, a linear dependence is observed in this range. In the range
from 130 to 150 Hz, a decrease in performance was recorded. This is due to a decrease
in the intensity of the vibration action, i.e. the plates made of electrical steel did not have
time to return to its original position (Fig. 3).
The minimum supply of spring wheat and amaranth seeds, respectively, was 3.1 kg/h
and 1.7 kg/h at a frequency of 10 Hz. The maximum feed was observed at a frequency
of 110 Hz—70.1 kg/h and 22.7 kg/h, respectively. At the same time, the minimum
unevenness of dosing in the range from 10 Hz to 110 Hz of wheat seeds was observed
at a magnetic ﬁeld frequency of 70 Hz and amounted to 1.8%, and for amaranth seeds
at a frequency of 90 Hz—1.6%, respectively. The maximum value of uneven dosing in
wheat and amaranth seeds was observed at a frequency of 10 Hz and amounted to 4.1%
and 3.6%, respectively, which is lower than acceptable values.
The stimulation time at a frequency from 10 Hz to 110 Hz for spring wheat varies
in the range from 90 s to 4 s, amaranth - from 89 s to 7 s. An additional increase in the
stimulation time is achieved by changing the position of the adjustable ﬂap 7 (Fig. 2,
b). Considering that the optimal stimulation time for these cultures is in the range from
30 s to 60 s, it can be concluded that the speciﬁed range of adjustable time with a large
margin corresponds to the stimulation time.

168
V. Syrkin et al.
a)
b)
0
20
40
60
80
0
50
100
150
200
Performance Q, kg/h
Magnetic field frequency f,Hz
0
5
10
15
20
25
0
50
100
150
200
Performance Q, kg/h
Magnetic field frequency f,Hz
Fig. 3. Results of research on the effect of the magnetic ﬁeld frequency on the supply of a vibrating
dispenser: (a) spring wheat seeds; (b) amaranth seeds
4
Conclusion
The use of a magnetic seed stimulation unit with a ﬂow-type vibrating dispenser will
increase the productivity of the processing process. By changing the frequency of the
magnetic ﬁeld and, as a consequence, the frequency of vibration of the plates of the
vibrating dispenser in the range from 10 Hz to 110 Hz, the feed will change, which
as a result will affect the stimulation time. For wheat and amaranth seeds, the set feed
range largely overlaps the required stimulation time range. At the same time, the uneven
dosing of seeds of both crops corresponds to acceptable values. As a result, knowing the
necessary time to stimulate a particular culture, it is possible to set the necessary supply
using the frequency of the magnetic ﬁeld.
References
1. Baev, V.I., Yudaev, I.V., Petrukhin, V.A., Prokofyev, P.V., Armyanov, N.K.: Electrotechnol-
ogy as one of the most advanced branches in the agricultural production development. In:
Handbook of Research on Renewable Energy and Electric Resources for Sustainable Rural
Development. IGI Global, Hershey, PA, USA (2018)
2. Yudaev, I.V., Daus, Y.V., Kokurin, R.G.: Substantiation of criteria and methods for estimating
efﬁciency of the electric impulse process of plant material. IOP Conf. Ser.: Earth Environ.
Sci. 488(1), 012055 (2020)

Research of the Dosing Process with the Installation of Magnetic
169
3. Yudaev, I.V., Ivushkin, D., Belitskaya, M., Gribust, I.: Pre-sowing treatment of ROBINIA
PSEUDOACACIA L. seeds with electric ﬁeld of high voltage. IOP Conf. Ser.: Earth Environ.
Sci. 403(1), 012078 (2019)
4. Syrkin, V.A., Vasiliev, S.I., Shishkin, P.A., Smolev, K.S.: Patent No. 204352. RU. Installation
for Pre-sowing Seed Stimulation. 2021105476 (2021) (in Russian)
5. Syrkin, V.A., Gridneva, T.S., Ishkin P.A., Fatkhutdinov, M.R.: Device for seed stimulation by
pulsed magnetic ﬁeld. Selskiymekhanizator 6, 28–29 (2019) (in Russian)
6. Vasilev, S.I., Mashkov, S.V., Syrkin, V.A., Gridneva, T.S., Yudaev, I.V.: Results of studies of
plant stimulation in a magnetic ﬁeld. Res. J. Pharm. Biol. Chem. Sci. 9(1), 706–710 (2018)
7. Yudaev, I.V., Daus, Y.V., Gamaga, V.V., Grachev, S.E., Kuligin, V.S.: Plant tissue sensitivity
to electrical impulse. Res. J. Pharm. Biol. Chem. Sci. 9(4), 734–739 (2018)
8. Mashkov, S.V., Vasilev, S.I., Fatkhutdinov, M.R., Gridneva, T.S.: Using an electric ﬁeld to
stimulate the vegetable crops growth. Int. Trans. J. Eng. Manage. Appl. Sci. Technol. 11(16),
11A16V (2020)
9. Tokarev, K., et al.: Monitoring and intelligent management of agrophytocenosis productivity
based on deep neural network algorithms. Lect. Notes Netw. Syst. 569, 686–694 (2023)
10. Yudaev, I., Eviev, V., Sumyanova, E., Romanyuk, N., Daus, Y., Panchenko, V.: Methodology
and modeling of the application of electrophysical methods for locust pest control. Lect. Notes
Netw. Syst. 569, 781–788 (2023)
11. Ivushkin, D., et al.: Modeling the inﬂuence of quasi-monochrome phytoirradiators on the
development of woody plants in order to optimize the parameters of small-sized LED
irradiation chamber. Lect. Notes Netw. Syst. 569, 632–641 (2023)
12. Petrukhin, V., et al.: Modeling of the device operating principle for electrical stimulation of
grafting establishment of woody plants. Lect. Notes Netw. Syst. 569, 667–673 (2023)

Investigation of Hydrodynamic Behaviour
in Rectangular Sheet Shaped Membrane Using
Computational Fluid Dynamics (CFD)
Anirban Banik1(B), Sushant Kumar Biswal2, Tarun Kanti Bandyopadhyay3,
Vladimir Panchenko4,5, Sunil Garhwal6, and Anil Garhwal7
1 Department of Civil Engineering, National Institute of Technology Sikkim, Ravangla, South
Sikkim 737139, India
anirbanbanik94@gmail.com
2 Department of Civil Engineering, National Institute of Technology Agartala, Jirania, Tripura
(W) 799046, India
3 Department of Chemical Engineering, National Institute of Technology Agartala, Jirania,
Tripura (W) 799046, India
4 Russian University of Transport, Obraztsova St., Moscow 127994, Russia
5 Federal Scientiﬁc Agroengineering Center VIM, 1st Institutsky Passage 5, 109428 Moscow,
Russia
6 Department of Civil Engineering, Maharishi Markandeshwar (Deemed to be University),
Mullana, Ambala, Haryana 133203, India
7 School of Architecture and Planning, Banasthali Vidyapith, Tonk, Rajasthan 304022, India
Abstract. Current research investigates hydrodynamic behaviour in rectangular
sheet shaped membrane implementing computational ﬂuid dynamics approach.
CFD mimics the ﬂow phenomena in stationary and rotating module. The ﬂow
within the stationary and rotating membranes was computed using commercially
available CFD software (ANSYS). It was well established that membrane can
generate permeate ﬂux of high quality but limited in its use due to its high fouling
tendency. So, the effect of rotation in improving the antifouling property was also
studied at 30 RPM. The results show good agreement with the experimental values.
Keywords: Computational ﬂuid dynamics · Membrane separation technique ·
Rotating membrane · Wastewater
1
Introduction
Membrane ﬁltration and separation processes have been critical in the industrial sepa-
ration process for decades [1]. Various studies have been conducted to select the best
method of membrane separation. The use of computational ﬂuid dynamics (CFD) app-
roach yields a wealth of information about the evolution of the process [2]. Various
technological advancements in the ﬁeld of membrane technology have simpliﬁed and
expedited the process of selecting a membrane for a particular procedure. Hydrody-
namics behaviour is vital in membrane separation and ﬁltration techniques [3–5]. Flow
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 170–180, 2024.
https://doi.org/10.1007/978-3-031-50158-6_18

Investigation of Hydrodynamic Behaviour in Rectangular
171
within Membrane separation is a process that combines free ﬂow within the module with
porous zone ﬂow within the membrane bed. Fluid ﬂow inside the module is simple to
mimic whereas ﬂow inside the porous zone is considered as the complex one to simulate
which is achieved by hybrid form of Darcy law coupled with Navier-Stokes equation
[6–8]. The approach of using Darcy law to predict solution for incompressible ﬂow
inside porous zone was found acceptable in the literature. The critical component is to
ensure adequate continuity of ﬂow ﬁeld variables at the interface between laminar and
porous ﬂow regions [9, 10]. Numerous studies demonstrate the application of compu-
tational ﬂuid dynamics (CFD) in membrane process optimization [7–13]. Gruber et al.
investigates the ability of computational ﬂuid dynamics to mimic the forward osmosis
membrane operation. Even when high cross-ﬂow velocity and slip velocity at the porous
surface are taken into account, simulations show a non-negligible external concentration
polarisation on the porous support [14]. Shirazian et al. employed CFD to predict hydro-
dynamics and mass transfer in membrane reactors implemented for ammonia separation
from aqueous solutions. The system’s mass transfer and hydrodynamics were investi-
gated by solving conservation equations. The ﬁndings of this study conﬁrmed that feed
and solvent velocities are the most signiﬁcant parameter in ammonia removal [15]. Wang
et al. simulated the zones of membrane ﬁltration in submerged hollow ﬁbre membrane
using CFD. To account for the hydrodynamic behaviour of a full-scale submerged MBR,
this porous media model was coupled with a 3D multiphase model [16].
Theobjectiveofthechapteristoemploycomputationalﬂuiddynamicstosimulatethe
ﬂow inside the membrane bed to investigate the hydrodynamic behaviour in rectangular
sheet membrane. Present study also investigates the effect of inlet velocity and membrane
rotation. CFD predicted results are compared and evaluated against the experimental
results.
2
Experimental Procedure
An experimental study of the Cellulose acetate (CA) rectangular sheet shaped membrane
was carried out in the laboratory to investigate the ﬂow phenomena, permeate ﬂux, and
other properties of the membrane and to improve the quality of efﬂuent from the rubber
industry in Tripura, India. The setup of rectangular sheet shaped membrane includes
neutralizing tank (used to monitor any undesirable variation in feed pH and temperature),
feed tank, membrane module, and permeate tank. The feed stream has been allowed to
ﬂow with the assistance of a centrifugal pump. Rubber industry Efﬂuents are collected
from the Rubber Park located in TIDC Bodhjung nagar comlex. Complex is located in
Bodhjung nagar village, which is about 12 km from the state capital Agartala, Tripura
India. Characterization of raw efﬂuent of rubber industry has been given in Table 1.
3
Computational Fluid Dynamics (CFD)
Computational ﬂuid dynamics (CFD) is a subﬁeld of ﬂuid mechanics that is used to
forecast ﬂuid ﬂow and related problems by solving the governing equations for the ﬂow
phenomena [17–19]. The CFD technique aids in the collection of data regarding the
ﬂow phenomena occurring within the membrane module. CFD minimizes time and cost

172
A. Banik et al.
Table 1. Characterization of raw efﬂuent.
Sl. No
Parameters
Units
Value
1
pH
–
5.4
2
Total suspended solids
mg/l
3900
3
Total dissolved solids
mg/l
380
4
Sulphide
mg/l
16.5
5
Oil and grease
mg/l
11.5
6
BOD5
mg/l
724
of experimentation and data collection. The model of rectangular sheet membrane was
developed implementing 3D hexahedral mesh using ANSYS. The ﬂow in membrane bed
is assumed to be laminar which was found from Reynolds number and it was found less
than 2300. The ﬂow through the membrane is governed by equations such as continuity,
Darcy law, momentum, and solute transfer demonstrated by Eqs. (1)–(4), respectively
[20].
−→
V

i
 d
dx

+ j
 d
dy

+ k
 d
dz

= 0
(1)
P = −
μ
α
−→
V
(2)
∇

ρu ⃗V

= −dP
dx +

μ∇2u

(3)
∇

ρ ⃗VC

=

ρD∇2C

(4)
In Eqs. (1)–(4), ρ, μ, ⃗V, and k, denotes the density, viscosity, velocity vector, and
permeability, respectively. Diffusive coefﬁcient and concentration are represented in
Eq. 4 by D and C, respectively.
3.1
Assumptions
While developing the theoretical and mathematical model describing efﬂuent ﬂow
through membrane bed, the following assumptions and concepts are taken into account:
I. Rubber industry efﬂuent was considered as Newtonian ﬂuid.
II. Efﬂuent was considered as incompressible and Isothermal.
III. Membrane bed was considered isotropic and homogeneous porous zone.
IV. The under-relaxation factor was reduced to 0.7–0.3 or lower due to the necessity
of the simulation work.
V. For computational simpliﬁcation, hexahedral grids are assumed.
VI. Reﬁned grids were used to resolve a large gradient of pressure and swirl velocities.
VII. Developed model for simulation was limited to ﬂow model only.

Investigation of Hydrodynamic Behaviour in Rectangular
173
3.2
Equations for Grid Discretization
The governing equation was obtained by using the integral method. The ﬂuid region of
the concerned problem discretized into a set of control volume or cells. The transport
equation for momentum, mass transfer or solute transfer, etc. has been applied to each
cell and discretized for cell q given by Eq. (6);
∂
∂t
	
V
(ρφ)dV +
	
A
(ρφu)dA =
	
A
(∇φ)dA +
	
V
SφdV
(6)
The general property of φ used for transport equation and each transport equation
discretized into a system of the algebraic equation which is then solved numerically by
rendering the solution domain of the problem using Eq. (7);

ρφp
t+t −

ρφp
t
t
V +

faces
ρf φf uf Af =

faces
f Af (∇φ)f + SφV
(7)
Discretized equations need information at the centers of cell and faces. Required
ﬁeld data like velocity, properties of the material, etc. are stored at the centers of the
cell. Face values are calibrated by interpolating the local and adjacent cell values. The
accuracy of discretization depends upon the size of the stencil. The discretized equation
expressed merely by using Eq. (8);
(αpφp) +

nb
(αnbφnb) = bp
(8)
3.3
Boundary Conditions
The above governing equation of the problem has been solved subject to the below
mentioned boundary conditions [21]:
(a) Inlet of the membrane was considered to be mass inlet.
(b) Outlet of the membrane was assumed to be pressure outlet.
(c) Membrane was assumed to porous zone.
(d) At the membrane module’s wall, no slip condition was taken into account.
3.4
Convergence and Test of Grid Independency
Except for the transport equation, which was set to 10–3, the default convergence criteria
for all equations were set to 10–5. A computational domain was used to calibrate the
ﬁndings of fully developed ﬂow that might be acquired for membrane. Study illustrates
that pronounced results get affected by the mesh size. To check if the chosen mesh/grid
resolution was adequate to produce results with a low error percentage, the mesh/grid
resolution was gradually increased and decreased by 50%. A reduction in mesh resolu-
tion was shown to contribute 8–15% of the inaccuracy in the pressure proﬁle, whereas
an increase of 50% in mesh resolution reduced the error to 1–5%. The grid size type

174
A. Banik et al.
is illustrated using Table 2. Grid independency test was reported in Table 3. The ﬁne
grid of size 192000 was determined to be the best grid size for carrying out the sim-
ulation procedure, since increasing the grid size results in no signiﬁcant change in the
pressure proﬁle of the membrane. Based on the ﬁndings, it is possible to infer that the
existing mesh/grid resolution is sufﬁcient for producing grid independent solutions for
the suggested membrane model.
Table 2. Grid size type
Sl. No
Grid type
Grid size type
Grid size
No. of nodes
No. of faces
1
Hexahedral
Coarse
98304
105633
302080
2
Hexahedral
Fine
192000
203401
587200
3
Hexahedral
Finer
375000
392751
1E+06
Table 3. Grid independency test
Sl. No
Grid type
Grid size
Time (s)
Expt. (kPa)
CFD (kPa)
Error (%)
1
Hexahedral
98304
80
46
55.6
20.8
2
Hexahedral
192000
110
46
43.4
5.6
3
Hexahedral
375000
156
46
43.4
5.6
3.5
Advantages and Disadvantages of CFD
CFD has the following advantages: it provides insight into the system that would be
impossible to obtain through experimentation, it predicts efﬁcient and optimal design,
it minimises experimentation costs, it can simulate ideal and real conditions, and it
can investigate a large number of locations. The following are the disadvantages of
CFD: The accuracy and efﬁciency of the CFD model are dependent on the physical
model of the real world; solving the governing equations numerically via a computer
results in the accumulation of numerical errors such as round-off and truncation errors.
Additionally, the accuracy of the model is inﬂuenced by the physical model’s initial
boundary condition.
4
Results and Discussions
4.1
CFD Analysis
The static pressure (Pa) contour for a rectangular sheet membrane used to improve the
efﬂuent quality of the rubber industry is shown in Fig. 1. The plot demonstrates that
pressure steadily declines over the membrane bed, which is caused by the membrane’s

Investigation of Hydrodynamic Behaviour in Rectangular
175
high resistance. Kinetic energy heads are lost along the membrane because to its high
resistance, and this loss of kinetic energy causes a pressure decrease across the mem-
brane. Figures 2 and 3 depict a rectangular sheet membrane’s wall shear stress (Pa) and
shear strain rate (sec−1). The ﬁgures demonstrate that wall shear stress and shear strain
are high at the membrane’s wall and pore wall because of the high skin friction.
Fig. 1. Contour of Static pressure (Pa)
Fig. 2. Contour of wall shear stress (Pa)
Fig. 3. Contour of Shear strain Rate (sec−1)

176
A. Banik et al.
4.2
Effect of Inlet Velocity
Figure 4a–d illustrates static pressure (Pa) contour for efﬂuent ﬂow inside the membrane
at various inlet velocities. According to the plot, as the feed stream velocity at the
membrane module’s entrance increases, so does the static pressure on the membrane
module. This increase in speed is depleting kinetic energy. The high membrane resistance
that obstructs the ﬂow and manifests as a type of pressure energy across the membrane
is to blame for this.
4.3
Effect of Membrane Rotation
The main disadvantage associated with the membrane separation and ﬁltration is fouling.
From the study of the static membrane, it is found that the membrane undergoes rapid
fouling due to the blockage in the membrane pores and formation of cake layer over the
membrane bed resulted from the deposition of macromolecules and impurities present
in the feed stream. The rotational rectangular sheet membrane is used to enhance the
permeate quality, ﬁltration time and antifouling property of the membrane. Figure 5
illustrates static pressure (Pa) contour of rotating rectangular sheet membrane with a
rotating speed of 30 RPM. From this ﬁgure, it found that the rotational rectangular sheet
membrane has more inlet pressure and pressure drop compared to the static membrane
bed. This is due to its high resistance and high shear which acts on the membrane bed.
The reason of generation of high shear is due to the rotation of the membrane bed. Red
and blue colour in the plot illustrates the region of high and low pressure respectively.
The plot also shows the wave formation over the membrane bed due to its rotation. The
rotation of membrane minimizes the fouling tendency by keeping the macromolecules
and impurities in the suspended forms. This prevents the formation of the cake layer over
the membrane. Figure 6 demonstrates the velocity (m/sec) contour of rotating rectangular
sheet membrane. The ﬂow through the permeable layer, over the membrane bed, and in
the bulk feed is depicted on the plot. Due to the large adhesive force between the wall
and ﬂuid particles compared to the cohesive force between the liquid molecules, it was
discovered that ﬂow tends to cease near the wall. However, rotation in the membrane
module forces the feed to ﬂow near the wall. This is due to the centrifugal force which
pushes the ﬂuid towards the wall.
4.4
Validation
The graphical illustration of pressure drop (kPa) and inlet velocity (m/sec) of rectangular
sheet membrane is shown in Fig. 7. Plot illustrated that increased inlet velocity inﬂuences
the ﬂuid momentum as the momentum is a function of velocity. The ﬂuid’s kinetic energy
head is turned into a kind of pressure head when the inlet velocity increases, resulting in a
greater pressure drop across the membrane. Figure 7 also illustrates the CFD pronounced
values follows the experimental value with higher accuracy.

Investigation of Hydrodynamic Behaviour in Rectangular
177
(a)
(b)
(c)
(d)
Fig. 4. Static pressure (Pa) contour for rectangular sheet membrane with velocity at inlet as: (a)
17.9 cm/sec (b) 30.14 cm/sec (c) 58.18 cm/sec and (d) 82.82 cm/sec

178
A. Banik et al.
Fig. 5. Contour of static pressure (Pa) for rectangular membrane with a rotating speed of 30 RPM.
Fig. 6. Contour of velocity (m/sec) for rectangular sheet membrane with a rotating speed of 30
RPM.
Fig. 7. Plot of the membrane’s inlet velocity versus pressure drop

Investigation of Hydrodynamic Behaviour in Rectangular
179
5
Conclusions, Limitations, and Future Research Scope
In present study, ﬂuid ﬂow in rectangular sheet shaped membrane was modelled using
ANSYS. To pronounce precise solution hexahedral mesh was implemented for meshing
purpose. Based on the outcome of grid independency test, grid size of 192000 was cho-
sen for simulation purpose. Moreover, it was also found that any additional reﬁnement
of the grid size has no effect on the pressure proﬁle of the rectangular sheet membrane.
According to the study, rotating membranes outperform static membranes in terms of
ﬁltering life. The predicted data of the CFD are compared to the experimental one, which
show excellent agreement with one another, and the error percentage typically ranges
between 1 and 5%. CFD analysis provides a clear picture about the distribution of pres-
sure, wall shear stress, velocity, and shear strain rate in membrane module. Experimental
run for rotating membrane was not conducted which is one of the major limitations of
the study. In future, experimental run will be conducted for better validation of the rotat-
ing membrane. Moreover, optimization of rotating speed of the membrane will also be
investigated in future. The study’s ﬁndings can be used to develop a low-cost membrane
separation technique for treating the efﬂuent generated from rubber industry.
References
1. Stopford, P.J.: Recent applications of CFD modelling in the power generation and combus-
tion industries. Appl. Math. Model. 26, 351–374 (2002). https://doi.org/10.1016/S0307-904
X(01)00066-X
2. Kim, S.E., Boysan, F.: Application of CFD to environmental ﬂows. J. Wind Eng. Ind. Aerodyn.
81, 145–158 (1999). https://doi.org/10.1016/S0167-6105(99)00013-6
3. Vasant, P., Zelinka, I., Weber, G.-W.: Intelligent Computing and Optimization-Proceedings
of the 3rd International Conference on Intelligent Computing and Optimization 2020 (ICO
2020) (2021). https://doi.org/10.1007/978-3-030-68154-8
4. Vasant, P., Zelinka, I., Weber, G.-W. (eds.): ICO 2019. AISC, vol. 1072. Springer, Cham
(2020). https://doi.org/10.1007/978-3-030-33585-4
5. Vasant, P., Zelinka, I., Weber, G.-W. (eds.): ICO 2018. AISC, vol. 866. Springer, Cham (2019).
https://doi.org/10.1007/978-3-030-00979-3
6. Karode, S.K.: Laminar ﬂow in channels with porous walls, revisited. J. Membr. Sci. 191,
237–241 (2001). https://doi.org/10.1016/S0376-7388(01)00546-4
7. Pak, A., Mohammadi, T., Hosseinalipour, S.M., Allahdini, V.: CFD modeling of porous
membranes. Desalination 222, 482–488 (2008). https://doi.org/10.1016/j.desal.2007.01.152
8. Berman, A.S.: Laminar ﬂow in channels with porous walls. J. Appl. Phys. 24, 1232–1235
(1953). https://doi.org/10.1063/1.1721476
9. Weber, G.W., Taylan, P., Alparslan-Gök, S.Z., Özöˇgür-Akyüz, S., Akteke-Öztürk, B.: Opti-
mization of gene-environment networks in the presence of errors and uncertainty with
Chebychev approximation. TOP 16, 284–318 (2008). https://doi.org/10.1007/s11750-008-
0052-5
10. Ozogur-Akyuz, S., Weber, G.W.: On numerical optimization theory of inﬁnite kernel learning.
J. Global Optim. 48, 215–239 (2010). https://doi.org/10.1007/s10898-009-9488-x
11. Ghadiri, M., Asadollahzadeh, M., Hemmati, A.: CFD simulation for separation of ion from
wastewater in a membrane contactor. J. Water Process Eng. 6, 144–150 (2015). https://doi.
org/10.1016/j.jwpe.2015.04.002

180
A. Banik et al.
12. Rahimi, M., Madaeni, S.S., Abolhasani, M., Alsairaﬁ, A.A.: CFD and experimental studies
of fouling of a microﬁltration membrane. Chem. Eng. Process. 48, 1405–1413 (2009). https://
doi.org/10.1016/j.cep.2009.07.008
13. Rezakazemi, M.: CFD simulation of seawater puriﬁcation using direct contact membrane
desalination (DCMD) system. Desalination 443, 323–332 (2018). https://doi.org/10.1016/j.
desal.2017.12.048
14. Gruber, M.F., Johnson, C.J., Tang, C.Y., Jensen, M.H., Yde, L., Hélix-Nielsen, C.: Computa-
tional ﬂuid dynamics simulations of ﬂow and concentration polarization in forward osmosis
membrane systems. J. Membr. Sci. 379, 488–495 (2011). https://doi.org/10.1016/j.memsci.
2011.06.022
15. Shirazian, S., Rezakazemi, M., Marjani, A., Moradi, S.: Hydrodynamics and mass transfer
simulation of wastewater treatment in membrane reactors. Desalination 286, 290–295 (2012).
https://doi.org/10.1016/j.desal.2011.11.039
16. Wang, Y., Brannock, M., Cox, S., Leslie, G.: CFD simulations of membrane ﬁltration zone
in a submerged hollow ﬁbre membrane bioreactor using a porous media approach. J. Membr.
Sci. 363, 57–66 (2010). https://doi.org/10.1016/j.memsci.2010.07.008
17. Banik, A., Bandyopadhyay, T.K., Biswal, S.K.: Computational ﬂuid dynamics (CFD) sim-
ulation of cross-ﬂow mode operation of membrane for downstream processing. In: Recent
Patents on Biotechnology, vol. 13 (2019). https://doi.org/10.2174/187220831266618092416
0017
18. Banik, A., Bandyopadhyay, T.K., Biswal, S.K., Majumder, M.: Prediction of maximum efﬁ-
ciency of vertical helical coil membrane using group method of data handling (GMDH) algo-
rithm. In: Vasant, P., Zelinka, I., Weber, G.-W. (eds.) ICO 2019. AISC, vol. 1072, pp. 489–500.
Springer, Cham (2020). https://doi.org/10.1007/978-3-030-33585-4_48
19. Debnath, A., Banik, A., Bandyopadhyay, T.K., Saha, A.K.: CFD and optimization study of
frictional pressure drop through bends. In: Recent Patents on Biotechnology, vol. 13 (2019).
https://doi.org/10.2174/1872208312666180820153706
20. Banik, A., Biswal, S.K., Bandyopadhyay, T.K.: Predicting the optimum operating parameters
and hydrodynamic behavior of rectangular sheet membrane using response surface method-
ology coupled with computational ﬂuid dynamics. Chem. Pap. 74(9), 2977–2990 (2020).
https://doi.org/10.1007/s11696-020-01136-y
21. Banik, A., Bandyopadhyay, T.K., Biswal, S.K.: Computational ﬂuid dynamics simulation of
disc membrane used for improving the quality of efﬂuent produced by the rubber industry.
Int. J. Fluid Mech. Res. 44, 499–512 (2017). https://doi.org/10.1615/InterJFluidMechRes.201
7018630

A Review on the Impacts of Social Media
on the Mental Health
Md. Abu Bakar Siddiq Tapu1, Rashik Shahriar Akash1, Haﬁz Al Fahim1,
Tanin Mohammad Jarin1, Touhid Bhuiyan1, Ahmed Wasif Reza2(B),
and Mohammad Shamsul Areﬁn1,3(B)
1 Department of Computer Science and Engineering, Daffodil International University, Birulia,
Bangladesh
{abu15-3782,rashik15-3825,hafiz15-3781,tanin15-3812}@diu.edu.bd,
sarefin@cuet.ac.bd
2 Department of Computer Science and Engineering, East West University, Dhaka, Bangladesh
wasif@ewubd.edu
3 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chattogram, Bangladesh
Abstract. There are numerous effects of social media use on people’s daily lives.
Every day we are connected a lot of time with social media. As a result, our brains
become unbalanced, and we feel a lot of illness in our bodies. As a result, the
primary objective of this analysis is to provide information on how social media
affects its users. Nineteen studies were included in this paper regarding the main
purpose. We categorized the papers into four types, Status and Post based research,
Research paper and Database analysis-based research, Survey based research, and
disease-based research. After a comprehensive analysis of the available literature,
we have concluded that utilizing the social side has an effect on our mental health
and actions. The results highlighted a number of critical aspects, such as the
varied approaches to determining the inﬂuence of social media, the limitations of
the studies, and our thoughts on what should be enhanced.
Keywords: Mental health · Database analysis · Disease-based research · Privacy
concern · COVID-19
1
Introduction
Advancementandsophisticationoftechnology,socialcommunicationmediahasbecome
a ubiquitous part of everyday life. We are connected with the media either by the need
of time or unknowingly. According to experts, excessive social media use becomes
addictive. As a result, our brains become unbalanced in the absence of social media.
People become dependent on social media to feel ‘normal’. And behavior controlled
by addiction is never healthy. Put another way, the mental health of users is highly
inﬂuenced by social media.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 181–195, 2024.
https://doi.org/10.1007/978-3-031-50158-6_19

182
Md. A. B. S. Tapu et al.
We analyzed a number of papers in which researchers have extensively discussed the
potential relationship between social media and psychological negative effects. Politi-
cally provocative posts seen on social media, anger-frustration, inﬂammatory, offensive
statements, etc. from various acquaintances also affect your mental health. Which will
only incite anger and resentment in your mind [2, 5]. Research has revealed that pro-
longeduseofsocialmediaisassociatedwithnegativeemotionslikedepression,isolation,
and violence [14].
A common keyword found in all the papers we selected was ‘concern’. In the social
media world, this concern is reﬂected in various aspects including security [1], mental
fatigue, failure, self-risk, misuse of time, etc. In addition, a large part of our knowledge
gained was related to the mental health of young adolescents [2, 6, 7, 9, 15]. Social media
has been found in multiple studies to have a signiﬁcant impact on how people organize
their time and lives. However, this issue is very worrisome and challenging. Experts point
out many factors as reasons for this, such as—misuse of technology, mismanagement of
time, downloading unnecessary apps, following trends, etc.
However, the papers show that data has been collected directly and indirectly from
people of different levels and professions and implemented through different algorithms
for outcome analysis. Considering all aspects, we can conclude that social media or social
networking is deeply related to the mental health of individuals. Which is a signiﬁcant
issue in today’s society.
2
Methodology
To summarize the most up-to-date research methodology practices in a substantive ﬁeld
or issue, a methodology review is a subset of systematic secondary research. In this
paper, we included nineteen papers from various sites and we included different types
of papers like journals, conferences, etc. This paper is a review of the details of these
Nineteen papers.
2.1
Phase 1: Planning
This is the section that describes how we selected the papers from various places. The
searching method was, we ﬁrst identiﬁed the topic, then We learned about the topic
or social media impact then we searched on various platforms like Google Scholar,
Refseek, Microsoft Academic, and IEEE access about the topic. Then we selected the
papers which are related to our purpose. We mostly try to collect the last 5 years’ papers
and some of them fulﬁlled our purpose. The search terms we used to ﬁnd the existing
papers include “Social Media Use and Mental Health” “The inﬂuence of social media on
depression, anxiety”, “is social media bad for mental health and well-being”, “Effects
of Social Media on Adolescents” etc.
2.2
Phase 2: Conducting
This section described the way how the existing papers were checked. We have taken
enough time to read the papers to get the best outcome from the papers. We read the

A Review on the Impacts of Social Media on the Mental Health
183
title and abstract ﬁrst to know whether the paper actually fulﬁlled our purpose or not.
Then if we thought the paper was right for our research, we read the conclusion then.
Then we read the methodologies, results, and discussion of every paper to get the proper
information.
2.3
Phase 3: Reporting
After choosing and reviewing the papers, the authors categorized the papers into four
types, Status and Post based research, Research paper and Database analysis based
research, Survey based research, and disease-based research.
3
Paper Collection
Over time, the technology of social media has also improved, as technology has
improved, it has attracted people in different ways to use social media or it has been
seen that over time, people have been attracted to use social media for different reasons.
It has been seen that various physical and mental changes have occurred among users
due to the use of social media over time. Based on these, it is seen that there is some
difference between the research paper data of one year and the research paper data of the
next or previous year. Since there are many research papers related to our work, we have
started from the year 2016 in the case of paper selection because it has been seen in the
previous data that there are no signiﬁcant physical or mental changes or problems among
social media users. Moreover, we have selected only those papers which are related to
our research. We have tried to do our research based on the analysis of four types of
categories: Status and Post based, research paper and database-based, survey-based, and
disease-based. So based on these four types, among the papers we found, some years
have fewer papers of that type. In this case, In Table 1, we have merged some years
like 2016–2017, and 2020–2021. And based on these, we have sorted the information
into 5 columns in the form of a paper collection table, it includes the topic type and
based on that the information of the paper available in the years 2016–2017, 2018, 2019,
2020–2021 (Fig. 1).
Table 1. Evolutions of paper collection on impact analysis of social media on the mental health
of the users
Topic types
2016–2017
2018
2019
2020–2021
Status and post based
Naslund, J.A [1]
Vogel, E.A [9]
Thorstad [12]
Tadesse [5]
Research paper and database
analysis based
Baker, D.A. [15]
Chancellor [2]
Keles [6]
Wongkoblap [16]
(continued)

184
Md. A. B. S. Tapu et al.
Table 1. (continued)
Topic types
2016–2017
2018
2019
2020–2021
Survey based
Pontes [17]
Berryman [3]
Scott [8]
Andreassen [13]
O’reilly [7]
Cao, X., Khan [10]
Lau, W.W [19]
Rahman [18]
Azizi [11]
Disease based
Zhao [4]
Ahmad, A. R [14]
The below graph represents that most of the papers we analyzed are based on ques-
tionnaires and availability of these types of papers is high but we have tried our best to
analyze every type of paper that is relevant to our research type.
Fig. 1. Paper collection described in pie format
4
Detailed Review of Papers
This section focuses on the issue addressed, source of data, usefulness and assessment
of each work. Table 2 illustrates the Status and Post based analysis where writers have
offered their ideas on effect analysis of social media on the mental health of the users.
In [1], They utilized a Qualtrics poll to get the information from 90 respondents.
Since May 2016, they have independently studied how individuals with mental illnesses
use the social media site Twitter. They learned about the user’s condition through this,
including reluctance, fear, interpersonal relationships, and changes in other people’s
mental states.

A Review on the Impacts of Social Media on the Mental Health
185
Table 2. Status and post based research
Paper title
Problem addressed
Source of data
Evaluation
Risks to privacy with
use of social media
[1]
They mainly tried to
identify the
interaction of users’
mental health with
personal security
concerns on social
media
They have studied
social media site
Twitter since May
2016
They discovered that
around a third of the
people in their study
admitted to having a
mental disorder. They
experience mental
weariness because
they are unaware of
some technical
problems and privacy
dangers
Effects of social
media on adolescents
[9]
Higher e-cigarette
usage intentions were
observed, and In such
a short amount of
time, individuals who
were exposed to
e-cigarette material
on social media
increased their usage
Ads and social media
content allegedly
written by teenagers
Highlights important
details, such as how
teens’ frequent usage
of social media and
their brief exposure to
content about
e-cigarettes on such
platforms inﬂuence
their use of those
platforms. It also
suggests that
government action be
done to prohibit
promoted e-cigarettes
on social media
Predicting future
mental illness [12]
Based on the
language and signals
used by social media
users, three topics
relating to mental
illness—Clinical
Subreddits,
Nonclinical
Subreddits, and
Future
Prediction—were
highlighted
Gathered from Reddit
media about users’
words and actions
Their ﬁndings showed
that even in patients
who did not have a
history of mental
illness, the impacts of
anxiety and disease
spread gradually. It
poses a serious mental
health challenge for
the next generation
(continued)

186
Md. A. B. S. Tapu et al.
Table 2. (continued)
Paper title
Problem addressed
Source of data
Evaluation
Detection of
depression [5]
This paper attempts
to identify the main
causes of emotional
exhaustion and
depression among
online users based on
the posts
Reddit media platform
users
Deﬁne a deeper
relationship between
depression and how
one speaks
They also identiﬁed
some vocabulary
words that are
associated with users’
feelings of depression,
anxiety, anger and
suicidal tendencies
which may badly
affect both in the
present and in the
future
Study of these authors [9] explained the impact of short exposure to social media
material, including advertisements and posts reportedly made by teenagers, and the
correlation between daily social media usage and attitudes and intentions toward e-
cigarettes and desire to use e-cigarettes among adolescents.
From Reddit media, they gathered indicators of users’ language and behavior. Based
on these posts, they concentrated on mental illness and nonclinical subreddits and cate-
gorized clinical subreddits as well. They analyzed the data using the clustering technique.
Based on theories that supported the patterns of mental disease, they predicted the future
[12].
They looked at the posts made by roughly 100 university students. For optimal
outcomes on Reddit, they further use Natural Language Processing (NLP) methods and
machine learning algorithms, they classiﬁed user-generated regular posts and postings
indicative of depression separately. They were able to predict 93% of F1 results after
putting each phase into practice, beginning with data pre-processing (via NPL). They
succeeded in establishing a more direct link between language use and depression [5].
Table 3 represents the Research paper and Database analysis based research. Where
[2] in order to forecast users’ moods, behaviors, and attitudes, they gathered informa-
tion using machine learning techniques, natural language processing (NLP), and human
computer interface (HCI). They have classiﬁed it depending on the ethical difﬁculties
that the consumers encountered. They divide “ethical tension” into the three sections
listed below: Participants and research monitoring are listed in I. Methods, validity, and
interpretability are listed in II. 3. Repercussions for Stakeholders.
In [6], This investigation suggests that a number of variables interact to inﬂuence
the extent to which adolescents’ use of social media is associated with increased rates of
depression, anxiety, and other forms of psychological distress. To reduce the prevalence

A Review on the Impacts of Social Media on the Mental Health
187
of anxiety disorders among teenagers at high risk, we must address not just the symptoms
of depression, but also associated outcomes such as anxiety and psychological distress.
Table 3. Research paper and database analysis based
Paper title
Problem addressed
Source of data
Evaluation
A taxonomy of
ethical tensions [2]
Based on the
behavior, mood, and
other characteristics
of social media users,
they produced a
forecast of mental
health here. The
propensity for people
to hurt themselves as
a result of being
persuaded by these
social media sites has
been the subject of
substantial research
Clinical data
They basically provide
a rough taxonomy of
issues with social
media data-based
algorithmic mental
health status
prediction. These
fundamental studies
on depression have
characterized new
psychiatric disorders
and their relationships
with contemporary
social media
A systematic review:
the inﬂuence [6]
Examined how teen
social media use
affects their level of
anxiety, despair, and
discomfort
Five databases of
other papers.
Analyzed (Medline,
Embase, PsychINFO,
CINAHL and SSCI)
Add to the current
literature by ﬁlling in
the blanks and
drawing attention to
the signiﬁcance of the
phenomena of the
mental health effect of
social media usage
among teenagers
The relationship
between online [15]
Determine whether
there is a correlation
between using social
networking sites and
experiencing
depressive symptoms
Five databases were
identiﬁed: PsycINFO,
Web of Science,
CINAHL, MEDLINE,
and EMBASE
Several different
psychological, social,
behavioral, and
individual variables
may interact to
produce a complicated
relationship between
online social
networking and
depressed symptoms
(continued)

188
Md. A. B. S. Tapu et al.
Table 3. (continued)
Paper title
Problem addressed
Source of data
Evaluation
Researching mental
health [16]
Examine the
applicability and
constraints of
cutting-edge
techniques employed
by scientists to
conduct mental health
predictive analytics
2010–2017 medical
and computer science
journals
Contributed to the
existing literature in a
way of providing
considerable evidence
for the mental health
impact of social media
use by focusing on not
only the symptoms of
depression but also
other related outcomes
The authors chose [15] Five databases—PsycINFO, Status and Web of Science,
CINAHL, MEDLINE, and EMBASE—for this study. Protocol for Improved Reporting
of Epidemiologic Observational Research The quality of cross-sectional research was
evaluated using a statement approach. In total, 35 044 people took part in the studies
that were examined. Studies were discovered in 14 different countries. The participants
ranged in age from 15 to 88. Only three of the remaining 27 studies were longitudinal or
had longitudinal components; the rest all utilized cross-sectional designs. Only correla-
tion analysis was performed in seven studies, whereas regression analysis was used in
23. Although some research concentrated on certain factors like the amount of time spent
social networking, All the investigations focused on the real links between depressive
symptoms and use of social networking sites. Cross-sectional studies’ reporting quality
ranges from 48 to 93%, according to the STROBE rating.
Inarticlenumber[16],theychose48publicationsforreviewoutofatotalof5386,and
theyfoundthatthesestudiesusetwodifferentmethodstogatherdata.Theycodedthedata
using articles’ data in accordance with essential characteristics, data collection methods,
and data preprocessing, and analyzed the applicability and limitations of cutting-edge
methods used by scientists to do predictive analytics on mental health. The sample data
used in the supervised learning techniques examined in this study contains labeled inputs
as well as outputs. These methods teach the model how to foresee and produce forecasts
for unlabeled inputs from various sources.
The results from the questionnaires are shown in Table 4. Scientists that wrote this
study [3]. The sample included 471 college freshmen located in the Southeast of the
United States. The results demonstrated that social media traits did not effectively predict
bad outcomes. The risk of feeling alone and having suicidal thoughts was predicted by
ambiguous booking with few exceptions. Neither how much time was spent online nor
the weight of social media had a role in determining the outcome.
This work [7] provides details that they incorporated three concepts, each of which
is reﬂected in this article because they address the goals and focus of the study’s subject
and show the range of participants’ social media views on mental health and wellness.
This paper [8] provides details that they used six self-report measures to gather
information from over 12,000 teenagers. Teenagers divulge details on their daily social

A Review on the Impacts of Social Media on the Mental Health
189
Table 4. Survey based research
Paper title
Problem addressed
Source of data
Evolution
Social media use [3]
A person’s mental or
physical health issue
may be traced back to
their time spent on
social media. It’s
possible that’s not the
case
The sample collected
from 471 university
students
Only social media use
is not blamed for
mental health
conditions rather than
social media, other
incidents are also
related to mental health
problems
Is social media bad
[7]
Addressed the
perception among
teenagers that their
psychological
well-being is being
compromised by their
social media use
11–18 years old
school students from
two cities
The three problems
highlighted mainly
(mood and anxiety
disorder,
cyberbullying, use of
social media addiction)
gives an idea and helps
future researchers to
ﬁnd out what are the
causes of mental health
problems because of
social media use
Social media use [8]
Dependencies on
online platforms in all
aspects affect
teenagers’ health and
sleep patterns
Data from nearly
twelve thousand
adolescents
Teenagers’ interactions
with online platforms
and sleep have many
negative side effects
that could cause them
great harm in the
future. Through this
survey, they have been
able to provide
The stimulators of
social media [10]
Mainly highlighted
the extent to which
students are exposed
to various negativity
psychologically on
online platforms,
which cause serious
damage to their
mental health
14–22 college students Their methodologies
show that SMA
functions as a
particular class of
stimulant that alters
users’ psychological
characteristics,
including their levels of
rage, agitation, and
irritability
(continued)
media and sleep routines. They looked into social media use and daily sleep charac-
teristics using binomial logistic regression. A survey on social media usage by gender
revealed that boys utilized it more frequently than girls.

190
Md. A. B. S. Tapu et al.
Table 4. (continued)
Paper title
Problem addressed
Source of data
Evolution
The relationship
between social [11]
Students’
participation in
extracurricular
activities and
academic
achievement are
adversely affected by
their use of social
media
Survey of 360 medical
science students
Among the pupils
questioned, there was a
mild trend toward
social media addiction.
Their compulsive
behavior, however, had
a detrimental effect on
their academic
performance
Investigating the
differential [17]
Examining the
interactions between
SNS addiction and
IGD, as well as how
they contribute to the
development of
psychiatric distress
Sample collected from
509 students
They discovered that
important demographic
factors can contribute
signiﬁcantly to the
understanding of SNS
addiction and IGD
FACEBOOK USE
[18]
Intended to examine
how Facebook
addiction and usage
affect mental health
209 University
students
The ﬁndings revealed
that women used
Facebook more
frequently than men
Andreassen [13]
Talk about the
physical symptoms
that might arise from
an addiction to social
media and video
gaming in adults
24,000 users, mainly
36-year-old users on
average
Explained that
addiction to social
media and video games
has been linked to a
variety of underlying
mental problems,
including ADHD,
OCD, anxiety, and
depression
Effects of social [19] How usage of social
media or multitasking
of social media affect
academic
performance
From 348 university
students
Those who have used
social media for study
purposes, have no
signiﬁcant impact on
their academic
performance rather
than using social media
for nonacademic
purposes. It has a
negative impact on
their life
Using a stratiﬁed sampling technique, data from college students between the ages
of 14 and 22 were gathered. They employed structural equation modeling to test their
measurement and structural model. They investigated the internal connections between

A Review on the Impacts of Social Media on the Mental Health
191
SMA and students’ psychological suffering as a result, and they conﬁrmed that their
ﬁndings were compatible with those of earlier research [10].
For this research [11], 360 Iranian students pursuing medical science participated
in a stratiﬁed random sampling survey. To determine the degree of these respondents’
addiction, the Bergen Social Media Addiction test was employed. They also looked at
the information related to the pupils’ performance and academic progress.
In this study [17], they get information from 509 students. The sample’s median age
was 13.02 years (SD: 1.64), and the gender distribution was roughly equal, with 53.5%
of men (n = 265). Age, gender, and relationship status-related demographic data were
gathered. By asking participants how much time they typically spend online each week,
information about SNS usage was gathered. The sample was given nine questions, the
total value of which ranged from 9 to 45, with each question’s value falling between 1
and 5. The more positive the value, the more GD is present. Credibility score was high
( = 0.87).
In this research [18], through the questionnaires, they collect data from 209 univer-
sity students. In questionnaires or data analysis they used these two: Bergen Facebook
Addiction Scale, BFAS (Andreassen et al., 2012) and the Bangla (Ahmed et al., 2018)
Mental Health Inventory(MHI)—18 (Veit and Ware, 1983) methods to compile a report
on respondents’ Facebook addiction and mental health. In the BFAS method, higher
scores indicate higher addiction to Facebook. Their MHI methods represent if there is a
higher score then there is higher mental health. However, their result found that women
spend more time on Facebook than men. As they spend more time on Facebook, users
cannot allocate proper time for daily work and social relations etc. Further they faced
physical or mental problems like anxiety, depression etc.
Their extensive data [13] collecting sample size (about 24,000). Nevertheless, the key
respondents’ average age was 36. They gathered user information and looked at their
histories of social media and video game addiction through an online cross-sectional
survey. Additionally, users provided self-reports of any current psychological conditions.
They looked into the addictiveness of social media and video games using regression
analysis techniques.
Table 5 represents the Disease based analysis. The ﬁndings [4] of this study sug-
gested that Poorer mental health was associated with more time spent on social media.
Participants with high (but not low) levels of the disaster stressor had a positive asso-
ciation between increased exposure to catastrophe news on social media and increased
depression. Route analysis also shown that negative emotions moderate the connection
between social media usage and psychological well-being.
This work [14] analysis, effects of online panic during the COVID-19 To learn more
about the pandemic, researchers in Iraqi Kurdistan developed and administered an online
survey to a representative sample of 516 users of social media in that region. Data from
this research were analyzed using content analysis. SPSS study of the data supports this
conclusion.

192
Md. A. B. S. Tapu et al.
Table 5. Disease based research
Paper title
Problem addressed
Source of data
Evaluation
Social media use [4]
Aiming to learn and
bring attention to the
fact that prolonged
exposure to
catastrophe coverage
on social media may
lead to psychological
distress
512 user data
The results indicate
that exorbitant
exposure to catastrophe
news on social media
has an effect on mental
health and that future
interventions aimed at
enhancing it should
take both disaster
stressor and adverse
effects into account
The impact of social
[14]
Self-reported mental
health and the
propagation of
COVID-19 panic are
inﬂuenced by social
media use
516 social media
users were sampled
A self-reported higher
frequency of social
media usage was
associated with a
higher likelihood of
COVID-19 fear
5
Discussion
In this paper, we have described nineteen papers about the “Impact Analysis of Social
Media on the Mental Health of the Users “. We reviewed all the factors and important
points of all nineteen papers and after reviewing and understanding the papers, we
have observed that there is a big impact of using the social side on our mental health but
mental health problems are not caused by social media usage alone. From all the research
paper analysis we notice that Higher levels of social media usage were related to worse
psychological wellbeing. There is a vast variety of social media material that teenagers
may encounter, which is very harmful to their mental health. Most of the papers did the
analysis based on surveys that are open to self-identifying biases. We can easily measure
the impact of the data analysis and survey which included the results of social media on
the mental health of the users.
6
Conclusion
It is worth noting that social media is inextricably intertwined with individual and social
life. There is no doubt that social media has brought people from different parts of the
world much closer to each other. However, according to the topic of our discussion, the
message that these studies give us is that Directly and indirectly, excessive social media
usage contributes to the development of a wide range of mental health issues. At least
for adolescents, it is threatening. Inﬂuenced by these, they do not hesitate to indulge
in various suicidal thoughts. However, the issues that could not be clearly described
in the studies are the relationship between the use of social media and maintaining

A Review on the Impacts of Social Media on the Mental Health
193
time management. In addition, if more information from direct sources (such as social
isolation) could have been added to the data collection, the outcome would have been
more accurate. However, ultimately as important as a healthy lifestyle is, it is impossible
to completely abstain from all social platforms. So, the beneﬁts of social media should
be taken by controlling its negative side effects. And in this way, it will be possible for
a person to remain mentally healthy even if he is connected to social media platforms.
References
1. Naslund, J.A., Aschbrenner, K.A.: Risks to privacy with use of social media: understanding
the views of social media users with serious mental illness. Psych. Serv. 70(7), 561–568
(2019)
2. Chancellor, S., Birnbaum, M.L., Caine, E.D., Silenzio, V.M., De Choudhury, M.: A taxonomy
of ethical tensions in inferring mental health states from social media. In: Proceedings of the
Conference on Fairness, Accountability, and Transparency, pp. 79–88 (2019)
3. Berryman, C., Ferguson, C.J., Negy, C.: Social media use and mental health among young
adults. Psych. Quart. 89(2), 307–314 (2018)
4. Zhao, N., Zhou, G.: Social media use and mental health during the COVID-19 pandemic:
moderator role of disaster stressor and mediator role of negative affect. Appl. Psychol. Health
Well-Being 12(4), 1019–1038 (2020)
5. Tadesse, M.M., Lin, H., Xu, B., Yang, L.: Detection of depression-related posts in reddit
social media forum. IEEE Access 7, 44883–44893 (2019)
6. Keles, B., McCrae, N., Grealish, A.: A systematic review: the inﬂuence of social media on
depression, anxiety and psychological distress in adolescents. Int. J. Adolesc. Youth 25(1),
79–93 (2020)
7. O’reilly, M., Dogra, N., Whiteman, N., Hughes, J., Eruyar, S., Reilly, P.: Is social media
bad for mental health and wellbeing? Exploring the perspectives of adolescents. Clin. Child
Psychol. Psych. 23(4), 601–613 (2018)
8. Scott, H., Biello, S.M., Woods, H.C.: Social media use and adolescent sleep patterns: cross-
sectional ﬁndings from the UK millennium cohort study. BMJ Open 9(9), e031161 (2019)
9. Vogel, E.A., et al.: Effects of social media on adolescents’ willingness and intention to use
e-cigarettes: an experimental investigation. Nicotine Tobacco Res. 23(4), 694–701 (2021)
10. Cao, X., Khan, A.N., Zaigham, G.H., Khan, N.A.: The stimulators of social media fatigue
amongstudents:roleofmoraldisengagement.J.Educ.Comput.Res.57(5),1083–1107(2019)
11. Azizi, S.M., Soroush, A., Khatony, A.: The relationship between social networking addiction
and academic performance in Iranian students of medical sciences: a cross-sectional study.
BMC Psychol. 7(1), 1–8 (2019)
12. Thorstad,R.,Wolff,P.:Predictingfuturementalillnessfromsocialmedia:abig-dataapproach.
Behav Res Methods 51(4), 1586–1600 (2019)
13. Andreassen, C.S., et al.: The relationship between addictive use of social media and video
games and symptoms of psychiatric disorders: a large-scale cross-sectional study. Psychol.
Addict. Behav. 30(2), 252 (2016)
14. Ahmad, A.R., Murad, H.R.: The impact of social media on panic during the COVID-19
pandemic in Iraqi Kurdistan: online questionnaire study. J. Med. Internet Res. 22(5), e19556
(2020)
15. Baker, D.A., Algorta, G.P.: The relationship between online social networking and depression:
a systematic review of quantitative studies. Cyberpsychol. Behav. Soc. Netw. 19(11), 638–648
(2016)

194
Md. A. B. S. Tapu et al.
16. Wongkoblap, A., Vadillo, M.A., Curcin, V.: Researching mental health disorders in the era of
social media: systematic review. J. Med. Internet Res. 19(6), e7215 (2017)
17. Pontes, H.M.: Investigating the differential effects of social networking site addiction and
Internet gaming disorder on psychological health. J. Behav. Addict. 6(4), 601–610 (2017)
18. Rahman, M., Ahmed, O.: Facebook use, facebook addiction, and mental health of chittagong
university students. Bulg. J. Sci. Educ. Policy 12(2), 14 (2018)
19. Lau, W.W.: Effects of social media usage and social media multitasking on the academic
performance of university students. Comput. Hum. Behav. 68, 286–291 (2017)
20. Stavridou, A., et al.: Psychosocial consequences of COVID-19 in children, adolescents and
young adults: a systematic review. Psych. Clin. Neurosci. 74(11), 615 (2020)
21. Naslund, J.A., Bondre, A., Torous, J., Aschbrenner, K.A.: Social media and mental health:
beneﬁts, risks, and opportunities for research and practice. J. Technol. Behav. Sci. 5, 245–257
(2020)
22. Fernandes,B.,etal.:InternetuseduringCOVID-19lockdownamongyoungpeopleinlow-and
middle-income countries: role of psychological wellbeing. Addict. Behave. Rep. 14, 100379
(2021)
23. Orben, A., Przybylski, A.K.: The association between adolescent well-being and digital
technology use. Nat. Hum. Behav. 3(2), 173–182 (2019)
24. Li, H., Wang, J., Wang, L.: A survey on the generalized problematic Internet use in Chinese
college students and its relations to stressful life events and coping style. Int. J. Mental Health
Addict. 7, 333–346 (2009)
25. Lin, L.Y., et al.: Association between social media use and depression among US young
adults. Depress. Anxiety 33(4), 323–331 (2016)
26. Marino, C., Gini, G., Vieno, A., Spada, M.M.: A comprehensive meta-analysis on problematic
Facebook use. Comput. Hum. Behav. 83, 262–277 (2018)
27. Woods, H.C., Scott, H.: # Sleepyteens: Social media use in adolescence is associated with
poor sleep quality, anxiety, depression and low self-esteem. J. Adolesc. 51, 41–49 (2016)
28. Chen, I.H., et al.: Comparing generalized and speciﬁc problematic smartphone/internet use:
longitudinal relationships between smartphone application-based addiction and social media
addiction and psychological distress. J. Behav. Addict. 9(2), 410–419 (2020)
29. Garrido, S., et al.: Young people’s response to six smartphone apps for anxiety and depression:
focus group study. JMIR Mental Health 6(10), e14385 (2019)
30. Toscos, T., et al.: Teens using screens for help: impact of suicidal ideation, anxiety, and
depression levels on youth preferences for telemental health resources. JMIR Mental Health
6(6), e13230 (2019)
31. Moreno, M.A., Jelenchick, L., Cox, E., Young, H., Christakis, D.A.: Problematic internet use
among US youth: a systematic review. Arch. Pediatr. Adolesc. Med. 165(9), 797–805 (2011)
32. Kreski,N.,etal.:Socialmediauseanddepressivesymptoms amongUnitedStates adolescents.
J. Adolesc. Health 68(3), 572–579 (2021)
33. Primack, B.A., Shensa, A., Sidani, J.E., Escobar-Viera, C.G., Fine, M.J.: Temporal asso-
ciations between social media use and depression. Am. J. Prevent. Med. 60(2), 179–188
(2021)
34. Dhir, A., Yossatorn, Y., Kaur, P., Chen, S.: Online social media fatigue and psychological
wellbeing: a study of compulsive use, fear of missing out, fatigue, anxiety and depression.
Int. J. Inform. Manag. 40, 141–152 (2018)
35. Shensa, A., Sidani, J.E., Escobar-Viera, C.G., Switzer, G.E., Primack, B.A., Choukas-Bradley,
S.: Emotional support from social media and face-to-face relationships: associations with
depression risk among young adults. J. Affect. Disord. 260, 38–44 (2020)
36. Bashir, H., Bhat, S.A.: Effects of social media on mental health: a review. Int. J. Indian
Psychol. 4(3), 125–131 (2017)

A Review on the Impacts of Social Media on the Mental Health
195
37. Memon, A.M., Sharma, S.G., Mohite, S.S., Jain, S.: The role of online social networking on
deliberate self-harm and suicidality in adolescents: a systematized review of literature. Indian
J. Psych. 60(4), 384 (2018)
38. Ivie, E.J., Pettitt, A., Moses, L.J., Allen, N.B.: A meta-analysis of the association between
adolescent social media use and depressive symptoms. J. Affect. Disord. 275, 165–174 (2020)
39. Watabe, M., et al.: Relationship between trusting behaviors and psychometrics associated
with social network and depression among young generation: a pilot study. PLoS ONE 10(4),
e0120183 (2015)
40. Karim, F., Oyewande, A.A., Abdalla, L.F., Ehsanullah, R.C., Khan, S.: Social media use and
its connection to mental health: a systematic review. Cureus 12(6), 15 (2020)

Factor Inﬂuencing Online Purchase Intention
Among University Students in Nepal
Deepesh Ranabhat1,2(B)
, Sujita Adhikari2
, and Narinder Verma1
1 Faculty of Management Sciences, Shoolini University, Bajhol, Himachal Pradesh 173229,
India
deepeshrana2000@gmail.com
2 Faculty of Management Studies, Pokhara University, Pokhara-30, Kaski, Nepal
Abstract. The popularity of online shopping has grown in recent years as it pro-
vides beneﬁts for both businesses and consumers. The purpose of this research is
to determine the factors that inﬂuence online purchase intention among univer-
sity students in Nepal. A total of 385 students are taken for data collection. The
researchers use a structured questionnaire using Likert scale statements to col-
lect the data. Both descriptive and inferential analysis such as frequency analysis,
exploratory factor analysis, and structural equation modelling are applied. The
result of factor analysis ﬁnds six factors: effort expectancy, online purchase inten-
tion, security and privacy, performance expectancy, social inﬂuence, and facilitat-
ing conditions related to online purchases. The structural equation modelling ﬁnds
security and privacy, and social inﬂuence have a positive and statistically signif-
icant impact on online purchase intention. These ﬁndings emphasize the need of
ensuring the security and privacy of customer data and leveraging social inﬂuence
in marketing strategies to increase online purchase intention.
Keywords: Online purchase intention · Security and privacy · Social inﬂuence ·
UTAUT model · University students · Nepal
1
Introduction
Marketing is the process of addressing consumers’ needs more effectively and efﬁciently
through better products and services, better prices, and improved access and delivery.
Consumers are the focus of all marketers, who aim to understand and satisfy their needs in
new and better ways. In recent years, the rise of online shopping has provided additional
opportunities for marketers to reach consumers, as the internet has changed the way of
shopping for goods and services to customers. Online shopping is a form of electronic
commerce in which customers use web browsers to make purchases from retailers on
the internet. It provides convenience and cost savings by eliminating the need to travel to
physical stores and also provides more options and information for comparing products
and prices. The growth of e-commerce is signiﬁcant, with worldwide retail e-commerce
sales projected to reach 7.4 trillion dollars by 2025 as per the report of Statista in 2021.
TheoriginsofNepalesee-commercecanbetracedbackto1999whenitwasprimarily
used by Nepalese residing in the United States to send gifts to their friends and family
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 196–206, 2024.
https://doi.org/10.1007/978-3-031-50158-6_20

Factor Inﬂuencing Online Purchase Intention Among University
197
in Nepal. At that time, internet access was limited and costly. However, the situation
has changed considerably in recent years. According to the Nepal Telecommunications
Authority (NTA), 91% of the country’s population now has internet connectivity, and this
ﬁgure is steadily rising. According to data, most people access the internet through their
mobile phones, making up 65.68% of all internet as of mid-June. There are over 12.052
million 3G internet users, with 9.590 million using Nepal Telecom and 2.462 million
using Ncell. Additionally, 25.27% of users have access to ﬁxed broadband internet,
including 785,000 with Asymmetric Digital Subscriber Line (ADSL) and 556,000 with
Fiber to the Home (FTTH).
Asinternetaccesshasbecomemorewidelyavailableandaffordable,thepopularityof
online shopping has grown rapidly in Nepal. Many online stores have been established
in Nepal, including SastoDeal, Daraz, Muncha, SmartDoko, OkDam, Oldpinch.com,
Esewapasal, Socheko.com, Thulo.com, RaraMart, Hamrobazar, Nepbay, Foodmandu,
Gyapu, Meroshopping, Merokirana.com, etc. With the spread of COVID-19 and result-
ing lockdowns, the demand for online shopping has surged, with the number of online
transactions reaching Rs. 4.93 trillion in mid-October and mid-November 2021, com-
pared to Rs. 2 trillion in the same period last year as per the report of Central bank of
Nepal. However, there are many issues related to online shopping such as the possibil-
ity of receiving a product that is different from what was advertised, limited shipping
options, risk of privacy, etc. Despite some disadvantages, online shopping is seen as a
convenient and cost-effective alternative to traditional retailing. It is growing in popu-
larity among young people as they can shop from a device rather than visiting physical
stores. As online purchase is increasing in Nepal, it is very important to ﬁnd out the
factors inﬂuencing it. So, the researchers’ main focus in this study is to ﬁnd out the
factors that inﬂuence online purchase intention among university students.
2
Literature Review
Online shopping refers to the act of purchasing goods or services through the internet
using a web browser [1]. Purchase intention is an individual’s willingness to buy a prod-
uct [2, 3]. Different models have been proposed by various researchers on technology
adoption including the technology acceptance model (TAM), theory of reasoned action
(TRA), innovation diffusion theory (IDT), and uniﬁed theory of acceptance and use
of technology (UTAUT). Among these, the UTAUT theory is considered to be a com-
prehensive and integrated model. It is based on eight theories and models that explain
the acceptance of technology [4]. Different researchers have used UTAUT to measure
the impact of user psychology on online purchase intention in various contexts. San
Martín and Herrero [5] employed the UTAUT to investigate the impact of psychological
factors on the intent to make online purchases for rural tourism, [6] studied online pur-
chase intention in Vietnam, and [7] conducted a study on travellers’ intention to make
a purchase of tourism products directly through online travel intermediaries (OTI) web-
sites. These studies found four important drivers of online intention are performance
expectation, effort expectation, social inﬂuence, and facilitating factors.
Likewise, [8] found social inﬂuence and performance expectancy have positively
inﬂuenced purchase intentions for online food delivery services. Tran and Nguyen [9]

198
D. Ranabhat et al.
found security has a signiﬁcant positive impact on online shopping intention. Tandon
and Kiran [10] indicates that several factors including performance expectation, effort
expectation, social inﬂuence, hedonic motivation, security and privacy, and ease of order-
ing play a signiﬁcant role in driving online shopping. This shows that several studies
have used UTAUT to investigate online purchase intention in different contexts. This
study also aims to understand the underlying factors that drive university students’ online
purchase intention and to identify areas where improvements can be made to increase
online purchase intention based on the UTAUT model. Figure 1 presents the model of
the study.
Fig. 1. Model of the study
3
Research Methodology
The researchers conducted a quantitative study that followed a descriptive and analytical
research design. The population of interest was students of universities in Nepal and 385
respondents were taken for this study. The data was collected through a structured ques-
tionnaire using Likert scale statements to measure the independent and dependent vari-
ables. Both descriptive and inferential analysis such as frequency analysis, exploratory
factor analysis (EFA) and structural equation modelling (SEM) were applied to examine
the impact of independent variables on the dependent variable using SPSS Amos. The
researcher used Cronbach Alpha and Composite Reliability (CR) to ensure reliability,
Average Variance Extracted (AVE) for construct validity, and Fornell and Larcker criteria
for discriminant validity.
4
Data Analysis and Results
4.1
Socio-demographic Proﬁle
The demographic information of the 385 survey participants, including their gender,
marital status, ethnic group, ﬁeld of study, level of education, monthly family income,
and expenditure is presented in Table 1.
The majority of respondents were female (64.7%) and unmarried (90.9%). The ethnic
group was mostly composed of Brahmin (54.8%), followed by Chhetri (16.6%), Janajati

Factor Inﬂuencing Online Purchase Intention Among University
199
Table 1. Socio-demographic characteristics
Variables
Categories
Frequency
Gender
Male
136 (35.3)
Female
249 (64.7)
Marital status
Married
35 (9.1)
Unmarried
350 (90.9)
Ethnic group
Brahmin
211 (54.8)
Chhetri
64 (16.6)
Janajati
68 (17.7)
Others
42 (10.9)
Education status
Bachelors
351 (91.2)
Masters and above
34 (8.8)
Faculty
Management
153 (39.7)
Humanities
48 (12.5)
Health and allied sciences
56 (14.5)
Science and technology
43 (11.2)
Others
85 (22.1)
Monthly income of family
Up to Rs. 25,000
59 (15.3)
Rs. 25,001 to Rs. 50,000
145 (37.7)
Rs. 50,001 to Rs. 75,000
84 (21.8)
Above Rs. 75,000
97 (25.2)
Monthly expenditure of family
Up to Rs. 25,000
134 (34.8)
Rs. 25,001 to Rs. 50,000
164 (42.6)
Rs. 50,001 to Rs. 75,000
46 (11.9)
Above Rs. 75,000
41 (10.6)
Total
385 (100)
(17.7%), and others (10.9%). The majority of participants were in management (39.7%),
followed by humanities (12.5%), health and allied sciences (14.5%), and science and
technology (11.2%). Most respondents were studying a bachelor’s degree (91.2%), with
only 8.8% studying a master’s degree or higher. The majority of participants had a
monthly family income between NPR 25,001 and 50,000 (37.7%) and monthly family
expenditure between NPR 25,001 and 50,000 (42.6%).
4.2
Exploratory Factor Analysis (EFA)
The EFA was run with 30 items related to the online shopping intention. All the com-
munalities were found more than 0.40, which is a minimum acceptable value for large

200
D. Ranabhat et al.
sample size above 200 (Hair et al., 2019). However, ﬁve items were loaded in more than
one factor, these items were removed. Finally, the EFA result was found with 25 items.
Kaiser–Meyer–Olkin (KMO) and Bartlett’s tests were used to measure the appropriate-
ness of a dataset for factor analysis. The KMO value is 0.875 which is considered as
good for factor analysis, as it is more than the generally accepted value of 0.60. Likewise,
Bartlett’s test of sphericity indicates that the variables are correlated with each other and
are suitable for factor analysis (sig. < 0.01). The result of EFA is presented below.
Table 2 lists the items that were included in the survey, along with the initial factor
loading and the extracted factor loading for each item. In general, loading > 0.5 is
considered as good, and loading more than 0.40 is also adequate for a sample size above
200 [11]. From the table, we can see that most of the items have a loading value of above
0.5 and ranges from 0.452 to 0.727, indicating that the items are strongly associated with
the corresponding factor and are adequate for performing factor analysis.
Table 3 presents the results of a factor analysis which is based on an eigenvalue
of more than 1.0. A total of six factors were identiﬁed in this study, which explains
60.21% of the variance. The factors identiﬁed in this analysis are Effort Expectancy (EE)
explains 12.73% of the variance, Online Purchase Intention (OPI) explains 11.798%
of the variance, Security and Privacy (SP) explains 10.473% of the variance, Perfor-
mance Expectancy (PE) explains 10.152% of the variance, Social Inﬂuence (SI) explains
8.235% of variance, and Facilitating Conditions (FC) explains 6.819% of the variance.
Each factor is composed of several items, which are listed in the “Items” column. Addi-
tionally, the Cronbach Alpha values of more than 0.60 suggest that the items within each
factor are internally consistent.
4.3
Structural Equation Modelling
This includes a measurement model for validating a set of measurement items and a
structural model for testing associations between the variables in the study.
Measurement Model
Conﬁrmatory factor analysis (CFA) is run in measurement model. Initially, conﬁrmatory
factor analysis was run with all 25 items extracted from EFA, and model ﬁtness was
attained with 23 items. The result of measurement model is presented below.
Table 4 presents ﬁt indices for a measurement model, along with their calculated
values. The indices include both absolute ﬁt indices and incremental ﬁt indices. The
calculated CMIN/DF value is 2.146, which is below 3, indicating a well-ﬁtted model.
Similarly, the GFI is 0.907, NFI is 0.848, CFI is 0.911 and RMSEA is 0.055. Though
the NFI is less than the required value of 0.90, it is near 0.90 and the calculated values
of GFI and CFI are above 0.90 and RMSEA is < 0.08 signifying a well-ﬁtted model.
Table 5 presents reliability and validity statistics for a study on online purchase
intention. Cronbach’s alpha measures internal consistency. A value > 0.7 is generally
considered acceptable and > 0.6 is also good for a large sample size of more than 200
[11]. In this case, all of the factors have a value > 0.6, indicating that they have good
internal consistency. Composite reliability (CR) measures the reliability of a scale. In
this case, all of the factors have a value greater than the minimum desired value of 0.7,
indicating good reliability.

Factor Inﬂuencing Online Purchase Intention Among University
201
Table 2. Communalities
Item Code
Items
Initial
Extraction
SP1
Online shopping system provides good protection of your
personal information
1.00
0.561
SP2
You feel secure in providing sensitive information for online
transactions
1.00
0.652
SP3
Internet vendors implement security measures to protect
internet buyers like you
1.00
0.537
SP4
You are conﬁdent that your personal information will not be
hampered when shopping online
1.00
0.567
SP5
You have trust in online shopping
1.00
0.482
EE1
Doing online shopping and web-based online transactions is
easy to you
1.00
0.547
EE2
You can easily use the shopping websites
1.00
0.668
EE3
The instruction of shopping websites is clear and easy to
understand for you
1.00
0.579
EE4
Online shopping procedures are quite simple to you
1.00
0.641
EE5
You can purchase online easily with instructions
1.00
0.587
PE1
You can save the effort of visiting stores when doing online
shopping
1.00
0.698
PE2
You have lots of chances to search for useful items on
internet
1.00
0.675
PE3
You are able to save time in online shopping
1.00
0.611
PE4
You do not need to visit traditional shops frequently in
online shopping
1.00
0.452
SI2
When online shopping is concerned, you usually do what
your friends are doing
1.00
0.550
SI4
Most of your relatives and friend recommend online
purchases
1.00
0.587
SI5
Your decision to purchase goods or services online is
inﬂuenced by the experiences you have had with friends and
family
1.00
0.497
SI6
Important people (family/relatives/friends) help you to
purchase online when you face difﬁculties
1.00
0.518
FC2
You have enough understanding of the online purchase
1.00
0.615
FC4
The shopping websites are linked to your various payment
methods
1.00
0.645
(continued)
The average variance extracted (AVE) measures the construct validity. A value >
0.5 is generally considered acceptable. While a value > 0.4 is also considered good

202
D. Ranabhat et al.
Table 2. (continued)
Item Code
Items
Initial
Extraction
FC5
The suppliers provides enough support and guidance for
online purchase
1.00
0.528
OPI1
You prefer to purchase online
1.00
0.707
OPI2
You keep purchasing online in the future
1.00
0.743
OPI3
You are going to purchase online more frequently if possible
1.00
0.680
OPI4
You recommend online purchases to my friends and family
1.00
0.727
if the CR is > 0.7 [11]. In this case, all of the factors have a value > 0.4, indicating
that they have good construct validity. In summary, the results from the reliability and
validity statistics show that all the factors have good internal consistency, reliability, and
construct validity.
Table 6 presents results from Fornell-Lacker’s criteria, which is a way to measure
the discriminant validity of the constructs. The table shows the correlation between the
latent variables (SP, EE, PE, SI, FC, and OPI), and the diagonal cells represent the
square root of the AVE. From the table, it is obvious that the square root of AVE of latent
variables SP, EE, PE, SI, FC, and OPI is greater than the correlation value between them.
So, Fornell-Lacker’s criteria are met for all the latent variables. This suggests that the
constructs have good discriminant validity, and the latent variables are distinct from each
other.
Structural Model
After ﬁtness of measurement model, path analysis was run to examine the impact of
independent variables on online shopping intention. The result of path analysis is present
in Table 7.
Table 7 presents the statistical results of the association between various independent
variables (SP, EE, PE, SI, and FC) and a dependent variable (OPI). Based on the table,
it appears that the relationships between the factors and OPI are as follows:
• The relationship between Security and Privacy (SP) and OPI is statistically signiﬁcant
(P-value < 0.01), with an estimate of 0.275, which suggests a positive relationship
between the two.
• The relationship between Effort Expectancy (EE) and OPI is not statistically signif-
icant (P-value > 0.05), with an estimate of 0.008, which suggests a weak and not
statistically signiﬁcant association between the two.
• The relationship between Performance Expectancy (PE) and OPI is not statistically
signiﬁcant (P-value > 0.05), with an estimate of 0.050, which suggests a weak and
not statistically signiﬁcant association between the two.
• The relationship between Social Inﬂuence (SI) and OPI is statistically signiﬁcant
(P-value < 0.01), with an estimate of 0.234, which suggests a positive relationship
between the two.

Factor Inﬂuencing Online Purchase Intention Among University
203
Table 3. Result of EFA
Factor
Items
Loading
% of Variance
Cronbach Alpha
Effort expectancy
EE1
0.657
12.730
0.830
EE2
0.761
EE3
0.710
EE4
0.749
EE5
0.715
Online purchase intention
OPI1
0.784
11.798
0.859
OPI2
0.816
OPI3
0.775
OPI4
0.768
Security and privacy
SP1
0.695
10.473
0.757
SP2
0.778
SP3
0.598
SP4
0.724
SP5
0.594
Performance expectancy
PE1
0.754
10.152
0.772
PE2
0.782
PE3
0.704
PE4
0.584
Social inﬂuence
SI2
0.704
8.235
0.646
SI4
0.718
SI5
0.544
SI6
0.642
Facilitating conditions
FC2
0.623
6.819
0.622
FC4
0.771
FC5
0.509
• The relationship between Facilitating Conditions (FC) and OPI is not statistically
signiﬁcant (P-value > 0.05), with an estimate of 0.274, which suggests a weak and
not statistically signiﬁcant relationship between the two.

204
D. Ranabhat et al.
Table 4. Measurement model
Indices
Criteria
Calculated value
Comments
Absolute ﬁt measures
CMIN/DF
< 3
2.146
Well ﬁtted
GFI
> 0.90
0.907
Well ﬁtted
RMSEA
< 0.08
0.055
Well ﬁtted
Incremental ﬁt measures
NFI
> 0.90
0.848
Moderately ﬁtted
CFI
> 0.90
0.911
Well ﬁtted
Table 5. Construct reliability and construct validity
Factors
Cronbach alpha
CR
AVE
Security and privacy (SP)
0.69
0.75
0.43
Effort expectancy (EE)
0.83
0.84
0.52
Performance expectancy (PE)
0.68
0.73
0.48
Facilitating condition (FC)
0.62
0.72
0.46
Social inﬂuence (SI)
0.65
0.75
0.43
Online purchase intention (OPI)
0.86
0.86
0.61
Table 6. Discriminant validity- Fornell-Lacker’s criteria
SP
EE
PE
SI
FC
OPI
SP
0.656
EE
0.408
0.719
PE
0.235
0.690
0.695
SI
0.355
0.365
0.374
0.656
FC
0.557
0.656
0.638
0.581
0.678
OPI
0.526
0.420
0.382
0.513
0.600
0.778

Factor Inﬂuencing Online Purchase Intention Among University
205
Table 7. Result of path analysis
Relationship
Estimate
S.E
C.R
P-value
Remarks
SP →OPI
0.275
0.117
3.194
0.001
Rejected
EE →OPI
0.008
0.122
0.077
0.938
Not rejected
PE →OPI
0.050
0.162
0.45
0.653
Not rejected
SI →OPI
0.234
0.097
2.762
0.006
Rejected
FC →OPI
0.274
0.201
1.854
0.064
Not rejected
5
Conclusion
This study was carried out to identify the variables inﬂuencing university students’
intentions to make online purchases. The factor analysis identiﬁed six factors: Effort
Expectancy (EE), Online Purchase Intention (OPI), Security and Privacy (SP), Perfor-
mance Expectancy (PE), Social Inﬂuence (SI), and Facilitating Conditions (FC) related
to online purchase. Based on the result of structural equation modelling it is found that
security and privacy, and social inﬂuence have a positive and statistically signiﬁcant
impact on online purchase intention while effort expectancy, performance expectancy,
and facilitating conditions have a weak and not statistically signiﬁcant relationship with
online purchase intention. This ﬁnding is consistent with [8], who discovered a positive
impact of social inﬂuence on online shopping intention, [9] who found positive impact of
security on online shopping intention and [10] who discovered signiﬁcant role of several
factors including social inﬂuence and security and privacy in driving online shopping.
This study concludes that online purchase intention among university students can be
increased by ensuring the security and privacy of the customers’ personal and ﬁnancial
information. Similarly, the recommendations and experienced shared by important peo-
ple like family and friends greatly inﬂuence an individual’s decision to purchase online.
This study suggests that merchants should ensure the security and privacy of customer
data in online shopping systems and leverage social inﬂuence in marketing strategies to
increase online purchase intention among university students in Nepal.
References
1. Aldhmour, F., Sarayrah, I.: An investigation of factors inﬂuencing consumers’ intention to
use online shopping: an empirical study in south of Jordan. J. Internet Bank. Commer. 21(2),
6393 (2016)
2. Tirtiroglu, E., Elbeck, M.: Qualifying purchase intentions using queueing theory. J. Appl.
Quant. Methods 3(2), 167 (2008)
3. Raza, M.A., Ahad, M.A., Shafqat, M.A., Aurangzaib, M., Rizwan, M.: The determinants of
purchase intention towards counterfeit mobile phones in Pakistan. J. Public Adm. Gov. 4(3),
1–19 (2014)
4. Venkatesh, V., Morris, M.G., Davis, G.B., Davis, F.D.: User acceptance of information
technology: toward a uniﬁed view. MIS Q. 14, 425–478 (2003)

206
D. Ranabhat et al.
5. San Martin, H., Herrero, A.: Inﬂuence of the user’s psychological factors on the online pur-
chase intention in rural tourism: integrating innovativeness to the UTAUT framework. Tour.
Manag. 33(2), 341–350 (2012). https://doi.org/10.1016/j.tourman.2011.04.003
6. Doana,T.-T.:Factorsaffectingonlinepurchaseintention:astudyofVietnamonlinecustomers.
Manag. Sci. Lett. 10(10), 2337–2342 (2020). https://doi.org/10.5267/j.msl.2020.2.031
7. Abd Murad, S.M., Aziz, N.A.: Examining factors inﬂuencing travellers purchase intentions
viaonlinetravelintermediarieswebsites:aconceptualmodel.Int.J.Econ.Res.14(2),289–304
(2017)
8. Hong,C.,Choi,E.-K.C.,Joung,H.-W.D.:Determinantsofcustomerpurchaseintentiontoward
online food delivery services: the moderating role of usage frequency. J. Hosp. Tour. Manag.
54, 76–87 (2023). https://doi.org/10.1016/j.jhtm.2022.12.005
9. Tran, V.D., Nguyen, T.D.: The impact of security, individuality, reputation, and consumer
attitudes on purchase intention of online shopping: the evidence in Vietnam. Cogent Psychol.
9(1), 5530 (2022). https://doi.org/10.1080/23311908.2022.2035530
10. Tandon, U., Kiran, R.: Study on drivers of online shopping and signiﬁcance of cash-on-
delivery mode of payment on behavioural intention. Int. J. Electron. Bus. 14(3), 212–237
(2018). https://doi.org/10.1504/IJEB.2018.095959
11. Hair, J.F., Anderson, R.E., Tatham, R.L., Black, W.C.: Multivariate data analysis 87(4) (2019)

Effect of Thin Polymer Interlayers
in the Spindle-Bearing Joint on the Stiffness
and Durability of Spindle Bearing Assemblies
of Mills
A. S. Kononenko1
, T. A. Kildeev1
, Ignatkin I. Yu2(B)
, and N. A. Sergeeva2
1 Bauman Moscow State Technical University, Moscow 105005, Russia
2 Russian State Agrarian University – Moscow Timiryazev Agricultural Academy,
Moscow 127550, Russia
ignatkin@rgau-msha.ru
Abstract. The aim of the work is to develop a scientiﬁcally justiﬁed method of
calculating technological parameters of the process of creating a polymer inter-
layer in the spindle-bearing joint to improve the service life of the spindle bearing
assembly of mills. Mathematical relationship between the stiffness of the spindle
bearing assembly and physical and mechanical properties and thickness of the
polymer interlayer in the spindle-bearing joint has shown that it is possible to
reduce the level of deformation by increasing the elastic modulus of polymer. To
investigate the effect of nanoﬁllers on the elastic modulus of polymer, a method
based on estimating the displacement of the moving part of a shaft-bearing sample
under uniformly increasing load was used. A method of comparative running-
in tests of spindle bearing assemblies with polymer nanocomposite interlayers
between the spindle and bearing inner rings has been developed. It is shown that
nanoﬁllers positively inﬂuence the elastic modulus of anaerobic polymer and the
use of nanocompositions in ﬁxing the spindle-bearing joint increases its durability
by more than 10%.
Keywords: Polymer interlayer · The spindle bearing assembly ·
Nanocomposition · Bearing · Beam on the elastic foundation · Stiffness ·
Durability
1
Introduction
The spindle bearing assembly is one of the most critical units of any machine tool
providing the main cutting motion, which is rotation of the ﬁxture with a workpiece
or with the tool. In operation, such components of the spindle bearing assembly as
bearings (75%) and spindle (9%) most frequently break out, the share of the ﬁxing
system breakdowns accounts for 6%, and that of all other elements—no more than 4%
of the total number of failures [1, 2].
A promising technology for improving the durability of the spindle bearing assembly
is the use of modiﬁed anaerobic polymer compositions. The essence of the method
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 207–216, 2024.
https://doi.org/10.1007/978-3-031-50158-6_21

208
A. S. Kononenko et al.
consists in creating a polymer interlayer between the spindle mounting surfaces and
the bearing inner rings. At the same time the modiﬁcation of polymer compositions
with nanosize metal oxide particles increases their elasticity modulus, accelerating the
polymerization process and providing the higher stiffness of the joint in comparison
with the initial composition. It is important to note that polymerization of anaerobic
composition occurs immediately after assembling.
For mill spindle bearings, an interference ﬁt of 0–5 μm should be ensured when
mounted on the spindle (depending on the technical requirements speciﬁed by the man-
ufacturer). Anaerobic nanoﬁll compounds are also suitable for the interference ﬁt. As a
result, the effect of fretting corrosion on the assembly durability is reduced and twisting
of the mating parts is prevented, thus avoiding premature failure of the spindle mounting
surface and the consequent costs for its restoring or producing a new spindle [3, 4].
The wear on the spindle mounting surfaces can be up to 15 μm. Depending on the
stiffness requirements, polymeric nanocomposites can be used to ﬁx a spindle bearing
assembly as an independent repair technology or in combination with electromechanical
machining, an additional repair part, chrome plating or surfacing. It is necessary to under-
stand that applying of polymeric materials having a negligibly small elasticity modulus to
the gap between bearing rings and spindle mounting surfaces in comparison with struc-
tural carbon steels, which are the basic material for spindle manufacturing [5, 6], will
inevitably reduce the stiffness of the spindle bearing assembly and increase deformations
under the inﬂuence of operating temperatures and loads, and, consequently, worsen the
machining accuracy and quality. A key decision factor in selecting polymer nanocom-
posites and determining mounting surface restoration requirements is the predicted stiff-
nessofthespindle-polymer-bearingjoint.Theoptimalsolutionistoselectrepairpolymer
compositions with the required physical-mechanical and elastic-deformation properties
[7–9].
2
Effect of the Polymer Interlayer on the Stiffness
of the Spindle-Bearing Joint
Let’s describe a mathematical model of interaction of a polymer interlayer with elements
of the spindle bearing assembly. The spindle is mounted in the inner rings of the bearing
supports with no gap. In turn, the outer rings of the bearings are rigidly ﬁxed in the
mounting bores of the stationary housing. A radial force is applied to the front end of
the spindle which gives rise to a radial reaction in the front bearing support. The radial
displacement of the spindle axis from the center of the housing bore is known and equals
0.
In the other case, the spindle is mounted in the inner rings of the bearing supports with
a gap which is ﬁlled with solid polymer—i.e. the thickness of the polymer interlayer is
equaltothegap.Forthefrontsupport,itsthicknessshouldbeconsideredthincomparedto
the spindle thickness and equal to h. The elasticity modulus of the polymer is considered
equal to Ep.
The radial misalignment  of the spindle axis in relation to the center of the housing
bore must be determined for the front bearing if there is a gap ﬁlled with a polymer
interlayer between the spindle and the bearing inner rings.

Effect of Thin Polymer Interlayers in the Spindle-Bearing
209
In solving the above mathematical model, the spindle is supposed to be mounted
in the bearings without misalignment and all the mating surfaces are considered to be
absolutely smooth, and the load is transmitted through the central rolling element. It is
supposed that the materials and geometric characteristics of the spindle and the bearing
are known [10].
For the case where the spindle is mounted in the bearing supports with a gap ﬁlled
with a polymer interlayer, we will assume that the total displacement in the bearing 
is the sum (1) of the deformation of the bearing 0 and the polymer interlayer y from
the applied load:
 = 0 + y,
(1)
The radial displacement ratio (2) can be expressed from the formula (1) by
mathematical transformations:

0
=  + y
0
= 1 + y
0
,
(2)
Let’s assume that the polymer interlayer is thick enough, i.e. the deﬂections that arise
are little compared to the thickness of the polymer interlayer, which operates within the
elasticity limits. The roughness of the mating surfaces is much lower than the thickness
of the polymer interlayer.
Let’s consider the plastic interlayer as an elastic foundation. Accordingly, let’s
consider the inner ring of the bearing as a beam resting on the elastic foundation.
The beam on the elastic foundation deforms under the action of a concentrated force.
The deformation of the beam at the central point is described by formula (3):
y0 =
−1
8EJβ3

P0 + 2P1e−βl1(sin βl1 + cos βl1) + 2P2e−2βl1(sin 2βl1 + cos 2βl1)

,
(3)
where E—elasticity modulus of the bearing material;
J—moment of inertia of the beam cross-section (outer ring of the bearing);
l1—distance between the balls;
P0, P1, P2—load of the central, ﬁrst and second ball, respectively;
β—coefﬁcient deﬁned by formula (4):
β =
4

k
4EJ ,
(4)
where k—modulus of subgrade reaction, k = kp·b;
kp—compliance ratio, N/mm3;
b—width of the inner ring of the bearing, mm;
E—elasticity modulus of bearing material, N/mm2;

210
A. S. Kononenko et al.
J—moment of inertia of the bearing ring cross-section, mm4.
The modulus of subgrade reaction kp is determined from Winkler formula (5) [11]:
kp = P
S ,
(5)
where P—pressure at the surface of the elastic foundation;
S—deﬂection (total settlement) of the compressible elastic foundation.
The deﬂection of polymer interlayer under pressure P can be determined from
formula (6):
S = P

1 + μp

1 −2μp

hp

1 −μp

Ep
,
(6)
where μp and Ep are the averaged Poisson’s ratio and the elasticity modulus of polymer
coating material within compressible thickness hp, respectively.
The distortion ratio of the polymer interlayer for different thicknesses and dif-
ferent elasticity moduli of compositions can be represented as (7) by mathematical
transformation of formulas (3), (4), (5) and (6):
 = 0 + P0
8EJ
4



	

4EJ

1 + μp

1 −2μp

hp
b

1 −μp

Ep
3
,
(7)
Figure 1 shows the response surface of the maximum deformation of the polymer
interlayer under 5 kN load as a function of the Ep and hp values.
Fig. 1. Response surfaces of maximum deformation value of polymer interlayer under 5 kN load
as a function of Ep and hp values.
The analysis of the presented response surfaces shows that increasing the elasticity
modulus of the polymer interlayer leads to a hyperbolic decrease in its deformation at
constant thickness and applied load. At the same time, the dependence of the interlayer
deformation on its thickness is expressed as a law, which is close to linear. This observa-
tion suggests that increasing the elasticity modulus of polymer compounds is the most
effective way of ensuring the stiffness of the spindle-polymer-bearing joint.

Effect of Thin Polymer Interlayers in the Spindle-Bearing
211
3
Increasing the Elasticity Modulus of Polymer Compositions
3.1
Materials and Methods
It should be noted that modifying original polymer compositions with nanosize particles
leads to a comprehensive improvement of their characteristics, including the elasticity
modulus.
Speciﬁc features of operation processes of the spindle bearing assembly require
careful selection of polymer compositions and ﬁllers [12, 13]. Taking into account that
the gap between bearing inner ring and worn spindle mounting surface can be up to
5…15 μm, preference should be given to anaerobic sealants designed for joints with
little gaps and even with slight interference.
The dispersion of nanosize metal particles in the volume of the anaerobic adhesive is
a catalyst for the polymerisation process. Assembling a bearing unit of a spindle head can
be a challenge because of the particular design features of the spindle bearing assembly,
resulting in extended service life of the ﬁnished repair compound.
Given that spindle bearing assemblies are subject to periodic maintenance that
involves disassembling, low-strength retainer compounds are the material of choice. Dis-
assembling of such joints can be carried out without heating or at minimum short-term
heating up to 200 °C.
Loctite 601 spindle-sleeve retainers manufactured by Henkel (England) [14] and
Unigerm-7 manufactured by AO NII POLYMEROV (Russia) [15] were selected for
further investigation. They polymerize in the absence of air oxygen in small gaps between
metal surfaces and ensure reliable ﬁxation of joints that operate under high loads and
vibrations.
In the case of interaction of the polymer matrix with nanosize particles (hereinafter—
NSPs), ﬁller particles may actively participate in cross-linking processes, forming addi-
tional units or centers of structure formation, around which oriented layers of the polymer
matrix with a dense packing of components comprising the nanocomposite are formed.
Such assemblies can withstand extreme operating conditions without fracture [16].
Elastic properties of polymer nanocomposites can be increased due to the fact that
at low volume content the NSPs move freely with the polymer matrix, which is freely
stretched. Aluminum oxide (Al2O3) and silicon oxide (SiO2) nanopowders were used
as ﬁllers in the polymer compositions.
The essence of the method of assessing the elasticity modulus of compounds is to
determine displacements in the boundaries of load variation in a sample under com-
pression, the scheme of which is given in Fig. 2. The material used to manufacture the
samples is steel 45. The roughness of the working surfaces is Ra = 3.2 μm.
The inner and outer parts of the samples were selected so that the test compound
had a thickness of 0.05 mm. After degreasing, a thin layer of sealant was applied to
the working surface of the encompassed cylindrical part, and then it was placed in the
second part of the sample.
The elasticity modulus was assessed under normal conditions on the INTRON 600
DX hydraulic bursting machine after full polymerization of the samples, at a loading
rate of 5 kN/min until the polymer layer was broken. The equipment is capable of

212
A. S. Kononenko et al.
Fig. 2. Schematic diagram of the sample: 1—pin; 2—polymer interlayer; 3—ring.
recording sample displacement from the applied loads, thus making it possible to assess
the elasticity modulus of the compounds.
The elasticity modulus for each sample was determined from formula (8):
Ei =
Fi
S · xi/l ,
(8)
where Fi—load change of the i-th sample;
Δxi—displacement of the i-th sample in the boundaries of the load change, mm;
S—area of the polymer interlayer;
l—original joint length.
The results were assessed according to [17, 18].
3.2
Results and Discussion
Table 1 shows the results of the studies of the effect of nanoﬁller concentration on the
elasticity modulus of nanocompositions based on sealants Unigerm-7 and Loctite-601.
Table 1. Test results.
Young’s modulus, MPa
Coctav
Composition with SiO2
Composition with Al2O3
Concentration, %
0
0.5
1
1.5
0.5
1
1.5
Unigerm-7
171.2
181.0
203.8
238.1
246.7
289.2
290.9
254.6
214.3
281.0
266.7
369.8
300.0
280.0
214.3
340.0
272.1
246.2
314.3
307.7
261.5
Loctite-601
217.6
248.2
257.2
323.2
249.3
274.9
257.1
216.8
242.9
274.8
240.0
273.9
275.0
200.0
223.5
271.4
242.8
200.0
342.9
280.0
314.3
The analysis of experimental data showed that mixing anaerobic sealant Unigerm-7
with SiO2 nanopowder at concentration of 1.0% increased the elasticity modulus of

Effect of Thin Polymer Interlayers in the Spindle-Bearing
213
the composition by 18.2% (from 213.5 to 252.3 MPa). Adding nanopowder Al2O3 at
concentration of 0.5% raised the elasticity modulus of composition by 45.3% (from
213.5 to 310.3 MPa). Modiﬁcation of a foreign compound Loctite-601 with SiO2 and
Al2O3 nanopowders at analogous concentrations enabled an increase of the elasticity
modulus of the composition by 17.8 (from 219.3 to 258.2 MPa) and 31.6% (from 219.3
to 288.7 MPa) respectively.
Thus, applying nanoparticles to the composition of anaerobic polymers enhances
the elastic properties of the latter.
The study of the extremes of the polynomial trend line equations constructed on the
basis of experimental data allowed for calculation of optimum proportions of mixtures.
For Loctite-601 the optimum concentrations of Al2O3 and SiO2 nanopowders were 0.41
and 1.26%, respectively. For Unigerm-7 the optimum concentrations of Al2O3 and SiO2
nanopowders were 0.49 and 1.38% respectively.
4
Study of the Durability of the Spindle-Polymer-Bearing Joint
4.1
Materials and Methods
The development of new technologies requires validation under near operational con-
ditions. Therefore, comparative running-in tests were carried out on spindle bearing
assemblies with and without polymer nanocomposite interlayers between the spindle
and bearing inner rings.
When determining the condition of the bearing supports, the following indicators
were taken into account: bearing noise, vibration, lubrication condition and operating
temperature, the uncontrolled growth of which at the constant running speed indicates
the beginning of the catastrophic wear of the assembly, i.e. the assembly has reached the
limit state.
The tests were performed on an experimental running-in bench, the general view of
which is shown in Fig. 3.
Fig. 3. General view of the bench for comparative running-in tests of bearing units: 1—fabricated
frame; 2—radial ball bearings; 3—tested assembly; 4—electric motor; 5—belt transmission; 6—
eccentric.
Prefabricated aluminum proﬁle frame 1 is mounted on the rubber isolation mounts
and consists of two parts, one of which is ﬁtted with an electric motor 4 and the other

214
A. S. Kononenko et al.
with the tested assembly. Transmission of the rotation from the electric motor shaft to
the tested assembly is effected by means of belt transmission 5. Tested assembly 3 is a
spindle made of steel 45, mounted on two self-centering self-aligning radial ball bearings
2. Eccentric 6 which can be ﬁtted with up to three loads of different masses is installed
at the front end of the spindle. The eccentric was selected with such parameters that the
permissible residual unbalance can be exceeded by at least 100 times. The permissible
residual unbalance was calculated according to formula (9) according to GOST ISO
1940-1-2007:
Uper = 1000 ·

eperω

m
ω
,
(9)
where Uper—value of the permissible residual unbalance, g·mm;
eperω—value of balancing grade, mm/s;
m—rotor mass, kg, kg;
ω—angular velocity, corresponding to the maximum running speed, rad/s.
Angular velocity, corresponding to a running speed of 3000 rpm, equals 314.2 rad/s.
The weight of the spindle with a length of 500 mm and a diameter of 20 mm is 1.23 kg.
The balancing grade is 2.5 mm/s according to GOST ISO 1940-1-2007. The permissible
residualunbalancevalueforthetestedassemblyis9.79g·mm.Itwassufﬁcienttoposition
a 25 g weight at an offset of 40 mm from the spindle axis to ensure the accelerated test
conditions.
The time was ﬁxed with a stopwatch. The total number of cycles N was calculated
as a product of the time T (min) and the running speed ω 3000 (rpm).
In operation, the bearing temperature to an accuracy of ± 0.5 °C was checked with
a CEM DT-8806H infra-red pyrometer, at least once every 0.5 h, and the pyrometer
readings were recorded in the test report, when the temperature rose steadily for 1.5
h, the test was interrupted. Noise level was checked by means of digital noise meter
MEGEON 92135 with 0.1 dB resolution and a measuring range of 30…130 dB.
4.2
Results and Discussion
For the ﬁrst assembly, the spindle was installed in the front support with an interference
of 0.01 mm. For the second assembly, the shaft was installed in the front bearing with
a 0.1 mm gap, ﬁlled with a polymer nanocomposition consisting of Loctite-601 and
nanosize silicon oxide particles at a concentration of 1.3%.
Abrupt changes in temperature during one hour of testing up to 6 °C as well as
a 20% increase in front bearing noise from the original level were recorded for the
ﬁrst assembly after 6.84·106 cycles and for the second assembly after 7.56·106 cycles.
Durability increased by 10%.
Thus, the results conﬁrmed the positive effect of polymer interlayers on the durability
of spindle bearing assemblies.
Important technological aspects when assembling the spindle-bearing joint with an
interference or with a wear gap of up to 10 μm include mixing a polymer nanocompo-
sition based on Loctite-601 or Unigerm-7 with applying of 1.3% by weight of silicon
oxide nanoparticles for 10 min by means of ultrasound.

Effect of Thin Polymer Interlayers in the Spindle-Bearing
215
5
Conclusion
1. The mathematical relationship has shown that increasing the elasticity modulus of the
polymer interlayer with unchanged thickness and applied load leads to a hyperbolic
decrease in its deformation.
2. Modiﬁcation of anaerobic polymer compositions Loktait-601 and Unigerm-7 with
nanosize aluminum oxide particles allows increasing their elasticity modulus up to
80%, and nanosize silicon oxide particles up to 63%.
3. The use of polymer interlayers in the spindle-bearing joint will increase the durability
of the spindle bearing assembly by more than 10%.
References
1. Shesterninov, A.V.: Designing Spindle Bearing Assemblies of Mills: Textbook, p. 96. UlGTU,
Ulyanovsk (2006)
2. Push, V.E.: Designing Mills, p. 391. M: Mashinostroenie (1997)
3. Latypov, R.A., Serov, A.V., Serov, N.V., Ignatkin, I.Y.: Utilization of the Wastes of Mechanical
Engineering and Metallurgy in the Process of Hardening and Restoration of Machine Parts.
Part 1. Metallurgist 65(5–6), 578–585 (2021). https://doi.org/10.1007/s11015-021-01193-y
4. Mikhalchenkov, A.M., Tyureva, A.A., Kozarez, I.V., Kononenko, A.S.: The effect of the
concentration of components and dispersion of particles of ﬁller of epoxy-sand composite
on hardness and its relationship with abrasive wear resistance. Polym. Sci. D 14(1), 17–20
(2021)
5. Efanov, S.A.: Provision of Parametric Reliability of Repair-Engineering Equipment by
Restoration of Spindle Bearing Assemblies by Polymeric Composite Materials: Phd (Eng)
Thesi, p. 131 (2015)
6. Mikhal’chenkov, A.M., Torikov, V.E., Filin, Y.I.: The inﬂuence of the concentration of com-
ponents of an epoxy–sandy composite on its abrasive-wear resistance. Polym. Sci. D 11,
47–49 (2018)
7. Kononenko, A.S., Khabbatullin, R.R.: Theoretical substantiation of the conditions for the
applicability of deformationless ﬁxation by means of a polymer glue for workpieces during
their mechanical processing on a milling machine with computer numerical control. Polym.
Sci. D 15(4), 523–528 (2022)
8. Panﬁlov, Y.V., Rodionov, I.A., Ryzhikov, I.A., Baburin, A.S., Moskalev, D.O., Lotkov, E.S.:
Ultrathin ﬁlm deposition for nanoelectronic device manufacturing/vacuum science and equip-
ment. In: Proceedings of the 26 Conference with International Participants IOP Conference
Series: Materials Science and Engineering, p. 781. IOP Publishing, New York (2020)
9. Kononenko, A.S., Ignatkin, I.Y., Drozdov, A.V.: Recovering a reducing-gear shaft neck by
reinforced-bush adhesion. Polym. Sci. D 15(2), 137–142 (2022)
10. Schaefﬂer Group Industrial. https://www.schaefﬂer.com/content.schaefﬂer.com/en/divisions/
industrial/industrial.jsp. Accessed 16 Dec 2022
11. Gorb, A.M.: Improvement of Analytical Methods of Calculation of Industrial Floor Structures
Made of Cement Concrete on the Elastic Foundation in Case of Using Local Resilience Model
[Text]: PhD (Eng) Thesis, p. 140 (2009)
12. Kononenko, A.S., Ignatkin, I., Drozdov, A.V.: Restoring the neck of a reducing-gear shaft by
gluing a reinforced bush. Polym. Sci. D 15(4), 529–534 (2022)

216
A. S. Kononenko et al.
13. Li, R.I., Psarev, D.N., Kiba, M.R.: Promising Nanocomposite Based on Elastomer F-40 for
Repairing Base Members of Machines. Polym. Sci. D Glues Seal. Mater. 12(2), 128–132
(2019)
14. Loctite company. https://loctayt.pyc/. Accessed 28 Dec 2022
15. AO NII POLYMEROV. http://www.nicp.ru/. Accessed 28 Dec 2022
16. Mikhalchenkov, A.M., Kravchenko, I.N., Filin, Y., Kozarez, I.V., Velichko, S.A., Erofeev,
M.N.: Abrasive wear mechanism of polymer composites with a dispersed ﬁller. Refract. Ind.
Ceram. 63, 174–177 (2022)
17. Tikhomirov, D., Kuzmichev, A., Rastimeshin, S., et al.: Energy-efﬁcient pasteurizer of liquid
products using IR and UV radiation. Adv. Intell. Syst. Comput. 866, 178–186 (2019). https://
doi.org/10.1007/978-3-030-00979-3_18
18. Dorokhov, A., Kirsanov, V., Pavkin, D., et al.: Recognition of cow teats using the 3D-ToF
camera when milking in the “herringbone” milking parlor. Adv. Intell. Syst. Comput. 1072,
128–137 (2020). https://doi.org/10.1007/978-3-030-33585-4_13

The Use of a Nutrient Solution Containing
Chelated Forms of Various Trace Elements
K. Pishchaeva1(B)
, S. Muradyan1
, E. Nikulina2
, S. Buleeva1
,
and A. Saproshina1
1 UNESCO Chair in Green Chemistry for Sustainable Development, Dmitry Mendeleev
University of Chemical Technology of Russia, Miusskaya sq. 9, 125047 Moscow, Russia
hurts.ivanova@yandex.ru, {pishchaeva.k.v,muradian.s.a,
buleeva.s.v,saproshina.a.a}@muctr.ru
2 National Research Center “Kurchatov Institute”, sq. Academician Kurchatova, 1, 123182
Moscow, Russia
Abstract. This study tested for the ﬁrst time the composition of a nutrient solu-
tion for hydroponic cultivation of daikon with chelated forms of 4 essential trace
elements (Fe, Zn, Cu, Mn) with carboxyl-containing ligands—ethylenediaminete-
traacetic acid (EDTA) and diethylenetriaminepentaacetic acid (DTPA). The results
of the study were compared with the common GHE nutrient solution, which is
widely used in hydroponics. The use of the proposed composition led to the great-
est growth and weight of plants. The value of the mass of plants grown in the
experimental nutrient solution was 30% more than with the GHE nutrient solu-
tion (General Hydroponics Europe, France), as well as the leaf area by 16%.
In all samples, the plants did not show chlorosis, there was no yellowing of the
leaves, and good turgor was observed. Thus, a micronutrient nutrient solution with
carboxyl-containing ligands (DTPA, EDTA) is promising for growing daikon in
hydroponic conditions and can be studied to optimize the cultivation of other
agricultural products.
Keywords: Hydroponic cultivation · Chelates · Daikon
1
Introduction
Global food security is one of the biggest challenges facing global agriculture [1].
According to demographers, by 2050 the world’s population will be 9.5 billion peo-
ple, of which more than 65% will live in urban areas [2]. To meet the projected increase
in the number of people, a signiﬁcant increase in crop yields is required. Given the
global trend in the reduction of arable land [3] and the complexity of modern natural
resource management, there is an urgent need to develop and further improve sustainable
cultivation systems. Growing food using hydroponic methods is one such system. The
beneﬁts of hydroponically grown foods are numerous. These include efﬁcient water use,
limited use of pesticides, higher yields, and year-round food production [4]. Hydroponic
systems provide the opportunity to grow agricultural products in non-arable regions of
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 217–222, 2024.
https://doi.org/10.1007/978-3-031-50158-6_22

218
K. Pishchaeva et al.
the world: arid regions or in urban areas, in the regions of the north and the Arctic Circle
[5]. This line of research is relevant, since agriculture in the northern boreal [6] and Arc-
tic regions of Russia is currently underdeveloped due to harsh climatic conditions, and
the use of traditional agricultural technologies is not enough to meet the food needs of
the local population. Traditional methods of infrastructure development and adaptation
of cropping systems are losing their relevance, as they can affect the transformation of
natural lands and carry risks for the environment of the Arctic zone. The widespread
introduction of soilless growing systems (vertical farms) [7] of agricultural products is
an important element in providing the population with food. Thus, studies on the cre-
ation of optimal nutrient compositions for growing test crops in hydroponic conditions
are of undoubted relevance. Daikon has numerous beneﬁcial properties and is therefore
very promising for growing in hydroponics. The fruits contain many vitamins, enzymes
and trace elements necessary for human health. In practice, daikon is often included
in recipes for the prevention and treatment of many diseases, general strengthening of
immunity [8].
For the vast majority of crops, including hydroponic daikon, there is no ideal growing
solution that allows plants to reach their maximum genetic and physiological potential.
New comprehensive studies are needed to study the exact requirements of plants for
the implementation of a complete metabolism. Low yields, lean plants, high water and
reagent costs, which are symptomatic of unsuccessful cultivation, are most often directly
related to incorrect formulations and improper management of the nutrient solution
[9]. At the same time, issues of sustainable agricultural intensiﬁcation apply equally to
hydroponic growing systems. Therefore, the issues of establishing the optimal amounts
of essential nutrients for speciﬁc crops, the search, and the study of formulas that can
additionally activate the internal reserves of plants continue to be relevant. This study
is devoted to approbation of the optimal composition for growing daikon in hydroponic
conditions using chelate forms of key trace elements (Fe, Zn, Cu and Mn) based on
carboxyl-containing ligands, analysis of the effect of the composition on the growth and
development of daikon.
2
Materials and Methods
To study the properties of the developed nutrient solution, the daikon Baseball F1,
Gavrish company, was chosen as a test culture. The hydroponic system consists of two
plastic containers with a volume of 5 L ﬁlled with a nutrient solution. The ﬁrst container
contained GHE nutrient solution (France), the second container contained a developed
analogue nutrient solution with trace elements, Cu, Zn, Mn.
Such trace elements Cu, Zn, Mn were added in a chelated form based on EDTA,
and Fe on the basis of DTPA. The nutrient solution contained the following concentra-
tions of macro- and micro-elements (%): (NO3)−—4.04; (NH4)+—1.71; K2O—3.98;
PO4—6.13; CaO—1.38; MgO—0.87; (SO4)2−—0.93; Fe—0.025; B—1.5; Cu—0.003;
Zn—0.004; Mn—0.01; Mo—0.18. It has been proposed to add a complex of Fe with
diethylenetriaminepentaacetic acid due to the instability of the complex of iron with
EDTA in alkaline solutions. Samples of nutrient solutions were prepared at the UNESCO
Chair in Green Chemistry for Sustainable Development, Dmitry Mendeleev University
of Chemical Technology of Russia.

The Use of a Nutrient Solution Containing Chelated Forms
219
Daikon seeds in the amount of 120 pieces in each container were placed on a gauze
napkin, which was irrigated with a nutrient solution. The containers were then placed in
a climate chamber with optimal humidity and temperature control, equipped with LED
lighting (Fig. 1).
Fig. 1. Daikon seeds in a growing container.
The nutrient solution was constantly aerated to maintain an oxygen concentration
above 15 mg/L. The pH values were up to 5.5–6.5. The air temperature in the chamber
was 24–26 °C, the humidity was maintained at 60%. The decrease in the water level in
the containers due to evaporation was compensated by adding water to the maximum
level. All experiments were prepared using triplicate. Sampling for measuring biometric
parameters was carried out on the 30th day after germination. To determine the main
growth parameters, 20 plants were selected from each container. To determine the dry
weight, the samples were crushed and dried in an oven at a temperature of 60–70 °C for
3 h to a constant weight.
Mathematical processing of the obtained data was carried out using the methods of
variation statistics in the form of calculations of the arithmetic mean, standard deviation,
coefﬁcient of variation, and Fisher’s test using Microsoft Excel.
3
Results and Discussion
During testing, changes in biometric and morphometric parameters of daikon growth
and development were recorded on the 30th day (Table 1).
The results showed that the selected optimal concentrations and ratios of macro-
and micro-elements, as well as the use of Cu, Zn, Mn based on EDTA and Fe based on
DTPA in the form of chelates, made it possible to achieve a signiﬁcant positive effect—an
increase in the growth and biomass of daikon compared to the GHE solution.

220
K. Pishchaeva et al.
Table 1. Parameters of the grown daikon
Nutrient
solution
Plant height
(cm)
Fresh leaf
mass (g)
Dry weight of
plants (g)
Dry weight of
roots (g)
Leaf surface
area of plants
(cm2)
GHE
14.2 ± 1.1
14.9 ± 0.015
0.67 ± 0.012
0.06 ± 0.01
1.1 ± 0.05
Experimental
solution
14.7 ± 1.5
21.2 ± 0.02
0.96 ± 0.018
0.17 ± 0.008
1.3 ± 0.08
The mass of plants in the container with the experimental nutrient solution exceeded
the mass of plants with the GHE solution by 30%. The leaf surface area of daikon in the
experimental solution is 16% larger compared to the GHE nutrient solution. Plant height
was almost the same in containers with GHE solution and developed nutrient solution.
In general, the plants in all containers had a shiny leaf plate without signs of chlorosis,
yellowness(Fig.2).However,itisworthnotingthatthedaikongrownontheexperimental
nutrient solution had a large developed root system (root mass was 65% higher compared
to plants grown on the GHE nutrient solution).
Fig. 2. Growing daikon in containers containing nutrient solutions.
The replacement of trace elements with chelated forms with a phosphorous-
containing ligand led to a positive side technological effect—the absence of an increase in
bacterial ﬁlms and mucus on surfaces in contact with the solution, which is an undoubted
advantage in exploitative terms. Together with an increase in the resistance of cultivated
crops to water and nutrient stress, the absence or very slow growth of bacterial ﬁlms
signiﬁcantly expand the possibilities for optimizing and intensifying hydroponic crop
production.

The Use of a Nutrient Solution Containing Chelated Forms
221
4
Conclusion
Thus, experimental tests have shown that a nutrient solution containing trace elements
(Fe, Zn, Cu, Mn) with carboxyl-containing ligands (EDTA, DTPA) has signiﬁcant poten-
tial for practical use in hydroponic systems. The mass of daikon grown in the experi-
mental nutrient solution increased by 30% more compared to the GHE nutrient solution,
leaf area by 16%. The developed nutrient solution contains micronutrients in digestible
forms, which is important for the growth of the daikon. Nutrient deﬁciency reduces
photosynthesis, causes growth inhibition, affects leaf area, and accelerates aging [11,
12]. Growing agricultural [13] products using hydroponics can contribute to the active
development of the infrastructure of the Arctic regions of Russia and their development.
Comparedtotheoccurrenceofgrowingcases,hydroponicmethodscoverdifferentplaces
and territories. The introduction of industrial systems of hydroponic cultivation in the
Arctic zone will quickly provide the population with high-quality, fresh and inexpensive
food all year round. At the same time, the developed optimal compositions of nutrient
solutions are important for the economic efﬁciency of production.
At the moment, research is expanding and experiments are being carried out with
the cultivation of basil. Based on the proposed approach using chelate forms of these
trace elements, it is planned to develop optimal nutritional compositions for growing
tomatoes, strawberries, various types of greens and other economically important crops
in vertical farms.
Acknowledgments. The reported study was funded by MUCTR within the framework of the
internal initiative grant No VIG-2022-037.
References
1. Rosegrant, M.W., Cline, S.A.: Global food security: challenges and policies. Science 302,
1917–1919 (2003)
2. Department of Economic and Social Affairs. https://www.un.org/sw/desa/68-world-popula
tion-projected-live-urban-areas-2050-says-un
3. Olsson, L., et al.: Land degradation. In: Shukla, P.R., et al. (eds.) Climate Change and Land:
An IPCC Special Report on Climate Change, Desertiﬁcation, Land Degradation, Sustainable
Land Management, Food Security, and Greenhouse Gas Fluxes in Terrestrial Ecosystems.
IPCC, Geneva (2019)
4. Radhakrishnan, G., Upadhyay, T.K., Singh, P., Sharma, S.K.: Impact of hydroponics: present
and future perspective for farmer’s welfare. Suresh Gyan Vihar Univ. Int. J. Environ. Sci.
Technol. 5(2), 19–26 (2019)
5. Interreg
Europe.
https://projects2014-2020.interregeurope.eu/cityzen/news/news-article/
11981/hydroponics-and-its-role-in-urban-agriculture/
6. Altdorff, D., et al.: Agriculture in boreal and Arctic regionsrequires an integrated global
approach for research and policy. Agron. Sustain. Dev. 41, 23 (2021)
7. Van Gerrewey, T., Boon, N., Geelen, D.: Vertical farming: the only way is up? Agronomy
12(1), 2 (2022)
8. Sela Saldinger, S., Rodov, V., Kenigsbuch, D., Bar-Tal, A.: Hydroponic agriculture and
microbial safety of vegetables: promises, challenges, and solutions. Horticulturae 9, 51 (2023)

222
K. Pishchaeva et al.
9. Velazquez-Gonzalez, R.S., Garcia-Garcia, A.L., Ventura-Zapata, E., Barceinas-Sanchez,
J.D.O., Sosa-Savedra, J.C.: A review on hydroponics and the technologies associated for
medium- and small-scale operations. Agriculture 12, 646 (2022)
10. Sakamoto, M., Komatsu, Y., Suzuki, T.: Nutrient deﬁciency affects the growth and nitrate
concentration of hydroponic radish. Horticulturae 7, 525 (2021)
11. Mu, X., Chen, Q., Chen, F., Yuan, L., Mi, G.: Within-leaf nitrogen allocation in adaptation to
low nitrogen supply in maize during grain-ﬁlling stage. Front. Plant 7, 699 (2016)
12. Mu, X., Chen, Y.: The physiological response of photosynthesis to nitrogen deﬁciency. Plant
Physiol. Biochem. 158, 76–82 (2021)
13. Hossain,K.A.,etal.:Paddy:diseasepredictionusingconvolutionalneuralnetwork.In:Vasant,
P., Zelinka, I., Weber, G.W. (eds.) Intelligent Computing and Optimization. ICO 2021. Lecture
Notes in Networks and Systems, vol. 371. Springer, Cham (2022). https://doi.org/10.1007/
978-3-030-93247-3_27

Designing an Inventory Control System in Food
and Beverage Industry
Tiovitus Flomando Tunga1 and Tanti Octavia1,2(B)
1 Industrial Engineering Department, Petra Christian University, Surabaya, Indonesia
tanti@petra.ac.id
2 Engineer Profession Education Department, Petra Christian University, Surabaya, Indonesia
Abstract. X Corp. is a company engaged in the food and beverage industry. With
the growth of the F&B industry, companies need to implement good inventory
management as part of their efforts to win the supply chain competition. Good
inventory management is achieved when the inventory costs incurred are at a min-
imum. A model or ordering system policy comparison needs to be done in an
effort to achieve optimal inventory management. The continuous review method
can minimize inventory costs compared to the periodic review method and the
current inventory control at the company with the lowest holding and ordering
costs. In 2021, this method can save up to IDR. 100,000,000 for the RM-SAC-
0042 type. In carrying out warehousing activities in new warehouses, companies
need to implement computer systems to increase the performance of warehousing
activities. A database system for stock management in the warehouse will accom-
modate the company in carrying out warehouse activities and make them more
effective. The most optimal inventory management method, which is the continu-
ous review method, is then implemented in the raw material warehouse database
system of X Corp.
Keywords: Supply chain · Inventory management · Warehouse database · Food
and beverage
1
Introduction
One of the most difﬁcult issues in business, particularly in the food and beverage sector,
is maintaining inventory of ﬁnished items and raw materials. X Corp., a company in
the food and beverage (F&B) industry, uses a make-to-order strategy to meet customer
demand. Currently, the company is having trouble keeping up with customer demand
while managing its raw material stocks. Galea-Pace [1] stated that all parts of the supply
chain converge in inventory, which is regarded as a basic component of supply chain
management or supply chain.
Many research papers have been published in the inventory management area.
Kandananond [2] examined various forecasting techniques for inventory requests and
assessed each one’s effectiveness using actual data. Stock et al. stated traditional logis-
tics decisions have received a lot of attention from researchers in the ﬁeld of logistics,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 223–235, 2024.
https://doi.org/10.1007/978-3-031-50158-6_23

224
T. F. Tunga and T. Octavia
but more lately, they have turned their attention to collaborative models for inventory
management [3]. While Wang [4] created a review of the several inventory management
strategies based on supply chain management (SCM), and organized your papers using
the one-, two-, and multi-echelon strategies. To establish the appropriate cycle duration
and the percentage of no shortages, Wu et al. [5] consider the two important and vital
factors described above in order to maximize the overall proﬁt. Ben-Ammar et al. [6]
strive to identify the appropriate order release dates for components at the ﬁnal level of
the BOM in order to decrease the sum of the average backlog and inventory holding costs
for the ﬁnished product as well as the average inventory holding costs for components.
Senthilnathan [7] implement branch and bound in the four-layer production inventory
model.
In order to help ﬁrms, maximize their inventory management efforts, this study sug-
gests using ABC analysis to categorize inventory items based on their relative value.
Inventory control or inventory management of raw materials is very much needed by
the company to handle problems like this. Inventory is considered a core component
of supply chain management and is a place where all areas of the supply chain come
together [1]. In an area of intense industrial competition, inventory managers are encour-
aged to reduce inventory costs, improve inventory ﬂow in the supply chain, and meet
customer demands on time. Therefore, the implementation of effective inventory con-
trol is needed by X Corp to continue to meet customer demands with minimal inventory
costs. In inventory control, the company also faces other challenges, such as the lack
of a database that can support the implementation of inventory management. Inventory
control performance can also be boosted by a database system that can speed up the ﬂow
of data information in the warehouse, such as stock, data for taking goods, and placing
goods, and reduce information errors. In this study, analysis and proposals will be made
regarding the design of inventory control for X Corp.
2
Methods
Before creating the warehouse database, this research compared three different inventory
control strategies. These techniques include continuous review, periodic review, and the
company’s current approach to inventory control in 2021.
2.1
Continuous Review
Step 1: Economic Order Quantity
How many products and when to make a purchase are two issues that frequently arise
while acquiring policies or purchasing goods. Finding the ideal order quantity with the
lowest ordering expenses and storage costs is the topic of the modeling approach known
as Economic Order Quantity [8]. The following is the formula for calculating the EOQ
value:
Q∗=

2DS
H
(1)

Designing an Inventory Control System in Food and Beverage
225
where:
Q* = Economic Order Quantity
S = Ordering cost
H = holding cost
D = Need for quantity of goods
Step 2: Safety stock
A safety stock is an inventory that is kept to guard against ﬂuctuating consumer demand.
The corporation maintains safety stock to meet with demand that is unexpected and
under/over forecasts [9]. This may depend on the wait period’s duration, desired service
quality, frequency of reorders, and ﬂuctuation of demand during the lead time. With the
formula below, safety stock may be calculated.
SS = Z × Sd
(2)
where:
SS = safety stock
Z = Service factor
Sd = Standard deviation during lead time
Step 3: Reorder Point
Reorder or reorder point (ROP) is established based on a number of factors, including
average need, material utilization, order grace period, and lead time [10]. The reorder
point must also consider additional criteria, one of which is safety stock. Calculations
utilizing the following formula can be used to determine whether a reorder is necessary
or to determine a reorder point.
ROP = (d × LT) + SS
(3)
where:
ROP = Reorder point
D = Average demand per day
LT = Lead time
SS = Safety stock.
2.2
Periodic Review
Companies can discover inventory movements by using periodic review policies or
procedures. In this instance, the corporation can determine slow-moving commodities
by examining the amount of inventory it has at each predetermined time interval. So,

226
T. F. Tunga and T. Octavia
businesses can lower their inventory levels [11]. These are the steps in the periodic review
approach calculation:
Step 1: Calculating Safety Stock
The calculation of the periodic review begins with calculating the safety stock and then
proceeds to the calculation of the maximum inventory. Safety stock in the periodic review
method can be calculated using the following formula:
SS = Z × SdR + LT
(4)
where:
SS = safety stock
Z = Service factor
Sd R + LT = Standard deviation during lead time plus time review
Step 2: Calculating Maximum Inventory.
The safety stock value obtained is then used in calculating the maximum inventory
or maximum inventory. Maximum inventory is the maximum amount or quantity of
inventory the inventory level is below the maximum inventory, then an order must be
placed to meet the maximum inventory.
Maximum inventory calculation can be done using the following formula.
T = d × (R + LT) + SS
(5)
where:
T = Maximum inventory
D = Average demand per day
R = Time review
LT = Lead time
SS = Safety stock.
3
Research Methodology
Problem identiﬁcation is the initial step in this research project. The company’s inventory
control issues are discussed verbally with X Corp. to identify the concerns. Based on
the interview, it was determined that the organization has issues managing raw material
inventories because of changes and ambiguity in client demand. Currently, the business
is preparing inventory control for three new warehouses. The types of raw materials
investigated include those from the goods with the highest degree of demand, namely
Tapioca Pearl products, the primary raw materials of which are RM-SAC-0042, RM-
CGC-0013, RM-GRI200-0011, and RM-TRB-001. The data is obtained by requesting
a monthly recap of raw material usage data for 2019 to 2021.

Designing an Inventory Control System in Food and Beverage
227
The raw material usage data obtained from the company will be calculated using 3
methods: continuous review, periodic review, and current inventory control of the com-
pany. Data processing is done with Microsoft Excel software. The results of the analysis
and comparison will be implemented into the inventory management database to be
designed. Inventory management system design is proposed after data processing, in
which two methods of raw material inventory control are carried out and analyzed. The
raw material inventory control method that produces the lowest cost will be simulated in
the inventory management database. The inventory management system is designed with
Visual Basic for Applications in Microsoft Excel software. The design of the inventory
management system is carried out in the raw material warehouse, ﬁnished goods ware-
house, and oatmeal raw material warehouse. After the inventory management database
design is complete, the analysis and conclusions are drawn regarding the overall research
and become suggestions for future research for the company.
4
Results and Discussions
4.1
Ordering and Holding Cost Calculation
One of the main variables in calculating the inventory control method is ordering cost
(order cost) and holding cost (storage cost). The ordering cost is the cost incurred by
the company to place an order for raw materials. The average duration of ordering raw
materials by telephone is 5 min, which costs IDR 6360. The company also assigned one
employee to make purchase orders for about 30 min. Thus, the total time required to
place an order is 35 min. The following is the calculation of the cost of ordering raw
materials.
Calling cost = IDR 636 per 30 s
= IDR 6360 for 5 min
Salary of 1 employee = IDR4, 262, 015/month
= IDR3405 per 35 min
Ordering Cost = IDR 6360 + 3405
= IDR 9766
There are several regulations from suppliers regarding the minimum purchase of raw
materials.IntheRM-SAC-0042andRM-TRB-001rawmaterials,theminimumpurchase
of raw materials is a multiple of 20 tons, or 20,000. For the raw materials RM-CGC-0013
and RM-GRI200-0011, ordering raw materials can only be done in multiples of 25 kg.
Holding costs are costs incurred by the company to store goods. The calculation of the
company’s storage costs is carried out by considering factors such as electricity costs
incurred per month, the total cost of all warehouse employees’ salaries, and warehouse
capacity for all raw materials. The company spent IDR 1,000,000 for electricity costs per
20 days. Thus, the company issued IDR 1,500,000 for 1 month’s electricity costs. The
company has 9 warehouse employees with a UMR Tangerang salary of IDR 4,262,015.
The amount of funds issued by the company per month for the salaries of nine warehouse
employees is IDR 38,358.135. The raw material warehouse can accommodate a total of

228
T. F. Tunga and T. Octavia
about 210 tons. The following is the calculation of the company’s warehouse storage
costs.
Holding Cost = 1, 500, 000 + 38, 358, 135
210, 000
= IDR190 per kg per month
Thus, the company’s holding cost is IDR 190 per kg per month and a one-time order
fee of IDR 9766 for 5 min of call duration. The results of this cost calculation will
be used in calculating inventory control methods such as continuous review, periodic
review, and current inventory control.
4.2
Continuous Review Calculation
These are detail calculations of continuous review method for RM-SAC-0042:
1. Economic Order Quantity
The ﬁrst stage in the analysis of inventory control using the continuous review method is
to calculate the economic order quantity. The following is the calculation of the economic
order quantity for the raw material RM-SAC-0042.
Calling cost = IDR 9.766
Holding cost = IDR 190 per kg per month
Q∗=

2 × 11, 150 × 9766
190
= 1.072 kg
The average demand or use of RM-SAC-0042 raw materials per month from January
to December 2021 is 11,150 kg. Through the calculation of the economic order quantity
formula, the ideal number of orders for the RM-SAC-0042 item for one order is 1072 kg,
but regarding the regulation from the supplier, the minimum order quantity that can be
placed is 20,000 kg.
2. Safety Stock
The calculation of safety stock is the second calculation stage before proceeding to the
calculation of the reorder point or total inventory cost. The following is the calculation
of safety stock.
Usage variance per month = 32.424687
Variance lead time(30 days) = 38, 909, 624
Standard deviation(lead time) = 5.694
Service level = 90%
Z(service factor) = 1.28
SS = Z × Sd
= 1.28 × 5.694 = 7297 kg

Designing an Inventory Control System in Food and Beverage
229
The company has a 30-day lead time from ordering raw materials until the order
arrives from the supplier. The service level value is obtained from information about the
percentage of the company’s probability of fulﬁlling all customer orders. Through the
calculation of safety stock, the minimum stock or inventory that must be owned by the
company is 7994 kg.
3. Reorder Points
The reorder point is calculated based on the safety stock value obtained and the lead
time required by the company to receive inventory from the customer. The following is
the calculation of the reorder point.
Average usage = 446(in days)
Lead time = 30 days
ROP = (d × LT) + SS
ROP = (446 × 30 days) + 7994 kg
= 21.374 kg
Through calculations, the reorder point value for the RM-SAC-0042 item was
obtained at 21.374 kg.
4.3
Periodic Review Calculation
These are detail calculations of periodic review method for RM-SAC-0042.
1. Safety Stock
In the periodic review method, the calculation of safety stock is the ﬁrst calculation stage
before proceeding to the calculation of maximum inventory or total inventory cost. The
following is the calculation of the safety stock with the periodic review method for the
raw material item RM-SAC-0042.
Time Review(R) = 15 days
Lead Time(LT) = 30 days
(R + LT) = 45 days
Variance(45 day) = 58, 364, 436
St. dev(45 days) = 7640
Service level = 90%
Z(service factor) = 1.28
Safety Stock = 1.28 × 7.640 = 9790 kg
The time review shows the time interval in which the company must reorder, which is
every 15 days. The service level value is obtained from information about the percentage
of the company’s probability of fulﬁlling all customer orders. Through the calculation of
safety stock, the minimum stock, or inventory item RM-SAC-0042, that must be owned
by the company for the periodic review method is 9790 kg.

230
T. F. Tunga and T. Octavia
2. Maximum Inventory
The results of the safety stock calculation are carried over to the maximum inventory
calculation. The following is the maximum inventory calculation for RM-SAC-0042 raw
materials.
Average usage = 446 kg(in days)
T = d × (R + LT) + SS
= 446 kg × (15 days + 30 days) + 9790 kg
= 2, 986, 038 kg
Through calculations, the maximum inventory value for the item RM-SAC-0042 is
29,860 kg. The number of orders that must be made by the company follows the available
stock at that time, namely Maximum Inventory (T)—Stock on Hand.
4.4
Total Inventory Cost Comparison
The results of the calculations of the two previous methods are then simulated into raw
material usage data in 2021. This aims to calculate the total inventory costs from the
implementation of each method (including the current inventory control) and compare
which ones are the most effective and economical.
Fig. 1. Total inventory cost comparison between each method
Based on Fig. 1, it can be seen that the continuous review method produces the
minimum inventory costs for most of the raw materials, including RM-SAC-0042, RM-
CGC-0013, and RM-GRI200-0011. Periodic review produces higher inventory levels
than continuous review. This could be due to the raw materials ordered at the end of the
year under the continuous review method arriving in 2022, so there will be a signiﬁ-
cant difference in inventory levels in 2021. For raw materials RM-SAC-0042, inventory
control using the continuous method can save up to IDR 110,000,000 of the inventory
control carried out by the company in 2021. Orders were only made three times, while
in the company’s inventory control in 2021, orders were made up to ﬁve times. In this
case, the cost of the message becomes more efﬁcient at around IDR 20,000.

Designing an Inventory Control System in Food and Beverage
231
There are substantial savings in storage costs. The company stores 39,432,807.46 kg
of raw materials, while with the continuous review method, the company only needs to
store 22,017,387.03 kg. This difference of 17,415,420.43 kg can save storage costs of
IDR 110,297,663. In the application of the continuous review method, orders for raw
materials for RM-SAC-0042 are made up of 20 tons, or 20,000 kg, and each inventory
ﬁgure is 21.374 kg or less. Reviews and periodic reviews, there is a backorder cost,
namely the inability to meet the needs of the use of raw materials. This happens because
the use of raw materials continues, but orders for raw materials have not been received
from the supplier.
Fig. 2. Main menu interface
Although the backorder cost for the continuous review method is higher, the total
inventory storage with the periodic review method is much larger, so the total holding
cost and the total inventory cost for the continuous review method are still the least
expensive. This can also be an evaluation for the company, seeing that the total costs
incurred with inventory control carried out in 2021 are the highest for the raw materials
RM-SAC-0042, RM-CGC-0013, and RM-GRI200-0011. Inventory control using the
continuous review method is more efﬁcient on ordering costs and storage costs than the
company’s current inventory control, with fewer orders and much less storage than the
company will do in 2021. The continuous review method on raw materials RM-CGC-
0013 and RM-GRI200-0011 can save inventory costs up to IDR 180,000,000 of the
total inventory costs arising from inventory control carried out by the company during
2021. Inventory control carried out by the company in 2021 can be considered good
for RM-TRB-001 raw materials because it incurs the smallest inventory costs compared
to the two simulated methods, which is IDR 33,164,505. The periodic review method
of inventory control is not suitable for RM-TRB-001 raw materials because it causes a
lot of storage even though the number of orders is small. The inventory control method
that produces the lowest cost for each type of raw material is then implemented into the
proposed database design for the raw material warehouse at the Teluk Naga plant.

232
T. F. Tunga and T. Octavia
A database is designed to accommodate warehouse activities such as item picking,
putting away, stock updating, and item tracing. There are 4 important menus in the
database, named main menu, item picking, item receiving, and item stock.
1. Main Menu
The main menu is the ﬁrst menu that will appear when the user opens the database. This
menu connects with three other important menus that were mentioned before. There is
also a visual layout to give the user more visual information about racks and the items
stored in those racks. The interface is shown in Fig. 2.
2. Item Picking
Item-picking menus are designed to accommodate item-picking activities in warehouses.
The item-picking interface is shown in Fig. 3.
Fig. 3. Item picking menu interface
All data input in this menu will be exported as a picking list. A picking list is an
order document for picking up goods that contains data on the slot code and the quantity
that has to be taken. Based on discussions with the head of the warehouse department,
it was decided that the warehouse staff did not need to know the type of item taken. The
picking list is shown in Fig. 4.
3. Item Receiving
The item receiving menu is intended to support item storing or putting activities. The
“receiving” term is better known by warehouse staff and admin. The item receiving
interface is shown in Fig. 5.
4. Item Stock

Designing an Inventory Control System in Food and Beverage
233
Fig. 4. Picking list
Fig. 5. Item receiving interface
Item stock is a menu designed to see the availability and quantity of items in the whole
warehouse. This menu is also the implementation of the continuous review method.
Every item that has reached the reorder level will display an alert on the menu. The item
stock interface is shown in Fig. 6.

234
T. F. Tunga and T. Octavia
Fig. 6. Item stock interface
5
Conclusion
X Corp. is a company engaged in the ﬁeld of food and beverage. In a ﬁercely competitive
industry, companies need to implement effective inventory management or control to
ensure supply chain management performance remains optimal and can meet customer
satisfaction. Good inventory control is achieved when the total inventory costs incurred
are low. Based on the calculation of the reorder point model, periodic reviews, and
current inventory control, each type of raw material does not cause costs that are always
low in one particular method. For raw materials RM-SAC-0042, RM-CGC-0013, and
RM-GRI200-0011, the method that produces the least inventory costs is continuous
review. In the RM-TRB-001 raw material, the method that produces the least cost is the
current inventory control of raw materials. In carrying out warehousing activities at the
new warehouse of the Teluk Naga plant, companies need to implement computerization
or digitization to improve the performance of warehousing activities. The design of a
database system for stock management in the warehouse will accommodate companies
in carrying out warehouse activities such as item picking, putting-away, and inventory
recording so that they become more effective, precise, and well-organized. The results of
the calculation of the most optimal raw material inventory control are then implemented
in the raw material warehouse database of X Corps Teluk Naga plant. Thus, the company
has a reference regarding when to place an order, what quantity of goods to order, and
which shelf locations the goods can be placed on.

Designing an Inventory Control System in Food and Beverage
235
References
Galea-Pace, S.: Why is inventory management in the supply chain important? (2020). https://
supplychaindigital.com/digital-supply-chain/why-inventory-management-supply-chain-imp
ortant. Accessed 22 Dec 2022
Kandananond, K.: A comparison of various forecasting methods for autocorrelated time series.
Int. J. Eng. Bus. Manag. 6, 18–23 (2012). https://doi.org/10.5772/51088
Williams, B.D., Tokar, T.: A review of inventory management research in major logistics journals.
Int. J. Logist. Manag. 19(2), 212–232 (2008). https://doi.org/10.1108/09574090810895960
Wang, K.: The research of inventory management modes based on supply chain management.
In: International Asia Conference on Industrial Engineering and Management Innovation
Proceedings, pp. 1319–1329 (2013). https://doi.org/10.1007/978-3-642-38445-5_137
Wu, J., Teng, J.-T., Chan, Y.L.: Inventory policies for perishable products with expiration dates and
advance-cash-credit payment schemes. Int. J. Syst. Sci. Oper. Logist. 5(4), 310–326 (2017).
https://doi.org/10.1080/23302674.2017.1308038
Ben-Ammar, O., Dolgui, A., Hnaien, F., Louly, M.: Supply planning and inventory control under
lead time uncertainty: a review. IFAC Proc. 46(9), 359–370 (2013). https://doi.org/10.3182/
20130619-3-RU-3018.00592
Senthilnathan, S.: Economic order quantity (EOQ). SSRN Electr. J. (2019). https://doi.org/10.
2139/ssrn.3475239
Panja, S., Mondal, S.K.: Analytics of an imperfect four-layer production inventory model under
two-level credit period using branch-and-bound technique. J. Oper. Res. Soc. China 10, 725–
748 (2022). https://doi.org/10.1007/s40305-020-00300-1
Covert, D.P., Millan, J.A.O., Efendigil, T.: Dynamic customer service levels: evolving safety stock
requirements for changing business needs. Enter. Bus. Manag. 15, 27–66 (2020). https://doi.
org/10.5771/9783828872301-27
Freeland, J.R., Landel, R.: Managing inventories—reorder point systems. SSRN Electr. J. (2006).
https://doi.org/10.2139/ssrn.911426
Etone, D.: The Establishment and Operation of the Universal Periodic Review, 1st edn. The Human
Rights Council (2020)

Evaluating Research Impact: A Comprehensive
Overview of Metrics and Online Databases
Seema Ukidve1, Ramsagar Yadav2(B), Mukhdeep Singh Manshahia2,
and Jasleen Randhawa3
1 Department of Mathematics, L. S. Raheja College of Arts and Commerce, Santacruz(W),
Maharashtra, India
2 Department of Mathematics, Punjabi University Patiala, Patiala, Punjab, India
ramsagar.yadav@lsraheja.org
3 Panjab University Chandigarh, Chandigarh, India
Abstract. The purpose of this research paper is to analyze and compare the vari-
ous research metrics and online databases used to evaluate the impact and quality
of scientiﬁc publications. The study focuses on the most widely used research
metrics, such as the h-index, the Impact Factor (IF), and the number of citations.
Additionally, the paper explores various online databases, such as Web of Science,
Scopus, and Google Scholar, that are utilized to access and analyze research met-
rics. The study found that the h-index and IF are the most commonly used metrics
for evaluating the impact of a publication. However, it was also found that these
metrics have limitations and cannot be used as the sole criteria for evaluating the
quality of research. The study also highlights the need for a comprehensive and
holistic approach to research evaluation that takes into account multiple factors
such as collaboration, interdisciplinary work, and societal impact. The analysis of
online databases showed that while Web of Science and Scopus are considered to
be the most reliable sources of research metrics, they may not cover all relevant
publications, particularly those in less well-established or interdisciplinary ﬁelds.
Google Scholar, on the other hand, is more inclusive but may not have the same
level of accuracy and reliability as the other databases.
Keywords: Research metrics · Online databases · H-index · Impact factor ·
Citations · Web of science · Scopus · Google scholar · Research evaluation
1
Introduction
Evaluating the impact of scientiﬁc research is crucial for researchers, institutions, and
funding agencies to make informed decisions about the allocation of resources and the
recognition of scientiﬁc achievements. In recent years, there has been a growing interest
in developing methods and tools to measure and assess the impact of scientiﬁc research.
One of the most commonly used methods is the analysis of citation data, which reﬂects
the recognition and dissemination of scientiﬁc ﬁndings.
In order to measure and assess the impact of scientiﬁc research, various research
metrics and online databases have been developed. Research metrics are mathematical
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 236–243, 2024.
https://doi.org/10.1007/978-3-031-50158-6_24

Evaluating Research Impact: A Comprehensive Overview of
237
formulas that are used to quantify the impact of scientiﬁc research based on citation data.
SomeofthemostwidelyusedresearchmetricsincludetheH-index,g-index,eigenvector-
based citation index, etc. Online databases, such as Scopus, Web of Science, Google
Scholar, Mendeley, and arXiv, provide a platform for tracking and analyzing scientiﬁc
publications and citations.
Despite the widespread use of research metrics and online databases for evaluating
scientiﬁc impact, there are also limitations and biases associated with these tools. For
example, some research metrics may not accurately reﬂect the impact of interdisciplinary
or emerging ﬁelds, and some online databases may not have comprehensive coverage of
all scientiﬁc disciplines.
The aim of this research paper is to provide a comprehensive and objective overview
of research metrics and online databases for evaluating scientiﬁc impact. The paper will
examine the strengths and weaknesses of each research metric, compare and contrast
the features and capabilities of online databases, and explore the limitations and biases
associated with using these tools. The paper will also provide recommendations for
researchers, institutions, and funding agencies for using research metrics and online
databases effectively in evaluating the impact of scientiﬁc research.
2
Literature Review
The H-index, a metric that measures both the productivity and impact of a researcher’s
scientiﬁc publications. The H-index takes into account the number of publications and
the number of citations received by each publication [1]. The H-index should be used
in conjunction with other metrics to obtain a comprehensive picture of a researcher’s
scientiﬁc impact [2]. The g-index takes into account the number of citations received by
each article, rather than just the total number of citations received [3]. The eigenvector-
based citation index, a metric that measures the impact of a researcher’s publications
by considering both the number of citations and the impact of the journals in which
the publications appear [4]. The Science Citation Index Expanded and the Social Sci-
ences Citation Index, two well-known databases that track scientiﬁc publications and
citations. The author argues that these databases provide valuable information for eval-
uating the impact of scientiﬁc research, but should be used with caution due to potential
biases and limitations [5]. Scopus is a database that provides information on scientiﬁc
publications, including the number of citations received by each publication. It covers
a wide range of scientiﬁc disciplines and is widely used for evaluating the impact of
scientiﬁc research [6]. Web of Science is a database that provides information on sci-
entiﬁc publications, including the number of citations received by each publication. It
covers a wide range of scientiﬁc disciplines and is widely used for evaluating the impact
of scientiﬁc research [7]. Google Scholar is a database that provides information on
scientiﬁc publications, including the number of citations received by each publication.
It covers a wide range of scientiﬁc disciplines and is widely used for evaluating the
impact of scientiﬁc research [8]. Mendeley is a database and a reference management
software that provides information on scientiﬁc publications, including the number of
citations received by each publication. It covers a wide range of scientiﬁc disciplines
and is widely used for organizing and evaluating the impact of scientiﬁc research [9].

238
S. Ukidve et al.
arXiv is a repository of electronic preprints of scientiﬁc articles in a variety of ﬁelds,
including physics, mathematics, computer science, and biology. It allows researchers to
share their work before it has been peer-reviewed or published in a journal, and provides
a platform for the dissemination of new and innovative research [10].
3
Objectives
The objectives of the research paper are as follows:
• To provide an overview of various research metrics that are used to evaluate the impact
of scientiﬁc research, such as the H-index, g-index, eigenvector-based citation index,
etc.
• To examine the strengths and weaknesses of each research metric and to identify the
most appropriate metric(s) for different types of scientiﬁc research.
• To present an overview of online databases that are used to track scientiﬁc publications
and citations, including Scopus, Web of Science, Google Scholar, Mendeley, and
arXiv.
• To compare and contrast the features and capabilities of these online databases and
to identify the most suitable database(s) for different types of scientiﬁc research.
• To explore the limitations and biases associated with using research metrics and online
databases to evaluate scientiﬁc impact, and to provide suggestions for overcoming
these limitations.
• To provide recommendations for researchers, institutions, and funding agencies for
using research metrics and online databases effectively in evaluating the impact of
scientiﬁc research.
4
Analysis and Comparison of the Various Research Metrics
and Online Databases
The evaluation of scientiﬁc publications is a complex process that involves multiple
factors. Research metrics and online databases play a crucial role in measuring the
impact and quality of scientiﬁc publications. In this paper, we will analyze and compare
the various research metrics and online databases used to evaluate the impact and quality
of scientiﬁc publications.
One of the most widely used research metrics is the h-index, which measures both
the productivity and citation impact of an author’s publications. The h-index provides a
composite measure of an author’s research impact, taking into account both the number
of publications and the number of citations received [11].
Another commonly used metric is the number of citations, which provides a measure
of the impact of a publication based on the number of times it has been cited by other
authors. The impact factor, which is calculated by dividing the number of citations in a
given year by the number of articles published in the previous two years, provides an
overall measure of the impact of a publication in a speciﬁc ﬁeld [12].
Online databases such as Google Scholar, Web of Science, and Scopus provide a
wealth of information about the research output of authors and institutions, and can be

Evaluating Research Impact: A Comprehensive Overview of
239
used to calculate these metrics. These databases also provide access to a large number
of scientiﬁc articles, making it possible to compare the impact of different publications.
It is important to note that these metrics have limitations, and it is essential to consider
other factors when evaluating the impact and quality of scientiﬁc publications. For exam-
ple, the quality of the research, the broader impact on society, and the interdisciplinary
nature of the research should also be taken into account.
A combination of metrics and databases is necessary to get a complete picture of
the impact and quality of scientiﬁc publications. While no single metric or database can
provide a deﬁnitive evaluation, using multiple sources of information can help provide
a more comprehensive view of the impact and quality of scientiﬁc research.
5
Widely Used Research Metrics and the Number of Citations
The h-index, Impact Factor (IF), and number of citations are some of the most widely
used research metrics for evaluating the impact and quality of scientiﬁc publications.
The h-index is a composite measure of an author’s research impact, taking into
account both the number of publications and the number of citations received. It is
calculated by ranking an author’s publications in terms of the number of citations they
have received, and then selecting the highest number h such that h of the author’s papers
have received at least h citations [13].
The Impact Factor (IF) is a metric that measures the average number of citations
received by articles published in a speciﬁc journal. It is calculated by dividing the number
of citations received by the number of articles published in the previous two years. The
IF is widely used to evaluate the impact of a journal, and higher IFs are often seen as an
indicator of higher quality research.
The number of citations provides a measure of the impact of a publication based on
the number of times it has been cited by other authors. It is widely used to evaluate the
impact of an author or publication, with higher numbers of citations indicating higher
impact [14].
It is important to note that these metrics have limitations, and should be used in
conjunction with other factors, such as the quality of the research, the broader impact
on society, and the interdisciplinary nature of the research, to provide a comprehensive
evaluation of the impact and quality of scientiﬁc publications.
6
Various Online Databases that Are Utilized to Access and Analyze
Research Metrics
Web of Science, Scopus, and Google Scholar are some of the most widely used online
databases for accessing and analyzing research metrics. These databases provide a
wealth of information about the research output of authors and institutions, including
information on the number of publications, citations, and other research metrics.
Web of Science is a database that provides access to scientiﬁc literature in the natural
sciences, social sciences, and humanities. It covers a broad range of disciplines and
includes more than 33,000 journals, as well as conference proceedings, books, and other

240
S. Ukidve et al.
types of scientiﬁc literature. Web of Science provides detailed information on the number
of citations received by each publication, as well as the h-index and Impact Factor of
authors and journals.
Scopus is a database that provides access to over 20,000 peer-reviewed journals, as
well as conference proceedings, books, and other types of scientiﬁc literature. It covers a
broad range of disciplines and provides detailed information on the number of citations
received by each publication, as well as the h-index and Impact Factor of authors and
journals. Scopus also provides access to a large number of citation reports, which provide
a comprehensive overview of the research impact of authors and institutions.
Google Scholar is a free online database that provides access to a large number of
scientiﬁc articles, as well as conference proceedings, books, and other types of scientiﬁc
literature. It covers a broad range of disciplines and provides information on the number
of citations received by each publication, as well as the h-index of authors. Google
Scholar also provides access to a large number of citation reports, which provide a
comprehensive overview of the research impact of authors and institutions [15].
These online databases are widely used by researchers, institutions, and funding
agencies to access and analyze research metrics, and to evaluate the impact and qual-
ity of scientiﬁc publications. They provide a valuable resource for understanding the
contribution of research to the advancement of knowledge.
7
The Need for a Comprehensive and Holistic Approach
to Research Evaluation
Research evaluation is an important process for understanding the impact and quality
of scientiﬁc publications, and for determining the allocation of research funding and
recognition. However, relying solely on research metrics, such as the h-index, Impact
Factor (IF), and number of citations, to evaluate research can be limited and can result
in an incomplete picture of the impact of a publication.
There is a growing recognition of the need for a comprehensive and holistic app-
roach to research evaluation that takes into account multiple factors, such as collabora-
tion, interdisciplinary work, and societal impact. Collaboration is becoming increasingly
important in the scientiﬁc community, and it is important to consider the impact of collab-
orations in research evaluation. Interdisciplinary work is also becoming more common,
and it is important to consider the interdisciplinary nature of research in evaluation, as
this can have a signiﬁcant impact on the impact of a publication [16].
Societal impact is another important factor that should be taken into account in
research evaluation. Research that has a signiﬁcant impact on society can be of great
value, even if it may not have a high number of citations. It is therefore important to
consider the broader impact of research on society when evaluating research.
A comprehensive and holistic approach to research evaluation requires taking into
account multiple factors and considering the interplay between different metrics. This
can provide a more complete picture of the impact and quality of research, and can help
ensure that research funding and recognition are allocated fairly and effectively.
A comprehensive and holistic approach to research evaluation is essential for under-
standing the impact and quality of scientiﬁc publications, and for determining the alloca-
tion of research funding and recognition. This approach should take into account multiple

Evaluating Research Impact: A Comprehensive Overview of
241
factors, such as collaboration, interdisciplinary work, and societal impact, to provide a
more complete picture of the impact of research.
8
The Need for Continuous Reﬁnement and Improvement
The use of research metrics, such as the h-index, Impact Factor (IF), and number of
citations, to evaluate the impact and quality of scientiﬁc publications has been widely
used for many years. However, as research becomes increasingly complex and multi-
dimensional, it is important to continuously reﬁne and improve these metrics to better
reﬂect the nature of scientiﬁc research.
Research is becoming increasingly interdisciplinary, with researchers working across
multiple ﬁelds and disciplines. This can result in the production of highly impactful and
innovative research, but it can also be difﬁcult to measure and evaluate using tradi-
tional metrics. It is important to consider the interdisciplinary nature of research in the
reﬁnement and improvement of research metrics [17].
Additionally, research is becoming more global and collaborations are becoming
increasingly common, leading to a more complex and diverse research landscape. The
impact of research is not limited to the number of citations it receives, and other factors,
such as societal impact, should also be considered in the evaluation of research [18].
Moreover, the rapid pace of technological development and the increasing avail-
ability of online platforms for research dissemination have changed the way research is
conducted and evaluated. As a result, it is important to continuously reﬁne and improve
research metrics to better reﬂect the multidimensional nature of scientiﬁc research in the
digital age [19].
The use of research metrics is a valuable tool for evaluating the impact and qual-
ity of scientiﬁc publications. However, as research becomes increasingly complex and
multidimensional, it is important to continuously reﬁne and improve these metrics to
better reﬂect the nature of scientiﬁc research. This will help ensure that research funding
and recognition are allocated fairly and effectively, and that the impact and quality of
research are accurately reﬂected in the evaluation process [20].
9
Best Practices in Using Research Metrics and Online Databases
A. Reliable Sources of Information
• Using ofﬁcial databases and platforms such as Scopus, Web of Science, and Google
Scholar
• Avoiding sources that may not be reliable or may manipulate metrics
B.. Comparison of Different Databases
• Understanding the differences in data sources and coverage of each database
• Using multiple databases to get a comprehensive view of an individual’s scientiﬁc
impact
C. Integration of Multiple Metrics
• Using a combination of metrics, such as the h-index and g-index, to get a more
nuanced view of an individual’s impact
• Understanding the strengths and limitations of each metric

242
S. Ukidve et al.
D. Limiting Personal Biases
• Being aware of potential biases in metrics and databases
• Using metrics objectively, avoiding using them for personal gain or reputation
building.
10
Conclusion
Research metrics and online databases play a crucial role in evaluating the impact of
scientiﬁc research. There are various research metrics, such as the H-index, g-index,
and eigenvector-based citation index, that have been developed to quantify the impact
of scientiﬁc research based on citation data. Online databases, including Scopus, Web
of Science, Google Scholar, Mendeley, and arXiv, provide a platform for tracking and
analyzing scientiﬁc publications and citations.
However, it is important to recognize the limitations and biases associated with
using research metrics and online databases to evaluate scientiﬁc impact. Some research
metrics may not accurately reﬂect the impact of interdisciplinary or emerging ﬁelds, and
some online databases may not have comprehensive coverage of all scientiﬁc disciplines.
Despite these limitations, research metrics and online databases can provide valuable
information for researchers, institutions, and funding agencies to make informed deci-
sions about the allocation of resources and the recognition of scientiﬁc achievements. It
is recommended that researchers, institutions, and funding agencies use research metrics
and online databases effectively, taking into account the strengths and weaknesses of
each tool and the limitations and biases associated with their use.
Acknowledgements. Authors are grateful to Punjabi University, Patiala for providing adequate
library and internet facility.
References
1. Hirsch, J.E.: An index to quantify an individual’s scientiﬁc research output. Proc. Natl. Acad.
Sci. USA 102(46), 16569–16572 (2005)
2. Batista, P.D., Campiteli, M.G., Kinouchi, O.: Is it possible to compare researchers with
differentscientiﬁcinterests?Ananalysisoftheh-index.Scientometrics 68(3),179–189(2006)
3. Egghe, L.: Theory and practise of the g-index. Scientometrics 69(1), 131–152 (2006)
4. Radicchi, F., Fortunato, S., Castellano, C.: Universality of citation distributions: toward an
objective measure of scientiﬁc impact. Proc. Natl. Acad. Sci. USA 105(45), 17268–17272
(2008)
5. Leydesdorff, L.: Mapping the global development of science by means of publication indica-
tors: a study of Science Citation Index Expanded and Social Sciences Citation Index. J. Am.
Soc. Inf. Sci. Technol. 61(7), 1386–1403 (2010)
6. Scopus (n.d.). https://www.elsevier.com/solutions/scopus
7. Web of Science (n.d.). https://clarivate.com/webofsciencegroup/
8. Google Scholar (n.d.). https://scholar.google.com/
9. Mendeley (n.d.). https://www.mendeley.com/
10. arXiv (n.d.). https://arxiv.org/

Evaluating Research Impact: A Comprehensive Overview of
243
11. Bollen, J., Van de Sompel, H., Hagberg, A., Chute, R.: A principal component analysis of 39
scientiﬁc impact measures. PLoS ONE 4(6), e6022 (2009)
12. Bornmann, L., Leydesdorff, L.: What do citation counts measure? A review of studies on
citing behavior. J. Document. 64(1), 45–80 (2008)
13. Garﬁeld, E.: Citation analysis as a tool in journal evaluation. Science 214(4520), 671–681
(1979)
14. Hirsch, J.E.: Does the Hirsch index have predictive power? arXiv preprint arXiv:0707.3168
(2007)
15. Garﬁeld, E.: Citation Indexing: Its Theory and Application in Science, Technology, and
Humanities. Wiley, New York (1995)
16. Ioannidis, J.P.: Why most published research ﬁndings are false. PLoS Med. 2(8), e124 (2005)
17. Radicchi, F., Fortunato, S., Castellano, C.: Diffusion of scientiﬁc credits and the ranking of
scientists. Phys. Rev. E 80(5), 056103 (2009)
18. Schreiber, M., Glassey, J.: A critical examination of the h-index in comparison with traditional
indices and with peer judgement. Scientometrics 71(2), 317–331 (2007)
19. Van Raan, A.F.J.: Comparison of the Hirsch-index with standard bibliometric indicators and
with peer judgment for 147 chemistry research groups. J. Am. Soc. Inform. Sci. Technol.
57(8), 1060–1071 (2006)
20. Waltman, L., van Eck, N.J.: A comparative study of four citation-based indices for ranking
journals. J. Inform. 4(2), 146–157 (2010)

Hazard Identiﬁcation, Risk Assessment
and Control (HIRAC) at the Wood Processing
Industry
Herry Christian Palit(B)
and Alexander
Industrial Engineering Department, Petra Christian University, Siwalankerto 121-131, Surabaya,
Indonesia
herry@petra.ac.id
Abstract. This study discusses the application of Hazard Identiﬁcation, Risk
Assessment, and control (HIRAC) at the wood processing industry which pro-
cesses raw materials in the form of logs into various sizes of wood. This research
was conducted due to the lack of awareness of the management concerning the
occupational health and safety of its employees. It can be seen from the high
number of accidents, where during 2016–2019 there were 16 cases. The results
of the risk assessment using HIRAC showed 6% potential hazard with a low risk
level, 29% with a moderate risk level, and 65% with a very high-risk level. Hazard
control is focused on potential hazards with a very high-risk level. Hazard control
consist of seven type of administrative controls, one type of elimination control,
four types of engineering controls, and one type of personal protective equipment
control.
Keywords: Hazard identiﬁcation · Risk assessment · Hazard control ·
Occupational health and safety
1
Introduction
Occupational health and safety belong to the essential factors inﬂuencing towards labor’s
productivity. In order to be able to work productively, working environment needs to be
guaranteed safe and healthy. Hazard Identiﬁcation and Risk Assessment is a process to
deﬁne and identify hazard potentials; as well as to assess risk level of possible hazard
occurrence by considering both probability and severity level. Every singly industry
is required to identify potential hazard and assess its risk level within the process of
establishing occupational safety and health guidelines. This risk assessment should be
performed by utilizing the guidelines and standards of risk assessment [1]. Based on
the proposed risk assessment, precise hazard prevention which eliminates and reduces
hazardous potentials can be established.
This research is performed in a wood processing industry in Surabaya cultivating
timber as the raw material, into various wood products such as sawn timber, Slice Four
Side (S4S), panels, decking, ﬂooring, ﬁnger joint, and many others. So far, the industry’s
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 244–252, 2024.
https://doi.org/10.1007/978-3-031-50158-6_25

Hazard Identiﬁcation, Risk Assessment and Control (HIRAC) at
245
management has yet to possess consideration towards its labors’ safety and health,
judging from the absence of reliable occupational hazard prevention and control. This
is supported by the data on work-related accidents during 2016–2019, pointing out 16
accidents in which 15 of them are ergonomics-related (as displayed on Table 1). In
addition, current working environment also potentially triggers occupational disease for
the labors. This can be seen from wood dust scattered on the production ﬂoor area, which
may trigger breathing and vision problems.
Table 1. Data on work-related accidents
Causing factors
Year
2016
2017
2018
2019
Physics
–
–
–
–
Chemics
1
–
–
–
Biology
–
–
–
–
Ergonomics
2
4
3
6
Psychics
–
–
–
–
Source Industrial database
Previous researchers have pointed out that HIRAC implementation in various indus-
tries brings various result on the risk hazard percentage level. The implementation of
HIRAC in a boiler operation at Indonesian Power Unit, Semarang Ltd. Points out high-
risk hazard percentage level in amount of 16.67% [2]. The implementation of HIRAC
in the fabrication process at Pertamina Balongan Ltd. Ambarani and Tualeka [3] points
out high risk hazard percentage level in amount of 45%, and very high-risk hazard per-
centage level at 5%. Indrawati et al. [4] points out 22% of high-risk hazard percentage
level as a result of HIRAC implementation in a furniture industry. The better the imple-
mentation of occupational safety and health guidelines in an industry, the lesser its high
and very high-risk hazard level percentage.
This research is aimed to assist corresponding industries in identifying hazard poten-
tials in a production ﬂoor, along with their risk level assessment. The higher the risk
level, the more effort towards hazard control needs to be done, in order to reduce work-
related accidents and work-related disease among the labors. Thus, hazard prevention
and control are focused on hazard potentials with high and very high-risk level.
2
Method
Observation and HIRAC implementation are performed by involving both the owner and
human resource manager. Their participation begins with identifying hazard potentials;
determining probability and severity level for risk assessment; and hazard control on the
production ﬂoor. There are 142 labors working on the production area, 106 males and
36 females, in which 6 are acting supervisors. They work approximately 40 h a week.
During worktime, they are prone to the hazard potentials scattered around their working
environment, both work-related accident and work-related disease.

246
H. C. Palit and Alexander
The initial stage for hazard identiﬁcation is performed to identify process and activ-
ities that may trigger work-accident and work-related disease related problems. This
is performed by observation utilizing ergonomic checkpoints from International Labor
Ofﬁce [5]. This tool is comprised of separate categories: Material storage and handling,
which is intended to observe material storing and mobilization activity on the workspace;
Hand tool, intended to observe activities related to the use of tools on the workspace;
Machine safety, intended to observe the safety on the use of production machineries on
the workspace; Workstation design, intended to observe both safety and convenience
of the labors while working on their workspace; Lighting, intended to observe overall
lighting and lighting intensity on the workspace; Premises, intended to observe air cir-
culation on the workspace and evacuation system towards potential hazards such as ﬁre
and others; Hazardous substance and agents, intended to observe hazardous agents on
the workspace; Welfare facilities, intended to observe general facilities; Work organi-
zation, intended to observe how the organization make decisions and policies. Hazard
identiﬁcation is afterwards continued by more detailed elaboration, in order to better
identify hazard potentials and risks that may occur during activities on the production
level area, both work-related accident and work-related disease.
Based on the hazard identiﬁcation stage, risk assessment is performed by considering
the probability of the hazard, and the severity that may occur as an impact. Based on the
multiplication process towards probability and severity scores, risk value is obtained.
Table 2 presents Risks Assessment Matrix Model to analyze the category of risk level
from each of the hazard potential.
Table 2. Risk assessment matrix model
Risk assessment matrix
model
Severity
Light injury
(1)
Moderate
injury (2)
Severe
injury (3)
Fatality (4)
Disaster (5)
Probability
Very high
(5)
Low
Moderate
High
Very high
Very high
High (4)
Low
Moderate
High
Very high
Very high
Moderate
(3)
Low
Moderate
Moderate
Tinggi
Tinggi
Low (2)
Low
Low
Moderate
Moderate
Moderate
Very low
(1)
Low
Low
Low
Low
Low
Source Rout and Sikdar [6]
Hazard control mechanism in this research is focused towards hazard potentials
with high and very high risk factor. According to the policy established by the Ministry
of Labor number 5, 2018 [7], hazard control on the working environment should be
performed according to the ﬁve hierarchical levels as pointed out by Fig. 1.

Hazard Identiﬁcation, Risk Assessment and Control (HIRAC) at
247
Fig. 1. The hierarchy of workplace environment hazard control
Hazard control in a working environment is performed by conforming priority levels
set on the Figure. The ﬁrst hierarchy, or control through hazard elimination is a ﬁrst
priority. If not possible, then second layer of the control can be utilized.
3
Results and Discussion
Hazard identiﬁcation is performed by analyzing both of the result of ﬁeld observation
and measurement on the production area. Based on the overall wood processing, 21
activities with hazardous potential are identiﬁed. The following are examples list hazard
potential during timber receiving, trimming, and maintaining stages.
Mobilizing timber from truck with forklift. This belongs to the receiving and mobiliz-
ing stage. Hazard potentials found during this stage include forklift path, which appears
to be the same one walked by the labors during their activity. This unorganized route
may potentially cause accident, such as a worker may be hit by the forklift.
Operatinglogtrimmachinery.Thisbelongstothetrimmingprocess,inwhichtimbers
are trimmed by using machineries. These machineries possess hazard potential for the
operators, such as amputation, pinching, and direct exposure from the wood dust, noise,
hot airﬂow, and high level of humidity.
Machineries check and repair. This belongs to one of the activities during maintaining
stage. Hazard potential in this activity may occur if the machineries suddenly turned on
during a repair process.
Risk assessment is performed after hazard identiﬁcation by utilizing matrix dis-
played on Table 2. This aims to determine policy priority regarding problems or poten-
tial problems. This analysis is divided into two types of risks: work-related accident and
work-related disease. Risk assessment is conducted by determining opportunity value
and severity rate towards existing hazard potential. Probability assessment and severity
risk level involve both owner and human resource manager. The probability of haz-
ard potential related with work-related accident and work-related disease is determined
through some values representing the likelihood an accident may occur. This probabil-
ity is divided into 5 different levels, starting from the lowest level of probability. The
description of each level can be seen on Table 3 (for work-related accident) and Table 4
(for work-related disease).

248
H. C. Palit and Alexander
Table 3. Work-related accident probability level description
Level Description
1
Never happened/heard on the equivalent industries around the world
2
Ever happened/heard on the equivalent industries around the world
3
Ever happened once or more on the late three years, on the equivalent industries around
the world
4
Ever happened once or more on the late three years, on the equivalent industries in
Indonesia
5
Ever happened once or more on the late three years, on the industry
Source Adaptation from Ardani et al.[8]
Table 4. Work-related disease probability level description
Level
Description
1
Rare exposure and low likelihood of causing disease
2
Low exposure and low likelihood of causing disease
3
Regular/irregular exposure and moderate likelihood of causing disease
4
Frequent exposure and high likelihood of causing disease
5
Constant exposure dan and very high likelihood of causing disease
Source Adaptation from Ambarani and Tualeka[3]
Severity level is assessed in order to ﬁnd out the impact that may occur if hazard
potential turns into realization. This severity level is divided into 5 different levels starting
from the lowest severity level. The description of each level can be seen on Table 5 (for
work-related accident) and Table 6 (for work-related disease).
Theresultofprobabilityandseveritylevelisusedtodeterminerisklevels,whichcate-
gorized from the lowest, as pointed out from Table 2. The followings are risk assessments
towards hazard potential belonging to very high-risk category.
Mobilizing timbers from a truck, mobilizing timbers on the production ﬂoor, and
mobilizing timbers to the production storage may inﬂict accident between forklift and
labors. The accident may cause broken bones and death; thus, severity value is given 5.
The accident probability is also given 5 considering the fact that such accident happened
once in 2019. Based on both severity and probability level, forklift accident is categorized
as very high risk.
Operators performing activity around trimming machine may potentially slipped and
pinched by a moving wheeled cart. This may result in broken bones or death; thus the
severity level is given 5. The probability of accident is also given 5 considering the fact
that such accident happened once in 2019. Based on both severity and probability level,
wheeled cart accident is categorized as very high risk.

Hazard Identiﬁcation, Risk Assessment and Control (HIRAC) at
249
Table 5. Work-related accident severity level description
Level
Description
1
Near miss requires documentation and action
2
Potentially causing injury which require ﬁrst aid kit
3
Require medical treatment, but does not cause work restriction or worktime lost
4
One or more injuries requiring medical treatment, which cause worktime lost, but does
not inﬂict permanent damage
5
Potentially causing permanent damage or death
Source Adaptation from Ardani et al. [8]
Table 6. Work-related disease severity level description
Level
Description
1
Exposure requires documentation and action
2
Potentially causing disease which require ﬁrst aid kit
3
Potentially causing disease, but does not cause work restriction or worktime lost
4
Potentially causing temporary disease, but require medical treatment which impacts on
worktime lost
5
Potentially causing permanent damage or death
Source Adaptation from Ambarani and Tualeka [3]
The operator of trimming machinery may constantly exposed from wood dust, as he
would always be on the same station during his worktime. The possibility of work-related
disease is therefore high and given value of 5. Wood dust may cause eyes irritation, occu-
pational asthma, or other chronical obstructive on the ones exposed, therefore severity
level is also given 5. Considering such probability and severity level, this exposure is
categorized as very high-risk category.
Machinery turned on during maintenance/repairis considered very dangerous and
may cause fatality on the victim, such as scratch or dismembered body parts, therefore
severity level is given 5. The probability value is also given 5, considering the fact that
such accident happened once in 2019. Considering such probability and severity level,
this malfunctioning is categorized as very high risk.
The result of assessment towards all activities within production area points out three
types of risk categories: high risk in amount of 65%, moderate risk in amount of 29%,
and low risk in amount of 6%, as displayed in Fig. 2. This implies that hazard control
will be prioritized towards activities containing very high-level risk.
This hazard control aims to reduce risk level and make risks more acceptable. Hazard
control is performed towards very high-risk activities. The followings are how hazard
control may be implemented towards very high-risk activities.

250
H. C. Palit and Alexander
Fig. 2. Working-environment potential risk category diagram
Establishing different path for transportation access and labors access. This control
is performed to overcome problems on timber receiving process, temporary storage,
and ﬁnished products storage. These three processes contain similar activities which is
the mobilization of timbers by forklift. This control belongs to technical manipulation
category, as the creation of speciﬁc path according to the purpose ensure the safety of
labors walking on the path. Labors path can be created by applying paint with a-meter
wide size in one of the sections of the transportation path. This paint functions to mark the
area that can be speciﬁcally passed through by labor only. Other transportation devices
such as forklift is not allowed to pass through it. Likewise, this control also forbids the
labor to walk on the transport path.
Labelling caution sign and designate safe area for the operator. This control aims to
overcome problems that may happen during trimming process. Labelling ‘caution’ sign
indicating the danger may prevent labors to walk too close with wheeled cart, therefore
reducing pinching accident. This label can be installed on the outer-lower side of the
cart. The exact location of the label can be determined by sticking out a sticker in the
area. This prevention belongs to administrative category, as it helps labors to avoid area
with high level of hazard. Installing wooden barrier on the surrounding of cart track can
also support the prevention effort. This belongs to technical manipulation category, as
this limits operator movement from slipping backwards.
Installing local exhaust in production machineries and wearing protection devices.
Theinstallationoflocalexhaustbelongstotechnicalmanipulationcategory,asitprevents
labors from being exposed to wood dust resulted from the production machineries. Wood
dust may inﬂict eyes irritation, occupational asthma, and chronic diseases for those
exposed. Local exhaust can be made in form of a pipe sucking scattered wood dust. The
pipe should be pointed to the trimming location where wood dust appears. This prevents
wood dust to reach labors’ breathing zone (300 mm diameter) [9]. This control can
also be achieved by wearing mask and safety google glass. The aforementioned mask
is quarter-mask-air-purifying respirator type, which equipped with dust ﬁlter made of
ﬁberglass. Safety google glass prevents operator’s eyes from the exposure of wood dust,
therefore preventing eyes irritation. The application of both mask and safety google glass
should be made compulsories for all the workers in the production area. This conforms
with the industry’s regulation regarding the use of protective devices.

Hazard Identiﬁcation, Risk Assessment and Control (HIRAC) at
251
Providing label and marking area with potential hazard (hand caught inside a
machine). This is performed especially in wood processing process involving the use of
machineries. Providing label or danger sign on the machineries utilized on this stage is
aimed to prevent accident caused by labors’ hand getting pulled by one of the machines.
This control is performed by marking areas forbidden to be touched by the labors’ hands.
Marking is done by sticking out stickers showing danger sign on the area. This belongs
to administrative category, as it helps the labors to avoid areas potentially hazardous
towards their body part (in this case, hand getting caught inside a machine).
Locking machine from the power source by using Lock Out/Tag Out (LOTO). LOTO
system is a combined system aimed to prevent a machine from turning on during main-
tenance process. Lock out system locks the power source of a machine to prevent sudden
activation, while tag out helps other labors in the vicinity to be aware of an ongoing main-
tenance process. LOTO system belongs to administrative control category, as labors are
required to apply the system ﬁrst, to ensure that the corresponding machine is isolated
from the power source. This can be done by switching off electrical panel of the machine,
followed by locking it with a padlock equipped with label explaining ongoing mainte-
nance process. This deactivated and isolated power unit prevents other unaware labors
from switching on the machine during maintenance.
For the future, the company is suggested to consider utilizing technology in order
to better manage the hazard control. The utilization of safety sensor installed on the
machineries and hazardous areas potentially be the hazard control [10, 11]. One of the
applications of safety sensor on machineries is proximity sensor. The way this sensor
worksmayvarydependingonthetransmissionoflight,ultrasonic,orotherkindofwaves.
In general, proximity sensor transmits wave and calculate its wavelength according to
the preset range. If it detects an object, the length of wave received back by the sensor
will be different as it is bounced by the object. If any object starts entering hazardous area
with conveyor, vacuum, or anything similar, integrated proximity sensor will release a
particular warning. Sound or machine that turns off automatically will be made on at the
moment proximity sensor detects an object in proximity. Passive Infrared Receiver (PIR)
sensor detects any foreign object being too close with the hazardous area will release a
loud sound, as a warning towards the object. In addition, the use of wearable technologies
to observe workers in real-time and to remove potential hazard is also important to
consider. Hazard originated from wood dust and machinery noise can be monitored by
wearable technology. The NIOSH Center for Direct Reading and Sensor Technologies
(NCDRST) has developed several wearable technologies to improve occupational health
and safety [12]. Respirable Dust Sensor Technology can be used to monitor the exposure
of wood dust, whether it is still safe or it has trespassed the threshold. Sound level
meter apps, utilizing embedded smartphone sensors is a tool to measure noise level in
a workplace. This should be supported with noise exposure perimeter to better reduce
hearing loss caused by noise in a workplace. With the help of technology, occupational
health and safety management is expected to better provide hazard-free environment
more effectively in a workplace.

252
H. C. Palit and Alexander
4
Conclusion
The result of this research points out that working environment in the production area
potentially harms labors’ health and triggers occupational hazards. This can be seen
from the number of very high-risk level of activities in amount of 65%, while the 29%
is moderate risk level activities, and the remaining 6% as low risk level activities. The
activities with very high level of risk becoming the subject priority of hazard control.
Proposed hazard control includes seven administrative control category, one elimination
control category, four technical manipulation category, and one protective device control
category.
References
1. Ramesh, R., Prabu, M., Magibalan, S., Senthilkumar, P.: Hazard identiﬁcation and risk
assessment in automotive industry. Int. J. ChemTech Res. 10(4), 352–358 (2017)
2. Zeinda, E.M., Hidayat, S.: Risk assessment kecelakaan kerja pada pengoperasian boiler di PT.
Indonesia Power Unit Pembangkitan Semarang. Indones. J. Occup. Saf. Health 5(2), 183–191
(2016)
3. Ambarani, A.Y., Tualeka, A.R.: Hazard identiﬁcation and risk assessment (HIRA) pada proses
fabrikasi plate tanki 42-T-501A PT. Pertamina (PERSERO) RU VI Balongan. Indones. J.
Occup. Saf. Health 5(2), 192–203 (2016)
4. Indrawati, S., Prabaswari, A.D., Fitriyanto, M.A.: Risk control analysis of a furniture produc-
tion activities using hazard identiﬁcation and risk assessment method. MATEC Web Conf.
154, 01102 (2018). https://doi.org/10.1051/matecconf/201815401102
5. International Labour Ofﬁce: Ergonomic Checkpoints: Practical and Easy-to-Implement Solu-
tions for Improving Safety, Health and Working Conditions. International Labour Ofﬁce,
Switzerland (2010)
6. Rout, B.K., Sikdar, B.K.: Hazard identiﬁcation, risk assessment, and control measures as an
effective tool of occupational health assessment of hazardous process in an iron ore pelletizing
industry. Indian J. Occup. Environ. Med. 21(2), 56–76 (2017). https://doi.org/10.4103/ijoem.
IJOEM_19_16
7. Kemenaker: Peraturan Menteri Ketenagakerjaan Republik Indonesia (PERMENAKER)
nomor 5 tahun 2018 tentang Keselamatan dan Kesehatan Kerja Lingkungan Kerja. Kemente-
rian Ketenagakerjaan Republik Indonesia, Jakarta (2018)
8. Ardani, H.N., Santoso, H., Rumita, R.: Analisis risiko kesehatan dan keselamatan kerja pada
pekerja divisi mill boiler (Studi Kasus di PT Laju Perdana Indah PG Pakis Baru, Pati). Ind.
Eng. Online J. 3(2), 1–6 (2014)
9. Harrianto, R.: Buku Ajar Kesehatan Kerja. Penerbit Buku Kedokteran EGC, Jakarta (2009)
10. Rajendran, S., Giridhar, S., Chaudhari, S., Gupta, P.: Technological advancements in occu-
pational health and safety. Meas. Sens. 15, 100045 (2021). https://doi.org/10.1016/j.measen.
2021.100045
11. Haas, E.J., Cauda, E.: Using core elements of health and safety management systems to
support worker well-being during technology integration. Int. J. Environ. Res. Public Health
19(21), 13849 (2022). https://doi.org/10.3390/ijerph192113849
12. The National Institute for Occupational Safety and Health (NIOSH). https://www.cdc.gov/
niosh/programs/cdrst/default.html

Location Selection of Rail Transportation Hub
Using TOPSIS Method
Kanokporn Sripathomswat(B), Nattawat Tipchareon, Worapat Aruntippaitoon,
Itiphong Trirattanasarana, and Sunarin Chanta
King Mongkut’s University of Technology North Bangkok, 1518, Pracharat 1
Road,Wongsawang, Bangsue, Bangkok 10800, Thailand
kanokporn.s@cit.kmutnb.ac.th, sunarin.c@itm.kmutnb.ac.th
Abstract. Nowadays, the transportation system has a signiﬁcant impact on the
economic drive, especially in densely populated areas. The rapid development
of the transportation system has resulted in people having several transportation
options. However, people still have problems accessing the origin station and
transiting from the terminal station to their destination. The purpose of this study is
to select the best location of the transportation hub for the Red Line suburban train,
Bang Sue—Rangsit route, Thailand. Since the problem involves several factors,
the multiple criteria decision-making method, TOPSIS, is selected to solve the
problem. We select the criteria based on the main factors, which are connectivity,
accessibility, the number of passengers, and the potential for station expansion.
Based on the concept of TOPSIS, the positive ideal value of each criterion is
identiﬁed, then the relative closeness score of each alternative is calculated. The
best location of the transportation hub of the Red Line is the station with the
highest relative closeness score. The results provide a decision support model and
help decision-makers in designing transportation hub location planning.
Keywords: Railway · Transportation hub · Public transportation · Multiple
criteria decision-making · TOPSIS
1
Introduction
The development of rail transportation is an important mechanism for driving the
economy and developing the country. However, the development of rail transportation
requires a very high investment budget. Entrepreneurs therefore need to plan for long-
term returns. In the early stages of enabling the rail transport system, many times there
is a loss problem, because consumers are familiar with the traditional transportation sys-
tem used to travel and inconvenient to change travel behavior. Moreover, rail transport
cannot provide a door-to-door service. It requires connecting a bus/taxi to the departure
station or from the terminal station to the destination, which causes inconvenience for
Please note that the AISC Editorial assumes that all authors have used the western naming con-
vention, with given names preceding surnames. This determines the structure of the names in the
running heads and the author index.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 253–261, 2024.
https://doi.org/10.1007/978-3-031-50158-6_26

254
K. Sripathomswat et al.
travel. Sometimes it takes more total travel time from the origin to the destination or
incurs more total travel expenses compared to traveling by road transport. In the early
phase of rail service operations, the planning of travel connection points may not yet
cover the needs of consumers. Especially, for a large station that sever a massive number
of passengers, it will cause long waiting times for connecting buses/taxis.
In this research, we present a guideline for analyzing the optimal location of the
transportation hub or connecting points for the new Red Line suburban train, Bang
Sue—Rang Sit route, Bangkok, Thailand. The objective is to ﬁnd the best location for
a railway transportation hub, which will be used as a connecting/transit point to other
transportation modes in order to provide traveling convenience for rail passengers. In
this case, not only the number of passengers that matter, but also other related factors
are taken into account such as connectivity, physical characteristics of the train station,
the available transportation modes and transits around the station, and the number of
government ofﬁces and business centers around, etc. Since the decision concerns many
factors, the multiple criteria decision-making method is selected as a tool for solving the
problem. The Technique for Order Preference by Similarity to Ideal Solution (TOPSIS)
is selected for deciding the best location of a rail suburban transportation hub. The
important level of the criteria is evaluated by railway experts. The results can be used as
a guideline for decision-makers in hub location planning.
2
Literature Review
Designing a public transport network system involves many components and factors.
The main structural components of a transport network are nodes, links, hubs, and ﬂows
[1]. Node is deﬁned as a location that has access to the transportation network, whereas
link is a transport that is connected between two nodes. Flow is the amount of trafﬁc
that is transported on a link between two nodes. Hub is a node that handles the amount
of ﬂow and connects transit for different transportation modes. In this study, we focus
on transportation hub selection problems, where the system performance depends on
hub location. Several works have been proposed for transport hub network design [2–5].
Nielsen and Lange [6] listed the success factors for transportation network design. Since
determining the appropriate location of the hub requires many factors to be considered,
multiple criteria decision-making (MCDM) methods are used to solve the problems.
Wang et al. [7] proposed an evaluation scheme based on TOPSIS including AHP
and Entropy for the city rail transit network. They selected 11 indices such as network
scale, passenger ﬂow, ticket revenue, etc. from an evaluation of 3 aspects: network
function, trafﬁc function, and economics. The evaluation matrix was standardized by
[0,1] linear transformation. Weights of inﬂuence parameters were determined by AHP
and entropy, considered expert knowledge and experience. The weighted standardized
decision-making matrix was built up through a standardized decision-making matrix and
parameter weights. The selection of highway network planning schemes was determined
bycalculatingtheclosedegreeoftheschemesandtheidealschemesbasedontheTOPSIS
method.
Zabihi et al. [8] considered the best location for a container transshipment hub in
the southern seas of Iran using a hybrid MCDM method based on Analytical Hierarchy

Location Selection of Rail Transportation Hub Using TOPSIS Method
255
Process (AHP) and Technique for Order Preference by Similarity to Ideal Solution
(TOPSIS). The problem was considered as three levels: goal, criteria, and alternatives.
To select the best location for a transshipment hub, six main criteria were considered,
which are port location, port physical, hinterland economy, port efﬁciency, cost, and
other condition. The criteria that have the highest weight was the port location, which
composited of three sub-criteria: the closeness to the import/export area, proximity to
the feeder port, and the closeness to navigation routes. A total of 18 sub-criteria were
evaluated as positive and negative dimensions using the distance of the positive-ideal
and negative-ideal solutions, respectively. The ﬁnal ranking showed that Bandar Abbas
port is the best location for a transshipment hub among 6 alternatives.
Chen et al. [9] proposed a selection model for logistic centers based on TOPSIS and
Multichoice Goal Programming (MCGP) methods. They integrated fuzzy technique for
order preference by similarity to an ideal solution and MCGP to obtain an appropriate
logistics center from many alternative locations for the airline industry. They selected
ﬁve criteria composited of both qualitative and quantitative data, which are resource
availability, location resistance, expansion possibility, investment cost, and information
abilities. The decision-making group (DM) was required to select the best logistic center
from 5 candidates by applying the Delphi technique. The importance fuzzy weights of the
criteria were determined by the DM. The fuzzy positive-ideal and fuzzy negative-ideal
were determined, then the distance of each location candidate from fuzzy positive-ideal
and fuzzy negative-ideal was calculated with respect to each criterion. The closeness
coefﬁcients obtained for each location candidate were used as priority values to build
the TOPSIS-MCGP model.
Zhao et al. [10] determined a location of intra-city distribution hub in the urban
metro network using a segmentation method. The indices for evaluating the importance
of each metro station were selected by complex network theory. The evaluation index
weight was calculated by AHP method, then the importance of each metro station was
evaluated using the TOPSIS model. The location model was formulated to ﬁnd the ﬁnal
metro distribution hubs from the candidate metro distribution hubs with consideration of
logistics demand. They chose the Shanghai metro system as a case study and provided
strategic planning for metro distribution hub location selection.
3
Material and Methods
3.1
Multiple Criteria Decision-Making Methods
MCDM is a complex decision-making tool that involves both quantitative and qualita-
tive factors [11]. MCDM is a branch of operations research for solving multiple crite-
ria problems. MCDM methods are classiﬁed into two categories: discrete MCDM or
Multiple Attribute Decision Making (MADM) and continuous Multiple Objective Deci-
sion Making (MODM) [12, 13]. In this study, we focus on the MADM. The MADM
methods include TOPSIS, AHP, Simple Additive Weighting (SAW), Weighted Product
Method (WPM), Elimination and Choice Expressing Reality (ELECTRE), and Linear
Programming Technique for Multidimensional Analysis of Preference (LINMAP), etc.

256
K. Sripathomswat et al.
3.2
Technique for Order Preference by Similarity to Ideal Solution (TOPSIS)
TOPSIS is one of several multi-criteria decision analysis methods to determine the best
alternative. The TOPSIS method is to try to ﬁnd an alternative whose overall performance
is close to the best value for each criterion and far from the worst in each criterion as well.
By applying TOPSIS, the analyst must make giving importance to each decision-making
criterion.TOPSISisarankingtoolamongchoicesbasedonevaluationdataofeachchoice
of multiple-criteria decision-making. Therefore, it is suitable for making decisions using
quantitative criteria that can be evaluated. The choice came out as incandescent numbers.
The TOPSIS data analysis process follows the next step.
Step 1: Create a decision matrix G that consists of m alternatives and n criteria, as
shows in Eq. (1). Bij is the attribute value of the i alternative with respect to the j criteria,
where i = 1, 2, …, n, j = 1, 2, …, m, n = number of alternatives, m = number of criteria,
Ai = Alternative i, Bj = Criteria j.
Step 2: Normalize the decision matrix to adjust the value of each criterion to be in the
same standard. The elements of the normalized decision matrix rij are given by Eq. (2).
The value should be in the range of 0–1, and the total weights of all values must be equal
to 1.
G =
A1
A2
...
Am
B1 B2 . . . Bn
⎡
⎢⎢⎢⎣
b11 b12 ... b1n
b21 b22 . . . b2n
...
...
bm1 bm2 . . . bmn
⎤
⎥⎥⎥⎦
(1)
rij =
bij

m	
i=1
b2
ij
; ∀i, j
(2)
Step 3: Calculate the weighted normalized decision matrix. Let vij be the element of
the weighted decision matrix, so it can be calculated by using Eq. (3) wj is the weight
of criterion j, and the element of a vector of weights W that consist of n criteria, where
the sum of the elements equal to 1, W = (w1, w2, …, wn).
vij = wjrij; ∀i, j
(3)
Step 4: Determine the positive ideal value and the negative ideal value, where S+ is
the best value in each criterion when comparing all available alternatives, and S−is the
worst value for each criterion when comparing all available alternatives. The
S+ =

v+
1 , v+
2 , . . . , v+
n

= max
i

vij

(4)
S−=

v−
1 , v−
2 , . . . , v−
n

= min
i

vij

(5)
Step 5: Compute the distance of each alternative from the positive ideal and the
negative ideal solutions. Let di+ and di−represent the distances of the ith alternative

Location Selection of Rail Transportation Hub Using TOPSIS Method
257
from the positive ideal solution and the negative ideal solution, respectively. Di and di
can computed as in Eqs. (6)–(7)
d+
i
=




n

j=1

Vij −V +
j
2
; ∀i
(6)
d−
i
=




n

j=1

Vij −V −
j
2
; ∀i
(7)
Step 6: Obtain the relative closeness of each alternative to the ideal solution, where
the relative closeness of the ith alternative to the ideal solution is deﬁned as in Eq. (8).
The highest Ci indicates the best alternative.
C∗
i =
d−
i
d−
i + d+
i
; ∀i
(8)
3.3
A Case Study of Red Line Rail, Bangkok, Thailand
The Red Line Rail is the ﬁrst phase of the suburban railway system project that operates
between Bang Sue Grand Station to Rangsit Station, Thailand. The Red Line sky train
starts operating service in August 2021. The line is located in the area of 2 provinces,
which are Bangkok (Dusit District, Phaya Thai District, Bang Sue District, Chatuchak
District, Lak Si District and Don Mueang District) and Pathum Thani Province (Muang
District, Thanyaburi District, Lam Luk Ka District and Khlong Luang District). There
are 10 railway stations in the ﬁrst phase, which compose of Bang Sue, Chatuchak, Wat
Samian Nari, Bang Khen, Thung Song Hong, Lak Si, Kan Kheha, Don Mueang, Lak
Hok, Rangsit. Figure 1 shows the connection of all 10 stations.
After reviewing the literature related to factors that should be considered for design-
ing transportation hubs, the factors are selected based on the urban planning concept in
Bangkok [14]. We deﬁned 10 criteria for making the decision on selecting the appropri-
ate transportation hub for the Red Line Rail: (1) the distance from the station to other
transportation mode transits (2) the variety of transportation system around the station
(3) the number of exits of station (4) size of the station (5) the number of ﬂoors or the
number of levels at the station (6) the number of transportation service points around
the station (7) the number of passenger pick-up points below the station (8) area of car
parking for passengers (9) the number of government ofﬁces and business centers around
the station (10) the number of passengers at the station.
Then each criterion is weighted by experts’ expertise. We rearrange all criteria by
their important scores using Rank Order Centroid method (ROC) [11] as the following:
(C1) the number of passengers at the station, (C2) the distance from the station to other
transportation mode transits, (C3) the number of government ofﬁces and business centers
around the station, (C4) the variety of transportation systems around the station, (C5)
the number of transportation service points around the station, (C6) area of car parking
for passengers, (C7) the number of passenger pick-up points below the station, (C8) the
number of exits of station, (C9) size of station, (C10) the number of ﬂoors or the number
of levels at the station.

258
K. Sripathomswat et al.
Fig. 1. The stations in Red Line Rail, Bangkok, Thailand
4
Results and Discussion
In this case, n represents the number of alternatives (stations), where n = 10, and m
represents the number of criteria (factors), where m = 10. The matrix of evaluated
value of the 10 criteria for the 10 stations as shown in Table 1, where C1, …, C10
represented criteria 1–10, respectively. Based on the TOPSIS procedure in the previous
section, we create a decision matrix G as shown in Table 2. Then, we calculated the
normalized weighted decision matrix as shown in Table 3. The positive ideal values (S+)
and negative ideal values (S−) of each criterion are calculated and reported in Table 4.
Next, we calculate the relative closeness (C+) and ranking of each alternative as shown
in Table 5. The best location for building a transportation hub is deﬁned as the highest
rank of the station that has the highest relative closeness value. In this case, the Rung
Sit station is the ﬁrst rank with the highest relative closeness score of 0.724.
5
Conclusion and Future Work
In this study, we consider the problem of selecting an appropriate railway transportation
hubusingtheMCDMmethod.Designingatransportationnetworkinvolvesmanyfactors.
TOPSIS is selected to handle this problem. A case study of the sky train, Red Line, in
Thailand is presented. There are 10 alternatives on the Red Line to be considered as a
hub. Ten important factors are selected as criteria for selecting the appropriate hub for the

Location Selection of Rail Transportation Hub Using TOPSIS Method
259
Table 1. Evaluated value matrix.
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
Station 1
9
6.33
3
9
9
9
3
5
9
5
Station 2
1
3
1
3
3
5
3
5
3
5
Station 3
3
5
3
3
5
3
9
7
3
5
Station 4
3
3.83
7
3
5
3
7
7
3
5
Station 5
1
7
3
5
3
3
5
7
3
5
Station 6
5
6.83
9
7
9
1
7
7
3
5
Station 7
1
4.59
1
7
3
3
5
7
7
5
Station 8
9
4.83
3
9
9
7
7
5
3
5
Station 9
3
1.16
1
1
1
1
3
3
3
3
Station 10
9
6
5
9
3
3
3
3
7
5
ROC weight
0.293
0.193
0.143
0.109
0.085
0.065
0.048
0.033
0.021
0.010
Table 2. Weighted decision matrix.
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
Station 1
0.521
0.388
0.215
0.453
0.495
0.633
0.169
0.272
0.579
0.327
Station 2
0.058
0.184
0.072
0.151
0.165
0.352
0.169
0.272
0.193
0.327
Station 3
0.174
0.307
0.215
0.151
0.275
0.211
0.051
0.381
0.193
0.327
Station 4
0.174
0.235
0.503
0.151
0.275
0.211
0.395
0.381
0.193
0.327
Station 5
0.058
0.429
0.215
0.252
0.165
0.211
0.282
0.381
0.193
0.327
Station 6
0.290
0.419
0.646
0.353
0.495
0.070
0.395
0.381
0.193
0.327
Station 7
0.058
0.281
0.072
0.353
0.165
0.211
0.282
0.381
0.450
0.327
Station 8
0.521
0.296
0.215
0.453
0.495
0.493
0.395
0.272
0.193
0.327
Station 9
0.174
0.071
0.072
0.050
0.055
0.071
0.169
0.163
0.193
0.196
Station 10
0.521
0.368
0.359
0.453
0.165
0.211
0.169
0.163
0.450
0.327
railway line, which include the physical characteristics of the train station, the number
of passengers, the available transportation modes and transits around the station, and the
number of government ofﬁces and business centers around the station. All factors are
assigned important weighted based on the railway expertise. Then, the decision matrix,
which consists of the attribute value with respect to the particular alternative and criteria
is created. Then, the decision matrix is normalized, and the positive and negative ideal
solutions of each criterion are determined. The distances from the positive and negative
ideal values and negative ideal values are calculated, and the relative closeness of each
alternative is obtained. Finally, the best alternative for selecting a railway transportation

260
K. Sripathomswat et al.
Table 3. Normalized weighted decision matrix.
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
Station 1
0.153
0.075
0.031
0.049
0.042
0.041
0.008
0.009
0.012
0.003
Station 2
0.017
0.036
0.010
0.017
0.014
0.023
0.008
0.009
0.004
0.003
Station 3
0.051
0.059
0.031
0.017
0.023
0.014
0.024
0.013
0.004
0.003
Station 4
0.051
0.045
0.072
0.017
0.023
0.014
0.019
0.013
0.004
0.003
Station 5
0.017
0.083
0.031
0.028
0.014
0.014
0.014
0.013
0.004
0.003
Station 6
0.085
0.081
0.092
0.038
0.042
0.005
0.019
0.013
0.004
0.003
Station 7
0.017
0.054
0.010
0.038
0.014
0.014
0.014
0.013
0.009
0.003
Station 8
0.153
0.057
0.031
0.049
0.042
0.032
0.019
0.009
0.004
0.003
Station 9
0.051
0.014
0.010
0.006
0.005
0.005
0.008
0.005
0.004
0.002
Station 10
0.153
0.071
0.051
0.049
0.014
0.014
0.008
0.005
0.009
0.003
Table 4. Positive ideal values and negative ideal values.
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10
S+
0.152
0.082
0.092
0.049
0.042
0.041
0.021
0.012
0.012
0.003
S−
0.017
0.013
0.010
0.005
0.004
0.004
0.008
0.005
0.004
0.002
Table 5. Relative closeness and ranking of alternatives.
ST1
ST2
ST3
ST4
ST5
ST6
ST7
ST8
ST9
ST10
C+
0.720
0.156
0.340
0.404
0.330
0.633
0.247
0.698
0.171
0.724
Ranking
2
10
6
5
7
4
8
3
9
1
hub is identiﬁed as the highest rank of relative closeness. The results can be used as a
guideline for hub location planning.
In this study, we consider the problem based on the viewpoint of operators. For future
work, we plan to consider the problem into 3 facets: customers, operators, and providers.
The MODM methods will be applied for solving the problem with 3 objectives based
on 3 stakeholders who involve in the railway transportation system. So, we can have a
practical solution from a broader point of view.
Acknowledgments. This research was funded by National Science, Research and Innovation
Fund (NSRF), and King Mongkut’s University of Technology North Bangkok with Contract no.
KMUTNB-FF-66-67.

Location Selection of Rail Transportation Hub Using TOPSIS Method
261
References
1. Rodrigue, J.: Structural Components of Transport Networks. The Geography of Trans-
portation System, Dept. of Global Studies and Geography, Hofstra University, New
York (2023). https://transportgeography.org/contents/chapter2/geography-of-transportation-
networks/transport-network-structural-components/
2. Campbell, J.F.: Integer programming formulations of discrete hub location problems. Eur. J.
Oper. Res. 72, 387–405 (1994)
3. Hwang, Y.H., Lee, Y.H.: Uncapacitated single allocation p-hub maximal covering problem.
Comput. Ind. Eng. 63, 382–389 (2012)
4. Peker, M., Kara, B.: The p-hub maximal covering problem and extensions for gradual decay
functions. Omega 54, 158–172 (2015)
5. Farahani, R.Z., Hekmatfar, M., Arabani, A.B., Nikbakhsh, E.: Hub location problem: a review
of models, classiﬁcation, solution techniques, and applications. Comput. Ind. Eng. 64, 1096–
1109 (2013)
6. Nielsen, G., Lange, T.: Network Design for Public Transport Success: Theory and Exam-
ples (2007). https://thredbo-conference-series.org/downloads/thredbo10_papers/thredbo10-
themeE-Nielsen-Lange.pdf
7. Wang, L., Chen H., L., X.W.: City rail transit network schemes evaluation based on TOPSIS,
including AHP and entropy. In: Proceedings of the 12th COTA International Conference of
Transportation Professional, August 3–6, pp. 1731–1738. Beijing, China (2012)
8. Zabihi, A., Gharakhani, M., Afshinfar, A.: A multi criteria decision-making model for
selecting hub port for Iranian marine industry. Uncert. Supply Chain Manag. 4, 195–206
(2016)
9. Chen, K., Liao, C., Wu, L.: A selection model to logistic centers based on TOPSIS and MCGP
methods: the case of airline industry. J. Appl. Math. 157, 470128 (2014)
10. Zhao, L., Li, H., Li, M., Sun, Y., Hu, Q., Mao, S.: Location selection of intra-city distribution
hubs in the metro-integrated logistics system. Tunnel. Undergr. Space Technol. 80, 246–256
(2018)
11. Mardani, A., Jusoh, A., Nor, K., Khalifah, Z., Zakwan, N., Valipour, A.: Multiple criteria
decision-making techniques and their applications: a review of the literature from 2000 to
2014. Econ. Res. Ekonomska Istraživanja 28(1), 516–571 (2015)
12. Chauhan, A., Vaish, R.: Magnetic material selection using multiple attribute decision making
approach. Mater. Des. 36, 1–5 (2012)
13. Zavadskas, E., Skibniewski, M., Antucheviciene, J.: Performance analysis of civil engineering
journals based on the web of science database. Arch. Civil Mech. Eng. 14, 519–527 (2014)
14. Pongprasert, P., Kubota, H.: TOD residents’ attitudes toward walking to transit station: a case
study of transit-oriented developments (TODs) in Bangkok. Thailand. J. Mod. Transp. 27,
39–51 (2019)

Developing a Transaction System in Blockchain
Afsana Nur Meem1, Lamya Ishrat Nodi1, Efte Kharul Islam1, Minhazul Amin Tomal1,
Ahmed Wasif Reza1(B), and Mohammad Shamsul Areﬁn2,3(B)
1 Department of CSE, East West University, Dhaka, Bangladesh
wasif@ewubd.edu
2 Department of CSE, Daffodil International University, Dhaka, Bangladesh
sarefin@cuet.ac.bd
3 Department of CSE, Chittagong University of Engineering and Technology, Chattogram,
Bangladesh
Abstract. Blockchain technology is advancing quickly and does not appear to be
slowing down. Many things that looked inconceivable in the last few decades—
like excessive transaction costs, double spending, net fraud, recovering lost data,
etc.—turned out to be true. But due to blockchain technology, all of this may
now be avoided. Every block on the blockchain includes some information and
the cryptographic hash of the previous block. This system is reliable. At present,
various types of transactions are done using various types of systems. This system
can be replaced by such. Current online transaction gateways are vulnerable to
hacking, allowing attackers to manipulate the network and cause ﬁnancial loss. As
transactions have to pass through multiple transaction systems, which takes time
and carries the risk of transaction failure. Based on our research, it is possible to
conclude that blockchain is the best technology for cryptocurrencies in commer-
cial transactions, as it enables cryptocurrencies to operate decentralized. This can
reduce risk as well as transaction costs. So, our system would change the security
change the current banking system is some criticism that there are uses that are
not legal and also some community affects the research circle has curiosity in it.
Keywords: Blockchain · Transaction system · Cryptocurrency · Ethereum
1
Introduction
To create the history of any digital asset or currency unchangeable and accessible
“Blockchain” is a well-known technology which is also called DLT (Distributed Ledger
Technology) where cryptographic hashing takes place. It is much faster, better, and more
precise. This technology delivers instantaneous, shareable, and completely transparent
data that is recorded on blockchain networks and can only be viewed by the network
users who have permission for that, we can say that the blockchain is great for providing
such information [1]. Accounts, orders, payments, productions, and among other things
can be handled by the blockchain network. Moreover, because all members have a single
understanding of the truth, you may view a transaction from beginning to end, giving
you a ﬁrm conviction as well as additional beneﬁts and opportunities.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 262–276, 2024.
https://doi.org/10.1007/978-3-031-50158-6_27

Developing a Transaction System in Blockchain
263
Blockchain relies on cryptography to secure all data and value storage and ensure
that transactions are conducted safely. Many different academic ﬁelds have investigated
blockchain technology. For example, a number of academics have looked at the underly-
ing technologies of the blockchain, such as distributed storage, peer-to-peer networking,
cryptography, smart contracts, and consensus methods.
Several academics, for example, have looked at the underlying technologies, such as
cloud databases, peer-to-peer connectivity, cryptography, crypto algorithms, and mutual
authentication. Bitcoin is a means of trade, a reserve of value, and a measurement unit
of cryptocurrency. Bitcoin was originally developed using a blockchain. It was not until
2014 that people understood the beneﬁts of blockchain, which extends beyond cryp-
tocurrencies [2]. Bitcoin is a digital currency (a form of payment), but it can also be
considered a speculation commodity (how much is it valued). It was created in 2009 and
is generally acknowledged as the ﬁrst digital asset. Crypto assets, or digital assets, are
digital representations of value accessible by encryption and blockchain.
Ethereum may be a blockchain-based computing platform that enables developers
to develop and deploy autonomous applications. Systems that are not being run by one
authority will be able to produce a distributed application during which the participants
square measure the decision-making authority. The awareness of blockchain and cryp-
tocurrencies has recently expanded exponentially, though in various directions according
to the viewpoints of the users. Several people became users and investors in cryptocur-
rencies such as Bitcoin and Ethereum, which have opened the way for the institution of
tiny and medium companies [3].
Thedrawbacksofcurrentsystemsareaddressedbytheblockchaintransactionsystem
we offer in this article, which is built on Python. Despite the fact that there have been
a number of studies in this ﬁeld in recent years, none of them, as far as we are aware,
have used Python to construct a blockchain transaction system. Our technology ﬁlls this
knowledgegapandoffersapracticalandeffectivewaytoexecuteblockchaintransactions
in Python.
Moreover, we aim to build a transaction system in blockchain that follows the power-
efﬁcient algorithms of blockchain which is proof of stake. And to build a transaction
system to transact among the users of the system with the help of a bank (which is the
source of cryptocurrency in our system). To reach our goal, we need to be speciﬁc with
some actions like building a system that’s going to be more power efﬁcient, we need to
follow the proof of stake algorithm. And if we speak in detail then let’s see some points
below there:
1. We ﬁrst need to deﬁne some modules to create the system in Python.
2. We must deﬁne our database as we cannot develop this through the local database.
The ﬂask could be the best option for this.
3. In the code, we will create the log-in, register, and log-out options (just to make it
look smarter).
4. We will create the ﬁrst block after that, and to get the valid transaction, we must input
a line of codes to test if the transaction hash of the previous user can be the new hash
of the present user.
5. We will deﬁne the blockchain on our app page.

264
A. N. Meem et al.
6. We will create a method on the app page that will make the transaction available for
different users and also create a buy page so that a user can buy Ethereum from the
bank through our system.
7. And all procedures will be done under the proof-of-stake algorithm, to make our
system an example of Green IT.
The key contributions of this paper are as follows:
1. Accomplished the limitation of existing paper which could not implement any
blockchain transaction system using Python.
2. Supports green IT as it has been built based on a power-efﬁcient algorithm.
3. By providing valid transactions, our system proved its authenticity in the ﬁeld of
Information Technology.
The body of the paper is structured as follows: Sect. 1 introduces this research study,
while Sect. 2 provides a brief assessment of related publications on the Blockchain
transaction system, Sect. 3 provides a transaction model, ﬂow diagram, system diagram,
algorithms, and calculation, Sect. 4 explains about the comparison of related works with
the proposed model, implementation and ﬁnally, Sect. 5 concludes with the limitations
of our research.
2
Related Work
This section focuses on existing and related blockchain transaction work. The authors
have spent most of their previous publications researching how to create a transaction
mechanism.
In [4] the two most pioneering Ethereum clients are Geth and Parity. In this study,
we decided to explore Ethereum’s performance under these two different clients.
Geth is one of the ﬁrst Ethereum clients (refer to Table 1) to be implemented in the
Go programming language. Geth is easy to build, and it has a mining option on its own.
Parity is an Ethereum newer client with an emphasis on efﬁciency. Recently, this
client has attracted the attention of many Ethereum decentralized developers due to the
faster sync process.
This document [5] goes into depth about the real-world use scenario, the idea, the
ﬁnal implementation, and the outcomes. It intends to serve as a guide for others by
highlighting potential challenges and opportunities when implementing a blockchain in
a sector other than ﬁnancial transactions.
This paper does not develop the blockchain system or the desired transaction mech-
anism. This paper has guidelines, it has the previous implementation records and some
very nice designs but does not give us the system and we know that for more efﬁciency
we need a system. We can assume and calculate many things by using the system, but
with only a design of a transaction mechanism, it is quite hard to think of how the system
will work. A transaction system is a sensitive one because there is a matter of value, and
we know that money is valuable and important to us. A secured system is all we need
to build a proper transaction system and without security, the system will be a clueless
one. Additionally, this paper used to scratch, whereas our system and paper used Python,
which is considerably simpler and intelligible and meets this paper’s limitations.

Developing a Transaction System in Blockchain
265
Table 1. Ethereum clients
Client
Language
Developers
Go-Ethereum (Geth)
Go
Ethereum foundation
Parity
Rust
Ethcore
Cpp-Ethereum
C++
Ethereum foundation
Ethereumjs-lib
Javascript
Ethereum foundation
Ethereum
Java
ether.camp
Ruby-Ethereum
Ruby
Jan Xie
EthereumH
Haskell
BlockApp
In [6], they begin with a review of blockchain technology, including fundamental
ideas, operations, advantages, and applications. Consensual methods are then described
brieﬂy, and the Proof-of-Work (PoW) mechanism and its existing difﬁculties are
explained. Following that, they provide an important emergent Proof-of-Stake (PoS)
consensus block.
This paper has some serious comparisons between proof-of-stake and proof-of-work
algorithms. These are two different types of blockchain algorithms that are used in
different types of cryptocurrencies. By using the proof of work algorithm huge amounts
of energy are being wasted all over the world and less amount of power is being wasted
in the proof of stake algorithm. And it explains how the blockchain’s algorithms work.
After reading the paper we may infer that it encourages us to work on the blockchain’s
effective algorithm. Utilizing their research, one may build a system by choosing which
algorithm they will work on. But there is still no implementation of the system that is
important for any kind of algorithm and design. And we would also love to say that our
papers also complete the limitation of this paper.
In numerous areas, our approach is more effective than the one outlined in the
reference [7]. First off, we used Python, a programming language renowned for its
effectiveness and user-friendliness. Second, our method calculates the transaction data
more efﬁciently, leading to quicker processing times. Last but not least, we thoroughly
evaluated our system using a variety of transaction instances to make sure it is both
trustworthy and efﬁcient.
The limitations of these papers and articles are that they provide us with a detailed
model for developing a blockchain or transaction system and also provide us with some
comparison between proof of stake and proof of work, but they do not provide any
transaction systems that we have overcome in our paper. Again, we have seen that the
transaction system algorithm and system are absent. By observing a system, we can
understand how the money will be transacted and also how secure the system is. We
have a proper transaction system in the blockchain that we have developed. The name
of our system is Ethereum (ETH) Cryptocurrency.

266
A. N. Meem et al.
3
Materials and Methods
To build a transaction system on the blockchain, we have chosen to follow the proof-of-
stake algorithm over proof-of-work, as we know that it is much more eco-friendly and
less power-consuming. For materials, we have used.
• Python and JavaScript for implementing the whole transaction system. We tried to
keep it look more convenient. That is why we did not go for scratch or solidity.
• CSS and HTML for designing the system and the page. We have used SCSS
particularly.
• The Django framework we have used to complete this system makes it so much
easier to use and understand at the same time.
Proof-of-stake (PoS) can be a more energy-efﬁcient replacement for PoW. The miner
does not need to spend a huge amount of computational power to solve the theoret-
ical challenge using this consensus methodology. Instead, participation in the block
generation stage is required to have a sufﬁcient interest in the system.
Fig. 1. The transaction model for blockchain.
Consider Fig. 1: You decide to pay your friend George in Ethereum for a pizza that
you ordered from him. You produce and publish an entry on the Ethereum blockchain
when you send George money in Ethereum. The network’s other computers will verify
that you have not already sent the information that represents Ethereum to another user,
stopping you from using a digital currency that has already been used. Each computer
in the Ethereum network maintains a log of all network transactions and keeps tabs on
each account’s balance [8].
For measuring truth and allowing both consumers and producers to certify that their
data are accurate and have not been modiﬁed blockchain offers the framework for it.
For example, if a blockchain has 10 blocks, a block numbered 10 provides the hash

Developing a Transaction System in Blockchain
267
of the previous subsequent block, and the contents of the current block are utilized to
create a brand-new block [9]. As a result, all of the blocks in the existing chain are
linked and connected. Even the transactions are connected. Now, a simple modiﬁcation
of any transaction will drastically alter the hash [10]. If someone intends to modify
any information, he must change all of the prior block’s hash data, which is considered
an enormously difﬁcult undertaking given the amount of work that needs to be done.
Furthermore, once a miner produces a block, it is acknowledged by other network users
[11]. As a result, any type of modiﬁcation or manipulation will be identiﬁed by the
network. As a result, the blockchain is extremely tamperproof and is considered an
immutable distributed network.
Fig. 2. Transaction ﬂow diagram.
In Fig. 2, we are trying to show the exact ﬂow of how a block or coin is being made
and transferred to our user’s account. First, the header of the block is generated by giving
its required information. That is the time of mining. Exactly after that, a cache using
sha256 computes a 16 MB operation and passes it to the next step to store the dataset it
collects storage. All this process continues according to the sha256 algorithm [12]. On
the other hand, none gets selected and as per the algorithm it increments and transforms
into random slices. When the threshold gets bigger than the result it generates a new
block.

268
A. N. Meem et al.
A transactional ﬂow is a form of data ﬂow diagram that illustrates the process of
transactions within an organization or department. Transactional ﬂow diagrams are used
to provide an overview of the process and allow for further expansion.
Fig. 3. System diagram.
In Fig. 3, we are trying to show the exact ﬂow of our system. When a user logs
in to our system by providing valid credentials, he/she directly transfers into the dash-
board/homepage. But if he/she fails to provide the required info, he/she directly returns
to the register/signup page. And after a successful log-in, he/she can go to any page con-
sisting of the transaction page and buy page just by logging in once. Users can transact
or buy just by being in the system with sufﬁcient amounts. And the user can log out of
the system on any page of the system.

Developing a Transaction System in Blockchain
269
A system diagram is a basic and exceptionally high description of an existing or to-
be-built system. It is a basic diagram that can be created interactively in a short period.
It can support a team in obtaining clear, accurate, and shared knowledge of a system.
To implement our proposed model, these below steps are needed. First the procedures
for implementing our entire model:
Procedure:
1: Log in
2: Create an account
3: Transact with another user
4: Buy coins
5: Provide valid credentials
6: Creating hash
7: Creating a unique block
8: Add and delete blocks
To solve our proposed method and automate the solution, we have included the
following algorithms.
Algorithm 1: wrap to deﬁne if the user is currently logged in from session // called when a
user tries to log in
Inputs: 1: login credentials given ()
2: session.check password()
3: if (credentials == right) {
4:
show homepage();
5: } else {
6:
return to sign up page;}
Output: Authenticity of the user is examined in this section
Algorithm 2: Registration page function // called when a new user tries to create an account
Inputs: 1: user info is given ()
2: session.check info()
3: if (info == valid)
4: user created (login page)
a. Check the validity of the user information
b. If the information is valid, create a new user account and direct the user to the login
page
c. If the information is invalid, prompt the user to input valid information
5: else:
6: input info as per command (register page)
Output: deﬁning registration/sign-up page

270
A. N. Meem et al.
Algorithm 3: Transaction page // called when a user tries to transact with another user of
this very same system
Inputs: 1: request transaction
2: session.get user_info
3: check_balance
4: if (balance = valid)
5: coins transfer to the user ()
a. Transfer coins to the other user
b. Update the transaction history
6: else
7: not enough/invalid amount
8: return to transaction page ()
Output: The transaction successfully executes in this section
Algorithm 4: Buy page // called when a user tries to buy coins from the bank through our
system
Inputs: 1: request buy (Ethereum coin)
2: session.get user info
3: check_validity of user
4: if (user = valid && amount == sufﬁcient)
a. Add coins to user’s account
b. Display success message
5: coins added to users account ()
6: else
7: insufﬁcient balance/invalid info
8: return to homepage ()
Output: Successfully coin purchased from the bank in this section
Algorithm 5: Home page // called when a user logged in providing valid credentials
Inputs: 1: session.log_in(user_id, password)
2: user_info = session.check_user_info()
3: if user_info[‘coin_balance’] > 0:
4: display_coin_balance(user_info[‘coin_balance’])
5: else:
6: display_username(user_info[‘username’])
Output: dashboard or homepage is shown to the user right after providing valid credentials
Algorithm 6: creating hash // called up initially when a user tries to buy or transact coins
(continued)

Developing a Transaction System in Blockchain
271
(continued)
Inputs: 1: request transaction
2: creating block(sha256)
3: checking_activity
3: if (previous hash == found)
5: create new hash ()
4: else
5: prefer creating unique block ()
Outputs: Hash creation occurs when a user tries to buy or transact a coin by looping each
argument
Algorithm 7: creating unique block // called when a different user tries to transact
Inputs: 1: request transaction
2: creating block(sha256)
3: checking_activity
4: self.previous_hash = previous_hash
5: self.nonce = nonce
6: updating hash
a. Set self.previous_hash to the previous hash value
b. Set self.nonce to a randomly generated number and update the hash
7: return blockchain ()
Output: unique block creates and hash updates in this session
Algorithm 8: Add/delete blocks
Inputs: 1: request transaction(bank)
2: checking_activity
3: if (previous block == found || unique block = prepared
4: new block added
3: else
4: delete the corrupted block ()
Output: To get the valid transaction this section makes your new block and delete for the
same purpose as well
Ethereum employs a particular elliptic curve and a set of theoretical constants
described in the secp256k1 standards speciﬁed by the US National Institute of Stan-
dards and Technology (NIST). The secp256k1 curve is deﬁned by the elliptic curve
produced by the following function:
y2 = (x3 + 7) over (F p)
(1)
or:
y2 mod p = (x3 + 7) mod p
(2)
The mod p (modulo prime number p) which indicates that the above curve is over a
ﬁnite ﬁeld of prime order p, also written as F p, where p = 2256 – 232 – 29 – 28 – 27 –
26 – 24 – 1, that means this is a very huge prime number.

272
A. N. Meem et al.
Fig. 4. Visualization of an elliptic curve.
Over a ﬁnite ﬁeld of prime order rather than over the real numbers the curve is
deﬁned, it looks like a pattern of dots scattered in two dimensions and for this reason,
it is quite difﬁcult to visualize. However, math is identical to that of an elliptical curve
with real numbers. As an example, Fig. 4 shows the same elliptic curve over a much
smaller ﬁnite ﬁeld of prime order 17, showing a pattern of dots on a grid. The secp256k1
Ethereum elliptic curve can be thought of as a much more complex pattern of dots on
an unfathomably large grid.
It shows us our transaction method. How to block is being made and the transaction
is happening.
4
Result and Discussion
To compare the description, aims, and limitations, we have prepared a comparison table
(Table 2) to classify the categories.
Here, we have discussed the descriptions, aims, and limitations of related papers and
compared them with our paper. From the above table, we have seen that no paper has done
a transaction system where we build a proper system. However, some of the papers do not
have proper calculations and systems which we have overcome in our paper. Our main
goal was to implement a proper transaction system with no limitations. For the system,
we focus on implementation which will support Green IT and minimize transaction risk.
An activity started by an externally owned account, or an account maintained by a person
rather than a contract is referred to as an Ethereum transaction.
We carried out multiple transactions and compared them to those of existing systems
to validate the proposed transaction system. We discovered that our approach effec-
tively and efﬁciently reduced transaction risks. Our suggested solution also addressed

Developing a Transaction System in Blockchain
273
Table 2. A comparison of related works.
References
Concept
Aims
Limitations
[4]
Most of the pioneer
clients are Geth and
Parity
Faster sync process
Does not develop any
system
[5]
Real-world use case
Serving as a guide to
others by showing
potential
Does not give a proper
efﬁcient calculation
[6]
Provide an overview of
blockchain technology
Utilizing their research
Still no implementation
of the system
[7]
Provide a detailed
model for the
blockchain transaction
system
Implement a system
All of the systems will
not work as they give
Proposed model
Build a transaction
system
Minimizing the markets
transaction risk through
our system
We have no limitations
and also, we have a
proper system
the drawbacks listed in Table 2, such as poor implementation and inaccurate computa-
tions. Overall, our technology has shown potential for offering a trustworthy and secure
platform for transactions.
Here, we have shown below a ﬂow diagram that gives a piece of basic information
about our Ethereum transaction and how it works.
Fig. 5. Ethereum transaction.

274
A. N. Meem et al.
Figure 5 shows us our transaction method. How the block is being made and what
the transaction is happening.
From the above ﬂow diagram, we can see that 100 Ethereum is being transferred to
the user from the bank. And
(000011a1410e75a409b404c0f5081dcfac9e15b8d77b1ba5c3d9f46303c7aca4) is trans-
action ID.
Ethereum deﬁnes itself as “the world’s programmatic blockchain”, offering its ser-
vices as an electronic, programmable network on which anybody may construct coins
and autonomous applications [13]. Unlike Bitcoin, which has a maximum circulation
of 21 million coins, the amount of ETH that can be produced is unlimited, although the
time it takes to process each ETH block restricts the number of ethers that can be gener-
ated each year [14]. Another contrast between Ethereum and Bitcoin is how the network
handles transaction management fees. Contemporary technology, as are modern issues
[15]. A signiﬁcant amount of electricity is required to use Blockchain technology. No
one is taking measures to prevent this problem and make it more energy efﬁcient due to
new technologies [16].
So, to overcome this problem, we have built an algorithm and also a system where
people can transact their cryptocurrency using our system. It is not only a secure system
for transactions but also an energy-efﬁcient system where a huge amount of power can be
saved, and it will be much more beneﬁcial for our modern environment. Our algorithm
is being developed using the Python programming language instead of a blockchain.
Questions can arise that the system is not very secure, but we have proven that by using
a programming language like Python, we can save an enormous amount of power and
also can do our daily cryptocurrency transactions.
5
Disadvantages of Using Blockchain
While blockchain technology has many advantages, it also has certain disadvantages.
Scalability of the blockchain is one of the main issues. The amount of computational
power needed to validate transactions grows together with the network’s node count.
It can result in longer transaction times and greater costs, which makes it less useful
for usage in some applications. The energy use of blockchain is another problem. The
processing power is very high if we want to mine new coins, which are then used to
validate the transactions and then it will add some new blocks into the chain. This has
raised questions about how blockchain may affect the environment, especially in light
of the rising need for energy-efﬁcient products.
6
Costs of Using Blockchain
There are a number of expenses related to blockchain technology. First off, because the
infrastructure takes a lot of computer power and energy, the initial cost of building it
up might be substantial. However, especially for large-scale deployments, the cost of
maintaining and upgrading the system might be signiﬁcant. A further expense is the
transaction fee, which is paid to the network nodes in order for the transaction to be
veriﬁed and recorded on the blockchain. Depending on network congestion, this charge
can change and can be costly, especially for high-value transactions.

Developing a Transaction System in Blockchain
275
7
Conclusion
Our sole purpose of this paper is to change the current banking system securely. The
system enables transactions that will be done online between two people of parties and
it will be based on cryptographic proof and also the transaction will not rely on and trust
any third party.
In the present world, energy is being wasted enormously for various kinds of reasons.
We can set the power consumption using blockchain at the top of the list. For this, power
is wasted more than a whole country like Argentina annually. To make our world more
advanced we cannot stop the use of blockchain but replace it with a more energy-efﬁcient
and secure system. Our system is fully matched with that kind of system which we need
to overcome the power consumption. Nowadays, people transact their money all over
the world using cryptocurrency. Behind all of the crypto, there is one algorithm called
the blockchain. Using this, the currencies are transacted in a very secure way. Sooner
all the banks will move on to cryptocurrency too. But there is a huge problem with it,
which is that it uses a huge amount of power, which is not good from the perspective of
green IT. So, keeping these points in mind, we have created a transaction system with
lower power consumption.
Our suggested approach makes use of green IT concepts to cut down on energy use
and environmental effect. We have created a system that is not only efﬁcient at managing
blockchain transactions but also sustainable by adopting effective algorithms and using
Python as a programming language. Moreover, we have addressed the shortcomings of
current models by offering a thorough blockchain data collecting and administration
system that may successfully reduce transaction risks. The development of a reliable
and effective blockchain transaction system that may lessen the environmental impact
of this developing technology is advanced signiﬁcantly by our suggested approach.
References
1. Bassey, E.: Elice Bassey, 1 Sept 2022. Retrieved 12 Sept 2022
2. Kelley, K.: What is Ethereum? Explained with Features and Applications. Simplilearn.com,
20 July 2022. Retrieved 12 Sept 2022
3. Bentov, I., Lee, C., Mizrahi, A., Rosenfeld, M.: Proof of activity: extending bitcoin’s proof of
work via proof of stake [extended abstract]y. ACM SIGMETRICS Perform. Eval. Rev. 42(3),
34–37 (2014)
4. Rouhani, S.: Ethereum Transaction In Private Blockchain, 04 Feb 2021
5. Knirsch, F.: Implementing a blockchain from scratch: why, how, and what we learned.
EURASIP J. Inform. Secur. (2019). Retrieved 12 Sept 2022
6. Cong, T., Dinh, T.: Proof-of-stake consensus mechanisms for future blockchain networks:
fundamentals, applications and opportunities. IEEE Access 7, 112411–112434 (2019). https://
doi.org/10.1109/ACCESS.2019.2933326
7. Hu, W.: A blockchain-based secure transaction model for distributed energy in Industrial
Internet of things. Alex. Eng. J. 60(1):491–500 (2021)
8. Erratum. Acad. Emerg. Med. 26(4), 462–462 (2019)
9. Shrier, D., Wu, W., Pentland, A.: Blockchain and Infrastructure (Identity Data Security), vol
1(3). Cambridge, MA, USA (2016)

276
A. N. Meem et al.
10. Karandikar, N., Chakravorty, A., Rong, C.: Blockchain based transaction system with fungible
and non-fungible tokens for a community-based energy infrastructure. Sensors 21(11), 3822
(2021)
11. Feng, Q., He, D., Zeadally, S., Khan, M.K., Kumar, N.: A survey on privacy protection in
blockchain system. J. Netw. Comput. Appl. 126, 45–58 (2019)
12. Peter, H., Moser, A.: Blockchain-applications in banking & payment transactions: Results of
a survey. Eur. Financ. Syst. 141, 141 (2017)
13. Cocco, L., Pinna, A., Marchesi, M.: Banking on blockchain: costs savings thanks to the
blockchain technology. Future Int. 9(3), 25 (2017)
14. Tasatanattakool, P.: Blockchain: challenges and applications. In: IEEE Conference Publica-
tion. IEEE Xplore. Retrieved 17 Sept 2022
15. Treleaven, P., Brown, R.G., Yang, D.: Blockchain technology in ﬁnance. Computer 50(9),
14–17 (2017)
16. Nguyen, Q.K.: Blockchain—a ﬁnancial technology for future sustainable development. In:
2016 3rd International Conference on Green Technology and Sustainable Development
(GTSD), pp. 51–54. IEEE (2016)

Using the Phi-Function Technique
for the Optimized Virtual Localization Problem
Sergiy Plankovskyy1
, Yevgen Tsegelnyk1(B)
, Tetyana Romanova2,3
,
Oleksandr Pankratov2
, Igor Litvinchev4
, and Volodymyr Kombarov1
1 O. M. Beketov National University of Urban Economy in Kharkiv, 17 Marshala Bazhanova
Street, Kharkiv 61002, Ukraine
y.tsegelnyk@kname.edu.ua
2 Anatolii Pidhornyi Institute of Mechanical Engineering, Problems of the National Academy of
Sciences of Ukraine, 2/10 Pozharskogo Str., Kharkiv 61046, Ukraine
3 University of Leeds, Maurice Keyworth Building, Leeds LS2 9JT, UK
4 Faculty of Mechanical and Electrical Engineering, Nuevo Leon State University, 66450
Monterrey, NL, Mexico
Abstract. Optimized virtual localization arises in manufacturing parts from
“nearly shaped” workpieces using computer numerical control (CNC) machin-
ing. To reduce the material consumption and duration of machining processes, a
part (a polygonal object) must be placed completely in a workpiece (a polygo-
nal domain) maximizing the distance between the part and the boundary of the
workpiece. Using the phi-function technique a continuous nonlinear model is con-
structed for this non-standard packing problem and a corresponding solution strat-
egy is proposed. Numerical examples are provided to demonstrate the efﬁciency
of the proposed approach.
Keywords: Polygonal workpiece · Virtual localization · Phi-functions · CNC
machining
1
Introduction
Technological processes for obtaining ﬁnished parts from workpieces with minimal
allowances for machining [1, 2], e.g., precision casting, stamping, or additive manu-
facturing (3D printing) [3, 4], are widely used in practice. The use of such workpieces
allows a signiﬁcant reduction in the consumption of material and the duration of machin-
ing operations. Correspondingly, the problem of smart localizing the parts in the work-
pieces arises. This is especially important for high-weighted parts/workpieces and those
having large dimensions or low rigidity [5, 6].
Traditionally, during machining, localization is carried out with the help of special
jigs that not only ﬁx the workpiece but also ensure its certain orientation in the machine
coordinate system. However, in many cases, especially in the manufacturing of complex
shape large-sized parts, the design, and manufacturing of such jigs require more time and
money than the part manufacturing, and the localization process requires considerable
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 277–287, 2024.
https://doi.org/10.1007/978-3-031-50158-6_28

278
S. Plankovskyy et al.
time and is associated with signiﬁcant difﬁculties for heavy parts. An alternative is
using adaptive machining [7] with virtual localization. In this case, the workpiece is
installed on the machine table with the simpliﬁed jigs, and its orientation in the machine
coordinate system and the shape is determined using built-in measuring systems. Thus,
the idea of adaptive machining with virtual localization is that instead of ﬁxing the
workpiece in a theoretically speciﬁed position, a computer numerical control (CNC)
correction program is used for its current position. This approach is successfully used
in the aerospace industry for the machining of large-sized thin-walled parts [8–10], the
manufacturing and restoration of gas turbine engine blades [11–13], the nozzles of liquid
rocket engines [14, 15], as well as in the manufacturing of complex shape large-sized
parts, such as paddles propellers, hydraulic and steam turbine blades, etc.
The solution of the localization problem in almost all studies devoted to the issues of
virtual localization is conducted in two stages. At the ﬁrst stage, preliminary localization
of the CAD model inside the workpiece is carried out. The starting position of the CAD
model in the majority of papers is determined by the condition of the alignment of the
centers of gravity of the thin shells that coincide with the surfaces of the workpiece and
the CAD model of the part [16–18]. The starting localization option proposed in [19] is
more accurate in the case when the machining allowance is approximately the same over
the entire surface of the part. In this case, it is suggested to combine not only the centers
of gravity of the shells, but also their main central inertia axes. At the second stage of
solving the virtual localization problem, accurate location is performed by translating
and rotating the CAD model relative to its initial position. For this, in the vast majority
of papers, the iterative Iterative Closest Point (ICP) algorithm proposed in [20] is used.
The criterion for the optimal mutual placement of geometric objects in this method is the
minimization of the sum of squared distances between their elements (for the problem
of virtual localization – distances between points from the scanned cloud and curves and
surfaces of the CAD model).
Some modiﬁcations of the ICP algorithm use global control points calculated as
average values according to coordinates [21], or special points obtained on the basis of
Bearing Angle Images [22], the use of which is not requires setting initial approximations
for the rotation matrix and the displacement vector, thereby allowing to avoid hitting
a local minimum, requires fewer iterations for the alignment of point clouds, and is
performed faster compared to the basic algorithm.
Besides ICP algorithms, several other methods have been proposed. In [23], a method
based on comparisons of obtained surfaces with nominal data or for direct comparison
of different scanned surfaces using an algorithm based on extended Gaussian curvature
and a method of comparing characteristics based on aggregate normal orientation was
studied.Inpaper[24],theproblemofmatchingthree-dimensionalobjectswasconsidered
from the point of view of a nonlinear polynomial equations system solutions, which
can be solved using an algorithm, where Gaussian and average curvature were used as
evaluation criteria. Nevertheless, for today, the ICP algorithm is essentially the basic
standard for the problems of combining geometric objects. The disadvantage of ICP
methods in relation to the virtual localization problem is the application of the criterion
of the average value of the square of the distance minimization between the cloud
of measurement points and the project surface. With such a criterion, the algorithm

Using the Phi-Function Technique for the Optimized
279
is not sensitive to where exactly the point from the transformed cloud is located in
relation to the CAD surface – inside or outside the CAD model. Obviously, the situation
cannot be changed by using other norms for determining the distance, insensitive to the
mutual location of points, for example, the Manhattan norm, which was used in some
papers devoted to the modiﬁcation of the ICP algorithm for the purpose of accelerating
convergence [25]. This applies to other similar papers, for example, [26, 27], where a
modiﬁcation of the distance function from the point to the surface was introduced and
the properties of the complex error of the surface were studied. From the point of view
of the virtual localization problem, it may be promising to use the method of Lagrange
multipliers used in [28] when considering the problem of localization a free-form surface.
In addition, we should highlight the works [29, 30] in which it is proposed to minimize
the functional U = max[ρi] + UP, where ρi is the distance between the point from
the cloud obtained by measurement to the surface of the CAD model; UP is a penalty
function. When the coefﬁcients of the penalty function are selected appropriately, it
acquires properties that allow to “push” points from the cloud outside the CAD model
area. The papers [29, 30] demonstrated the effectiveness of using this approach in the
virtual localization problem of complex shape parts.
At the same time, in the known papers, a reasonable approach to the formation of
penalty functions, which are used when recording the objective function, has not been
developed.Thus,thevirtuallocalizationproblemofcomplexshapeobjectsonthebasisof
the combined use of a CAD model and a point cloud obtained during the measurement of
theworkpieceremainsadifﬁculttask.Thedevelopmentofnewapproachestosolvingthis
problem is expedient to start with the example of two-dimensional problems. In addition
to reducing computational costs, this approach can be directly applied to solving virtual
localization problems in contour milling [31].
As shown in [31, 32], for computer modeling of the parts contour milling problem on
CNC machines, the representation of geometric models in the form of broken lines can
be successfully used. For this purpose, the ﬂat geometric contours of the workpiece and
the part are speciﬁed by two-dimensional digital arrays of points with a given step. The
position of the workpiece on the machine table is usually determined with the built-in
measuring heads, which are capable of automatically determining the coordinates of the
points of the workpiece contour in the machine coordinate system with an error of up to
0.001 mm. In Fig. 1 shown the contours of part D and workpiece B, which are represented
bydigitalarrayscontainingthecoordinatesoftwosetsofpointsD = {d1, d2, . . . , dn}and
B = {b1, b2, . . . , bm}. The virtual localization problem is aiming at ﬁnding placement
parameters that ensure the minimization of the maximum machining allowance (that is,
the distance between the part and workpiece contours).
Note that even in such a simpliﬁed setting when applying the algorithm based on the
maximization of the Hausdorff distance, in [31] cases of intersection of the contours of
the part and the workpiece were reported. To prevent such errors, virtual localization in
[31] provided for the ﬁnal decision to complete the virtual localization procedure by the
technological engineer.
Thus,thetaskofvirtuallocalizationduringCNCmachiningrequiresthedevelopment
of new algorithms that would enable it to be carried out without errors and in automatic

280
S. Plankovskyy et al.
X
0
Y
d1
b1
b2
b4
b3
d2
d3
b5
d4
b6
B
D
Fig. 1. An example of virtual localization during contour milling: D is the contour of the part, B
is the contour of the workpiece [31]
mode. The paper considers an approach that uses the phi-functions technique [33–35]
to solve the virtual localization problem.
2
Problem Formulation
Let B ⊂R2 be a ﬁxed bounded covex polygonal domain (workpiece) given by their
verticies uk = (x′
k, y′
k) for k = 1, . . . , n, and
B = {(x, y) : ϕk(x, y) ≥0, k = 1, . . . , n},
where ϕk(x, y) = αkx −βky + γk, α2
k + β2
k = 1, αk = (y′
k+1 −y′
k)/Ak, βk = (x′
k+1 −
x′
k)/Ak, γk = −αkx′
k −βky′
k, Ak =

(x′
k+1 −x′
k)2 + (y′
k+1 −y′
k)2, for k = 1, . . . , n,
subject to un+1 = u1.
With each irregular part we accociate its polygonal convex hull D ⊂R2 given by
their verticies ˜vi = (˜xi, ˜yi), i = 1, . . . , m.
The location and orientation of D is deﬁned by a variable vector of its placement
parameters (xd, yd, θ). A translation of D by the vector vd = (xd, yd) and a rotation of
D trough the angle θ ∈[0, 2π) is deﬁned as
D(vd, θ) = {t ∈R2 : t = vd + M (θ)˜t, ∀˜t ∈D},
where M (θ) is a standard rotation matrix.
Thus each point ˜t = (˜x, ˜y) ∈D in the local coordinate system of D is transformed
into point t = (x, y), where x = ˜x · cos θ + ˜y · sin θ + xd, y = −˜x · sin θ + ˜y · cos θ + yd.
Optimized virtual localization problem. Place a given object D completely inside a ﬁxed
polygonal domain (workpiece) B maximizing Euclidean distance between the object
(part) D and the boundary of B.
Denote the variable Euclidean distance between the object D and the boundary of
B by ρ, i.e. ρ = dist(D, B∗) =
min
d∈D,b∈B∗∥d −b∥, B∗= R2/int(B). Therefore, the
placement problem is aiming to search for a vector (xd, yd, θ) maximizing ρ.

Using the Phi-Function Technique for the Optimized
281
3
Mathematical Model
Using the phi-function technique [33–35] the optimized virtual localization problem can
be formulated as the following nonlinear programming model:
max
(vd,θ,ρ)∈W ρ
(1)
subject to
W =

(vd, θ, ρ) ∈R4B∗D(vd, θ) ≥ρ, ρ ≥0

.
(2)
In the model (vd, θ, ρ) = (xd, yd, θ, ρ) is a vector of variables, B∗D(vd, θ) is the
normalized phi-function for objects D(vd, θ) and B∗deﬁned in the following form:
B∗D(vd, θ) =
min
k=1,...,n,i=1,...,m ϕk(xdi, ydi),
ϕk(xdi, ydi) = αkxdi −βkydi + γk,
xdi = ˜xdi · cos θ + ˜ydi · sin θ + xd,
ydi = −˜xdi · sin θ + ˜ydi · cos θ + yd.
The variable ρ can be considered as ρ =
min
i=1,...,m,k=1,...,n ϕk(xdi, ydi).
4
Solution Strategy
The solution algorithm involves three main stages:
Stage 1. Constructing a set of feasible starting points of the problem (1)–(2).
Stage 2. Searching for a local-optimal maximum of the problem (1)–(2) using IPOPT
for each starting point found at Stage 1.
Stage 3. Choosing the best local optimal solution from those found at Stage 2.
The heuristic algorithm to search for feasible starting point of the problem (1)–(2)
involves the following principal steps:
Step 1. Generate point v0
d = (x0
d, y0
d) ∈B randomly.
Step 2. Generate a rotation angle θ0 ∈[0, 2π] randomly.
Step 3. Solve the following nonlinear programming subproblem starting from the point
(x0
d, y0
d, θ0):
max η
(3)
subject to
αk(η(˜xdi · cos θ + ˜ydi · sin θ) + xd) −βk(η(−˜xdi · sin θ + ˜ydi · cos θ) + yd) + γk ≥0,
(4)

282
S. Plankovskyy et al.
for k = 1, . . . , n, i = 1, . . . , m,
0 ≤η ≤1,
(5)
where (vd, θ, η) is a vector of variables.
Step 4. If η∗< 1 is a solution of the problem (3)–(5) then goto Step 1 otherwise (η∗= 1)
take point (v∗
d, θ∗) as a feasible starting point of the problem (1)–(2).
5
Computational Results
In this section numerical results for several problems instances solved by the algo-
rithm are presented in Table 1 and Fig. 2. All experiments were performed using AMD
FX(tm)-6100, 3.30 GHz computer, C++ programming language and the operating sys-
tem Windows 7. The open access solver IPOPT [36] was used for local optimization
under default options.
Fig. 2. Localoptimalplacement:(a)–Example1;(b)–Example2;(c)–Example3;(d)–Example
4; (e) – Example 5; ( f ) – Example 6
6
Conclusions
The numerical results demonstrate that using the phi-functions technique is promising for
solving virtual localization problems. The proposed approach eliminates false solutions
characterized by intersections between the workpiece and part surfaces.
Such errors during localization without the operator’s participation will inevitably
lead to production defects. The algorithm to solve the 2D virtual localization problem
for convex polygonal part and workpiece was proposed. For all problem instances the
solutiontimedidnotexceed2.5s,whichfullymeetstherequirementsoffurtherindustrial
use of the proposed virtual localization method. An interesting direction for the future
research is extending the proposed method to problems where the CAD model of the part
has an arbitrary shape and/or is formed by Boolean operations with simple geometric
shapes. Considering 3D virtual localization problems is also planned in near future.
In the proposed approach large-scale nonlinear optimization problems must be
solved. To simplify these problems various linearizations of the principal model can
be used. In particular, grid approximations of the domains involved in the problem for-
mulation permit reducing approximately the nonlinear continuous packing problem to
a linear mixed integer optimization [37–39].

Using the Phi-Function Technique for the Optimized
283
Table 1. Numerical results
No.
Domain B
Object D
ρ∗
(x∗
d, y∗
d, θ∗)
Time,
s/iteration
1
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 10} =
{(144, 438), (138,
465), (132, 486),
(120, 492), (99,
489), (78, 480), (72,
465), (75, 450), (96,
435), (117, 432)
4.391339
(518.804021,
169.748525,
1.709054)
0.031/1
2
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 10} =
{(144, −438), (138,
−465), (132, −
486), (120, −492),
(99, −489), (78, −
480), (72, −465),
(75, −450), (96, −
435), (117, −432)}
4.245713
(522.010811,
217.167852, −
1.803106)
0.031/1
3
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 17} =
{(217.5, 380),
(207.5, 382.5),
(195, 385), (180,
385), (170, 382.5),
(165, 380), (157.5,
372.5), (155, 367.5),
(153.75, 362.5),
(152.5, 342.5),
(157.5, 332.5),
(162.5, 325), (175,
322.5), (190, 325),
(200, 330), (210,
347.5), (215, 365)}
4.327072
(−93.555847,
590.598979, −
2.311687)
0.078/3
(continued)

284
S. Plankovskyy et al.
Table 1. (continued)
No.
Domain B
Object D
ρ∗
(x∗
d, y∗
d, θ∗)
Time,
s/iteration
4
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 17} =
{(217.5, −380),
(207.5, −382.5),
(195, −385), (180,
−385), (170, −
382.5), (165, −
380), (157.5, −
372.5), (155, −
367.5), (153.75, −
362.5), (152.5, −
342.5), (157.5, −
332.5), (162.5, −
325), (175, −
322.5), (190, −
325), (200, −330),
(210, −347.5),
(215, −365)}
5.587589
(100.605448,
613.501998, −
0.609559)
0.203/6
5
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 10} =
{(33.15, 248.82),
(66.19, 239.94),
(82.08, 222.47),
(79.22, 197.17),
(71.08, 176.73),
(55.85, 166.60),
(40.63, 176.73),
(30.07, 195.89),
(24.11, 221.43),
(27.16, 242.08)}
3.74537
(193.096076,
374.228730, −
3.620032)
0.078/3
6
{(x′
k, y′
k), k =
1, . . . , n = 10} =
{(48.76, 263.12),
(75.70, 252.60),
(81.47, 230.65),
(78.82, 201.42),
(69.55, 178.14),
(52.41, 155.70),
(29.12, 178.14),
(12.77, 200.58),
(9.27, 232.35),
(27.74, 251.55)}
{(xdi, ydi), i =
1, . . . , m = 10} =
{(33.15, −248.82),
(66.19, −239.94),
(82.08, −222.47),
(79.22, −197.17),
(71.08, −176.73),
(55.85, −166.60),
(40.63, −176.73),
(30.07, −195.89),
(24.11, −221.43),
(27.16, −242.08)}
4.448
(151.075074,
23.172399,
3.397443)
0.172/4

Using the Phi-Function Technique for the Optimized
285
Acknowledgements. The research is supported by Volkswagen Foundation (grant #97775), Min-
istry of Education and Science of Ukraine (scientiﬁc research project No. 0121U109639), British
Academy (grant #100072).
References
1. Marini, D., Cunningham, D., Corney, J.R.: Near net shape manufacturing of metal: a review
of approaches and their evolutions. Proc. Inst. Mech. Eng. Part B J. Eng. Manufact. 232(4),
650–669 (2018). https://doi.org/10.1177/0954405417708220
2. Kurin, M.O.: Investigation of the method of hyperbolic ﬂow around the wedge with free cut-
ting. Metalloﬁz. Noveishie Tekhnol. 40(7), 859–876 (2018). https://doi.org/10.15407/mﬁnt.
40.07.0859
3. Riveiro, A., del Val, J., Comesaña, R., et al.: Laser additive manufacturing processes for near
net shape components. In: Gupta, K. (eds.) Near Net Shape Manufacturing Processes. MFMT,
pp. 105–141. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-10579-2_5
4. Kritskiy, D., Pohudina, O., Kovalevskyi, M., et al.: Powder mixtures analysis for laser cladding
using OpenCV library. In: Nechyporuk, M., et al. (eds.) Integrated Computer Technologies in
Mechanical Engineering—2021 (ICTM 2021). LNNS, vol. 367, pp. 924–937 (2022). https://
doi.org/10.1007/978-3-030-94259-5_72
5. Kondratiev, A., Píštˇek, V., Smovziuk, L., et al.: Stress-strain behaviour of reparable composite
panel with step-variable thickness. Polymers 13(21), 3830 (2021). https://doi.org/10.3390/pol
ym13213830
6. Dobrotvorskiy, S., Kononenko, S., Basova, Y., et al.: Development of optimum thin-walled
parts milling parameters calculation technique. In: Ivanov, V., et al. (eds.) Advances in Design,
Simulation and Manufacturing IV. LNME, pp. 343–352. Springer, Cham (2021). https://doi.
org/10.1007/978-3-030-77719-7_34
7. Kombarov, V., Sorokin, V., Tsegelnyk, Y., et al.: Numerical control of machining parts from
aluminum alloys with sticking minimization. Int. J. Mech. Appl. Mech. 9, 209–216 (2021).
https://doi.org/10.17683/ijomam/issue9.30
8. Bi, Q., Huang, N., Zhang, S., et al.: Adaptive machining for curved contour on deformed large
skin based on on-machine measurement and isometric mapping. Int. J. Mach. Tools Manuf
136, 34–44 (2019). https://doi.org/10.1016/j.ijmachtools.2018.09.001
9. Del Sol, I., Rivero, A., López de Lacalle, L.N., Gamez, A.J.: Thin-wall machining of light
alloys: a review of models and industrial approaches. Materials 12(12), 2012 (2019). https://
doi.org/10.3390/ma12122012
10. Plankovskyy, S., Myntiuk, V., Tsegelnyk, Y., et al.: Analytical methods for determining the
static and dynamic behavior of thin-walled structures during machining. In: Shkarlet, S., et al.
(eds.) Mathematical Modeling and Simulation of Systems (MODS’2020). AISC, vol. 1265,
pp. 82–91. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-58124-4_8
11. Pavlenko, D., Dvirnyk, Y., Przysowa, R.: Advanced materials and technologies for compressor
blades of small turbofan engines. Aerospace 8(1), 1–16 (2021). https://doi.org/10.3390/aer
ospace8010001
12. Wu, D., et al.: Analysis of machining deformation for adaptive CNC machining technology of
near-net-shaped jet engine blade. Int. J. Adv. Manufact. Technol. 104(9), 3383–3400 (2019).
https://doi.org/10.1007/s00170-019-03898-6
13. Denkena, B., Boess, V., Nespor, D., Floeter, F., Rust, F.: Engine blade regeneration: a literature
review on common technologies in terms of machining. Int. J. Adv. Manufact. Technol. 81(5),
917–924 (2015). https://doi.org/10.1007/s00170-015-7256-2

286
S. Plankovskyy et al.
14. Poznyakov, V.D., et al.: Cold cracking resistance of butt joints in high-strength steels with
different welding techniques. Strength Mater. 51(6), 843–851 (2020). https://doi.org/10.1007/
s11223-020-00132-7
15. Gradl, P.R.: Rapid fabrication techniques for liquid rocket channel wall nozzles. In: 52nd
AIAA/SAE/ASEE Joint Propulsion Conference, Paper No. 2016-4771. AIAA (2016). https://
doi.org/10.2514/6.2016-4771
16. Shen, B., Huang, G., Mak, K., Wang, X.: A best-ﬁtting algorithm for optimal location of
large-scale blanks with free-form surfaces. J. Mater. Process. Technol. 139, 310–314 (2003).
https://doi.org/10.1016/S0924-0136(03)00241-3
17. Sun, Y., Wang, X., Guo, D., Liu, J.: Machining localization and quality evaluation of parts with
sculptured surfaces using SQP method. Int. J. Adv. Manufact. Technol. 42(11), 1131–1139
(2009). https://doi.org/10.1007/s00170-008-1673-4
18. Mehrad, V., Xue, D., Gu, P.: Robust localization to align measured points on the manufactured
surface with design surface for freeform surface inspection. Comput. Aided Des. 53, 90–103
(2014). https://doi.org/10.1016/j.cad.2014.04.003
19. Plankovskyy, S., Nikolaev, A., Shypul, O., et al.: Balance layout problem with the optimized
distances between objects. In: Vasant, P., et al. (eds.) Data Analysis and Optimization for
Engineering and Computing Problems. EAISICC, pp. 85–93. Springer, Cham (2020). https://
doi.org/10.1007/978-3-030-48149-0_7
20. Besl, P.J., McKay, N.D.: A method for registration of 3-D shapes. IEEE Trans. Pattern Anal.
Mach. Intell. 14(2), 239–256 (1992). https://doi.org/10.1109/34.121791
21. Du,S.,Xu,Y.,Wan,T.,etal.:Robustiterativeclosestpointalgorithmbasedonglobalreference
point for rotation invariant registration. PLoS ONE 12(11), e0188039 (2017). https://doi.org/
10.1371/journal.pone.0188039
22. Lin, C.-C., Tai, Y.-C., Lee, J.-J., Chen, Y.-S.: A novel point cloud registration using 2D image
features. EURASIP J. Adv. Sig. Process. 2017(1), 1–11 (2017). https://doi.org/10.1186/s13
634-016-0435-y
23. Orazi, L., Tani, G.: Geometrical inspection of designed and acquired surfaces. Int. J. Adv.
Manufact. Technol. 34(1), 149–155 (2007). https://doi.org/10.1007/s00170-006-0587-2
24. Ko, K.H., Maekawa, T., Patrikalakis, N.M.: An algorithm for optimal free-form object
matching. Comput. Aided Des. 35(10), 913–923 (2003). https://doi.org/10.1016/S0010-448
5(02)00205-1
25. Mora, H., Mora-Pascual, J.M., Garcia-Garcia, A., Martinez-Gonzalez, P.: Computational
analysis of distance operators for the iterative closest point algorithm. PLoS ONE 11(10),
e0164694 (2016). https://doi.org/10.1371/journal.pone.0164694
26. Zhu, L., Xiong, Z., Ding, H., Xiong, Y.: A distance function based approach for localization
and proﬁle error evaluation of complex surface. J. Manuf. Sci. Eng. 126(3), 542–554 (2004).
https://doi.org/10.1115/1.1763186
27. Zhu, L., Zhang, X., Ding, H., Xiong, Y.: Geometry of signed point-to-surface distance function
and its application to surface approximation. J. Comput. Inf. Sci. Eng. 10(4), 041003 (2010).
https://doi.org/10.1115/1.3510588
28. Sun, Y., Xu, J., Guo, D., Jia, Z.: A uniﬁed localization approach for machining allowance
optimization of complex curved surfaces. Precis. Eng. 33(4), 516–523 (2009). https://doi.org/
10.1016/j.precisioneng.2009.02.003
29. Chatelain, J.F., Fortin, C.: A balancing technique for optimal blank part machining. Precis.
Eng. 25(1), 13–23 (2001). https://doi.org/10.1016/S0141-6359(00)00050-7
30. Chatelain, J.F.: A level-based optimization algorithm for complex part localization. Precis.
Eng. 29(2), 197–207 (2005). https://doi.org/10.1016/j.precisioneng.2004.07.002
31. Petrakov, Y., Shuplietsov, D.: Contour milling programming technology for virtual basing
on a CNC machine. Eastern Eur. J. Enterp. Technol. 2(1), 54–60 (2019). https://doi.org/10.
15587/1729-4061.2019.162673

Using the Phi-Function Technique for the Optimized
287
32. Petrakov, Y., Shuplietsov, D.: Programming of adaptive machining for end milling. Mech.
Adv. Technol. 1, 34–40 (2017). https://doi.org/10.20535/2521-1943.2017.79.97342
33. Stoyan, Y., Pankratov, A., Romanova, T.: Placement problems for irregular objects: mathe-
matical modeling, optimization and applications. In: Butenko, S., et al. (eds.) Optimization
Methods and Applications. SOIA, vol. 130, pp. 521–559. Springer, Cham (2017). https://doi.
org/10.1007/978-3-319-68640-0_25
34. Pankratov, A.V., Romanova, T.E., Chugay, A.M.: Optimal packing of convex polytopes using
quasi-phi-functions. J. Mech. Eng. 18(2), 55–65 (2015)
35. Romanova, T., Pankratov, A., Litvinchev, I., et al.: Sparsest packing of two-dimensional
objects. Int. J. Prod. Res. 59(13), 3900–3915 (2021). https://doi.org/10.1080/00207543.2020.
1755471
36. Wächter, A., Biegler, L.T.: On the implementation of an interior-point ﬁlter line-search algo-
rithm for large-scale nonlinear programming. Math. Program. 106(1), 25–57 (2006). https://
doi.org/10.1007/s10107-004-0559-y
37. Litvinchev, I., Infante, L., Ozuna Espinosa, E.L.: Approximate circle packing in a rectangular
container: integer programming formulations and valid inequalities. In: González-Ramírez,
R.G., et al. (eds.) Computational Logistics. LNTCS, vol. 8760, pp. 47–60. Springer, Cham
(2014). https://doi.org/10.1007/978-3-319-11421-7_4
38. Litvinchev, I., Ozuna Espinosa, E.L.: Integer programming formulations for approximate
packing circles in a rectangular container. Math. Probl. Eng. 2014, 317697 (2014). https://
doi.org/10.1155/2014/317697
39. Litvinchev, I., Ozuna, L.: Approximate packing circles in a rectangular container: valid
inequalities and nesting. J. Appl. Res. Technol. 12(4), 716–723 (2014). https://doi.org/10.
1016/S1665-6423(14)70088-4

COVID-19 Detection from Chest X-Ray Images
Using CNN Models and Deep Learning
Naﬁsha Binte Moin1, Shamima Sultana1, Abdullah Al Munem1,
Omar Tawhid Imam2, Ahmed Wasif Reza1, and Mohammad Shamsul Areﬁn3,4(B)
1 Department of Computer Science and Engineering, East West University, Dhaka, Bangladesh
wasif@ewubd.edu
2 Department of Computer Science and Engineering, Bangladesh University of Engineering and
Technology, Dhaka, Bangladesh
3 Department of Computer Science and Engineering, Daffodil International University,
Dhaka 1341, Bangladesh
sarefin@cuet.ac.bd
4 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chattogram 4349, Bangladesh
Abstract. The dangerous disease and ailment known as COVID-19 has caused a
global pandemic and immeasurable damage to people all around the globe. Chest
X-Rays are being used to identify COVID-19 in current times due to their low
cost and efﬁciency. In this paper, we developed Convolutional Neural Network
models to detect COVID-19 from Chest X-Ray images. CNN models from the
Keras library such as VGG16, VGG19, Xception, ResNet101, ResNet152, Incep-
tion, InceptionResNet, MobileNetV2, DenseNet201, NASNetLarge, and Efﬁ-
cientNetB3 have been used to perform experimentations. The CNN models used
ImageNet as its pre-trained weights for transfer learning. Additionally, a multi-
layered self-designed model has been implemented as well to see the performance.
A comparative analysis has been completed in order to ﬁnd the best-performing
CNN model for COVID-19 detection in Chest X-Ray images. From the experi-
ments, we found that the proposed CNN gave the best results. Additionally, it has
been observed that MobileNetV2, Inception, ResNet101, and VGG16 give the
highest accuracy over 99%, while the lowest accuracy is found by EfﬁcientNetB3
at only around 50%. The self-designed multi-layered model gives a training accu-
racy of 97.22% and a validation accuracy of 96.42%. A signiﬁcant increase in
accuracy and excellent performance has been seen from the CNN models and the
proposed framework.
Keywords: Convolutional neural network · COVID-19 · Chest X-ray
1
Introduction
In current times, Coronavirus and “COVID-19” is a well-known and feared terms all
across the globe. Currently, it is known as the most dangerous and widespread disease
all around the world causing the deaths of millions and harm to all sectors and part of
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 288–300, 2024.
https://doi.org/10.1007/978-3-031-50158-6_29

COVID-19 Detection from Chest X-Ray Images
289
regular life. In general deﬁnition, Coronavirus disease (shortly COVID-19) is an airborne
and infectious viral disease caused by a virus known as the SARS-CoV-2 virus which
mainly affects the lungs of the human respiratory system [1].
The symptoms that are more commonly observed for this disease include fever,
cough, loss of taste and smell, and tiredness. There may be pains and difﬁculty breathing,
shortness of breath, and chest pain as well. There are other additional symptoms of this
disease as well. This dangerous disease has damaged the medical and ﬁnancial stability
of almost all countries in the world and has caused a global pandemic.
Over the last two years, many methods and treatments have been discovered and
researched to treat and cure this deadly disease. Along with treatment and cure, many
steps have also been taken to ensure the prevention of this deadly and epidemic dis-
ease. One method for the identiﬁcation and detection of this viral disease is using X-
Ray images of the Chest or Lungs to detect the virus. This process can speed up the
identiﬁcation process compared to time-consuming testing in clinics [2].
Over the last few years, the ﬁeld of artiﬁcial intelligence has rapidly advanced in the
use of image classiﬁcation. In the case of medical science, a popular image classiﬁcation
Deep Learning model known as the Convolutional Neural Networks (or CNNs) is widely
used. CNNs can be used for the detection of diseases in different organs of human beings
and even plants [3]. CNN is an effective tool in the ﬁeld of medical science in tasks such as
image classiﬁcation, segmentation, and localization with its performance outperforming
humans for diseases related to the brain, breast, lungs, and other organs [4].
2
Related Work
COVID-19 has become a widely known and feared term all across the globe since the
year 2020. In order to tackle the issues rising due to COVID-19 many previous works
have been done in recent years to work on fast prediction and detection of the disease. In
most earlier related works, the majority of authors worked on improving the performance
of the CNNs for the diagnosis of the disease.
Panwar et al. [5] used an open-source dataset to conduct experimentations on the
proposed model ‘nCOVnet’ with transfer learning. The designed model used VGG16 as
its base model and ImageNet as the pre-trained weights. Training accuracy gives 97%
accuracy with 98.68% conﬁdence.
Jahid et al. [6] performed experimentations on COVID-19 X-Rays, Pneumonia
(another lung disease) X-Rays, and unaffected Chest X-Rays using the models three
CNNs. The dataset was from Kaggle, where the images were resized with augmenta-
tions. The models’ performances were measured by calculating the precision, recall, and
other scores of the models which were between the ranges of 0.98–1.
Basu et al. [7], it can be seen that CNNs have been used for the identiﬁcation of
COVID-19 images collected from four image databases. Experimentations on the CNN
models such as AlexNet (82.98% accuracy), VGGNet (90.13% accuracy), and Resnet
(85.98% accuracy) were performed.
Ismael et al. [8] work on ﬁne-tuning along with end-to-end training of CNNs for
disease detection. ResNet and VGG were the two CNN models which were used where

290
N. B. Moin et al.
feature extraction was performed by using different kernels of SVM. The accuracies of
the models range from 85.26 to 92.63%.
Hussain et al. [9] proposed a model (with 22 layers) using X-Rays and CTs of
Normal, COVID-19, and Pneumonia as input. CoroDet model gives high accuracy for
both training and validation along with high conﬁdence scores.
Nayak et al. [10] perform experiments to test the performance of CNNs for the
identiﬁcation of diseased lungs. The CNNs used namely, AlexNet, VGG16, GoogleNet,
MobileNetV2, SqeezeNet, ResNet34, ResNet50, and InceptionV3 has been used to per-
form the experiments. The paper shows the model ResNet34 performs better than the
rest of the models with an accuracy of 98.33%.
Heidari et al. [11] showed that work has been done to improve CNN predictions.
Image preprocessing techniques such as histogram equalization algorithms and bilateral
low-pass ﬁlters are used. The dataset images are used to form pseudo-colored images.
The study yields a high conﬁdence interval and high sensitivity.
Minaee et al. [12] used transfer learning techniques. Using Chest radiology images
from open datasets experiments were performed on the CNN models namely ResNet
(18 and 50), as well as SqueezeNet, and also DenseNet-121. The models received a
sensitivity of 98% and a speciﬁcity rate of 90%.
Mangal et al. [13] introduced a COVID detection CNN model called CovidAID. The
paper works on the use of X-Rays for further testing of RT-PCR. The model achieves an
accuracy of around 90.5% after it has been tested on a publically available dataset.
Alazab et al. [14] created datasets that have been used to create the COVID detection
model. The models yield an F1 score of around 95–99%. In addition, multiple deep
learning methods like the prophet algorithm, ARIMA, and LSTM were implemented to
carry out predictions.
Wang et al. [15] used deep learning methods. The model was trained on a dataset
created from 13,975 images (collected and compiled from 13,870 patients). Projection
and expansion designs have been implemented. In this paper, CNN models VGG and
ResNet achieve an accuracy of 83% and 90.6% respectively, while COVID-Net achieved
93.3% accuracy.
Zhang et al. [16] use deep learning by using the X-Ray image dataset from a GitHub
repository. The developed models show a sensitivity of 96% for COVID-19-positive
cases and a sensitivity of 70.65% for COVID-19-negative cases.
Tabik et al. [17] use a dataset known as the COVIDGR dataset which implemented
the COVID-SDNet model. Here, the dataset has been created by collaborating with a
hospital.Themodelachievesresultsof97.72%(severe),86.90%(moderate),and61.80%
(mild) accuracy in different severity levels.
Abbas et al. [18] performed COVID-19 diagnosis using the DeTrac CNN model.
The deep learning technique transfer learning has also been used. The model can deal
with irregularities and achieved a very high accuracy of around 93.1%.
Alghamdi et al. [19] performed a survey about using Deep Learning and CNNs
for disease detection from Chest X-Rays. The study highlights the necessity of diverse
datasets which should be publicly available. The common CNN models that were popular
for experimentation among researchers are ResNet, DenseNet, GoogleNet/Inception,

COVID-19 Detection from Chest X-Ray Images
291
and VGGNet. The work in [21–25] focuses different techniques those deployed mainly
image analysis techniques.
3
System Architecture and Design
Table1 highlights the proposed framework of the study, showing layers and types of
outputs shape and the total count of parameters used. The model contains a total of 20
layers where there are 6 are Convolutional, 4 of them are Max Pooling, and 6 Dropout.
Then Flatten Layers and Dense Layer have also been used for the ﬁnal predictions of
the model.
Table 1. Model architecture of proposed model
Layer
Output shape
Parameters
Convolution
(None, 329, 329, 32)
896
Convolution
(None, 327, 327, 64)
18,496
Max pooling
(None, 163, 163, 64)
0
Dropout
(None, 163, 163, 64)
0
Convolution
(None, 161, 161, 64)
36,928
Max pooling
(None, 80, 80, 64)
0
Dropout
(None, 80, 80, 64)
0
Convolution
(None, 78, 78, 128)
73,856
Max pooling
(None, 39, 39, 128)
0
Dropout
(None, 39, 39, 128)
0
Convolution
(None, 37, 37, 128)
147,584
Max pooling
(None, 18, 18, 128)
0
Dropout
(None, 18, 18, 128)
0
Convolution
(None, 16, 16, 128)
147,584
Max pooling
(None, 8, 8, 128)
0
Dropout
(None, 8, 8, 128)
0
Flatten
(None, 8192)
0
Dense
(None, 64)
524,352
Dropout
(None, 64)
0
Dense
(None, 2)
130
Total parameters
949,826
Trainable parameters
949,826
Non-trainable parameters
0
Table 1 shows the layers used for the model. In addition to the self-designed model,
the dataset has experimented on multiple built-in Keras CNN models such as VGG16,

292
N. B. Moin et al.
VGG19, Xception, ResNet101V2, ResNet152V2, InceptionV3, InceptionResNetV2,
MobileNetV2, DenseNet201, NASNetLarge, and EfﬁcientNetB3. The models used the
Batch Normalization layers (renorm = True) and Global Average Pooling2D layers.
Then, layers such as Dense Layers and Dropouts were used. The activations Relu and
Softmax were used for the dense layers.
3.1
Dataset Description
The dataset for disease detection dataset was obtained and collected from the data science
platform Kaggle [20]. The dataset contains three directories where the images from the
training and validation dataset were used for the study to train the models. The dataset
contains two class labels, one for diseased lung X-rays and another for healthy X-ray
images.
The training and validation ﬁles have an equal distribution of images. In this dataset,
a total of 348 images have been used as the inputs for the proposed method and the
experimental CNN models.
Figure 1 displays some of the sample images for both healthy and diseased X-Ray
images from the training directory of the dataset.
Fig. 1. Sample input images data set (train)

COVID-19 Detection from Chest X-Ray Images
293
Table 2 shows detailed information about the distribution of images throughout the
dataset for both Covid and Normal class labels in the case of both training and validation
image datasets.
Table 2. Image distribution of dataset
Train/validation set
Class labels
Total images
Train dataset
Covid
144
Normal
144
Validation dataset
Covid
30
Normal
30
Total
348
Intotal,348imagesampleswerecollectedandusedforanalysisandexperimentation.
All the images are in png format in this open-sourced dataset. Total Covid images are
174 and healthy Chest X-Ray images are 174. A completely balanced distribution of
images has been observed for the collected dataset as there is an equal number of images
for both Covid and Normal samples.
3.2
Data Preprocessing
In the proposed method for this study, a total of 348 images of both class labels have
been used. For preprocessing the dataset and in order to make it suitable for training, the
images have been resized where the input images have a size of 331 × 331 pixels. The
images were rescaled.
Data augmentation using the ImageDataGenerator function of Keras has been per-
formed on the training and validation images. Horizontal and vertical ﬂips have been
applied after rescaling and the selected color mode was RBG. The training and validation
split of the dataset was already created and separated into respective directories of the
dataset.
4
Implementation and Experimental Result
4.1
Experimental Setup
The experimentations were performed using the online platform Google Colaboratory.
Google Colab contains the required latest version of Python along with additional depen-
dencies and libraries such as Keras and Tensorﬂow. Keras has been used to create the
CNN models and perform all the experimentations.
The total number of epochs is 25, with a batch size equal to 8. Adam was the optimizer
and cross-entropy (categorical) was the loss function. The chosen metric for evaluating
the model performance for each epoch was the accuracy measure. The callback functions
(early-stopping) with patience and restoring best weights have also been implemented
into the model. The models were evaluated on all the images in training, validation, and
testing.

294
N. B. Moin et al.
4.2
Performance Evaluation
In order to accurately and comprehensively evaluate the efﬁciency of the system and all
the experimented CNN models, the accuracy has been calculated using the following
equations All the calculated values were obtained automatically from the Keras model
training period. In addition to the accuracy, the calculation of the loss, precision, recall,
and f1-score have also been performed for model evaluation.
Accuracy = No. of corrected correspondence
No. of correspondence
× 100%
(1)
Precision =
True Positives
True Positives + False Positives
(2)
Recall =
True Positives
True Positives + False Negatives
(3)
F1 −score = 2 × (Precision × Recall)
Precision + Recall
(4)
The metrics and evaluation scores with the equations show how well the model
is performing along with how good the model is for real-world experimentations and
implementation.
Table 3 shows that the accuracy of the proposed framework surpasses many previous
works and is around 96.77%. The proposed framework gives a very low loss where the
average loss is approximately 0.10429. Additionally, a high value of the other metrics
can be observed which exceeds more than 93%. The obtained f1-score for the proposed
system is 95%. In the case of precision, Covid-19 positive cases received a percentage
of 94% and negative cases had 97%. While for recall, Covid-19 positive case is 97%
and the negative case is 93%.
Figure 2 shows the accuracies while Fig. 3 shows the losses over 25 epochs in both
train and validation sets. The proposed framework shows an exponential increase in
accuracy and a slow and smooth decrease in loss over the epochs. The graphs plotting
the accuracy and loss for the epochs show an exponential curve. The model generalizes
and learns from the data without errors. Along with the proposed model results, the
results for the experimental built-in Keras models are brieﬂy discussed in the following
tables highlighting the accuracy, loss, etc.
In Table 4, almost all the CNN models give a very high accuracy except Efﬁcent-
NetB3. Out of all the models, VGG16, ResNet101V2, InceptionV3, and MobileNetV2
give the highest accuracy of 99%. Almost all the models here give higher accuracy than
the proposed framework. VGG19, ResNet152V2, InceptionResNetV2, and NASNet-
Large have the second-highest values of 97%. The most underperforming CNN model
is the EfﬁcientNetB3 having less than 50% accuracy. There was a decrease in accuracy
percentages for the models when using validation data. The validation accuracies are
similar to the training accuracies.
Table 5 shows the loss calculated for all the models during training, validation,
and evaluation. The lower the value of the model loss, the more efﬁciently the model
is performing and the less prone to show errors. Low loss determines how the model

COVID-19 Detection from Chest X-Ray Images
295
Table 3. Performance analysis for proposed system
Train accuracy
97.22%
Train loss
0.09587
Validation accuracy
96.42%
Validation loss
0.09375
Validation evaluation accuracy
96.67%
Validation evaluation loss
0.12327
Precision (COVID)
0.94
Precision (normal)
0.97
Recall (COVID)
0.97
Recall (normal)
0.93
F1-score (COVID)
0.95
F1-score (normal)
0.95
Fig. 2. Training and validation loss for proposed framework
Fig. 3. Training and validation accuracy for proposed framework
generalizes per epoch and improves performance and learning. The table shows that

296
N. B. Moin et al.
Table 4. Performance analysis for built-in Keras CNN models
Models
Train accuracy (%)
Validation accuracy (%)
Validation evaluation
accuracy (%)
VGG16
99.65
100.00
98.33
VGG19
93.40
91.07
91.67
Xception
98.95
96.42
95.00
ResNet101V2
99.30
98.21
96.67
ResNet152V2
97.22
94.64
95.00
InceptionV3
98.95
96.42
96.67
InceptionResNetV2
97.22
98.21
96.67
MobileNetV2
99.65
98.21
98.33
DenseNet201
98.26
96.42
98.33
NASNetLarge
96.87
96.42
96.67
EfﬁcientNetB3
46.52
48.21
50.00
VGG16 and MobileNetV2 have the lowest loss values. Therefore, they are performing
bettercomparedtotheothermodels.Xception,ResNet101V2,andInceptionV3alsohave
a comparatively lower loss, while ResNet152V2, InceptionResNetV2, DenseNet201,
and NASNetLarge have comparatively higher loss values. EfﬁcientNetB3 is the worst-
performing model with a loss higher than 0.69.
Table 5. Loss for built-in Keras CNN models
Models
Train loss
Validation loss
Validation evaluation loss
VGG16
0.15574
0.14944
0.17977
VGG19
0.27254
0.31001
0.30134
Xception
0.19411
0.23464
0.26204
ResNet101V2
0.18148
0.19645
0.22280
ResNet152V2
0.20594
0.25528
0.24951
InceptionV3
0.19447
0.23364
0.23008
InceptionResNetV2
0.23756
0.21918
0.24458
MobileNetV2
0.17348
0.19489
0.19274
DenseNet201
0.20210
0.23204
0.20017
NASNetLarge
0.23647
0.24278
0.23910
EfﬁcientNetB3
0.69470
0.69357
0.69323

COVID-19 Detection from Chest X-Ray Images
297
Table 6 shows the values obtained for all the mentioned metrics in the case of all
the models. The values were obtained from the validation data. It can be seen that
the scores for all the models and the 20-layered framework are comparatively higher
than the required benchmark scores for this evaluation. Xception, ResNet152V2, and
InceptionResNetV2 give comparatively higher scores where the values range from 94
to 100%. EfﬁcientNetB3 gives the worst results with an f1-score of 67%.
Table 6. Precision, recall, and F1-score for built-in models
Models
Precision
(C)
Precision
(N)
Recall
(C)
Recall
(N)
F1-score
(C)
F1-score
(N)
VGG16
0.94
1.00
1.00
0.93
0.97
0.97
VGG19
1.00
0.94
0.93
1.00
0.97
0.97
Xception
0.97
1.00
1.00
0.97
0.98
0.98
ResNet101V2
0.97
1.00
1.00
0.97
0.98
0.98
ResNet152V2
0.88
1.00
1.00
0.87
0.94
0.93
InceptionV3
0.86
1.00
1.00
0.83
0.92
0.91
InceptionResNetV2
0.97
1.00
1.00
0.97
0.98
0.98
MobileNetV2
0.97
1.00
1.00
0.97
0.98
0.98
DenseNet201
0.94
1.00
1.00
0.93
0.97
0.97
NASNetLarge
0.97
1.00
1.00
0.97
0.98
0.98
EfﬁcientNetB3
0.00
0.50
0.00
1.00
0.00
0.67
Tables 3 through 6 highlight the experimental results for all the CNN models and
self-designed novel proposed framework along with a comparison of which models
performed the best and which models performed the worst for the identiﬁcation of the
disease from the dataset images.
4.3
Comparison with Other Existing Frameworks
Table 7 given below highlights the accuracies in other existing frameworks in contrast
to the proposed framework and the built-in models with the previous works for both
self-designed models and well-established CNN models.
From the comparison, it can be said that most of the experimental models are showing
high accuracies compared to the previous related works. VGG16, MobileNetV2, and
ResNet101 show better performances compared to the previous works. The proposed
framework’s accuracy does not exceed the previously designed frameworks but there is
a very slight difference between the accuracies of the models.

298
N. B. Moin et al.
Table 7. Comparison with previous works
Paper No.
Previous works results (%)
Previous works models
Experimented models
performance (%)
[5]
97.00
Self-designed
96.77
[7]
90.13
VGG16
99.32
[7]
85.98
ResNet101
98.06
[8]
85.26
VGG16
99.32
[8]
87.37
ResNet101
98.06
[8]
89.47
VGG19
92.04
[9]
99.10
Self-designed
96.77
[10]
95.83
VGG16
99.32
[10]
95.83
MobileNetV2
98.73
[10]
92.50
InceptionV3
97.34
[15]
83.00
VGG19
92.04
[15]
93.30
Self-designed
96.77
5
Conclusion
In this study, 11 popular Convolutional Neural Networks such as VGG16, VGG19, Xcep-
tion, ResNet101V2, ResNet152V2, InceptionV3, InceptionResNetV2, MobileNetV2,
DenseNet201, NASNetLarge, EfﬁcientNetB3 were used. In addition to the eleven CNN
models, a self-designed framework has been. All the models except EfﬁcicentNetB3 give
high accuracy and low loss. Most of the models outperform the previous works. The
framework designed for this work gives an accuracy of 96.77% with a loss of 0.10429.
In conclusion, the proposed framework and the built-in Keras models, especially
VGG16, MobileNetV2, and ResNet101V2 give excellent performance in the damaged
and diseased lungs from Chest X-Ray images for Covid. All those models can be
implemented and tested on different datasets to compare and improve performances.
From the eleven experimental and built-in Keras models used for this study, VGG16,
MobileNetV2, and ResNet101V2 outperform the rest of the models. The proposed
framework carries the lowest loss value out of all the models.
Limitations to this study are fewer data usage for training. More data along with
additional class labels for Pneumonia and Tuberculosis will further improve the predic-
tion range and working range. Additionally, various techniques for transforming images
can be applied to improve performance. These proposed techniques may also improve
the precision, recall, and f1 scores. Additionally, kit tests can also be used together with
this method for the accuracy of results. Future work for this study would include creat-
ing a web interface or a mobile application where the models will be used to conduct
predictions in real time and as an end to end models.

COVID-19 Detection from Chest X-Ray Images
299
References
1. WHO: WHO coronavirus disease (COVID-19). https://www.who.int/health-topics/corona
virus. Last accessed 21 May 2022
2. Rahaman, M.M., et al.: Identiﬁcation of COVID-19 samples from chest X-ray images using
deep learning: a comparison of transfer learning approaches. J. X-Ray. Sci. Technol. 28,
821–839 (2020). https://doi.org/10.3233/XST-200715
3. Moin, N.B., Islam, N., Sultana, S., Chhoa, L.A., Ruhul Kabir Howlader, S.M., Ripon, S.H.:
Disease detection of Bangladeshi crops using image processing and deep learning—a com-
parative analysis. In: 2022 2nd International Conference on Intelligent Technologies. CONIT
2022, pp. 1–8 (2022). https://doi.org/10.1109/CONIT55038.2022.9847715
4. Sarvamangala, D.R., Kulkarni, R.V.: Convolutional neural networks in medical image under-
standing: a survey. Evol. Intel. 15(1), 1–22 (2021). https://doi.org/10.1007/s12065-020-005
40-3
5. Panwar, H., Gupta, P.K., Siddiqui, M.K., Morales-Menendez, R., Singh, V.: Application of
deep learning for fast detection of COVID-19 in X-rays using nCOVnet. Chaos Solitons
Fractals 138, 109944 (2020). https://doi.org/10.1016/j.chaos.2020.109944
6. Hasan Jahid, M., Alom Shahin, M., Ali Shikhar, M.: Deep learning based detection and seg-
mentation of COVID-19 pneumonia on chest X-ray image. In: 2021 International Conference
on Information and Communication Technology for Sustainable Development. ICICT4SD
2021—Proceedings, pp. 210–214 (2021). https://doi.org/10.1109/ICICT4SD50815.2021.939
6878
7. Basu,S.,Mitra,S.,Saha,N.:DeeplearningforscreeningCOVID-19usingchestX-rayimages.
In: 2020 IEEE Symposium Series on Computational Intelligence. SSCI 2020, pp. 2521–2527
(2020). https://doi.org/10.1109/SSCI47803.2020.9308571
8. Ismael, A.M., ¸Sengür, A.: Deep learning approaches for COVID-19 detection based on chest
X-ray images. Expert Syst. Appl. 164 (2021). https://doi.org/10.1016/j.eswa.2020.114054
9. Hussain, E., Hasan, M., Rahman, M.A., Lee, I., Tamanna, T., Parvez, M.Z.: CoroDet: a
deep learning based classiﬁcation for COVID-19 detection using chest X-ray images. Chaos
Solitons Fractals 142, 110495 (2021). https://doi.org/10.1016/j.chaos.2020.110495
10. Nayak, S.R., Nayak, D.R., Sinha, U., Arora, V., Pachori, R.B.: Application of deep learning
techniques for detection of COVID-19 cases using chest X-ray images: a comprehensive
study. Biomed. Signal Process. Control. 64, 102365 (2021). https://doi.org/10.1016/j.bspc.
2020.102365
11. Heidari, M., Mirniaharikandehei, S., Khuzani, A.Z., Danala, G., Qiu, Y., Zheng, B.: Improving
the performance of CNN to predict the likelihood of COVID-19 using chest X-ray images
with preprocessing algorithms. Int. J. Med. Inform. 144, 104284 (2020). https://doi.org/10.
1016/j.ijmedinf.2020.104284
12. Minaee,S.,Kaﬁeh,R.,Sonka,M.,Yazdani,S.,JamalipourSouﬁ,G.:Deep-COVID:predicting
COVID-19fromchestX-rayimagesusingdeeptransferlearning.Med.ImageAnal.65(2020).
https://doi.org/10.1016/j.media.2020.101794
13. Mangal, A., Kalia, S., Rajgopal, H., Rangarajan, K., Namboodiri, V., Banerjee, S., Arora, C.:
CovidAID: COVID-19 Detection Using Chest X-Ray, pp. 1–10 (2020)
14. Alazab, M., Awajan, A., Mesleh, A., Abraham, A., Jatana, V., Alhyari, S.: COVID-19 pre-
diction and detection using deep learning. Int. J. Comput. Inf. Syst. Ind. Manag. Appl. 12,
168–181 (2020)
15. Wang, L., Lin, Z.Q., Wong, A.: COVID-Net: a tailored deep convolutional neural network
design for detection of COVID-19 cases from chest X-ray images. Sci. Rep. 10, 1–12 (2020).
https://doi.org/10.1038/s41598-020-76550-z

300
N. B. Moin et al.
16. Zhang, J., et al.: Viral pneumonia screening on chest X-rays using conﬁdence-aware anomaly
detection. IEEE Trans. Med. Imaging 40, 879–890 (2021). https://doi.org/10.1109/TMI.2020.
3040950
17. Tabik, S., et al.: COVIDGR dataset and COVID-SDNet methodology for predicting COVID-
19 based on chest X-ray images. IEEE J. Biomed. Heal. Inform. 24, 3595–3605 (2020). https://
doi.org/10.1109/JBHI.2020.3037127
18. Abbas, A., Abdelsamea, M.M., Gaber, M.M.: Classiﬁcation of COVID-19 in chest X-ray
images using DeTraC deep convolutional neural network. Appl. Intell. 51(2), 854–864 (2020).
https://doi.org/10.1007/s10489-020-01829-7
19. Alghamdi, H.S., Amoudi, G., Elhag, S., Saeedi, K., Nasser, J.: Deep learning approaches
for detecting COVID-19 from chest X-ray images: a survey. IEEE Access. 9, 20235–20254
(2021). https://doi.org/10.1109/ACCESS.2021.3054484
20. Fenta, F.: Chest X-ray for covid-19 detection. https://www.kaggle.com/datasets/fusicfenta/
chest-xray-for-covid19-detection. Last accessed 15 May 2022
21. Saha, R., Debi, T., Areﬁn, M.S.: Developing a framework for vehicle detection, tracking
and classiﬁcation in trafﬁc video surveillance. In: Vasant, P., Zelinka, I., Weber, GW. (eds.)
Intelligent Computing and Optimization. ICO 2020. Advances in Intelligent Systems and
Computing, vol. 1324. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-68154-
8_31
22. Fatema, K., Ahmed, M.R., Areﬁn, M.S.: Developing a system for automatic detection of
books. In: Chen, J.I.Z., Tavares, J.M.R.S., Iliyasu, A.M., Du, K.L. (eds.) Second International
Conference on Image Processing and Capsule Networks. ICIPCN 2021. Lecture Notes in
Networks and Systems, vol. 300. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-
84760-9_27
23. Rahman, M., Laskar, M., Asif, S., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Flower recognition
using VGG16. In: Chen, J.I.Z., Tavares, J.M.R.S., Shi, F. (eds.) Third International Conference
on Image Processing and Capsule Networks. ICIPCN 2022. Lecture Notes in Networks and
Systems, vol. 514. Springer, Cham (2022). https://doi.org/10.1007/978-3-031-12413-6_59
24. Yeasmin, S., Afrin, N., Saif, K., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Image classiﬁcation
for identifying social gathering types. In: Vasant, P., Weber, G.W., Marmolejo-Saucedo, J.A.,
Munapo, E., Thomas, J.J. (eds.) Intelligent Computing & Optimization. ICO 2022. Lecture
Notes in Networks and Systems, vol. 569. Springer, Cham (2023). https://doi.org/10.1007/
978-3-031-19958-5_10
25. Ahmed, F., et al.: Developing a classiﬁcation CNN model to classify different types of ﬁsh. In:
Vasant, P., Weber, G.W., Marmolejo-Saucedo, J.A., Munapo, E., Thomas, J.J. (eds.) Intelligent
Computing & Optimization. ICO 2022. Lecture Notes in Networks and Systems, vol. 569.
Springer, Cham (2023). https://doi.org/10.1007/978-3-031-19958-5_50

A Note on Solving the Transportation Model
by the Hungarian Method of Assignment:
Uniﬁcation of the Transportation
and Assignment Models
Santosh Kumar1, Trust Tawanda2, Elias Munapo3(B), and Philimon Nyamugure2
1 Department of Mathematical and Geospatial Sciences, School of Sciences, RMIT University,
Melbourne, Australia
Santosh.kumar@rmit.edu.au
2 Department of Statistics and Operations Research, National University of Science and
Technology, Bulawayo, Zimbabwe
{trust.tawanda,philimon.nyamugure}@nust.ac.zw
3 School of Economics and Decision Sciences, North West University, Maﬁkeng Campus,
Maﬁkeng, South Africa
elias.munapo@nwu.ac.za
Abstract. This short note extends the Hungarian method for the assignment for
solving a transportation model. In the proposed approach, the optimality of the
solution is established when the solution is feasible, hence a degenerate transporta-
tion solution poses no difﬁculties as is the case of the conventional transportation
approach.
Keywords: Hungarian method · Assignment · Transportation · Degeneracy
1
Introduction
The assignment and transportation models are well-known in the OR literature, and they
have many industrial applications in production planning, telecommunication, schedul-
ing, military operations etc., see Hillier and Lieberman [1], Munapo and Kumar [2],
Taha [3], Tawanda [4] and Winston [5]. Both models are essentially degenerate linear
programs, hence solution by the simplex approach was considered inefﬁcient for solv-
ing these two models. In search for solving these two special models, special methods
were developed to solve the assignment and transportation models, which have been
well documented in OR books, for example, Taha [3] and Winston [5]. Models for both
problems are totally uni-modular, hence these two LP models result in an integer solu-
tion. Solution of the assignment model was proposed by Kuhn [6] and he called his
approach, ‘The Hungarian method for assignment’. The method by Kuhn uses some
properties associated with the cost matrix for the assignment problem and developed
an algorithm that gets to a solution which requires no proof of optimality; hence the
approach is free of difﬁculties associated with a degenerate LP solution. However, for
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 301–313, 2024.
https://doi.org/10.1007/978-3-031-50158-6_30

302
S. Kumar et al.
the transportation model, a special method was developed that used the properties of
the dual of the transportation LP and developed an approach that requires an initial
feasible solution, a testing procedure for optimality and if the solution does not satisfy
optimality conditions, a process to move to a better solution. This process is repeated,
until the optimal solution is reached. Efﬁciency of this approach is dependent on the
initial starting solution and the number of basic variables, which is required to be (m +
n −1), where m denotes the number of rows and n denotes the number of columns in
the transportation model. Therefore, methods were developed to ﬁnd an initial solution,
which was subjected to an optimality test and if the solution failed the test for optimality,
iteratively the solution was improved until it satisﬁed the optimality test. Recently the
‘method of subtractions’ was developed and is presented in Munapo and Kumar [2].
For a degenerate solution, computational load for the test of optimality increases and
makes it more difﬁcult to establish the optimality of a solution. Degeneracy in LP or for
this special class of model can cause difﬁculties as the value of the objective function
may not improve in successive iterations. In many instances, one may have reached the
optimal solution, but the test of optimality is unable to recognise its optimality due to
degeneracy. The assignment and transportation are both degenerate with respect to LP
model and order of degeneracy for the transportation problem as a LP model is 1 or more
and for the assignment model, degeneracy order is n.
In this short note, without any loss of generality, a balanced transportation problem
is considered as an assignment model, and the Hungarian method of assignment is
modiﬁed to solve the transportation model. Although the uniﬁcation of transportation
andassignmentwasestablishedearlierbyMunapoetal.[7],butinthisnotetheHungarian
method is slightly modiﬁed to solve the transportation model also.
The paper has been organized in 5 sections. Mathematical models and necessary
background have been discussed in Sect. 2. In this section, the modiﬁed Hungarian
method for the transportation problem is also presented. Some interesting properties are
presented in Sect. 3. Three numerical illustrations have been presented in Sect. 4, and
ﬁnally some concluding remarks have been discussed in Sect. 5.
2
Mathematical Models and Necessary Background
First, we recall the mathematical models of both the assignment and the transportation
models in Sect. 173.1 and relook at the transportation model as an assignment model in
Sect. 173.2.
2.1
Mathematical Formulations of Transportation and Assignment Models
The mathematical model of a (m × n) transportation model is given by:
Min Z =
m

i=1
n

j=1
Cijxij
n

j=1
xij = ai,
m

i=1
xij = bj,
i = 1, 2, . . . , m; j = 1, 2, . . . , n.
(1)

A Note on Solving the Transportation Model
303
Further, without any loss of generality, it is assumed that the problem is a balanced
transportation problem, i.e.
m

i=1
ai =
n

j=1
bj = M
xij ≥0 and an integer for all i and j
(2)
Note Cij denotes the per unit cost of transportation of a unit from warehouse i to the
destination j, ai and bj denote supply at the warehouse i and demand at the destination
j where i = 1, 2, . . . , m and j = 1, 2, …, n.
Similarly, the mathematical model of an (m by n, where m = n) assignment problem
is given by (3)
Min Z =
m

i=1
n

j=1
Cijxij
n

j=1
xij = 1,
m

i=1
xij = 1,
i = 1, 2, . . . , m; j = 1, 2, . . . , n, m = n
xij = 0 or
1 for all i and j.
(3)
For an assignment problem, all ai = bj = 1 and m = n.
2.2
Consideration of the Transportation Model as an Assignment Model
If each row i in the transportation model is repeated for ai number of times and similarly
each column j is repeated for bj number of times, each element Cij in the transportation
cost matrix can be seen as a submatrix of dimension ai and bj, as shown below:
⎡
⎣
Cij,11 . . . Cij,1bj
. . .
. . .
. . .
Cij,ai1 . . . Cij,aibj
⎤
⎦
When each Cij for all i and j is expanded in the form of a matrix as given by the
matrix above, the new problem will be a M × M size assignment problem with demand
and supply 1 at each supply and demand point, Here M = m
i=1 ai = n
j=1 bj. Since the
new M × M size problem will be an assignment model, it is natural that the Hungarian
method of assignment will be applicable and unlike the conventional transportation
method, the problem can be solved as an assignment problem by using the Hungarian
method of assignment. First, we make some observations in Sect. 173.3 and develop the
Hungarian method for the transportation problem in Sect. 173.4.

304
S. Kumar et al.
2.3
Some Observations
Observation 1
The Hungarian approach develops equivalent cost matrices and attempts to ﬁnd the
assignment at zero total cost which is a natural minimum, hence optimality proof is not
required.
Observation 2
The Hungarian method of assignment, deals with the modiﬁed cost elements and after
detecting an independent zero element, makes an allocation and draws a horizontal or
a vertical line, to indicate no further assignments are possible in that row or column.
However, in the case of transportation problem, if the requirement of such independent
zero elements is represented by a number p, then the number p must satisfy the condition
(4).
Max (m, n) ≤p ≤(m + n −1)
(4)
Observation 3
In a non-degenerate balanced transportation problem, the number of allocations will be
given by (m + n −1). It means the number of horizontal lines must not exceed (m −1)
and similarly the number of vertical lines also must not exceed (n −1) for further
modiﬁcation of the relative cost elements Cij. This observation is obvious, as when all
elements are on a line, no element will be left as a positive element, which acts like a
seed for further modiﬁcation of the cost elements Cij.
Observation 4
In a non-degenerate balanced transportation problem, maximum number of independent
zero elements that can be created by the Hungarian method of assignment can be at
most (m + n −1), which is exactly the requirement for determination of the optimal
solution of the transportation model. Hence the Hungarian method of assignment can
be extended to sole the balances transportation model.
2.4
Modiﬁed Hungarian Method of Assignment for Solving Transportation
Problem
The steps of the Hungarian Method of assignment for the balanced transportation model
will be as follows:
Step 1. Consider the balanced transportation model, as all transportation models can
be easily converted to a balanced model by the introduction of an appropriate dummy
supply or a demand point.
Step 2. Subtract the minimum element in each row from all other elements in that row.
This process will generate at least one zero element in each row. Repeat the same process
for each column, thus once again, each column will have at least one zero element in
each column in the given balanced (m × n) transportation model.

A Note on Solving the Transportation Model
305
Step 3. Inspect row i, where i = 1, 2, . . . , m, if the row i has a unique zero element, i.e.
the modiﬁed Cij = 0 in column j, where j = 1, 2, . . . , n; ﬁnd the min

ai, bj
	
.
(1) If ai = bj, draw a vertical line through the column j, as was done in the case of an
assignment model. The modiﬁed value of ai will be zero and similarly the modiﬁed
value of bj will also be zero. In other words, the (m × n) model will be reduced to
((m −1) × (n −1)) model.
(2) If ai ≤bj, draw a horizontal line through the row i, if the number of horizontal lines
is less than (m −1). Change the value ai to zero, and the bj value is changed to

bj −ai
	
.
(3) if ai ≥bj, draw a vertical line in column j, if the number of vertical lines is less
that (n −1). Change the value of bj to zero, and the value of ai will be changed to

ai −bj
	
.
Repeat the same process for each row and column, and when all zero elements have
been covered, go to Step 4.
If all zero elements have not been covered by a horizontal or a vertical line, ﬁnd for
each zero the possible maximum allocation and draw the line as per the rule described
(1) to (3). Repeat this process until all zero elements have been covered. Go to Step 4.
Step 4. If the solution is feasible, go to Step 6, and if the solution is not feasible, and all
zero elements in the modiﬁed matrix have been covered, do the following:
If the number of lines are less than (m + n −2), ﬁnd the minimum positive modiﬁed
element, which have not been crossed out by a vertical or the horizontal line. Using the
value of this element, update the cost matrix as follows:
(i). Subtract this minimum value form all elements which have not been crossed out.
(ii). Do not alter values which are on a single line.
(iii). Add the minimum value to all elements which are on the intersection of two lines.
If the number of lines are equal to (m + n −2), only one element will be left as
uncrossed out. If this number is large compared to other elements in that row or column,
we get the feasible solution, and go to Step 7.
Step 5. Go to Step 2, i.e. make sure once again that each row and column has a zero.
Step 6. Each allocation results in a horizontal or a vertical line, and if total number of
lines is equal to (m + n −2), then the current allocation must lead to a feasible solution,
and that will be an optimal solution, if this is minimum in that column or row, as may
be the case. The process will terminate.
Step 7. If the last element is not minimum, obtain a feasible solution, and subject it to
the test of optimality, and obtain the optimal solution by the conventional approach.
This process will be able to handle degenerate transportations models, without any
difﬁculty.

306
S. Kumar et al.
3
Interesting Properties of a Transportation Model
3.1
Property 1
Unlike a linear program, a special characteristic of a balanced transportation model is
that any feasible solution can be tested for optimality without any past information.
Proof The LP approach is sequential, for example, the extreme point search at any
point of the LP convex space is dependent on the previous extreme point, in other words,
if a LP is given and its corresponding optimal solution is given, we must resolve to prove
the optimality of the given solution. In the case of a transportation model, the optimality
proof is based on values of the dual variables of the transportation model, which are
directly linked to the given basic variables. Therefore, given a basic feasible solution, it
can be tested for optimality at any time. This the property we use in Step 7.
3.2
Property 2
The Hungarian approach discussed in this paper deals with infeasible solutions, which
is partly situated at zero modiﬁed cost elements and partly it will be at non-zero positive
modiﬁed cost elements. Hence one can easily move from the Hungarian approach to
conventional transportation approach to test the optimality of the solution at any stage
of iterations. However, the Hungarian approach is sequential similar to the LP i.e. the
process will have to start from the very beginning, if previous information is missing.
4
Numerical Illustrations
We present three illustrations, dealing with alternative, unique and degenerate solutions.
4.1
Illustration 1
Consider an example of a transportation problem taken from Eppen et al. [8, p. 283].
The cost per unit, demand and supply is given in Table 1.
Table 1. The transportation problem
1
2
3
4
Supply
1
12
13
4
6
500
2
6
4
10
11
700
3
10
9
12
4
800
Demand
400
900
200
500
2000
Note the minimum element in each row is 4. For creating a zero in each row, we
subtract 4 from each element, we obtain Table 2 as given below.

A Note on Solving the Transportation Model
307
Table 2. Modiﬁed values of the unit transportation cost
1
2
3
4
Supply
1
8
9
0
2
500
2
2
0
6
7
700
3
6
5
8
0
800
Demand
400
900
200
500
2000
Since column 1 does not have a zero element, we again subtract 2 from the elements
in column 1, the costs per unit are further modiﬁed as give in Table 3. Allocated values are
shown in parenthesis and horizontal lines through a row are indicated by →and a vertical
line through a column is indicated ↑in the supply and demand column respectively. A
number with an arrow indicates the order of the line. For example, in Table 3, column 3
has a vertical arrow with a number 1, it means ﬁrst allocation was made in this column
in the cell (1, 3).
Table 3. Modiﬁed values of the unit transportation cost
1
2
3
4
Supply
1
6
9
0 (200)
2
500, 300
2
0 (400)
0 (300)
6
7
700, 300, 0 →4
3
4
5
8
0 (500)
800, 300
Demand
400, 0 ↑3
900, 600
200, 0 ↑1
500, 0 ↑2
2000, 600
Note that columns 1, 3, and 4 have been crossed out by vertical lines and row 2 by a
horizontal line and the solution is not yet feasible, as 600 more units must be allocated.
We need to create more zero elements. Among the two not crossed out elements in
column 2, the minimum is 5, situated in the cell (3, 2). The updated values are given in
Table 4.
Table 4. Modiﬁed values of the unit transportation cost from Table 3
1
2
3
4
Supply
1
6
4
0
2
500
2
5
0
11
12
700
3
4
0
8
0
800
Demand
400
900
200
500
2000

308
S. Kumar et al.
Note that column 1 has no zero elements, hence subtracting the minimum, a zero
element is created as shown in Table 5.
Table 5. Modiﬁed values of the unit transportation cost
1
2
3
4
Supply
1
2
4
0 (200)
2
500, 300
2
1
0 (700)
11
12
700, 0 →2
3
0 (400)
0 (200)
8
0 (200)
800, 400, 200
0 →5
Demand
400, 0 ↑3
900, 2000 ↑4
200, 0 ↑1
500, 300
2000, 300
The solution is still infeasible, hence more zeros must be created. Columns 1, 2 and 3
have been covered by vertical lines, rows 2 and 3 by the horizontal lines. Hence the only
element not crossed out is in the cell (1, 4). Its value is 2, which is minimum element in
row 1. The new updated values are shown in Table 6.
Table 6. Modiﬁed values of the unit transportation cost
1
2
3
4
Supply
1
2
4
0
0
500
2
3
2
13
12
700
3
2
2
10
0
800
Demand
400
900
200
500
2000
Now columns 1 and 2 have no zero elements, subtracting the minimum, and making
allocations, we obtain Table 7.
Table 7. Modiﬁed values of the unit transportation cost from Table 6
1
2
3
4
Supply
1
0
2
0 (200)
0 (300)
500, 300, 0
2
1
0 (700)
13
12
700, 0, 0 →1
3
0 (400)
0 (200)
10
0 (200)
800, 600, 200, 0
Demand
400, 0 ↑4
900, 200, 0 ↑2
200, 0 ↑3
500, 300, 0
2000, 0
In Table 7, we have two zero elements in row 1 and 3 and two zeros in column 1 and
4, hence more than one optimal solution may exist. One possible solution is displayed

A Note on Solving the Transportation Model
309
in Table 7, which is optimal and total cost is $12,000. The second alternate solution is
shown in Table 8.
Table 8. The alternate optimal solution
1
2
3
4
Supply
1
12 (300)
13
4 (200)
6
500
2
6
4 (700)
10
11
700
3
10 (100)
9 (200)
12
4 (500)
800
Demand
400
900
200
500
2000
4.2
Example 2
Consider one more transportation problem taken from Munapo and Kumar [2, p. 132]
as given below in Table 9.
Table 9. The transportation model
1
2
3
4
5
6
Supply
1
8
9
20
4
4
6
20
2
2
1
1
1
1
1
30
3
2
8
19
7
3
14
40
4
2
6
6
4
11
8
50
5
2
15
5
12
8
7
60
Demand
30
40
50
20
20
40
200
After subtracting 4 from element of row 1, 1 from the elements in row 2, and 2 from
the elements in row 3, 4 and 5. This results in Table 10.
Note that each column in Table 10 has a zero element, hence no further modiﬁca-
tions will arise in the modiﬁed cost elements. Following the allocation procedure, we
get allocations as shown in Table 11.
The solution in Table 11 is not feasible, hence we must update the cost elements in
Table 11 following the Step 4, as shown in Table 12. The minimum non-zero element is
1, which has not been crossed out. Note the horizontal lines are in row 1 and 2 and the
vertical line is in column 1.
The solution in Table 12 is still infeasible. The minimum uncrossed element is 1.
The updated table is given in Table 13.
Note the minimum uncrossed element is 1, which will be used to update the elements
as shown in Table 14.

310
S. Kumar et al.
Table 10. With at least one zero in each row
1
2
3
4
5
6
Supply
1
4
5
16
0
0
2
20
2
1
0
0
0
0
0
30
3
0
6
17
5
1
12
40
4
0
4
4
2
9
6
50
5
0
13
3
10
6
5
60
Demand
30
40
50
20
20
40
200
Table 11. Allocation in cells with modiﬁed cost 0
1
2
3
4
5
6
Supply
1
4
5
16
0 (20)
0
2
----
20, 0 →3
2
1
0 (30)
0
0
0
0
----
30, 0, →2
3
0 (30)
6
17
5
1
12
----
40, 10
4
0
4
4
2
9
6
----
50
5
0
13
3
10
6
5
60
Demand
30, 0 ↑1
----
40, 10
50
----
20, 0
20
40
----
200, 120
Table 12. Updated elements as shown below
1
2
3
4
5
6
Supply
1
5
5
16
0 (20)
0
2
20, 0 →3
2
2
0 (30)
0
0
0
0
----
30, 0 →2
3
0
5
16
4
0 (20)
11
----
40, 20
4
0 (30)
3
3
1
8
5
----
50, 20
5
0
12
2
9
5
4
60
Demand
30, 0 ↑1
----
40, 10
50
----
20, 0
----
20, 0 ↑4
40
----
200, 100
Note the minimum of those elements which have not been crossed out is 1, which is
again minimum. The updated information is given in Table 15.
The solution in Table 15 is feasible, hence optimal. Note that the optimal solution
mentioned by Munapo and Kumar [2] is 780, which is the same as obtained by the
modiﬁed Hungarian method for the transportation problem.

A Note on Solving the Transportation Model
311
Table 13. Updated elements from Table 12 as shown below
1
2
3
4
5
6
Supply
1
6
5
16
0 (20)
1
2
20, 0
2
3
0 (30)
0
0
1
0
----
30, 0 →3
3
0
4
15
3
0 (20)
10
----
40, 20
4
0 (30)
2
2
0
8
4
----
50, 20
5
0
11
1
8
5
3
60
Demand
30, 0 ↑2
----
40, 10
50
----
20,
0 ↑1
----
20,
0 ↑4
40
----
200, 100
Table 14. Updated elements from Table 13 as shown below
1
2
3
4
5
6
Supply
1
6
4
15
0 (20)
1
1
20, 0
2
4
0 (30)
0
1
2
0
----
30, 0 →3
3
0
3
14
3
0 (20)
9
----
40, 20
4
0 (30)
1
1
0
8
3
----
50, 20
5
0
10
0 (50)
8
5
2
----
60, 10
Demand
30, 0 ↑2
----
40, 10
----
50
0 ↑5
----
20,
0 ↑1
----
20,
0 ↑4
40
----
200, 50
Table 15. Updated elements from Table 6 as shown below
1
2
3
4
5
6
Supply
1
6
3
15
0 (10)
1
0 (10) 20,----
10, 0 →9
2
5
0
1
2
3
0 (30) 30, 0 →6
3
0 (20)
2
14
3
0 (20)
8
----
40,----
20, 0 →3
4
0
0 (40)
1
0 (10)
8
2
----
50,----
10, 0 →7
5
0 (10)
9
0 (50)
8
5
1
----
60, 10, 0
Demand 30, 10 0 ↑4* ----
40, 0 ↑5* ----
50, 0 ↑1* ----
20, 0 ↑8* ----
20, 0 ↑2* ----
40, 20 ------
200, 0
4.3
A Degenerate Transportation Model
We now consider a degenerate transportation model taken from Sharma [9] as given in
Table 16.
After creating a zero element in row and column, we get from Tables 16 and 17, as
given below with allocations.

312
S. Kumar et al.
Table 16. Degenerate transportation model
D1
D1
D1
Supply
S1
8
5
6
120
S1
15
10
12
80
S1
3
9
10
80
Demand
150
80
50
280
Table 17. With modiﬁes per unit transportation cost
D1
D1
D1
Supply
S1
3
0
0 (50)
120, 70
S1
5
0 (80)
1
80, 0
S1
0 (80)
6
6
80, 0 →2
Demand
150, 70
80, 0 ↑1
50, 0, ↑3
280, 70
Since the solution is infeasible, more zero elements will have to be created. Vertical
lines in column 2 and 3, and horizontal line is in row 3. The minimum of the element
not crossed out is 3. The updated values are given in Table 18.
Table 18. Updated values from Table 17
D1
D1
D1
Supply
S1
0 (70)
0
0 (50)
120, 70, 0, →4
S1
2
0 (80)
1
80, 0
S1
0 (80)
9
9
80, 0 →2
Demand
150, 70, 0
80, 0 ↑1
50, 0, ↑3
280, 70, 0
The solution in Table 18 is feasible, hence optimal. Once again, it is the same solution
as given by Sharma [9]. Normally for this problem, the test for optimality will require
5 allocations, but by the Hungarian method for the transportation problem, degeneracy
does not cause any difﬁculty for establishing the optimality of the solution.
5
Concluding Remarks
In this short note, the Hungarian method of assignment has been extended to solve the
transportation model. Optimality of the feasible solution is independent of the order of
degeneracy. Several problems were solved, and they all resulted in the optimal solution.
Testing optimality of a feasible is easy in the transportation model, and can be easily
switched on for testing and optimality.

A Note on Solving the Transportation Model
313
In this paper, since transportation model has been considered as an assignment model,
we can once again ﬁnd the Kth best solution of the transportation model, where k ≥2.
Please see, Murthy [10] and Kumar et al. [11]. In the case of a transportation model, if
we have ‘p’ number of allocations, where the value of ‘p’ is governed by the Eq. (7), the
2nd best solution to the transportation model can be determined by approaches discussed
by Murthy [10] and Kumar et al. [11] and Munapo and Kumar [2].
References
1. Hillier, F.S., Lieberman, G.J.: Introduction to Operations Research. ISBN 13:79812 59545962
(2015)
2. Munapo, E., Kumar, S.: Linear Integer Programming: Theory, Applications, Recent Develop-
ments, De Gruyter Series on the Applications of Mathematics in Engineering and Information
Sciences. ISBN 978-3-11-070292-7 (2022)
3. Taha, H.A.: Operations Research: An Introduction, Pearson Educators, 10th edn. (2017)
4. Tawanda, T.: A node merging approach to the transhipment problem. Int. J. Syst. Assur. Eng.
Manag. 8(Suppl 1), 370–378 (2017). https://doi.org/10.1007/s13198-015-0396-9
5. Winston, W.L.: Operations Research Applications and Algorithms, Duxbury Press, 4th edn.
(2004)
6. Kuhn, H.W.: The Hungarian method of assignment problem. Naval Res. Log. Quart. 2, 83–97
(1955)
7. Munapo, E., Nyamugure, P., Lesaoana, M., Kumar, S.: A note on uniﬁed approach to solving
transportation and assignment models in operations research. Int. J. Math. Model. Simul.
Appl. 5, 140–149 (2012)
8. Eppen, G.D., Gould, F.J., Schmidt, C.P.: Introduction to Management Science, 4th edn.
Prentice Hall, New Jersey (1993)
9. Sharma, J.K.: Operations Research: Theory and Applications. Trinity Press (2018)
10. Murthy, K.G.: An algorithm for ranking all the assignments in order of increasing costs. Oper.
Res. 16, 682–687 (1968)
11. Kumar, S., Al-Hasani, A., Al-Rabeeah, M., Ebehard, A.: A random search method for ﬁnding
‘k ≥2’ number of ranked optimal solution to an assignment problem. J. Phys. Conf. Ser.
1490, 1–3 (2020). https://doi.org/10.1088/1742-6596/1490/1/012063

Automatic Crack Detection Approach
for the Offshore Flare System Inspection
Teepakorn Tosawadi1(B), Pakcheera Choppradit1, Satida Sookpong2, Sasin Phimsiri1,
Vasin Suttichaya1, Chaitat Utintu1, and Ek Thamwiwatthana1
1 AI and Robotics Ventures Co.,Ltd., Bangkok, Thailand
Pakcheerac@arv.co.th
2 Skyller Solutions Co., Ltd., Bangkok, Thailand
Abstract. Asset inspection using unmanned aerial vehicle (UAV) is gaining more
attention in surveillance and exploratory engineering. UAV surveillance enables
higher efﬁciency in terms of operating budget and time. This technique is usually
used to gather asset information for detecting the anomalies, such as cracks and
rust. However, cracking detection on the images from UAV’s camera also requires
expertise to examine each image, which is time-consuming. To mitigate the issue,
automatic crack detection on the ﬂare stack images is proposed in this paper. This
research used UAV with high-resolution cameras to collect the ﬂare stack images
from the oil reﬁneries. The proposed method introduced crack detection using
three object detection models, YOLOv5, YOLOv7, and DINO. Additionally, the
Sobel operator and Canny edge detector are also applied to preprocessing images.
With an inference time of 50 ms and performance of 65.4% mAP0.5 and 32.2%
mAP0.5:0.95, the results show that YOLOv7-W6 on RGB images had the best
performance and fastest inference time.
Keywords: Flare stack inspection · Crack detection · Flare stack crack
1
Introduction
In oil and gas industry assessment inspection, the ﬂare stack of off-shore and on-shore oil
reﬁneries is one of the key components. The ﬂare stack is used for discharging unwanted
gas combustion and must be fully functional for ensuring safety in the petrochemical and
reﬁnery sectors. The failure can cause devastating damage to both human society and the
environment. With this consequence, the ﬂare stack is required to be inspected routinely.
In recent years, UAVs with the high-resolution camera was deployed for inspection task
widely. An inspection using UAVs beneﬁts faster and safer inspection. Additionally, the
reﬁnery can operate normally during the inspection process. However, the above method
requires inspector expertise, which is costly and consumes time. Each inspector also has
unique skills and experience, that result in different analysis results on the same image.
Additionally, expert fatigue and throughput can lead to error inspection. Therefore, the
tools are required to set standardization and relieve inspector workload.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 314–323, 2024.
https://doi.org/10.1007/978-3-031-50158-6_31

Automatic Crack Detection Approach for the Offshore Flare…
315
Many organizations are aware of crack detection. The earlier research failed to detect
the crack in our dataset. The reason is the datasets used in many previous works were
obtained from controlled environments. Moreover, most of them focus on concrete and
road surfaces. For ﬂare stack crack, there is no open dataset because of the difﬁculty of
data collection. UAV with a camera is the most effective way to collect images of the ﬂare
stacks, but the dynamic scene will be an issue. Additionally, the large coverage area and
high quality of the images from UAV will affect the model’s performance and efﬁciency
since they increase the amount of model training and necessitate picture downscaling,
which results in the loss of features from small objects.
This paper investigates the most effective way to apply the object detection models
to locate the cracks on the ﬂare stack surface. Two image preprocessing techniques,
including the Sobel operator and Canny edge detector, are applied to create an image
emphasizing edges. We further investigate the effect of slicing the high-resolution image
into several small images. Three object detection models, including YOLOv5, YOLOv7,
and DINO, are trained to detecting cracks in images.
2
Related Works
2.1
Object Detection Methods
DETR with Improved deNoising anchOr boxes (DINO) [1] was developed by The Inter-
national Digital Economy Academy (IDEA). It was improved from two extensions of the
transformer-based model, namely DAB-DETR [2] and DN-DETR [3] to increase per-
formance and efﬁciency. The DINO model introduced two different backbones, namely
ResidualNetwork(Resnet)[4]andSwintransformer[5].Finally,itachievedgoodperfor-
mance with both small model size and tiny data size compared with other State-of-the-art
models.
You only look once (YOLO) [6] was developed to focus on real-time prediction by
Joseph Redmon et al. Later that, the YOLO families such as YOLOv2 [7], YOLOv3
[8], YOLOv4 [9], YOLOv5 [10], YOLOv6 [11] and YOLOv7 [12] were published in
order to enhance the original YOLO model. YOLOv5 was developed by Ultralytics. It
suggested the spatial pyramid pooling (SPP) technique which improves the detection
of small objects. It is also the most widely utilized in particle applications due to the
model’s ability to export in a variety of formats, including TensorFlow-lite and Tensorrt-
engine. YOLOv7 is the most recent version of the YOLO family, which offers signiﬁcant
improvements through the use of focal loss and nine anchor boxes. As compared to other
YOLO models, it achieved the fastest inference speed.

316
T. Tosawadi et al.
2.2
Crack Detection
There are several approaches to detecting cracks in images. These techniques can be
divided into two categories: conventional computer vision approach and deep learning
approach[13–15].Computervision-basedmethods,suchasthresholding,edgedetectors,
and matched ﬁltering algorithm, have the disadvantage of being sensitive to light noise
and requiring preprocessing and postprocessing. Therefore, many papers introduce the
machine learning approach to detect cracks. Haﬁz Suliman Munawar et al. applied Cycle
Generative Adversarial Network (CycleGAN) [13] to detect the damages on roads,
buildings, and bridges. Qin Zou et al. developed an end-to-end road crack segmentation
method called DeepCrack [14] by a deep hierarchical convolutional neural network
(CNN). Another road crack segmentation was introduced by Ju Huyan et al., namely
CrackU-net [15]. It improved the other traditional method by the use of advanced CNN
and “U”-shaped model architecture.
3
Methodology
3.1
Data Acquisition
This research is conducted on our image dataset. The data were gathered from the off-
shore and on-shore oil reﬁnery ﬂare stack. An image is taken using a high-resolution
camera attached to the UAV, operated by an experienced inspector. The data was taken
under actual ﬂare stack inspection operation where the light condition is dynamic and
uncontrollable. Even though exposure and brightness are adjustable using camera set-
tings, the standard policy remains unable to be exactly deﬁned. Due to data scarcity and
high operation costs, we can collect the data of 780 images in total. The data contains a
set of images with 4:3 and 16:9 aspect ratios, and resolution varies from 1600 × 1200
to 6784 × 3816 pixels. The dataset composes of various operation sites, different ﬂare
stack categories, and a diversity of shot types.
3.2
Proposed Method
In this work, crack detection using object detection is proposed to ﬁnd a capability
and suitable approach for ﬂare stack inspection applications. The proposed method is
illustrated in Fig. 1. We introduced a combination of three input types and three object
detection models. Firstly, we fed raw RGB input images to each object detection model to
produce a baseline experiment result. Then, we applied two edge detection techniques,
including the Sobel operator and the Canny edge detector, to emphasize the edge of
cracks.
Typically, the RGB image is used as an input for deep learning object detection since
it was developed to eliminate some complex handcrafted feature extraction. However,
in some speciﬁc tasks like crack detection, adding preprocessing using traditional image
processing techniques may enhance deep learning performance. Input images are pro-
cessed before they were fed into deep-learning object detectors instead of RGB images.
The Sobel operator is constructed using 3 × 3 kernels to ﬁnd a gradient in both ver-
tical and horizontal directions. Then, the gradient magnitude is combined to generate

Automatic Crack Detection Approach for the Offshore Flare…
317
Fig. 1. The overview of proposed method.
an image with edge feature enhancement. A Canny edge detector is developed on the
Sobel operator with additional improvement. We also applied a Gaussian ﬁlter to blur the
image in the Sobel operator. Applying the Gaussian ﬁlter is necessary since it resulted
in edge detection with less noise. Since the object detection was pre-trained with three-
channel data, preprocessed images are also converted from grayscale to three-channel
RGB images.
Regarding proposed object detection approaches, YOLOv5 [10] and YOLOv7 [12]
are selected for CNN-based detectors. YOLOv5 is the most popular YOLO architecture,
which was published by Ultralytic. Recently, YOLOv7 is the latest version proposed by
Alexey Bochkovskiy, the original author of YOLO. There are many variant subsidiary
models scaling based on architecture complexity, parameters, and input size. Since we
want to preserve the quality and features of drone images, the network that is scaled
for larger input image resolution is chosen. In the transformer-based architecture, we
trained the detector using a state-of-the-art DINO algorithm. DINO was placed with two
types of backbone which are ResNet101 and SwinTransformer Small.
4
Experimental Results and Discussion
The experiment in this research was conducted to answer the following research
questions:
Q1: Which is an appropriate combination of preprocessing and object detection?
Q2: Which approach gives the best result in terms of cost-accuracy compensation?
Q3: Is the image-slicing approach suitable for crack detection on a multi-scaled dataset?
4.1
Data Preparation
The image dataset is randomly split into the train, validation, and test subset. Our dataset
consisted of 70% train images, 10% validation images, and 20% test images. The objects
are annotated with one class category deﬁned as crack. A random split is performed ﬁve
times using different random seeds. Each model will be trained ﬁve times repeatedly,
then averaging evaluation matrices to reduce performance ﬂuctuation.

318
T. Tosawadi et al.
4.2
Experiment Setup
In the proposed method, each algorithm has many variants of its based architecture
and many ﬁne-tuning options. The experiment must be conducted in a similar limited
environment. Thus, the model capabilities are explored on similar standards and avoid
a biased performance result comparison. All the experiments used pre-trained models.
The model was trained on obtained input images with resolution 1280 × 1280 pixels
across every experiment. This resolution was selected based on the largest input size of
the pre-trained model in YOLOv5 and YOLOv7. It also applied to the DINO algorithm
to create a comparable experiment, although the transformer backbone was pre-trained
using smaller input images. The model variants of each architecture were considered
based on a limited hardware resource on NVIDIA T4 with 16GB GPU memory. A study
on the capability of the detector mainly relies on three following models: YOLOv5-l6,
YOLOv7-W6, and DINO with Swin-S transformer backbone and Resnet101 backbone.
Each object detection approach also provides a variety of preset setups for data aug-
mentation and hyperparameters. However, hyperparameter and augmentation options
remain unchanged as default setup or recommendation settings.
4.3
Answer to Q1: Which Is an Appropriate Combination of Preprocessing
and Object Detection?
This experiment section explored the effect of image preprocessing techniques on the
performance of object detection models. We aimed to study whether the preprocessing
method is suitable for UAV inspection applications.
(a)
(b)
(c)
Fig. 2. Comparison of image preprocessing methods. a Cropped RGB image focusing on crack
area. b Output from Sobel operator. c Output from Canny edge detector.
The experiment attempts to compare the output from the Sobel operator and Canny
edge detector on RGB images. The results are shown in Fig. 2, preprocessing methods
were performed on raw RGB images at full resolution. Figure 2a illustrates a crack
area that was cropped from full resolution image. Preprocessed inputs with the Sobel
operator and Canny edge detector are shown in Figure 2b, c, respectively. The output
of the Sobel operator shows that unnecessary background details were eliminated while

Automatic Crack Detection Approach for the Offshore Flare…
319
enhancing the metal crack features. In contrast, the result of the Canny edge detector
can be clearly noticed that some part of a crack was removed as well. This outcome is
caused by hysteresis thresholding which is a part of the Canny edge detector. Therefore,
we selected the Sobel operator as a preprocessing step for input data.
Table 1. Model performance comparison
Model
Params (M)
RGB
Sobel
mAP0.5
mAP0.5:0.95
mAP0.5
mAP0.5:0.95
YOLOv5-m6
35.7
0.6534
0.3132
0.5642
0.2594
YOLOv5-l6
76.8
0.6536
0.321
0.5782
0.278
YOLOv7-W6
70.4
0.654
0.322
0.565
0.2686
DINO-SwinS
69.3
0.592
0.2834
0.478
0.2246
DINO-Resnet101
66.4
0.567
0.2752
0.4516
0.2112
We proceeded to the model training experiment with RGB image and preprocessing
method using the Sobel operator. The results are shown in Table 1. The model’s perfor-
mance was compared using mAP0.5 and mAP0.5:0.95 from COCO metrics. YOLOv5-
l6 and YOLOv7-W6 achieved approximately 65% mAP0.5 and 32% mAP0.5:0.95 on
RGB images. In the transformer-based DINO algorithm, DINO with SwinS backbone
has 59.2% mAP0.5 and 28.3% mAP0.5:0.95. Applying a ResNet101 as a backbone gave
a slightly lower performance at 56.7% mAP0.5 and 27.5% mAP0.5:0.95.
Considering the outcome using the Sobel operator, the performance is drastically
dropped when compared to RGB input. The trend of model performance is similar to
RGB images in both metrics. YOLOv5 and YOLOv7 still have almost identically to
each other and gave higher performance than DINO. Decreasing detection performance
is affected by the blurry images that have the consequence of missing edge detection.
Due to harsh wind conditions in real operation, a blurry image cannot be avoided in the
UAV inspection task. Although an image stabilizer and post-processing are embedded
in UAV’s camera, there are some blurry images remained in our data.
In summary, the results indicate that the best solution for crack detection is train-
ing YOLOv7-W6 using RGB images. YOLOv7-W6 noticeably outperforms the DINO
algorithm and has slightly fewer parameters than YOLOv5-l6. Digital image processing
techniques like the Sobel operator and Canny edge detector heavily suffered from this
scenario. Thus, we recommended performing object detection on raw RGB images.

320
T. Tosawadi et al.
4.4
Answer to Q2: Which Object Detection Approach Is Appropriate
for Deployment?
In the model deployment phase, memory usage and inference time are critical. These
two factors are directly reﬂected in the budget cost. More memory allocation requires a
better GPU for higher memory capacity. The computing power of GPU also correlated
with inference time. This section discusses budget efﬁciency for model deployment on
AWS resources. The budget is calculated based on instance type and instance time usage.
In our application, a well-trained model will be deployed on ml.g4dn.xlarge AWS EC2
instance which is an entry-level GPU instance. AWS SageMaker Notebooks Instances
in the US West (Oregon) region were utilized to conduct all of the experiments in this
study. This instance is powered by NVIDIA Tesla T4 with 16 GB GPU memory and price
$0.7364 per h. An inferences time is measured during model prediction, not including
Non-maximum Suppression (NMS) process.
Table 2. Inferences time comparison of each model
Method
Number of parameters (M)
Inferences time (ms)
YOLOv5-m6
35.7
68
YOLOv5-l6
76.8
83
YOLOv7-W6
70.4
50
DINO-Swins
69.3
920
DINO-ResNet101
66.4
950
The hardware condition is deﬁned, so less inference time leads to better budget
efﬁciency. Alternatively, sacriﬁcing a model complexity help to reduce inference time but
also decreases detection performance. To investigate further on speed-accuracy trade-off,
we have trained an additionally available model, YOLOv5-m6. According to inference
time in Table 2, the fastest inference time is obtained from YOLOv7-W6 at 50ms.
YOLOv7-W6 has 8.35% less parameter than YOLOv5-l6, but inference time is greatly
reduced by 39.75%. On the other hand, the DINO algorithm required almost 1 s in both
backbones. YOLOv7-W6 is outstandingly faster by 20 times.
Comparing the inference time and mAP in Tables 1 and 2, the result shows that
YOLOv5-m6 cannot overcome the inference time of YOLOv7-W6 by compensating an
accuracy. Although YOLOv5-m6 has 50.71% of YOLOv7-W6 parameters, YOLOv7-
W6 performed faster in terms of inference time and higher detection performance. From
the experiment result, we preferred YOLOv7-W6, which has no trade-off on speed
accuracy and gave a faster inference time.

Automatic Crack Detection Approach for the Offshore Flare…
321
4.5
Answer to Q3: Is Image Slicing Approach Is Suitable for Crack Detection
on Flare Stack?
In drone inspection service, images were taken in wide shots and close shots depending
on purposes. A crack on the ﬂare stack is clearly seen in close-shot images. In the wide
shot, an image contains the overall detail of inspecting an object but a crack is relatively
small to an image scale. Thus, we adopted the slicing inference technique for our crack
detection task. The experiment starts by slicing the image in the dataset to 1280 × 1280
pixels with 20% overlapped window size. Then, the models will be trained in sliced
images.
(a) full resolution image
(b) sliced image 1
(c) sliced image 2
(d) sliced image 3
Fig. 3. Crack detail comparison between full image (a) and sliced image (b)–(d).
After the slicing process, we found that the crack is unable to be clearly distinguish-
able. Figure 3b–d are sliced images from the wide shot image shown in Fig. 3a. In Fig. 3d,
we ensured that the object is cracked because the sliced images contain the full crack
objects. However, we are not capable to identify whether Fig. 3c is a crack or a large metal
gap. Training a model using an image with ambiguous features will decrease detection
performance. The sliced image in Fig. 3 shows that a slicing inference is not suitable
for multi-scale object detection. Especially, a crack detection that requires a full crack
defect to ensure correctness. The image slicing also affected the DINO performance
where the transformer-based model relied on patches position encoding.
4.6
Compare with the Previous Works
All previous research proposed pixel-wise segmentation on concrete, building, and road
crack. Their dataset usually contains a clear crack trace that passes through the entire

322
T. Tosawadi et al.
image. Moreover, the crack trace usually stands out from the material surface. In contrast,
the sizes of crack traces in our dataset are very diverse, ranging from the small dots to
the large crack line strips. The crack traces in our dataset also appear in various patterns,
such as dot, strip, rift, and hot tearing, concerning the ﬂare types. All of these patterns
are too sensitive for image segmentation.
5
Conclusion
This research proposed a combination of input images and object detection models for
crack detection on ﬂare stack. The experiments were conducted based on our data collec-
tion. The data were gathered from actual operations by the experienced inspector. RGB
images were proposed as input along with output from the Sobel operator and Canny
edge detector. The inputs were paired with three object detection models, e.g., YOLOv5,
YOLOv7, and DINO. The research ﬁndings indicate that the Sobel operator and Canny
edge detector suffered from a dynamic environment in drone imagery. Among the three
input types, RGB images gave a higher outcome in any object detection model. The
maximum detection performances are 65.4% mAP0.5 and 32.2% mAP0.5:0.95 which
is achievable by training YOLOv7-W6 on RGB images. In terms of inference speed, the
fastest inference time at 50 ms was also obtained by using YOLOv7-W6. YOLOv7 has
slightly higher mAP and inference faster than YOLOv5. Moreover, YOLOv7 outstand-
ingly outperformed DINO in both mAP and inference speed. Since sliced images are
unable to identify whether an object is a crack or metal gap, slicing inference is not rec-
ommended in multi-scaled crack detection. From all conducted experiments, YOLOv7
with raw RGB image is an appropriate approach that satisﬁed detection performance
and inference speed.
Acknowledgments. This project was supported by PTT Exploration and Production Public Com-
pany Limited (PTTEP) and AI and Robotics Ventures Co., Ltd. (ARV). All in all, We would like
to express our gratitude to Skyller Solutions Company for providing us with the dataset.
References
1. Zhang, H. et al.: DINO: DETR with improved DeNoising anchor boxes for end-to-end object
detection. arXiv: 2203.03605 [cs.CV] (2022)
2. Liu, S., et al.: DAB-DETR: dynamic anchor boxes are better queries for DETR. In: Inter-
national Conference on Learning Representations. https://openreview.net/forum?id=oMI9Pj
Ob9Jl (2022)
3. Li, F., et al.: DN-DETR: accelerate DETR training by introducing query denoising. In:
Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pp. 13619–13627 (2022)
4. He, K., et al.: Deep residual learning for image recognition. arXiv (2015). https://doi.org/10.
48550/ARXIV.1512.03385
5. Liu, Z., et al.: An image is worth 16 × 16 words: transformers for image recognition at scale.
CoRR (2021). https://doi.org/10.48550/arXiv2010.11929

Automatic Crack Detection Approach for the Offshore Flare…
323
6. Redmon, J., et al.: You only look once: uniﬁed, real-time object detection. In: 2016 IEEE
Conference on Computer Vision and Pattern Recognition (CVPR) (2016). https://doi.org/10.
1109/cvpr.2016.91
7. Redmon, J., Farhadi, A.: YOLO9000: better, faster, stronger. In: 2017 IEEE Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 6517–6525 (2017). https://doi.org/
10.1109/CVPR.2017.690
8. Redmon, J., Farhadi, A.: YOLOv3: an incremental improvement arxiv.org/abs/1804.02767
(2018)
9. Bochkovskiy, A., Wang, C.-Y., Liao, H.-Y.: YOLOv4: optimal speed and accuracy of object
detection (2020)
10. Ultralytics/yolov5, Oct 2020. https://doi.org/10.5281/zenodo.4154370
11. Li, C., et al.: YOLOv6 v3.0: a full-scale reloading. https://doi.org/10.48550/ARXIV/2301.
05586. arxiv.org/abs/2301.05586 (2023)
12. Wang, C.-Y., Bochkovskiy, A., Mark Liao, H.-Y.: YOLOv7: trainable bag-of-freebies sets
new state-of-the-art for real-time object detectors. ArXiv preprint arXiv:2207.02696 (2022)
13. Munawar, H.S., et al.: Civil infrastructure damage and corrosion detection: an application of
machine learning. Buildings 12. https://doi.org/10.3390/buildings12020156
14. Zou, Q., et al.: DeepCrack: learning hierarchical convolutional features for crack detection.
IEEE Trans. Image Process. 1. https://doi.org/10.1109/TIP.2018.2878966
15. Huyan, J., et al.: CrackU-net: a novel deep convolutional neural network for pixelwise pave-
ment crack detection. Struct Control Health Monit 27, e2551. https://doi.org/10.1002/stc.
2551

The Recent Trend of Artiﬁcial Neural Network
in the Field of Civil Engineering
Aditya Singh(B)
School of Civil Engineering, Lovely Professional University, Phagwara, India
aditya.11602217@lpu.in
Abstract. This paper explains the concept of Artiﬁcial Neural Network brieﬂy in
general and it also particularly focuses the current trends and applications of ANN
in the area of civil engineering for various purposes. The author also performed
a comprehensive review of numerous scientiﬁc and research papers published in
the recent years relevant to the topic of the study, in order to ﬁnd the gaps in
the research. Further, the author collected data from various sources in order to
perform graphical analysis and support the study. Major beneﬁts and challenges
of using Artiﬁcial Neural Network are analysed on the basis of the graphs shown
in the paper.
Keywords: Artiﬁcial neural network · Convolutional neural network · Civil
engineering · Recurrent neural network · Structural engineering
1
Introduction
Artiﬁcial Neural Network is a term which was motivated from a biological human brain
and it is a subﬁeld of Artiﬁcial Intelligence. Generally it is a computational network
which was made on the basis of biological neural networks which are responsible in
making the brain’s structure. The way human brain has neurons which are intercon-
nected to the other neurons, the same way Artiﬁcial Neural Networks also have neurons
which are connected to the other neurons in different layers of the networks, such neu-
rons present in Artiﬁcial Neural Networks are called as Nodes [24]. According to the 1st
neurocomputer’s inventor Dr. Robert Hecht-Nielsen, a neural network can be described
as a computing system which was created by various simple as well as highly intercon-
nected processing components that has the ability to process info with the help of their
vibrant state response to the external inputs received [16].
1.1
Artiﬁcial Neural Network’s Basic Structure
Similar to the way the brain of a human works, Artiﬁcial Neural Networks was an
artiﬁcial copy of it with the assistance wires as well as silicon which acted as living
dendrites as well as neurons. In case of the brain of a human, it is made of around
eighty six billion nerve cells which are known as neurons. These are then linked to
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 324–333, 2024.
https://doi.org/10.1007/978-3-031-50158-6_32

The Recent Trend of Artiﬁcial Neural Network
325
other thousand cells which are called as Axons. Dendrites accepts the stimuli from an
external environ or simply said inputs with the help if sensory organs. Then, the earlier
mentioned inputs are able to generate electric impulses that can rapidly travel through
the given neural network. Then it depends upon the neuron to choose whether to not
send the message forward or to send it forward for the next neuron to handle it. Then,
in the case of Artiﬁcial Neural Networks, they are made of multiple nodes that has the
ability to copy human brain’s biological neurons. These neurons with the help of links
are connected and it allows the neurons to interact with the other neurons smoothly.
Then the nodes are able to receive the input data, in order to execute simple operations
on it. Then such operation’s results are further passed to the next neurons. Node Value
or Activation is named to the output generated at each such nodes. It is said that each
link is connected with Weight. Artiﬁcial Neural Networks have the ability to learn which
happens by changing the values of weights [16].
1.2
Is There Any Need to Use Advanced Technologies like ANN in the Area
of Civil Engineering?
Advanced technologies like Artiﬁcial Neural Network is able to help in solving complex
problemsandhiddenproblemswhichcivilengineersnormallyfaceonaregularbasis.For
instance, Fuzzy logic, ANN are one of the important parts of ITS, which is also a major
advancement in trafﬁc and transportation engineering, which is a major branch of civil
engineering [11, 12]. Since the world is going in the direction of Construction 4.0, where
Digitalization of the construction sector is important, which is also a subﬁeld of civil
engineering [13]. So, ANN will also contribute towards the digital transformation of the
construction sector. These technologies will not only contribute towards the development
of the areas of civil engineering, but also improve the efﬁciency, quality of work, etc. in
the civil engineering domain.
1.3
Objectives
In this paper, the following objectives are considered:
• To understand the concept of Artiﬁcial Neural Network and its types.
• To understand the market trends and value of Artiﬁcial Neural Network not only in
present but also in the future.
• To understand the various subﬁelds of Civil Engineering where Artiﬁcial Neural
Networks can be applied.
2
Motivation
The author had observed that Artiﬁcial Neural Network is becoming more and more
popular as the time passes as well as the market of Artiﬁcial Neural Network is expected
to grow rapidly in the Asia Paciﬁc region in the future. Then, a lot of research is still
being going on to apply Artiﬁcial Neural Network in the subﬁelds of Civil Engineering.
Being a Civil Engineer, the author felt that it is necessary to highlight and understand
the concept of ANN as well as its growing applicability in area of Civil Engineering
along with expected increase in the market value in the future.

326
A. Singh
3
Literature Review
De Souza et al. worked on comparing adaptive as well as non-adaptive optimization
algorithms aimed at ANN training which can be used for the purpose of damage diagnosis
especially in the civil structures [1]. Yang et al. studied ANN and they worked on a review
paper where ANN was applied in a subﬁeld of civil engineering which was transportation
engineering, particularly in pavement engineering area [2]. Falcone et al. studied ANN
and they worked in regards to the technical feasibility forecast of seismic retroﬁtting in
the already present Reinforced Concrete Structures [3]. Freitag et al. studied ANN and
they utilized it on optimizing structural topologies which was reliability based one [4].
Wang et al. studied ANN and they amalgamated it with damage parameters in order to
forecast beginning lifetime of fretting fatigue cracks [5]. Safoklov et al. studied ANN
and they used it on overhaul in addition on aircraft maintenance repair model [6]. Li
et al. studied ANN and they used it to forecast the thermal conductivity of the given soils
which was done on the basis of an orderly database [7]. Khaleghi et al. studied ANN
and they used it to characterize fracture from the given noisy displacement data [8].
El Jery et al. studied ANN and they used it with numerical simulation to forecast heat
transfer as well as hydrodynamic in a given geothermal heat exchanger in order to attain
a certain tube’s optimal diameter with the minimum entropy by utilizing water nanoﬂuid
or Al2O3 as well as water [9]. Jin et al. studied ANN and they used it to forecast the
chloride diffusivity of a certain aggregate concrete which is a recycled one [10].
4
Research Gaps
Based on the recent advancement of the technology many researchers are implementing
ANN successfully in the various branches of civil engineering, which can be observed
through the recent scientiﬁc and research publications. However, particularly in the
branches of civil engineering like water resource engineering as well as construction
technology and management, the current researches are insufﬁcient and more research
can be done using ANN in the future.
5
Main Focus of the Paper Along with Issues and Problems
The author explained the concept of Artiﬁcial Neural Network and its basic structure in
this paper. The author covered the history of ANN brieﬂy and major type of ANN in the
paper. Current market trend of ANN was highlighted by performing graphical analysis by
the author. Some major beneﬁts of using ANN in the area of civil engineering along with
major problems were discussed. However, it can’t be ignored that many civil engineers
are still unfamiliar about ANN or the ways to implement it practically in a project. Then
terrain and location problems along with long time required to learn ANN for engineers
with different background and so on, creates major obstacles in applying ANN on a large
scale in civil engineering projects.

The Recent Trend of Artiﬁcial Neural Network
327
6
Different Types of Artiﬁcial Neural Networks
There are various kinds of artiﬁcial neural networks, which are as follows [17]:
• Recurrent Neural Networks
• Modular Neural Networks
• Convolutional Neural Networks
• Feed-forward Neural Networks
• De-convolutional Neural Networks.
Recurrent Neural Networks
They are a complex type of neural network, as they work by saving the processing node’s
output as well as feed the acquired result back into the given model. Through this way
the mode learns the way to forecast a given layer’s outcome. Every node in this model
behaves as a memory cell, which continues the computation in addition to execution of
the given operations.
Modular Neural Networks
They consists of numerous neural networks which work independently, without relying
on the other neural networks. At the time of the computation process, these networks
never inhibit the activities of the other networks or they don’t even communicate with
the other networks.
Convolutional Neural Networks
They are considered as part of the most prevalent models which are actively used at
present. They uses a variation of multilayer perceptron in addition to it consists of at
least one or more than one convolutional layers which has the ability to completely
pooled or connected.
Feed-Forward Neural Networks
They are the simplest type of neural networks. With the help of numerous input nodes,
they pass info in one given direction till the point it reaches the output node. FFNN
can have or hidden node layers or they might not even have it, which also makes its
functioning highly interpretable.
De-convolutional Neural Networks
They can be said opposite of CNN model as they use a reverse of the latter model process.
Their goal is to search for the missing signals or features which initially might have been
thought as insigniﬁcant to the system’s task of CNN.
7
Methodology
In this section the recent trends and applications of Artiﬁcial Neural Network applied in
the various branches of civil engineering is explained through the following ﬂow chart
shown in Fig. 1.

328
A. Singh
Artificial Neural 
Network
Civil Engineering
Transportation Engineering: 
control, automobile scheduling, 
truck brake system diagnosis, 
routing systems, automotive, crack 
propagation on concrete pavement, 
road safety and accidents.
Disaster Management: 
prediction of natural 
calamities.
Construction Technology and 
Management: project bidding, 
quality inspection systems, 
project planning and 
management, analysis of 
welding quality, appraisal of 
real estate, etc.
Water Resource Engineering: 
detecting loss of water through cracks 
(pipe system present in underground); 
analysis, modelling as well as 
mathematical formulation of water 
distribution systems; rehabilitation 
and retrofitting of water networks.
Geotechnical Engineering: 
examination of a) liquefaction 
of soil, b) foundation studies, c) 
soil – pile interactions, d) 
loading conditions as well as 
potential failures.
Structural Engineering: 
studying of concrete behaviour: 
shear strength development 
based on beam’s different 
dimensions, crack propagation, 
failure prediction. Structural 
health monitoring, composite 
structure modelling and so on.
Fig. 1. Showing the trends and applications of ANN in the area of civil engineering [14, 16]
8
Result and Discussion
In this section, data from various sources like Data Bridge, Emergen Research, etc.
are collected by the author to perform graphical analysis, in order to support the study
(Tables 1, 2 and 3).
9
Advantages of Artiﬁcial Neural Network
In this section, some major advantages of Artiﬁcial Neural Network particularly in the
area of civil engineering are discussed, which are as follows:
• According to Fig. 2, the market size of Artiﬁcial Neural Network from the year 2019
is sharply increasing till the year 2030, which means that more and more application
of ANN is required in the various branches of civil engineering which might deﬁnitely
be helpful in the growth of civil engineering.

The Recent Trend of Artiﬁcial Neural Network
329
Table 1. Showing the comparison of expected market size of ANN by different sources [15,
18–22, 25]
Market size of ANN
In million USD
Year 2019 by MarketsandMarkets
117
Year 2021 by Emergen Research
160.8
Year 2021 by Proﬁcient Market Insights
169.3
Year 2021 by Data Bridge
171.58
Year 2021 by Maximize Market Research Pvt. Ltd.
177.49
Year 2022 (estimated) by Research and Markets
171.88
Year 2024 (expected) by MarketsandMarkets
296
Year 2027 (expected) by Research and Markets
258.45
Year 2028 (expected) by Proﬁcient Market Insights
542.3
Year 2029 (expected) by Maximize Market Research Pvt. Ltd.
763.2
Year 2029 (expected) by Data Bridge
793.69
Year 2030 (expected) by Emergen Research
743
Table 2. Showing the expected and past NN software market [23]
Neural network software market
In (USD)
Year 2020
8,300,000,000
Year 2026 (expected)
50,660,000,000
Table 3. Comparing market revenue share of North America with the rest of the world [18]
Market revenue share
In (%)
North America in 2021
30.8
Rest of the world in 2021
69.2
• According to Fig. 3, the software market of Neural Network is expected to grow
tremendously by the year 2026, which means that more NN software must be utilised
in the various branches of civil engineering to improve the efﬁciency and quality
aspects.
• According to Fig. 4, the market revenue share of ANN in North American continent
is signiﬁcantly high in the world, which can also be related to the advancement and
development of the North American continent is also higher than most of the countries
present in other continents of the world.

330
A. Singh
In Million USD
0
200
400
600
800
Market Size of ANN 
Fig. 2. Showing the comparison of expected market size of ANN by different sources [15, 18–22,
25]
0
20,000,000,000
40,000,000,000
60,000,000,000
In (USD)
8,300,000,000
50,660,000,000
Neural Network Soware Market
Year 2020
Year 2026 (Expected)
Fig. 3. Showing the expected and past NN software market [23]
• According to Figs. 2 and 3, there is a major expansion of ANN through its growing
practical applications or the need and use of various types of NN software for different
situations, similar growth of ANN can be expected in the area of civil engineering in
the future.
• Based on the above ﬁgures, positive trend of ANN can be seen in the future, so it
will be wise to focus on the practical development and implementation of ANN in
the area of civil engineering as well.

The Recent Trend of Artiﬁcial Neural Network
331
0
10
20
30
40
50
60
70
In (%)
30.8
69.2
Market Revenue Share
Rest of the world in 2021
North America in 2021
Fig. 4. Comparing market revenue share of North America with the rest of the world [18]
10
Major Challenges of ANN in the Area of Civil Engineering
In this section, some major challenges of using ANN in the civil engineering area, are:
• Based on Figs. 2 and 3, the traditional civil engineers might face difﬁculty in getting
used to ANN in their day to day work in the future.
• According to the above ﬁgures, civil engineers are needed to learn ANN in their
university courses or some practical training is required for them to successfully
implement ANN in their domain based on different situations in the future.
• Incorporating ANN on a large scale civil engineering works and projects will also
be a big challenge for people who are familiar to accomplishing their tasks though
traditional ways.
11
Conclusion
In this paper, the basic concept of ANN was explained and its structure was discussed.
The author talked about the types of ANN in brief in the paper. The trends and applica-
tions of ANN in the subﬁelds of Civil Engineering was brieﬂy mentioned in this paper.
Further, the author performed some graphical analysis after collecting data from differ-
ent sources to check the future trends, market value of ANN in the paper. Then, some
major advantages as well as challenges of using ANN in the area of civil engineering
were discussed in short in the paper, based on the analysis of the graphs. As ANN is
expected to grow tremendously in the future, it is better to make all the civil engineers
prepared to adapt and learn ANN technologies in order to implement them in projects
at will.

332
A. Singh
References
1. de Souza, C.P.G., Kurka, P.R.G., Lins, R.G., de Junior, J.M.: Performance comparison of non-
adaptive and adaptive optimization algorithms for artiﬁcial neural network training applied
to damage diagnosis in civil structures. Appl. Soft Comput. 104, 107254 (2021)
2. Yang, X., Guan, J., Ding, L., You, Z., Lee, V.C.S., Mohd Hasan, M.R., Cheng, X.: Research
and applications of artiﬁcial neural network in pavement engineering: a state-of-the-art review.
J. Trafﬁc Transp. Eng. (Engl. Ed.) 8(6), 1000–1021 (2021)
3. Falcone, R., Ciaramella, A., Carrabs, F., Strisciuglio, N., Martinelli, E.: Artiﬁcial neural
network for technical feasibility prediction of seismic retroﬁtting in existing RC structures.
Structures 41, 1220–1234 (2022)
4. Freitag, S., Peters, S., Edler, P., Meschke, G.: Reliability-based optimization of structural
topologies using artiﬁcial neural networks. Probab. Eng. Mech. 70, 103356 (2022)
5. Wang, C., Li, Y., Tran, N.H., Wang, D., Khatir, S., Wahab, M.A.: Artiﬁcial neural network
combined with damage parameters to predict fretting fatigue crack initiation lifetime. Tribol.
Int. 175, 107854 (2022)
6. Safoklov, B., Prokopenko, D., Deniskin, Y., Kostyshak, M.: Model of aircraft maintenance
repair and overhaul using artiﬁcial neural networks. Transp. Res. Proc. 63, 1534–1543 (2022)
7. Li, K.-Q., Kang, Q., Nie, J., Huang, X.: Artiﬁcial neural network for predicting the thermal
conductivity of soils based on a systematic database. Geothermics 103, 102416 (2022)
8. Khaleghi, M., Haghighat, E., Vahab, M., Shahbodagh, B., Khalili, N.: Fracture characteriza-
tion from noisy displacement data using artiﬁcial neural networks. Eng. Fract. Mech. 271,
108649 (2022)
9. El Jery, A., Khudhair, A.K., Abbas, S.Q., Abed, A.M., Khedher, K.M.: Numerical simulation
and artiﬁcial neural network prediction of hydrodynamic and heat transfer in a geothermal
heat exchanger to obtain the optimal diameter of tubes with the lowest entropy using water
and Al2O3/water nanoﬂuid. Geothermics 107, 102605 (2023)
10. Jin, L., et al.: Prediction of the chloride diffusivity of recycled aggregate concrete using
artiﬁcial neural network. Mater. Today Commun. 32, 104137 (2022)
11. Bhowmik, S., Singh, A., Misengo, C.: A case study on intelligent transport system using
trafﬁc lights. Our Herit. 67(7), 96–110 (2019)
12. Singh, A.: Importance of fuzzy logic in trafﬁc and transportation engineering. In: Vasant, P.,
Zelinka, I., Weber, G.W. (eds.) Intelligent Computing and Optimization. ICO 2021. Lecture
Notes in Networks and Systems, vol. 371, pp. 87–96. Springer, Cham (2022)
13. Singh, A.: The signiﬁcance of digitalization of the construction sector. In: Vasant, P., Weber,
G.W., Marmolejo-Saucedo, J.A., Munapo, E., Thomas, J.J. (eds.) Intelligent Computing and
Optimization. ICO 2022. Lecture Notes in Networks and Systems, vol. 569, pp. 1069–1077.
Springer, Cham (2022)
14. CivilDigital: https://www.google.com/amp/s/civildigital.com/all-about-artiﬁcial-neural-net
work-ann-in-civil-engineering/amp/
15. Data Bridge Market Research: https://www.google.com/amp/s/www.databridgemarketresea
rch.com/reports/global-artiﬁcial-neural-network-ann-market/amp
16. Tutorialspoint: https://www.tutorialspoint.com/artiﬁcial_intelligence/artiﬁcial_intelligence_
neural_networks.htm
17. TechTarget:
https://www.google.com/amp/s/www.techtarget.com/searchenterpriseai/defini
tion/neural-network%3famp=1
18. Emergen Research: https://www.emergenresearch.com/amp/industry-report/artiﬁcial-neural-
network-market
19. MarketsandMarkets: https://www.marketsandmarkets.com/Market-Reports/artiﬁcial-neural-
network-market-21937475.html

The Recent Trend of Artiﬁcial Neural Network
333
20. BioSpace:
https://www.biospace.com/article/artiﬁcial-neural-network-market-size-worth-
usd-743-0-million-in-2030-emergen-research-/
21. GlobeNewsWire: https://www.globenewswire.com/news-release/2022/11/14/2554650/0/en/
Artiﬁcial-Neural-Networks-Market-2022-Will-Revenue-to-Cross-reach-US-542-3-million-
by-2028-Research-by-Business-Opportunities-Top-Companies-report-covers-Market-spe
ciﬁc-challenge.html
22. Business Wire: https://www.businesswire.com/news/home/20220920006083/en/Global-Art
iﬁcial-Neural-Network-Market-to-Reach-258.45-Million-by-2027---ResearchAndMarkets.
com
23. Mordor Intelligence: https://www.mordorintelligence.com/industry-reports/neural-network-
software-market
24. Javatpoint: https://www.javatpoint.com/artiﬁcial-neural-network
25. Maximize Market Research: https://www.maximizemarketresearch.com/market-report/glo
bal-artiﬁcial-neural-network-market/83873/

Analyzing Price Forecasting of Grocery
Products in Bangladesh: A Comparison of Time
Series Modeling Approaches
Md Mahmudul Hoque1, Ikbal Ahmed1
, Nayan Banik2
,
and Mohammed Moshiul Hoque3(B)
1 Department of Computer Science and Engineering, CCN University of Science and
Technology, Kotbari, Cumilla 3506, Bangladesh
2 Department of Computer Science and Engineering, Comilla University, Kotbari,
Cumilla 3506, Bangladesh
3 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chittagong 4349, Bangladesh
moshiul_240@cuet.ac.bd
Abstract. Bangladesh is a south Asian agriculturally reliant country producing
essential grocery products. Among such products, the most notable commodities
are rice, wheat ﬂour, and lentils (massor). The unforeseen increase in those product
prices puts wholesalers, retailers, dealers, and buyers in tension as the purchas-
ing ability remains the same. Several factors are liable in this scenario, making
price forecasting by category an extremely challenging task for decision-makers.
Investigation of underlying factors helps policymakers monitor product prices
and keep the market in equilibrium. Several studies addressed this issue from
Bangladesh’s perspective. However, there is still scoped to perform further analy-
sis to better model the forecasting premises. This study focuses on using multiple
time series analysis of three common grocery goods (rice, lentils, and wheat ﬂour)
and comparing several modeling approaches by Mean Absolute Error (MAE).
The experiment covers forecasting of items in divisional regions on a monthly
basis. Evaluation of the results showed that the RNN and Naïve Ensemble model
accurately generalizes price changes. This work aims to help future research in
this domain by conveying guidelines and comparative analysis of other modern
approaches.
Keywords: Price forecasting · Prediction · Machine learning · Time series
analysis · Evaluation
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 334–341, 2024.
https://doi.org/10.1007/978-3-031-50158-6_33

Analyzing Price Forecasting of Grocery Products…
335
1
Introduction
Though most people in Bangladesh live within the food purchase limit, 14% of the peo-
ple are still living in poverty [6]. Many low-income individuals, particularly those who
live on the borderline, are critical sufferers. The general people have been protesting the
uncontrolled increase in commodity prices. Nonetheless, the price ﬂuctuation of grocery
items has increased substantially due to alliances. The general population has recently
been spotted lining up behind Trading Corporation of Bangladesh (TCB) cars in search
of fair-priced goods. The most talked-about subject right now is the unexpected spike in
grocery costs. Therefore, forecasting grocery items is essential nowadays. In addition,
predicting for businesses, the weather, risk management, national development, and gro-
cery prices are commonplace today. There is a total area of 15 million hectares, more
than half of Bangladesh’s land used for agricultural purposes [12]. Bangladesh, a self-
sufﬁcient country in food production, heavily relies on a few staple commodities such
as rice, wheat ﬂour, and lentils, which are an integral part of the daily diet. However, due
to its developing and middle-income status, many farmers and low-income individuals
struggle to afford these essential commodities when prices are high. Although the gov-
ernment is trying to curb the price hike caused by unscrupulous traders, it is not always
successful. Therefore, time series forecasting is essential at all industry levels.
This study analyzes using machine learning techniques to forecast commodity prices
based on the purchase history. There are numerous time series forecasting algorithms,
including Facebook Prophet (FB Prophet), Long Short-Term Memory (LSTM), Expo-
nential Smoothing, Autoregressive Integrated Moving Average (Arima), and Seasonal
Autoregressive Integrated Moving Average (Sarima) [2]. In this study, we used 12 Time
Series algorithms and Compared them by measuring their performance (MAE). Predict-
ing the prices of these essential commodities in advance could greatly aid in regulating
the market prices, as the general public would be willing to purchase them at the prede-
termined price, and sellers would be obliged to sell at that price. Such an approach could
potentially disrupt market syndicates that aim to manipulate prices to their advantage.
This study will beneﬁt individuals who plan to work in this ﬁeld in the future and those
who set the retail price of a product.
2
Related Work
Hasan et al. [8] focused on predicting rice prices based on one-year data. They investi-
gated ﬁve machine learning methods (KNN, NB, DT, SVM, and RF), where RF achieved
the highest prediction accuracy (70%). Hossain et al. [9] used CNN and linear regression
models to predict paddy prices. They surveyed local markets for predictions of paddy
pricesandfoundsomefactorsforﬂuctuationsinpaddyprices.Ganesanetal.[1]proposed
a model to make a day-ahead prediction of food sales in one of the top multiplexes in
India. They investigated prices using XGBoost, Artiﬁcial Neural Network (ANN) Extra
Trees, and LSTM. To predict airfare prices in Greece, Tziridis et al. [13] applied eight
machine learning algorithms to predict air ticket prices where the Bagging regression
tree performed best than other techniques by gaining 87.42% accuracy. Pudaruth et al.
[11] employed various ML techniques to predict the price of used cars in Mauritius.

336
M. M. Hoque et al.
They employed four machine learning techniques (LR, KNN, Naïve Bayes, and Deci-
sion Trees) to anticipate used car prices. Didem et al. [7] used LSTM and Facebook
Prophet (FBP) algorithm to forecast future trends in Brent oil prices based on historical
prices to improve accuracy and stability. The authors gathered weekly Brent crude oil
price data from the NASDAQ Commodities. The LSTM model performed well in accu-
racy in predicting brent oil, with an R2 value of 0.89. Chadalavada et al. [3] predicted
electricity demand for households, ofﬁces, or buildings using the FBP and Autoregres-
sive Integrated Moving Average (Arima) model. The data was gathered from the UCI
Machine Learning Repository, and they predicted data for one month. The accuracy of
different models varies depending on the dataset.
3
Methodology
Figure 1 illustrates the proposed approach of price prediction consisting of data
collection, preprocessing, and model training.
Fig. 1. Proposed method of price prediction
3.1
Data Collection
The information was gathered from the Humanitarian Data Exchange (HDX) website
[4]. It is a free online platform that allows users to share data from various disasters and
organizations [5]. There are 18,735 datasets available on the HDX websites. However,
we will be working with Bangladeshi food prices. The HDX collects information from
the “World Food Programme (WFP)” and updates the dataset monthly based on WFP
Market Monitoring data. WFP is a UN agency that monitors domestic food prices in
more than 80 countries [10]. Every month, the WFP publishes a “Market Monitor”
report; this organization works in several areas, including market monitoring. The United
Nations World Food Programme oversees food aid. It is the world’s largest humanitarian
organization dedicated to preventing hunger and ensuring food security [14]. Rice, wheat
ﬂour, lentils, and palm oil include 7732 entries and 14 wholesale and retail pricing
attributes. Table 1 shows the statistics of the dataset.

Analyzing Price Forecasting of Grocery Products…
337
Table 1. Dataset statistics
Location
Period
Total samples
Dhaka
Nov 2005–Dec 2021
2584
Chattogram
Jan 2006–Dec 2021
1110
Rajshahi
Jul 2006–Dec 2021
1040
Khulna
Oct 2005–Dec 2021
1063
Barishal
Dec 2006–Dec 2021
825
Sylhet
Jan 2007–Dec 2021
763
3.2
Data Preprocessing
For data preparation, we utilized the Scaler transformer in Darts. Before converting it to
a time series object in Darts, the scaler ﬁrst constructed a random time series with 100
data points. Then use the ‘ﬁt’ technique to ﬁt an instance of the Scaler transformer to
the time series. The transform technique transforms the time series to the desired range.
The parameters determined during the relevant phase apply the scaling transformation
to the time series. The ‘inverse transform’ technique returned the scaled time series to
its original scale.
Table 2 illustrates various features and their corresponding dataset descriptions.
However, it should be noted that additional attributes within the dataset have not been
included in this table. These attributes include Unit, Currency, Country, Unit ID, Market
ID, Commodity ID, and Category ID. While these attributes may not have been explic-
itly discussed, they are nevertheless essential components of the dataset and should be
considered when analyzing the data.
3.3
Model Training
This study exploited 12 algorithms, including TCN (Temporal Convolutional Network),
NBEATS (Neural Basis Expansion Analysis for Interpretable Time Series Forecasting),
(T) BATS (Trend, Seasonal, and Exogenous (external) Bayesian Time Series), Auto-
ARIMA, Theta (Thresholding Expectation-Maximization), Naïve Drift, Naïve Seasonal,
Facebook Prophet, ARIMA, FFT (Fast Fourier Transform) RNN (Recurrent Neural
Network). The suggested work modiﬁed several factors to reduce forecasting inaccuracy.
For instance, 40 epochs were sufﬁcient to minimize error for NBEATS when using
Temporal Convolutional Network, where epochs were 400. RNN required 500 epochs
to perform better, in any case.

338
M. M. Hoque et al.
Table 2. Attribute and feature description of the dataset
Attribute name
Description
date
The primary attribute within this dataset is a monthly time series data
cmname
The name of commodities is a important attribute within this dataset, which
includes four distinct items: rice, wheat ﬂour, Lentils, and Palm oil. The
attribute includes pricing information for both wholesale and retail categories
for each commodity
category
The dataset comprises four commodity items, classiﬁed into three categories:
‘cereals and tubers’, ‘pulses and nuts’, and ‘oil and fats’
admname
The name of regions is another attribute within this dataset, which includes
seven primary regions. However, for this study, we utilized data from six of
these regions
price
The price of commodities is the target attribute within this dataset
mktname
The market name is another crucial attribute within this dataset, which
includes divisional and district-wise information. However, for the purposes
of this research, we focused solely on the divisional data
4
Results and Discussion
To implement the time series algorithms, ‘Darts’ was employed, while graphical repre-
sentations were visualized using Plotly Express. 70% of the data were utilized for train-
ing, while the remaining 30% were used for validation. This study uses the MAE (Mean
Absolute Error) measure to evaluate the performance. Figure 2 shows the percentage of
the MAE score of rice price prediction.
Results show that the MAE score for Sylhet is 1.92, whereas it is 2.86 for Dhaka.
In contrast, Chattogram, Rajshahi, Khulna, and Barishal divisions of the RNN model
behaved well, with corresponding MAE scores of 1.99, 2.79, 2.47, and 1.98. The Naïve
Ensemble model performed admirably and provided reduced inaccuracy for the divisions
of Sylhet and Dhaka.
Figure 3 shows the prediction of wheat ﬂour in various divisions. For the Chat-
togram and Rajshahi Divisions, the RNN model performed as expected better than any
other model, with MAE scores of 1.37 and 0.59, respectively. On the other hand, Naïve
Ensemble provided the Khulna and Barishal divisions with less inaccuracy, and the MAE
scores are 0.62 and 0.71, respectively. Out of 12 models, the TCN and NBEATS model
impressively outperformed them all for the Dhaka and Sylhet divisions. The MAE score
for the Sylhet division of the NBEATS model is 0.38, while the TCN model’s score for
the Dhaka division was 0.79.

Analyzing Price Forecasting of Grocery Products…
339
Fig. 2. Prediction of rice prices (MAE score)
Fig. 3. Prediction of wheat ﬂour prices (MAE score)
Figure 4 shows the percentage of the MAE score each model for lentils received in
each division. The chart compares the MAE scores for models with a 12-times series.
The chart shows that the Naïve Ensemble model performed exceptionally well across
all divisions and that RNN also provided reduced error while achieving the same score
as the Naïve Ensemble for the Barishal division. The Barishal division of RNN and
Naïve Ensemble received a 4.97 MAE score. Moreover, Naïve Ensemble received MAE

340
M. M. Hoque et al.
scores of 4.83 in the Dhaka division, 4.54 in the Chattogram division, 5.03 in the Rajshahi
division, and 4.99 in the Khulna 5.65 in the Sylhet division.
Fig. 4. Prediction of lentils prices (MAE score)
5
Conclusion
Commodity prices are unpredictable and usually increasing in nature in Bangladesh.
Several factors (such as increased demand, lower supply, rising production costs, unfair
warehousing practices, and lacking market monitoring) contribute to grocery items’ price
increases. Moreover, the prices of grocery items varied from time to time. This work
investigated the prices of the three most consumed items (rice, wheat ﬂour, and lentils)
in six divisions of Bangladesh. Multiple time series regressor models are exploited to
analyze the monthly periodic data and evaluate each model’s performance using MAE.
Results showed that the RNN and Naïve ensemble models performed better in predicting
prices in most cases. In the future, we aim to utilize deep learning techniques to forecast
more grocery items weekly with greater accuracy.

Analyzing Price Forecasting of Grocery Products…
341
References
1. Adithya Ganesan, V., Divi, S., Moudhgalya, N.B., Sriharsha, U., Vijayaraghavan, V.: Fore-
casting food sales in a multiplex using dynamic artiﬁcial neural networks. In: Science and
Information Conference, pp. 69–80. Springer (2019)
2. Advancing Analytics: 10 incredibly useful time series forecasting algorithms, 15 Feb
2022. https://www.advancinganalytics.co.uk/blog/2021/06/22/10-incredibly-useful-time-ser
ies-forecasting-algorithms
3. Chadalavada, R., Raghavendra, S., Rekha, V.: Electricity requirement prediction using time
series and Facebook’s prophet. Indian J. Sci. Technol. 13(47), 4631–4645 (2020)
4. Humanitarian Data Exchange: Bangladesh—food prices. https://data.humdata.org/dataset/
wfp-food-prices-for-bangladesh
5. Humanitarian Data Exchange: The humanitarian data exchange (2014). https://data.humdata.
org/
6. Giménez, L., Jolliffe, D., Sharif, I.: Bangladesh, a middle income country by 2021: what will
it take in terms of poverty reduction? Bangladesh Dev. Stud. 37(1 & 2), 1–19 (2014)
7. Güleryüz, D., Özden, E.: The prediction of brent crude oil trend using lstm and facebook
prophet. Avrupa Bilim ve Teknoloji Dergisi 20, 1–9 (2020)
8. Hasan, M.M., Zahara, M.T., Sykot, M.M., Nur, A.U., Saifuzzaman, M., Haﬁz, R.: Ascer-
taining the ﬂuctuation of rice price in Bangladesh using machine learning approach. In: 2020
11th International Conference on Computing, Communication and Networking Technologies
(ICCCNT), pp. 1–5. IEEE (2020)
9. Hossain, K.A., Raihan, M., Biswas, A., Sarkar, J.P., Sultana, S., Sana, K., Sarder, K.,
Majumder, N.: Paddy disease prediction using convolutional neural network. In: Intelligent
Computing and Optimization: Proceedings of the 4th International Conference on Intelligent
Computing and Optimization 2021 (ICO2021), vol. 3, pp. 268–276. Springer (2022)
10. World Food Programme: Where we work (1961). https://www.wfp.org/countries
11. Pudaruth, S.: Predicting the price of used cars using machine learning techniques. Int. J. Inf.
Comput. Technol. 4(7), 753–764 (2014)
12. Sarkar, J.P., Raihan, M., Biswas, A., Hossain, K.A., Sarder, K., Majumder, N., Sultana, S.,
Sana, K.: Paddy price prediction in the South-Western region of Bangladesh. In: Intelligent
Computing and Optimization: Proceedings of the 4th International Conference on Intelligent
Computing and Optimization 2021 (ICO2021), vol. 3, pp. 258–267. Springer (2022)
13. Tziridis, K., Kalampokas, T., Papakostas, G.A., Diamantaras, K.I.: Airfare prices prediction
using machine learning techniques. In: 2017 25th European Signal Processing Conference
(EUSIPCO), pp. 1036–1039. IEEE (2017)
14. (WFP): Un world food programme (1961). http://wfp.org

Big Data: Identiﬁcation of Critical Success
Factors
Leo Willyanto Santoso(B)
Petra Christian University, 121–131 Siwalankerto, Surabaya 60296, Indonesia
leow@petra.ac.id
Abstract. Nowadays, the organization try to implement big data technology. By
implementing it, they can develop big data analytics as a tools for competitive
advantage. Top management realize this phenomenon and should respond to the
challenges of information revolution. Data-driven business decision gains more
and more prominence. Data could be used to see exactly what’s happening in
organization and use the information to make the business more agile. However,
there are several factors that are critical for a Big Data project’s success. The
objective of this research is to recognise the important critical success factors to
obtain organizational value from big data. We evaluate each factor from panel that
consist of 18 professionals to make a rank based on the degree of importance by
using Delphi method. Delphi methodology was divided in three phases and two
rounds. The test result revealed that there are ﬁve main critical success factors
are: (1) aligning big data strategy to customer insights and business goals; (2)
having adequately IT infrastructure; (3) having technologies and tools used to
enable big data analytics processes; (4) data-driven culture in the organizations
and management; (5) automated or semi-automated data analytics tool.
Keywords: Big data · Critical success factors · Business goals · Organizational
value · Delphi
1
Introduction
New technologies capable of collecting and analyzing data know and understand the
client better, leading to a decision-making process more informed. According to IBM,
around of 2.5 quintillion bytes of data every day [1]. The same source and consultant
Gartner further points out that by 2015 4.4 million data scientists will be needed [2]. It can
be said that the importance of optimizing management of data within the organization,
becoming a critical factor in success in obtaining competitive advantage [3].
The combination of the Internet and democratization of the creation and different
formats, has given rise to many types of data. The data thus increased in volume, variety,
and velocity. This phenomenon has been called the big data [4].
The potential impact of big data on organizations continues to increase, the manage-
ment structures and processes, thus giving rise to the need for basic and applied research
among the various disciplines (such as information systems, marketing, accounting,
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 342–349, 2024.
https://doi.org/10.1007/978-3-031-50158-6_34

Big Data: Identiﬁcation of Critical Success Factors
343
human Resources). It will be necessary develop and improve analytical methods
appropriate to big data consider the volume, variety, speed and accuracy of data.
Although many organizations can use the provided insights [5], it is also required
to consider the increasing importance of data to improve the supply of products and
services [6].
Thus, the research question of this study is “what are the critical success factors more
important to withdraw value of the big data?”. Intends clarify the factors that beneﬁt a big
data strategy as well as organizational beneﬁts at a time when unstructured data grows 15
times more than structured data [7]. When using the Delphi methodology, this research
intends to elaborate an exploratory study [4, 8]. The study has the following structure:
ﬁrst, a review of the literature on big data in Sect. 2, a brief critic of success, critical
success factors of IS, and ﬁnally a focus on the critical success factors for withdrawing
big data value. Section 3 discusses the methodology, the presentation of the panel, as
well as collection and processing of data. Section 4 presents the analysis of the results
obtained, discussion and confrontation with the literature. Finally, Sect. 5 expresses the
conclusions, practical implications, limitations and research related to the subject under
study.
2
Critical Success Factors in IS and Big Data
The terminology of critical success factors has acquired different interpretations since
it became popular in the 1980s [9]. This author deﬁned them as areas whose results, if
they are satisfactory, allow a competitive performance of the organization. In this way,
it is essential to identify the critical factors. Critical success factors can still be used
as assistance in the organization’s planning processes, to improve the communication
(internal or external), and to assist in the development of information systems [10]. There
are a number of factors beyond the control of management, and determine the success
or otherwise of a project [11], these are the critical success or failure. On the other hand,
the critical successes can also as resources, capabilities, and organizational attributes
that are essential for performance [12]. The organizations can develop different critical
success factors according to the with the structure, competitive strategy, position in
industry, location, and environmental and temporal factors, integrated with the strategic
objectives [13]. It’s already recognized the importance of the critical success factors
since they end up inﬂuencing the achievement of the organization’s strategic objectives.
According to the survey, it indicates that about 44% of IS projects are delivered out
of time, above budget and without meeting the requirements [14]. In addition, about 24%
are canceled before completion or are delivered but never used. These data highlight the
need to overcome a multiplicity of obstacles in order to achieve results happened.
The critical success factors in IS as small claims of limited number areas where
“everything must run well” for the SI function to succeed [15]. The author carried
out case studies nine organizations, and after the interviews the author arrived at two
main conclusions: ﬁrst, the critical success factors of IS different from organization to
organization, however it is possible to identify a common set of four critical success
factors. Secondly, each department of IS establishes its management tools, techniques
and processes to achieve goals in critical areas.

344
L. W. Santoso
The set of four critical IS success factors identiﬁed in all the study organizations were:
(1) the tools, techniques and processes should be in the right places in the organization
so that the management of effective; (2) make sure that people are working in the correct
project. With limited resources, the organization’s priorities should be carefully selected;
(3) the services delivered by IS should be on time and within budget. Just as you should
that the service that is delivered to you is good, otherwise the quality of the service
becomes irrelevant; (4) Finally, IS priorities should be aligned with business needs and
organizational goals.
In turn, the concept of success in IS can be understood as the ﬁnal efﬁciency in
carrying out the task for which the information system is developed [16]. The authors
set 12 case studies to small and medium-sized enterprises success in adopting IS. From
the results, the authors identiﬁed two sets of factors: (1) secondary factors, which are
important but not critical to success. Examples of these factors are the availability of
ﬁnancial resources, the quality of the software market, availability, and quality of human
resources; (2) the second set of factors were classiﬁed as determinants, since clearly
demonstrate why certain organizations are more successful in the adoption of IS. These
factors are the IS skills (people and available knowledge in IS), and the attitudes and
perspectives of the management adoption and use of IS.
It indicates that approaching people, processes and technologies maximizes the
chances of success in the implementation of IS, while at the same time enhancing the
organizational beneﬁts expected top management [17].
For this study we adopted interpretation of critical success factors as the essential
factors to achieve success. While withdrawing value means getting insights, information
and to originate knowledge relevant to the organization (e.g. a pattern of consumption;
identify a need that translates into a new product; identify a threat). In this way, we intend
to identify what critical success factors are most important for withdraw value from the
big data. For this component of the study, analyzed scientiﬁc articles, books, content of
different types, among them press releases, online articles and news.
3
Methodology and Data
The main research question of this study is “what are the factors critics of success more
important to withdraw value of the big data?”. Following the collection of critical success
factors based on the literature review, this research proposes a qualitative exploratory
study and a Delphi rankings [18], to better understand the subject.
For this study we considered the Delphi methodology of rankings that uses nonpara-
metric statistics as the most appropriate for several reasons. First of all, because it will
be necessary to set out the assumptions and information that different interpretations
and seek to generate consensus in a group [19]. Secondly, the Delphi methodology is an
appropriate way to work with opinions and perspectives rather than objective facts [18].
Thirdly, it is suitable for studies which seek to investigate an uncertain and speculative
question, where the general population is not able to questions raised by the research
[20].
Finally, because it is intended to present the results in a way that their understanding is
easy,makingthemaccessibletoresearchers,academics,andactorsintheindustry.Delphi

Big Data: Identiﬁcation of Critical Success Factors
345
rankings can still be advantageous for the initial phase of theoretical developments, such
as the big data, since they help the researcher to identify the variables of interest and
generate propositions.
3.1
First Phase
The ﬁrst phase focuses on the collection of information about members of the panel, and
the selection and compilation of the list items [14]. The ideal panel size is ten to eighteen
people and a maximum of four panels at the same time [20]. The constitution of the panel
aimed at the selection of professionals who are knowledgeable, when they stated that they
should be made up primarily of persons with respond to the investigator’s inquiries. The
identiﬁcation of these professionals was made through direct contacts with consultants
and professionals in the IS sector, and academic leaders. The panel was composed by
six professionals assigned to big data projects and activities, ﬁve consultants, three IS
teachers, one journalist specialized in IS, a CTO of an organization specialized in an
IT partner, an organization partner focused on the development and implementation of
software solutions, and a director of a start up which also operates in the IT sector.
Regarding the characterization of the educational qualiﬁcations, it was found that 29%
had degree, 40% master’s degree, 20% doctorate and 11% postgraduate.
All panelists were contacted in advance via email address, where they were explained
the methodology and the objectives, to guarantee participation in this study.
At this stage, it was decided to identify the main questions from the literature review.
It was decided to create the list with 20 items since it seemed to be a satisfactory
quantity,basedonthethreecriticalaspectsoftheDelphi[19]:thetimemustbeconsidered
for the study; the complexity and quantity of issues that the must respond and therefore
panel fatigue. This is how it came to be conclusion that more items could discourage
participation in this study.
3.2
Second Phase
It indicates that at this stage a list with 10–20 items is maximum [18]. Therefore, it was
decided to establish as objective identify the 10 critical success factors to most important
date according to the panel. The list of identiﬁed and asked the panel to according to a
Likert-type numerical scale [21, 22]. The values presented by the scale went from one,
equivalent to nothing important, up to ten, equivalent to very important. The literature
presents different views regarding the ideal size of the scales. The ideal size of the
scale should reﬂect above all quality [23], i.e. the ability to faithfully demonstrate the
attitude/opinion that is being measure. In this way it is necessary to seek a balance: scales
with few classiﬁcations may lose the discriminatory capacities of the respondents; per
other scales with too many ratings can go beyond the respondents’ real capacities [24].
In this study, we chose 10-class scales. The 10-point scales present the advantages when
compared to the most used ﬁve and feel points: they offer greater variations; higher
measurement accuracy; opportunities to detect change and powers for explanations.
Larger scales also help minimize problems of benevolence, central tendency, and the
“halo” effect [25].

346
L. W. Santoso
Considering the hourly availability of panel members, it was decided to use online
surveys. The second phase of the study began on April 12, 2020 and ended on May 2,
2020. The questionnaires were sent to the eighteen and the response rate was 100%.
3.3
Third Phase
For this phase it was decided to carry out one to three at the most because of this is the
ideal number. In turn the rate of response to each round was at least 70%, since this is
the minimum rate indicated by the same authors for the study to become relevant.
Regarding the level of consensus, for this study the value proposed equal to 0.5
representing high agreement. However, if this value is not reached, the process should
be ﬁnalized. On the other hand, the coefﬁcient of Spearman’s correlation between rounds
is intended to be the nearest possible of 1.
At this stage, a speciﬁc resource was used for surveys designed to the online survey
tool, which allows you to create rankings. It was then asked the panel to order the ten
factors, placing ﬁrst the most important and tenth or the least important. This phase
started on May 07, 2020, and closed on the day May 21, 2020. The response rate was
83%. At this stage it was also SPSS tool for data processing (Table 1).
Table 1. Final ordering of critical success factor (CSFs).
Ranking
Critical success factor
Point
1
Aligning big data strategy to customer insights and business goals
8.2
2
Having adequately IT infrastructure
7.6
3
Having technologies and tools used to enable big data analytics processes
7.3
4
Data-driven culture in the organizations and management
6.5
5
Automated or semi-automated data analytics tool
5.8
6
Possess Business Intelligence practices
5.1
7
Ability to innovate and adapt the business model
4.7
8
The organization should allow and encourage experimentation
4.5
9
Organization should be able to implement big data scalable solutions
3.3
10
Supported decision making in automated algorithms
3.2
At this stage it is still given the hypothesis of the panel to comment or justify the
ranking created. However, only the following comment was received: The option was
to give relevance to the big data aspects aligned with the strategy. Without a strategy
that recognizes the contribution of the big data little sense would measures. Then the
tactical factors and dependent on infrastructure, for example, it makes no sense to think
of scalable solutions without adequate infrastructure.

Big Data: Identiﬁcation of Critical Success Factors
347
4
Analysis and Discussion
After the data collection it was possible to validate that although the panel was formed by
knowledgeable professionals. Having these concordance values, the study may become
a reﬂection of the big data, which still does not seem to meet much consensus.
It became clear, the importance of aligning the big approaches organizations with
their strategic objectives. The big data approach should be seen as strategic rather than
as a discipline within the from you. In this way it is possible to deﬁne: objectives; an
eventual ROI; the impacts; the possibility of improving processes; and other important
criteria for obtaining success in a big data project. The second position in both rounds
was occupied by CSF is owning adequate IT infrastructure. The beneﬁt from big data
is clear, it is inevitable to improve the technological infrastructure of organization and
may be reﬂected in changes in applications and business processes. The IT infrastructure
must a low and predictable catch latency and simple queries to data; to be capable of
handling large volumes of transacted volumes, many times in distributed environments;
and support ﬂexible data structures and dynamics. “Having new models and tools capable
of processing large volumes of data” was the third placed in both rounds. Organizations
should increase their ability to analyze new volumes of data. It is in this the need to
develop new models for the vast amount of data currently generated.
The fourth position was occupied by the CSF “Organizational Culture Oriented for
the management of the data”. The impact of the big data, focusing on the performance
of organizations oriented to the management of data.
At the bottom of the top 5 was “Automated or semi-automated data analysis”. This
analysis are techniques to detect patterns, identify anomalies, and extract knowledge.
The news forms of computation that combine statistical analysis, processes and artiﬁ-
cial intelligence, are able to build statistical models from large data sets. For example,
Netﬂix uses automatic recommendation system for predicting the interests of the client
comparing their movie history with a statistical model generated by from the habits of
millions of other customers. The sixth place in the ﬁnal round was occupied by “Possess-
ing Business Intelligence practices (such as predictive analytics and big data antics”).
More insightful organizations already recognize the importance of analytics. This due
to analytics lead to a better understanding of the customer and the Marketplace. The
best performing organizations had analytics efﬁcient as a factor of differentiation. In
this way the main barrier found to gain a competitive advantage from the big data was
the lack of understanding how to use analytics to improve the organization—essentially
capture, represent and deliver. The big size of the data sets does not automatically mean
that it will be possible to extract enough information from them, only that the potential
of learning is greater. It is essential to have analytical skills and in the organization to
draw the necessary conclusions.
The CSF “Ability to innovate and adapt the business model” is in seventh place
in the ranking. Organizations are bringing together large datasets, however in many
cases they do not know what to do with them. Being thus transforming this data into
information and knowledge capable of competitive advantage to the organization, it will
be necessary to develop new management skills and practices, and reshape the business
model of organization. The new model of business, regardless of the sector of activity

348
L. W. Santoso
of the organization, should understand that capacity management and data mining will
be a key factor in critical to organizational success.
TheCSF“Organizationshouldallowandexperimentation(e.g.,rapidandsmall-scale
tests) is in the eighth place. In the context of the big data, rapid and small tests are essential
because they allow for an solutions for the entire organization. The experimentation in
the big data also leads to the identiﬁcation of needs, exposing variability and enabling
an improvement in organizational performance.
Ninth place in the ranking was occupied by CSF “Organization should be capable
of implementing scalable big data solutions”. The capacities of scalability in solutions
focused on data storage and management are essential today. Whether they are structured
data or not structured. The adoption of big data solutions is essentially inﬂuenced by the
scalability characteristics the same. Since the NoSQL (Not Only SQL) databases are an
example solution capable of handling the big data, thanks to the ability to overcome the
difﬁculties of scalability present in the relational databases.
Finally, the ranking is closed by CSF “Supported Decision Making in automated
algorithms”. The explosion of data in terms of volume and format has led to the need
for new approaches to process and analyze large amounts of data in real time. Essen-
tially through new algorithms or adaptations, capable of making useful insights for the
organization in its decision.
5
Conclusion
The present research aimed to recognize and order according to with their degree of
signiﬁcance and relevance, the main critical success factors for that the organization is
able to withdraw value from the big data. The great motivation in choosing this topic is
that the data is currently—in all sectors, in all economies, in all organizations in their
business processes. The big data is still at an early stage of research.
In this way it is possible to conclude that organizations and managers should not
understand the technological trends. It is also necessary a strategic action for organiza-
tional adaptation to respond to new demands and to be able to withdraw value from the
transformations. Data-driven decision making could promote the company’s growth.
The main limitation of the present study was the degree of agreement obtained. The
result of the Kendall W value was 0.32 indicating consensus moderate. This value may
reﬂect the diversity of opinions panel, on a subject that has not yet been studied. While a
round in the third phase could increase this value, it was also considering the increasing
difﬁculty in obtaining answers and the fatigue of the panel regarding the study.
However, the study offer a fundamental knowledge for practitioners as well as for
future research. It would be interesting to differences between the critical success factors
for withdrawing big data value and to extract value from structured data.
From the obtained results it could still be valuable to try develop frameworks to
ensure that the identiﬁed CSF in the study are the metrics to measure success.
Finally it would be important to realize how the big data is being addressed and to
see if the presented factors in the study greatly differ those that the identiﬁed reality of
Indonesia organizations.

Big Data: Identiﬁcation of Critical Success Factors
349
References
1. IBM: https://researcher.watson.ibm.com/researcher/view_group.php?id=4933
2. Business
Wire:
https://www.businesswire.com/news/home/20121022006228/en/Gartner-
Says-Big-Data-Creates-Big-Jobs-4.4-Million-IT-Jobs-Globally-to-Support-Big-Data-By-
2015
3. Santoso, L.: Cloud technology: opportunities for cybercriminals and security challenges. In:
Proceedings 12th International Conference on Ubi-Media Computing, Ubi-Media, pp. 18–23,
9049539 (2019)
4. Baesens, B., Bapna, R., Marsden, J.R., Vanthienen, J., Zhao, J.L.: Transformational issues of
big data and analytics in networked business. MIS Quart. (2014)
5. Jahan, T., Hasan, S.B, Naﬁsa, N., Chowdhury, A.A., Uddin, R., Areﬁn, M.S.: Big data for
smart cities and smart villages: a review. In: Lecture Notes in Networks and Systems Book
Series (LNNS), vol. 371 (2022)
6. Buhl, H., Heidemann, J., Moser, F., Röglinger, M.: Big data—a fashionable topic with(out)
sustainable relevance for research and practice? Bus. Inf. Syst. Eng. 5(2), 65–69 (2013)
7. DeRoos, D., Deutsch, T., Eaton, C., Lapis, G., Zikopoulos, P.C.: Understanding Big Data:
Analytics for Enterprise Class Hadoop and Streaming Data. McGraw-Hill, New York (2012)
8. Agrawal, D., Bernstein, P., Bertino, E., Davidson, S., Dayal, U., Franklin, M., Widom, J.:
Challenges and opportunities with big data—a community white paper developed by leading
researchers across the United States. http://cra.org/ccc/docs/init/bigdatawhitepaper.pdf, Mar
2012 (2012)
9. Ahmed, G.: Critical success factors and why CSFs are important for business success (2014)
10. Bullen, C.V., Rockart, J.: A Primer on Critical Success Factors. Center for Information
Systems Research Working Paper, 69 (1981)
11. Belassi, W., Tukel, O.I.: A new framework for determining critical success/failure factors in
projects. Int. J. Project Manage. 14(3), 141–151 (1996)
12. Lynch, R.: Corporate Strategy. Harlow. Financial Times/Prentice Hall (2003)
13. Pal, R., Torstensson, H.: Aligning critical success factors to organizational design: a study of
Swedish textile and clothing ﬁrms. Bus. Process. Manag. J. 17(3), 403–436 (2011)
14. Deng, T., Keil, M., Lee, H.K.: Understanding the most critical skills for managing IT projects:
a Delphi study of IT project managers. Inform. Manag. 50(7), 398–414 (2013)
15. Rockart, J.F.: The changing role of the information systems executive: a critical success factors
perspective. Massachusetts Institute of Technology, Boston (1982)
16. Caldeira, M.M., Ward, J.M.: Using resource-based theory to interpret the successful adoption
and use of information systems and technology in manufacturing small and medium-sized
enterprises. Eur. J. Inf. Syst. 12(2), 127–141 (2003)
17. Turner, M.K.: Three Keys to IT System Success: People, Process, Technology (2014)
18. Schmidt, R.C.: Managing Delphi surveys using nonparametric statistical techniques. Decis.
Sci. 28(3), 763–774 (1997)
19. Hasson, F., Keeney, S., McKenna, H.: Research guidelines for the Delphi survey technique.
J. Adv. Nurs. 32(4), 1008–1015 (2000)
20. Okoli, C., Pawlowski, S.D.: The Delphi method as a research tool: an example, design
considerations and applications. Inform. Manag. 42(1), 15–29 (2004)
21. Likert, R.: A technique for the measurement of attitudes. Arch. Psychol. (1932)
22. Boone, H.N., Boone, D.A.: Analyzing Likert data. J. Ext. 50(2), 1–5 (2012)
23. Munshi, J.: A method for constructing Likert scales (2014)
24. Jacoby, J., Matell, M.S.: Is there an optimal number of alternatives for Likert scale items?
Study. Educ. Psychol. Measur. 31, 657–674 (1971)
25. Walker, D.H.T.: An investigation into factors that determine building construction time
performance (1985)

Effect of Parameter Value of a Hybrid
Algorithm for Optimization of Truss Structures
Melda Yücel, Sinan Melih Nigdeli(B), and Gebrail Bekda¸s
Department of Civil Engineering, Istanbul University-Cerrahpa¸sa, 34320 Avcılar, Istanbul,
Turkey
{melihnig,bekdas}@iuc.edu.tr
Abstract. For optimization methodologies and also metaheuristic algorithms,
there are different parameters speciﬁc-algorithm. Besides, the correct determina-
tion of the value of these parameters is an extremely important issue to reach the
desired solutions optimally in a short time. In addition to this, the performance
of single algorithms can be developed by combining them with different method-
ologies or metaheuristics. In this respect, a hybrid algorithm, which was created
via a combination of ﬂower pollination algorithm (FPA) and Jaya algorithm (JA)
as metaheuristics, is beneﬁcial to generate optimal modeling for truss structures.
Also, to detect the best properties for bar members of the truss structure are targeted
by using this algorithm. In this regard, the single phase of Jaya algorithm (JA) is
combined with the local search phase of the ﬂower pollination algorithm (FPA).
Also, all of the metaheuristics as independent algorithms, and the hybridized one
were compared and evaluated in terms of the best weight by determining vari-
ous statistical measurements like mean and standard deviation values of the best
weights.
Keywords: Truss structures · Optimization · Metaheuristic algorithms · Flower
pollination algorithm · Jaya algorithm · Hybridization · Parameter-adjusting
1
Introduction
The usage of a metaheuristic algorithm is an effective way for the optimization of engi-
neering problems and effective optimum results can be obtained. In order to increase the
efﬁciency of the algorithm, hybrid algorithms have been also developed to increase the
convergence capacity of the algorithms and prevent local optima problems. The iterative
optimization process may last too long and a small improvement may be also useful in
the practical application of the method.
As known, metaheuristics use a metaphor, but the main concern is to solve the
problemcorrectly.Inordertomakeaseriesofsimilarfeaturestodevelopanewalgorithm,
hybrid methods using the feature of existing ones may be logical.
In structural engineering, hybrid methods have been used in the optimum design of
steel frames [1], precast-prestressed beams [2], prestressed slab [3], tuned mass dampers
[4], trusses [5–7], cantilever beams [8] and retaining walls [9, 10].
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 350–358, 2024.
https://doi.org/10.1007/978-3-031-50158-6_35

Effect of Parameter Value of a Hybrid Algorithm
351
In the present study, the optimization of truss structures is done via a hybrid algorithm
which is a combination of Jaya Algorithm (JA) and Flower Pollination Algorithm (FPA).
The parameter sensitivity of the method was also tested.
2
The Hybrid Algorithm and Parameter-Adjusting Process
In the present study, a hybrid optimization algorithm is utilized for the designing of a
structural truss model. This algorithm consists of the combination of ﬂower pollination
algorithm (FPA) (was developed by Xin-She Yang in 2012 [11]) and Jaya algorithm (JA)
(was proposed by Rao in 2016 [12]. Furthermore, this hybridized algorithm was inves-
tigated in the previous study including optimal modeling of several structural designs
[13].
In this direction, to determine the optimum solutions for the desired parameters, a
speciﬁc-parameter of FPA called switch probability (sp) was evaluated. In this process,
the individual performances of JA and FPA algorithms are observed and the hybrid
algorithm comprised of JA-FPA is dealt with by adjusting the parameter as switch prob-
ability. So, for this case, different values of sp between 0.1 and 1.0 by increasing 0.1 with
constant population, and iteration numbers as 25 and 50,000 are handled, respectively.
If sp value is bigger than a random number, then the phase of JA is operated in Eq. (1).
Xnew,i = Xold,i + rand(0, 1)

gb −
Xold,i


−rand(0, 1)

gw −
Xold,i

(1)
The best and worst solutions are expressed via gb and gw. Xnew,i and Xold,i are also
the updated namely new, and old/existing solutions. Moreover, rand(0, 1) provides to
produce a random number between (0, 1).
Otherwise, the basic formulation of FPA can be seen in Eq. (2). In Eq. (3), Levy
ﬂight distribution is also described as a mathematical situation.
Xnew,i =
sp < rand(0, 1) Xold,i + rand(0, 1)(Xj,i −Xk,i)
sp ≥rand(0, 1) Xold,i + Levy(gb −Xold,i)
(2)
Levy =

1
√
2
	
ε−1.5e

−1
2ε

(3)
Here, if sp value is smaller than the produced random number, the local pollination phase
of FPA was applied by following Eq. (2) and added to the hybrid algorithm. Also, Xj,i
and Xk,i reﬂect the two different solutions, which are selected as randomly among all of
the current solutions.
3
Numerical Examples
3.1
Details of Truss Model
While the optimization process is designed, a structural model as a 72-bar truss (Fig. 1)
is used to evaluate the performance of the hybrid algorithm. In this respect, the major aim
is also determined as the minimization of the total structural weight by optimal sizing
of bar members. On the other hand, the grouping approach in terms of truss bars is not
applied to the design process, too.

352
M. Yücel et al.
Fig. 1. Details of the structural model for a 72-bar truss with design properties [7].
Moreover, the design information including the parameters, and constant properties
can be seen in Table 1. Here, the slenderness cases must be controlled for bar members
under compression forces. For this reason, the multiple loading cases are operated on
the truss model (Table 2). Also, the mentioned structural constraints are deﬁned within
Table 3 in terms of the occurred displacement and stress factors, respectively.
Table 1. Some details for operating of the optimization process.
Property
Notation
Ranges/Values
Unit
Design parameters
Bar cross-section
As
0.1–3.0
in.2
Design constants
Elasticity modulus
Es
107
psi
Weight per unit of volume of
steel
ρs
0.1
lb/in.3
Bar number
–
72
–
Node number
–
20
–
3.2
Optimum Design for Truss Structure
In this application, the objective function is determined as minimizing the total structural
weight for a 72-bar truss design. While this process is realizing, the optimum parameters

Effect of Parameter Value of a Hybrid Algorithm
353
Table 2. Multiple loading conditions on nodes for a 72-bar truss model.
Case
Node
Loading conditions
Unit
Px
Py
Pz
1
17
5000
5000
−5000
lb/in.2
2
17
0
0
−5000
18
0
0
−5000
19
0
0
−5000
20
0
0
−5000
Table 3. The design constraints for a 72 bar truss structure.
Structural Member
Description
Constraints
Unit
Nodes
Displacement
All
Displacement
limitation for nodes
δ < |∓0.25|
in.
Bars
Stress for
compression force
Stress for tension
force
A1–72
Stress limitation for
bar members
σc > −25,000
σt < + 25,000
psi
for design were tried to ﬁnd, too. In this respect, to make real of this aim, both independent
metaheuristics as FPA and JA together with the hybrid algorithm as JA-FPA were utilized
by applying multiple populations, and iteration numbers as 25 and 500,000 together with
variable values of sp parameter ranging in 0.1–1 with the increment as 0.1, respectively.
In Tables 4, 5, and 6, the optimized parameters with the best weight for structure can be
seen in terms of JA, FPA, and JA-FPA algorithms, respectively.
Also, in Fig. 2, the changing of minimum weight values can be seen in terms of
different sp values for FPA and JA-FPA. According to the results, it can be recognized that
the best algorithm is JA-FPA due to the best objective function value namely minimum
total weight (305.0372 lb). It can be found by differentiating with the smallest deviation
namely the error value as 0.0321 lb in terms of all population members.
4
Results
According to the optimization results provided by the classic version of JA and FPA,
besides the hybrid algorithm as JA-FPA, which is a combined algorithm generated by
beneﬁting of local pollination phase of FPA with classic JA, it can be noticed that JA-FPA
is the most successful and effective method in terms of minimization of total structural
weightatthevalueof305.0372lbbythedifferenceas0.0321lbamongall25independent
candidate solutions (Table 6). Although the smallest value of standard deviation can be

354
M. Yücel et al.
Table 4. Optimization results with statistical measurements for design parameters provided by
the best solution value in terms of JA.
Bar
Optimum value
Bar
Optimum value
Bar
Optimum value
A1
1.8350
A25
0.7267
A49
0.1000
A2
0.1013
A26
0.1547
A50
0.1001
A3
2.9994
A27
0.1384
A51
0.1000
A4
0.1000
A28
0.7248
A52
0.1000
A5
0.1000
A29
0.1000
A53
0.1000
A6
0.6946
A30
0.6876
A54
0.1000
A7
0.1072
A31
0.1008
A55
0.2826
A8
0.7692
A32
0.1021
A56
0.6515
A9
0.7767
A33
0.1000
A57
0.6478
A10
0.1000
A34
0.1001
A58
0.4721
A11
0.6954
A35
0.1000
A59
0.8638
A12
0.1000
A36
0.1000
A60
0.1000
A13
0.1000
A37
0.5284
A61
0.6577
A14
0.1000
A38
0.3900
A62
0.1004
A15
0.1000
A39
0.8949
A63
0.1005
A16
0.1004
A40
0.2806
A64
0.6481
A17
0.1000
A41
0.1000
A65
0.1000
A18
0.1001
A42
0.7761
A66
0.8743
A19
1.6881
A43
0.1025
A67
0.1000
A20
0.1000
A44
0.7557
A68
0.1000
A21
2.2099
A45
0.7531
A69
0.1006
A22
0.1351
A46
0.1104
A70
0.1000
A23
0.7218
A47
0.7398
A71
0.8934
A24
0.1000
A48
0.1006
A72
0.1000
Minimum weight
308.3894
Mean of minimum weights
308.4156
Std. deviation of minimum weights
0.0129
sp
–
Total iteration
50000
Population
25
found with FPA for minimum weights towards all of the candidate solutions, JA-FPA
is more effective to reach the best weight value. On the other side, the comment can be
made that this algorithm namely the hybrid method as JA-FPA has an extremely stabile

Effect of Parameter Value of a Hybrid Algorithm
355
Table 5. Optimization results with statistical measurements for design parameters provided by
the best solution value in terms of FPA.
Bar
Optimum value
Bar
Optimum value
Bar
Optimum value
A1
1.8917
A25
0.7961
A49
0.1011
A2
0.1019
A26
0.1097
A50
0.1000
A3
2.2641
A27
0.1068
A51
0.1006
A4
0.1106
A28
0.7683
A52
0.1001
A5
0.1004
A29
0.1046
A53
0.1008
A6
0.7083
A30
0.7255
A54
0.1002
A7
0.1103
A31
0.1001
A55
0.2576
A8
0.8000
A32
0.1001
A56
0.4132
A9
0.8221
A33
0.1000
A57
0.5457
A10
0.1006
A34
0.1004
A58
0.3957
A11
0.6964
A35
0.1001
A59
0.8982
A12
0.1048
A36
0.1002
A60
0.1000
A13
0.1001
A37
0.5213
A61
0.7076
A14
0.1003
A38
0.2660
A62
0.1000
A15
0.1000
A39
0.7169
A63
0.1014
A16
0.1001
A40
0.2970
A64
0.6309
A17
0.1002
A41
0.1019
A65
0.1009
A18
0.1000
A42
0.8185
A66
0.8573
A19
1.6229
A43
0.1073
A67
0.1004
A20
0.2925
A44
0.7586
A68
0.1000
A21
2.0611
A45
0.7768
A69
0.1001
A22
0.2576
A46
0.1014
A70
0.1037
A23
0.7223
A47
0.8002
A71
0.8911
A24
0.1001
A48
0.1001
A72
0.1001
Minimum weight
305.3980
Mean of minimum weights
305.4480
Std. deviation of minimum weights
0.0283
sp
0.8
Total iteration
50000
Population
25
behavior in the way of providing the best amount of the minimized weight towards each
sp value in comparison to FPA (Fig. 2).

356
M. Yücel et al.
Table 6. Optimization results with statistical measurements for design parameters provided by
the best solution value in terms of JA-FPA.
Bar
Optimum value
Bar
Optimum value
Bar
Optimum value
A1
1.7761
A25
0.7543
A49
0.1003
A2
0.1045
A26
0.1233
A50
0.1009
A3
2.2557
A27
0.1296
A51
0.1001
A4
0.1000
A28
0.7925
A52
0.1000
A5
0.1005
A29
0.1027
A53
0.1000
A6
0.7309
A30
0.7351
A54
0.1002
A7
0.1013
A31
0.1003
A55
0.3094
A8
0.8001
A32
0.1000
A56
0.4120
A9
0.7964
A33
0.1000
A57
0.5644
A10
0.1000
A34
0.1000
A58
0.4112
A11
0.7411
A35
0.1000
A59
0.8855
A12
0.1057
A36
0.1000
A60
0.1003
A13
0.1000
A37
0.4034
A61
0.6993
A14
0.1000
A38
0.2826
A62
0.1004
A15
0.1001
A39
0.7022
A63
0.1000
A16
0.1000
A40
0.3117
A64
0.6237
A17
0.1000
A41
0.1000
A65
0.1050
A18
0.1000
A42
0.7858
A66
0.9056
A19
1.6487
A43
0.1000
A67
0.1011
A20
0.2611
A44
0.7404
A68
0.1001
A21
2.1146
A45
0.7519
A69
0.1000
A22
0.2476
A46
0.1005
A70
0.1010
A23
0.7581
A47
0.7834
A71
0.8963
A24
0.1039
A48
0.1005
A72
0.1000
Minimum weight
305.0372
Mean of minimum weights
305.1410
Std. deviation of minimum weights
0.0321
sp
0.8
Total iteration
50000
Population
25

Effect of Parameter Value of a Hybrid Algorithm
357
Fig. 2. Minimum structural weight according to variable sp values.
5
Conclusion
As a summarization, JA-FPA hybrid algorithm is the most effective method to minimize
the total structural weight in terms of 72-bar truss model among the other algorithms as
classical versions of JA FPA. Also, this algorithm can make a convergence to the best
amounts of weights with extremely low standard deviations of objective functions for
each candidate solution and sp values. Besides these results, it is clear that the best sp
value is determined as 0.8 in terms of reaching the minimized value of weight by both
FPA and hybrid algorithm as JA-FPA.
References
1. Akin, A., Aydogdu, I.: Optimum design of steel space frames by hybrid teaching-learning
based optimization and harmony search algorithms. World Acad. Sci. Eng. Technol. Civ.
Environ. Eng. 2(7), 739–745 (2015)
2. Yepes, V., Martí, J.V., García-Segura, T.: Cost and CO2 emission optimization of precast–
prestressed concrete U-beam road bridges by a hybrid glowworm swarm algorithm. Autom.
Constr. 49, 123–134 (2015)
3. Talaei, A.S., Nasrollahi, A., Ghayekhloo, M.: An automated approach for optimal design of
prestressed concrete slabs using PSOHS. KSCE J. Civ. Eng. 21(3), 782–791 (2017). https://
doi.org/10.1007/s12205-016-1126-9
4. Nigdeli, S.M., Bekda¸s, G., Yang, X.S.: Optimum tuning of mass dampers by using a hybrid
method using harmony search and ﬂower pollination algorithm. In: International Conference
on Harmony Search Algorithm, pp. 222–231. Springer, Singapore (2017)
5. Panagant, N., Bureerat, S.: Truss topology, shape and sizing optimization by fully stressed
designbasedonhybridgreywolfoptimizationandadaptivedifferentialevolution.Eng.Optim.
50(10), 1645–1661 (2018)
6. Omidinasab, F., Goodarzimehr, V.: A hybrid particle swarm optimization and genetic algo-
rithm for truss structures with discrete variables. J. Appl. Comput. Mech. 6(3), 593–604
(2020)
7. Bekda¸s, G., Yucel, M., Nigdeli, S.M.: Evaluation of metaheuristic-based methods for opti-
mization of truss structures via various algorithms and Lèvy ﬂight modiﬁcation. Buildings
11(2), 49 (2021)

358
M. Yücel et al.
8. Toklu, Y.C., Bekda¸s, G., Kayabekir, A.E., Nigdeli, S.M., Yücel, M.: Total potential opti-
mization using metaheuristics: analysis of cantilever beam via plane-stress members. In:
International Conference on Harmony Search Algorithm, pp. 127–138. Springer, Singapore
(2020)
9. Yücel, M., Kayabekir, A.E., Bekda¸s, G., Nigdeli, S.M., Kim, S., Geem, Z.W.: Adaptive-hybrid
harmony search algorithm for multi-constrained optimum eco-design of reinforced concrete
retaining walls. Sustainability 13(4), 1639 (2021)
10. Sharma, S., Saha, A.K., Lohar, G.: Optimization of weight and cost of cantilever retaining
wall by a hybrid metaheuristic algorithm. Eng. Comput., 1–27 (2021)
11. Yang, X.S.: Flower pollination algorithm for global optimization. In: International Conference
on Unconventional Computing and Natural Computation, pp. 240–249. Springer, Berlin,
Heidelberg (2012)
12. Rao, R.: Jaya: a simple and new optimization algorithm for solving constrained and
unconstrained optimization problems. Int. J. Ind. Eng. Comput. 7(1), 19–34 (2016)
13. Yücel, M., Bekda¸s, G., Nigdeli, S.M.: Optimization of truss structures with sizing of bars
by using hybrid algorithms. In: International Conference on Intelligent Computing and
Optimization, pp. 592–601. Springer, Cham (2021)

Brain MRI Classiﬁcation for Alzheimer’s
Disease Based on Convolutional Neural Network
Md. Saiful1, Arpita Saha1, Faria Tabassum Mim1, Naﬁsa Tasnim1,
Ahmed Wasif Reza1(B), and Mohammad Shamsul Areﬁn2,3(B)
1 Department of Computer Science and Engineering, East West University, Dhaka 1212,
Bangladesh
wasif@ewubd.edu
2 Department of Computer Science and Engineering, Daffodil International University,
Dhaka 1216, Bangladesh
sarefin_406@yahoo.com
3 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chattogram 4349, Bangladesh
Abstract. Alzheimer’s disease is a severe disorder of the brain that gradually
increases and affects the function of the brain. It mainly affects middle-aged people
or old aged person. Many researchers tried to train their model to classify or
detect Alzheimer’s disease from MRI images automatically. In this paper, we
also tried to classify four classes (Mild Demented, Moderate Demented, Non-
Demented, Very Mild Demented) of Alzheimer’s diseases using ResNet (Residual
neural network) on 6400 MRI images. In the paper, ResNet50v2 and ResNet101v2
used. By comparing their performance, ResNet101v2 gave a better result. The
model’s precision is 74%, 27%, 75%, and 54%, recall percentage is 28%, 25%,
65%, and 77%, and f1 scores are 40%, 26%, 70%, and 63% for mild demented,
moderate demented, non-demented, and very mild demented, respectively. By
applying ResNet101v2, the percentage of accuracy is 98.35%.
Keywords: Brain MRI · Alzheimer’s disease · Deep learning · ResNet · CNN
1
Introduction
Alzheimer’s disease (AD) is the type of dementia that is most prevalent. Minor memory
loss is where it starts, and it might eventually proceed to communication and environ-
mental awareness loss. One in every 100 persons has Alzheimer’s disease, an incurable
brain disorder [1]. Alzheimer’s disease typically impacts individuals who are close to
65 years old, although it can also affect young people in rare cases. Approximately
10.7% of individuals aged 65 and above have this condition. The likelihood of having
Alzheimer’s increases with age, with 5.0% of people aged 65–74, 13.1% of people aged
75–84, and 33.2% of people aged 85 or older being affected by the disease [2].
All forms of Alzheimer’s disease go through four stages: preclinical, early, middle,
and late [3]. In the past, preclinical Alzheimer’s disease was described as a condi-
tion where individuals had brain abnormalities associated with the disease despite hav-
ing normal cognitive function during their lifetime. This was typically identiﬁed upon
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 359–367, 2024.
https://doi.org/10.1007/978-3-031-50158-6_36

360
Md. Saiful et al.
examination of the brain after the person’s death [4]. Years before a person exhibits
any symptoms of the disease, preclinical stage alterations in the brain start. The most
prevalent symptoms in the early stages include forgetfulness, such as forgetting a recent
event or activity. Alzheimer’s disease can affect the brain in its middle stage, making
it difﬁcult to speak and do daily tasks. A person can also lose their memory for two to
four years, repeating the same error in a different circumstance, and so on. In the last
stage, Alzheimer’s patient ﬁnally loses the capacity to speak and care for themselves.
A person loses their capacity to identify, eat, go to the bathroom, and perform other
essential functions. They become physically unable to sit, walk, and do other activities.
Histological ﬁndings explain that AD is characterized by hyperphosphorylated tau
protein deposition in internal neuroﬁbrillary tangles and excessive amyloid protein
deposition in exterior plaques, which can affect neuronal mortality.
The hemispherical structural uniformity of the human brain is highly developed and
declines with age. Pathological diseases, such as AD, have a more signiﬁcant nega-
tive impact on this symmetry. Regional grey matter loss frequently characterizes the
anatomical hemispherical asymmetrical progression in AD, with the left hemisphere
regions being more severely and ﬁrstly affected by the degeneration process. To create
biomarkers for Alzheimer’s disease (AD), various research has looked at the extent of the
symmetric brain loss in Magnetic Resonance (MRI) Images, particularly of the temporal
areas. Shi performed a meta-analysis of MRI studies to assess shrinkage and inequity
patterns in MCI and AD. They discovered that MCI had lower bilateral hippocampal
shrinkage and atrophy than AD did and that all groups shared a consistent pattern of
left-less-than-right asymmetry, albeit varying degrees. Knowledge of anatomy, anatom-
ical landmarks and sectoral charts, and Machine Learning and Artiﬁcial Intelligence
structures that execute learning and memory analysis or study the entire brain have been
designed to assist decision-making [5].
Advanced machine learning techniques have made it possible to detect the presence
of diseases with greater reliability through the use of Positron Emission Tomography
(PET), Computed Tomography (CT), and Magnetic Resonance Imaging (MRI). These
imaging technologies rely on complex computational algorithms to theoretically iden-
tify the existence of diseases. The anatomical atrophic alterations in the brain are visible
on brain MRIs. The computerized brain MRI analysis beneﬁted greatly from machine
learning experiments employing neuroimaging data. One of the most helpful machine
learning techniques for diagnosing Alzheimer’s disease is deep learning (DL). DL mod-
els can be used to extract discriminative features from raw data automatically. The most
sophisticated DL architectures are created for image segmentation, regression, and clas-
siﬁcation using real-world images. To learn all characteristics encoded in images, these
models need a lot of data, such as the report of an MR scan of the brain. The advantage
of using DL models is that they do not require manually generated AD features because
learned features are immediately taken from input photos [6].
Our main goal behind this research is to classify AD from brain MRI Images
and implement deep learning for analyzing data. Moreover, we want to analyze the
model’s efﬁciency for AD classiﬁcation. We are trying to show the comparison between
ResNet50v2 and ResNet101v2.

Brain MRI Classiﬁcation for Alzheimer’s Disease
361
2
Literature Review
Alinsaif tried classifying AD using 3D-ST and CNN. Here, they proposed a system that
consists of two pipelines. In the ﬁrst step, they worked with MRI samples. In the second
step, they used CNN models. Applying the 3D-ST, they obtained 62–68% accuracy [7].
Acharya, Mehta, and Kumar worked with CNN, VGG16, ResNet, and AlexNet.
They mainly tried to classify AD into four categories. Transfer learning helps to reduce
training time. By using CNN, VGG16, and ResNet50, they got better results than other
models. The accuracy of the model is 95.70% [3].
Tuan, Bao, Kim, Manuel, and Tavares used 3D brain MRI to diagnose AD. They got
0.96 accuracies for segmentation as well as 0.88 for classiﬁcation. In order to segment,
or identify distinct regions or structures, GMM is employed. To classify Alzheimer’s
disease, a Support Vector Machine is utilized [8].
Ebrahimi and Luo trained MRI slices in three ways: in single-view mode using a 2D
CNN, in multi-view and single-view modes using an LSTM model, and in 3D CNN mode
using an MRI volume. Here, different MRI-based AD detection methods employing a
2D CNN methodology were employed to transfer learning between SqueezeNet and
ResNet-18 datasets. CNN signiﬁcantly enhanced the ﬁndings with transfer learning,
reaching 96.88% accuracy [9].
Murugan applied deep learning techniques, and Alzheimer’s disease is categorized
in MRI scans using the CNN model. They used six layers: input layer, convolution
layers, pooling layers, DEMNET block, dropout layers, and Dense layers. The accuracy
of DEMNET was 95.23%, and AUC was 97% [10].
Odusami, Maskeli¯unas, and Damaševiˇcius used ResNet18 and DenseNet201 models.
A technique known as a gradient class activation map was utilized to identify the speciﬁc
areaoftheMRIimage.AfterapplyingResNet18andSVM,theygot78.72%.Ontheother
hand, they got 98.21% accuracy for using ResNet18 and DenseNet201 with weights.
Comparing the two models, they got better results in ResNet18 and DenseNet201 [11].
The works in [12–16] also focused on image analysis for performing different
important tasks.
After analyzing some papers, different researchers used different models like CNN,
VGG, ResNet, AlexNet, DEMNET, DenseNet, LSTM, and SqueezeNET. By observ-
ing these models’ performance, we proposed ResNet50v2 and ResNet101v2 models to
classify AD from MRI scans.
3
Methodology
The workﬂow of our suggested model is shown in Fig. 1. Images are ﬁrst loaded and
then go through some important pre-processing steps. After that, the dataset is divided
for training and testing.
3.1
Dataset
For this research, we fetched our dataset from Kaggle [17] because the range of the
dataset is fair enough. We did not follow any process to handle this dataset because it is

362
Md. Saiful et al.
Fig. 1. A workﬂow diagram for the proposed model
already balanced. The entire dataset consisted of 6400, The data was divided into two
sets for training and testing purposes. There were a total of 717, 52, 2560, and 1792
images included in the training set for each category of Mild, Moderate, Non, and Very
Mild Demented. Additionally, there were 179, 12, 640, and 448 test images for each of
the respective categories. However, some categories, like Moderately Demented, were
represented less frequently than others. To account for this, data augmentation was used
during the training process to increase class balance. Some images are given in Fig. 2
from our dataset.
Fig. 2. Sample images from the dataset
3.2
Preprocessing
In order to achieve optimal accuracy when training a model using the provided dataset,
it is necessary to preprocess the images as they are not uniform in size. This involves
resizing all images to a standard size of 10 × 10 × 1 pixels. Additionally, data augmen-
tation is utilized during the training process to improve model performance and accuracy
in predicting results. Failure to preprocess the dataset in this way would likely result in
lower accuracy when training the model.

Brain MRI Classiﬁcation for Alzheimer’s Disease
363
3.3
Proposed Model
The focus of this study was to classify Alzheimer’s disease using MRI images as data.
A Convolutional Neural Network (CNN) was implemented for feature extraction and
classiﬁcation, with the MRI slices being used as input. The CNN model served as the
classiﬁer, grouping each image into one of four categories: Mild, Moderate, Non, and
Very Mild Demented. A ResNet-101v2 and ResNet-50v2 model was used to classify
CNN blocks.
The summary of ResNet-50v2:
Layer(type)
Output shape
Param #
input_2(InputLayer)
rescaling(Rescaling)
resnet50v2(Functional)
global_average_pooling2d(globalAveragePooling2D)
dropout(Dropout)
dense(Dense)
[(None, 176, 208, 3)]
(None, 176, 208, 3)
(None, 6, 7, 2048)
(None, 2048)
(None, 2048)
(None, 4)
0
0
23564800
0
0
8196
In total params: 23,572,966
In trainable params: 18,068,996
In non-trainable params: 5,504,000
The summary of ResNet-101v2:
Layer(type)
Output shape
Params
input_4(InputLayer)
rescaling(Rescaling)
resnet101v2(Functional)
global_average_pooling2d_1(GlobalAveragePooling2D)
dropout_1(Dropout)
dense_1(Dense)
[(None, 176, 208, 3)]
(None, 176, 208, 3)
(None, 6, 7, 2048)
(None, 2048)
(None, 2048)
(None, 4)
0
0
42626560
0
0
8196
In total params: 42,634,756
In trainable params: 8196
In non-trainable params: 42,626,560
3.4
Architecture
ResNet-101 (Residual Neural Networks) are used in the architecture for classiﬁcation
applications. In ResNet design, The (1 × 1) layers perform dimensionality reduction
and restoration before and after the (3 × 3) layer. ResNet-101 architecture includes
information on the building elements and how many blocks are stacked. In Fig. 3, the
convolutions of conv3 1, conv4 1, and conv5, and the image obtained are down-sampled.
The bottleneck blocks are unique for each stage.

364
Md. Saiful et al.
Fig. 3. Architecture of the proposed model
4
Experimental Results
In our proposed model, we used ResNet50v2 and ResNet101v2 in CNN architecture.
For ResNet50v2 and ResNet101v2, we used two types of epochs to evaluate our models:
epoch 50 and epoch 70, respectively. The accuracy was good for ResNet101v2 (epoch
70). Table 1 shows the accuracy percentage after applying ResNet101v2 (50 epochs vs.
70 epochs). In the same way, we include Fig. 4 to visualize the accuracy percentage in
graphs.
Table 1. Accuracy of ResNet101v2
Class
ResNet101v2
Epoch 50 (accuracy) (%)
Epoch 70 (accuracy) (%)
Mild-demented
61.64
100
Moderate-demented
33.98
100
Non-demented
66.81
99.24
Very-mild-demented
36.13
94.14
Table 2 shows the accuracy percentage after applying ResNet50v2 (50 epochs vs.
70 epochs). In the same way, we include Fig. 5 to visualize the accuracy percentage in
graphs.
4.1
Performance Evaluation
We have evaluated our models using different metrics such as precision, recall, and
f1-score, which are represented by Eqs. (1), (2), and (3) respectively. To determine
the overall accuracy of our model, we used Eq. (4). We have presented the percentage
values of precision, recall, F1-score, support, and accuracy for the ResNet101v2 model
in Table 3.
precision of the model =
TP
TP + FP
(1)

Brain MRI Classiﬁcation for Alzheimer’s Disease
365
Fig. 4. Model accuracy and loss for ResNet101v2
Table 2. Accuracy of ResNet50v2
Class
ResNet50v2
Epoch 50 (accuracy) (%)
Epoch 70 (accuracy) (%)
Mild-demented
51.28
100
Moderate-demented
25.63
99.85
Non-demented
54.21
99.14
Very-mild-demented
29.69
92.5
Fig. 5. Model accuracy and loss for ResNet50v2
recall of the model =
TP
TP + FN
(2)
F1 score of the model = 2(precision ∗recall)
(precision + recall)
(3)
Accuracy of the model = correct prediction
all predictions
(4)

366
Md. Saiful et al.
Table 3. Performance evaluation of the ResNet101v2 model
Alzheimer
Precision (%)
Recall (%)
F1-score (%)
Support
Accuracy (%)
Mild-demented
74
28
40
179
98.35
Moderate-demented
27
25
26
12
Non-demented
75
65
70
640
VeryMD
54
77
63
448
To train this dataset, we used ResNet50v2 and ResNet101v2. ResNet models are used
in the research because it is one of the most powerful models which can detect AD from
MRI scans. From ResNet101v2, we got better results comparatively. Figures 4 and 5
show the model’s accuracy and loss for 50 and 70 epochs, respectively. In ResNet101v2,
377 layers are used in this model. Here the accuracy of 50 epochs is 49.64%. Similarly,
the accuracy of 70 epochs is 98.35%. In Fig. 4, there are also decreases for 70 epochs
exponentially. In the ResNet50v2 model, 190 layers are used. Here the accuracy of 50
epochs is 40.20%. Similarly, the accuracy of 70 epochs is 97.87%. In Fig. 5, loss accuracy
is decreased for 50 epochs. From some previous discussions, our model accuracy was
pretty good from the other models. However, some researchers applied to VGG-16 and
ResNet50v2, for which they got 95.70%, whereas after applying Resnet101v2, we got
98.35% accuracy. For this reason, this model can be used to classify AD.
5
Conclusion
In this paper, we used the ResNet architecture to identify four classes of Alzheimer’s
disease. We use 377 layers in the ResNet101v2 model and 190 in the ResNet50v2 model.
The accuracy of ResNet50v2 is 97.87%, and the accuracy of ResNet101v2 is 98.35%
(proposed model). By comparing RestNet50v2 and ResNet101v2, ResNet101v2 gets
better accuracy. In the future, there will be a greater emphasis on categorizing and
identifying areas in Positron Emission Tomography (PET) and Functional Magnetic
Resonance Imaging (FMRI) scans related to Alzheimer’s Disease, along with region
detection.
References
1. Subramoniam, M., Aparna, T.R., Anurenjan, P.R., Sreeni, K.G.: Deep learning based pre-
diction of Alzheimer’s disease from magnetic resonance images [Online] (2021). Available:
http://arxiv.org/abs/2101.04961
2. Wong, S., et al.: Editorial Board Editor-in-Chief [Online] (2023). Available: www.elsevier.
com/locate/compmedimag
3. Acharya, H., Mehta, R., Kumar Singh, D.: Alzheimer disease classiﬁcation using transfer
learning. In: Proceedings—5th International Conference on Computing Methodologies and
Communication, ICCMC 2021, April 2021, pp. 1503–1508. http://doi.org/10.1109/ICCMC5
1019.2021.9418294

Brain MRI Classiﬁcation for Alzheimer’s Disease
367
4. Hubbard,M.,Fentont,G.W.,Anderson,J.M.:Aquantitativehistologicalstudyofearlyclinical
and preclinical Alzheimer’s disease (1990)
5. Poloni, K.M., Duarte de Oliveira, I.A., Tam, R., Ferrari, R.J.: Brain MR image classiﬁcation
for Alzheimer’s disease diagnosis using structural hippocampal asymmetrical attributes from
directional 3-D log-Gabor ﬁlter responses. Neurocomputing 419, 126–135 (2021). https://
doi.org/10.1016/j.neucom.2020.07.102
6. Sh Aaraji, Z., Abbas, H.H.: Automatic classiﬁcation of Alzheimer’s disease using brain MRI
data and deep convolutional neural networks (2022)
7. Alinsaif, S., et al.: 3D shearlet-based descriptors combined with deep features for the classiﬁ-
cation of Alzheimer’s disease based on MRI data ARTICLEINFO [Online] (2021). Available:
http://adni.loni.usc.edu/wp-content/
8. Tuan, T.A., The Bao, P., Kim, J.Y., Manuel, J., Tavares, R.S.: Alzheimer’s diagnosis using
deep learning in segmenting and classifying 3D brain MR images (2022)
9. Ebrahimi, A., Luo, S., Alzheimer’s Disease Neuroimaging Initiative: Convolutional neural
networks for Alzheimer’s disease detection on MRI images. J. Med. Imaging 8(02) (2021).
http://doi.org/10.1117/1.jmi.8.2.024503
10. Murugan, S., et al.: DEMNET: a deep learning model for early diagnosis of Alzheimer
diseases and dementia from MR images. IEEE Access 9, 90319–90329 (2021). https://doi.
org/10.1109/ACCESS.2021.3090474
11. Odusami, M., Maskeli¯unas, R., Damaševiˇcius, R.: An intelligent system for early recognition
of Alzheimer’s disease using neuroimaging. Sensors 22(3) (2022). http://doi.org/10.3390/s22
030740
12. Saha, R., Debi, T., Areﬁn, M.S.: Developing a framework for vehicle detection, tracking
and classiﬁcation in trafﬁc video surveillance. In: Vasant, P., Zelinka, I., Weber, G.W. (eds.)
Intelligent Computing and Optimization. ICO 2020. Advances in Intelligent Systems and
Computing, vol. 1324. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-68154-
8_31
13. Fatema, K., Ahmed, M.R., Areﬁn, M.S.: Developing a system for automatic detection of
books. In: Chen, J.IZ., Tavares, J.M.R.S., Iliyasu, A.M., Du, K.L. (eds.) Second International
Conference on Image Processing and Capsule Networks. ICIPCN 2021. Lecture Notes in
Networks and Systems, vol. 300. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-
84760-9_27
14. Rahman, M., Laskar, M., Asif, S., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Flower recognition
using VGG16. In: Chen, J.IZ., Tavares, J.M.R.S., Shi, F. (eds.) Third International Conference
on Image Processing and Capsule Networks. ICIPCN 2022. Lecture Notes in Networks and
Systems, vol. 514. Springer, Cham (2022). http://doi.org/10.1007/978-3-031-12413-6_59
15. Yeasmin, S., Afrin, N., Saif, K., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Image classiﬁcation
for identifying social gathering types. In: Vasant, P., Weber, G.W., Marmolejo-Saucedo, J.A.,
Munapo, E., Thomas, J.J. (eds.) Intelligent Computing & Optimization. ICO 2022. Lecture
Notes in Networks and Systems, vol. 569. Springer, Cham (2023). http://doi.org/10.1007/
978-3-031-19958-5_10
16. Ahmed, F., et al.: Developing a classiﬁcation CNN model to classify different types of ﬁsh. In:
Vasant, P., Weber, G.W., Marmolejo-Saucedo, J.A., Munapo, E., Thomas, J.J. (eds.) Intelligent
Computing & Optimization. ICO 2022. Lecture Notes in Networks and Systems, vol. 569.
Springer, Cham (2023). http://doi.org/10.1007/978-3-031-19958-5_50
17. “Alzheimer’s Dataset (4 Class of Images).” | Kaggle, https://www.kaggle.com/datasets/touris
t55/alzheimers-dataset-4-class-of-images

Drivers and Barriers for Going Paperless
in Tertiary Educational Institute
Raﬁd Mahmud Haque1, Lamyea Tasneem Maha1, Oshin Nusrat Rahman1,
Noor Fabi Shah Safa1, Rashedul Amin Tuhin1, Ahmed Wasif Reza1(B),
and Mohammad Shamsul Arein2,3(B)
1 Department of Computer Science and Engineering, East West University, Dhaka, Bangladesh
{2019-1-60-085,2019-1-60-055,2019-1-60-014,
2019-1-60-060}@std.ewubd.edu, {mcctuhin,wasif}@ewubd.edu
2 Department of CSE, Daffodil International University, Dhaka, Bangladesh
sarefin@cuet.ac.bd
3 Department of CSE, Chittagong University of Engineering and Technology, Chattogram,
Bangladesh
Abstract. Tertiary educational institutes especially universities have become one
of the major consumers of paper. Since paper uses natural resources and energy,
universities are being encouraged to turn processes paperless. However, there
are certain barriers along with drivers which lead to turning processes paperless.
Although some work has been done on similar work, most of these works are not
from Bangladesh’s perspective and do not show as-is or to-be models of processes.
This paper presents the current scenario of paperless initiatives, the impact of
going paperless, and the driving forces and barriers to making processes paperless.
Due to limitations in time and resources, only personnel working at East West
University were interviewed for this qualitative study. To analyze the data from the
interviews, the ‘Gap Analysis’ method. The interview data was then used to build
as-is and to-be models for two processes at East West University which are not
yet paperless. The drivers and barriers to these processes turning paperless were
also explored. Going paperless not only reduces paper usage, but also reduces
manpower, physical storage space, and time needed to maintain the processes.
However, barriers speciﬁc to different processes still remain which include but are
not limited to fear of data center failure and transparency maintenance.
Keywords: Paperless · Drivers · Barriers · Process model
1
Introduction
In a world full of modern technology, going paperless is a sustainable form of practice.
Going paperless – as the name describes is a process to reduce the usage of paper and
shift to digital documentation.
Going paperless is time efﬁcient, easy to store, and lastly environment friendly. As
papers are made from ﬁber that comes from trees, yearly, millions of trees are cut down
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 368–378, 2024.
https://doi.org/10.1007/978-3-031-50158-6_37

Drivers and Barriers for Going Paperless in Tertiary
369
for making paper. Going paperless can reduce the number of trees cut and hence help to
be more environmentally friendly.
Organizations use a lot of paper to store information. As a result, a lot of trees are
cut down to meet the required amount of paper used by the current population resulting
in environmental damage [1]. Moreover, using papers lead to slow transfer or loss of
information and low data security [1]. Also, manufacturing papers use water, trees,
and energy which is environmentally expensive [2]. In fact, if resource preservation,
pollution prevention, and cost reduction are considered, waste minimization is more
beneﬁcial compared to recycling [2]. Hence, organizations must focus on reducing paper
consumption rather than on ways to deal with the used paper.
To solve these issues, the concept of going paperless emerged in 1975 for ofﬁces that
were to use computers instead of paper to store information [3]. Now, when educational
institutes, especially universities, have become one of the largest consumers of paper
usage, they are being encouraged to reduce the amount of paper used. Examples of
institutes that have realized the beneﬁts of turning processes paperless include Yale
University, the University of California, and Xavier University [3].
The more papers used in tertiary educational institutes, the more resources, like trees
and energy, will be used. Since the ﬁrst emergence of the idea of turning organizations
paperless back in 1975, many universities have been trying to turn organizational pro-
cesses paperless. There have not been many empirical studies to know why universities
in Bangladesh are not implementing paperless environments at the service level and if
some universities did implement such environments what were the drivers and barriers.
This study will focus on the following objectives
1. To identify which processes can be paperless.
2. To identify the process’s as-is model and to-be model.
3. Find out the drivers and barriers of identiﬁed processes.
Thisstudywillfocusonﬁndingthedriversandbarrierstomakingprocessespaperless
in tertiary educational institutes in Bangladesh and drawing the as-is and to-be models
of those processes. This study will only be focusing on one departmental (departmental
archiving of the Computer Science and Engineering department) and one administrative
(graduation application) process at East West University.
2
Related Work
Similar work has been done related to this ﬁeld in the past but most of those works are
not empirical and not from Bangladesh’s perspective.
In one of the studies [4], the sectors which are trying to implement paperless applica-
tions in Indonesia were identiﬁed and compared with those that are moving most towards
paperless. The authors collected data from the education, government, and industry
sectors and found that the education sector uses the most paperless applications.
In another study [5], the authors described the practices and methods that minimize
paper usage. In order to reduce the usage of paper and enhance the system workﬂow, these
existing problems need to be solved. The authors mentioned the government initiatives
taken at the institutional level, ECM guidelines, etc. for making paperless administration.

370
R. M. Haque et al.
In another study [6], the negative impacts of paper-based systems on the environment
which include, increasing deforestation and hindering sustainability, are mentioned. The
paper-based system led to harmful effects on the environment. After the ﬁndings, the
authors came to the conclusion that 325 people from the paperless campus could protect
195 trees. To conclude, the paper recommends that universities should embrace paperless
campuses for ensuring a sustainable low carbon society.
A study [3] was carried out to ﬁnd paperless models in Uzbekistan universities. Paper
usagethereledtothelossofinformation,duplicationofpapers,andcorruption.Thepaper
suggests services like timetable management for students, teachers, and transportation,
online appointment, and usage of E-student cards.
In another study [1], the barriers to the implementation of paperless processes in
Nigerian universities were described. Paper-based processes were slow and unreliable.
The suggested solutions include getting political actors’ ﬁnancial support, ﬁlling the
infrastructural gap in the ICT area, training staff members, making electronics and the
internet affordable, and replacing old computers with powerful ones.
Another study [7] was carried out to ﬁnd out the writing and reading preferences
of students in a paperless classroom. The authors found that students prefer to read
short texts or articles on computers but, prefer to read longer texts or stories in printed
papers. The authors also found that students prefer to use the computer for long texts or
assignments rather than writing on paper.
In one study [8], authors attempted to seek the possibility of a paperless Higher Edu-
cation system in Nigeria which sought to unravel the concept, challenges, and prospects
of the paperless school system as a means to solving the current challenges of record
keeping in their school system. The study established that the paperless system is a 21st-
century trend and should be embraced by a higher system of knowledge and identiﬁed
areas to begin the paperless system which can be achieved with committed efforts by
management, decisionmakers/implementationpersonnel, andthegeneral school system.
In another study [9], the theory and practice of paperless classrooms are analyzed
critically, and an observational analysis of the undergraduate students of Dhofar Uni-
versity and their engagement with digital devices in the classroom has been done. The
study ﬁnds that paperless classrooms are more dynamic, engaging, and productive will
enable learners not only to develop autonomous learning, data collection, and analy-
sis, collaboration, and teamwork in the classroom but also delimit the geographical and
time restrictions which can be done by Paperless classrooms equip learners with the
technological skills.
The other study [10] aims to investigate how capable, and available the students
at senior high schools are at implementing paperless classrooms with substitutes such
as the digital mode in learning activities. For the amount of 108 students majoring in
Computer Network Engineering in the Bekasi area from different grades are chosen as
a sample and found that there are signiﬁcant inﬂuences from Environmental Awareness,
digital literacy, and habit affected the student’s readiness to implement the paperless
Concept of 41.3%.
Another study [11] was carried out to ﬁnd the student’s perception regarding paper-
less English classrooms at Japan IT University. The authors ﬁnd out that, the major 4
concerning factors for a paperless classroom are - skill, tools, vocabulary, and notes.

Drivers and Barriers for Going Paperless in Tertiary
371
The authors also described that the transition from paper to a paperless classroom won’t
be smooth. The instructors of the university have to show students, how to use digital
materials for academic purposes.
3
Methodology
3.1
Data Collection Method
The method of collecting data is one of the most necessary parts of the research process.
In order to investigate qualitative research, some examples of data collection methods
are interviews, focus groups, and observation [12]. The interview was chosen as the data
collection method so that data regarding the processes can be collected from the chosen
experienced people. For the research purpose, the authors have chosen to take interviews
in the semi-structured form which is preferable since it offers a great deal of ﬂexibility
to the interviewer.
3.2
Participants and Sampling
With the given time, the authors were able to take 5 interviews. The 5 participants of
the interview were from East West University and were interviewed to know about 2
processes; graduation application and departmental archiving.
The participants for the interview were selected based on two factors; the position
they are working in and the number of years they worked at the university. To know
about the graduation application process, the authors interviewed one person from the
Controller of Examination, who is also involved with policy-making for this process,
and another personnel who is in charge of overviewing this process. To know about
departmentalarchiving,theauthorsinterviewed2seniorfacultiesandinchargepersonnel
who have been in the CSE department for a long time and thus have good organizational
memory.
3.3
Data Analysis
For analyzing the gathered data from interviews, the authors chose the “Gap Analysis”
method. This method is a combination of narrative and grounded theory analysis. Here,
4 major steps which lead to the answers to the research questions are [13]:
• Identify the present scenario: From the interviews, authors identiﬁed 2 processes that
can be paperless. Authors have drawn their as-is process using BPMN.
• Determine the ideal scenario: Authors have drawn the to-be model of the 2 processes
using the interviewee’s suggestions.
• Highlight the gaps that exist: Authors identiﬁed the major barriers from data.
• Plans to ﬁll the gaps: Authors extracted the ideas to implement the to-be model.

372
R. M. Haque et al.
3.4
Research Ethics
The major ethical issues that need to be considered include informed consent, protection
of the conﬁdentiality of the participants, and respect for the participants’ privacy. A
consent form was provided to each participant of the interview which was to be read
by them before the researcher could take any interview. All the data that will be used,
will be kept conﬁdential and anonymous. No data will be used for the malicious purpose
which may result in any kind of distresses to the participants.
4
Results
As mentioned, qualitative data for this study was collected through interviews. The
authors interviewed each of the 2 faculty members twice. From the ﬁrst interview, they
gathered information on processes that could be paperless. From the different processes
mentioned, they chose to focus on departmental archiving and the graduation application
process. In a second interview with the faculties, the authors speciﬁcally concentrated
on the chosen two processes. The authors also interviewed 3 personnel related to these
two processes. The collected data were analyzed to ﬁnd an estimate of paper usage and
drivers and barriers to those processes.
From interviews authors took from the CSE department and Controller of Exam-
ination ofﬁce, authors found that 2 major processes can go paperless which are the
graduation application process and departmental archiving. The current models of these
2 processes are very complex and are time and paper-consuming.
The as-is model of the processes is given below
Fig. 1. As-is model of the graduation application process
In Fig. 1, we see that the graduation application process starts with the student
makingapayment.Afterthis,thestudentmustcometotheuniversityandsubmitdifferent
documents like grade reports, application forms, and course checklists. These documents

Drivers and Barriers for Going Paperless in Tertiary
373
are then sent to the Exam Ofﬁce, Registrar’s Ofﬁce, Library, Accounts, and ﬁnally to
the respective department of the student for clearance. After the ﬁnal clearance, the
documents are then sent to the Exam ofﬁce. If all the clearances are successful, the
grade report is stored in physical storage and the student is notiﬁed.
We can see that this process requires many clearances and manual conﬁrmations.
This requires a lot of time and manpower. They also maintain a student application ﬁle
for all the students which uses a lot of paper.
Fig. 2. As-is model of departmental archiving
In Fig. 2, the departmental archiving uses a huge amount of paper as they photocopy
and archive course ﬁles, mid and ﬁnal scripts, and departmental documents. This is also
time and money-consuming.
From the interviews and the author’s idea, the following to-be model is proposed.
Figure 3 shows that almost all the processes can be conducted online from submit-
ting documents to notifying different departments like the Registrar’s Ofﬁce, Library,
Accounts, and Exam ofﬁce. Without receiving paper applications physically, the univer-
sity can accept these documents through the East West University online student portal.
The ﬁles can get clearances one after the other without the need to manual handling and
send the paper documents.
A graduation application ﬁle consists of 3 papers for a student. i.e. main application,
course checklist, and grade report. According to the interviewees, storing grade reports
online is risky since they fear data center failure and hence strongly suggested physical
storage of the grade reports which means 2 pages per student can be still saved with the
proposed model.
In Fig. 4, the authors proposed scanning ﬁles rather than photocopying. After
scanning it can be stored in cloud storage or the university’s server.

374
R. M. Haque et al.
Fig. 3. To-be model of Graduation application process.
Fig. 4. To-be model of departmental archiving
If only course ﬁle archiving is considered, each semester 9 exam scripts (each con-
sisting of 8 pages on average), 3 exam questions, 1 outline, 1 grade list, and an average
of 5 lab manuals are archived for each course.
Table 1 shows the approximate pages required for the Graduation Application pro-
cess. 3 pages per student are required for this process. This number summed up to 5079
pages for all the students in the year 2022.

Drivers and Barriers for Going Paperless in Tertiary
375
Table 1. Paper usage of the graduation application process
For 1 student (pages)
For all students per year (pages)
Graduation application
3
5079 (year 2022)
Table 2. Paper usage of course archiving process
For 1 course (pages)
For all courses per year (pages)
Course archiving
82
29,192 (year 2019)
Table 2 shows the approximate pages required for the Course Archiving process. 82
pages per course is required for this process. This summed up to 29,192 pages for all
courses in the year 2019.
Paper usage of the last 5 years for both processes is given below.
Fig. 5. Paper usage of last 5 years of giving two processes.
Figure 5 summarizes the paper usage of the 2 processes between the years 2017 and
2021. The blue and orange colored plot shows the paper usage in the last 5 years of the
Graduation Application and the Departmental Archiving process respectively.
5
Discussion
Previous works do not present any estimation of paper usage, particularly on gradua-
tion applications and departmental archiving processes. Moreover, though some men-
tioned certain ways to turn different processes paperless, no as-is or to-be models of the
processes were shown.

376
R. M. Haque et al.
From the interviews taken, authors got to know that, as of today no speciﬁc paperless
initiatives have been taken. University authority is discussing turning the graduation
application process online, but since the student portal is still developing the initiative
has yet to be taken.
The results from this paper can be used as a guide to estimate the paper usage of
the mentioned processes and the two to-be models proposed in this paper can be used
if needed. Figure 5 shows the amount of paper used between the years 2017 and 2019
for the two processes: The graduation Application process and Departmental Archiving.
However, in the years 2020 and 2021, these processes took place online for Covid-19,
and hence the ﬁgure shows 0 paper usage for departmental archiving. However, the
graduation application process still required storing a single copy of the grade report of
each student manually during these two years which is represented by the reduced but
still visible amount of paper used by this process in the graph. All these thousands of
papers could be saved by implementing the to-be model.
The 5 interviewees showed great enthusiasm to turn these two processes paperless.
However, some barriers were also found. The authors have tried to summarize the main
drivers and barriers to going paperless in the following two tables:
Table 3. Drivers for going paperless
Graduation application
Departmental archiving
Maintenance of student ﬁles needless
Storing paper documents needless, reduces
physical storage space
Ensuring several clearances on paper is
troublesome
Faster and easier sorting and searching of ﬁles
according to dates, course, and section
Manual checking course checklists is
needless, saves time and manpower
Disposing of documents like mid and ﬁnal scripts
is needless
Cost reduction as less number of papers
will be needed
Cost reduction as less number of papers will be
needed
Table 3 summarizes the main drivers for a university wanting to go paperless. The ﬁrst
and second column shows the drivers for the Graduation Application and Departmental
Archiving process respectively.
Table 4 summarizes the main barriers that stand in the way of the university making
the processes paperless. The ﬁrst and second column shows the barriers to the Graduation
Application and Departmental Archiving process respectively.

Drivers and Barriers for Going Paperless in Tertiary
377
Table 4. Barriers to going paperless
Graduation application
Departmental archiving
Course checklist is considered to be a very
critical section which is risky to leave it to the
IT section only
To archive the mid and ﬁnal scripts, they must
be scanned ﬁrst which needs manpower
Grade reports encouraged to be saved
physically fearing data center failure
Ofﬁcials who come for accreditation purposes
prefer paper-based ﬁles to review than
scanned/soft copies of the paper
6
Conclusion
The objective of this study was to ﬁnd the drivers and barriers to going paperless in
education institutes. The authors found that no speciﬁc paperless initiative has been
taken by the university yet. The author described that by changing the existing process
model university can reduce paper usage, reduce manpower, save time, and also save
physical space.
Due to the limitation of time and resources, the authors considered only the CSE
department for the departmental process and only one administrative process of East
West University. For less time authors couldn’t conduct the study at other universities.
As a continuation of this study, different departments and processes of different
universities could be explored since paper usage will vary among different departments,
processes, and universities. As this study was based on a private university, so same
study on a public university might tell some other results. This then can also be used to
make a comparison between different departments and universities.
References
1. Kayode, A.A., Lawan, B.M., Ajadi, I.A., Lukman, J.A.: E-government, information and com-
munications technology support and paperless environment in Nigerian public universities:
issues and challenges. J. Technol. Manag. Bus. 7 (2020). http://doi.org/10.30880/jtmb.2020.
07.01.006
2. Khan, R.A., Al Mesfer, M.K., Khan, A.R., Khan, S., Van Zutphen, A.: Green examination:
integration of technology for sustainability. Environ. Dev. Sustain. 19(1), 339–346 (2015).
https://doi.org/10.1007/s10668-015-9736-9
3. Isaeva, M., Yoon, H.Y.: Paperless university—how we can make it work? In: 2016 15th
International Conference on Information Technology Based Higher Education and Training,
ITHET 2016 (2016)
4. Prastyo, P.H., Sumi, A.S., Kusumawardani, S.S.: A systematic literature review of application
development to realize paperless application in Indonesia: sectors, platforms, impacts, and
challenges. Indonesian J. Inf. Syst. 2, 111–129 (2020). http://doi.org/10.24002/ijis.v2i2.3168
5. Srimathi, H., Krishnamoorthy, A.: Paperless administration in Indian higher education. Int.
J. Eng. Adv. Technol. 8, 760–764 (2019)
6. Haﬁz Iqbal, M.: Paperless campus: the real contribution towards a sustainable low carbon
society (2015). https://doi.org/10.9790/2402-09811017

378
R. M. Haque et al.
7. Meishar-Tal, H., Shonfeld, M.: Students’ writing and reading preferences in a paperless
classroom. Interact. Learn. Environ. 27 (2019). https://doi.org/10.1080/10494820.2018.150
4306
8. Genesis, E., Oluwole, O.: Towards a “paperless” higher education system in Nigeria: concept,
challenges and prospects. J. Educ. Soc. Behav. Sci. 24 (2018). https://doi.org/10.9734/jesbs/
2018/19913
9. Baby, K.T., Saeed, M.A.: Beyond the classroom through the paperless mode. Int. J. Linguist.
Lit. Transl. 2 (2020). http://doi.org/10.32996/ijllt.2020.3.1.9
10. Soﬁa, L.M., Umaima, F.F., Rumyaru, B.: Going paperless concept implementation at senior
high school in Bekasi, Indonesia. J. Environ. Eng. Waste Manag. 6 (2021). https://doi.org/10.
33021/jenv.v6i1.1344
11. Ochi, K.: Students’ perception of paperless English classroom: a case study of a Japanese it
university campus. Teach. English Technol. 21, 35–50 (2021)
12. Galanis, P.: Methods of data collection in qualitative research. Arch. Hell. Med. 35 (2018)
13. Peltier, T.R.: Gap analysis. Inf. Secur. Risk Anal. 116–127 (2021). https://doi.org/10.1201/
ebk1439839560-9

Impact of Lifestyle on Career: A Review
Md. Jabed Hosen1, Md. Injamul Haque1, Saiful Islam1, Mohammed Nadir Bin Ali2,
Touhid Bhuiyan1, Ahmed Wasif Reza3(B), and Mohammad Shamsul Areﬁn1,4(B)
1 Department of Computer Science and Engineering, Daffodil International University, Dhaka,
Bangladesh
{hosen15-3834,injamul15-3798,saiful15-3809}@diu.edu.bd,
sarefin@cuet.ac.bd
2 Department of Tourism and Hospitality Management, Daffodil International University,
Dhaka, Bangladesh
it@daffodilvarsity.edu.bd
3 Department of Computer Science and Engineering, East West University, Dhaka, Bangladesh
wasif@ewubd.edu
4 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chattogram 4349, Bangladesh
Abstract. In recent years, the impact of lifestyle on career has grown in signiﬁ-
cance. The way a person lives has a signiﬁcant impact on different aspects of life
and this has been a topic of interest among many scholars and practitioners alike.
The type of lifestyle a person considers, as well as their potential for success in
their career, can be inﬂuenced by their lifestyle choices. This article has reviewed
more than ﬁfty papers based on various lifestyle choices. The review emphasizes
the relationship between lifestyle choices and career outcomes, implying that liv-
ing a healthy lifestyle can lead to increased work productivity and career success.
This study also aims to shed light on the major lifestyle factors and how these
factors inﬂuence various aspects of life such as work-ability, mental and physical
health, recreation, travel, sleep, smoking, diet, and life.
Keywords: Career · Lifestyle · Work-ability · Life expectancy · Health · Mental
health · Physical health · Recreation · Travel · Sleep · Smoking · Diet
1
Introduction
Lifestyle refers to the way people live, including the place they live in with their family
and society, the job they do, and the activities they enjoy. Lifestyle has an impact on
almost every aspect of life. The way in which a person lives reveals a lot of things about
that person such as physical health, mental health, life expectancy, productivity, and
other variables of life. Lifestyle describes the traits of people who reside in a particular
time and area. It comprises people’s regular activities and behaviors related to their work,
hobbies, and food [1]. It is also commonly seen that people who have a mental illness
also have poor physical health, and in comparison to the general population they have
signiﬁcantly higher rates of different physical disorders [2–4]. When examining another
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 379–393, 2024.
https://doi.org/10.1007/978-3-031-50158-6_38

380
Md. Jabed Hosen et al.
aspect of lifestyle it is found that there are a number of unhealthy habits, attitudes and
behaviors which can lead to chronic disease and mortality. Lack of physical exercise,
tobacco use, poor diet [5–9], high cholesterol levels [10, 11], and inadequate dental care
appears to be signiﬁcant factors in poor physical health [12, 13].
Understanding the effects of different lifestyle factors allows us to make more
informed decisions about our own behaviors and work toward promoting healthier habits
that can improve people’s overall health and well-being. The paper will also consider
how larger societal factors can inﬂuence individual lifestyle choices and community
health outcomes.
Overall, the goal of this article is to provide insights into the relationship between
lifestyle and various aspects of life, emphasizing the importance of making informed
behavioral choices and promoting healthier lifestyles to improve our overall well-being
[14]. Healthy life and good productivity can lead a person to a better career. So, it will
be possible to draw a relationship between lifestyle and career.
2
Methodology
This study summarizes the available research on lifestyle variables that contribute to
poor physical health, and mental health, including low concentrations of exercise, poor
diet and nutrition, elevated cholesterol levels, smoking use, and recreation all contribute
to a higher risk for cardiovascular disease and productivity. A multidisciplinary search
of online databases and journals was done to create an integrative review, with a focus
on the career, health, and lifestyle issues that were most frequently discussed in the
literature.
2.1
Phase 1-Planning
This section describes how the relevant papers were chosen. The papers were collected
from prestigious sources such as Springer, SAGE, Science Direct, BMC, and MDPI, as
well as PubMed. This audit described the following search terms: “Impact of Lifestyle
on Physical Health,” “Impact of Lifestyle on Mental Health,” “Impact of Lifestyle on
Work-ability,” “Impact of Lifestyle on Recreation,” “Impact of Lifestyle on Expectancy,”
and “Impact of Lifestyle on Sleep”.
2.2
Phase 2-Conducting
In this phase, all articles are rigorously checked for reliability and validity in order to be
accepted as the ﬁnal sample article for review.
2.3
Phase 3-Reporting
After careful consideration, 51 relevant research papers were selected for review. Then
we tried to ﬁnd a relationship between the career and those six factors from the review
papers. The categorized evaluation identiﬁes contributions to research, work processes,
and ﬂaws. As a result, all papers are carefully selected to meet the goal of this research
(Fig. 1).

Impact of Lifestyle on Career: A Review
381
Fig. 1. Shows the six lifestyle factors.
3
Paper Collection
The main purpose of this section is to describe how we collected papers for our research.
Despite a lack of available papers on recreation, we were able to ﬁnd research dating
back to 1956. Then we started reviewing papers that were published in 1956 and other
papers that were published in 1996. Unfortunately, it was difﬁcult to ﬁnd papers in 1996,
1997, 2001, 2004, 2007, 2009, 2011, 2013, 2019 and 2021 individually. So, we merged
these years as there were few papers found in these years. In the end, we managed to
gather 51 papers which fulﬁll the requirements of six categories mentioned in Sect. 2.
Figure 2: There are many papers published on Physical Health and Mental Health, but
there are few articles on Work-ability, Recreation, Life Expectancy, and Sleep, among
the six categories mentioned above.
The distribution of categories
Fig. 2. Shows the distribution of selected papers by category.

382
Md. Jabed Hosen et al.
Table 1 represents our selected papers based on publishing time scale and above
mentioned six categories.
4
Detailed Review of Papers
Impact of Lifestyle on Career
Lifestyle has a relation to physical health, mental health and productivity. And these
variables which are partially or fully dependable on lifestyle have a connection to a
career. These variables are:
4.1
Impact of Lifestyle on Physical Health
Living a healthy lifestyle and being healthy are related. People having chronic diseases
have some prevalent lifestyle traits, including smoking, eating unhealthy, being inactive,
and also being obese [15]. The main causes of mortality in the United States are tobacco
use, a poor diet, and a lack of physical activity [16]. The inclusion of these lifestyle
factors in important diet-related health reports can help establish their signiﬁcance for
public health [17]. The change of lifestyle is emphasized as a crucial component of
prevention and control in treatment guidelines regarding blood pressure [21], cholesterol
[22], and obesity [23], as well as with regard to smoking [18], physical activity [19],
and blood pressure [20]. Among adult individuals, there is a concentration of risky
lifestyle behaviors such as smoking, insufﬁcient consumption of fruits and vegetables,
drinking too much alcohol, and not doing enough physical exercise [24]. A process that
alters lifestyles and gives people more control over their health is personal health or
healthy lifestyle practices. Based on the decisions you make about your daily routines,
leading a healthy lifestyle makes you ﬁt, energetic, and at a lower risk for disease. A
balanced diet, consistent exercise, and sufﬁcient sleep are the cornerstones of keeping
one’s health. According to research, employees who are healthy have the highest chance
of succeeding [25]. Ozvurmaz S, Mandiracioglu A, et al. They said a workplace has a
direct impact on employees’ physical, emotional, economic, and social wellness [26]. In
addition, he said that 2 million individuals worldwide pass away every year as a result of
workplace diseases and accidents. A healthy diet, physical activity, weight control, and
stress reduction are just a few of the strategies to live a long and healthy life. Additionally,
some data suggests that in some situations, higher levels of well-being may have a direct
effect on levels of work performance (Table 2).
4.2
Impact of Lifestyle on Mental Health
According to the hypotheses that guide the science of social epidemiology, contempo-
rary society has a persistent negative impact on public health [27]. Jonsdottir et al.; Xu
et al. In addition to characteristic indicators, everyday circumstances, and big life events,
rising research suggests that everyday actions that can be changed by a person can have
an impact on mental health. Prospective studies commonly ﬁnd a symbiotic relation-
ship between many lifestyle factors and both mental and physical wellness, with notable
health beneﬁts and well-being happening in reply to very little changes in lifestyle [28,

Impact of Lifestyle on Career: A Review
383
Table 1. Evolutions of impact of lifestyle on career
1996–2000
2001–2005
2006–2010
2010–2023
Physical health based research
Physical activity [20]
Healthy lifestyle [17]
Women and smoking [19]
Psychiatric patients [11]
Modern health [14]
Obesity in adults [23]
Physical illness [2]
Adult population [24]
Lifestyles and job performance
[25]
Reducing tobacco [18]
Bipolar disorder [5]
Adult treatment panel [22]
Healthy lifestyles [26]
Chronic disease [15]
Causes of death [16]
High blood pressure [21]
Public health [1]
1996–2000
2001–2005
2006–2010
2010–2023
Mental health based
research
Oral health [12]
Surveillance system [30]
Health and lifestyle
[6]
Metabolic syndrome[3]
Psychosocial health [26]
Depressive disorder
[10]
Mental health [28]
Mental health programs
[32]
Public health [31]
Comorbid mental
illness[4]
Nutrition and mental
health [9]
Health [33]
Mental health problems [8]
Mental illness [7]
Physical exercise and
health [34]
Somatic healthcare [13]
Prospective study [28]
Work-ability based
research
Lifestyle index [35]
Career [37]
Systematic review
[36]
Meta-analytic [39]
Physical activity [38]
Recreation based
research
Leisure and lifestyle [40]
1959
Lifestyle proﬁling work [41]
1979
2001–2005
2006–2010
2010–2023
Life expectancy based research
Lifestyle factors [43]
Life expectancies [42]
Population based study [46]
Lifestyle risk factors [44]
Global health [47]
(continued)

384
Md. Jabed Hosen et al.
Table 1. (continued)
2001–2005
2006–2010
2010–2023
Life expectancy [45]
Healthy lifestyle [48]
Sleep based research
Prospective study [51]
Systematic review [49]
Sleep loss [50]

Impact of Lifestyle on Career: A Review
385
Table 2. Physical health based research
Paper title
Contribution
Dataset
Evaluation
Healthy lifestyle [17]
American adults’
propensity for
healthy lifestyle
behaviors
National data for the
year 2000 (telephone
surveys)
Research has been
promoted
Reducing tobacco
[18]
Effective strategies
to reduce tobacco use
Did not use a speciﬁc
dataset
Provided a strong
scientiﬁc basis for
policy interventions to
control tobacco use
Women and smoking
[19]
Raise awareness
about the harmful
effects of smoking
among women
Comprehensive
analysis of the
literature and research
on the health impacts
of smoking on women
Targeted efforts to
reduce smoking
among women
Physical activity [20]
Increasing public
knowledge of the
value of exercise for
health and wellbeing
Did not use a speciﬁc
dataset for analysis
Promoting public
health initiatives to
increase physical
activity levels
Modern health [14]
Sustainable approach
to modern health care
Does not provide any
original data
Promotion of modern
health and well-being
29]. Brown DW et al. their studies state that the metrics used in demographic studies to
assess both mental and physical well-being have evolved over time. While considered
essential for patient studies in medical contexts, lengthy equipment is impractical for
observational studies. In population studies that have been written up in the interna-
tional literature, one item has come to be the standard for evaluating overall health [27].
Although cross-sectional research like this one cannot be used to draw conclusions about
causality, the notion that lifestyle will affect self-rated mental health is well established.
Rarely are results related to mental health included in evaluations of public health ini-
tiatives intended to promote behaviors [30]. Based on a systematic review of published
research on academic well-being, academics are more likely to have mental health prob-
lems than people in other professions. According to research conducted by Hsiao et al.
[31], factors affecting academics’ health include a lack of employment security, limited
management support, and the responsibility of job demands on their time. Uedo and
Niino, et al.The authors of research on the impact of mental health programs on worker
productivity discovered that there is a statistically signiﬁcant link between health and
productivity and that the performance of the ﬁrms that offer more highly regarded health
program practices is higher [32]. Similar to vein, Yu and Bang [33] explore the effects of
better health on organizational performance. The ﬁndings showed that employees with
poor well-being are signiﬁcantly more likely than employees with high well-being to
engage in behaviors that would have a detrimental impact on organizational outputs,
both in regard to direct healthcare expenses and organizational evaluation methods [33].

386
Md. Jabed Hosen et al.
Drannan [34] carries out research on the connection between physical activity and work
performance; the ﬁndings showed that there was a strong connection. The typical justi-
ﬁcation for starting and maintaining a regular exercise regimen is the health advantages
of physical activity. Research has shown that exercise signiﬁcantly enhances both mood
and productivity. Numerous psychologists and leading businesses have included physical
activity in their eight corporate strategies to improve employee productivity by boosting
mood and job performance [34] (Table 3).
Table 3. Mental health based research
Paper title
Contribution
Dataset
Evaluation
Prospective study [2] Leisure - time physical
activity (PA) and
depression and anxiety
symptoms, burnout,
and perceived stress
Data from a cohort of
health and social
insurance employees
in west Sweden
between 2004 and
2006
Able to ﬁnd
association between
physical activity and
different types of
anxiety
Somatic healthcare
[13]
Research on the use of
community mental
health services by
people with severe
mental illness
200 dataset where 100
with schizophrenia
and 100 with affective
disorder
The general
population to report
having sought some
form of medical care
in the past year
Health and lifestyle
[6]
Design and implement
a structured
educational program
Survey data
Improving the health
and mental health
services and
promoting healthy
behavioral changes
Surveillance system
[30]
Encouraging physical
exercise to enhance
overall quality of life
related to health
(BRFSS) surveyData
Exercise improves
health-related quality
of life
Public health [31]
Positive impact on the
students’ knowledge
Dataset collected from
the nursing students
The development of
effective health
promotion
interventions for
nursing students
4.3
Impact of Lifestyle on Work-Ability
DOROTA KALETA et al. state that lifestyle has a direct impact on workability [35]. Tilja
van den Berg et al. said that certain personal traits and work lifestyle requirements are
linked to workability. And they state that some factors that are associated with decreased
workability are older age, obesity, absence of intense exercise during free time, and many
more [36]. Hussein Isse Hassan Abdirahman et al. developed three hypotheses and the

Impact of Lifestyle on Career: A Review
387
outcome of those hypotheses states that Job balancing and productivity are positively
correlated [37]. Also, sometimes it affects students’ college life. JESSE CALESTINE
et al. discovered that, compared to younger age groups, college students may have a
distinct link between academic results and physical exercise. This study sheds light on the
creation of upcoming university college health interventions that will jointly prioritize
academic results and physical exercise [48]. Mesmer-Magnus, J.R., Viswesvaran, C.
et al. Their review has discovered that parental involvement for company culture helps
with work-life balance management. Employee motivation is increased, and the negative
effects of work-lifebalancearelessenedwhenthereis support fromsuperiors, coworkers,
and ﬂexible working hours. Proper maternity leave, etc. Conﬂicts between work and life
were further exacerbated by this [39] (Table 4).
Table 4. Work-ability based research
Paper title
Contribution
Dataset
Evaluation
Lifestyle index [35]
Lifestyle factors in
maintaining good work
ability among
employees
Datasets were used
in this project:
interview
Employees with a
healthier lifestyle had
higher work ability
Career [37]
Organizational
behavior and human
resource management
Data collected from
184 employees
using a survey
questionnaire
Job satisfaction and
enhancing employee
performance
Meta-analytic [39]
How family-friendly
workplaces can reduce
conﬂict between work
and family
30 research studies
that looked into this
meta-analysis
Correlated with reduced
levels of conﬂict
between work and
family
Physical activity
[38]
Association between a
college student’s work
habits and their level of
ﬁtness and physical
activity
Collected primary
data through surveys
and questionnaires
College students who
engage in regular
physical activity have
better work habits
Systematic review
[36]
Identiﬁes important
personal and
professional aspects
that affect job
performance
Using various
databases such as
PubMed, Embase,
and PsycINFO
Promote healthy and
productive work
environments
4.4
Impact of Lifestyle on Recreation
ROBERT J. HAVIGHURST et al. states that There are two main categories of leisure
style: one that is focused on the public as well as one that is focused on the home [40].
Both a society living and a residence lifestyle include leisure activities that are focused on

388
Md. Jabed Hosen et al.
the local area. Based on the interview questionnaire they gave scores to eight social roles.
From the score of the eight social roles, they divided leisure time into home-centered
and community-centered.
The relationship between a person’s lifestyle and their leisure activities is not always
close. Approximately 5% of adults fall into one of these categories. They are people who
live prosperous lives and have good personal adjustment but do little to no leisure activity.
These men and women typically devote the majority of their energy to their jobs or to
their homes and children, with little leisure time or desire. Nearly 6% of adults belong
to a different group. They have a lot of hobbies but struggle to be successful as workers,
parents, or husbands because they feel inadequate in these roles. Traveling is another
popular recreational activity. JAMES R. ABBEY discovered that tourists prefer tours
created using vacation lifestyle information over those created using de demographics
(Age, Gender, Relationship Status, Degree, Profession, Income, Family Make-up, Prior
Travel Experience) data, and this demographic preference is consistent for both different
trip types (air and motor-coach) and different priced tours (budget and ﬁrst-class) [41].
So, it is clear that the selection of a travel package depends a lot on the lifestyle of
individuals (Table 5).
Table 5. Recreation based research
Paper title
Contribution
Dataset
Evaluation
Leisure and lifestyle
[40]
Understanding of the
role of leisure in shaping
lifestyles and social
identities
Interview
questionnaire
Framework for
understanding the
relationship between
leisure and lifestyle
Lifestyle proﬁling
work [41]
Understanding of
lifestyle proﬁling as a
marketing tool in the
travel industry
Questionnaire
Lifestyle proﬁling could
effectively predict travel
4.5
Impact of Lifestyle on Expectancy
Their studies state that good lifestyle decisions, such as maintaining a balanced diet,
exercising frequently, and giving up smoking and drinking too much alcohol, have a
positive effect. on health, particularly in terms of increased lifespan [42–44]. Mehta
N et al. in their review, it is pertinent to inquire as to if smokers who give up early
in life are still likely to enjoy long, ailment-free lives as the largest smoking group in
their sample was former smokers (41 Percentage of the overall population). Separate
analyses (data not shown) revealed that non-obese individuals who had quit smoking at
least 10 years before the survey and who drank moderately had lifespans that were only
one year less than non-obese people who had not previously smoked and who drank
moderately [45]. Rizzuto D et al. state that, Smokers had a year poorer survival rate than
non-smokers among those who survived to just be 75 years old. In the Kungsholmen

Impact of Lifestyle on Career: A Review
389
Sample population, 83% of ex-smokers had quit 15–35 years earlier than baseline, and
17% had quit ﬁve to 14 years earlier [46]. Li Y et al. According to their calculations,
adopting 5 low-risk lifestyle-related traits may increase a person’s average lifespan at
the age of 50 by 14.0 and 12.2 years, respectively, for male and female US citizens
[42]. As the life expectancy estimates are simpler to comprehend by both the whole
public as well as medical specialists, they have grown in popularity as a statistic for
establishing public health objectives. The relationship between single and combination
lifestyle choices including eating, drinking, and tobacco and expected lifespan has not
yet been studied with regard to the occurrence of multimorbidity [47, 48]. Rizzuto D et al.
Among individuals with one or more chronic conditions, only one study examined the
connection between a combined healthy lifestyle and life expectancy [46], Chudasama
YV et al. While their studies involved people from the general community, the results
revealed that a healthy lifestyle overall was linked to an average lifespan of between 5.4
and 18.9 years [48] (Table 6).
Table 6. Life expectancy based research
Paper title
Contribution
Dataset
Evaluation
Lifestyle factors [43]
Signiﬁcant factor
affecting life
expectancy in the
Japanese population
Over 100,000
participants aged
40–79 years the
baseline survey
collected data on
various lifestyle
factors
Alcohol consumption
and physical activity
had a more modest
impact on life
expectancy
Lifestyle risk factors
[44]
Living a healthy
lifestyle has a negative
impact on Germans’
remaining life
expectancy
Over 10,000
participants aged
40–79 years for a
period of 10 years
Healthy lifestyles and
reducing the burden of
non-communicable
diseases
Life expectancy [45]
Healthy lifestyle can
lead to signiﬁcant
population health
beneﬁts
Data from the
(HRS), a survey
Healthy lifestyle
participants lived
7.6 years longer
Global health [47]
Multimorbidity is
associated with
increased healthcare
utilization
Paper draws on a
range of sources,
including
published research
studies, reports,
and policy
documents
Effective models of
care for individuals
with multimorbidity

390
Md. Jabed Hosen et al.
4.6
Impact of Lifestyle on Sleep
Shneerson J et al. According to this study, there isn’t enough data from randomized con-
trolled trials to determine how well weight loss, exercise, and sleep hygiene approaches
work to cure obstructive sleep apnea. This is particularly signiﬁcant because this preva-
lent ailment is one for which these procedures are frequently advised [49]. Akerstedt T.
et al. state that, in addition to these variations in compensation calculations, there are
also variations in how health status is viewed. In certain American jurisdictions, doctors
are required to disclose whether a professional driver has a sleep issue. Such a reporting
requirement might materially affect the driver’s ability to maintain their employment
[50]. In contrast to the transportation industry, the statistics of sleep-related incidents in
industrial and health. One explanation might be because, in contrast to the transportation
industry, where errors can have catastrophic consequences, there are not as many jobs in
these industries that include a lot of duties with a signiﬁcant injury risk [41]. However, it
was shown in epidemiological research by Akerstedt et al. that shift work and disrupted
sleep had a 50% database with a larger danger of fatal workplace accidents of more than
50,000 people who had been randomly recruited from the community [51] (Table 7).
Table 7. Sleep based research
Paper title
Contribution
Dataset
Evaluation
Systematic review
[49]
Effectiveness of
lifestyle
modiﬁcations in the
treatment of OSA
Reviewed 13
randomized controlled
trials that evaluated
various lifestyle
modiﬁcations
Effectiveness of other
lifestyle modiﬁcations
is less clear
Sleep loss [50]
Research of how
sleep loss affects
several facets of
human ability
Information from
multiple studies and
sources
Lack of sleep has a
detrimental impact on
performance and raises
the possibility of
accidents
Prospective study
[51]
The role of sleeping
difﬁculties and
occupational factors
in fatal occupational
accidents
A prospective study of
121,390 male workers
in Sweden
The prevalence of
sleeping difﬁculties
and improving working
conditions
5
Discussion
After reviewing these 51 studies, it’s clear that lifestyle has a direct or indirect impact
on other aspects of life. Physical, mental and life expectancy are all highly dependent on
lifestyle. Common variables that were analyzed to ﬁnd the relationship between lifestyle
and health are smoking, alcohol consumption level, sleeping time, stress level, and diet.

Impact of Lifestyle on Career: A Review
391
A healthy person has a better life expectancy. With a healthy mind and healthy body,
a person can have better work efﬁciency. While recreation is important to keep good
mental health, lifestyle has a signiﬁcant impact on the choice of recreational activity as
well. To live a healthy life with a healthy mind and have better work efﬁciency a person
needs to have a healthy lifestyle.
6
Conclusion
This literature review provides useful insights into the impact of lifestyle on various
aspects of life. It emphasizes the importance of adopting healthy lifestyle practices like
frequent exercise and a balanced diet, and adequate sleep in promoting better physical
and mental health outcomes. We did a thorough analysis of the literature on human
behavior and lifestyle, focusing on 51 publications and case studies that were published
between 1959 and December 2023, in order to accomplish our aim. We ﬁnally chose
51 main papers for our study out of the 80 initially chosen publications using various
inclusion and exclusion criteria. We did a thorough assessment of all of the research
articles. The analysis of these research papers discovered that people who are physically
active throughout the day and in generally good spirits have a healthy life as a result
of healthy lifestyles. Overall, people with a healthy body give consideration to getting
enough sleep and eating a balanced diet. And a person’s lifestyle can have a signiﬁcant
impact on their career. People who have a better career live a healthier lifestyle, which
allows them to work more efﬁciently.
References
1. Farhud, D.D.: Impact of lifestyle on health. Iran. J. Public Health 44(11), 1442 (2015)
2. Holman, D.: Duty to Care, Preventable Physical Illness in People with Mental Illness.
Department of Public Health, University of Western Australia (2001)
3. John, A.P., Koloth, R., Dragovic, M., Lim, S.C.: Prevalence of metabolic syndrome among
Australians with severe mental illness. Med. J. Aust. 190(4), 176–179 (2009)
4. Mitchell, A.J., Malone, D., Doebbeling, C.C.: Quality of medical care for people with and
without comorbid mental illness and substance misuse: systematic review of comparative
studies. Br. J. Psychiatry 194(6), 491–499 (2009)
5. Elmslie, J.L., Mann, J.I., Silverstone, J.T., Romans, S.E.: Determinants of overweight and
obesity in patients with bipolar disorder. J. Clin. Psychiatry 62(6), 4297 (2001)
6. O’sullivan, J., Gilbert, J., Ward, W.: Addressing the health and lifestyle issues of people with
a mental illness: the healthy living programme. Australas. Psychiatry 14(2), 150–155 (2006)
7. Osborn, D.P., Nazareth, I., King, M.B.: Physical activity, dietary habits and coronary heart
disease risk factor knowledge amongst people with severe mental illness: a cross sectional
comparative study in primary care. Soc. Psychiatry Psychiatr. Epidemiol. 42, 787–793 (2007)
8. Soundy,A.,Faulkner,G.,Taylor,A.:Exploringvariabilityandperceptionsoflifestylephysical
activity among individuals with severe and enduring mental health problems: a qualitative
study. J. Ment. Health 16(4), 493–503 (2007)
9. Porter, J., Evans, S.: Nutrition and mental health research in Australia and New Zealand: a
review of progress and directions for the future. Nutr. Diet. 65(1), 6–9 (2008)
10. Jow, G.M., Yang, T.T., Chen, C.L.: Leptin and cholesterol levels are low in major depressive
disorder, but high in schizophrenia. J. Affect. Disord. 90(1), 21–27 (2006)

392
Md. Jabed Hosen et al.
11. Ojala, K., Repo-Tiihonen, E., Tiihonen, J., Niskanen, L.: Statins are effective in treating
dyslipidemia among psychiatric patients using second-generation antipsychotic agents. J.
Psychopharmacol. 22(1), 33–38 (2008)
12. Sjögren, R., Nordström, G.: Oral health status of psychiatric patients. J. Clin. Nurs. 9(4),
632–638 (2000)
13. Dickerson, F.B., McNary, S.W., Brown, C.H., Kreyenbuhl, J., Goldberg, R.W., Dixon, L.B.:
Somatic healthcare utilization among adults with serious mental illness who are receiving
community psychiatric services. Medical Care 560–570 (2003)
14. Warbrick, I., Makiha, R., Heke, D., Hikuroa, D., Awatere, S., Smith, V.: Te Maramataka—
an indigenous system of attuning with the environment, and its role in modern health and
well-being. Int. J. Environ. Res. Public Health 20(3), 2739 (2023)
15. Brownson, R.C., Remington, P.L., Davis, J.R.: Chronic Disease. Epidemiology and Control.
American Public Health Association, Washington, DC (1998)
16. Mokdad, A.H., Marks, J.S., Stroup, D.F., Gerberding, J.L.: Actual causes of death in the
United States, 2000. JAMA 291(10), 1238–1245 (2004)
17. Reeves, M.J., Rafferty, A.P.: Healthy lifestyle characteristics among adults in the United
States, 2000. Arch. Intern. Med. 165(8), 854–857 (2005)
18. US Department of Health and Human Services: Reducing tobacco use: a report of the Surgeon
General (2000)
19. Ofﬁce on Smoking and Health: Women and smoking: a report of the Surgeon General (2001)
20. National Center for Chronic Disease Prevention, Health Promotion (US), President’s Council
on Physical Fitness and Sports (US): Physical activity and health: a report of the Surgeon
General. US Department of Health and Human Services, Centers for Disease Control and
Prevention, National Center for Chronic Disease Prevention and Health Promotion (1996)
21. Chobanian, A.V., et al.: The seventh report of the Joint National Committee on prevention,
detection, evaluation, and treatment of high blood pressure: the JNC 7 report. JAMA 289(19),
2560–2571 (2003)
22. National Cholesterol Education Program (NCEP) Expert Panel on Detection, Evaluation, and
Treatment of High Blood Cholesterol in Adults (Adult Treatment Panel III). Third Report of
the National Cholesterol Education Program (NCEP) Expert Panel on Detection, Evaluation,
and Treatment of High Blood Cholesterol in Adults (Adult Treatment Panel III) ﬁnal report.
Circulation 106(25), 3143–3421. PMID: 12485966
23. Expert Panel on the Identiﬁcation, Evaluation, Treatment of Overweight, Obesity in Adults
(US), National Heart, Lung, Blood Institute, National Institute of Diabetes, Digestive and
Kidney Diseases (US) (1998) Clinical guidelines on the identiﬁcation, evaluation, and treat-
ment of overweight and obesity in adults: the evidence report (No. 98). National Institutes of
Health, National Heart, Lung, and Blood Institute
24. Schuit, A.J., van Loon, A.J.M., Tijhuis, M., Ocké, M.C.: Clustering of lifestyle risk factors
in a general adult population. Prev. Med. 35(3), 219–224 (2002)
25. Agboola, I.O., Ikonne, C.N.: Healthy lifestyles and job performance of academics: a
theoretical perspectives. Libr. Philos. Pract. (2019)
26. Ozvurmaz, S., Mandiracioglu, A.: Healthy lifestyle behavior of employees in small and
medium-sized enterprises in Aydin, Turkey. Pak. J. Med. Sci. 33(2), 404 (2017)
27. Pikhart, H., Bobak, M., Rose, R., Marmot, M.: Household item ownership and self-rated
health: material and psychosocial explanations. BMC Public Health 3(1), 1–7 (2003)
28. Jonsdottir, I.H., Rödjer, L., Hadzibajramovic, E., Börjesson, M., Ahlborg, G., Jr.: A prospec-
tive study of leisure-time physical activity and mental health in Swedish health care workers
and social insurance ofﬁcers. Prev. Med. 51(5), 373–377 (2010)
29. Xu, Q., Anderson, D., Courtney, M.: A longitudinal study of the relationship between lifestyle
and mental health among midlife and older women in Australia: ﬁndings from the healthy
aging of women study. Health Care Women Int. 31(12), 1082–1096 (2010)

Impact of Lifestyle on Career: A Review
393
30. Brown,D.W.,etal.:Associationsbetweenrecommendedlevelsofphysicalactivityandhealth-
related quality of life ﬁndings from the 2001 Behavioral Risk Factor Surveillance System
(BRFSS) survey. Prev. Med. 37(5), 520–528 (2003)
31. Hsiao, Y.C., Chen, M.Y., Gau, Y.M., Hung, L.L., Chang, S.H., Tsai, H.M.: Short-term effects
of a health promotion course for Taiwanese nursing students. Public Health Nurs. 22(1),
74–81 (2005)
32. Ueda, Y., Niino, H.: The effect of mental health programs on employee satisfaction with
beneﬁt programs, jobs and the organization. Bus. Manag. Rev. 2(1), 27–38 (2012)
33. Yu, K., Bang, S.C.: What is the Impact of Improved Health to Organizational Performance?
(2013)
34. Drannan, J.D.: The relationship between physical exercise and job performance: the mediating
effects of subjective health and good mood. Doctoral dissertation, Bangkok University (2016)
35. Kaleta, D., Makowiec-Dabrowska, T., Jegier, A.: Lifestyle index and work ability. Int. J.
Occup. Med. Environ. Health 19(3), 170 (2006)
36. Van den Berg, T., Elders, L., de Zwart, B., Burdorf, A.: The effects of work-related and
individual factors on the work ability index: a systematic review. Occup. Environ. Med. 66,
211–220 (2008)
37. Abdirahman, H.I.H.: The relationship between job satisfaction, work-life balance and
organizational commitment on employee performance (2018)
38. Calestine, J., Bopp, M., Bopp, C.M., Papalia, Z.: College student work habits are related to
physical activity and ﬁtness. Int. J. Exerc. Sci. 10(7), 1009 (2017)
39. Mesmer-Magnus, J.R., Viswesvaran, C.: How family-friendly work environments affect
work/family conﬂict: a meta-analytic examination. J. Lab. Res. 27(4), 555–574 (2006)
40. Havighurst, R.J., Feigenbaum, K.: Leisure and life-style. Am. J. Sociol. 64(4), 396–404 (1959)
41. Abbey, J.R.: Does life-style proﬁling work? J. Travel Res. 18(1), 8–14 (1979)
42. Li, Y., et al.: Impact of healthy lifestyle factors on life expectancies in the US population.
Circulation 138(4), 345–355 (2018)
43. Tamakoshi, A., et al.: Impact of smoking and other lifestyle factors on life expectancy among
Japanese: ﬁndings from the Japan Collaborative Cohort (JACC) study. J. Epidemiol. 20(5),
370–376 (2010)
44. Li, K., Hüsing, A., Kaaks, R.: Lifestyle risk factors and residual life expectancy at age 40: a
German cohort study. BMC Med. 12(1), 1–10 (2014)
45. Mehta, N., Myrskylä, M.: The population health beneﬁts of a healthy lifestyle: life expectancy
increased and onset of disability delayed. Health Aff. 36(8), 1495–1502 (2017)
46. Rizzuto, D., Orsini, N., Qiu, C., Wang, H.X., Fratiglioni, L.: Lifestyle, social factors, and
survival after age 75: population based study. Bmj 345 (2012)
47. Academy of Medical Sciences (Royaume uni): Multimorbidity: a priority for global health
research. Academy of Medical Sciences (2018)
48. Chudasama, Y.V., et al.: Healthy lifestyle and life expectancy in people with multimorbidity
in the UK Biobank: a longitudinal cohort study. PLoS Med. 17(9), e1003332 (2020)
49. Shneerson, J., Wright, J.J., Cochrane Airways Group: Lifestyle modiﬁcation for obstructive
sleep apnoea. Cochrane Database Syst. Rev. 2010(1) (1996)
50. Åkerstedt, T., Philip, P., Capelli, A., Kecklund, G.: Sleep loss and accidents—work hours,
lifestyle, and sleep pathology. Prog. Brain Res. 190, 169–188 (2011)
51. Åkerstedt, T., Fredlund, P., Gillberg, M., Jansson, B.: A prospective study of fatal occupational
accidents–relationship to sleeping difﬁculties and occupational factors. J. Sleep Res. 11(1),
69–71 (2002)

Deep Learning Approach for COVID-19
Detection: A Diagnostic Tool Based on VGG16
and VGG19
Fardin Rahman Akash1, Ajmiri Afrin Priniya1, Jahani Shabnam Chadni1,
Jobaida Ahmed Shuha1, Ismot Ara Emu1, Ahmed Wasif Reza1(B),
and Mohammad Shamsul Areﬁn2,3(B)
1 Department of Computer Science and Engineering, East West University, Dhaka 1212,
Bangladesh
wasif@ewubd.edu
2 Department of Computer Science and Engineering, Daffodil International University,
Dhaka 1341, Bangladesh
sarefin@cuet.ac.bd
3 Department of Computer Science and Engineering, Chittagong University of Engineering and
Technology, Chattogram, Bangladesh
Abstract. The coronavirus disease 2019 is a new contagious illness affecting the
lungs and upper respiratory tract. It has various complications that can affect the
quality of life. One of the most common factors that can be used to diagnose this
illness is chest computed radiography. According to studies, deep learning can
identify COVID-19 using chest radiography results. We created a CNN network
to ﬁnd COVID-19 in patients with Pneumonia and normal controls after a full
chest X-ray. For the testing and training of the VGG16 model, we focused on its
deep features. The study’s results revealed that the VGG16 model had the highest
accuracy score, at 70.0021%.
Keywords: VGG16 · VGG19 · Convolutional neural networks · Lung X-rays ·
Pneumonia
1
Introduction
The outbreak of COVID-19 was ﬁrst detected in Wuhan, China. Within a few months, it
had already affected various parts of the world. Due to the nature of the disease and the
number of people who died, it has led to a pandemic [1]. The WHO reported that over
300,000 people died globally within two months after the COVID-19 pandemic was
declared. Because of the restrictions placed on people, COVID-19 severely affects every
aspect of life in the world [2]. The clinical signs of an infected individual are non-speciﬁc.
This means that molecular methods are required to conﬁrm the virus [3]. COVID-19 can
cause a range of illnesses from mild to severe, with high rates of morbidity and mortality.
Bats are believed to be carriers of the virus, which primarily affects the respiratory system
and can lead to symptoms such as fever, cough, and dyspnea. RT-PCR is commonly used
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 394–403, 2024.
https://doi.org/10.1007/978-3-031-50158-6_39

Deep Learning Approach for COVID-19 Detection
395
for diagnosis but has limitations in accuracy and sensitivity [4]. Since the COVID-19
epidemic began in 2020, many papers have been released detailing remedies to the many
different issues it caused [5]. Computer-based approaches have been developed to tackle
various technological challenges, including the need for scientiﬁc tools to aid in the
analysis of COVID-19. Early detection of the disease can save lives, and deep learning
algorithms have emerged as promising tools for disease identiﬁcation, reducing human
error. Diagnostic methods such as chest X-rays and CT scans can also aid in detecting
COVID-19. In a study involving over a thousand patients, Chinese researchers found that
chest CT scans identiﬁed 97% of COVID-19 infections. COVID-19-induced pneumonia
severelydamagesthelungs,ultimatelyresultingintheirfailure,unliketypicalpneumonia
which affects only a portion of the lung [6]. Our goal is to simplify the process by which
patients may identify their medical issues. Because the disease can spread fast and easily
from patient to patient, our key goal is for people to identify their COVID simply using
their CT scan picture at home and keep others safe. This would give readers an overview
of what to expect in the paper and help them understand the main points. In this case, the
Introduction should include a summary of the COVID-19 outbreak and its impact, the
importance of early detection and diagnosis, and the limitations of current diagnostic
methods. The authors’ proposed approach using deep learning techniques, speciﬁcally
comparing the VGG-16 and VGG-19 models, should also be highlighted. The potential
beneﬁts of this approach, such as its speed and practicality, should also be mentioned.
Finally, the paper’s structure should also be brieﬂy introduced, including the dataset and
methods used, the results, and the authors’ conclusions and future directions.
2
Related Work
Suman Chaudhary et al. [7] proposed an approach to Deep learning techniques used to
identify COVID-19 in a chest X-ray image of a patient. They then trained two nets with
different weights using SE-ResNext. Three techniques are used to classify the ﬁndings
of X-ray images: normal, Pneumonia, and Covid. The researchers tested the proposed
technique against a database of chest X-ray images for the Postgraduate Challenge.
They found that it performed well with a sensitivity of 0.9592, speciﬁcity of 0.9597, and
accuracy of 0.9592.
Naufal Hilmizen et al. [8] proposed an approach to classify CT-Scan images into nor-
mal and COVID-19 Pneumonia categories, a combination of image recognition models
and transfer learning methods, such as MobileNet, Xception, VGG16, and InceptionV3
wereused.Amulti-modalityfusionapproachcombiningVGG16andResNet50achieved
99.87% accuracy, surpassing single-modality methods.
Dimas Reynaldi et al. [9] proposed an approach to a deep learning-based approach
that uses CNN architecture to identify COVID-19 infection in patients through CT-Scans.
The images are preprocessed using the CLAHE method and then used to train CNN and
Resnet-101 models. The approach achieved good results in accuracy and sensitivity
during the testing and training phases.
Ayesh Meepaganithage et al. [10] proposed an approach to develop a method that
can identify patients with COVID-19 Pneumonia, non-COVID-Pneumonia, and ordinary
Pneumonia using chest radiography. We then created two deep-learning models that

396
F. R. Akash et al.
could analyze the multiple images of the chest. In the COVID-19 class, the training
group achieved an overall accuracy of 79%, 96% precision, and 83% recall. In the second
section, the group was able to use AP view X-ray images to improve their accuracy. The
suggested VGG19 model, extended from the previous model, was then used to create
activation maps showing where the group could detect the virus.
Amir Sorayaie Azar et al. [11] proposed an approach to identify COVID-19 patients
and distinguish them from those infected with viral or bacterial Pneumonia using chest
radiography images. A four-class model and a ﬁve-fold cross-validation procedure
achieved the model’s accuracy. This model is lightweight and resilient; clinicians can
use it to diagnose COVID-19. It has fewer parameters and training epochs and a reduced
framework. Clinicians can readily recognize COVID-19 using this model as an additional
diagnostic tool.
Mohammad Ayyaz Azeem et al. [12] proposed an approach to analyze and evaluate
the various deep-learning techniques that can be used to detect COVID-19, Pneumonia,
and normal chest radiography images. Four different transfer learning methods were used
fortheclassiﬁcationtasks.TheVGG16modelsurpassedthecompetingmodelsregarding
sensitivity, speciﬁcity, and accuracy. It also performed well in detecting Pneumonia and
COVID-19.
ZeynepÖzdemıretal.[13]proposedanapproachtouseneuralnetworksandCOVID-
19 data to detect abnormalities in images. They developed binary class classiﬁcation
programs based on data from NIH Chest X-ray and COVID-19 databases and used the
Adaptive Sigmoid function and Contrast Limited adaptive histogram equalization to
analyze the data. Three CNN models from the ImageNet library were used, and the
EfﬁcientNetB5 model performed the best.
S. Bhuvana et al. [14] proposed an approach to a computer vision model trained
on datasets collected from the internet was proposed to detect COVID-19 in its early
stages and reduce mortality rates. The model was trained using various tools, including
TensorFlow, and recommended the use of CNNs on chest X-ray scans.
Zakariya A. Oraib et al. [15] proposed an approach to use machine learning and chest
CT scans to identify COVID-19 patients. It used local binary patterns and the Random
Forests classiﬁer with a multiresolution approach to improve efﬁciency. The accuracy
was evaluated with speciﬁcity and sensitivity, similar to deep learning systems. There
are signiﬁcant contributions in [16–20] in the ﬁeld of image analysis for performing
different tasks.
3
Architecture and Design of System
3.1
Dataset Description
The dataset contains 11,884 chest X-ray images from children aged one to ﬁve, orga-
nized into three primary folders: Train, Test, and Validation. The dataset consists of
three categories, including Pneumonia (3875), Normal (4228), and Covid (2216). The
authors also mentioned that the dataset was obtained from the Kaggle website and was
collected by the Guangzhou Women and Children’s Medical Center in Guangzhou.
Before processing the images, the physicians reviewed the chest radiographs for qual-
ity control, and any low-quality or unreadable images were eliminated. Moreover, two

Deep Learning Approach for COVID-19 Detection
397
qualiﬁed physicians reviewed the diagnoses in the images before training the AI model.
The authors also mentioned that they would manage missing values, encode categorical
data, balance the dataset, and perform exploratory data analysis before choosing the
most suitable predictive model with the highest accuracy score. Finally, the authors also
provided information on the image preprocessing steps, which included resizing, adding
multiple views, and applying various augmentation techniques. In Fig. 1, we showed
input images from different perspectives.
Fig. 1. Input image from the dataset
3.2
Data Preprocessing
To prepare the dataset for analysis, we will handle missing values, encode categorical
data, and partition the data. We will also balance the dataset using various oversampling
techniques and review it for false or redundant values. Exploratory data analysis will be
conducted to gain insights from the preprocessed data. We will then use multiple deep
learning models and evaluation methods like precision, recall, and f1-score to select
the most suitable predictive model with the highest accuracy score. To categorize an
object, we will resize the input image to 224 * 224 pixels, and the test image will be
resized using the input image and uploaded again. We will also apply various image
transformations such as scaling, rotation, shifting, ﬂipping horizontally and vertically,
and zooming to augment the dataset. As for citing the dataset, the authors should include
the proper citation for the Kaggle website where they obtained the dataset. They should
also cite the dataset source, the Guangzhou Women and Children’s Medical Center in
Guangzhou. Additionally, the authors should reference any relevant papers or articles
that have used the same dataset for their research.
3.3
Analysis and Findings
Figure 2 shows that the target variable ratios for Covid, Normal, and Pneumonia are
35.9:40:24.2, and the bar graph shows the validation data of our dataset.
The variable goal ratio of Covid, Normal, and Pneumonia is 21.5:41:37.5. The pie
chart and bar chart (Fig. 3) show the train data of our dataset.

398
F. R. Akash et al.
Fig. 2. Percentage of our validation data and number of our validation data.
Fig. 3. Percentage of our train data and number of our train data.
3.4
Proposed Algorithms
To train model art using our dataset, we need to be able to load it into memory. To proceed,
mount Google Drive and import the RGB image dataset. Then, using our sorted dataset,
we divide it into train and test sets. Furthermore, develop and build the VGG16 model.
We choose it as a classiﬁcation algorithm because it is one of the most common picture
classiﬁcation algorithms and is simple to utilize with transfer learning. Finally, train the
model on a training dataset and test it on a test dataset. It should also be noted that we
use VGG19 for improved performance. We have used VGG16 and added some extra
layers keeping VGG16 as the base model. Here is the pseudocode of the VGG16 neural
network:
1. Import necessary libraries such as TensorFlow, Keras, etc.
2. Deﬁne the VGG16 model in keras.
3. Specify the input dimension and the number of classes to classify.
4. Add the convolutional layers to the model with 64 ﬁlters of size 3 × 3 with relu
activation.
5. Add a max pooling layer of size 2 × 2 to downscale the features.
6. Add another convolutional layer with 128 ﬁlters of size 3 × 3 with relu activation
followed by another max pooling layer.
7. Add three more convolutional layers with 256, 512, and 512 ﬁlters of size 3 × 3, all
with relu activation followed by max pooling layers.
8. Flatten the output of the last max pooling layer and pass it to a fully connected layer
with 4096 neurons and relu activation, followed by another fully connected layer of
4096 neurons with relu activation.
9. Add the ﬁnal output layer with softmax activation to obtain the probability
distribution of the classes.

Deep Learning Approach for COVID-19 Detection
399
10. Compile the model with appropriate loss function, optimizer, and evaluation metric.
11. Train the model with the training dataset and validate it with the dataset.
12. Evaluate the model with the test dataset.
13. Predict the classes for new data using the trained model.
14. Save and export the trained model for future use.
3.5
Model Architecture and Design
The most notable aspect of VGG16 in this regard is that rather than a large number of
hyper-parameters, they focused on creating convolution layers of 3 × 3 ﬁlter with stride
1 and constantly employed the exact padding and max pool layer of 2 × 2 ﬁlter with
stride 2. On the other hand, in VGG19, They also used kernels, which allowed them to
cover the entire visual idea where the spatial resolution of the image was preserved using
spatial padding. With stride 2, max pooling was conducted over a 2 * 2-pixel window.
To execute, we used 224 * 224 pixels for both the VGG16 and VGG19 models.
4
Installation and Evaluation Outcome
4.1
Evaluation Environment
We have 16 GB of RAM and 120 GB of disk space set up for us on the Google Colab
laptop scenario where the envisaged system is fully constructed. The backend is Python,
and for quicker development, Google’s Compute Engine has the GPU option enabled.
The layers and constructed functions of the Keras Deep Learning framework are used
to create the models.
4.2
Evaluation Outcome
We classiﬁed a large number of Chest X-ray images to assess the performance of our
model. The program’s overall effectiveness is shown in Table 1 when the chest X-ray
images are divided into various groups.
4.3
Implementation
We used the convolutional neural network architecture to create two unique models for
the classiﬁcation job. The VGG19 model and a modiﬁed version of the VGG16 model
were implemented, which were previously trained to recognize distinct types of Covid-
19 Chest X-ray pictures. This dataset notably differs from the original dataset used to
build the ﬁrst VGG16 framework. It has a resolution of 224 × 224 pixels. Because we
only had three classes at the time, the models’ output layer was also required to contain
three layers. To ﬁnish both models’ architecture, we used convolutional, max-pooling,
ﬂattened, and thick layers. The outcomes were good. Before arriving at our ﬁnal model,
which had the highest accuracy, we examined a variety of architectures and attributes.
The photographs in the collection were all taken from the same angle. We alter each
image to present a variety of views. Versions VGG16 and VGG19 were tested.

400
F. R. Akash et al.
Table 1. The models’ image detection precision for each Covid-19 chest X-ray category
Model name
Starting to learn the rate
Epoch
Batch size
Accuracy %
VGG 16
0.001
5
8
66.5268
VGG 16
0.001
16
16
70.6667
VGG 16
0.01
5
8
68.4927
VGG 16
0.01
16
16
69.2231
VGG 19
0.001
5
8
67.9923
VGG 19
0.001
16
16
70.0021
VGG 19
0.01
5
8
66.0294
VGG 19
0.01
16
16
69.0111
4.4
Performance Assessment
The performance of VGG16 and VGG19 models for COVID-19 detection using chest
X-rays. We have used a dataset of 22,698 images that includes COVID-19, normal, and
pneumonia samples. We have also used data augmentation techniques to increase the
size of the dataset. In the validation section, we report that VGG16 performs better than
VGG19 with a precision of 70.6667% compared to 70.0021% (Table 2). However, both
models are correct when tested on individual patients’ data. We have also provided graphs
that show the precision and failure of the VGG16 model across 16 epochs. The VGG16
model takes 60 s per epoch to train, while VGG19 requires just 13 s. Our approach differs
from previous studies as we have used ten angles to capture the X-ray images, resulting
in a large amount of data. We also cite several other studies that have used deep learning
models for COVID-19 detection using chest X-rays. Overall, having lower scores than
some of the earlier research described indicates that their proposed technique provides
accurate ﬁndings. Using the submitted dataset, which included COVID-19, normal, and
pneumonia samples, we achieved an overall accuracy of 70.6667% using VGG16. For
covid sample, we take 2358 pictures in the dataset, and for normal and Pneumonia,
respectively, 15,575 and 4365 pictures.
Table 2. Modeling precision and failure of VGG 19 and VGG 16
Framework
Precision in
training
Failure in training
Precision in
validation
Failure in
validation
VGG16
0.7631
0.6253
0.6527
0.7092
VGG19
0.7253
0.6033
0.6344
0.6999
We evaluated our model in two ways: the precision of the test set and supplying an
input image of a Covid-19 Chest X-ray to see if our model could detect it. The VGG19
and VGG16 models are both correct. The graphs below show the precision and failure
of the VGG16 model across 16 epochs (Fig. 4).

Deep Learning Approach for COVID-19 Detection
401
Fig. 4. Model failure and precision of VGG16
When the two models are compared, the VGG16 structure needs minimal epochs
to reach greater accuracy than the VGG19 structure. The VGG16 model takes 60 s per
epoch to train, but the VGG19 structure requires just 13 s. As a result, the VGG19 model
is substantially faster to train than the VGG16 structure.
Table 3. Other works
Model
Result (%)
VGG19 [9]
CNN [6]
98
95.92
Robust feature Forests classiﬁer [14]
VGG16 [11]
91.3
94
From Table 3, in our paper, the best accuracy is 70.6667% in VGG16 which is
22,737 data. Our data have been transformed into extra data through augmentation. In
most papers, only three angles were used to capture the photographs, and accuracy was
90–98%; however, in our paper, ten angles were used to capture the images. Using a
single image with numerous dimensions resulted in an excessively enormous amount of
data, which caused our paper’s accuracy to be low. Although we have low scores, our
results are accurate, which is highly valuable for our medical section.

402
F. R. Akash et al.
5
Conclusion
In this study, the authors compared two techniques for identifying a patient’s chest X-
rays using the Deep Learning technique to identify COVID-19. By comparing VGG-16
and VGG-19, the VGG-19 produced the ﬁnest outcomes. The excellent accuracy of
VGG-16 is 70.6667, and the accuracy of VGG-19 is 70.0021. This approach might
serve as a signiﬁcant and, using a quick procedure, patients can be identiﬁed as hav-
ing COVID-19, which is speedier and more appropriate medical care. To evaluate for
high computing speed, performance, and practical application of deep learning tech-
niques, future work on a large dataset utilizing GPU will consider many more attributes.
The proposed VGG-16 and VGG-19 models can circumvent these constraints. The pro-
posed approach can potentially detect a COVID-19-positive individual in a reasonable
amount of time. We anticipate that new biomarkers other than CT-Scan and X-Ray with
Covid-19 pneumonia cases and larger datasets will be available in the future. Our model
ought to be able to classify COVID-19 Pneumonia cases utilizing a range of various
biomarkers with relevant information as the amount of biomarker modalities increases,
and various biomarkers may provide supplementary information for the diagnosis of
COVID-Pneumonia.
References
1. Wang, W., Xu, Y., Gao, R., Lu, R., Han, K., Wu, G., Tan, W.: Detection of SARS-CoV-2 in
different types of clinical specimens. JAMA 323, 1843–1844 (2020)
2. Ji, T., Liu, Z., Wang, G., Guo, X., Lai, C., Chen, H., Huang, S., Xia, S., Chen, B., Jia, H., et al.:
Detection of COVID-19: a review of the current literature and future perspectives. Biosens.
Bioelectron. 166, 112455 (2020)
3. Raoult, D., Zumla, A., Locatelli, F., et al.: Coronavirus infections: epidemiological, clinical
and immunological features and hypotheses. Cell Stress 4(4), 66 (2020)
4. Harmon, S.A., Sanford, T.H., Xu, S., Turkbey, E.B., Roth, H., Xu, Z., et al.: Artiﬁcial intel-
ligence for the detection of COVID-19 pneumonia on chest CT using multinational datasets.
Nat. Commun. 11(1), 4080 (2020)
5. Chu, D.K.W., Pan, Y., Cheng, S.M.S., Hui, K.P.Y., Krishnan, P., Liu, Y., et al.: Molecular
diagnosis of a novel coronavirus (2019-nCoV) causing an outbreak of pneumonia. Clin.
Chem. 66, 549–555 (2020)
6. Jahmunah, V., et al.: Future IoT tools for COVID-19 contact tracing and prediction: a review
of the state-of-the-science. Int. J. Imaging Syst. Technol. 31(2), 455–471 (2021)
7. Chaudhary, S.: Ensemble deep learning method for Covid-19 detection via chest X-rays.
http://doi.org/10.1109/EE-RDS53766.2021.9708581
8. Hilmizen, N.: The multimodal deep learning for diagnosing COVID-19 pneumonia. http://
doi.org/10.1109/ISRITI51436.2020.9315478
9. Reynaldi, D.: COVID-19 classiﬁcation for chest X-ray images using deep learning and resnet-
101. http://doi.org/10.1109/ICOTEN52080.2021.9493431
10. Meepaganithage, A.: Detecting COVID-19 pneumonia using chest X-rays through deep
learning techniques. http://doi.org/10.1109/ICARC54489.2022.9753784
11. Azar, A.S.: Lightweight method for the rapid diagnosis of coronavirus disease 2019 from
chest X-ray images using deep learning technique. http://doi.org/10.1109/NSS/MIC44867.
2021.9875630

Deep Learning Approach for COVID-19 Detection
403
12. Azeem, M.A.: COVID-19 detection via image classiﬁcation using deep learning on chest
X-ray. http://doi.org/10.1109/EE-RDS53766.2021.9708588
13. Özdemır, Z.: Covid-19 detection in chest X-ray images with deep learning. http://doi.org/10.
1109/SIU53274.2021.9478028
14. Bhuvana, S.: Covid-19 detection using chest X-rays with image-based deep learning. http://
doi.org/10.1109/ICESC54411.2022.9885573
15. Oraib, Z.A.: Prediction of COVID-19 from chest X-ray images using multiresolution tex-
ture classiﬁcation with robust local features. http://doi.org/10.1109/COMPSAC51774.2021.
00096
16. Saha, R., Debi, T., Areﬁn, M.S.: Developing a framework for vehicle detection, tracking
and classiﬁcation in trafﬁc video surveillance. In: Vasant, P., Zelinka, I., Weber, G.W. (eds.)
Intelligent Computing and Optimization. ICO 2020. Advances in Intelligent Systems and
Computing, vol. 1324. Springer, Cham (2021). https://doi.org/10.1007/978-3-030-68154-
8_31
17. Fatema, K., Ahmed, M.R., Areﬁn, M.S.: Developing a system for automatic detection of
books. In: Chen, J.IZ., Tavares, J.M.R.S., Iliyasu, A.M., Du, K.L. (eds.) Second International
Conference on Image Processing and Capsule Networks. ICIPCN 2021. Lecture Notes in
Networks and Systems, vol. 300. Springer, Cham (2022). https://doi.org/10.1007/978-3-030-
84760-9_27
18. Rahman, M., Laskar, M., Asif, S., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Flower recognition
using VGG16. In: Chen, J.IZ., Tavares, J.M.R.S., Shi, F. (eds.) Third International Conference
on Image Processing and Capsule Networks. ICIPCN 2022. Lecture Notes in Networks and
Systems, vol. 514. Springer, Cham (2022). http://doi.org/10.1007/978-3-031-12413-6_59
19. Yeasmin, S., Afrin, N., Saif, K., Imam, O.T., Reza, A.W., Areﬁn, M.S.: Image classiﬁcation
for identifying social gathering types. In: Vasant, P., Weber, GW., Marmolejo-Saucedo, J.A.,
Munapo, E., Thomas, J.J. (eds.) Intelligent Computing & Optimization. ICO 2022. Lecture
Notes in Networks and Systems, vol. 569. Springer, Cham (2023). http://doi.org/10.1007/
978-3-031-19958-5_10
20. Ahmed, F., et al.: Developing a classiﬁcation CNN model to classify different types of ﬁsh. In:
Vasant, P., Weber, G.W., Marmolejo-Saucedo, J.A., Munapo, E., Thomas, J.J. (eds.) Intelligent
Computing & Optimization. ICO 2022. Lecture Notes in Networks and Systems, vol. 569.
Springer, Cham (2023). http://doi.org/10.1007/978-3-031-19958-5_50

A Survey of Modeling the Healthcare Inventory
for Emerging Infectious Diseases
Tatitayakorn Limsakul(B) and Sompoap Taladgaew
Department of Teacher Training in Mechanical Engineering, Faculty of Technical Education,
King Mongkut’s University of Technology North Bangkok, Bangkok, Thailand
Tatitayakorn@gmail.com, Sompoap.t@fte.kmutnb.ac.th
Abstract. Inventory management is a critical process in the healthcare industry.
Challenges to the healthcare industry, such as supply shortages or overstocking,
especially during the pandemic, make healthcare inventory management highly
important. Efﬁcient inventory management can help ensure quality patient care
whilereducinginventorycosts.Overtheyears,severalapproaches andmethods for
modeling healthcare inventory management have been developed by researchers.
This paper aims to provide an overview of inventory management (modeling) to
handle inventory management problems under several constraints, such as limited
budget and resources in both deterministic and probabilistic demand scenarios
by focusing on the challenges posed by the COVID-19 pandemic. In this paper,
techniques and methods, including Economic Order Quantity, Mathematical Opti-
mization Models, Stochastic Programming, and Metaheuristics are presented and
critically reviewed as guidance for future research.
Keywords: Inventory management · Covid-19 pandemic · Optimization
models · Deterministic demand · Probabilistic demand
1
Introduction
In the healthcare industry, effective inventory management is critical for maintaining
patient trust and providing seamless care. The drug management system is an essen-
tial component of a hospital information system, as stated by Little and Coughlan [1].
Despite its importance, inventory management in healthcare faces several challenges,
such as overstocking and understocking. Overstocking can result in increased storage
costs, waste, and the expiration of drugs before they are dispensed. It also exacerbates
ﬁnancial losses and reduces available storage space [2, 3]. Conversely, stock-outs can
hinder patient care by making medical supplies and equipment unavailable when needed.
The limitations of storage space and budget constraints also pose challenges for inven-
tory management in the healthcare industry [2, 3]. Common problems in inventory
management of healthcare include inaccurate demand forecasting, lack of standardiza-
tion, poor inventory visibility, inefﬁcient ordering processes, expiry of items, inadequate
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 404–413, 2024.
https://doi.org/10.1007/978-3-031-50158-6_40

A Survey of Modeling the Healthcare Inventory
405
record-keeping, and inappropriate storage conditions. Addressing these issues can help
healthcare organizations optimize their inventory management processes and provide
high-quality care to their patients.
Excessive inventory ties up capital and represents a signiﬁcant ﬁnancial investment
that could be used elsewhere [3]. Effective inventory management helps to prevent stock-
outs, reduce the risk of disruptions to patient care, and ensure that medical supplies and
equipment are readily available when needed. This highlights the importance of striking
a balance between maintaining conﬁdence in the system and avoiding overstocking.
Inventory management in healthcare involves managing and controlling a large number
and great variety of items stocked in a healthcare system [4, 5].
In the healthcare industry, striking a balance between ensuring an adequate supply
of medical supplies and avoiding overstocking is a challenging task. Effective inventory
management practices must be implemented to minimize the risk of stock-outs and
excess inventory. This includes regularly monitoring stock levels, predicting demand,
and ordering supplies in a timely manner. The use of Heuristic and Meta-Heuristic
methods and regular stocktaking can also help minimize excess inventory.
However, predicting demand in a healthcare system is difﬁcult due to uncertain-
ties and randomness, such as changes in patient conditions, dynamics in physicians’
prescriptions, and individual patient responses to treatment procedures [6, 7]. Drug
shortages pose a signiﬁcant challenge for healthcare institutions and often interfere with
patient care. During shortages, alternate therapeutic agents are typically selected, but
these agents can present challenges and raise safety concerns, leading to adverse events,
medication errors, and patient complaints [8]. The timing, location, type, and amount of
demand in most humanitarian settings [9, 10] bring signiﬁcant challenges in developing
effective inventory policies [11].
During the Covid-19 pandemic, the sudden increase in demand for medication had
a signiﬁcant impact on the supply chain. Medication shortages occur when the available
supply of a medication is insufﬁcient to meet current or projected demand at the patient
level. The frequency of worldwide medication shortages has been rising in recent years,
partly due to pharmaceutical procurement plans facing shortages of raw materials at the
national level. This is due to national lockdowns in countries that produce pharmaceuti-
cal raw materials, such as the United States, China, India, and Europe [12]. Therefore,
the shortage of medicines for patients is a major problem that has been widely studied
[8, 13]. Different authors have developed optimization models to address the problem of
medicine inventory management by considering various constraints and approaches to
model uncertainty [14, 15]. The demand for healthcare items is a signiﬁcant factor affect-
ing inventory systems, and traditional forecasting techniques can predict the stationary
demand [16]. However, the demand is inﬂuenced by several sources of randomness,
such as patient number and treatment stage, patient condition, medication reaction, and
physician recommendation [7].
In conclusion, an effective inventory management is crucial for maintaining patient
satisfaction and trust in the healthcare system. By balancing the availability of supplies
and the reduction of excessive inventory, healthcare facilities can establish a strong
and dependable supply chain, promoting the delivery of high-quality patient care. The
demand can be divided into two categories: 1) Deterministic Demand and 2) Probabilistic
or Stochastic Demand.

406
T. Limsakul and S. Taladgaew
2
The Deterministic Demand: An Overview
Deterministic demand in inventory management refers to demand for a product that can
be predicted with certainty based on historical sales data and future demand forecasts.
In contrast to probabilistic demand, which is subject to random ﬂuctuations and external
factors, deterministic demand is consistent and predictable. This type of demand is usu-
ally observed in stable markets where buying habits are consistent and well-established,
such as seasonal demand for certain products or the demand for a single product in a
steady-state market.
In the healthcare industry, examples of deterministic demand include the demand for
certain medical supplies, such as gloves and masks, during a pandemic, or the demand
for seasonal. In these cases, the demand can be accurately forecasted based on histori-
cal data, and the healthcare organization can make informed decisions about how much
inventory to order and when to order it. By having a good understanding of deterministic
demand, healthcare organizations can avoid stock shortages, which can lead to opera-
tional disruptions and negatively impact patient care, or excessive inventory, which can
result in waste and increase costs.
TheEconomicOrderQuantity(EOQ)modelisacommonlyuseddeterministicmodel
for inventory control. This model assumes a constant and known demand for an item,
constantleadtime,constantunitprice,inventoryholdingcostbasedonaverageinventory,
constant ordering cost and no backorder allowed. The EOQ model has been applied in
various hospital settings to improve inventory management and reduce costs, such as at
Georgetown University Hospital (GUH) by Kapur and Moberg [17] in a 558 bed general
hospital by Ballentine [18] at Ramathibodi Hospital by Laeiddee [19] formulas (1)
EOQ =

2DS
H
(1)
S Setup costs (per order, generally including shipping and handling)
D Demand rate (quantity sold per year)
H Holding costs (per year, per unit)
Excess inventories in hospitals have also been studied, such as in a local hospital
by Hafnika et al. [20] using continuous review policy, EOQ, reorder point, average
inventory level, and ABC classiﬁcation. Kritchanchai and Meesamut [21] develop the
total inventory costs by EOQ model ABC classiﬁcation in a large public hospital in
Thailand. Other applications of deterministic demand in hospitals include blood plasma
management by Ma et al. [22] using EOQ model, safety stock, and reorder point, and
inventory management for a pharmaceutical company and hospital by Uthayakumar and
Priyan [23] using an operations research model. Operations research has also been used
to optimize a hospital’s inventory costs, such as in a hospital’s central pharmacy by
Stecca et al. [24] and through the application of Model Predictive Control (MPC) by
Maestre et al. [3]. These studies demonstrate the continued efforts to improve inventory
management in healthcare using deterministic demand models.

A Survey of Modeling the Healthcare Inventory
407
3
The Probabilistic Demand: An Overview
Probabilistic demand in inventory management takes into account the uncertainty and
variability of demand for a product over time. It recognizes that demand is not always
predictable with certainty and that there is a degree of randomness in the process. This
randomness can be due to a variety of factors such as changes in consumer behavior,
ﬂuctuations in the economy, and unexpected events. To account for this uncertainty, orga-
nizations use probabilistic methods to model and predict demand. This involves creating
a probability distribution for demand and using it to simulate different scenarios and
make decisions about inventory levels. By considering the uncertainty of demand, orga-
nizations can make more informed decisions about safety stock levels, reorder points,
and order quantities. This helps to ensure that inventory is always at the right level to
meet customer demand, while avoiding waste and inefﬁciencies.
The healthcare industry can beneﬁt greatly from the application of probabilistic
demand analysis in inventory management. Due to the nature of the industry, demand
for medical supplies and equipment can be highly variable and unpredictable, especially
in the case of emergencies or outbreaks like COVID-19. By using probabilistic meth-
ods to model demand, healthcare organizations can better prepare for ﬂuctuations in
demand and ensure that they have the necessary inventory levels to meet patient needs.
For example, during a pandemic, healthcare organizations can use probabilistic demand
analysis to determine the likelihood of increased demand for certain medical supplies
and plan their inventory accordingly. Moreover, probabilistic demand analysis can also
help healthcare organizations reduce waste and improve cost efﬁciency by avoiding over-
stocking and excessive inventory levels. By understanding the range of possible demand
scenarios, organizations can make informed decisions about the optimal inventory levels
to maintain and the frequency of reordering.
Probabilistic demand analysis is a valuable tool for the healthcare industry in manag-
ing inventory and ensuring that critical medical supplies and equipment are available to
meet patient needs. It helps organizations to be more proactive, efﬁcient, and resilient in
the face of uncertainty and variability in demand. Healthcare systems face many uncer-
tainties, including changes in patient numbers, clinical conditions, and the availability
of medicines, as highlighted by Addis et al. [25]. In response, several studies have aimed
to develop models for optimizing inventory management in hospitals.
3.1
Mathematical Optimization Models
Mathematical optimization models are a valuable tool for analyzing and optimizing
complex systems. These models incorporate objectives or goals that are represented
by mathematical functions, allowing for a systematic exploration of trade-offs and the
identiﬁcation of optimal solutions. In the healthcare sector, optimization models can be
particularly useful for decision making and resource allocation. For example, Najaﬁ,
Ahmadi, and Zolfagharinia [26] applied optimization to blood inventory management,
developing a model that considers uncertain demand and supply. Franco and Alfonso-
Lizarazo [27] used mixed integer programming (MIP) approaches for optimizing the
pharmaceutical supply chain, taking into account elements such as demand and lead

408
T. Limsakul and S. Taladgaew
times. Additionally, Khoukhi, Bojji, and Bensouda [28] presented an inventory opti-
mization model aimed at minimizing total cost while considering constraints such as
storage space, order frequency, and service level and evaluated its performance through
mathematical model and Monte Carlo simulation. Overall, the use of optimization mod-
els in healthcare can lead to improved patient care, reduced costs, and a more efﬁcient
healthcare system.
Mathematical optimization models have advantages and disadvantages in decision-
making. They offer a structured and systematic approach to decision-making and can
formalize constraints and objectives, providing mathematical guarantees for optimality
and feasibility. They can handle complex and large-scale problems and identify the best
solution among a large number of alternatives. The other hand, the models also have
several disadvantages. One signiﬁcant disadvantage is that these models require a high
level of mathematical expertise to formulate and solve. Furthermore, the models may
not always reﬂect real-world complexity and uncertainty, leading to inaccuracies in the
results. The computational intensive and time-consuming nature of these models, partic-
ularly for large-scale and complex problems, is another disadvantage. Finally, the solu-
tions generated by mathematical optimization models may not always be interpretable
or have practical relevance.
3.2
Stochastic Programming
Stochastic Programming, also referred to as Stochastic Optimization is a mathematical
framework that models decision-making under uncertainty [29]. In the area of inventory
managementinhospitalpharmacies,itisutilizedtominimizetheexpectedtotalinventory
costs while satisfying service level and space constraints. The complexity of the problem
is due to the presence of a large number of variables, non-linearity, and stochastic con-
straints, where the latter refers to the requirement of maintaining a speciﬁc probability
of no shortage of medicines. The utilization of two-stage stochastic programming has
also been noted in the solution to the problem of allocating surgical supplies in multiple
locations within a healthcare system [30, 31]. In 2014, Priyan and Uthayakumar [32]
proposed a stochastic model to minimize the impact of drug shortages. Rajendran and
Ravindran [33] created a mixed integer stochastic programming model to tackle demand
uncertainty and presented three heuristic rules for determining the platelet ordering pol-
icy. In 2023, Meneses, Marques, and Barbosa-Póvoa [34] addressed the challenges of
blood product ordering policies through a two-stage stochastic programming model that
considers demand uncertainty.
Stochastic Programming provides several advantages for decision-making under
uncertainty. The ability to incorporate uncertainty leads to more accurate and realistic
models, and the use of expected value objectives results in robust solutions. Additionally,
stochastic programming can deal with multiple objectives, such as cost minimization and
service level maximization, in a single model. However, stochastic programming also
has some limitations. The models can be complex and computationally intensive, and
the solution accuracy is dependent on the accuracy of the probability distribution used
to model uncertain parameters. Additionally, the models require a signiﬁcant amount of
data, which can be challenging to obtain, and the validity of the model depends on the
accuracy of the assumptions made about the distribution of uncertain parameters.

A Survey of Modeling the Healthcare Inventory
409
3.3
Metaheuristics
In recent years, metaheuristics have become a popular tool for healthcare inventory man-
agement due to their fast and efﬁcient solution-ﬁnding capabilities. Metaheuristics were
ﬁrst proposed by Glover in 1986 and have since been widely used in various optimization
problems. The most commonly used metaheuristics in healthcare inventory management
are the Tabu Search Algorithm (TS), Simulated Annealing Method (SAM), Ant Colony
Optimization (ACO), Genetic Algorithm (GA), Emperor Penguin Optimizer (EPO),
Seagull Optimizer, and Guided Local Search. The Genetic Algorithm (GA) is one of the
most widely used metaheuristics in healthcare inventory management due to its ability
to provide near-optimal solutions in a relatively short amount of time. This algorithm
is based on Charles Darwin’s theory of natural selection and genetics, which is referred
to as “Survival of the Fittest” [35]. The GA has been applied to various problems in
healthcare inventory management, including reducing the total cost of the pharmaceuti-
cal supply chain from manufacturer to patient [36] and developing an optimal inventory
of platelets [37]. In addition, Du, Luo, Wang, & Liu [38] proposed a model for a hospital
pharmacy system using a combination of genetic algorithms and a BP neural network to
improve the efﬁciency of pharmaceutical inventory management. The model was based
on actual conditions and performed a sensitivity analysis to provide guidelines for drug
inventory management in hospitals.
Metaheuristics have proven to be a valuable tool in healthcare inventory manage-
ment, offering a fast and efﬁcient way to solve complex optimization problems. Despite
some disadvantages, such as computational intensity and potential limitations in ﬁnding
the global optimum solution, metaheuristics are a versatile and adaptable tool that can
provide near-optimal solutions in a variety of inventory management scenarios. Over-
all, the use of metaheuristics in healthcare inventory management can lead to improved
decision-making and better outcomes.
Fig. 1. Optimization model between deterministic and probabilistic demand
Figure 1, various approaches for managing healthcare inventory are presented, which
includes both items with deterministic demand, such as masks and gloves, as well as
items with probabilistic demand arising from uncertainty. Therefore, a joint analysis
may be necessary for optimizing healthcare inventory management.

410
T. Limsakul and S. Taladgaew
4
Conclusion
As the world is currently facing the Covid-19 pandemic, it continues to pose signiﬁcant
challenges for healthcare supply chains. Therefore, ensuring the availability of health-
care facilities by balancing demand and supply to avoid high storage costs as well as
stockouts is of utmost importance. The Covid-19 situation also emphasizes that not only
should patients be treated, but they must receive prompt and timely treatment. Hence,
efﬁcient healthcare inventory management will beneﬁt patients, especially when lives
are at stake, by providing quick and quality care. A reﬁned and optimized approach to this
challenge is essential to ensure that patients receive prompt and effective treatment, even
during a pandemic. The techniques and methods proposed in this study can be improved
for managing healthcare inventory under demand uncertainty and will be beneﬁcial for
future research in handling healthcare inventory problems, such as unpredictable demand
caused by emerging infectious diseases. The conclude that to solve healthcare supply
chain problems during the pandemic should consider the model of stochastic demand,
which is also observed in later research focusing on stochastic demand with different
problem-solving methods. In future research needs to perform experiments from equa-
tions and methods applied to real data situations to ﬁnd out which method is most suitable
for managing healthcare inventory. This must take into account other conditions as well,
such as difﬁculty in actual application, calculation period, and delivery uncertainty.
Table 1 appraises a number of research papers on modelling and analyzing inventory
management systems in healthcare. The papers discuss types of healthcare inventory
problems, existing modelling approaches, and solution methods.
Table 1. Research papers of healthcare inventory
Author
Published
year
Demand
Methodology
Deterministic
Stochastic
EOQ
Mathematical
model
Stochastic
programming
Genetic
algorithm
Ballentine
et al. [18]
1976
√
√
Kapur et al.
[17]
1987
√
√
Laeiddee [19]
2010
√
√
Kelle et al.
[30]
2012
√
√
Ma et al. [22]
2013
√
√
Uthayakumar
et al. [23]
2013
√
√
Priyan et al.
[32]
2014
√
√
Kritchanchai
et al. [21]
2015
√
√
Hafnika et al.
[20]
2016
√
√
(continued)

A Survey of Modeling the Healthcare Inventory
411
Table 1. (continued)
Author
Published
year
Demand
Methodology
Deterministic
Stochastic
EOQ
Mathematical
model
Stochastic
programming
Genetic
algorithm
Stecca et al.
[24]
2016
√
√
Najaﬁet al.
[26]
2017
√
√
Rajendran
et al. [33]
2017
√
√
Maestre et al.
[3]
2018
√
√
Khoukhi
et al. [28]
2019
√
√
Rajendran
et al. [37]
2019
√
√
Franco et al.
[27]
2020
√
√
Du et al. [38]
2020
√
√
Nasrollahi
et al. [36]
2021
√
√
Meneses
et al. [34]
2023
√
√
References
1. Little, J., Coughlan, B.: Optimal inventory policy within hospital space constraints. Health
Care Manag. Sci. 11(2), 177–183 (2008)
2. Bijvank, M., Vis, I.F.: Inventory control for point-of-use locations in hospitals. J. Oper. Res.
Soc. 63, 497–510 (2012)
3. Maestre, J.M., Fernández, M.I., Jurado, I.J.C.E.P.: An application of economic model pre-
dictive control to inventory management in hospitals. Control Eng. Pract. 71, 120–128
(2018)
4. Gebicki, M., Mooney, E., Chen, S.J., Mazur, L.M.: Evaluation of hospital medication
inventory policies. Health Care Manag. Sci. 17, 215–229 (2014)
5. Nicholson, L., Vakharia, A.J., Erenguc, S.S.: Outsourcing inventory management decisions
in healthcare: models and application. Eur. J. Oper. Res. 154(1), 271–290 (2014)
6. Montoya, R., Netzer, O., Jedidi, K.: Dynamic allocation of pharmaceutical detailing and
sampling for long-term proﬁtability. Mark. Sci. 29(5), 909–924 (2010)
7. Vila-Parrish, A.R., Ivy, J.S., King, R.E., Abel, S.R.: Patient-based pharmaceutical inven-
tory management: a two-stage inventory and production model for perishable products with
Markovian demand. Health Syst. 1(1), 69–83 (2012)
8. McLaughlin, M., et al.: Effects on patient care caused by drug shortages: a survey. J. Manag.
Care Pharm. 19(9), 783–788 (2013)
9. Hughes, D., McGuire, A.: Stochastic demand, production responses and hospital costs. J.
Health Econ. 22(6), 999–1010 (2003)
10. Darmian, S.M., Fattahi, M., Keyvanshokooh, E.: An optimization-based approach for the
healthcare districting under uncertainty. Comput. Oper. Res. 135, 105425 (2021)

412
T. Limsakul and S. Taladgaew
11. Whybark, D.C.: Issues in managing disaster relief inventories. Int. J. Prod. Econ. 108(1–2),
228–235 (2007)
12. Goodarzian, F., Taleizadeh, A.A., Ghasemi, P., Abraham, A.: An integrated sustainable
medical supply chain network during COVID-19. Eng. Appl. Artif. Intell. 100, 104188 (2021)
13. Caulder, C.R., Mehta, B., Bookstaver, P.B., Sims, L.D., Stevenson, B., South Carolina Society
of Health-System Pharmacists: Impact of drug shortages on health system pharmacies in the
Southeastern United States. Hosp. Pharm. 50(4), 279–286 (2015)
14. Fox, E.R., Sweet, B.V., Jensen, V.: Drug shortages: a complex health care crisis. In: Mayo
Clinic Proceedings, vol. 89, no. 3, pp. 361–373. Elsevier, Amsterdam (2014)
15. Kwon, I.W.G., Kim, S.H., Martin, D.G.: Healthcare supply chain management; strategic areas
for quality and ﬁnancial improvement. Technol. Forecast. Soc. Chang. 113, 422–428 (2016)
16. Ramirez, A.L., Jurado, I., Garcia, M.F., Tejera, B.I., Llergo, J.D.P., Torreblanca, J.M.: Opti-
mization of the demand estimation in hospital pharmacy. In: Proceedings of the 2014 IEEE
emerging technology and factory automation (ETFA), pp. 1–6. IEEE (2014)
17. Kapur, R., Moberg, C.: Evaluating inventory turns for a hospital environment. Comput. Ind.
Eng. 13(1–4), 73–77 (1987)
18. Ballentine, R., Ravin, R.L., Gilbert, J.R.: ABC inventory analysis and economic order quantity
concept in hospital pharmacy purchasing. Am. J. Hosp. Pharm. 33(6), 552–555 (1976)
19. Laeiddee, C.: Improvement of re-order point for drug inventory management at Ramathibodi
Hospital. Doctoral dissertation, Mahidol University (2010)
20. Hafnika, F., Farmaciawaty, D.A., Adhiutama, A., Basri, M.H.: Improvement of inventory
control using continuous review policy in a local hospital at Bandung City, Indonesia. Asian
J. Technol. Manag. 9(2), 109 (2016)
21. Kritchanchai, D., Meesamut, W.: Developing inventory management in hospital. Int. J. Supply
Chain Manag. 4(2), 11–19 (2015)
22. Ma, J., Lei, T., Okudan, G.E.: EOQ-based inventory control policies for perishable items:
the case of blood plasma inventory management. In: IIE Annual Conference, Proceedings,
pp. 1683–1692. Institute of Industrial and Systems Engineers (IISE) (2013)
23. Uthayakumar, R., Priyan, S.: Pharmaceutical supply chain and inventory management strate-
gies: optimization for a pharmaceutical company and a hospital. Oper. Res. Health Care 2(3),
52–64 (2013)
24. Stecca, G., Baffo, I., Kaihara, T.: Design and operation of strategic inventory control system
for drug delivery in healthcare industry. IFAC-PapersOnLine 49(12), 904–909 (2016)
25. Addis, B., Carello, G., Grosso, A., Lanzarone, E., Mattia, S., Tànfani, E.: Handling uncer-
tainty in health care management using the cardinality-constrained approach: advantages and
remarks. Oper. Res. Health Care 4, 1–4 (2015)
26. Najaﬁ, M., Ahmadi, A., Zolfagharinia, H.: Blood inventory management in hospitals: consid-
ering supply and demand uncertainty and blood transshipment possibility. Oper. Res. Health
Care 15, 43–56 (2017)
27. Franco, C., Alfonso-Lizarazo, E.: Optimization under uncertainty of the pharmaceutical
supply chain in hospitals. Comput. Chem. Eng. 135, 106689 (2020)
28. Khoukhi, S., Bojji, C., Bensouda, Y.: Optimal inventory control policy for a hospital case. In:
2019 5th International Conference on Optimization and Applications (ICOA), pp. 1–5. IEEE
(2019)
29. Birge, J.R., Louveaux, F.: Introduction to Stochastic Programming. Springer Science &
Business Media, Berlin (2011)
30. Kelle, P., Woosley, J., Schneider, H.: Pharmaceutical supply chain speciﬁcs and inventory
solutions for a hospital case. Oper. Res. Health Care 1(2–3), 54–63 (2012)
31. Ahmadi, E., Masel, D.T., Hostetler, S.: A robust stochastic decision-making model for inven-
tory allocation of surgical supplies to reduce logistics costs in hospitals: a case study. Oper.
Res. Health Care 20, 33–44 (2019)

A Survey of Modeling the Healthcare Inventory
413
32. Priyan, S., Uthayakumar, R.: Optimal inventory management strategies for pharmaceutical
company and hospital supply chain in a fuzzy–stochastic environment. Oper. Res. Health
Care 3(4), 177–190 (2014)
33. Rajendran, S., Ravindran, A.R.: Platelet ordering policies at hospitals using stochastic integer
programming model and heuristic approaches to reduce wastage. Comput. Ind. Eng. 110,
151–164 (2017)
34. Meneses, M., Marques, I., Barbosa-Póvoa, A.: Blood inventory management: ordering poli-
cies for hospital blood banks under uncertainty. Int. Trans. Oper. Res. 30(1), 273–301
(2023)
35. Holland, J.H.: Adaptation in Natural and Artiﬁcial Systems: An Introductory Analysis with
Applications to Biology, Control, and Artiﬁcial Intelligence. MIT Press (1992)
36. Nasrollahi, M., Razmi, J.: A mathematical model for designing an integrated pharmaceutical
supply chain with maximum expected coverage under uncertainty. Oper. Res. Int. J. 21(1),
525–552 (2019). https://doi.org/10.1007/s12351-019-00459-3
37. Rajendran, S., Ravindran, A.R.: Inventory management of platelets along blood supply chain
to minimize wastage and shortage. Comput. Ind. Eng. 130, 714–730 (2019)
38. Du, M., Luo, J., Wang, S., Liu, S.: Genetic algorithm combined with BP neural network in
hospital drug inventory management system. Neural Comput. Appl. 32(7), 1981–1994 (2020)

MEREC-MABAC Based-Parametric
Optimization of Chemical Vapour Deposition
Process for Diamond-Like Carbon Coatings
Sellamuthu Prabhukumar1, Jasgurpeet Singh Chohan2, and Kanak Kalita3(B)
1 Department of Mechanical Engineering, Presidency University, Bangalore, India
prabhukumar.sellamuthu@presidencyuniversity.in
2 Department of Mechanical Engineering, University Centre for Research and Development,
Chandigarh University, Mohali, India
3 Department of Mechanical Engineering, Vel Tech Rangarajan Dr, Sagunthala R&D Institute of
Science and Technology, Avadi, India
drkanakkalita@veltech.edu.in
Abstract. This research paper presents an application of the Multi-attributive
Border Approximation Area Comparison (MABAC) method combined with the
Method based on the Removal Effects of Criteria (MEREC) for the parametric
optimization of the Chemical Vapor Deposition (CVD) process in Diamond-like
Carbon (DLC) coatings. A decision matrix is formulated using a case study from
the literature. Four response parameters namely, Hardness (H), Young’s modulus
(E),CoefﬁcientofFriction(COF),andWearRate(WR)areconsidered.Theweight
allocation for these response parameters is calculated using ﬁve different meth-
ods, namely MEREC, mean weight (Mean), Standard deviation (StDev), Entropy,
and Criteria Importance Through Intercriteria Correlation (CRITIC) method. The
MABAC method was employed to obtain the optimal parametric combination for
the DLC coatings. Results showed a clear superior combination of the CVD pro-
cess parameters can be achieved using the MEREC-MABAC methodology. Thus,
the study successfully demonstrates the effectiveness of the MEREC-MABAC-
based approach for the simultaneous optimization of multiple responses in the
CVD process for DLC coatings.
Keywords: Diamond-like carbon coatings · Chemical vapor deposition ·
Parametric optimization · MABAC · MEREC · Multi-criteria decision making
1
Introduction
Diamond-like Carbon (DLC) coatings have been widely researched due to their excep-
tional properties, such as high hardness, low wear rate, and low coefﬁcient of friction,
which make them suitable for various applications in industries such as automotive,
aerospace, and biomedical. The Chemical Vapor Deposition (CVD) process is com-
monly used to develop these coatings, but the quality of the coatings is signiﬁcantly
inﬂuenced by the process parameters. Among the numerous CVD parameter, the H2
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 414–422, 2024.
https://doi.org/10.1007/978-3-031-50158-6_41

MEREC-MABAC Based-Parametric Optimization of Chemical
415
ﬂow rate, C2H2 ﬂow rate and deposition temperature (T) are found to be some of the
most signiﬁcant ones. Therefore, optimizing these parameters is crucial to achieving
coatings with desired properties.
In recent years, various research studies have been conducted to investigate the
optimization of deposition parameters for DLC coatings using different techniques and
methodologies. Ghadai et al. [1] employed a thermal CVD process and used input param-
eters such as temperature (T), H2 ﬂow rate and C2H2 ﬂow rate to optimize hardness (H)
and Young’s modulus (E). Jatti et al. [2] investigated the optimization of H, E, and ID/IG
ratio using the Inductively Coupled PECVD process. They considered input parameters
like voltage (V), frequency (f ), pressure (P), and gas composition. An L9 experimental
design was used with nine experiments, and the Taguchi methodology was applied for
optimization. Ghadai et al. [3] used the CVD process and input parameters V, f , P, and
gas composition to optimize H, E, and ID/IG ratio. They employed an L9 experimen-
tal design with nine experiments and utilized the Grey fuzzy logic. Singh and Jatti [4]
focused on the IC-PECVD process, optimizing H and E with input parameters like V, f ,
P and gas composition. They used an L9 experimental design with nine experiments and
the Taguchi methodology for optimization. Ghadai et al. [5] used the PECVD process
to optimize H with input parameters like T, H2 ﬂow rate, and C2H2 ﬂow rate. They
employed a CCD experimental plan with 20 experiments and utilized a single-objective
GA for optimization. Ebrahimi et al. [6] employed a CVD process with input parameters
T and H2 ﬂow rate to optimize the wear rate (WR) and the coefﬁcient of friction (COF).
They used a CCD experimental plan with 13 experiments and applied the desirability
function approach for optimization. Ebrahimi et al. [7] utilized a CVD process and input
parameters T, duty cycle, H2 ﬂow rate, and argon/methane ﬂow ratio to optimize WR,
wear durability, and H. They employed a CCD experimental plan with 23 experiments
and used the desirability function approach for optimization. Kumar and Swain [8] used
a thermal CVD process with input parameters T, H2 ﬂow rate, and N2 ﬂow rate to opti-
mize H, E, and ID/IG ratio. Pancielejko et al. [9] used a modiﬁed cathodic vacuum arc
method with input parameters V, argon pressure, coating thickness (t), and thickness of
chromium interlayer (tcr) to optimize H and WR. They employed an L9 experimental
design with nine experiments and utilized the Taguchi methodology for optimization.
Czyzniewski et al. [10] investigated the optimization of H, WR, adhesion, and H/E
parameter using sputtering. They used input parameters like V, C2H2 ﬂow rate, t, and
tcr. An L9 experimental design was employed with nine experiments, and the Taguchi
methodology was applied for optimization.
From the literature review, it is found that several optimization methods have been
applied in the literature for multi-objective problems. However, there is no application
of newer methods like MEREC and MABAC in CVD process optimization. Thus, in
this study, the Multi-attributive Border Approximation Area Comparison (MABAC)
method, combined with the Method based on the Removal Effects of Criteria (MEREC)
is employed for parametric optimization of the CVD process for DLC coatings. The
main aim is to ﬁnd the optimal CVD process parameters that yield the best compromise
between Hardness (H), Young’s modulus (E), Coefﬁcient of Friction (COF), and Wear
Rate (WR).

416
S. Prabhukumar et al.
2
Methodology
2.1
MEREC
MEREC is a method for determining the weights of various criteria in multi-criteria
decision-making (MCDM) problems [11]. MEREC focuses on the removal effect of
each criterion on the alternative’s performance. Criteria with higher effects on the per-
formances receive greater weights. A logarithmic measure calculates alternatives’ per-
formances, and the absolute deviation measure identiﬁes the effects of removing each
criterion. The pseudo-code for MEREC is as follows:
1. Deﬁne the decision matrix (X ) with elements xij.
2. Normalize the decision matrix (N) using nxij
nx
ij =
⎧
⎨
⎩
min
k xkj
xij
if j ∈B
xij
max
k
xkj if j ∈C
(1)
where B is the set of beneﬁcial criteria and C is the set of non-beneﬁcial (cost) criteria.
3. Calculate overall performance Si for each alternative i using a logarithmic measure
Si = ln
⎛
⎝1 +
⎛
⎝1
m

j
ln
	
nx
ij


⎞
⎠
⎞
⎠
(2)
4. Calculate performance Sij′ of each alternative i by removing criterion j
S′
ij = ln
⎛
⎝1 +
⎛
⎝1
m

k,k̸=j
ln

nx
ik

⎞
⎠
⎞
⎠
(3)
5. Compute the summation of absolute deviations Ej for each criterion j
Ej =

i
|S′
ij −Si|
(4)
6. Determine the ﬁnal weights wj of the criteria
wj =
Ej

k Ek
(5)
2.2
MABAC
The MABAC method is an MCDM technique designed to evaluate, rank, and select the
best alternatives among a set of decision alternatives based on multiple criteria [12].
MABAC is particularly effective in dealing with complex decision-making problems
that involve conﬂicting criteria, as it incorporates the concept of border approximation
area to determine the relative importance of each alternative. This approach facilitates
the ranking of alternatives by comparing their proximity to an ideal solution, which is
represented by a border approximation area. The pseudo-code for MABAC is as follows:

MEREC-MABAC Based-Parametric Optimization of Chemical
417
1. Develop the decision matrix (X ) with m alternatives and n criteria. xij are the elements
of X .
2. Normalize X to form the normalized matrix R (with elements rij) using the following
rules
rij =
xij −x−
j
x+
j −x−
j
for beneﬁt criteria
(6)
rij =
xij −x+
j
x−
j −x+
j
for cost criteria
(7)
x+
j and x−
j are the maximum and minimum values of the J th criterion.
3. Compute the weighted normalized decision matrix V (with elements vij)
vij = wj ·

rij + 1

(8)
wj is the weight of the J th criterion.
4. Compute the border approximation area (BAA) matrix B (with elements bj)
bj =
 m

i=1
vij
1/ m
(9)
5. Compute the distance matrix of alternatives (Q) from the BAA. qij are the elements
of Q.
Q = V −B
(10)
6. Compute the criteria function (Si) values and ranking the alternatives:
Si =
n

j=1
qij, j = 1, 2, . . . , n, i = 1, 2, . . . , m
(11)
7. Rank the alternatives in descending order of Si values.
3
Problem Description
In this work, the objective is to select the optimal CVD process parameters to develop
an optimized DLC coating. The challenge is to ﬁnd a suitable compromise solution
wherein multiple responses are looked upon and optimized simultaneously. In the context
of this study, four response parameters, namely Hardness (H), Young’s modulus (E),
Coefﬁcient of Friction (COF) and Wear Rate (WR) need to be optimised simultaneously.
The CVD deposition process parameters are H2 ﬂow rate, C2H2 ﬂow rate and deposition
temperature (T). Based on a central composite design of experiments, 15 experiments
were conducted by Kalita et al. [13]. Those experiments are used as the decision matrix

418
S. Prabhukumar et al.
in this study for further analysis. Thus, in the context of this study, the decision matrix
(D) iexpressed as,
D =
A1
A2
A3
A4
A5
A6
A7
A8
A9
A10
A11
A12
A13
A14
A15
⎡
⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎢⎣
H
E
COF
WR
13.37 141.74 0.24 0.00084
20.56 272.73 0.14 0.00033
14.31 146.36 0.21 0.00072
23.22 289.65 0.074 0.00035
16.69 170.52 0.146 0.00065
39.35 350.24 0.06 0.00012
18.21 183.73 0.185 0.00056
20.11 250.36 0.159 0.00045
24.59 292.35 0.086 0.00031
22.48 283.75 0.105 0.000268
34.61 312.18 0.074 0.000132
21.05 275.48 0.16 0.00038
36.33 325.49 0.094 0.000128
23.22 287.77 0.142 0.00032
30.12 298.56 0.125 0.00025
⎤
⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎥⎦
(8)
As indicated earlier, in this paper, the MABAC method is used for multi-criteria
decision-making. The weights for the four criteria are calculated using the MEREC
method. However, for the sake of comprehensive comparison, the analysis is also car-
ried out using other weight allocation methods namely, mean weight (Mean), Standard
deviation (StDev), Entropy and Criteria Importance Through Intercriteria Correlation
(CRITIC) method.
4
Results and Discussion
4.1
Multi-criteria Decision Making
Initially, the weights for the four criteria i.e., Hardness (H), Young’s modulus (E),
Coefﬁcient of Friction (COF) and Wear Rate (WR) are calculated by using the ﬁve
different weight allocation methods. Figure 1 shows the weights allocated by the various
weight allocation methods. It is observed that StDev has almost the same allocation as the
mean method. However, the Entropy method is seen to have allocated skewed weights
with excessive weightage to WR response.
The MABAC calculations are carried out using all these weights and the Q-values
are derived. The correlation between the solutions by the various weighted MABACs is
shown in Fig. 2, which shows that there is a 100% correlation among the methods for
this case study. This indicates that the parametric combination in the CCD-based CVD
experiments is such that there is a clear superior combination.
Figure 3 shows the changes in the Q-values with respect to the various parametric
combination in the CCD-based CVD experimental dataset. It should be noted here that
the Q-value can be thought of as a ‘combined proxy index’ for the goal of simultaneous

MEREC-MABAC Based-Parametric Optimization of Chemical
419
Fig. 1. Weights assigned to different criteria as per various weight allocation methods
Fig. 2. Correlation among the various weight allocation methods
maximization of H and E while minimizing COF and WR. Thus, the higher the Q-
value, the better the compromise solution. It is observed that irrespective of the weight
allocation method, the Q-values follow a similar trend. Experiment number 6 is seen to
be a clear winner that represents a good parametric combination. The various parameter
value for this experiment is 60 sccm of H2 ﬂow rate, 2.5 sccm of C2H2 ﬂow rate and
deposition temperature (T) of 800 ˚C.
4.2
Parametric Optimization of CVD
The MABAC Q-values are aggregated level-wise for each of the three process parameters
to ﬁnd out the optimal parametric combination. A higher value of aggregated Q-value
corresponds to a better parametric combination. Figure 4 shows the inﬂuence of the H2
the ﬂow rate on Q-values. It is observed that as the H2 ﬂow rate is increased the Q-values
improve. However, at 80 sccm of H2 ﬂow rate and 95 sccm of H2 ﬂow rate, the Q-values
are similar.

420
S. Prabhukumar et al.
Fig. 3. Variation of MABAC Q-values with respect to CCD-based CVD experiments
Fig. 4. Effect of H2 ﬂow rate on aggregated Q-values
Figure 5 shows the inﬂuence of the C2H2 ﬂow rate on the MABAC Q-values. The
Q-values monotonically decrease as the C2H2 ﬂow rate increases. 2.5 sccm of C2H2 ﬂow
rate is found to be the most optimal. The drop in Q-value between 2.5 sccm of C2H2
ﬂow rate and 9.5 sccm of C2H2 ﬂow rate is 151.92%. This indicates the importance
of choosing the optimal parameters for achieving the best performance from the DLC
coatings.
Figure 5 shows the inﬂuence of the deposition temperature (T) on Q-values. A higher
deposition temperature (T) is seen to be beneﬁcial for achieving a better optimized DLC
coating. Thus, as per the MEREC-MABAC analysis, a deposition temperature (T) of
900 ˚C is most beneﬁcial in achieving the optimized DLC (Fig. 6).

MEREC-MABAC Based-Parametric Optimization of Chemical
421
Fig. 5. Effect of C2H2 ﬂow rate on aggregated Q-values
Fig. 6. Effect of deposition temperature (T) on aggregated Q-values
5
Conclusions
In this research, MEREC-MABAC-based approach was applied for the parametric opti-
mization of the CVD process for DLC coatings. The optimal parametric combination
for achieving the best compromise between Hardness (H), Young’s modulus (E), Coef-
ﬁcient of Friction (COF) and Wear Rate (WR) was determined. The study demonstrated
that irrespective of the weight allocation method used, the parametric combination in the
CCD-based CVD experiments showed a clear superior combination, with experiment
number 6 having the highest Q-value.
The analysis revealed that higher H2 ﬂow rate, lower C2H2 ﬂow rate and higher
deposition temperature (T) were beneﬁcial for achieving the optimized DLC coatings.
The results of this study can provide useful insights for researchers and practitioners

422
S. Prabhukumar et al.
in the ﬁeld of DLC coatings. The proposed MEREC-MABAC-based approach can be
extended to other multi-objective optimization problems in various ﬁelds.
References
1. Ghadai, R.K., Kalita, K., Mondal, S.C., Swain, B.P.: Genetically optimized diamond-like
carbon thin ﬁlm coatings. Mater. Manuf. Process 34, 1476–1487 (2019)
2. Jatti, V.S., Laad, M., Singh, T.P.: Taguchi approach for diamond-like carbon ﬁlm processing.
Proc. Mater. Sci. 6, 1017–1023 (2014)
3. Ghadai, R.K., Das, P.P., Shivakoti, I., Mondal, S.C., Swain, B.P.: Grey fuzzy logic approach
for the optimization of DLC thin ﬁlm coating process parameters Using PACVD technique.
IOP Conf. Ser. Mater. Sci. Eng. 1–6 (2017)
4. Singh, T.P., Jatti, V.S.: Optimization of the deposition parameters of DLC coatings with the
IC-PECVD method. Part Sci. Technol. 33(2), 119–123 (2015)
5. Ghadai, R.K., Kalita, K., Mondal, S.C., Swain, B.P.: PECVD process parameter optimization:
towards increased hardness of diamond-like carbon thin ﬁlms. Mater. Manuf. Process. 33,
1905–1913 (2018)
6. Ebrahimi, M., Mahboubi, F., Naimi-Jamal, M.R.: RSM base study of the effect of deposition
temperature and hydrogen ﬂow on the wear behavior of DLC ﬁlms. Tribol. Int. 91, 23–31
(2015)
7. Ebrahimi, M., Mahboubi, F., Naimi-Jamal, M.R.: Optimization of pulsed DC PACVD
parameters: toward reducing wear rate of the DLC ﬁlms. Appl. Surf. Sci 389, 521–531 (2016)
8. Kumar, D., Swain, B.: Investigation of structural and mechanical properties of silicon
carbonitride thin ﬁlms. J. Alloys Compd. 789, 295–302 (2019)
9. Pancielejko, M., Czy˙zniewski, A., Zavaleyev, V., Pander, A., Wojtalik, K.: Optimization of
the deposition parameters of DLC coatings with the MCVA method. Arch. Mater. Sci. Eng.
54(2), 60–67 (2012)
10. Czyzniewski, A.: Optimising deposition parameters of W-DLC coatings for tool materials of
high-speed steel and cemented carbide. Vacuum 86(12), 2140–2147 (2012)
11. Kalita, K., Ghadai, R.K., Chakraborty, S.: Parametric optimization of CVD process for DLC
thin ﬁlm coatings: a comparative analysis. S¯adhan¯a 47(2), 57 (2022)
12. Keshavarz-Ghorabaee, M., Amiri, M., Zavadskas, E.K., Turskis, Z., Antucheviciene, J.: Deter-
mination of objective weights using a new method based on the removal effects of criteria
(MEREC). Symmetry 13(4), 525 (2021)
13. Pamuˇcar, D., ´Cirovi´c, G.: The selection of transport and handling resources in logistics centers
using multi-attributive border approximation area comparison (MABAC). Expert Syst. Appl.
42(6), 3016–3028 (2015)

Author Index
A
Addawe, Rizavel C.
125
Adhikari, Sujita
196
Ahmed, Ikbal
334
Akash, Fardin Rahman
394
Akash, Rashik Shahriar
181
Al Fahim, Haﬁz
181
Al Munem, Abdullah
288
Alanas, Rostum Paolo B.
125
Alexander,
244
Ali, Mohammed Nadir Bin
379
Anisimov Alexander, A.
153
Ansari, Sarfarazali
95
Anwar, Jawed
95
Areﬁn, Mohammad Shamsul
181, 262, 288,
359, 379, 394
Arein, Mohammad Shamsul
368
Aruntippaitoon, Worapat
253
B
B.S., Goryachkin
76
Bandyopadhyay, Tarun Kanti
170
Banik, Anirban
145, 170
Banik, Nayan
334
Bekda¸s, Gebrail
350
Bhuiyan, Touhid
181, 379
Biswal, Sushant Kumar
170
Boojhawon, Ravindra
65
Buleeva, S.
217
C
Chadni, Jahani Shabnam
394
Chanta, Sunarin
14, 253
Chhetri, Shanti Devi
3, 22
Chohan, Jasgurpeet Singh
104, 414
Choppradit, Pakcheera
314
D
Daus, Yu.
163
Doctolero, Angela Ronice A.
125
Domínguez-Miranda, Sergio Arturo
134
Doomah, Mohammad Zaheer
65
E
Emu, Ismot Ara
394
G
Garhwal, Anil
145, 170
Garhwal, Sunil
145, 170
Gukhool, Oomesh
65
H
Haque, Raﬁd Mahmud
368
Higashi, Masaki
84
Hoque, Md Mahmudul
334
Hoque, Mohammed Moshiul
334
I
Ignatkin, I. Yu.
153
Imam, Omar Tawhid
288
Injamul Haque, Md.
379
Ishkin, P.
163
Islam, Efte Kharul
262
Islam, Saiful
379
J
Jabed Hosen, Md.
379
Jarin, Tanin Mohammad
181
Jeong, Seung Ryul
33
K
Kalita, Kanak
104, 414
Kazantsev Sergey, P.
153
Khan, Raashid
95
Kildeev, T. A.
207
Kildeev, T.
153
Kombarov, Volodymyr
277
Kononenko, A. S.
207
Kumar, Devesh
3
Kumar, Santosh
301
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2023
P. Vasant et al. (Eds.): ICO 2023, LNNS 855, pp. 423–425, 2024.
https://doi.org/10.1007/978-3-031-50158-6

424
Author Index
L
Libatique, Criselda P.
125
Limsakul, Tatitayakorn
404
Litvinchev, Igor
277
M
Maha, Lamyea Tasneem
368
Manshahia, Mukhdeep Singh
236
Mashkov, S.
163
Maza, Guinness G.
125
Meem, Afsana Nur
262
Mim, Faria Tabassum
359
Moin, Naﬁsha Binte
288
Moskvin, Valery
115
Munapo, Elias
301
Muradyan, S.
217
N
Nakata, Kazuhide
84
Nigdeli, Sinan Melih
350
Nikolsky, D. R.
76
Nikulina, E.
217
Nodi, Lamya Ishrat
262
Nyamugure, Philimon
301
O
Octavia, Tanti
223
P
Pagunsan, Clarence Kyle L.
125
Palit, Herry Christian
244
Panchenko, Vladimir
170
Pankratov, Oleksandr
277
Pathan, Azazkhan Ibrahimkhan
95
Phimsiri, Sasin
314
Pishchaeva, K.
217
Plankovskyy, Sergiy
277
Poudel, Sudip
33
Prabhukumar, Sellamuthu
104, 414
Priniya, Ajmiri Afrin
394
R
Rahman, Oshin Nusrat
368
Rajkarnikar, Neesha
33
Ranabhat, Deepesh
3, 22, 45, 196
Randhawa, Jasleen
236
Rebolledo, Jonathan
56
Reza, Ahmed Wasif
181, 262, 288, 359,
368, 379, 394
Rodriguez-Aguilar, Roman
56, 134
Romanova, Tetyana
277
S
Safa, Noor Fabi Shah
368
Saha, Arpita
359
said, Saif
95
Saiful, Md.
359
Sangsawang, Ornurai
14
Santoso, Leo Willyanto
342
Sapkota, Pradeep
3, 22, 45
Saproshina, A.
217
Sergeeva, N. A.
207
Serov, A. V.
153
Serov, N. V.
153
Sharma, Sandeep Kumar
145
Sharma, Shruti
145
Shestov, Dmitry
115
Shilin, Denis
115
Shrestha, Deepanjal
33
Shrestha, Deepmala
33
Shuha, Jobaida Ahmed
394
Sidek, Lariyah Mohd
95
Singh, Aditya
324
Skorokhodov, D. M.
153
Sookpong, Satida
314
Soosor, Nooswaibah Binti Nooroodeen
65
Sripathomswat, Kanokporn
253
Sukhobokov, A. A.
76
Sultana, Shamima
288
Suttichaya, Vasin
314
Syrkin, V.
163
T
Taladgaew, Sompoap
404
Tapu, Md. Abu Bakar Siddiq
181
Tasnim, Naﬁsa
359
Tawanda, Trust
301
Thamwiwatthana, Ek
314
Tipchareon, Nattawat
253
Tomal, Minhazul Amin
262
Tosawadi, Teepakorn
314
Trirattanasarana, Itiphong
253
Tsegelnyk, Yevgen
277
Tuhin, Rashedul Amin
368
Tunga, Tiovitus Flomando
223
U
Ukidve, Seema
236

Author Index
425
Utintu, Chaitat
314
Utsumi, Yoshimasa
84
V
Vashisth, Kamal Kant
45
Vasilev, S.
163
Vasiliev, Alexey
115
Verma, Narinder
22, 196
Y
Yadav, Ramsagar
236
Yu, Ignatkin I.
207
Yücel, Melda
350
Z
Zante, Kendrick Jules G.
125

