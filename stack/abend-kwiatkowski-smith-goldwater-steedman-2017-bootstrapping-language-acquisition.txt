Bootstrapping language acquisition q
Omri Abend ⇑,1, Tom Kwiatkowski 2, Nathaniel J. Smith 3, Sharon Goldwater, Mark Steedman
Informatics, University of Edinburgh, United Kingdom
a r t i c l e
i n f o
Article history:
Received 10 February 2016
Revised 1 November 2016
Accepted 22 February 2017
Parts of this work were previously
presented at EMNLP 2010, EACL 2012, and
appeared in Kwiatkowski’s doctoral
dissertation
Keywords:
Language acquisition
Syntactic bootstrapping
Semantic bootstrapping
Computational modeling
Bayesian model
Cross-situational learning
a b s t r a c t
The semantic bootstrapping hypothesis proposes that children acquire their native language through
exposure to sentences of the language paired with structured representations of their meaning, whose
component substructures can be associated with words and syntactic structures used to express these
concepts. The child’s task is then to learn a language-speciﬁc grammar and lexicon based on (probably
contextually ambiguous, possibly somewhat noisy) pairs of sentences and their meaning representations
(logical forms).
Starting from these assumptions, we develop a Bayesian probabilistic account of semantically boot-
strapped ﬁrst-language acquisition in the child, based on techniques from computational parsing and
interpretation of unrestricted text. Our learner jointly models (a) word learning: the mapping between
components of the given sentential meaning and lexical words (or phrases) of the language, and (b) syn-
tax learning: the projection of lexical elements onto sentences by universal construction-free syntactic
rules. Using an incremental learning algorithm, we apply the model to a dataset of real syntactically com-
plex child-directed utterances and (pseudo) logical forms, the latter including contextually plausible but
irrelevant distractors. Taking the Eve section of the CHILDES corpus as input, the model simulates several
well-documented phenomena from the developmental literature. In particular, the model exhibits syn-
tactic bootstrapping effects (in which previously learned constructions facilitate the learning of novel
words), sudden jumps in learning without explicit parameter setting, acceleration of word-learning
(the ‘‘vocabulary spurt”), an initial bias favoring the learning of nouns over verbs, and one-shot learning
of words and their meanings. The learner thus demonstrates how statistical learning over structured rep-
resentations can provide a uniﬁed account for these seemingly disparate phenomena.
 2017 Elsevier B.V. All rights reserved.
1. Introduction
One of the fundamental challenges facing a child language lear-
ner is the problem of generalizing beyond the input. Using various
social and other extralinguistic cues, a child may be able to work
out the meaning of particular utterances they hear, like ‘‘you read
the book” or ‘‘Eve will read Lassie”, if these are encountered in the
appropriate contexts. But merely memorizing and reproducing
earlier utterances is not enough: children must also somehow
use these experiences to learn to produce and interpret novel
utterances, like ‘‘you read Lassie” and ‘‘show me the book”. There
are many proposals for how this might be achieved, but abstractly
speaking it seems to require the ability to explicitly or implicitly (a)
decompose the utterance’s form into syntactic units, (b) decom-
pose the utterance’s meaning into semantic units, (c) learn lexical
mappings between these syntactic and semantic units, and (d)
learn the language-speciﬁc patterns that guide their recombination
(so that e.g. ‘‘Eve will read Lassie to Fraser”, ‘‘will Eve read Fraser
Lassie?”, and ‘‘will Fraser read Eve Lassie?” have different meanings,
despite using the same or nearly the same words). A further chal-
lenge is that even in child-directed speech, many sentences are
more complex than ‘‘you read Lassie”; the child’s input consists
of a mixture of high- and low-frequency words falling into a vari-
ety of syntactic categories and arranged into a variety of more or
less complex syntactic constructions.
In this work, we present a Bayesian language-learning model
focused on the acquisition of compositional syntax and semantics
in an incremental, naturalistic setting. That is, our model receives
training examples consisting of whole utterances paired with noisy
http://dx.doi.org/10.1016/j.cognition.2017.02.009
0010-0277/ 2017 Elsevier B.V. All rights reserved.
q The work was supported in part by ERC Advanced Fellowship 249520
GRAMPLUS, EU IST Cognitive Systems IP EC-FP7-270273 Xperience, ARC Discovery
grants DP 110102506 and 160102156, and a James S McDonnell Foundation Scholar
Award.
⇑Corresponding author at: School of Computer Science and Engineering, Roth-
berg Building A527, Edmond Safra Campus, Givat Ram, The Hebrew University,
Jerusalem 91904, Israel.
E-mail address: oabend@cs.huji.ac.il (O. Abend).
1 Now at the Departments of Computer Science & Cognitive Science, The Hebrew
University of Jerusalem.
2 Now at Google Research.
3 Now at the Berkeley Institute of Data Science, University of California, Berkeley.
Cognition 164 (2017) 116–143
Contents lists available at ScienceDirect
Cognition
journal homepage: www.elsevier.com/locate/COGNIT

representations of the whole utterance’s meaning, and from these
it learns probabilistic representations of the semantics and syntax
of individual words, in such a way that it becomes able to recom-
bine these words to understand novel utterances and express novel
meanings. This requires that the model simultaneously learn how
to parse syntactic constructions, assign meaning to speciﬁc words,
and use syntactic regularities (for example, in verb argument
structure) to guide interpretation of ambiguous input. Our training
data consists of real, syntactically complex child-directed utter-
ances drawn from a single child in the CHILDES corpus, and our
training is incremental in the sense that the model is presented
with each utterance exactly once, in the same order that the child
actually encountered them.
The work described here represents an advance over previous
models that focused on learning either word meanings or syntax
given the other (see below for a review). By developing a joint
learning model we are able to explore how these phenomena inter-
act during learning. A handful of other joint learning models have
been presented in the literature, but these have either worked from
synthetic input with varying degrees of realism (Beekhuizen, 2015;
Maurits, Perfors, & Navarro, 2009) or have not yet been evaluated
on speciﬁc phenomena known from child language acquisition,
as we do here (Chrupała, Kádár, & Alishahi, 2015; Jones, 2015). In
particular, we show in a series of simulations that our model exhi-
bits syntactic bootstrapping effects (in which previously learned
constructions facilitate the learning of novel words), sudden jumps
in learning without explicit parameter setting, acceleration of
word-learning (the ‘‘vocabulary spurt”), an initial bias favoring
the learning of nouns over verbs, and one-shot learning of words
and their meanings. These results suggest that there is no need
to
postulate
distinct
learning
mechanisms
to
explain
these
various phenomena; rather they can all be explained through a sin-
gle
mechanism
of
statistical
learning
over
structured
representations.
1.1. Theoretical underpinnings
Our model falls under the general umbrella of ‘‘Semantic Boot-
strapping” theory, which assumes that the child can access a struc-
tural representation of the intended semantics or conceptual
content of the utterance, and that such representations are sufﬁ-
ciently homomorphic to the syntax of the adult language for a
mapping
from
sentences
to
meanings
to
be
determined
(Bowerman, 1973; Brown, 1973; Clark, 1973; Grimshaw, 1981;
Pinker, 1979; Schlesinger, 1971; cf. Wexler & Culicover, 1980:78–
84; Berwick, 1985:22–24). By ‘‘homomorphic”, we simply mean
that meaning representation and syntax stand in a ‘‘type-to-type”
relation, according to which every syntactic type (such as the Eng-
lish intransitive verb) corresponds to a semantic type (such as the
predicate), and every rule (such as English S ! NP
VP) corre-
sponds to a semantic operation (such as function application of
the predicate to the subject).
Early accounts of semantic bootstrapping (e.g. Berwick, 1985;
Wexler & Culicover, 1980) assumed perfect access to a single
meaning representation in the form of an Aspects-style Deep Struc-
ture already aligned to the words of the language. Yet, as we shall
see, semantic bootstrapping is sufﬁciently powerful that such
strong assumptions are unnecessary.
Since, on the surface, languages differ in many ways—for exam-
ple with respect to the order of heads and complements, and in
whether such aspects of meaning as tense, causality, evidentiality,
and information structure are explicitly marked—the meaning rep-
resentations must be expressed in a universal prelinguistic concep-
tual representation, in whose terms all such distinctions are
expressable. The mapping must further be learned by general prin-
ciples that apply to all languages. These general principles are often
referred to as ‘‘universal grammar”, although the term is somewhat
misleading in the present context since the model we develop is
agnostic as to whether these principles are unique to language or
apply more generally in cognition.
A number of speciﬁc instantiations of the semantic bootstrap-
ping theory have been proposed over the years. For example, ‘‘pa-
rameter
setting”
accounts
of
language
acquisition
assume,
following Chomsky (1981), that grammars for each natural lan-
guage can be described by a ﬁnite number of ﬁnitely-valued
parameters, such as head-position, pro-drop, or polysynthesis
(Hyams, 1986 and much subsequent work). Language acquisition
then takes a form that has been likened to a game of Twenty-
Questions (Yang, 2006 Ch:7), whereby parameters can be set when
the child encounters ‘‘triggers”, or sentences that can only be ana-
lyzed under one setting of a parameter. For example, for Hyams
(1986), the fact that English has lexical expletive subjects (e.g., it
in it rained) is unequivocal evidence that the pro-drop parameter
is negative, while for others the position of the verb in simple
intransitive sentences in Welsh is evidence for head-initiality. Such
triggers are usually discussed in purely syntactic terms. However,
in both examples, the child needs to know which of the words is
the verb, which requires a prior stage of semantic bootstrapping
at the level of the lexicon (Hyams, 1986:132–133).
Unfortunately, parameter setting seems to raise as many ques-
tions as it answers. First, there are a number of uncertainties con-
cerning the way the learner initially identiﬁes the syntactic
categories of the words, the speciﬁc inventory of parameters that
are needed, and the aspects of the data that ‘‘trigger” their setting
(Fodor, 1998; Gibson & Wexler, 1994; Niyogi & Berwick, 1996).
Second, several combinatoric problems arise from simplistic search
strategies in this parameter space (Fodor & Sakas, 2005). Here, we
will demonstrate that step-like learning curves used to argue for
parameter-setting approaches (Thornton & Tesan, 2007) can be
explained
by
a
statistical
model
without
explicit
linguistic
parameters.
A further variant of the semantic bootstrapping theory to be
discussed below postulates a second, later, stage of ‘‘syntactic
bootstrapping”
(Braine,
1992;
Gleitman,
1990;
Landau
&
Gleitman, 1985; Trueswell & Gleitman, 2007), during which the
existence of early semantically bootstrapped syntax allows rapid
or even ‘‘one-shot” learning of lexical items, including ones for
which the situation of utterance offers little or no direct evidence.
Early discussions of syntactic bootstrapping implied that it is a
learning mechanism in its own right, distinct from semantic boot-
strapping. However, we will demonstrate that these effects attrib-
uted to syntactic bootstrapping emerge naturally under the theory
presented here. That is, our learner exhibits syntactic bootstrap-
ping effects (using syntax to accelerate word learning) without
the need for a distinct mechanism: the mechanism of semantic
bootstrapping is sufﬁcient to engender the effects.
Although varieties of semantic bootstrapping carry considerable
currency, some researchers have pursued an alternative distribu-
tional approach (Redington, Chater, & Finch, 1998), which assumes
that grammatical structure can be inferred from statistical proper-
ties of strings alone. Many proponents of this approach invoke Arti-
ﬁcial
Neural
Network
(ANN)
computational
models
as
an
explanation for how this could be done—see Elman et al. (1996)
for examples—while others in both cognitive science and computer
science have proposed methods using structured probabilistic
models (Cohn, Blunsom, & Goldwater, 2010; Klein & Manning,
2004; Perfors, Tenenbaum, & Regier, 2011). The distributional
approach is appealing to some because it avoids the assumption
that the child can access meanings expressed in a language of mind
that is homomorphic to spoken language in the sense deﬁned
above, but inaccessible to adult introspection and whose detailed
character is otherwise unknown.
O. Abend et al. / Cognition 164 (2017) 116–143
117

There has been some success in using this kind of meaning-free
approach to learn non-syntactic structure such as word- and
syllable-level boundaries (Goldwater, Grifﬁths, & Johnson, 2009;
Johnson & Goldwater, 2009; Phillips & Pearl, 2014). However,
attempts to infer syntactic structures such as dependency or con-
stituency structure, and even syntactic categories, have been nota-
bly less successful despite considerable effort (e.g. Abend, Reichart,
& Rappoport, 2010; Christodoulopoulos, Goldwater, & Steedman,
2010; Cohn et al., 2010; Klein & Manning, 2004, 2005). Within
the context of state-of-the-art natural language processing (NLP)
applications, ANN models that have no explicit structured repre-
sentations have yielded excellent language modeling performance
(i.e., prediction of probable vs. improbable word sequences) (e.g.,
Mikolov,
Karaﬁát,
Burget,
Cernocky` ,
&
Khudanpur,
2010;
Sundermeyer, Schlüter, & Ney, 2012). They have also been used
to learn distributed word representations that capture some
important semantic and syntactic information (Mikolov, Yih, &
Zweig, 2013). Yet the reason these models work as well as they
do in NLP tasks arises from the way they mix sentence-internal
syntax and semantics with pragmatics and frequency of colloca-
tion. Thus, they often learn to conﬂate antonyms as well as syn-
onyms to similar representations (Turney & Pantel, 2010). This
representation creates problems for the compositional semantics
of logical operators (such as negation) of a kind that the child never
exhibits.
1.2. Overview of the learner
In this work, we take a close relation between syntax and com-
positional semantics as a given. We also follow the basic premise of
semantic bootstrapping that the learner is able to infer the mean-
ing of at least some of the language she hears on the basis of non-
linguistic
context.
However,
unlike
some
other
versions
of
semantic bootstrapping, we assume that the available meanings
are at the level of utterances rather than individual words, and that
word meanings (i.e., the mapping from words to parts of the utter-
ance meaning) are learned from such data.
We represent the meaning of an utterance as a sentential logical
form. Thus, the aim of the learner is to generalize from the input
pairs of an observed sentence and a possible meaning in order to
interpret new sentences whose meaning is unavailable contextu-
ally, and to generate new sentences that express an intended
meaning. Because of the limitations of available corpus annota-
tions, the logical forms we consider are restricted to predicate-
argument relations, and lack the interpersonal content whose
importance in language acquisition is generally recognized. We
examine the nature of such content in Section 4.4, where we argue
that our model can be expected to generalize to more realistic
meaning representations.
Recent work suggests that the infant’s physically limited view
of the world, combined with social, gestural, prosodic, and other
cues, may lead to considerably less ambiguity in inferring utter-
ance meanings than has previously been supposed (Yu & Smith,
2012, 2013; Yurovsky, Smith, & Yu, 2013). Nevertheless, most con-
texts of utterance are likely to support more than one possible
meaning, so it is likely that the child will have to cope with a num-
ber of distracting spurious meaning candidates. We model propo-
sitional ambiguity in the input available to the learner by
assuming each input utterance s is paired with several contextually
plausible logical forms fm1; . . . ; mkg, of which only one is correct
and the rest serve as distractors. While these meaning representa-
tions are assumed to reﬂect an internal language of mind, our
model is general enough to allow the inclusion of various types
of content in the logical forms, including social, information-
structural, and perceptual.
Within this general framework, we develop a statistical learner
that jointly models both (a) the mapping between components of
the given sentential meaning and words (or phrases) of the lan-
guage, and (b) the projection of lexical elements onto constituents
and sentences by syntactic rules. In earlier work, we deﬁned the
learner and gave preliminary simulation results (Kwiatkowski,
Goldwater, Zettlemoyer, & Steedman, 2012); here we expand con-
siderably on the description of the learner, the range of simula-
tions,
and
the
discussion
in
relation
to
human
language
acquisition.
There has been considerable previous work by others on both
word learning and syntactic acquisition, but they have until very
recently
been
treated
separately.
Thus,
models
of
cross-
situational word learning have generally focused on learning either
word-meaning mappings (mainly object referents) in the absence
of syntax (Alishahi, Fazly, & Stevenson, 2008; Frank, Goodman, &
Tenenbaum,
2009;
McMurray,
Horst,
&
Samuelson,
2012;
Plunkett, Sinha, Møller, & Strandsby, 1992; Regier, 2005; Siskind,
1996; Yu & Ballard, 2007), or learning verb-argument structures
assuming nouns and/or syntax are known (Alishahi & Stevenson,
2008, 2010; Barak, Fazly, & Stevenson, 2013; Beekhuizen, Bod,
Fazly,
Stevenson,
&
Verhagen,
2014;
Chang,
2008;
Morris,
Cottrell, & Elman, 2000; Niyogi, 2002).4 Conversely, most models
of syntactic acquisition have considered learning from meaning-
free word sequences alone (see discussion above), or have treated
word-meaning mapping and syntactic learning as distinct stages of
learning, with word meanings learned ﬁrst followed by syntax
(Buttery, 2006; Dominey & Boucher, 2005; Villavicencio, 2002).
Several previous researchers have demonstrated that correct
(adult-like) knowledge of syntax (e.g., known part-of-speech cate-
gories or syntactic parses) can help with word-learning (Mellish,
1989; Göksun, Küntay, & Naigles, 2008; Ural, Yuret, Ketrez,
Koçbas, & Küntay, 2009; Fazly, Alishahi, & Stevenson, 2010;
Thomforde & Steedman, 2011; Yu & Siskind, 2013), and a few
(Alishahi & Chrupała, 2012; Yu, 2006) have gone further in show-
ing that learned (and therefore imperfect) knowledge of POS cate-
gories can help with word learning. However, these models are not
truly joint learners, since the learned semantics does not feed back
into further reﬁnement of POS categories.
By treating word learning and syntactic acquisition jointly, our
proposal provides a working model of how these two aspects of
language can be learned simultaneously in a mutually reinforcing
way. And unlike models such as those of Maurits et al. (2009)
and Beekhuizen (2015), our model learns from real corpus data,
meaning it needs to handle variable-length sentences and predi-
cates with differing numbers of arguments, as well as phenomena
such as multiple predicates per sentence (including hierarchical
relationships, e.g., want to go) and logical operators, such as nega-
tion and conjunction. To tackle this challenging scenario, we adopt
techniques originally developed for the task of ‘‘semantic parsing”
(more properly, semantic parser induction) in computational lin-
guistics
(Kwiatkowski,
Zettlemoyer,
Goldwater,
&
Steedman,
2010; Kwiatkowski et al., 2011; Thompson & Mooney, 2003;
Zettlemoyer & Collins, 2005, 2007).
Our model rests on two key features which we believe to be
critical to early syntactic and semantic acquisition in children.
The ﬁrst, shared by most of the models above, is statistical
learning. Our model uses a probabilistic grammar and lexicon
whose model parameters are updated using an incremental
learning algorithm. (By parameters here, we mean the probabil-
4 Most of these verb-learning models actually model argument structure general-
ization (e.g., predicting that if a verb has been seen in a double object construction, it
might also be acceptable in a dative construction). We do not address this type of
generalization directly in the model presented here. However, Kwiatkowski,
Zettlemoyer, Goldwater, and Steedman (2011) have developed lexical generalization
methods for similar models that could begin to address this problem.
118
O. Abend et al. / Cognition 164 (2017) 116–143

ities in the model; our model does not include linguistic param-
eters in the sense noted earlier of Chomsky (1981) and Hyams
(1986).) This statistical framework allows the model to take
advantage of incomplete information while being robust to
noise. Although some statistical learners have been criticized
for showing learning curves that are too gradual—unlike the
sudden
jumps
in
performance
sometimes
seen
in
children
(Thornton & Tesan, 2007)—we show that our model does not
suffer from this problem.
The second key feature of our model is its use of syntactically
guided semantic compositionality. This concept lies at the heart
of most linguistic theories, yet has rarely featured in previous com-
putational models of acquisition. As noted above, many models
have focused either on syntax or semantics alone, with another
large group considering the syntax-semantics interface only as it
applies to verb learning. Of those models that have considered both
syntax and semantics for full sentences, many have assumed that
the meaning of a sentence is simply the set of meanings of the
words in that sentence (Alishahi & Chrupała, 2012; Allen &
Seidenberg, 1999; Fazly et al., 2010; Yu, 2006). Connor, Fisher,
and Roth (2012) addressed the acquisition of shallow semantic
structures (predicate-argument structures and their semantic
roles) from sentential meaning representations consisting of the
set of semantic roles evoked by the sentence. Villavicencio
(2002) and Buttery (2006) make similar assumptions to our own
about semantic representation and composition, but also assume
a separate stage of word learning prior to syntactic learning, with
no ﬂow of information from syntax back to word learning as in
our joint model. Thus, their models are unable to capture syntactic
bootstrapping effects. Chrupała et al. (2015) have a joint learning
model, but no explicit syntactic or semantic structure. The model
most similar to our own in this respect is that of Jones (2015),
but as noted above, the simulations in that work are more limited
than those we include here.
Much of the power of our model comes from this assumption
that syntactic and semantic composition are closely coupled. To
implement this assumption, we have based our model on Combi-
natory Categorial Grammar (CCG, Steedman, 1996b, 2000, 2012).
CCG has been extensively applied to parsing and interpretation
of unrestricted text (Auli & Lopez, 2011; Clark & Curran, 2004;
Hockenmaier, 2003; Lewis & Steedman, 2014), and has received
considerable attention recently in the computational literature
on semantic parser induction (e.g., Artzi, Das, & Petrov, 2014;
Krishnamurthy & Mitchell, 2014; Kwiatkowski et al., 2010, 2011;
Matuszek, Fitzgerald, Zettlemoyer, Bo, & Fox, 2012; Zettlemoyer
& Collins, 2005, 2007).
This attention stems from two essential properties of CCG.
First,
unlike
Lexicalized
Tree-Adjoining
Grammar
(Joshi
&
Schabes, 1997), Generalized Phrase Structure Grammar (Gazdar,
Klein, Pullum, & Sag, 1985), and Head-driven PSG (Pollard &
Sag, 1994)—but like LFG (Bresnan, 1982) and the Minimalist pro-
gram (Chomsky, 1995)—a single nondisjunctive lexical entry
governs both in situ and extracted arguments of the verb. Second,
the latter kind of dependencies are established without the over-
head for the learner of empty categories and functional uncer-
tainty
or
movement,
of
the
kind
used
in
LFG
and
the
Minimalist Program.
These properties of CCG, together with its low near-context-free
expressive power and simplicity of expression, have made it attrac-
tive both for semantic parsing, and for our purposes here.
Nevertheless, the presented framework can in principle be imple-
mented with any compositional grammar formalism as long as
(1) it allows for the effective enumeration of all possible syntac-
tic/semantic derivations given a sentence paired with its meaning
representation; and (2) it is associated with a probabilistic model
that decomposes over these derivations.
Using our incremental learning algorithm, the learner is trained
on utterances from the Eve corpus (Brown, 1973; Brown & Bellugi,
1964) in the CHILDES database (MacWhinney, 2000), with mean-
ing representations produced automatically from an existing
dependency
annotation
of
the
corpus
(Sagae,
Davis,
Lavie,
MacWhinney, & Wintner, 2010). We use these automatically pro-
duced meaning representations as a proxy for the child’s actual
meaning representation in the hidden conceptual language of
mind. (Crucially, our model entirely ignores the alignment in these
data between logical constants and English words, as if the sen-
tences were in a completely unknown language.)
We evaluate our model in several ways. First, we test the lear-
ner’s ability to correctly produce the meaning representations of
sentences in the ﬁnal Eve session (not included in the training
data). This is a very harsh evaluation, since the learner is trained
on only a small sample of the data the actual child Eve was exposed
to by the relevant date. Nevertheless, we show a consistent
increase in performance throughout learning and robustness to
the presence of distractor meaning representations during training.
Next, we perform simulations showing that a number of disparate
phenomena from the language acquisition literature fall out natu-
rally from our approach. These phenomena include sudden jumps
in learning without explicit parameter setting; acceleration of
word-learning (the ‘‘vocabulary spurt”); an initial bias favoring
the learning of nouns over verbs; and one-shot learning of words
and their meanings, including simulations of several experiments
previously used to illustrate syntactic bootstrapping effects. The
success of the model in replicating these ﬁndings argues that sep-
arate accounts of semantic and syntactic bootstrapping are unnec-
essary; rather, a joint learner employing statistical learning over
structured representations provides a single uniﬁed account of
both.
2. Semantic bootstrapping for grammar acquisition
The premise of this work is that the child at the onset of
language
acquisition
enjoys
direct
access
to
some
form
of
pre-linguistic conceptual representations. We are not committed
to
these
representations
taking
any
particular
symbolic
or
non-symbolic form, but for the compositional learning process to
proceed, there must be some kind of structure in which complex
concepts can be decomposed into more primitive concepts, and
here we will abstractly represent this conceptual compositionality
using a logical language. Such universal semantics or conceptual
structure, broadly construed, has often been argued to be the most
plausible source for the universal learning biases that enable
language acquisition (Chomsky, 1965:27–30; Crain & Nakayama,
1987;
Chomsky,
1995:54–55;
Pinker,
1979;
Croft,
2001;
Ambridge, Pine, & Lieven, 2014). The child’s task is therefore to
consider all the different ways that natural language allows chunks
of meaning representation to be associated with lexical categories
for the language they are presented with.
Concretely, we address the following learning problem. Given a
corpus of transcribed utterances, paired with representations of
their meaning (henceforth, their logical forms), the goal of learning
is to induce a mapping between utterances (or transcribed text) to
meaning representations, so as to support the correct interpreta-
tion of unseen text, namely by assigning it correct logical forms.
The model represents two types of information: a (probabilis-
tic) pairing between lexical items and their meaning representa-
tion, and a distribution over the syntactic derivations the learner
has been exposed to (a probabilistic grammar). The sensitivity of
the infant learning language to statistical trends observed in the
data has been demonstrated in multiple levels of linguistic repre-
sentation (Gómez & Maye, 2005; Mintz, 2003; Saffran, Aslin, &
O. Abend et al. / Cognition 164 (2017) 116–143
119

Newport, 1996 inter alia) and is a key factor in this account of
semantic bootstrapping.
Much of the difﬁculty in this setting stems from not being able
to directly observe the meaning of individual words and the
derivation trees, as the only supervision provided is of sentential
meaning representations. In order to infer the probabilistic gram-
mar and word meanings, the model entertains all possible deriva-
tions and word-meaning pairings, and weighs these based on their
likelihood according to its hitherto acquired beliefs. This weighted
space of derivations is subsequently used to update the beliefs of
the learner when exposed to the next utterance.
2.1. Meaning representations
Meanings of sentences in CCG (and in our model) are expressed
in ﬁrst-order predicate logic. The example
(1) a. ‘‘you like the doggies!”
b. like(you, doggies)
expresses a relation like between the entity you and the entity
doggies.
Of course, we do not believe that the child’s conceptual repre-
sentation is as simple as this. As Tomasello (1999) has pointed
out, the content that the child is actually working with and that
actually provides it with the incentive to engage with language is
highly interpersonal and social—probably more like something
paraphrasable as ‘‘Mum is sharing my liking the doggies”. Never-
theless, our model needs to represent the child’s semantics some-
how. For simplicity, we’ll use terms of a naive English-centric
lambda-calculus, and defer the question of what a more psycholog-
ically realistic human logical language might actually look like
until Section 4.4.
The lambda-calculus uses the k-operator to deﬁne functions.
These may be used to represent functional meanings of utterances
but they may also be used as a ‘‘glue language” to compose ele-
ments of ﬁrst order logical expressions like the above. For example,
the function kxky.like(y, x) can be combined with the argument
doggies to give the phrasal meaning ky.like(y, doggies) (correspond-
ing to the VP ‘‘like the doggies”) via the lambda-calculus operation
of function application. In function application, the formal argument
x (introduced by the lambda notation) is replaced by doggies in the
remainder of the expression.
Our model uses typed k-expressions, where each variable and
constant is given a semantic type. These types may either be
atomic—such as e for entities (e.g., you, doggies) and t for truth val-
ues—or they may be complex—for instance, sleep is a function from
entities to a truth value, of type (e, t), and like is a (Curried) function
from pairs of entities to a truth value, of type (e, (e, t)). Lambda-
calculus operations such as function application and composition
are only allowed between forms of compatible types. For brevity,
we will often not state semantic types explicitly.
Despite the shortcomings of such an approach to semantic
representation, the logical forms we use do meet the minimal cri-
teria of (1) not encoding word order information and (2) not
encoding information about the correct segmentation of the
utterance’s semantic forms above the level of atomic semantic
symbols. For example, the logical form like(you, doggies) does
not determine the number of lexical items that will give rise to
this composite representation. The space of possible derivations
that can give rise to like(you, doggies) includes the possibilities
that there are four words in the sentence, one matching like,
one matching you and one matching doggies, but also the possi-
bility that the meaning is partitioned into other numbers of
words, including the possibility that the whole meaning corre-
sponds to a single word.
2.2. The grammar
Combinatory Categorial Grammar (CCG) is a strongly lexicalised
linguistic formalism that tightly couples syntactic types and corre-
sponding semantic k-terms. Each CCG lexical item in the lexicon L
is a triplet, written
word ‘ syntactic category : semantic form:
Examples include5:
you ‘ NP : you
sleep ‘ SnNP : kx:sleepðxÞ
like ‘ ðSnNPÞ=NP : kxky:likeðy; xÞ
the ‘ NP=N : kf:theðx; fðxÞÞ
doggies ‘ N : kx:dogsðxÞ
Syntactic categories may be atomic (e.g., S or NP) or complex
(e.g., SnNP or ðSnNPÞ=NP). Slash operators in complex categories
deﬁne functions from the domain on the right of the slash to the
result on the left in much the same way as lambda operators do
in the corresponding semantic forms. The difference is that the
direction of the slash operator deﬁnes the linear order of function
and argument in the sentence. For example, S=NP corresponds to a
constituent that along with an NP to its right forms a constituent
with the category S, while SnNP is similar but requires an NP to
its left to form an S. For example, in the English sentence ‘‘you
sleep”, the verb ‘‘sleep” has the latter category, SnNP. In a verb-
initial language like Welsh, the corresponding intransitive verb
meaning ‘‘sleep” has the former category, S=NP.
CCG uses a small set of combinatory rules to concurrently com-
bine constituents according to their syntactic types and compose
their semantic representations. Two simple combinatory rules
are forward (>) and backward (<) application:
X=Y : f
Y : g
)
X : fðgÞ
ð>Þ
Y : g
XnY : f
)
X : fðgÞ
ð<Þ
Given the lexicon above, the phrase ‘‘you like the doggies” can
be parsed using these rules as follows, where each step in the parse
is labeled with the combinatory rule (> or <) as in Fig. 1a.
It is standard to write CCG parse trees as in Fig. 1a, with leaves
at the top and combinatory operations written next to constituent
demarcation lines. However, the same derivation could also
be written in phrase-structure form as in Fig. 1b (semantics
omitted).
A bit more needs to be said about transitive verbs. Unlike unary
intransitive predicates and the determiner categories, transitive
verbs in English, such as ‘‘like” in the example above, could in prin-
ciple be associated with either of the two syntactic categories in
(2), both of which support a derivation of the ﬁnal logical form.
(2) a. like ‘ (SnNP)/NP: kxky.like(y, x)
b. like ‘ (S/NP)nNP: kykx.like(y, x)
However, we will assume here that the universally permitted
set of transitive categories excludes cases of verb-medial cate-
gories (SVO or OVS) where, as in 2(b), the verb attaches ﬁrst to
the semantic subject.6 The learner therefore considers only the fol-
lowing six possible categories/constituent orders for transitive verbs,
only one of which will work for English7:
5 Determiner semantics is represented as generalized quantiﬁers. Namely,
the(x,f(x)) should be read as the generalized quantiﬁer the over x, applied to f(x).
6 This assumption rules out verb-medial ergative languages but can be circum-
vented by subcategorizing the subject and object NPs based on their case.
7 We assume, following Baldridge (2002), that free word-order languages simply
have more than one of these categories (which may of course be schematized).
120
O. Abend et al. / Cognition 164 (2017) 116–143

(3) a. SOV ‘ (SnNP)nNP: kykx.like(x, y)
b. SVO ‘ (SnNP)/NP: kykx.like(x, y)
c. VSO ‘ (S/NP)/NP: kxky.like(x, y)
d. VOS ‘ (S/NP)/NP: kykx.like(x, y)
e. OVS ‘ (S/NP)nNP: kykx.like(x, y)
f. OSV ‘ (SnNP)nNP: kxky.like(x, y)
Finally, we note that our model also includes CCG combinatory
rules of forward (> B) and backward (< B) composition:
X=Y : f
Y=Z : g
)
X=Z : kx:fðgðxÞÞ
ð> BÞ
YnZ : g
XnY : f
)
XnZ : kx:fðgðxÞÞ
ð< BÞ
These rules are crucial to the CCG analysis of unbounded
dependency-inducing
constructions
such
as
relativization,
to
which we return in the discussion of later learning in Section 4.4.
2.3. The probabilistic model
The goal of the presented model is to learn a form-meaning
mapping that can support the correct interpretation of unseen
text. We assume that this mapping involves a latent syntactic
structure that mediates between phonological/orthographic and
semantic form. Given the uncertainty involved in grammar induc-
tion (as a ﬁnite number of examples does not uniquely determine
a CCG grammar, or any other plausible class of grammars), we
formulate the problem of acquisition as one of probabilistic
inference.
There are two ways to construe the use of probabilistic
modeling. In one, probabilities serve to make rational infer-
ences in an uncertain environment. In this case, the target
grammar can be formulated without the use of probabilities,
while the probabilistic model represents the ‘‘belief” of the
model in each of the possible non-probabilistic hypotheses.
Alternatively, the target grammar itself is assumed to be prob-
abilistic, either because it captures information that is proba-
bilistic in nature or because the model does not have access
to some of the information that would be needed to disam-
biguate (for example, semantic distinctions not represented in
the logical forms we use). Our model and results are consis-
tent with either interpretation; we will return to discuss this
point in Section 4.
We take a Bayesian inference approach and explore learning
through an idealized rational learner that induces the grammar
most supported by the observed data given the assumptions made
by the model. We depart from this idealized setting only in assum-
ing that learning is done incrementally, so that the learner has no
access to previously observed examples beyond what has already
been learned from them (i.e., the updated parameters of the statis-
tical model). Using an incremental algorithm makes it easy to
examine how the learner’s grammar and behavior develop over
time as examples are presented, and is of course more psycholog-
ically plausible than a batch algorithm.
As discussed above, our model takes as input a sequence of
sentences, each paired with one or more possible meanings.
For the moment, assume each sentence s is paired with a sin-
gle meaning m. The learner also maintains a probability distri-
bution over model parameters H. These parameters specify the
probabilities of using or combining lexical items into phrases
according to different rules, and can be thought of as the lear-
ner’s grammar (including the lexicon, as CCG is a lexicalized
formalism). The distribution over H thus represents the lear-
ner’s belief about which grammars are most plausible given
the input so far.
Before observing a particular ðs; mÞ pair, the learner has some
prior probability distribution over the grammar, PðHÞ. At the
beginning of the learning process, this prior will be entirely ﬂat,
and will assign equal probability to symmetric options (e.g.,
verb-ﬁnal vs. verb-initial) as the learner doesn’t know anything
speciﬁc about the language being learned.8 However, the prior
will be updated with each new example. Upon observing an
ðs; mÞ pair, the learner infers the probabilities of all possible syn-
tactic derivation trees t that could provide a derivation of the
ðs; mÞ pair, along with a posterior distribution over H. That is,
the
learner
infers
a
distribution
Pðt; Hjs; mÞ,
which
can
be
expressed using Bayes’ rule as:
Pðt; H j s; mÞ ¼ Pðs; m; t j HÞPðHÞ
Pðs; mÞ
ð1Þ
As learning progresses, the prior for each new example is in fact
the posterior distribution learned after observing the previous
example, so over time the prior will become much more peaked,
assigning high probability to lexical items and rules used fre-
quently in the previously inferred derivations.
For the model to be complete, we also need to specify how to
compute the likelihood Pðs; m; t jHÞ, which determines how proba-
ble different derivations are under a particular grammar. (For any
ðs; mÞ pair, Pðs; mÞ is a constant that simply normalizes the poste-
rior distribution to sum to 1, so we don’t need to deﬁne it
explicitly.)
As is usual in Bayesian models, we will deﬁne the likelihood
using a generative formulation, i.e., we will deﬁne a probabilistic
process that describes how ðs; m; tÞ triples (derivations) are
assumed to be generated given a particular set of parameters H.
This process will consist of multiple steps; by multiplying together
the probabilities at each step we can determine the probability of
the ﬁnal complete derivation Pðs; m; t j HÞ. The learner that we pre-
sent in Section 2.4 incrementally changes the parameters H to
(a)
(b)
Fig. 1. Parse trees for the sentence ‘‘you like the doggies” using the grammar in
Section 2.2. (a) using standard CCG notation. (b) using phrase-structure notation,
with semantics omitted.
8 There is nothing in our model to exclude asymmetric priors, such as those
proposed by Culbertson, Smolensky, and Wilson (2013). We assume a ﬂat initial prior
as it is the most conservative option.
O. Abend et al. / Cognition 164 (2017) 116–143
121

increase the probabilities of derivations that support the ðs; mÞ
pairs seen during training.9
Consider now a particular ðs; m; tÞ derivation, such as the one
given in Fig. 1 for ‘‘you like the doggies”: like(you, the(x, dogs(x))).
The probability of such a derivation will have three parts. First,
the probability of all of the function applications needed to decom-
pose each parent node into each child combination of nodes. These
are essentially the rules of the syntax. Second, the probability with
which each of the resulting leaf syntactic category child nodes (e.g.,
NP or NP=N) is assigned a given meaning (e.g., you or kf.the(x, f(x))).
Third, the probability with which each meaning corresponds to a
word in the sentence (e.g., ‘you’ or ‘the’).
Formally,
let
w1; . . . ; wn
be
the
words
in
the
sentence
s, m1; . . . ; mn be their meaning representations (i.e., the meanings
of each leaf in the derivation tree), and c1; . . . ; cn be their syntactic
categories. The derivation is generated as follows:
1. Generate each syntactic rule in the derivation, with probability
Pðc j aÞ for a rule a ! c with parent category a and child cate-
gories c. Each parent category X either generates a pair of child
categories through inverse application or composition (e.g., X
may generate X=Y and Y; X=Z may generate X=Y and Y=Z; See
Appendix B for the formal deﬁnition of this procedure), or gen-
erates a leaf by generating the symbol XLEX. We assume all
derivations start with the START symbol, but for brevity omit
this and the XLEX categories from the example derivations
shown. The probability of this step is given as:
PSYNTAXðtÞ ¼
Y
a!c2t
Pðc j aÞ
ð2Þ
2. For each leaf syntactic category ci in c1; . . . ; cn, generate the cor-
responding
meaning
representation
mi
with
probability
Pðmi j ciÞ. The probability of this step is given as:
PMEANINGSðmjtÞ ¼
Y
n
i¼1
Pðmi j ciÞ
ð3Þ
3. For each mi, generate the corresponding word wi with probabil-
ity Pðwi j miÞ. The probability of this step is given as:
PWORDSðsjmÞ ¼
Y
n
i¼1
Pðwi j miÞ
ð4Þ
Putting these steps together, we get the overall probability of a
derivation:
Pðs; m; tÞ ¼ PSYNTAXðtÞ  PMEANINGSðmjtÞ  PWORDSðsjmÞ
ð5Þ
For instance, the probability of the derivation of ‘‘you like the
doggies” given in Fig. 1 is the product of the following three terms:
PSYNTAXðtÞ ¼ PðS j STARTÞ  PðNP;SnNPjSÞ  PðSnNP=NP;SnNPjSnNPÞ
PðNP=N;NjNPÞ  PðNPLEX j NPÞ
Pð½SnNP=NPLEX j SnNP=NPÞ
Pð½NP=NLEX j NP=NÞ  PðNLEX j NÞ
PMEANINGSðmjtÞ ¼ Pðyou j NPLEXÞ  Pðkxky:likeðy;xÞ j ½SnNP=NPLEXÞ
Pðkf:theðx;fðxÞÞ j ½NP=NLEXÞ  Pðkx:dogsðxÞ j NLEXÞ
PWORDSðsjmÞ ¼ Pðyou j youÞ  Pðlike j kxky:likeðy;xÞÞ
Pðthe j kf:theðx;fðxÞÞÞ  Pðdogs j kx:dogsðxÞÞ
This model is essentially a probabilistic context-free grammar
(PCFG). While naive PCFGs impose conditional independence
assumptions that are too strong to characterize natural language,
they can be augmented to overcome this problem (Charniak,
1997; Collins, 1997). PCFGs are a simple and well-understood class
of models with standard parsing and learning algorithms, and are
powerful enough to do well in prediction of both phrase-
structure
trees
(Collins,
1997)
and
CCG
derivation
trees
(Hockenmaier & Steedman, 2002) in naturalistic settings. Future
work will explore the use of richer families of distributions.
In order to complete the model, we still need to deﬁne how the
distributions used in Eqs. (2)–(4) are calculated: Pðc j aÞ; Pðm j cÞ,
and Pðw j mÞ. First, consider Pðc j aÞ, the distribution over possible
expansions c given a syntactic production head a. Notice that this
distribution needs to be deﬁned over an inﬁnite set of possible
expansions, since the learner does not know ahead of time which
expansions will be needed for their language. Recent probabilistic
models of language acquisition (Alishahi & Stevenson, 2008, 2010;
Feldman, Grifﬁths, Goldwater, & Morgan, 2013; Frank, Feldman, &
Goldwater, 2014; Goldwater et al., 2009), have demonstrated that
Dirichlet Processes can be used to successfully model such inﬁnite
distributions, so we adopt that approach here. When applied to our
problem (see Appendix A for details), the Dirichlet Process deﬁnes
the probability of a particular expansion given the production head
as
Pðc j aÞ ¼ na!c þ asynHa
na þ asyn
ð6Þ
where na!c is the number of times the rule a ! c has been used pre-
viously in a derivation, na is the number of times any rule expanding
a has been used, and asyn is a ﬁxed parameter of the model which
determines the learner’s tendency to infer derivations containing
(any) new rule that has not been used previously. Ha, another ﬁxed
parameter of the model, is a prior distribution over rules: of all the
rules headed by a that have not been used before, which ones are
more likely to be used?
Notice that under this deﬁnition, the more a rule has been used
relative to other expansions of the same parent, the higher its
probability will be. Over time, rules with higher probability will
be used more, incurring higher counts, which raises their probabil-
ity even more: a virtuous cycle of learning. However, even rules
that have never been used before (with na!c ¼ 0) have some
non-zero probability associated with them, as determined by the
values of asyn and Ha. We deﬁne Ha without any particular bias
towards any of the CCG operators, or any of the atomic syntactic
categories. However, we do include a bias towards simpler syntac-
tic categories (e.g., expanding an NP into ðNP=N; NÞ would be a pri-
ori more likely than expanding it into ðNP=ðN=NÞ; N=NÞ).
The remaining two distributions Pðm j cÞ (the distribution over
leaf meaning representations given the syntactic category) and
Pðw j mÞ (the distribution over wordforms given a meaning repre-
sentation m) are also deﬁned using Dirichlet Processes. Again,
these provide well-deﬁned distributions over the inﬁnite sets of
possible meaning representations and wordforms, assigning higher
probability to meanings and words that have been seen more fre-
quently, but non-zero probability to novel meanings and words.
The prior distribution over unseen meanings prefers simpler logi-
cal expressions (those containing fewer logical constants), and
the prior distribution over unseen words prefers shorter words.
See Appendix A for details.
2.4. The learning algorithm
The previous subsections described the input, internal repre-
sentations, and probabilistic model used by our learner. This
9 Since the syntactic tree t for any particular ðs; mÞ pair is never directly observed,
the likelihood term favors values of H that assign high probability to the set of possible
derivations tðs;mÞ of the ðs; mÞ pair in question—Appendix B describes how the space of
all possible derivations is deﬁned. When multiple ðs; mÞ pairs are observed, the
likelihood favors H that assign high probability to the set of all derivations of all
observed pairs. This goal can be achieved by assigning high probability to derivations
with subparts that can be used to explain many different ðs; mÞ pairs. Assigning high
probability to an idiosyncratic parse could raise the probability of tðs;mÞ for a particular
ðs; mÞ pair, but will steal probability mass from other parses that are more generally
applicable, thus making the possible derivations for all other ðs; mÞ pairs less likely.
122
O. Abend et al. / Cognition 164 (2017) 116–143

subsection explains how the learner processes the input data in
order to determine the possible derivations of each (sentence,
meaning) pair and update its knowledge of the parameters H on
the basis of the input and its derivations.
At a high level, the learning algorithm is straightforward. The
corpus contains sentences s1 . . . sN, and we assume for the moment
that each si is paired with a single unambiguous meaning represen-
tation mi. The learner processes the corpus incrementally, at each
step considering the pair ðsi; miÞ and computing the set of possible
derivations ti for this pair. Roughly speaking, a single derivation
can be found by recursively splitting si and mi into smaller and
smaller chunks that, when recombined using the CCG application
and composition rules, will yield the original input pair. By consid-
ering all possible splits at each step in this process, all possible
derivations can be found and their probabilities computed based
on the learner’s current beliefs about H and the probabilistic model
described in Section 2.3.10 Details of the splitting process, including
how the possible syntactic categories are determined for each chunk,
can be found in Appendix B.
Next, the learner updates its beliefs about H. Each derivation in
ti will include binary (combinatory) rules as well as unary rules
that generate the leaf syntactic categories, meaning representa-
tions, and wordforms. The learner updates its probabilities for
any rules used in ti, weighting the number of times a rule is used
by the probability of the derivation(s) in which it is used. In other
words, the learner will increase the probability of a rule more if it is
used frequently, or in a derivation that has high probability accord-
ing to the current model parameters. It is also important to account
for the fact that early on, the learner’s prior is based on little evi-
dence, whereas later it is based on much more evidence. We there-
fore assume that early in learning the learner makes larger
updates, weighting the evidence from the current example more
heavily relative to the model’s previously learned prior. The rate
at which learning drops off is controlled by an additional ﬁxed
parameter, the learning rate. A more gradual change in the learning
rate means the learner will continue to make large updates for
longer, effectively forgetting more about early examples and taking
longer to converge. Details of the update procedure can be found in
Appendix C.11
The only change needed in order to handle the more realistic
condition where the child/learner has to consider more than one
possible meaning for each sentence is that the set of derivations
considered is the union of all possible derivations for the current
sentence paired with each possible meaning. In this scenario, rules
that are used in derivations for the incorrect meaning may gain
some probability mass, especially early on when the learner has lit-
tle evidence of the correct rules. However, they will quickly be
swamped by the correct rules, for the same reason that rules used
in incorrect derivations of the correct meaning are. That is, the cor-
rect rules apply consistently (and therefore frequently) across a
wide range of sentences, leading to increasingly high probabilities
for those rules and a virtuous cycle of learning. As we shall see,
even when there is an unknown word in the sentence, the learner’s
increasing knowledge about the rules that are typically used in
other sentences can lead to very rapid learning of new words (both
their meanings and syntax) later on in learning.
To illustrate the learning algorithm, consider a very simple
example in which the child’s ﬁrst input is ‘‘more doggies”, which
we will assume, simplifying as usual, that she understands as
more(dogs). Upon hearing this sentence, the child inversely applies
the CCG combinators to retrieve every derivation possible under
CCG that yields the input pair ‘‘more doggies”: more(dogs). In this
case, there are three possible derivations:
The following set of candidate lexical entries can be read off the
three derivations in (4):
(5) a. more ‘ NP/N: more
b. more ‘ N: dogs
c. doggies ‘ NP/N: more
d. doggies ‘ N: dogs
e. more doggies ‘ NP: more(dogs)
Since this is the ﬁrst utterance the child hears, the correct lexi-
cal entries (a, d) and incorrect entries (b, c) are completely sym-
metrical and therefore equiprobable: if the child wants to say
more she is as likely to choose ‘‘dogs” as ‘‘more”. Lexical entry (e)
will have lower probability because the priors over wordforms
and logical forms favor shorter and simpler entries.
Now suppose the child encounters a second utterance, ‘‘more
cookies”, which will have three derivations and ﬁve possible lexical
entries analogous to those for ‘‘more doggies”, with the correct
derivation being:
In this case the correct derivation will receive higher probability
than the incorrect ones, due to the child’s previous experience with
‘‘more doggies”. In particular, because ‘‘more”: more has occurred
already in a possible derivation, Pðmore j moreÞ will be higher than
Pðmore j cookiesÞ or Pðcookies j moreÞ, and this will give a higher
probability overall to the correct derivation, even though at this
point there is no preference based on syntax. That is, prior to
observing ‘‘more cookies”, noun-initial and noun-ﬁnal NPs have
both
been
seen
once
in
equiprobable
derivations,
so
PðN; NP=N j NPÞ is equal to PðNPnN; N j NPÞ. Importantly for future
learning, however, the child is now able to update her syntactic
probabilities based on this example. Because the derivation in (6)
has
higher
probability
than
the
competing
derivations,
the
NP ! NP=N
N rule will be updated to receive higher probability
than the alternative NP ! N
NPnN, and in future examples this
will allow the child to use syntax to help infer the meanings of
10 We efﬁciently store and compute probabilities over the set of possible derivations
by using a ‘‘packed forest” representation of the kind used in chart parsing in
computational linguistics.
11 We show later that within broad limits, the learning rate has no interesting
effects on the overall performance of the learner.
O. Abend et al. / Cognition 164 (2017) 116–143
123

novel words. Our simulations in Sections 3.4 and 3.5 suggest that
this effect leads to an acceleration of learning over time, and Sec-
tion 3.7 shows that the same effect can explain results from classic
syntactic bootstrapping experiments, where children use word
order to correctly learn the meaning of a novel verb used with
two familiar nouns (e.g., the man daxed the baby).
It should be noted that, due to the incremental nature of our
learning algorithm, observing an utterance like ‘‘more cookies”
does not cause the model probabilities for dogs to be updated;
i.e., the probability that dogs is realized as ‘‘doggies” will still be
equal to the probability that it is realized as ‘‘more”. This behavior
would differ with a batch learning algorithm, since ‘‘more cookies”
provides evidence that more is realized as ‘‘more”, so that in ‘‘more
doggies”, dogs is more likely providing the semantics for ‘‘doggies”.
In our incremental algorithm, the parameters for any particular
word are only updated when that word is seen. In that sense, the
model built by the incremental learner is an approximation to
what would be learned by an exact batch learner.
2.5. Generating meanings for novel sentences
To evaluate our account of language learning, we apply the
resulting parser to unseen sentences and ask it to supply the corre-
sponding meaning representation. Because of the generative for-
mulation of the model, there is a straightforward probabilistic
interpretation of this task. In particular, given an input sentence s
and the model’s current estimate of the parameters H, we predict
the most probable, i.e., maximum a posteriori (MAP), meaning rep-
resentation m and syntactic tree t:
t; m ¼ argmax
m;t
Pðm; t j s; HÞ
ð7Þ
¼ argmax
m;t
Pðm; t; s j HÞ
Pðs j HÞ
ð8Þ
¼ argmax
m;t
Pðm; t; s j HÞ
ð9Þ
where Eq. (9) follows from (8) because Pðs j HÞ doesn’t depend on m
or t.
As the model is an instance of a PCFG, we use the standard prob-
abilistic CKY algorithm for computing the most likely ðm; t; sÞ tri-
plet given the parameters of the model.12 We note that the model
is unable to predict m if s contains unknown words, as the logical
constants used to represent them can be any of an inﬁnite set of
unknown items. Instead, the model predicts the most likely shell log-
ical form, where anonymous but semantically typed place-holders
replace unseen constants. For example, suppose the model sees the
sentence ‘‘Mary blicks John” with no paired logical form (corre-
sponding to a situation where the child hears the sentence but can-
not infer the semantics from non-linguistic context). If it has learned
enough about the syntax and semantics of other verbs and about the
meaning of ‘‘Mary” and ‘‘John”, then it will be able to infer the mean-
ing of the sentence as PLACE HOLDERðMary; JohnÞ, indicating that it
knows ‘‘blick” is a verb taking two arguments in a particular order,
but does not know the speciﬁc lexical meaning of ‘‘blick”. We also
investigate learning of this kind, using artiﬁcial unseen words.
3. Simulations
We conduct a range of simulations with our model, looking at
four main types of effects: (1) learning curves, both in terms of
the model’s overall generalization ability, and its learning of speci-
ﬁc grammatical phenomena; (2) syntactic bootstrapping effects,
where previously acquired constructions accelerate the pace at
which words are learned (we show overall trends and simulate
speciﬁc behavioral experiments); (3) one-shot learning effects,
showing that the model is able to infer the meaning of newly
observed words, given that enough about the syntax and the rest
of the words in the sentence is known; (4) ﬁndings as to the rela-
tive pace of learning of nouns and verbs.
Our simulations are summarized in Table 1 and described in
detail below, following the description of the input data.
3.1. Input to the learner
3.1.1. Corpus
We know of no corpus of child-directed utterances manually
annotated with compatible logical forms. Instead, we use the Eve
corpus
(Brown
&
Bellugi,
1964)
in
the
CHILDES
database
(MacWhinney, 2000) and construct the logical representations
semi-automatically based on the dependency and morphosyntac-
tic category annotations of Sagae et al. (2010), using the general
method of Çakıcı (2005) and Ambati, Deoskar, and Steedman
(2017).
The Eve corpus contains 20 roughly two-hour sessions, two per
month, collected longitudinally from a single child aged 18–
27 months. The dependency annotations were converted to logical
forms using a manually deﬁned mapping that addresses major
constructions, such as verb-argument structure, coordination,
WH and yes/no questions, copulas and auxiliaries. The mapping
covers 41% of the utterances in the corpus, yielding 5831 utter-
ances. About a quarter of the discarded utterances consist of one-
word interjections such as ‘‘hmm” or ‘‘yeah”. We further ﬁltered
Table 1
Navigation table for the presented simulations and main effects.
Section
Figure(s)
Simulation and main effect
3.2
1
Parsing accuracy improves over time; performance is
robust to propositional uncertainty
3.3
2–5
The model learns word ordering: that transitive
sentences are SVO, and that determiners and
prepositions are pre-nominal. Steep learning curves are
exhibited despite having no explicit parameter-setting
3.4
6–9
The model demonstrates one-shot learning of verbs,
nouns, determiners and prepositions in later stages of
acquisition. These results can be seen as a naturalistic
(corpus-based) analogue to the carefully controlled
simulations reported in Figs. 15–17, and therefore as a
separate illustration of how learned syntactic
knowledge can help accelerate word learning (the
‘‘syntactic bootstrapping” effect; Gleitman, 1990)
3.5
10, 11
The model exhibits acceleration in the learning pace of
transitive verbs and of nouns, similar to the
‘‘vocabulary spurt” observed in children (Reznick &
Goldﬁeld, 1992). Again, these results can be viewed as
(partly) due to syntactic bootstrapping
3.6
12–14
The model obtains a larger production vocabulary of
nouns than of verbs, despite not being explicitly biased
to do so (Gentner, 1982; Tomasello, 1992:210). More-
over, in the presence of distractors, verbs are initially
under-represented in the learner’s production vocabu-
lary (compared to nouns), relative to what can be
predicted from the verb to noun input ratio in the
child-directed utterances (Gentner & Boroditsky, 2001
and references therein)
3.7
15–17
The model simulates results from classic syntactic
bootstrapping experiments. It performs one-shot
learning, using its acquired knowledge of syntax to
disambiguate between possible meanings for a
transitive verb (following Fisher et al., 1994) and
between noun-like and preposition-like meanings for a
novel word (following Fisher et al., 2006)
12 Strictly speaking, the model is an inﬁnite PCFG, which (unlike a standard PCFG),
places some probability mass on unseen expansions. However, such expansions are
excluded when predicting the MAP representation.
124
O. Abend et al. / Cognition 164 (2017) 116–143

out sentences of 10 words or more, and of 10 or more extractable
sub-expressions, leaving a ﬁnal corpus of 5123 utterances. We
divide this into a training set of 4915 utterances (sessions 1–19),
and a held-out test set of 208 utterances (session 20). Some exam-
ple sentence-meaning pairs can be found in Appendix D.13
This is the only dataset of its kind that we know of. However, it
should be noted that it constitutes what we estimate to be less
than 2% of the data that the actual child Eve was learning from over
this same period in order to achieve the competence implicit in
session 20, the held-out test set. It will become apparent below
that this dataset is only just large enough to make stable and con-
vergent learning possible, given our current model and learning
algorithm.
3.1.2. Propositional uncertainty
In order to relax the overly optimistic assumption that the child
is exposed to entirely unambiguous situations, we add some
uncertainty as to the correct logical form. We do so by pairing each
utterance with several logical forms, only one of the which is the
correct one. Rather than choosing distractor logical forms at ran-
dom, we follow Fazly et al. (2010) in choosing distractors from
nearby utterances in the corpus, so they are likely to share some
semantic content with correct logical forms (as they were uttered
in a similar situation). In particular, distractors are taken to be the
logical forms of the w utterances preceding and following the tar-
get training example, where w 2 f0; 1; 2; 3g. We hence explore four
settings: one with no propositional uncertainty (target logical form
alone), and three with increasing amount of propositional uncer-
tainty (target plus two, four and six distractors).
3.2. Parsing unseen sentences
We begin by evaluating our parser’s learning performance by
training it on varying amounts of data, from 1 to 19 sessions. After
training on the ﬁrst i sessions (for i ¼ 1 . . . 19), we test the learner’s
ability to parse the sentences in the held-out test ﬁle (session 20).
We use a single test ﬁle rather than testing each time on session
i þ 1 to isolate the changes in performance due to learning, rather
than confounding results with possible changes in the test data
over time.
For each utterance in the test ﬁle, we use the parsing model to
predict the MAP estimate for m and compare this to the target
meaning. As described in Section 2.5, when a word has never been
seen at training time our parser has the ability to ‘‘guess” a shell
logical form with placeholders for unseen constant and predicate
names. Fig. 2 presents the prediction accuracy, i.e., the proportion
of utterances for which the prediction m matches the target
meaning with and without word-meaning guessing.
The low performance at all levels of uncertainty is largely due to
the sparsity of the data with 21% of all training sentences contain-
ing a previously unseen word. Nevertheless, while increasing the
propositional uncertainty slows the learning down, the learner
shows increasing performance over time in all cases. This is an
indication of the model’s robustness to noise, a major issue of con-
cern in computational models of acquisition (Siskind, 1996; Yang,
2002).
3.3. Learning word order
We next examine the model’s learning of syntax, starting with
transitive verbs. There are eight possible ‘‘word orders” (categories
including semantic forms) that a transitive may be associated with,
two of which are only applicable to ergative languages, and are
ruled out by stipulation (see Section 2.2). The proposed model does
not have a parameter that directly represents its belief as to which
of the categories is the correct one. However, the model does main-
tain a distribution over syntactic expansions and logical forms,
which we use to compute the model’s learned prior distribution
over the six remaining transitive verb categories (where the mod-
el’s beliefs after seeing the training data are its prior over the cat-
egories used to parse any new utterance).
Fig. 3 presents the relative probability for the transitive verb
categories in the various settings. In all cases, the correct SVO cat-
egory is learned to be the most probable upon convergence. In the
three cases with propositional uncertainty, we see that the SVO
category receives very little probability after 100 iterations (after
an initial probability of 1
6). The model is, however, able to recover,
and eventually the correct SVO order is learned to be the most
probable. However, in the 2 distractor setting the SVO category is
never overwhelmingly favored over the other categories (as it is
in all the other settings). This stems from the difﬁculty in setting
a learning rate that works well across many different experimental
settings given the small size of the training set. Consequently, in
this case the algorithm does not manage to recover from its initial
Fig. 2. The model is trained by incrementally viewing sentences from the ﬁrst i (x-axis) sessions, each paired with a correct meaning and up to six distractor meanings; it is
then tested on its accuracy (y-axis) at correctly assigning full sentence-level meanings to unseen test set sentences, using the 20th session as a test set. In (a), sentences
containing any previously unseen word are automatically marked as incorrect (since no speciﬁc semantics can be assigned to that word); in (b), the model can guess
placeholder meanings for unseen words, and receives credit if the rest of the semantic form is correct.
13 The derived corpus is available online through the corresponding author’s
website.
O. Abend et al. / Cognition 164 (2017) 116–143
125

under-estimation of the relative probability of SVO, since the
learning rate is too steep to allow effectively weighing in further
evidence observed later in learning. To conﬁrm this, we experi-
mented with a more gradual learning rate (details in Appendix
C), and present the relative probabilities of the six categories for
this setting in Fig. 4, which indeed shows a clear preference for
SVO.
Results with a few other values for the learning rate present
similar trends with some changes in the details. Under all settings
the correct grammatical categories are favored (and with deter-
miners and prepositions, overwhelmingly so) and the learning of
individual words follows similar patterns to those described in
the following section. Changes in details include more ﬂuctuating
learning curves when the learning rate is more moderate, and
somewhat different parameter values upon convergence.
To show that the model also learns the correct word order for
other categories, we examined determiners (Fig. 5) and preposi-
tions (Fig. 6), both in the high-uncertainty settings.14 (It is uninfor-
mative to look at nouns since the logical form in this case uniquely
determines the syntactic category, namely N.) Results show that the
model quickly converges to the correct options (pre-nominal rather
than post-nominal determiners, and prepositions rather than post-
positions), even in the presence of high propositional uncertainty.
In fact, the learning curves are even steeper than for verbs, despite
having no explicit ‘‘head-direction” parameter to set.
3.4. Learning curves of individual words
We next examine the learning curves for individual words,
again starting with transitive verbs. To do so, we estimate the
probability
Pðv; SVO j mvÞ
(henceforth,
‘‘correct
production
Fig. 3. Learning that English is an SVO language. Plots show the relative posterior probability assigned by the model to the six possible categories of transitive verbs. The x-
axis is the number of training utterances seen so far. In the 2 distractor setting, the failure to assign most of the probability mass to the correct SVO category is an artefact of
the small dataset (see text).
Fig. 4. The relative probability of the six categories considered for transitive verbs
in the 2 distractor setting, as a function of the number of seen utterances, where the
learning rate is modiﬁed to be more gradual. The correct SVO category is learned
quickly and effectively.
14 We consider only prepositions that introduce an argument rather than an
adjunct. Adjunct prepositions have different syntactic and semantic types and are
hence excluded by conditioning on the logical form.
126
O. Abend et al. / Cognition 164 (2017) 116–143

probability” or CPP) for a given transitive verb form v and its asso-
ciated logical form mv. This probability is a proxy to the production
beliefs of the child, i.e., her ability to utter the correct word and
syntactic category, given an intended meaning. This is a strict mea-
sure, as it requires learning both the correct logical form and the
correct syntactic category for the verb. We condition on the logical
form instead of the word form in order to avoid penalizing homo-
graphs, which are particularly common in our model, which e.g.
treats the word ‘‘run” in ‘‘they run”, ‘‘run along now”, and ‘‘I want
to run” as having three unrelated syntactic and logical forms which
just happen to have the same surface form. This means that even a
perfect learner would have Pðmthird person plural run j runÞ < 1, simply
because
some
probability
mass
must
also
be
reserved
for
Pðmimperative run j runÞ, and this effect would vary in unpredictable
ways for different word forms, making it difﬁcult to assess the suc-
cess of learning. By contrast, in our training data each logical form
uniquely determines the correct syntactic type and word form, so
that a perfectly successful learner should always approach a CPP
of 1.
Fig. 7 shows how the CPP varies over the course of training for a
few transitive verbs, both frequent (‘‘move”, ‘‘want”, ‘‘ﬁx”) and less
frequent (‘‘cracked”, ‘‘needs”). With no propositional uncertainty,
all of these verbs are learned rapidly within a short timeframe after
they are encountered. In the cases of 4 and 6 distractors, learning of
word order is more gradual, revealing how inaccurate beliefs about
the structure of the language inﬂuence the learning of speciﬁc
verbs. Indeed, verbs that are ﬁrst observed in an earlier stage of
learning (e.g., ‘‘want”, ‘‘ﬁx”, ‘‘move”) no longer exhibit one-shot
learning: instead, they are learned only gradually (and even seem
to be ‘‘un-learned” on occasion; though the model is able to
recover from the error).
Interestingly, the model’s knowledge about these words can
actually improve even without seeing them again (see utter-
ances 500–1000 in the 4 and 6 distractor cases) as its implicit
higher-order
beliefs
about
grammatical
structure
(e.g.,
SVO
word order) change. As the model learns more about the lan-
guage, its acquired knowledge also provides a strong prior
guiding the interpretation of utterances containing new verbs.
This leads to much more rapid learning of verbs ﬁrst observed
late in learning (‘‘cracked”, ‘‘needs”). There are two ways in
which prior learning can help: ﬁrst, knowing the meanings of
(some) other words in the sentence reduces the uncertainty
about the meaning of the verb. Second, as we demonstrate
in Section 3.7, even if no other words in the sentence are
known, simply having high prior probability associated with
rules instantiated by SVO categories will help to disambiguate
the verb’s meaning: syntactic bootstrapping at work. In the
naturalistic setting of this simulation, these two sources of
knowledge are inextricably linked; Section 3.7 isolates the syn-
tactic effect by simulating a controlled laboratory experiment.
(a) Syntactic disposition for determiners, 4 distractors
(b) Syntactic disposition for determiners, 6 distractors
Fig. 5. Learning word order for determiners. Plots show the relative probability of generating a pre-nominal vs. a post-nominal determiner in the (a) 4 distractor and (b) 6
distractor setting, as a function of the number of training utterances seen.
(a) Syntactic disposition for prepositions, 4 distractors
(b) Syntactic disposition for prepositions, 6 distractors
Fig. 6. Learning word order for prepositions. Plots show the relative probability of generating a preposition vs. a postposition in the (a) 4 distractor and (b) 6 distractor setting,
as a function of the number of training utterances seen.
O. Abend et al. / Cognition 164 (2017) 116–143
127

Of course, if our analysis is correct, then these patterns of learn-
ing should not be restricted to verbs. Fig. 8 presents the CPP for
various
nouns
of
different
frequencies,
showing
the
high-
uncertainty scenarios where the differences between early and late
learning should be more pronounced. These plots suggest that
nouns are easier to learn than verbs (we return to this point in Sec-
tion 3.6), since most of the nouns are learned rapidly, even early on
(note the zoomed in plots only go to 1000 utterances here). Never-
theless, nouns observed in the very earliest part of learning do tend
to require more observations to learn than those ﬁrst observed
Fig. 7. Learning curves showing the correct production probability for verbs of different frequencies in the (top) 0-distractor, (middle) 4-distractor, and (bottom) 6-distractor
settings, as a function of the number of utterances observed. (Results from the 2 distractor setting in this case are uninformative since the model fails to learn SVO word order
well: see Fig. 3(b) and associated discussion.) Plots on the right zoom in on the ﬁrst stage of learning, and also show horizontal ‘‘timelines” for each word with tickmarks
indicating, for each 20-utterance block, whether the word occurred in that block. (The color and top-to-bottom order of the timelines matches the words in the legend.) Large
jumps in the plots generally correspond to observations in which the given verb appears, while smaller changes can occur due to ﬁne-tuning of higher-order model
parameters (e.g., the overall probability of transitive verbs having SVO syntax). The frequencies for these verbs are: f ðmoveÞ ¼ 18; f ðwantÞ ¼ 70; f ðcrackedÞ ¼ 1;
f ðneedsÞ ¼ 3; fðfixÞ ¼ 11.
128
O. Abend et al. / Cognition 164 (2017) 116–143

later: in some cases learning is more gradual (as for ‘‘man”); in
others the word may be observed a few times with little or no
learning, before then being learned all at once (as for ‘‘girl”). Figs. 9
and 10 show similar effects for some individual determiners and
prepositions.
To summarize, results from looking at learning curves for
individual words suggest that words ﬁrst observed early in
the learning process are initially learned gradually (or in some
cases suddenly but after several occurrences), while those that
ﬁrst appear later on are learned more rapidly, often after only
a single occurrence. These ﬁndings, which we explore further
in the following section, suggest that qualitative changes in
the shape of the learning curves can be explained by a single
statistical learning mechanism, which improves as it learns
(see
also
Fazly
et
al.,
2010;
Regier,
2005
and
references
therein). This single mechanism can also explain the ability
to learn from just one example, a phenomenon we return to
in Section 3.7.
3.5. Acceleration of learning and the vocabulary spurt
The learning curves for individual words presented in the
previous section suggested that words encountered early in
acquisition are learned more slowly than those ﬁrst encountered
at a later point. We now show that this ﬁnding is not restricted
to the particular words we examined, and we also explore the
consequences for vocabulary size as word learning accelerates
over time.
For the ﬁrst analysis, we look at transitive verbs. Notice that, on
average, a high-frequency verb will appear ﬁrst earlier in the corpus
than a low-frequency verb. Thus, if words that ﬁrst occur late tend
to be learned more rapidly, then on average low-frequency verbs
will be learned in fewer examples, and high-frequency verbs will
require more examples.
To test this hypothesis, we divide verbs into two bins: high-
frequency (occurs more than 10 times in the corpus) or low-
frequency (occurs 10 or fewer times). For each bin, we compute
the average CPP for all verbs after seeing n instances of that verb
type.15 We plot this average CPP as a function of n in Fig. 11, which
indeed shows the predicted trend.
Our second analysis, which focuses on nouns, is more directly
comparable to observations from real children. Speciﬁcally, after
an initial period in which the vocabulary of the child is restricted
to no more than a few words, a sudden increase in the production
vocabulary size is observed, usually around the age of 18–
24 months (Reznick & Goldﬁeld, 1992). The trends we have already
observed in individual words, when taken together over the whole
vocabulary, should yield exactly this kind of accelerated learning.
Fig. 8. Learning curves showing the CPP for nouns of different frequencies in the (top) 4-distractor and (bottom) 6-distractor settings, as a function of the number of
utterances observed. Right-hand plots zoom in on the earlier
stage of learning, as in Fig. 7. The frequencies are: f ðskunkÞ ¼ 2; fðbirdÞ ¼ 3; f ðbabyÞ ¼ 12;
f ðmanÞ ¼ 29; f ðgirlÞ ¼ 52.
15 Many verbs are ambiguous between multiple logical forms. We therefore deﬁne a
verb type according to its mv rather than its written form.
O. Abend et al. / Cognition 164 (2017) 116–143
129

Fig. 12 conﬁrms this prediction (see Fazly et al., 2010 for a
related ﬁnding). We plot an estimate of the size of the noun pro-
duction vocabulary as a function of the number of observed
utterances. The noun vocabulary size is deﬁned as the number
of nouns whose CPP exceeds a threshold of 0.8. We present
results for the 6 distractors setting in which the trends can be
seen most clearly, as the initial learning stage is longer. Results
show that after an initial period (up to about 200 utterances)
in which the vocabulary size increases slowly, the slope of the
curve changes and nouns are learned at a faster rate. This could
be partly due to syntactic bootstrapping from knowledge about
determiner and preposition ordering, which is acquired very
early (as shown in Figs. 5 and 6). It is likely also due to the fact
that as more nouns are learned, any sentence containing those
nouns has fewer other unknown words, so overall uncertainty
is reduced. In any case, the results suggest that no special mech-
anism or change in processing is needed to explain the vocabu-
lary spurt, it simply arises from the nature of the statistical
learner.
3.6. Comparing noun and verb learning
It is well-known that nouns predominate over verbs in the early
lexicons of English-learning children, although there is consider-
able individual and cross-linguistic variation, and some verbs or
predicative categories are among the earliest vocabulary items
acquired
for
some
children
(Gentner,
1982;
Tomasello,
1992:210). Fig. 13 shows that our learner shows a similar trend,
accumulating nouns faster than transitive verbs, but learning some
verbs right from the start.16
This observed bias towards nouns cannot be ascribed to a dif-
ference in the conceptual complexity of nouns and verbs, or a dif-
ference in their level of concreteness (see Snedeker & Gleitman,
2004 for a review of proposals based on these ideas), as our
model does not distinguish nouns and verbs on any such
dimensions.
Moreover, the larger noun vocabulary in our model doesn’t sim-
ply reﬂect the input frequencies of nouns and verbs: Fig. 14 pre-
sents the ratio of verb types to noun types in the input, as well
as in the production vocabulary for the different distractor settings.
In the more realistic 2-, 4- and 6-distractor settings, the verb:noun
ratio starts off lower than that in the input, and only later con-
verges to the input ratio. That is, early on (especially in the ﬁrst
few hundreds of sentences), verbs are under-represented relative
to their input rate. In the no-distractors setting, the learned rate
Fig. 9. Learning curves showing the CPP for determiners of different frequencies in the (top) 4-distractor and (bottom) 6-distractor settings, as a function of the number of
utterances observed. Right-hand plots zoom in on the earlier stage of learning, as in Fig. 7. The frequencies are: fðhisÞ ¼ 17; fðanyÞ ¼ 9; f ðaÞ ¼ 351; f ðmoreÞ ¼ 23;
f ðtheÞ ¼ 321.
16 There are many different ways to count word types, e.g., deciding whether to
lump or split a verb by subcategorization, tense, agreement, mood and other
grammatical systems may result in different counts. Here we focus on transitive
verbs, counting types of all inﬂections, while counting only singular nouns against the
noun’s vocabulary. Thus, any difference between the noun and transitive verb
vocabulary sizes is a conservative estimate. These methodological issues play a lesser
role when comparing the ratios of the input and learned vocabularies (see below).
130
O. Abend et al. / Cognition 164 (2017) 116–143

is similar to the input rate.17 This under-representation of verbs in
the early phase of learning is consistent with claims of noun bias
in the literature, i.e., that the the ratio of verbs to nouns is lower
in child productions than in the child directed speech (see discussion
in Gentner & Boroditsky, 2001).
Comparing the plots in Fig. 14 with the learning curves for word
order shown earlier suggests an explanation for the noun bias in
our model, namely that: (1) early on, nouns are over-represented
in the vocabulary because the syntactic constructions of noun
phrases (involving determiners and prepositions) are learned more
Fig. 10. Learning curves showing the CPP for prepositions of different frequencies in the (top) 4-distractor and (bottom) 6-distractor settings, as a function of the number of
utterances observed. Right-hand plots zoom in on the earlier stage of learning, as in Fig. 7. The frequencies are: fðwithÞ ¼ 53; fðatÞ ¼ 17; fðinÞ ¼ 86; fðaboutÞ ¼ 7; fðonÞ ¼ 78.
Fig. 11. The average CPP (y-axis) for frequent and infrequent verbs, after observing n examples (x-axis). Because the nth example of a low-frequency verb occurs later in
training (on average), fewer examples are required to reach a given level of proﬁciency. We use a frequency threshold of 10, leading to 99 infrequent verbs and 23 frequent
verbs. We do not plot 7–9 occurrences because only eight of the infrequent verbs have 7 or more occurrences. Results are shown only for the 4- and 6-distractor settings
because in the low-distraction setting, all verbs are learned so quickly that it obscures the effect.
17 The plot appears to show that verbs are over-represented for the ﬁrst 80
utterances, but as the vocabulary is very small at this point (no more than ten words
of each category), the difference between the learned and input ratios is likely not
statistically signiﬁcant.
O. Abend et al. / Cognition 164 (2017) 116–143
131

quickly than those of transitive clauses (chieﬂy, the category of
transitive verbs), allowing syntactic bootstrapping to start earlier
for nouns and increase the rate of noun learning; (2) as learning
progresses, and once the structure of transitive clauses is learned
too, the rate of verb learning ‘‘catches up” to that of nouns, and
the larger noun vocabulary can be directly ascribed to the differing
frequencies of verbs and nouns in child-directed speech. Indeed,
the point of acquisition of the correct SVO category in the different
settings (Fig. 3) largely matches the later phase of convergence
between the input and learned rate. The no-distractor setting thus
does not display an early under-representation of verbs simply
because the syntax of transitives is learned very early on.
This explanation is indeed consistent with the proposal of
Snedeker and Gleitman (2004) that verbs lag behind nouns
because learning verbs requires greater linguistic (rather than con-
ceptual) development on the part of the child. In particular, they
argue that verb learning requires more knowledge about the sur-
rounding sentential context (including its syntax) so the learner
gets additional cues beyond just the non-linguistic situational
context.
Beyond this hypothesis, our model also suggests that the use of
syntactic cues might kick in earlier for nouns than for verbs.
Whether this holds true in infants likely requires further behav-
ioral study, since most behavioral work on syntactic bootstrapping
has focused on verbs (see Fisher, Gertner, Scott, & Yuan, 2010 for a
review). Nevertheless, some studies do suggest that infants are
able to use syntactic and morphological cues earlier in noun pro-
cessing than verb processing (see Cauvet et al., 2014 for a review).
It is therefore plausible that syntactic information relevant to
nouns is learned and used earlier, and that the resulting bootstrap-
ping effect could further explain the early noun bias in children’s
vocabulary.
3.7. Syntactic bootstrapping for one-shot learning of nonce words
So far, all our analyses have focused on the ‘‘natural” timecourse
of learning, studying what the simulated child might know at a
particular moment in time as a result of their particular and
idiosyncratic history of exposure to language. Much of the litera-
ture on child language learning, however, focuses on what children
of different ages are able to learn based on controlled exposure to a
small number of examples in a laboratory setting. In this section,
Fig. 12. The size of the production vocabulary of nouns as a function of the number
of utterances seen, for the 6 distractors setting. The production vocabulary is
deﬁned as all nouns with a CPP of at least 0.8.
Fig. 13. A comparison of the noun and transitive verb vocabulary sizes in the (a) 4-distractor and (b) 6-distractor settings. The curves present the sizes of the production
vocabularies of nouns and transitive verbs (deﬁned as words with CPP at least 0.8) as a function of the number of observed utterances. Results indicate that nouns account for
a larger part of the vocabulary than transitive verbs. The relative vocabulary sizes for the 0- and 2-distractor settings are nearly identical to what is shown, but more words are
learned in the 0-distractor setting.
Fig. 14. A comparison of the ratio of transitive verbs and nouns in the child-
directed input, relative to this ratio in the child’s production vocabulary (deﬁned as
words with CPP at least 0.8). Curves are presented for all four distractor settings.
Results indicate that the ratios converge to similar values in all settings, but that in
the 2, 4 and 6 distractor settings verbs are initially under-represented relative to
their presence in the input.
132
O. Abend et al. / Cognition 164 (2017) 116–143

we follow suit, by ﬁrst training our model on the ﬁrst n examples
from the corpus (to simulate the background knowledge a child of
a particular age might bring to the lab), and then exposing it to a
small number of hand-crafted training examples and studying
what it learns from these.
In particular, we examine the model’s ability to deal with cases
like Fisher et al.’s (Fisher, Hall, Rakowitz, & Gleitman, 1994) chase/
ﬂee example, where the same physical situation seems to support
more than one logical form. Fisher et al. (1994) ask how it is that
children faced with (made up) examples like the following avoid
the error of making an OVS lexical entry for ‘‘ﬂee” with the mean-
ing chase:
(7)
‘‘Kitties ﬂee doggies”.
It is important that words like ‘‘ﬂee”, where the instigator of the
event is realized as an object rather than a subject, are relatively
rare. (For example, there are eight occurrences of any form of ‘‘ﬂee”
in the entire English CHILDES corpus—over 17 million tokens—of
which none is transitive.) Therefore, the child will have encoun-
tered plenty of normal transitive verbs (where the instigator is
the subject) before encountering examples like (7).
In terms of our model, this means that, by the time (7) is
encountered, the learned prior probability of the instantiated rules
for combining transitive SVO verbs with their object and subject
will be substantially greater than the priors for OVS verbs. This will
lead to a much higher probability for the correct meaning and SVO
syntax for ‘‘ﬂee” than for an OVS lexical item with the same mean-
ing as ‘‘chase”. Of course, the model also entertains other possibil-
ities, such as that ‘‘ﬂee” means cats, that ‘‘kitties” means ﬂee, etc.
However, assuming that ‘‘kitties” and ‘‘doggies” have been encoun-
tered relatively often, so that ‘‘ﬂee” is the only unfamiliar word in
the sentence, then the model will assign a very low probability to
these additional spurious hypotheses.
In fact, we will see that after training, the model can determine
the correct meaning for a transitive verb even in the face of signif-
icantly greater ambiguity, when the meanings of other words in
the sentence are not known, because the learned prior over the
OVS syntactic category, which is not attested elsewhere in the data,
becomes exceedingly low.
Gleitman and colleagues have described the process by which
the child resolves this type of contextual ambiguity as ‘‘syntactic
bootstrapping”, meaning that lexical acquisition is guided by the
child’s knowledge of the language-speciﬁc grammar, as opposed
to the semantics (Gleitman, 1990; Gleitman, Cassidy, Nappa,
Papafragou, & Trueswell, 2005). However, in present terms such
an inﬂuence on learning simply emerges from the statistical model
required for semantic bootstrapping. We will return to this point in
the General Discussion.
3.7.1. Nonce verbs
Our ﬁrst simulation is inspired by the experiments of Gertner,
Fisher, and Eisengart (2006), where children at the age of 21 and
25 months were presented with two pictures, one in which a girl
is performing some action over a boy, and one in which it is the
other way around. The pictures were paired with a description,
either ‘‘the boy gorped the girl” or ‘‘the girl gorped the boy”. Results
showed a disposition of the children to select the interpretation
(that is, to look at the picture) in which the actor coincided with
the subject rather than the object of the sentence.
Here, we examine the model’s ability to correctly interpret the
nonce verb ‘‘dax” in the sentence ‘‘the man daxed the baby”, when
presented with two possible meaning representations: one in
which the man is performing an action over the baby, and one in
which the baby is performing an action over the man. In order to
correctly infer the meaning of ‘‘daxed” (analogous to Fisher
et al.’s ‘‘ﬂee” and Gertner et al.’s ‘‘gorped”) the model has to rely
on its learned prior to favor SVO over OVS interpretations, as the
two interpretations are otherwise symmetrical. Results show the
model quickly learns to favor the correct SVO interpretation for
‘‘daxed”.
Speciﬁcally, we ﬁrst train the model on the ﬁrst n examples of
the CHILDES corpus, in intervals of 100 examples (4 and 6 dis-
tractors settings), and then present the model with one more sen-
tence – literally, the string ‘‘the man daxed the baby” – which is
paired with both the SVO- and OVS-supporting logical forms.
(Intuitively, the SVO-supporting logical form involving the man
daxing the baby is ‘correct’, while the OVS-supporting logical
form involving the baby daxing the man is a ‘distractor’, but the
model is simply presented both as two a priori equally likely pos-
sibilities.) We then query the model’s state to compute several
predictions.
First, we ask which of the two logical forms the model assigns a
higher posterior probability, and ﬁnd that from n ¼ 200 examples
and onwards the correct SVO interpretation is always preferred.
Next, we use the strict CPP statistic to assess the model’s success
at learning the nonce verb ‘‘daxed”: kxky.daxed(y, x) based on this
single training example (Fig. 15) and ﬁnd that it is quite successful
despite the syntactic complexity of the surrounding sentence.
Fig. 15. Syntactic bootstrapping: The model’s CPP for the unknown word ‘‘daxed” (y-axis) after training on n utterances from the corpus (x-axis) followed by a single
presentation of the sentence ‘‘The man daxed the baby” with SVO/OVS ambiguous semantics. During training only, either four (a) or six (b) distractors are included along with
each correct logical form.
O. Abend et al. / Cognition 164 (2017) 116–143
133

3.7.2. Multiple novel words
The above simulation examined the model’s ability to perform
syntactic bootstrapping to learn an unknown verb in the presence
of uncertainty between an SVO and an OVS interpretation. We next
examine the model’s performance in a simulated experiment where
the utterance is ambiguous between all possible word orders. We
ﬁnd that also in this setting, the model is able to determine the cor-
rect SVO order, due to its high (learned) prior over rules instantiated
by the SVO category as opposed to the alternatives.
Concretely, in this simulation the model is trained on n exam-
ples from the corpus, and then receives the single utterance ‘‘Jacky
daxed Jacob”, where ‘‘Jacky”, ‘‘Jacob” and ‘‘daxed” are all unknown
words, paired with the logical form daxed(jacky, jacob). This corre-
sponds to a situation where the child is exposed to a scene in
which some hitherto unnamed person performs an unusual action
on another unnamed person. A priori, there are many possible
interpretations that are consistent with this utterance; for exam-
ple, in a VSO language we would expect a child to interpret ‘‘Jacky”
as a verb meaning daxed(x, y), and ‘‘daxed” as a noun meaning
jacky, and if we exposed our model to this example alone without
any other training data, then it would indeed assign this interpre-
tation just as high a probability as the intended SVO interpretation.
What this simulated experiment tests, therefore, is the model’s
ability to make higher-order syntactic generalizations from the
English-based training corpus, and apply them to this novel
utterance.
First, we ask which derivation tree for ‘‘Jacky daxed Jacob”:
daxed(jacky, jacob) is assigned the highest probability, and ﬁnd that
the correct SVO derivation is preferred in all cases where n P 500
for the 4 distractor case and n P 800 for the 6 distractor case. We
further examine the model’s ability to assign ‘‘daxed” the correct
SVO interpretation by plotting the CPP of the word ‘‘daxed” after
training on n training examples, followed by the dax example.
Fig. 16(a) and (b) presents this probability for the 4 and 6 distractor
settings respectively. These results demonstrate the model’s ability
to correctly interpret ‘‘daxed” (and consequently the sentence) in
this more ambiguous setting after training on a single example of
the verb, proving the model’s ability to learn and apply syntactic
generalizations even in the presence of propositional uncertainty.
3.7.3. Nonce nouns and prepositions
Finally, we examine the cases of nouns and prepositions, con-
ﬁrming the model’s ability to perform syntactic bootstrapping in
these categories as well. We take inspiration from Fisher, Klingler,
and Song (2006), who demonstrated 2-year old’s disposition to
interpret ‘‘corp” in ‘‘this is a corp” as a noun (and therefore assign
it a noun-compatible meaning), while interpreting ‘‘acorp” in ‘‘this
is acorp the baby” as a preposition (with a preposition-like mean-
ing). In our case, we feed the model with either of the two utter-
ances: ‘‘this is the corp” and ‘‘this is corp Mommy”, paired with
two possible logical forms: one where ‘‘corp” is compatible with
the noun category and another where ‘‘corp” is compatible with
the preposition category.18 The model assigns ‘‘corp” a noun inter-
pretation in the ﬁrst example, and a preposition interpretation in
the second. Concretely, as before the model is trained on n examples
in the 4 and 6 distractors settings, and then given a single instance of
either of the ‘‘corp” examples, paired with the two possible logical
forms. The steep learning curves of this simulation are given in
Fig. 17, indicating that once again the model is able to use its knowl-
edge of syntax to correctly disambiguate between the meanings, just
as the children in the experiment did.
4. General discussion
We have presented an incremental model of language acquisi-
tion that learns a probabilistic CCG grammar from utterances
paired with one or more potential meanings. The learning model
assumes no knowledge speciﬁc to the target language, but does
assume that the learner has access to a universal functional map-
ping from syntactic to semantic types (Klein & Sag, 1985), as well
as a Bayesian model favoring grammars with heavy reuse of exist-
ing rules and lexical types. It is one of only a few computational
models that learn syntax and semantics concurrently, and the only
one we know of that both (a) learns from a naturalistic corpus and
(b) is evaluated on a wide range of phenomena from the acquisi-
tion literature. Moreover, the model is fully generative, meaning
that it can be used for both parsing (understanding) and genera-
tion, and that probability distributions over subsets of the variables
(e.g., individual lexical items) are well-deﬁned—a property we used
in many of our analyses. Like some very recent proposals
(Beekhuizen, 2015; Jones, 2015) but unlike earlier work (Buttery,
2006; Gibson & Wexler, 1994; Sakas & Fodor, 2001; Siskind,
1992; Villavicencio, 2002; Yang, 2002), we also evaluate our lear-
ner by parsing sentences onto their meanings, rather than just
examining the learner’s ability to set a small number of predeﬁned
syntactic parameters.
Fig. 16. Syntactic bootstrapping: The model’s CPP for the word ‘‘daxed” (y-axis) after training on n utterances from the corpus (x-axis) plus one exposure to ‘‘Jacky daxed
Jacob”, where all three words are unknown. During training only, either four (a) or six (b) distractors are included along with each correct logical form. In both cases, the
correct interpretation of the test sentence is the most likely one upon convergence.
18 The slight modiﬁcations of Fisher et al. (2006)’s examples were done in order to
arrive at a pair of utterances of the same length, in which the non-nonce words are
learned very early on.
134
O. Abend et al. / Cognition 164 (2017) 116–143

Together, these properties of our model and evaluation allow us
to investigate the relationship between word learning and syntac-
tic acquisition in a more realistic setting than previous work.
Results from our simulations suggest that the kind of probabilistic
semantic bootstrapping model proposed here is sufﬁcient to
explain a range of phenomena from the acquisition literature:
acceleration of word learning, including one-shot learning of many
items later in acquisition; the predominance of nouns over verbs in
early acquisition; and the ability to disambiguate word meanings
using syntax. All of these behaviors emerge from the basic assump-
tions of the model; no additional stipulations or mechanisms are
required. Finally, our work addresses a criticism levelled against
some statistical learners—that their learning curves are too gradual
(Thornton & Tesan, 2007). By demonstrating sudden learning of
word order and of word meanings, our model shows that statistical
learners can account for sudden changes in children’s grammars.
In the remainder of the discussion, we expand on a few of these
points, highlight some connections between our model and earlier
theories of acquisition, and discuss how our model might general-
ize to more realistic meaning representations.
4.1. Syntactic bootstrapping revisited
It has long been noted that the onset of syntactically productive
language is accompanied by an explosion of advances in qualita-
tively
different
‘‘operational”
cognitive
abilities
(Vygotsky,
1934/1986). These ﬁndings suggest that language has a feedback
effect that facilitates access to difﬁcult concepts, a view that is fur-
ther supported by the early work of Oléron (1953) and Furth
(1961), who demonstrated that deaf children who were linguisti-
cally deprived (by being denied access to sign) showed speciﬁc
cognitive deﬁcits in non-perceptually evident concepts.
This view is also consistent with Gleitman’s (Gleitman, 1990)
proposal that the availability of syntax enables the child to ‘‘syn-
tactically bootstrap” lexical entries for verbs (such as ‘‘promise”)
that may not be situationally evident. However, we have argued
that such syntactic bootstrapping is only possible when the child
has access to a model of the relation between language-speciﬁc
syntax and the universal conceptual representation. Our simula-
tions of learning novel transitive verbs, inspired by the chase/ﬂee
ambiguity and related behavioral studies, support this argument
and demonstrate that syntactic bootstrapping need not be postu-
lated as a distinct mechanism, but is emergent from a semantic
bootstrapping approach such as the one proposed here.
In that sense, the so-called ‘‘syntactic” bootstrapping effect
might better be termed ‘‘structural”, since it relies crucially on
(and arises directly from) the homomorphism between syntactic
and semantic types. Although the child might eventually learn
the meanings of novel predicates in any case, structural bootstrap-
ping serves to highlight the meaning that is most plausible in light
of the child’s previously acquired grammatical knowledge. Our
model operationalizes that previously acquired knowledge as a
Fig. 17. The correct production probability (y-axis) for the unknown word ‘‘corp” in the sentences ‘‘This is corp Mommy”, where it should be interpreted as a preposition (a
and b), and ‘‘This is the corp” (c and d), where it should be interpreted as a noun, as a function of the number of training examples seen (x-axis). The model is trained in the 4
and 6 distractors settings.
O. Abend et al. / Cognition 164 (2017) 116–143
135

probabilistic grammar, where the probabilities learned from previ-
ous examples act as the prior distribution when faced with a new
example. When learning from a naturalistic corpus, these priors
become sufﬁciently peaked that the learner is able to identify the
correct verb meanings in examples with ambiguous semantic roles,
like ‘‘the man daxed the baby”. More generally, the learner’s accu-
mulating syntactic knowledge (along with knowing more word
meanings) causes an acceleration of learning, which we demon-
strated both for verbs and nouns.
4.2. Relationship to non-probabilistic and parameter-setting
approaches
Like other probabilistic approaches to language acquisition, our
proposal avoids many of the stipulations required by non-
probabilistic approaches. In particular, there is no need for a Subset
Principle (Berwick, 1985) or the ordered presentation of unam-
biguous parametric triggers, both of which appear to present seri-
ous problems for the language learner (Angluin, 1980; Becker,
2005; Fodor & Sakas, 2005). Nor does our approach contradict
widely-held assumptions concerning the lack of negative evidence
in the child’s input. The child can progress from the universal
superset
grammar
to
the
language-speciﬁc
target
grammar
entirely on the basis of positive evidence. This positive evidence
raises the probability of correct hypotheses at the expense of incor-
rect ones, including those introduced by error and noise. The only
evidence that the child needs is a reasonable proportion of utter-
ances that are sufﬁciently short for them to deal with.
Nevertheless, our proposal does share some commonalities
with earlier non-probabilistic approaches. For example, it resem-
bles the proposal of Fodor (1998) as developed in Sakas and Fodor
(2001) and Niyogi (2006) in that it treats the acquisition of gram-
mar as arising from parsing with a universal ‘‘supergrammar”,
which includes all possible syntactic structures allowed in any
language that are compatible with the data (Fodor, Bever, &
Garrett, 1974:475). However, our learner uses a different algo-
rithm to converge on the target grammar. Rather than learning
rules in an all or none fashion on the basis of parametrically
unambiguous sentences, it uses all processable utterances to
adjust the probabilities of all elements of the grammar for which
there is positive evidence. In this respect, it more closely resem-
bles the proposal of Yang (2002). However it differs from both in
eschewing the view that grammar learning is parameter setting,
and from the latter in using a Bayesian learning model rather
than
Mathematical
Learning
Theory
(Atkinson,
Bower,
&
Crothers, 1965).
We eschew parameter setting because if the parameters are
implicit in the rules or categories themselves, which can be
learned directly, neither the child nor the theory need be con-
cerned with parameters other than as a prior distribution over
the
seen
alternatives.
For
the
child,
all-or-none
parameter-
setting would be counterproductive, making it hard to learn the
many languages that have exceptional lexical items or inconsis-
tent settings of parameters across lexical types, as in the English
auxiliary verb system and Italian subject-postposing intransitives.
Of course, languages do tend toward consistent (if violable) val-
ues of parameters like headedness across categories for related
semantic types (such as verbs and prepositions). Such consistency
will make learning easier under the present theory, by raising the
model’s probabilities for recurring rules and categories, leading to
higher prior probabilities for those rules and categories when
they are re-encountered in future. It is less clear that representing
this consistency explicitly, rather than implicitly in the model and
the evolving prior, will help the individual child learning its ﬁrst
language.
4.3. Cognitive plausibility and interpretation of probabilities
Our model is based on similar mathematical principles to other
Bayesian models of language acquisition (e.g., Feldman et al., 2013;
Frank et al., 2009; Goldwater et al., 2009; Grifﬁths & Tenenbaum,
2005, 2006; Perfors, Tenenbaum, & Wonnacott, 2010; Xu &
Tenenbaum, 2007), and shares with them the attractive properties
noted above: the probabilistic framework permits the learner to
accumulate evidence over time and eventually make strong infer-
ences about the language-speciﬁc grammar, without the need for
negative evidence and in a way that is robust to noise. However,
unlike many Bayesian models, our learner uses an incremental
learning
algorithm
rather
than
adopting
an
ideal
observer
approach. We adopted this approach in the interests of cognitive
plausibility and in order to easily examine the progress of learning
over time. As a result our learner has no guarantee of optimality,
but is aligned with other recent work investigating cognitively
plausible approximate learning strategies for Bayesian learners
(Pearl, Goldwater, & Steyvers, 2010; Sanborn, 2017; Sanborn,
Grifﬁths, & Navarro, 2010; Shi, Grifﬁths, Feldman, & Sanborn,
2010).
Nevertheless it is worth noting that, despite the incremental
algorithm, some other aspects of our learner are less cognitively
plausible. In particular, to process each input example the learner
generates all parses consistent with that example. This can require
signiﬁcant memory and computation, especially for longer sen-
tences, and indeed is not incremental at the level of words, as
human sentence processing is generally accepted to be. However,
it might be possible to develop further approximations to reduce
the computational requirements, for example by computing
parameter updates based on a high probability subset of the possi-
ble parses. Another approach might be particle ﬁltering, which has
been used by some other recent models to model memory limita-
tions in incremental sentence processing and other tasks (Levy,
Reali, & Grifﬁths, 2009; Sanborn et al., 2010; Shi et al., 2010).
Regardless of the learning algorithm, the use of probabilities
raises the question of how exactly a notion like ‘‘the adult gram-
mar” should be understood in this model, where the grammar rep-
resents a probability distribution over all hypotheses that the child
learner has ever entertained, albeit one that has converged with
most of the probability mass concentrated in correct lexical entries
and rules of derivation, as opposed to spurious determiners like
‘‘doggies” and rules for combining them with nouns like ‘‘more”.
Such a grammar/model could in principle be pruned to elimi-
nate such obvious dead wood. However, the grammar would
remain probabilistic in nature. This conclusion might seem to con-
ﬂict with traditional theories of competence, but such probabilistic
grammars are now widespread in models of gradience in language
processing and gradient grammaticality (Bresnan & Nikitina, 2003;
Hale, 2001; Jurafsky, 1996; Levy, 2008; Sorace & Keller, 2005).
Some have argued that gradience in competence is distinct from
that in performance (Crocker & Keller, 2005). Our model’s probabil-
ities closely reﬂect processing (i.e., frequency of use). However,
even if interpreted as a model of competence, it can in principle
model both gradient and hard judgements, the latter either as
extremely low-probability events, or as structures that are entirely
absent from the probabilistic grammar. Our model would then
instantiate a theory of learning for such gradient grammars, anal-
ogous to various learning models proposed for stochastic Optimal-
ity Theoretic grammars (e.g., Boersma & Hayes, 2001; Goldwater &
Johnson, 2003; Hayes & Wilson, 2008).
4.4. Towards more realistic meaning representation
The meaning representations for child-directed utterance con-
sidered in this work have been necessarily simpliﬁed in content,
136
O. Abend et al. / Cognition 164 (2017) 116–143

because the CHILDES annotations that we have to work with are
limited to narrow, propositional literal meaning. However, the con-
tent that the child learns from surely also includes aspects of the
social interaction that the child is engaged in (Tomasello &
Farrar, 1986 passim). It is the latter that provides the primary moti-
vation for the child to learn language, particularly in the earliest
stages. Such interpersonal content is lacking in the CHILDES
annotation.
We know of no fully convincing or widely accepted formal
semantic representation for these interpersonal aspects of dis-
course meaning, much less any such representation that we
believe human annotators could apply consistently to labelled cor-
pora of child directed utterance (Calhoun et al., 2010; Cook &
Bildhauer, 2011).
However, we do believe that the knowledge representation
identiﬁed by Tomasello can be expressed as compositional seman-
tic formulas, not fundamentally different from the ones we use
here
to
demonstrate
the
workings
of
our
learner.
Indeed,
Steedman (2014) proposed such a compositional formulation for
one major component of interactional learning, namely informa-
tion structure.
The term ‘‘information structure” refers to the packaging of the
content of an utterance along a number of dimensions that have
been referred to in the literature—not always consistently—as
‘‘topic”, ‘‘comment”, ‘‘contrast” and the like, together with markers
of speaker and hearer orientation of these meaning components
(Bolinger, 1965; Halliday, 1967; Ladd, 1996; Steedman, 2014),
often expressed by prosodical markers such as pitch contour and
metrical alignment (Calhoun, 2010, 2012). Intonational prosody
is used exuberantly in speech by and to children, and we know that
infants are sensitive to the interpersonal signiﬁcance of intonation
from a very early age (Fernald, 1993; Fernald et al., 1989).
It is therefore likely that when a child hears her mother pointing
at newly observed dogs and saying ‘‘More doggies!”, the child’s
representation of the utterance includes the information that a
new entity or property is introduced to the common ground, and
that she considers the possibility that this information is conveyed
by the speaker’s intonation contour. Using the intonational nota-
tion of Pierrehumbert and Hirschberg (1990) (H⁄ and LL% are a high
accent and low boundary tone respectively) and sub-categorizing
the syntactic categories to correspond to information-structural
distinctions, the derivation might look as follows (þ;q marks a
positive polarity rheme or comment, and / marks a complete
phonological phrase):
The semantics of this sentence can be glossed as: ‘‘Mum makes
the property afforded by more dogs common ground”. The details
of the semantics of information structure go beyond the immediate
concerns of this paper: the example is merely intended to suggest
that interpersonal aspects of meaning can be treated as formally as
more standard content.
Intonation structure does not always align with traditional syn-
tactic structure even in adult dialog, and the two diverge even
more in child-directed and child-originated speech (Fisher &
Tokura, 1996; Gerken, 1996; Gerken, Jusczyk, & Mandel, 1994).
However, in CCG, intonation structure is united with a freer notion
of derivational structure. Consider the child in a similar situation to
the above, who hears example (1) from Section 2.1 ‘‘You like the
doggies!” with the intonation contour discussed by Fisher herself
and by Steedman (1996a)19:
‘‘Mum supposes the properties the dogs afford to be common
ground, Mum makes it common ground it’s what I like.”
Fisher points out that the L intermediate phrase boundary that
she observed after the verb makes the intonation structure incon-
sistent with standard assumptions about English NP-VP surface
constituency. However, CCG embodies a more permissive notion
of constituency, allowing for constituents such as ‘‘you like”, con-
sistent with the intonation structure above. As the derivation
requires the use of the forward composition rule, indicated as
> B, the intonation contour provides positive evidence for applying
the composition rule to the ﬁrst two categories, supporting syntac-
tic learning from prosodic cues. Far from being inconsistent with
the syntax of English, the intonation structure is aligned to the
notion of constituent structure that the child will later ﬁnd essen-
tial to the analysis of relative clause structures like the following:
(10) [The [doggie [that [you like]S/NP]NnN]N]NP
Thus, the child can acquire the relative construction without
needing to call on a distinct and otherwise unmotivated mecha-
nism of movement. This strong homomorphism between interpre-
tation and syntactic constituency motivates the use of CCG as the
model of grammar in a theory of language acquisition.
It is also worth noting that even the content that is expressed in
our input meaning representations is inevitably more shallow and
English-speciﬁc than we would like, even though it abstracts away
from English-speciﬁc word order. For example, comparison with
other Germanic languages suggests that the logical form that the
child brings to (9) may be something more like give(pleasure
(you),dogs), so that the lexical entry for ‘‘like” of type ðe; ðe; tÞÞ is
the following, exhibiting the same ‘‘quirky” relation between
(structural) nominative case and an underlying dative role that Ice-
landic exhibits morphologically for the corresponding verb:
(11) like ‘ ðSnNPÞ=NP : kxky:giveðpleasureðyÞ; xÞ
However, dealing with these further issues is beyond the scope
of the present investigation, because of the small amount of data
we have to work with, and the restricted nature of the annotation
available to us.
19 This derivation crucially involves type-raising of the subject You. Type raising
simply exchanges the roles of function and argument, making the subject apply to the
predicate to give the same result. Although, in the interests of simpliﬁcation, we have
glossed over the point in the preceding discussion of the learner, all NPs are always
type-raised in this sense by a lexical process analogous to morphological case, or what
the linguists call ‘‘structural” case, so the object is also type-raised. Thus, this example
involves the CCG equivalent of nominative and accusative case. The responsibility for
deciding which structural case is relevant lies with the parsing model, rather than the
grammar.
O. Abend et al. / Cognition 164 (2017) 116–143
137

5. Conclusion
This paper has presented, to our knowledge, the ﬁrst model of
child language acquisition that jointly learns both word meanings
and syntax and is evaluated on naturalistic child-directed sen-
tences paired with structured representations of their meaning.
We have demonstrated that the model reproduces several impor-
tant characteristics of child language acquisition, including rapid
learning of word order, an acceleration in vocabulary learning,
one-shot learning of novel words, and syntactic bootstrapping
effects.
Taken together, these results support our basic claim that syn-
tax is learned by semantic bootstrapping from contextually avail-
able
semantic
interpretations
using
a
statistical
learning
mechanism that operates over a universally available set of gram-
matical possibilities. This learning mechanism gradually shifts
probability mass towards frequent candidates, allowing the child
to rapidly acquire an (approximately) correct grammar even in
the face of competing ambiguous alternative meanings. Altogether,
the model offers a uniﬁed account of several behavioral phenom-
ena, demonstrating in particular that both syntactic bootstrapping
effects and changes in the pace of learning emerge naturally from
the kind of statistical semantic bootstrapping model presented
here—no additional mechanisms are needed.
Acknowledgements
We thank Julia Hockenmaier and Mark Johnson for guidance,
and Inbal Arnon, Jennifer Culbertson, and Ida Szubert for their
feedback on a draft of this article.
Appendix A. The probabilistic model
Recall the probabilistic model, deﬁning a family of distributions
over ðs; m; tÞ triplets consisting of an utterance, meaning represen-
tations for the leaves, and a derivation tree. The model is an
instance of a Probabilistic Context-free Grammar (PCFG), and
decomposes over the derivation steps (or expansions). Speciﬁcally,
let w1; . . . ; wn be the words in the sentence s, m1; . . . ; mn be their
meaning representations and c1; . . . ; cn be their syntactic categories
(the syntactic categories at the leaves of t). The probability of an
ðs; m; tÞ triplet is:
Pðs; m; tÞ ¼
Y
a!c2t
Pðc j aÞ 
Y
n
i¼1
Pðmi j ciÞ 
Y
n
i¼1
Pðwi j miÞ
ð10Þ
Deﬁning the model consists of deﬁning the distribution of the
different expansions: the distribution Pðc j aÞ over each syntactic
expansion c (consisting of one or more nonterminal syntactic cat-
egories) given its parent nonterminal category a; the distribution
Pðmi j ciÞ over each leaf meaning representation given its (terminal)
syntactic category; and the distribution Pðwi j miÞ over each word
given its meaning representation. We refer to all these cases collec-
tively as expansions of the parent category (a; ci or mi). The distribu-
tions of the expansions are deﬁned using Dirichlet Processes (DPs).
A DP can serve as the prior over a discrete distribution with inﬁnite
support, and (as noted in Section 2.3) yields the following posterior
distribution over expansions:
Pðc j aÞ ¼ na!c þ asynHa
na þ asyn
ð11Þ
where na!c is the number of times the expansion a ! c has been
used previously and na is the number of times any expansion
headed by a has been used (in fact, since we never actually observe
any parses, the algorithm we describe in Appendix C deﬁnes na!c
and na as the expectations of these numbers). The model has two
hyperparameters: the base distribution Ha
and concentration
parameter aa, discussed below. We use here the notation for syntac-
tic expansions, but the same equation applies to the expansion of a
category into meaning representation c ! m or meaning represen-
tation into wordform m ! w.
A Dirichlet Process is therefore deﬁned by the concentration
parameter and base distribution (which are stipulated in this
work), and the sufﬁcient statistics na!b for every a; b, which are
estimated from the training data. As only a ﬁnite number of ða; bÞ
pairs are non-zero at any given point of the inference procedure,
we store the pseudo-counts for these pairs explicitly. The following
subsections
deﬁne
the
base
distributions
and
concentration
parameters used in each part of the model.
A.1. Syntactic expansions
First we deﬁne the base distribution of syntactic expansions.
We begin by deﬁning an auxiliary distribution over the space of
syntactic categories, i.e., the set of all categories that can be deﬁned
using brackets and slashes over the set of atomic categories. For a
category c, we deﬁne nslashes as the number of slash symbols in c
(e.g., nslashes of an atomic category is 0 and nslashesðSnNP=NPÞ is 2).
Then:
PrcatsðcÞ / 0:2nslashesðcÞþ1
ð12Þ
While this does not deﬁne a proper distribution over all syntac-
tic categories of any length (as RcPrcatsðcÞ is unbounded), it can be
normalized if we assume the vocabulary and formula length are
bounded, resulting in a well-deﬁned distribution.
The possible expansions from a syntactic category X depend on
its identity. We ﬁrst deﬁne nopðXÞ as the number of possible CCG
inverted operators that can apply to X. Any category X can expand
using right and left inverted application, resulting in ðX=Y; YÞ and
ðY; XnYÞ. If X is complex (e.g., X ¼ X1=X2), it can also be expanded
through inverted composition (e.g., to X1=Y and Y=X2). In general,
if X is complex and takes its ﬁrst k arguments to its right (left), it
can be expanded through right (left) inverted composition of
orders 1; . . . ; k. nop can be deﬁned accordingly. For each of the
allowable operators, the expansion is fully determined by the iden-
tity of the introduced category (Y in the examples above). In addi-
tion, a category may always be expanded into a lexical item by
generating the symbol XLEX. Ha for a syntactic category a is then
deﬁned as:
HaðbÞ /
0:5  CðbÞ
b ¼ aLEX
PrcatsðYÞ
2nop
b is a syntactic expansion; introducing Y
(
ð13Þ
CðbÞ equals 1 if nslashesðbÞ < 2, equals 4
3 if b’s innermost slashes are in
the same direction (both arguments to the right or both to the left,
as in X=Y=Z), and equals 2
3 otherwise (innermost slashes in different
directions, as in X=YnZ). This term corrects the base distribution to
be uniform over the six possible constituent orders for a transitive
verb after the exclusion of the two cases where the verb attaches
ﬁrst to the ﬁrst semantic argument (see Section 2.2).
A.2. Meaning representations
The distribution of the expansion of leaf categories into mean-
ing representations Pðmi j ciÞ is done in two steps. First, ci generates
a shell-meaning representation, which does not contain any con-
stants but instead has (typed) placeholders. Another generation
step is then used to expand the shells into fully-ﬂedged meaning
representations. For instance, to generate the meaning representa-
138
O. Abend et al. / Cognition 164 (2017) 116–143

tion mi ¼ kx:cookieðxÞ, the corresponding leaf ci ﬁrst generates
msh
i ¼ kx:PðxÞ, and then generates mi given msh
i .
The reason for performing this two-step derivation is to allow
the model to learn a disposition towards sub-classes of logical
forms. For instance, assume the learner is exposed to a Verb-
Subject-Object language, and correctly learns that the category
S=NP=NP is more likely than the other transitive categories. Now
assume the learner faces a transitive clause, in which none of the
words are known w1w2w3 : pðarg1; arg2Þ. The model will correctly
assign high probability to analyses where w1 corresponds to p,
but without the additional ci ! msh will be unable to determine
whether w2 corresponds to arg1 and w3 to arg2 or the other way
around. Recall that the VSO and VOS derivations differ only in
the meaning representation assigned to w1 (Section 2.2). Speciﬁ-
cally, VSO involves the expansion kxky:pðx; yÞ ! w1 and VOS the
expansion kykx:pðx; yÞ ! w1. Both of these derivations have not
been previously observed and are symmetrical. A single derivation
step would therefore assign them equal probabilities, regardless of
whether other learned transitive verbs have VSO or VOS lexical
entries.
Using the intermediate shell generation step addresses this prob-
lem. If the observed data better attests for VSO than VOS verbs, the
derivation
S=NP=NP ! kxky:PLACE HOLDERðx; yÞ
would
gain
a
higher probability mass than S=NP=NP ! kykx:PLACE HOLDERðx; yÞ.
Consequently, the VSO interpretation for w1 would be preferred over
the VOS.
Both derivation steps ci ! msh
i
and msh
i ! mi are modelled as
DPs.20 The base distribution in both cases is similarly deﬁned21:
HsemðmÞ / e#symþ2#vars
ð14Þ
where #sym is the number of constants, logical operators and quan-
tiﬁers appearing in m, and #vars is the number of variable types
(not instances) appearing in m. Again this is an improper distribu-
tion, but can be normalized if we assume the vocabulary and for-
mula length are bounded.
The inclusion of shell logical forms in the derivation process
serves an additional purpose when inferring meaning representa-
tions for sentences s that contain unknown words. While accurate
inference is impossible in this case, as the model has no informa-
tion what logical constants correspond to the unknown words, it
is able to infer the most likely logical form, where for unknown
words wi; msh is inferred instead of mi. This may result in a meaning
representation for the sentence that contains place-holders instead
of some of the logical constants, but is otherwise accurate.
A.3. Wordforms
The base distribution of the generation of words from logical
types mi ! wi is deﬁned as follows:
HwordðwÞ ¼ 0:002jwj
ð15Þ
where jwj is the number of characters in w.
A.4. Concentration parameters
The concentration parameters we use are asyn ¼ 1 for the syn-
tactic expansions, ash ¼ 1000 for the expansions of syntactic cate-
gories to shells, am ¼ 500 for the expansions of shells to meaning
representations, and aw ¼ 1 for the expansion of meaning repre-
sentations to words.
Appendix B. The space of possible derivations
The set of allowed parses t is deﬁned by the function T,
mapping pairs of strings and meaning representations ðs; mÞ into
a set of possible derivations. Here we review the splitting proce-
dure of Kwiatkowski et al. (2010) that is used to generate CCG
lexical items and describe how it is used by T to create a
packed chart representation of all parses t that are consistent
with ðs; mÞ.
The splitting procedure takes as input a CCG category X : h,
such as NP : aðx; cookieðxÞÞ, and returns a set of category splits.
Each category split is a pair of CCG categories ðCl : ml; Cr : mrÞ
(ordered by their linear order) that can be recombined to give
X : h using one of the CCG combinators in Section 2.2. The CCG
category splitting procedure has two parts: logical splitting of
the category semantics h; and syntactic splitting of the syntactic
category X. Each logical split of h is a pair of lambda expressions
ðf ; gÞ in the following set:
fðf; gÞ j h ¼ fðgÞ _ h ¼ kx:fðgðxÞÞg;
ð16Þ
which means that f and g can be recombined using either function
application or function composition to give the original lambda
expression h. A number of further linguistically motivated con-
straints on possible splits apply, such as an ‘‘Across the Board” con-
straint that says that abstraction over a term A must apply to all
instances of A, and an ‘‘A over A” constraint that says you only con-
sider the topmost term of type A for abstraction, and not any terms
of the same type A that it may contain (cf. Ross, 1986). These are sim-
ilar to the constraints described in Kwiatkowski (2012, 116–118). An
example split of the lambda expression h ¼ aðx; cookieðxÞÞ is the pair
ðky:aðx; yðxÞÞ; kx:cookieðxÞÞ;
ð17Þ
where ky:aðx; yðxÞÞ applied to kx:cookieðxÞ returns the original
expression aðx; cookieðxÞÞ.
If f and g can be recombined to give h with function application,
and X is the syntactic category of h, then X is split by a reversal of
the CCG application combinators in Section 2.2, yielding the fol-
lowing set of possible categories for f and g:
fðX=Y:f
Y:gÞ; ðY:g
XnY:fÞg
ð18Þ
where X : h and h = f(g). Similarly, if f and g can be recombined to
give h with function composition and if X = X1 = X2, then X is split
by a reversal of the CCG composition combinator, yielding:
fðX1=Y:f
Y=X2:gÞg
ð19Þ
where X : h and h = kx.f(g(x)). Analogous inverted combinators
apply for the case of X ¼ X1nX2 and higher-order inverted composi-
tion combinators.
The categories that are introduced as a result of a split (Y) are
labelled via a functional mapping cat from semantic type T to syn-
tactic category:
catðTÞ ¼
AtomicðTÞ
if T 2 Fig:D1
catðT1Þ=catðT2Þ
if T ¼ hT1; T2i
catðT1ÞncatðT2Þ
if T ¼ hT1; T2i
8
>
<
>
:
9
>
=
>
;
which uses the Atomic function illustrated in Fig. D1 to map
semantic-type to basic CCG syntactic category.
20 We use the same DP for ci’s that only differ in the slash direction. For instance,
ci ¼ S=NP and ci ¼ SnNP share a distribution of Pð j ciÞ. This was done to prevent an
artefact where unknown meaning representations generated from an infrequent
category receive disproportionally large probability mass relative to their probability
for being generated from a frequent category. This modiﬁcation introduces an
additional simplifying independence assumption (namely that the distribution of
meaning representations does not depend on the directionality of the corresponding
leaf syntactic category), which we intend to review in future work.
21 Much of the space over which the base distribution is deﬁned cannot be attested
for due to type constraints. For instance, where the leaf category is of type ci ¼ NP, the
resulting meaning representation must be of type e. Nevertheless, we use this base
distribution uniformly for its simplicity.
O. Abend et al. / Cognition 164 (2017) 116–143
139

As an example, the logical split in (17) supports two CCG cate-
gory splits, one for each of the CCG application rules.
ðNP=N : ky:aðx; yðxÞÞ; N : kx:cookieðxÞÞ
ð20Þ
ðN : kx:cookieðxÞ; NPnN : ky:aðx; yðxÞÞÞ
ð21Þ
The parse generation algorithm T uses the function split to
generate all CCG category pairs that are an allowed split of an input
category X : h:
fðCl : ml; Cr : mrÞg ¼ splitðX : hÞ;
ð22Þ
and then packs a chart representation of t in a top-down fashion
starting with a single cell entry Cm : m for the top node shared by
all parses t. T cycles over all cell entries in increasingly small spans
and populates the chart with their splits. For any cell entry X : h
spanning more than one word T generates a set of pairs represent-
ing the splits of X : h. For each split ðCl : ml; Cr : mrÞ and every binary
partition ðwi:k; wk:jÞ of the word-span T creates two new cell entries
in the chart: ðCl : mlÞi:k and ðCr : mrÞk:j.
Algorithm 1. Generating t with T.
Input: Sentence ½w1; . . . ; wn, top node Cm : m
Output: Packed parse chart Ch containing t
Ch ¼ ½½fg1; . . . ; fgn1; . . . ; ½fg1; . . . ; fgnn
Ch½1½n  1 ¼ Cm : m
for i ¼ n; . . . ; 2;
j ¼ 1 . . . ðn  iÞ þ 1 do
for X : h 2 Ch½j½i do
for ðCl : ml; Cr : mrÞ 2 splitðX : hÞ do
for k ¼ 1; . . . ; i  1 do
Ch½j½k  Cl : ml
Ch½j þ k½i  k  Cr : mr
Algorithm 1 shows how the learner uses T to generate a packed
chart representation of t in the chart Ch. The function T massively
overgenerates parses for any given natural language. The proba-
bilistic parsing model (see Appendix A) is used to choose the best
parse from the overgenerated set.
Appendix C. The learning algorithm
The parameters of the model are estimated incrementally based
on one example at a time. Each example is a pair ðs; mÞ of an input
sentence s and a set of possible meanings m. The algorithm esti-
mates the posterior distribution Pðt; H j s; mÞ: the joint distribution
over possible derivations t of s and the model parameters H.
Since estimating this distribution exactly is intractable, we use
a mean ﬁeld approximation, considering a simpler family of
distributions Qðt; HÞ such that Q decomposes as follows:
Qðt; HÞ ¼ QðtÞQðHÞ
We may now approximate D ¼ Pðt; H j s; mÞ by ﬁnding the distribu-
tion Qðt; HÞ that minimizes the KL divergence from D using the
Variational Bayes EM algorithm (VBEM). We adapt existing tech-
niques for online VBEM (Beal, 2003; Hoffman, Blei, & Bach, 2010;
Sato, 2001).
The
algorithm
iteratively
performs
two
steps.
Given
an
instance ðs; mÞ, the ﬁrst step (‘‘oVBE-step”) creates a packed for-
est representation t of the space of possible derivations that
yield any of the observed sentence-meaning pairs (see Appendix
B for details). This packed representation is used to estimate
the expected number of times the expansion a ! c appears in
the derivation space for the current example: Et½a ! c. (As
above, we use the notation for syntactic expansions, but the
algorithm also considers the expansion of a category into mean-
ing representation c ! m or meaning representation into word-
form m ! w.) The second step (‘‘oVBM-step”) then computes
the estimated number of times that a ! c appeared in any
derivation of any of the observed examples. These estimates
correspond to the pseudo-counts na!c that deﬁne the Dirichlet
Process deﬁning the distribution over the expansions of a. The
computation is done by extrapolating the previous value of
na!c with Et½a ! c. In short, the oVBE step estimates the distri-
bution over latent derivations t given a (sentence, possible
meanings) pair, while the oVBM step estimates the parameters
of the model, represented as pseudo-counts na!c, given that
distribution.
The full algorithm is presented in Algorithm 2. For each s and
possible meaning representation m0 2 m it uses the function T
to generate a set of consistent parses t0.
In the parameter update step, the training algorithm updates
the pseudo-counts associated with each of the expansions a ! c
that have been observed in any of the derivations generated for
this example.
We follow Hoffman et al. (2010) in using an online update
schedule that knows, prior to training, the number of training
instances that will be seen N. This is a small contravention of the
conditions required for the algorithm to be truly online. In practice,
the value of N is used by the algorithm to weight the effects of the
prior against those of the observed data and need not exactly
match the amount of data to be seen. We further follow Hoffman
et al. (2010) in introducing a learning rate gi ¼ ð50 þ iÞ0:8, where
i is the iteration number. The more gradual learning rate, discussed
in Section 3.3, is gi ¼ ð50 þ iÞ0:6.22
Fig. D1. Atomic syntactic categories and their corresponding semantic types.
22 For brevity, some of the ﬁgures in Section 3.3 are only presented for the 4 and 6
distractor settings. For transitives with the more gradual rate, the correct SVO
category is only favored by a small margin over the alternatives, which is reﬂected in
the CPP for individual transitives and in the CPP of the nonce transitive after a single
exposure: they are both low in absolute terms. In all other experiments and settings,
the model behaves similarly as in the 4 and 6 distractor settings.
140
O. Abend et al. / Cognition 164 (2017) 116–143

Algorithm 2. Learning the lexicon and the model parameters.
Input: Corpus D ¼ fðsi; miÞji ¼ 1; . . . ; Ng, Function T,
Semantics to syntactic category mapping cat.
Output: Model Parameters fna!cg
for i ¼ 1; . . . ; N do
ti ¼ fg
for m0 2 mi do
Cm0 ¼ catðm0Þ
ti ¼ ti [ Tðsi; Cm0 : m0Þ
for a ! c 2 ti do
oVBE-step: Compute Eti½a ! c
oVBM-step: Update na!c  ð1 giÞna!c þgi  N  Eti½a ! b
Appendix D. Examples of sentence-meaning pairs used in the
simulation experiments
Below are examples of sentence-meaning pairs taken from the
converted dependency-annotated Eve corpus used in the simula-
tion experiments. Predicate symbols and constants are written in
the form POSjaf , where POS is the symbol’s part of speech, and f
are additional features, where applicable (often inﬂectional fea-
tures). However, for the purposes of this work, all symbols are
viewed as non-decomposable. The parts of speech determine the
semantic types, which are here omitted for brevity.
you go: : kev:vjgoðprojyou; evÞ
you get a fly: : kev:vjgetðprojyou; det jaðe; njflyðeÞÞ; evÞ
who are you calling ? :
ke:kev:auxjbePRESENTðpartjcallPROGRESSIVEðprojyou; e; evÞ; evÞ
you read about the choochoo: :
kev:vjreadZEROðprojyou; evÞ
^prepjaboutðdet jtheðe; njchoochooðeÞÞ; evÞ
References
Abend, O., Reichart, R., & Rappoport, A. (2010). Improved unsupervised POS
induction through prototype discovery. In Acl (pp. 1298–1307).
Alishahi, A., & Chrupała, G. (2012). Concurrent acquisition of word meaning and
lexical categories. In Proceedings of the 2012 joint conference on empirical
methods in natural language processing and computational natural language
learning (pp. 643–654). Jeju Island.
Alishahi, A., Fazly, A., & Stevenson, S. (2008). Fast mapping in word learning: What
probabilities tell us. In Proceedings of the twelfth conference on computational
natural language learning (pp. 57–64).
Alishahi, A., & Stevenson, S. (2008). A computational model of early argument
structure acquisition. Cognitive Science, 32, 789–834.
Alishahi, A., & Stevenson, S. (2010). A computational model of learning semantic
roles from child-directed language. Language and Cognitive Processes, 25(1),
50–93.
Allen, J., & Seidenberg, M. S. (1999). The emergence of grammaticality in
connectionist networks. The Emergence of Language, 115–151.
Ambati, B. R., Deoskar, T., & Steedman, M. (2017). Hindi CCGbank: A CCG treebank
from the Hindi dependency treebank. Language Resources and Evaluation, 1–34.
Ambridge, B., Pine, J., & Lieven, E. (2014). Child language acquisition: Why universal
grammar doesn’t help. Language, 90, e53–e90.
Angluin, D. (1980). Inductive inference of formal languages from positive data.
Information and Control, 45, 117–135.
Artzi, Y., Das, D., & Petrov, S. (2014). Learning compact lexicons for CCG semantic
parsing. In Proceedings of the 2014 conference on empirical methods in natural
language processing (pp. 1273–1283).
Atkinson, R., Bower, G., & Crothers, E. (1965). Introduction to mathematical learning
theory. Wiley.
Auli, M., & Lopez, A. (2011). A comparison of loopy belief propagation and dual
decomposition for integrated CCG supertagging and parsing. In Proceedings of
the 49th annual meeting of the Association for Computational Linguistics: Human
language technologies (pp. 470–480). Portland, OR: ACL.
Baldridge, J. (2002). Lexically speciﬁed derivational control in Combinatory Categorial
Grammar (Unpublished doctoral dissertation). University of Edinburgh.
Barak, L., Fazly, A., & Stevenson, S. (2013). Modeling the emergence of an exemplar
verb in construction learning. In Proceedings of the 35th annual conference of the
Cognitive Science Society. Berlin.
Beal, M. J. (2003). Variational algorithms for approximate Bayesian inference
(Unpublished doctoral dissertation). University of London.
Becker, M. (2005). Raising, control, and the subset principle. In Proceedings of the
24th West Coast conference on formal linguistics (pp. 52–60). Somerville, MA:
Cascadilla Proceedings Project.
Beekhuizen, B. (2015). Constructions emerging: A usage-based model of the acquisition
of grammar (Unpublished doctoral dissertation). Leiden University.
Beekhuizen, B., Bod, R., Fazly, A., Stevenson, S., & Verhagen, A. (2014). A usage-based
model of early grammatical development. In Proceedings of the ACL workshop on
cognitive modeling and computational linguistics. Baltimore, MD.
Berwick, R. (1985). The acquisition of syntactic knowledge. Cambridge, MA: MIT Press.
Boersma, P., & Hayes, B. (2001). Empirical tests of the gradual learning algorithm.
Linguistic Inquiry, 32(1), 45–86.
Bolinger, D. (1965). Forms of English. Cambridge, MA: Harvard University Press.
Bowerman, M. (1973). Structural relationships in children’s utterances: Syntactic or
semantic?InCognitivedevelopmentandtheacquisitionoflanguage.AcademicPress.
Braine, M. (1992). What sort of innate structure is needed to bootstrap into syntax?
Cognition, 45, 77–100.
Bresnan, J. (Ed.). (1982). The mental representation of grammatical relations.
Cambridge, MA: MIT Press.
Bresnan, J., & Nikitina, T. (2003). On the gradience of the dative alternation.
Unpublished manuscript. Stanford University.
Brown, R. (1973). A ﬁrst language: The early stages. Cambridge, MA: Harvard
University Press.
Brown, R., & Bellugi, U. (1964). Three processes in the child’s acquisition of syntax.
In E. Lenneberg (Ed.), New directions in the study of language (pp. 131–161).
Cambridge, MA: MIT Press.
Buttery, P. (2006). Computational models for ﬁrst language acquisition (Unpublished
doctoral dissertation). University of Cambridge.
Calhoun, S. (2010). The centrality of metrical structure in signaling information
structure: A probabilistic perspective. Language, 86, 1–42.
Calhoun,
S.
(2012).
The
theme/rheme
distinction:
Accent
type
or
relative
prominence? Journal of Phonetics, 40, 329–349.
Calhoun, S., Carletta, J., Brenier, J., Mayo, N., Jurafsky, D., & Steedman, M. (2010). The
NXT-format Switchboard corpus: A rich resource for investigating the syntax,
semantics,
pragmatics,
and
prosody
of
dialog.
Language
Resources
and
Evaluation, 44, 387–419.
Cauvet, E., Limissuri, R., Millotte, S., Skoruppa, K., Cabrol, D., & Christophe, A. (2014).
Function words constrain on-line recognition of verbs and nouns in French 18-
month-olds. Language Learning and Development, 10(1), 1–18.
Çakıcı, R. (2005). Automatic induction of a CCG grammar for Turkish. In Proceedings
of the student workshop, 43rd annual meeting of the ACL, Ann Arbor, MI
(pp. 73–78). ACL.
Chang, N. C.-L. (2008). Constructing grammar: A computational model of the
emergence of early constructions. ProQuest.
Charniak, E. (1997). Statistical parsing with a context-free grammar and word
statistics. In Proceedings of the 14th national conference of the American
association for artiﬁcial intelligence, Providence, RI., july (pp. 598–603).
Chomsky, N. (1965). Aspects of the theory of syntax. Cambridge, MA: MIT Press.
Chomsky, N. (1981). Lectures on government and binding. Dordrecht: Foris.
Chomsky, N. (1995). The minimalist program. Cambridge, MA: MIT Press.
Christodoulopoulos, C., Goldwater, S., & Steedman, M. (2010). Two decades of
unsupervised POS tagging—How far have we come? In Proceedings of the
conference on empirical methods in natural language processing (pp. 575–584).
ACL.
Chrupała, G., Kádár, Á., & Alishahi, A. (2015). Learning language through pictures. In
Proceedings of the 53nd annual meeting of the Association for Computational
Linguistics (pp. 112–118).
Clark, E. (1973). What’s in a word? On the child’s acquisition of semantics in his ﬁrst
language. In T. Moore (Ed.), Cognitive development and the acquisition of language
(pp. 65–110). Academic Press.
Clark, S., & Curran, J. R. (2004). Parsing the WSJ using CCG and log-linear models. In
Proceedings of the 42nd annual meeting of the Association for Computational
Linguistics (pp. 104–111). Barcelona, Spain: ACL.
Cohn, T., Blunsom, P., & Goldwater, S. (2010). Inducing tree-substitution grammars.
The Journal of Machine Learning Research, 11, 3053–3096.
Collins, M. (1997). Three generative lexicalized models for statistical parsing. In
Proceedings of the 35th annual meeting of the Association for Computational
Linguistics (pp. 16–23). Madrid: ACL.
Connor, M., Fisher, C., & Roth, D. (2012). Starting from scratch in semantic role
labeling: Early indirect supervision. In Cognitive aspects of computational
language acquisition (pp. 257–296). Springer.
Cook, P., & Bildhauer, F. (2011). Annotating information structure: The case of topic.
In Beyond semantics: Corpus based investigations of pragmatic and discourse
phenomena (pp. 45–56). Ruhr Universität, Bochum: Bochumer Linguistische
Arbeitsberichte.
Crain, S., & Nakayama, M. (1987). Structure dependence in grammar formation.
Language, 522–543.
Crocker, M. W., & Keller, F. (2005). Probabilistic grammars as models of gradience in
language processing. In Gradience in grammar: Generative perspectives. Oxford,
UK: Oxford University Press.
Croft, W. (2001). Radical construction grammar: Syntactic theory in typological
perspective. Oxford: Oxford University Press.
O. Abend et al. / Cognition 164 (2017) 116–143
141

Culbertson, J., Smolensky, P., & Wilson, C. (2013). Cognitive biases, linguistic
universals, and constraint-based grammar learning. Topics in Cognitive Science,
5, 392–424.
Dominey, P. F., & Boucher, J.-D. (2005). Learning to talk about events from narrated
video in a construction grammar framework. Artiﬁcial Intelligence, 167(1),
31–61.
Elman, J., Bates, E., Johnson, M. H., Karmiloff-Smith, A., Parisi, D., & Plunkett, K.
(1996). Rethinking innateness: A connectionist perspective on development.
Cambridge, MA: MIT Press.
Fazly, A., Alishahi, A., & Stevenson, S. (2010). A probabilistic computational model of
cross-situational word learning. Cognitive Science, 34, 1017–1063.
Feldman, N. H., Grifﬁths, T. L., Goldwater, S., & Morgan, J. L. (2013). A role for the
developing lexicon in phone tic category acquisition. Psychological Review, 120
(4), 751–778.
Fernald, A. (1993). Approval and disapproval: Infant responsiveness to vocal affect
in familiar and unfamiliar languages. Child Development, 64, 657–667.
Fernald, A., Taeschner, T., Dunn, J., Papousek, M., Boysson-Bardies, B., & Fukui, I.
(1989). A cross-language study of pros odic modiﬁcations in mothers’ and
fathers’ speech to infants. Journal of Child Language, 16, 477–501.
Fisher, C., Gertner, Y., Scott, R., & Yuan, S. (2010). Syntactic bootstrapping. Wiley
Interdisciplinary Reviews: Cognitive Science, 1, 143–149.
Fisher, C., Hall, G., Rakowitz, S., & Gleitman, L. (1994). When it is better to receive
than to give: Syntactic and conceptual constraints on vocabulary growth.
Lingua, 92, 333–375.
Fisher, C., Klingler, S. L., & Song, H.-j. (2006). What does syntax say about space? 2-
year-olds use sentence structure to learn new prepositions. Cognition, 101(1),
B19–B29.
Fisher, C., & Tokura, H. (1996). Prosody in speech to infants: Direct and indirect
acoustic cues to syntactic structure. In J. Morgan & K. Demuth (Eds.), Signal to
syntax: Bootstrapping from speech to grammar in early acquisition (pp. 343–363).
Erlbaum.
Fodor, J. A., Bever, T., & Garrett, M. (1974). The psychology of language. New York:
McGraw-Hill.
Fodor, J. D. (1998). Unambiguous triggers. Linguistic Inquiry, 29, 1–36.
Fodor, J. D., & Sakas, W. (2005). The subset principle in syntax: Costs of compliance.
Journal of Linguistics, 41, 513–569.
Frank, M., Goodman, N., & Tenenbaum, J. (2009). Using speakers’ referential
intentions to model early cross-situational word learning. Psychological Science,
20, 578–585.
Frank, S., Feldman, N., & Goldwater, S. (2014). Weak semantic context helps
phonetic learning in a model of infant language acquisition. In Proceedings of the
52nd annual meeting of the association of computational linguistics.
Furth, H. (1961). The inﬂuence of language on the development of concept
formation in deaf children. Journal of Abnormal and Social Psychology, 63,
386–389.
Gazdar, G., Klein, E., Pullum, G. K., & Sag, I. (1985). Generalized phrase structure
grammar. Oxford: Blackwell.
Gentner, D. (1982). Why nouns are learned before verbs: Linguistic relativity versus
natural partitioning. In S. Kuczaj
(Ed.). Language development (Vol. 2,
pp. 301–334). Hillsdale, NJ: Erlbaum.
Gentner, D., & Boroditsky, L. (2001). Individuation, relativity, and early word
learning. In M. Bowerman & S. Levinson (Eds.) (pp. 215–256). Cambridge:
Cambridge University Press.
Gerken, L. (1996). Prosodic structure in young children’s language production.
Langauge, 72, 683–712.
Gerken, L., Jusczyk, P., & Mandel, D. (1994). When prosody fails to cue syntactic
structure. Cognition, 51, 237–265.
Gertner, Y., Fisher, C., & Eisengart, J. (2006). Learning words and rules: abstract
knowledge of word order in early sentence comprehension. Psychological
Science, 17(8), 684–691.
Gibson, E., & Wexler, K. (1994). Triggers. Linguistic Inquiry, 25, 355–407.
Gleitman, L. (1990). The structural sources of verb meanings. Language Acquisition,
1, 1–55.
Gleitman, L., Cassidy, K., Nappa, R., Papafragou, A., & Trueswell, J. C. (2005). Hard
words. Language Learning and Development, 1, 23–64.
Göksun,
T.,
Küntay,
A.
C.,
&
Naigles,
L.
R.
(2008).
Turkish
children
use
morphosyntactic bootstrapping in interpreting verb meaning. Journal of Child
Language, 35, 291–323.
Goldwater, S., Grifﬁths, T., & Johnson, M. (2009). A Bayesian framework for word
segmentation: Exploring the effects of context. Cognition, 112, 21–54.
Goldwater, S., & Johnson, M. (2003). Learning OT constraint rankings using a
maximum entropy model. In Proceedings of the workshop on variation within
Optimality Theory (pp. 113–122). Stockholm University.
Gómez, R., & Maye, J. (2005). The developmental trajectory of nonadjacent
dependency learning. Infancy, 7(2), 183–206.
Grifﬁths, T., & Tenenbaum, J. (2005). Structure and strength in causal induction.
Cognitive Psychology, 51, 334–384.
Grifﬁths, T., & Tenenbaum, J. (2006). Optimal predictions in everyday cognition.
Psychological Science, 17, 767–773.
Grimshaw, J. (1981). Form, function and the language acquisition device. In L. Baker
& J. McCarthy (Eds.). The logical problem of language acquisition (pp. 165–182).
Cambridge, MA: MIT Press.
Hale, J. (2001). A probabilistic Earley parser as a psycholinguistic model. In
Proceedings of the 2nd meeting of the North American chapter of the Association for
Computational Linguistics (pp. 159–166). Pittsburgh, PA.
Halliday, M. (1967). Intonation and grammar in British English. The Hague: Mouton.
Hayes, B., & Wilson, C. (2008). A maximum entropy model of phonotactics and
phonotactic learning. Linguistic Inquiry, 39(3), 379–440.
Hockenmaier, J. (2003). Parsing with generative models of predicate-argument
structure. In Proceedings of the 41st meeting of the Association for Computational
Linguistics, Sapporo (pp. 359–366). San Francisco: Morgan-Kaufmann.
Hockenmaier, J., & Steedman, M. (2002). Generative models for statistical parsing
with Combinatory Categorial Grammar. In Proceedings of the 40th meeting of the
Association for Computational Linguistics (pp. 335–342). Philadelphia.
Hoffman,
M.,
Blei,
D.,
&
Bach,
F.
(2010).
Online
learning
for
latent
Dirichlet allocation. Advances in Neural Information Processing Systems, 23,
856–864.
Hyams, N. (1986). Language acquisition and the theory of parameters. Dordrecht:
Reidel.
Johnson, M., & Goldwater, S. (2009). Improving nonparametric Bayesian inference:
Experiments on unsupervised word segmentation with adaptor grammars. In
Proceedings of human language technologies: The 2009 annual conference of the
north American chapter of the Association for Computational Linguistics.
Jones, B. K. (2015). Learning words and syntactic cues in highly ambiguous contexts
(Unpublished doctoral dissertation). University of Edinburgh.
Joshi, A., & Schabes, Y. (1997). Tree-adjoining grammars. In G. Rozenberg & A.
Salomaa
(Eds.). Handbook of formal languages (Vol. 3, pp. 69–124). Berlin:
Springer.
Jurafsky, D. (1996). A probabilistic model of lexical and syntactic access and
disambiguation. Cognitive Science, 20(2), 137–194.
Klein, D., & Manning, C. D. (2004). Corpus-based induction of syntactic structure:
Models of dependency and constituency. In Proceedings of the 42nd annual
meeting of the Association for Computational Linguistics (pp. 479–486). Barcelona:
ACL.
Klein, D., & Manning, C. D. (2005). Natural language grammar induction with a
generative constituent-context model. Pattern Recognition, 38, 1407–1419.
Klein, E., & Sag, I. A. (1985). Type-driven translation. Linguistics and Philosophy, 8,
163–2 01.
Krishnamurthy, J., & Mitchell, T. (2014). Joint syntactic and semantic parsing with
combinatory categorial grammar. In Proceedings of the 52nd annual meeting of
the Association for Computational Linguistics (Vol. 1: Long papers, pp. 1188–
1198). Baltimore, MD.
Kwiatkowski,
T.
(2012).
Probabilistic
grammar
induction
from
sentences
and
structured
meanings
(Unpublished
doctoral
dissertation).
University
of
Edinburgh.
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., & Steedman, M. (2012). A
probabilistic model of syntactic and semantic acquisition from child-directed
utterances and their meanings. In Proceedings of the 13th conference of the
european chapter of the ACL (EACL 2012) (pp. 234–244). Avignon: ACL.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2010). Inducing
probabilistic CCG grammars from logical form with higher-order uniﬁcation. In
Proceedings of the conference on empirical methods in natural language processing
(pp. 1223–1233). Cambridge, MA: ACL.
Kwiatkowski, T., Zettlemoyer, L., Goldwater, S., & Steedman, M. (2011). Lexical
generalization in CCG grammar induction for semantic parsing. In Proceedings of
the
conference
on
empirical
methods
in
natural
language
processing
(pp. 1512–1523). Edinburgh: ACL.
Ladd, D. R. (1996). Intonational phonology (2nd edition revised 2008). Cambridge:
Cambridge University Press.
Landau, B., & Gleitman, L. (1985). Language and experience: Evidence from the blind
child. Cambridge, MA: Harvard University Press.
Levy, R.
(2008). Expectation-based
syntactic comprehension. Cognition, 106,
1126–1177.
Levy, R., Reali, F., & Grifﬁths, T. L. (2009). Modeling the effects of memory on human
online sentence processing with particle ﬁlters. In Proceedings of the 22nd
conference on neural information processing systems (NIPS).
Lewis, M., & Steedman, M. (2014). A⁄ CCG parsing with a supertag-factored model.
In Proceedings of the conference on empirical methods in natural language
processing (pp. 990–1000). Doha, Qatar: ACL.
MacWhinney, B. (2000). The CHILDES project: Tools for analyzing talk. Mahwah, NJ:
Erlbaum.
Matuszek, C., Fitzgerald, N., Zettlemoyer, L., Bo, L., & Fox, D. (2012). A joint model of
language and perception for grounded attribute learning. In Proceedings of the
29th international conference on machine learning (ICML).
Maurits, L., Perfors, A., & Navarro, D. (2009). Joint acquisition of word order and
word reference. In Proceedings of the 31st annual conference of the Cognitive
Science Society (pp. 1728–1733).
McMurray, B., Horst, J. S., & Samuelson, L. K. (2012). Word learning emerges from
the interaction of online referent selection and slow associative learning.
Psychological Review, 119(4), 831.
Mellish, C. (1989). Some chart-based techniques for parsing ill-formed input. In
Proceedings of the 27th annual meeting of the Association for Computational
Linguistics (pp. 102–109).
Mikolov, T., Karaﬁát, M., Burget, L., Cernocky`, J., & Khudanpur, S. (2010). Recurrent
neural network based language model. In Interspeech 2010, 11th annual
conference of the international speech communication association (pp. 1045–1048).
Mikolov, T., Yih, W.-t., & Zweig, G. (2013). Linguistic regularities in continuous space
word representations. In Proceedings of the 2013 conference of the North
American chapter of the Association for Computational Linguistics: Human
language technologies (pp. 746–751). Atlanta: ACL.
Mintz, T. (2003). Frequent frames as a cue for grammatical categories in child
directed speech. Cognition, 90, 91–117.
142
O. Abend et al. / Cognition 164 (2017) 116–143

Morris, W. C., Cottrell, G. W., & Elman, J. (2000). A connectionist simulation of the
empirical acquisition of grammatical relations. In Hybrid neural systems
(pp. 175–193). Springer.
Niyogi,
P.
(2006).
Computational
nature
of
language
learning
and
evolution.
Cambridge, MA: MIT Press.
Niyogi, P., & Berwick, R. (1996). A language learning model for ﬁnite parameter
spaces. Cognition, 61, 161–193.
Niyogi, S. (2002). Bayesian learning at the syntax-semantics interface. In Proceedings
of the 24th annual conference of the Cognitive Science Society (Vol. 36, pp. 58–63).
Oléron, P. (1953). Conceptual thinking of the deaf. American Annals of the Deaf, 98,
304–310.
Pearl, L., Goldwater, S., & Steyvers, M. (2010). How ideal are we? Incorporating
human limitations into Bayesian models of word segmentation. In Proceedings
of the 34th annual Boston University conference on child language development.
Somerville, MA: Cascadilla Press.
Perfors, A., Tenenbaum, J., & Regier, T. (2011). The learnability of abstract syntactic
principles. Cognition, 118, 306–338.
Perfors, A., Tenenbaum, J., & Wonnacott, E. (2010). Variability, negative evidence,
and the acquisition of verb argument constructions. Journal of Child Language,
37, 607–642.
Phillips, L., & Pearl, L. (2014). Bayesian inference as a cross-linguistic word
segmentation strategy: Always learning useful things. In Proceedings of the
computational and cognitive models of language acquisition and language
processing workshop at EACL.
Pierrehumbert, J., & Hirschberg, J. (1990). The meaning of intonational contours in
the interpretation of discourse. In P. Cohen, J. Morgan, & M. Pollack (Eds.),
Intentions in communication (pp. 271–312). Cambridge, MA: MIT Press.
Pinker, S. (1979). Formal models of language learning. Cognition, 7, 217–283.
Plunkett, K., Sinha, C., Møller, M. F., & Strandsby, O. (1992). Symbol grounding or the
emergence of symbols? Vocabulary growth in children and a connectionist net.
Connection Science, 4(3–4), 293–312.
Pollard, C., & Sag, I. (1994). Head driven phrase structure grammar. Stanford, CA: CSLI
Publications.
Redington, M., Chater, N., & Finch, S. (1998). Distributional information: A powerful
cue for acquiring syntactic categories. Cognitive Science, 22, 425–469.
Regier, T. (2005). The emergence of words: Attentional learning in form and
meaning. Cognitive Science, 29(6), 819–865.
Reznick, J. S., & Goldﬁeld, B. A. (1992). Rapid change in lexical development in
comprehension and production. Developmental Psychology, 28(3), 406.
Ross, J. R. (1986). Inﬁnite syntax. Norton, NJ: Ablex.
Saffran, J., Aslin, R., & Newport, E. (1996). Statistical learning by 8-month-old
infants. Science, 274, 1926–1928.
Sagae, K., Davis, E., Lavie, A., MacWhinney, B., & Wintner, S. (2010). Morphosyntactic
annotation of CHILDES transcripts. Journal of Child Language, 37, 705–729.
Sakas, W., & Fodor, J. D. (2001). The structural triggers learner. In S. Bertolo (Ed.),
Language acquisition and learnability (pp. 172–233). Cambridge: Cambridge
University Press.
Sanborn, A. (2017). Types of approximation for probabilistic cognition: Sampling
and variational. Brain and Cognition, 112, 98–101.
Sanborn, A., Grifﬁths, T., & Navarro, D. (2010). Rational approximations to rational
models: Alternative algorithms for category learning. Psychological Review, 117,
1144–1167.
Sato, M.-A. (2001). Online model selection based on the variational Bayes. Neural
Computation, 13(7), 1649–1681.
Schlesinger, I. (1971). Production of utterances and language acquisition. In D.
Slobin (Ed.), The ontogenesis of grammar (pp. 63–101). New York: Academic
Press.
Shi, L., Grifﬁths, T., Feldman, N., & Sanborn, A. (2010). Exemplar models as a
mechanism for performing Bayesian inference. Psychonomic Bulletin & Review,
17(4), 443–464.
Siskind, J. (1992). Naive physics, event perception, lexical semantics, and language
acquisition (Unpublished doctoral dissertation). MIT.
Siskind, J. (1996). A computational study of cross-situational techniques for
learning word-to-meaning mappings. Cognition, 61, 39–91.
Snedeker, J., & Gleitman, L. (2004). Why it is hard to label our concepts. In G. Hall &
S. Waxman (Eds.), Weaving a lexicon (pp. 257–294). MIT Press.
Sorace, A., & Keller, F. (2005). Gradience in linguistic data. Lingua, 115, 1497–1524.
Steedman, M. (1996a). The role of prosody and semantics in the acquisition of
syntax. In J. Morgan & K. Demuth (Eds.), Signal to syntax (pp. 331–342).
Hillsdale, NJ: Erlbaum.
Steedman, M. (1996b). Surface structure and interpretation. Cambridge, MA: MIT
Press.
Steedman, M. (2000). The syntactic process. Cambridge, MA: MIT Press.
Steedman, M. (2012). Taking scope: The natural semantics of quantiﬁers. Cambridge,
MA: MIT Press.
Steedman, M. (2014). The surface-compositional semantics of English intonation.
Language, 90, 2–57.
Sundermeyer, M., Schlüter, R., & Ney, H. (2012). LSTM neural networks for language
modeling. In Proceedings of interspeech (pp. 194–197).
Thomforde, E., & Steedman, M. (2011). Semi-supervised CCG lexicon extension. In
Proceedings of the conference on empirical methods in natural language processing
(pp. 1246–1256). ACL.
Thompson, C., & Mooney, R. (2003). Acquiring word-meaning mappings for natural
language interfaces. Journal of Artiﬁcial Intelligence Research, 18, 1–44.
Thornton, R., & Tesan, G. (2007). Categorical acquisition: Parameter setting in
Universal Grammar. Biolinguistics, 1, 49–98.
Tomasello, M. (1992). First verbs: A case study in early grammatical development.
Cambridge: Cambridge University Press.
Tomasello, M. (1999). The cultural origins of human cognition. Cambridge, MA:
Harvard University Press.
Tomasello, M., & Farrar, M. (1986). Joint attention and early language. Child
Development, 1454–1463.
Trueswell, J., & Gleitman, L. (2007). Learning to parse and its implications for
language acquisition. In G. Gaskell (Ed.), Oxford handbook of psycholinguistics
(pp. 635–656). Oxford: Oxford University Press.
Turney, P., & Pantel, P. (2010). From frequency to meaning: Vector space models of
semantics. Journal of Artiﬁcial Intelligence Research, 37, 141–188.
Ural, A. E., Yuret, D., Ketrez, F. N., Koçbas, D., & Küntay, A. C. (2009). Morphological
cues vs. number of nominals in learning verb types in Turkish: The syntactic
bootstrapping mechanism revisited. Language and Cognitive Processes, 24,
1393–1405.
Villavicencio, A. (2002). The acquisition of a uniﬁcation-based generalised categorial
grammar (Unpublished doctoral dissertation). University of Cambridge.
Vygotsky, L. (1934/1986). Thought and language (A. Kozulin, Trans.). Cambridge, MA:
MIT Press.
Wexler, K., & Culicover, P. (1980). Formal principles of language acquisition.
Cambridge, MA: MIT Press.
Xu, F., & Tenenbaum, J. B. (2007). Word learning as Bayesian inference. Psychological
Review, 114(2), 245.
Yang, C. (2002). Knowledge and learning in natural language. Oxford: Oxford
University Press.
Yang, C. (2006). The inﬁnite gift. New York: Scribner.
Yu, C. (2006). Learning syntax–semantics mappings to bootstrap word learning. In
Proceedings of the 28th annual conference of the Cognitive Science Society (pp.
924–929).
Yu, C., & Ballard, D. H. (2007). A uniﬁed model of early word learning: Integrating
statistical and social cues. Neurocomputing, 70, 2149–2165.
Yu, C., & Smith, L. B. (2012). Embodied attention and word learning by toddlers.
Cognition, 125(2), 244–262.
Yu, C., & Smith, L. B. (2013). Joint attention without gaze following: Human infants
and their parents coordinate visual attention to objects through eye-hand
coordination. PloS One, 8(11), e79659.
Yu, H., & Siskind, J. (2013). Grounded language learning from video described with
sentences. In Proceedings of the 51st annual meeting of the Association for
Computational Linguistics (pp. 53–63). Soﬁa.
Yurovsky, D., Smith, L. B., & Yu, C. (2013). Statistical word learning at scale: The
baby’s view is better. Developmental Science, 16(6), 959–966.
Zettlemoyer, L., & Collins, M. (2005). Learning to map sentences to logical form:
Structured classiﬁcation with Probabilistic Categorial Grammars. In Proceedings
of the 21st conference on uncertainty in AI (UAI) (pp. 658–666). Edinburgh: AAAI.
Zettlemoyer, L., & Collins, M. (2007). Online learning of relaxed CCG grammars for
parsing to logical form. In Proceedings of the joint conference on empirical
methods in natural language processing and computational natural language
learning (pp. 678–687). Prague: ACL.
O. Abend et al. / Cognition 164 (2017) 116–143
143

