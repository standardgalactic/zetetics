Understanding Self-Predictive Learning for Reinforcement Learning
Yunhao Tang 1 Zhaohan Daniel Guo 1 Pierre Harvey Richemond 1 Bernardo ´Avila Pires 1 Yash Chandak 2
R´emi Munos 1 Mark Rowland 1 Mohammad Gheshlaghi Azar 1 Charline Le Lan 3 Clare Lyle 1
Andr´as Gy¨orgy 1 Shantanu Thakoor 1 Will Dabney 1 Bilal Piot 1 Daniele Calandriello 1 Michal Valko 1
Abstract
We study the learning dynamics of self-predictive
learning for reinforcement learning, a family of al-
gorithms that learn representations by minimizing
the prediction error of their own future latent rep-
resentations. Despite its recent empirical success,
such algorithms have an apparent defect: trivial
representations (such as constants) minimize the
prediction error, yet it is obviously undesirable to
converge to such solutions. Our central insight is
that careful designs of the optimization dynamics
are critical to learning meaningful representations.
We identify that a faster paced optimization of
the predictor and semi-gradient updates on the
representation, are crucial to preventing the rep-
resentation collapse. Then in an idealized setup,
we show self-predictive learning dynamics carries
out spectral decomposition on the state transition
matrix, effectively capturing information of the
transition dynamics. Building on the theoretical
insights, we propose bidirectional self-predictive
learning, a novel self-predictive algorithm that
learns two representations simultaneously. We
examine the robustness of our theoretical insights
with a number of small-scale experiments and
showcase the promise of the novel representation
learning algorithm with large-scale experiments.
1. Introduction
Self-prediction is one of the fundamental concepts in re-
inforcement learning (RL). In value-based RL, temporal
difference (TD) learning (Sutton, 1988) uses the value func-
tion prediction at the next time step as the prediction target
for the current time step V (xt) ←R(xt) + γV (xt+1), a
procedure also known as bootstrapping. We can understand
TD-learning as self-prediction specialized to value learning,
where the value function makes predictions about targets
1DeepMind 2Stanford University 3University of Oxford. Corre-
spondence to: Yunhao Tang <robintyh@deepmind.com>.
Preliminary work.
constructed from itself.
Recently, the idea of self-prediction has been extended
to representation learning with much empirical suc-
cess (Schwarzer et al., 2021; Guo et al., 2020; 2022). In self-
predictive learning, the aim is to learn a representation Φ
jointly with a transition function P which models the transi-
tion of representations in the latent space Φ(xt) →Φ(xt+1),
by minimizing the prediction error
∥P (Φ(xt)) −Φ(xt+1)∥2
2 .
Intuitively, minimizing the prediction error should encour-
age the algorithm to learn a compressed latent representation
Φ(x) of the state x. However, despite the intuitive construct
of the prediction error, there is no obvious theoretical jus-
tiﬁcation why minimizing the error leads to meaningful
representations at all. Indeed, the trivial solution Φ(xt) ≡c
for any constant vector c minimizes the error but retains
no information at all. Comparing self-predictive learning
with value learning, the key difference lies in that the value
function is grounded in the immediate reward R(xt). In
contrast, the prediction error that motivates self-predictive
learning is apparently not grounded in concrete quantities
in the environment.
A number of natural questions ensue: how do we reconcile
the apparent defect of the prediction error objective, with
the empirical success of practical algorithms built on such
an objective? What are the representations obtained by self-
predictive learning, and are they useful for downstream RL?
With obvious theory-practice conﬂicts in place, it is difﬁcult
to establish self-predictive learning as a principled approach
to representation learning in general.
We present the ﬁrst attempt at understanding self-predictive
learning for RL, through a theoretical lens. In an idealized
setting, we identify key elements to ensure that the self-
predictive algorithm avoids collapse and learns meaningful
representations. We make the following theoretical and
algorithmic contributions.
Key algorithmic elements to prevent collapse.
We iden-
tify two key algorithmic components: (1) the two time-scale
optimization of the transition function P and representation
arXiv:2212.03319v1  [cs.LG]  6 Dec 2022

Understanding Self-Predictive Learning for Reinforcement Learning
Φ; and (2) the semi-gradient update on Φ, to ensure that the
representation maintains its capacity throughout learning
(Section 3). As a result, self-predictive learning dynamics
does not converge to trivial solutions starting from random
initializations.
Self-prediction as spectral decomposition.
With a few
idealized assumptions in place, we show that the learning
dynamics locally improves upon a trace objective that char-
acterizes the information that the representations capture
about the transition dynamics (Section 3). Maximizing this
objective corresponds to spectral decomposition on the state
transition matrix. This provides a partial theoretical under-
standing as to why self-predictive learning proves highly
useful in practice.
Bidirectional self-predictive learning.
Based on the the-
oretical insights, we derive a novel self-predictive learn-
ing algorithm: bidirectional self-predictive learning (Sec-
tion 5). The new algorithm learns two representations si-
multaneously, based on both a forward prediction and a
backward prediction. Bidrectional self-predictive learning
enjoys more general theoretical guarantees compared to self-
predictive learning, and obtains more consistent and stable
performance as we validate both on tabular and deep RL
experiments (Section 7).
2. Background
Consider a reward-free Markov decision process (MDP)
represented as the tuple (X, A, p, γ) where X is a ﬁnite
state space, A the ﬁnite action space, p : X × A →P(X)
the transition kernel and γ ∈[0, 1) the discount factor. Let
π : X →P(A) be a ﬁxed policy. For convenience, let
P π : X →P(X) be the state transition kernel induced
by the policy π. We focus on reward-free MDPs instead of
regular MDPs because we do not need reward functions for
the rest of the discussion.
Throughout, we assume tabular state representation where
each state x ∈X is equivalently encoded as a one-hot vector
x ∈RX . This representation will be critical in establish-
ing results that follow. In general, a representation matrix
Φ ∈R|X|×k embeds each state x ∈X as a k-dimensional
real vector ΦT x ∈Rk. In practice, we tend to have k ≪|X|
where |X| is the cardinal of X. The representation is gener-
ally shaped by learning signals such as TD-learning or aux-
iliary objectives. Good representations should entail sharing
information between states, and facilitate downstream tasks
such as policy evaluation or control.
2.1. Self-predictive learning
We introduce a mathematical framework for analyz-
ing self-predictive learning, which seeks to capture the
Figure 1. A diagram that outlines the conceptual components of
self-predictive learning. The black arrow indicates sampling the
transition y ∼P π(·|x). the blue arrow indicates algorithmic
components of self-predictive learning: predicting the next state
representation ΦT y from the ﬁrst state representation ΦT x using
prediction matrix P. In practice as shown in Equation (2), self-
predictive learning stops the gradient on the prediction target.
high level properties of its various algorithmic instanti-
ations (Schwarzer et al., 2021; Guo et al., 2020; 2022).
Throughout, assume we have access to state tuples x, y ∈X
sampled sequentially as follows,
x ∼d, y ∼P π(·|x),
where we use x ∼d to denote sampling the state from a
distribution deﬁned by the probability vector d ∈R|X|. To
model the transition in the representation space ΦT x →
ΦT y, we deﬁne P ∈Rk×k as the latent prediction ma-
trix. The predicted latent at the next time step from ΦT x is
P T ΦT x. The goal is to minimize the reconstruction loss in
the latent space,
min
Φ,P L(Φ, P) :=Ex∼d,y∼P π(·|x)
hP T ΦT x −ΦT y
2
2
i
.
(1)
As alluded to earlier, naively optimizing Equation (1) may
lead to trivial solutions such as Φ∗= 0, which also produces
the optimal objective L(Φ∗, P) = 0. We next discuss how
speciﬁc optimization procedures entail learning meaningful
representation Φ and prediction function P.
3. Understanding learning dynamics of
self-predictive learning
Assume the algorithm proceeds in discrete iterations t ≥0.
In practice, the update of Φ follows a semi-gradient update
through L(Φ, P) by stopping the gradient via the prediction
target ΦT y
Φt+1 ←Φt −η∇ΦtE
hP T
t ΦT
t x −sg
 ΦT
t y
2
2
i
,
(2)
where sg stands for stop-gradient and η > 0 is a ﬁxed
learning rate. In the expectation, x ∼d, y ∼P π(·|x) unless
otherwise stated.

Understanding Self-Predictive Learning for Reinforcement Learning
0
100
200
300
400
500
Iterations
10
8
6
4
2
Log10 abs. normalized inner product
Optimal predictor + Semi-gradient
Remove semi-gradient
Remove optimal predictor
Figure 2. Absolute value of the inner product between the two
(normalized) columns of Φ, versus the number of iterations, for
different variants of Algorithm 1. Each light curve corresponds to
one of 100 independent runs over randomly generated MDPs, and
the solid curve shows the median over runs. The experiments are
based on the discretized dynamics (Equation (2)) with a small but
ﬁnite learning rate.
3.1. Non-collapse property of self-predictive learning
We identiﬁed a key condition to ensure that the solution
does not collapse to trivial solutions, that Pt be optimized
at a faster pace than Φt. Intriguingly, this condition is com-
patible with a number of empirical observations made in
prior work (e.g., Appendix I of Grill et al., 2020 and Chen
and He, 2021 have both identiﬁed the importance of near
optimal predictors). More formally, the prediction function
Pt is computed as one optimal solution to the loss function,
ﬁxing the representation Φt,
Pt ∈arg min
P E
hP T ΦT
t x −ΦT
t y
2
2
i
.
(3)
In practice, the above assumption might be approximately
satisﬁed by the fact that the prediction function Pt is often
parameterized by a smaller neural network (e.g., an MLP
or LSTM) compared to the representation Φt (e.g., a deep
ResNet, Schwarzer et al., 2021; Guo et al., 2020; 2022), and
hence can be optimized at a faster pace even with gradient
descent. The pseudocode for such a self-predictive learning
algorithm is in Algorithm 1.
To understand the behavior of the joint updates in Equa-
tions (2) and (3), we propose to consider the behavior of
the corresponding continuous time system. Let t ≥0 be
the continuous time index, the ordinary differential equation
Algorithm 1 Self-predictive learning.
Representation matrix Φ0 ∈R|X|×k for k ≤|X|
for t = 1, 2...T do
Compute prediction matrix Pt based on Equation (3).
Update representation Φt based on Equation (2).
end for
Output ﬁnal representation ΦT .
(ODE) systems jointly for (Φt, Pt) is
Pt ∈arg min
P L(Φt, P),
˙Φt = −∇ΦtE
hP T
t ΦT
t x −sg
 ΦT
t y
2
2
i
.
(4)
Our theoretical analysis consists in understanding the behav-
ior of the above ODE system. The key result shows that the
learning dynamics in Equation (4) does not lead to collapsed
solutions.
Theorem 1. Under the dynamics in Equation (4), the co-
variance matrix ΦT
t Φt ∈Rk×k is constant over time.
The representation matrix decomposes into k representation
vectors Φt = [φ1,t · · · φk,t], where φi,t ∈RX for each
i = 1, . . . , k. Geometrically, we can visualize (φi,t)k
i=1
as forming a basis of a k-dimensional subspace of RX .
Throughout the learning process, all basis vectors rotate
in the same direction, keeping the relative angles between
basis vectors and their lengths unchanged. As a direct im-
plication of the rotation dynamics, it is not possible for
(φi,t)k
i=1 to start as different vectors but then converge to
the same vector. That is, the representation cannot collapse.
Corollary 2. Under the dynamics in Equation (4), the rep-
resentation vectors (φi,t)k
i=1 cannot converge to the same
vector if they are initialized differently.
With the non-collapse behavior established under the dy-
namics in Equation (4), we ask in hindsight what elements
of the algorithm entail such a property. In addition to the
faster paced optimization of the prediction matrix Pt, the
semi-gradient update to Φt is also indispensable. Our result
above provides a ﬁrst theoretical justiﬁcation to the “latent
bootstrapping” technique in the RL case, which has been
effective in empirical studies (Grill et al., 2020; Schwarzer
et al., 2021; Guo et al., 2020). See Section 6 for more dis-
cussions on the relation between our analysis and prior work
in the non-contrastive unsupervised learning algorithms.
We illustrate the importance of the optimality of Pt and the
semi-gradient update with an empirical study. We learned
representations with k = 2 vectors on randomly generated
MDPs using three baselines: (1) using semi-gradient up-
dates on Φt and with optimal predictors Pt, which strictly
adheres to Algorithm 1; (2) using the optimal predictors Pt

Understanding Self-Predictive Learning for Reinforcement Learning
but replacing the semi-gradient update by a full gradient on
Φt, i.e., allowing gradients to ﬂow into ΦT y; (3) using the
semi-gradient update but corrupting the optimal predictor
with some zero-mean noise at each iteration.
Figure 2 shows the absolute value of the inner product be-
tween the two (normalized) columns of Φ, also known as
the cosine similarity, versus the number of iterations.
The matrix Φ is initialized to be orthogonal, so we expect the
two columns in Φ to remain close to orthogonal throughout
the learning dynamics (Equation (2)) with a small but ﬁnite
learning rate η. Therefore, the larger values the curves take
in Fig. 2, the stronger the evidence of collapse, and indeed as
we claimed, when either semi-gradient or optimal predictor
are removed from the learning algorithm, the representation
columns start to collapse. In practice, having an optimal
predictor is a stringent requirement; we carry out more
extensive ablation study in Section 7. See Appendix G for
more details on the tabular experiments.
Non-collapse property for a general loss function.
To
make our analysis simple, we focused on the squared loss
function between the prediction P T ΦT x and target ΦT y.
We note that the non-collapse property in Theorem 6 holds
more generally for losses of the form L (Φ, P). Our result
also applies to a slightly modiﬁed variant of the cosine
similarity loss, which is more commonly used in practice
(Grill et al., 2020; Chen and He, 2021; Schwarzer et al.,
2021; Guo et al., 2022). See Appendix C for more details.
The non-collapse property shows that the covariance matrix
ΦT
t Φt, which measures the level of diversity across represen-
tation vectors (φi,t)t
i=1 is conserved. In the next section, we
will discuss the connection between the learning dynamics
and spectral decomposition on the transition matrix P π. To
facilitate the discussion, we make an idealized assumption
that the representation columns are initialized orthonormal.
Assumption 3. (Orthonormal Initialization) The repre-
sentations are initialized orthonormal: ΦT
0 Φ0 = Ik×k.
Note that the assumption is approximately valid e.g., when
entries of Φ0 are sampled i.i.d. from an isotropic distribu-
tion and properly scaled, and when the state space is large
relative to the representation dimension k ≪|X| (see, e.g.,
(Gonz´alez-Guill´en et al., 2018) as a related reference). This
is the case if the representation capacity is smaller than the
state space (e.g., small network vs. complex image observa-
tion).
3.2. Self-predictive learning as eigenvector
decomposition
For simplicity, henceforth, we assume a uniform distribu-
tion over the ﬁrst-state. This assumption is made implicitly
in a number of prior work on TD-learning or representa-
tion learning for RL (Parr et al., 2008; Song et al., 2016;
Behzadian et al., 2019; Lyle et al., 2021).
Assumption 4. (Uniform distribution) The ﬁrst-state dis-
tribution is uniform: d = |X|−11|X|.
For the rest of the paper, we always assume Assumption 3
and Assumption 4 to hold. As a result, the learning dynam-
ics in Equation (4) reduces to the following:
Pt = ΦT
t P πΦt,
˙Φt =
 I −ΦtΦT
t

P πΦt(Pt)T
(5)
We provide detailed derivations of the ODE in Appendix A.
Remarks.
As a sanity check and a special case, assume
the representation matrix is the identity Φt = I, in which
case ΦT
t x = x recovers the tabular representation. In this
case we have Pt = P π and the latent prediction recovers
the original state transition matrix.
A useful property of the above dynamical system is the
set of critical points where ˙Φt = 0. Below we provide a
characterization of such critical points.
Lemma 5. Assume P π is real diagonalizable and let
(ui)|X|
i=1 be its set of |X| distinct eigenvectors. Let CP π be
the set of critical points of Equation (5). Then CP π contains
all matrices whose columns are orthonormal, and have the
same span as a set of k eigenvectors.
Remarks on the critical points.
Lemma 5 implies that
when P π only has eigenvectors, any matrix consisting of a
subset of k eigenvectors (uij)k
j=1 (as well as any set of k
orthonormal columns with the same span) is a critical point
to the self-predictive dynamics in Equation (5). However,
this does not mean that CP π only consists of such critical
points. As a simple example, consider the transition matrix
P π =
0.1
0.9
0.9
0.1

(6)
and when k = 1, in which case Φt is a 2-d vector. In
addition to the two eigenvectors of P π, there are at least
four other non-eigenvector critical points (shown in Fig. 3).
See Appendix D for more detailed derivations. We leave
a more comprehensive study of such critical points in the
general case to future work. When P π has complex eigen-
vectors, the structure of the critical points to Equation (5)
also becomes more complicated.
Importantly, under the assumption in Lemma 5, not all crit-
ical points are equally informative. Arguably, the top k
eigenvectors of P π with the largest absolute valued eigen-
values, should contain the most information about the matrix
because they reﬂect the high variance directions in the one-
step transition. This is the motivation behind compression
algorithms such as PCA. We now show that when P π is
symmetric, intriguingly, the learning dynamics maximizes a

Understanding Self-Predictive Learning for Reinforcement Learning
bottom
eigenvector
top eigenvector
Figure 3. Critical points and local dynamics of the example MDP
in Equation (6). We consider k = 1 so representations Φt are
2-d unit vectors. There are four eigenvector critical points (light
and dark blue) and four non-eigenvector critical points (red) of the
ODE, shown on the unit circle. The black arrows show the local up-
date direction based on the ODE. Initialized near the bottom eigen-
vector, the dynamics converges to one of the four non-eigenvector
critical points and not to the top eigenvector. See Appendix D for
more detailed explanations.
trace objective that measures the variance information con-
tained in P π, and that this objective is maximized by the
top k eigenvectors.
Theorem 6. If P π is symmetric, then under Assumption 3
and learning dynamics Equation (5), the trace objective is
non-decreasing ˙f ≥0, where
f (Φt) := Trace
 ΦT
t P πΦt
T  ΦT
t P πΦt

.
If Φt /∈CP π, then ˙f > 0. Under the constraint ΦT Φ = I,
the maximizer to f(Φ) is any set of k orthonormal vectors
which span the principal subspace, i.e., with the same span
as the k eigenvectors of P π with top absolute eigenvalues.
To see that the trace objective f measures useful information
contained in P π, for now let us constrain the arguments to f
to be the set of k eigenvectors (uij)k
j=1 of P π. In this case,
f ([ui1...uik]) = Pk
j=1 λ2
ij is the sum of the corresponding
squared eigenvalues. This implies f is a useful measure on
the spectral information contained in P π.
Theorem 6 also shows that as long as Φt is not at a stationary
point contained in CP π, the trace objective f(Φt) makes
strict improvement over time. Equivalently, this means Φt
has the tendency to move towards representations with high
trace objective, e.g., subspaces of eigenvectors with high
trace objective. In other words, we can understand the
dynamics of Φt as principal subspace PCA on the transition
matrix.
Remarks on the convergence.
Thus far, there is no guar-
antee that Φt converges to the top k eigenvectors as in gen-
eral there is a chance that the dynamics converges other crit-
ical points. We revisit the simple example in Equation (6),
where in Fig. 3 we mark all critical points on the unit circle
(four eigenvector and four non-eigenvector critical points).
When initialized near the bottom eigenvector, the dynam-
ics converges to one of the non-eigenvector critical points
instead of the top eigenvector.
Nevertheless, the local improvement property of the learning
dynamics can be very valuable in large-scale environments.
We leave a more reﬁned study on the convergence properties
of self-predictive learning dynamics to future work.
Remarks on the case with non-symmetric P π.
Theo-
rem 6 is obtained under the idealized assumption that P π
is symmetric. In general, when P π is non-symmetric, the
improvement in the trace objective f(Φt) is not necessar-
ily monotonic. In fact, it is possible to ﬁnd instances of
Φt where the dynamics decreases the trace objective f. A
plausible explanation is that since the dynamics of Φt can
be understood as gradient-ascent based PCA on P π, the
PCA objective is only well deﬁned when the data matrix
P π is symmetric. Motivated by the limitation of the self-
predictive learning and its connection to PCA, we propose
a novel self-predictive algorithm with two representations.
We will introduce such a method in Section 5 and reveal
how it generalizes self-predictive learning to carrying out
SVD instead of PCA on P π.
Before moving on, we empirically assess how much impact
that the level of symmetry of P π has on the trace maximiza-
tion property. We carried out simulations on 100 randomly
generated tabular MDPs, by unrolling the exact ODE dy-
namics in Equation (4) and measured the evolution of the
trace objective ft = Trace
 (ΦT
t P πΦt)T ΦT
t P πΦt

. Fig-
ure 4 shows the ratio between the trace objective ft and the
the value of the objective for the top k eigenvectors of P π,
versus the number of training iterations t.
Fig. 4 shows that when P π is symmetric, the trace objective
smoothly improves over time, as predicted by theory. When
P π is non-symmetric, the improvement in trace objective is
not guaranteed to be monotonic and, indeed, this is the case
for some runs. However in our experiments, over time, the
objective improved by a large margin compared to initial-
ization, though not necessarily converging to the maximum
possible values. The numerical evidence shows that the
learning dynamics can still capture useful information about
the transition dynamics for certain non-symmetric MDPs. It
is, however, possible to design non-symmetric P π on which
self-predictive dynamics barely increases the trace objective
(see Section 5). Appendix G contains additional results and
more details about the experiment details.

Understanding Self-Predictive Learning for Reinforcement Learning
0
10
20
30
40
50
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
Normalized trace objective
Symmetric MDPs
Non-symmetric MDPs
Figure 4.
Ratio
between
the
trace
objective
ft
=
Trace
 (ΦT
t P πΦt)T ΦT
t P πΦt
 and the value of the objec-
tive for the top k eigenvectors of P π, versus the number of
training iterations. Each light curve corresponds to one of 100
independent runs over randomly generated MDPs, and the solid
curve shows the median over runs. The experiments are based on
the exact ODE dynamics in Equation (5).
4. Extensions of the theoretical analysis
We have chosen to analyze the learning dynamics under
arguably the simplest possible model setup. This helps elu-
cidate important features about the self-predictive learning
dynamics, but also leaves room for extensions. We discuss
a few possibilities.
Additional prediction function.
Practical algorithms
such as SPR (Schwarzer et al., 2021) and BYOL-RL (the
representation learning component of BYOL-Explore (Guo
et al., 2022)) usually employ an additional prediction func-
tion, on top of the prediction function P which models the
latent transition dynamics. In our framework, this can be
modeled as an additional prediction matrix Q ∈Rk×k with
the overall loss function as follows
E
hQT P T ΦT x −ΦT y
2
2
i
.
The roles of P and Q are different. P is meant to model
the latent transition dynamics, while Q provides extra de-
grees of freedom to match the predicted latent QT P T ΦT x
to the next state representation ΦT y. Though such a combi-
nation of Q, P seems redundant at the ﬁrst sight, the extra
ﬂexibility entailed by the additional prediction proves very
important in practice (Q is usually implemented as a MLP
on top of the output of P, which is implemented as a LSTM
(Schwarzer et al., 2021; Guo et al., 2020)). Our theoretical
result can be extended to this case by treating the composed
matrix PQ as a whole during optimization, to ensure the
non-collapse of Φ.
Multi-step and action-conditional latent prediction.
In
practice, making multi-step predictions signiﬁcantly im-
proves the performance (Schwarzer et al., 2021; Guo et al.,
2020; 2022). In our framework, this can be understood as
the loss function
Exn∼(P π)n(·|x0)
hP T ΦT x0 −ΦT xn
2
2
i
.
In this case, our result in Section 3 suggests that the self-
prediction carries out spectral decomposition on the n-step
transition (P π)n. Another important practical component is
that latent transition models are usually action-conditional.
In the one-step case, this can be understood as parameteriz-
ing multiple prediction matrices and representation matrices
(Pa, Φa)a∈A and . The loss function naturally becomes
Exn∼P π(·|x0,a),a∼π(·|x0)
hP T
a ΦT
a x0 −ΦT
a xn
2
2
i
.
The latent prediction matrix Pa and representation Φa ef-
fectively carry out spectral decomposition on the Markov
matrix P πa, which is the transition matrix of policy πa that
takes action a in all states.
Partial observability.
In many practical applications, the
environment is better modeled as a partially observable
MDP (POMDP; (Cassandra et al., 1994)). As a simpliﬁed
setup, consider at time t the agent has access to the current
history ht = (os)s≤t ∈H which consists of observations
os ∈O in past time steps. Fixing the agent’s policy π :
H →P(A), let ˜P π(h′|h) denote the distribution over the
next observed history h′ given h. Drawing direct analogy to
the MDP case, one possible loss function is
Eh′∼˜
P π(·|h)
hP T ΦT h −ΦT h′2
2
i
.
In practice, Φ ∈R|H|×k is often implemented as a recurrent
function such as LSTM (see, e.g., BYOL-RL (Guo et al.,
2020) as one possible implementation and ﬁnd its details in
Appendix E), to avoid the explosion in the size of the set of
all histories |H|. Under certain conditions, our analysis can
be extended to spectral decomposition on the history tran-
sition matrix P π(h′|h). However, given recent empirical
advances achieved by self-predictive learning algorithm in
partially observable environments (Guo et al., 2022), poten-
tially a more reﬁned analysis is valuable in better bridging
the theory-practice gap in the POMDP case.
Finite learning rate and other factors.
Our analysis and
result heavily rely on the assumption of continuous time dy-
namics. In practice, updates are carried out on discrete time

Understanding Self-Predictive Learning for Reinforcement Learning
Algorithm 2 Bidirectional self-predictive learning.
Representation matrix Φ0 ˜Φ0 ∈R|X|×k for k ≤|X|
for t = 1, 2...T do
Compute optimal forward and backward prediction
matrix (Pt, ˜Pt) based on Equation (8).
Update representations (Φt, ˜Φt) based on Equation (7).
end for
Output ﬁnal representation (ΦT , ˜ΦT ).
steps with a ﬁnite learning rate. Through experiments, we
observe that representations tend to partially collapse when
learning rates are ﬁnite, though they still manage to capture
spectral information about the transition matrix. We also
study the impact of other factors such as non-optimal pre-
diction matrix and delayed target network, see Appendix G
for such ablation study. Formalizing such results in theory
would be an interesting future direction.
5. Bidirectional self-predictive learning with
left and right representations
Thus far, we have established a few important properties of
the self-predictive learning dynamics. However, we have
alluded to the fact that self-predictive learning dynamics can
ill-behave in certain cases.
With insights derived from previous sections, we now intro-
duce a novel self-predictive learning algorithm that makes
use of two representations Φt, ˜Φt ∈R|X|×k and two latent
prediction matrices P, ˜P ∈Rk×k. We refer to Φt as the left
representation and ˜Φt the right representation, for reasons
that will be clear shortly. With Φt, we make forward predic-
tion through P, using prediction target computed from ˜Φt;
with ˜Φt, we make backward prediction through ˜P, using
prediction target computed from Φt. Both representations
follow semi-gradient updates:
Φt+1 ←Φt −η∇ΦtE
P T
t ΦT
t x −sg

˜ΦT
t y

2
2

,
˜Φt+1 ←˜Φt −η∇˜ΦtE
 ˜P T
t ˜ΦT
t y −sg
 ΦT
t x

2
2

.
(7)
Similar to the analysis before, we assume Pt, ˜Pt are op-
timally adapted to the representations, by exactly solving
the forward and backward least square prediction problems.
This is a key requirement to ensure non-collapse in the new
learning dynamics (similar to the self-predictive dynam-
ics in Equation (5)). The pseudocode for the bidirectional
self-predictive learning algorithm is in Algorithm 2.
For notational simplicity, we denote the forward and back-
ward prediction losses as Lf(Φ, P) and Lb(˜Φ, ˜P) respec-
tively. The continuous time ODE system for the joint vari-
able (Pt, ˜Pt, Φt, ˜Φt) is
Pt ∈arg min
P Lf(Φt, P),
˙Φt = −∇ΦtE
P T ΦT x −sg

˜ΦT y

2
2

,
˜Pt ∈arg min
˜
P
Lb(˜Φt, ˜P),
˙˜Φt = −∇˜ΦtE
 ˜P T ˜ΦT y −sg
 ΦT x

2
2

.
(8)
Similar to Theorem 1, the non-collapse property for both
the left and right representations follows.
Theorem 7. Under the bidirectional self-predictive learning
dynamics in Equation (8), the covariance matrices ΦT
t Φt ∈
Rk×k and ˜ΦT
t ˜Φt ∈Rk×k are both constant matrices over
time.
As before, to simplify the presentation, we make the assump-
tion that both left and right representations are initialized
orthonormal (cf. Assumption 3):
Assumption 8. (Orthonormal Initialization) The left
and right representations are both initialized orthonormal
ΦT
0 Φ0 = ˜ΦT
0 ˜Φ0 = Ik×k.
5.1. Bidirectional self-predictive learning as singular
value decomposition
In self-predictive learning, the forward prediction derives
from the fact that the forward process x →y follows from
a Markov chain. Following a similar argument, for the back-
ward prediction to be sensible, we need to ensure that the
reverse process y →x is also a Markov chain. Technically,
this means we require (P π)T to be a transition matrix too,
which models the backward transition process. Importantly,
this is a much weaker assumption than P π be symmetric,
as required by the self-predictive learning dynamics (Theo-
rem 6).
Assumption 9. P π is a doubly stochastic matrix, i.e.,
(P π)T is also a transition matrix.
Under assumptions above, the learning dynamics in Equa-
tion (8) reduces to the following set of ODEs:
Pt = ΦT
t P π ˜Φt,
˙Φt =
 I −ΦtΦT
t

P π ˜Φt(Pt)T
˜Pt = ˜ΦT
t (P π)T Φt,
˙˜Φt =

I −˜Φt ˜ΦT
t

(P π)T Φt( ˜Pt)T
(9)
Let P π = UΣV T be the singular value decomposition
(SVD) of P π, where Σ = diag(σ1, σ2...σ|X|) is a diagonal
matrix with non-negative diagonal entries. We call any i-th
column of U and V , denoted as (ui, vi), a singular vector
pair. As before, we start by examining the critical points of
the bidirectional self-predictive dynamics.

Understanding Self-Predictive Learning for Reinforcement Learning
Lemma 10. Let ˜CP π ⊂R|X|×k × R|X|×k be the set of
critical points to Equation (9). Then ˜CP π contains any pair
of matrices, whose columns are orthonormal and have the
same span as k of singular vector pairs.
Lemma 10 implies that any k pairs of SVD vectors are a
critical point to the learning dynamics. However, similar to
the self-predictive learning dynamics, not all critical points
are equally informative. We propose a SVD trace objec-
tive ˜f(Φt, ˜Φt), which measures the information contained
in k singular vector pairs. Interestingly, the bidirectional
self-predictive learning dynamics locally improves such an
objective.
Theorem 11. Under Assumption 8 and the learning dynam-
ics in Equation (9), the following SVD trace objective is
non-decreasing ˙˜f ≥0, where
˜f

Φt, ˜Φt

:= Trace

ΦT
t P π ˜Φt
T 
ΦT
t P π ˜Φt

.
If (Φt, ˜Φt) /∈˜CP π, then ˙˜f > 0. Under the constraint
ΦT Φ = ˜ΦT ˜Φ = I, the maximizer to ˜f(Φ, ˜Φ) is any two
sets of k orthonormal vectors with the same span as the k
singular vector pairs of P π with top singular values.
To verify that the SVD trace objective ˜f provides an infor-
mation measure on the representation vectors (Φt, ˜Φt), we
constrain arguments of ˜f to be the set of k singular vector
pairs (uij, vij)k
j=1 of P π, then ˜f ([ui1...uik], [vi1...vik]) =
Pk
j=1 σ2
ij is the sum of the corresponding squared singular
values. The top k singular vector pairs maximize this ob-
jective, and hence contain the most information about P π
based on this measure.
Theorem 11 shows that as long as either one of the two
representations are not at the critical points, i.e., ˙Φt ̸= 0 or
˙˜Φt ̸= 0, the SVD trace objective ˜f(Φt, ˜Φt) is being strictly
improved under the bidirectional self-predictive learning
dynamics. Equivalently, this implies the left and right rep-
resentations (Φt, ˜Φt) tend to move towards singular vector
pairs with high SVD trace objective, i.e., seeking more in-
formation about the transition dynamics.
Two representations vs. one representation.
Looking
beyond the ODE analysis, we explain why having two sepa-
rate representations are inherently important for represen-
tation learning in general. For a transition matrix P π, its
left and right singular vectors in general differ. bidirectional
self-predictive learning provides the ﬂexibility to learn both
left and right singular vectors in parallel, without having
to compromise their differences. On the other hand, a sin-
gle representation will need to interpolate between left and
right singular vectors, which may lead to non-monotonic
behavior in the trace objective as alluded to earlier.
0
20
40
60
80
100
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
Relative SVD trace objective
Bidirectional self-predictive learning
Self-predictive learning
Figure 5. Ratio between the trace objectives (ft in red for self-
predictive learning, following Equation (4); and ˜ft in blue for
bidirectional self-predictive learning, following Equation (8)) and
the value of ˜f for the top k singular vector pairs of P π, versus the
number of training iterations. Each light curve corresponds to one
of 100 independent runs over random orthonormal initializations
of Φ on the same MDP designed so that the left and right singular
vectors of P π are very different. The solid curve shows the median
over runs.
Consider a very simple transition matrix with |X| = 3 states
that illustrate the failure mode of the self-predictive learning
dynamics with a single representation,
P π =


0
1/2
1/2
0
1/2
1/2
1
0
0

.
By construction, its top left and right singular vectors differ
greatly. We simulated the self-predictive learning dynamics
(with a single representation, Equation (4)) and the bidi-
rectional self-predictive learning dynamics (Equation (8))
in an MDP with this transition matrix, and measured the
evolution of the two trace objectives, ft and ˜ft. Figure 5
shows the ratio between the trace objectives and maximum
value of ˜f obtained at the top k singular vector pairs, ver-
sus the number of training iterations t. The bidirectional
self-predictive learning improves the objective steadily over
time. In contrast, the single representation dynamics mostly
halt at the initialized value, due to the limited capacity of
one representation to combine two highly distinct singular
vectors. See more details in Appendix F.
In addition to the improved stability shown in the example
above, bidirectional self-predictive learning also captures
more comprehensive information about the transition dy-
namics, compared to a single representation. While it is

Understanding Self-Predictive Learning for Reinforcement Learning
known that left singular vectors of P π can approximate
value functions V π with provably low errors for general
transition matrix (Behzadian et al., 2019), it is challenging
to learn the left singular vectors as standalone objects. Bidi-
rectional self-predictive learning captures both left and right
representations at the same time, entailing a better approxi-
mation to both left and right top singular vectors. In large-
scale experiments, since bidirectional self-predictive learn-
ing consists of both forward and backward predictions, it can
provide richer signals for representation learning when com-
bined with nonlinear function approximation (Section 7).
6. Prior Work
Non-collapse mechanism of self-predictive learning dy-
namics.
Self-prediction based RL representation learning
algorithms were partly inspired from the non-contrastive
unsupervised learning algorithm (Grill et al., 2020; Chen
and He, 2021). A number of prior work attempt to un-
derstand the non-collapse learning dynamics of such al-
gorithms, through the roles of semi-gradient and regular-
ization (Tian et al., 2021), prediction head (Wen and Li,
2022) and data augmentation (Wang et al., 2021). Although
our analysis is specialized to the RL case, the non-collapse
mechanism (Theorem 6) is qualitatively different from prior
work. Such a result is potentially useful in understanding
the behavior of unsupervised learning as well.
RL representation learning via spectral decomposition.
One primary line of research in representation learning for
RL is via spectral decomposition of the transition matrix
P π or successor matrix (I −γP π)−1. These methods are
generally categorized as: (1) eigenvector-decomposition
based approach, which typically assumes symmetry or real
diagonizability of P π (Mahadevan, 2005; Machado et al.,
2018; Lyle et al., 2021); (2) SVD-based approach, which
is more generally applicable (Behzadian et al., 2019; Ren
et al., 2022) and shows theoretical beneﬁts to downstream
RL tasks. Our work draws the connections between spectral
decomposition and more empirically oriented RL algorithm,
such as SPR (Schwarzer et al., 2021), PBL (Guo et al.,
2020) and BYOL-RL (Guo et al., 2022), and is one step
in the direction of formally chracterizing high performing
representation learning algorithms.
Forward-backward representations.
Closely related
to bidirectional self-predictive learning is the forward-
backward (FB) representations (Touati and Ollivier, 2021;
Blier et al., 2021). By design, the forward representation
learns value functions and backward representation learns
visitation distributions. This design bears close connections
to the left and right singular vectors of the transition matrix
P π, which bidirectional self-predictive learning seeks to ap-
proximate. Despite the high level connection, bidirectional
self-predictive learning is purely based on self-prediction,
and hence has much simpler algorithmic design.
Algorithms for PCA and SVD with gradient-based up-
date.
The ODE systems in Equations (5) and (8) bear
close connections to ODE systems used for studying
gradient-based incremental algorithms for PCA and SVD
of empirical covariance matrices in classical unsupervised
learning. Example algorithms include Oja’s subspace algo-
rithm (Oja and Karhunen, 1985; Oja, 1992) and its exten-
sion to SVD (Diamantaras and Kung, 1996; Weingessel and
Hornik, 1997). A primary historical motivation for such
algorithms is that they entail computing top k eigenvectors
or singular vectors with incremental gradient-based updates.
This echoes with the observation we make in this paper,
that self-predictive learning dynamics can be understood
as gradient-based spectral decomposition on the transition
matrix.
7. Deep RL implementation
We considered the single representation self-predictive learn-
ing dynamics as an idealized theoretical framework that
aims to capture some essential aspects of a number of exist-
ing deep RL representation learning algorithms (Schwarzer
et al., 2021; Guo et al., 2020). Importantly, our theoretical
analysis suggests that we can get more expressive repre-
sentations by leveraging the bidirectional self-predictive
learning dynamics in Section 5.
Inspired by the theoretical discussions, we introduce the
deep bidirectional self-predictive learning algorithm for
representation learning for deep RL. We build the deep
bidirectional self-predictive learning algorithm on top of
the representation learning used in BYOL-Explore (Guo
et al., 2022). While BYOL-Explore uses the prediction
loss as a signal to drive exploration, we do not use such
exploration bonuses in this work and focus only on the
effect of representation learning.
We now provide a concise summary of how BYOL-
RL works and how it is adapted for bidirectional self-
predictive learning. In general partially observable envi-
ronments, BYOL-RL encodes a history of observations
ht = (f(os))s≤t into its latent representation Φ(ht) ∈Rk
through a convnet f : O →Rk and a LSTM. Then,
the algorithm constructs a multi-step forward prediction
p (Φ(ht), at:t+n−1)) with an open loop LSTM p : Rk ×
(A)n →Rd. This forward prediction is against a back-
up target computed at the n-step forward future time step
f(ot+n). The bidirectional self-predictive learning algo-
rithm hints at a backward latent self-prediction objective,
i.e., predicting the past latent observations based on future
representations. In a nutshell, for deep bidirectional self-
predictive learning, we implement the backward prediction

Understanding Self-Predictive Learning for Reinforcement Learning
in the same way as forward prediction, but with a reversed
time axis. See Appendix E for more technical details on
BYOL-RL and how it is adapted for backward predictions.
A related but different idea has been adopted in PBL (Guo
et al., 2019), where they propose a reverse prediction that
matches ˜f(ot+n) for some function ˜f : O →Rk against
the encoded representation Φ(ht+n). The design of PBL
is motivated by partial observability, and does not carry
out backward predictions over time. See Appendix E for
details on PBL and how it differs signiﬁcantly from deep
bidirectional self-predictive learning.
7.1. Experiments
We compare the deep bidirectional self-predictive learning
algorithm with BYOL-RL (Guo et al., 2020). BYOL-RL
is built on V-MPO (Song et al., 2020), an actor-critic algo-
rithm which shapes the representation using policy gradient,
without explicit representation learning objectives.
Our testbed is DMLab-30, a collection of 30 diverse partially
observable cognitive tasks in the 3D DeepMind Lab (Beat-
tie et al., 2016). We consider the multi-task setup where
the agent is required to solve all 30 tasks simultaneously.
Representation learning has proved highly valuable in such
a setting (Guo et al., 2019). See Fig. 11 in Appendix G
for results on how BYOL-RL signiﬁcantly improves over
baseline RL algorithm per each game.
In Fig. 6, we show the per-game improvement of deep bidi-
rectional self-predictive learning over BYOL-RL. In aggre-
gate across all 30 tasks, the two algorithms perform com-
parably as they either under-perform or over-perform each
other in a similar number of games. We measure the perfor-
mance of each task with the human normalized performance
(zi −ui)/(hi −ui) with 1 ≤i ≤30, where ui and hi are
the raw score performance of random policy and humans. A
normalized score of 1 indicates that the agent performs as
well as humans on the task. Interestingly, there are a num-
ber of tasks on which backward predictions signiﬁcantly
improve over the baseline by as much as 0.4 human nor-
malized score. This comparison shows the promise of the
deep bidirectional self-predictive learning algorithm when
combined with deep RL agents.
8. Conclusion
In this work, we have presented a new way to understand
the learning dynamics of self-predictive learning for RL.
By identifying key algorithmic components to guarantee
non-collapse, and drawing close connections between self-
prediction and spectral decomposition of the transition dy-
namics, we have provided a ﬁrst justiﬁcation to the practical
efﬁcacy of a few empirically motivated algorithms. Our
insights also naturally led to a novel representation learn-
natlab_fixed_large_map
natlab_varying_map_regrowth
rooms_keys_doors_puzzle
psychlab_arbitrary_visuomotor_mapping
skymaze_irreversible_path_hard_v2
skymaze_irreversible_path_varied_v2
language_answer_quantitative_question
explore_obstructed_goals_large
rooms_exploit_deferred_effects_train
explore_object_locations_small
language_select_located_object
language_execute_random_task
psychlab_visual_search
psychlab_sequential_comparison
explore_object_rewards_many
psychlab_continuous_recognition
explore_goal_locations_large
explore_object_rewards_few
language_select_described_object
explore_obstructed_goals_small
explore_object_locations_large
rooms_collect_good_objects_train
natlab_varying_map_randomized
rooms_watermaze
explore_goal_locations_small
rooms_select_nonmatching_object
lasertag_one_opponent_large
lasertag_one_opponent_small
lasertag_three_opponents_large
lasertag_three_opponents_small
0.2
0.1
0.0
0.1
0.2
0.3
0.4
Improvement in human normalized scores
Figure 6. Per-game improvement of bidirectional self-predictive
learning compared to baseline BYOL-RL, in terms of mean human
normalized scores averaged across 3 seeds. The scores are ob-
tained at the end of training. Bidirectional self-predictive learning
provides noticeable gains over BYOL-RL in certain cases, while it
degrades performance in some others. See Appendix G for details.
ing algorithm, which mirrors SVD-based algorithms and
enjoys more ﬂexibility than self-prediction with a single
representation. A deep RL instantiation of the algorithm
has also proved empirically comparable to state-of-the-art
performance.
Our results open up many interesting avenues for future
research. A few examples: in hindsight, many assumption
are due to the limitations of linear models; it will be of
signiﬁcant interest to study non-linear latent predictions and
representations. Another interesting direction would be to
study how self-prediction interacts with RL algorithms, such
as TD-learning or policy gradient algorithms. Lastly, our
analysis also provides insights to the unsupervised learning
case, and hopefully motivates more investigation along this
direction in the space.
Acknowledgements.
Many thanks to Jean-Bastien Grill
and Csaba Szepesv´ari for their feedback on an earlier draft
of this paper. We are appreciative of the collaborative re-
search environment within DeepMind. The experiments in
this paper were conducted using Python 3, and made heavy
use of NumPy (Harris et al., 2020), SciPy (Virtanen et al.,
2020a), Matplotlib (Hunter, 2007), Reverb (Cassirer et al.,
2021), JAX (Bradbury et al., 2018) and the DeepMind JAX

Understanding Self-Predictive Learning for Reinforcement Learning
Ecosystem (Babuschkin et al., 2020).
References
Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhu-
patiraju, Jake Bruce, Peter Buchlovsky, David Budden,
Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci,
Jonathan Godwin, Chris Jones, Ross Hemsley, Tom Hen-
nigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski,
Thomas Keck, Iurii Kemaev, Michael King, Markus
Kunesch, Lena Martens, Hamza Merzic, Vladimir Miku-
lik, Tamara Norman, John Quan, George Papamakar-
ios, Roman Ring, Francisco Ruiz, Alvaro Sanchez, Ros-
alia Schneider, Eren Sezener, Stephen Spencer, Srivat-
san Srinivasan, Luyu Wang, Wojciech Stokowiec, and
Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL
http://github.com/deepmind.
Charles Beattie, Joel Z Leibo, Denis Teplyashin, Tom Ward,
Marcus Wainwright, Heinrich K¨uttler, Andrew Lefrancq,
Simon Green, V´ıctor Vald´es, Amir Sadik, et al. Deepmind
lab. arXiv preprint arXiv:1612.03801, 2016.
Bahram Behzadian, Soheil Gharatappeh, and Marek Petrik.
Fast feature selection for linear value function approxima-
tion. In Proceedings of the International Conference on
Automated Planning and Scheduling, volume 29, pages
601–609, 2019.
L´eonard Blier, Corentin Tallec, and Yann Ollivier. Learning
successor states and goal-dependent values: A mathemat-
ical viewpoint. arXiv preprint arXiv:2101.07123, 2021.
James
Bradbury,
Roy
Frostig,
Peter
Hawkins,
Matthew James Johnson, Chris Leary, Dougal Maclaurin,
George Necula, Adam Paszke, Jake VanderPlas, Skye
Wanderman-Milne, and Qiao Zhang. JAX: composable
transformations of Python+NumPy programs, 2018.
URL http://github.com/google/jax.
Anthony R Cassandra, Leslie Pack Kaelbling, and Michael L
Littman. Acting optimally in partially observable stochas-
tic domains. In Aaai, volume 94, pages 1023–1028, 1994.
Albin Cassirer, Gabriel Barth-Maron, Eugene Brevdo,
Sabela Ramos, Toby Boyd, Thibault Sottiaux, and
Manuel Kroiss. Reverb: a framework for experience
replay. arXiv preprint arXiv:2102.04736, 2021.
Xinlei Chen and Kaiming He. Exploring simple siamese
representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition,
pages 15750–15758, 2021.
Konstantinos I Diamantaras and Sun Yuan Kung. Principal
component neural networks: theory and applications.
John Wiley & Sons, Inc., 1996.
Carlos E Gonz´alez-Guill´en, Carlos Palazuelos, and Ignacio
Villanueva. Euclidean distance between haar orthogonal
and gaussian matrices. Journal of Theoretical Probability,
31(1):93–118, 2018.
Jean-Bastien Grill, Florian Strub, Florent Altch´e, Corentin
Tallec, Pierre Richemond, Elena Buchatskaya, Carl Do-
ersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad
Gheshlaghi Azar, et al. Bootstrap your own latent-a new
approach to self-supervised learning. Advances in neural
information processing systems, 33:21271–21284, 2020.
Yijie Guo, Jongwook Choi, Marcin Moczulski, Samy Ben-
gio, Mohammad Norouzi, and Honglak Lee. Efﬁcient
exploration with self-imitation learning via trajectory-
conditioned policy. arXiv preprint arXiv:1907.10247,
2019.
Zhaohan Daniel Guo, Bernardo Avila Pires, Bilal Piot, Jean-
Bastien Grill, Florent Altch´e, R´emi Munos, and Mo-
hammad Gheshlaghi Azar. Bootstrap latent-predictive
representations for multitask reinforcement learning. In
International Conference on Machine Learning, pages
3875–3886. PMLR, 2020.
Zhaohan Daniel Guo, Shantanu Thakoor, Miruna Pˆıslar,
Bernardo Avila Pires, Florent Altch´e, Corentin Tallec,
Alaa Saade, Daniele Calandriello, Jean-Bastien Grill,
Yunhao Tang, et al. Byol-explore: Exploration by boot-
strapped prediction. arXiv preprint arXiv:2206.08332,
2022.
Charles R. Harris, K. Jarrod Millman, St´efan J van der Walt,
Ralf Gommers, Pauli Virtanen, David Cournapeau, Eric
Wieser, Julian Taylor, Sebastian Berg, Nathaniel J. Smith,
Robert Kern, Matti Picus, Stephan Hoyer, Marten H.
van Kerkwijk, Matthew Brett, Allan Haldane, Jaime
Fern´andez del R´ıo, Mark Wiebe, Pearu Peterson, Pierre
G´erard-Marchant, Kevin Sheppard, Tyler Reddy, War-
ren Weckesser, Hameer Abbasi, Christoph Gohlke, and
Travis E. Oliphant. Array programming with NumPy.
Nature, 585(7825):357–362, 2020.
John D. Hunter. Matplotlib: A 2D graphics environment.
Computing in science & engineering, 9(03):90–95, 2007.
Clare Lyle, Mark Rowland, Georg Ostrovski, and Will Dab-
ney. On the effect of auxiliary tasks on representation
dynamics. In International Conference on Artiﬁcial Intel-
ligence and Statistics, pages 1–9. PMLR, 2021.
Marlos C. Machado, Clemens Rosenbaum, Xiaoxiao Guo,
Miao Liu, Gerald Tesauro, and Murray Campbell.
Eigenoption discovery through the deep successor rep-
resentation. In International Conference on Learning
Representations, 2018. URL https://openreview.
net/forum?id=Bk8ZcAxR-.

Understanding Self-Predictive Learning for Reinforcement Learning
Sridhar Mahadevan. Proto-value functions: Developmental
reinforcement learning. In Proceedings of the 22nd inter-
national conference on Machine learning, pages 553–560,
2005.
Erkki Oja. Principal components, minor components, and
linear neural networks. Neural networks, 5(6):927–935,
1992.
Erkki Oja and Juha Karhunen. On stochastic approximation
of the eigenvectors and eigenvalues of the expectation of
a random matrix. Journal of mathematical analysis and
applications, 106(1):69–84, 1985.
Ronald Parr, Lihong Li, Gavin Taylor, Christopher Painter-
Wakeﬁeld, and Michael L Littman. An analysis of linear
models, linear value-function approximation, and feature
selection for reinforcement learning. In Proceedings of
the 25th international conference on Machine learning,
pages 752–759, 2008.
Tongzheng Ren, Tianjun Zhang, Lisa Lee, Joseph E Gon-
zalez, Dale Schuurmans, and Bo Dai. Spectral decompo-
sition representation for reinforcement learning. arXiv
preprint arXiv:2208.09515, 2022.
Max Schwarzer, Ankesh Anand, Rishab Goel, R Devon
Hjelm, Aaron Courville, and Philip Bachman.
Data-
efﬁcient reinforcement learning with self-predictive rep-
resentations. In International Conference on Learning
Representations, 2021. URL https://openreview.
net/forum?id=uCQfPZwRaUu.
H. Francis Song, Abbas Abdolmaleki, Jost Tobias Sprin-
genberg, Aidan Clark, Hubert Soyer, Jack W. Rae, Seb
Noury, Arun Ahuja, Siqi Liu, Dhruva Tirumala, Nicolas
Heess, Dan Belov, Martin Riedmiller, and Matthew M.
Botvinick. V-mpo: On-policy maximum a posteriori pol-
icy optimization for discrete and continuous control. In
International Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=SylOlp4FvH.
Zhao Song, Ronald E Parr, Xuejun Liao, and Lawrence
Carin. Linear feature encoding for reinforcement learning.
Advances in neural information processing systems, 29,
2016.
Richard S. Sutton. Learning to predict by the methods of
temporal differences. Machine learning, 3(1):9–44, 1988.
Yuandong Tian, Xinlei Chen, and Surya Ganguli. Under-
standing self-supervised learning dynamics without con-
trastive pairs. In International Conference on Machine
Learning, pages 10268–10278. PMLR, 2021.
Ahmed Touati and Yann Ollivier. Learning one representa-
tion to optimize all rewards. Advances in Neural Infor-
mation Processing Systems, 34:13–23, 2021.
Pauli Virtanen, Ralf Gommers, Travis E. Oliphant, Matt
Haberland, Tyler Reddy, David Cournapeau, Evgeni
Burovski, Pearu Peterson, Warren Weckesser, Jonathan
Bright, St´efan J. van der Walt, Matthew Brett, Joshua
Wilson, K. Jarrod Millman, Nikolay Mayorov, Andrew
R. J. Nelson, Eric Jones, Robert Kern, Eric Larson, C J
Carey, ˙Ilhan Polat, Yu Feng, Eric W. Moore, Jake Vander-
Plas, Denis Laxalde, Josef Perktold, Robert Cimrman, Ian
Henriksen, E. A. Quintero, Charles R. Harris, Anne M.
Archibald, Antˆonio H. Ribeiro, Fabian Pedregosa, Paul
van Mulbregt, and SciPy 1.0 Contributors. SciPy 1.0: Fun-
damental Algorithms for Scientiﬁc Computing in Python.
Nature Methods, 17:261–272, 2020a.
Pauli Virtanen, Ralf Gommers, Travis E Oliphant, Matt
Haberland, Tyler Reddy, David Cournapeau, Evgeni
Burovski, Pearu Peterson, Warren Weckesser, Jonathan
Bright, et al.
Scipy 1.0: fundamental algorithms for
scientiﬁc computing in python. Nature methods, 17(3):
261–272, 2020b.
Xiang Wang, Xinlei Chen, Simon S Du, and Yuandong
Tian.
Towards demystifying representation learning
with non-contrastive self-supervision.
arXiv preprint
arXiv:2110.04947, 2021.
Andreas Weingessel and Kurt Hornik.
Svd algorithms:
Apex-like versus subspace methods. Neural Processing
Letters, 5(3):177–184, 1997.
Zixin Wen and Yuanzhi Li.
The mechanism of predic-
tion head in non-contrastive self-supervised learning.
In Alice H. Oh, Alekh Agarwal, Danielle Belgrave,
and Kyunghyun Cho, editors, Advances in Neural In-
formation Processing Systems, 2022.
URL https:
//openreview.net/forum?id=d-kvI4YdNu.

Understanding Self-Predictive Learning for Reinforcement Learning
APPENDICES: Understanding Self-Predictive Learning for Reinforcement Learning
A. Detailed derivations of ODE systems
We provide a derivation of the ODE systems in Equations (5) and (9) below. We start with a few useful facts: recall that
x ∼d, y ∼P π(·|x) are one-hot encoding of states. Let D be a diagonal matrix with d its diagonal entries Dii = di, ∀1 ≤
i ≤|X|. Then, we have the following properties:
E

xxT 
= D, E

xyT 
= DP π.
A.1. Equation (5) for self-predictive learning
Starting with Equation (4), the ﬁrst-order optimality condition for Pt can be made more explicit
 ΦT
t E

xxT 
Φt

Pt = ΦT
t E

xyT 
Φt ⇒
 ΦT
t DΦt

Pt = ΦT
t DP πΦt.
We can expand the dynamics for Φt as follows,
˙Φt =

D −DΦt
 ΦT DΦt
−1 ΦT
t D

P πΦt(Pt)T .
Under Assumptions 3 and 4, the above dynamics simpliﬁes into
Pt = ΦT
t P πΦt,
˙Φt =
 I −ΦtΦT
t

P πΦt(Pt)T ,
which is the ODE system in Equation (5).
A.2. Equation (9) for bidirectional self-predictive learning
Since the bidirectional self-predictive learning dynamics introduces least square regression from y to x, we need to calculate
expectations such as E[yyT ] and E[yxT ]. In general, it is challenging to express E[yyT ] as a function of D and P π. When
D is identity (Assumption 4) and when P π is doubly-stochastic (Assumption 9), we have D as a stationary distribution of
P π and hence E[yyT ] = D and
E

yxT 
= E

(xyT )T 
=
 E[xyT ]
T = (DP π)T = (P π)T D.
From Equation (7), we can make explicit the form of the prediction matrix
 ΦT
t E

xxT 
Φt

Pt = ΦT
t E

xyT  ˜Φt ⇒
 ΦT
t DΦt

P = ΦT
t DP π ˜Φt,

˜ΦT
t E

yyT  ˜Φt

˜Pt = ˜ΦT
t E

yxT 
Φt ⇒

˜ΦT
t DΦt

˜Pt = ˜ΦT
t (P π)T DΦt,
Next, we can expand the dynamics of Φt and ˜Φt as follows
˙Φt =

D −DΦt
 ΦT DΦt
−1 ΦT
t D

P π ˜Φt(Pt)T
˙˜Φt =

D −D˜Φt

˜ΦT D˜Φt
−1 ˜ΦT
t D

D−1 (P π)T D
|
{z
}
˜
P π
Φt( ˜Pt)T .
Interestingly, ˜P π is also a Markov transition matrix that corresponds to the reverse Markov chain. Finally, plugging into
D = I (Assumption 4) and thanks to Assumption 8, we recover the dynamics in Equation (9)
˙Φt =
 I −ΦtΦT
t

P π ˜Φt(Pt)T
˙˜Φt =

I −˜Φt ˜ΦT
t

(P π)T Φt( ˜Pt)T .

Understanding Self-Predictive Learning for Reinforcement Learning
A.3. Equivalence between assumptions in deriving Equation (9)
Now we provide a discussion on the equivalence between assumptions in deriving the ODE for bidirectional self-predictive
learning dynamics. An alternative assumption to Assumption 9 is
Assumption 12. Given the sampling process x ∼d, y ∼P π(·|x), the marginal distribution over next state y is uniform.
Our claim is that given the uniformity assumption on the ﬁrst-state distribution Assumption 4, Assumption 9 and As-
sumption 12 are equivalent. To see why, given Assumption 9, it is straightforward to see that uniform distribution is a
stationary distribution to P π. Starting from the ﬁrst-state distribution, which is uniform, the next-state distribution is also
uniform, which proves the condition in Assumption 12. Now, given Assumption 12, we conclude the uniform distribution
u = |X|−11|X| is a stationary distribution to P π. By deﬁnition of the stationary distribution, this means
uT P π = u.
The above implies that each column of P π sums to 1, and so P π is doubly-stochastic (Assumption 9).
B. Proof of theoretical results
Theorem 1. Under the dynamics in Equation (4), the covariance matrix ΦT
t Φt ∈Rk×k is constant over time.
Proof. Under the dynamics in Equation (4), the prediction matrix Pt optimally minimizes the loss function L(Φt, Pt) given
the representation Φt. Let At = ΦtPt ∈R|X|×k be the matrix product. The chain rule combined with the ﬁrst-order
optimality condition on Pt implies
∇PtL(Φt, Pt) = ΦT
t ∂AtL(Φt, Pt) = 0.
(10)
On the other hand, the semi-gradient update for Φt can be written as
˙Φt = −∇ΦtEx∼d,y∼P π(·|x)
hP T
t ΦT
t x −sg
 ΦT
t y
2
2
i
= −∂AtL(Φt, Pt)(Pt)T .
Thanks to Equation (10), we have
ΦT
t ˙Φt = −ΦT
t ∂AtL(Φt, Pt)(Pt)T = 0.
Then, taking time derivative on the covariance matrix
d
dt
 ΦT
t Φt

= ˙ΦT
t Φt + ΦT
t ˙Φt =

ΦT
t ˙Φt
T
+ ΦT
t ˙Φt = 0,
which implies that the covariance matrix is constant.
Corollary 2. Under the dynamics in Equation (4), the representation vectors (φi,t)k
i=1 cannot converge to the same vector
if they are initialized differently.
Proof. Take any two representation vectors φi,t and φj,t with i ̸= j, which at initialization are different. This implies the
cosine similarity ⟨φi,0, φj,0⟩̸= 1. Since under the dynamics in Equation (4), the covariance matrix ΦT
t Φt is preserved, this
means φT
i,tφj,t, φT
i,tφ1,t and φT
i,tφj,t are all constants over time, which implies
⟨φi,t, φj,t⟩= ⟨φi,0, φj,0⟩̸= 1.
This means the two vectors cannot be aligned along the same direction for all time t ≥0.
Lemma 5. Assume P π is real diagonalizable and let (ui)|X|
i=1 be its set of |X| distinct eigenvectors. Let CP π be the set of
critical points of Equation (5). Then CP π contains all matrices whose columns are orthonormal, and have the same span as a
set of k eigenvectors.

Understanding Self-Predictive Learning for Reinforcement Learning
Proof. Without loss of generality, consider the subset of ﬁrst k right eigenvectors U = (u1...uk). Then P πU = UΛ for
some diagonal matrix Λ = diag(λ1...λk), where λi is the eigenvalue corresponding to ui.
If Φt = U, then
˙Φt = (I −UU T )P πU(Pt)T = (I −UU T )UΛU(Pt)T = 0.
Next, for any set of k orthonormal vectors with the same span as U, we can write them as U ′ = UQ for some orthogonal
matrix Q ∈Rk×k. If Φt = U ′ = UQ, then
˙Φt = (I −UQQT U)P πUQ(Pt)T = (I −UU T )UΛUQ(Pt)T = 0,
which concludes the proof.
Theorem 6. If P π is symmetric, then under Assumption 3 and learning dynamics Equation (5), the trace objective is
non-decreasing ˙f ≥0, where
f (Φt) := Trace
 ΦT
t P πΦt
T  ΦT
t P πΦt

.
If Φt /∈CP π, then ˙f > 0. Under the constraint ΦT Φ = I, the maximizer to f(Φ) is any set of k orthonormal vectors which
span the principal subspace, i.e., with the same span as the k eigenvectors of P π with top absolute eigenvalues.
Proof. We ﬁrst show that the objective is non-decreasing. We calculate
d
dtf(Φt) = 4 · Trace
 ΦT
t P πΦt
T ΦT
t P π ˙Φt

=(a) 4 · Trace
 PtΦT
t P π  I −ΦtΦT
t

P πΦtP T
t

=(b) 4 · Trace
 P πΦtP T
t
T  I −ΦtΦT
t

P πΦtP T
t

,
where (a) follows from Pt = ΦT
t P πΦt; (b) follows from the fact that P π is symmetric and as a result Pt is symmetric. Now,
let At = P πΦtP T
t and denote its column vectors as At = [a1,t...ak,t]. The above derivative rewrites as
4 ·
k
X
i=1
aT
i,t
 I −ΦtΦT
t

ai,t.
We remind that a projection matrix M satisﬁes M 2 = M and M T = M and corresponds to an orthogonal projection onto
certain subspace. Since I −ΦtΦT
t is a projection matrix, we have aT
i,t
 I −ΦtΦT
t

ai,t ≥0 for any ai,t ∈RX . Hence
d
dtf(Φt) ≥0. Now, if Φt /∈CP π, this means there exists certain columns ai,t of At such that ai,t /∈span(Φt). This means
aT
i,t
 I −ΦtΦT
t

ai,t > 0 and therefore ˙f > 0.
Finally, we examine the maximizer to f(Φ) under the constraint ΦT Φ = Ik×k. Since ΦT P πΦ is symmetric, there exists an
orthogonal matrix Q such that
QT ΦT P πΦQ = Λ,
for some diagonal matrix Λ. Note that since (ΦQ)T ΦQ = Ik×k, it is equivalent to consider the optimization problem under
a stronger constraint ΦT Φ = Ik×k and ΦT P πΦ = Λ for some diagonal matrix Λ. Therefore, the optimization problem
becomes
max
ΦT Φ=Ik×k,ΦT P πΦ=Λ
k
X
i=1
Λ2
ii.

Understanding Self-Predictive Learning for Reinforcement Learning
Let Φ = [φ1, ...φk] with column vectors φi ∈R|X| for all 1 ≤i ≤k, then we have the equivalent optimization problem
max
ΦT Φ=Ik×k,ΦT P πΦ=Λ
k
X
i=1
 φT
i P πφi
2 ≤(a)
max
ΦT Φ=Ik×k,ΦT P πΦ=Λ
k
X
i=1
∥P πφi∥2
2
=
max
ΦT Φ=Ik×k,ΦT P πΦ=Λ
k
X
i=1
φT
i (P π)T P πφi.
Here, (a) follows from the fact that φi is a unit-length vector and the application of the inequality aT b ≤∥a∥2 ∥b∥2. It is
straightforward to see that the optimal solution to the last optimization problem is the set of eigenvectors of P π with top
squared eigenvalues. Hence,
max
ΦT Φ=Ik×k
f(Φ) ≤
k
X
i=1
λ2
i .
On the other hand, the k eigenvectors of P π with top k absolute eigenvalues is a feasible solution and therefore
maxΦT Φ=Ik×k f(Φ) ≥Pk
i=1 λ2
i . The above implies maxΦT Φ=Ik×k f(Φ) = Pk
i=1 λ2
i , and the k eigenvectors of P π
with top k absolute eigenvalues is a maximizer to the constrained optimization problem. It is then also clear that any k
orthonormal vectors with the same span as the top k eigenvectors also achieves the maximum objective.
Theorem 7. Under the bidirectional self-predictive learning dynamics in Equation (8), the covariance matrices ΦT
t Φt ∈
Rk×k and ˜ΦT
t ˜Φt ∈Rk×k are both constant matrices over time.
Proof. We consider the forward and backward loss function separately. Following the arguments in the proof of Theorem 1,
we see that since Pt is computed as the optimal solution to Lf(Pt, Φt), it satisﬁes the ﬁrst-order optimality condition and as
a result, ΦT
t ˙Φt = 0. This implies ΦT
t Φt is a constant matrix over time. Applying the same set of arguments to the backward
loss function Lb(˜Φt, ˜Pt), we conclude ˜ΦT
t ˜Φt is also a constant matrix over time.
Lemma 10. Let ˜CP π ⊂R|X|×k × R|X|×k be the set of critical points to Equation (9). Then ˜CP π contains any pair of
matrices, whose columns are orthonormal and have the same span as k of singular vector pairs.
Proof. Without loss of generality, consider the subset of k top singular vector pairs U = (u1...uk), V = (v1...vk). By
construction, they satisfy the equality P πV = UΣ and (P π)T U = V Σ where Σ = diag(σ1...σk) is the diagonal matrix
with corresponding singular values.
Setting Φt = U, ˜Φt = V , we ﬁrst verify the critical conditions for ˙Φt = 0, ˙˜Φt = 0:
(I −ΦtΦT
t )P π ˜Φt(Pt)T = (I −UU T )P πV V T (P π)T U =(a) (I −UU T )UΣ2 = 0.
(I −˜Φt ˜ΦT
t ) (P π)T Φt( ˜Pt)T = (I −V V T ) (P π)T UU T P πV =(b) (I −V V T )V Σ2 = 0.
Here, (a) and (b) both follow from the property of the singular vector pairs. The above indicates that any k singular vector
pairs constitute a member of ˜CP π.
Any orthonormal vectors with the same vector span as U, V can be expressed as UQ, V R for some orthogonal matrix
Q, R ∈Rk×k. Let U ′ = UQ, V ′ = V R, we verify the critical conditions when Φt = U ′, ˜Φt = V ′,
(I −ΦtΦT
t )P π ˜Φt(Pt)T = (I −U ′(U ′)T )P πV ′(V ′)T (P π)T U ′ =(a) (I −UU T )UΣ2Q = 0.
(I −˜Φt ˜ΦT
t ) (P π)T Φt( ˜Pt)T = (I −V ′(V ′)T ) (P π)T U ′(U ′)T P πV ′ =(b) (I −V V T )V Σ2R = 0,
where (a) and (b) follow from straightforward matrix operations. We have hence veriﬁed that any orthonormal vectors with
the same vector span as any subset of k singular vector pairs constitute a critical point.
Theorem 11. Under Assumption 8 and the learning dynamics in Equation (9), the following SVD trace objective is
non-decreasing ˙˜f ≥0, where
˜f

Φt, ˜Φt

:= Trace

ΦT
t P π ˜Φt
T 
ΦT
t P π ˜Φt

.

Understanding Self-Predictive Learning for Reinforcement Learning
If (Φt, ˜Φt) /∈˜CP π, then ˙˜f > 0. Under the constraint ΦT Φ = ˜ΦT ˜Φ = I, the maximizer to ˜f(Φ, ˜Φ) is any two sets of k
orthonormal vectors with the same span as the k singular vector pairs of P π with top singular values.
Proof. We start by showing the SVD trace objective is non-decreasing on the ODE ﬂow.
d
dt
˜f

Φt, ˜Φt

= 2 · Trace

ΦT
t P π ˜Φt
T 
ΦT
t P π ˙˜Φt

+ 2 · Trace

ΦT
t P π ˜Φt
T 
˙ΦT
t P π ˜Φt

.
Examining the ﬁrst term on the right above, plugging in the dynamics for ˙˜Φ,
Trace

ΦT
t P π ˜Φt
T 
ΦT
t P π ˙˜Φt

= Trace

ΦT
t P π ˜Φt
T
ΦT
t P π 
I −˜Φt ˜ΦT
t

(P π)T Φt( ˜Pt)T

=(a) Trace

(P π)T Φt( ˜Pt)T T 
I −˜Φt ˜ΦT
t

(P π)T Φt( ˜Pt)T .

Here, (a) follows from the form of the prediction matrix ˜Pt = ˜ΦT
t (P π)T Φt. Now, deﬁne ˜At = [a1,t...ak,t] :=
(P π)T Φt( ˜Pt)T , the above rewrites as
Trace(AT
t

I −˜Φt ˜ΦT
t

At) =
k
X
i=1
aT
i,t

I −˜Φt ˜ΦT
t

ai,t.
Since

I −˜Φt ˜ΦT
t

is an orthogonal projection matrix, we conclude the above quantity is non-negative. Similarly, we can
show that the second term on the right above is also non-negative, which concludes ˙˜f ≥0.
Now, we assume (Φt, ˜Φt) /∈˜CP π. Without loss of generality, we assume in this case ˙˜Φt ̸= 0, which implies there exists
certain column i such that ai,t is not in the span of ˜Φt. This means aT
i,t

I −˜Φt ˜ΦT
t

ai,t > 0 and subsequently ˙˜f > 0.
Finally, we show that under the constraint ΦT Φ = ˜ΦT ˜Φ = I, the maximizer to ˜f(Φ, ˜Φ) is any two set of k orthonormal
vectors with the same span as the k singular vector pairs of P π with top singular values. In general, the matrix ΦT P π ˜Φ is
not diagonal. Consider its SVD
ΦT P π ˜Φ = ˜U ˜Σ ˜V T ,
then we have

Φ ˜U
T
P π ˜Φ ˜V = ˜Σ. Consider the new representation variable Φ′ = Φ ˜U and ˜Φ′ = ˜Φ ˜V and note
they also satisfy the orthonormal constraint. Under the new variable, the matrix (Φ′)T P π ˜Φ′ = ˜Σ is diagonal. Since
f(Φ, ˜Φ) = f(Φ ˜U, ˜Φ ˜V ), it is equivalent to solve the optimization problem under an additional diagonal constraint
max
ΦT Φ=˜ΦT ˜Φ=I
f(Φ, ˜Φ) =
max
ΦT Φ=˜ΦT ˜Φ=I,ΦT P π ˜Φ diagonal
f(Φ, ˜Φ)
When ΦT P π ˜Φ is diagonal, the objective rewrites as
k
X
i=1

φT
i P π ˜φi
2
≤(a)
k
X
i=1
P π ˜φi

2
2 =
k
X
i=1
˜φT
i (P π)T P π ˜φi,
where (a) follows from the fact that φi is a unit-length vector and the application of the inequality aT b ≤∥a∥2 ∥b∥2.
Now, under the constraint ˜ΦT ˜Φ = I, the right hand side is upper bounded by the sum of squared top k singular values
of P π: Pk
i=1 σ2
i . Let f ∗be the optimal objective of the original constrained problem. We have hence established
f ∗≤Pk
i=1 σ2
i . On the other hand, if we let Φ, ˜Φ to be the top k singular vector pairs, they satisfy the constraint and this
shows f ∗≥Pk
i=1 σ2
i .
In summary, we have f ∗= Pk
i=1 σ2
i and the top k singular vector pairs U, V are the maximizer. Any k orthonormal
vectors with the same span as top k singular vector pairs can be expressed as U ′ = UQ, V ′ = V R for some orthogonal
matrix Q, R ∈Rk×k. Since (U ′)T U ′ = (V ′)T V ′ = I and f(U, V ) = f(U ′, V ′) = f ∗, they are also the maximizer to the
constrained problem.

Understanding Self-Predictive Learning for Reinforcement Learning
C. Extension of non-collapse property to general loss function
Thus far, we have focused on the squared loss function for understanding the self-predictive learning dynamics,
Pt ∈arg min
P L(Φt, P),
˙Φt = −∇ΦtE
hP T
t ΦT
t x −sg
 ΦT
t y
2
2
i
.
(11)
We can extend the result to a more general class of loss function L(Φt, Pt). Such a loss function needs to be computed
as an expectation over a function F of the product prediction and representation matrix ΦtPt, used for computing the
prediction; and another argument for the representation matrix Φt, used for computing the target. More formally, we can
write L(Φt, Pt) = F (PtΦt, Φt) = E

f
 P T
t ΦT
t x, ΦT
t y

for some function f : Rk × Rk →k. For the least squared case,
we have f(a, b) = ∥a −b∥2
2. We consider the self-predictive learning dynamics with such a general loss function,
Pt ∈arg min
P F (ΦtP, Φt) ,
˙Φt = −∇ΦtF (ΦtPt, sg (Φt)) .
(12)
We show that the non-collapse property also holds for the above dynamics.
Theorem 13. Assume the general loss function L(Φ, P) is such that the minimizer to L(Φ, P) satisﬁes the ﬁrst-order
optimality condition, then under the dynamics in Equation (12), the covariance matrix ΦT
t Φt ∈Rk×k is constant over time.
Proof. The proof follows closely from the proof of Theorem 1. Let At = ΦtPt ∈R|X|×k be the matrix product, the
assumption implies
∇PtF(ΦtPt, Φt) = ΦT
t ∂AtF(ΦtPt, Φt) = 0.
(13)
On the other hand, the semi-gradient update for Φt can be written as
˙Φt = −∇ΦtF (ΦtPt, Φt) = −∂AtL(Φt, Pt)(Pt)T .
Thanks to Equation (13), we have
ΦT
t ˙Φt = −ΦT
t ∂AtF (ΦtPt, Φt) (Pt)T = 0.
Then, taking time derivative on the covariance matrix
d
dt
 ΦT
t Φt

= ˙ΦT
t Φt + ΦT
t ˙Φt =

ΦT
t ˙Φt
T
+ ΦT
t ˙Φt = 0,
which implies that the covariance matrix is constant.
Notable examples of loss functions that satisfy the above assumptions include L1 loss f(a, b) = |a −b| and the regularized
cosine similarity loss f(a, b) = −aT b/ (∥a∥2 ∥b∥2 + ϵ), where ϵ > 0 is a regularization constant.
D. Discussions on critical points to the self-predictive learning dynamics
We provide further discussions on the critical points to the self-predictive learning dynamics in Equation (5). For convenience,
we recall the set of ODEs
Pt = ΦT
t P πΦt,
˙Φt =
 I −ΦtΦT
t

P πΦt(Pt)T .
According to Lemma 5, under the assumption that P π is real diagonizable, any matrix with orthonormal columns with the
same span as a set of k eigenvectors constitutes a critical point to the ODE. Let C be the set of such matrices and recall that
CP π is the set of critical points, we have C ⊆CP π.
We now consider two symmetric transition matrices, under which we have C = CP π and C ⊊CP π respectively. In both
cases, we have |X| = 2 and k = 1 for simplicity. In this case, Φ can be expressed as a column vector Φt is a 2-dimensional
vector and the prediction matrix Pt is now a scalar.

Understanding Self-Predictive Learning for Reinforcement Learning
Case I where C = CP π.
Consider the following transition matrix
P π =

0.9
0.1
0.1
0.9

The transition matrix is symmetric and has eigenvalue λ1 = 1, λ2 = 0.8. Note that
U = [u1, u2] =
" 1
√
2
1
√
2
1
√
2
−1
√
2
#
is the matrix of eigenvectors. For convenience, we write
Φt = U
αt
βt

with coefﬁcients αt, βt ∈R. Assumption 3 dictates α2
t + β2
t = 1. Now, we can calculate
Pt = ΦT
t P πΦt = α2
t + 0.8β2
t > 0,
which is strictly positive and hence always non-zero. Letting ˙Φt = 0, since Pt ̸= 0, we conclude span(P πΦt) ⊂span(Φt).
This means Φt is invariant and must be a direct sum of subspaces spanned by eigenvectors. In other words, we have CP π ⊂C
and hence the two sets are in fact equal.
Case II where C ⊊CP π.
Consider the following transition matrix we mentioned in Equation (6)
P π =

0.1
0.9
0.9
0.1

.
The transition matrix is symmetric and has eigenvalue λ1 = 1, λ2 = −0.8. Note that
U = [u1, u2] =
" 1
√
2
1
√
2
1
√
2
−1
√
2
#
is the matrix of eigenvectors. For convenience, we write
Φt = U
αt
βt

with coefﬁcients αt, βt ∈R. Assumption 3 dictates α2
t + β2
t = 1. As before, we calculate
Pt = ΦT
t P πΦt = α2
t −0.8β2
t > 0.
Now, we can identify αt = ±
1
√
1.8, βt = ±
√
0.8
√
1.8 as four non-eigenvector critical points. Indeed, since Pt = 0, we have
˙Φt = 0. However, since this critical point is a combination of two eigenvectors u1, u2, it does not span the same subspace
as either just u1 or u2. In other words, we have found a critical point which does not belong to the set C and this implies
C ⊊CP π.
Convergence of the dynamics in Case II.
We replicate the diagram Fig. 3 here in Fig. 7, where we graph critical points
to the self-predictive learning dynamics with the above transition matrix on a unit circle. Recall that we have k = 1 so that
representations Φt are 2-d vectors. In addition to the eigenvector critical points plotted as blue dots (Lemma 5), we have also
identiﬁed other four critical points in red, corresponding to αt = ±
1
√
1.8, βt = ±
√
0.8
√
1.8 in four quadrants of the 2-d plane.
The only local update dynamics consistent with the local improvement property (Theorem 6) is shown as black arrows
on the unit circle. When the representation is initialized near the bottom eigenvector, it will converge to one of the four
non-eigenvector critical points and not the top eigenvector.

Understanding Self-Predictive Learning for Reinforcement Learning
bottom
eigenvector
top eigenvector
Figure 7. Critical points and local dynamics of the example MDP in Equation (6). We consider k = 1 so representations Φt are 2-d
vectors. There are four eigenvector critical points (light and dark blue) and four non-eigenvector critical points (red) of the ODE, shown on
the unit circle. The black arrows show the local update direction based on the ODE. Initialized near the bottom eigenvector, the dynamics
converges to one of the four non-eigenvector critical points and not to the top eigenvector.
E. Algorithmic and implementation details on BYOL-RL
We provide background details on BYOL-RL (Guo et al., 2022), a deep RL agent based on which our deep bidirectional
self-predictive learning algorithm is implemented. We start with a relatively high-level description of the algorithm.
BYOL-RL is designed to work in the POMDP setting, where the agent observes a sequence of observations over time ot ∈O.
Deﬁne history ht ∈H at time t as the combination of previous observations ht = (os)s≤t (note here the observation os can
contain action as−1). BYOL-RL adopts a few functions to represent the raw observation and history
• An observation embedding function f : O →Rd which maps the observation ot into d-dimensional embeddings. In
practice, this is implemented as a convolutional neural network.
• A recurrent embedding function g : Rk × Rd →Rk which processes the observation in a recurrent way. In practice, this
is implemented as the core output of a LSTM.
In POMDP, we can consider the history ht as a proxy to the state in the MDP case. Let Φ(ht) ∈Rk be the k-dimensional
representation of history, we use the recurrent function to embed the history recursively
Φ(ht) = g (Φ(ht−1), f(ot)) .
BYOL-RL parameterizes the latent prediction function p : Rk × (A)n →Rk, which can be understood as predicting the
history representation n-step from now on, using only intermediate action sequence at:t+n−1
p (Φ(ht), at:t+n−1) ∈Rk.
Finally, another projection function q : Rk →Rd maps the predicted history representation, into the d-dimensional
embedding space of the observation. Overall, the prediction objective is
E
h
∥q (p (Φ(ht), at:t+n−1)) −sg (f(ot+n))∥2
2
i
.
The notation sg indicates stop-gradient on the prediction target. All parameterized functions in BYOL-RL f, g, p, q, are
optimized via semi-gradient descent.
Note that there are a number of discrepancies between theory and practice, such as multi-step prediction, action-conditional
prediction and partial observability. See Section 6 for some discussions on possible extensions of the theoretical model to
the more general case. We refer readers to the original paper (Guo et al., 2022) for detailed description of the neural network
architecture and hyper-parameters.

Understanding Self-Predictive Learning for Reinforcement Learning
E.1. Details on deep bidirectional self-predictive learning with BYOL-RL
We build the deep bidirectional self-predictive learning algorithm on top of BYOL-RL. The bidirectional self-predictive
learning dynamics motivates a backward prediction loss function, which we instantiate as follows in the POMDP case.
For the backward prediction, we can instantiate an observation embedding function ˜f : O →Rd and recurrent embedding
function ˜g : Rk × Rd →Rk, analogous to the forward prediction case. We also parameterize a backward latent dynamics
function ˜p : Rk × (A)n →Rk. Finally, we parameterize a projection function ˜q : Rk →Rd. The overall backward
prediction objective is
E
˜q

˜p

˜Φ(ht+n), at:t+n−1

−sg

˜f(ot)

2
2

,
where the backward recurrent representation is computed recursively as ˜Φ(ht−1) = ˜g

˜Φ(ht), ˜f(ot−1)

. In other words,
we can understand the backward prediction problem as almost exactly mirroring the forward prediction problem.
As a design choice, we share the observation embedding in both the forward and backward process f = ˜f. The motivation
for such a design choice is that one arguably expects the observation embedding to share many common features for both the
forward and backward process, when the input observations are images; secondly, since the embedding function is usually a
much larger network compared to rest of the architecture, parameter sharing helps reduce the computational cost.
E.2. Differences from PBL (Guo et al., 2019)
PBL is a representation learning algorithm based on both forward and backward predictions. In the POMDP case, PBL
simply parameterizes a projection function ˜qpbl : Rk →Rd. The backward prediction loss is computed as
E
˜qpbl

˜f (ot+n)

−sg (Φ(ht+n))

2
2

.
In other words, the backward prediction seeks to predict the recurrent history embedding Φ(ht+n) from the observation
embedding ˜f(ot+n). By design, the backward prediction shares the same observation embedding function as the forward
prediction ˜f = f.
F. Transition matrix to illustrate the failure mode of self-predictive learning
To design examples that illustrate the failure mode of single representation learning dynamics, it is useful to review the
intuitive interpretations of left and right singular vectors of P π as clustering states with certain similar features. Left singular
vectors cluster together states with similar outgoing distribution, i.e., states with similar rows in P π. Meanwhile, right
singular vectors cluster together states with similar incoming distributions, i.e., states with similar columns in P π.
The example transition matrix with |X| = 3 states is
P π =


0
1/2
1/2
0
1/2
1/2
1
0
0


We can calculate the top-1 left and right singular vectors as
u0 =

−1
√
2, −1
√
2, 0

, ˜u0 =

0, −1
√
2, −1
√
2

,
which concides with the previous intuition that the top left singular vector should cluster together the ﬁrst two states. Indeed,
by assigning a value of −1/
√
2 to the ﬁrst two states, the top left singular vector effectively considers the ﬁrst two states as
being identical. Meanwhile, the top right singular vector should cluster together the last two states.
G. Experiment details
We provide additional details on the experimental setups in the paper.

Understanding Self-Predictive Learning for Reinforcement Learning
G.1. Tabular experiments
Throughout, tabular experiments are carried out on randomly generated MDPs. Instead of explicitly generating the MDPs,
we generate the state transition matrix P π ∈R|X×|X| where |X| = 20 by default. In general, the algorithm learns k = 2
representation columns.
Generating random P π.
To generate random doubly-stochastic transition matrix, we start by randomly initialize entries
of P to be i.i.d. Uniform(0, 1). Then we carry out column normalization and row normalization
Pij ←Pij/
X
k
Pik, Pij ←Pij/
X
k
Pkj
until convergence. It is guaranteed that P has row sum and column sum to be both 1, and is hence doubly-stochastic. The
transition matrix is computed as
P π = αP + (1 −α)Pperm,
where Pperm is a randomly generated permutation matrix and α ∼Uniform(0, 1). It is straightforward to verify P π is
doubly-stochastic. To generate symmetric transition matrix, we follow the above procedure and apply a symmetric operation
(P π + (P π)T )/2, which produces P π as a symmetric transition matrix. In Fig. 4, we generate doubly-stochastic matrices as
examples of non-symmetric matrices.
Normalized trace objective.
When plotting trace objectives, we usually calculate the normalized objective. For symmetric
matrix P π, the normalizer is the sum of top-k squared eigenvalue of P π: let λi be the eigenvalues of P π ordered such that
|λi| ≥|λi+1|, then normalizer is Pk
i=1 |λi|2. For double-stochastic matrix P π, the normalizer is the squaresum of topd
ma-kximum singular singular value: let σi be the singular values of P π, the normalizer is Pk
i=1 σ2
i . Such normalzers upper
bound the trace objective and SVD trace objective respectively.
ODE vs. discretized update.
We carry out experiments in the tabular MDPs with setups. In Figs. 4 and 5, we simulate
the exact ODE dynamics using the Scipy ODE solver (Virtanen et al., 2020b). In Figs. 2 and 10, we simluate the discretized
process using a ﬁnite learning rate η on the representation matrix Φt. This corresponds to implementing the update rule in
Equation (2). By default, in discretized updates we adopt η = 10−3 so that the non-coallpse property is almost satisﬁed.
G.1.1. COMPLETE RESULT TO FIG. 4
We present the complete result to Fig. 4 in Fig. 8, where we display the evolution of the trace objective for a total of 100
iterations in Fig. 8(a). Note that as the simulation runs longer, in the non-symmetric MDP case, we can observe very small
oscillations in the trace objective. However, overall, the trace objective improves signiﬁcantly compared to the initial values.
In Fig. 2(b), we make more clear the individual runs across different MDPs. Though in most MDPs, the trace objective is
improved signiﬁcantly given 100 iterations, there exists MDP instances where the improvement is very slow and appear
highly monotonic in the non-symmetric case. In the symmetric case, there can exist MDPs where the rate of improvement
for the trace objective is slow too.
G.1.2. EFFECT OF TARGET NETWORK
In practice, it is common to maintain a target network for constructing prediction targets for self-prediction (Guo et al.,
2019; Schwarzer et al., 2021; Guo et al., 2020). Under our framework, the prediction loss function is
˜L(Φ, P, Φ′) = Ex∼d,y∼P π(·|x)
hP T ΦT x −(Φ′)T y
2
2
i
,
where Φ′ is the target network, or target representation matrix. The target representation matrix is updated via moving
average towards the online representation matrix
d
dtΦ′
t = β(Φt −Φ′
t).

Understanding Self-Predictive Learning for Reinforcement Learning
0
20
40
60
80
100
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
Normalized trace objective
Symmetric MDPs
Non-symmetric MDPs
(a) Complete run
0
20
40
60
80
100
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
Normalized trace objective
(b) Individual runs
Figure 8. Complete result for Fig. 4 where we now show the result over a large number of iterations in (a). In (b), we make more clear the
individual runs across different MDPs.
The overall self-predictive learning dynamics with target representation is:
Pt = arg min
P
˜L(Φt, P, Φ′
t),
˙Φ′
t = β(Φt −Φ′
t),
˙Φt = −∇ΦtE
hP T
t ΦT
t x −sg
 (Φ′
t)T y
2
2
i
.
(14)
In Fig. 9, we carry out ablation on the effect of β. We consider the symmetric MDP case where the self-predictive learning
dynamics in Equation (4) should monotonically improve the trace objective. Here, we also plot the trace objective. When
β = 0, the result shows that the trace objective still improves compared to the initialization, though such an improvement is
much more limited and can be very non-monotonic. When β > 0, we see that the learning dynamics behaves very similarly
to Equation (4).
G.1.3. ABLATION ON HOW HYPER-PARAMETERS IMPACT NON-COLLAPSE DYNAMICS
We now present results on how a number of different hyper-parameters impact the non-collapse dynamics: ﬁnite learning
rate and non-optimal predictor.
Finite learning rate.
Thus far, our theory has been focused on the continuous time case, which corresponds to a
inﬁnitesimally small learning rate. With ﬁnite learning rate, we expect the non-collapse to be violated. Fig. 10(a) shows the
effect of ﬁnite learning rate on the preservation of the cosine similarity between two representation vectors φ1,t and φ2,t.
The two vectors are initialized to be orthogonal, so their cosine similarity is initialized at 0. We consider a grid of learning
rate η ∈{0.01, 0.1, 1, 10}; to ensure fair comparison, for learning rate η, we showcase the cosine similarity ⟨φ1,t, φ2,t⟩at
iteration T/η with T = 10000. Interpreting η as the magnitude of the step-size, this is to ensure that at each learning rate,
the result is obtained after updating for a total step-size of η · T/η = T.
As Fig. 10(a) shows, when the learning rate increases, the cosine similarity ⟨φ1,t, φ2,t⟩increases, indicating a more severe
violation of the non-collapse property. As ⟨φ1,t, φ2,t⟩→1, the two representation vectors become more and more aligned
with each other, and eventually coallpse to the same direction.

Understanding Self-Predictive Learning for Reinforcement Learning
0
20
40
60
80
100
Iterations
0.0
0.2
0.4
0.6
0.8
1.0
Normalized trace objective
= 0
= 0.01
= 0.1
Figure 9. Impact of target representation matrix on the learning dynamics. We introduce a target representation matrix Φ′
t whose dynamics
is
d
dtΦ′
t = β(Φt −Φ′
t), i.e., based on a moving average update towards the main representation matrix Φt. With the target representation
matrix in place, the trace objective still improves overall, but the improvement is likely to be non-monotonic.
Non-optimal predictor.
Our theory has suggested that the optimal predictor is important for the non-collapse of the
self-predictive learning dynamics. To assess how sensitive the non-collapse property is to the level of imperfection of the
predictor, at each time step t, let P ∗
t be the optimal predictor. We set the prediction matrix as a corrupted version of the
optimal prediction matrix,
Pt = P ∗
t + ϵ,
where ϵ ∈Rk×k is a noise matrix whose entries are sampled i.i.d. from Gaussian distribution N(0, σ2). Here, σ determines
the level of noise used for corrupting the predictor. This is meant to emulate a practical setup where the prediction matrix is
not learned perfectly.
In Fig. 10(b), we show the cosine similarity between φ1,t and φ2,t after a ﬁxed number of iterations T = 10000. As the
noise scale σ increases, we observe an increasing tendency to collapse.
G.2. Deep RL experiments
We provide details on the deep RL experiments.
We compare the deep bidirectional self-predictive learning algorithm, an algorithm inspired from the bidirectional self-
predictive learning dynamics in Equation (8), with BYOL-RL (Guo et al., 2020). BYOL-RL can be understood as an
application of self-predictive learning dynamics in the POMDP case. BYOL-RL is built on V-MPO (Song et al., 2020),
an actor-critic algorithm which shapes the representation using policy gradient, without explicit representation learning
objectives. See Appendix E for a more detailed description of the BYOL-RL agent and how it is adapted to allow for
bidirectional self-predictive learning.
BYOL-RL implements a forward prediction loss function Lfwd, which is combined with V-MPO’s RL loss function
LBYOL-RL = Lrl + Lfwd.
The bidirectional self-predictive learning algorithm introduces a backward prediction loss function
Lbidirectional = Lrl + Lfwd + αLbwd,
where α ≥0 is the only extra hyper-parameter we introduce. Throughout, we set α = 1 which introduces an equal
weight between the forward and backward predictions, as this most strictly adheres to the theory. All hyper-parameters and

Understanding Self-Predictive Learning for Reinforcement Learning
3.0
2.5
2.0
1.5
1.0
0.5
0.0
Log10 learning rate
8
6
4
2
0
Log10 abs. normalized inner product
(a) Finite learning rate
2.0
1.5
1.0
0.5
0.0
0.5
1.0
Log10 noise scale
7
6
5
4
3
2
Log10 abs. normalized inner product
(b) Non-optimal predictor
Figure 10. Ablation experiments to assess the sensitivity of the non-collapse property to ﬁnite learning rate and non-optimal prediction
matrix. Across all plots, y-axis shows the cosine similarity ⟨φ1,t, φ2,t⟩after some iterations of learning. Since the two vectors are
initialized to be orthogonal, as the inner product increases from 0 to 1, we expect the representations to collapse to the same direction.
architecture are shared across experiments wherever possible. We refer readers to Guo et al. (Guo et al., 2020) for complete
information on the network architecture and hyper-parameters.
Test bed.
Our test bed is DMLab-30, a collection of 30 diverse partially observable cognitive tasks in the 3D DeepMind
Lab (Beattie et al., 2016). DMLab-30 has visual input vt to the agent, along with the agent’s previous action at−1 and
reward function rt−1, form the observation at time t: ot = (vt, at−1, rt−1).
To better illustrate the importance of representation learning, we consider the multi-task setup where the agent is required to
solve all 30 tasks simultaneously. In practice, at each episode, the agent uniformly samples an environment out of the 30
tasks and generates a sequence of experience. Since the task id is not provided, the agent needs to implicitly infer the task
while interacting with the sampled environment. This intuitively explains why representation learning is valuable in such a
setting, as observed in prior work (Guo et al., 2019).
In Fig. 11, we compare the per-game performance of BYOL-RL with the RL baseline, measured in terms of the human
normalized scores. Here, let zi be the raw score for the i-th game, ui the raw score of a random policy and hi the raw
score of humans, then the human normalized score is calculated as zi−ui
hi−ui . Indeed, we see that BYOL-RL signiﬁcantly
out-performs the RL baseline across most games.

Understanding Self-Predictive Learning for Reinforcement Learning
language_select_described_object
natlab_varying_map_randomized
psychlab_continuous_recognition
lasertag_three_opponents_large
psychlab_visual_search
psychlab_sequential_comparison
lasertag_three_opponents_small
lasertag_one_opponent_large
lasertag_one_opponent_small
language_execute_random_task
rooms_collect_good_objects_train
language_select_located_object
psychlab_arbitrary_visuomotor_mapping
explore_object_locations_large
skymaze_irreversible_path_varied_v2
explore_object_rewards_few
language_answer_quantitative_question
rooms_watermaze
rooms_select_nonmatching_object
explore_object_rewards_many
explore_goal_locations_small
explore_obstructed_goals_large
rooms_exploit_deferred_effects_train
explore_obstructed_goals_small
explore_object_locations_small
explore_goal_locations_large
skymaze_irreversible_path_hard_v2
rooms_keys_doors_puzzle
natlab_varying_map_regrowth
natlab_fixed_large_map
0.1
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
Improvement in human normalized scores
Figure 11. Per-game improvement of BYOL-RL compared to baseline RL algorithm, in terms of mean human normalized scores averaged
across 3 seeds. The scores are obtained at the end of training. The improvement in performance is signiﬁcant in most games.

