3736
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 15, NO. 12, DECEMBER 2006
Image Denoising Via Sparse and Redundant
Representations Over Learned Dictionaries
Michael Elad and Michal Aharon
Abstract—We address the image denoising problem, where
zero-mean white and homogeneous Gaussian additive noise is to
be removed from a given image. The approach taken is based
on sparse and redundant representations over trained dictio-
naries. Using the K-SVD algorithm, we obtain a dictionary that
describes the image content effectively. Two training options are
considered: using the corrupted image itself, or training on a
corpus of high-quality image database. Since the K-SVD is limited
in handling small image patches, we extend its deployment to
arbitrary image sizes by deﬁning a global image prior that forces
sparsity over patches in every location in the image. We show how
such Bayesian treatment leads to a simple and effective denoising
algorithm. This leads to a state-of-the-art denoising performance,
equivalent and sometimes surpassing recently published leading
alternative denoising methods.
Index Terms—Bayesian reconstruction, dictionary learning, dis-
crete cosine transform (DCT), image denoising, K-SVD, matching
pursuit, maximum a posteriori (MAP) estimation, redundancy,
sparse representations.
I. INTRODUCTION
I
N THIS paper, we address the classic image denoising
problem: An ideal image
is measured in the presence of
an additive zero-mean white and homogeneous Gaussian noise,
, with standard deviation
. The measured image
is, thus
(1)
We desire to design an algorithm that can remove the noise from
, getting as close as possible to the original image,
.
The image denoising problem is important, not only because
of the evident applications it serves. Being the simplest possible
inverse problem, it provides a convenient platform over which
image processing ideas and techniques can be assessed. Indeed,
numerous contributions in the past 50 years or so addressed
this problem from many and diverse points of view. Statistical
estimators of all sorts, spatial adaptive ﬁlters, stochastic anal-
ysis, partial differential equations, transform-domain methods,
splines and other approximation theory methods, morphological
analysis, order statistics, and more, are some of the many direc-
tions explored in studying this problem. In this paper, we have
no intention to provide a survey of this vast activity. Instead,
Manuscript received December 18, 2005; revised May 3, 2006. The associate
editor coordinating the review of this manuscript and approving it for publica-
tion was Dr. Tamas Sziranyi.
The authors are with the Department of Computer Science, The Technion—
Israel Institute of Technology, Haifa 32000, Israel (e-mail: elad@cs.technion.
ac.il; michalo@cs.technion.ac.il).
Digital Object Identiﬁer 10.1109/TIP.2006.881969
we intend to concentrate on one speciﬁc approach towards the
image denoising problem that we ﬁnd to be highly effective and
promising: the use of sparse and redundant representations over
trained dictionaries.
Using redundant representations and sparsity as driving
forces for denoising of signals has drawn a lot of research
attention in the past decade or so. At ﬁrst, sparsity of the unitary
wavelet coefﬁcients was considered, leading to the celebrated
shrinkage algorithm [1]–[9]. One reason to turn to redundant
representations was the desire to have the shift invariance
property [10]. Also, with the growing realization that regular
separable 1-D wavelets are inappropriate for handling images,
several new tailored multiscale and directional redundant
transforms were introduced, including the curvelet [11], [12],
contourlet [13], [14], wedgelet [15], bandlet [16], [17], and the
steerable wavelet [18], [19]. In parallel, the introduction of the
matching pursuit [20], [21] and the basis pursuit denoising [22]
gave rise to the ability to address the image denoising problem
as a direct sparse decomposition technique over redundant
dictionaries. All these lead to what is considered today as some
of the best available image denoising methods (see [23]–[26]
for few representative works).
While the work reported here is also built on the very same
sparsity and redundancy concepts, it is adopting a different
point of view, drawing from yet another recent line of work
that studies example-based restoration. In addressing general
inverse problems in image processing using the Bayesian
approach, an image prior is necessary. Traditionally, this has
been handled by choosing a prior based on some simplifying
assumptions, such as spatial smoothness, low/max-entropy,
or sparsity in some transform domain. While these common
approaches lean on a guess of a mathematical expression for the
image prior, the example-based techniques suggest to learn the
prior from images somehow. For example, assuming a spatial
smoothness-based Markov random ﬁeld prior of a speciﬁc
structure, one can still question (and, thus, train) the derivative
ﬁlters to apply on the image, and the robust function to use in
weighting these ﬁlters’ outcome [27]–[29].
When this prior-learning idea is merged with sparsity and re-
dundancy, it is the dictionary to be used that we target as the
learned set of parameters. Instead of the deployment of a pre-
chosen set of basis functions as the curvelet or contourlet would
do, we propose to learn the dictionary from examples. In this
work we consider two training options: 1) training the dictio-
nary using patches from the corrupted image itself or 2) training
on a corpus of patches taken from a high-quality set of images.
This idea of learning a dictionary that yields sparse represen-
tations for a set of training image-patches has been studied in
a sequence of works [30]–[37]. In this paper, we propose the
1057-7149/$20.00 © 2006 IEEE

ELAD AND AHARON: IMAGE DENOISING VIA SPARSE AND REDUNDANT REPRESENTATIONS
3737
K-SVD algorithm [36], [37] because of its simplicity and efﬁ-
ciency for this task. Also, due to its structure, we shall see how
the training and the denoising fuse together naturally into one
coherent and iterated process, when training is done on the given
image directly.
Since dictionary learning is limited in handling small image
patches, a natural difﬁculty arises: How can we use it for gen-
eral images of arbitrary size? In this work, we propose a global
image prior that forces sparsity over patches in every location
in the image (with overlaps). This aligns with a similar idea, ap-
pearing in [29], for turning a local MRF-based prior into a global
one. We deﬁne a maximum a posteriori probability (MAP) es-
timator as the minimizer of a well-deﬁned global penalty term.
Its numerical solution leads to a simple iterated patch-by-patch
sparse coding and averaging algorithm that is closely related to
the ideas explored in [38]–[40] and generalizes them.
When considering the available global and multiscale alter-
native denoising schemes (e.g., based on curvelet, contourlet,
and steerable wavelet), it looks like there is much to be lost
in working on small patches. Is there any chance of getting a
comparable denoising performance with a local-sparsity based
method? In that respect, the image denoising work reported in
[23] is of great importance. Beyond the speciﬁc novel and highly
effective algorithm described in that paper, Portilla and his coau-
thors posed a clear set of comparative experiments that stan-
dardize how image denoising algorithms should be assessed and
compared one versus the other. We make use of these exact ex-
periments and show that the newly proposed algorithm performs
similarly, and, often, better, compared to the denoising perfor-
mance reported in their work.
To summarize, the novelty of this paper includes the way
we use local sparsity and redundancy as ingredients in a global
Bayesian objective—this part is described in Section II, along
with its emerging iterated numerical solver. Also novel in this
work is the idea to train dictionaries for the denoising task, rather
than use prechosen ones. As already mentioned earlier, when
training is done on the corrupted image directly, the overall
training-denoising algorithm becomes fused into one iterative
procedure that comprises of steps of denoising of the image,
followed by an update of the dictionary. This is described in
Section III in detail. In Section IV, we show some experimental
results that demonstrate the effectiveness of this algorithm.
II. FROM LOCAL TO GLOBAL BAYESIAN RECONSTRUCTION
In this section, we start the presentation of the proposed de-
noising algorithm by ﬁrst introducing how sparsity and redun-
dancy are brought to use. We do that via the introduction of
the Sparseland model. Once this is set, we will discuss how
local treatment on image patches turns into a global prior in a
Bayesian reconstruction framework.
A. Sparseland Model for Image Patches
We consider image patches of size
pixels, ordered
lexicographically as column vectors
. For the construc-
tion of the Sparseland model, we need to deﬁne a dictionary
(matrix) of size
(with
, implying that it is
redundant). At the moment, we shall assume that this matrix is
known and ﬁxed. Put loosely, the proposed model suggests that
every image patch,
, could be represented sparsely over this
dictionary, i.e., the solution of
(2)
is indeed very sparse,
. The notation
stands
for the count of the nonzero entries in
. The basic idea here
is that every signal instance from the family we consider can
be represented as a linear combination of few columns (atoms)
from the redundant dictionary
.
This model should be made more precise by replacing the
rough constraint
with a clear requirement to allow a
bounded representation error,
. Also, one needs
to deﬁne how deep is the required sparsity, adding a requirement
of the form
, that states that the sparse represen-
tation uses no more than
atoms from the dictionary for every
image patch instance. Alternatively, a probabilistic characteri-
zation can be given, deﬁning the probability to obtain a repre-
sentation with
nonzeros as a decaying function of some
sort. Considering the simpler option between the two, with the
triplet
in place, our model is well deﬁned.
Now assume that
indeed belongs to the
-Sparse-
land signals. Consider a noisy version of it,
, contaminated by
an additive zero-mean white Gaussian noise with standard de-
viation
. The MAP estimator for denoising this image patch is
built by solving
(3)
where
is dictated by
. The denoised image is, thus, given
by
[22], [41], [42]. Notice that the above optimization
task can be changed to be
(4)
so that the constraint becomes a penalty. For a proper choice of
, the two problems are equivalent. We will use this alternative
terminology from now on, as it makes the presentation of later
parts simpler to follow.
While this problem is, in general, very hard to solve, the
matching and the basis pursuit algorithms can be used quite
effectively [20]–[22] to get an approximated solution. Recent
work established that those approximation techniques can be
quite accurate if the solution is sparse enough to begin with
[41], [42]. In this work, we will make use mainly of the ortho-
normal matching pursuit (OMP) because of its simplicity [21]
and efﬁciency.
B. From Local Analysis to a Global Prior
If we want to handle a larger image
of size
, and we are still interested in using the above
described model, one option is to redeﬁne the model with a
larger dictionary. Indeed, when using this model with a dictio-
nary emerging from the contourlet or curvelet transforms, such
scaling is simple and natural [26].
However, when we insist on using a speciﬁc ﬁxed and small
size dictionary
, this option no longer exists. Thus,

3738
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 15, NO. 12, DECEMBER 2006
a natural question arises concerning the use of such a small dic-
tionary in the ﬁrst place. Two reasons come to mind: 1) when
training takes place (as we will show in the next section), only
small dictionaries can be composed; and furthermore; 2) a small
dictionary implies a locality of the resulting algorithms, which
simpliﬁes the overall image treatment.
We next describe possible ways to use such a small dictionary
when treating a large image. A heuristic approach is to work on
smaller patches of size
and tile the results. In doing
so, visible artifacts may occur on block boundaries. One could
also propose to work on overlapping patches and average the
results in order to prevent such blockiness artifacts, as, indeed,
practiced in [38]–[40]. As we shall see next, a systematic global
approach towards this problem leads to this very option as a core
ingredient in an overall algorithm.
If our knowledge on the unknown large image
is fully ex-
pressed in the fact that every patch in it belongs to the
-
Sparseland model, then the natural generalization of the above
MAP estimator is the replacement of (4) with
(5)
In this expression, the ﬁrst term is the log-likelihood global force
that demands the proximity between the measured image,
,
and its denoised (and unknown) version
. Put as a constraint,
this penalty would have read
, and this
reﬂects the direct relationship between
and
.
The second and the third terms are the image prior that makes
sure that in the constructed image,
, every patch
of size
in every location (thus, the summation by
)
has a sparse representation with bounded error. Similar conver-
sion has also been practiced by Roth and Black when handling
an MRF prior [29].
The matrix
is an
matrix that extracts the
block
from the image. For an
image
, the summation
over
includes
items, considering all image
patches of size
in
with overlaps. As to the coef-
ﬁcients
, those must be location dependent, so as to comply
with a set of constraints of the form
.
C. Numerical Solution
When the underlying dictionary
is assumed known, the
proposed penalty term in (5) has two kinds of unknowns: the
sparse representations
per each location, and the overall
output image
. Instead of addressing both together, we pro-
pose a block-coordinate minimization algorithm that starts with
an initialization
, and then seeks the optimal
. In
doing so, we get a complete decoupling of the minimization task
to many smaller ones, each of the form
(6)
handling one image patch. Solving this using the orthonormal
matching pursuit [21] is easy, gathering one atom at a time, and
stopping when the error
goes below
. This way,
the choice of
has been handled implicitly. Thus, this stage
works as a sliding window sparse coding stage, operated on each
block of
at a time.
Given all
, we can now ﬁx those and turn to update
.
Returning to (5), we need to solve
(7)
This is a simple quadratic term that has a closed-form solution
of the form
(8)
This rather cumbersome expression may mislead, as all it says is
that averaging of the denoised patches is to be done, with some
relaxation obtained by averaging with the original noisy image.
The matrix to invert in the above expression is a diagonal one,
and, thus, the calculation of (8) can be also done on a pixel-by-
pixel basis, following the previously described sliding window
sparse coding steps.
So far, we have seen that the obtained denoising algorithm
calls for sparse coding of small patches, and an averaging of
their outcomes. However, if minimization of (5) is our goal,
then this process should proceed. Given the updated
, we can
repeat the sparse coding stage, this time working on patches
from the already denoised image. Once this is done, a new av-
eraging should be calculated, and so on, and so forth. Thus,
we obtain exactly what Guleryuz suggested in his work—iter-
ated denoising via sparse representation, and we may regard the
analysis proposed here as a rigorous way to justify such iterated
scheme [38]–[40].
III. EXAMPLE-BASED SPARSITY AND REDUNDANCY
The entire discussion so far has been based on the assumption
that the dictionary
is known. We can certainly make
some educated guesses as to which dictionaries to use. In fact,
following Guleryuz’s work, the DCT seems like such a plausible
choice [38]–[40]. Indeed, we might do better by using a redun-
dant version of the DCT,1 as practiced in [36]. Still, the question
remains: Can we make a better choice for
based on training?
We now turn to discuss this option. We start with the simpler
(and less effective) option of training the dictionary on a set of
image patches taken from good quality images, and then turn to
discuss the option of training on the corrupted image itself.
A. Training on the Corpus of Image Patches
Given a set of image patches
, each of
size
, and assuming that they emerge from a spe-
ciﬁc
-Sparseland model, we would like to estimate
this model parameters,
. Put formally, we seek the
dictionary
that minimizes
(9)
Just as before, the above expression seeks to get a sparse rep-
resentation per each of the examples in
, and obtain a small
1Such a version is created by using a redundant Fourier dictionary and a
mirror extension of the signal to restrict the transform to real entries.

ELAD AND AHARON: IMAGE DENOISING VIA SPARSE AND REDUNDANT REPRESENTATIONS
3739
Fig. 1. Denoising procedure using a dictionary trained on patches from the corrupted image. For our experiments, we used the OMP pursuit method, and set
J = 10;  = 30= and C = 1:15.
representation error. The choice for
dictates how those two
forces should be weighted, so as to make one of them a clear
constraint. For example, constraining
implies
speciﬁc values for
, while requiring
leads to others.
The K-SVD proposes an iterative algorithm designed to
handle the above task effectively [36], [37]. Adopting again
the block-coordinate descent idea, the computations of
and
are separated. Assuming that
is known, the penalty
posed in (9) reduces to a set of
sparse coding operations,
very much like the ones seen in (6). Thus, OMP can be used
again to obtain the near-optimal (recall that OMP is an ap-
proximation algorithm, and, thus, a true minimization is not
guaranteed) set of representation vectors
.
Assuming these representation vectors ﬁxed, the K-SVD pro-
poses an update of the dictionary one column at a time. As it
turns out, this update can be done optimally, leading to the need
to perform a SVD operation on residual data matrices, computed
only on the examples that use this atom. This way, the value of
is guaranteed to drop per an update of each dic-
tionary atom, and along with this update, the representation co-
efﬁcients change as well (see [36] and [37] for more details).
When adopted to the denoising task at hand, a crucial step
is the choice of the examples to train on. Is there really a uni-
versal dictionary that ﬁts all images well? If there is one, which
examples shall we use to ﬁnd it? The experiments that follow
in the next section bring us to the conclusion that while a rea-
sonably good dictionary that ﬁts all is indeed within reach, ex-
tracting state-of-the-art denoising performance calls for a more
complex model that uses several dictionaries switched by con-
tent—an option we do not explore in this work.
Also, since the penalty minimized here in (9) is a highly non-
convex functional, local minimum solutions are likely to haunt
us. Thus, a wise initialization could be of great worth. In our

3740
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 15, NO. 12, DECEMBER 2006
TABLE I
SUMMARY OF THE DENOISING PSNR RESULTS IN DECIBELS. IN EACH CELL. FOUR DENOISING RESULTS ARE REPORTED. TOP LEFT: RESULTS OF
PORTILLA ET AL. [23]. TOP RIGHT: OVERCOMPLETE DCT. BOTTOM LEFT: GLOBAL TRAINED DICTIONARY. BOTTOM RIGHT: ADAPTIVE
DICTIONARY TRAINED ON NOISY IMAGE. IN EACH SUCH SET WE HIGHLIGHTED THE BEST RESULT. THE THREE LATTER
METHODS WERE EXECUTED WITH  = 30=. ALL NUMBERS ARE AN AVERAGE OVER FIVE EXPERIMENTS.
THE LAST TWO COLUMNS PRESENT THE AVERAGE RESULTS OVER ALL IMAGES AND THEIR VARIANCE
experiments we started with the already mentioned redundant
DCT, which proves to be a good dictionary choice. This also
enabled us to apply fewer number of iterations.
Another puzzling issue is the redundancy factor
—How
should we choose
, the number of columns in
? Is there an
optimal choice? In this work, we do not address this important
question, and simply choose a value we ﬁnd empirically to per-
form well. Further work is required to explore this matter.
B. Training on the Corrupted Image
Instead of supplying an artiﬁcial set of examples to train on, as
proposed above, one could take the patches from the corrupted
image,
, where
. Since
the K-SVD dictionary learning process has in it a noise rejec-
tion capability (see experiments reported in [36]), this seems
like a natural idea. Furthermore, rather than using unrelated ex-
amples that call for the universality assumption of the Sparse-
land model, this option tailors the dictionary to the image to be
treated.
At ﬁrst sight, this change in the origin of the examples to
train on seems to be of technical worth, and has no impact on
the overall algorithm. However, a close inspection of both the
functional
in (9), and the global MAP penalty
in (5), reveals the close resemblance between the two. This im-
plies that the dictionary design could be embedded within the
Bayesian approach. Returning to (5), we can regard also
as
an unknown, and deﬁne our problem as
(10)
Following the previously constructed algorithm, we can assume
a ﬁxed
and
, and compute the representations
. This
requires, as before, a sparse coding stage that deploys the OMP.
Given those representations, the dictionary can be now updated,
using a sequence of K-SVD operations.
Once done, the output image can be computed, using (8).
However, an update of the output image
changes the noise
level
, which up until now has been considered as known, and
was used in the preceding two stages. Therefore, we choose to
perform several more iterations of representation computation
and dictionary update, using the same value of
, before ﬁnding
the output image
. This algorithm is described in detail in
Fig. 1.
In evaluating the computational complexity of this algorithm,
we consider all three stages—sparse coding (OMP process), dic-
tionary update (these stages are iterated
times), and ﬁnal av-
eraging process. All stages can be done efﬁciently, requiring
operations per pixel, where
is the block dimen-
sion,
is the number of atoms in the dictionary, and
is the

ELAD AND AHARON: IMAGE DENOISING VIA SPARSE AND REDUNDANT REPRESENTATIONS
3741
Fig. 2. Left: Overcomplete DCT dictionary. Right: Globally trained dictionary.
Fig. 3. Sample from the images used for training the global dictionary.
number of nonzero elements in each coefﬁcient vector.
de-
pends strongly on the noise level, e.g., for
, the average
is 2.96, and for
, the average
is 1.12.
IV. RESULTS
In this section, we demonstrate the results achieved by ap-
plying the above methods on several test images, and with sev-
eral dictionaries. The tested images, as also the tested noise
levels, are all the same ones as those used in the denoising ex-
periments reported in [23], in order to enable a fair comparison.
Table I summarizes these denoising results for the DCT dic-
tionary, the globally trained dictionary, and training on the cor-
rupted images directly (referred to hereafter as the adaptive dic-
tionary). In all this set of experiments, the dictionaries used were
of size 64
256, designed to handle image patches of size 8
8
pixels
. Every result reported is an average
over 5 experiments, having different realizations of the noise.
The redundant DCT dictionary is described on the left side
of Fig. 2, each of its atoms shown as an 8
8 pixel image. This
dictionary was also used as the initialization for all the training
algorithms that follow. The globally trained dictionary is shown
on the right side of Fig. 2. This dictionary was produced by
the K-SVD algorithm (executed 180 iterations, using OMP for
sparse coding with
), trained on a data-set of 100 000
8
8 patches. Those patches were taken from an arbitrary set
of clean natural images (unrelated to the test images), some of
which are shown in Fig. 3.
In all experiments, the denoising process included a sparse-
coding of each patch of size 8
8 pixels from the noisy image.
Using the OMP, atoms were accumulated till the average error
passed the threshold, chosen empirically to be
. This
means that our algorithm assumes the knowledge of
—very
much like that assumed in [23]. The denoised patches were av-
eraged, as described in (8), using
(see below for an
explanation for this choice of
). We chose to apply only one
iteration in the iterative process suggested in Section II-C. Fol-
lowing iterations requires knowledge of the new noisy param-
eter
, which is unknown after ﬁrst changing
.
When training the dictionary on overlapping patches from
the noisy image itself, each such experiment included
patches (all available patches from the 256
256
images, and every second patch from every second row in the
512
512 size images). The algorithm described in detail in
Fig. 1 was applied.

3742
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 15, NO. 12, DECEMBER 2006
Fig. 4. Comparison between the three presented methods (overcomplete DCT,
global trained dictionary, and adaptive dictionary trained on patches from the
noisy image) and the results achieved recently in [23] for three test images.
As can be seen from Table I, the results of all methods are very
close to each other in general. Averaging the results that corre-
spond to [23] in this table for noise levels lower than,2
2The strong noise experiments are problematic to analyze, because clipping
of the dynamic range to [0, 255], as often done, causes a severe deviation from
the Gaussian distribution model assumed.
Fig. 5. Improvement in the denoising results after each iteration of the K-SVD
algorithm, executed on noisy patches of the image “Peppers.”
the value is 34.62 dB. A similar averaging over the DCT dic-
tionary results gives 34.45 dB, implying an average difference
of 0.17 dB, in favor of Portilla’s method. This is the same case
with the globally trained dictionary, which means that our at-
tempt to train one global dictionary for images performs as good
as the ﬁxed redundant DCT. However, for the method of the
image-adaptive dictionary, an average of 34.86 dB is obtained,
giving an average advantage of 0.24 dB over Portilla’s method.
For the higher noise power experiments, our approach deterio-
rates faster and achieves weaker results.
In order to better visualize the results and their comparison
to those in [23], Fig. 4 presents the difference of the denoising
results of the two proposed methods and the overcomplete
DCT compared with those of [23] (which appears as a zero
straight reference line). This comparison is presented for the
images “Peppers,” “House,” and “Barbara.” Notice that, for
these images, the adaptive dictionary outperforms the reported
results of Portilla et al. for all noise levels lower than
,
while the global dictionary often achieves very close results. In
the image “Barbara,” however, which contains high-frequency
texture areas, the adaptive dictionary that learns the speciﬁc
characteristics has a clear advantage over the globally trained
dictionary.
Fig. 5 further describes the behavior of the denoising algo-
rithm that uses the adaptive dictionary. Each K-SVD iteration
improves the denoising results, with the initial dictionary set to
be the overcomplete DCT. A graph presenting this consistent
improvement for several noise levels is presented in Fig. 5. All
graphs show the improvement over the ﬁrst iteration, and, there-
fore, all curves start at zero, going towards positive values. As
can be seen, a gain of up to 1 dB is achievable. Fig. 6 shows
the results of the proposed algorithms for the image “Barbara,”
and for
. The ﬁnal adaptive dictionary that leads to those
results is presented in Fig. 7.
We now turn to study the effect of the parameter
in (8). As
expected, we found that a proper choice for
is dependent on
the noise level. As the noise increases, better results are achieved

ELAD AND AHARON: IMAGE DENOISING VIA SPARSE AND REDUNDANT REPRESENTATIONS
3743
Fig. 6. Example of the denoising results for the image “Barbara” with  = 20—the original, the noisy, and two restoration results.
Fig. 7. Example of the denoising results for the image “Barbara” with  =
20—the adaptively trained dictionary.
with small values of , and vice versa. This is, indeed, expected,
as relatively “clean” images should have a stronger effect on
the outcome, while very noisy ones should effect the outcome
weakly, if at all. We tested several values for this parameter, and
found empirically that the best results are achieved with
. It is interesting to see that all three denoising methods
(overcomplete DCT, global dictionary, and adaptive dictionary
trained on noisy patches), and all noise levels generally agree
with this choice. In Fig. 8, we present the improvement (and
later, deterioration) achieved when increasing the value of
in
the averaging process (8). In Fig. 8, one image (“Peppers”) was
tested with four noise levels
and with all
three methods, resulting with 12 curves. The choice
seems to be near the peak for all these graphs.
To conclude this experimental section, we refer to our arbi-
trary choice of
dictionary atoms (this choice had an
effect over all three experimented methods). We conducted an-
other experiment, which compares between several values of .
In this experiment, we tested the denoising results of the three
proposed methods on the image “House” for an initial noise
level of
(24.61 dB) and
. The tested re-
dundancy values (of
) were 64, 128, 256, and 512. The av-
erage results of four executions (per each test) are presented in
Fig. 9. As can be seen, the increase of the number of dictionary
elements generally improves the results, although this improve-
ment is small (0–0.16 dB). This increase is most effective in the
adaptive dictionary method.

3744
IEEE TRANSACTIONS ON IMAGE PROCESSING, VOL. 15, NO. 12, DECEMBER 2006
Fig. 8. Improvement (and later, deterioration) of the denoising results when
increasing the value of  in the averaging process in (8).
Fig. 9. Effect of changing the number of dictionary elements (k) on the ﬁnal
denoising results for the image “House” and for  = 15.
V. CONCLUSION AND FURTHER WORK
This work has presented a simple method for image de-
noising, leading to state-of-the-art performance, equivalent to
and sometimes surpassing recently published leading alterna-
tives. The proposed method is based on local operations and
involves sparse decompositions of each image block under one
ﬁxed over-complete dictionary, and a simple average calcula-
tions. The content of the dictionary is of prime importance for
the denoising process—we have shown that a dictionary trained
for natural real images, as well as an adaptive dictionary trained
on patches of the noisy image itself, both perform very well.
There are several research directions that we are currently
considering, such as using several dictionaries and switching
between them by content, optimizing the parameters, replacing
the OMP by a better pursuit technique, and more. Beyond these,
one direction we consider to be promising is a generalization to
multiscale dictionaries. This work concentrated on small image
patches, completely overlooking the global structure of the
image, and the multiscale analysis that other techniques have
exploited rather well. We are studying ways to extend this work
to multiscale dictionaries, as it is clear that K-SVD cannot be
directly deployed on larger blocks.
REFERENCES
[1] D. L. Donoho and I. M. Johnstone, “Ideal spatial adaptation by wavelet
shrinkage,” Biometrika, vol. 81, no. 3, pp. 425–455, Sep. 1994.
[2] D. L. Donoho, “De-noising by soft thresholding,” IEEE Trans. Inf.
Theory, vol. 41, no. 3, pp. 613–627, May 1995.
[3] D. L. Donoho, I. M. Johnstone, G. Kerkyacharian, and D. Picard,
“Wavelet shrinkage—Asymptopia,” J. Roy. Statist. Soc. B—Method-
ological, vol. 57, no. 2, pp. 301–337, 1995.
[4] D. L. Donoho and I. M. Johnstone, “Adapting to unknown smoothness
via wavelet shrinkage,” J. Amer. Statist. Assoc., vol. 90, no. 432, pp.
1200–1224, Dec. 1995.
[5] D. L. Donoho and I. M. Johnstone, “Minimax estimation via wavelet
shrinkage,” Ann. Statist., vol. 26, no. 3, pp. 879–921, Jun. 1998.
[6] E. P. Simoncelli and E. H. Adelson, “Noise removal via Bayesian
wavelet coring,” in Proc. Int. Conf. Image Processing, Laussanne,
Switzerland, Sep. 1996.
[7] A. Chambolle, R. A. DeVore, N.-Y. Lee, and B. J. Lucier, “Non-
linear wavelet image processing: Variational problems, compression,
and noise removal through wavelet shrinkage,” IEEE Trans. Image
Process., vol. 7, no. 3, pp. 319–335, Mar. 1998.
[8] P. Moulin and J. Liu, “Analysis of multiresolution image denoising
schemes using generalized Gaussian and complexity priors,” IEEE
Trans. Inf. Theory, vol. 45, no. 3, pp. 909–919, Apr. 1999.
[9] M. Jansen, Noise Reduction by Wavelet Thresholding.
New York:
Springer-Verlag, 2001.
[10] R. Coifman and D. L. Donoho, “Translation invariant de-noising,” in
In Wavelets and Statistics, Lecture Notes in Statistics.
New York:
Springer-Verlag, 1995, pp. 125–150, 1995.
[11] E. J. Candes and D. L. Donoho, “Recovering edges in ill-posed inverse
problems: Optimality of curvelet frames,” Ann. Statist., vol. 30, no. 3,
pp. 784–842, Jun. 2002.
[12] E. J. Candès and D. L. Donoho, “New tight frames of curvelets and the
problem of approximating piecewise C
images with piecewise C
edges,” Commun. Pure Appl. Math., vol. 57, pp. 219–266, Feb. 2004.
[13] M. N. Do and M. Vetterli, Contourlets, Beyond Wavelets, G. V.
Welland, Ed.
New York: Academic, 2003.
[14] M. N. Do and M. Vetterli, “Framing pyramids,” IEEE Trans. Signal
Process., vol. 51, pp. 2329–2342, Sep. 2003.
[15] D. L. Donoho, “Wedgelets: Nearly minimax estimation of edges,” Ann.
Statist., vol. 27, no. 3, pp. 859–897, Jun. 1998.
[16] S. Mallat and E. LePennec, “Sparse geometric image representation
with bandelets,” IEEE Trans. Image Process., vol. 14, no. 4, pp.
423–438, Apr. 2005.
[17] S. Mallat and E. LePennec, “Bandelet image approximation and com-
pression,” SIAM J. Multiscale Model. Simul., 2005, to be published.
[18] W. T. Freeman and E. H. Adelson, “The design and use of steerable
ﬁlters,” IEEE Pattern Anal. Mach. Intell., vol. 13, no. 9, pp. 891–906,
Sep. 1991.
[19] E. P. Simoncelli, W. T. Freeman, E. H. Adelson, and D. H. Heeger,
“Shiftable multi-scale transforms,” IEEE Trans Inf. Theory, vol. 38,
no. 2, pp. 587–607, Mar. 1992.
[20] S. Mallat and Z. Zhang, “Matching pursuit in a time-frequency dic-
tionary,” IEEE Trans. Signal Process., vol. 41, no. 12, pp. 3397–3415,
Dec. 1993.
[21] Y. C. Pati, R. Rezaiifar, and P. S. Krishnaprasad, “Orthogonal matching
pursuit: Recursive function approximation with applications to wavelet
decomposition,” presented at the 27th Annu. Asilomar Conf. Signals,
Systems, and Computers, 1993.
[22] S. S. Chen, D. L. Donoho, and M. A. Saunders, “Atomic decomposition
by basis pursuit,” SIAM Rev., vol. 43, no. 1, pp. 129–59, 2001.
[23] J. Portilla, V. Strela, M. J. Wainwright, and E. P. Simoncelli, “Image
denoising using scale mixtures of gaussians in the wavelet doma,” IEEE
Trans. Image Process., vol. 12, no. 11, pp. 1338–1351, Nov. 2003.
[24] J.-L. Starck, E. J. Candes, and D. L. Donoho, “The curvelet transform
for image denoising,” IEEE Trans. Image Process., vol. 11, no. 6, pp.
670–684, Jun. 2002.
[25] R. Eslami and H. Radha, “Translation-invariant contourlet transform
and its application to image denoising,” IEEE Trans. Image Process.,
vol. 15, no. 11, pp. 3362–3374, Nov. 2006.

ELAD AND AHARON: IMAGE DENOISING VIA SPARSE AND REDUNDANT REPRESENTATIONS
3745
[26] B. Matalon, M. Elad, and M. Zibulevsky, “Improved denoising of im-
ages using modeling of the redundant contourlet transform,” presented
at the SPIE Conf. Wavelets, Jul. 2005.
[27] S. C. Zhu and D. Mumford, “Prior learning and Gibbs reaction-dif-
fusion,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 19, no. 11, pp.
1236–1250, Nov. 1997.
[28] E. Haber and L. Tenorio, “Learning regularization functionals,” Inv.
Probl., vol. 19, pp. 611–626, 2003.
[29] S. Roth and M. J. Black, “Fields of experts: A framework for learning
image priors,” in Proc. IEEE Conf. Computer Vision and Pattern
Recognition, Jun. 2005, vol. 2, pp. 860–867.
[30] B. A. Olshausen and D. J. Field, “Sparse coding with an overcomplete
basis set: A strategy employed by V1?,” Vis. Res., vol. 37, pp. 311–325,
1997.
[31] K. Engan, S. O. Aase, and J. H. Hakon-Husoy, “Method of optimal
directions for frame design,” in IEEE Int. Conf. Acoustics, Speech, and
Signal Processing, 1999, vol. 5, pp. 2443–2446.
[32] K. Kreutz-Delgado and B. D. Rao, “Focuss-based dictionary learning
algorithms,” presented at the Wavelet Applications in Signal and Image
Processing VIII, 2000.
[33] K. Kreutz-Delgado, J. F. Murray, B. D. Rao, K. Engan, T. Lee, and T. J.
Sejnowski, “Dictionary learning algorithms for sparse representation,”
Neur. Comput., vol. 15, no. 2, pp. 349–396, 2003.
[34] M. S. Lewicki and T. J. Sejnowski, “Learning overcomplete represen-
tations,” Neur. Comput., vol. 12, pp. 337–365, 2000.
[35] L. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya, “Learning unions
of orthonormal bases with thresholded singular value decomposition,”
presented at the IEEE Intl Conf. Acoustics, Speech, and Signal Pro-
cessing, 2005.
[36] M. Aharon, M. Elad, and A. M. Bruckstein, “The K-SVD: An algo-
rithm for designing of overcomplete dictionaries for sparse representa-
tion,” IEEE Trans. Signal Process., to be published.
[37] M. Aharon, M. Elad, and A. M. Bruckstein, “On the uniqueness of over-
complete dictionaries, and a practical way to retrieve them,” J. Linear
Algebra Appl.
[38] O. G. Guleryuz, “Weighted overcomplete denoising,” presented at the
Asilomar Conf. Signals and Systems, Paciﬁc Grove, CA, Nov. 2003.
[39] O. G. Guleryuz, “Nonlinear approximation based image recovery
using adaptive sparse reconstructions and iterated denoising: Part
I—Theory,” IEEE Trans. Image Process., vol. 15, no. 3, pp. 539–553,
Mar. 2005.
[40] O. G. Guleryuz, “Nonlinear approximation based image recovery
using adaptive sparse reconstructions and iterated denoising: Part
II—Adaptive algorithms,” IEEE Trans. Image Process., vol. 15, no. 3,
pp. 554–571, Mar. 2005.
[41] D. L. Donoho, M. Elad, and V. Temlyakov, “Stable recovery of sparse
overcomplete representations in the presence of noise,” IEEE Trans.
Inf. Theory, vol. 52, no. 1, pp. 6–18, Jan. 2006.
[42] J. A. Tropp, “Just relax: Convex programming methods for subset se-
lection and sparse approximation,” IEEE Trans. Inf. Theory, vol. 51,
no. 3, pp. 1030–1051, Mar. 2005.
Michael Elad received the B.Sc., M.Sc., and
D.Sc. degrees from the Department of Electrical
Engineering, The Technion–Israel Institute of Tech-
nology, Haifa, in 1986, 1988, and 1997, respectively.
From 1988 to 1993, he served in the Israeli
Air Force. From 1997 to 2000, he was with
Hewlett-Packard Laboratories as an R&D Engineer.
From 2000 to 2001, he headed the Research Division
at Jigami Corporation, Israel. From 2001 to 2003,
he was a Research Associate with the Computer
Science Department, Stanford University, Stanford,
CA (SCCM program). Since September 2003, he has been with the Department
of Computer Science, The Technion, as an Assistant Professor. He works in
the ﬁeld of signal and image processing, specializing particularly in inverse
problems, sparse representations, and over-complete transforms.
Dr. Elad received The Technion’s Best Lecturer Award four times (1999,
2000, 2004, and 2005). He is also the recipient of the Guttwirth and the Wolf
fellowships.
Michal Aharon received the B.Sc. and M.Sc. degees
from the Department of Computer Science, The
Technion–Israel Institute of Technology, Haifa,
in 2001 and 2004, respectively. She is currently
pursuing the Ph.D. degree, working closely with
Prof. M. Elad and A. Bruckstein.
During her studies, she worked at Intel and IBM.
From 1997 to 1999, she served in the Israeli Military
Intelligence.
Ms. Aharon is the recipient of the Guttwirth and
the Neeman fellowships.

