Table of Contents
Papers
       Self-Reference and Computational Complexity      
       Cheating the Millennium
 Essays
A Very Brief History of Time
The Theory of Theories
An Interdisciplinary Approach to Reality
On Absolute Truth and Knowledge
Introduction to the CTMU
Physics and Metaphysics
Megaboard Discussions
CTMU Q and A
CTMU and God
A Dialogic Response to an Atheist
Discussion with CTMU Conference Members
Discussion on the Ultranet List
Discussion with a Mega Society Easy Member
Discussion at the Utne Cafe
Discussion between Christopher Michael Langan and Russell Fred Vaughan, Ph.D.
Colloquy Discussions
God, the Universe, and Theories of Everything
On Nagarjuna and the Heart Sutra
Altered States and Psi Phenomena (Part One)
Altered States and Psi Phenomena (Part Two)
On Society and Socialization
ISCID Discussions
Cosmogony, Holography and Causality
Critical Questions
CTMU and the Axiom of Choice
Evolutionary Intelligence
Karl D. Stephen: Tegmark’s Parallel Universes: A Challenge to Intelligent Design?
On Progress, Readdressing Reality Theory, and Information in the Holographic Universe
Organisms Using GA vs. Organisms being built by GAs
T-Duality Universe
Virtues of Scientists
Miscellaneous 
Brains As Models of Intelligence
On the Paradoxical Connection Between Money and Brains
On the Differences between People, Birds, and Bees
The Resolution of Economic Paradox: Toward a Theory of Economic Relativity
Letter from Chris Langan
Noesis Discussion #1
Noesis Discussion #2
In Clarification of the CTMU and its Applications in Noesis
Chris Langan to Rick Rosner
Reply to Chris Langan on Isomorphisms, Models and Objectivity

      Superscholar Interview
      A Prologue to Buffoonery
      A Day in the Life of JoJo Einstein, Street Clown (part 1 and 2)
Response to CTMU Critiques 
Another Crank Comes to Visit: The Cognitive-Theoretic Model of the Universe
Abandon All Hope, Ye Who Enter This Thread
Wikepedia 
 
 Debate
 
 
Self-Reference and Computational Complexity
© 2001 by Christopher Michael Langan
[This paper contains background information on the P?NP problem. Being the first part of a 
much longer work-in-progress, it is for introductory purposes only. If anyone finds any errors 
– and there are sure to be a few, given the time crunch under which this was written – kindly 
let me know. The bibliography can wait for a future installment (meanwhile, thanks to 
Stephen Cook, A.K. Dewdney and Robert Rogers).]
Introduction: The Problematic Role of Self-Reference in Mathematics
Self-reference is an ever-popular topic in recreational mathematics, where it is usually 
characterized as a pathological rarity that must be treated with great care in order to avoid 
vicious paradoxes like this statement is false. To some extent, this reflects the history of 
mathematics; scarcely had Bertrand Russell presented his eponymous paradox in set 
theory, namely "the set of all sets that do not include themselves as elements", than he 
proposed to outlaw it with a stupefyingly complex theory that boiled down to a blanket 
prohibition on self-inclusion and its semantic equivalent, self-reference. Since then, 
mathematicians have regarded themselves as having two choices with regard to self-
reference: follow Russell’s prescription and avoid it, or use it (with great care) for the 
express purpose of extracting paradoxical inconsistencies from, and thereby disproving, 
certain especially problematic conjectures.
Mathematical reasoning equates to the direct or indirect derivation of theorems from 
primitive axioms. Ordinarily, a mathematical conjecture is proven correct by direct 
(deductive or inductive) derivation within an axiomatic system, or by showing that its 
negation leads to a contradiction or an absurdity within the system; similarly, a conjecture is 
proven incorrect by either directly deriving its negation, finding a counterexample, or 
showing that it leads to a contradiction or an absurdity, again within a single axiomatic 
system. Where each of these maneuvers is confined to a formal axiomatic system, and 
where the symbols, formulae and rules of inference of a formal system are taken to refer 
only to each other, they unavoidably involve self-reference. Indeed, any algebraically closed 
language treated apart from external meaning is self-referential by definition, and so is any 
process relying on the properties of such a language. To the extent that mathematics 
consists of such languages, mathematical reasoning is such a process. It follows that 
mathematics as a whole is implicitly self-referential in nature.

But certain limits are maintained nonetheless. For example, in order to avoid introducing a 
forbidden subjective component into what is supposed to be an objective enterprise (the 
quest for "absolute" mathematical truth), at no point is a mathematician ever supposed to be 
caught referring to himself or his own thought processes as part of a mathematical proof. 
Even when deliberately employing self-reference in proving a conjecture, the mathematician 
must punctiliously externalize and thereby objectivize it, hygienically confining it to a formula 
or abstract machine model. By confining self-reference to the object level of discourse, the 
mathematician hopes to ensure the noncircularity of a higher level of discourse from which 
the predicates true and false can be downwardly assigned with no troublesome rebound…a 
level hopefully inhabited by the mathematician himself. By avoiding self-reference on the 
"business" level of discourse, he hopes to avoid having his own reasoning falsified "from 
above" on the basis of what would otherwise be its circularity. In short, the trapper does not 
propose to be caught in his trap.
Unfortunately, this is a philosophically dubious position. It begins to unravel as we approach 
the foundations of mathematics, where the object is to trace the roots and account for the 
very origins of mathematical reasoning. At this stage of explanation, it becomes impossible 
to avoid the realization that in order to be consistent, mathematics must possess a kind of 
algebraic closure, and to this extent must be globally self-referential. Concisely, closure 
equals self- containment with respect to a relation or predicate, and this equates to self-
reference. E.g., the self-consistency of a system ultimately equates to the closure of that 
system with respect to consistency, and this describes a scenario in which every part of the 
system refers consistently to other parts of the system (and only thereto). At every internal 
point (mathematical datum) of the system mathematics, the following circularity applies: 
"mathematics refers consistently to mathematics". So mathematics is distributively self-
referential, and if this makes it globally vulnerable to some kind of implacable "meta-
mathematical" paradox, all we can do in response is learn to live with the danger. 
Fortunately, it turns out that we can reason our way out of such doubts…but only by 
admitting that self-reference is the name of the game.
When a language self-refers, it stratifies or separates into levels, with reference flowing 
directionally from metalanguage to language. E.g., each the following statements – "this 
statement is about itself", "the subject of this statement is this statement", and "this formula 
x is a description of x" – is actually a statement and the object of a statement, with 
statement and object occupying the metalanguage and object levels respectively. The 
operative rule in such cases is that reference never flows upward from object to statement, 
but only downward (from metalanguage to object) or laterally (from object to object, by 
virtue of the expression of these objects within a higher-level metalanguage mediating their 
mutual influence). This stratification is very important from a proof-theoretic standpoint, as 
the following example shows.
Theorem: "This statement is false" is false.
Proof: If the statement in quotes is indeed false, then it is true. On the other hand, if it is 
true, then it is false. This is a contradiction. Since the quoted statement generates a 
contradiction, it is logically inconsistent and therefore false. (Q.E.D.)

But wait! Unfortunately, if the quoted statement is false, then it is true (as stated in the 
proof). This would seem to contradict not only the overall statement including the quoted 
statement, i.e. the "theorem", but the proof as well…unless we have a rule saying that the 
statement in quotes can refer to neither the overall statement of which it is part, nor to the 
proof of the overall statement. In that case, it can invalidate only itself, which is exactly what 
it is taken to be doing, and can do so only within a metalanguage capable of expressing the 
reflexive self-invalidating relationship. It should be noted that technically, "this statement is 
false" is invalid on purely formal grounds; it is in fact a forbidden instance of self- reference. 
But since it is analogous to any statement that implies its own negation in an axiomatic 
context - and such statements are routinely dealt with in mathematics without immediate 
concern for their "bad syntax" - its clarity makes it valuable for illustrative purposes.
In the above example, self- reference is confined to a formula that pronounces itself false. 
Because this formula refers negatively to its own veracity, it is a self-contained paradox 
attempting to double as its own falsifying metalanguage and thus possessing a whole new 
level of "falsehood". But aside from true or false, what else could a formula say about itself? 
Could it pronounce itself, say, unprovable? Let’s try it: "This formula is unprovable". If the 
given formula is in fact unprovable, then it is true (and therefore a theorem). But sadly, we 
cannot recognize it as such without a proof. On the other hand, suppose it is provable. Then 
it is false (because its provability contradicts what it states of itself) and yet true (because 
provable)! It seems that we still have the makings of a paradox…a statement that is 
"provably unprovable" and therefore absurd.
But what if we now introduce a distinction between levels of proof, calling one level the 
basic or "language" level and the other (higher) level the "metalanguage" level? Then we 
would have either a statement that can be metalinguistically proven to be linguistically 
unprovable, and thus recognizable as a theorem conveying valuable information about the 
limitations of the basic language, or a statement that cannot be metalinguistically proven to 
be linguistically unprovable, which, though uninformative, is at least not a paradox. Presto: 
self-reference without the possibility of paradox! In the year 1931, an Austrian mathematical 
logician named Kurt Godel actually performed this magic trick for the entertainment and 
edification of the mathematical world.
Undecidability: the Power of Paradox
As part of his Erlangen Program for 20th century mathematics, the eminent German 
mathematician David Hilbert had wanted to ascertain that mathematics is "complete" when 
formalized in a system like the predicate calculus…i.e., that all true theorems (and no false 
ones) can be proven. But after helpfully setting out to show that every true formula 
expressed in the predicate calculus is provable, Kurt Godel sadly arrived at a different 
conclusion. As it turns out, not even arithmetic is complete in this sense. In any consistent 
formal system containing arithmetic, there are true but unprovable statements. Although 
their truth can be directly apprehended, they cannot be formally derived from a finite set of 
axioms in a stepwise fashion.
First, Godel posited axioms and rules of inference for deriving new true formulas from true 
formulas already in hand. These were just the axioms and rules of the first - order predicate 
calculus, comprising the logic of proof throughout the mathematical world. To these he 

adjoined the Peano postulates of standard arithmetic, a set of axioms and rules for the 
natural numbers. These consist of 6 basic axioms incorporating the equality symbol "=", 3 
additional axioms to define the meaning of that symbol, and a rule of induction 
corresponding to the meta-level rule of inference adjoined to the six basic axioms of the 
predicate calculus. The predicate calculus and the Peano postulates, together comprising 
15 axioms and 2 rules of inference, define a powerful and comprehensive system for the 
expression and proof of arithmetical truth.
After defining his arithmetical system, Godel’s next step was to enable the formulation of 
self-referential statements within it. Because arithmetic refers to numbers, this meant 
assigning numbers to arithmetical formulae. He accomplished this by (1) assigning a natural 
number or "code number" Î {1,2,…,15} to each of the 15 distinct logical and arithmetical 
symbols employed in the system; (2) numbering the symbols in each formula from left to 
right with the consecutive prime "placeholder" numbers 2,3,5,7,…; (3) raising each prime 
placeholder number to a power equal to the code number of the symbol in that place; and 
(4) multiplying all of the resulting (large but computable) numbers together to get the Godel 
number of the formula (note that this procedure is reversible; given any Godel number, one 
may compute the expression it encodes by factoring it into a unique set of prime factors). In 
addition, Godel numbered proofs and partial derivations, i.e. deductive sequences of r 
consecutive formulae, with the products of r numbers 2 m ´ 3n ´ 5p ´ …, where the bases are 
the first r primes and the exponents m, n, p,… are the Godel numbers of the first, second, 
third,… formulae in the sequence.
In this way, every arithmetical predicate expressible by a formula or sequence of formulae is 
Godel-numbered, including the predicate "is a proof of (some formula)" and "(this formula) 
cannot be proven" [equivalent formulations with universal and existential quantifiers: "for all 
numbers x, x is not the Godel number of a proof of (this formula)"; "there does not exist a 
number…that is the Godel number of a proof of (this formula)"]. It is important to realize that 
these predicates actually correspond to real numeric relationships "isomorphically 
hybridized" with logical relationships. The biological flavor of this terminology is not 
accidental, for the isomorphism guarantees that numeric relationships areheritable as their 
logical isomorphs are derived. When one logical formula is derived by substitution from 
another, they become numerically related in such a way that a distinctive numeric predicate 
of the Godel number of the ancestral formula is effectively inherited by the Godel number of 
the descendant formula. This maintains a basis for consistency, ensuring that the negation 
of the numeric predicate of a formula cannot be derived from the numeric predicate of the 
formula itself.
Let us be a bit more specific. Godel’s idea was to express the logical syntax of arithmetic, 
which is ordinarily formulated in terms of logical and arithmetical symbols, in terms of pure 
numeric relationships. To do this, the various logical relationships within and among 
syntactic formulae must be mirrored by numeric relationships among the (Godel-numeric) 
images of these formulae under Godel’s logic-to-number mapping. The key property that 
allows this mirroring to occur is called representability. An n-ary numeric relation R(x1,…,xn) 
is representable in the first-order Peano arithmetic N iff there is in N a formula A(a1,…,an) 
with n free variables such that for all natural numbers k1,…,kn, the following conditions hold:
1. If R(k1,…,kn) holds, then |--NA(k1,…,kn), i.e. A(k1,…,kn) is provable in N 

2. If R(k1,…,kn) does not hold, then |--N~A(k1,…,kn) 
In this case, we say that A(a1,…,an) represents the numeric relation R. E.g., let R be
the "less than" relation among natural numbers. Then R is representable in N because there 
is a formula x<y with two free variables (x and y) such that for any natural numbers k1 and 
k2, (1) k1<k2 is provable in N whenever k1 is less than k2, and (2)
not-( k1<k2) is provable in N whenever k1 is not less than k2. Representability is similarly 
defined for numeric functions (as opposed to relations).
Now consider the predicate "Proof(x,y,z)", meaning "x is the Godel number of a proof X of a 
formula Y which has one free variable, Godel number y, and the integer z substituted for its 
free variable". This predicate corresponds to a rather long expression which includes 
several computable procedures reducible to formulae, including (1) given an integer, find 
the string of which it is the Godel number; (2) given a string, determine whether it is a 
formula (i.e., syntactically well-formed); and (3) given a sequence of formulae, determine 
whether it is a proof for the last formula in the sequence.
Suppose that the Godel number y of a formula Y is substituted for its one free variable, 
resulting in the formula "Proof(x,y,y)", and construct an expression which denies the 
existence of a proof of this formula within formal arithmetic: ~$xProof(x,y,y) ("x is the Godel 
number of a proof X of a formula Y which has one free variable, Godel number y, and its 
own Godel number y substituted for its free variable"). This formula G, which already makes 
a statement about the "self-referential" formula Y, obviously has a Godel number g; to make 
the entire formula G "self-refer" like Y, we need merely write ~ $xProof(x,g,g) = G’, where G’ 
has Godel number g’ (G’ is a logical descendant of G whose Godel number is related to that 
of G in a distinctive way). This leads to
Godel’s theorem: ~$xProof(x,g,g) is true but unprovable in formal arithmetic.
Metamathematical Proof: Suppose there is a proof P for ~$ xProof(x,g,g). If so, let its 
Godel number be p, so that Proof(p,g’,g’) is true. But this contradicts the content of the 
formula ~$xProof(x,y,y) represented by g, which is the ancestor of ~$xProof(x,g,g), an 
integral part of Godel’s theorem that inherits its characteristic arithmetical attribute. The 
contradiction is not merely formal; Proof(p,g’,g’) expresses an arithmetical relationship that 
is the negation of that expressed by ~$xProof(x,g,g) . Consequently, P cannot exist; it lacks 
(and in fact negates) the required arithmetical attribute. Form is out of sync with content; in 
the variable g’, P = Proof(p,g’,g’) contains its antithesis, ~$xProof(x,g,g). But this is just the 
content of the theorem to be proven. (Q.E.D.)
Notice that Godel utilizes the same levels of reference that we utilized in proving the 
theorem "This sentence is false" is false. The statement ~$xProof(x,g,g) corresponds to 
"This sentence is false"; the metastatement ~$xProof(x,g,g) is true but unprovable in formal 
arithmetic corresponds to "This sentence is false" is false; and the proof of each 
metastatement resides on a higher level still (the genius of Godel’s proof is the way in which 
all of these levels of "metamathematical" discourse, including formal arithmetic, are mapped 

into the number system, thus letting the number system serve as the logical metalanguage 
of its own arithmetical syntax). Godel’s proof is called metamathematical because it 
effectively refers to every part of mathematics incorporating the principle of transfinite 
induction. I.e., in proving that there exists a true but axiomatically unprovable theorem of 
arithmetic, it is proven that mathematics as a whole contains such theorems. This is just 
what David Hilbert and many other mathematicians had not wanted to hear.
Concisely, Godel was able to show that if his formula ~$xProof(x,g,g) were provable, then 
its negation or "formal contradictory" would be provable as well; conversely, if the negation 
were provable, then so would be the formula. But any set of axioms from which both a 
formula and its negation can be derived is inconsistent. So if the axioms of formal arithmetic 
are consistent, then neither the formula nor its negation is demonstrable. In other words, 
mathematics – which includes formal arithmetic - cannot be both complete and consistent; if 
it is consistent, then it is necessarily incomplete (note that the converse is not implied; 
consistency does not necessarily follow from incompleteness). The technical name for this 
eternal tug-of-war between consistency and completeness is undecidability. As we have just 
seen, the predicate undecidable, as applied to individual formulae, is synonymous with "true 
but unprovable" within a given formal system, implying that the system in question is 
consistent but incomplete.
In some respects, proving a theorem is like solving a problem. Indeed, the proof of a 
theorem is merely the "solution" of the "problem" of how to derive the theorem, and the 
rules of proof, i.e. of derivation from axioms, thus define a problem-solving procedure or 
"algorithm". Conversely, using an algorithm to find a solution Y for any problem X amounts 
to deriving a theorem stating that Y is a solution of X. Prior to Godel, it had been thought 
that all theorems were axiomatically provable, and thus that all problems were 
algorithmically soluble. But with Godel’s results came the realization that problems can to 
some extent be classified according to their "degree of solubility". Obviously, those 
problems whose solutions were not axiomatically (or algorithmically) derivable comprised a 
new and previously unknown class, that of undecidable problems in general.
Loosely, a complexity class is a set of distinct but similar problems sharing the same worst-
case requirements for computing a solution (at the time of Godel’s proof, there was no 
obvious way to express these requirements, and thus a shortage of distinct complexity 
classes). In effect, Godel’s proof was a diagonalization taking all members of the complexity 
class of solvable or decidable problems, constructing an element differing from each of 
these members (~ $xProof(x,g,g)), and adjoining this element to the original class in order 
to create an extension thereof. By the nature of the construction of this element, it clearly 
belongs to a new and distinct complexity class…the one embodied by the extension. This 
gives rise to new kind of theorem, a separation theorem asserting the distinctness of two 
complexity classes, those of decidable and undecidable problems respectively. Since 
Godel’s day, many other separation theorems and related conjectures have been 
introduced. But to follow the action, we need to know how to formulate the computational 
requirements for solving problems.
Computation Theory
Ordinarily, we think of computation as "what computers do", and thus as an objective 

mechanical process independent of the specific nature of that which is to be computed. This 
is reflected in an actual computer’s ability to accept any kind of program and any 
appropriate set of inputs. But for certain purposes, this view is not entirely adequate; 
ultimately, the terms problem, solution and computation are mutually defined. Simplistically, 
computation is an input-to-output transformation relating certain configurations of data and 
information, namely problems and their solutions; problems are mathematical objects 
presented as input to computational entities which then yield solutions as output; and 
solutions comprise a subset of the terminal outputs of these computational entities upon 
being presented with problems as input.
While these definitions can naturally be refined, the point is that none of the defined 
concepts is meaningful without the others. In particular, computers are problem-solving 
entities and therefore essentially algorithmic in nature; if there were no problems to be 
solved, and no algorithmic computer programs to solve them, there would be nothing to 
compute (an algorithm is a step-by-step procedure that solves a problem or produces a 
desired result). At best, there would be only a random, aimless processing of data to which 
the notion of "computational requirements" would be irrelevant. The structure of any 
computer, including its architecture and the full range of compatible software, is thus that of 
a "generalized self-executing algorithm". It was in this utilitarian form, with no explicit 
hardware-software distinction, that modern computers were first conceived.
Like any process, computation requires a model. A model of computation is just an 
"abstract computer" with respect to which it is possible to express computational 
requirements like execution time and memory space for various problem-algorithm 
combinations. Different models of computation differ in "computing power", including the 
kind of computations that can be performed and their costs in time and space (i.e., in the 
numbers of computational steps and the amounts of memory they require). However, 
because these differences are a matter of various "mechanical constraints" built into the 
abstract models themselves – e.g., rules about how memory is accessed, the orders in 
which various operations can be performed, and how many things can be processed at 
once - the requirements for one model can generally be translated into those of another 
simply by modifying the constraints and monitoring the consequences.
The standard model of computation is the Turing machine, a "read-write" automaton whose 
design incorporates one or more tapes from which input is read and to which output is 
written. Such a machine has internal structure and programming in terms of which its input 
is interpreted as a problem, and through the dynamics and execution of which the solution 
is (or cannot be) delivered as output. Where a solution is delivered, the machine halts in an 
affirmative state; where not, it halts in a negative state (or in the event of "incomputability", 
never halts). Given a Turing machine M, there are two cases in which a solution cannot be 
delivered and M halts in a negative state: (1) M cannot interpret its input I as a meaningful 
problem in terms of its architecture and programming (hard and soft structure), and 
therefore cannot accept or recognize it; (2) after accepting I as a meaningful problem, M 
cannot go on to accept I or one of its transforms as a soluble problem, and therefore 
terminates its computation without having reached a solution.
Information consists of patterns of data with locative or predictive value. Like human 
mathematicians, Turing machines are information transducers that convert one form of 

information to another…i.e., problems into solutions. The tape(s) are analogous to a 
mathematician’s scratch pad, containing cells in which symbols can be read, written or 
erased; its movable read and write/erase heads are analogous to his eyes and pencil; and 
its state transitions are analogous to his mental processes as he pursues his calculations. 
What is written on the tape is informative with respect to not just the tape’s own contents, 
but the internal states of the machine – the output of each successive step complements 
the machine’s own structure and programming as a basis for predicting the next state - and 
the machine’s stepwise passage through a sequence of intermediate internal and external 
(machine and tape) states is information transduction. That is, a Turing machine is an 
"information transducer". If a given property holds for information transducers, it holds a 
fortiori for Turing machines, and in order to prove something true of Turing machines, it 
suffices to prove it for information transducers.
Now let us refine the model a bit. A Turing machine, which can assume any of a finite 
number of internal states, consists of a read/write head and one or more (finite or infinite) 
tapes divided into squares that each contains a 0 or a 1. Beginning in an initial state, the 
machine scans a square, erases what it finds there, prints a 0 or 1, moves to an adjacent 
square and enters a new state. All information on its behavior is contained in three 
parameters: its state, the number in the square being scanned, and a finite instruction table 
called its state transition function. For each state-input combination, the table tells the 
machine what to write, in which direction to move, and what state to enter (the machine’s 
functional states are implicitly defined by their occurrences in the table). A Turing machine 
with one state transition per state- input pair is deterministic; one with several possible 
transitions is nondeterministic; one whose table gives probabilities for various possible 
transitions is probabilistic; and one that can query a hidden set on whether it contains a 
given item is oracular. A deterministic Turing machine, whose software-like abstractness 
lets it be implemented in various physical devices, can compute any computable function 
given enough time and tape.
Like computation theory in general, Turing machines have one foot in the realm of abstract 
mathematical objects and one foot in the world of concrete physical devices. This is what 
makes them so useful as models of practical, real-world mathematical computation. So it is 
natural to ask whether self reference and diagonalization can be extended to the realm of 
Turing machines. The answer was provided by their inventor, the British mathematician 
Alan Turing, in his celebrated attack on the Halting problem. The Halting Problem is a 
refined form of Hilbert’s Decision Problem, which in turn was closely related to his 
Completeness Problem as solved by Godel. The Decision Problem asks for a yes-or-no 
answer to the question of whether there is one effective procedure for predicting whether or 
not a given conclusion follows from a given set of assumptions… i.e., whether there is a 
single master algorithm that can solve all problems or decide all mathematical questions. 
Unfortunately, even after the labors of Godel, it was not entirely clear what a so-called 
"effective procedure" should look like. Turing’s contribution was to provide a model of 
computation in terms of which to define effective procedures, and then to reformulate and 
solve the Decision Problem in terms of this model and definition.
Incomputability
The Halting Problem asks whether or not there exists a Turing Machine TD which,

when given an input tape holding a description dT of another Turing machine T and a 
duplicate of its arbitrary semi-infinite tape t, can decide whether T halts on t. If
TD exists, then it has just two possible halting states:
1. If T halts on t, then TD says YES and transitions from a penultimate state qi to an 
affirmative halting state qh. (YES: qi◊ qh) 
2. If T does not halt on t, then TD says NO and transitions from a penultimate state qj to a 
negative halting state qk. (NO: qj◊ qk) 
Can TD decide whether T halts on a self-description, i.e. a tape containing dT? Suppose that 
TD contains a "copying machine" TC which converts TD’s semi-infinite
tape, with contents dT and t, to a compact tape containing only dT, and then transitions 
from its own final state ("finished copying dT") to TD’s initial state. Let
T’D be the compound machine formed by grafting the copying machine TC onto TD: T’D = {TC, 
TD} (notice that T’ D has the same halting transitions 1 and 2 as TD.) Now transform T’D to T"D 
by modifying, i.e. bypassing, the halting transitions 1 and 2 as follows:
T’D
T"D
(YES:qi◊qh) ◊ (qi◊qn/loop) [the affirmative halting state qh of T’D is bypassed
in T"D]
(NO:qj◊ qk) ◊ (qj ◊qy) [the negative halting state qk of T’D is bypassed in T"D]]
That is, once T"D attains state qn, there is a looping transition back to q n for every possible 
state/input combination, i.e. configuration, of T’D. Thus, once in state qn, T"D will never halt. 
Once in state qy, however, T"D halts (because qy is by definition a halting state). In other 
words, T"D is a machine that halts iff it determines that
TD does not halt on T, and does not halt otherwise (note that this Turing machine T"D , 
despite its contrary behavior in not halting when T’D halts, is defined in a perfectly legitimate 
way, without breaking any of the rules governing such devices). What happens when T"D is 
given the tape dT"D (as opposed to the "generic" tape dT)? If T"D halts on dT"D, then it 
transitions from the penultimate state qi of TD and T’D to the looping state qn and cycles 
forever; i.e. if T"D halts on dT"D, then
T"D does not halt on dT"D. Similarly, if T"D does not halt on dT"D, then it transitions from the 
penultimate state qj of TD and T’ D to the halting state qy and halts; i.e. if T"D does not halt on 
dT"D, then T" D does halt on dT"D. These contradictions imply that no such machine as TD 
exists.
Notice the way self- reference is used to effect diagonalization. Self-reference occurs when 
T"D is given a self-description (the tape dT"D), and diagonalization occurs when this instance 
of self-reference leads to a contradiction implying the existence of an incomputable "halting 
function" that accepts a Turing machine and its tape as arguments and outputs a yes-or-no 

decision on whether or not that machine will halt on that tape. Turing’s proof diagonalized 
the set of computable functions (algorithms) to produce an element not included in that set 
(the halting function), thus distinguishing a whole new set of incomputable functions. 
Because the set of computable functions and the set of incomputable functions are 
complexity classes - sets of problems, algorithms or functions sharing the same worst-case 
computational requirements - Turing’s proof gives rise to a separation theorem asserting 
their distinctness. Just as Godel had (in effect) classified problem solutions as decidable or 
undecidable by derivation from axioms, Turing classified functions as computable or 
incomputable by means of effective procedures.
Thanks to Turing and an American counterpart named Alonzo Church (who managed to 
achieve parallel results by defining effective procedures arithmetically rather than in terms 
of a machine model), self-reference and diagonalization were taken to a whole new level…a 
level uniting the abstract and the concrete. The power of paradox had been harnessed as 
never before. So great was this achievement that it might have seemed the end of the line 
for this particular route to mathematical knowledge, at least in terms of major insights. But 
as is often the case in mathematics, new insight eventually became a platform for new 
problems and unforeseen investigations. For
once we have a distinction between the decidable and the undecidable, and the computable 
and the incomputable, we have the makings of a scale spanning each pair of poles…a 
scale requiring calibration in degrees of decidability and computability respectively. With 
respect to the difficulty of mathematical problems, the computational complexity parameter 
is called tractability.
P?NP
For practical purposes, the classes of undecidable problems and incomputable functions 
mark the upper end of the tractability scale. No matter how big our computer or how long we 
let it work on an undecidable problem or incomputable function, a solution will never be 
algorithmically produced. Even were a solution handed to us, it could not be checked and 
validated. On the other hand, what we find down towards the middle of the scale are 
problems considerably more tractable in nature. Even in the worst possible case, 
algorithmically computing an exact solution for one of these "tractable" problems requires a 
number of computational steps and/or an amount of memory that can be expressed as a 
polynomial function of the number of variables it contains.
But here ends the simplicity. For between the middle and the upper extreme of the scale, 
there resides a species of problem that can only be called intractable. While the solutions of 
such problems, should we be lucky enough to guess them, can always be checked for 
correctness by a polynomial algorithm, they cannot be polynomially computed. While exact 
solutions for intractable problems can be computed by exhaustion algorithms that check all 
possibilities at every decisive juncture, the costs of using such an algorithm are prohibitive 
in time and memory. In the worst case, using such an algorithm requires a number of 
computational steps and/or an amount of memory that cannot be expressed as a 
polynomial function of the number of variables in the problem. Bigger, faster-growing 
functions, e.g. exponential or factorial functions, seem to be required. Unfortunately, such 
functions expand so rapidly that it is not uncommon for the requirements they express to 
exceed the capacity of a computer bigger and older than the physical universe! Where the 

only available algorithm is exhaustive, the only way to get an exact solution in polynomial 
time is to let a nondeterministic Turing machine "guess" it and then use a polynomial-time 
"checking function" to see if the guess was right.
Among the problems whose solutions can be checked in polynomial time and are thus 
known to be in the class NP, some are known to be "universal" in the sense that all other 
problems in the genre can be reduced to them in polynomial time. That is, any instance of 
any problem in the class NP can be polynomially converted into an instance of the universal 
problem in such a way that if the reduced instance is solved, the solution can be 
polynomially translated back into a solution of the original problem. These universal 
problems are called NP-complete. Because of this reducibility, an algorithm that can solve 
any NP-complete problem in polynomial time can in principle solve any problem in the 
class. All we would have to do is graft a set of polynomial reduction algorithms onto the 
main NP-complete algorithm, convert any NP problem we might encounter to the one 
solved by the main algorithm, solve the reduction, and then translate the solution back into 
the language of the original problem. But reducibility works the other way as well. Were we 
to prove that any NP-complete problem is not polynomially solvable, we would know the 
same of every problem in the class. (If nothing else, this would tell us not to feel guilty for 
concentrating on the development of fast approximation algorithms.)
That which seems to be true is not always true in fact, and this applies as often in 
mathematics as in everyday life. So in the absence of hard mathematical proof, 
mathematicians are reluctant to assume that the classes of tractable and intractable 
problems are really different. Instead, they consider two possibilities. (1) All problems are 
ultimately tractable, but we haven’t found good (polynomial) algorithms for all of them yet. 
(2) Some problems, being innately intractable,have no good algorithms; thus, while we can 
always trust to luck and use a nondeterministic Turing machine to guess and check their 
solutions in polynomial time, we cannot do so reliably by using a deterministic algorithm. 
Where we call the complexity class of tractable problems P (for Polynomial, as in "solvable 
in polynomial time and space by a deterministic Turing machine") and the class containing 
both tractable and intractable-but-checkable problems NP (for Nondeterministic Polynomial, 
as in "solvable in polynomial time and space by a nondeterministic Turing machine"), these 
possibilities can be formulated as follows: (1) P=NP; (2) P¹ NP. Because the true relation 
sign is unknown, the problem of deciding between these two hypotheses can be written P?
NP. P?NP is widely considered one of the most urgent open problems of mathematics.
When trying to solve hard problems, it usually pays to try methods that have worked against 
similar problems in the past. Because P¹ NP would amount to a separation theorem 
distinguishing between two disjoint complexity classes, namely P and (NP– P), it is only 
natural to try the same method used to prove other separation theorems: self-reference and 
diagonalization. As we have seen, this was the technique successfully employed by Godel 
and Turing to build the scale of complexity on which the boundary between tractability (P) 
and intractability (NP-P) would be a critical division. However, there are certain differences. 
The separation theorems thereby established were of an all-or-nothing variety; where no 
algorithms exist, algorithmic efficiency is irrelevant. From the original perspectives of Godel 
and Turing, making distinctions regarding algorithmic efficiency would probably have 
amounted to hairsplitting. So it is by no means assured that diagonalization and reduction 
will work on P?NP. And as we are about to see, recent work on the problem provides no 

special hope that it will.
Oracles and Relativization
A favorite technique of mathematicians everywhere is to assume that a difficult unsolved 
problem can be solved, and then develop the implications of this assumption in the hope of 
identifying some of the properties a solution would have if it existed. In the field of 
computational complexity, this often boils down to the invocation of oracles. An oracle is a 
set, encoded on a special tape in an OTM (Oracle Turing Machine), to which the machine 
can issue queries. Essentially, the oracle is just what its name implies: a fund of easy 
answers for questions that would otherwise be difficult or impossible for the machine to 
answer. Thus, if an OTM needs to know whether a particular instance of a particular 
problem is solvable, all it needs to do is ask an oracle containing all solvable instances of 
that problem whether or not it contains that instance. Where the oracle can answer 
instantaneously, the total time required for this operation is just the time needed to access 
the oracle and issue the query, plus the time needed to receive the answer. So as long as 
both of these times are polynomial, the entire process can be completed in polynomial time. 
If a polynomial-time solution exists given this oracle, then the intractable part of the problem 
has been conveniently isolated within the oracle.
A certain general relationship obtains between oracles and the worst-case lower bounds on 
computation established by diagonalization and reduction: in general, such bounds 
relativize, i.e. continue to apply in a context in which membership queries can be made to 
an arbitrary oracle set. This kind of relativizability can be regarded as a "measure of 
robustness" of such results; for example, Turing’s solution of the Halting problem relativizes 
because it is "too strong" to be affected by any particular choice of oracle. Unfortunately, 
hopes that the P?NP problem would be solved by diagonalization with reduction were 
dampened by the 1975 paper of Baker, Gill and Solovay, which employed an oracle set 
relative to which P=NP (as well as another relative to which P¹ NP). This suggests that the 
two classes P and (NP-P) cannot be separated by diagonalization and reduction, much as a 
pair of Siamese twins cannot be separated with a chainsaw. This, of course, throws into 
doubt the prospect that the P?NP problem can be resolved at all, giving rise to another hard 
question: where next?
The answer: nobody seems to know. In fact, one of the current trends is to shift attention 
away from Turing machines to a new model of computation involving acyclic Boolean 
circuits or "language processors". To understand how these work, we need a bit more 
background. Just as functions generalize the concept of algorithms, languages are 
generalizations of problems. Where a language is a set of strings over some fixed alphabet, 
and a problem is a set of instances(input strings) to be fed to an algorithm, the set of 
languages clearly includes the set of problems. So instead of asking whether a problem can 
be solved, we can ask whether its language – i.e., its set of YES instances - can be 
recognized by a given computational model (Turing machine, Boolean circuit). E.g., instead 
of asking if there exists an algorithm to solve the NP- complete problem of determining 
whether a graph can be colored with 3 colors so that no two adjacent vertices have the 
same color, we can ask if any Turing machine or Boolean circuit can "recognize the 
language" consisting of all 3-colorable graphs.

A Boolean circuit is a finite acyclic graph with n input nodes and one output node, where 
each non-input node is a "logic gate" typically functioning as one of the three Boolean 
connectives AND, OR, NOT (in most contexts, "acyclic" is synonymous with "non-self-
referential"). The input nodes are labeled with the names x1,…,xn of variables comprising an 
instance of a problem to be solved – e.g., the |V|C2 possible edges of a |V|-vertex graph to 
be 3-colored - and for each assignment of 1 or 0 to each variable, the circuit computes a bit 
value at each gate (including the output gate). In effect, the structure of Bn is that of an 
algorithm which computes truth values for strings of length n on any assignment of truth 
values to its input variables (e.g., on any prospective "instance" of G3C or Graph 3-
Colorability). Equivalently, Bn is a hard-wired recognition algorithm for the language L 
consisting of strings {s} in n variables for which the output of Bn is 1. If L is a language over 
{0,1} in complexity class P, then there is a polynomial-size family {Bn} of Boolean circuits 
such that Bn has n inputs, and for each input string s of length n, the output bit of Bn is
1 iff s Î L (when s is applied to the inputs of Bn). In this case, it is fair to say that
Bn computes L. Because of the relationship between the complexity of a problem and the 
size of its family of Boolean circuits, establishing an exponential lower bound on the size of 
the family of circuits corresponding to any NP-complete problem would effectively prove that 
P¹ NP.
Unfortunately, the Boolean circuit model has proven less fruitful than was hoped for 
establishing computational bounds. As a result, researchers now find themselves as needful 
as ever of a breakthrough in computation theory. In all likelihood, many of these 
researchers continue to wonder whether the diagonalization concept can somehow be 
taken farther than it already has been…whether the brilliant successes of Godel and Turing 
can be repeated. This interesting question will be the topic of a future paper.
Chris Langan
Cheating the Millennium: The Mounting Explanatory Debts of Scientific Naturalism
2003 Christopher Michael Langan
1.  Introduction: Thesis + Antithesis = Synthesis 
2.  Two Theories of Biological Causality 
3.  Causality According to Intelligent Design Theory 
4.  Causality According to Neo-Darwinism 
5.  A Deeper Look at Causality: The Connectivity Problem 
6.  The Dualism Problem 
7.  The Structure Problem 
8.  The Containment Problem 
9.  The Utility (Selection) Problem 
10. The Stratification Problem 
11. Synthesis: Some Essential Features of a Unifying Model of Nature and Causality 
Introduction: Thesis + Antithesis = Synthesis

In agreeing to write this essay, I have promised to explain why I find Darwinism 
unconvincing. In order to keep this promise, I will be compelled to acknowledge the 
apparently paradoxical fact that I find it convincing as well. I find it convincing because it is 
in certain respects correct, and in fact tautologically so in the logical sense; I find it 
unconvincing because it is based on a weak and superficial understanding of causality and 
is therefore incomplete. Explaining why this is so will require a rather deep investigation of 
the nature of causality. It will also require not only that a direction of progress be indicated, 
but that a new synthesis embracing the seemingly antithetical notions of teleology and 
natural selection be outlined. But first, some essential background.
It would be hard to imagine philosophical issues bearing more strongly on the human 
condition than the nature of life and the meaning of human existence, and it would be hard 
to imagine a scientific issue bearing more strongly on the nature and meaning of life than 
biological origins. Our view of evolutionary biology, whatever it happens to be at any 
particular juncture, tells us much of what we believe about who and what we are and why 
we are here, unavoidably affecting how we view (and ultimately, treat) ourselves and each 
other. Unfortunately, the prevailing theory of biological origins seems to be telling us that at 
least one of these questions, why are we here?, is meaningless1…or at least this is the 
message that many of us, whether or not we are directly aware of it, seem to have received. 
As a result, the brightest hope of the new millennium, that we would see the dawn of a New 
Enlightenment in which the Meaning of it All would at last be revealed, already seems to 
have gone the way of an extravagant campaign promise at an inauguration ceremony.
The field of evolutionary biology is currently dominated by neo-Darwinism, a troubled 
marriage of convenience between post-Mendelian genetics and natural selection, a concept 
propounded by the naturalist Charles Darwin in his influential treatise On the Origin of 
Species .2 It has often been noted that the field and the theory appear to be inseparable; in 
many respects, it seems that evolutionary biology and Darwinism originated and evolve 
together, leading some to conclude that the field properly contains nothing that is not 
already accommodated by the theory.
Those attempting to justify this view frequently assert that the limitations of the theory are 
just the general limitations imposed on all scientific theories by standard scientific 
methodology, and that to exceed the expressive limitations of the theory is thus to 
transgress the boundaries of science. Others have noted that this seems to assume a prior 
justification of scientific methodology that does not in fact exist – merely that it works for 
certain purposes does not imply that it is optimal, particularly when it is evidently useless for 
others - and that in any case, the putative falsifiability of neo-Darwinism distinguishes it from 
any definition of science according to which the truth or falsity of such theories can be 
scientifically determined.3 Nevertheless, neo-Darwinism continues to claim exclusive 
dominion over the “science” of evolutionary biology.
Until the latter part of the 18th century, the story was quite different. People tended to regard 
the matter of biological origins in a religious light. The universe was widely considered to 
have been freely and purposively designed and created by God as described in the Book of 
Genesis, and divine purpose was thought to be immanent in nature and open to observation 
and study. This doctrine, called teleology, drew rational support from traditional theological 
“arguments from design” holding that nature could only have been designed and created by 

a supreme intelligence. But teleology began to wane with the rise of British empiricism, and 
by the time Darwin published his theory in 1859, the winds of change were howling his 
anthem. Since then, the decline of teleology has accelerated to a point at which every 
supposedly universal law of nature is confidently presented as “irrefutable evidence” that 
natural events unfold independently of intent, and that purpose, divine or otherwise, is 
irrelevant to natural causation.
The concept of teleology remains alive nonetheless, having recently been granted a 
scientific reprieve in the form of Intelligent Design theory. “ID theory” holds that the 
complexity of biological systems implies the involvement of empirically detectable intelligent 
causes in nature. Although the roots of ID theory can be traced back to theological 
arguments from design, it is explicitly scientific rather than theological in character, and has 
thus been presented on the same basis as any other scientific hypothesis awaiting scientific 
confirmation.4
Rather than confining itself to theological or teleological causation, ID theory technically 
allows for any kind of intelligent designer – a human being, an artificial intelligence, even 
sentient aliens. This reflects the idea that intelligence is a generic quality which leaves a 
signature identifiable by techniques already heavily employed in such fields as 
cryptography, anthropology, forensics and computer science. It remains only to note that 
while explaining the inherent complexity of such a material designer would launch an 
explanatory regress that could end only with some sort of Prime Mover, thus coming down 
to something very much like teleology after all, ID theory has thus far committed itself only 
to design inference. That is, it currently proposes only to explain complex biological 
phenomena in terms of design, not to explain the designer itself.5 With regard to deeper 
levels of explanation, the field remains open.
Because neo-Darwinism is held forth as a “synthesis” of Darwinian natural selection and 
post-Mendelian genetics, it is sometimes referred to as the “Modern Synthesis”. However, it 
appears to fall somewhat short of this title, for not only is its basic approach to evolutionary 
biology no longer especially modern, but despite the fact that it is a minority viewpoint 
counterbalanced by cogent and far more popular alternatives including theistic evolution6 
and ID theory7, it actively resists meaningful extension. Many of its most influential 
proponents have dismissed ID theory virtually on sight, declaring themselves needless of 
justification or remedial dialectic despite the many points raised against them, and this is not 
something that the proponents of a “modern synthesis” would ordinarily have the privilege of 
doing. A synthesis is ordinarily expected to accommodate both sides of a controversy 
regarding its subject matter, not just the side favored by the synthesist.8
Given the dissonance of the neo-Darwinist and teleological viewpoints, it is hardly surprising 
that many modern authors and scientists regard the neo-Darwinian and teleological theories 
of biological evolution as mutually irreconcilable, dwelling on their differences and ignoring 
their commonalities. Each side of the debate seems intent on pointing out the real or 
imagined deficiencies of the other while resting its case on its own real or imagined virtues. 
This paper will take a road less traveled, treating the opposition of these views as a problem 
of reconciliation and seeking a consistent, comprehensive framework in which to combine 
their strengths, decide their differences, and unite them in synergy. To the extent that both 
theories can be interpreted in such a framework, any apparent points of contradiction would 

be separated by context, and irreconcilable differences thereby avoided.
The ideal reconciliatory framework would be self-contained but comprehensive, meaning 
that both theories could be truthfully interpreted within it to the maximum possible extent, 
and consistent, meaning that irreconcilable differences between the theories could not 
survive the interpretation process. It would also reveal any biconditionality between the two 
theories; were they in any way to imply each other, this would be made explicit. For 
example, were a logical extension of neo-Darwinism to somehow yield ID-related concepts 
such as teleological agency and teleological causation, these would be seen to emerge 
from neo-Darwinist premises; conversely, were ID-theoretic concepts to yield ingredients of 
neo-Darwinism, this too would be explicated. In any case, the result would wear the title of 
“synthesis” far more credibly than neo-Darwinism alone.
Two Theories of Biological Causality
In order to talk about origins and evolution, one must talk about causality, and because 
causality is a function of the system called “nature”, one must talk about nature. Theories of 
biological origins and evolution like Neo-Darwinism and ID theory are both theories of 
causality restricted to the context of biological origins and evolution, and because causality 
is a function of nature, each points toward an underlying theory of nature incorporating an 
appropriate treatment of causality. That is, biological origins and evolution, being for 
scientific purposes instances of causation or the outcomes of causal processes, require 
definitions, theories and models of nature and causality. But these definitions, theories and 
models involve deeper and more complex criteria than meet the casual eye, and even to 
experts in science and philosophy, it is not entirely obvious how to satisfy them. This is why 
causality remains a controversial subject.
A cause is something that brings about an effect or result, and causality is the quality or 
agency relating cause and effect. Because there are different requirements for bringing 
about an event or situation, there are different kinds of causation. In common usage, a 
“cause” may be an event which causes another event, the reason or rationale for an event, 
an agent or the motive thereof, the means by which an event transpires, supporting 
conditions for an event, or in fact anything satisfying any logical or physical requirement of a 
resultant effect. Because causal relationships would seem to exist in a causal medium 
providing some sort of basic connection between cause and effect, the study of causation 
has typically focused on the medium and its connectivity…i.e., on the “fabric of nature”.
The kinds of causation that are required in order to explain natural changes or events were 
enumerated by Aristotle in the 4th century BC. He posed four questions involving four types 
of causes: (1) What is changed to make the entity (Of what is it composed)? (2) What 
makes the entity change, and how? (3) What is the shape or pattern assumed by the entity 
as it changes?
(4) What is the goal toward which the change of the entity is directed? He respectively 
defined the answers to these questions as the material cause, the efficient cause, the 
formal cause, and the final cause. With its explicit allowance for formal and final causation, 
Aristotle's classification ultimately implies the existence of a purposive, pattern-generating 
Prime Mover, and thus laid the groundwork for a teleological explanation of nature that went 

all but unchallenged for well over a millennium.
But when the Age of Reason (circa 1650-1800) had finished taking its toll on traditional 
Scholastic doctrines largely based on Aristotelian insight, only material and efficient causes 
retained a place in scientific reasoning…and in the hands of philosophers like Hume and 
Kant, even these modes of causation were laid open to doubt. Hume claimed that causal 
relationships are nothing more than subjective expectations that certain sequences of 
events observed in the past will continue to be observed in the future9, while Kant went on 
to assert that causality is a category of cognition and perception according to which the 
mind organizes its experience of basically unknowable objects.10 Nevertheless, 
contemporary science retains its concern for material and efficient causes while letting 
formal and final causes languish in a state of near-total neglect.11
Distilled to a single sentence, the prevailing scientific view of nature and causality is roughly 
this: “Nature is associated with a space, generalizable to a spacetime manifold, permeated 
by fields under the causal influence of which objects move and interact in space and time 
according to logico-arithmetical laws of nature.” Despite its simplicity, this is a versatile 
causal framework with the power to express much of our scientific knowledge. But the 
questions to which it leads are as obvious as they are unanswered. For example, where do 
these laws reside? Of what are they composed? How and why did they originate? What are 
their properties? How do they function, and how are they sustained?12
In addition to generating questions about natural laws in general, the prevailing 
oversimplification of causality contains further gaps which have done as much to impede 
our understanding of nature as to further it.13 The associated problems are numerous, and 
they lead to yet another set of questions. For example, is causality formally and dynamically 
contained, uncontained or self-contained? What is its source, on what does it function, and 
what additional structure does it predicate of that on which it functions? What is its 
substance - is it mental, physical or both? How does it break down, and if it is stratified, then 
what are its levels? These questions lead in turn to further questions, and until all of these 
questions are answered at least in principle, no theory of biological causality stands on terra 
firma.
But before attempting to answer this question, let us have a look at the models of causality 
on which neo-Darwinism and ID theory are currently based.
Causality According to Intelligent Design Theory
Teleological causation is “top-down” causation in which the design and design imperative 
reside at the top, and the individual actualization events that realize the design reside at the 
bottom. The model universe required for teleological causality must therefore incorporate 
(1) a source and means of design, i.e. a designer or designing agency; (2) a design stage in 
which designs are generated and/or selected; (3) an actualization stage in which designs 
become physically real from the viewpoints of physical observers; and (4) a means or 
mechanism for passing from the design stage to the actualization stage. If such a model 
universe permits these observers to empirically detect interesting instantiations of teleology, 
so much the better.

Particular teleological model universes that have been proposed include any number of 
celestial hierarchies and heavenly bureaucracies with God at the top giving the orders, 
angels of various ranks serving on intermediate levels as messengers and functionaries, 
and humans at or near the bottom; the Aristotelian universe, incorporating formal and final 
causation and embodying the telos of a Prime Mover; teleologically “front-loaded” 
mechanistic universes in which causation resembles clockwork that has been set in 
autonomous motion by a purposive, mechanically talented designer; and the panentheistic 
universe explicated by (among others) Alfred North Whitehead, in which the teleological will 
of the designer is immanent in nature because in some sense, nature is properly contained 
within the designer.14 Although each has its strengths, these and other well-known 
teleological models are inadequate as formulated, failing to support various logical 
implications of requirements 1-4.
The model universe of ID theory, which can be regarded as a generalization of traditional 
teleological design theory with respect to causal agency, has essentially the same 
requirements. However, it also contains certain novel ingredients including a focus on 
intelligence, an emphasis on mathematical and information-theoretic concepts, and two 
novel ingredients called irreducible complexity and specified complexity.
Irreducible complexity, which is intended to describe biological systems and subsystems 
unlikely to have been produced by gradual (piece-by-piece) evolution, is by definition a 
property of any integrated functional system from which the removal of any one or more 
core components critically impairs its original function.15 Although proposed examples have 
drawn fire – such examples include the bacterial flagellum, the human eye, the blood 
clotting cascade, and even the conventional spring-loaded mousetrap - the concept has a 
valid basis with roots in logic, graph theory and other branches of mathematics and 
engineering.
Specified complexity , which is intended as a more general description of the products of 
intelligent causation, is by definition a property of anything that exhibits a recognizable 
pattern with a very low probability of occurring by chance. Whereas irreducible complexity is 
based on the sheer improbability of complex functionally-coherent systems, specified 
complexity adds an intelligence (rational pattern generation and recognition) criterion that 
lets functional complexity be generalized to a pattern-based form of complexity better suited 
to probabilistic and information-theoretic analysis.16
Specified complexity amounts to a relationship between three attributes: contingency, 
complexity and specification. Contingency corresponds to freedom and variety (as when 
there are many distinct possibilities that may be selectively actualized), complexity 
corresponds to improbability, and specification corresponds to the existence of a meaningful 
pattern which, in conjunction with the other two attributes in sufficient measure, indicates an 
application of intelligence. Wherever all three of these attributes are coinstantiated, 
specified complexity is present.
Contingency is associated with specificational and replicational probabilistic resources. 
Specificational resources consist of a set or class of distinct pre- specified target events, 
while replicational resources consist of chances for at least one of the specified target 
events to occur. The chance of occurrence of an instance of specified complexity is the 
chance that these two kinds of resource will intersect in light of total contingency.

For example, the total contingency of a 4-digit lottery consists of the set of all possible 
drawings over unlimited trials and is associated with the numbers from 0000 to 9999, the 
specificational resources consist of a subset of distinct pre-specified 4-digit winning 
numbers to be replicated (matched or predicted), and the replicational resources consist of 
the tickets purchased. The chance that the lottery will have at least one winner equals the 
probability of intersection of the set of winning numbers and the set of tickets, given that 
there are ten thousand distinctly-numbered tickets that might have been purchased.
More topically, the total contingency of a particular evolutionary context consists of all 
possible (productive or dead-end) lines of evolution that might occur therein, the 
specificational resources consist of instances of specified complexity or “intelligent design”, 
and the replicational resources consist of all possible lines of evolution which can occur 
within some set of practical constraints imposed on the context, for example time or space 
constraints tending to limit replication. The chance that an instance of specified complexity 
will evolve equals the probability of intersection of the set of instances and the set of 
constrained lines of evolution, given the multiplicity of all of the possible lines of evolution 
that could occur. Where this probability is extremely low, some form of intelligent design is 
indicated.
Specified complexity is a powerful idea that yields insight crucial to the meaning and 
satisfaction of requirements 1-4. First, probability estimates for instances of specified 
complexity are so low as to require that specificational and replicational resources be linked 
in such a way that such events can actually occur, in effect raising their probability. It must 
therefore be determined whether the satisfaction of this requirement is consistent with the 
premise that low probabilities can actually be calculated for instances of specified 
complexity, and if so, how and why this can be reliably accomplished. And next, it must be 
shown that the required relationship implies intelligence and design.
Up to its current level of detail and coherence, the model universe of ID theory does not 
necessarily conflict with that of neo-Darwinism with respect to causality, but rather contains 
it, requiring only that causality be interpreted in light of this containment.
Causality According to Neo-Darwinism
Neo-Darwinism is the application of Darwinian natural selection to modern (post-Mendelian) 
genetics, which indifferently assumes that genetic mutations occur due to “random” DNA 
copying errors. This short but revealing description contains a certain amount of useful 
information. First, it reveals that causality is being at least partially reduced to some (ontic or 
epistemic) form of randomness. Even more revealingly, the phrase natural selection 
explicitly implies that nature is selective. Indeed, the term natural alone is instructive, for it 
reflects a naturalistic viewpoint according to which existence is ascribed exclusively to the 
natural world, i.e. “nature”.
In practice, most scientists consider nature to consist of that which is physical, observable 
and amenable to empirical investigation as prescribed by the scientific method, in their 
adherence to which they see themselves as following a naturalistic agenda. This is in 
keeping with scientific naturalism, a worldview of which neo-Darwinism is considered 

representative. Scientific naturalism ascribes existence strictly to the physical or natural 
world consisting of space, time, matter and energy. Two strains of naturalism are 
sometimes distinguished, philosophical and methodological. While philosophical naturalism 
claims ontological force, methodological naturalism is epistemological in flavor, merely 
asserting that nature might as well equal the physical world for scientific purposes. But in 
either case, scientific naturalism effectively confines the scientific study of nature to the 
physical. So inasmuch as neo-Darwinism is exemplary of scientific naturalism, it is physical 
or materialistic in character.17
In the picture of causality embraced by scientific naturalism, processes are either random or 
deterministic. In deterministic processes, objects are affected by laws and forces external to 
them, while in random processes, determinacy is either absent or unknown. A process can 
be “random” due to ignorance, statistics or presumed acausality…that is, because 
epistemological or observational limitations prevent identification of its hidden causal 
factors, because its causal outcomes are symmetrically or “randomly” distributed in the 
large, or because it is presumed to be nondeterministic. The first two of these possibilities 
are basically deterministic, while the last is (unverifiably) nondeterministic. So a neo-
Darwinist either takes a deterministic view of causality or sees it in terms of the dichotomy 
between determinism and nondeterminism, in either case relying heavily on the theory of 
probability.
In fact, given that natural selection is based on the essentially trivial observation that nature 
imposes constraints on survival and reproduction18, neo-Darwinism boils down to little more 
than probability theory, genetics and a very simple abstract but nominally physical model of 
biological causality based on “survival and reproduction of the fittest” or some minor variant 
thereof. Thus, when its practitioners claim to have generated a prediction, it is generally not 
a deep secret of nature unearthed by means of advanced theoretical manipulation, but 
merely the result of applying what amounts to a principle of indifference19 to some question 
about mutation, adaptation, selection or reproduction, running the numbers, and tracking 
the implications through its simplistic model universe. If there were no such “theory” as neo-
Darwinism, the same conclusion might have been reached with a straightforward 
combination of biology, genetics, chemistry, physics, a statistics calculator and a bit of 
common sense. This is why neo-Darwinism is so astonishingly able to absorb new effects 
and mechanisms the minute they come out of the core sciences.
Something else that neo-Darwinism seems to do with astonishing ease is absorb what 
appear on their faces to be contradictions. For example, many people, some might say a 
large majority, find it to some degree incredible that what amounts to a principle of 
indifference can be seriously offered as a causal explanation for the amazing complexity of 
the biological world, or for that matter any other part of the world. The fact that a principle of 
indifference is essentially devoid of information implies that neo-Darwinism yields not a 
causal explanation of biological complexity, but merely an open-ended simulation in which 
every bit of complexity delivered as output must have been present as input, any 
appearances to the contrary notwithstanding. This implies that neo-Darwinism per se, as 
distinguished from the core sciences from which it routinely borrows, adds precisely nothing 
to our knowledge of biological complexity or its source.
In order to deal with this seemingly inescapable problem, the proponents of neo-Darwinism 

have eagerly adopted the two hottest slogans in the theory of complex systems, self-
organization and emergence. Self-organization is a spontaneous, extrinsically unguided 
process by which a system develops an organized structure, while emergence refers to 
those global properties (functions, processes) of composite hierarchical systems that cannot 
be reduced to the properties of their component subsystems…the properties in which they 
are more than the sums of their parts. But the fact that these terms have been superficially 
defined does not imply that they have been adequately explained. Actually, they remain as 
much of a mystery in complexity theory as they are in biology, and can do nothing for neo-
Darwinism but spin the pointer toward another hapless and equally helpless field of inquiry.
Because scientific naturalism denies that existence of any kind is possessed by anything of 
a supernatural or metaphysical character, including a designing intelligence, the definitions, 
theories and models of nature and causality on which it implicitly relies must be “physical”, 
at least in name. However, as we have already noted and will shortly explain in detail, what 
currently passes for an understanding of causality in the physical sciences leaves much to 
be desired. In particular, since the kind of causality treated in the physical sciences is 
ontologically and functionally dependent on the origin and evolution of the cosmos, scientific 
naturalists trying to answer questions about causality are obliged to consider all stages of 
causation and generation all the way back to the cosmic origin, constantly testing their 
answers to see if they continue to make sense when reformulated in more fundamental 
terms.
Unfortunately, this obligation is not being met. One reason is the reluctance of those who 
most need an understanding of causality to admit the extent of their ignorance. Another is 
the seeming intractability of certain problems associated with the causality concept itself.
A Deeper Look at Causality: The Connectivity Problem
Because causal relationships would seem to exist in a causal medium providing some sort 
of basic connection between cause and effect, the study of causation has typically focused 
on the medium and its connectivity…i.e., on the “fabric of nature”. How does this fabric 
permit different objects to interact, given that to interact is to intersect in the same events 
governed by the same laws and thus to possess a degree of sameness? How can multiple 
objects each simultaneously exhibit two opposite properties, sameness and difference, with 
respect to each other?
Equivalently, on what underlying form of connectivity is causality defined? When one 
asserts that one event “causes” another, what more general connection does this imply 
between the events? If there is no more general connection than the causal connection 
itself, then causality is underivable from any logically prior condition; it is something that 
happens ex nihilo, the sudden synthesis of a connection out of nothing. As Hume 
maintained, causal relationships are mere accidental correlations of subjectively-associated 
events.
But it can’t be quite that simple. In fact, Hume’s characterization of causality as mere 
random correlation presupposes the existence of a correlating agent who recognizes and 
unifies causal correlations through experience, and the abstractive, experiential coherence 
or consciousness of

this correlation-inducing agent constitutes a prior connective medium. So in this case, 
explaining causality requires that the subjective medium of experience, complete with its 
correlative “laws of causality”, be related to the objective world of real events.
Unfortunately, Hume’s thesis includes a denial that any such objective world exists. In 
Hume’s view, experience is all there is. And although Kant subsequently registered his 
qualified disagreement, asserting that there is indeed an objective outside world, he 
pronounced it unknowable, relegating causality to the status of a category of perception.20 
This, of course, perpetuated the idea of causal subjectivity by continuing to presuppose the 
existence of an a priori subjective medium.
How can the nature of subjective causality be understood? As Kant observed, perception 
and cognition are mutually necessary; concepts without percepts are empty, and percepts 
without concepts are blind.21 It must therefore be asked to what extent perceptual reality 
might be an outward projection of cognitive processes, and natural processes the mirror 
images of mental processes.
This leads to another problem, that of mind-matter dualism.
The Dualism Problem
The Kantian distinction between phenomenal and noumenal reality, respectively defined as 
those parts of reality22 which are dependent on and independent of perception, mirrors a 
prior philosophical viewpoint known as Cartesian (mind-matter) dualism. Associated with 
René Descartes, the polymath mercenary who laid the groundwork for analytic geometry by 
helping to develop the concept of coordinate spaces, this is a form of substance dualism 
which asserts that reality consists of two immiscible “substances”, mind and matter. 
Cartesian dualism characterizes a certain influential approach to the problem of mental 
causation: how does the mind influence the physical body?
Cartesian dualism leads to a problem associated with the connectivity problem we have just 
discussed: if reality consists of two different “substances”, then what connects these 
substances in one unified “reality”? What is the medium which sustains their respective 
existences and the putative difference relationship between them? One possible (wrong) 
answer is that their relationship is merely abstract, and therefore irrelevant to material reality 
and devoid of material influence; another is that like the physical epiphenomenon of mind 
itself, it is essentially physical. But these positions, which are seen in association with a 
slew of related philosophical doctrines including physicalism, materialism, naturalism, 
objectivism, epiphenomenalism and eliminativism, merely beg the question that Cartesian 
dualism was intended to answer, namely the problem of mental causation.
Conveniently, modern logic affords a new level of analytical precision with respect to the 
Cartesian and Kantian dichotomies. Specifically, the branch of logic called model theory 
distinguishes theories from their universes, and considers the intervening semantic and 
interpretative mappings. Calling a theory an object language and its universe of discourse 
an object universe, it combines them in a metaobject domain consisting of the 
correspondences among their respective components and systems of components, and 
calls the theory or language in which this metaobject domain is analyzed a metalanguage. 

In like manner, the relationship between the metalanguage and the metaobject domain can 
be analyzed in a higher-level metalanguage, and so on. Because this situation can be 
recursively extended, level by level and metalanguage by metalanguage, in such a way that 
languages and their universes are conflated to an arbitrary degree, reality can with unlimited 
precision be characterized as a “metalinguistic metaobject”.
In this setting, the philosophical dichotomies in question take on a distinctly mathematical 
hue. Because theories are abstract, subjectively-formed mental constructs23, the mental, 
subjective side of reality can now be associated with the object language and 
metalanguage(s), while the physical, objective side of reality can be associated with the 
object universe and metauniverse(s), i.e. the metaobject domain(s). It takes very little effort 
to see that the mental/subjective and physical/objective sides of reality are now combined in 
the metaobjects, and that Cartesian and Kantian “substance dualism” have now been 
transformed to “property dualism”24 or dual- aspect monism. That is, we are now talking, in 
mathematically precise terms, about a “universal substance” of which mind and matter, the 
abstract and the concrete, the cognitive-perceptual and the physical, are mere properties or 
aspects.
Translating this into the scientific status quo is not difficult. Science regards causality as 
“objective”, taking its cues from observation while ignoring certain philosophical problems 
involving the nature of objectivity. But science also depends on theoretical reasoning, and 
this involves abstract analogues of causality to which science is equally indebted. To the 
extent that scientific theories accurately describe the universe, they are isomorphic to the 
universe; in order that nature be amenable to meaningful theorization, science must 
therefore assume that the basic cognitive ingredients of theories, and for that matter the 
perceptual ingredients of observations, mirror the corresponding ingredients of nature up to 
some minimal but assured level of isomorphism. Consistent theories of science thus require 
that physical and abstract causation be brought into basic correspondence as mandated by 
this necessity.
Abstract analogues of physical causation are already well-understood. Logically, causality is 
analogous to implication, an active or passive relationship between antecedents and 
consequents; theoretically, it is analogous to the application of rules of inference to 
expressions formulated within a theory; linguistically, it amounts to substitution or production 
according to the rules of a generative grammar; and mathematically, it amounts to the 
application of a rule, mapping, function, operation or transformation. In every case, the 
analogue is some form of recursive25 or iterative morphism to which a nonlogical 
interpretation may be attached.26 The object is therefore to understand physical reality in 
terms of such operations defined on an appropriate form of dual-aspect monism.
This leads directly to the structure problem.
The Structure Problem
A description or explanation of causality can only be formulated with respect to a particular 
“model universe” in which space, time and matter are defined and related to each other in 
such a way as to support the description. This relationship must account for the laws of 
nature and their role in natural processes. A little reflection should reveal that both neo-

Darwinism and ID theory, as well as all other scientific theories, are currently deficient in this 
regard. At best, scientists have a very limited idea where the laws of nature reside, how 
they came to be, and how they work, and due to the limitations of their empirical 
methodology27, they have no means of clarification.
We have already encountered Aristotle’s four modes of causation: material, efficient, formal 
and final. These follow no special prescription, but are merely generic answers to questions 
about certain features of Aristotle’s mental representation of nature…his model universe. 
There are as many additional modes of causation as there are meaningful questions 
regarding the structure and dynamics of a given model universe. For example, in addition to 
Aristotle’s questions of what, who and how, what and why, we could also ask where 
(positional causation), when (order or timing of causation), by virtue of what (facilitative 
causation), and so forth. Thus, we could say that something happened because it was 
positioned in a medium containing its material cause and supporting its efficient cause, 
because the time was right or certain prerequisites were in place, because certain 
conditions were present or certain tools were available, et cetera.
On what kinds of model universe can a causality function be defined? Among the 
mathematical structures which science has long favored are coordinate spaces and 
differentiable manifolds. In differentiable coordinate spaces, laws of physics formulated as 
algebraic or differential equations may conveniently define smooth geometric curves which 
faithfully represent (e.g.) the trajectories of physical objects in motion. A model universe 
based on these constructs supports certain causal relationships to an impressive level of 
accuracy. However, it fails with respect to others, particularly those involving discrete or 
nonlocal28 changes or requiring high levels of coherence. In particular, it is incapable of 
modeling certain generative processes, including any generative process that might have 
led to its own existence, and beyond a certain point, its “continuity” attribute has eluded a 
completely satisfactory explanation.29
These and other difficulties have prompted some theorists to suggest model universes 
based on other kinds of mathematical structure. These include a new class of models to 
which the concepts of information and computation are essential. Called “discrete models”, 
they depict reality in terms of bits, quanta, quantum events, computational operations and 
other discrete, recursively-related units. Whereas continuum models are based on the 
notion of a continuum, a unified extensible whole that can be subdivided in such a way that 
any two distinct points are separated by an infinite number of intermediate points, discrete 
models reflect the fact that it is impossible to describe or define a change or separation in 
any way that does not involve a sudden finite jump in some parameter. Discrete models 
reflect the rising investment of the physical sciences in a quantum-theoretic view of reality, 
and the increasing dependence of science on computer simulation as an experimental 
tool.30
Discrete models have the advantage that they can more easily incorporate modern 
cybernetic concepts, including information, computation and feedback, which conduce to an 
understanding of reality as a control and communication system. In the context of such 
models, informational and computational reductionism is now pursued with a degree of 
enthusiasm formerly reserved for attempts to reduce the universe to matter and energy. 
However, certain difficulties persist. Discrete models remain dualistic, and they still cannot 

explain their own origins and existences. Nonlocality is still a problem for them, as are the 
general-relativistic spacetime deformations so easily formulated in continuum models. 
Because they allow the existence of discrete gaps between events, they tend to lack 
adequate connectivity. And in the shadow of these deficiencies, they can illuminate the 
interrelationship of space, time and object no more successfully than their continuum 
counterparts.
The unregenerate dualism of most discrete models demands particular attention. As we 
reasoned above, solving the problem of dualism requires that the mental and physical 
aspects of reality be brought into coincidence. Insofar as information and computation are 
essentially formal and abstract, reducing the material aspects of nature to information and 
computation should bring the concrete and the abstract, the material and the mental, into 
perfect coincidence. But because most discrete models treat information and computation 
as objective entities, tacitly incorporating the assumption that bits and computations are on 
the same ontological footing as particles and collisions, their mental dimension is 
overlooked. Because making no explicit provision for mind amounts to leaving it out of the 
mix, mind and matter remain separate, and dualism persists.
Is there another alternative? The model-theoretic perspective, which simultaneously 
juxtaposes and conflates subjective languages and their objective universes, suggests that 
reality embodies an ontic-nomothetic medium with abstract and physical aspects that are 
respectively related as syntax is related to language. For example, because scientific 
observation and theorization must be consistent, and logic is the backbone of consistency, 
the syntax of every scientific theory must incorporate logic. In the case of a geometric 
theory of physical reality like classical mechanics or relativity theory, this amounts (by 
model-theoretic implication) to the requirement that logic and geometry literally coincide. But 
where geometry is a property of “physical” spacetime, so then is logic, and if logic resides in 
spacetime, then so must logical grammar. This leads to the
requirement that physical dynamics be objectively reconciled with the formal grammar of 
logic and logic-based theories, ultimately including abstract causality in its entirety.
Obviously, conventional continuum and discrete models of reality fail to meet this 
requirement. As far as they and those who embrace them are concerned, the physical world 
is simply not answerable to any theory whatsoever, even logic. According to the standard 
empirical doctrine of science, we may observe reality but never impose our conceptions 
upon it, and this means that theory – even a theory as necessary to cognition and 
perception as logic - is always the beggar and never the master at the scientific table. The 
reason for this situation is clear; scientists need a means of guarding against the human 
tendency to confuse their inner subjective worlds, replete with fantasy and prejudice, with 
the factual external world conventionally studied by science.
But there is a very clear difference between logic on one hand, and fantasy and prejudice 
on the other. While science never needs the latter, it always needs the former. By excluding 
logic from nature, mainstream science has nothing to gain and everything to lose; in not 
attributing its own most basic requirements to its subject matter, it is cheating itself in a 
crucial way. Whether or not a theory which fails to predicate on its universe the wherewithal 
of its own validity turns out to be valid, it can be neither more nor less so for its false and 
subtly pretentious humility. On the other hand, failing to attribute these requirements to its 

universe when its universe in fact exhibits them, and when its universe would in fact be 
unintelligible without them, can ultimately cost it every bit of truth that it might otherwise 
have had…particularly if its methodology is inadequate to identify the problem and mandate 
a remedy.
Because they fail to provide definitive answers for questions about causality, conventional 
continuum and discrete models of reality devolve to acausality or infinite causal regression. 
No matter what causal explanations they seem to offer, one of two things is implied: (1) a 
cause prior to that which is cited in the explanation, or (2) random, spontaneous, acausal 
emergence from the void, no explanation supposedly required. Given the seeming absence 
of alternatives to determinism or randomness, or extrinsic31 and null causation, how are 
meaningful causal explanations to be completed?
The Containment Problem
A certain philosophically controversial hypothesis about causality presently rules the 
scientific world by fiat. It asserts that physical reality is closed under causal regression: “no 
physical event has a cause outside the physical domain."32 That is, if a physical event has a 
cause, then it has a physical cause. Obviously, the meaning of this principle is strongly 
dependent on the definition of physical, which is not as cut and dried as one might 
suppose.33 It also contradicts the obvious fact that causality is an abstraction, at best 
indirectly observable through its effects on matter, which functions independently of any 
specific item of material content. How, then, does this principle manage to maintain its hold 
on science? The answer: false parsimony and explanatory debt. Concisely, false parsimony 
is when a theory achieves deceptive simplicity in its native context by sweeping its unpaid 
explanatory debts (explanatory deficiencies) into unkempt piles located in or between other 
areas of science.
It is an ill-kept secret that the scientific community, far from being one big happy family of 
smoothly-connected neighborhoods, consists of isolated, highly-specialized enclaves that 
often tend toward mutual ignorance and xenophobia. Under these circumstances, it is only 
natural to expect that when caught between an observational rock and a theoretical hard 
place, some of these enclaves will take advantage of the situation and “pass the 
explanatory buck”, neither knowing nor caring when or where it comes to rest as long as the 
maneuver takes some of the heat off them and frees them to conduct business as usual. 
While the explanatory buck-passing is almost never productive, this can be conveniently 
hidden in the deep, dark cracks and crevices between disciplines. As a result, many 
pressing explanatory obligations have been successfully
exiled to interdisciplinary limbo, an intellectual dead zone from which they cannot threaten 
the dominance of the physical causal closure thesis.
However, this ploy does not always work. Due to the longstanding scientific trend toward 
physical reductionism, the buck often gets passed to physics, and because physics is 
widely considered more fundamental than any other scientific discipline, it has a hard time 
deferring explanatory debts mailed directly to its address. Some of the explanatory debts for 
which physics is holding the bag are labeled “causality”, and some of these bags were sent 
to the physics department from the evolutionary biology department. These debt-filled bags 
were sent because the evolutionary biology department lacked the explanatory resources to 

pay them for itself. Unfortunately, physics can’t pay them either.
The reason that physics cannot pay explanatory debts generated by various causal 
hypotheses is that it does not itself possess an adequate understanding of causality. This is 
evident from the fact that in physics, events are assumed to be either deterministic or 
nondeterministic in origin. Given an object, event, set or process, it is usually assumed to 
have come about in one of just two possible ways: either it was brought about by something 
prior and external to it, or it sprang forth spontaneously as if by magic. The prevalence of 
this dichotomy, determinacy versus randomness, amounts to an unspoken scientific axiom 
asserting that everything in the universe is ultimately either a function of causes external to 
the determined entity (up to and including the universe itself), or no function of anything 
whatsoever. In the former case there is a known or unknown explanation, albeit external; in 
the latter case, there is no explanation at all. In neither case can the universe be regarded 
as causally self-contained.
To a person unused to questioning this dichotomy, there may seem to be no middle ground. 
It may indeed seem that where events are not actively and connectively produced according 
to laws of nature, there is nothing to connect them, and thus that their distribution can only 
be random, patternless and meaningless. But there is another possibility after all: self-
determinacy. Self-determinacy involves a higher-order generative process that yields not 
only the physical states of entities, but the entities themselves, the abstract laws that govern 
them, and the entire system which contains and coherently relates them. Self-determinism 
is the causal dynamic of any system that generates its own components and properties 
independently of prior laws or external structures. Because self-determinacy involves 
nothing of a preexisting or external nature, it is the only type of causal relationship suitable 
for a causally self-contained system.
In a self- deterministic system, causal regression leads to a completely intrinsic self-
generative process. In any system that is not ultimately self-deterministic, including any 
system that is either random or deterministic in the standard extrinsic sense, causal 
regression terminates at null causality or does not terminate. In either of the latter two 
cases, science can fully explain nothing; in the absence of a final cause, even material and 
efficient causes are subject to causal regression toward ever more basic (prior and 
embedding) substances and processes, or if random in origin, toward primitive acausality. 
So given that explanation is largely what science is all about, science would seem to have 
no choice but to treat the universe as a self-deterministic, causally self-contained system.34
And thus do questions about evolution become questions about the self-generation of 
causally self-contained, self-emergent systems. In particular, how and why does such a 
system self-generate?
The Utility (Selection) Problem
As we have just noted, deterministic causality transforms the states of preexisting objects 
according to preexisting laws associated with an external medium. Where this involves or 
produces feedback, the feedback is of the conventional cybernetic variety; it transports 
information through the medium from one location to another and then back again, with 
transformations at each end of the loop. But where objects, laws and media do not yet exist, 

this kind of feedback is not yet possible. Accordingly, causality must be reformulated so that 
it can not only transform the states of natural systems, but account for self-deterministic 
relationships between states and laws of nature. In short, causality must become 
metacausality.35
Self-determination involves a generalized atemporal36 kind of feedback between physical 
states and the abstract laws that govern them. Whereas ordinary cybernetic feedback 
consists of information passed back and forth among controllers and regulated entities 
through a preexisting conductive or transmissive medium according to ambient sensory and 
actuative protocols – one may think of the Internet, with its closed informational loops and 
preexisting material processing nodes and communication channels, as a ready example - 
self-generative feedback must be ontological and telic rather than strictly physical in 
character.37 That is, it must be defined in such a way as to “metatemporally” bring the formal 
structure of cybernetics and its physical content into joint existence from a primitive, 
undifferentiated ontological groundstate. To pursue our example, the Internet, beginning as 
a timeless self-potential, would have to self-actualize, in the process generating time and 
causality.
But what is this ontological groundstate, and what is a “self-potential”? For that matter, what 
are the means and goal of cosmic self-actualization? The ontological groundstate may be 
somewhat simplistically characterized as a complete abeyance of binding ontological 
constraint, a sea of pure telic potential or “unbound telesis”. Self- potential can then be seen 
as a telic relationship of two lower kinds of potential: potential states, the possible sets of 
definitive properties possessed by an entity along with their possible values, and potential 
laws (nomological syntax) according to which states are defined, recognized and 
transformed.38 Thus, the ontological groundstate can for most purposes be equated with all 
possible state-syntax relationships or “self-potentials”, and the means of self-actualization is 
simply a telic, metacausal mode of recursion through which telic potentials are refined into 
specific state-syntax configurations. The particulars of this process depend on the specific 
model universe – and in light of dual-aspect monism, the real self-modeling universe - in 
which the telic potential is actualized.
And now we come to what might be seen as the pivotal question: what is the goal of self-
actualization? Conveniently enough, this question contains its own answer: self-
actualization, a generic analogue of Aristotelian final causation and thus of teleology, is its 
own inevitable outcome and thus its own goal.39 Whatever its specific details may be, they 
are actualized by the universe alone, and this means that they are mere special instances 
of cosmic self-actualization. Although the word “goal” has subjective connotations – for 
example, some definitions stipulate that a goal must be the object of an instinctual drive or 
other subjective impulse – we could easily adopt a reductive or functionalist approach to 
such terms, taking them to reduce or refer to objective features of reality. Similarly, if the 
term “goal” implies some measure of design or pre-formulation, then we could easily 
observe that natural selection does so as well, for nature has already largely determined 
what “designs” it will accept for survival and thereby render fit.
Given that the self-containment of nature implies causal closure implies self-determinism 
implies self-actualization, how is self-actualization to be achieved? Obviously, nature must 
select some possible form in which to self-actualize. Since a self-contained, causally closed 

universe does not have the luxury of external guidance, it needs to generate an intrinsic 
self-selection criterion in order to do this. Since utility is the name already given to the 
attribute which is maximized by any rational choice function, and since a totally self-
actualizing system has the privilege of defining its own standard of rationality40, we may as 
well speak of this self-selection criterion in terms of
global or generic self-utility. That is, the self-actualizing universe must generate and retrieve 
information on the intrinsic utility content of various possible forms that it might take.
The utility concept bears more inspection than it ordinarily gets. Utility often entails a 
subject-object distinction; for example, the utility of an apple in a pantry is biologically and 
psychologically generated by a more or less conscious subject of whom its existence is 
ostensibly independent, and it thus makes little sense to speak of its “intrinsic utility”. While 
it might be asserted that an apple or some other relatively non-conscious material object is 
“good for its own sake” and thus in possession of intrinsic utility, attributing self-interest to 
something implies that it is a subject as well as an object, and thus that it is capable of 
subjective self-recognition.41 To the extent that the universe is at once an object of selection 
and a self-selective subject capable of some degree of self-recognition, it supports intrinsic 
utility (as does any coherent state-syntax relationship). An apple, on the other hand, does 
not seem at first glance to meet this criterion.
But a closer look again turns out to be warranted. Since an apple is a part of the universe 
and therefore embodies its intrinsic self-utility, and since the various causes of the apple 
(material, efficient and so on) can be traced back along their causal chains to the intrinsic 
causation and utility of the universe, the apple has a certain amount of intrinsic utility after 
all. This is confirmed when we consider that its taste and nutritional value, wherein reside its 
utility for the person who eats it, further its genetic utility by encouraging its widespread 
cultivation and dissemination. In fact, this line of reasoning can be extended beyond the 
biological realm to the world of inert objects, for in a sense, they too are naturally selected 
for existence. Potentials that obey the laws of nature are permitted to exist in nature and are 
thereby rendered “fit”, while potentials that do not are excluded.42 So it seems that in 
principle, natural selection determines the survival of not just actualities but potentials, and 
in either case it does so according to an intrinsic utility criterion ultimately based on global 
self-utility.
It is important to be clear on the relationship between utility and causality. Utility is simply a 
generic selection criterion essential to the only cosmologically acceptable form of causality, 
namely self-determinism. The subjective gratification associated with positive utility in the 
biological and psychological realms is ultimately beside the point. No longer need natural 
processes be explained under suspicion of anthropomorphism; causal explanations need 
no longer implicitly refer to instinctive drives and subjective motivations. Instead, they can 
refer directly to a generic objective “drive”, namely intrinsic causality…the “drive” of the 
universe to maximize an intrinsic self-selection criterion over various relational strata within 
the bounds of its internal constraints.43 Teleology and scientific naturalism are equally 
satisfied; the global self-selection imperative to which causality necessarily devolves is a 
generic property of nature to which subjective drives and motivations necessarily “reduce”, 
for it distributes by embedment over the intrinsic utility of every natural system.
Intrinsic utility and natural selection relate to each other as both reason and outcome. When 

an evolutionary biologist extols the elegance or effectiveness of a given biological “design” 
with respect to a given function, as in “the wings of a bird are beautifully designed for flight”, 
he is really talking about intrinsic utility, with which biological fitness is thus entirely 
synonymous. Survival and its requisites have intrinsic utility for that which survives, be it an 
organism or a species; that which survives derives utility from its environment in order to 
survive and as a result of its survival. It follows that neo-Darwinism, a theory of biological 
causation whose proponents have tried to restrict it to determinism and randomness, is 
properly a theory of intrinsic utility and thus of self-determinism. Athough neo-Darwinists 
claim that the kind of utility driving natural selection is non-teleological and unique to the 
particular independent systems being naturally selected, this claim is logically 
insupportable. Causality ultimately boils down to the tautological fact that on all possible 
scales, nature is both that which selects and that which is selected, and this means that 
natural selection is ultimately based on the intrinsic utility of nature at large.
But in light of causal self-containment, so is teleology. Why, then, do so many supporters of 
teleology and neo-Darwinism seem to think them mutually exclusive?
The Stratification Problem
It is frequently taken for granted that neo-Darwinism and ID theory are mutually 
incompatible, and that if one is true, then the other must be false. But while this assessment 
may be accurate with regard to certain inessential propositions attached to the core theories 
like pork-barrel riders on congressional bills44, it is not so obvious with regard to the core 
theories themselves. In fact, these theories are dealing with different levels of causality.
The scientific method says that experiments must be replicable, and this means that the 
same laws must govern the same kinds of events under the same conditions throughout 
nature. So where possible, the laws of nature are scientifically formulated in such a way that 
they distribute over space and time, the same laws applying under similar conditions at all 
times and places. Science also requires that the laws of nature be formulated in such a way 
that the next state of an object depends only on its present state, including all of the forces 
impinging on it at the present moment, with no memory of prior states required. Little 
wonder that science enforces these two conditions with extreme prejudice wherever 
possible, for in principle, they guarantee its ability to predict the future of any physical 
system from a mere knowledge of its current state and the distributed laws of nature.45
Science imposes yet further constraints on causality. One, the empirical discernability 
criterion of the scientific method46, guarantees the recognizability of physical states by 
insisting that they be formulated in terms of first-order properties47 called observables that 
can be unambiguously measured in conjunction with physical objects. Another, which we 
have already encountered, is the locality principle, which says that there can be no 
“nonlocal” jumps from one point in a physical manifold to another non-adjacent point.48 This 
adds an adjacency or continuity constraint to the Laplacian ideal; the laws of nature must 
not only be formulated in such a way that the next state of an object depends only on its 
present state, but in such a way that successive states are “near” each other, i.e. so that 
smaller amounts of time and energy correspond to smaller distances. This proportionality of 
distance and effect permits the laws of causality to be consistently applied on the 
macroscopic and microscopic scales.

Of all the preconceived restrictions and unnecessary demands imposed on causality by 
science, the least questioned is the requirement that the relationship between physical 
states and laws of nature be one-way, with states depending on laws but not vice versa. 
Science regards the laws of nature as immutable, states as existing and transforming at 
their beck and call, and the directional dependency relationship between laws and states as 
something that has existed for all time. When the laws dictate that an event should happen, 
it happens; on the other hand, any event chancing to occur without their guidance is 
uncaused and totally “random”. This leads to the determinacy-versus-randomness 
dichotomy already discussed in connection with the containment and utility problems.
Due to these criteria, what science calls a “law of nature” is typically an autonomous 
relationship of first-order properties of physical objects, and so for the laws of state 
transformation that govern causation. There can be little doubt that science has succeeded 
in identifying a useful set of such laws. Whether or not they suffice for a full description of 
nature and causality (and they do not), they are an important part of the total picture, and 
wherever possible, they should indeed be tracked down and exploited to their full 
descriptive and prescriptive potential. But at least one caveat is in order: they should be 
regarded as explaining only that which they can be empirically and/or rationally shown to 
explain. As with any other scientific assertion, they must be kept pure of any metaphysical 
prejudice tending to artificially inflate their scope or explanatory weight.
It is thus a matter of no small concern that in pursuing its policy of causal simplification, the 
scientific mainstream seems to have smuggled into its baggage compartment a certain 
piece of contraband which appears, despite its extreme resistance to rational or empirical 
justification, to be masquerading as a tacit “meta-law” of nature. It states that every higher-
order relationship of objects and events in nature, regardless of complexity or level of 
dynamic integration, must be strictly determined by distributed laws of nature acting 
independently on each of its individual components. Along with the other items on the neo-
Laplacian wish-list of causal conveniences to which the scientific mainstream insists that 
nature be held, this criterion betrays a marked preference for a “bottom-up” approach to 
causation, suggesting that it be called the bottom-up thesis.49
The bottom-up thesis merely underscores something that we already know about the 
scientific mainstream: it wants with all of its might to believe that in principle, the whole 
destiny of the natural world and everything in it can be exhaustively predicted and explained 
on the basis of (1) a Laplacian snapshot of its current details, and (2) a few distributed laws 
of nature from which to exhaustively develop the implications. So irresistable is this desire 
that some of those caught in its grip are willing to make a pair of extraordinary claims. The 
first is that science has completely explained some of nature’s most complex systems in 
terms of microscopic random events simply by generically classifying the microscopic 
events that might possibly have been involved in their realization. The second is that 
observed distributions of such events, which they again call “random”, prove that no system 
in nature, regardless of its complexity, has ever come into being from the top down.
The genotype-to-phenotype mapping is a case in point. Many neo-Darwinists seem to have 
inferred that what happens near the endpoints of this mapping - the seemingly random 
mutation of genotypes and the brutal, deterministic competition among phenotypes – offers 

more insight regarding nature and causality than does the delicate, exquisitely complex 
ontogenic symphony performed by the mapping itself. In response to the observation that 
the theoretical emphasis has been lopsided, one hears that of course neo-Darwinists 
acknowledge the involvement of intermediate processes in the emergence of biological 
complexity from strings of DNA. For are not genes converted to proteins, which fold into 
functional forms and interact with other molecules to alter the timing of gene expression, 
which can lead to cytodifferentiation, pattern formation, morphogenesis and so on, and is 
this whole self-organizational process not highly sensitive to developmental interactions with 
the environment?
Unfortunately, where the acknowledged processes and interactions are still assumed to be 
micro-causal and deterministic, the acknowledgement is meaningless. In fact, the higher-
order structure and processing of complex biological systems has only been shoveled into 
an unkempt pile sexily labeled “emergent phenomena” and bulldozed across the 
interdisciplinary divide into complex systems theory. And thus begins a ramose paper trail 
supposedly leading to the final owners of the explanatory debt, but instead looping, dead-
ending or petering out in interdisciplinary limbo. The explanatory buck is thereby passed 
into oblivion, and the bottom-up thesis rolls like righteous thunder over any voice daring to 
question it.
In fact, the top-down and bottom-up approaches to causality are not as antithetical as they 
might seem. In the bottom-up view of causality, states evolve according to laws of nature in 
a temporal direction preferred by the second law of thermodynamics, which holds under the 
assumption that physical states are governed by laws of nature independent of state. But 
this assumption can hold only up to a point, for while the prevailing model universe supports 
only bottom-up causation, the situation is dramatically reversed with respect to cosmology. 
Because cosmological causal regression terminates with an ancestral cosmic singularity 
representing the whole of nature while omitting all of its details, standard cosmology 
ultimately supports only a top-down approach. The natural affinity of the cosmos for top-
down causation – the fact that it is itself an instance of top-down causation - effectively 
relegates bottom-up causation to secondary status, ruling out the bottom-up thesis and thus 
making room for a new model universe supporting and reconciling both approaches.
It turns out that in a certain kind of model universe, the top-down and bottom-up approaches 
are to some degree mutually transparent50. Two necessary features of such a model 
universe are (1) sufficient causal freedom to yield probabilistic resources in useful amounts, 
and (2) structural support for metacausal access to those resources. As it happens, a well-
known ingredient of nature, quantum uncertainty, provides the required sort of causal 
freedom. But while nature exhibits quantum uncertainty in abundance and can thus 
generate probabilistic resources at a certain respectable rate, the prevailing model universe 
supports neither metacausal relationships nor sufficient access to these resources. In fact, it 
fails to adequately support even quantum mechanics itself.
The new model universe must remedy these shortcomings…but how?
Synthesis: Some Essential Features of a Unifying Model of Nature and Causality
Classical mechanics, inarguably one of the most successful theories in history, is often cited 

as a model of theoretical progress in the sciences. When certain problems arose that could 
not be solved within its conceptual framework, it was extended to create a metatheory in 
which it exists as a “limiting case”. In fact, this was done thrice in fairly rapid succession. 
The first extension created the Special Theory of Relativity, in which classical mechanics 
holds as a low-to-medium velocity limit. The second created the General Theory of 
Relativity, in the curved spacetime manifold of which the flat Minkowskian manifold of 
Special Relativity holds as a local limit. And the third created quantum mechanics, in which 
classical mechanics holds as a “decoherence limit”.51 Indeed, whenever a theory is 
extended by adjoining to it one or more new concepts, this creates a metatheory expressing 
the relationship between the adjoint concept(s) and the original theory.
The model universe of neo-Darwinism is just a special-purpose refinement of the 
continuous coordinate spaces of classical mechanics, and its causal limitations are shared 
by neo-Darwinism and most other scientific theories. This is because most sciences, not 
including certain branches of physics and engineering, have been unable to absorb and 
utilize the relativistic and quantum extensions of the classical model, each of which suffers 
in any event from many of the same difficulties with causality. It follows that another 
extension is required, and since neo-Darwinism holds true within a limited causal domain, it 
must hold in this extension as a limiting case (minus its inessential philosophical baggage). 
In other words, causality must become the objective, distributive limit of metacausality.
Such an extension has already been described52, and it embodies solutions for all of the 
problems discussed in this paper. Concisely, it embeds physical reality in an extended 
logico-algebraic structure, a Self-Configuring Self-Processing Language or SCSPL. SCSPL 
incorporates a pregeometric53 conspansive manifold in which the classical spacetime 
manifold is embedded as a limiting configuration. SCSPL brings formal and physical 
causality into seamless conjunction by generically equating the laws of nature with SCSPL 
syntax, and then contracting the semantic, model-theoretic correspondence between syntax 
and state (or laws and observables)54 so that they coincide in syntactic operators, physical 
quanta of self-transducing information. Through properties called hology (syntactic self-
similarity) and triality (space-time-object conflation), total systemic self-containment is 
achieved. In particular, the system is self-deterministically closed under causation.
SCSPL evolves by telic recursion, a higher-order process55 of which causality is the physical 
limit (as required). In standard causality, physical states evolve according to laws of nature; 
in telic recursion, syntax-state relationships evolve by maximization of intrinsic utility. The 
temporal phase of telic recursion is conspansion, a dual-aspect process coordinating 
formal/telic and physical modes of evolution. By virtue of conspansive duality, SCSPL 
simultaneously evolves like a (metacausal, telic-recursive) generative grammar and a 
physical dynamical system, at once implementing top-down and bottom-up causation. 
Conspansion involves an alternation between self-replication and self-selection, thus 
constituting a generalization of Darwinian evolution in which specificational and replicational 
probabilistic resources are rationally linked. In this way, neo-Darwinist and design-theoretic 
(bottom-up and top-down) modes of causality become recognizable as complementary 
aspects of a single comprehensive evolutionary process.
From a formal standpoint, SCSPL has several unique and interesting features. Being based 
on logic,56 it identifies itself with the logical syntax of its perceptual universe on grounds of 

logical-perceptual isomorphism. This eliminates the conventional model-theoretic distinction 
among theory, universe and theory-universe correspondence, contracting the problematic 
mapping between abstract and concrete reality on the syntactic (nomological) level. This 
brings the physical world into coincidence with its logical counterpart, effecting dual-aspect 
monism and putting logical attributes on the same explanatory footing as physical attributes. 
SCSPL thus adjoins logic to nature, injecting57 nature with the abstract logical infrastructure 
of perception and theorization and endowing physical reality with the remedial conceptual 
apparatus demanded by the problems, paradoxes and explanatory deficiencies straining its 
classical descriptions. At the same time, it adjoins nature to logic in the form of perceptual 
categories and necessary high-level properties including closure, comprehensiveness, 
consistency and teleo-nomological coherence, thus opening logical routes to physical 
insight.
SCSPL offers yet further advantages. In defining nature to include logic and cognition, it 
relates physics and mathematics on a basic level, thus merging the rational foundations of 
mathematics with the perceptual foundations of physics and letting each provide crucial 
support for the other. By affording an integrated conceptual framework for prior conflicting 
extensions of classical reality, it sets the stage for their ultimate reconciliation. And its cross-
interpretation of the cognitive and physical aspects of nature renders the universe self-
explaining and self-modeling, thus effecting self-containment on the theoretic and model-
theoretic levels. That is, SCSPL self-containment effects not just causal and generative 
closure, but closure under the inverse operations of explanation and interpretation, thus 
permitting nature to physically model and teleo-nomologically justify its own self-
configurative determinations. In SCSPL, natural laws and physical states are seen as 
expressions of the intrinsic utility of nature by and for nature.
The reflexive self-processing and (telic) self-configuration functions of SCSPL imply that 
nature possesses generalized functional analogues of human self-awareness and volition, 
and thus a generalized capacity for utilitarian self-design. The self-design and self- 
modeling capacity of nature suggests that the universe is a kind of stratified “self-simulation” 
in which the physical and logico-telic aspects of reality can be regarded as respectively 
“simulated” and “simulative” in a generalized quantum-computational sense. This makes 
SCSPL relevant to self-organization, emergence and other complexity-theoretic phenomena 
increasingly attractive to the proponents of neo-Darwinism and other causally-challenged 
theories. At the same time, the fact that SCSPL evolution is both nomologically coherent 
and subject to a rational intrinsic utility criterion implies that the universe possesses 
properties equivalent to generalized intelligence, suggesting the possibility of an integrated 
SCSPL approach to the problems of consciousness and evolution.
The overall theory which logically extends the concepts of nature and causality to SCSPL 
and telic recursion, thereby merging the perceptual manifold with its cognitive and telic 
infrastructure, is known as the Cognitive-Theoretic Model of the Universe or CTMU, and its 
approach to biological origins and evolution is called Teleologic Evolution.58 Based on the 
concept of telic-recursive metacausation, Teleologic Evolution is a dynamic interplay of 
replication and selection through which the universe creates itself and the life it contains. 
Teleologic Evolution is a stratified process which occurs on levels respectively associated 
with the evolution of the cosmos and the evolution of life, thus permitting organic evolution 
to mirror that of the universe in which it occurs. It improves on traditional approaches to 

teleology by extending the concept of nature in a way eliminating any need for 
"supernatural" intervention, and it improves on neo-Darwinism by addressing the full extent 
of nature and its causal dynamics.
Due to their implicit reliance on different notions of causality, teleology and evolution were 
once considered mutually exclusory. While teleology appears to require a looping kind of 
causality consistent with the idea that ends are immanent in nature (even in beginnings), 
evolution seems to require that mutation and natural selection exhibit some combination of 
nondeterminacy and linear determinacy. In contrast, the phrase Teleologic Evolution 
reflects their complementarity within a coherent self-configurative ensemble identifying 
nature with its own utilitarian self-actualization imperative. In the associated metacausal 
extension of physical reality, the two central processes of evolution, replication and 
selection, are seen to occur on at least two mutually-facilitative levels respectively 
associated with the evolution of the universe and that of organic life.59 Meanwhile, the 
intrinsic utility criterion of self-selection implies that nature, as rationally defined in the 
CTMU, possesses a generalized form of intelligence by which all levels of evolution are 
driven and directed, equating selection with specification and metacausally relating it to 
replication.60 Reality is united with its generative principle by the rational linkage between 
the domain and codomain of the teleological, meta-Darwinian level of natural selection.
Because nature consists of all that is logically relevant to perception, and logic consists of 
the rules of thought and therefore comprises an essential theory of cognition, the CTMU 
couples mind and nature in a way suggestive of Ouroboros divided and reunited…two 
intimately entwined constrictors, estranged centuries ago by mind-body dualism but now 
locked in a renewed embrace, each swallowing the other’s entailments. Perhaps this 
reunion will deter the militant torch-bearers of scientific naturalism from further reneging on 
their explanatory debts and fleecing mankind of its millennial hopes and dreams after all. 
And if so, then perhaps mankind can snuff the rapidly dwindling fuse of its insidious 
ontological identity crisis while these hopes and dreams still have a fighting chance of 
realization, and the intrinsic utility of mankind is still salvageable.
1 
”Meaning” entails recognition, referring specifically to a recognizable and therefore 
informational relationship among related entities. Since information is abstract, so is 
recognition, and so is meaning (whether or not the related entities are themselves physical 
and concrete). Naturalism, of which the theory of evolution is an example, is an essentially 
materialistic viewpoint which denies or disregards abstract modes of existence, thus limiting 
meaning to “material” drives and instincts. But where the abstract contains the physical, 
capturing its structure in the form of meaningful informational patterns called “laws of 
nature”, abstraction and meaning are plainly essential to both science and nature. 
2 
Darwin, C. [1859] 1999. The Origin of Species. New York: Bantam Classic. 
3 
Science is a two-step, two-level process concerned with (1) formulating hypotheses 
about nature, and (2) proving or disproving these hypotheses to some degree of 
confirmation. Relative to level 1, level 2 requires a higher level of discourse incorporating 
truth-functional criteria independent of any particular falsifiable hypothesis. Because 
maintaining this distinction helps to insure that false hypotheses do not figure in their own 

“validation”, purportedly falsifiable (level 1) theories like neo-Darwinism should not be 
confused with the confirmational level of science. 
4 
Properly speaking, science includes both the empirical and mathematical sciences. 
Most of those who call themselves “scientists”, as well as many proponents of ID theory, 
assume that scientific confirmation can only be achieved by strict application of the scientific 
method and must thus be empirical. However, this is an oversimplification. The empirical 
sciences are not only mathematical in structure, but too heavily indebted to mathematical 
reasoning to exclude mathematical methods as possible means of confirming facts about 
nature. So with regard to the scientific status of ID theory, both empirical and mathematical 
methods of confirmation must be duly considered. 
5 
It can also be cogently argued that the design inference requires the establishment 
of means, motive and opportunity for a designer to act, and that meeting these 
requirements amounts to explaining the designer. 
6 
Theistic evolution is a simple conjunction of theism and Darwinism which pays no 
real attention to their mutual consistency or the model-theoretic implications of combining 
them. 
7 
Robinson, B. 1995. Public Beliefs about Evolution and Creation. Ontario Consultants 
on Religious Tolerance, [Online]. Available: URL: 
  http://www.religioustolerance.org/ev_publi.htm
 
  
8 
Ho, M.W. and Saunders, P.T. 1979. Beyond neo-Darwinism - An Epigenetic 
Approach to Evolution. Journal of Theoretical Biology, Vol. 78, pp. 573-591. See p.574: “…
neo-Darwinism exhibits a great power of assimilation, incorporating any opposing viewpoint 
as yet another ‘mechanism’ in the grand ‘synthesis’. But a real synthesis should begin by 
identifying conflicting elements in the theory, rather than in accommodating contradictions 
as quickly as they arise.” 
9 
Hume, D. 1975 Enquiries Concerning Human Understanding and Concerning the 
Principles of Morals, ed. by L. A. Selby-Bigge, 3rd revised ed., ed. by P. H. Nidditch, Oxford: 
Oxford University Press. 
10 
Kant, I. 1929, 1965. The Critique of Pure Reason, trans. Norman Kemp Smith. New 
York: St. Martin's Press. Available: URL:   http://www.arts.cuhk.edu.hk/Philosophy/Kant/cpr
 
  
See 
11 
While material and efficient causation are superficially physical and can be described 
in more or less materialistic terms, formal and final causation are more abstract. Francis 
Bacon, who strongly influenced scientific methodology, classified these abstract modes of 
causation as metaphysics rather than physics. [Bacon, F. 1997. Thoughts on the Nature of 
Things. Kessinger Publishing] 
12 
These questions about laws of causality address the nature and origin of causality 
itself, and are thus metacausal analogues of Aristotle’s questions about causality. The 
answers presented in this paper - roughly, that laws are elements of syntax of the language 

of nature, that they are composed of telesis and self-transducing metainformation, that they 
reside in syntactic (space-time-object) operators whose states they govern, that they arose 
through the metacausal self-configuration of the language of nature, that their properties 
include closure, comprehensiveness, consistency and coherence, and that their 
functionality and maintenance rely on intrinsic features of the language of nature – are thus 
metacausally analogous to Aristotle’s modes of causation. 
13 
For example, there is the gap between mind and matter; the gap between abstract 
and concrete existence; the gap between causality and generative cosmogony; the gap 
between classical and quantum mechanics, and so on. Because these gaps are serious, 
there is no reason to think that causality can be adequately explained as long as they exist. 
14 
Whitehead, A.N. 1985. Process and Reality. New York: The Free Press. 
15 
Behe, M. 1996. Darwin's Black Box. New York: The Free Press. 
16 
Dembski, W.A. 1998. The Design Inference: Eliminating Chance Through Small 
Probabilities. Cambridge University Press. 
17 
Pigliucci, M. 2000. Methodological vs. philosophical naturalism, or why we should be 
skeptical of religion. 
Tales of the Rational : Skeptical Essays About Nature and Science, Atlanta 
Freethought Society. Available: URL: 
  http://fp.bio.utk.edu/skeptic/Essays/methodological_naturalism.htm
 
  
18 
At the time that Charles Darwin made this observation and formulated his natural 
selection thesis, it was still obscured by centuries of teleological dominance. 
19 
The canonical principle of indifference (or insufficient reason) states that where there 
is no positive reason for assigning different probabilities to competing statistical or predictive 
assertions, e.g. different possible mutations weighted by relative frequency, equal 
probabilities must be assigned to all. Since this is essentially how neo-Darwinism calculates 
its random distributions of mutations and other events, it is just a biological variant of the 
principle of indifference. 
20 
Kant, The Critique of Pure Reason, p. 83: “Things which we see are not by 
themselves what we see ... It remains completely unknown to us what the objects may be 
by themselves and apart from the receptivity of our senses. We know nothing but our 
manner of perceiving them.”; p. 147: “We ourselves introduce that order and regularity in 
the appearance which we entitle ‘nature’. We could never find them in appearances had we 
not ourselves, by the nature of our own mind, originally set them there.” 
21 
Ibid., p. 93: “Thoughts without content are empty, intuitions without concepts are 
blind.” 
22 
For cognitive (and thus for theoretical and scientific) purposes, reality consists of 
perception plus the abstract cognitive apparatus required to generate, support and sustain 
it. 

23 
It makes no difference that scientific theories are based on “objective” empirical 
observations; the key point is that scientific observation and theorization require subjectively 
conscious agents called “scientists”, and that there exists no possible means of ruling out 
subjectivity on the part of any other kind of observer-theorist. Whatever reality “might have 
been without us”, our presence immediately implies that it possesses a subjective 
dimension. 
24 
Property dualism asserts that the properties mental and physical, while essentially 
different, apply to the same objects. Dual aspect monism asserts that these two properties 
together characterize the fundamental “substance” of nature. 
25 
The Church-Turing Thesis asserts that the class of recursive functions and the class 
of effectively computable functions are the same. This is generally taken to imply an 
isomorphism between the formal, abstract realm of recursive functions and the physical, 
mechanical realm in which abstract Turing machines are instantiated. For theoretical 
purposes, this isomorphism must be taken for granted; without it, theoretical instances of 
recursion could not be model-theoretically interpreted in physical reality, and physical reality 
could not be scientifically explained. 
26 
… even if what gets iterated is a “continuous” function representing motion in a 
differentiable manifold. 
27 
Scientific methodology conforms to the scientific method, which prescribes that 
nature be treated as if it were everywhere both discernable and replicable, and the related 
doctrine of falsifiability, which asserts that science is concerned only with hypotheses that 
are conceivably false and susceptible to empirical disproof. However, nature cannot be 
meaningfully defined in such a way that these criteria always hold within it. For example, no 
full description of nature can exclude references to universal, unconditional and therefore 
unfalsifiable properties of nature, and such unfalsifiable properties need not be scientifically 
trivial. 
28 
In physics, spatial and spatiotemporal manifolds are usually constrained by the 
locality principle, according to which nothing travels faster than light. Locality can be more 
fundamentally defined as the condition that in relocating from one point to another in a 
metric space, an object must traverse the entire sequence of adjacent finite or infinitesimal 
intervals comprising some intervening path within the metric on which locality is being 
enforced. In other words, locality means “no sudden jumps from one point to another, 
through the space containing the points or any external space thereof.” The bearing on 
causality is obvious. 
29 
Continuity is understood in terms of infinitesimal displacements. Several approaches 
exist to the topic of infinitesimals, some more controversial than others. The most common 
is the Cauchy-Weierstrass epsilon-delta formalism; the most sophisticated is that of which 
A. Robinson’s nonstandard analysis is the earliest and most successful representative. 
30 
Perhaps the most fashionable discrete model universe is explicitly based on a 
computational paradigm, the cellular automaton. An encyclopedic account of this paradigm 

can be found in Wolfram, S. 2002. A New Kind of Science. Champaign, IL: Wolfram Media. 
31 
This means “extrinsic to the object affected by causality”. For example, consider the 
problem of the origin of the real universe. Where the real universe is defined to contain all 
that is perceptible and/or of relevance to that which is perceptible, anything sufficiently real 
to have originated, caused or influenced it is contained within it by definition. Thus, extrinsic 
causality (standard determinacy) cannot be invoked to explain the origin of the real 
universe. Because every instance of causation within the real universe ultimately leads back 
to the origin of reality by causal regression, standard determinacy fails as a causal 
paradigm. 
32 
This particular formulation of the “physical causal closure thesis” is due to the 
contemporary philosopher Jaegwon Kim. [Kim, J. 2000. Mind in a Physical World. 
Cambridge, MA: MIT Press.] By the mathematical definition of closure, causal closure 
implies reflexive self-determinism. Because the physical causal closure 
thesis instead relies on standard determinism, it is conceptually deficient and powerless to 
effect causal closure.
33 
Physical is a rather ambiguous term that currently means “of or relating to matter and 
energy or the sciences dealing with them, especially physics”. It thus refers to a relationship 
of unspecified extent, namely the extended relational plexus generated by the concepts of 
matter and energy. While causality does indeed relate to matter and energy, it can be 
neither held in the hand nor converted to heat, and because it thus bears description as 
neither matter nor energy, it resides elsewhere in this extended relationship. It follows that 
causality is more than physical. Where physical is further defined as “belonging to the class 
of phenomena accessible to the scientific method”, only those levels of causality which are 
both discernable and replicable may be called “physical”. 
34 
In any case, the self-containment of the real universe is implied by the following 
contradiction: if there were any external entity or influence that were sufficiently real to affect 
the real universe, then by virtue of its reality, it would by definition be internal to the real 
universe. 
35 
Metacausality is the causal principle or agency responsible for the origin or 
“causation” of causality itself (in conjunction with state). This makes it responsible for its 
own origin as well, ultimately demanding that it self-actualize from an ontological 
groundstate consisting of unbound ontic potential. 
36 
Where time is defined on physical change, metacausal processes that affect 
potentials without causing actual physical changes are by definition atemporal. 
37 
Telesis is a convergent metacausal generalization of law and state, where law 
relates to state roughly as the syntax of a language relates to its expressions through 
generative grammar…but with the additional stipulation that as a part of syntax, generative 
grammar must in this case generate itself along with state. Feedback between syntax and 
state may thus be called telic feedback. 

38 
Beyond a certain level of specificity, no detailed knowledge of state or law is required 
in order to undertake a generic logical analysis of telesis. 
39 
To achieve causal closure with respect to final causation, a metacausal agency must 
self-configure in such a way that it relates to itself as the ultimate utility, making it the 
agency, act and product of its own self-configuration. This 3-way coincidence, called triality, 
follows from self-containment and implies that self-configuration is intrinsically utile, thus 
explaining its occurrence in terms of intrinsic utility. 
40 
It might be objected that the term “rationality” has no place in the discussion…that 
there is no reason to assume that the universe has sufficient self-recognitional coherence or 
“consciousness” to be “rational”. However, since the universe does indeed manage to 
consistently self-recognize and self-actualize in a certain objective sense, and these 
processes are to some extent functionally analogous to human self-recognition and self-
actualization, we can in this sense and to this extent justify the use of terms like 
“consciousness” and “rationality” to describe them. This is very much in the spirit of such 
doctrines as physical reductionism, functionalism and eliminativism, which assert that such 
terms devolve or refer to objective physical or functional relationships. Much the same 
reasoning applies to the term utility. 
41 
In computation theory, recognition denotes the acceptance of a language by a 
transducer according to its programming or “transductive syntax”. Because the universe is a 
self-accepting transducer, this concept has physical bearing and implications. 
42 
The concept of potential is an essential ingredient of physical reasoning. Where a 
potential is a set of possibilities from which something is actualized, potential is necessary 
to explain the existence of anything in particular (as opposed to some other partially 
equivalent possibility). 
43 
Possible constraints include locality, uncertainty, blockage, noise, interference, 
undecidability and other intrinsic features of the natural world. 
44 
Examples include the atheism and materialism riders often attached to neo-
Darwinism, and the Biblical Creationism rider often mistakenly attached to ID theory. 
45 
This view was captured by the French astronomer and mathematician Pierre Simon 
Laplace (1749-1827) in his Philosophical Essay on Probabilities (1814): “An intellect which 
at any given moment knew all the forces that animate Nature and the mutual positions of 
the beings that comprise it, if this intellect were vast enough to submit its data to analysis, 
could condense into a single formula the movement of the greatest bodies of the universe 
and that of the lightest atom: for such an intellect nothing could be uncertain; and the future 
just like the past would be present before our eyes.” This view, called Laplacian 
determinism, went virtually unchallenged until the first half of the 20th century, when it was 
undermined by such new concepts as quantum uncertainty and theoretic undecidability. But 
even though such problems seem to rule out an explicit calculation of the sort that Laplace 
envisioned, his ideal is still very much a driving force in science. 
46 
The scientific method mandates a constructive relationship between empirical 

observation and rational theorization that is designed for the investigation of phenomena 
possessing two criteria, discernability and replicability. That is, it confines scientific attention 
to that which can be exclusively and repeatedly observed under similar conditions anywhere 
in time or space; it does not cover any indiscernible or localized natural influence that is not 
conditionally (and thus falsifiably) distributed over space and time. Yet, indiscernables and 
unconditional universals must exist in order for nature to be stable – e.g., the universal, 
unconditional and intangible logical syntax which enforces consistency throughout the 
universe - and the exclusion of localized causal influences from nature is rationally 
insupportable. 
47 
In addition to other criteria, relations and properties are distinguished by arity and 
order. The arity (adicity, cardinality) of a relation is just the number of relands or things 
related, while its order depends on whether its relands are individual elements, relations of 
elements, relations of relations of elements, or so on. Similarly, a property (attribute, 
predicate) is distinguished by whether it is attributed to individual elements, properties of 
elements, properties of properties of elements, or et cetera. 
48 
Some manifolds come with special provisions for motion and causality, e.g. metrics 
defining the notion of distance, derivatives defining the notion of movement, and affine 
connections permitting the parallel transport of vectors through space and thereby 
supporting the concept of fields. 
49 
The bottom-up thesis is insidious in the way it carries the apparent randomness of 
experimental distributions of mutation events upward from low-order to high-order 
relationships, all the way to the phenotypic and social realms. This is what encourages 
many neo-Darwinists (and those whom they influence) to view mankind, and life in general, 
as “random” and “purposeless”. 
50 
Within a given set of constraints, many possible future states of a physical system 
may be causally compatible with a single present state, and many alternative present states 
may be causally compatible with a single future state. Thus, higher-order and lower-order 
causal relationships describing the same system need not uniquely determine each other by 
top-down and bottom-up causation respectively. The physical situation is suggestive of 
formal model-theoretic ambiguity as captured by (e.g.) the Duhem-Quine thesis, according 
to which a given set of observations may be consistent with multiple theories of causation, 
and a single Laplacian snapshot can result in many possible predictions or retrodictions 
depending on the causal influences that are physically active or theoretically presumed to 
be active. Dual-aspect monism ultimately transforms model-theoretic ambiguity into causal 
freedom, revealing nature as its own creative theoretician and physical modeler and thereby 
effecting causal closure. 
51 
These extensions are to some extent mutually incompatible. In order to reconcile the 
outstanding conflicts and conceptual dissonances between General Relativity and quantum 
mechanics, yet another metatheoretic extension is now required. 
52 
Langan, C. M. 2002. The Cognitive-Theoretic Model of the Universe: A New Kind of 
Reality Theory. 
Progress in Complexity, Information and Design, Vol. I.2/1.3. 

53 
Patton, C. M. and J.A. Wheeler, Is physics legislated by cosmogony? in Quantum 
Gravity, ed Isham, Penrose, and Sciama, Clarendon Press, Oxford, 1975: 538-605. In this 
paper, the term pregeometry is used in reference to "...something deeper than geometry, 
that underlies both geometry and particles…no perspective seems more promising than the 
view that it must provide the Universe with a way to come into being." The SCSPL 
extension of physical reality fits this description. 
54 
Where laws of nature incorporate not only observables but the abstractions relating 
them, bringing physical states and natural laws into coincidence reduces the set of physical 
(observable) properties to a subset of the set of abstract properties. Thus, the abstract is 
recognized as a natural generalization of the concrete. 
55 
Telic recursion is quantum metaprocess based on a generalized form of recursion 
maximizing intrinsic utility over entire (pregeometric) regions of spacetime through telic 
feedback under the guidance of coherent metacausal invariants called telons. 
56 
SCSPL is developed by adjoining to (propositional and predicate) logic a limiting 
form of model theory from which it acquires certain necessary high-level properties of any 
possible valid theory of reality at large. Thus, its syntactic and semantic validity can be 
logically established. By its method of construction, SCSPL is classified as a metaphysical 
tautology or supertautology. 
57 
Inflationary cosmology, membrane theory and various other theories have been 
assumed to require extensions external to physical reality. In contrast, SCSPL conspansive 
duality permits the extension mandated by SCSPL, as well as all other valid extensions of 
physical reality, to be physically internalized in a certain specific sense relating to 
conspansiive duality. 
58 
See   http://www.teleologic.org
 
   for a description of Teleologic Evolution. 
59 
Human psycho-intellectual, sociopolitical and technological modes of evolution may 
also be distinguished on various levels of aggregation. 
60 
In the CTMU, instances of irreducible and specified complexity are metacausally 
generalized to dynamic syntax-state relationships called telons which self-actualize by telic 
recursion. 
A Very Brief History of Time
I just had a chance encounter with a garden slug, and it got me thinking about time.
In this ostensibly inanimate, impersonal universe, a garden is a miracle. All the more so is 
a garden slug, an animal that can extract sufficient energy from the garden’s vegetable 

matter to move from place to place under its own power. When one is in the right mood, 
watching the shimmering spotted slug slide over the mulch evokes the miracle of biology in 
all its splendor; the creature’s pulsating aliveness is hypnotic. But then one recovers his 
bearings and realizes that this is only, after all, a garden slug, and that the ladder of biology 
goes much higher. The miracle of life has culminated in one’s own species, man. Unlike 
the slug, whose nervous system has barely enough complexity to let it interface with the 
environment, a man’s nervous system, nucleated by the adaptive and inventive human 
brain, can abstractly model its surroundings and project itself consciously and creatively 
through time.
A slug can learn. The small neural network that serves as its brain can be modified by 
sensory input from its environment, and the slug’s behavior modified accordingly. To this 
extent, the slug "remembers" the input. But because its simple brain cannot form an 
internal model of its changing relationship with the garden, the slug cannot recognize its 
memories as "changes"; the state of its nervous system at any given moment can pass for 
all that it has ever known. Because the neural function by which the slug identifies self is 
instinctual and perceptual as opposed to cognitive – because the slug "defines itself" 
strictly by nonreflective instinctual processing of environmental stimuli - the dependent 
neural function time is limited to here-and-now. The slug recognizes no past self or future 
self on which to define an extended temporal relationship.
As the slug’s primitive example shows, our awareness of time depends on the extent to 
which our mental models of reality reflect change. To see an object change, one must 
recall its former state for comparison to its present state, and to do that, one must recall 
one’s former perception of it. Because perception is an interaction between self and 
environment, this amounts to bringing one’s former self into conjunction with one’s present 
self. That past and present selves can be brought into conjunction across a temporal 
interval implies that momentary selves remain sufficiently alike to be conjoined; that they 
can intersect at any given moment to compare content means that the intersection is 
changeless. So when self is generalized as the intersection of all momentary selves, it 
acquires a property called time invariance. It is the rock of perception, the unchanging 
observation post from which the net of temporal connections is cast and to which it remains 
anchored. Indeed, it is the fabric from which the net is woven, its relationship with the 
environment serving as the universal template for all temporal relationships.
Through learning, mental models of time evolve in time. As the brain’s neural connections 
are modified and the strengths of existing connections are adjusted to account for new 
information regarding both self and environment – as it learns - its model of time changes 
as a function of time. In other words, the model changes with that which is modeled. If the 
brain is smart enough, then it can model itself in the process of being changed, and depict 
its own learning process as a higher level of time. But even as the self absorbs its 
educational history and deepens its reflexive understanding, it remains static at its core. 
Otherwise, it would lose temporal cohesion and fall apart. Since self is static, time too 
should possess a static description that does not change in the temporal flow it describes 
(if time were the water flowing in a river, then a static description of time would be 
analogous to the rocky banks that determine the river’s course).
Such a description arises by abstraction. As cognitive models become more sophisticated, 

cognition becomes increasingly abstract; concepts become increasingly independent of the 
particular objects they describe. Among the first things to be abstracted are space and 
time. The most general abstract system incorporating both is a language. Although the 
term "language" usually refers to a natural language like English, it is actually more 
general. Mathematically, a formal language consists of three ingredients: a set of elements 
to be combined as strings (e.g., symbols, memes), a set of structural rules governing their 
arrangement in space, and a set of grammatical rules governing their transformations in 
time. Together, the latter two ingredients form the syntax of the language. It follows that 
neural, cognitive-perceptual, and physical systems can be described as languages, and the 
laws which govern them as their syntaxes. On a subjective level, time itself can be 
abstractly characterized as the grammar of the joint language of cognition and perception. 
The rules of this grammar are the general ingredients of subjective time.
Because time is defined in terms of transformations among spatial arrangements of 
objects, it is conceptually entwined with space. Thus, it is actually part of a linguistic 
complex called spacetime. Spatiotemporal relations exist on many levels; if level one 
consists of simple relationships of objects in space and time, then level two consists of 
relationships of such relationships, and so on. Because logic is stratified in much the same 
way, one can say that time is stratified in a manner corresponding to predicate logic. This 
must be true in any case, since any meaningful description of time is logically formulated. 
Spatiotemporal stratification allows time to be viewed on various scales corresponding to 
ascending series of contexts: e.g., personal awareness, interpersonal relationships, social 
evolution, evolutionary biology, and so on. The histories of people, institutions, cultures, 
and species are nested like Chinese boxes, with the abstract principles of each history 
occupying a level of temporal grammar corresponding to an order of predicate logic.
Because of the relation between self-awareness and temporal awareness, temporal 
stratification induces a stratification of self. What we have already described as the static 
intersect of momentary selves becomes a stratified relationship…a terrace of temporal 
vantages conducing to long-term self-integration. As the self becomes stratified, the 
principles abstracted from higher orders of experience tend to be objectivized due to their 
generality, with science and philosophy among the results. Thus, the subjective and 
objective sides of reality – the self and the environment – tend to merge in a symmetric 
way. On one hand, the environment is absorbed by the self through experience, and the 
laws of nature are thereby abstracted; on the other hand, the self is projected onto the 
environment in such a way that it "selects" the laws of nature by analogy to its own internal 
laws. Either way, the core self tends to intersect with the environment as momentary selves 
are intersected within it. This brings the subjective and objective phases of reality - and 
time - into closer correspondence, blurring the distinction between them from an analytic 
standpoint.
As time grows more abstract, ways are sought to measure it, diagram it and analyze it 
numerically. This requires a universal depiction of space and time against which arbitrary 
processes can be differentially graphed and metered. Such a depiction was introduced by 
the Frenchman René Descartes in the first half of the 17th century. It was called analytic 
geometry, and it depicted time and the dimensions of space as straight, mutually 
perpendicular axes. In analytic geometry, any set of numerically-scaled space and time 
axes associated with any set of properties or attributes defines a coordinate system for 

assigning numbers to points, and simple processes appear as the graphs of algebraic 
functions. A few decades later, Newton and Leibniz independently discovered a new kind 
of mathematics, the infinitesimal calculus, by which to numerically quantify the rates of 
such processes. These innovations, which laid the foundations of modern science and 
engineering, suffice to this day in many practical contexts. Even though garden-variety 
analytic geometry was technically superseded by the Theory of Relativity – which was itself 
constructed on an analytic-geometric foundation - it gives a very close approximation of 
relativity in most situations.
Unfortunately, the conveniences of analytic geometry came at the price of mind-body 
dualism. This was Descartes’ idea that the self, or "mind", was a nonphysical substance 
that could be left out of physical reasoning with impunity. For some purposes, this was true. 
But as we saw in the next-to-last paragraph, the relationship of mind to reality is not that 
simple. While the temporal grammar of physics determines the neural laws of cognition, 
cognitive grammar projects itself onto physical reality in such a way as to determine the 
form that physical grammar must assume. Because the form of physical grammar limits the 
content of physical grammar, this makes cognition a potential factor in determining the laws 
of nature. In principle, cognitive and physical grammars may influence each other 
symmetrically.
The symmetric influence of cognitive and physical grammars implies a directional 
symmetry of time. Although time is usually seen as a one-way street, it need not be; the 
mere fact that a street is marked "one way" does not stop it from being easily traveled in 
the unauthorized direction. Indeed, two-way time shows up in both quantum physics and 
relativity theory, the primary mainstays of modern physics. Thus, it is not physically 
warranted to say that cognition cannot influence the laws of physics because the laws of 
physics "precede cognition in time". If we look at the situation from the other direction, we 
can as easily say that cognition "precedes" the laws of physics in reverse time…and point 
to the strange bidirectional laws of particle physics to justify our position. These laws are of 
such a nature that they can as well be called laws of perception as laws of physics.
Before we get to the final word on time, there is one more aspect of physical grammar that 
must be considered. Physical reasoning sometimes requires a distinction between two 
kinds of time: ordinary time and cosmic time. With respect to observations made at normal 
velocities, ordinary time behaves in a way described by Newtonian analytic geometry; at 
higher velocities, and in the presence of strong gravitational fields, it behaves according to 
Einstein’s Special and General Theories of Relativity. But not long after Einstein formulated 
his General Theory, it was discovered that the universe, AKA spacetime, was expanding. 
Because cosmic expansion seems to imply that the universe began as a dimensionless 
point, the universe must have been created, and the creation event must have occurred on 
a higher level of time: cosmic time. Whereas ordinary time accommodates changes 
occurring within the spacetime manifold, this is obviously not so for the kind of time in 
which the manifold itself changes.
Now for the fly in the cosmological ointment. As we have seen, it is the nature of the 
cognitive self to formulate models incorporating ever-higher levels of change (or time). 
Obviously, the highest level of change is that characterizing the creation of reality. Prior to 
the moment of creation, the universe was not there; afterwards, the universe was there. 

This represents a sizable change indeed! Unfortunately, it also constitutes a sizable 
paradox. If the creation of reality was a real event, and if this event occurred in cosmic 
time, then cosmic time itself is real. But then cosmic time is an aspect of reality and can 
only have been created with reality. This implies that cosmic time, and in fact reality, must 
have created themselves!
The idea that the universe created itself brings a whole new meaning to bidirectional time, 
and thus to the idea that cognition may play a role in the creation of reality. As a self-
creative mechanism for the universe is sought, it becomes apparent that cognition is the 
only process lending itself to plausible interpretation as a means of temporal feedback from 
present to past. Were cognition to play such a role, then in a literal sense, its most 
universal models of temporal reality would become identical to the reality being modeled. 
Time would become cognition, and space would become a system of geometric relations 
that evolves by distributed cognitive processing.
Here comes the surprise: such a model exists. Appropriately enough, it is called the 
Cognition-Theoretic Model of the Universe, or CTMU for short. A cross between John 
Archibald Wheeler’s Participatory Universe and the Stephen Hawking-James Hartle 
"imaginary time" theory of cosmology proposed in Hawking’s phenomenal book A Brief 
History of Time, the CTMU resolves many of the most intractable paradoxes known to 
physical science while explaining recent data which indicate that the universe is expanding 
at an accelerating rate. Better yet, it bestows on human consciousness a level of meaning 
that was previously approached only by religion and mysticism. If it passes the test of time 
– and there are many good reasons to think that it will - then it will be the greatest step that 
humanity has yet taken towards a real understanding of its most (or least?) timeless 
mystery.
And so the circle closes. Time becomes a cosmogonic loop whereby the universe creates 
itself. The origin of our time concept, the self, becomes the origin of time itself. Our 
cognitive models of time become a model of time-as-cognition. And the languages of 
cognition and physics become one self-configuring, self-processing language of which time 
is the unified grammar. Talk about "time out of mind"!
And all this because of a little garden slug.
The Theory of Theories
You know what they say about theories: everybody’s got one.  In fact, some people have a 
theory about pretty much everything.  That’s not one Master Theory of Everything, mind 
you…that’s a separate theory about every little thing under the sun.  (To have a Master 
Theory, you have to be able to tie all those little theories together.)
But what is a “theory”?  Is a theory just a story that you can make up about something, 
being as fanciful as you like?  Or does a theory at least have to seem like it might be true? 
Even more stringently, is a theory something that has to be rendered in terms of logical and 
mathematical symbols, and described in plain language only after the original chicken-

scratches have made the rounds in academia?
A theory is all of these things.  A theory can be good or bad, fanciful or plausible, true or 
false. The only firm requirements are that it (1) have a subject, and (2) be stated in a 
language in terms of which the subject can be coherently described.  Where these criteria 
hold, the theory can always be “formalized”, or translated into the symbolic language of 
logic and mathematics. Once formalized, the theory can be subjected to various 
mathematical tests for truth and internal consistency. 
But doesn’t that essentially make “theory” synonymous with “description”?  Yes.  A theory 
is just a description of something.  If we can use the logical implications of this description 
to relate the components of that something to other components in revealing ways, then 
the theory is said to have “explanatory power”.  And if we can use the logical implications of 
the description to make correct predictions about how that something behaves under 
various conditions, then the theory is said to have “predictive power”. 
From a practical standpoint, in what kinds of theories should we be interested?  Most 
people would agree that in order to be interesting, a theory should be about an important 
subject…a subject involving something of use or value to us, if even on a purely abstract 
level.  And most would also agree that in order to help us extract or maximize that value, 
the theory must have explanatory or predictive power.  For now, let us call any theory 
meeting both of these criteria a “serious” theory.   
Those interested in serious theories include just about everyone, from engineers and 
stockbrokers to doctors, automobile mechanics and police detectives.  Practically anyone 
who gives advice, solves problems or builds things that function needs a serious theory 
from which to work.   But three groups who are especially interested in serious theories are 
scientists, mathematicians and philosophers.  These are the groups which place the 
strictest requirements on the theories they use and construct. 
While there are important similarities among the kinds of theories dealt with by scientists, 
mathematicians and philosophers, there are important differences as well.  The most 
important differences involve the subject matter of the theories.  Scientists like to base their 
theories on experiment and observation of the real world…not on perceptions themselves, 
but on what they regard as concrete “objects of the senses”.  That is, they like their 
theories to be empirical. Mathematicians, on the other hand, like their theories to be 
essentially rational…to be based on logical inference regarding abstract mathematical 
objects existing in the mind, independently of the senses.  And philosophers like to pursue 
broad theories of reality aimed at relating these two kinds of object.  (This actually 
mandates a third kind of object, the infocognitive syntactic operator…but another time.)     
Of the three kinds of theory, by far the lion’s share of popular reportage is commanded by 
theories of science.  Unfortunately, this presents a problem.  For while science owes a 
huge debt to philosophy and mathematics – it can be characterized as the child of the 
former and the sibling of the latter - it does not even treat them as its equals.  It treats its 
parent, philosophy, as unworthy of consideration.  And although it tolerates and uses 
mathematics at its convenience, relying on mathematical reasoning at almost every turn, it 
acknowledges the remarkable obedience of objective reality to mathematical principles as 

little more than a cosmic “lucky break”.  
Science is able to enjoy its meretricious relationship with mathematics precisely because of 
its queenly dismissal of philosophy.  By refusing to consider the philosophical relationship 
between the abstract and the concrete on the supposed grounds that philosophy is 
inherently impractical and unproductive, it reserves the right to ignore that relationship even 
while exploiting it in the construction of scientific theories.  And exploit the relationship it 
certainly does!  There is a scientific platitude stating that if one cannot put a number to 
one's data, then one can prove nothing at all.  But insofar as numbers are arithmetically 
and algebraically related by various mathematical structures, the platitude amounts to a 
thinly veiled affirmation of the mathematical basis of knowledge. 
Although scientists like to think that everything is open to scientific investigation, they have 
a rule that explicitly allows them to screen out certain facts.  This rule is called the scientific 
method.  Essentially, the scientific method says that every scientist’s job is to (1) observe 
something in the world, (2) invent a theory to fit the observations, (3) use the theory to 
make predictions, (4) experimentally or observationally test the predictions, (5) modify the 
theory in light of any new findings, and (6) repeat the cycle from step 3 onward.  But while 
this method is very effective for gathering facts that match its underlying assumptions, it is 
worthless for gathering those that do not. 
In fact, if we regard the scientific method as a theory about the nature and acquisition of 
scientific knowledge (and we can), it is not a theory of knowledge in general.  It is only a 
theory of things accessible to the senses.  Worse yet, it is a theory only of sensible things 
that have two further attributes: they are non-universal and can therefore be distinguished 
from the rest of sensory reality, and they can be seen by multiple observers who are able 
to “replicate” each other’s observations under like conditions.  Needless to say, there is no 
reason to assume that these attributes are necessary even in the sensory realm.  The first 
describes nothing general enough to coincide with reality as a whole – for example, the 
homogeneous medium of which reality consists, or an abstract mathematical principle that 
is everywhere true - and the second describes nothing that is either subjective, like human 
consciousness, or objective but rare and unpredictable…e.g. ghosts, UFOs and yetis, of 
which jokes are made but which may, given the number of individual witnesses reporting 
them, correspond to real phenomena. 
The fact that the scientific method does not permit the investigation of abstract 
mathematical principles is especially embarrassing in light of one of its more crucial steps: 
“invent a theory to fit the observations.”  A theory happens to be a logical and/or 
mathematical construct whose basic elements of description are mathematical units and 
relationships.  If the scientific method were interpreted as a blanket description of reality, 
which is all too often the case, the result would go something like this: “Reality consists of 
all and only that to which we can apply a protocol which cannot be applied to its own 
(mathematical) ingredients and is therefore unreal.” Mandating the use of “unreality” to 
describe “reality” is rather questionable in anyone’s protocol.  
What about mathematics itself?  The fact is, science is not the only walled city in the 
intellectual landscape.  With equal and opposite prejudice, the mutually exclusionary 
methods of mathematics and science guarantee their continued separation despite the 

(erstwhile) best efforts of philosophy.  While science hides behind the scientific method, 
which effectively excludes from investigation its own mathematical ingredients, 
mathematics divides itself into “pure” and “applied” branches and explicitly divorces the 
“pure” branch from the real world. Notice that this makes “applied” synonymous with 
“impure”.  Although the field of applied mathematics by definition contains every practical 
use to which mathematics has ever been put, it is viewed as “not quite mathematics” and 
therefore beneath the consideration of any “pure” mathematician.   
In place of the scientific method, pure mathematics relies on a principle called the 
axiomatic method.  The axiomatic method begins with a small number of self-evident 
statements called axioms and a few rules of inference through which new statements, 
called theorems, can be derived from existing statements.  In a way parallel to the scientific 
method, the axiomatic method says that every mathematician’s job is to (1) conceptualize a 
class of mathematical objects; (2) isolate its basic elements, its most general and self-
evident principles, and the rules by which its truths can be derived from those principles; 
(3) use those principles and rules to derive theorems, define new objects, and formulate 
new propositions about the extended set of theorems and objects; (4) prove or disprove 
those propositions; (5) where the proposition is true, make it a theorem and add it to the 
theory; and (6) repeat from step 3 onwards. 
The scientific and axiomatic methods are like mirror images of each other, but located in 
opposite domains.  Just replace “observe” with “conceptualize” and “part of the world” with 
“class of mathematical objects”, and the analogy practically completes itself.  Little wonder, 
then, that scientists and mathematicians often profess mutual respect.  However, this 
conceals an imbalance.  For while the activity of the mathematician is integral to the 
scientific method, that of the scientist is irrelevant to mathematics (except for the kind of 
scientist called a “computer scientist”, who plays the role of ambassador between the two 
realms).  At least in principle, the mathematician is more necessary to science than the 
scientist is to mathematics. 
As a philosopher might put it, the scientist and the mathematician work on opposite sides 
of the Cartesian divider between mental and physical reality.  If the scientist stays on his 
own side of the divider and merely accepts what the mathematician chooses to throw 
across, the mathematician does just fine.  On the other hand, if the mathematician does not 
throw across what the scientist needs, then the scientist is in trouble.  Without the 
mathematician’s functions and equations from which to build scientific theories, the 
scientist would be confined to little more than taxonomy.  As far as making quantitative 
predictions were concerned, he or she might as well be guessing the number of jellybeans 
in a candy jar.  
From this, one might be tempted to theorize that the axiomatic method does not suffer from 
the same kind of inadequacy as does the scientific method…that it, and it alone, is 
sufficient to discover all of the abstract truths rightfully claimed as “mathematical”.  But 
alas, that would be too convenient.  In 1931, an Austrian mathematical logician named Kurt 
Gödel proved that there are true mathematical statements that cannot be proven by means 
of the axiomatic method. Such statements are called “undecidable”.  Gödel’s finding rocked 
the intellectual world to such an extent that even today, mathematicians, scientists and 
philosophers alike are struggling to figure out how best to weave the loose thread of 

undecidability into the seamless fabric of reality. 
To demonstrate the existence of undecidability, Gödel used a simple trick called self-
reference. Consider the statement “this sentence is false.”  It is easy to dress this 
statement up as a logical formula.  Aside from being true or false, what else could such a 
formula say about itself? Could it pronounce itself, say, unprovable?  Let’s try it: "This 
formula is unprovable".  If the given formula is in fact unprovable, then it is true and 
therefore a theorem.  Unfortunately, the axiomatic method cannot recognize it as such 
without a proof.  On the other hand, suppose it is provable.  Then it is self-apparently false 
(because its provability belies what it says of itself) and yet true (because provable without 
respect to content)!  It seems that we still have the makings of a paradox…a statement that 
is "unprovably provable" and therefore absurd.  
But what if we now introduce a distinction between levels of proof?  For example, what if 
we define a metalanguage as a language used to talk about, analyze or prove things 
regarding statements in a lower-level object language, and call the base level of Gödel’s 
formula the "object" level and the higher (proof) level the "metalanguage" level?  Now we 
have one of two things: a statement that can be metalinguistically proven to be linguistically 
unprovable, and thus recognized as a theorem conveying valuable information about the 
limitations of the object language, or a statement that cannot be metalinguistically proven 
to be linguistically unprovable, which, though uninformative, is at least no paradox.  Voilà: 
self-reference without paradox!  It turns out that "this formula is unprovable" can be 
translated into a generic example of an undecidable mathematical truth.  Because the 
associated reasoning involves a metalanguage of mathematics, it is called 
“metamathematical”.
It would be bad enough if undecidability were the only thing inaccessible to the scientific 
and axiomatic methods together. But the problem does not end there.  As we noted above, 
mathematical truth is only one of the things that the scientific method cannot touch.  The 
others include not only rare and unpredictable phenomena that cannot be easily captured 
by microscopes, telescopes and other scientific instruments, but things that are too large or 
too small to be captured, like the whole universe and the tiniest of subatomic particles; 
things that are “too universal” and therefore indiscernable, like the homogeneous medium 
of which reality consists; and things that are “too subjective”, like human consciousness, 
human emotions, and so-called “pure qualities” or qualia.  Because mathematics has thus 
far offered no means of compensating for these scientific blind spots, they continue to mark 
holes in our picture of scientific and mathematical reality. 
But mathematics has its own problems.  Whereas science suffers from the problems just 
described – those of indiscernability and induction, nonreplicability and subjectivity - 
mathematics suffers from undecidability.  It therefore seems natural to ask whether there 
might be any other inherent weaknesses in the combined methodology of math and 
science.  There are indeed.  Known as the Lowenheim-Skolem theorem and the Duhem-
Quine thesis, they are the respective stock-in-trade of disciplines called model theory and 
the philosophy of science (like any parent, philosophy always gets the last word).  These 
weaknesses have to do with ambiguity…with the difficulty of telling whether a given theory 
applies to one thing or another, or whether one theory is “truer” than another with respect 
to what both theories purport to describe.  

But before giving an account of Lowenheim-Skolem and Duhem-Quine, we need a brief 
introduction to model theory.  Model theory is part of the logic of “formalized theories”, a 
branch of mathematics dealing rather self-referentially with the structure and interpretation 
of theories that have been couched in the symbolic notation of mathematical logic…that is, 
in the kind of mind-numbing chicken-scratches that everyone but a mathematician loves to 
hate.  Since any worthwhile theory can be formalized, model theory is a sine qua non of 
meaningful theorization.  
Let’s make this short and punchy. We start with propositional logic, which consists of 
nothing but tautological, always-true relationships among sentences represented by single 
variables. Then we move to predicate logic, which considers the content of these sentential 
variables…what the sentences actually say.  In general, these sentences use symbols 
called quantifiers to assign attributes to variables semantically representing mathematical 
or real-world objects. Such assignments are called “predicates”.  Next, we consider 
theories, which are complex predicates that break down into systems of related predicates; 
the universes of theories, which are the mathematical or real-world systems described by 
the theories; and the descriptive correspondences themselves, which are called 
interpretations.  A model of a theory is any interpretation under which all of the theory’s 
statements are true.  If we refer to a theory as an object language and to its referent as an 
object universe, the intervening model can only be described and validated in a 
metalanguage of the language-universe complex. 
Though formulated in the mathematical and scientific realms respectively, Lowenheim-
Skolem and Duhem-Quine can be thought of as opposite sides of the same model-
theoretic coin. Lowenheim-Skolem says that a theory cannot in general distinguish 
between two different models; for example, any true theory about the numeric relationship 
of points on a continuous line segment can also be interpreted as a theory of the integers 
(counting numbers).  On the other hand, Duhem-Quine says that two theories cannot in 
general be distinguished on the basis of any observation statement regarding the universe. 
Just to get a rudimentary feel for the subject, let’s take a closer look at the Duhem-Quine 
Thesis.  Observation statements, the raw data of science, are statements that can be 
proven true or false by observation or experiment.  But observation is not independent of 
theory; an observation is always interpreted in some theoretical context. So an experiment 
in physics is not merely an observation, but the interpretation of an observation.  This leads 
to the Duhem Thesis, which states that scientific observations and experiments cannot 
invalidate isolated hypotheses, but only whole sets of theoretical statements at once.  This 
is because a theory T composed of various laws {Li}, i=1,2,3,… almost never entails an 
observation statement except in conjunction with various auxiliary hypotheses {Aj}, j=1,2,3,
… .  Thus, an observation statement at most disproves the complex {Li+Aj}.  
To take a well-known historical example, let T = {L1,L2,L3} be Newton’s three laws of 
motion, and suppose that these laws seem to entail the observable consequence that the 
orbit of the planet Uranus is O.  But in fact, Newton’s laws alone do not determine the orbit 
of Uranus.  We must also consider things like the presence or absence of other forces, 
other nearby bodies that might exert appreciable gravitational influence on Uranus, and so 
on.  Accordingly, determining the orbit of Uranus requires auxiliary hypotheses like A1 = 

“only gravitational forces act on the planets”, A2 = “the total number of solar planets, 
including Uranus, is 7,” et cetera.  So if the orbit in question is found to differ from the 
predicted value O, then instead of simply invalidating the theory T of Newtonian 
mechanics, this observation invalidates the entire complex of laws and auxiliary 
hypotheses {L1,L2,L3;A1,A2,…}.  It would follow that at least one element of this complex is 
false, but which one?  Is there any 100% sure way to decide? 
As it turned out, the weak link in this example was the hypothesis A2 = “the total number of 
solar planets, including Uranus, is 7”.  In fact, there turned out to be an additional large 
planet, Neptune, which was subsequently sought and located precisely because this 
hypothesis (A2) seemed open to doubt.  But unfortunately, there is no general rule for 
making such decisions. Suppose we have two theories T1 and T2 that predict observations 
O and not-O respectively. Then an experiment is crucial with respect to T1 and T2 if it 
generates exactly one of the two observation statements O or not-O.  Duhem’s arguments 
show that in general, one cannot count on finding such an experiment or observation.  In 
place of crucial observations, Duhem cites le bon sens (good sense), a non-logical faculty 
by means of which scientists supposedly decide such issues.  Regarding the nature of this 
faculty, there is in principle nothing that rules out personal taste and cultural bias.  That 
scientists prefer lofty appeals to Occam’s razor, while mathematicians employ justificative 
terms like beauty and elegance, does not exclude less savory influences.
So much for Duhem; now what about Quine?  The Quine thesis breaks down into two 
related theses.  The first says that there is no distinction between analytic statements (e.g. 
definitions) and synthetic statements (e.g. empirical claims), and thus that the Duhem 
thesis applies equally to the so-called a priori disciplines.  To make sense of this, we need 
to know the difference between analytic and synthetic statements.  Analytic statements are 
supposed to be true by their meanings alone, matters of empirical fact notwithstanding, 
while synthetic statements amount to empirical facts themselves.  Since analytic 
statements are necessarily true statements of the kind found in logic and mathematics, 
while synthetic statements are contingently true statements of the kind found in science, 
Quine’s first thesis posits a kind of equivalence between mathematics and science.  In 
particular, it says that epistemological claims about the sciences should apply to 
mathematics as well, and that Duhem’s thesis should thus apply to both. 
Quine’s second thesis involves the concept of reductionism.  Reductionism is the claim that 
statements about some subject can be reduced to, or fully explained in terms of, 
statements about some (usually more basic) subject.  For example, to pursue chemical 
reductionism with respect to the mind is to claim that mental processes are really no more 
than biochemical interactions.  Specifically, Quine breaks from Duhem in holding that not 
all theoretical claims, i.e. theories, can be reduced to observation statements.  But then 
empirical observations “underdetermine” theories and cannot decide between them.  This 
leads to a concept known as Quine’s holism; because no observation can reveal which 
member(s) of a set of theoretical statements should be re-evaluated, the re-evaluation of 
some statements entails the re-evaluation of all. 
Quine combined his two theses as follows.  First, he noted that a reduction is essentially an 
analytic statement to the effect that one theory, e.g. a theory of mind, is defined on another 
theory, e.g. a theory of chemistry.  Next, he noted that if there are no analytic statements, 

then reductions are impossible.  From this, he concluded that his two theses were 
essentially identical.  But although the resulting unified thesis resembled Duhem’s, it 
differed in scope. For whereas Duhem had applied his own thesis only to physical theories, 
and perhaps only to theoretical hypothesis rather than theories with directly observable 
consequences, Quine applied his version to the entirety of human knowledge, including 
mathematics.  If we sweep this rather important distinction under the rug, we get the so-
called “Duhem-Quine thesis”. 
Because the Duhem-Quine thesis implies that scientific theories are underdetermined by 
physical evidence, it is sometimes called the Underdetermination Thesis.  Specifically, it 
says that because the addition of new auxiliary hypotheses, e.g. conditionals involving “if…
then” statements, would enable each of two distinct theories on the same scientific or 
mathematical topic to accommodate any new piece of evidence, no physical observation 
could ever decide between them.  
The messages of Duhem-Quine and Lowenheim-Skolem are as follows: universes do not 
uniquely determine theories according to empirical laws of scientific observation, and 
theories do not uniquely determine universes according to rational laws of mathematics. 
The model-theoretic correspondence between theories and their universes is subject to 
ambiguity in both directions. If we add this descriptive kind of ambiguity to ambiguities of 
measurement, e.g. the Heisenberg Uncertainty Principle that governs the subatomic scale 
of reality, and the internal theoretical ambiguity captured by undecidability, we see that 
ambiguity is an inescapable ingredient of our knowledge of the world.  It seems that math 
and science are…well, inexact sciences. 
How, then, can we ever form a true picture of reality?  There may be a way.  For example, 
we could begin with the premise that such a picture exists, if only as a “limit” of theorization 
(ignoring for now the matter of showing that such a limit exists).  Then we could educe 
categorical relationships involving the logical properties of this limit to arrive at a description 
of reality in terms of reality itself.  In other words, we could build a self-referential theory of 
reality whose variables represent reality itself, and whose relationships are logical 
tautologies.  Then we could add an instructive twist.  Since logic consists of the rules of 
thought, i.e. of mind, what we would really be doing is interpreting reality in a generic 
theory of mind based on logic.  By definition, the result would be a cognitive-theoretic 
model of the universe.  
Gödel used the term incompleteness to describe that property of axiomatic systems due to 
which they contain undecidable statements.  Essentially, he showed that all sufficiently 
powerful axiomatic systems are incomplete by showing that if they were not, they would be 
inconsistent. Saying that a theory is “inconsistent” amounts to saying that it contains one or 
more irresolvable paradoxes.  Unfortunately, since any such paradox destroys the 
distinction between true and false with respect to the theory, the entire theory is crippled by 
the inclusion of a single one. This makes consistency a primary necessity in the 
construction of theories, giving it priority over proof and prediction.  A cognitive-theoretic 
model of the universe would place scientific and mathematical reality in a self-consistent 
logical environment, there to await resolutions for its most intractable paradoxes. 
For example, modern physics is bedeviled by paradoxes involving the origin and 

directionality of time, the collapse of the quantum wave function, quantum nonlocality, and 
the containment problem of cosmology.  Were someone to present a simple, elegant 
theory resolving these paradoxes without sacrificing the benefits of existing theories, the 
resolutions would carry more weight than any number of predictions.  Similarly, any theory 
and model conservatively resolving the self-inclusion paradoxes besetting the 
mathematical theory of sets, which underlies almost every other kind of mathematics, could 
demand acceptance on that basis alone.  Wherever there is an intractable scientific or 
mathematical paradox, there is dire need of a theory and model to resolve it. 
If such a theory and model exist – and for the sake of human knowledge, they had better 
exist – they use a logical metalanguage with sufficient expressive power to characterize 
and analyze the limitations of science and mathematics, and are therefore philosophical 
and metamathematical in nature.  This is because no lower level of discourse is capable of 
uniting two disciplines that exclude each other’s content as thoroughly as do science and 
mathematics.  
Now here’s the bottom line: such a theory and model do indeed exist.  But for now, let us 
satisfy ourselves with having glimpsed the rainbow under which this theoretic pot of gold 
awaits us.
An Interdisciplinary Approach to Reality
1.   General Introduction
Since the dawn of our species, human beings have been asking difficult questions about 
themselves, the universe and the nature of existence, but have lacked a unified 
conceptual framework strong and broad enough to yield the answers.  Enter the 
Cognitive-Theoretic Model of the Universe (CTMU). 
Scientific theories are mental constructs that take objective reality as their content, with 
the scientific method putting observation first and letting explanation be determined by 
experiment.  But because all theories have logical properties that are abstract and 
mathematical, and therefore independent of observation - it is these very properties that 
let us recognize and understand our world - we could equally well start with them and see 
what they might tell us about objective reality.  Just as scientific observation makes 
demands on theories, the logic of theories makes demands on scientific observation, and 
these demands tell us in a general way what we can observe about the universe.
In other words, a comprehensive theory of reality is not just about observation, but 
theories and their logical requisites.  Since theories are mental constructs, and mental 
means "of the mind", this can be rephrased as follows: mind and reality are linked in 
mutual dependence on the most basic level of understanding.  It is this linkage of the 
abstract and the concrete, the subjective and the objective, the internal and the external, 
that constitutes the proper focus of a Theory of Everything (TOE).  Since reality forever 
retains the ability to surprise us, the task of scientific observation can never be completed 
with absolute certainty, and this means that a comprehensive theory of reality cannot be 
based on scientific observation alone.  Instead, reality theory must be based on the logic 

underlying the general process of making and interpreting scientific observations.  Since 
observation and interpretation are predicated on the relationship holding between mind 
and reality, the Cognitive-Theoretic Model of the Universe delivers an account of that 
relationship. 
For the most part, ordinary theories are linguistic and mathematical descriptions of 
specific observations.  In contrast, the CTMU is a metatheory about the general 
relationship between theories and observations…i.e., about science, knowledge and 
cognition themselves.  In explaining this relationship, the CTMU shows that reality 
possesses a complex property akin to self-awareness; just as the mind is real, reality is in 
some respects like a mind.  But when we attempt to answer the obvious question "whose 
mind?", the answer turns out to qualify by reasonable standards as a mathematical and 
scientific definition of God.  This implies that we all exist in what can be called "the Mind 
of God”, and that our individual minds are parts of this Universal Mind.  As such, they are 
directly connected to the greatest source of knowledge and power that exists.  This 
connection of our minds to the Mind of the Universe, which we sometimes call the soul or 
spirit, is the most essential part of being human.
In its exciting development of these and other ideas, the Cognitive-Theoretic Model of the 
Universe helps us to understand not only the nature of reality, but the integral role played 
by human beings in the creation and maintenance of the world they inhabit.  In the 
process, the CTMU enables us to comprehend the psychological, metaphysical, and 
ethical ramifications of the relationship between man and the cosmos, and thus what it 
means to be human.
Among the questions that are answered within the framework of the CTMU: What is the 
nature of humanity's relationship with God?  What is our relationship with each other on 
individual and cultural levels? Do human beings possess free will?  Is there life after 
death?  Is there a physical basis for spirituality?  Where did the universe come from?   Is 
there such a thing as absolute good or absolute evil?  These are just a few of the many 
burning philosophical dilemmas that mankind has pondered since its infancy.  When 
these dilemmas are finally considered in the advanced conceptual framework of a true 
TOE like the CTMU, their answers fall into place as though under the influence of an 
attractive force.  The mystery melts away, but not the wonder.
In its role as a comprehensive theory of reality, the Cognitive-Theoretic Model of the 
Universe serves as a firm basis for the unification of science and theology, leading us 
inevitably in the direction of intellectual enlightenment and a collective spiritual 
awakening.  The traditional Cartesian divider between body and mind, matter and 
thought, science and spirituality is penetrated by logical reasoning of a higher order than 
ordinary scientific reasoning, but no less scientific than any other kind of mathematical 
truth.  Accordingly, it serves as the long-awaited gateway between science and 
humanism, a bridge of reason over what has long seemed an impassable gulf.  
2.   The CTMU and Physics
The avowed goal of physics is to produce what is sometimes called a "Theory of 
Everything" or TOE. As presently conceived, the TOE is thought to consist of one 
equation describing a single "superforce" unifying all the forces of nature (gravity, 

electromagnetism, and the strong and weak nuclear forces). But this is actually an 
oversimplification; every equation must be embedded in a theory, and theories require 
modelsfor their proper interpretation. Unfortunately, the currently available theory and 
model lack three important properties: closure, consistency and comprehensivity. That is, 
they are not self-contained; they suffer from various intractable paradoxes; and they 
conspicuously exclude or neglect various crucial factors, including subjective ones like 
consciousness and emotion. Since the excluded factors fall as squarely under the 
heading everything as the included ones, a real TOE has no business omitting them. So 
as now envisioned by physicists, the TOE is misnamed as a "theory of everything".
The CTMU, on the other hand, is a TOE framework in which “everything” really means 
everything.  Whereas the currently-envisioned TOE emphasizes objective reality at the 
expense of its subjective counterpart (mind), the CTMU places mind on the agenda at the 
outset.  It does this not by making assumptions, but by eliminating the erroneous scientific 
assumption that mind and objective reality can be even tentatively separated.  To do this, 
it exploits not just what we know of objective reality – the so-called “everything” of the 
standard TOE – but also what we know of the first word in “TOE”, namely theory.  In other 
words, it brings the logic of formalized theories to bear on reality theory.
Although this is a mathematically obvious move, it has been almost completely 
overlooked in the physical and mathematical sciences.  By correcting this error, the 
CTMU warrants description as a theory of the relationship between the mind of the 
theorist and the objective reality about which it theorizes, completing the program of 
subjective-objective unification already inherent in certain aspects of the formalisms of 
relativity and quantum mechanics.  In the process, it also brings the quantum and 
classical realms of physics into the sort of intimate contact that can only be provided by a 
fundamentally new model of physical and metaphysical reality…a model truly worthy of 
being called a “new paradigm”.
Fundamental to this new model are revisions of basic physical concepts including space, 
time, matter and motion.  Space, once a featureless medium aimlessly proliferating 
through cosmic expansion, becomes a distributed syntactic structure iteratively reborn of 
matter and subject to conspansive evacuation and rescaling.  Time, previously envisioned 
as a quasi-spatial linear dimension along which the cosmos hurtles like a runaway 
locomotive, becomes the means by which the universe self-configures…an SCSPL-
grammatical symphony of logico-linguistic transformations played by the self-creating 
cosmos.  Lumps of matter, no longer the inert pawns of external laws of physics, become 
SCSPL syntactic operators containing within themselves the syntactic rules by which they 
internally process each other to create new states of physical reality.  And motion, once 
seen as the passage of material attribute-ensembles through adjacent infinitesimal cells 
of empty space displaying them as content, becomes an iterative, self-simulative 
sequence of endomorphic self-projections by moving bodies themselves. 
3.   The CTMU and Mathematics
Mathematics, like the empirical sciences, requires protection from subjective 
contamination. Just as the scientific method is designed to exclude bias from science, the 
axiomatic method is designed to specify an initial set of definitive or “self-evident” 

principles and exclude all further assumptions from the deductive process. Accordingly, 
mathematical entities are objectified by mathematicians in the same way that physical 
entities are objectified by scientists; they are considered to reside outside the subject, 
concealed in objective reality as “hidden programming” or secreted away in an ideal 
Platonic realm beyond the reach of the senses. Rejecting these notions in absence of a 
model of reality explicitly relating the mathematical and material worlds, others choose to 
believe that mathematics is a feature or even an invention of the human mind. But since 
any meaningful explanation can be logically formalized, and since logic is an essential 
ingredient of mathematics, subjectivizing mathematics within a (Cartesian) dualistic 
framework would again seem to leave the essential regularities of nature without an 
objective explanation.
 
Replacing Cartesian dualism with an advanced form of dual-aspect monism, the CTMU 
treats the abstract and mathematical, and the concrete and physical, as coincident 
aspects of the same reality.  Reality becomes a self-distributed “hological” system whose 
essential structure is replicated everywhere within it as mathematical rules of self-
recognition and self-processing.  Hology, the central attribute of any self-recognizing, self-
processing entity, is a logical species of self-similarity according to which such an entity 
distributes over itself as rules of structure and evolution…rules that inhere in, and are 
obeyed by, every interacting part of the system.  Thus, what the system becomes is 
always consistent with what it already is(and vice versa); its causal integrity is 
tautologically preserved.  In the CTMU, these rules – the syntax of the language spoken 
to reality by reality itself - are understood to be largely mathematical in nature.
 
The theoretic vantage of the CTMU is essentially logical, with an accent on model theory. 
Its perspective is associated with the mathematical discipline governing the formulation 
and validation of theories, namely logic, with emphasis on the branch of logic which deals 
with the mapping of theories to their universes, namely model theory.  This elevates it to a 
higher level of discourse than ordinary scientific theories, which are simply compact 
mathematical descriptions of observational data, and even most mathematical theories, 
which are compact mathematical descriptions of mathematical objects, structures and 
processes.  This is reflected in the name of the theory; “CTMU” is just a way of saying 
“the metatheory that describes a model, or valid interpretation, of the theory of cognition, 
including logic and mathematics, in the real universe (and vice versa).”
 
Since the elementary categories of cognition and perception can be viewed as syntactic 
elements of cognitive and perceptual languages, cognition and perception can be model-
theoretically treated as mappings of cognitive and perceptual languages onto their 
respective dual universes, and can thus be related by the CTMU within a single monic 
universe.  Because mathematics and science are based on cognition and observation 
respectively, the CTMU is naturally equipped to address the foundations of both 
mathematics and science.  The CTMU reposes mathematics in reality, and reality in 
mathematics, in a way that puts each at the other’s immediate disposal for solving 
foundational problems and resolving paradoxes.
4.   The CTMU and Cosmology
A curious child often asks “why” questions, and when an answer is given, immediately 

asks another why question about the answer.  Such a child is unsatisfied with superficial 
explanations, craving instead an ultimate rationale for existence.  Example: “Why is grass 
green?” “Chlorophyll’s green.” “Why does grass have chlorophyll?” “Because it needs to 
photosynthesize.” “Why?” “Because it gets its energy from the sun.” “Why does the sun 
make energy?” “Because it’s a huge fusion reactor that takes energy from atoms.” “Why 
do atoms have energy?” “Because, as a man named Einstein showed, matter is energy.” 
“Why?” “Because that’s the way the universe is made.” “What’s the universe and who 
made it?” At this point, the weary adult has exhausted his scientific knowledge and must 
begin to deal with the most general and philosophically controversial abstractions in his 
mental vocabulary…or give up.
Stephen Hawking is among those who have proposed a way out of the regress.  In 
collaboration with James Hartle, he decided to answer the last question - what is the 
universe and who made it? - as follows.  “The universe made itself, and its structure is 
determined by its ability to do just that.”  This is contained in the No Boundary Proposal, 
which Hawking describes thusly: “This proposal incorporates the idea that the universe is 
completely self-contained, and that there is nothing outside the universe.  In a way, you 
could say that the boundary conditions of the universe are that there is no boundary.”  To 
mathematically support this thesis, Hawking infuses the quantum wavefunction of the 
universe with a set of geometries in which space and time are on a par.  The fact that 
time consists of a succession of individual moments thus becomes a consequence of 
spatial geometry, explaining the “arrow of time” by which time flows from past to future.
Unfortunately, despite the essential correctness of the “intrinsic cosmology” idea (to make 
the universe self-contained and self-explanatory), there are many logical problems with its 
execution.  These problems cannot be solved simply by choosing a convenient set of 
possible geometries (structurings of space); one must also explain where these geometric 
possibilities came from.  For his own part, Hawking explains them as possible solutions of 
the equations expressing the laws of physics.  But if this is to be counted a meaningful 
explanation, it must include an account of how the laws of physics originated…and there 
are further requirements as well.  They include the need to solve paradoxical physical 
conundrums like ex nihilo cosmogony (how something, namely the universe, can be 
created from nothing), quantum nonlocality (how subatomic particles can instantaneously 
communicate in order to preserve certain conserved physical quantities), accelerating 
cosmic expansion (how the universe can appear to expand when there is no external 
medium of expansion, and accelerate in the process to boot), and so on.  Even in the 
hands of experts, the conventional picture of reality is too narrow to meaningfully address 
these issues.  Yet it is too useful, and too accurate, to be “wrong”.  In light of the 
fundamentality of the problems just enumerated, this implies a need for additional logical 
structure, with the extended picture reducing to the current one as a limiting case.
The CTMU takes the reflexive self-containment relationship invoked by Hawking and 
some of his cosmological peers and predecessors and explores it in depth, yielding the 
logical structures of which it is built.  Together, these structures comprise an overall 
structure called SCSPL, acronymic for Self-Configuring Self-Processing Language.  The 
natural terminus of the cosmological self-containment imperative, SCSPL is a 
sophisticated mathematical entity that possesses logical priority over any geometric 
explanation of reality, and thus supersedes previous models as a fundamental explanation 

of the universe we inhabit.  In doing so, it relies on a formative principle essential to its 
nature, the Telic Principle.  A logical analogue of teleology, the Telic Principle replaces the 
usual run of ontological hypotheses, including quasi-tautological anthropic principles such 
as “we perceive this universe because this universe supports our existence,” as the basis 
of cosmogony.
5.   The CTMU and Modern Philosophy
20th century philosophy has been dominated by the linguistic and logico-analytic schools, 
whose joint program was to convert concepts and observations to language, and scientific 
and philosophical reasoning to the logical analysis of language.  As long as language was 
by assumption held apart from reality, this program had the inevitable and unfortunate 
effect of demoting philosophy to the status of an irrelevant word game.  In consequence, 
philosophers have lapsed into a pervasive cynicism that eats away at the foundations of 
human knowledge like a slow cancer, looking resignedly to scientific materialism to solve 
problems that science itself has frankly admitted are philosophical and empirically 
unsolvable.  Perceiving themselves as hopelessly beaten in the name of philosophy, the 
vast majority of modern philosophers have dejectedly become traitors to their cause.
The CTMU ignores philosophical defeatism and takes the obvious next step in the logico-
linguistic tradition, which is to subject the problematic relationship between language and 
reality itself to logical analysis.  When this is properly done, it turns out that reality and 
language are not so easily held apart.  In fact, it turns out that they can ultimately be 
treated as identical for purposes of philosophical and scientific reasoning...that they can 
be melded in a single self-similar medium, SCSPL, which is everywhere both real and 
linguistic, i.e., “monic” in CTMU dual-aspect sense (as distinguished from the separative 
dualism pervading the sciences).  This restores philosophy to a position from which it can 
effect the resolution of scientific paradox, which is just what the CTMU does in its capacity 
as reality theory.
More specifically, because it comprises a higher-level metalanguage of the language of 
physics, the CTMU is technically a theory of metaphysics in the formal sense (a 
metalanguage is a higher-level language in which one can talk about the structure of a 
lower-level language and its correspondence to whatever it describes). Thus, it has 
bearing on the traditional ingredients of metaphysics including cosmology (the study of 
the origin and composition of the universe), ontology (the study of being), and 
epistemology (the study of the nature and limits of knowledge).  The CTMU is thus closely 
related not only to cosmological science, but also to the burgeoning philosophies of mind 
and language.
6.   The CTMU and Theology
Basically, the Scientific Method says that science should be concerned with objective 
phenomena meeting at least two criteria: distinguishability, which means that they 
produce distinctive effects, and replicability, which means that they can be experimentally 
recreated and studied by multiple observers who compare their data and confirm each 
other’s findings.  Unfortunately, God nowhere fits into this scheme.  First, God is 
considered to be omnipresent even in monotheistic schemata, which means “distributed 

over reality as a whole” and therefore lacking any specific location at which to be 
“distinguished”.  Second, there is such a thing as being too replicable.  If something is 
distributed over reality, then it is present no matter where or when it is tested, and one 
cannot distinguish what is being “replicated”.  And then, of course, we have the “Creator” 
aspect of God; if God is indeed the Creator of reality, then He need not make His works 
replicable by mere scientists.  Thus, the God concept is unavoidably ambiguous in both 
spatial and temporal location, and no amount of scientific experimentation can overcome 
this logical difficulty.
In short, while the God concept may be amenable to empirical confirmation, e.g. through 
the discovery of vanishingly improbable leaps of biological evolution exceeding available 
genetic information, it is by definition resistant to scientific verification.  God, like 
consciousness, is a predicate whose extended logical structure, including a supporting 
conceptual framework, exceeds what science is presently equipped to analyze.  This, of 
course, means that arguments for or against God cannot be decided on empirical 
grounds, all but precluding a working relationship between the scientific and religious 
communities.  Even the sincerest attempts to foster dialogue between the two camps are 
obstructed by the unrealistic expectations of each regarding the ability of the other to 
meet it on its own ground; whereas the obvious first step towards meaningful 
communication is a basis for mutual understanding, no amount of encouragement or 
monetary incentive can provide it for those whose languages stubbornly resist translation. 
Since this describes the relationship between science and religion, the first step toward 
reconciliation must be to provide a logical bridge between their internal languages…a 
master language in which both languages are embedded.  The CTMU, conceived as the 
most general and comprehensive of logical languages, is designed to serve as that 
bridge.
It has been written that God is dead.  This might more accurately have been written about 
faith.  Mankind is creating an increasingly complex and mechanized world, and the 
essence of complexity and mechanism is not faith, but logic.  So the time for a logical 
approach to theology is nigh.   Accordingly, the CTMU does not begin with a 
preconceived definition of God; rather, it determines within the extended logical structure 
of reality how the properties most often attributed to God are logically related, reserving 
the title for the relationship actually educed.  In this way, it avoids the circular, faith-driven 
explanations to which religion so often resorts, and at which science so often revolts.  And 
meanwhile, by eliminating the barrier between subjective and objective reality, it permits 
recognition of the subjective dimension of the universe as a whole…the dimension by 
virtue of which the universe can truly be described as “the Mind of God.”
7.    The CTMU and Teleology
Historically, the Telic Principle can be understood as a logical analogue of teleology 
incorporating John Archibald Wheeler’s Observer Participation Thesis (approximately, 
“man participates in the ongoing quantum-scale creation of reality by observing it and 
thereby collapsing the wavefunction representing its potential”). More directly, the Telic 
Principle says that reality is a self-configuring entity that emerges from a background of 
unbound potential as a protean recursive construct with a single imperative: self-
actualization.  In other words, existence and its amplification is the tautological raison 

d’être of the cosmos. The phrase “raison d’être” has a literal significance; in order to exist, 
a self-contained universe must configure itself to recognize its own existence, and to 
configure itself in this way it must, by default, generate and parameterize its own self-
configuration and self-recognition functions.  This describes a situation in which the 
universe generates its own generalized utility: to self-configure, the universe must have a 
“self-actualization criterion” by which to select one of many possible structures or “futures” 
for itself, and this criterion is a generalized analogue of human utility…its raison d’être.
In addition to generalized utility and generalized volition (teleology), the universe also 
possesses generalized cognition (coherent self-recognition).  By any reasonable definition 
of the term “mental”, this makes the universe mental in a generalized sense, where 
“generalized” means that these attributes conform to general functional descriptions of 
what humans do in the process of volition, cognition and mentation.  The “coherent self-
recognition” feature of reality appears as an explicit feature of conspansive spacetime, a 
model-theoretic dual of the expanding cosmos.  Whereas the expanding cosmos is 
simplistically depicted in terms of a model called ERSU, short for Expanding Rubber-
Sheet Universe, conspansive spacetime is depicted by a model-theoretic dual of ERSU 
called USRE, short for the Universe as a Self-Representational Entity.  While ERSU is a 
product of Cartesian mind-matter dualism that effectively excludes mind in favor of matter, 
USRE, which portrays the universe as a “self-simulation”, is a form of dual-aspect monism 
according to which reality is distributively informational and cognitive in nature.
It is important to understand that the CTMU does not arbitrarily “project” human attributes 
onto the cosmos; it permits the logical deduction of necessary general attributes of reality, 
lets us identify any related human attributes derived from these general attributes, and 
allows us to explain the latter in terms of the former. CTMU cosmology is thus non-
anthropomorphic.  Rather, it uses an understanding of the cosmological medium of 
sentience to explain the mental attributes inherited by sentient organisms from the 
cosmos in which they have arisen.  Unlike mere anthropomorphic reasoning, this is a 
logically correct description of human characteristics in terms of the characteristics of the 
universe from which we derive our existence.
8.   The CTMU and Consciousness Studies
Modern science seems to offer many intriguing clues about the nature of consciousness, 
mainly involving the structure and dynamics of the brain.  But for all its success and 
sophistication, science is ill-equipped to turn these clues into an ultimate explanation of 
consciousness or any other subjective property.  Among the reasons are its empiricism, 
which means that it prefers observation to introspection; its objectivism, which means 
that it deliberately excludes “subjective contamination” from that which is observed; and 
its physical reductionism, which means that it tries to account for all scientific 
observations strictly in terms of matter and energy distributed in space and time. 
Unfortunately, this perspective is innately opposed to the study of anything that is 
subjective by nature, i.e., any property not directly observable except by the mind that 
embodies it.  In its zeal to eliminate “subjective contamination”, science unwittingly 
discards the baby with the bath water.
As an example of the scientific perspective, consider the Turing Test, a version of which 

says that the humanoid consciousness of a machine intelligence can only be tested by 
letting it hold an anonymous conversation with a real human.  If the human is fooled into 
thinking that he is conversing with another human, then the machine is taken to possess 
human consciousness.  But in fact, all that such a machine has demonstrated are certain 
objective effects usually associated with the property “consciousness”, but which may 
have been mindlessly produced by a set of logical instructions executed by inanimate 
hardware.  The only thing that knows whether the machine is truly conscious is the 
machine itself, and only if it is truly conscious at that.  For its own part, science lacks the 
conceptual apparatus to even make this distinction.  
Unfortunately, philosophy – the mother of science – appears to have abdicated its 
parental responsibility, failing to tell science how to enlarge its horizons to accommodate 
subjective phenomena.  The CTMU fills this breach, using advanced logic to expand our 
conceptual framework in a way that permits the meaningful analysis of subjective 
properties.  It does this by enlarging and deepening the relationship between the most 
fundamental ingredients of scientific analysis - ingredients like space, time, and object – 
and developing the implications in a mathematical and scientific context.  The 
explanations that emerge not only possess an unprecedented degree of logical structure, 
but are automatically integrated with science and compatible with all of its best insights.  
9.   The CTMU and Intelligent Design 
Among the most exciting recent developments in science are complexity theory, the 
theory of self-organizing systems, and the modern incarnation of Intelligent Design 
Theory, which investigates the deep relationship between self-organization and 
evolutionary biology in a scientific context not preemptively closed to theological 
causation.  Bucking the traditional physical reductionism of the hard sciences, complexity 
theory has given rise to a new trend, informational reductionism, which holds that the 
basis of reality is not matter and energy, but information.  Unfortunately, this new form of 
reductionism is as problematic as the old one.  As mathematician David Berlinski writes 
regarding the material and informational aspects of DNA: “We quite know what DNA is: it 
is a macromolecule and so a material object. We quite know what it achieves: apparently 
everything. Are the two sides of this equation in balance?” More generally, Berlinski 
observes that since the information embodied in a string of DNA or protein cannot affect 
the material dynamic of reality without being read by a material transducer, information is 
meaningless without matter.  
The relationship between physical and informational reductionism is a telling one, for it 
directly mirrors Cartesian mind-matter dualism, the source of several centuries of 
philosophical and scientific controversy regarding the nature of deep reality.  As long as 
matter and information remain separate, with specialists treating one as primary while 
tacitly relegating the other to secondary status, dualism remains in effect.  To this extent, 
history is merely repeating itself; where mind and matter once vied with each other for 
primary status, concrete matter now vies with abstract information abstractly 
representing matter and its extended relationships.  But while the abstractness of 
information seems to make it a worthy compromise between mind and matter, Berlinski’s 
comment demonstrates its inadequacy as a conceptual substitute.  What is now required 
is thus what has been required all along: a conceptual framework in which the 

relationship between mind and matter, cognition and information, is made explicit.  This 
framework must not only permit the completion of the gradual ongoing dissolution of the 
Cartesian mind-matter divider, but the construction of a footworthy logical bridge across 
the resulting explanatory gap. 
Mathematically, the theoretical framework of Intelligent Design consists of certain 
definitive principles governing the application of complexity and probability to the 
analysis of two key attributes of evolutionary phenomena, irreducible complexity and 
specified complexity.  On one hand, because the mathematics of probability must be 
causally interpreted to be scientifically meaningful, and because probabilities are 
therefore expressly relativized to specific causal scenarios, it is difficult to assign definite 
probabilities to evolutionary states in any model not supporting the detailed 
reconstruction and analysis of specific causal pathways.  On the other hand, positing the 
“absolute improbability” of an evolutionary state ultimately entails the specification of an 
absolute (intrinsic global) model with respect to which absolute probabilistic deviations 
can be determined.  A little reflection suffices to inform us of some of its properties: it 
must be rationally derivable from a priori principles and essentially tautological in nature, 
it must on some level identify matter and information, and it must eliminate the 
explanatory gap between the mental and physical aspects of reality.  Furthermore, in 
keeping with the name of that to be modeled, it must meaningfully incorporate the 
intelligence and design concepts, describing the universe as an intelligently self-
designed, self-organizing system. 
The CTMU is exactly the model required.  It describes reality as a Self-Configuring Self-
Processing Language, an ultimate kind of self-organizing, self-emergent system 
consisting of "infocognitive" syntactic operators at once material and informational in 
nature.  It is a rationally-bootstrapped model-theoretic extension of logic that shares its 
tautological structure, approximating an answer for Berlinski's question what brings a 
universe into being?  Owing to conspansion, a distributed evolutionary process by virtue 
of which the universe can be described as a Universal Self-Representational Entity, it 
unites mind and matter in a self-configuring, self-processing system endowed with a 
species of coherence substantiating its possession of higher-order sentience.  And last 
but not least, it puts everything under the control of a logical analogue of teleology called 
the Telic Principle, conspansively adjoining to the laws of physics an "extended 
superposition principle" that provides irreducible and specified complexity with a 
mechanism they would otherwise have continued to lack.    
10.   Summary: The Bottom Line 
To summarize, the CTMU is a theory of reality-as-mind, in principle spanning all of 
science while permitting a logical analysis of consciousness and other subjective 
predicates (this does not mean that it has “solved all of the problems” of science and the 
philosophy of mind, but only that it has laid the preliminary groundwork).  It provides the 
logical framework of a TOE, yielding an enhanced model of spacetime affording 
preliminary explanations of cosmogony, accelerating cosmic expansion, quantum 
nonlocality, the arrow of time, and other physical and cosmological riddles that cannot be 
satisfactorily explained by other means.  The CTMU penetrates the foundations of 
mathematics, describing the syntactic relationships among various problematic 

mathematical concepts in a reality-theoretic context.  It is the culmination of the modern 
logico-linguistic philosophical tradition, reuniting the estranged couple consisting of 
(rationalistic) philosophy and (empirical) science.  It provides an indispensable logical 
setting for Intelligent Design.  And perhaps most importantly, the CTMU enables a logical 
description of God and an unprecedented logical treatment of theology, comprising a 
metaphysical framework in which to unite people of differing faiths and resolve religious 
conflicts.
On Absolute Truth and Knowledge
First, a word on the title of this essay.  Absolute knowledge is absolutely true, and 
absolute truth is the definitive predicate of absolute knowledge.  That is, if something is 
known with absolute certainty, then it can be subjected to tests affirming its truth, while if 
something can be affirmatively tested for truth, then it is known with certainty by the 
tester.  This applies whether the tests in question are perceptual or inferential.  Where 
knowledge can denote either direct embodiment or internal modeling by an arbitrary 
system, and test denotes a straightforward systemic efficacy criterion, knower and tester 
can refer to reality at large.  In this generalization, truth and knowledge are identical. 
While it is possible to split, splice and braid countless philosophical hairs over the 
connotations respectively attached to truth and knowledge, this simple generalized 
relationship conveniently spares us the necessity.  It is with this express understanding 
that these terms and phrases are employed herein.
To perceive one and the same reality, human beings need a kind of "absolute knowledge" 
wired into their minds and nervous systems.  The structure and physiology of their brains, 
nerves and sense organs provide them, at least in part, with elementary cognitive and 
perceptual categories and relationships in terms of which to apprehend the world.  This 
"absolute" kind of knowledge is what compels the perceptions and logical inferences of 
any number of percipients to be mutually consistent, and to remain consistent over time 
and space.  Without the absoluteness of such knowledge - without its universality and 
invariance - we could not share a common reality; our minds and senses would lie and 
bicker without respite, precipitating us into mental and sensory chaos.  Time and space, 
mind and matter, would melt back into the haze of undifferentiated potential from which 
the universe is born. 
Given the fact that absolute knowledge is a requisite of our collective ability to sustain a 
perceptually consistent universe, it is nothing short of astonishing that there are people 
who react with incredulity or derision at any mention of its possible existence.  Their 
attitude seems to be that the very idea smacks of "hubris", being nothing but an empty 
pretense exceeding the capacity of the small and overly-challenged human mind.  The 
truth, however, is that hubris is nowhere more evident than among those holding irrational 
opinions in contempt of logic, and denying the existence of absolute knowledge is a case 
in point.  In fact, the entire history of philosophy and science can be characterized as an 
undying quest for absolute knowledge...a timeless attempt to comprehensively extend the 
a priori and analytical into the realm of the apparently a posteriori and synthetic.  This 
quest includes the efforts of researchers from many fields, from physics and cosmology to 

philosophy and computer science.  
The Holy Grail of this quest is known as the TOE, or Theory of Everything.  A TOE 
purports to be absolute truth by an implicit reductio ad absurdum: if it does not constitute 
absolute truth, then its truth can be relativized to a partial context within reality at large, in 
which case it is not a theory of everything.  Thus, if a TOE exists, it falls squarely under 
the heading of absolute knowledge.  But unfortunately, the proper method for constructing 
such a theory has not been entirely obvious, particularly to theorists steeped in the 
ambiguities and paradoxes of four centuries of post-Cartesian science and philosophy. 
As science has advanced and philosophy has wearily tried to keep pace, their once-
stentorian claims of absolute truth have been all but extinguished, and  the mainstream 
search for a TOE has lately been pursued without a clear understanding of what is being 
sought. 
The apparent absence of a TOE notwithstanding, has any kind of absolute knowledge 
ever been scientifically formulated?  Yes, in the form of logical tautologies.  A tautology is 
a sentential relation, i.e. a formula consisting of variables and logical connectives, with the 
property that it is true for all possible assignments of Boolean truth values (true or false) 
to its variables.  For example, the statement "if x is a sentence, then either x or not-x (but 
not both) must be true" is a tautology because no matter which truth values are 
consistently applied to x and not-x, the statement is unequivocally true.  Indeed, 
tautologies comprise the axioms and theorems of 2-valued logic itself, and because all 
meaningful theories necessarily conform to 2-valued logic, define the truth concept for all 
of the sciences.  From mathematics and physics to biology and psychology, logical 
tautologies reign supreme and inviolable.   
That a tautology constitutes absolute truth can be proven as follows.  First, logic is 
absolute within any system for which (a) the complementary truth values T (true) and F 
(false) correspond to systemic inclusion and exclusion, a semantic necessity without 
which meaningful reference is impossible; and (b) lesser predicates and their 
complements equal subsystemic inclusion and exclusion.  Because a tautology is an 
axiom of 2-valued logic, violating it disrupts the T/F distinction and results in the 
corruption of informational boundaries between perceptual and cognitive predicates 
recognized or applied in the system, as well as between each predicate and its negation. 
Thus, the observable fact that perceptual boundaries are intact across reality at large 
implies that no tautology within its syntax, or set of structural and functional rules, has 
been violated; indeed, if such a tautology ever were violated, then reality would 
disintegrate due to corruption of the informational boundaries which define it.  So a 
tautology is "absolute truth" not only with respect to logic, but with respect to reality at 
large.   
What does this mean?  Uncertainty or non-absoluteness of truth value always involves 
some kind of confusion or ambiguity regarding the distinction between the sentential 
predicates true and false. Where these predicates are applied to a more specific 
predicate and its negation - e.g., "it is true that the earth is round and false that the earth 
is not-round" - the confusion devolves to the contextual distinction between these lesser 
predicates, in this case round and not-round within the context of the earth.  Because all 
of the ambiguity can be localized to a specific distinction in a particular context, it presents 

no general problem for reality at large; we can be uncertain about whether or not the 
earth is round without disrupting the logic of reality in general.  However, where a 
statement is directly about reality in general, any disruption of or ambiguity regarding the 
T/F distinction disrupts the distinction between reality and not-reality. Were such a 
disruption to occur at the level of basic cognition or perception, reality would become 
impossible to perceive, recognize, or acknowledge as something that "exists".  
By definition, this is the case with regard to our cognitive-perceptual syntax, the set of 
structural and inferential rules governing perception and cognition in general.  Since a 
tautology is a necessary and universal element of this syntax, tautologies can under no 
circumstances be violated within reality. Thus, they are "absolute knowledge".  We may 
not be able to specify every element of absolute knowledge, but we can be sure of two 
things about it: that it exists in reality to the full extent necessary to guarantee its non-
violation, and that no part of it yet to be determined can violate absolute knowledge 
already in hand.  Whether or not we can write up an exhaustive itemized list of absolute 
truths, we can be sure that such a list exists, and that its contents are sufficiently 
"recognizable" by reality at large to ensure their functionality.  Absolute truth, being 
essential to the integrity of reality, must exist on the level of reference associated with the 
preservation of global consistency, and may thus be duly incorporated in a theory of 
reality. 
On the other hand, the fact that any reasonable definition of "absolute truth" amounts to 
tautology can be shown by reversing this reasoning.  Since absolute truth must be 
universal, it is always true regardless of the truth values of its variables (where the 
variables actually represent objects and systems for which specific state-descriptions vary 
in space and time with respect to truth value).  Moreover, it falls within its own scope and 
is thus self-referential.  By virtue of its universality and self-reference, it is a universal 
element of reality syntax, the set of structural and functional rules governing the spatial 
structure and temporal evolution of reality. As such, it must be unfalsifiable, any 
supposition of its falsehood leading directly to a reductio ad absurdum.  And to ice the 
cake, it is unavoidably implicated in its own justification; were it ever to be violated, the T/
F boundary would be disrupted, and this would prevent it (or anything else) from being 
proven.  Therefore, it is an active constraint in its own proof, and thus possesses all the 
characteristics of a tautology. 
To recap, the characteristic attributes of a logical tautology are as follows: (1) it cannot be 
disobeyed, which implies that it has universal scope and thus accepts and truthfully 
predicates all closed sentential (predicative) structures, including itself and logic in its 
entirety, under assignment to its own variables; and (2) it is self-affirming or self-justifying 
and figures in its own definition or demonstration within the associated grammar. 
Obviously, (1) and (2) are not independent; (1) implies that a tautology is a universal, self-
similar, metalogical element of syntax of the language and metalanguages of which it is a 
part, while (2) says that it is a critical element of syntax that cannot be eliminated without 
compromising the integrity of the syntax as a whole (thus, any supposition that it is false 
or eliminable reduces itself to absurdity by syntactic rules of inference, forcing the syntax 
to “protect itself” through reductio ad absurdum).  Since any reasonable syntactic and/or 
semantic definition of absolute truth bestows upon it the properties of necessity and 
truthwise invariance with respect to content, it is unquestionably tautological in nature.   

Accordingly, it is desirable to formulate reality theory as a tautology.  To whatever extent 
this can be done, the theory constitutes "absolute knowledge" and is therefore eligible as 
a TOE. This suffices to show that if the form of absolute knowledge hopefully referred to 
as a TOE exists, it must be tautological.  Next we will show that a TOE and its universe 
can be related in such a way that the theory is semantically tautological with respect to its 
universe, i.e. that (a) the theory is intrinsically tautological, and (b) its tautological 
structure is modeled by its universe.  And in the course of doing so, we will show that it is 
indeed possible to ensure by the method of constructing this theory that its universe 
coincides with reality at large, and thus that it constitutes a valid theory of reality. 
Specifically, the construction will incorporate one or more attributes that are necessarily 
modeled by reality at large, and that simultaneously ensure the theory's tautological 
structure. 
How can a TOE, or comprehensive theory of reality, be structured as a tautology?  First, 
by definition, a TOE is universal; this is implied by the E, which stands for Everything. 
Thus, it is comprehensive.  Second, it is self-referential; a theory of everything, being a 
part of the "everything" to which it refers, must refer to itself.  More precisely, a TOE must 
be totally recursive in a manner analogous to logic, each atom referring exclusively to 
other parts of the theory, and be able to refer to itself in part and in whole in order to 
possess full logical closure. This can be arranged by incorporating one or more self-
representative variables and their definitive relationships, up to and including a dynamic 
variable representing the theory as a whole (in fact, the theory can incorporate a “hology” 
predicate that goes considerably farther; instead of merely containing itself as a variable, 
a theory equipped with such a predicate can everywhere contain itself by virtue of self-
similarity or self-distribution).  Because it represents a theory of perceptual reality, this 
variable contains all elements of cognitive syntax and their perceptual contents; since 
variables can be defined in general terms without specifically enumerating their contents, 
we do not need to know exactly what it contains in order to use it. And third, because 
logic is the primary ingredient of cognitive-perceptual syntax, the self-referential TOE 
refers to logic in part and in whole and is therefore metalogical.  Thus, it can incorporate a 
kind of ultimate truth predicate that asserts its own tautological structure and guarantees 
that no matter what (semantic and other) kinds of paradox may arise within the theory, 
they can always be resolved within the theory.  A theory possessing all three of these 
properties is called a supertautology, denoting the reality-theoretic counterpart of a logical 
tautology.  
Let us now attend to some of the details of constructing a supertautology.  First, we 
repose the a priori and analytic knowledge that we are given in the form of cognitive 
syntax, including logic and all of its implications, in a variable to which we apply (a) the 
rules of logic itself; (b) three recursively-related metalogical axioms that are themselves 
true a priori and analytically implied by each other (in a word, self-evident).  Note again 
that in creating and assigning content to this variable, we do not have to enumerate all of 
its contents; we can refer to them en masse by their joint characteristic, namely the 
"absoluteness" necessary to ensure perceptual and inferential consistency.  Since a 
theory falls under the mathematical definition of a language, it is natural to refer to the 
contents in question as the "rules of syntax" of that language, or simply as its syntax; 
thus, the TOE recursively contains a variable representing its own syntax, permitting the 

manipulation of that variable and the grammatical extraction of its implications according 
to syntactic rules.  This recursive construction makes the "absoluteness" of the variable 
(and theory) logically heritable, conferring absoluteness on whatever is inferred within the 
system.  Together, the “implicate” variable and its “explicate” theoretic medium comprise 
a bootstrapped extension of the self-referential syntax of logic itself, letting that syntax be 
“mined” for a potential wealth of hidden analytic content. 
The key to applying this knowledge scientifically is the semantic functionality of the three 
metalogical axioms adjoining the object-level syntax.  Conveniently, these (recursively 
related) axioms can be thought of in terms of a trio of property-principle pairs, the "Three 
Cs" and the "Three Ms".  The Cs are three properties that a TOE must inevitably possess, 
namely Comprehensiveness, Closure and Consistency, while the Ms are metalogical 
axioms respectively associated with those properties.  These principles are the Mind 
Equals Reality Principle(associated with comprehensiveness), the Metaphysical Autology 
Principle (associated with closure), and the Multiplex Unity Principle (associated with 
consistency), respectively abbreviated M=R, MAP and MU.  We have already been 
partially introduced to these principles in all but name, and in any case need only one of 
them to proceed farther: M=R.  Concisely, M=R asserts that there exists a semantic 
(language-to-universe) correspondence between objective reality and the absolute 
subjective rules of perception and inference, i.e. cognitive and perceptual syntax.  This 
correspondence defines a morphism, incoversion, predicating the assignment of a certain 
structural predicate, hology, to the universe-language/metalanguage system (see 
Introduction to the CTMU). 
Hology, a special kind of self-similarity conferring supertautological status, equals the 
relationship of the TOE and its universe to the self-representative variable by which it is 
encapsulated.  Hology means that the syntax by which reality configures, recognizes and 
processes itself is the image of a distributed endomorphism, the incoversion morphism, 
surjecting the objective self-intersect (distributed component) of reality onto every interior 
point and region of reality as transductive syntactic potential, i.e. as general rules of 
transduction to be variously expressed by objects at any location.  Although real objects 
generally access and express only a small part of this syntax, combinations of interacting 
objects may express and access more of it by mutual input-to-output behavioral 
transduction; through this kind of behavioral transduction, the self-intersect, though 
generally composed of distributed rules applying everywhere in reality, resolves to a 
Distributed Conditional Form (DCF) explicitly containing all of the local systems and 
states generated by those rules.  The self-intersect and its DCF resolution comprise the 
syntax and language of reality.  Because hology maps the reality syntax to our cognitive 
syntax - because the self-intersect plays dual objective and subjective roles - perceptible 
objects and processes tautologically conform to our innate perceptual categories, making 
the TOE a supertautology comprising the purest and most universal kind of absolute truth. 
The above reasoning subjects the absolute (a priori) knowledge in our minds to a kind of 
recursive "squaring" operation, causing it to self-explicate as its own medium and 
projecting it onto external reality.  This repetitive operation resembles the mutual 
reflection of a pair of polymorphic mirrors, one labeled mind and the other labeled reality, 
that faithfully reflect each other's evolving image.  Although one might suspect that the 
tautological nature of the construction renders it barren of interest, this would be akin to 

saying that a squaring operation never yields more than the original number.  While that 
might be true for a featureless numeric identity (e.g. 12 = 1), the cognitive syntax of the 
human mind is far from "featureless".  In recursive self-combination, it is capable of 
generating a universe, and a theory constructed according to this recursive relationship is 
capable of veridically capturing that universe.  Indeed, there is a sense in which the TOE, 
and all of the absolute knowledge it holds, is identical to the universe it describes.  But the 
meaning of this statement - and it is a statement that is pregnant with meaning - lies 
beyond the purpose at hand. 
The CTMU is a theory of reality, or TOE, that has been constructed according to this 
blueprint. If, as a rationalist, one insists that absolute truth and knowledge are exclusively 
mathematical, then the CTMU is mathematics; if, as an empiricist, one insists that they 
reside exclusively in our direct perceptions of reality, then the CTMU is embodied in our 
direct perceptions of reality (including our direct perceptions of the comprehensiveness, 
closure and consistency of reality).  The truth, of course, is that by the method of its 
construction, it is both.  But in any case, would-be pundits who cling blindly to folk-
epistemological "absolutes" like truth is never more than provisional, science is inherently 
without stability and there are no such things as absolute truth and knowledge are 
urgently in need of an intellectual awakening, and until it comes, should refrain from 
disseminating their irrational opinions to others who might gullibly mistake them for fact. 
Such truisms have their contexts, but these contexts do not include the highest levels of 
discourse regarding truth and knowledge, and they do not include the CTMU.
There is, of course, more to the CTMU than just its supertautological structure.  For 
example, it incorporates a new conceptualization of spacetime, resolves numerous high 
level reality-theoretic paradoxes, and establishes a bridge between science and theology, 
all with considerably more detail than this brief monograph allows.  But as regards 
"absolute truth and knowledge", its status as a supertautology is necessary and sufficient 
to explain why it is uniquely qualified for the title.  If the simplicity and elegance of its 
design seems "too obvious", "too convenient" or "too good to be true", this is certainly no 
fault of the theory or its author; at best, it testifies to the opacity of certain formerly useful 
but outworn conceptual barriers erected in science and philosophy over the last several 
centuries, and to the inertia of the scientific and academic establishments which tend 
them. 
One final note.  The CTMU is neither intended nor presented as an encyclopedic 
compendium of absolute truth.  It is meant only to provide a comprehensive, consistent 
and self-contained (and to that extent "absolute") logical framework for bridging the gaps 
between apparently unrelated fields of knowledge, helping to locate and correct 
fundamental inconsistencies within and among these fields, and developing new kinds of 
knowledge that might arise from the intersections of fields which already exist.  Because 
the real universe is everywhere in the process of self-creation, human knowledge can and 
must continue to grow.  The CTMU is intended not as a brittle, undersized pot that will 
root-bind and choke this growing knowledge, but as fertile and well-aerated soil through 
which it can spread.  By its very design, the CTMU will continue to accommodate and 
accelerate our intellectual progress...and since there is no other theory that fully shares its 
design, it is irreplaceable for that purpose.  

This completes our introduction to the topic of absolute truth and knowledge. 
Introduction to the CTMU
The real universe has always been theoretically treated as an object, and specifically as 
the composite type of object known as a set. But an object or set exists in space and time, 
and reality does not. Because the real universe by definition contains all that is real, there 
is no "external reality" (or space, or time) in which it can exist or have been "created". We 
can talk about lesser regions of the real universe in such a light, but not about the real 
universe as a whole. Nor, for identical reasons, can we think of the universe as the sum of 
its parts, for these parts exist solely within a spacetime manifold identified with the whole 
and cannot explain the manifold itself. This rules out pluralistic explanations of reality, 
forcing us to seek an explanation at once monic (because nonpluralistic) and holistic 
(because the basic conditions for existence are embodied in the manifold, which equals the 
whole). Obviously, the first step towards such an explanation is to bring monism and holism 
into coincidence.
When theorizing about an all-inclusive reality, the first and most important principle is 
containment, which simply tells us what we should and should not be considering. 
Containment principles, already well known in cosmology, generally take the form of 
tautologies; e.g., "The physical universe contains all and only that which is physical." The 
predicate "physical", like all predicates, here corresponds to a structured set, "the physical 
universe" (because the universe has structure and contains objects, it is a structured set). 
But this usage of tautology is somewhat loose, for it technically amounts to a predicate-
logical equivalent of propositional tautology called autology, meaning self-description. 
Specifically, the predicate physical is being defined on topological containment in the 
physical universe, which is tacitly defined on and descriptively contained in the predicate 
physical, so that the self-definition of "physical" is a two-step operation involving both 
topological and descriptive containment. While this principle, which we might regard as a 
statement of "physicalism", is often confused with materialism on the grounds that 
"physical" equals "material", the material may in fact be only a part of what makes up the 
physical. Similarly, the physical may only be a part of what makes up the real. Because the 
content of reality is a matter of science as opposed to mere semantics, this issue can be 
resolved only by rational or empirical evidence, not by assumption alone.
Can a containment principle for the real universe be formulated by analogy with that just 
given for the physical universe? Let's try it: "The real universe contains all and only that 
which is real." Again, we have a tautology, or more accurately an autology, which defines 
the real on inclusion in the real universe, which is itself defined on the predicate real. This 
reflects semantic duality, a logical equation of predication and inclusion whereby perceiving 
or semantically predicating an attribute of an object amounts to perceiving or predicating 
the object's topological inclusion in the set or space dualistically corresponding to the 
predicate. According to semantic duality, the predication of the attribute real on the real 
universe from within the real universe makes reality a self-defining predicate, which is 
analogous to a self-including set. An all-inclusive set, which is by definition self-inclusive as 

well, is called "the set of all sets".  Because it is all-descriptive as well as self-descriptive, 
the reality predicate corresponds to the set of all sets. And because the self-definition of 
reality involves both descriptive and topological containment, it is a two-stage hybrid of 
universal autology and the set of all sets.
Now for a brief word on sets.  Mathematicians view set theory as fundamental.  Anything 
can be considered an object, even a space or a process, and wherever there are objects, 
there is a set to contain them.  This "something" may be a relation, a space or an algebraic 
system, but it is also a set; its relational, spatial or algebraic structure simply makes it a 
structured set.  So mathematicians view sets, broadly including null, singleton, finite and 
infinite sets, as fundamental objects basic to meaningful descriptions of reality.  It follows 
that reality itself should be a set…in fact, the largest set of all.  But every set, even the 
largest one, has a powerset which contains it, and that which contains it must be larger (a 
contradiction). The obvious solution: define an extension of set theory incorporating two 
senses of “containment” which work together in such a way that the largest set can be 
defined as "containing" its powerset in one sense while being contained by its powerset in 
the other. Thus, it topologically includes itself in the act of descriptively including itself in the 
act of topologically including itself..., and so on, in the course of which it obviously becomes 
more than just a set.
In the Cognitive-Theoretic Model of the Universe or CTMU, the set of all sets, and the real 
universe to which it corresponds, take the name (SCSPL) of the required extension of set 
theory. SCSPL, which stands for Self-Configuring Self-Processing Language, is just a 
totally intrinsic, i.e. completely self-contained, language that is comprehensively and 
coherently (self-distributively) self-descriptive, and can thus be model-theoretically 
identified as its own universe or referent domain. Theory and object go by the same name 
because unlike conventional ZF or NBG set theory, SCSPL hologically infuses sets and 
their elements with the distributed (syntactic, metalogical) component of the theoretical 
framework containing and governing them, namely SCSPL syntax itself, replacing ordinary 
set-theoretic objects with SCSPL syntactic operators.  The CTMU is so-named because 
the SCSPL universe, like the set of all sets, distributively embodies the logical syntax of its 
own descriptive mathematical language.  It is thus not only self-descriptive in nature; where 
logic denotes the rules of cognition (reasoning, inference), it is self-cognitive as well.  (The 
terms "SCSPL" and "hology" are explained further below; to skip immediately to the 
explanations, just click on the above links.)
An act is a temporal process, and self-inclusion is a spatial relation.  The act of self-
inclusion is thus "where time becomes space"; for the set of all sets, there can be no more 
fundamental process.  No matter what else happens in the evolving universe, it must be 
temporally embedded in this dualistic self-inclusion operation.  In the CTMU, the self-
inclusion process is known as conspansion and occurs at the distributed, Lorentz-invariant 
conspansion rate c, a time-space conversion factor already familiar as the speed of light in 
vacuo (conspansion consists of two alternative phases accounting for the wave and 
particle properties of matter and affording a logical explanation for accelerating cosmic 
expansion).  When we imagine a dynamic self-including set, we think of a set growing 
larger and larger in order to engulf itself from without. But since there is no "without" 

relative to the real universe, external growth or reference is not an option; there can be no 
external set or external descriptor. Instead, self-inclusion and self-description must occur 
inwardly as the universe stratifies into a temporal sequence of states, each state 
topologically and computationally contained in the one preceding it (where the 
conventionally limited term computation is understood to refer to a more powerful SCSPL-
based concept, protocomputation, involving spatiotemporal parallelism). On the present 
level of discourse, this inward self-inclusion is the conspansive basis of what we call 
spacetime.
Every object in spacetime includes the entirety of spacetime as a state-transition syntax 
according to which its next state is created. This guarantees the mutual consistency of 
states and the overall unity of the dynamic entity the real universe. And because the sole 
real interpretation of the set-theoretic entity "the set of all sets" is the entire real universe, 
the associated foundational paradoxes are resolved in kind (by attributing mathematical 
structure like that of the universe to the pure, uninterpreted set-theoretic version of the set 
of all sets). Concisely, resolving the set-of-all-sets paradox requires that (1) an 
endomorphism or self-similarity mapping D:S-->rÎS be defined for the set of all sets S and 
its internal points r; (2) there exist two complementary senses of inclusion, one topological 
[S Ét D(S)] and one predicative [D(S) Éd S], that allow the set to descriptively "include itself" 
from within, i.e. from a state of topological self-inclusion (where Ét denotes topological or 
set-theoretic inclusion and Éd denotes descriptive inclusion, e.g. the inclusion in a language 
of its referents); and (3) the input S of D be global and structural, while the output D(S) = (r 
Éd S) be internal to S and play a syntactic role. In short, the set-theoretic and cosmological 
embodiments of the self-inclusion paradox are resolved by properly relating the self-
inclusive object to the descriptive syntax in terms of which it is necessarily expressed, thus 
effecting true self-containment: "the universe (set of all sets) is that which topologically 
contains that which descriptively contains the universe (set of all sets)."
This characterizes a system that consistently perceives itself and develops its own 
structure from within via hology, a 2-stage form of self-similarity roughly analogous to 
holography. (Hology is a logico-cybernetic form of self-similarity in which the global 
structure of a self-contained, self-interactive system doubles as its distributed self-
transductive syntax; it is justified by the obvious fact that in a self-contained system, no 
other structure is available for that purpose.) The associated conspansive mapping D is 
called incoversion in the spatiotemporally inward direction and coinversion in the reverse 
(outward, D-1) direction. Incoversion carries global structure inward as state-recognition and 
state-transformation syntax, while coinversion projects syntactic structure outward in such 
a way as to recognize existing structure and determine future states in conformance with it. 
Incoversion is associated with an operation called requantization, while coinversion is 
associated with a complementary operation called inner expansion. The alternation of 
these operations, often referred to as wave-particle duality, comprises the conspansion 
process. The Principle of Conspansive Duality then says that what appears as cosmic 
expansion from an interior (local) viewpoint appears as material and temporal contraction 
from a global viewpoint. Because metric concepts like "size" and "duration" are undefined 
with respect to the universe as a whole, the spacetime metric is defined strictly intrinsically, 
and the usual limit of cosmological regress, a pointlike cosmic singularity, becomes the 
closed spacetime algebra already identified as SCSPL. 

Thus, the real universe is not a static set, but a dynamic process resolving the self-
inclusion paradox. Equivalently, because any real explanation of reality is contained in 
reality itself, reality gives rise to a paradox unless regarded as an inclusory self-mapping. 
This is why, for example, category theory is increasingly preferred to set theory as a means 
of addressing the foundations of mathematics; it centers on invariant relations or mappings 
between covariant or contravariant (dually related) objects rather than on static objects 
themselves. For similar reasons, a focus on the relative invariants of semantic processes is 
also well-suited to the formulation of evolving theories in which the definitions of objects 
and sets are subject to change; thus, we can speak of time and space as equivalent to 
cognition and information with respect to the invariant semantic relation processes, as in 
"time processes space" and "cognition processes information". But when we define reality 
as a process, we must reformulate containment accordingly. Concisely, reality theory 
becomes a study of SCSPL autology naturally formulated in terms of mappings. This is 
done by adjoining to logic certain metalogical principles, formulated in terms of mappings, 
that enable reality to be described as an autological (self-descriptive, self-recognizing/self-
processing) system.
The first such principle is MAP, acronymic for Metaphysical Autology Principle. Let S be 
the real universe, and let T = T(S) be its theoretical description or "TOE". MAP, designed to 
endow T and S with mathematical closure, simply states that T and S are closed with 
respect to all internally relevant operations, including recognition and description. In terms 
of mappings, this means that all inclusional or descriptive mappings of S are 
automorphisms (e.g., permutations or foldings) or endomorphisms (self-injections). MAP is 
implied by the unlimited scope, up to perceptual relevance, of the universal quantifier 
implicitly attached to reality by the containment principle. With closure thereby established, 
we can apply techniques of logical reduction to S without worrying about whether the lack 
of some external necessity will spoil the reduction. In effect, MAP makes T(S) "exclusive 
enough" to describe S by excluding as a descriptor of S anything not in S. But there still 
remains the necessity of providing S with a mechanism of self-description.
This mechanism is provided by another metalogical principle, the M=R or Mind Equals 
Reality Principle, that identifies S with the extended cognitive syntax D(S) of the theorist. 
This syntax (system of cognitive rules) not only determines the theorist's perception of the 
universe, but bounds his cognitive processes and is ultimately the limit of his theorization 
(this relates to the observation that all we can directly know of reality are our perceptions of 
it). The reasoning is simple; S determines the composition and behavior of objects (or 
subsystems) s in S, and thus comprises the general syntax (structural and functional rules 
of S) of which s obeys a specific restriction. Thus, where s is an ideal observer/theorist in 
S, S is the syntax of its own observation and explanation by s. This is directly analogous to 
"the real universe contains all and only that which is real", but differently stated: "S contains 
all and only objects s whose extended syntax is isomorphic to S." M=R identifies S with the 
veridical limit of any partial theory T of S [limT(S) = D(S)], thus making S "inclusive 
enough" to describe itself. That is, nothing relevant to S is excluded from S @D(S).

Mathematically, the M=R Principle is expressed as follows. The universe obviously has a 
structure S. According to the logic outlined above, this structure is self-similar; S distributes 
over S, where "distributes over S" means "exists without constraint on location or scale 
within S". In other words, the universe is a perfectly self-similar system whose overall 
structure is replicated everywhere within it as a general state-recognition and state-
transition syntax (as understood in an extended computational sense). The self-distribution 
of S, called hology, follows from the containment principle, i.e. the tautological fact that 
everything within the real universe must be described by the predicate "real" and thus fall 
within the constraints of global structure. That this structure is completely self-distributed 
implies that it is locally indistinguishable for subsystems s; it could only be discerned 
against its absence, and it is nowhere absent in S. Spacetime is thus transparent from 
within, its syntactic structure invisible to its contents on the classical (macroscopic) level. 
Localized systems generally express and utilize only a part of this syntax on any given 
scale, as determined by their specific structures. I.e., where there exists a hological 
incoversion endomorphism D:Sà{rÎS} carrying the whole structure of S into every internal 
point and region of S, objects (quantum-geometrodynamically) embedded in S take their 
recognition and state-transformation syntaxes directly from the ambient spatiotemporal 
background up to isomorphism. Objects thus utilize only those aspects of D(S) of which 
they are structural and functional representations. 
The inverse D-1 of this map (coinversion) describes how an arbitrary local system s within S 
recognizes S at the object level and obeys the appropriate "laws", ultimately giving rise to 
human perception. This reflects the fact that S is a self-perceptual system, with various 
levels of self-perception emerging within interactive subsystems s (where perception is just 
a refined form of interaction based on recognition in an extended computational sense). 
Thus, with respect to any class {s} of subsystems of S, we can define a homomorphic 
submap d of the endomorphism D: d:Sà{s} expressing only that part of D to which {s} is 
isomorphic. In general, the si are coherent or physically self-interactive systems exhibiting 
dynamical and informational closure; they have sometimes-inaccessible internal structures 
and dynamics (particularly on the quantum scale), and are distinguishable from each other 
by means of informational boundaries contained in syntax and comprising a "spacetime 
metric". 
According to the above definitions, the global self-perceptor S is amenable to a theological 
interpretation, and its contents {s} to "generalized cognitors" including subatomic particles, 
sentient organisms, and every material system in between. Unfortunately, above the object 
level, the validity of s-cognition - the internal processing of sentient subsystems s - 
depends on the specific cognitive functionability of a given s...the extent to which s can 
implicitly represent higher-order relations of S. In General Relativity, S is regarded as given 
and complete; the laws of mathematics and science are taken as pre-existing. On the 
quantum scale, on the other hand, laws governing the states and distributions of matter 
and energy do not always have sufficient powers of restriction to fully determine quantum 
behavior, requiring probabilistic augmentation in the course of quantum wavefunction 
collapse. This prevents a given s, indeed anything other than S, from enclosing a complete 
nomology (set of laws); while a complete set of laws would amount to a complete 
deterministic history of the universe, calling the universe "completely deterministic" 
amounts to asserting the existence of prior determinative constraints. But this is a logical 

absurdity, since if these constraints were real, they would be included in reality rather than 
prior or external to it (by the containment principle). It follows that the universe freely 
determines its own constraints, the establishment of nomology and the creation of its 
physical (observable) content being effectively simultaneous and recursive. The 
incoversive distribution of this relationship is the basis of free will, by virtue of which the 
universe is freely created by sentient agents existing within it.
Let's elaborate a bit. Consider the universe as a completely evolved perceptual system, 
including all of the perceptions that will ultimately comprise it. We cannot know all of those 
perceptions specifically, but to the extent that they are interactively connected, we can refer 
to them en masse. The set of "laws" obeyed by the universe is just a minimal set of logical 
relations that suffices to make these perceptions noncontradictory, i.e. mutually consistent, 
and a distributed set of laws is just a set of laws formulated in such a way that the 
formulation can be read by any part of the system S. Obviously, for perceptions to be 
connected by laws, the laws themselves must be internally connected according to a 
syntax, and the ultimate syntax of nomological connectedness must be globally valid; 
whatever the laws may be at any stage of system evolution, all parts of S must be able to 
unambiguously read them, execute and be acted upon by them, and recognize and be 
recognized as their referents ("unambiguously" implies that 2-valued logic is a primary 
ingredient of nomology; its involvement is described by a third metalogical principle 
designed to ensure consistency, namely MU or Multiplex Unity). This implies that the action 
and content of the laws are merged together in each part of the system as a single (but 
dual-aspect) quantity,infocognition. The connectedness and consistency of infocognition is 
maintained by refinement and homogenization as nomological languages are superseded 
by extensional metalanguages in order to create and/or explain new data; because the 
"theory" SCSPL model-theoretically equates itself to the real universe, its "creation" and 
causal "explanation" operations are to a certain extent identical, and the SCSPL universe 
can be considered to create or configure itself by means of "self-theorization" or "self-
explanation".
The simplest way to explain "connected" in this context is that every part of the (object-
level) system relates to other parts within an overall structural description of the system 
itself (to interpret "parts", think of events rather than objects; objects are in a sense defined 
on events in a spatiotemporal setting). Obviously, any part which fails to meet this criterion 
does not conform to a description of the system and thus is not included in it, i.e. not 
"connected to" the system (on the other hand, if we were to insist that it isincluded or 
connected, then we would have to modify the systemic description accordingly). For this 
description to be utile, it should be maximally compact, employing compact predictive 
generalizations in a regular way appropriate to structural categories (e.g., employing 
general "laws of physics"). Because such laws, when formulated in an "if conditions 
(a,b,c…) exist, then (X and Y or Z) applies" way, encode the structure of the entire system 
and are universally applicable within it, the system is "self-distributed". In other words, 
every part of the system can consistently interact with every other part while maintaining an 
integral identity according to this ("TOE") formulation. Spatiotemporal relations can be 
skeletally depicted as edges in a graph whose vertices are events (physical interactions), 
i.e. spacetime "points". In this sense, graph-theoretic connectivity applies. But all those 
object-level connections must themselves be connected by more basic connections, the 

basic connections must be connected by even more basic connections, and so on. 
Eventually - perhaps sooner than later - we reach a basic level of connectivity whose 
syntax comprises a (partially undecidable) "ultimate nomology" for the level of reality we’re 
discussing.
Is this nomology, and the cognitive syntax in which it is expressed, wholly embodied by 
matter? In one sense the answer is yes, because S is distributed over each and every 
material sÎS as the reality-syntax D(S). Thus, every axiom and theorem of mathematics can 
be considered implicit in material syntax and potentially exemplified by an appropriate 
material pattern, e.g. a firing of cerebral neurons. Against holism - the idea that the 
universe is more than the sum of its parts - one can further object that the holistic entity in 
question is still a material ensemble, thus insinuating that even if the universe is not the 
"sum" of its parts, it is still a determinate function of its parts. However, this fails to explain 
the mutual consistency of object-syntaxes, without the enforcement of which reality would 
disintegrate due to perceptual inconsistency. This enforcement function takes matter as its 
argument and must therefore be reposed in spacetime itself, the universal substrate in 
which matter is unconditionally embedded (and as a geometrodynamic or quantum-
mechanical excitation of which matter is explained). So the background has logical 
ascendancy over derivative matter, and this permits it to have aspects, like the power to 
enforce consistency, not expressible by localized interactions of compact material objects 
(i.e., within the bounds of materialism as invoked regarding a putative lack of "material 
evidence" for God, excluding the entire material universe).
On the other hand, might cognitive syntax reside in an external "ideal" realm analogous to 
Plato's world of Parmenidean forms? Plato’s ideal abstract reality is explicitly set apart from 
actual concrete reality, the former being an eternal world of pure form and light, and the 
latter consisting of a cave on whose dirty walls shift murky, contaminated shadows of the 
ideal world above. However, if they are both separate and in mutual correspondence, these 
two realities both occupy a more basic joint reality enforcing the correspondence and 
providing the metric of separation. If this more basic reality is then juxtaposed to another, 
then there must be a more basic reality still, and so on until finally we reach the most basic 
level of all. At this level, there will (by definition) be no separation between the abstract and 
concrete phases, because there will be no more basic reality to provide it or enforce a 
remote correspondence across it. This is the inevitable logical terminus of "Plato’s 
regress". But it is also the reality specified by the containment principle, the scope of 
whose universal quantifier is unlimited up to perceptual relevance! Since it is absurd to 
adopt a hypothesis whose natural logical extension is a negation of that hypothesis, we 
must assume that the ideal plane coincides with this one…but again, not in a way 
necessarily accessible to identifiable physical operations. Rather, physical reality is 
embedded in a more general or "abstract" ideal reality equating to the reality-syntax D(S), 
and the syntax D(S) is in turn embedded in physical reality by incoversion. Thus, if D(S) 
contains supraphysical components, they are embedded in S right along with their physical 
counterparts (indeed, this convention is already in restricted use in string theory and M-
theory, where unseen higher dimensions get "rolled up" to sub-Planck diameter).
What does this say about God? First, if God is real, then God inheres in the 

comprehensive reality syntax, and this syntax inheres in matter. Ergo, God inheres in 
matter, and indeed in its spacetime substrate as defined on material and supramaterial 
levels. This amounts to pantheism, the thesis that God is omnipresent with respect to the 
material universe. Now, if the universe were pluralistic or reducible to its parts, this would 
make God, Who coincides with the universe itself, a pluralistic entity with no internal 
cohesion. But because the mutual syntactic consistency of parts is enforced by a unitary 
holistic manifold with logical ascendancy over the parts themselves - because the universe 
is a dual-aspected monic entity consisting of essentially homogeneous, self-consistent 
infocognition - God retains monotheistic unity despite being distributed over reality at large. 
Thus, we have a new kind of theology that might be called monopantheism, or even more 
descriptively, holopantheism. Second, God is indeed real, for a coherent entity identified 
with a self-perceptual universe is self-perceptual in nature, and this endows it with various 
levels of self-awareness and sentience, or constructive, creative intelligence. Indeed, 
without a guiding Entity whose Self-awareness equates to the coherence of self-perceptual 
spacetime, a self-perceptual universe could not coherently self-configure. Holopantheism is 
the logical, metatheological umbrella beneath which the great religions of mankind are 
unknowingly situated.
Why, if there exists a spiritual metalanguage in which to establish the brotherhood of man 
through the unity of sentience, are men perpetually at each others' throats? Unfortunately, 
most human brains, which comprise a particular highly-evolved subset of the set of all 
reality-subsystems, do not fire in strict S-isomorphism much above the object level. Where 
we define one aspect of "intelligence" as the amount of global structure functionally 
represented by a given sÎS, brains of low intelligence are generally out of accord with the 
global syntax D(S). This limits their capacity to form true representations of S (global 
reality) by syntactic autology [d(S) Éd d(S)] and make rational ethical calculations. In this 
sense, the vast majority of men are not well-enough equipped, conceptually speaking, to 
form perfectly rational worldviews and societies; they are deficient in education and 
intellect, albeit remediably so in most cases. This is why force has ruled in the world of 
man…why might has always made right, despite its marked tendency to violate the 
optimization of global utility derived by summing over the sentient agents of S with respect 
to space and time.
Now, in the course of employing deadly force to rule their fellows, the very worst element of 
humanity – the butchers, the violators, i.e. those of whom some modern leaders and 
politicians are merely slightly-chastened copies – began to consider ways of maintaining 
power. They lit on religion, an authoritarian priesthood of which can be used to set the 
minds and actions of a populace for or against any given aspect of the political status quo. 
Others, jealous of the power thereby consolidated, began to use religion to gather their 
own "sheep", promising special entitlements to those who would join them…mutually 
conflicting promises now setting the promisees at each other’s throats.
But although religion has often been employed for evil by cynics appreciative of its power, 
several things bear notice. (1) The abuse of religion, and the God concept, has always 
been driven by human politics, and no one is justified in blaming the God concept, whether 
or not they hold it to be real, for the abuses committed by evil men in its name. Abusus non 

tollit usum. (2) A religion must provide at least emotional utility for its believers, and any 
religion that stands the test of time has obviously been doing so. (3) A credible religion 
must contain elements of truth and undecidability, but no elements that are verifiably false 
(for that could be used to overthrow the religion and its sponsors). So by design, religious 
beliefs generally cannot be refuted by rational or empirical means.
Does the reverse apply? Can a denial of God be refuted by rational or empirical means? 
The short answer is yes; the refutation follows the reasoning outlined above. That is, the 
above reasoning constitutes not just a logical framework for reality theory, but the outline of 
a logical proof of God's existence and the basis of a "logical theology". While the 
framework serves other useful purposes as well, e.g. the analysis of mind and 
consciousness, we'll save those for another time.
Physics and Metaphysics 
At a January, 1998 meeting of the American Astronomical Society, it was announced 
that a survey of distant supernovae by an international team of astronomers indicates 
that cosmic expansion is being accelerated by an unknown repulsive force…a force that 
appears strong enough to prevent gravity from collapsing the universe. 
The supernovae are of an extensively studied variety known as type Ia.  Type Ia 
supernovae, which result from the explosion of certain white dwarf stars, are so similar 
and steady in luminosity that they can be used as standard candles, or reliable measures 
of astral brightness.  Because they always shine with the same intrinsic intensity, it is 
possible to infer their distances by the dimness of their light.  Distance can then be 
plotted against speed of recession as calculated from frequency reductions due to 
cosmic redshift.  Such a plot turns out to be increasingly nonlinear as distance increases, 
suggesting an unexpected acceleration. 
Most of those analyzing the data agree on the implications.  Because the observed 
supernovae are dimmer than would be expected had the rate of cosmic expansion 
remained constant since their parent stars exploded, expansion must have accelerated 
since the supernovae were born, and some kind of accelerative force must pervade the 
universe.  As early as February, 1998, an independent group calling itself the High-z 
Supernova Search Team (z means redshift) confirmed this finding with a statistical 
confidence of 98.7-99.99%.  Since then, further observations have provided yet more 
confirmation. 
Among the candidates for a repulsive force are Einstein’s cosmological constant, 
representing a form of antigravity associated with General Relativity, and quantum 
mechanical vacuum energy, which produces outward pressure through the spontaneous 
creation and annihilation of virtual particle-antiparticle pairs.  Also receiving welcome air 
time are more bizarre ideas like X-matter and quintessence, strange forms of 
background energy that might be produced by mysterious physical processes as yet 
unidentified.

In the minds of most theorists, the best bet is a combination of cosmological constant 
and vacuum energy.  That would deliver the steady effect that has been observed, as 
opposed to the fluctuations that might accompany X-matter and quintessence. 
Unfortunately, none of the experts can give a constructive reason for why any of these 
influences should themselves exist.  No conventional theory incorporating the Standard 
Big Bang Model of cosmology can explain why an expansive force should be evident or 
how it might have originated.
However, an alternative theory does exist.  It is virtually unknown to most physicists, 
whose stock repertoire is limited to theories that have been published by other physicists 
in exclusive scholastic journals unreceptive to strange new ideas emanating from 
unknown, academically uncredentialed sources.  It has, however, been published for a 
highly intelligent readership in a journal called Noesis.  This theory is called the CTMU. 
Although Noesis is not, strictly speaking, a peer-reviewed journal, the CTMU has been 
extensively criticized and defended within it over the last decade on the basis of 
published descriptions.
The CTMU is not an ordinary physical theory.  Instead of being just another system of ad 
hoc equations purporting to describe some limited aspect of the physical universe, it has 
a unique logical structure designed to embed the very foundations of mathematics and 
philosophy.  Yet, unlike many predominantly rationalistic theories, it is designed to 
resolve paradox on both rational and observational levels, and thus makes verifiable 
statements regarding the nature of observable reality.  Some of these statements, which 
range in application from quantum physics to cosmology, relate to cosmic expansion.
To query the universe regarding its true nature is to ask a very deep question.  There 
was always a strange and touching mixture of humility and hubris in the idea that 
physicists could obtain an answer to this question simply by looking through their 
instruments and carefully describing what they saw.  After all, reality contains features 
like mind, cognition and consciousness that do not lend themselves to empirical 
techniques or scientific reductionism. Yet, they are basic, unavoidable ingredients of 
every scientific measurement ever performed and every theoretical explanation ever 
devised.  Without conscious minds subjecting reality to cognition, science could not exist.
The CTMU provides physics and cosmology with a logical framework that incorporates 
these missing ingredients in a way reflecting the nature of their involvement in 
measurement and theorization.  And as a bonus, it does what no other theory can: while 
painlessly absorbing the bulk of modern physical science and resolving many of its most 
stubborn and destructive paradoxes, it coherently explains the cosmological implications 
of the evidence for accelerative recession of type Ia supernovae in the context of a self-
contained, self-creating universe.
 
Physics and Metaphysics  (© 1998 - 2002 by C.M. Langan)
 
Today : Metaphysics :: Tomorrow : Physics
Today’s dominant theory of small-scale physics, quantum mechanics, did not begin its 
long and successful run as a physical theory.  The reason is logical; its major premise, 
the Heisenberg Uncertainty Principle, sets absolute limits on the accuracy to which 

quanta can be measured, and cursory logical analysis reveals that this defines a relation 
between measurer and measured object that cannot be expressed in a language 
describing measured objects alone. Since classical physics was the latter kind of 
language, neither the uncertainty principle nor quantum mechanics could be immediately 
classified as “physics”.  Rather, they belonged to a logical metalanguage of physics 
called metaphysics.  Indeed, even at a time when physics routinely explains what the 
ancients would have seen as “magic”, some physicists view quantum mechanics with a 
touch of uneasy skepticism.  The reason: it raises too many metaphysical-looking issues 
without satisfactorily resolving them.
Relativity too was initially a metaphysical theory based on the formation of higher-order 
predicates, spacetime and spacetime curvature, that had not existed in physics, drawing 
in the interests of self-containment a higher-order causal relationship between the 
fundamental physical parameters space, time and matter on a combination of empirical 
and mathematical grounds (a higher-order relation is a relation of relations…of relations 
of primitive objects, defined at the appropriate level of predicate logic).  Since this 
describes a semantic operation that cannot be effected within the bare language of 
physics as it existed at the time, relativity was metaphysical rather than physical in 
nature.  Nevertheless, it achieved quick recognition as a physical theory…not only 
because Einstein was already recognized as a professional physicist, but because it 
made physical predictions that classical physics alone did not make. 
It was recognized long before Einstein that observations of physical objects vary 
subjectively in certain parameters.  For example, although objects are identical when 
viewed under identical conditions, they vary in size when viewed from different 
distances, display different shapes from different angles, and seem to be differently 
colored and shaded under different lighting conditions.  Einstein expanded the range of 
subjective variation of physical phenomena by showing that objects also look different 
when viewed at different relative velocities.  But in keeping with classical objectivism, he 
showed in the process that such perceptual variations were a matter of objective 
circumstance, in effect treating perception itself as an objective phenomenon.  Because 
this kind of empirical objectivization is exactly what is expected of the objective empirical 
science of physics, attention was diverted from nagging metaphysical questions involving 
the interplay of rational and empirical factors in perception. 
Although he never got around to enunciating it, Einstein may well have sensed that 
perception cannot be understood without understanding the logic of this interplay, and 
that this logic is instrumental to the inherently metaphysical operation of theoretical 
unification.  Nevertheless, perhaps encouraged by his own apparent success in 
sidestepping this particular metaphysical issue, he spent the second half of his career on 
a doomed attempt to unify physics in a purely physical context - that is, in the context of 
a spacetime model which went only as far as Relativity Theory.  Since then, many bright 
and well-educated people have repeated roughly the same error, never realizing that 
physical theory truly advances only by absorbing profoundly creative metaphysical 
extensions on which no ordinary physicist would wittingly sign off.
Like quantum mechanics and the Theory of Relativity, the CTMU is a metaphysical 
theory that makes distinctive predictions and retrodictions not made by previous theories. 

However, the CTMU makes no attempt in the process to sidestep difficult metaphysical 
issues.  For example, Einstein introduced the cosmological constant to stabilize the size 
of the universe, but then dropped it on the empirical grounds of apparent universal 
expansion.  In contrast, the CTMU goes beyond empiricism to the rational machinery of 
perception itself, providing cosmic expansion with a logical basis while predicting and 
explaining some of its features.  Relativity pursues the goal of explanatory self-
containment up to a point; spacetime contains matter and energy that cause spacetime 
fluctuations that cause changes in matter and energy.  The CTMU, on the other hand, 
pursues the goal of self-containment all the way up to cosmogenesis.  And while neither 
GR nor QM does anything to resolve the fatal paradoxes of ex nihilo creation and 
quantum nonlocality, the CTMU dissolves such paradoxes with a degree of logical 
rectitude to which science seldom aspires. 
To understand how the CTMU is a natural extension of modern physics, let us review the 
history of the physicist’s and cosmologist’s art in the context of Cartesian coordinate 
systems. 
 
The Cartesian Architecture of Split-Level Reality
Modern physics is based on, and can be described as the evolution of, rectilinear 
coordinate systems.  Called Cartesian coordinate systems after their leading exponent, 
René Descartes, they are particularly well-suited to the objective modeling of physical 
phenomena, that is, to the algebraic representation of static and dynamic relationships 
without respect to their locations or the special vantages from which they are observed 
or considered.  This property of Cartesian spaces relates directly to another invention of 
Monsieur Descartes called mind-body dualism, which merely states in plain language 
what Cartesian coordinate systems seem to graphically depict: namely, that cognition 
and physical reality can be factored apart at our philosophical and scientific convenience.
Since the time of Descartes, there have been numerous attempts to clarify the exact 
relationship between mind and reality and thereby solve the "mind-body problem". 
Hume, for example, held that reality consists exclusively of sense impressions from 
which concepts like “mind” and “matter” are artificially abstracted.  Kant countered with 
the view that the mind knows deep reality only through cryptic impressions molded to its 
own cognitive categories. More recently, Lewes’ dual-aspect monism maintained that 
reality consists of a neutral substance of which mind and body are just complementary 
aspects, while the mind-stuff theory of Clifford and Prince was a form of psychical 
monism positing that mind is the ultimate reality, that ordinary material reality is simply 
mind apprehended by mind, and that the higher functions of mind emerge from smaller 
pieces of mind that do not of themselves possess higher mental attributes (an idea 
previously attributable to Leibniz, Spencer and others).
But while each of these theories contains a part of the truth, none contains the whole 
truth. Only recently have the parts been assembled in light of modern logic and science 
to create a dynamic, comprehensive theory of reality that solves the mind-body problem 
for good. 
 
Classical Mechanics: The Infinitesimal Worm in Newton’s Apple
The next big representational advance in physics, Newtonian mechanics, relied on a new 

kind of analytic geometry based on vector modeling and vector analysis of physical 
motion in Cartesian coordinate systems in which space and time are represented as 
independent (orthogonal) axes. Its Langrangian and Hamiltonian formulations occupy the 
same mathematical setting.  From the beginning, the featureless neutrality of Cartesian 
space accorded well with the evident nonpreference of physical laws for special 
coordinate systems…i.e., for the homogeneity and isotropy of space and time.  However, 
this property of Cartesian spaces also rendered position and motion completely relative 
in the Galilean sense.  Finding that this made it hard to explain certain physical 
phenomena - e.g., inertia, and later on, electromagnetic wave propagation - Newton and 
his followers embraced the concept of a stationary aether against whose background all 
dimensions, durations and velocities were considered to be fixed.  The aether was an 
unusual kind of “field” over which the center of mass of the universe was perfectly 
distributed, explaining not only inertia but the evident fact that all of the matter in the 
universe was not gravitating toward a central point in space.
In order to work with his vector model of physical reality, Newton had to invent a new 
kind of mathematical analysis known as infinitesimal calculus.  Specifically, he was 
forced in calculating the velocity of an object at each point of its trajectory to define its 
“instantaneous rate of change” as a ratio of "infinitesimal" vectors whose lengths were 
“smaller than any finite quantity, but greater than zero”.  Since this is a paradoxical 
description that does not meaningfully translate into an actual value – the only number 
usually considered to be smaller than any finite quantity is 0 - it is hard to regard 
infinitesimal vectors as meaningful objects. But even though they could not give a 
satisfactory account of infinitesimal vectors, post-Newtonian physicists at least knew 
where such vectors were located: in the same Cartesian space containing ordinary finite 
vectors.  Better yet, the celebrated mathematician Gauss discovered that they could be 
confined within the curves to which they were tangential, an insight developed by 
Riemann into a comprehensive theory of differential geometry valid for curved surfaces 
in any number of dimensions.  However, although differential geometry would later prove 
useful in formulating a generalization of Cartesian space, its “intrinsic” nature did not 
resolve the paradox of infinitesimals.
For many years after Newton, mathematicians struggled to find a way around the 
infinitesimals paradox, first lighting on the Weierstrass epsilon-delta formalism purporting 
to characterize infinitesimals within standard Cartesian space.  But later – in fact, years 
after Newtonian mechanics was superseded by a more general theory of physics – they 
finally satisfied their yearning to understand infinitesimals as timeless mathematical 
objects.  They accomplished this by reposing them in a hypothetical nonstandard 
universe where each point of the standard universe is surrounded by a nonstandard 
neighborhood containing infinitesimal objects (in logic, a theory is justified by 
demonstrating the existence of a model for which it is true; as a model of infinitesimals 
and their relationships with finite numbers, the nonstandard universe justifies the 
infinitesimal calculus).  Ignoring the obvious fact that this could have a metaphysical 
bearing on physical reality, mathematicians took the nonstandard universe and carried it 
off in the purely abstract direction of nonstandard analysis.
 
Quantum Mechanics: Space and Time Get Smeared (and Worse)
After standing for over two centuries as the last word in physics, the differential 

equations comprising the deterministic laws of Newtonian mechanics began to run into 
problems.  One of these problems was called the Heisenberg Uncertainty Principle or 
HUP.  The HUP has the effect of “blurring” space and time on very small scales by 
making it impossible to simultaneously measure with accuracy certain pairs of attributes 
of a particle of matter or packet of energy.  Because of this blurring, Newton’s differential 
equations are insufficient to describe small-scale interactions of matter and energy. 
Therefore, in order to adapt the equations of classical mechanics to the nondeterministic, 
dualistic (wave-versus-particle) nature of matter and energy, the more or less ad hoc 
theory of quantum mechanics (QM) was hastily developed.  QM identifies matter quanta 
with “probability waves” existing in ¥-dimensional complex Hilbert space, a Cartesian 
space defined over the field of complex numbers a+bi (where a and b are real numbers 
and i = Ö-1) instead of the pure real numbers, and replaces Hamilton’s classical 
equations of motion with Schrodinger’s wave equation.  QM spelled the beginning of the 
end for Laplacian determinism, a philosophical outgrowth of Newtonianism which held 
that any temporal state of the universe could be fully predicted from a complete 
Cartesian description of any other.  Not only uncertainty but freedom had reentered the 
physical arena.
Unfortunately, the HUP was not the only quantum-mechanical problem for classical 
physics and its offshoots.  Even worse was a phenomenon called EPR (Einstein-
Podolsky-Rosen) nonlocality, according to which the conservation of certain physical 
quantities for pairs of correlated particles seems to require that information be 
instantaneously transmitted between them regardless of their distance from each other. 
The EPR paradox juxtaposes nonlocality to the conventional dynamical scenario in 
which anything transmitted between locations must move through a finite sequence of 
intervening positions in space and time.  So basic is this scenario to the classical 
worldview that EPR nonlocality seems to hang over it like a Damoclean sword, poised to 
sunder it like a melon.  Not only does no standard physical theory incorporating common 
notions of realism, induction and locality contain a resolution of this paradox - this much 
we know from a mathematical result called Bell's theorem - but it seems that the very 
foundations of physical science must give way before a resolution can even be 
attempted!
 
The Special Theory of Relativity: Space and Time Beget Spacetime
Another problem for Newton’s worldview was the invariance of c, the speed of light in 
vacuo. Emerging from Maxwell’s equations and direct experimentation (e.g., Michelson-
Morley), c-invariance defies representation in an ordinary Cartesian vectorspace. 
Einstein’s Special Theory of Relativity (SR), arising at about the same time as quantum 
mechanics, was designed to fill the breach.  To accomplish this, Einstein had to 
generalize Cartesian space in such a way that distances and durations vary with respect 
to distinct coordinate systems associated with various states of relative motion.  The 
particular generalization of Cartesian space in which these motion-dependent coordinate 
systems are related, and in which the velocity of light can be invariantly depicted, is 
called Minkowski spacetime.  In spacetime, space and time axes remain perpendicular 
but no longer represent independent dimensions.  Instead, spacelike and timelike 
domains are separated by null or lightlike geodesics (minimal trajectories) representing 
the "paths" traced by light, and the constant velocity of light is represented by the 
constant (usually 45°) orientation of the corresponding spacetime vectors.  The space 

and time axes of moving coordinate systems are skewed by the Lorentz transformation 
functions according to their relative angles of propagation (velocities) through timelike 
domains, resulting in relative distortions of space and time vectors between systems.
 
General Relativity: Spacetime Develops Curvature
Flat or Euclidean spacetime suffices for the representation of all kinds of physical motion 
up to constant linear acceleration.  However, gravity – which Newton had regarded as a 
force and represented as an ordinary Cartesian vector – causes other kinds of motion as 
well, e.g. orbital motion.  So in order to generalize Minkowski spacetime to explain 
gravity, Einstein had to undertake a further generalization of Cartesian space 
accommodating non-flat or curved spacetime.  In this generalization of Cartesian space, 
spacetime curvature is represented by algebraically well-behaved generalizations of 
vectors called tensors, which are just mathematical functions that take ordinary 
spacetime vectors as input and yield other vectors (or numbers) as output.  Calculating 
these entities can be as exacting and tedious as counting sand grains, but they are 
mathematically straightforward.  By modeling physical reality as a curved tensor 
manifold, Einstein was able to explain how gravity affects motion and thus to create the 
gravitational extension of Special Relativity known as General Relativity or GR. While the 
gravitational calculations of GR match those of classical mechanics under most 
conditions, experiment favors GR decisively in certain situations.
Although GR is based on differential geometry intrinsic to curved surfaces – geometry in 
which one need never leave a surface in order to determine its properties – distances 
and curvatures are ultimately expressed in terms of minute spatiotemporal vectors or 
"line elements" which must be made infinitesimal to ensure that they never leave the 
"surface" of spacetime.  Thus, GR avoids the mensural necessity of an external 
hyperspace only by inheriting Newton’s infinitesimals paradox.  Since GR, like classical 
mechanics, treats these line elements as objects to be used in forming ratios and 
tensors, it requires an "object-oriented" (as opposed to a Weierstrass-style procedural) 
definition of infinitesimals.  But such a definition requires a nonstandard universe on 
model-theoretic grounds.  So GR depends on a nonstandard universe as much as its 
predecessors, and is not as self-contained as the concept of intrinsic curvature might 
lead one to expect.  
Strictly speaking, Newtonian mechanics and all subsequent theories of physics require a 
nonstandard universe, i.e. a model that supports the existence of infinitesimals, for their 
formulation.  The effect of this requirement is to blur the distinction between physics, 
which purports to limit itself to the standard universe of measurable distances, and 
metaphysics, which can describe the standard universe as embedded in a higher-
dimensional space or a nonstandard universe containing infinitesimals.  The fast 
acceptance of GR as a "physical" theory thus owed at least in part to the fact that 
physicists had already learned to swallow the infinitesimals paradox every time they 
used the infinitesimal calculus to do classical mechanics!  Only much later, with the 
advent of an infocognitive spacetime internally accommodating necessary metaphysical 
self-extensions, could the infinitesimal line elements of GR be properly embedded in a 
neoCartesian model of reality as abstract ingredients of certain distributed computations 
(see Appendix A).

The moral of the story up to this point is abundantly clear: both before and after Newton, 
the greatest advances in physics have come through the creation and absorption of 
metaphysical extensions.  Unfortunately, most physicists are sufficiently unclear on this 
fact that the word “metaphysics” remains all but unmentionable in their deliberations.
But change finds a way.
 
The Search for a Unified Field: Spacetime Gains New Dimensions
Having found so satisfying a mathematical setting for gravitation, Einstein next tried to 
create a Unified Field Theory (UFT) by using the same model to explain all of the 
fundamental forces of nature, two of which, electricity and magnetism, had already been 
unified by Maxwell as electromagnetism or EM for short  As a natural first step, Einstein 
tried to formulate the EM force as a tensor.  Unfortunately, EM force is governed by 
quantum mechanics, and 4-dimensional GR tensors lack intrinsic quantum properties. 
This alone limits General Relativity to a calculational rather than explanatory bearing on 
electromagnetism.  Because the branch of quantum mechanics called quantum 
electrodynamics (QED), which treats the EM force as a particle interchange, better 
explained the properties and dynamics of electrons and electromagnetic fields, Einstein’s 
geometrodynamic approach to the UFT was widely abandoned.
After a time, however, the trek was resumed.  Kaluza-Klein theory had already added an 
extra dimension to spacetime and curled it up into a tight little loop of radius equal to the 
Planck length (10-33 cm, far smaller than any known particle).  This curling maneuver 
explained more than the extra dimension’s invisibility; because only a discrete number of 
waves can fit around a loop, it also seemed to explain why particle energies are 
quantized, and thus to provide a connection between relativity theory and QM (in 1921, 
Kaluza observed that light could be viewed as the product of fifth-dimensional 
vibrations).  Though temporarily forgotten, this idea was ultimately revived in connection 
with supersymmetry, an attempt to unify the fundamental forces of nature in a single 
theory by defining GR-style tensors accommodating 7 additional spacetime dimensions. 
Shortened and rolled up in bizarre topological configurations, these dimensions would 
exhibit fundamental frequencies and quantized harmonics resembling the quantum 
properties of tiny particles of matter.
Although supersymmetry was eventually dropped because its 11-dimensional structure 
failed to explain subatomic chirality (whereby nature distinguishes between right- and 
left-handedness), its basic premises lived on in the form of 10-dimensional superstring 
theory.  Again, the basic idea was to add additional dimensions to GR, slice and splice 
these extra dimensions in such a way that they manifest the basic features of quantum 
mechanics, and develop the implications in the context of a series of Big Bang phase 
transitions (“broken symmetries”) in which matter changes form as the hot early universe 
cools down (mathematically, these phase transitions are represented by the arrows in 
the series GàHà…àSU(3) x SU(2) x U(1)àSU(3) x U(1), where alphanumerics represent 
algebraic symmetry groups describing the behavioral regularities of different kinds of 
matter under the influence of different forces, and gravity is mysteriously missing) 
Unfortunately, just as General Relativity did nothing to explain the origin of 4-D 
spacetime or its evident propensity to “expand” when there would seem to be nothing for 

it to expand into, string theory did nothing to explain the origin or meaning of the n-
dimensional strings into which spacetime had evolved.  Nor did it even uniquely or 
manageably characterize higher-dimensional spacetime structure; it required the same 
kind of nonstandard universe that was missing from GR in order to properly formulate 
quantum-scale dimensional curling, and eventually broke down into five (5) incompatible 
versions all relying on difficult and ill-connected kinds of mathematics that made even the 
simplest calculations, and extracting even the most basic physical predictions, virtually 
impossible.  Worse yet, it was an unstratified low-order theory too weak to accommodate 
an explanation for quantum nonlocality or measurable cosmic expansion. 
Recently, string theory has been absorbed by a jury-rigged patchwork called “membrane 
theory” or M-theory whose basic entity is a p-dimensional object called, one might almost 
suspect eponymically, a “p-brane” (no, this is not a joke).  P-branes display mathematical 
properties called S- and T-duality which combine in a yet-higher-level duality called the 
Duality of Dualities (again, this is not a joke) that suggests a reciprocity between particle 
size and energy that could eventually link the largest and smallest scales of the universe, 
and thus realize the dream of uniting large-scale physics (GR) with small-scale physics 
(QM).  In some respects, this is a promising insight; it applies broad logical properties of 
theories (e.g., duality) to what the theories “objectively” describe, thus linking reality in a 
deeper way to the mental process of theorization.  At the same time, the “membranes” or 
“bubbles” that replace strings in this theory more readily lend themselves to certain 
constructive interpretations.   
But in other ways, M-theory is just the same old lemon with a new coat of paint.  Whether 
the basic objects of such theories are called strings, p-branes or bubble-branes, they 
lack sufficient structure and context to explain their own origins or cosmological 
implications, and are utterly helpless to resolve physical and cosmological paradoxes like 
quantum nonlocality and ex nihilo(something-from-nothing) cosmogony… paradoxes 
next to which the paradoxes of broken symmetry “resolved” by such theories resemble 
the unsightly warts on the nose of a charging rhinoceros.  In short, such entities 
sometimes tend to look to those unschooled in their virtues like mathematical physics run 
wildly and expensively amok. 
Alas, the truth is somewhat worse.  Although physics has reached the point at which it 
can no longer credibly deny the importance of metaphysical criteria, it resists further 
metaphysical extension. Instead of acknowledging and dealing straightforwardly with its 
metaphysical dimension, it mislabels metaphysical issues as “scientific” issues and 
festoons them with increasingly arcane kinds of mathematics that hide its confusion 
regarding the underlying logic. Like a social climber determined to conceal her bohemian 
roots, it pretends that it is still dealing directly with observable reality instead of 
brachiating up vertiginous rationalistic tightropes towards abstractions that, while 
sometimes indirectly confirmed, are no more directly observable than fairies and 
unicorns.  And as science willfully distracts itself from the urgent metaphysical questions 
it most needs to answer, philosophy ignores its parental responsibility.
 
Reality as a Cellular Automaton: Spacetime Trades Curves for Computation
At the dawn of the computer era, the scientific mainstream sprouted a timely alternative 
viewpoint in the form of the Cellular Automaton Model of the Universe, which we hereby 

abbreviate as the CAMU.  First suggested by mathematician John von Neumann and 
later resurrected by salesman and computer scientist Ed Fredkin, the CAMU represents 
a conceptual regression of spacetime in which space and time are re-separated and 
described in the context of a cellular automaton.  Concisely, space is represented by 
(e.g.) a rectilinear array of computational cells, and time by a perfectly distributed state 
transformation rule uniformly governing cellular behavior.  Because automata and 
computational procedures are inherently quantized, this leads to a natural quantization of 
space and time.  Yet another apparent benefit of the CAMU is that if it can be made 
equivalent to a universal computer, then by definition it can realistically simulate anything 
that a consistent and continually evolving physical theory might call for, at least on the 
scale of its own universality. 
But the CAMU, which many complexity theorists and their sympathizers in the physics 
community have taken quite seriously, places problematic constraints on universality. 
E.g., it is not universal on all computational scales, does not allow for subjective 
cognition except as an emergent property of its (assumedly objective) dynamic, and 
turns out to be an unmitigated failure when it comes to accounting for relativistic 
phenomena.  Moreover, it cannot account for the origin of its own cellular array and is 
therefore severely handicapped from the standpoint of cosmology, which seeks to 
explain not only the composition but the origin of the universe. Although the CAMU array 
can internally accommodate the simulations of many physical observables, thus allowing 
the CAMU’s proponents to intriguingly describe the universe as a “self-simulation”, its 
inability to simulate the array itself precludes the adequate representation of higher-order 
physical predicates with a self-referential dimension.
 
Reality as Reality Theory: Spacetime Turns Introspective
Now let us backtrack to the first part of this history, the part in which René Descartes 
physically objectivized Cartesian spaces in keeping with his thesis of mind-body dualism. 
Notice that all of the above models sustain the mind-body distinction to the extent that 
cognition is regarded as an incidental side effect or irrelevant epiphenomenon of 
objective laws; cognition is secondary even where space and time are considered non-
independent.  Yet not only is any theory meaningless in the absence of cognition, but the 
all-important theories of relativity and quantum mechanics, without benefit of explicit 
logical justification, both invoke higher-level constraints which determine the form or 
content of dynamical entities according to properties not of their own, but of entities that 
measure or interact with them.  Because these higher-level constraints are cognitive in a 
generalized sense, GR and QM require a joint theoretical framework in which 
generalized cognition is a distributed feature of reality.
Let’s try to see this another way.  In the standard objectivist view, the universe gives rise 
to a theorist who gives rise to a theory of the universe.  Thus, while the universe creates 
the theory by way of a theorist, it is not beholden to the possibly mistaken theory that 
results.  But while this is true as far as it goes, it cannot account for how the universe 
itself is created.  To fill this gap, the CTMU Metaphysical Autology Principle or MAP 
states that because reality is an all-inclusive relation bound by a universal quantifier 
whose scope is unlimited up to relevance, there is nothing external to reality with 
sufficient relevance to have formed it; hence, the real universe must be self-configuring. 
And the Mind-Equals-Reality (M=R) Principle says that because the universe alone can 

provide the plan or syntax of its own self-creation, it is an "infocognitive" entity loosely 
analogous to a theorist in the process of introspective analysis. Unfortunately, since 
objectivist theories contain no room for these basic aspects of reality, they lack the 
expressive power to fully satisfy relativistic, cosmological or quantum-mechanical criteria. 
The ubiquity of this shortcoming reflects the absence of a necessary and fundamental 
logical feature of physical analysis, a higher order of theorization in which theory 
cognitively distributes over theory, for which no conventional theory satisfactorily 
accounts. 
   
In view of the vicious paradoxes to which this failing has led, it is only natural to ask 
whether there exists a generalization of spacetime that contains the missing self-
referential dimension of physics.  The answer, of course, is that one must exist, and any 
generalization that is comprehensive in an explanatory sense must explain why.  In 
Noesis/ECE 139, the SCSPL paradigm of the CTMU was described to just this level of 
detail.  Space and time were respectively identified as generalizations of information and 
cognition, and spacetime was described as a homogeneous self-referential medium 
called infocognition that evolves in a process called conspansion.  Conspansive 
spacetime is defined to incorporate the fundamental concepts of GR and QM in a simple 
and direct way that effectively preempts the paradoxes left unresolved by either theory 
alone.  Conspansive spacetime not only incorporates non-independent space and time 
axes, but logically absorbs the cognitive processes of the theorist regarding it.  Since this 
includes any kind of theorist cognitively addressing any aspect of reality, scientific or 
otherwise, the CTMU offers an additional benefit of great promise to scientists and 
nonscientists alike: it naturally conduces to a unification of scientific and nonscientific 
(e.g. humanistic, artistic and religious) thought.
 
CTMU >> CAMU in Camo
Before we explore the conspansive SCSPL model in more detail, it is worthwhile to note 
that the CTMU can be regarded as a generalization of the major computation-theoretic 
current in physics, the CAMU.  Originally called the Computation-Theoretic Model of the 
Universe, the CTMU was initially defined on a hierarchical nesting of universal 
computers, the Nested Simulation Tableau or NeST, which tentatively described 
spacetime as stratified virtual reality in order to resolve a decision-theoretic paradox put 
forth by Los Alamos physicist William Newcomb (see Noesis 44, etc.).  Newcomb’s 
paradox is essentially a paradox of reverse causality with strong implications for the 
existence of free will, and thus has deep ramifications regarding the nature of time in 
self-configuring or self-creating systems of the kind that MAP shows it must be. 
Concisely, it permits reality to freely create itself from within by using its own structure, 
without benefit of any outside agency residing in any external domain.
Although the CTMU subjects NeST to metalogical constraints not discussed in 
connection with Newcomb’s Paradox, NeST-style computational stratification is essential 
to the structure of conspansive spacetime.  The CTMU thus absorbs the greatest 
strengths of the CAMU – those attending quantized distributed computation – without 
absorbing its a priori constraints on scale or sacrificing the invaluable legacy of Relativity. 
That is, because the extended CTMU definition of spacetime incorporates a self-
referential, self-distributed, self-scaling universal automaton, the tensors of GR and its 
many-dimensional offshoots can exist within its computational matrix.

An important detail must be noted regarding the distinction between the CAMU and 
CTMU.  By its nature, the CTMU replaces ordinary mechanical computation with what 
might better be called protocomputation.  Whereas computation is a process defined with 
respect to a specific machine model, e.g. a Turing machine, protocomputation is logically 
"pre-mechanical".  That is, before computation can occur, there must (in principle) be a 
physically realizable machine to host it.  But in discussing the origins of the physical 
universe, the prior existence of a physical machine cannot be assumed.  Instead, we 
must consider a process capable of giving rise to physical reality itself...a process 
capable of not only implementing a computational syntax, but of serving as its own 
computational syntax by self-filtration from a realm of syntactic potential.  When the word 
"computation" appears in the CTMU, it is usually to protocomputation that reference is 
being made.    
It is at this point that the theory of languages becomes indispensable.  In the theory of 
computation, a "language" is anything fed to and processed by a computer; thus, if we 
imagine that reality is in certain respects like a computer simulation, it is a language.  But 
where no computer exists (because there is not yet a universe in which it can exist), 
there is no "hardware" to process the language, or for that matter the metalanguage 
simulating the creation of hardware and language themselves.  So with respect to the 
origin of the universe, language and hardware must somehow emerge as one; instead of 
engaging in a chicken-or-egg regress involving their recursive relationship, we must 
consider a self-contained, dual-aspect entity functioning simultaneously as both.  By 
definition, this entity is a Self-Configuring Self-Processing Language or SCSPL. 
Whereas ordinary computation involves a language, protocomputation involves SCSPL.  
Protocomputation has a projective character consistent with the SCSPL paradigm.  Just 
as all possible formations in a language - the set of all possible strings - can be 
generated from a single distributed syntax, and all grammatical transformations of a 
given string can be generated from a single copy thereof, all predicates involving a 
common syntactic component are generated from the integral component itself.  Rather 
than saying that the common component is distributed over many values of some 
differential predicate - e.g., that some distributed feature of programming is distributed 
over many processors - we can say (to some extent equivalently) that many values of 
the differential predicate - e.g. spatial location - are internally or endomorphically 
projected within the common component, with respect to which they are "in 
superposition".  After all, difference or multiplicity is a logical relation, and logical 
relations possess logical coherence or unity; where the relation has logical priority over 
the reland, unity has priority over multiplicity.  So instead of putting multiplicity before 
unity and pluralism ahead of monism, CTMU protocomputation, under the mandate of a 
third CTMU principle called Multiplex Unity or MU, puts the horse sensibly ahead of the 
cart.
To return to one of the central themes of this article, SCSPL and protocomputation are 
metaphysical concepts.  Physics is unnecessary to explain them, but they are necessary 
to explain physics.  So again, what we are describing here is a metaphysical extension of 
the language of physics.  Without such an extension linking the physical universe to the 
ontological substrate from which it springs - explaining what physical reality is, where it 

came from, and how and why it exists - the explanatory regress of physical science 
would ultimately lead to the inexplicable and thus to the meaningless. 
        
Spacetime Requantization and the Cosmological Constant
The CTMU, and to a lesser extent GR itself, posits certain limitations on exterior 
measurement. GR utilizes (so-called) intrinsic spacetime curvature in order to avoid the 
necessity of explaining an external metaphysical domain from which spacetime can be 
measured, while MAP simply states, in a more sophisticated way consistent with 
infocognitive spacetime structure as prescribed by M=R and MU, that this is a matter of 
logical necessity (see Noesis/ECE 139, pp. 3-10).  Concisely, if there were such an 
exterior domain, then it would be an autologous extrapolation of the Human Cognitive 
Syntax (HCS) that should properly be included in the spacetime to be measured.  [As 
previously explained, the HCS, a synopsis of the most general theoretical language 
available to the human mind (cognition), is a supertautological formulation of reality as 
recognized by the HCS.  Where CTMU spacetime consists of HCS infocognition 
distributed over itself in a way isomorphic to NeST – i.e., of a stratified NeST computer 
whose levels have infocognitive HCS structure – the HCS spans the laws of mind and 
nature.  If something cannot be mapped to HCS categories by acts of cognition, 
perception or reference, then it is HCS-unrecognizable and excluded from HCS reality 
due to nonhomomorphism; conversely, if it can be mapped to the HCS in a physically-
relevant way, then it is real and must be explained by reality theory.] 
Accordingly, the universe as a whole must be treated as a static domain whose self and 
contents cannot “expand”, but only seem to expand because they are undergoing 
internal rescaling as a function of SCSPL grammar.  The universe is not actually 
expanding in any absolute, externally-measurable sense; rather, its contents are 
shrinking relative to it, and to maintain local geometric and dynamical consistency, it 
appears to expand relative to them. Already introduced as conspansion (contraction qua 
expansion), this process reduces physical change to a form of "grammatical substitution" 
in which the geometrodynamic state of a spatial relation is differentially expressed within 
an ambient cognitive image of its previous state. By running this scenario backwards and 
regressing through time, we eventually arrive at the source of geometrodynamic and 
quantum-theoretic reality: a primeval conspansive domain consisting of pure physical 
potential embodied in the self-distributed "infocognitive syntax" of the physical universe…
i.e., the laws of physics, which in turn reside in the more general HCS.
Conspansion consists of two complementary processes, requantization and inner 
expansion. Requantization downsizes the content of Planck’s constant by applying a 
quantized scaling factor to successive layers of space corresponding to levels of 
distributed parallel computation.  This inverse scaling factor 1/R is just the reciprocal of 
the cosmological scaling factor R, the ratio of the current apparent size dn(U) of the 
expanding universe to its original (Higgs condensation) size d0(U)=1.  Meanwhile, inner 
expansion outwardly distributes the images of past events at the speed of light within 
progressively-requantized layers.  As layers are rescaled, the rate of inner expansion, 
and the speed and wavelength of light, change with respect to d0(U) so that relationships 
among basic physical processes do not change…i.e., so as to effect nomological 
covariance.  The thrust is to relativize space and time measurements so that spatial 
relations have different diameters and rates of diametric change from different spacetime 

vantages. This merely continues a long tradition in physics; just as Galileo relativized 
motion and Einstein relativized distances and durations to explain gravity, this is a 
relativization for conspansive “antigravity” (see Appendix B).
Conspansion is not just a physical operation, but a logical one as well.  Because physical 
objects unambiguously maintain their identities and physical properties as spacetime 
evolves, spacetime must directly obey the rules of 2VL (2-valued logic distinguishing 
what is true from what is false).  Spacetime evolution can thus be straightforwardly 
depicted by Venn diagrams in which the truth attribute, a high-order metapredicate of 
any physical predicate, corresponds to topological inclusion in a spatial domain 
corresponding to specific physical attributes.  I.e., to be true, an effect must be not only 
logically but topologically contained by the cause; to inherit properties determined by an 
antecedent event, objects involved in consequent events must appear within its logical 
and spatiotemporal image.  In short, logic equals spacetime topology.
This 2VL rule, which governs the relationship between the Space-Time-Object and 
Logico-Mathematical subsyntaxes of the HCS, follows from the dual relationship 
between set theory and semantics, whereby predicating membership in a set 
corresponds to attributing a property defined on or defining the set.  The property is a 
“qualitative space” topologically containing that to which it is logically attributed.  Since 
the laws of nature could not apply if the sets that contain their arguments and the 
properties that serve as their parameters were not mutually present at the place and time 
of application, and since QM blurs the point of application into a region of distributive 
spacetime potential, events governed by natural laws must occur within a region of 
spacetime over which their parameters are distributed.
Conspansive domains interpenetrate against the background of past events at the inner 
expansion rate c, defined as the maximum ratio of distance to duration by the current 
scaling, and recollapse through quantum interaction.  Conspansion thus defines a kind of 
“absolute time” metering and safeguarding causality.  Interpenetration of conspansive 
domains, which involves a special logical operation called unisection (distributed 
intersection) combining aspects of the set-theoretic operations union and intersection, 
creates an infocognitive relation of sufficiently high order to effect quantum collapse. 
Output is selectively determined by ESP interference and reinforcement within and 
among metrical layers.
Because self-configurative spacetime grammar is conspansive by necessity, the 
universe is necessarily subject to a requantizative “accelerative force” that causes its 
apparent expansion. The force in question, which Einstein symbolized by the 
cosmological constant lambda, is all but inexplicable in any nonconspansive model; that 
no such model can cogently explain it is why he later relented and described lambda as 
“the greatest blunder of his career”.  By contrast, the CTMU requires it as a necessary 
mechanism of SCSPL grammar.  Thus, recent experimental evidence – in particular, 
recently-acquired data on high-redshift Type Ia supernovae that seem to imply the 
existence of such a force – may be regarded as powerful (if still tentative) empirical 
confirmation of the CTMU.
 
Metrical Layering

In a conspansive universe, the spacetime metric undergoes constant rescaling. 
Whereas Einstein required a generalization of Cartesian space embodying higher-order 
geometric properties like spacetime curvature, conspansion requires a yet higher order 
of generalization in which even relativistic properties, e.g. spacetime curvature inhering 
in the gravitational field, can be progressively rescaled.  Where physical fields of force 
control or program dynamical geometry, and programming is logically stratified as in 
NeST, fields become layered stacks of parallel distributive programming that decompose 
into field strata (conspansive layers) related by an intrinsic requantization function 
inhering in, and logically inherited from, the most primitive and connective layer of the 
stack.  This "storage process" by which infocognitive spacetime records its logical history 
is called metrical layering (note that since storage is effected by inner-expansive 
domains which are internally atemporal, this is to some extent a misnomer reflecting 
weaknesses in standard models of computation).
The metrical layering concept does not involve complicated reasoning.  It suffices to note 
that distributed (as in “event images are outwardly distributed in layers of parallel 
computation by inner expansion”) effectively means “of 0 intrinsic diameter” with respect 
to the distributed attribute.  If an attribute corresponding to a logical relation of any order 
is distributed over a mathematical or physical domain, then interior points of the domain 
are undifferentiated with respect to it, and it need not be transmitted among them. 
Where space and time exist only with respect to logical distinctions among attributes, 
metrical differentiation can occur within inner-expansive domains (IEDs) only upon the 
introduction of consequent attributes relative to which position is redefined in an 
overlying metrical layer, and what we usually call “the metric” is a function of the total 
relationship among all layers. 
The spacetime metric thus amounts to a Venn-diagrammatic conspansive history in 
which every conspansive domain (lightcone cross section, Venn sphere) has virtual 0 
diameter with respect to distributed attributes, despite apparent nonzero diameter with 
respect to metrical relations among subsequent events.  What appears to be nonlocal 
transmission of information can thus seem to occur.  Nevertheless, the CTMU is a 
localistic theory in every sense of the word; information is never exchanged “faster than 
conspansion”, i.e. faster than light (the CTMU’s unique explanation of quantum 
nonlocality within a localistic model is what entitles it to call itself a consistent “extension” 
of relativity theory, to which the locality principle is fundamental). 
Metrical layering lets neo-Cartesian spacetime interface with predicate logic in such a 
way that in addition to the set of “localistic” spacetime intervals riding atop the stack (and 
subject to relativistic variation in space and time measurements), there exists an 
underlying predicate logic of spatiotemporal contents obeying a different kind of metric. 
Spacetime thus becomes a logical construct reflecting the logical evolution of that which 
it models, thereby extending the Lorentz-Minkowski-Einstein generalization of Cartesian 
space.  Graphically, the CTMU places a logical, stratified computational construction on 
spacetime, implants a conspansive requantization function in its deepest, most 
distributive layer of logic (or highest, most parallel level of computation), and rotates the 
spacetime diagram depicting the dynamical history of the universe by 90° along the 
space axes.  Thus, one perceives the model’s evolution as a conspansive overlay of 
physically-parametrized Venn diagrams directly through the time (SCSPL grammar) axis 

rather than through an extraneous z axis artificially separating theorist from diagram. 
The cognition of the modeler – his or her perceptual internalization of the model – is 
thereby identified with cosmic time, and infocognitive closure occurs as the model 
absorbs the modeler in the act of absorbing the model. 
To make things even simpler: the CTMU equates reality to logic, logic to mind, and (by 
transitivity of equality) reality to mind.  Then it makes a big Venn diagram out of all three, 
assigns appropriate logical and mathematical functions to the diagram, and deduces 
implications in light of empirical data.  A little reflection reveals that it would be hard to 
imagine a simpler or more logical theory of reality.
 
The CTMU and Quantum Theory 
The microscopic implications of conspansion are in remarkable accord with basic 
physical criteria.  In a self-distributed (perfectly self-similar) universe, every event should 
mirror the event that creates the universe itself.  In terms of an implosive inversion of the 
standard (Big Bang) model, this means that every event should to some extent mirror the 
primal event consisting of a condensation of Higgs energy distributing elementary 
particles and their quantum attributes, including mass and relative velocity, throughout 
the universe.  To borrow from evolutionary biology, spacetime ontogeny recapitulates 
cosmic phylogeny; every part of the universe should repeat the formative process of the 
universe itself. 
Thus, just as the initial collapse of the quantum wavefunction (QWF) of the causally self-
contained universe is internal to the universe, the requantizative occurrence of each 
subsequent event is topologically internal to that event, and the cause spatially contains 
the effect.  The implications regarding quantum nonlocality are clear.  No longer must 
information propagate at superluminal velocity between spin-correlated particles; 
instead, the information required for (e.g.) spin conservation is distributed over their joint 
ancestral IED…the virtual 0-diameter spatiotemporal image of the event that spawned 
both particles as a correlated ensemble.  The internal parallelism of this domain – the 
fact that neither distance nor duration can bind within it – short-circuits spatiotemporal 
transmission on a logical level.  A kind of “logical superconductor”, the domain offers no 
resistance across the gap between correlated particles; in fact, the “gap” does not exist! 
Computations on the domain’s distributive logical relations are as perfectly self-
distributed as the relations themselves.
Equivalently, any valid logical description of spacetime has a property called hology, 
whereby the logical structure of the NeST universal automaton – that is, logic in its 
entirety - distributes over spacetime at all scales along with the automaton itself.  Notice 
the etymological resemblance of hology to holography, a term used by physicist David 
Bohm to describe his own primitive nonlocal interpretation of QM.  The difference: while 
Bohm’s Pilot Wave Theory was unclear on the exact nature of the "implicate order" 
forced by quantum nonlocality on the universe - an implicate order inevitably associated 
with conspansion - the CTMU answers this question in a way that satisfies Bell's theorem 
with no messy dichotomy between classical and quantum reality.  Indeed, the CTMU is a 
true localistic theory in which nothing outruns the conspansive mechanism of light 
propagation. 

The implications of conspansion for quantum physics as a whole, including wavefunction 
collapse and entanglement, are similarly obvious.  No less gratifying is the fact that the 
nondeterministic computations posited in abstract computer science are largely 
indistinguishable from what occurs in QWF collapse, where just one possibility out of 
many is inexplicably realized (while the CTMU offers an explanation called the Extended 
Superposition Principle or ESP, standard physics contains no comparable principle).  In 
conspansive spacetime, time itself becomes a process of wave-particle dualization 
mirroring the expansive and collapsative stages of SCSPL grammar, embodying the 
recursive syntactic relationship of space, time and object. 
There is no alternative to conspansion as an explanation of quantum nonlocality.  Any 
nonconspansive, classically-oriented explanation would require that one of the following 
three principles be broken: the principle of realism, which holds that patterns among 
phenomena exist independently of particular observations; the principle of induction, 
whereby such patterns are imputed to orderly causes; and the principle of locality, which 
says that nothing travels faster than light.  The CTMU, on the other hand, preserves 
these principles by distributing generalized observation over reality in the form of 
generalized cognition; making classical causation a stable function of distributed SCSPL 
grammar; and ensuring by its structure that no law of physics requires faster-than-light 
communication.  So if basic tenets of science are to be upheld, Bell’s theorem must be 
taken to imply the CTMU. 
As previously described, if the conspanding universe were projected in an internal plane, 
its evolution would look like ripples (infocognitive events) spreading outward on the 
surface of a pond, with new ripples starting in the intersects of their immediate ancestors. 
Just as in the pond, old ripples continue to spread outward in ever-deeper layers, 
carrying their virtual 0 diameters along with them.  This is why we can collapse the past 
history of a cosmic particle by observing it in the present, and why, as surely as 
Newcomb’s demon, we can determine the past through regressive metric layers 
corresponding to a rising sequence of NeST strata leading to the stratum corresponding 
to the particle’s last determinant event.  The deeper and farther back in time we regress, 
the higher and more comprehensive the level of NeST that we reach, until finally, like 
John Wheeler himself, we achieve “observer participation” in the highest, most 
parallelized level of NeST...the level corresponding to the very birth of reality.
 
Appendix A
Analysis is based on the concept of the derivative, an "instantaneous (rate of) change". 
Because an "instant" is durationless (of 0 extent) while a "change" is not, this is an 
oxymoron. Cauchy and Weierstrass tried to resolve this paradox with the concept of 
"limits"; they failed. This led to the discovery of nonstandard analysis by Abraham 
Robinson. The CTMU incorporates a conspansive extension of nonstandard analysis in 
which infinitesimal elements of the hyperreal numbers of NSA are interpreted as having 
internal structure, i.e. as having nonzero internal extent. Because they are defined as 
being indistinguishable from 0 in the real numbers Rn, i.e. the real subset of the 
hyperreals Hn, this permits us to speak of an "instantaneous rate of change"; while the 
"instant" in question is of 0 external extent in Rn, it is of nonzero internal extent in Hn. 
Thus, in taking the derivative of (e.g.) x2, both sides of the equation

Dy/Dx = 2x + Dx
(where D = "delta" = a generic increment) are nonzero, simultaneous and in balance. 
That is, we can take Dx to 0 in Rn and drop it on the right with no loss of precision while 
avoiding a division by 0 on the left. More generally, the generic equation
limDxÎH®0ÎRDy/Dx = limDxÎH®0ÎR[f(x +Dx) - f(x)]/Dx 
no longer involves a forbidden "division by 0"; the division takes place in H, while the 
zeroing-out of Dx takes place in R. H and R, respectively "inside" and "outside" the limit 
and thus associated with the limit and the approach thereto, are model-theoretically 
identified with the two phases of the conspansion process L-sim and L-out, as 
conventionally related by wave-particle duality. This leads to the CTMU "Sum Over 
Futures" (SOF) interpretation of quantum mechanics, incorporating an Extended 
Superposition Principle (ESP) under the guidance of the CTMU Telic Principle, which 
asserts that the universe is intrinsically self-configuring.
In this new CTMU extension of nonstandard analysis, the universe can have an 
undefined ("virtually 0") external extent while internally being a "conspansively 
differentiable manifold". This, of course, describes a true intrinsic geometry incorporating 
intrinsic time as well as intrinsic space; so much for relativity theory. In providing a 
unified foundation for mathematics, the CTMU incorporates complementary extensions 
of logic, set theory and algebra. Because physics is a blend of perception (observation 
and experiment) and mathematics, providing mathematics with a unified foundation (by 
interpreting it in a unified physical reality) also provides physics with a unified foundation 
(by interpreting it in a unified mathematical reality). Thus, by conspansive duality, math 
and physics are recursively united in a dual-aspect reality wherein they fill mutually 
foundational roles.
[If you want to know more about how the CTMU is derived using logic and set theory, 
check out these on-line papers:
- On Absolute Truth
- Introduction to the CTMU
- CTMU: A New Kind of Reality Theory (pdf)
I'm currently working on additional papers.]
 
Appendix B
Because the value of R can only be theoretically approximated, using R or even R-1 to 
describe requantization makes it appear that we are simply using one theory to justify 
another.  But the R-to-R-1 inversion comes with an addition of logical structure, and it is 
this additional structure that enables us to define a high-level physical process, 
conspansion, that opposes gravity and explains accelerating redshift. Conspansive 
requantization is uniform at all scales and can be seen as a function of the entire 
universe or of individual quanta; every part of the universe is grammatically substituted, 

or injectively mapped, into an image of its former self…an image endowed with 
computational functionability.  To understand this, we must take a look at standard 
cosmology.
Standard cosmology views cosmic expansion in terms of a model called ERSU, the 
Expanding Rubber Sheet Universe.  For present purposes, it is sufficient to consider a 
simplified 2-spheric ERSU whose objects and observers are confined to its expanding 2-
dimensional surface.  In ERSU, the sizes of material objects remain constant while 
space expands like an inflating balloon (if objects grew at the rate of space itself, 
expansion could not be detected).  At the same time, spatial distances among comoving 
objects free of peculiar motions remain fixed with respect any global comoving 
coordinate system; in this sense, the mutual rescaling of matter and space is symmetric. 
But either way, the space occupied by an object is considered to “stretch” without the 
object itself being stretched. 
Aside from being paradoxical on its face, this violates the basic premise of the pure 
geometrodynamic view of physical reality, which ultimately implies that matter is “space 
in motion relative to itself”.  If we nevertheless adhere to ERSU and the Standard Model, 
the expansion rate (prior to gravitational opposition) is constant when expressed in terms 
of material dimensions, i.e., with respect to the original scale of the universe relative to 
which objects remain constant in size.  For example, if ERSU expansion were to be 
viewed as an outward layering process in which the top layer is “now”, the factor of linear 
expansion relating successive layers would be the quotient of their circumferences. 
Because object size is static, so is the cosmic time scale when expressed in terms of 
basic physical processes; at any stage of cosmic evolution, time is scaled exactly as it 
was in the beginning.
The idea behind the CTMU is to use advanced logic, algebra and computation theory to 
give spacetime a stratified computational or cognitive structure that lets ERSU be 
“inverted” and ERSU paradoxes resolved.  To glimpse how this is done, just look at the 
ERSU balloon from the inside instead of the outside.  Now imagine that its size remains 
constant as thin, transparent layers of parallel distributed computation grow inward, and 
that as objects are carried towards the center by each newly-created layer, they are 
proportionately resized.  Instead of the universe expanding relative to objects whose 
sizes remain constant, the size of the universe remains constant and objects do the 
shrinking…along with any time scale expressed in terms of basic physical processes 
defined on those objects.  Now imagine that as objects and time scales remain in their 
shrunken state, layers become infinitesimally thin and recede outward, with newer levels 
of space becoming “denser” relative to older ones and older levels becoming “stretched” 
relative to newer ones.  In the older layers, light – which propagates in the form of a 
distributed parallel computation – “retroactively” slows down as it is forced to travel 
through more densely-quantized overlying layers.
To let ourselves keep easy track of the distinction, we will give the ERSU and inverted-
ERSU models opposite spellings.  I.e., inverted-ERSU will become USRE.  This turns 
out to be meaningful as well as convenient, for there happens to be an apt descriptive 
phrase for which USRE is acronymic: the Universe as a Self-Representational Entity. 
This phrase is consistent with the idea that the universe is a self-creative, internally-

iterated computational endomorphism. 
It is important to be clear on the relationship between space and time in USRE.  The 
laws of physics are generally expressed as differential equations describing physical 
processes in terms of other physical processes incorporating material dimensions. 
When time appears in such an equation, its units are understood to correspond to basic 
physical processes defined on the sizes of physical objects.  Thus, any rescaling of 
objects must be accompanied by an appropriate rescaling of time if the laws of physics 
are to be preserved.  Where the material contents of spacetime behave in perfect accord 
with the medium they occupy, they contract as spacetime is requantized, and in order for 
the laws of physics to remain constant, time must contract apace. 
E.g., if at any point it takes n time units for light to cross the diameter of a proton, it must 
take the same number of units at any later juncture. If the proton contracts in the interval, 
the time scale must contract accordingly, and the speed and wavelength of newly-
emitted light must diminish relative to former values to maintain the proper distribution of 
frequencies.  But meanwhile, light already in transit slows down due to the distributed 
“stretching” of its deeper layer of space, i.e., the superimposition of more densely-
quantized layers.  Since its wavelength is fixed with respect to its own comoving scale 
(and that of the universe as a whole), wavelength rises and frequency falls relative to 
newer, denser scales.
Complementary recalibration of space and time scales accounts for cosmic redshift in 
the USRE model.  But on a deeper level, the explanation lies in the nature of space and 
time themselves. In ERSU, time acts externally on space, stretching and deforming it 
against an unspecified background and transferring its content from point to point by 
virtual osmosis.  But in USRE, time and motion are implemented wholly within the spatial 
locales to which they apply.  Thus, if cosmic redshift data indicate that “expansion 
accelerates” in ERSU, the inverse USRE formulation says that spacetime requantization 
accelerates with respect to the iteration of a constant fractional multiplier…and that 
meanwhile, inner expansion undergoes a complementary "deceleration" relative to the 
invariant size of the universe.  In this way, the two phases of conspansion work together 
to preserve the laws of nature.
The crux: as ERSU expands and the cosmological scaling factor R rises, the USRE 
inverse scaling factor 1/R falls (this factor is expressed elsewhere in a time-independent 
form r).  As ERSU swells and light waves get longer and lower in frequency, USRE 
quanta shrink with like results.  In either model, the speed of light falls with respect to any 
global comoving coordinate system; cn/c0 = R0/Rn = Rn
-1/R0
-1 (the idea that c is an 
“absolute constant” in ERSU is oversimplistic; like material dimensions, the speed of light 
can be seen to change with respect to comoving space in cosmological time).  But only 
in USRE does the whole process become a distributed logico-mathematical 
endomorphism effected in situ by the universe itself…a true local implementation of 
physical law rather than a merely localistic transfer of content based on a disjunction of 
space and logic.  The point is to preserve valid ERSU relationships while changing their 
interpretations so as to resolve paradoxes of ERSU cosmology and physics.
In Noesis/ECE 139, it was remarked that if the universe were projected on an internal 
plane, spacetime evolution would resemble spreading ripples on the surface of a pond, 

with new ripples starting in the intersects of old ones.  Ripples represent events, or 
nomological (SCSPL-syntactic) combinations of material objects implicit as ensembles of 
distributed properties (quantum numbers).  Now we see that outer (subsurface) ripples 
become internally dilated as distances shorten and time accelerates within new ripples 
generated on the surface.
CTMU monism says that the universe consists of one “dual-aspect” substance, 
infocognition, created by internal feedback within an even more basic (one-aspect) 
substance called telesis. That everything in the universe can manifest itself as either 
information or cognition (and on combined scales, as both) can easily be confirmed by 
the human experience of personal consciousness, in which the self exists as information 
to its own cognition…i.e., as an object or relation subject to its own temporal processing. 
If certain irrelevant constraints distinguishing a human brain from other kinds of object 
are dropped, information and cognition become identical to spatial relations and time.
In a composite object (like a brain) consisting of multiple parts, the dual aspects of 
infocognition become crowded together in spacetime.  But in the quantum realm, this 
“monic duality” takes the form of an alternation basic to the evolution of spacetime itself. 
This alternation usually goes by the name of wave-particle duality, and refers to the 
inner-expansive and collapsative phases of the quantum wave function.  Where ripples 
represent the expansive (or cognitive) phase, and their collapsation into new events 
determines the informational phase, the above reasoning can be expressed as follows: 
as the infocognitive universe evolves, the absolute rate of spatiotemporal cognition cn at 
time n, as measured in absolute (conserved) units of spacetime, is inversely proportional 
to the absolute information density Rn/R0 of typical physical systems...i.e., to the 
concentration of locally-processed physical information.  As light slows down, more 
SCSPL-grammatical (generalized cognitive) steps are performed per unit of absolute 
distance traversed.  So with respect to meaningful content, the universe remains steady 
in the process of self-creation.
CTMU Q and A
Q:  Chris, I'm not a mathematician or physicist by any stretch, but I am a curious person 
and would like to know more about the CTMU (Cognitive-Theoretic Model of the 
Universe).  I am particularly interested in the theological aspects.  Can you please 
explain what the CTMU is all about in language that even I can understand?
A:  Thanks for your interest, but the truth is the CTMU isn't all that difficult for even a 
layperson to understand.  So sit back, relax, kick off your shoes and open your mind…
Scientific theories are mental constructs that have objective reality as their content. 
According to the scientific method, science puts objective content first, letting theories 
be determined by observation.  But the phrase "a theory of reality" contains two key 
nouns,theory and reality, and science is really about both. Because all theories have 
certain necessary logical properties that are abstract and mathematical, and therefore 
independent of observation - it is these very properties that let us recognize and 

understand our world in conceptual terms - we could just as well start with these 
properties and see what they might tell us about objective reality.  Just as scientific 
observation makes demands on theories, the logic of theories makes demands on 
scientific observation, and these demands tell us in a general way what we may observe 
about the universe. 
In other words, a comprehensive theory of reality is not just about observation, but 
about theories and their logical requirements.  Since theories are mental constructs, and 
mental means "of the mind", this can be rephrased as follows: mind and reality are 
linked in mutual dependence at the most basic level of understanding.  This linkage of 
mind and reality is what a TOE (Theory of Everything) is really about.  The CTMU is 
such a theory; instead of being a mathematical description of specific observations (like 
all established scientific theories), it is a "metatheory" about the general relationship 
between theories and observations…i.e., about science or knowledge itself.  Thus, it 
can credibly lay claim to the title of TOE.
Mind and reality - the abstract and the concrete, the subjective and the objective, the 
internal and the external - are linked together in a certain way, and this linkage is the 
real substance of "reality theory".  Just as scientific observation determines theories, the 
logical requirements of theories to some extent determine scientific observation.  Since 
reality always has the ability to surprise us, the task of scientific observation can never 
be completed with absolute certainty, and this means that a comprehensive theory of 
reality cannot be based on scientific observation alone.  Instead, it must be based on 
the process of making scientific observations in general, and this process is based on 
the relationship of mind and reality.  So the CTMU is essentially a theory of the 
relationship between mind and reality.
In explaining this relationship, the CTMU shows that reality possesses a complex 
property akin to self-awareness.  That is, just as the mind is real, reality is in some 
respects like a mind. But when we attempt to answer the obvious question "whose 
mind?", the answer turns out to be a mathematical and scientific definition of God.  This 
implies that we all exist in what can be called "the Mind of God", and that our individual 
minds are parts of God's Mind.  They are not as powerful as God's Mind, for they are 
only parts thereof; yet, they are directly connected to the greatest source of knowledge 
and power that exists.  This connection of our minds to the Mind of God, which is like 
the connection of parts to a whole, is what we sometimes call the soul or spirit, and it is 
the most crucial and essential part of being human. 
Thus, the attempt to formulate a comprehensive theory of reality, the CTMU, finally 
leads to spiritual understanding, producing a basis for the unification of science and 
theology.  The traditional Cartesian divider between body and mind, science and 
spirituality, is penetrated by logical reasoning of a higher order than ordinary scientific 
reasoning, but no less scientific than any other kind of mathematical truth.  Accordingly, 
it serves as the long-awaited gateway between science and humanism, a bridge of 
reason over what has long seemed an impassable gulf.
Q:  Hey Chris, what's your take on the theory of Max Tegmark, physicist at the Institute 
for Advanced Study at Princeton.  He has a paper on the web which postulates that 

universes exist physically for all conceivable mathematical structures.  Is it as "wacky" 
as he postulates?
A:  Since Max claims to be on the fast track to a TOE of his own, I just thought I'd offer a 
few remarks about his approach, and point out a few of the ways in which it differs from 
that of the CTMU.
Many of us are familiar with the Anthropic Principle of cosmology (the AP) and 
Everett's Many Worlds (MW) interpretation of quantum theory.  These ideas have 
something in common: each is an attempt to make a philosophical problem disappear 
by what amounts to Wittgensteinian semantic adjustment, i.e., by a convenient 
redefinition of certain key ingredients.  Specifically, MW attempts to circumvent the 
quantum measurement problem - the decoherence of the quantum wave function - by 
redefining every quantum event as a divergence of universes, shifting the question 
"what happens to the unrealized possible results of a measurement when one possibility 
is exclusively actualized?" to "why can we not perceive the actualizations of these other 
possible results?", while the AP shifts the question "why does the universe exist?" to 
"why is this particular universe perceived to exist?"  Both MW and the AP thus shift 
attention away from objective reality by focusing on the subjective perception of 
objective reality, thereby invoking the distinction between subjectivity and objectivity 
(what usually goes unstated is that mainstream physical and mathematical science have 
traditionally recognized only the objective side of this distinction, sweeping the other 
side under the rug whenever possible).
Perhaps intuiting the MW-AP connection, Max Tegmark (formerly at the Institute of 
Advanced Studies at Princeton) has effectively combined these two ideas and tried to 
shift the focus back into the objective domain.  First, noting that MW is usually 
considered to involve only those universes that share our laws of physics (but which 
differ in the initial and subsequent conditions to which those laws are applied), Tegmark 
extends MW to include other universes with other sets of physical laws, noting that 
since these sets of laws are mathematical in nature, they must correspond to 
mathematical structures...abstract structures that we can investigate right here in this 
universe.  And that, he says, may explain why this universe is perceived to exist: the 
conditions for the evolution of perceptual entities may simply be the mean of a 
distribution generated by distinct physical nomologies corresponding to these 
mathematical structures.  In other words, the conditions for the existence of "self-aware 
substructures" (perceptual life forms) may simply be the most likely conditions within the 
distribution of all possible universes.  And since the latter distribution corresponds to the 
set of mathematical structures in this universe, the hypothesis can be tested right here 
by mathematical physicists.
Of course, Tegmark's attempt at a TOE leaves unanswered a number of deep 
philosophical questions.  First, what good does it do to "anthropically" explain this 
universe in terms of an MW metauniverse unless one can explain where the 
metauniverse came from?  What is supposed to prevent an informationally barren 
infinite regress of universes within metauniverses within meta-metauniverses..., and so 
on?  Second, what good is such a theory unless it contains the means to resolve 
outstanding paradoxes bedeviling physics and cosmology - paradoxes like quantum 
nonlocality, ex nihilo cosmology, the arrow of time, and so forth? Third, what is the true 

relationship between mathematics and physics, that one can simply identify sets of 
physical laws with mathematical structures?  It's fine to say that physics comes from 
mathematics, but then where does mathematics come from?  Fourth, where are the 
mathematical tools for dealing with the apparently ultra-complex problem of computing 
the probability distribution of universes from the set of all mathematical structures, 
including those yet to be discovered?  Fifth, what is the real relationship between 
subjective and objective reality, on which distinction both Many Worlds and the 
Anthropic Principle are ultimately based?  (Et cetera.)
Since one could go on for pages, it seems a little premature to be calling Tegmark's 
theory a TOE (or even a reasonable TOE precursor).  And although I 'm not saying that 
his theory contains nothing of value, I'm a bit puzzled by the absence of any mention of 
certain obvious mathematical ingredients.  For example, topos theory deals with topoi, 
or so-called "mathematical universes" consisting of mathematical categories (mapping 
algebras) equipped not only with the objects and morphisms possessed by categories in 
general, but special logics permitting the assignment of truth values to various 
superficially nonalgebraic (e.g. "physical") expressions involving the objects.  Why 
would any "TOE" purporting to equate physical universes to mathematical structures 
omit at least cursory mention of an existing theory that seems to be tailor-made for just 
such a hypothesis?  This in itself suggests a certain amount of oversight.  Tegmark may 
have a few good ideas knocking around upstairs, but on the basis of what his theory 
omits, one can't avoid the impression that he's merely skirting the boundary of a real 
TOE.  
In contrast, the CTMU deals directly with the outstanding paradoxes and fundamental 
interrelationship of mathematics and physics.  Unlike other TOEs, the CTMU does not 
purport to be a "complete" theory; there are too many physical details and undecidable 
mathematical theorems to be accounted for (enough to occupy whole future generations 
of mathematicians and scientists), and merely stating a hypothetical relationship among 
families of subatomic particles is only a small part of the explanatory task before us. 
Instead, the CTMU is merely designed to be consistent and comprehensive at a high 
level of generality, a level above that at which most other TOEs are prematurely aimed.
The good news is that a new model of physical spacetime, and thus a whole new 
context for addressing the usual round of quantum cosmological problems, has 
emerged from the CTMU's direct attack on deeper philosophical issues.  
Q:  Einstein says that gravity is a result of "mass-energy" causing a curvature in the four 
dimensional space time continuum. At the planck scale, (10^(-33)) centimeters, is space 
still continuous?, or is space discontinuous?  I have read books saying space time may 
have holes or breaks in continuity.  Are these holes related in any way to "gravitons", or 
reverse time causality? (Question from Russell Rierson)
A:  A mathematical space is continuous if it has a metric that withstands infinitesimal 
subdivision. To understand what this means, one must know what a "metric" is. 
Simplistically, a metric is just a general "distance" relationship defined on a space as 
follows: if a and b are two points in a space, and c is an arbitrary third point, then the 
distance between a and b is always less than or equal to the sum of the distances 

between a and c, and b and c. That is, where d(x,y) is the distance between two points x 
and y,
d(a,b) <= d(a,c) + d(b,c). 
If this relationship continues to hold no matter how close together the points a, b and c 
might be, then the space is continuous. On the other hand, where the distance concept 
is undefined below a certain threshold, metric continuity breaks down on that scale. 
Since the Planck limit is such a threshold, space is discontinuous below the Planck 
scale...implying, of course, that it is discontinuous, period. Not only is it "granular" in a 
slippery kind of way, but the grains in question are effectively without spatial extent. 
Because space and time are undefined below quantum limits, they no longer have 
extensionality or directionality. But if we interpret this to mean that anything, including 
causality, can "flow" in any direction whatsoever, then reverse causality is conceivable 
on sub-Planck scales. In fact, some theorists conjecture that on these scales, 
continuous spacetime becomes a chaotic "quantum foam" in which distant parts of the 
universe are randomly connected by microscopic "wormholes". That's pretty much the 
party line among physicists. 
Now let's bring philosophy to bear on the issue. At one time, space was considered to 
consist of "ether", a quasimaterial "substance" through which physical objects were 
thought to swim like fish through water. But since the introduction of Einstein's Theory of 
Relativity, nothing material remains of empty space; although it is permeated by fields 
and "vacuum energy", these are merely contained by space and are not equivalent to 
space itself. Space has instead become a mathematical abstraction called a "tensor 
field" that confers relative attributes like location, direction, orientation, distance, linear 
and angular velocity, and geometry on physical objects and energy fields. Because 
empty space, as abstracted from its contents, cannot be observed and has no 
observable effect on anything, it is not "physical" in the usual sense. 
That which is immaterial is abstract, and abstraction is a mental process that "abstracts" 
or educes general relationships from observations. So from a philosophical viewpoint, 
saying that space is immaterial and therefore abstract amounts to saying that it is 
"mental"...that it is to some extent composed of mind rather than matter. Although this 
runs against the scientific grain, it is consistent with our dominant physical theories of 
the very large and the very small, namely relativity and quantum mechanics. In relativity, 
space and time are combined in an abstract manifold called "spacetime" whose "points" 
are physical events that can be resolved in terms of mutual behavioral transduction of 
material objects, a process fundamentally similar to mentation. And quantum mechanics 
characterizes matter in terms of abstract, immaterial wave functions that are physically 
actualized by interactions of an equally immaterial nature.
What does this mean regarding the continuity of spacetime? Simply that like spacetime 
itself, continuity and its quantum-scale breakdown are essentially mental rather than 
material in character. As Berkeley observed centuries ago, reality is ultimately 
perceptual, and as we know from the subsequent debate between Hume and Kant, 
perception conforms to mental categories... categories like space and time. So rather 

than being purely objective and "physical" in a materialistic sense, space has a 
subjective aspect reflecting the profoundly mental nature of our reality. 
Gravitons, though subject to some of the same reasoning, are another matter.
Q:  Does the CTMU allow for  the existence of souls and reincarnation?
A:  From the CTMU, there emerge multiple levels of consciousness.  Human temporal 
consciousness is the level with which we're familiar; global (parallel) consciousness is 
that of the universe as a whole.  The soul is the connection between the two...the 
embedment of the former in the latter.
In the CTMU, reality is viewed as a profoundly self-contained, self-referential kind of 
"language", and languages have syntaxes.  Because self-reference is an abstract 
generalization of consciousness - consciousness is the attribute by virtue of which we 
possess self-awareness - conscious agents are "sublanguages" possessing their own 
cognitive syntaxes. Now, global consciousness is based on a complete cognitive syntax 
in which our own incomplete syntax can be embedded, and this makes human 
consciousness transparent to it; in contrast, our ability to access the global level is 
restricted due to our syntactic limitations. 
Thus, while we are transparent to the global syntax of the global conscious agency 
"God", we cannot see everything that God can see.  Whereas God perceives one total 
act of creation in a parallel distributed fashion, with everything in perfect superposition, 
we are localized in spacetime and perceive reality only in a succession of locally 
creative moments.  This parallelism has powerful implications. When a human being 
dies, his entire history remains embedded in the timeless level of consciousness...the 
Deic level.  In that sense, he or she is preserved by virtue of his or her "soul". And since 
the universe is a self-refining entity, that which is teleologically valid in the informational 
construct called "you" may be locally re-injected or redistributed in spacetime.  In 
principle, this could be a recombinative process, with the essences of many people 
combining in a set of local injections or "reincarnations" (this could lead to strange 
effects...e.g., a single person remembering simultaneous "past lifetimes").
In addition, an individual human sublanguage might be vectored into an alternate 
domain dynamically connected to its existence in spacetime.  In this scenario, the entity 
would emerge into an alternate reality based on the interaction between her local level 
of consciousness and the global level embedding it...i.e., based on the state of her 
"soul" as just defined.  This may be the origin of beliefs regarding heaven, hell, 
purgatory, limbo and other spiritual realms.
Q:  If I have interpreted you correctly, you maintain that the universe created itself. How 
did this come about? What existed before the Universe and when did the Universe 
create itself or come into being?                    
- Celia Joslyn
A:  You're asking three distinct but related questions about cosmology: how, when and 
as what did the universe self-create? 

The universe can be described as a cybernetic system in which freedom and constraint 
are counterbalanced. The constraints function as structure; thus, the laws of physics are 
constraints which define the structure of spacetime, whereas freedom is that which is 
bound or logically quantified by the constraints in question. Now, since there is no real 
time scale external to reality, there is no extrinsic point in time at which the moment of 
creation can be located, and this invalidates phrases like  "before reality existed" and 
"when reality created itself".  So rather than asking "when" the universe came to be, or 
what existed "before" the universe was born, we must instead ask "what would remain if 
the structural constraints defining the real universe were regressively suspended?" First, 
time would gradually disappear, eliminating the "when" question entirely. And once time 
disappears completely, what remains is the answer to the "what" question: a realm of 
boundless potential characterized by a total lack of real constraint. In other words, the 
real universe timelessly emerges from a background of logically unquantified potential to 
which the concepts of space and time simply do not apply. 
Now let's attend to your "how" question. Within a realm of unbound potential like the one 
from which the universe emerges, everything is possible, and this implies that 
"everything exists" in the sense of possibility. Some possibilities are self-inconsistent 
and therefore ontological dead ends; they extinguish themselves in the very attempt to 
emerge into actuality. But other possibilities are self-consistent and potentially self-
configuring by internally defined evolutionary processes. That is, they predicate their 
own emergence according to their own internal logics, providing their own means and 
answering their own "hows". These possibilities, which are completely self-contained not 
only with respect to how, what, and when, but why, have a common structure called 
SCSPL (Self-Configuring Self-Processing Language). An SCSPL answers its own 
"why?" question with something called teleology; where SCSPL is "God" to whatever 
exists within it, teleology amounts to the "Will of God".
Q:   Is there a reason for reality to exist?  Specifically, some mathematical proof that 
would prove that a reality must exist?  This would of course lead in to the more common 
type of questions, like "Does *this* reality exist?"  Perhaps there's a mathematical or 
logical proof somewhere that shows that *something* must exist (reality by default), or 
that total-non-existence can't exist by it's very definition.      
- Bill  
A:  See the above response to Celia.   
Q:   I think I got clued in by an old Alan Watts text that said (I think) "If you can agree that 
you are not separated from reality, then you must agree that your 'self'-awareness is also 
reality's awareness of itself."  This is of course continued to "if God exists and cannot be 
separated from reality, then your awareness is also his awareness of himself, etc etc".  I 
think this approximates some of what you have said, but doesn't require the upper level 
math(!). - Bill
A:  If Watts said these things, then he anticipated the CTMU (loosely speaking, of 
course). But whereas Watts used conditional (if...then) formulations, similar statements 
are unconditionally supported by certain elements of mathematical structure that he 
omitted.

Q:   Given my own self-awareness and inability to separate from reality, *I* have no 
doubt that this reality *does* exist (the proof is in the pudding).  So while I do not need 
"proof" that there is a reality, that I am part of that reality, and that my awareness is 
reality's awareness of itself - I do not know WHY all of this stuff exists (myself included).
If there *is* a reason that reality MUST exist, then that would also be the reason that *I* 
exist. Which is probably what I am really wondering.  Is the answer that giving myself a 
reason to exist is the reason for my existence?     
- Bill     
A:   The first part of your "why" question is answered at the end of the above response 
to Celia.  Since the meaning of life is a topic that has often been claimed by religion, 
we'll attempt to answer the second part with a bit of CTMU-style "logical theology". 
Within each SCSPL system, subsystems sharing critical aspects of global structure will 
also manifest the self-configuration imperative of their inclusive SCSPL; that is, they 
exist for the purpose of self-actualization or self-configuration, and in self-configuring, 
contribute to the Self-configuration of the SCSPL as a whole. Human beings are such 
subsystems. The "purpose" of their lives, and the "meaning" of their existences, is 
therefore to self-actualize in a way consistent with global Self-actualization or 
teleology...i.e., in a way that maximizes global utility, including the utility of their fellow 
subsystems. Their existential justification is to help the universe, AKA God, express its 
nature in a positive and Self-beneficial way.   
If they do so, then their "souls", or relationships to the overall System ("God"), attain a 
state of grace and partake of Systemic timelessness ("life eternal"). If, on the other 
hand, they do not - if they give themselves over to habitual selfishness at the expense of 
others and the future of their species - then they are teleologically devalued and must 
repair their connections with the System in order to remain a viable part of it. And if they 
do even worse, intentionally scarring the teleological ledger with a massive net loss of 
global utility, then unless they pursue redemption with such sincerety that their intense 
desire for forgiveness literally purges their souls, they face spiritual interdiction for the 
sake of teleological integrity.  
Such is the economy of human existence. Much of what we have been taught by 
organized religions is based on the illogical literalization of metaphorical aspects of their 
respective doctrines. But this much of it is true: we can attain a state of grace; we can 
draw near to God and partake of His eternal nature; we can fall from God's grace; we 
can lose our souls for doing evil. In all cases, we are unequivocally answerable to the 
System that grants and sustains our existence, and doing right by that System and its 
contents, including other subsystems like ourselves, is why we exist. Sometimes, "doing 
right" simply means making the best of a bad situation without needlessly propagating 
one's own misfortune to others; the necessary sufferance and nonpropagation of 
personal misfortune is also a source of grace. Further deontological insight requires an 
analysis of teleology and the extraction of its ethical implications. 
Now for a couple of qualifiers. Because we are free, the teleologically consistent 
meaning of our lives is to some extent ours to choose, and is thus partially invested in 
the search for meaning itself. So the answer to the last part of your question is "yes, 

determining the details of your specific teleologically-consistent reason to exist is part of 
the reason for your existence". Secondly, because God is the cosmos and the human 
mind is a microcosm, we are to some extent our own judges. But this doesn't mean that 
we can summarily pardon ourselves for all of our sins; it simply means that we help to 
determine the system according to whose intrinsic criteria our value is ultimately 
determined. It is important for each of us to accept both of these ethical responsibilities.
Q:  Humor me, Chris. Why does this work, and why in God's name would someone do 
this?  I'm sending it to you because you know EVERYTHING!!!!!  Thanks!!!!  
Eleanor Mondale, Southampton, NY
It only takes about a minute.......Work this out as you read.  Be sure you don't read the 
bottom until you've worked it out!  This is not one of those waste of time things, it's fun.
1. First of all, pick the number of times a week that you would like to have chocolate. (try 
for more than once but less than 10)
2. Multiply this number by 2 (Just to be bold)
3. Add 5. (for Sunday)
4. Multiply it by 50 (being a bit stupid)  I'll wait while you get the calculator................
5. If you have already had your birthday this year add 1751.... If you haven't, add 
1750 ..........
6. Now subtract the four digit year that you were born. (if you remember)
You should have a three digit number .....
The first digit of this was your original number (i.e., how many times you want to have 
chocolate each week).   The next two numbers are your age.
THIS IS THE ONLY YEAR (2001) IT WILL EVER WORK, SO SPREAD IT AROUND 
WHILE IT LASTS. IMPRESSIVE ISN'T IT?
A:  One reason people find this amazing is that it seems to reveal a mysterious 
mathematical connection between your age and your appetite for chocolate.  Otherwise, 
why would it yield your age even though all you're feeding in is the number of times per 
week you want to eat chocolate?  Shouldn't the randomness of your appetite for 
chocolate mess up your age?  How does your age get in there in the first place?  It must 
happen when you subtract your year of birth.  But shouldn't subtracting your year of birth 
destroy any information regarding your appetite for chocolate?
No. The procedure is structured in such a way that the number you choose simply gets 
bumped up a couple of place values, where it can't cross wires with the basic age and 
birth-year arithmetic. To see this, assume that you don't like chocolate and want to eat 
chocolate 0 times per week...i.e., that your chocoholic index is 0.  Then what you start 
with is: 
(0 x 2 + 5) x 50 = 5 x 50 = 250.
Now subtract 250 from 2001. What do you get? Presto!
2001 - 250 = 1751
That is, because
250 + 1751 = 2001,
you're simply calculating the current year by adding 1751.
So now we've got the current year, 2001.  But what happens when you subtract your 
year of birth from the current year, provided you've already had your birthday?  You get 
your age!  That's how the age and birth-year arithmetic was reverse-engineered.

Now what happens if you start upping your chocoholic index one binge at a time?  If you 
up it from 0 to 1, you get
(1 x 2 + 5)50 = 350
instead of 250, which means you're adding 350 - 250 = 100 to your age.  If you up it to 
2, you get
(2 x 2 + 5)50 = 450
which means you're adding 450 - 250 = 200 to your age.  And so on and so forth. 
Multiplying your chocoholic index by 2 x 50 = 100 simply moves it up to the 102 
(hundreds) place, where it can't affect the 101 and 100 (tens and ones) places containing 
your age.  It's a red herring!
The author of this trick states that it can only be used this year (2001).  Is that true? 
Well, yes and no.  It's true as long as we insist on adding the "magic number" 1751.  But 
it's false in the sense that we can update or backdate the trick to any year we like by 
instead adding a number equal to that year minus 250.  For example, next year we'd 
add 1752, while in the year 2101, we'd add 1851.
What if you want to eat chocolate ten or more times per week?  No problem.  But in that 
case, you end up with a number of more than three digits.  The 101 and 100 places still 
contain your two-digit age, while the higher places contain your 2, 3 or n-digit chocoholic 
index.
Can we change this trick into a new one?  Sure!  Choose the number of fast-food 
burgers you want to eat per day - your "Wimpy index" - multiply it by 4, add 12, multiply 
the result by 25, add 1701 (1700 if you haven't had your birthday yet), subtract your year 
of birth, and marvel at the results.  This is sufficiently close to the old trick that you 
should be able to see how to cook up as many such tricks as you like.  [Note that the 
product of the first and third numbers equals 100 - that's the multiplier that bumps your 
Wimpy index up two places - while the fourth number equals the current year minus the 
product of the second and third numbers.]
Why would someone do something like this?  It's just a bit of mathematical legerdemain 
that probably has the person who cooked it up laughing himself (or herself) silly over 
how gullible, innumerate and greedy for chocolate most of us are!
Q: ...a couple of  questions about the CTMU:
Christopher Michael Langan said in his introduction following:   "Thus, if D(S) contains 
supraphysical components, they are embedded in S right along with their physical 
counterparts (indeed, this convention is already in restricted use in string theory and M-
theory, where unseen higher dimensions get "rolled up" to sub-Planck diameter)."
 If I understood it right, the supraphysical component in string- and M-theory is called 
supraphysical, because the model does not assume it to be part of the physical 
universe. Taking on that thought and considering the definition of the REAL UNIVERSE 
in the CTMU I have to doubt that the supraphysical component is even part of the REAL 
universe.  Does anyone know where my mistake in thought lies?
A:  As noted by Berkeley, we can know reality only through perception. So our theories 

of reality necessarily have a perceptual or observational basis.  But as noted by Kant, 
the process of observation has substantial internal complexity; it is a relationship of 
subject and object with sensory (phenomenal) and cognitive (categorical) components. 
So reality is at once monic, because uniformly perceptual, and dualistic, because 
perception has two complementary aspects. Thus, the "dual aspect monism" of the 
CTMU.  Now consider physics. Because physics is governed by the scientific method, it 
deals exclusively with phenomena. Thus, it effectively diverts attention away from the 
cognitive, categorical aspect of perceptual reality, without which neither phenomena nor 
scientific theories could exist. Because physics is irreducibly dualistic and takes the 
fundamental separation of mind and matter as axiomatic, it cannot provide us with a 
complete picture of reality. It can tell us only what lies outside the subjective observer, 
not within.
By definition, reality must contain all that it needs to exist; equivalently, anything on 
which the existence of reality depends is real by definition (if it were not, then reality 
would be based on nonreality and would itself be unreal, a semantic contradiction). So 
attempts to explain reality entirely in terms of physics are paradoxical; reality contains 
not only the physical, but the abstract machinery of perception and cognition through 
which "the physical" is perceived and explained. Where this abstract machinery is what 
we mean by "the supraphysical", reality has physical and supraphysical aspects. 
Physical and supraphysical reality are respectively "concrete" and "abstract", i.e. 
material and mental in nature.  
The question is, do we continue to try to objectivize the supraphysical component of 
reality as do the theories of physics, strings and membranes, thus regenerating the 
paradox? Or do we take the CTMU approach and resolve the paradox, admitting that 
the supraphysical aspect of reality is "mental" in a generalized sense and describing all 
components of reality in terms of SCSPL syntactic operators with subjective and 
objective aspects?
My advice: we take the CTMU approach, relegating the scientific method to the 
phenomenal side of reality theory - after all, M-theory is beyond the empirical scope of 
the scientific method already - and recognizing that the universe is everywhere both 
subjective and objective, rational and empirical, mental and material. Anything else 
would lead to reductio ad absurdum.
Q:  Scientists understand how the universe was made. My question is, where did the 
matter, or the energy which eventually became the matter, come from to form the 
universe?
Bob Cannarsa, Glen Head, NY
A:  By definition, there is nothing outside of reality that is real enough to contain reality. 
So reality is self-contained. A self-contained medium must provide that which is 
necessary to its own existence. So if energy is necessary for the existence of reality, 
reality must find that energy within itself. Because matter consists of energy according 
to Einstein’s famous equation e=mc2, this applies to matter as well.  That is, the 
universe, using its own energy, made its own matter.  How could it do this?  By 
configuring itself in such a way that the matter it made would be “recognized” as such by 

other matter.  
Q: What came first, the chicken or the egg?  Those are your choices...either the chicken 
or the egg.  Any other answer is wrong.
John Harras
A:  Contrary to popular belief, this age-old dilemma actually has a very straightforward 
solution.  First, you must specify what kind of egg you mean. If you mean “any kind of 
egg”, then the egg came first (because chickens were preceded on the evolutionary 
timeline by, for example, egg-laying fish, insects and dinosaurs). If, on the other hand, 
you mean “a chicken egg”, then you must specify whether this means (a) “an egg laid by 
a chicken”, (b) “an egg containing a chicken”, or (c) “an egg laid by and containing a 
chicken”. In cases (a) and (c), the answer is by definition “the chicken” (if the answer 
were “the egg”, then the egg could not have been laid by a chicken).
In case (b), the usual and most interesting interpretation, the answer is “the egg”. This is 
because interspecies mutations separating a new species from its parent species occur 
in reproductive rather than somatic DNA, i.e. in germ cells rather than body cells. (Germ 
cells include the sperm and egg cells produced in the reproductive tracts of male and 
female animals respectively.) Since germ cells are merely produced, but not somatically 
expressed, by the parents of the organism(s) whose biological information they encode, 
their expression begins in the egg containing the offspring.  So the egg contains a 
chicken, but was not laid by a chicken.  (See how easy that was?)
Q: My question is this:  If you could answer the question what is the mathematical 
difference between visible light and invisible light, i.e. ultraviolet rays, wouldn't this 
answer the question concerning the importance of further study into what is defined as 
physical.  After all how do you perceive ultraviolet rays-- as a sunburn or plant growth. 
Therefore, although not visible there indeed may be other energy forms that coexist 
right where we are, having an impact on us, without our knowing its source.  It is not 
visibly physical yet its effect on us is very physical.
A: Visible and UV light differ in frequency, or number of waves transmitted or received 
per second.  Because light always travels at the same speed (c = ~300K km/sec), 
higher frequency means shorter waves: 
lambda = c/frequency  (where lambda = wavelength)
I.e., more energetic, higher-frequency light has a smaller wavelength than less 
energetic, lower-frequency light.  Unfortunately, the tiny light sensors in our retinas, 
called rods and cones, cannot detect short-wavelength UV light.  
Your question seems to be this: how can we call UV light “physical” when we cannot 
directly detect it?  The answer is twofold but simple: we can call it “physical” because of 
(1) its perceptible physical effects on animals, plants, minerals and detection devices, 
and (2) our need to acknowledge the full definitions and logical implications of our 
perceptions and concepts.  

Answer (2) is why reality is not merely “physical” in the concrete or material sense.  In 
order to exist as a self-consistent perceptible entity, reality must ultimately make logical 
sense; our perceptions of it must conform to a coherent cognitive syntax containing the 
rules of perception and cognition and incorporating logic.  This syntax tells us that if light 
exists below the maximum visible frequency, then in the absence of any extra 
constraints, it can exist above it as well.  
Specifically, having identified the physical cause of light to be photon emission by 
subatomic oscillators called electrons, we are compelled to recognize the existence of 
"light" at whatever frequencies such oscillators may exhibit, right up through X and 
gamma radiation.  The logical component of our cognitive syntax ultimately forces us to 
define and cross-relate all of the concepts in terms of which we apprehend reality, 
including light, in a logically consistent way.
Q: Hello Chris.  I saw you on TV and heard what you had to say about God.  I have also 
read your description of the CTMU.  I have had the same thoughts as to our existence in 
the mind of GOD.  I think that the evidence of evolution that exists in the universe has to 
be linked with creation as a tool.  I have only 11.75 years of School and not very high IQ 
so please excuse the grammar, etc.
A: Hi!  You don’t need to apologize for the 11.75 years of school – I don’t have much 
more myself!  Regarding evolution and creationism, the linkage is simple: since Biblical 
accounts of the genesis of our world and species are true but metaphorical, our task is 
to correctly decipher the metaphor in light of scientific evidence also given to us by God. 
Hence, the CTMU.  
Q: God said he would reveal his existence (reality) with numbers.  Do you see yourself 
as part of this end time promise?
A: If God made such a promise, then one could say that the CTMU is at least a part of 
its fulfillment.  This is because the number concept is actually far more general than 
most people think it is.  
At one time, a "number" was a positive integer.  As the years passed, new kinds of 
number were discovered: 0, negative numbers, rational numbers or fractions, irrational 
numbers that cannot be expressed as fractions, complex numbers, and even 
transcendental and transfinite or "infinite" numbers.  Noting that each kind of number is 
associated with an algebraic system like a number field, we finally realized that a 
“number” is any element of an algebraic system.  Because the CTMU embodies an 
algebraic system called SCSPL, it too is “numeric”.  And since this system is the basis 
of a proof of God’s existence, the CTMU might be said to “reveal the existence of God 
with numbers”.
Q: I have read your CTMU and some of the Q & A on the Ubiquity website regarding the 
CTMU and find it extremely fascinating.  Much of the information resonated with many 
of the things I have been contemplating for the last year (or so).  I wanted to know if you 
had any further writings on the topic especially related to the following areas.  (1) The 
nature of the interaction(s) of the multiple levels of consciousness.  (2) The nature of the 

connection with God via our "souls".  Or just in general, the nature of the soul.  Is it a 
more complex syntax in which we are embedded that facilitates this communication with 
God?  Are we all embedded in it?  (3) The nature of morality.  Do "moral laws" have a 
basis in reality (loosely speaking).  That is, if moral laws are mental constructs, how do 
the mental constructs of higher levels of consciousness affect the lower levels?  That is, 
how does what "God thinks is right" affect us (lower forms of consciousness)?   I realize 
that, to a degree, the above questions are really all the same, but if you have any 
essays or thoughts on these matters I would love to hear them.
I have more questions and thoughts but I can save those for later...
A: Yes, such writings exist, but they are (as yet) mostly unpublished.  Don’t worry, I'll get 
them out there somehow.  As for your specific questions on morality, the following 
should suffice.  In the CTMU, “what God thinks is right” is encapsulated by the Telic 
Principle.  This principle, a generalization of the Cosmological Anthropic Principle, 
asserts that by logical necessity, there exists a deic analogue of 
human volition called teleology.  
However, due to the fact that God’s Self-creative freedom is distributed over the 
universe, i.e. His “Mind”, human volition arising within the universe is free to be locally 
out of sync with teleology.  This requires a set of compensation mechanisms which 
ensure that teleology remains globally valid despite the localized failure of any individual 
or species to behave consistently with it.  In part, these mechanisms determine the state 
of your relationship to God, i.e. your soul.  If you are in harmony with teleology – with 
the self-realization and self-expression of God – then your soul is in a state of grace.  If 
you are not, then your soul is in danger of interdiction by teleological mechanisms built 
into the structure of the universe.      
Q: What does cognition have to do with physics or math?  The laws of nature (physics) 
are not related with perception of those, I think.  Animals use those laws better than 
human without having a single idea what gravity or electricity is.  Math is abstract.  The 
laws of nature are discovered, not invented (I study psychology which from my point of 
view is not science for many reasons, maybe it will be some day).  If theories are mental 
constructs does that mean that gravity (not as a term) exists only as an abstract 
concept?
A: Abstract laws are more general than the concrete, physical matter-and-field systems 
that obey them.  If we divide reality into the concrete, and the abstract but non-concrete, 
math falls under the latter heading due to its generality.  That is, concrete physical 
reality exemplifies mathematics, but mathematics is not confined to any particular 
physical model; equivalently, the laws of physics are a mere subset of the laws of 
mathematics.  So mathematics inhabits a higher (or alternatively, more basic) level of 
reality than the material world.  
Since the human mind can reason both inductively (from the specific to the general) and 
deductively (from the general to the specific), it spans both levels.  Therefore, 
mathematics is mental as opposed to merely physical in nature.  Because, as we have 
just noted, the laws of physics are a mere subset of the laws of mathematics, and 

because (as you write) the laws of nature are discovered, not invented, physical reality 
is ultimately mental in character as well.  However, although this applies even to gravity, 
we are corporeally locked into a physical compartment of abstract mental reality within 
which we are not free to treat gravity as a mere “concept”.  This helps explain why we 
can’t fly by the power of thought alone.
Q: In the book "The Age of Spiritual Machines" Ray Kurzweil believes by 2029 humans 
will live among machines that convincingly claim they are self-aware.  How does the 
CTMU deal with non-biologic consciousness? 
A: Kurzweil’s prediction implies that within three decades from now, AI machines will be 
able to pass something called the Turing Test, a hypothetical scenario devised by the 
mathematician and seminal computer scientist Alan Turing.  To pass this test, a 
machine located behind a partition must convince a human interlocutor that it too is 
"human", i.e. that is possesses "human consciousness".  
Since the CTMU shows that a generalized form of self-awareness or consciousness 
distributes over every part of reality, any machine that exhibits self-awareness or 
explicitly claims to be "conscious" will to some extent be telling the truth (indeed, if your 
toaster could make such a claim right now, it too would be “telling the truth”, although 
not in a fully human sense).  On the other hand, it is not yet clear whether Kurzweil’s AI-
style machine consciousness, though ostensibly of a higher nature than that of your 
toaster, will be entirely human in character. 
CTMU and God
Q: Chris, what does your theory of reality, the Cognitive-Theoretic Model of the Universe 
(CTMU), say about God? 
A: The CTMU says that God, as embodied by the universe, Self-configures. To do this, He 
needs two things: (1) active sensors (agents, internal proxies) who can recognize and affect 
the state of the universe from local internal vantages; (2) a stratified utility function allowing 
Him and His agents to prefer one possible future over another. Human beings and other 
intelligent life forms are useful to God on both of these counts. Thus, the first criterion of His 
development is the possibility, and in fact the inevitability, of their existence. 
To understand this, consider an extraordinarily wise child responsible for the development 
and maintenance of its own body and physiology (because the universe is in the process of 
self-configuration, we can liken it to a child). To meet this responsibility, the child requires 
internal sensors that provide information on exactly what is happening deep inside its 
growing body, preferably at the intracellular level, and that permit feedback. The child 
further requires that these sensors be able to register the utility of what they detect... 
whether it is "good" or "bad" from their own local perspectives. That way, the child can 
weigh the perceptions and utilities of all of its internal sensors to form overall developmental 
goals. 

In order to meet the Self-configurative goals that it sets (as aggregates of the goals of its 
sensors), the child has the power to establish internal self-optimizative tendencies that 
affect the behavior of its internal agents, influencing them to perform such local operations 
and make such repairs as are necessary for the good of the whole child. To this end, they 
are equipped with global utility functions, "consciences", that combine with intelligence to 
make them responsive to the welfare of the whole organism (as opposed to their own 
individual welfares). 
For want of a better name, we can use the term "soul" to describe the channel through 
which individual and global utility functions are put in consistent mutual contact. This 
channel permits the sensors to make more informed, more global, and more valid 
judgments about what is "good" and what is "bad", and gives them the internal strength to 
do what is good even if it means sacrificing individual utility (because global utility 
is an aggregate function of individual utility, serving global utility ultimately makes 
individuals happier). 
Q: I do not see why perception of "the universe" necessitates a link between the perceiver 
and the perceived. The very fact that one object perceives another seems to necessitate a 
disunion between them. Can you clarify? - Joshua Freeman
A: Here's a little background on the logical structure of relations. Consider the relation x R y, 
meaning "object x relates to object y (in such and such a way)". R has two aspects, one 
predicating the sameness of x and y ("x and y share the property of being relatable by the 
relation R") and one predicating difference. In other words, although x and y are "different" 
(at least typographically), R represents a structured set to which x and y both belong, and to 
that extent, x and y share the same description ("are the same").
Now let's narrow the context. "Perception" refers to a mapping between the mind of the 
perceiver, which is by definition internal to the perceiver, and some aspect of "external" 
reality. This perceptual mapping relates the structure of mind to that of reality in a way 
amounting to mutual containment: while the mind perceives itself as part of the content of 
reality, reality is the content of perception. Because the perceptual mapping is a 
containment mapping, and a containment mapping predicates coincidence, perception 
predicates coincidence and is therefore a similarity mapping (homomorphism).
So the perceptual similarity mapping "Joshua R Universe", where R denotes perception and 
mutual containment, is an assertion of similarity regarding Joshua and the Universe. 
I hope this helps.
Q: In a vacuum, a train is traveling at the speed of light. The engineer turns on the front 
lantern. Does he see the light ahead of him (is the light traveling twice the speed of light) or 
does the light not escape the lantern? 
A: First, let’s change “a train is traveling at the speed of light” to the more correct “a train is 
traveling at almost the speed of light” (due to something called the “relativistic mass 
increase”, nothing made of matter can be accelerated to the speed of light; it gets too 
massive and is subject to too much inertia). According to Einstein’s Special 

Theory of Relativity, the engineer sees the light beam from his lantern race away from him 
at the speed of light in the direction of motion of the train. But a relatively stationary 
observer standing by the tracks sees the light beam inching slowly out in front of a very 
short, very fast train, again at the speed of light, and the engineer appears to be frozen. 
This is because the mutual perceptions of observers in relative motion conform to a 
mathematical function called the “Lorentz transformation”. Designed to make sure that the 
speed of light is absolutely the same for everybody, the Lorentz transformation can play 
weird tricks with space, time and common sense! 
Q: I just found out that I only got 125 for my first professional IQ test and I’m very 
disappointed. Should I be? Btw, I’m 16 years old.
A: First, 125 is a pretty high IQ score. Second, since anyone can have a bad day, that 125 
is not necessarily written in stone. Third, Richard Feynman, a Nobelist sometimes ranked 
with Einstein as one of the two greatest physicists of the 20th century, had just about the 
same IQ score as you do. Yet, his name was, and is, considered synonymous with 
genius (and take my word for it, people are not merely being kind to him in saying that). The 
score didn't bother him at all; in effect, he congratulated himself for being so brilliant in spite 
of his "low IQ".
Most important of all, Feynman is not the only example of this. Many powerful thinkers have 
not scored especially well on IQ tests, which measure only a certain range of intellectual 
performance factors. My advice would be to hold off on self-judgment until you figure out 
what your real intellectual strengths are.
Q: The following was given to me on 10/4/99 and to date I remain stumped. I sent it to 
Marilyn at Parade magazine but did not receive a reply. (I'm beginning to think that it really 
does not have an answer.) I am giving you the riddle in the exact manner in which I 
received it, in case there is a clue somewhere that appears to be only a typographical or 
grammatical error. Here it is: 
There are three words in the english language that end in "gry". ONE is angry and the other 
is hungry. EveryONE knows what the third ONE means and what it stands for. EveryONE 
uses them everyday, and if you listened very carefully, I've given you the third word. What is 
it? _____ gry? Please help!!!!!!
A: Congratulations for taking into account the possibility that the wording of the riddle might 
be critical to its solution. Here’s the riddle in its original form. Note the differences, which 
suggest that it may have been passed on to you by someone who got it wrong or wanted to 
mess with your mind.
"Think of words ending in -GRY. Angry and hungry are two of them. There are only three 
words in the English language. What is the third word? The word is something that 
everyone uses every day. If you have listened carefully, I have already told you what it is." 

When we restore the riddle to its original wording, it becomes clear that the first two 
sentences are for distraction purposes only. The conditions of solution don’t even begin until 
sentence number three! If you haven’t figured it out yet, the answer is “language”. That’s the 
third word in the phrase “the English language”, and it’s certainly “something that everyone 
uses every day”. In fact, we’re using it right now! 
However, I once used maugry, an obsolete word meaning spite, in a college paper. (It got 
red-inked.) 
A Dialogic Response to an Atheist
RFV:  The noun "reality" would seem to have a consensus definition as "all that exists" 
and the adjective "real" as a distinguishing quality of a particular thus-designated 
abstraction whose existence is deemed independent of the mind of the subject for whom 
the so-defined "object" *seems* to be *real*.  I won't bore you with Oxford English or 
Webster.
My ontological preference would be to set such definitive statements as well as those 
such as, "The *Universe* is real," apart from the Universe itself.  These would not be 
members of a commensurable set, mind you, but merely examples of an infinitude of 
inane assertions whose truth, falsehood, or undecidability involves logic applied to other 
statements of the same class.  There are many dumb assertions that do not even merit 
interment in *real* phrase cemeteries by *real* intellectuals whether true or not; others 
may be profound but to my mind similarly are totally *other* than the Universe itself. 
Homo Sapiens seem particularly vulnerable to the vanity of ennobling the blather of their 
vocal cords and cortical folds with some special meaning and in so doing confusing the 
obvious.  How else can one account for Ms. Campbell presuming that Carl Sagan should 
have been convinced by her flaw of reasoning involving the *existence* of "Love" being 
provable by anecdotes as somehow implying that, therefore, so must the existence of 
"God."  (See for example, my article "Does Love Exist?" on pages 13 and 14 of Gof #85.) 
An anecdote may of course embody the actuality of a relationship between *known* 
objects (*real* or otherwise), any one of which could then prove that such relationships 
*occur* -- even a misinterpretation of which might confirm the argued relation.  Is it too 
subtle to just affirm here, however, that *objects* differ in certain salient respects from 
such *relations*?  People other than perhaps Jesus (for whom antinomies were often 
claimed by the ecstatic Gnostic) *are not* "love" in the sense of *being* Love; no portion 
of anyone is *comprised of* love, and yet of course, we *do* love. It's a verb for Christ's 
sake!  Just so, the universe need not (nor ever can) *be* (nor yet *contain*) love nor any 
other relation mathematical or emotional in any *real* sense in order for the asserted 
relation to *occur*.  Is this heavy?  I don't think so.
One must start somewhere, of course, with merely *presumed* consensus meaning such 
as what is meant by "exist."  However, for a credible TOE (Theory Of Everything - if that is 
not in itself the most oxymoronic phrase that could ever be uttered), 
CML:  What we seem to have here is a misunderstanding of the nature and expressive 

power of language.  By the mathematical definition of a language, “Everything”, not to 
mention “Theory of Everything”, is a language.  That is, it is a string of characters that has 
a descriptive meaning, a semantic mapping into the universe of ideas and perceptions. 
Since a meaningful language can be construed as a theory, “Everything” is a theory. 
Of what is “Everything” a theory?  Well, let’s see here.  “Everything” by definition refers to 
everything, including…er, everything.  Therefore, “Everything” is a theory of everything. 
Granted, there are those who would say that a one-word theory is a poor theory indeed. 
But value judgments aside, where’s the oxymoron? 
The fact is, human minds have the power to formulate one-word characterizations of the 
universe (indeed, I just used a common one: “universe”).  Since (broadly speaking) “to 
characterize” is “to theorize about”, humans have the power to form one-word theories of 
reality.  The axioms and rules of inference of these microtheories may not be explicit, but 
they are theories nonetheless.  And that’s a good thing, since it can easily be shown that 
without this ability, humans could not formulate their customary long and involved theories 
about reality.  Since any complex theory can be characterized using a single word or 
phrase, e.g. “cosmology” or “quantum mechanics”, blocking this universal referential 
mechanism would effectively block theorization, and because “theorization” characterizes 
virtually all meaningful cognitive activity, human mentation in general.
Of course, such a blockage may already be in effect over in Fred’s neck of the theoretical 
woods.  If so, I suggest that he break out the old chainsaw and clear the road before 
making any more of his own attempts at theorization…especially regarding theories of 
mine with which he disagrees.
RFV:  …what constitutes "the real," i. e., "everything" that *is*, would seem to require 
some sort of minimal definitions with justifications for nontraditional varieties before we 
encounter sound bites such as:  "…because any real explanation of reality is contained in 
reality itself, reality gives rise to a paradox unless regarded as an inclusory self-mapping." 
And again, "…thus, we can speak of time and space as equivalent to cognition and 
information with respect to the invariant semantic relation processes, as in 'time 
processes space' and 'cognition processes information'. But when we define reality as a 
process, we must reformulate containment accordingly."  Say what?  "Time *processes* 
space?"  It is particularly important that definitions be as precise as possible when these 
seem to differ so considerably from what at least this reader would expect them to entail. 
CML:  Time is change of state, and change of state occurs according to laws of 
mathematics and physics.  The laws of mathematics and physics govern changes of 
spatial configurations of matter and energy.  Therefore, time incorporates laws according 
to which space is changed or “processed”.  That is, the present state of reality is fed into a 
black box or transducer labeled “laws of temporal transformation” which then returns the 
next state.  The box, AKA time, AKA nomology, is a temporal processor.  The structure of 
the box is the invariant structure of spacetime.  That is, spacetime is the history of the 
temporal processing of one spatial cross-section of reality into the next according to the 
mathematical and physical laws comprising the transductive syntax of the box.  Thus, 
time, described as a black box, processes space as input and output.

Is this precise enough?
RFV:  Chris Langan seems quite enchanted by the strange teleological loop of conceiving 
a universe structured in accordance with consistent *conceptions* where these 
conceptions are thereby deified in a *real* sense to the status of reality. 
CML:  “Deified”?  Talk about loose semantics.  Actually, Kant observed that “objective” 
phenomenal reality has a mental, i.e. conceptual, component a long time before I did.  He 
was right in the sense that insofar as the mind is that which perceives and theorizes, and 
insofar as there can be no perception or theorization without the mind, the structure of the 
mind necessarily intersects with that of phenomenal reality.  On the categorical level, this 
intersection is total; there is no phenomenal disjunction.  That’s because by definition, 
perceptual reality exists entirely within the intersect.
Kant then went on to hedge his bets by postulating the existence of a really, really 
objective “noumenal” reality that exists independently of the mind.  However, he did so 
completely on faith and in violation of logic.  For the only reality we can ever know by 
perception or cognition is phenomenal, and anything “independent” of this phenomenal 
reality is by definition just that: independent of it, i.e. related to it by no causal mapping. 
In fact, as defined by Kant, pure objective reality is so utterly independent of phenomenal 
reality that it cannot be considered a part of the same reality at all.  Our minds filter it out 
because its total logical disconnection from our perceptual universe severs any possible 
perceptual or relational mechanism that might connect it to us. 
Fortunately, dropping Kant’s noumenon fallacy leaves the sounder parts of his 
philosophy, those relating to phenomena, more or less intact.  One then has a platform 
from which to reason about deep reality in such a way that no matter how many layers of 
“illusion” lie between it and our awareness, we remain logically connected to it.
RFV:  There is definitely a worthy enchantment here - who, for example, has not in 
reverie mused concerning the universe in its entirety being constrained by necessity to 
act in ways determined by the evident relations of its humblest components and that the 
descriptions of these relations are somehow of extreme significance beyond the mere 
words and equations used to formulate them.  However, in Chris's scheme the *reality* of 
the very words in such statements seems to be guaranteed (if they happen to be true 
statements), whereas similar, if ultimately non-confirmed statements would seem to be 
denied the same twinkling with stardust of the *real* word hall-of-fame. 
CML:  But that’s not quite what I’ve been saying, is it?  What I’ve actually been saying is 
that our descriptions of “the evident relations of (the) humblest components (of the 
universe)” are “guaranteed” only on the syntactic level.  That is, they do not qualify as 
perceptual input unless they conform to our mental structures - our “cognitive and 
perceptual syntax” - and this implies that all perceptual input conforms to the laws of 
mathematics and physics as they exist implicitly in our minds.  That’s what I mean when I 
equate mind and reality on the syntactic level; particular theoretical languages can be 
wrong, but the basic syntactic descriptions of their perceptual contents are true in any 
case.  Indeed, they comprise the basis of perceptual reality.  Since the core of the CTMU 
embodies the tightest implications of the most general logic, it qualifies as “cognitive and 

perceptual syntax”.
RFV:  Thus, true descriptions "exist" as a structure of the universe itself and are, 
therefore, available to be "perceived" in an inverse mapping whereby sentient 
constituents of the universe may become attuned to the whole enchilada in accordance 
with their intellectual abilities - this "ultimately giving rise to human perception."  Where in 
turn, "the set of 'laws' obeyed by the universe is just a minimal set of logical relations that 
suffices to make these perceptions noncontradictory."  Of course we can counter such 
statements - whether willfully or not - but perhaps in so doing we thereby exile ourselves 
forever (past and future) from reality and the bliss of the presence of the only true and 
almighty God of King James, "S." 
CML:  “Perhaps”, indeed.  Actually, the theological definition hereto referred as “the God 
of King James” needs a little revision.  But with that understanding, Fred’s theological 
insight will do just nicely!  
RFV:  There are many problems with this, of course.  One of which is that in this scheme 
it would seem to be the intellectually challenged who are the ones endowed with special 
imaginations capable of envisioning truly original ideas outside the mind of God and that 
do not, therefore, correspond to (or even exist in) Chris's *reality*.  They either do not 
exist at all or only *seem* to exist in the surrealistic idyllic limbo of we *idiots* who 
conceive them. 
CML:  Say what?  Where did Fred come up with this?  In fact, since our minds, and every 
thought they contain, do indeed exist in reality, this makes no sense.  I think that Fred’s 
confusion may devolve to the possibility of thinking not just original ideas, but fantastic 
ones as well.  If so, I assure Fred that a functional brain scan would readily reveal that 
such thoughts are quite real within the brains of those who think them.  What is “irreal” is 
the assumed veridical mapping of personalized fantasies onto general (perceptual) 
reality. 
Again, we have the distinction between particular theoretic languages, which may indeed 
be fantastic, and the basic syntactic relations of which they are composed. With respect 
to ordinary theories, the map is not the terrain; with respect to cognitive and perceptual 
syntax, it sure as hell is, right up to isomorphism. (By the way, "up to isomorphism" is not 
an escape clause.  Beyond the maximum scope and resolution at which the real universe 
can internally sustain a syntax ßà content, mind ßà reality, cognition ßà information, time 
ßà space isomorphism, there is no more reality; the isomorphism is the entire basis of the 
coherence of the real universe!  This should readily become apparent to anyone who 
tries, logically of course, to find an alternative basis.)
But something tells me that I’ve explained all this to Fred before. 
RFV:  Another problem is the basic one involving what constitutes a perception.  My 
seeing a pixel of green via rods, cones and neurons exterior to my brain certainly is a far 
cry from my having contemplated a *description* of a corresponding co-reality and further 
yet from a cognized statement of a law of physics.  What is obvious about the usual 
definition of perception is that it *exists* as the most basic element of reality we know and 
yet here its very existence seems to hang in the balance of compatibility with some all-

inclusive abstraction.  The whole perception of the reality of "perceptions" seems to have 
been hijacked by some contortion of thinking to depend upon a level of reality where 
poodles and noodles argue differences and similarities rather than just *being*.  (See 
Nicholas Bucska's article, "Meta-Questions and mega fallacies" in issue #88 pp.25 -- 26, 
where he says:  
"In nature there is a poodle and a noodle but there is no such thing as a difference 
between them:  the difference is the mental result of an arbitrary mental process 
performed on abstract objects: our mental images created from our sensory data of real 
objects."
CML:  Uh...with all due respect to Nicholas, I’d have to maintain that in nature, there both 
IS and IS NOT a difference between a poodle and a noodle.  That there IS a difference is 
evident from the fact that nature gives one the power to bark and chew bones, but not the 
other. That there is NOT a difference is evident (e.g.) from the fact that both have mass 
and are therefore composed of energy.  But this “paradox” is resolved by recognizing that 
reality is stratified; things are “the same” with respect to shared generalities (“all matter 
consists of energy”), while they “differ” with respect to distinctive specifics (women perm 
their hair not to put noodles on their poodles, but to put poodles on their noodles). 
In order for perceptual reality to exist, this “mental” stratification must be built into the 
structure of spacetime.  If it were not, then spacetime and its contents would be totally 
imperceptible and therefore unreal. 
RFV:  Notwithstanding, Chris maintains that, "thus, every axiom and theorem of 
mathematics can be considered implicit in material syntax and potentially exemplified by 
an appropriate material pattern, e.g. a firing of cerebral neurons."  And again, "so the 
background has logical ascendancy over derivative matter, and this permits it to have 
aspects, like the power to enforce consistency, not expressible by localized interactions of 
compact material objects (i.e., within the bounds of materialism…)."  This platonic 
structure of reality, he clarifies by saying:  "…physical reality is embedded in a more 
general or 'abstract' ideal reality equating to the reality-syntax D(S), and the syntax D(S) 
is in turn embedded in physical reality.  Thus, if D(S) contains supraphysical components, 
they are embedded in S right along with their physical counterparts (indeed, this 
convention is already in restricted use in string theory and M-theory, where unseen higher 
dimensions get 'rolled up' to sub-Planck diameter),"  whatever that means.
CML:  The CTMU property to which Fred is referring here is called “hology”.  That’s a 
logical form of self-similarity in which the structure of an infocognitive system is distributed 
over the system as self-transductive syntax.  [An infocognitive system is a system whose 
components all simultaneously possess the nature of information and cognition. For 
example, you (the reader) are internally a cognitive entity, but others perceive you as an 
informational construct related to your external environment. In a generalized sense, the 
same applies to inanimate objects; they simultaneously possess intrinsic (subjective) and 
extrinsic (objective) descriptions that are respectively temporal and spatial in character. 
That is, the purely intrinsic description of an object, being utterly confined to the object, is 
a self-description and thus a recursive (temporal, sequential) process, while an extrinsic 
description specifies the position or meaning of an object relative to other objects and 

thus has a spatial (relative, parallelized) aspect.] 
As for rolled-up dimensions, that’s a convention that goes back to Kaluza-Klein theory, in 
which light is characterized as a “fifth-dimensional vibration”. Supposedly, this fifth 
dimension is imperceptible because it gets rolled up into a tiny little knot of sub-Planck 
diameter (string theory and M-theory have many more of these knotty little dimensions, 
the ravelling of which turns out to present some knotty topological problems).  But here, 
I’m just pointing out a convention of M-theory not necessarily shared by the CTMU.
RFV:  So clothed in the priestly garb of words we are now ready to address God.  I will 
reiterate the question with Chris:  "What does this say about God?"  With regard to Chris's 
answer, I find that reiteration does not suffice for me.  Where he says, "So by design, 
religious beliefs generally cannot be refuted by rational or empirical means," I think he 
may be intentionally playing on words.  Certainly one could never refute that there is 
*belief* in God as long as anyone still *believes* in God.  That is an irrefutable fact like the 
occurrence of love and other relations between humans and other objects whether *real* 
or not!
However, that key tenets of most "religious beliefs generally cannot be refuted by rational 
or empirical means" is an assertion that should probably be denied subjective honors of 
existence in his scheme if he is to remain consistent!  It is, in simple fact, absurd.
CML:  I think that Fred may be overlooking the important fact that while the existence of 
God is merely a “belief” to the adherents of traditional organized religions, it is more than 
a belief in the CTMU.  It is an unavoidable syntactic relation.  Simply using the term 
“belief” with regard to religious ideas does not imply that this is all they are, or that this is 
their only means of justification.  May I respectfully suggest that in the future, Fred 
acknowledge the discursive context instead of mangling it beyond all recognition?
RFV:  Going further he asks and answers the question, "Can a denial of God be refuted 
by rational or empirical means? The short answer is yes."  He proceeds by claiming that 
by his having merely associated the term "God" with the abstract level of true statements 
in his CTMU that he conceives as "structuring" the universe, that that structure 
"constitutes not just a logical framework for reality theory, but the outline of a logical proof 
of God's existence and the basis of a new 'logical Theology'."  To my mind God was no 
more marginalized by Gnostic abstraction in the Synoptic Gospel than by Chris Langan in 
his article, "Introduction to CTMU" that summarizes for neophytes his "Cognitive-
Theoretic Model of the Universe (CTMU)".  Nor has God's existence ever been 
conditioned on more theoretically questionable foundations. 
CML:  Fred is (of course) entitled to his opinion, ill-supported as it may be.  But even if 
he's unable to see that the CTMU is a proof of God’s existence, he must admit that either 
it is a new approach to same, or he doesn’t even come close to understanding it.  If he 
prefers, we can leave the success of this approach to the tests of time and logic.

Discussion with CTMU Conference Members
Marc Geddes: What does CTMU have to say about 'the purpose' of the universe (if there 
is one?) I understand that CTMU postulates an extension of the anthropic principle - could 
you please give a brief summary of this concept of "unbound telisis"
Chris Langan: The purpose of the universe is optimal self-actualization. This is called 
"teleology". Accordingly, the telelogical CTMU version of the Anthropic Principle is called 
the Telic Principle.
In logic, attribution (of a predicate to an object or relation) is called quantification. 
Quantifiers "bind" variables. Thus, when one regressively strips away the attributes of 
something in order to understand its essence, one "unbinds" it from its quantifiers. 
Suppose that in semantic consistency with the concept of teleology, we describe reality as 
"telesis". Then the limit of the process of unbinding reality from its quantifiers in order to 
understand its essence is "unbound telesis" or UBT.
 
MG: Is it really true that there is no distinction between 'knowledge and truth'?
CL: Knowledge is a noun, while truth can be either a noun or an attribute. However, 
considered as nouns, they are identical up to isomorphism. The latter phrase "up to 
isomorphism" leaves open the question of whether one is greater than the other. Insofar as 
some truths are unknowable to the human mind, truth is greater than (human) knowledge. 
But when we talk about "the universe's knowledge of itself", they are synonymous. In order 
to ascribe "knowledge" to the universe, we need an appropriate generalization of cognition. 
That's why the CTMU incorporates such a generalization.
MG: For instance, even if absolute truth exists, why does this have to imply that this 
absolute truth is knowable to human beings?
CL: It doesn't.
MG: Even if CTMU is right and tautologies can yield useful information, we run into the 
problem of Godel's theorem and the limitations of formal systems again ie. How can we 
ever be absolutely certain that there are no mistakes in our reasoning?
CL: First, if tautologies do not yield useful information, then logic does not yield useful 
information. If you believe this, then the first thing you should do is throw out your "useless" 
personal computer, which was designed according to the tautological principles of Boolean 
logic. (And that's only the beginning.)
Second, we can be absolutely certain that there are no mistakes in our reasoning by using 
logic itself as the universal template for knowledge and reasoning. That's why the CTMU is 
designed as a "supertautological" extension of logic, and why it is formed by the 
straightforward adjunction to logic of certain metalogical necessities of cognition and 

perception.
 
MG: Does CTMU really rigorously show that 'reality' is entirely 'self creating and self-
describing' or is the use of the tautology simply faulty? For instance if reality is 'x' , why in 
plain English, can we really be sure that 'x' has any explanation at all?
CL: Because in this usage, "explanation" is identical to "structure". In order to fully specify 
the structure of a system, one must explain why its aspects and components are related in 
certain ways (as opposed to other possible ways). If one cannot explain this, then one is 
unable to determine the truth values of certain higher-order relations without which 
structure cannot be fully specified. On the other hand, if one claims that some of these 
higher-order structural components are "absolutely inexplicable", then one is saying that 
they do not exist, and thus that the systemic structure is absolutely incomplete. Since this 
would destroy the system's identity, its stability, and its ability to function, it is belied by the 
system's very existence.
MG: CTMU tries to argue that we should say 'x exists because x exists' but this seems 
uncomfortably like a semantic 'trick' I am still not wholly convinced that a tautology can 
yield useful information and that reality has an explanation.
CL: See above comments. Incidentally, the sentence 'x exists because x exists' is simply 
an expression of self-containment with respect to causality, and therefore necessary in any 
self-contained theory. Obviously, a comprehensive theory of reality must be self-contained 
by definition. Indeed, so must reality itself.
MG: Why is there a large school of top class logicians who still totally deny that a tautology 
can yield useful information?
CL: Let me guess. Because they are hiding in ivory towers in a weak and cowardly attempt 
to protect themselves and their absurd worldviews from anyone who might know better? ;-)
MG: The oldest debate in philosophy! To what extent do things have an existence which is 
platonic? (Platonic= Timeless, Universal, Objective).
CL: Well, which is it? Is Platonic = timeless, universal, or objective?
Say that Platonic = timeless. Then the answer to this question is entirely dependent on 
one's model of time. With respect to the advanced model of spacetime incorporated by the 
CTMU, the universe is ultimately timeless and therefore Platonic.
Say that Platonic = universal. With respect to itself, the real universe is indeed "universal". 
So again, the universe is Platonic.
Finally, say that Platonic = objective. The latter term refers to an item of shared perception 
located outside the self, i.e. outside the subjective selves of the perceivers. Because the 

universe is ultimately its own perceiver, and because it is self-contained, the answer is now 
"No, the universe is not Platonic. It is subjective, or better yet, metasubjective."
MG: For instance should we consider the entities referred to by scientific theories as simply 
calculational devices or as 'real things' existing 'out there'?
CL: Algebraically, theories are languages. Languages have syntax. At the syntactic limit, 
theory = universe; the syntax controls the process of cognitive theorization as well as the 
process of perception through which the universe is apprehended. Where theory = 
universe, the "or" in your question is not exclusive.
MG: What about mathematical concepts - are mathematical concepts 'things' which exist 
independently of the human mind or not?
CL: Mathematics is the basis of our cognitive and perceptual syntax. So again, the "or" is 
not exclusive.
MG: What about moral principles? Should we take a 'utilitarian' view that moral principles 
are simply human inventions useful for specific social purposes, or is there more to them 
than that?
CL: The global limit of purpose is teleology, the purpose of the universe as a whole. Since 
morality is a function of purpose and the justification of purpose, the limit of morality is the 
teleological "morality" of the universe as a whole. Obviously, where morality is defined with 
respect to telelogy, it is as absolute as teleology itself.
MG: If moral principles have some sort of 'real' existence, how could we use our reason to 
discover what they are?
CL: It's really very simple. Start with the utility of the universe, as embodied in telelology. 
Then parse this utility according to the structure of the universe.
 
MG: Needs clarifying. CTMU appears to show that the universe is a self-perceptual system 
and thus there exists a 'supreme mind' . But what specific properties does this mind have?
CL: Omniscience, omnipresence, and even an attribute resembling omnipotence...but only 
as interpreted within the CTMU, as opposed to the nonsensical way these terms are 
defined and interpreted by mainstream theologians and their critics.
MG: The existence of a 'Universal Mind' does not necessarily imply the God concept as 
understand by conventional religion. (In conventional religion God is a supernatural, omni 
potent being, In CTMU, God appears to be neither supernatural nor omni potent)

CL: In the CTMU, God is quite "natural". But where this term is confused with "material" or 
"merely physical", God has a supernatural aspect as well. In other words, where reality is 
like a Mind, "God" is the answer to the question "Whose mind?", and this answer is more 
than merely physical in content.
 
MG: A brief description of SCSPL please!
CL: An SCSPL is a completely self-contained language (it writes itself, reads itself, 
interprets itself and executes itself in the computational sense).
MG: The whole idea of 'extending set theory' to resolve the paradox of the set of all sets 
needs clarifying.
CL: Already done.
MG: Is SCSPL really a 'formal system' at all or is it something over and above this?
CL: You can think of it as the extensional limit of a formal system. Bear in mind that the 
underlying logic of most of the formal systems dealt with in math and science is just first or 
second order predicate logic. There are no "attributes of attributes...of attributes". If you 
want to scale the Everest of metaphysics, you have to deal (at least in principle) with *all* 
orders of underlying logic. Since "all" may in this case refer to an infinite stratification, this 
requirement amounts to metalogic.
MG: V. Intriguing Chris's earlier discussion in the archives where he talked about the 
universe unfolding being analogous to a formal system SCSPL which is in the process of 
'appending' Godel assertions to itself to continously expand itself! And the possibility of 
God being able to ensure completeness and closure in one fell swoop. These things are 
definitly outside the scope of any ordinary formal system! (As first defined by Church-
Turing)
CL: Yes. But the formal system required by metaphysical reasoning is far deeper and more 
powerful than those currently required by mainstream math and science, where only 
object-level descriptions are necessary. Where (as in metaphysics) a formal system is 
required to incorporate descriptions of descriptions...of descriptions of itself, provisions 
must be made for all orders of relationship. This requires metalogical rules like those of the 
CTMU.
Conveniently for the theorist, metalogic is implicit in logic.
 
MG: I'd like a quick summary of the model of space-time that CTMU leads to. And also a 

summary of why you think that CTMU implies a non-zero cosmological constant leading to 
an accelerating universe? Can CTMU tell us how the universe will end? How it begun? 
Why is there an 'arrow of time?
CL: Because of the condition that reality must evolve in a way parallel to the logical 
evolution of the theory that describes it - that is, by logical substitution - the CTMU depicts 
reality in terms of a logical nesting of states. The operative model, USRE (Universe as a 
Self-Representational Entity), is the "conspansive dual" of the ERSU (Expanding Rubber-
Sheet Universe) model of standard cosmology.
In ERSU, local space and time scales remain constant while the universe expands. In 
USRE, local space and time scales shrink while the universe remains constant in extent, 
and do so in a way that can look from a local, internal vantage like accelerating cosmic 
expansion.
By the way in which it internalizes space and time scales, USRE supports certain 
distinctions between the forward and reverse directions of time that ERSU does not 
support; these explain the arrow of time. But by the same token, USRE says that the 
universe will not end until its self-actualization is complete, and that if and when this 
occurs, it will occur (from a global perspective) at the precise instant that the universe 
began to evolve in the first place. More precisely, concepts like the "beginning" and "end" 
of the universe are meaningful only when interpreted within the universe itself, which is 
actually timeless from a global perspective.
For more details, please consult some of the material at the Mega Foundation site...for 
example, the long article I submitted to the last issue of Noesis-E.
Regarding Larens' link to some of the work of Vladimir Trifunov, Trifunov seems to be 
working strictly within the standard ERSU model of cosmology. So his assertions must be 
distinguished from those of the CTMU, which are made with respect to USRE.
 
MG: I'd like a quick summary of how CTMU deals with the paradoxs of quantum physics. 
What 'picture of reality' does CTMU present of the quantum world? (as compared, for 
example, to interpretations such as the 'Many World's Interpretation' What about the 
apparent contradiction with relativity caused by non-locality? What picture of reality would 
CTMU present to try to resolve these contradictions?)
CL: The CTMU is the only logically consistent interpretation of quantum mechanics. To 
begin to see why, please read the article "Supernovae and the CTMU" in the May-June 
issue of Noesis-E.
 
Ingvar: I think it is almost impossible to show anything metaphysical at all about the
world as we know it using logic or math alone.
CML: In fact, Ingvar, logic and math are the *only* ways to establish universal truths. There 

are two reasons for this. (1) Empirical knowledge is always local and never global. 
Concisely, the universe is spatially and temporally too big to observe in its entirety, and it 
breaks down into pieces too small to see clearly. Due to the problem of induction - the fact 
that we cannot reason infallibly from the part to the whole in the empirical realm - 
observation and experimentation can never take us to the global limit. (2) To know 
anything, we must know it with our minds. But our minds obey a "cognitive syntax" 
composed of the invariant logical and mathematical rules governing cognition and 
perception. What is the ultimate generalization of these rules? Cognition and perception as 
a whole, i.e. cognitive and perceptual reality. Since we know reality solely through cognition 
and perception, this amounts to reality per se. It follows that we can use these logical and 
mathematical rules to generalize from the mind to the universe, and thus to solve the 
problem of induction.
Ingvar: For example, there are troubles with the paradox "the Cantor's set" and the set 
theory Chris uses.
CML: I don't use set theory, Ingvar. As Russell has pointed out in this thread, I use an 
extension of set theory called SCSPL, with respect to which the concept of inclusion or 
containment is a duality. Containment is a dual-aspect affair that amounts to a duality 
relationship between descriptive and topological aspects: "The universe (or set of all sets) 
topologically contains the syntax that descriptively contains the universe (or set of all sets) 
and its powerset." The set-of-all-sets paradox is thereby resolved, allowing us to achieve a 
self-contained formulation of the structure of reality.
Ingvar: The paradox does not show up if you use a set theory that differentiates between 
different types of set hierarchies (see Berty Russell's theory of types). On the other hand, if 
you use a theory that allow a paradox to exist, the theory will be useless, since it will be
possible to imply any proposition within it.  So either way, it doesn't seem to work the
way Chris want it to.
CML: To prevent the paradox from "showing up", it is sufficient to extend set theory with 
the inclusive duality concept just described. When you do this, you end up with SCSPL.
Ingvar: Still, I think the CTMU (the little I know about it) is an interesting theory.
Yes Russell, I agree it looks very promising, to be able to say that much with the CTMU 
using no empirical data at all. But, as I said before, it is too promising. That is, you can 
show too much with it.
CML: No, you can't. It is true that at the current stage of development of the CTMU, you 
can't use it to demonstrate every empirical truth. But neither can you formulate an 
irresolvable paradox within it. In fact, it contains a metalogical axiom called MU which 
states, in effect, that SCSPL can always be intensionally extended to prevent the existence 
of such a paradox. This is self-evident insofar as reality would be inconsistent if it were not 
true, whereas our ability to perceive reality shows that it is in fact consistent.

Ingvar: If the paradox leads to a contradiction, and it is allowed in the theory, then the 
whole CTMU, and everything you implies from it is useless. (I don't think there is a way of 
refuting the refutation that I came up with, but I welcome any tries :)
CML: See above. The theory of sets is extended by adjoining the concept of dual-aspect 
containment, which is formally effected by the adjunction of certain metalogical axioms 
supporting this concept.
Ingvar: For examples of different ways of dealing with truth, logic, and paradoxes, see
Stephen Read's "Thinking about logic".  It's an excellent book, for beginners as well as 
experts.
CML: Thanks for the tip, Ingvar. I'm sure some of us would enjoy reading such a book.
 
Cliff Wigtil: I would like to see, at some point, a discussion of whether a TOE or GUT 
could directly assimilate of a psyche/matter continuum, as ventured by Jung et al, that 
(might) explain occultic magick, synchronicities, UFOs, poltergeist/telekinetic phenomena, 
bilocation, etc.
CL: Since those who believe in these phenomena claim that they possess (at least) 
metaphysical reality, metaphysics is the natural realm of discourse in which to analyze 
them. The CTMU is an optimum theory of metaphysics (because it is based on cognitive 
and perceptual syntax, beyond which nothing can be known or perceived). So as we might 
expect, the CTMU supports possible definitions and realizations of at least some of these 
terms.
To put it as simply as possible, the CTMU shows that the universe is a Mind. Like any other 
mind, this Mind has certain powers of association and analogy. The means by which these 
functions are implemented include something called, suggestively enough, the "ESP" 
(short for Extended Superposition Principle). This refers to a global analogue of quantum 
mechanical state superposition, but acting over the global wave function with respect to 
space and time. The ESP relates to the teleological collapse of the global wave function as 
the SP relates to quantum wave function collapse in material entities.
CW: As far as I know, I have yet to see anything tangible that explains these "paranormal" 
events, with the exception of Bell's theorem, although clearly, it just scratches the 
surface. :)
CL: You know, there used to be a problem with Bell's Theorem: it was impossible to 
interpret in the standard model of spacetime. But with the advent of the CTMU, this 
problem no longer exists. So if you really value a logical and scientific approach to these 
issues, the CTMU will become your model of necessity.

CW: Nice to see the exchanges developing!
CL: I very much agree, Cliff!
Discussion on the Ultranet List
Russell: My question is not about whether the universe is conspanding or expanding, 
but how can nothing be "outside" the universe? Because if nothing is defined as zero, 
then how can "zero" contain or be outside something? Zero is a mathematical point; 
zero dimensional space.
Chris: The symbol for “nothingness” is 0, and 0 is a term that requires interpretation. 
For example, in the context of the integers, 0 represents a center or cancellation point 
between the positive and the negative, while in the context of the non-negative integers, 
it simply reflects absence. Moreover, 0 can apply to information, or to that which 
information determines. Now, 0 information corresponds at once to homogeneity and 
therefore to unity, and to unbounded potential and thus to infinity. So depending on how 
we interpret 0, “nothingness” can mean absence, cancellation, unity or infinity. 
This tells us that what is important is not the symbol 0, but its interpretation, and thus 
the entire cognitive matrix on which the interpretation is based. And within this matrix, 
nothingness is a complex interplay of, you guessed it, absence, cancellation, unity and 
infinity. It has *structure*. Within the cognitive matrix of our minds, the structure of the 
ultimate groundstate of existence called “nothingness” is UBT...and this is no mere 
“empty set”. 
Russell: The universe must be infinite in some way as to be "all there is". Un-reality 
could be hidden within reality?
Chris: No, unreality is ultimately contained in UBT, which for present purposes we might 
as well call "prereality". But every time a quantum wave function collapses, a new piece 
of "unreality" is defined (as the complement of that which is actualized by the collapse). 
In other words, just as reality is stratified, so is unreality. One level of unreality is 
unbound; other levels are bound by this or some other reality.
Russell: If un-reality and reality are distinctly separated, then what contains them both? 
Chris: UBT, and at derivative levels, the stratified constraints in terms of which reality 
defines itself.
Russell: Reality must contain un-reality. Reality is infinite.
Chris: Of course, if by "reality" you mean UBT. But *this* reality that we inhabit, 
characterized as it is by specific laws and material configurations, is self-contained on a 
secondary level of existence and therefore "less infinite". Fortunately, it is still capable of 

"containing" as much unreality as it needs to in order to define its own configuration. 
The logical vessel of containment is the cosmic wave function, which collapses to a 
single actualization and leaves the rest of its potential unrealized and therefore "unreal".
Russell: This seems to be an absurd circular question, but at the sub microscopic 
scale, nature is absurd.
Chris: Not exactly. What's absurd are the *standard interpretations* of nature on the 
submicroscopic scale.
Russell: If "nothing" is undefined, then "outside" the universe is undefined. That is still 
an asymptote approaching infinity?
Chris: Cosmologically, "outside the universe" simply means "unbound by the 
characteristic constraints of the universe". The exclusion is logical rather than 
topological. Remember, while the size of the universe is externally undefined (due to the 
absence of a real external metric in which to define it), its size and laws are internally 
(intrinsically) definite. 
Ian: Gina sent me your comments below on zero. The answer to Russell's question 
might be that if the sum of components within the space-time manifold (STM) equal 
zero, and the STM is surrounded by zero, it is simply surrounded by itself, and thus is 
not surrounded by anything mathematically different. Self surrounding Self simply = Self. 
It's perfect!
Chris: Hi, Ian! 
To some extent, I'm in agreement with your reasoning (as I told Johnny Asia and others 
here quite a while back, I find your own reality-theoretic ideas surprisingly harmonious 
with my own :-). Specifically, if we assume that cosmogony begins with something 
called UBT (unbound telesis, in CTMU lingo), the cosmos must inherit the nature of 
UBT...must, as you put it, be "surrounded by itself" in the logical sense. Setting 0 equal 
to UBT gets us around a problem: cancellation is not an operation that can be taken for 
granted. 
Let me explain. If you really start out with nothing, then there is no preexisting 
information. You must first tell cancellation how to occur, and tell zero how to "fission" 
into two or more mutually cancelling entities. Since these "instructions" on how things 
should cancel consist of constraint - of restrictions that limit the nature of "nothing" - 
constraint has logical priority over cancellation. So we have to explain how the 
constraint got there in the first place, and we have to do it without appealing to any 
preexisting cancellation operation supposedly inherent in "nothing". 
By default, the entities comprising the system that you call "the sum of components 
within the space-time manifold" must themselves combine to form the constraint, and 
since neither the constraint nor its constituents equal "nothing", we're forced to introduce 
"something" at the very outset. This tells us that we're not really starting out with 
"nothing", but with unbound potential or UBT... something which, by its nature, contains 

everything that can 
possibly exist. This amounts to interpreting 0 as "0 information" and replacing it with a 
combination of unity and infinity.
I.e., by interpreting 0 as the absence of something called "information", which implies 
that it equates to superficially unrestricted homogeneous (and therefore unitary) 
informational potential, which in turn equates to existential infinity, we're making it 
possible for 0 to represent the sort of cancellation point you envision. Since this implies 
that existence is a self-defined attribute - after all, there is no preexisting informational 
framework capable of defining it - only possibilities with the intrinsic ability to define and 
sustain their own existences can be actualized. This is what it takes to get "something" 
from "nothing".
The moral of this story is that numbers, even 0, are concepts which cannot be 
symbolically isolated within our cognitive matrix. Every time we use them, we implicitly 
attach the entire load of conceptual baggage in their extended mathematical definitions, 
and every time we apply them, we're selecting an interpretation consistent therewith. So 
in using them to (e.g.) do cosmology, we must attend to every aspect of their meanings. 
Otherwise, they'll almost certainly prove inadequate to do the explanatory jobs for which 
we intend them.
(Hey, they don't call it the "Cognitive-Theoretic Model of the Universe" for nothing!)
Ian: That's what's so beautiful about a zero-sum universe, it can appear and disappear 
without violating a nothing (no space-time) from which it arose, and in effect is another 
face of no space-time. A contradiction cannot be shown by raising a no space-time as 
Russell's question purports to do. I see a direct relation between consciousness and 
zero, although I've not tried to map that out. In my last post I site an important major 
study of the universe that finds that net universal energy equals zero. I intuitively 
deduced this out before these findings and before I found out that it was known that if 
the universe is closed then net energy equals zero. These findings say to me zero is 
definitely on the right track.
Chris: I agree. It's just that we have to be careful how we interpret "0"!
Ian: Chris, your observations on the many ways of seeing zero is great. I wonder if any 
other number has such a wide range of interpretations. Still absorbing the CTMU. The 
nonstop rigor of your work Chris is an inspiration for me to expand my own field of 
knowledge.
Thanks guys!
Chris: You're very welcome, Ian. We feel the same way about you! And thank you from 
all of us for sharing your fine investigative work with us in Ubiquity and Noesis-E. It's 
some of the best I've ever read, and has an unmistakable ring of truth. ;-)
Best Regards,

Chris
Discussion with a Mega Society Easy Member
GUY: I don't know if any of the following has anything to do with the concept of 
"conspansion" or not, but these thoughts grew out of the recent set of e-mails 
regarding conspansion, T-duality, etc.
Question: Does the universe have some kind of absolute scale (e.g., a fundamental 
length scale)?
Initial answer: Of course! There's the Planck length, the Hubble constant, and many, 
many others. For example, if I am 1.7 meters tall, this "size" is absolutely determined 
by its relation to the fundamental scales at the microscopic level and the cosmological 
level. These fundamental scales are provided by the universe itself.
Chris: Good point. These constants, along with their dimensions, define invariant 
relationships among the contents of the universe. But while the overall size of the 
universe is externally undefined and can only be defined intrinsically (as curvature), 
the sizes of objects change with respect to this curvature. Because our experiments 
indicate that nothing is changing locally, we impute this to global rather than local 
change:
“the cosmos is expanding (relative to our local experimental framework)”. 
Mathematically, we represent this in terms of a geometric model called ERSU, short 
for “Expanding Rubber Sheet Universe”, in which points retain their positions with 
respect to a comoving coordinate system painted on the 2D surface of an inflating 
balloon (the lost dimension is considered expendable for modeling purposes).
But the cosmos can’t be expanding in any absolute sense, because there’s nothing for 
it to be expanding into. Therefore, we must invert the model in a way that “conserves 
spacetime”; the total “amount” of spacetime must remain constant. When we do so, 
the cosmos ceases to resemble a balloon inflating (extending outward) over time, and 
instead becomes an inward superposition of sequentially related states. The best way 
to think of it
is in terms of a cumulative embedment of Venn diagrams (of state) on the inside 
surface of a sphere of extrinsically indeterminate size. As matter shrinks relative to the 
sphere, time speeds up. In Eddington’s words, “smaller and smaller, faster and 
faster…”. Eddington’s phrase for his primitive glimpse of conspansion was the “Theory 
of the Shrinking Atom”.
As it turns out, the model implies temporal superposition, requiring an ex post facto 
“parallelization” of consequent events with respect to antecedent events. The CTMU 
calls this the ESP (for “Extended Superposition Principle”) and adjoins it to quantum 
mechanics as an extension of the Superposition Principle. It permits temporal closure, 
something conspicuously lacking in the standard model.
[From a later email by Guy]

Chris: Nothing that leads to logical inconsistency is "confirmed by 
evidence". Expansion leads to logical inconsistency analytically. To 
wit, if there were something outside reality that were sufficiently real 
to contain the "expansion" of reality, it would be contained in reality. 
That's a contradiction; ergo, the hypothesis is false.
GUY: Chris, I am confused by this statement. Expansion can be defined consistently 
without reference to some external space in which to expand. For example, the 2-
sphere is defined by it's (constant) curvature and it's total surface area. 
Mathematically, its easy to increase its surface area. There's no contradiction.
Chris: The contradiction results from the concept of local invariance…the idea that 
because we can’t detect any change in the local metric by experimental means, it must 
be invariant in some “absolute” sense. The contradiction is tacit, but it exists 
nonetheless. The contradiction also surfaces with respect to our concept of motion. In 
the expansive model,
photons and planets alike move by rotation and translation through a background 
called “space”, which, while possessing sophisticated properties like Lorentz 
invariance, is still considered an independent medium containing particles and fields. 
In the conspansive model, on the other hand, motion is an endomorphism defined with 
respect to those nested Venn diagrams we mentioned, with outer circles playing the 
part of quantum wavefunctions with respect to inner ones, and inner ones playing the 
role of “moving particles” with respect to outer ones.
Because the circles in the Venn diagrams are just lightcone cross-sections 
corresponding to wavefunctions – because of the conspansive identification of 
lightcones with wave functions and worldliness with particle trajectories – the CTMU 
defines a general relationship between relativity and quantum mechanics. We end up 
with two kinds of motion: a primary
“inner expansive” kind representing the growth of Venn diagrams and the 
interpenetration of their spherical wave functions, and a secondary “requantizative” 
kind representing the relative shifting of diagrams as particles and galaxies undergo 
“incidental” spatial rotation and translation. But this implies a logical distinction; 
simplistically, the first kind of motion characterizes a “simulative” phase of matter, and 
the second kind characterizes a “simulated” phase. Time, calibrated in terms of 
primary motion, is a process of dualization between these logically distinct levels. The 
universe becomes a “self-simulation”...a
self-representational entity.
GUY: You make a similar statement in the M&F article:
"The idea that you have a singularity that explodes and expands . . . what does it 
expand into? Thank about that for a minute. If there was something there for that 
universe to expand into, now you're obligated to explain what that something is." [p. 
269]
 
The metric properties of a manifold are describable intrinsically, without reference to 
any larger space the manifold is embedded in. And this description is completely 
consistent. This is what Gauss realized in his amazing *Theorema Egregium*. I was 

quite surprised by this the first time I worked through the proof. It's amazing that 
something like curvature can
be defined without any reference to what a manifold is "curving into." Similarly, 
expansion can be defined in a consistent manner without the need for anything to 
"expand into." A space in which to embed the manifold is not necessary.
I'm sure you must know this stuff, Chris. What am I missing? (This is a sincere 
request!) I really don't see the contradiction or paradox here. 
Chris: As I think I’ve explained above, you’re missing (1) the fact that ignoring extrinsic 
considerations by resorting to intrinsic definitions does not really dispose of them, and 
(2) some of the implications of inverting ERSU to USRE...of dualizing the Expanding 
Rubber Sheet Universe to its conspansive dual model, the “Universe as a Self-
Representational
Entity”. In the General Relativistic version of ERSU, time is just another linear 
dimension in which the curvature of the universe progressively changes. In USRE, on 
the other hand, time is the wave-particle dualization of space to matter (to space... and 
so on), with cumulative parallelization of time.
Again, the ramifications are significant. We have two sets of invariants, one associated 
with the covariance of distributed physical laws (including all those dimensional 
constants relating matter) and one associated with the intrinsic invariance of the whole 
(including the conservation of space and the “cumulative present” in which the 
universe evolves). For practical purposes, the Standard Model ignores the second set 
entirely. In order to
maintain  both sets of invariants, we need to change our understanding of the 
relationship of space, time and object. Spacetime becomes an “SCSPL” evolving 
endomorphically, by internal self-mapping, rather than by the motion of matter through 
an independent medium. Though nothing fancy, this paradigm shift turns out to be 
philosophically profound. I’ve already written papers on conspansion...they were 
originally published in Noesis.
[From another email by Guy]
GUY: In a previous post I demonstrated operationally how to take a *passive* 
(changing only the description of a physical system) change of units and produce an 
*active* (changing the physical system itself) physical rescaling of the universe, and 
demonstrated that this rescaled universe is isomorphic to the universe you began with. 
From inside, you
can't tell whether you're in Universe A or Universe B.
Most scientists would say that, since the difference is not measurable, the concept has 
no meaning or is merely metaphysical.
I disagree. This result is interesting. I think it's clear that Universe A and Universe B 
are different (though isomorphic). One way to see this would be to take a meter stick 
from Universe B and put it in Universe A, where it would be 100 times as long. But the 
concept of moving objects between universes lacks coherence. A better way to see 

that these two
universes really are different is to note that you can go from Universe A to Universe B 
in independent steps: (1) rescale space, (2) rescale mass, (3) rescale charge, and (4) 
rescale the laws of physics (which are just relationships between the primitives of 
space, time, mass, charge, and maybe a few others). Each of these independent 
steps, if conducted without the others, would yield observable results. It's hard for me 
to believe, even if the final step results in a universe isomorphic to where you started, 
that Universe B is identical to Universe A.
I find this result fascinating. Does that mean I've gone off the deep end??
Chris: Or maybe it’s just CTMU time for Guy Fogleman. [Universe A and B would 
appear to be the same in some respects and different in others. But this can be 
asserted *only* with respect to a basis of comparison, in this case arithmetic and 
geometry, which are brought to bear as parts of the cognitive syntax of one Guy 
Fogleman in the throes of a gedankenexperiment. But the conclusions drawn from this 
syntax are valid only if it
characterizes not just the thought processes of Guy Fogleman, but a joint medium 
containing both universes...i.e., only if arithmetic and geometry are distributed over an 
objective medium containing the compared entities. What infrastructure does this 
imply? First, it requires a connection between the cognitive syntax of Guy Fogleman 
and the objective medium containing both universes…an identity of Guy’s mind and 
objective reality.
More generally, it requires a conspansive CTMU infrastructure with logical priority over 
the gedankenexperiment itself.]
At 02:40 PM 3/30/2001 , Langan wrote:
Chris: But the cosmos can't be expanding in any absolute sense, 
because there's nothing for it to be expanding into.
GUY: Again, I still don't see this.
Chris: There are two ways to describe things: extrinsically, according to a syntax or 
scale of measurement external to that which is being described, or intrinsically, using 
only the described entity itself (an intrinsic explanation is to this extent "self-
referential"). Because reality is self-contained, we ultimately require a global intrinsic 
explanation of it. If, in moving towards this goal, we happen to find a more or less self-
contained subtheory within the comprehensive theory we seek - as did Riemann with 
respect to geometry, and Einstein with respect to classical physics - we then have a 
*partial* intrinsic explanation. However, that doesn't mean we've finished the job. The 
intrinsic explanation achieved by General Relativity is in poor accord with certain 
aspects of an even more important physical theory, quantum mechanics, and therefore 
cannot be taken to imply that a global intrinsic explanation has been achieved (if it 
could, we'd already have a Unified Field Theory and more).
You say that you see no contradiction in the idea of an expanding universe. 
Apparently, you feel this way because differential geometry can be used to replace the 
concept of externally-measured extent with that of intrinsic curvature. But does this 
mean that the need for an external medium has been obviated? Hardly, unless (e.g.) 

the universe thereby
provides the wherewithal for its own existence...the stuff of which it is made. If you 
respond that the universe is made of energy, then you have to say where *that* came 
from. Suppose you say, "the positive energy is cancelled by negative energy as part of 
a true 0-sum system!" Unfortunately, that doesn't sound like the "0" most of us are 
familiar
with. Why not? Because you still have all that messy structure to account for...the 
informational framework within which "cancellation" is achieved. Intrinsic curvature 
alone isn't up to the job.   
Chris: Therefore, we must invert the model in a way that 
"conserves spacetime"; the total "amount" of spacetime must 
remain constant. When we do so, the cosmos ceases to resemble 
a balloon inflating (extending outward) over time, and instead 
becomes an inward superposition of sequentially related states. 
The best way to think of it is in terms of a cumulative embedment 
of Venn diagrams (of state) on the inside surface of a sphere of 
extrinsically indeterminate size. As matter shrinks relative to the 
sphere, time speeds up. In Eddington's words, "smaller and 
smaller, faster and faster". Eddington's phrase for his primitive 
glimpse of conspansion was the "Theory of the Shrinking Atom".
GUY: Do you have the Eddington reference handy? I have his "New Pathways in 
Science," but, in a cursory survey, couldn't find any similar thoughts in that book.
Chris: Try "The Expanding Universe", but prepare to be underwhelmed. Eddington, 
having found the Big Bang theory to be conceptually unsatisfactory, observed that it 
has an "inverse formulation": the Theory of the Shrinking Atom. He apparently worked 
out some of the rudimentary implications and noted as much in his book (total length 
of explanation:
1 or 2 pages). But he did so in a way calculated to make other mainstream 
cosmologists think he was joking...no doubt to protect his professional reputation, 
which might not have survived the incredulity of his colleagues. Indeed, he quickly 
dropped the subject and never (to my knowledge) returned to it.
I originally found the reference in "Cosmology" by E.R. Harrison and managed to track 
down the source material. But this was long after I'd already arrived at the idea of 
conspansion in connection with a seemingly unrelated problem: how to account for the 
strangely harmonious relationship of mathematics and physical observation. [For 
another kind
of "shrinking atom" theory, check out the Hoyle-Narlikar model, which places the 
shrinking atoms in a static universe of infinite age.]
Chris: As it turns out, the model implies temporal superposition, 
requiring an ex post facto "parallelization" of consequent events 
with respect to antecedent events. The CTMU calls this the ESP 
(for "Extended Superposition Principle") and adjoins it to quantum 
mechanics as an extension of the Superposition Principle. It 

permits temporal closure, something conspicuously lacking in the 
standard model.
GUY: I tend to not really feel I understand new theories or principles
until I work through most of the details myself, or at least see how the
details could be worked through. In the CTMU discussions I've read so
far, I think you present your results without adequate (for my purposes,
anyway) details for me to be able to reproduce them on my own (or in my
own language). This is probably intentional on your part. Anyway, I
plan to buy your book when it comes out and will try to work through the
details then.
Chris: Thanks.
[From a later email by Guy]
Chris: Nothing that leads to logical inconsistency is "confirmed by 
evidence". Expansion leads to logical inconsistency analytically. 
To wit, if there were something outside reality that were 
sufficiently real to contain the "expansion" of reality, it would be 
contained in reality. That's a contradiction; ergo, the hypothesis is 
false.
GUY: Chris, I am confused by this statement. Expansion can be 
defined
consistently without reference to some external space in which to 
expand. For example, the 2-sphere is defined by it's (constant) 
curvature and it's total surface area. Mathematically, its easy to 
increase its surface area. There's no contradiction.
Chris: The contradiction results from the concept of local 
invariance...the idea that because we can't detect any change in 
the local metric by experimental means, it must be invariant in 
some "absolute" sense. The contradiction is tacit, but it exists 
nonetheless.
GUY: I honestly do not see it. You are still not *required* to have the *something* to 
expand into, so there is no logical contradiction.
Chris: "Intrinsic expansion" is a contradiction in terms. If something is expanding, then 
it has to be expanding *with respect to* a fixed referent, and if it is, then it has to be 
extending into an external medium with respect to which the fixity of the referent has 
been established. On the other hand, saying that something is shrinking relative to that 
which
contains it presents  no such problem, for in that case, nothing is really "expanding". 
An inclusive relationship, like that whereby the universe includes its contents, can 
change intrinsically only if its total extent does not change; where its total extent is just 
that of the inclusive entity, this means that the extent of the *inclusive entity* cannot
change. Ergo, no expansion; it's logically analytic. Reason in any other  fashion, and 

the term "expansion" becomes meaningless.
Our only point of disagreement is that despite the above argument, you regard 
expansion as "no contradiction". But because you deny that the universe is expanding 
in any absolute sense - because you define expansion in such a way that it does not 
require an extensional medium - the point is moot. Essentially, you agree with me that 
matter is shrinking relative
to the invariant size of the universe (I'm calling it invariant because we're both 
regarding it as the fixed referent with respect to which matter is contracting). 
Therefore, you tacitly embrace conspansion.
Let's try another approach. The universe is something, and since you can't get 
something from nothing, you need an ontological substrate to get a universe. This 
substrate, which has the nature of infinite (unbounded) potential, can be approximately 
characterized as "the absence of that which defines and delimits the contents of the 
universe". That which
defines and delimits the contents of the universe is *constraint*, and the complement 
of constraint is potential. Therefore, the universe does indeed require a "medium" in 
which it can come into being and evolve: unbound potential, the ultimate logical 
generalization of "existence". However, by the nature of its definition, this medium 
cannot endow the
universe with "size" or a place in which to "expand". That would amount to assigning it 
a constraint, and the medium is constraint-free.
Saying that the universe consists of a fixed amount of this potential is meaningless, 
because the very concept of "amount" is extrinsically undefined. But saying that there 
exists an invariant intrinsic relationship between the relative sizes of the universe and 
its contents is almost the same thing, and this is a matter of absolute necessity. 
Without such a relationship, one can neither define nor perceive a "rate" of cosmic 
expansion, for the rate function, including the algebraic invariants in terms of which it is 
defined, is just the invariant relationship in question. In other words, something is 
conserved - that of which the invariant relationship consists - and it is with respect to 
this  "something" that conspansion must occur. In the CTMU, this something is called 
"SCSPL syntax".
Chris: The contradiction also surfaces with respect to our 
concept of motion. In the expansive model, photons and planets 
alike move by rotation and translation through a background 
called "space", which, while possessing sophisticated properties 
like Lorentz invariance, is still considered an independent medium 
containing particles and fields.
GUY: I also find the idea that space is a given independent medium to be 
unsatisfactory. But I don't see it as a contradiction.
Chris: Oh, it's a contradiction, alright. Unfortunately, the explanation exceeds my 
current time constraints. But here's a hint. I've already noted that if there were 
something outside reality that were real enough to topologically contain it, it would be 
intrinsic to reality (and

therefore contained within it). We can make a similar statement regarding matter: if 
there were something outside matter that were material enough to contain it, it would 
to exactly that extent be intrinsic to matter. In order to accommodate matter, space 
must be potentially identical to it... must "share its syntax". In other words, matter 
doesn't "displace" space, but occupies it in perfect superposition...intersects with it. So 
space consists of material potential and is thus a "potential phase of matter". Denying 
this leads to a contradiction.   
Chris: In the conspansive model, on the other hand, motion is an 
endomorphism
GUY: intriguing so far ...
Chris: defined with respect to those nested Venn diagrams we 
mentioned, with outer circles playing the part of quantum 
wavefunctions with respect to inner ones, and inner ones playing 
the role of "moving particles" with respect to outer ones.  Because 
the circles in the Venn diagrams are just lightcone  cross-sections 
corresponding to wavefunctions because of the conspansive 
identification of lightcones with wave functions and worldliness 
with particle trajectories
GUY: but this last part sounds a bit "hokey" to my highly-indoctrinated-with-the-
orthodoxy ears. But I'll reserve judgment until I see the details.
Chris: It shouldn't sound hokey at all - in fact, that's already more or less the 
understanding. I suspect you may be referring to the fact that lightcones are part of an 
abstract model constructed for explanatory purposes - they don't really specify the 
locations of photons relative to their sources - and that by the same token, quantum 
mechanics has disposed of the idea that there are such things as continuous 
worldlines.
But lightcones and wave functions are already tacitly understood to be co-delimiting, 
and worldlines can be approximated by path integrals evaluated over lightcone cross 
sections. Or maybe you mean that such an identification is "too convenient"? But if so, 
that might be because you think the amazing harmony between mathematics and 
physical reality, for
all its ubiquity, seems "too good to be true", and that anything but a tentative use of 
mathematical models is to be avoided. Fortunately, yet another advantage of 
conspansion is its support for a meaningful approach to this problem. In fact, it was in 
that connection that I first thought of it.     
Chris: the CTMU defines a general relationship between 
relativity and quantum mechanics.
We end up with two kinds of motion: a primary "inner 
expansive" kind representing the growth of Venn diagrams and 
the interpenetration of their spherical wave functions, and a 

secondary "requantizative" kind representing the relative 
shifting of diagrams as particles and galaxies undergo 
"incidental" spatial rotation and translation. But this implies a 
logical distinction; simplistically, the first kind of motion 
characterizes a "simulative" phase of matter, and the second 
kind characterizes a "simulated" phase. Time, calibrated in 
terms of primary motion, is a process of dualization between 
these logically distinct levels. The universe becomes a "self-
simulation"...a self-representational entity.
GUY: You make a similar statement in the M&F article ...I'm 
sure you must know this stuff, Chris. What am I missing? (This 
is a sincere request!) I really don't see the contradiction or 
paradox here.
Chris: As I think I've explained above, you're missing (1) the 
fact that ignoring extrinsic considerations by resorting to 
intrinsic definitions does not really dispose of them,
GUY: Maybe not, but I don't see anything that requires them, either.
Chris: If not, that's only because you're taking on faith the idea that we'll one day 
achieve an all-encompassing intrinsic explanation for reality, and that this explanation 
will include no unforeseen additions (and no great surprises for the physics 
community). But given the history of science, that's not necessarily a good bet. After 
all, there remain some very challenging unanswered questions, including "Of what 
does the universe consist?", "How did the universe some to exist?" and "Why does the 
universe exist?", that show no sign of being satisfactorily answered by the partial 
intrinsic explanation of General Relativity. One of two
things is therefore required: forbidden reference to a measurable external medium, or 
a more thorough job of intrinsic theorization. So there are indeed crucial requirements 
left to be filled.
Chris: and (2) some of the implications of inverting ERSU to 
USRE...of dualizing the Expanding Rubber Sheet Universe to its 
conspansive dual model, the "Universe as a Self-Representational 
Entity". In the General Relativistic version of ERSU, time is just 
another linear dimension in which the curvature of the universe 
progressively changes. In USRE, on the other hand, time is the 
wave-particle dualization of space to matter (to space... and so 
on), with cumulative parallelization of time.
Again, the ramifications are significant. We have two sets of 
invariants, one associated with the covariance of distributed 
physical laws (including all those dimensional constants relating 
matter) and one associated with the intrinsic invariance of the 
whole (including the conservation of space and the "cumulative 

present" in which the universe evolves).
GUY: I'm intrigued and like the sound of this. In my opinion, the "present" is something 
on which modern physics does not do a satisfactory job. But, I'll have to see the details 
of your theory before I comment on what you say above.
Chris: Fair enough.
Chris: For practical purposes, the Standard Model ignores the 
second set entirely.
GUY: The Standard Model is, for all practical purposes silent on "the present."
Chris: Correct. That's because there is insufficient understanding of, and no scientific 
consensus on, the nature of time. Most theorists know that General Relativity regards 
it as a linear dimension of the manifold called "spacetime", but few have any coherent 
thoughts on how this can be reconciled with quantum indeterminacy. The CTMU offers 
a new perspective.
In order to maintain both sets of invariants, we need to change our 
understanding of the relationship of space, time and object. 
Spacetime becomes an "SCSPL" evolving endomorphically, by 
internal self-mapping, rather than by the motion of matter through 
an independent medium.
GUY: Sounds interesting.
Chris: Or maybe it's just CTMU time for Guy Fogleman. [Universe 
A and B would appear to be the same in some respects and 
different in others. But this can be asserted *only* with respect to a 
basis of comparison, in this case arithmetic and geometry, which 
are brought to bear as parts of the cognitive syntax of one Guy 
Fogleman in the throes of a gedankenexperiment.
But the conclusions drawn from this syntax are valid only if it
characterizes not just the thought processes of Guy Fogleman, but 
a joint medium containing both universes...i.e., only if arithmetic and 
geometry are distributed over an objective medium containing the 
compared entities.
What infrastructure does this imply? First, it requires a connection
between the cognitive syntax of Guy Fogleman and the objective 
medium containing both universes an identity of Guy's mind and 
objective reality. More generally, it requires a conspansive CTMU 
infrastructure with logical priority over the gedankenexperiment 
itself.]
GUY: Well, I also like the idea of a theory containing both the "object" of the theory 
and the subjective theorizer. Again, I look forward to reading more about details of this 

approach in your book.
Chris: They'll be there.
GUY: What prerequisites (i.e., how much symbolic logic, higher math, and 
understanding of modern physics?) will you expect from someone wanting to read 
your book?
Chris: I'm trying to make it as self-contained as possible, even if that means some 
compact appendices in the back. But I won't deny that some exposure to logic, 
algebra, language theory, physics and cosmology might help the medicine go down.
Regards,
Chris
Discussion at the Utne Cafe
This page is intended not as a self-contained discussion of the CTMU, but a more or 
less random Q&A derived from sundry discussions on various lists.  Although many of 
the questions are intelligent and well-intentioned, there is a recurrent tendency on the 
parts of certain correspondents to complain to the effect that that the CTMU is 
"unconvincing" or "unsupported".  But while some people seem to expect the author to 
type his entire theory into their email lists, thus sparing them the dire necessity of using 
the informative hyperlinks with which they are conspicuously provided as background, 
he is in fact obligated to do nothing of the kind.  Suffice it to say that those in need of 
background information can find it elsewhere on this site.  (If this is unacceptable, 
never fear; nobody will force you to accept free access to something for which you can 
pay later on.  The author is presently writing a book on the CTMU, and when it is 
published, it can be yours for a modest price.)
Perhaps I should also mention an occasional tendency for a certain kind of person to 
deplore the lack of bibliographies and academic credentials that I might bring to a given 
discussion.  Concisely, such decorations are utterly irrelevant to content, which means 
that in all but a few specialized forums, they are a waste of time.  As far as glossaries 
are concerned, I generally try to define my terms as I go along...or in any case, those 
terms that a reasonably intelligent and conscientious reader cannot be expected to 
understand by context or by consulting a dictionary.  And to the rare gadfly who accuses 
me of being completely unoriginal and "using the ideas" of previous authors without due 
credit, I can only point out that while every philosopher and scientist who has ever lived 
has "stood on the shoulders of giants", this entails no obligation to reel off a complete 
list of credits with every gesture of self-expression.  Since my work contains a great deal 
that is original, those guilty of such accusations or insinuations merely display a 
hypocritical ignorance of its content.

That being said, it is sometimes hard for a theorist to gauge what his audience does or 
does not know.  For example, there is a tendency to assume that one's interlocutors 
understand the very basics of a field under discussion (otherwise, why bother to discuss 
it?)  Similarly, one tends to assume that if a reader is deficient in the basics, or is 
unfamiliar with one's own work, he or she will either politely request clarification or 
withdraw to privately fill the gaps in his or her knowledge before delivering an 
uninformed critique.  Unfortunately, certain questionable aspects of human nature 
sometimes conspire to invalidate these assumptions.  Accordingly, the author wearily 
offers a brief but informative essay (On Absolute Truth and Knowledge) to help readers 
understand the theoretical basis of what might otherwise look like his "unwarranted 
certainty" or "unjustified air of authority".  It is sincerely hoped that this will help even the 
most irritable reader avoid the perils and pratfalls of premature judgment.
Happy reading,
Chris Langan
 
Johnny Asia Mon, 23 Oct 2000 22:00:32 CDT (67 lines) initiation of thread
Time Out of Mind - Reader-friendly introduction to the CTMU
http://www.ctmu.org/CTMU_Articles/Time.html
The idea that the universe created itself brings a whole new meaning to bidirectional 
time, and thus to the idea that cognition may play a role in the creation of reality. As a 
self-creative mechanism for the universe is sought, it becomes apparent that cognition 
is the only process lending itself to plausible interpretation as a means of temporal 
feedback from present to past. Were cognition to play such a role, then in a literal 
sense, its most universal models of temporal reality would become identical to the 
reality being modeled. Time would become cognition, and space would become a 
system of geometric relations that evolves by distributed cognitive processing.
Here comes the surprise: such a model exists. Appropriately enough, it is called the 
Cognition-Theoretic Model of the Universe, or CTMU for short. A cross between John 
Archibald Wheeler's Participatory Universe and the Stephen Hawking-James Hartle 
"imaginary time" theory of cosmology proposed in Hawking's phenomenal book A Brief 
History of Time, the CTMU resolves many of the most intractable paradoxes known to 
physical science while explaining recent data which indicate that the universe is 
expanding at an accelerating rate. Better yet, it bestows on human consciousness a 
level of meaning that was previously approached only by religion and mysticism. If it 
passes the test of time - and there are many good reasons to think that it will - then it 
will be the greatest step that humanity has yet taken towards a real understanding of its 
most (or least?) timeless mystery.
Introduction to the CTMU (Cognitive-Theoretic Model of the Universe)
by Christopher Michael Langan
The real universe has always been theoretically treated as an object, and specifically as 
the composite type of object known as a set. But an object or set exists in space and 
time, and reality does not. Because the real universe by definition contains all that is 
real, there is no "external reality" (or space, or time) in which it can exist or have been 
"created". We can talk about lesser regions of the real universe in such a light, but not 
about the real universe as a whole. Nor, for identical reasons, can we think of the 
universe as the sum of its parts, for these parts exist solely within a spacetime manifold 
identified with the whole and cannot explain the manifold itself. This rules out pluralistic 
explanations of reality, forcing us to seek an explanation at once monic (because 

nonpluralistic) and holistic (because the basic conditions for existence are embodied in 
the manifold, which equals the whole). Obviously, the first step towards such an 
explanation is to bring monism and holism into coincidence.
continued:
http://www.ctmu.org/CTMU/Articles/IntroCTMU.html
http://megafoundation.org
Mission Statement
The Mega Foundation is a tax-exempt non-profit corporation established to create and 
implement programs that aid in the development of severely gifted individuals and their 
ideas. The Mega Foundation also supports and develops innovative projects, in the arts 
and sciences, based solely upon the merit of the projects and the vision of their 
creators.
 
Chris Langan Tue, 24 Oct 2000 04:00:56 CDT (78 lines) in reply to Don Carlos 
d'Espinosa
Carlos: I can easily see why we can't speak of the universe as existing 'in' space and 
time, but I don't see why we cannot, nevertheless, argue that the universe simply is a 
spatiotemporal, causally-closed system.
Chris: Of course. The crucial point is that spacetime is self-generative and logically self-
contained. More precisely, it embodies both the logic according to which it is self-
generated and self-contained, and the wherewithal to reflexively apply that logic.
This means that to a species of sentience based on its own structure, the universe 
exhibits something analogous to self-awareness...a generalization of that to which we 
rather ambiguously refer as "consciousness". I.e., the universe is understood by its own 
sentient agents to possess a predicate akin to consciousness, provided they reason 
correctly about their own status with respect to it. 
Carlos: Surely if the universe _is_ all of space and all of time, there is no 'larger' 'space' 
or 'time' in which it could reside.  But, for just this reason, there is no reason to think that 
there is more to the universe than the totality of causally-interacting objects over space 
and time. Is this what Langan is also arguing, or would he disagree with this claim?
Chris: Reality consists of more than material objects; it also possesses aspects that 
cannot be reduced to matter alone. For example, space and time are relations of 
material objects that cannot be localized to the individual objects themselves, at least as 
locality is generally understood; they are greater than the objects they relate, permitting 
those objects to interact in ways that cannot be expressed solely in terms of their 
individual identities. Equivalently, we cannot reduce the universe to a set of material 
objects without providing a medium (spacetime) in which to define and distribute the set. 
It follows that spacetime possesses a level of structure defying materialistic explanation. 
Indeed, insofar as they possess spatiotemporal extension, so do material objects!
Carlos: I certainly don't see why cognition is the only model for self-organizing systems. 
Our science here is in its infancy. Perhaps there are larger classes of such systems, the 
universe itself being the largest, and cognition as we know it is only a sub-species of 
such a system.
Chris: Suppose that cognition is not the only model for self-organizing systems, i.e. that 
such systems can be essentially non-homomorphic to cognitive processing. If so, then 
they lack meaningful cognitive representations, defying characterization in terms of 
mental categories like space, time and object. But then they fall outside reality itself, 
being indistinguishable as causes, effects, or any kind of phenomena whatsoever. In a 

word, they are irrelevant with respect to that part of reality isomorphic to human 
cognition. It follows that by any reasonable definition, reality is "cognitive" up to 
isomorphism with our own mental structures. 
Carlos: It seems irresponsible of us to confuse a sub-species with a larger class. For 
one thing, cognition involves many properties and relations which I would not want to 
attribute to the universe as a whole. Cognition involves beliefs and intentions, and you'll 
have a hard time convincing me that the universe as a whole has either. But, as I said 
before, there could be many types of system that exhibit some characteristics of 
cognition.
Chris: Cognition can be partitioned into general and nongeneral functions. When we 
take the intersect of all of our individual cognitive functions, nongeneral "options" like 
beliefs and intentions cancel out and leave only those functions which are general to 
all...the mental functions by virtue of which we all directly apprehend one and the same 
causally-connected reality. In the CTMU, this cognitive intersect is called the Human 
Cognitive Syntax or HCS.
By the above reasoning, the universe (aka reality, aka spacetime) is "cognitive" up to 
isomorphism with the HCS. To avoid this conclusion, one must break out of the above 
reasoning. But this is not as easy as one might previously have been led to believe.
 
Chris Langan Tue, 24 Oct 2000 18:38:00 CDT (157 lines) in reply to Bill Lindsley
Bill: There are some interesting paths in this forest, but I suspect the unconvincing 
irrelevance of the tautology when language is crunched to include huge chunks of rock 
and ice hurtling through vast regions of near empty space as mere subsets or 
projections of "cognition." The problem with defining "cognition" to include the entirety of 
reality is that by definition, then, it includes the entirety of reality and really does not say 
anything much about the characteristics, the features, not even the behaviors of the 
many facets and elements of this reality.
Chris: "...huge chunks of rock and ice hurtling through vast regions of near empty 
space" involves the processing, within a near-empty space, of the physical states of 
huge chunks of rock. This processing conforms to a state transition function included in 
a set of such functions known as "the laws of physics". Calling these functions 
"cognitive" is hardly a stretch, since it is largely in terms of such functions that cognition 
itself is understood.  Moreover, since "the characteristics, the features, ... the behaviors 
of the many facets and elements of this reality" are generally attributed to the laws of 
physics, it is not clear why cognition should not generally apply. After all, spacetime 
consists of events and separations, and events can be described as the mutual 
processing of interacting objects. So where physical interaction is just mutual input-to-
output behavioral transduction by physical objects, and cognition is mutual input-to-
output behavioral transduction by neurons and their inclusive brain structures, physical 
interaction is just a generalization of human cognition. If this seems like a tautology, 
indeed it is; self-contained self- referential systems are tautological by definition.
Bill: Do not get me wrong. I am intrigued. Less philosophically, however, than from the 
perspective of religion. The entirety of the universe as "cognition" sounds a lot like an 
all-knowing supreme being.
Chris: Quite so.  
Bill: The comment about "beliefs and intentions" is not a valid criticism, because 
"knowing" (the most signigicant import of cognition) is very different from "believing" and 
may have nothing to do with "intention" at all.

I have no problem, either, with imperfect human, or partial "cognition" constituting 
fleeting glimpses into this universal cognition. I just think, though, it starts to sound 
awfully convenient, and it leans toward an isolated character sort of anthropomorphism 
(the isolated character meaning the ability of the human to think, thus man creating the 
ultimate universe in his own image, with his own image focused on the "I think, therefore 
I am" characteristic of being human).  
Chris: The Anthropic Principle, a controversial (but in the final analysis, necessary) 
ingredient of modern cosmology, is already about as anthropocentric as it gets. The 
CTMU replaces it with something called the Telic Principle, which avoids reference to 
particular agents such as human beings. The Telic Principle says merely that the 
universe, in addition to being self-configuring and self-processing, is self-justifying. Only 
the real universe can formulate and provide a real answer for the question of why it 
exists, and it does so through internal self-projections (distributed syntactic 
endomorphisms) called sentient agents.
By the way, your mention of Descartes' cogito ergo sum is quite appropriate. Cartesian 
mind-body dualism underlies our ongoing reluctance to equate physical and mental 
processes. Yet, "I think, therefore I am" explicitly implicates cognition in a general form 
of existence, thus implying that this form of existence transcends the merely physical. 
This suggests that there is a deeper way to view reality than through the empirical lens 
of scientific methodology, and that on this deeper level of reality, there is no hard divider 
between physics and cognition.
Bill: As in most metaphysical systems, however, I tend to not approach them with the 
idea that what is important is whether they are true or false representations of *what is* 
but more with the perspective of how can this system, this way of looking at things, help 
us to understand better the various disciplines of human understanding, including the 
sciences, and also including the arts, and most definitely the inquiry into understanding 
who we are and what it means *to be* and *to be in the world* (or universe, or reality, 
whatever word one prefers to signify all that is).  
Chris: There will, of course, be a book on all this.
Bill: I will note this: cognition, in my mind, means a process of *knowing* and to know 
requires a knower and an object, that (or who) which knows and that which is known. I 
am very cognizant of some apriori or pure knowledge sorts of things, but they are of 
course all tautological. Such as mathematics, logic. (No physical reality required, one 
plus two equals three.) But to know of any physical reality requires "sentience" and 
there again that demands the observer and the observed. But we also recognize that 
the observer (and with some applied ability, the observer becomes a knower) is part of 
that being observed (the knower attempts to include herself in that known). We are also 
very cognizant of the inherent paradoxes that arise in self-references (Russell's 
paradox? I forget, though I know he addressed this at length in his discussions of set 
theory).
Thus we quickly run into a problematic wall, for it is one thing for the knower to know 
oneself, partially, incompletely, as part of what the knower perceives to not be the self, 
but as soon as this all-knowing entity attempts to refer to its own state of knowledge and 
being it then invites the paradoxes, the inextricable morass of apparent sense without 
comprehensible meaning. What happens when "cognition" stumbles upon the 
proposition:
"This sentence is false."
(which if false, is true)

We tend to attribute such paradoxes to the imperfections of language, and one might 
simply make a rule that it is improper syntax to use self-referential statements, but then 
the problem is we need self-referential statements, we use them extensively, 
meaningfully.
Chris: Such paradoxes are properly viewed not as static objects, but as dynamic 
alternations associated with a metalinguistic stratification that is constructively open-
ended but transfinitely closed (note that this is also how we view the universe). 
Otherwise, the paradox corrupts the informational boundary between true and false and 
thus between all logical predicates and their negations, which of course destroys all 
possibility of not only its cognitive resolution, but cognition and perception themselves. 
Yet cognition and perception exist, implying that nature contrives to resolve such 
paradoxes wherever they might occur. In fact, the value of such a paradox is that it 
demonstrates the fundamental necessity for reality to incorporate a ubiquitous 
relativization mechanism for its resolution, namely the aforementioned metalinguistic 
stratification of levels of reference (including levels of self-reference, i.e. cognition). A 
paradox whose definition seems to preclude such stratification is merely a self-
annihilating construct that violates the "syntax" (structural and inferential rules) of reality 
and therefore lacks a real model.
In other words, to describe reality as cognitive, we must stratify cognition and organize 
the resulting levels of self-reference in a self-contained mathematical structure called 
Self-Configuring Self-Processing Language or SCSPL. SCSPL, which consists of a 
monic, recursive melding of information and cognition called infocognition, incorporates 
a metalogical axiom,Multiplex Unity or MU, that characterizes the universe as a 
syndiffeonic relation or "self-resolving paradox" (the paradox is "self-resolving" by virtue 
of SCSPL stratification). A syndiffeonic relation is just a universal quantum of inductive 
and deductive processing, i.e. cognition, whereby "different" objects are acknowledged 
to be "the same" with respect to their mutual relatedness.
Bill: So, when this supreme cognition includes itself in what it knows, what happens? 
Perhaps it quivers and a black hole is created?
Chris: Quite the opposite. In fact, it transforms itself from a featureless (or more 
precisely, homogeneously self-similar) "cosmic singularity" to an internally variegated 
syndiffeonic relation, fractionating into layers of spacetime in a process called 
conspansion (materialcontraction-qua-spatial expansion). The associated logico-
algebraic structure, SCSPL, reflects the cognitive structure of that "all-knowing supreme 
being" you mentioned. As one might expect, the theological ramifications are extensive.
 
Chris Langan Wed, 25 Oct 2000 12:09:46 CDT (135 lines) in reply to Paul Kisak
[Chris' itemized responses to Paul's numbered questions]
1. Do you contend that the Universe is better modeled as a dynamic object of a dynamic 
set, a static object of a static set or something else entirely?
CL: First, sets and objects are objectivized concepts and therefore merely informational; 
the subjective side of their existence has been wrung out of them. In contrast, the 
universe isinfocognitive, which means that it is everywhere both objective and 
subjective. So sets are of limited value in describing the universe, at least as they are 
normally understood. Second, since time is a function of scale, the universe is both 
static and dynamic (depending on descriptive scale). From its own global vantage, the 
universe is timeless, or if you prefer, evolves in a single instant; from a local internal 
vantage like ours, it is dynamic.  Equivalently, the ultimate laws of the universe are 

invariant and therefore timeless, while the internal states of the universe are dynamic 
and temporal.
2. My personal opinion regarding a definition of reality, would lean more towards the 
potential of known truths rather than the status of known truths, when attempting to 
develop cosmological models of our universe. Likewise I would agree that spacetime is 
self-generative; possibly at the thermodynamic expense of another similar set, system 
and/or dimension. At this point, I am uncertain as to whether or not the 
conceptualization of such a dynamic system is outside cognition in its entirety. 
Personally I visualize the attempt to render closure on such a modeling effort to be akin 
to the conceptualization that is required when intuiting a mathematical singularity. The 
math goes ballistic at the asymptote but verbal and other cognitive processes step into 
bridge the ever dwindling gap between the explanation/model and the understanding. Is 
it your belief and/or position that my presumption of this asymptotic gap is an 
everpresent illusion or that at some point the fundamental decomposition of matter (for 
example) will cease as we know it or evolve into a further division of energy quanta for 
instance.
CL: I agree that there is a kind of "asymptotic gap" between reality and nothingness 
where the laws of physics, like matter itself, break down. However, certain syntactic 
invariants will remain in place nevertheless. For example, certain features of logic must 
still apply - otherwise, there is no point to the logical exercise called "theorization" - and 
because logic defines our cognitive framework, this means that some degree of 
cognitive isomorphism still applies. To put it another way, only tautological systems like 
logic and cognition are capable of emerging from the background of pure potential, and 
the fact that such systems are tautological isolates them to the point of mutual 
irrelevance. This means that the best we can do is to start with what we are given in this 
reality and attempt to generalize it into the "asymptotic realm". Reality theory seeks a 
description of reality on its most basic, that is to say its most general, level. So if our 
local reality is asymptotically embedded in some kind of metareality, the purpose of 
reality theory is to describe the latter. But this description will to some extent describe 
our local reality as well; since the embedment of reality in metareality is analogous to 
that of a closed, self-contained subgroup in a larger group, reality and metareality will 
share the same systemic identity. The tautology persists (indeed, the hallmark of a real 
tautology is that you can't break out of it).
3. Do you think that such a model would transcend the current triad of philisophical 
taxonomies (axiological, metaphysical, epistemological)?
CL: Technically, metaphysics includes epistemology (along with cosmology and 
ontology). Because axiology, which has to do with values, extends to the "values" 
according to which a universe is created, it overlaps with metaphysics; in fact, since 
axiology is predicated on real contexts and reality is ultimately metaphysical, it can be 
regarded as wholly included in the metaphysical realm of discourse.
So it all comes down to the most basic and general science of all, metaphysics. In fact, 
since anything worth talking about assumably exists, and metaphysics contains 
ontology (the study of existence), we can rest assured that this will remain true.
4. Do you think that the basic laws of thermodynamics, as they are currently applied to 
the behavior of tangible systems, can have a direct corollary to intangible systems?
CL: The laws of thermodynamics are a subsyntax of the laws of logic and mathematics 
in terms of which they are expressed. So the question is, how does their generality 
compare to that of the overall reality-syntax - in how general a context can they 

reasonably be applied?
Well, whenever we have a system, we have a conserved systemic identity, and the 
conservation of an identity is always in a sense "thermodynamic". So the answer is 
"very general indeed". But where there exists a systemic stratification involving nested 
closed subsystems, we must be careful to observe the various levels of closure; we 
can't just excerpt thermodynamic relationships from the physical (or material) level of 
reality and apply them to more basic (hypothetical) levels.
5. Do you currently have a bibliography that is relevant to 'The CTMU'?
CL: Unfortunately, no. Due to the generality of the theory - the fact that a general theory 
of reality, it is designed to apply to everything real - a thorough bibliography would have 
to be quite extensive, and in any case, the theory contains certain mathematical 
ingredients that (to my knowledge) cannot be found in the standard literature. The fact is 
that while many books contain clues on how to construct a comprehensive theory of 
reality, there are no currently available books that include them all. On the other hand, 
since I prefer to work from first principles wherever possible, I'm not much for 
bibliographies anyway; we've reached the point in many disciplines where "keeping 
abreast of the literature" takes more work than synthesizing theoretical ingredients from 
scratch (as data access technology moves forward, this may change for the better). If 
you want to know how to prepare for the CTMU, I'd recommend a good history of 
Western philosophy (e.g. Russell); a good book on mathematical logic and formalized 
theories (e.g. Rogers); and a good book on general cosmology (e.g. Harrison). 
6. Do you plan to publish a bibliography at some time in the future that pertains to 'The 
CTMU'?
CL: Only if I absolutely have to (see above explanation). 
7. Could you make a bibliography that is relevant to your development of 'The CTMU' 
available?
CL: Those three books I mentioned might be a good start. The history of philosophy will 
inform you on the "state of the art" in reality theory, which (unfortunately) was reached 
several centuries ago with the still-unresolved dispute between Hume and Kant. The 
book on logic and formalized theories will inform you on the basic mechanics of 
reasoning and the criteria for constructing meaningful theories.  And the book on 
general cosmology will inform you on the application of mathematics, and particularly 
algebraic systems, to observable reality. The rest you can infer!
 
Chris Langan Wed, 25 Oct 2000 15:00:45 CDT (102 lines) in reply to "Vatayan"
CL: Uh...yes. Well, Vatayan,I think you're making a mistake here. You're pointing to 
word usage, grammatical formations, styles of expression and so forth, but passing your 
judgment on content rather than form (which is actually what you're criticizing). See 
comments below. 
Vatayan: Here we have strings of inferences and speculations that are carefully 
couched and put together with the implication that they mean more than they say. 
Notice the nature of the initial string of suppositions.
Because cosmic expansion SEEMS to imply that the universe began as a 
dimensionless point, the universe MUST have been created, and the creation event 
must have occurred on a higher level of time: cosmic time.
CL: You want to keep your eye on the ball here. These "suppositions" are shared by the 
wider community of scientists and philosophers, who subscribe to something called the 
Standard Model. What we're doing is establishing a basis for dialogue with that 

community by adducing an inference, namely cosmic expansion from an initial 
singularity, from a set of observations, namely cosmic redshift attributed to galactic 
recession, of interest to that community. The reason I write "seems" is that the CTMU 
offers a new set of inferences distinct from those of the Standard Model.
Vatayan: A speculation that only SEEMS to have validity is immediately is the IF. Based 
on the IF we have a THEN which MUST be real.
CL: See above remark.
Vatayan: (Langan writes) "Prior to the moment of creation, the universe was not there; 
afterwards, the universe was there."
This is speculation taken as final truth. The universe has also been speculated as 
existing perhaps in some state prior to the expansion events of the big bang. We should 
be extremely careful about supposing that our speculations offer final truth.
CL: Once again, I'm referring to a set of inferences shared by many members of the 
scientific community, not necessarily MY inferences. A close reading of this passage, 
not to mention the article, would have sufficed to make this clear to anyone without an 
axe to grind.
Vatayan: (Langan writes) "If the creation of reality was a real event, and if this event 
occurred in cosmic time, then cosmic time itself is real. This implies that cosmic time, 
and in fact reality, must have created themselves!"
Ifs and their implied implications once again turn into must haves.
CL: Wrong again. The "if's" refer to a chain of widely assumed conditions, and the "this 
implies" refers to a consequence following from those widely assumed conditions in light 
of other widely assumed conditions mentioned previously. But that's almost beside the 
point, because the CTMU conspicuously places no reliance on these widely assumed 
conditions to derive the given consequence. It relies on pure logic instead. Inasmuch as 
I'm busy describing that logic for dissemination to people who are truly interested in the 
theory, I don't think it's fair to insist that I repeat it all here.
Vatayan: (Langan writes) "The idea that the universe created itself brings a whole new 
meaning to bidirectional time, and thus to the idea that cognition may play a role in the 
creation of reality. As a self-creative mechanism for the universe is sought, it becomes 
apparent that cognition is the only process lending itself to plausible interpretation as a 
means of temporal feedback from present to past."
The apparent bidrectionality of time poses difficult questions as to the nature of 
perceived reality. Once again the speculation that is offered as to the meaning begins 
with an IF, proceeds to a MAY HAVE and then ends with being an ONLY PLAUSIBLE 
INTERPRETATION.
This method of discourse is repeated over and over. In the end we only have strings of 
speculation that show that the author is familiar with a great body of cosmology and has 
put out his own model. The difficulty I have with it is that it pretends to be more than a 
string of inflated inferences.  It implies more than it actually delivers.
CL: Unfortunately, you're now making another bad assumption: that what is implied has 
not been delivered. In fact, it has been delivered (in other forums), and what we're doing 
here is merely notifying people of the theory's existence and offering a very brief 
description. In fact, I'm only participating because a buddy of mine posted the excerpt to 
which you refer without prompting from me, and I felt that Cafe Utne partcipants might 
appreciate a follow-up from the "horse's mouth". However, that doesn't mean that I have 
time for content-free discussions of my writing style. 
 

Chris Langan Thu, 26 Oct 2000 16:08:21 CDT (132 lines) in reply to Paul Saad and 
Johnny Asia
Paul Saad: Just a small question. Is this science or metaphysics or something entirely 
different? Did not Spinoza try something very similar but from a different perspective?
Johnny Asia: That is a BIG question! I'm sure you'll get a good answer.
CL: Far be it from me to refuse such a friendly challenge!
Empiricists believe that knowledge of reality is based solely on observation or sensory 
input. Although they do not generally dispute the reality of rational conceptualization, 
they maintain that it is somehow "separate" from physical reality, and that only sensory 
experience can justify its physical application (thus, many empiricists subscribe to 
dualism). Rationalists, on the other hand, hold that because the senses can lie, sense 
data are always unreliable; thus, certainty is the exclusive province of logic and 
mathematics, and reality can be known only by "fitting" it to abstract logico-mathematical 
structures. In practice, most modern scientists adhere to a model-theoretic blend of 
these two viewpoints, rationalistically applying logic and mathematics to empirical data 
obtained through observation and experimentation. In the spirit of this dual allegiance, 
they avoid things that they (mistakenly) believe can be neither directly observed nor 
mathematically expressed...things like purpose, emotion and consciousness.
Like Descartes and Leibniz, Baruch Spinoza is considered to be one of the supreme 
rationalists of Western philosophy. As a true member of the breed, he believed that 
knowledge comes from logic and reason alone, and that real truth is obtained solely 
through the rational analysis of ideas without regard to observation, emotion, or 
arguments from authority. Accordingly, he presented his own rationalistic views as 
"geometrical" inferences, deducing theorems about reality in strict mathematical style 
from what he considered to be self-evident axioms (like a geometrician, he proceeded 
from definitions, axioms and postulates to propositions, demonstrations, corollaries and 
"lemmas" or intermediate theorems). Relegating empirical information to a merely 
suggestive role, he proposed to base science as well as theology on a rationalistic 
conceptual framework consisting of an axiomatic system from which the laws of reality 
can be deduced.
When applied in the context of religion, rationalism opposes the acceptance of beliefs 
that cannot be logically justified. For example, the Reformation was characterized by 
rationalistic challenges to such Christian doctrines as the Creation, the Flood, the Holy 
Trinity, the Incarnation and the Resurrection. In addition to being a scientific rationalist, 
Spinoza was a religious rationalist bent on devising a purely logical brand of theology. 
Thus, he is remembered for his attempt to devise a comprehensive theory of reality 
seamlessly combining logical theology with a rationalistic approach to science. Since 
this goal is philosophically inevitable, it is not surprising that the CTMU shares it. But 
despite the similarities between Spinoza's approach and my own, there are significant 
differences as well.
Because Spinoza was primarily concerned with religion and virtue, he brought God into 
the mix from square one, characterizing "God or Nature" as the one substance of which 
reality in general is composed. That is, he subscribed to a dual-aspect monism in which 
God, equating to Nature, possessed two aspects called thought and extension. 
Although thought and extension are to be understood in terms of the Cartesian 
distinction between mind and matter, Spinoza broke with Descartes in making God the 
basis of a dual-aspect monism instead of a third basic ingredient of reality. In the first 
part of his definitive work Ethics, he defines God as that outside of which "no other 

substance can be given or even conceived." While acknowledging this Divine 
Substance to have an infinite number of attributes, Spinoza insisted that human beings 
can know only two of them: thought and extension (ideic and spatial existence). That is, 
while stating that there is a one-to-many, and in fact a one-to-infinity correspondence 
between each attribute and particular things or "modes", he asserted that humans can 
know only those modes entailed by the two specific attributes in question. While ideas 
and bodies appear to human experience to be separate things, they are simply twin 
aspects of one Divine Substance.
Now let us review three facts. First, Spinoza's rationalistic outlook is logical in essence 
and therefore imposes on reality the consistency of reason. Second, Spinoza embraced 
dual-aspect monism, further stating that "The order and connection of ideas is the same 
as the order and connection of things" and thereby asserting the coincidence of thought 
(or idea) and extension (or physical reality). Thus, he believed that there is a sense in 
which mind equals or coincides with material reality, and thus that no part of material 
reality eludes "mind" in its distributed Deic sense...that the realm of ideas, which might 
be viewed as comprising the "syntax" of the Deic level of mind, is comprehensive with 
respect to reality. And third, Spinoza subscribed to a deterministic form of cosmic self-
containment or closure, using the phrase "Divine Freedom" to refer to a kind of global 
Self-determinacy associated with a total absence of external constraint. (Note that 
Spinoza did not extend this freedom to human intentionality; he excluded contingency 
from the Divine Substance and its attributes and modes, equating it to mere ignorance 
of the true causes of one's actions.)
Enter the CTMU. The CTMU amounts to a metalogical reality-syntax incorporating three 
high-level principles or "metalogical axioms" corresponding to the above three elements 
of Spinoza's metaphysic. Although Spinoza long predated the study of formalized 
theories and could not avail himself of its mathematical machinery - machinery far more 
advanced than his tool of choice, ancient Greek geometry - the three formal attributes 
consistency, comprehensivity and closure turn out to be sine qua non for what we now 
refer to as a "Theory of Everything". Specifically, they are high-level syntactic attributes 
of the mathematical structure, Self-Configuring Self-Processing Language or SCSPL, in 
terms of which the CTMU explains reality. Thus, although Spinoza could not know how 
to precisely formulate or apply these three principles, we can say that his system was to 
some extent CTMU-like...a sort of "entry-level CTMU prototype".
But while there are other intriguing parallels between Spinozan metaphysics and the 
CTMU, the similarity is strictly limited by certain crucial points of departure largely 
predicated on several centuries of philosophical and scientific progress. For example, 
the CTMU regards what Spinoza called "Divine Freedom" as a distributed aspect of 
reality available to human agents by virtue of a new conception of spacetime structure. 
Next, while the CTMU is also a dual-aspect monism, it asserts that the basic substance 
of reality is "infocognition", a blend of information and cognition analogous to this new 
version of spacetime. And whereas Spinoza began with God as a primitive or 
"axiomatic" ingredient of reality, the CTMU defers all mention of God until a rational 
basis for His/Her existence has been laid by scientific reasoning from the metalogical 
axioms whose existence his work could only adumbrate. In fact, the CTMU uses these 
metalogical requirements to construct a whole new proof of God's existence.
There are other differences as well. But for now, suffice it to say that on the holodeck of 
the Enterprise, Baruch Spinoza and I would enjoy a mutually sympathetic discussion or 
two.

 
Chris Langan Thu, 26 Oct 2000 18:35:23 CDT (240 lines) in reply to Nick Blyth, Don 
Carlos d'Espinosa, and Mackenzie Anderson
Nick Blyth: Mmmm.. Why cognitive and not just information?, sorry if I'm being thick.
CL: Because you can't define information without implicitly including cognition, 
mechanical computation, or some other form of transduction in the definition (and vice 
versa). I.e., information and cognition are recursively defined in a medium called 
"infocognition". On the logical level, space and time are recursively defined in exactly 
the same way. That's how we achieve a monic, infocognitive explication of spacetime.
II
Don Carlos d'Espinosa: The argument, very roughly, goes like this:
1) The universe is 'self-created';
2) Our best example of a self-created process is cognition;
3) therefore, cognition is a model for the universe.
CL: Unfortunately, Carlos, that's wrong. (1) is correct. (2) should read "our only 
example" rather than "our best example"; in order to call something an "example", we 
must subject it to cognition, and by virtue of cognitive embedment, it is isomorphic to 
cognition and therefore cognitive (regardless of whatever other descriptions it might 
support). And technically, (3) should read "generalized cognition" instead of "cognition". 
As I've already stated, the universe is cognitive up to isomorphism with human 
cognition; beyond that point, it is cognitive only in a precise generalized sense, namely 
that of input-to-output transduction of information. Because generalized cognition 
logically embeds human cognition, it is necessarily even more powerful with respect to 
transductive potential.
Carlos: If I agree to (1) and (2), (3) does not follow. Cognition is only an analogy for the 
universe, and an analogy is not an explanation. However, I don't agree to either (1) or 
(2).
CL: Since you've got (2) and (3) wrong, responding to subsequent inferences is 
unnecessary. But for clarity's sake, let's do so anyway.
Carlos: (1) claims that the universe must be 'self-caused', i.e. a cause that caused all 
other causes. This argument goes at least as far back as Aristotle, and it's a traditional 
definition of God within Western theology.
CL: Yes, it's Aristotle's "Prime Mover" argument, and you're correct in asserting that it is 
a traditional definition of God within Western theology. Hence the theological 
implications of the CTMU.
Carlos: The Standard Model of the Big Bang attributes 'self-causation' to the universe 
itself. However, there's still legitimate controversy within the cosmological community 
over this: 
Martin Rhees and Lee Smolin, two of the 'hottest' names, have both offered 
cosmologies which do away with the 'self-caused' character of the Big Bang, by 
suggesting that our universe is not the only one.  There's no way to _know_ (i.e. 
empirically) if there are other universes, but there are good mathematical reasons to 
think so.
CL: And there are better mathematical reasons to think not, at least when it comes to 
incorporating them in an explanation of why this particular reality exists. What it comes 
down to is this: reality by definition contains all that is real. So in the widest sense of 
"reality", reality theory must explain genesis within reality itself. Rees and Smolin are 
merely avoiding this necessity by pushing the genesis problem back a step to some kind 

of "metareality". But we can easily rephrase our questions about genesis so that they 
refer to this "more basic" level of reality, and unfortunately, Messrs. Rees and Smolin 
are neither prepared nor equipped to answer those reformulated questions. The CTMU 
does not share this drawback. The reason: the CTMU is logically structured in a way 
that forces it to refer directly to the most basic and general level of reality, without 
speculative departures based on mere physical analyses ofmaterial reality. The fact that 
physics is embedded in logic should, perhaps, have clued these gentlemen in on the 
necessity to attend first to the logical structure of their theories and only secondarily to 
the associated physics.
Carlos: As for (2), I simply don't see how cognition can be 'self-caused'--even if 'strange 
loops' (a la Hofstadter) are employed in logic and other cognitive processes. Cognition 
coordinates sensory 'input' and motor 'output' (to use the computational model which is 
convenient, though misleading). There must be a world for cognition to work from and 
act on; no 'self' without a 'world'.
CL: Yet, if we begin by identifying cognition and world by saying that reality is cognitive, 
then cognition has "a world to work from and act on", namely itself. After all, in addition 
to acting on external input, cognition is primarily self-active; otherwise, it is incoherent 
and therefore non-cognitive in any integrated sense. When your brain undergoes 
cognition, its own neurons are processing each other's behavior by input-to-output 
transduction. It's all inside your brain, and that means that the macro-neuronal level of 
processing associated with your brain is self-contained with respect to your brain. This 
general reflexive relationship applies to any engine of cognition whatsoever...even the 
coherent wave functions of subatomic particles and the universes which contain them.
Carlos: Perhaps one might think of cognition as 'self-caused' insofar as the structure of 
the mind in the Kantian system is transcendental (i.e. not subject to categories such as 
causation). But it is difficult to defend Kant without having some response to the 
criticisms of 'post-Kantians' such as Hegel, Nietzsche, or Heidegger. (There are 
substantial disagreements between _them_ as well; I'm not suggesting that there is a 
single, coherent 'post-Kantian' position.)
CL: I'd be willing to make a considerable wager that I've got it all covered, at least in 
principle. 
III
Mackenzie Andersen: I have no argument with what you are saying and I find that you 
have a great deal clarity. However I'm wondering why you haven't mentioned David 
Bohm, since in "Wholeness and the Implicate Order" (1980), he has already described 
an enfolding and unfolding universe using the descriptors implicate and explicate. He 
described "all that is" as "undivided wholeness in flowing movement", which seems to 
be what you are saying.
CL: The difference is that unlike Professor Bohm, I'm not using poetic license to gloss 
over the fact that I lack a comprehensive mathematical structure supporting my 
assertions. When Bohm appealed to his nebulous holographic analogy, he was merely 
hinting at SCSPL. Don't get me wrong; much of Bohm's work is very insightful. But to 
make his ideas work, you need a concept called "hology" predicating a distributed 
endomorphism from the overall informational structure of reality to every internal point of 
reality as cognitive syntax. This is what expresses the identity of information and 
cognition I've been referring to as "infocognition" - it's a key characteristic of SCSPL - 
and you need a whole new model of spacetime to define it. Furthermore, you need a lot 
of additional logical and mathematical apparatus that Bohm left out of his inspired 

speculations about quantum nonlocality. This additional apparatus resolves certain 
longstanding paradoxes to which Bohm apparently devoted little of his thought and 
energy.
Mackenzie: There is also a book called 'The Conscious Universe' by Robert Nadeau 
and Menas Kafatos, who also wrote 'The Non-local Universe' ( 1999).  They likewise 
make the point that it can not be argued that scientific theory does not support the 
existence of God, although they acknowledge that it is interpretational. One can accept 
the philosophy of wholeness which out identifying such concept with the God concept.
CL: No, one cannot. It comes down to coherence; the wave function of the universe 
must be coherent in order for the universe to be self-consistent. Otherwise, it 
pathologically decoheres into independent and mutually irrelevant subrealities. Because 
the universe is both cognitive and coherent, it is a Mind in every sense of the word. 
"God" is simply a name that we give in answer to the unavoidable question "Whose 
Mind?"
Mackenzie: Paul Case, a recognized world authority on the Kaballa and the Tarot, 
describes the universe being created and sustained through self contemplation in his 
published meditations, 'The Book of Tokens" However in the Kabalistic sense it is not 
cognition which is responsible for creation, but desire.  Personally it seems logical to 
accept that there is a consciousness which permeates everything, once one accepts the 
concept of wholeness, weather it be from the point of view of mystical oneness, or from 
the point of view of a single quantum system. If the parts have consciousness why 
would the whole of which they are a part also not have consciousness? The spooky 
action at a distance of paired photons supports the idea that the universe is conscious. 
However I don't know how the idea that only certain parts of it are conscious is 
supported. I have heard it said quite dogmatically that stones have no consciousness 
and I always wonder what this assertion is based upon. If stones have a consciousness 
but do not use it to communicate to humans or in any way, which is recognizable to 
humans, how would we know about it? The idea that they don't have consciousness is 
just an assumption. I do know from the subjective experience of holding stones in my 
hand that they have strong vibrational energy.
CL: We know that rocks are cognitive because we observe their molecules to interact, 
that is to say, effect mutual input-to-output behavioral transduction. That's the rock's 
very own "internal dialogue", and there is absolutely no valid reason to deny its cognitive 
nature. The rock even has a generalized sort of "desire" to self-interact; this "desire" is 
logically inherited from a global attribute previously described as the Telic Principle. 
Concisely, the rock self-cognites because its medium, namely the universe of which it is 
a part, self-cognites out of a "desire" to self-create. This "desire" corresponds to the 
ability of reality to internally support a desire for its own existence; like reality in general, 
it's a recursive function.
Mackenzie: If wave-particle duality can be applied to all that is, no matter how many 
universes that might include, in the sense that all that is becomes one complete 
quantum system, the implication is that there are at least three epistemologies involved, 
that of the wave and that of the particle, and that of the whole. Empiricism may be a 
valid approach from the perspective of the particle, alone, but when wholeness is taken 
into consideration the action of the wave has to be accounted for and that can not be 
known from the dualistic particle perspective. Which brings us to St John of the Cross-
and his "infused meditation", which is the tradition of inspiration upon which religions 
have been born.

CL: In the CTMU, the decoherence of the quantum wave function is simply the 
conversion of a coherent or "cognitive" state potential, or wave, into actualized 
information, i.e. a particle, by cognitive interaction (radiative intersection) with another 
wave function. Wave and particle, cognition and information, alternate with stable 
identity in conspansive spacetime. On the limiting scale of reality as a whole, this says 
that the wave function of the universe is self-interfering and self-collapsing; the universe 
is an eigenfunction of its own teleological operator. This works ONLY in conspansive 
(CTMU) spacetime, where ingredients like the Telic Principle (already described) and 
the ESP (Extended Superposition Principle) allow the self-cognitive universe to self-
select from a background of unbound potential defined on an utter lack of constraint 
(note the cybernetic terminology). Without these ingredients, reality theory ultimately 
falls flat.
Mackenzie: Of course wholeness can never be proved to the satisfaction of those who 
require empirical proof since one can never step outside of wholeness and observe it 
without creating a state of duality. This in a sense describes the creative function of self-
contemplation.
CL: Yes, but it can be proven to non-empiricists by showing that empiricism is only half 
of the route to knowledge, and that the other half (rationalism) is the key to proof. 
(Although you'd never know it by talking to some "scientists", radical empiricism went 
out with Hume.) As for the rest, well enough said.
By the way, welcome to the Third Millennium AD.
 
Chris Langan Fri, 27 Oct 2000 15:57:02 CDT (190 lines) in reply to Johnny Asia, 
Lucas, andMackenzie Anderson
Thanks, Johnny - see you next week!
Lucas: I haven't read everything in regard to your theory, but I personally feel that, the 
need for a new math for this notwithstanding, any real answers will finally be found 
between UFT and Chaos Theory. Frankly, I am relieved I'm not the person for that job!
CL: You're evidently talking about a lower order of explanation than that of the CTMU. 
Otherwise, you'd be neglecting a very important necessity: to explain the UFT and 
chaos theory themselves (in reality theory, one is not free to inexplicably parametrize an 
explanation; you have to regress to self-explanatory constructs, and neither UFT nor 
chaos theory fills the bill). To do that, you have to get down to the foundations of 
mathematics. But I agree that UFT and chaos theory are definitely a part of the larger 
answer.
Johnny: Maybe God is not a Being after all, but rather Being itself.
CL: Excellent. As we usually conceive of a "being", it requires a medium; specifically, a 
"being" usually exists in space and time. But an "Ultimate Being" must serve as its own 
existential medium. 
Mackenzie: ...which still sounds quite a bit like what you have been talking about with 
your nested sets.
CL: I haven't been talking about nested sets; I've been talking about nested SCSPL 
sublanguages. There's a world of difference (some of which was explained earlier in this 
thread).
Mackenzie: You then say in order for SCSPL to work you need a whole new model of 
space-time to support it, - without describing that model. I'll take a guess, that what you 
mean is a form of simultaneous time in which space and time exist as a unity rather than 
as a complimentarity. I think such is implied whenever the concept of wholeness is 

discussed, including the Bohm's discussion.
CL: Close, but no cigar. When first conceived, the CTMU was heavily influenced by the 
mathematics of computation theory. Thus, what you call "simultaneous time in which 
space and time exist as a unity" is treated as a primitive form of parallel distributed 
processing (PDP) built into the structure of spacetime. Time is actually a higher-order 
sequential relationship of parallel-computative, to be generalized as "cognitive", 
domains. Bohm left this model-theoretic nicety out of his discussion. And regarding 
"what is implied whenever wholeness is discussed", there are those who would say that 
the entire universe is "implied" by a single electron. Unfortunately, to get credit for 
tracking such implications, one has to be more explicit. 
Mackenzie: Next you go on to say that one needs a "a lot of additional logical and 
mathematical apparatus that Bohm left out of his inspired speculations about quantum 
nonlocality". However you have "glossed over" whatever apparatus that may be.
CL: I don't have time to spoon-feed the entire theory to the sort of casual reader who 
leaps to negative conclusions without even seeing it. 
The concept of conspansive spacetime has been thoroughtly explained in Noesis, the 
journal of the Mega Society, to the readers of various email lists, and even to an editor 
of Scientific American. But I will give you a couple of important facts. In conspansive 
spacetime, it is more correct to say that you are shrinking than that the universe is 
expanding (the size of the universe is an "undefined absolute", i.e. externally 
immeasurable but invariant). And in conspansive spacetime, there are two levels of 
motion, one logical in character and the other algebraic...and neither is what you 
ordinarily think of as "motion".
Mackenzie: You close by saying "This additional apparatus resolves certain 
longstanding paradoxes to which Bohm devoted too little of his thought and energy." 
Which is a damn glossy statement. Who can possibly argue with "certain longstanding 
paradoxes" since you haven't given a clue as to what you are actually referring?
CL: The paradoxes in question are common knowledge among those familiar with this 
topic: ex nihilo cosmogony (how to get something, namely a universe, from nothing), the 
cosmological paradox of external embedment (how reality can exist without a medium to 
contain it), the arrow of time, and so on. Since all of these "reality paradoxes" are 
interconnected, Bohm could not achieve a complete resolution of the paradoxes of 
quantum collapse and nonlocality without resolving other reality paradoxes in the 
process. He didn't do that. But that doesn't mean I see no value in his work. In fact, I 
freely acknowledge his genius.
Mackenzie: Once again I can't understand what it is you are denying about the works of 
Robert Nadeau and Menas Kafatos. In fact I can not tell from what you have written if 
you have actually read their works.
CL: At the risk of looking like a barbarian, may I make a little confession? I've never 
read anything by either of these authors. But then again, my analytical approach doesn't 
seem to require it. Please bear in mind that as a nonacademic on a limited budget, I 
don't always have access to everything a given person thinks I should read. And then 
there's the time factor...
Mackenzie: I have to tell you, Chris, I much prefer Bohm's poetry, as exemplified in 
phrases like "undivided wholeness in flowing movement" to "cybernetic terminology". 
When one hears Bohm's phrase, one doesn't have to resort to Bohm's own personal 
dictionary with all of his personally invented words and concepts in order to understand 
what he means. His "poetic" phraseology communicates to a much wider audience 

because it uses generally understood words. I think this matters.
CL: Other than a couple of acronyms, I haven't used any neologisms in my posts here. I 
suspect that you may simply be unfamiliar with some of the mathematical terminology 
(isomorphism, endomorphism, sublanguage, etc.)...but in that case, you should avail 
yourself of a dictionary or two before criticizing my vocabulary. The down side of Bohm's 
poesy, which I can appreciate as much as you do, is that it doesn't explain the logical 
mechanics of the topic at hand; while his language captured some of the mechanics, it 
left some gaping holes as well. In formal reality theory, the logical mechanics are 
everything; poetry comes second.
Mackenzie: I disagree with you about your key to "proof". Imho, Rationalism and 
empiricism are functions of particle nature. Gnosis is the complimentary way of knowing, 
which is analogous to the wave function, which relates the particle to the whole.
CL: In the sciences, "knowing" equals definite cognitive modeling through observation 
and/or inference (compare "empiricism and/or rationalism"). While intuition and gnosis 
are more direct forms of cognition - and your assertion that they are associated with the 
wave phase of material cognitors happens to be very insightful - the "knowledge" 
thereby acquired can only be confirmed by scientific means. Since everything I 
contribute to reality theory is subject to scientific confirmation, I don't have the luxury of 
appealing to the intuition or gnosticism of my readers. Please bear this in mind the next 
time you find yourself tempted to deplore my less-than-poetic writing style.
Johnny Asia: You might enjoy the work of an acquaintance of mine, Chris King -self 
described nabi and biocosmologist. 
CL: You bet I do! A scintillating website by a very insightful author. Make no mistake, 
the Transactional Interpretation is a very useful bit of reasoning, and Chris King is right 
on top of some of its implications.
Lucas: Chris, do you think that cognition does not require an observer (and that QM 
does not, a la Prigogine et al)?
CL: Better to say "the observer may coincide with that which is observed." In other 
words, subjective self-observation is just another way to say "cognition". Regarding QM, 
the best way to handle observation is to distribute it over that which is observed, i.e. to 
regard particles not only as measurables, but as measurers. That's implicit in their 
generalized cognitive functionability.
Mackenzie: However upon reading (Langan's post) again, it might be interpreted 
otherwise. I thought there was another place where he had made claim to an absolute 
proof but for the moment I can't find it. Perhaps I was mistaken...
CL: I most certainly do lay claim to an absolute proof of sorts.  What I mean by 
"absolute" is precisely this: (1) you can't relativize your way out of it by changing the 
context; (2) finding it in error equates to destroying your own basis for inference. These 
criteria are built into the theory from the ground up using some very effective, that is to 
say ironclad, techniques. Logically, there is no way out. But don't take my word for it; 
wait for the book and then give it your best shot.
The ability to rely on blind faith is a wonderful thing. But it can also be an extremely 
destructive thing, and too often in the course of human history, it has been just that. The 
world is still full of people willing to kill (and die) for their beliefs...willing to destroy the 
planet and everyone on it for the sake of their faith. As long as they can continue to 
deny the existence of any "absolute truth" but their own - as long as they can hold their 
religious or political dogmas above the very laws of logic themselves - this perilous 
situation will continue to threaten our world. The CTMU aims to change all that, 

ultimately preventing the baser elements of humanity from continuing to risk our lives 
and welfare, and deprive us of our freedom, under cover of arbitrary anti-rational 
justifications devolving to blind faith.
You may not like this degree of "absolutism", and being a lover of intellectual freedom, I 
sympathize. But it's a logical universe, logic unavoidably constrains our view of reality, 
and that's the way it has to be. If I hadn't come up with the CTMU myself, it would 
eventually have been discovered by someone else.
Again, welcome to the New Millennium!
From Chris Langan to:  Don Carlos d’Espinosa    Thu, 02 Nov 2000 23:29:26 CST
Carlos:  What I find objectionable is not the content of the CTMU model, since I lack the 
proficiency to judge it.  I object, rather, to the cult of personality that surrounds men and 
women of “genius”.  This “cultish” behavior entails at least the following:  genius can 
only be recognized (i.e. measured) by other genius; if someone is a genius, what they 
say can only be refuted by another genius; if a non-genius expresses disagreement with 
a genius, or refuses to participate in the cult of genius, it can only be a result of 
resentment.
CL:  At the risk of deviating from the main content of this list, I dislike intellectual 
snobbery as much as the next person (and to show that I put my money where my 
mouth is, I’ve worked many blue-collar jobs that most soi-disant “geniuses” wouldn’t 
touch).  But make no mistake; geniuses do think more efficiently than most people – 
that’s what being a genius is all about - and it makes very little sense not to pay them at 
least a little extra attention.  It’s amazing how many people who would never dream of 
trying to replace a star athlete at the plate or the free throw line think nothing of holding 
themselves “equal” to a certified genius when it comes to solving philosophical riddles. 
It’s as though they equate philosophy with free-for-all. Unfortunately, that’s a bad call. 
Carlos:  (Personal note—feel free to disregard): When I was five, I was given an IQ test. 
As a result, I was placed in a “gifted” program in grade school, and remained in it 
throughout primary and secondary school.  In retrospect, I feel that the opportunities 
and resources made available to me through that program—teachers who actually care 
and have the time to devote to individual students, opportunities to develop my own 
interests, etc.--_should_ have been available to everyone.  Maybe some of my fellow 
students would still have become burger-flippers or drug-dealers, but they should have 
been given a chance.  Instead, some arbitrary test determined that I and a few others 
would be given what should have been given to all.) 
CL:  How true!  But sadly, we do not have limitless educational resources, and this 
means that we must maximize the resources we have.  Since the great bulk of human 
intellectual progress has been the work of geniuses, it seems more efficient from a 
survival-of-the-species standpoint to make sure that where geniuses are identified in the 
course of the educational process, they are provided with the resources to achieve their 
potential.  Let’s try an analogy: if instead of first fueling the fastest and longest-range 
vessels in our exploratory fleet, we insist on evenly distributing a finite fuel supply 
among all of the vessels, what we may end up with is a fuel crisis halfway to our 
destination.  Possible result: exploratory mission cancelled, new (cure, technology, 
energy source,…) cancelled, mankind cancelled, et cetera.  
Paying special attention to gifted children can hardly be characterized as “penalizing” 
average children when the fruits of their inimitable labors may eventually prove to be the 
keys to human survival.  In fact, such a characterization is groundless.  For if ostensibly 
normal children succeed in distinguishing themselves within the ranks of the “average”, 

they can always be reclassified as geniuses or genius material and become the 
recipients of increased resources and attention.  If a child can show uncommon 
intellectual ability in a gifted program, then he or she can certainly do so in a 
mainstream program…provided, of course, that the programs are conscientiously taught 
and administered.  But that’s another problem entirely. 
Carlos:  As far as the CTMU is concerned, Langan develops one idea that I find rather 
interesting, although I would proceed differently than he does: a true TOE (Theory Of 
Everything, for those of you just tuning in) must necessarily be a tautology.  What I find 
curious, however, is that he does not draw what I would consider to be the next step: 
that because a TOE is tautologous, it is useless and we should spend our time as 
scientists on something more interesting. 
CL:  This idea was specifically addressed in my brief essay On Absolute Truth and 
Knowledge. The idea that tautologies are inherently “uninteresting” is based on the 
obvious fact that if a piece of circular reasoning deals with only a small part of the world, 
then it can tell us nothing about other parts and in fact can mislead us by falsely 
purporting to universality.  But this does not mean that tautologies are “useless”…not 
even close.  In fact, propositional logic is a mathematical structure that consists of 
nothing but tautologies; they are its axioms and theorems.  Thus, your statement 
translates roughly as follows: “Logic is useless.”  How many intelligent people would 
agree with your hypothesis when thusly formulated?  Probably not very many, which of 
course makes one wonder just who has been teaching this ridiculous platitude, and 
why, to the many people who seem to embrace it. 
In fact, tautological logic is the most powerful tool in our intellectual arsenal, supporting 
the derivation of countless new logical expressions and embedding every past, present 
and future mathematical and scientific theory.  Without it, we not only could not theorize 
about reality; we couldn’t even observe it.  Without it, there would be no such thing as a 
mathematical proof.  And without it, you wouldn’t even have a computer with which to 
register your philosophical views.  The CTMU is a metalogical extension of logic 
designed to embrace reality in general, and as such, it must embody the reality-theoretic 
counterpart of a tautology called a “supertautology”.  That you still don’t know what this 
entails is unsurprising, because it’s only one of a large number of original mathematical 
ideas that were necessary to formulate the extension of logic called a TOE (again, see 
On Absolute Truth and Knowledge).  But in any event, you can be sure that by the 
method of its construction, this metalogical extension of logic is even more powerful 
than logic alone.  And that means “very powerful indeed”. 
Carlos:  On the other hand, the search for the Holy Grail that physicists call a TOE is 
really a theory that unifies general relativity and quantum mechanics.  
CL:  As does the CTMU, at least in part.  But understanding how would require an 
advanced comprehension of conspansive spacetime.  Suffice it to say that while the 
CTMU does not yet qualify as a Unified Field Theory, a UFT alone does not a TOE 
make, and those who call it a TOE are guilty of a misnomer.  Rather, the UFT being 
sought by the physics community is something that can be well-formulated only in a real 
TOE like the CTMU, and until it is, it is in danger of unraveling like an old sweater due to 
deep cosmological and mathematical paradoxes threatening its foundations. 
Carlos:  Such a theory would be a theory of everything, and as such be tautologous, if 
we could also reduce psychology to biology, biology to chemistry, chemistry to physics, 
and physics to the TOE.  Contemporary complexity theory, as I understand it, gives us 
reasons for rejecting reductionism.  Now, it is possible that complexity theory itself could 

be reduced to a TOE—in that case, the TOE might very well ‘encompass’ everything, 
although in a holistic rather than reductionistic sense.  
CL:  In fact, complexity theory is a disconnected set of mathematical vignettes that 
scarcely merits description as a “theory”.  Part of the motivation behind the CTMU was 
to provide the field of “complexity” with a more coherent theoretical framework (this goal 
has been at least partially realized). 
Carlos:  Here the connections between CTMU and Spinoza’s conception of God (God 
as the one substance that truly exists) are again evident, although I’m not sure it’s a 
model of God that the traditional (i.e. Judeo-Christian) believer would find compelling. 
CL:  Believe it now or believe it later; the choice is one’s own to make.  The only point 
I’mtrying to make at the present time is that as soon as one subjects the God concept to 
true logical reasoning, it’s CTMU time.  And although I can already hear the howls and 
imprecations, I’ll add a little something for the sake of honesty: there is no (logical) 
escape.  But if you haven’t been following my explanations to date, then I’m afraid that’s 
just another piece of friendly advice. 
Carlos:  Suppose however, that we develop a TOE which does indeed explain 
everything (whether this is the CTMU or some other model), and this we call “God”.  Is 
this really an explanation of anything?  Or, rather, is it that because it encompasses 
everything, it actually ‘explains’ nothing, because there is nothing outside of it in which it 
is explained?  Would it not then be, not the ultiimate explanation, but the limit (or end?) 
of all explanation? 
CL:  A TOE would be the “end” or “limit” of logical explanation only with respect to 
generality. With regard to observable specifics that are not completely determined by its 
limiting generalities, it would remain “internally open” despite its algebraic closure, and 
therefore thermodynamically able to accommodate new information.  Again, that’s a 
function of conspansive spacetime, which is a direct consequence of the CTMU’s 
supertautological structure. 
Carlos:  The reason why mysticism explains nothing (as the advocates of 
Enlightenment rationalism claim) is because mysticism is not an explanation, but 
something entirely different.  The failure of the Enlightenment lies in this: not that they 
failed to perceive something which can be explained, though perhaps not in scientific 
terms, but that they failed to see that explanation is not the end of the matter. There can 
be light if there is also darkness; there can only be explanation if there is also mystery. 
CL:  Quite so.  That’s why there is plenty of room within the CTMU for new mysteries 
and new explanations.   
From Chris Langan to:  Lucas    Thu, 02 Nov 2000 23:34:16 CST 
Lucas:  Well, Chris might be right.  I didn’t really see any math to check.  Did I miss 
something?  I’m sure Eva is qualified to check it, as are Ted, Simon, Dan WS and a 
bunch of other folk here.  I have tried to forget all that stuff, myself, but have dusted off a 
book or two since Simon put me up to reading Prigogine’s “End of Certainty” business. 
CL:  Actually, it’s all mathematics, mainly advanced logic including a lot of model theory 
and algebra.  I’ve just been putting it in a such a way that readers with no math 
background can understand it with minimal effort.  Although people sometimes complain 
about the lack of awe-inspiring strings of mathematical symbols in my explanations, I’m 
actually being exceptionally considerate by omitting them.  For example, while many 
people are “familiar” with Kurt Godel’s undecidability theorems due to popularizations 
following their introduction, very few people are actually able to sort through his original 
mind-boggling sequence of cumulative mathematical definitions in symbolic notation.  If 

Eva is one of those people and thinks she’s qualified to check or critique my work, I 
welcome her attention.  But she’d better be thorough and correct in her judgments, 
because when she’s finished, I’ll critique her critique…and with all due respect, I usually 
come out on top in the event of conflicts (academic credentials never seem to play a 
meaningful part). 
From Chris Langan to:  Mackenzie Andersen    Fri, 03 Nov 2000 08:49:22 CST 
Mackenzie (first quoting Gina LoSasso): “Chris exhibits the utmost patience in 
responding to every sincere question that is put to him, and is happy to continue the 
dialog as long as content remains the focus.” 
This discussion changed it’s form after Chris left. Imho, he wasn’t willing to answer 
questions put to him. It’s seems he was here to recruit people to join his own discussion 
forumn, not as an equal member of this one. 
CL:  Unfortunately, this isn’t quite correct.  Vatayan began relieving himself of bigoted 
absurdities, and you (Mackenzie) started accusing me of paying criminally short shrift to 
David Bohm, before I withdrew.  There were other impolities as well.  Please, there’s no 
point in discussion unless we stick to the truth. 
From Chris Langan to: Lucas    Fri, 03 Nov 2000 10:25:23 CST 
Lucas:  Tautologies are generally thought of as useless because they can only be 
defined in terms of the+mselves, e.g., if you define “consciousness” as “awareness,” 
that’s a tautology, and it has to be since there really is no other way to define 
consciousness except to use other words that mean the same thing. 
CL:  Good point about the self-definitive nature of tautologies; in effect, they are 
recursively defined.  (Notice that through the conceptual intersect of tautology and 
recursive definition, tautology is related to the more “practical” mathematical realms 
usually associated with recursion).  Unfortunately, when we theorize about reality in 
general, tautology is the name of the game.  That’s because “reality” is a universal 
autologous (self-descriptive) predicate that is self-contained and therefore needful of 
some enlightened circular reasoning.  This can easily be proven by contradiction: if 
there were anything outside of reality that were real enough to affect it in any real way, 
then it would not be “outside” of reality, but an integral part of reality (QED).  Ergo, it is 
an analytically demonstrable fact that we can reason about reality only in a self-
contained or “circular” fashion.  But don’t let this get you down; algebra in its entirety, 
including arithmetic, geometry, calculus and every other branch of mathematics, is 
based on circularity or “closure”.  In fact, circularity is ultimately the only valid basis for 
mathematical and scientific reasoning. 
Lucas:  So you go round and round, but never really get anywhere.  That’s what Carlos 
meant by such arguments finally being useless, I think, no matter how elegant or how 
detailed they might be. 
CL:  See above response. 
Lucas:  As I stated from the beginning, I rather agree with Chris, but there is no 
escaping that Carlos is correct regarding the above and that Eva, Tim and others are 
correct as well:  there’s no way to check it …no observational point.  Of course, Chris 
admits that from the start. 
CL:  Not quite.  We have just escaped from Carlos’ reasoning, and can continue to do 
so ad infinitum no matter what might be added to it.  As shown above, circularity is 
ultimately the whole story in science; the illusion of openness is something that we can 
enjoy only in specific localized contexts.  The “problems” with circularity arise only when 
claims of generality are erroneously made for circular contexts that turn out to be 

nongeneral (which can’t happen here; in the above proof-by-contradiction, we saw that 
the “claim of generality” associated with a supertautology is analytically based on the 
recursive definition of reality itself).  
While tautology has gotten a bad rep because circular rationalizations have too often 
been erroneously mapped onto parts of reality that didn’t really afford them a model, 
logical and metalogical tautologies are absolutely universal and do not share this risk. 
Don’t be misled by the pontifications of your high school teachers et al regarding the 
impossibility of meaningful tautological reasoning in science; if a given piece of scientific 
reasoning is logically correct, then it ultimately has to be tautological on the 
propositional level.  As a quintessential case in point, the CTMU supertautology has 
plenty of ramifications that end up expressing themselves in terms of nonlogical, 
“nontautological” symbols and their associated mathematical machinery.  But until we 
can break through certain fundamental misunderstandings about the role of tautology in 
science, there is no point in discussing them at length.
Discussion between Christopher Michael Langan and Russell 
Fred Vaughan, Ph.D.
Part I
RFV: I've been meaning to begin a dialog with you that would help us both understand each 
other better. Bob Seitz thinks most highly of you and such a friend is the best 
recommendation a man can have in my book. I would be foolish not to know you better than 
I do with you being only an e-mail away. 
What I'd like before attempting any major intellectual challenge like your CTMU -- that I 
would like to get into later after I have some background -- is to compare our perspectives 
on various topics without the public displays and distractions of being on the list. If you have 
any questions about my leanings on any topic, please let fly with questions and I'll answer 
them as straight-forwardly or as stupidly as I happen to hold my opinions. I would like to 
discuss them in such a non-confrontational way -- and slowly so that it sinks in. I will be as 
intellectually fair with you as it is possible for me to be. I do find it easy to acknowledge 
others ideas but that does not mean that I have none of my own that I would deny having 
had before a conversation. I am personally not nearly so worried about my own ideas as I 
once was, but naturally we all have some level of paranoia with regard to such things. I 
hope we can converse in such a way that you are always convinced of my good faith in this 
regard. Assurances are best proven rather than stated.
CML: Of course. I share and appreciate your willingness to progress towards mutual 
understanding. I know that some of my ideas look somewhat strange from a conventional 
viewpoint, but they're not lightly formulated, and I always welcome a chance to explain the 
underlying logic.
RFV: If there's a different agenda or different approach to this one that is better, then let's 
do it! But in the mean time could we investigate one of the questions that I have that seems 
to involve a major difference of opinion between us.
I happen to think that one cannot "know" the world without data and that our most abstractly 

generalized notion of the world must be based on inductive reasoning from that data. 
Having developed a generalized notion or theory one can then deduce specifics that are 
warranted (and our theory is only warranted) to the extent that these deductions map to 
actualities. It is my impression that you hold a somewhat different
opinion or place emphasis differently within such a scheme. 
Teleological arguments are not something I think should be tossed out without 
consideration, but I do have much more skepticism with regard to them. However, I hold 
(what can only be called "faith") that the ultimate physical theoretical solution will ahve a 
"necessity" about it which is very convincing -- sor of Einstein's "God would have done it
this way!"
CML: I agree that we can't know the world without data. But since we necessarily receive 
and interpret these data mentally, they provide important clues about our mental structure. 
The idea that this mental structure shapes the phenomenal world has been known since 
Kant; the CTMU simply pushes Kant's ideas to their logical conclusion, dealing directly with 
the relationship between mind and reality. 
RFV: Could we just talk about this topic quite narrowly for a beginning. That is, unless you 
have a question of me or a different agenda.
CML: Sure! No problem. 
RFV: Thanks in advance for whatever this engenders.
Part II
RFV: >> I agree that we can't know the world without data. << 
Ok. That one's out of the way:) "But since we necessarily receive and interpret these data 
mentally, they provide important clues about our mental structure." Obviously this is the big 
swinging door:) Obviously on the one hand we have photons and on the other complex 
ideas. Perceptions are somewhere between.
CML: There's the first catch. Your clear distinction between photons and ideas is not 
globally valid, but a provisional convention. Photons have a general abstract existence as 
well as a specific physical one. This is what happens when you embed physical reality in a 
space of abstractions in order to theorize about it; real objects necessarily conform to more 
or less exact configurations of an abstract distributed syntax. Since there's no way out of 
this requirement, it's pointless to theorize about photons without admitting it from the outset. 
A theory that doesn't is nothing but an oxymoron; it contains an implied logical 
inconsistency from jump street and invalidates itself. 
RFV: I never want to underestimate you, but at the same time I want to be sure that 
something that I happen to know that I consider relevant is known to both of us. So, let me 
just ask. Are you aware of biophysics studies by Hoffmans (and probably others later) 
demonstrating the development of Lie groups (as in the Norwegian mathematician) in the 
neurons between the eyes and the brain which effect the transformations of rotation, 
translation, etc. outside of the brain?

CML: If we examine a brain under a microscope, we see neurons and synapses. A neuron 
is either firing or not; its state is either 1 or 0, corresponding to the truth values of 2-valued 
logic. If we think of a neuron as a "brain quantum", we get a brain quantum, and therefore a 
brain, conforming to 2VL. Nothing that cannot be expressed in this 2VL
language can be formulated within the brain (or any larger neural system including, e.g., 
optic neurons), including nondiscrete mathematics (e.g., continuous Lie groups). On the 
other hand, anything that can be formulated in this language can be cerebrally formulated. 
The exact location of visual transformations is beside the point; the neurons that effect them 
are still "microbrains" obeying a 2VL "mental syntax". In this sense, visual transformations 
are still "mental" in character even when they occur outside the brain.
However, although Hoffmans' research sounds familiar, I'd appreciate a web source (if 
available).
RFV: These capabilities are staged in development with the shape size transformations not 
being completed until around the age of eight? The reason I bring this up is that one 
actually takes a tremendous leap from photons upon the eye to "perception" to the brain 
without any mental operations whatsoever. Perhaps a newborn has to work the problem 
from beginning to end, but it's simplified for us. This also, of course, gives us "theory-laden 
data from the outset:) And these transformations can fail to be developed properly if eyes 
are crossed and not 'fixed" until after that particular transformation is "programmed," etc..
CML: Again, when you say "mental operations" in a CTMU context, you're talking about a 
generalized form of mentation (not just the mentation of a human brain). Since a photon and 
a brain both transform information, albeit on different levels of complexity, they are both 
"mental" in nature. In other words, you seem to be making an a priori semantical distinction 
between levels of information transduction - the levels represented by a brain and a photon 
- and (again on an a priori basis) assigning "mental" status to only one of them. There's no 
scientific basis on which you can do that; it amounts to anthropocentrism. What we seek in 
any scientific context is a distributed formulation of reality that applies inside the brain as 
well as without it, and although the terms "mental" and "brain" are often assumed to be 
associated, this need not be exclusively so.
Where the universe is described as existing within an abstract space with exhaustive 
mathematical structure - a kind of self-distributed Platonic protouniverse - everything 
employs this structure as transductive syntax up to isomorphism with the intrinsic structure 
of the transducing object. But the intrinsic structure of an object need not be directly 
observable, for the simple reason that it's distributed and thus cannot be locally discerned 
by observation. The intrinsic syntactic ("mental") structure of an object can only be inferred 
from its behavior, which is where a need for data enters the picture. In other words, 
something doesn't have to have observable structural complexity to have a complex 
behavioral syntax; if it did, then the theory of so-called "elementary particles" would be free 
of mathematical complexity.
RFV: >> The idea that this mental structure shapes the phenomenal world has been known 
since Kant; << 

So to what extent does/can the "external world" shape the mind, i.e., affect its 
programming? Certainly there must be some of that going on within the brain as well, 
mustn't there?
CML: Of course. But to achieve a closed-form theory of reality, we need to admit that this 
"shaping" or "programming" process is mutual. That's why the CTMU employs a model of 
spacetime in which mind programs universe even as universe programs mind.
RFV: >> the CTMU simply pushes Kant's ideas to their logical conclusion, dealing directly 
with the relationship between mind and reality. << 
Let us hold off a bit longer on this till we have a common understanding of percepts.
CML: In the conventional model, percepts are objective observables that actively imprint 
themselves on, and thereby shape and determine, the passive mind and its internal 
processes. But a more general (and therefore more scientific, less tautological) description 
of percepts portrays them also as having a subjective perceptual aspect that is identified, at 
a high level of generality, with the Creation Event itself. This is just an extension of 
Wheeler's Observer Participation thesis ("only intelligent entities are observers") to physical 
reality in general ("everything is an observer", with the caveat that it's still possible, and 
indeed necessary, to reserve a special place for intelligence in the scheme of things).
The surprising part, in my opinion, is this. This reduction of all reality to simultaneously 
active and passive "infocognition" amounts to defining reality as did Hume...as pure 
experience ("infocognition" is just a technical synonym of "experience" that opens the 
concept up to analysis from the dual standpoints of information theory and cognition or 
computation theory). Thus, the Kantian mind-matter distinction, as embodied in the term 
"infocognition", is simply distributed over Hume's experiential reality by synonymy, bringing 
the metaphysics of Hume and Kant into perfect coincidence. 
Part III
CML: Let's see if we can cut to the chase here. 
Suppose you're wearing blue-tinted glasses. At first, you think that the world you see 
through them is blue. Then it occurs to you that this need not be true; maybe it's the 
glasses. Given this possibility, you realize that you really have no business thinking that the 
world is blue at all; indeed, due to Occam's razor, you must assume that the world is 
chromatically neutral (i.e., not blue) until proven otherwise! Finally, managing to remove 
your glasses, you see that you were right; the world is not blue. This, you conclude, proves 
that you can't assume that what is true on your end of perception (the blue tint of your 
lenses) is really true of reality.
Fresh from this victory of reason, you turn to the controversial hypothesis that mind is the 
essence of reality...that reality is not only material, but mental in character. An obvious 
argument for this hypothesis is that since reality is known to us strictly in the form of ideas 
and sensations - these, after all, are all that can be directly "known" - reality must be ideic. 
But then it naturally occurs to you that the predicate "mental" is like the predicate "blue"; it 
may be something that exists solely on your end of the process of perception. And so it 

does, you reflect, for the predicate "mental" indeed refers to the mind! Therefore, by 
Occam's razor, it must be assumed that reality is not mental until proven otherwise.
However, there is a difference between these two situations. You can remove a pair of blue 
sunglasses. But you cannot remove your mind, at least when you're using it to consider 
reality. This means that it can never be proven that the world isn't mental. And if this can 
never be proven, then you can't make an assumption either way. Indeed, the
distinction itself is meaningless; there is no reason to even consider a distinction between 
that which is mental and that which is not, since nature has conspired to ensure that such a 
distinction will never, ever be perceived. But without this distinction, the term "mental" can 
no longer be restrictively defined. "Mental" might as well mean "real" and vice versa. And for 
all practical purposes, so it does.
A theory T of physical reality exists as a neural and conceptual pattern in your brain (and/or 
mind); it's related by isomorphism to its universe U (physical reality). T<--(isomorphism)--
>U. T consists of abstract ideas; U consists of supposedly concrete objects like photons 
(perhaps not the best examples of "concrete objects"). But the above argument shows that 
we have to drop the abstract-concrete distinction (which is just a different way of expressing 
the mental-real distinction). Sure, we can use these terms to distinguish the domain and 
range of the perceptual isomorphism, but that's as far as it goes. For all practical purposes, 
what is mental is real, and vice versa. The T-U isomorphism seamlessly carries one 
predicate into the other.
Now let's proceed with your questions. 
RFV: >>>> But since we necessarily receive and interpret these data mentally, they provide 
important clues about our mental structure. <<<<
>>> Obviously this is the big swinging door:) Obviously on the one hand we have photons 
and on the other complex ideas. Perceptions are somewhere between. <<<
>> There's the first catch. Your clear distinction between photons and ideas is not globally 
valid, but a provisional convention. <<
I knew there'd be a catch here somewhere:)
I'm treading lightly on "photons" now, OK? Maybe we should accept a continuum between 
these? Or doesn't that work either? I can remain "provisional" on photons for a little while 
yet.
CML: No. Photons are at once abstract and concrete. In fact, the distinction might as well 
not even be discussed, because it's oxymoronic (as shown above).
RFV: >> Photons have a general abstract existence as well as a specific physical one. <<
OK, take me through this one gently:) I don't think I'll have a problem here if this level of 
abstractness involves alternative equivalent representations prior to collapse of a wave 
function or something like that. What do you have in mind here?

CML: As explained above. Or if you prefer, assume that a photon has no abstract 
existence. Then since ideas are abstract, it corresponds to no idea. But you know the world 
through ideas. So for you, a non-abstract photon has no existence. But you're discussing 
photons that exist. So the photon you're talking about is abstract (as well as concrete).
RFV: >> This is what happens when you embed physical reality in a space of abstractions 
in order to theorize about it;... <<
Now are we supposing that our "physical reality" is but one in a space of all possible 
realities? And then what is "a" reality in this case?  An instantaneous snapshot of "all 
things?"
CML: No. It's a self-perceptual system with a combined abstract and concrete nature (as 
explained above) that possesses closure with respect to other realities.
RFV: >>... real objects necessarily conform to more or less exact configurations of an 
abstract distributed syntax. <<
Please do not get impatient with me here. I *really* do not understand what you mean by 
your words without definitions and explanations and you are being very patient so far.
In what sense do you mean "more or less" exact configurations? Further, and maybe with a 
little more detail, what does "an abstract distributed syntax" mean to *you*? And how have 
we concluded that "real objects" "necessarily conform" to such a thing. These are major 
leaps - perhaps not for you, perhaps not for anyone you might hope I were, but for *me* 
they are *major*! "Necessarily" implies to me that it should be self-evident since it could be 
no other way (at least after I understand it). "Conforms" is a verb one doesn't usually see in 
this context, so I assume you mean to imply that there is an insomorphism that could be 
established between real objects and such a more orderly abstract structure. If that's what 
you mean, I'm with you. If not, > > please provide a little remedial instruction:)
CML: If we form a theory of some aspect of reality, it may or may not be correct (i.e., it will 
be "more or less exact"). But even if not, we can be sure that the correct theory will conform 
to our mental structures, since otherwise we won't be able to create a conceptual 
isomorphism with it (and that's what a theory is). A "syntax" is to be interpreted as any set of 
structural and functional constraints applied to any system. A syntax takes the form of 
general information and is implemented by generalized cognition. 
RFV: >> Since there's no way out of this requirement, ... <<
I understand that idealization at some level is required to theorize about anything if that's 
what you mean.
>> it's pointless to theorize about photons without admitting it from the outset. <<
If you mean that I must suspend belief about what a photon is until the theory is completely 
understood, that is no problem since no one else knows what a photon is yet in its fullest 

sense either.
CML: A photon is a configuration of the syntax of reality; as already explained, that's 
assured. It's also assured that a photon is at once abstract and real.
RFV: >> A theory that doesn't is nothing but an oxymoron; it contains an implied logical 
inconsistency from jump street and invalidates itself. <<
This last sentence seems to break with what I understood we were talking about. I thought 
we were addressing provisionality of what a photon "is" and the fact that "real objects" must 
be abstracted into a more formal structure in order to theorize. I buy that, but I have not yet 
considered the class of theories about such real objects. But if you are meaning to imply 
that a theory that begins with a preconceived notion about an object (say photon) it won't be 
of much use with regard to clarifying what a photon is, I can buy that. But, there are 
hierarchically ordered theories for which it might work just fine. We are not hot on the trail of 
one of that kind I presume:)
CML: Again, the combined abstract/real nature of a photon is not provisional, but given from 
the outset. It's only the exact corresponding configuration of reality-syntax that remains a 
mystery (and in a sense, not even that).
RFV: >> ... continuous Lie groups). On the other hand, anything that can be formulated in 
this language can be cerebrally formulated. The exact location of visual transformations is 
beside the point; the neurons that effect them are still "microbrains" obeying a 2VL "mental 
syntax". In this sense, visual transformations are still "mental" in character even when they 
occur outside the brain. <<
I have no disagreement with any of this but the 2VL. Neurons are not limited to 2VL types to 
my understanding -- they may be quite analog computer-like, in which case I don't know 
where 2VL fits into this scheme. I noticed that you placed a lot of credence on distinctions 
with this kind of logic, and that was actually the next area that I
wanted to question you about your background assumptions. So maybe you could tip-toe 
through some of this for me.
CML: Analog computers are as bound by 2VL as are digital computers; an analogy either 
exists (1) or not (0). All distinctions are 2VL; one thing is either distinct from another (1) or 
not (0) on a given level of definition. Since information consists of distinctions - constraints 
differentiating one thing from another - information is 2VL. Since theories consist of 
information, theories are 2VL. And since theories are isomorphic to reality, reality is 2VL. 
There is no escape...no crack in the stone of truth versus falsehood. Even to explain why 
there might be, you would be forced to resort to 2VL...and your putative exception would be 
embedded in 2VL by fiat.
RFV: >> However, although Hoffmans' research sounds familiar, I'd appreciate a web 
source (if available). <<
I have a couple of copies of papers circa 1965 around here somewhere. Come to the 
shindig next summer and I'll find them for you:)

CML: Hey, I'm 3,000 miles away! But I appreciate the invite. Maybe if I make a lot more 
money than I have now, I'll surprise you and show up.
RFV: >> Again, when you say "mental operations" in a CTMU context, ... <<
That was why I wished to remain outside that context until I get some background that 
presumes none of CTMU. Any word I use for now will be a lay definition term except as our 
mutual backgrounds accommodates more extensive terminology. I can't do this any other 
way, I'm sorry. It's just a limitation I have.
CML: As explained in the preface, mental operations cannot be distinguished from real 
operations except in a provisional way. That's what I mean by "a CTMU context". It's given 
a priori.
RFV: >> ... you're talking about a generalized form of mentation (not just the mentation of a 
human brain). Since a photon and a brain both transform information, albeit on different 
levels of complexity, they are both "mental" in nature. <<
I have no problem with this in general and the existence of levels of consciousness as other 
that human mental operations is something I can readily accept. A photon? I don't know, 
what's the information? Blue?
CML: Actually, the example of a photon is problematic for reasons too advanced to cite 
here. But for now, let's just say that a photon transforms the state of an atom by causing an 
electron to jump from one orbit to another. Even more basically, it "transforms its own 
coordinates" as it "moves" (due to the CTMU conspansive model of spacetime, the 
quotation marks are necessary). Atomic states and position coordinates are informational.
RFV: >> In other words, you seem to be making an a priori semantical distinction between 
levels of information transduction - the levels represented by a brain and a photon - and 
(again on an a priori basis) assigning "mental" status to only one of them. <<
I did in fact. And now I think I'm a little more in tune with where you are in this.
CML: Sounds promising.
RFV: >> There's no scientific basis on which you can do that; it amounts to 
anthropocentrism. <<
No need to disagree here since we don't, but I'm not sure that those who do disagree might 
not consider their approaches scientifically palatable:)
>> What we seek in any scientific context is a distributed formulation of reality that applies 
inside the brain as well as without it, ... <<
Whoa! Now here is another *big* hop. I, for example, have never presumed so great a 
scope for limited scientific endeavors. Let's leave this one for now unless there is some brief 

clarification that lessens the hurdle for me now.
CML: What I said is an implication of the scientific method, and specifically the experimental 
replicability criterion. If an experiment is replicable, then it can be repeated anywhere in 
spacetime provided the proper conditions are present. The possibility of the existence of 
those conditions at any point in spacetime, given the presence of supporting conditions, 
means that it is "distributed over spacetime" (but in a conditional way). Because all scientific 
truth is obtained by the scientific method, it conforms to this description; it has a conditional 
distributed formulation. This relates to the concept of spacetime homogeneity...its a logical 
necessity of meaningful theorization.
RFV: >> and although the terms "mental" and "brain" are often assumed to be associated, 
this need not be exclusively so. <<
Absolutely with you here.
>> Where the universe is described as existing within an abstract space with exhaustive 
mathematical structure - a kind of self-distributed Platonic protouniverse - ... <<
I know I'm breaking this sentence apart, but it's necessary for me to do so. I understand the 
agenda of the first part of this sentence, but I get confused here:
>> everything employs <<
What do you mean here? Do you mean for example that any analysis of the universe 
necessarily employs the mathematical analog of the universe? I think so and if so, I buy 
that.
CML: No, it goes even farther. Any material object or field has intrinsic mathematical 
structure that acts as a "syntax" to process information (e.g., the photon described above). 
Again, it goes along with the absence of an absolute distinction between the abstract 
(syntactic) and concrete sides of reality. 
RFV: >> this structure as transductive syntax up to isomorphism <<
The isomorphism between universe and mathematical structure is, of course, all that can 
justify the the approach, but...
>> with the intrinsic structure of the transducing object. <<
Now this I have problems with. This says that the fact (which I accept but some would 
doubt) that we can create an isomorphism between the universe and a mathematical model 
necessarily implies that the agent that can do this must have a structure isomorphic to the 
universe as well. That requires some very critical thought. Let me do it.
CML: Let me save you some time. The isomorphism is given on a de facto basis. Indeed, it 
follows from the preface.

RFV: >> But the intrinsic structure of an object need not be directly observable, for the 
simple reason that it's distributed and thus cannot be locally discerned by observation. <<
No problem here.
>> The intrinsic syntactic ("mental") structure of an object can only be inferred from its 
behavior, which is where a need for data enters the picture. <<
Very definitely. But here you need an extreme amount of data it would seem to me and i 
don't know off the top of my head that I can think of what kind of data would justify such a 
claim. It strikes me somewhat of the "I can test of 4 sigma, therefore I are one!":) A test 
would be good:)
CML: Remember, theories are made to be superseded by better theories; data can always 
be extended. Only tautological self-distributed systems possess "certainty". That's why we 
can be so sure of the 2VL component of reality syntax. Theories generally aren't so 
ironclad.
RFV: >> In other words, something doesn't have to have observable structural complexity to 
have a complex behavioral syntax; if it did, then the theory of so-called "elementary 
particles" would be free of mathematical complexity. <<
Ah, but regularity ought not be confused with simplicity.
CML: True enough. 
RFV: >>>> The idea that this mental structure shapes the phenomenal world has been 
known since Kant; <<<<
>>> So to what extent does/can the "external world" shape the mind, i.e., affect its 
programming? Certainly there must be some of that going on within the brain as well, 
mustn't there? <<<
>> Of course. But to achieve a closed-form theory of reality, we need to admit that this 
"shaping" or "programming" process is mutual. That's why the CTMU employs a model of 
spacetime in which mind programs universe even as universe programs mind. <<
We're going a little ahead of my schedule here, but let me address the "If a closed form 
exists, then..." What makes you jump for such a solution. Don't get me wrong, I like closed 
form too, but one usually adapts ones desires to the problem. I guess...never mind:)
CML: The discernability and stable identity of objects within the universe can be shown to 
imply that the universe is informationally closed in the following sense: it is possible to 
distinguish that which it contains (reality, perceptions) from that which it does not contain 
(unreality). I.e., there's an "informational boundary" separating that which the universe 
includes from that which it excludes, and this boundary is what is "closed". No closure 
means no boundary means no information.

RFV: >>>> the CTMU simply pushes Kant's ideas to their logical conclusion, dealing 
directly with the relationship between mind and reality. <<<<
>>> Let us hold off a bit longer on this till we have a common understanding of percepts. 
<<<
Still hold off a little. I'm sure it will do us well.
>> In the conventional model, percepts are objective observables that actively imprint 
themselves on, and thereby shape and determine, the passive mind and its internal 
processes. <<
Yes.
>> But a more general (and therefore more scientific, <<
We have somewhat different definitions of "scientific."
CML: Again, "scientific" means replicable, replicable means distributed, and distributed 
means general. Moreover, science carefully excludes false tautologies, or circular but 
nongeneral forms of reasoning.
RFV: >> less tautological) description of percepts portrays them also as having a subjective 
perceptual aspect that is identified, at a high level of generality, with the Creation Event 
itself. <<
You mention "description;" that is the key word. How are percepts best *described*? Data is 
needed here too it seems to me.
CML: Description can here be considered synonymous with "theory", and thus related to 
perception (data) by isomorphism.
RFV: >> This is just an extension of Wheeler's Observer Participation thesis "(only 
intelligent entities are observers") to physical reality in general ("everything is an observer", 
with the caveat that it's still possible, and indeed necessary, to reserve a special place for 
intelligence in the scheme of things). <<
This is a whole nuther topic. A great one! We got started on that when I was accused of 
being a murderer:) What constitutes a "real" event? My understanding of at least one of 
Wheeler's notions was that an irreversible change had to have taken place -- intelligent 
observers be damned.
CML: If you're right, then Wheeler's title for his thesis ("Observer Participation thesis") 
would seem to indicate his agreement with me about what constitutes an "observer", 
namely anything able to participate in an irreversible change. This reflects the distributed 
subjectivism and self-processing character of reality...the fact that mind (or "observership") 
and reality can't be absolutely distinguished. 

RFV: >> The surprising part, in my opinion, is this. This reduction of all reality to 
simultaneously active and passive "infocognition" amounts to defining reality as did 
Hume...as pure experience ("infocognition" is just a technical synonym of "experience" that 
opens the concept up to analysis from the dual standpoints of information theory and 
cognition or computation theory). Thus, the Kantian mind-matter distinction, as embodied in 
the term "infocognition", is simply distributed over Hume's experiential reality by synonymy, 
bringing the metaphysics of Hume and Kant into perfect coincidence. <<
I see what you are saying here, but I'm not *here* yet:)
CML: It's even simpler than I put it above. Experience entails something that experiences 
and something that gets experienced. So if reality consists everywhere of experience, it is 
everywhere self-experiencing, and has everywhere the character of both mind (the 
subjective experiencer) and objective reality (that which gets experienced). So the 
categorical imperative relating the subjective and objective aspects of reality everywhere 
coincides with experience; they're the same thing described in two ways, singular (Hume) 
and dual (Kant).
RFV: Thanks again. Maybe you'll see a transition in understanding as I went down this 
message; I hope so. I also hope this doesn't bore you too badly.
CML: I see improvement, by God! But in any event, just don't say I didn't try my best! Keep 
up the good work.
Part IV
RFV: (blue shades/mind analogy...) >> But without this distinction, the term "mental" can no 
longer be restrictively defined. "Mental" might as well mean "real" and vice versa. And for all 
practical purposes, so it does. <<
>> A theory T of physical reality exists as a neural and conceptual pattern in your brain 
(and/or mind); it's related by isomorphism to its universe U (physical reality). T<--
(isomorphism)-->U. T consists of abstract ideas; U consists of supposedly concrete objects 
like photons (perhaps not the best examples of "concrete objects"). But the above argument 
shows that we have to drop the abstract-concrete distinction (which is just a different way of 
expressing the mental-real distinction). Sure, we can use these terms to distinguish the 
domain and range of the perceptual isomorphism, but that's as far as it goes. For all 
practical purposes, what is mental is real, and vice versa. The T-U isomorphism seamlessly 
carries one predicate into the other. <<
I'm pretty much on board here.
>>>> Photons have a general abstract existence as well as a specific physical one. <<<<
I'm getting the scheme here. There is an isomorphic relationship between the abstract and 
concrete photons, which in some very real sense leans heavily toward the abstract since 
everything we know we know because of knowledge of the abstract version. Ok.
>> As explained above. Or if you prefer, assume that a photon has no abstract existence. 

Then since ideas are abstract, it corresponds to no idea. But you know the world through 
ideas. So for you, a non-abstract photon has no existence. But you're discussing photons 
that exist. So the photon you're talking about is abstract (as well as concrete). <<
Ok, I made that turn.
>> No. It's a self-perceptual system with a combined abstract and concrete nature (as 
explained above) that possesses closure with respect to other realities. <<
So, do you mean a conceptual model of "reality" which could be very different than our own 
reality? I think so. I see "self-perceptual" as a block to my acceptance here. I think you 
make the leap that isomorphic structures are the *same* structure to say that. Am I right? If 
so, how is that justified?
CML: It's an implication of the fact that no meaningful distinction can exist between mind 
and reality; with respect to these two predicates, they're the same. That is, we took the 
isomorphism M:T<-->U and predicated p1 = "mental" (abstract, ideic) on T and p2 = "real" 
on U. Then we realized that no meaningful distinction exists between the predicates p1 and 
p2. So now we're dealing with U on an abstract basis (just as we're dealing with T on a 
"real" basis, which it is because at the very least, it exists as a real neural pattern in your 
real brain). 
Now consider an isomorphism between two distinct sets A,B consisting of 10 red golf balls 
each for a total of twenty distinct balls: N:A<-->B.  Obviously, we can assign the predicate 
"red" to each set. This means that the sets are identical with respect to the predicate "red". 
But so what? There's still a distinction between A and B, because they in fact contain 
different golf balls. [Note that if the golf balls were electrons, it wouldn't be entirely possible 
to say this; electrons to some extent share their identities. Note also that matter consists of 
such particles.] And since red = red just as surely as p1 = p2 in the last paragraph, there's 
still a distinction between T and U...or is there?
We can call the sets A and B "different" because the identical red golf balls of which they 
consist are located at different coordinates. We can ascertain this with the help of nothing 
but a marking pen and a tape measure. There's the meat of our distinction with respect to A 
and B. But are T and U located at different spatiotemporal coordinates? How can we prove 
it?
As we've already seen, we know the universe U directly only through the abstract ideas and 
sensations T to which it corresponds in our minds, and that includes any sequence of 
measuring events we might undertake. In this sense, T and U are "at the same 
coordinates". So to prove that T and U are actually at *different* coordinates, we'd have to 
prove that their coincidence is an "illusion". But if we could do that, then we'd have the basis 
for distinguishing "mental" from "real" after all...something we already know we can't do. So 
after a little reflection, we conclude that as it is for knowledge, so it is for measurement; the 
only things we can directly measure are the ideas and sensations corresponding to U, and 
we can only do it with an abstract tape measure at that!
This, of course, is what philosophers call "solipsism", but without the assumption of 

personal control. In a word, it's impossible to prove that the universe doesn't exist entirely 
inside our minds, and that all that "objective reality" out there isn't just some kind of 
subjective internal simulation. While it's true that control of the simulation seems to be 
distributed among many people and influences, this merely amounts to a kind of "distributed 
solipsism" after all...an intersect of solipsistic viewpoints in one and the same meta-
solipsistic universe. In other words, the solipsistic abstract/concrete "Self" of the universe 
distributes over all of its contents, and our own "selves" are just mini-solipsistic aspects of it.
So the objective and solipsistic models are as indistinguishable as U and T, abstract and 
concrete, and we have to drop the distinction. And in this case, (objective) isomorphism 
equals (abstract, subjective) identity; when we talk about T and U, we're really talking about 
different aspects or characterizations of the same thing. So the perceived universe and the 
perceiver are one, and the universe is "self-perceptual". This is where reality theory meets 
the philosophy of mind. But that's fine, because as should by now be obvious, they need 
each other.
RFV: >>>> real objects necessarily conform to more or less exact configurations of an 
abstract distributed syntax. <<<<
...
>> If we form a theory of some aspect of reality, it may or may not be
correct (i.e., it will be "more or less exact"). <<
"More or less" "correct?" Right? Exactness is a small case in point?
CML: Yes. There are two kinds of truth, syntactic and semantic. Syntactic truth amounts to 
internal consistency, and semantic truth amounts to external isomorphism or quantification. 
If T is true on neither the syntactic nor semantic level of reality, then it has 0 scope or 
content. But then the theory has no model and is not a theory in the strictest sense. At best, 
it's just an empty (content-free) pattern of neural events or an inky set of typographical 
symbols on paper, with irrelevant fantasy for content.
That part of a theory that's wrong is empty, but the part that's right has real content. The 
theory tautologically corresponds to just that part of reality for which it's true. In fact, 
logicians already know that insofar as they can often either extend or qualify a troubled 
theory so as to adjust its referential quantifier (think Lowenstein-Skolem, Duhem-Quine, 
etc.) and thereby restore its validity (think classical physics as the low-velocity "limit" of 
relativity physics), the idea of an "absolutely false" theory is not what it seems.
The point of all of this is that a theory can evolve, changing its form to absorb more and 
more real content. That's what I meant by "more or less exact". Perhaps "more or less 
correct" would have been better.
RFV: >> But even if not, we can be sure that the correct theory will conform to our mental 
structures, since otherwise we won't be able to create a conceptual isomorphism with it (and 
that's what a theory is). <<
I have a hard time with this. An analog computer is an isomorphic structure of a screen door 
when it is set up to solve the screen door problem, however, a general purpose computer is 

*not* isomorphic with a screen door when it solves the screen door problem. General 
structures are adaptable in a sense which allows them to accommodate an isomorphic 
model without themselves being limited to anything like that model
structurally. So in what sense (and to what degree) do you insist on this conformal 
mapping?
CML: There is a very general abstraction called logic that permits various mappings among 
devices and other physical contexts. Have you ever heard the term "up to isomorphism"? It 
generally refers to a relationship that contains an isomorphism, but exceeds it. Some logical 
abstractions permit multiple isomorphisms, and these isomorphisms may be either 
structural or functional or both (in the above question, you seem to be talking about these 
two distinct kinds of isomorphism). In any case, for computational purposes, the 
isomorphisms you mention are logically equivalent up to isomorphism with the screen door 
problem (whatever that is). The "abstractions" being employed by the two types of computer 
are always expressible in terms of the penultimate abstraction, logic. Nothing I've said so far 
is incompatible with this fact.
RFV: >> A "syntax" is to be interpreted as any set of structural and functional constraints 
applied to any system. <<
Ok, that's pretty much as I had concluded.
>> A syntax takes the form of general information and is implemented by generalized 
cognition. <<
Go easy now. "Implemented by generalized cognition" is a big bite. I assume that a "mental" 
awareness of the constraints on the physical model is good enough for a neophyte:) 
Perhaps I even see a sense in which all but the "generalized" makes sense, but why is 
"generalized" in the phrase.
CML: Most people think that "cognition" is only done by brains. The phrase "generalized 
cognition", actually synonymous with time, simply extends human cognition to generic 
information processing. It's this sense in which generic information processing obeys a 
"cognitive syntax". Thus, time is any grammatical implementation of the SCSPL reality-
syntax.
RFV: >> A photon is a configuration of the syntax of reality; <<
Chris, you're going to have to lighten up just slightly on some of this stuff:) What you appear 
to be saying is that a photon is properly speaking a particular instance of the general 
structure of reality. That isn't very informative. What *do* you mean here?
CML: Just that a photon obeys certain structural and behavioral rules, and those rules exist 
as constraints within the overall set of constraints called "the reality syntax" (including the 
laws of math and physics, of which our exhaustive knowledge is not assumed). These are 
the abstractions to which the photon primarily corresponds. Stay with it, Fred - this is all 
really very simple (given the unnecessary compartmentalization of physical concepts in 
diverse formalisms, maybe the problem is that it seems *too* simple).

RFV: >> as already explained, <<
Then I missed it.
>> that's assured. It's also assured that a photon is at once abstract and real. <<
Got that!
... 
>> Again, the combined abstract/real nature of a photon is not provisional, <<
You did entreat me to be provisional about it though:)
CML: That a photon is both real and abstract is an absolute, nonprovisional fact. However, 
what counts as "real" and "abstract" in any given case is a relative (or provisional) matter, 
and sometimes even a matter of semantic convenience. I sometimes tend to assume that 
my meaning is obvious when it isn't. 
RFV: >> but given from the outset. It's only the exact corresponding configuration of reality-
syntax that remains a mystery (and in a sense, not even that). <<
I assume you mean that the actualities of the behavior of photons are all that is (may be) 
unknown? Its place in the structure of reality *is* known?
CML: Some aspects of a photon's behavior, and corresponding details of its place in the 
structure of reality, are known. But the intrinsic structure of the photon - the syntax it obeys - 
is not known except through this behavior. You can't hold a photon under a microscope and 
dissect it; all you can do with any subatomic particle is collide it and monitor the resulting 
events. Sometimes, you get what appear to be objective subparticles, but again, you can 
only see them through their effects. So all you can say for sure is that one configuration of 
SCSPL reality-syntax is merely decomposing into others. Getting too hung up on "objects" 
and their "innate properties" has been a recurrent problem in science; one generation's 
"particles" are the "subparticle interactions" of the next. We need to realize that what we're 
dealing with are transformations of a global nomology or "syntax" (or to oversimplify and 
appeal to popular terminology, of the "TOE ultimate equation", along with the syntactic 
structure of the language in which it will be expressed).
RFV: >> Analog computers are as bound by 2VL as are digital computers; an analogy either 
exists (1) or not (0). All distinctions are 2VL; one thing is either distinct from another (1) or 
not(0) on a given level of definition. Since information consists of distinctions - constraints 
differentiating one thing from another - information is 2VL. Since theories consist of 
information, theories are 2VL. And since theories are isomorphic to reality, reality is 2VL. 
There is no escape...no crack in the stone of truth versus falsehood. Even to explain why 
there might be, you would be forced to resort to 2VL...and your putative exception would be 
embedded in 2VL by fiat. <<
This is not very convincing to me. First of all, accepting your premise that all distinctions, 
knowledge, etc. are 2VL does *not* prove anything about "reality." Ideational objects -- even 

given the isomorphism - are *not* *the* objects themselves or exact equivalents - there are, 
for example, things which are not known about photons that are true of photons but not yet 
integrated into the ideational counterpart. So we have isomorphic structures of the reality of 
which we are aware and the reality we thereby presume. And we are in some sense limited 
such that our truths pertain to such a reality, but what about undecidability and such 
notions. How do these not end up being cracks in the stone? One must reason decidably 
about undecidability which doesn't, thereby, prove that all things are decidable.
CML: First, logical consistency equals the syntactic phase of truth. The stability (objective 
perceptibility) of reality implies that it's consistent, and the fact that it's consistent implies 
that it conforms to 2VL. Again, something either exists (1) or does not exist (0), so as soon 
as you say that it's real or that it exists, you tag it with a logical predicate (1 = true = reality-
inclusion) that anchors it to the entire logical syntax of which this predicate is a part. Of 
course, we can still make a distinction about which level of reality includes this something, 
but whatever that level is, it obeys the distributed tautological syntax of reality (logic). 
Second, decidability is a logical concept whose definition is based solely on logical 
(syntactic) operations, namely those executed by Godel in defining it. So you can't use it 
against logic without undermining its definition and thereby invalidating it.
I suspect that what you might be positing is an abstraction exceeding logic and 
consistency...an even more general predicate. Indeed, that's the only thing your description 
would fit. No problem; the CTMU contains an ultimate predicate called UBT (unbound 
telesis). But logic is a closed tautological system, and the relationship of this "pre-logical" 
predicate to logical reality is appropriately constrained. After all, what I'm trying to tell you 
about is the *logical* structure of reality, without which it would be no reality at all. For God's 
sake, don't interfere...your world could dissolve into an illogical chaos of ambiguities right 
before your eyes (and you're a part of it)!
RFV: >> Hey, I'm 3,000 miles away! But I appreciate the invite... <<
It was great. And...it's a damned cheap vacation -- all you have to do is get here! We're 
going to have a lot more people next year.
CML: I appreciate the invite, and I'll try to make it. Just remember that I'm not retired yet - 
not even close - and the saloon barons who pass as my employers don't know the meaning 
of "vacation time". And speaking of saloon barons, you apparently run one of the great 
"salons" of the 20th-21st centuries!
CML: >> As explained in the preface, mental operations cannot be distinguished from real 
operations except in a provisional way. That's what I mean by "a CTMU context". It's given 
a priori. <<
Nor of course can they be equated in other than a provisional way. I understand now, but 
this need not be a "CTMU context" for the sense to have real meaning.
CML: Yes, they can be absolutely equated, because as I explained above, we ultimately 
can't prop up the distinction between them (distributed solipsism, remember?). Therefore, 
it's only the distinction itself that exists in a "provisional way". As far as the name's 

concerned, I'm the guy who's done the most to analyze reality from this angle lately, and 
CTMU's my name for it. But as long as we both know what we mean, that's good enough.
RFV: >>>> you're talking about a generalized form of mentation (not just the mentation of a 
human brain). Since a photon and a brain both transform information, albeit on different 
levels of complexity, they are both "mental" in nature. <<<<
In view of what follows, I need a little definition of what this generalized "mentation" is. Does 
it relate to that which takes one form of reality into another, or what?
CML: Any construction you place on "mental" above and beyond pure information 
processing, information transduction, state transformation, etc. is, as we observed last time 
around, superfluous and anthropocentric. These are what is meant by "generalized 
mentation"... what you get when you take the "men" out of "mentation". Again, think "time" 
(the grammatical implementation of reality-syntax by reality itself, i.e., reality self-
processing). Remember, where "abstract" and "real" are the same, so are mind and reality, 
and so are human and nonhuman info-processing; Cognition <---> time. The logic is so 
simple it hurts! Human cognition, assumed to be fundamentally unique by anthropocentrists 
everywhere, is just the temporal processing that goes on in a brain. [Of course, there may 
be more to it than that - and there is - but for present purposes, that's good enough. The 
"more" in question has to do with distributed solipsism, and the consequent fact that there is 
a sense in which sentient cognition is primary...that what goes on in reality, namely 
temporal processing of all kinds, really goes on in the minds of sentient observers.]
RFV: >> Actually, the example of a photon is problematic for reasons too advanced to cite 
here. But for now, let's just say that a photon transforms the state of an atom by causing an 
electron to jump from one orbit to another. Even more basically, it "transforms its own 
coordinates" as it "moves" <<
Is its mentation different in this sense than a baseball going from one hand to the other? 
You're going to have to stretch me a little here too I think.
CML: Yes. A photon is a very fundamental object that can be described as "expanding" at 
the speed of light, and then "collapsing" someplace on its own expanding wavefront. This 
amounts to "motion of primary type", and is how the spacetime metric is created layer-by-
layer in the conspansive model (CTMU spacetime). When a baseball moves, it's moving in 
a much more continuous fashion through a sequence of metric domains created by 
conspansion (as described in that Noesis/ECE 142 file I sent you, geomusser.doc*). That's 
"motion of secondary type". But I hope we're not getting ahead of ourselves here. 
RFV: >> (due to the CTMU conspansive model of spacetime, the quotation marks are 
necessary). Atomic states and position coordinates are informational. <<
...
>>>> What we seek in any scientific context is a distributed formulation of reality that 
applies inside the brain as well as without it, <<<<
It was rather the "that applies inside within the brain as well as without it" that boggled my 
mind. The isomorphism we developed earlier would accommodate my mentally embracing 

the distributed formulation, but you are implying here either that the formulation applies also 
to the brain (which it well may not if the the formulations doesn't pertain to brains) *or* that 
the formulation can be made *without* a brain. The latter is what I think you mean and I'm 
certainly not *there* yet. Allowing a level of mentation to objects does not in itself open the 
door to complex formulations being possible without brains.
CML: I simply meant that since the brain is embedded in spacetime (or physical reality) and 
obeys its laws, a distributed formulation of physical laws must apply to those parts of 
spacetime containing brains as well as those that don't. Obviously, the kind of mentation 
transpiring inside a brain entails the presence of the brain in the part of reality where it 
occurs. But because the presence of the brain is possible anywhere in spacetime - since 
the laws governing cerebral structure and functionability are everywhere operative - that 
possibility is distributed. So no matter where in spacetime the brain goes, it can still 
function. Clearly, the same applies to inanimate objects and their generalized cognition. 
[Again, we neglect the distributed-solipsistic ramifications.] 
RFV: >>>> and although the terms "mental" and "brain" are often assumed to be 
associated, this need not be exclusively so.<<<<
>>> Absolutely with you here. <<<
However, that is a long ways from saying that I accept that complex formulations of reality 
exist outside the brain! Not that I couldn't ultimately be convinced, but am currently not.
CML: Again, Fred, your distinction regarding what's "inside" and "outside" the brain doesn't 
necessarily work for the mind corresponding to the brain. In a solipsistic model, the brain is 
merely the localized objective terminus of a material endomorphism (internal self-mapping) 
of the subjective, distributive mind. Because the mind shares the syntax of reality, and that 
syntax is scale-invariant (or geometrically self-distributed in the sense of a hologram), its 
boundaries are those of the reality it perceives. But you can't fully understand this without 
understanding the conspansive model of spacetime, in which objects literally "include 
themselves" in a spatiotemporal sense. That is, their SCSPL syntactic images, which are 
basically cognitive in nature and correspond to the subjective phase of their existence, 
temporally configure the metric in a cumulative step-by-step way by "expanding through 
space" at the common rate c (that's inner expansion) and then "recollapsing" to define the 
objective phase of their existence (that's requantization). Inner expansion and 
requantization correspond to coherence and decoherence of the quantum wavefunction.
RFV: >> Again, it goes along with the absence of an absolute distinction between the 
abstract (syntactic) and concrete sides of reality. <<
>>>> this structure as transductive syntax up to isomorphism with the intrinsic structure of 
the transducing object. <<<<
>>> Now this I have problems with. This says that the fact (which I accept but some would 
doubt) that we can create an isomorphism between the universe and a mathematical model 
necessarily implies that the agent that can do this must have a structure isomorphic to the 
universe as well. That requires some very critical thought. Let me do it. <<<

>> Let me save you some time. The isomorphism is given on a de facto basis. Indeed, it 
follows from the preface. <<
This is my *big* problem so far. It occurred above and now here. This is where we need to 
concentrate effort!!!! I have *not* bought in here yet.
CML: Again, the syntax of the mind is that of reality as a whole; that's the distinguishing 
characteristic of SCSPL. The deepest, most general level of this syntax is like a hologram 
whose image exists everywhere within it on all scales. That's due solely to a lack of prior 
constraint on scale; since the syntax is primal, no prior constraints can exist (this is a 
mathematical criterion, not the assumption of some big holographic crystal dangling in Ur-
space). The brain partakes of this distributed holographic syntax - the technical term is 
"hological" - by virtue of its topological embedment in physical space. But through 
conspansion, this topological embedment is computationally reciprocated by the mind 
corresponding to the brain; the mind (cognitively) contains the reality that (topologically) 
contains the brain objectively embodying the mind.
Logically, this comes down to certain CTMU duality principles mentioned in that 
geomusser.doc file. In particular, a form of duality that can be referred to as "spacetime-
object", "wave-particle" or just plain "conspansive" defines an alternation in which the mind 
solipsistically configures reality by a cumulative selective filtration of syntactic potential; 
during the inner-expansion phase of conspansion, the mind
builds a set of exhaustive potential future realities by superposition, and in the 
requantization phase, selects a future reality in which to re-embody itself as a material 
brain. Just in case this conjures up some kind of bizarre "brains-in-vats" scenario, please 
remember that the mind, just like a photon, is regarded as a configuration of the reality 
syntax. Indeed, all matter behaves alike in this regard; as does the mind, so do its contents.
RFV: >>>> The intrinsic syntactic ("mental") structure of an object can only be inferred from 
its behavior, which is where a need for data enters the picture. <<<<
>>> Very definitely. But here you need an extreme amount of data it would seem to me and 
i don't know off the top of my head that I can think of what kind of data would justify such a 
claim. It strikes me somewhat of the "I can test of 4 sigma, therefore I are one!":) A test 
would be
good:) <<<
In re-reading yours/mine here I have a little different picture/problem. "Intrinsic syntactic" 
means "mental?" I see the intrinsic form of an object as the "ideal" for that object but not 
that object's idea of itself. Clearly the "form" or ideal structure of an object can only be 
inferred from data. But its ideational capabilities are altogether different.
[[Here, more explanation was required. The object corresponds to a configuration of an 
absolute hological syntax whether or not we or the object have data or can infer what that 
configuration is, or for that matter what the hological syntax itself is. This is given by the 
abstract = real or M=R relationship.]]

>> Remember, theories are made to be superseded by better theories; <<
Easy to remember.
>> data can always be extended. <<
Ok.
>> Only tautological self-distributed systems possess "certainty". <<
The "certainty" or necessity of such a consistent system would indeed be very convincing 
but the "only" here bothers me.
CML: It inheres in the definition of "tautology". Tautology, which denotes informational 
closure, exists wherever stable perception is possible. Specifically, it must exist in an 
embedding system of any perceptible nontautological system precisely because the latter is 
stable enough to perceive; closure is what forms the informational boundaries that 
distinguish and define the percepts. Unfortunately, we seldom know exactly what the 
embedding system is (e.g.) because definitive axioms are missing. This, of course, relates 
to undecidability, which states that axioms are always missing in logico-arithmetic systems 
of sufficient complexity. But the important thing to realize is that because perceptible 
informational boundaries exist - because the system has enough distributed structure to be 
coherent and self-connected - closure is given at various levels of systemic self-reference. 
It's an existential necessity in the mathematical sense; no informational closure ---> no 
stable perceptual invariants. 
To have certainty at arbitrary levels of specificity, we'd need to know all the axioms of the 
overall system...i.e., a full description of the global systemic tautology allowing perception. 
Obviously, any nontautological subsystem is "open" to undecidable data and therefore 
uncertain. In the case of reality at large, this uncertainty - which, by the way, is associated 
with quantum uncertainty - owes to the fact that increasingly specific axioms are even now 
being created by sentient agents. That's why reality, AKA SCSPL, is said to be "self-
configuring". That's what intrinsic cosmogony is ultimately all about.
Sorry if I lost you up there, but explaining this is hard.
RFV: >> That's why we can be so sure of the 2VL component of reality syntax. Theories 
generally aren't so ironclad. <<
I still have the problem of not having accepted the ubiquitous nature of 2VL.
CML: It's ubiquitous because if it weren't, we'd have a breach in the boundary between 1 
and 0 (true and false), which would compromise informational boundaries at all orders of 
predicate logic (in the abstract logical phase of reality), which implies the perceptual 
inconsistency of reality, which means logical collapse of the entire perceptual system (due 
to the systemic ubiquity of the truth or systemic inclusion predicate, inconsistency can't be 
isolated in logical and perceptual systems; a hole in truth means holes everywhere!). Note 
that undecidability relates two aspects of informational closure or perceptual stability, 

namely consistency (the integrity of the T/F boundary) and incompleteness (the fact that 
certain axioms are even now being decided and are therefore missing from our worldview).
RFV: >> The discernability and stable identity of objects within the universe can be shown 
to imply that the universe is informationally closed in the following sense: it is possible to 
distinguish that which it contains (reality, perceptions) from that which it does not contain 
(unreality). <<
In what sense do you assume this "closure" encompasses "completeness?"  Completely, I 
assume, but is that warranted? What possibilities are excluded by including only those 
forms that we currently incorporate into the isomorphism?
CML: Explained above. At any given point, we possess incomplete theories that are good 
only up to isomorphism with what we currently perceive. I.e., there's an "informational 
boundary" separating that which the universe includes from that which it excludes, and this 
boundary is what is "closed". No closure means no boundary means no information.
RFV: Why would the infusion or exit of a few forms every few billion years mean that there 
is "no information?"
CML: Because on the basic level of this discussion, the "forms" of interest are syntactic 
invariants...invariants of perception. If any of these forms were to cross each other, the 
result would be perceptual inconsistency, loss of closure, and the collapse of perceptual 
reality, i.e., the formation of a gap between two discrete, mutually disconnected, internally 
consistent realities in which the contradictory perceptual invariants are isolated...two 
mutually irrelevant parallel universes. We're only interested in the universe we inhabit.
RFV: >>>>>> the CTMU simply pushes Kant's ideas to their logical conclusion, dealing 
directly with the relationship between mind and reality. <<<<<<
I don't know that I'm ready -- I'm not:) But please administer the firehose in accordance with 
your perception of my readiness. I know there are a couple of concepts that seem like 
prerequisites that we need to consider terminological deficits to be worked on as we go:)
CML: I've been hosing on this for hours now...I hope it helps.
RFV: >>>> But a more general (and therefore more scientific, <<<<
>>> We have somewhat different definitions of "scientific." <<<
>> Again, "scientific" means replicable, replicable means distributed, <<
I buy that.
>> and distributed means general. <<
I don't buy this -- yet:) We could have a very widely distributed specific thing for which its 
wide distribution implies nothing but its wide distribution.

CML: You seem to mean "distributed" in a statistical sense....something sprinkled through a 
preexisting metric space. The kind of distribution we're talking about here is logical and 
refers to the syntax of reality itself, which obviously distributes over that which it governs 
(otherwise it wouldn't be where it has to be in order to do its job). Your kind of distribution is 
defined on the metric; mine defines it.
[[Actually, RFV is probably talking here about the distinction between conditional and 
unconditional logical distribution; only the latter is really "general". See below, 2 CML entries 
down the page.]]
RFV: >> Moreover, science carefully excludes false tautologies, <<
I don't see "science" as excluding much at all. Early stages of science in a given field would 
probably *include* false notions. It would still be "science" in the fullest sense of the term, 
just subject to being found out later to have been wrong -- but that's almost the definition, 
and certainly much of the value of science.
CML: I simply mean that when science becomes aware of a false tautology, it breaks its 
back to get rid of it. 
RFV: >> or circular but nongeneral forms of reasoning. <<
I see it as excluding circular arguments but definitely not "nongeneral forms of reasoning." 
The ultimate scientific achievement may very well be described as you have, but not in its 
current state. Are we arguing different things here?
CML: Whoops - I meant "circular but nongeneral forms of reasoning that pretend to 
generality". I sometimes tend to forget that not all scientists are seeking a theory of 
everything, perhaps because they run into such spectacular problems when they do. Let me 
explain.
Once again, the scientific method entails both local distinguishability and replicability, two 
predicates which become opposed to each other on the syntactic level. To be universally 
operative or "replicable", reality syntax ("the laws of science") must be distributed 
throughout spacetime; but since it is, it's not locally distinguishable. The limitations of the 
scientific method are those of science. But since science doesn't yet realize these 
limitations, it continues to seek laws with distributed (replicable) formulations by empirical 
(locally distinguishable) means *whether the laws are general or conditional*. Where the 
laws are indeed general, this is a logical impossibility; the only thing that can make a 
scientific law locally distinguishable is the presence of specific definitive conditions that are 
elsewhere absent. Science often ends up chasing its own tail when it tries to go too general 
for its objectivist limitations, as it does in fields like cosmology and consciousness.
So what it seems to come down to is this: I'm talking about what science is ultimately trying 
to do (seek general laws that apply everywhere in spacetime), while you're talking about 
what it actually can and does do in its current objectivist form (seek nongeneral laws that 
are nonetheless distributive in effect, or replicable under more or less specific sets of 

conditions anywhere in spacetime). I guess I tend to view science in terms of trying to make 
it what it wants to be. 
RFV: >>> You mention "description;" that is the key word. How are percepts best 
*described*? Data is needed here too it seems to me. <<<
>> Description can here be considered synonymous with "theory", <<
I like this notion. It also flies with current philosophical presuppositions.
CML: ...and glossaries. In most usages, the two terms are equivalent. A "description" lacks 
the mathematical structure of a "theory", having no explicit axioms and rules of inference, 
but borrows them implicitly from the syntax and semantics of the language in which it is 
expressed. 
RFV: >>> and thus related to perception (data) by isomorphism. <<<
You just don't let me catch up, do you:) I see perception more as a dot or stroke in an 
impressionist painting; you tend to see it as a Rembrandt:) I don't see a theory as being 
"perceived." I see it as ultimately being understood on the basis of many, many perceptions. 
I guess that's what I'm doing here -- forcing you to strain your theory
through my sieve:)
CML: Maybe that's a good thing for me. In the CTMU, as in Hume, reality coincides with 
perception (and also with its cognitive underpinnings as described by Kant). Indeed, science 
already effectively describes reality as a de facto blend of thought and perception, with 
theories using rational relationships to weld together our shared perceptions. Scientific 
theories can thus be regarded as "common generalizations" of
those little impressionistic dots you mention. So when we set out to construct scientific 
theories - which is, after all, what we're doing here - we must first buy into the assumption 
that there are commonalities among our perceptions. And this isn't really a questionable 
assumption, because those perceptions (as opposed to our personal interpretations of 
them) can always be scientifically reconciled. This reflects the fact that reality is logically 
self-consistent, which in any case can be established by reasoning along the lines 
described above.
RFV: >> If you're right, then Wheeler's title for his thesis ("Observer Participation thesis") 
would seem to indicate his agreement with me about what constitutes an "observer", 
namely anything able to participate in an irreversible change. <<
That happens also to be mine:) It is a good concept that concedes what has to be conceded 
without selling out altogether. It also is in stark conflict with relativistic concepts -- whoops, 
let's stay on course here:)
CML: That's good. Then you're amenable to a formalism in which the universe perceives 
itself through the objects it contains. That's SCSPL, and that's the CTMU.
RFV: >> This reflects the distributed subjectivism and self-processing character of 

reality...the fact that mind (or "observership") and reality can't be absolutely distinguished. 
<<
Perhaps. What I am pushing on to understand.
CML: Compare "distributed solipsism" to "distributed subjectivism". 
RFV: >> It's even simpler than I put it above. Experience entails something that experiences 
and something that gets experienced. So if reality consists everywhere of experience, it is 
everywhere self-experiencing, and has everywhere the character of both mind (the 
subjective experiencer) and objective reality (that which gets experienced). <<
A good notion, but I see this from my background as more of a demonstration of 
observer/controller equivalence duality. From absorber theory (Wheeler also) there is no 
significant distinction to be made between emitter and absorber. In tracking technology, the 
algorithm for tracking many targets to minimize uncertainty is essentially the same (an 
inverse) as that to situate sensors so as to maximize observability.
CML: Two different languages describing precisely the same thing. The main difference: my 
language has been developed in the direction I've been outlining, resulting in a global reality 
theory.
RFV: >> So the categorical imperative relating the subjective and objective aspects of 
reality everywhere coincides with experience; <<
I have always seen the duality as action/observation -- both fairly
closely associated with a subjective view of reality overlaying a
physical level. Not worth persuing here probably:)
CML: Maybe so. One of the best ways to achieve understanding is to see the same things 
in as many different ways as possible. But remember; according to distributed solipsism, 
which is simply a correlate of yesterday's "blue shades" analogy, "subjective reality overlays 
physical reality" dualizes as "physical reality overlays subjective reality". The dual 
viewpoints are complementary and mutually essential, and this is the point. From this 
duality, plus a big load of logic, the CTMU follows.
RFV: >> I see improvement, by God! But in any event, just don't say I didn't try my best! 
Keep up the good work. <<
You've done well! Please keep it up.
CML: Hope you like this as much.
Part V
RFV: Chris: Thanks for the continued help in understanding where you're coming from. It is 
indeed a great privilege to get such insights from the originator of a theory.
I'll chop old conversation that it seems like we agree that I understand and continue on the 

ones where I've had difficulty.
...
>>> No. It's a self-perceptual system with a combined abstract and concrete nature (as 
explained above) that possesses closure with respect to other realities. <<<
>> So, do you mean a conceptual model of "reality" which could be very
different than our own reality? I think so. I see "self-perceptual" as a block to my acceptance 
here. I think you make the leap that
isomorphic structures are the *same* structure to say that. Am I
right? If so, how is that justified? <<
...
> So the objective and solipsistic models are as indistinguishable as U and T, abstract and 
concrete, and we have to drop the distinction. And in this case, (objective) isomorphism 
equals (abstract, subjective) identity; when we talk about T and U, we're really talking about 
different aspects or characterizations of the same thing. So the perceived universe and the 
perceiver are one, and the universe is "self-perceptual". This is where reality theory meets 
the philosophy of mind. But that's fine, because as should by now be obvious, they need 
each other. <
RFV: Your treatise was excellent! (I am a bit surprised that you've never mentioned Bishop 
Berkeley though:)
I do think I understand where you are coming from on this and I would like to suspend 
judgment on it for the time being. There is very much a hard-nosed aspect I like in accepting 
what we know without wistfully assuming more. It's a quite scientific approach to my mind. 
However, I believe that solipsism can *not* intuit distributed self, i. e., "I
think, therefore I am" does not work for "we" or an amorphous ubiquitous self any more than 
it worked in proving the existence of God. And if one were to reject the distributed subjective 
aspect, it would seem to me that the feasibility of an extensive isomorphic relationship 
between "my" mind and the universe comes into serious question.
CML: Those unfortunates who formerly tried to prove the existence of God using solipsism, 
cogito ergo sum, or comparable failed because they unwittingly remained in the context of 
the objective (Cartesian, mind-versus-matter) model without correctly discarding the 
absolute distinction between the subjective and objective pictures of reality and availing 
themselves of the implications. The whole point of what has gone so far is that you CAN'T 
"reject the distributed solipsistic aspect"; the distinction between solipsism and objectivity is 
ultimately imperceptible and therefore empty of content in any perceptual reality. You have 
to reason without the distinction when reasoning about reality, and that means that the 
abstract and concrete phases of reality coincide. Restoring the distinction on an a priori 
basis is simply unjustifiable.
RFV: You have set up an analogy to the mapping between the inside and the outside of a 
sphere, i.e., x --> 1/x, x complex. The analogy works fairly well. The problem I see is that 
awareness of the mapping -- or creation of the mapping isn't included. That remains either a 
fact or not. T remains a subjective map of the objective U. How can T become
the map of U *and* the map? That is the question.

CML: Where the universe is a self-similar structure whose overall characteristics are locally, 
endomorphically replicated everywhere within it – and that’s what “distributed over it” means 
with regard to its syntax - the mapping of which you speak is just the endomorphism in 
question. It’s inescapable and does not have to be justified in particular cases. Equivalently, 
your absolute distinction between what's inside and outside of a sphere is really a 
provisional distinction that vanishes above a certain level of abstraction; although the 
interior of a particular sphere is localized within its boundary, the raw syntactic potential for 
"being inside a sphere" is distributive (because the abstract mathematical laws that 
characterize this state are distributive). 
Whereas your distinction applies with respect to the geometric interior of a particular 
sphere, the mind exists on a higher level of generality than that. When we talk about the 
laws of cognition, including those of math and physics, we're talking not merely about actual 
brains enclosed in actual skulls, but the abstract essence of brains, i.e. mind. Mind 
distributes over this entire universe, and with it, the POSSIBILITY of being inside or outside 
a particular brain. Possibility is a generalization of actuality representing all conceivable 
actualizations, and this describes the reality syntax.
RFV: Another aspect that bothers me is the following: A theoretical basis of holography is 
that if one can determine the electromagnetic field throughout a window surface, one can 
determine the field throughout the volume on the other/either/both side(s). This is a powerful 
but nonetheless legitimate statement. But with it there is an extreme implied simulation 
capability -- I guess we should have known from our screens:) So (and importantly) the 
window itself may be all there is -- it is at any rate the *best* place to watch for something 
new! The value of the *window* seems somewhat diminished by some of your 
presumptions. In fact it seems to have paled into insignificance. If I were to develop a theory 
with the scope of yours, it would be centered on the window itself and the window treated as 
the only thing "real." Did you happen to read my "Working with the Given" in GoF #99?
CML: I'm not making presumptions. Rather, you're presuming that there's an absolute 
distinction between the window and what's outside it. The holographic analogy was just that: 
an objective approximation of a self-contained (globally subjective), self-perceptual system 
whose existence we established by eliminating the empty distinction between the mental 
and the objective, and thus between the observer and the observed. In a so-called 
“holographic universe”, the interference pattern cast by the hologram is internally projected 
or simulated. As I've previously explained, the universe can be regarded as "self-simulating" 
(realitycheck.doc; geomusser.doc).
RFV: Another thing is that "reality" is more than what I (and I believe anyone) can 
"mentate." There are aspects which currently defy modeling although I do believe ultimately 
they *will be* modeled. These aspects of U find no place in T and are carried in my(?) mind 
(or not) as little unmodeled chunks of data {u_i}. So the isomorphism must hinge on T <--> 
U-{u_i} or T+{t_i} <--> U. And since there are so many of these chunks and the {t_i} are so 
unmentatable currently (at a minimum), the perfectness of the isomorphism is somewhat 
destroyed.
CML: You have a limited capacity to mentate because your brain is able to explicitly 
replicate only a part of the reality syntax; your cerebral neurons don’t fire in explicit 

isomorphism to the entire syntax, but only a restricted part of it. But the overall syntax of 
which your own is a restriction is, as I've been trying to point out, distributed. You seem to 
be assuming that T exists strictly inside your head, and if T is empty of physical content, 
perhaps it does. But for purposes of this discussion, T equals a *global distributed 
formulation of the reality syntax* in which *all* aspects of U "find a place". Such a 
formulation certainly exists whether or not we know what it is (and we don't); specifically, it 
exists in the conspansive T=0 state in which the evolution of the universe is instantaneously 
realized, "after" we've selected it from the set of all possible evolutions and formulated a 
nomology to render mutually consistent the perceptions we create. At this stage, theory 
equals reality: M(T(U)) = T(U) = U.
One distributed ingredient of this distributed formulation that we can be sure of right now is 
the set of 2VL tautologies, which cannot be violated with regard to perception; you'll never 
have a real perception that is both T and F unless you simultaneously inhabit two parallel, 
mutually inconsistent universes. That's just the way it is; any two people who won't both 
admit that the perceptual basis of reality is 2VL have nothing to discuss. Were 2VL 
mistakenly acknowledged to fail for the real content of this discussion, then insofar as this 
discussion is taking place inside the reality being discussed – i.e., inside its own content - 
2VL would inevitably fail for the discussion itself. That would make this discussion, and 
indeed every discussion, irresolvable. But in that event, math and science would be a 
useless farce, because what one of us "perceives" to be true (and even tautologically true) 
may not be acceptable to the other.
RFV: Yet another is the equivalence you assign to isomorphism. It's admirable to the extent 
of going with *only* what one knows, but less admirable I believe in possibilities of 
enshrining error.
CML: As I've already explained, any isomorphism to whose range and domain you can't 
assign different qualitative or quantitative coordinates at a given level of discourse is really 
an identity relation (at that level). In that case, the range and domain differ only as 
coincident "aspects" of one and the same reality. You're still talking about T being 
essentially different from U, even though we've shown that at a sufficiently high level of 
content for T, T = U. Again, T is in this case the reality syntax - the mathematical and 
physical laws that the universe obeys everywhere - and objective reality (U) is merely the 
variegated actualized content of that distributed form. 
For example, the law of conservation of momentum, and its underlying arithmetic, is 
everywhere valid, but only active and perceptible in the presence of actual moving bodies. 
Similarly, physical reality is merely an evolving set of instantiations of a set of relations that 
are everywhere valid. These instantiations are metrically separated, but the metric itself is 
completely characterized by a distributed form that exists in every part of space that could 
ever be occupied by your brain (even if your brain has only managed to figure a little bit of it 
out by firing isomorphically to it). The distribution of this form throughout space distributes 
your mind over reality in the form of general mental syntax. 
RFV: Those are my reservations, but I understand what you have said, appreciate it, and it 
will ride with me, perhaps ultimately persuading me. You may continue hammering on this 
or accept this limited *conversion* as you see fit. I *do* enjoy the dialogs on the subject.

CML: As do I. It helps me understand where the rough spots are, and encourages me to 
find ways to get past them. I think one of the problems so far is that you haven't really 
absorbed the fact that the distinction between mental (abstract) and objective (concrete 
physical) reality is a stratified distinction that gradually disappears as we approach the top 
stratum (global distribution). Less general, more conditional features of reality appear to be 
localized within their specific instantiations, permitting internal-external distinctions. In this 
sense, cognitive (or computational) distribution relations are equivalent to topological 
inclusion relations (that's a CTMU duality principle). If distribution is general or 
unconditional, on the other hand, it applies to all things, and the distinctions among these 
things exist within it in the sense that it resembles a “self-executing program instruction” that 
internally “simulates” them. In other words, general features of (simulative) spacetime 
include specific aspects as output, which is then uploaded to spacetime as further 
programming, and so on. Topological inclusion follows from computational inclusion 
because topology is computationally simulated.
>>>>> real objects necessarily conform to more or less exact configurations of an abstract 
distributed syntax. <<<<<
...
>>> If we form a theory of some aspect of reality, it may or may not be
correct (i.e., it will be "more or less exact"). <<<
>> "More or less" "correct?" Right? Exactness is a small case in point? <<
> Yes. There are two kinds of truth, syntactic and semantic. Syntactic truth amounts to 
internal consistency, and semantic truth amounts to external isomorphism or quantification. 
If T is true on neither the syntactic nor semantic level of reality, then it has 0 scope or 
content. But then the theory has no model and is not a theory in the strictest sense. At best, 
it's just an empty (content-free) pattern of neural events or an inky set of typographical 
symbols on paper, with irrelevant fantasy for content. <
RFV: But such things may become the "Eureka!" source next week! Is the entire universe 
reborn in that case?
CML: If T has no model, then it has no real content, and in that case, "eureka" becomes 
"you-reeka". Now, maybe its apparent lack of content is only temporary because we're 
misinterpreting it, which means that we haven't accurately identified its constituent 
concepts; or perhaps the only thing that prevents it from having all kinds of real content is 
one little piece or two that we'll eventually recognize as unnecessary and discard, leaving a 
big pile of "eureka" material. But then the part of the theory that is found to be correct will 
have possessed real content all along. I.e., the content of a theory is not necessarily 
dependent on what we recognize it to be at any point in time. 
> That part of a theory that's wrong is empty, but the part that's right has real content. <
RFV: The wrong part of a theory may be the *handle* of its replacement and thence the 
most real aspect of it.

CML: As my last entry acknowledges.
> The theory tautologically corresponds to just that part of reality for which it's true. In fact, 
logicians already know that insofar as they can often either extend or qualify a troubled 
theory so as to adjust its referential quantifier (think Lowenheim-Skolem, Duhem-Quine, 
etc.) and thereby restore its validity (think classical physics as the low-velocity "limit" of 
relativity physics), the idea of an "absolutely false" theory is not what it seems. <
RFV: I'm not sure I fully understand the last part of this last sentence -- it may, of course, be 
that I didn't understand the *first* part or the sentences that preceded it:) Would you please 
try to explain this to me? It's interesting.
CML: The falsifiability criterion of scientific theories is rather tricky. If you have a theory that 
all cats are gray, including a reason for that prediction, it’s “falsifiable” only in that part of the 
universe containing non-gray cats. And even after it’s been falsified, we can still resurrect it 
in that part of the universe where all cats are grey and your reason applies. All we have to 
do is create an “extension” of your theory that applies to the rest of the universe. Once your 
theory has been embedded in a correct overall theory, one can no longer say it’s been 
“falsified”. It simply applies to a smaller part of the universe than originally intended, and is 
quite true for that part. 
In addition, once we do away with Occam’s razor, every interpretation of a theory becomes 
a theory in its own right, each of them as correct as the next. And even worse, every 
possible unverifiable (and thus unfalsifiable) extension of theoretical content screams for air 
time, all kinds of “true” but bizarre and mutually contradictory theories raising their ugly 
heads for keeps.
Most telling of all, there are two kinds of falsifiability: rational and empirical. The former kind 
is employed in rejecting erroneous mathematical proofs and even mathematical theories of 
physics; the latter, in rejecting physical theories. A true logical or mathematical theory is not 
empirically falsifiable, and neither is the 2VL logical syntax validating the falsification 
process itself! The syntax is literally tautological.
> The point of all of this is that a theory can evolve, changing its form to absorb more and 
more real content. That's what I meant by "more or less exact". Perhaps "more or less 
correct" would have been better.<
RFV: Yeah, OK. That's what I thought, but in the explanation I got some interesting 
understandings. This does put my notion of the {u_i} into a dismal light and I don't exactly 
know what to do about that. I think I'll leave it as a record of my discontents along the way:)
>>> But even if not, we can be sure that the correct theory will conform to our mental 
structures, since otherwise we won't be able to create a conceptual isomorphism with it (and 
that's what a theory is).<<<
>> I have a hard time with this. An analog computer is an isomorphic structure of a screen 
door when it is set up to solve the screen door problem, however, a general purpose 
computer is *not* isomorphic with a screen door when it solves the screen door problem. 

General structures are adaptable in a sense which allows them to accommodate an 
isomorphic model without themselves being limited to anything like that model structurally. 
So in what sense (and to what degree) do you insist on
this conformal mapping? <<
> There is a very general abstraction called logic that permits various
mappings among devices and other physical contexts. Have you ever heard
the term "up to isomorphism"? It generally refers to a relationship that contains an 
isomorphism, but exceeds it. Some logical abstractions permit multiple isomorphisms, and 
these isomorphisms may be either structural or functional or both (in the above question, 
you seem to be talking about these two distinct kinds of isomorphism). <
Indeed. More-or-less as levels of isomorphism? BUT, if one allows levels or degrees, then 
isomorphism can certainly not *mean* equivalence -- nor does it. Then to substitute 
"equivalence" seems unwarranted without being able to demonstrate isomorphism at *all* 
levels.
CML: Isomorphism always denotes “equivalence with respect to ( )”. Two triangles of the 
exact same size and shape are isomorphic with respect to structure, but if one’s red and 
one’s colorless, the isomorphism holds only with respect to geometry and gives out with 
respect to color. Were one triangle to have more structure than the other, homomorphism 
would be the best we could do; but since color is independent of (and in fact irrelevant to) 
geometry, isomorphism holds. It’s just that for the colorless triangle, including color in the 
isomorphism doesn’t even make sense. So the triangles have equivalent structures but 
differ in aspects like color and location. On the other hand, perfect isomorphism with respect 
to all predicates describing two objects means not just equivalence, but identity. 
> In any case, for computational purposes, the isomorphisms you mention are logically 
equivalent up to isomorphism with the screen door problem (whatever that is). <
RFV: The problem is to determine the angle of a screen door that has a spring and dash-
pot. It's solution is that of a simple differential equation. In an analog computer there is a 
resister whose resistance is scaled to the resistance in the dash pot on the door, the 
inductance to the spring constant, etc. and the current in the electric circuit is related in the 
same way to the angle of the door. It's all one-to-one. In a GP computer of course the 
analogies become much less obvious in the sense of there not being a continuous nor 
continual solution, i. e., it doesn't hardly map at all. In a DDA (Digital Differential Analyzer) 
the physical structural analogy of components is maintained with digital registers and the 
flow of overflow bits becomes and analogy for the current/angle. Just a side tidbit:)
> The "abstractions" being employed by the two types of computer are always expressible 
in terms of the penultimate abstraction, logic. Nothing I've said so far is incompatible with 
this fact. <
First of all, it is in a very obscure sense that one can say an analog version is implemented 
in logic because it *isn't* anymore than an electric current is logic. Secondly, you seem to 
be implying that *logic* is a structural level of the brain -- maybe that's where 2VL comes in. 
But I don't think so. I believe logic in the sense of AND,

OR, NOT and IMPLIES is an acquired capability of the mind superimposed many layers 
above the structure of the mind. I don't believe we are *primarily* a "logic machine." We are 
pieces of U (conceivably:) capable of implementing limited complexity logic circuits.
CML: The brain is indeed a logic device (a neural network), but this is hidden by the fact 
that it employs logic to construct modes of activity that are apparently illogical in character. 
Electric currents are logical in essence as well, obeying a 2VL theory of electromagnetic 
interactions whose statements and their negations are T and F respectively. The EM 
currents behave isomorphically to the theory, and the theory behaves isomorphically to the 
logic inherent in its structure. Every “nonlogical” predicate in the theory is in fact embedded 
in one or more logical variables or relations just like a seed is embedded in an apple. 
EM current “is” logic because insofar as it behaves coherently enough to conform to a 
theory, it must coincide with something that permits it to do so. Whatever that something is, 
it obviously includes logic (since if it didn’t, the current could not behave in a coherent way 
or even maintain its existence = T (as opposed to nonexistence = F). The mistake is to 
underestimate the tendency of logic, often disguised by “other” elements of reality syntax 
which are themselves logical in form, to naturally assemble itself into seemingly illogical 
configurations and processes.
RFV: At this point I'm going to repeat your original sentence from above:
>>> But even if not, we can be sure that the correct theory will conform to our mental 
structures, since otherwise we won't be able to create a conceptual isomorphism with it (and 
that's what a theory is).<<<
There is a sense in which M(T) <--> T for any possible T. Of course there are probably even 
existing theories that have not yet been completely comprehended, M(T), although already 
formulated, T, in which case you seem to be content with M(T) *being* what a theory *is*. 
And there is definitely a sense in which T <--> T(U) and you seem to be
content with T(U) *being* U because if there were more *known* of U it would be in T(U). I 
think what I feel certain of at this point is M(T) <--> M(T(U)) <??> U.
CML: Theories consist of abstract relationships. To whatever extent they are correct, i.e. 
apply to some part of reality, they coincide with that part. Why? Because the abstractions 
that correctly describe a part of objective reality, whether or not you are aware of the actual 
correspondence, are exactly the ones that are indistinguishable from its concrete 
components (as in the blue shades analogy of yore).
...
>>> A syntax takes the form of general information and is implemented by generalized 
cognition.
...
RFV: Whoa. Chris, you've got to start me all over on this one. I thought I was about through 
the original sentence:) I think I buy the original sentence with the extension of "generalized" 
cognition within my limitations expressed above. BUT...

> The phrase "generalized cognition", actually synonymous with time,
How in hell did we get here?:) I take a "synonym" to mean that a dictionary can help me, but 
it doesn't here.
CML: All you need is your memory, because we’ve already been through this. The 
operative analogy is time : space :: cognition : information. I.e., time = info-processing = 
generalized cognition (where information here refers to the abstract dimensions of real 
objects and processes, which, as we’ve already seen, coincide with them).
> simply extends human cognition to generic information processing. <
RFV: How does/can "time" perform this function?
CML: Time is not a “thing”; it’s a function. . .a function of spacetime. Spacetime performs 
this function because it’s full of syntax, and some of this syntax is grammatical as opposed 
to structural. Grammar governs the evolution or state-transition of the language of reality.
> It's this sense in which generic information processing obeys a "cognitive syntax". <
RFV: I was left on the other side of the river on this one. Help!!!
CML: See my last entry above.
> Thus, time is any grammatical implementation of the SCSPL reality-syntax. <
RFV: I think we have to hold off on SCSPL a little longer.
...
>>> A photon is a configuration of the syntax of reality; <<<
>> Chris, you're going to have to lighten up just slightly on some of this stuff:) What you 
appear to be saying is that a photon is properly speaking a particular instance of the general 
structure of reality. That isn't very informative. What *do* you mean here? <<
> Just that a photon obeys certain structural and behavioral rules, and those rules exist as 
constraints within the overall set of constraints called "the reality syntax" (including the laws 
of math and physics, of which our exhaustive knowledge is not assumed). These are the 
abstractions to which the photon primarily corresponds. Stay with it, Fred - this is all really 
very simple (given the unnecessary compartmentalization of physical concepts in diverse 
formalisms, maybe the problem is that it seems *too* simple). <
RFV: This part I have gotten largely because of your patience, so stay with me:)
...
> That a photon is both real and abstract is an absolute, nonprovisional fact. However, what 
counts as "real" and "abstract" in any given case is a relative (or provisional) matter, and 
sometimes even a matter of semantic convenience. I sometimes tend to assume that my 

meaning is obvious when it isn't. <
RFV: This was very clear.
>>> but given from the outset. It's only the exact corresponding configuration of reality-
syntax that remains a mystery (and in a sense, not even that). <<<
>> I assume you mean that the actualities of the behavior of photons are all that is (may be) 
unknown? Its place in the structure of reality *is* known? <<
> Some aspects of a photon's behavior, and corresponding details of its
place in the structure of reality, are known. But the intrinsic structure of the photon - the 
syntax it obeys - is not known except through this behavior. You can't hold a photon under a 
microscope and dissect it; all you can do with any subatomic particle is collide it and monitor 
the resulting events. Sometimes, you get what appear to be objective subparticles, but 
again, you can only see them through their effects. So all you can say for sure is that one 
configuration of SCSPL reality-syntax is merely decomposing into others. Getting too hung 
up on "objects" and their "innate properties" has been a recurrent problem in science; one 
generation's "particles" are the "subparticle interactions" of the next. <
RFV: Other than the reference to SCSPL (which I'm deferring) I think I
understand this and certainly agree with it!
> We need to realize that what we're dealing with are transformations of a global nomology 
or "syntax" (or to oversimplify and appeal to popular terminology, of the "TOE <
"theory of everything?"
CML: Yes. The TOE is envisioned as a master equation governing all of reality . . . a 
condensation of all the laws of physics.
> ultimate equation", along with the syntactic structure of the language in which it will be 
expressed). <
RFV: I see where you're coming from but slipping the word "nomology" in there was a dirty 
trick because that's the part I'm having trouble accepting:)
CML: “Nomology” refers to the abstractions governing the evolution of reality. It’s inherent in 
spacetime. There is no assumption that we can ever do more than approximate it in any 
human theory. All we know about nomology is that it is what connects events, and is what 
effects the consistency of our perceptions. Nomology must exist up to determinacy, or its 
absence would show up in the form of irresolvable perceptual paradox. A distributed 
abstract nomology to which perceptual reality must conform ensures the resolvability of 
paradox, which is just a way of saying “the self-consistency of reality”.
...
>>> Analog computers are as bound by 2VL as are digital computers; an analogy either 
exists (1) or not (0). <<<

RFV: But that analogy is *not* performed by the analog computer, so I still don't see where 
you get that such a computer is 2VL. The use of its results may involve 2VL, but its 
operation doesn't.
CML: Without 2VL, it would have no coherent “results”.
>>> All distinctions are 2VL; <<<
RFV: *It* doesn't make distinctions.
CML: Yes it does. It distinguishes between what it is and what it isn’t, and how it must 
behave and how it must not behave. These distinctions are encoded in the spacetime it 
occupies.
>>> one thing is either distinct from another (1) or not(0) on a given level of definition. <<<
RFV: Is an electron in a circuit of my computer distinct from one in yours? I think yes. Is an 
electron in an adder circuit in my computer distinct from one in my instruction register 
circuit? Is an electron in *a* circuit distinct from another bound electron in a copper atom of 
the wire through which the electron travels at that moment? Is it distinct from adjacent band 
electrons in the flow? Distinctions get a bit fuzzy.
CML: Our respective electrons are distinct images of a single distributed “syntactic photon” 
consisting of the rules that all photons must obey regardless of where they are. The images 
reside in different locations, and these differences are significant. But it’s like Wheeler said 
to Feynman: they’re all one photon.
>>> Since information consists of distinctions - constraints differentiating one thing from 
another - information is 2VL. Since theories consist of information, theories are 2VL. <<<
RFV: I can accept Whitehead and Russell on that:)
>>> And since theories are isomorphic to reality, reality is 2VL. <<<
Where I'm at, you've just convinced me that M(T(U)) is 2VL.
>>> There is no escape...no crack in the stone of truth versus falsehood. Even to explain 
why there might be, you would be forced to resort to 2VL...and your putative exception 
would be embedded in 2VL by fiat. <<<
That's certainly true for M(T(U)).
CML: Great! Because the isomorphism M is a perfect equivalence relation with respect to 
everything but degree of generality (the abstract is more general than the concrete because 
one abstraction can possess many concrete actualizations). So M is an equivalence relation 
between T and U up to isomorphism (i.e., up to exhaustion of the structure of U).

>> This is not very convincing to me. First of all, accepting your premise that all distinctions, 
knowledge, etc. are 2VL does *not* prove anything about "reality." Ideational objects -- even 
given the isomorphism -- are *not* *the* objects themselves or exact equivalents -- there 
are, for example, things which are not known about photons that are true of photons but not 
yet integrated into the ideational counterpart. So we have isomorphic structures of the 
reality of which we are aware and the reality we thereby presume. And we are in some 
sense limited such that our truths pertain to such a reality, but what about undecidability and 
such notions. How do these not end up being cracks in the stone? One must reason 
decidably about undecidability which doesn't, thereby, prove that all things are decidable. 
<<
> First, logical consistency equals the syntactic phase of truth. <
RFV: I buy this except that I'd mellow out "truth" just a little to something like "correctness" 
or "verifiability."
CML: No, the syntactic phase of truth equals consistency. That’s what the predicate “true” 
means with respect to syntax.
> The stability (objective perceptibility) of reality implies that it's
consistent, <
RFV: *per*sistent at least. Perhaps one could argue from persistence in U to consistency in 
T(U).
CML: Yes, one could. As T converges on the reality syntax, U and T(U) become coincident. 
> and the fact that it's consistent implies that it conforms to 2VL. <
RFV: I think there's a logic hop here I didn't make. An analog computer's solution is 
persistent but is not thereby (in my opinion) 2VL.
CML: See above. Consistency is a 2VL concept synonymous with the tautologies (T OR F) 
and NOT(T AND F). Since reality by definition contains all truths - otherwise, they wouldn’t 
be “true” - its consistency implies that it contains no falsehood (since such a falsehood 
would contradict the truth corresponding to its negation). So consistent means true means 
included in existential reality.
> Again, something either exists (1) or does not exist (0), so as soon
as you say that it's real or that it exists, you tag it with a logical
predicate (1 = true = reality-inclusion) that anchors it to the entire
logical syntax of which this predicate is a part. <
RFV: I totally disagree here. Saying that something "exists" adds nothing of logical import to 
a concept in this context. Do the true (1) statements, "Mastadons do not exist," (1/0) and 
"Unicorns do not exist" (1/0) imply the same level of nonreality (0)? If "existence" is 2VL, it 
should.

CML: “Exists” = “is included in an existential system”, which implies “is (at least) included in 
(the most basic existential level of) reality”, which implies “is consistent with the rest of 
reality”, which equals the syntactic phase of truth. Now, T and F only become contradictory 
when they coincide in some actuality; fortunately, “Mastodons do not exist” need never 
coincide with “mastodons exist” in any actual context. That is, “Mastodons do not exist” can 
be interpreted over a range of actual times for which it is true. On the other hand, “Unicorns 
do not exist” can be interpreted over an even larger real context, and to that extent is 
quantifiably truer in a concrete context.
> Of course, we can still make a distinction about which level of reality includes this 
something, but whatever that level is, it obeys the distributed tautological syntax of reality 
(logic). <
RFV: Granted you can probably escape the former argument with a defined "level of 
reality." But more importantly an electric field is not defined in terms of whether it "exists" or 
not. It is more, "given x, then y," such that existence becomes very much a side issue in T 
at least *and* there may be "hidden" parameters {p_i} in a theory for which not everyone 
envisions a counterpart in U. So the "existence" of {p_i} in T leaves its existence in U (and 
therfore at all) quite up in the air.
CML: If relevant, the parameters p_i are (at the very least) permissible transformations of 
reality syntax that may be realized in physical reality. Color force among quarks is a pretty 
weird parameter that has theoretic utility, but can’t yet be empirically established. The 
configuration of syntax we’re currently working with won’t actively display it. But one day we 
may contrive to force a transformation of syntax that does. Moreover, this transformation will 
itself be governed by the syntax that is transformed. I.e., the syntax is autologous. 
> Second, decidability is a logical concept whose definition is based solely on logical 
(syntactic) operations, namely those executed by Godel in defining it. So you can't use it 
against logic without undermining its definition and thereby invalidating it. <
RFV: I am not very enthused about Godel's destruction of _Principia M._ myself and I'm 
somewhat interested tangentially in your statement here.
> I suspect that what you might be positing is an abstraction exceeding
> logic and consistency...an even more general predicate.
No. But thanks for thinking so highly of me:)
> Indeed, that's the only thing your description would fit. No problem; the CTMU contains an 
ultimate predicate called UBT (unbound telesis). But logic is a closed tautological system, 
and the relationship of this "pre-logical" predicate to logical reality is appropriately 
constrained.
Whoa again! If I'm going to stay in this monastery, you have to go
slow.
CML: Information consists of distinctions in which one thing is syntactically constrained to 

differ from another. Because concrete reality mirrors the abstract information coinciding with 
it, it too can be regarded as information (we’re information too, and the reason all this info 
seems “concrete” is that there’s a syntactic displacement principle in effect). So the 
question arise: what does this information become when all the binding constraints are 
removed? The answer is Unbound Telesis or UBT, the groundstate of being. This universe 
has only arisen within the UBT groundstate because it has constrained itself to do so; this is 
the sense in which it’s “self-configuring”.
> After all, what I'm trying to tell you about is the *logical* structure of reality, without which it 
would be no reality at all. For God's sake, don't interfere...your world could dissolve into an 
illogical chaos of ambiguities right before your eyes (and you're a part of it)! <
RFV: It has! It has:) But I'm feeling better. Please don't do that to me again:)
> ...
> employers don't know the meaning of "vacation time". And speaking of
> saloon barons, you apparently run one of the great "salons" of the
> 20th-21st centuries!
Slipping logic by me is one thing, but when someone tries to get humor by me, I have a 
hard time with it:) What on earth do you mean?
CML: The salons of the nobility used to be the most lively, scintillating conversational 
settings in the world. Salon Baronßàsaloon baron.
...
>>> As explained in the preface, mental operations cannot be distinguished from real 
operations except in a provisional way. That's what I mean by "a CTMU context". It's given 
a priori. <<
>> Nor of course can they be equated in other than a provisional way. I understand now, 
but this need not be a "CTMU context" for the sense to have real meaning. <<
> Yes, they can be absolutely equated, because as I explained above, we
ultimately can't prop up the distinction between them (distributed
solipsism, remember?). <
RFV: Yes, but you must remember that you left me back there on the peg labeled M(T(U)).
Not really, at least when T converges on reality syntax.
...
>> In view of what follows, I need a little definition of what this
generalized "mentation" is. Does it relate to that which takes one form of reality into another, 
or what? <<
> Any construction you place on "mental" above and beyond pure information processing, 
information transduction, state transformation, etc. is, as we observed last time around, 

superfluous and anthropocentric. <
RFV: Let me backslide just for a little reverie here. What about "likes," is that a 
"construction" on "mental?" I think so. So if it isn't carried into "generalized 'mentation'" 
where is it? Ok, so it's "anthropocentric," so what? Do we then have M(a,T(U))?
CML: No, what we have is a component of the cognitive syntax called ETS, short for Emo-
Telic Subsyntax. The emotions are subjective predicates like qualia; special subsyntaxes 
are reserved for the predicates in these groups. But the relationships of these predicates 
with other syntactic elements are logical in nature. Enough on that for now.
> These are what is meant by "generalized mentation"...what you get when you take the 
"men" out of "mentation".
RFV: Sometimes your technique reminds me of men of God in my past:)
CML: Against my better judgment, I’ll take that as a compliment.
> Again, think "time" (the grammatical implementation of reality-syntax by reality itself, i.e., 
reality self-processing). <
RFV: Chris, Chris,... I'm not ready for baptism. This is only my second lesson for god's 
sake:)
CML: That’s OK, we’ve been through it at least twice up there.
> Remember, where "abstract" and "real" are the same, so are mind and
reality, and so are human and nonhuman info-processing; <
RFV: Got it!
> Cognition <---> time. <
DON'T got it!!!!
CML: That’s OK, we’ve been through it at least thrice up there.
> The logic is so simple it hurts!
RFV: I know, but I still ain't got it!:)
CML: That’s OK, we’ve been through it at least fource up there.
> Human cognition, assumed to be fundamentally unique by anthropocentrists everywhere, 
is just the temporal processing that goes on in a brain. [Of course, there may be more to it 
than that - and there is - but for present purposes, that's good enough. The "more" in 
question has to do with distributed solipsism, and the consequent fact that there is a sense 
in which sentient cognition is primary...that what goes on in reality, namely temporal 

processing of all kinds, really goes on in the minds of sentient observers.]
RFV: This soothes my mind, but all of this is *not* just "time!"
CML: Sure it is. It’s the refined kind of time that passes in a brain.
...
> Yes. A photon is a very fundamental object that can be described as
"expanding" at the speed of light, and then "collapsing" someplace on its own expanding 
wavefront. This amounts to "motion of primary type", and is how the spacetime metric is 
created layer-by-layer in the conspansive model (CTMU spacetime). When a baseball 
moves, it's moving in a much more continuous fashion through a sequence of metric 
domains created by conspansion (as described in that Noesis/ECE 142 file I sent you, 
geomusser.doc).
RFV: If that was more than a brief page or two of text, I'm quite sure I did not get it *or* *get* 
it!
CML: Re-sent and acknowledged.
> That's "motion of secondary type". But I hope we're not getting ahead of ourselves here. <
RFV: I think we may well be.
...
>> However, that is a long ways from saying that I accept that complex
formulations of reality exist outside the brain! Not that I couldn't
ultimately be convinced, but am currently not. <<
> Again, Fred, your distinction regarding what's "inside" and "outside" the brain doesn't 
necessarily work for the mind corresponding to the brain. In a solipsistic model, the brain is 
merely the localized objective terminus of a material endomorphism (internal self-mapping) 
of the subjective, distributive mind. Because the mind shares the syntax of reality, and that 
syntax is scale-invariant (or geometrically self-distributed in the sense of a hologram), its 
boundaries are those of the reality it perceives. But you can't fully understand this without 
understanding the conspansive model of spacetime, <
You're right. I'm not there yet.
> in which objects literally "include themselves" in a spatiotemporal sense. That is, their 
SCSPL syntactic images, which are basically cognitive in nature and correspond to the 
subjective phase of their existence, temporally configure the metric in a cumulative step-by-
step way by "expanding through space" at the common rate c (that's inner expansion) and 
then "recollapsing" to define the objective phase of their existence (that's requantization). 
Inner expansion and requantization correspond to coherence and decoherence of the 
quantum wavefunction. <
this part looks interesting but so far from the outside.

...
>>> Let me save you some time. The isomorphism is given on a de facto basis. Indeed, it 
follows from the preface. <<<
>> This is my *big* problem so far. It occurred above and now here. This is where we need 
to concentrate effort!!!! I have *not* bought in here yet. <<
> Again, the syntax of the mind is that of reality as a whole; that's the distinguishing 
characteristic of SCSPL. <
Even ignoring SCSPL for the present, this equivalence from isomorphism of T to U is still a 
difficulty in as much as I have only accepted M(T(U)) <??> U, and now I must accept that 
U(T(M)) <--> M. I don't know...
CML: Again, where T stands for the distributed reality syntax (or “ultimate theory”), M:TßàU, 
and the isomorphism M collapses because you can’t keep the abstract and concrete sides 
of reality out of coincidence up to U isomorphism (due to the blue shades analogy), M(T(U)) 
= U. 
> The deepest, most general level of this syntax is like a hologram whose image exists 
everywhere within it on all scales. That's due solely to a lack of prior constraint on scale; 
since the syntax is primal, no prior constraints can exist (this is a mathematical criterion, not 
the assumption of some big holographic crystal
dangling in Ur-space). <
RFV: Yes, but saying "the syntax is primal" does not make it so.
CML: No, but *describing* it as such certainly does, provided that description is embedded 
in the kind of model we’ve been talking about. After all, if syntax isn’t primal, then the 
process of its creation required a more basic level of syntax that *was* primal. That’s what 
Hawking is saying (in effect, without dealing with the logic) - you ultimately have to deal with 
reality on a primal level on which its syntax (or nomology) is self-configuring. His No 
Boundary Proposal states that the universe is nomologically self-contained, the laws of 
physics guiding Creation itself. The CTMU is just a more logical treatment of that existential 
necessity.
> The brain partakes of this distributed holographic syntax - the technical term is "hological" 
- by virtue of its topological embedment in physical space. But through conspansion, this 
topological embedment is computationally reciprocated by the mind corresponding to the 
brain; the mind (cognitively) contains the reality that (topologically) contains the brain 
objectively embodying the mind. <
RFV: hmm.
> Logically, this comes down to certain CTMU duality principles mentioned in that 
geomusser.doc file. <

Evidently I need it.
> In particular, a form of duality that can be referred to as "spacetime-object", "wave-
particle" or just plain "conspansive" defines an alternation in which the mind solipsistically 
configures reality by a cumulative selective filtration of syntactic potential; during the inner-
expansion phase of conspansion, the mind builds a set of exhaustive potential future 
realities by superposition, and in the requantization phase, selects a future reality in which 
to re-embody itself as a material brain. Just in case this conjures up some kind of bizarre 
"brains-in-vats" scenario, please remember that the mind,
just like a photon, is regarded as a configuration of the reality syntax. Indeed, all matter 
behaves alike in this regard; as does the mind, so do its contents. <
You have really gone too far too fast for me. let's get me over the two big hurdles first.
...
>>> Only tautological self-distributed systems possess "certainty". <<<
>> The "certainty" or necessity of such a consistent system would indeed be very 
convincing but the "only" here bothers me. <<
> It inheres in the definition of "tautology". Tautology, which denotes informational closure, 
exists wherever stable perception is possible. <
Is that last obvious to you? It isn't to me. Why is it obvious?
CML: Because truth is identified with tautology in 2VL, which is a necessary framework of 
the reality syntax. Otherwise, you could not distinguish which perceptions are “real” (T) and 
exclude those which are not (F). I keep explaining this point.
> Specifically, it must exist in an embedding system of any perceptible nontautological 
system precisely because the latter is stable enough to perceive; <
RFV: We've gone from a "perceptible" system to one that "perceives" without even an 
argument.
CML: Hypothesis: The universe is a piece of self-executing (self-recognizing, self-
processing) code; the universe must configure itself so as to interpret its syntax in situ.
Proof: There is no external hardware on which reality can run, and no external programmer/
executor/recognizer to keep it in touch with itself and running. So the universe must provide 
its own recognizer, i.e. perceive itself. Now suppose there’s a separation between 
recognizer and syntax. Then the syntax can’t recognize its own parts to remain connected, 
and there must be a remote feedback loop within the U enforcing syntactic coherence. But 
then how do the components of this loop recognize each other so as to remain connected, 
and how is the info conveyed by this loop enforced on either end of the loop? This leads to 
a buck-passing regress in which the remote enforcement of coherence must ultimately 
close on a level of syntax that is distributive and completely self-recognizing. And since any 
lesser syntax must be embedded in it, its distributed self-recognition function continues to 

apply.
> closure is what forms the informational boundaries that distinguish and define the 
percepts. <
RFV: You seem to be saying that that which is "perceptible" ipso facto "perceives."
CML: See last entry.
> Unfortunately, we seldom know exactly what the embedding system is (e.g.) because 
definitive axioms are missing. This, of course, relates to undecidability, which states that 
axioms are always missing in logico-arithmetic systems of sufficient complexity. But the 
important thing to realize is that because perceptible informational boundaries exist - 
because the system has enough distributed structure to be coherent and self-connected - 
closure is given at various levels of systemic self-reference. It's an existential necessity in 
the mathematical sense; no informational closure ---> no stable perceptual
invariants. <
RFV: This looks like a place where the basis of holography, i. e., fields on a boundary could 
be used in the argument, but I can't put it together and make it work as you envision it.
CML: That’s because you still aren’t imagining the hologram as a self-contained entity.
> To have certainty at arbitrary levels of specificity, we'd need to know all the axioms of the 
overall system...i.e., a full description of the global systemic tautology allowing perception. 
Obviously, any nontautological subsystem is "open" to undecidable data and therefore 
uncertain. In the case of reality at large, this uncertainty - which, by the way, is associated 
with quantum uncertainty - owes to the fact that increasingly specific axioms are even now 
being created by sentient agents. That's why reality, AKA SCSPL, is said to be "self-
configuring". That's what intrinsic cosmogony is ultimately all about.<
RFV: ?
CML: That should read “open to undecidable perception” for which a T or F distinction can’t 
be made due to the absence of a tautologically-defined truth predicate.
> Sorry if I lost you up there, but explaining this is hard. <
RFV: Yeah. Jesus, if I ever get this I hope I don't become a CTMU bible thumper:) It could 
be worse than being a Jehovah's Witness:)
CML: Especially if you’re trying to explain it to people who know nothing about the subject 
matter, or for that matter logic.
...
>> I still have the problem of not having accepted the ubiquitous nature of 2VL. <<
> It's ubiquitous because if it weren't, we'd have a breach in the boundary between 1 and 0 

(true and false), which would compromise informational boundaries at all orders of predicate 
logic (in the abstract logical phase of reality), which implies the perceptual inconsistency of 
reality, which means logical collapse of the entire perceptual system (due to the systemic 
ubiquity of the truth or systemic inclusion predicate, inconsistency can't be isolated in logical 
and perceptual systems; a hole in truth means holes everywhere!). Note that undecidability 
relates two elements of informational closure or perceptual stability, namely consistency 
(the integrity of the T/F boundary) and completeness (the fact that certain axioms are even 
now being decided and are therefore missing from our worldview). <
RFV: Chris, Chris, Chris,...all I get from this is that if I don't accept it I may be sicker than a 
dog for another two days:)
CML: Yes, by punitive decree of that Big Logician in the Sky.
...
>> Why would the infusion or exit of a few forms every few billion years mean that there is 
"no information?" <<
> Because on the basic level of this discussion, the "forms" of interest are syntactic 
invariants...invariants of perception. If any of these forms were to cross each other, the 
result would be perceptual inconsistency, loss of closure, and the collapse of perceptual 
reality, i.e., the formation of a gap between two discrete, mutually disconnected, internally 
consistent realities in which the inconsistent perceptual invariants are isolated...two mutually 
irrelevant parallel universes. We're only interested in the universe we inhabit. <
RFV: ?
CML: Two conflicting elements of perceptual syntax operating in your mind would by 
definition produce conflicting perceptions between which there exists an irresolvable 
paradox. But your own personal helping of 2VL logical syntax will filter that out of your 
perception. So no such conflict can exist. That’s 2VL tautology put into perceptual practice (I 
already explained why 2VL is an essential ingredient of perceptual syntax).
>> I don't know that I'm ready -- I'm not:) But please administer the
firehose in accordance with your perception of my readiness. I know
there are a couple of concepts that seem like prerequisites that we need to consider 
terminological deficits to be worked on as we go:) <<
> I've been hosing on this for hours now...I hope it helps. <
RFV: Jesus I'm glad to hear you say that because it's been coming out my nose, my ears 
and eyes for hours! I was afraid you weren't even getting up a sweat:) I think we need to cut 
down the pressure just a little, but proceed as you perceive I'm able to gobble this. Maybe 
your perceptions will somehow make a perceptible change in my mental state:)
...
>>> and distributed means general. <<<

>> I don't buy this -- yet:) We could have a very widely distributed
specific thing for which its wide distribution implies nothing but its
wide distribution. <<
> You seem to mean "distributed" in a statistical sense....something sprinkled through a 
preexisting metric space. The kind of distribution we're talking about here is logical and 
refers to the syntax of reality itself, which obviously distributes over that which it governs 
(otherwise it wouldn't be where it has to be in order to do its job). Your kind of distribution is 
defined on the metric; mine defines it. <
RFV: Here's a fruitful patch for more picking I think. A little more on your definition of 
"distributed" might stand me in good stead.
CML: “Distributed” means “without constraint on location”. E.g., when we say that 
multiplication distributes over addition in an expression like a(b + c), we mean that the 
multiplier (a) is active everywhere in the parentheses (b and c) without constraint. 
Distribution has slightly different meanings with respect to the actual and potential phases of 
reality. With regard to the actual, conditions are constraints on location, so saying that 
something distributes “conditionally” means that it distributes only where the condition is 
actually present. When we say that the global reality syntax “distributes over reality” in an 
actual context, we mean that it distributes to conditional and unconditional extents 
according to the quantifiers of its elements and the metric distribution of instantiations. But 
the situation is different with respect to potentiality; insofar as it is possible in principle for 
any set of conditions to exist at any location, we can say that even conditional elements of 
syntax are globally distributed. When we say or imply that the entire reality syntax 
distributes over all of reality, we mean that the potential for any set of conditions to be 
actualized exists everywhere.
Note that any law L that distributes only where a set of conditions A(B(C…)) is actually 
present can be globally distributed by reformulating it like this: “Where A(B(C…)), L”. This 
formulation applies everywhere because it is formulated in a metalanguage of L in which the 
conditions A(B(C…)) may or may not be present. 
...
> Whoops - I meant "circular but nongeneral forms of reasoning that pretend to generality". I 
sometimes tend to forget that not all scientists are seeking a theory of everything, perhaps 
because they run into such spectacular problems when they do. Let me explain. <
RFV: Good! And yes, I am one of those extremely skeptical of theories of everything not 
because I don't think that is desirable or achievable, but because I believe it is not scientific 
to leapfrog the scientific "process."
> Once again, the scientific method entails both local distinguishability and replicability, two 
predicates which become opposed to each other on the syntactic level. To be universally 
operative or "replicable", reality syntax ("the laws of science") must be distributed 
throughout spacetime; but since it is, it's not locally distinguishable. The limitations of the 
scientific method are those of science. But since science doesn't yet realize these 
limitations, it continues to seek laws with distributed (replicable) formulations by empirical 

(locally distinguishable) means *whether the laws are general or conditional*. Where the 
laws are indeed general, this is a logical impossibility; the only thing that can make a 
scientific law locally distinguishable is the presence of specific definitive conditions that are 
elsewhere absent. Science often ends up chasing its own tail when it tries to go too general 
for its objectivist limitations, as it does in fields like cosmology and consciousness. 
So what it seems to come down to is this: I'm talking about what science is ultimately trying 
to do (seek general laws that apply everywhere or at least anywhere in spacetime), while 
you're talking about what it actually can and does do in its current objectivist form (seek 
nongeneral laws that are nonetheless distributive in effect, or replicable under more or less 
specific sets of conditions anywhere in spacetime). I guess I tend to view science in terms 
of trying to make it what it wants to be. <
RFV: This clarifies a lot. I think it would for most people who look at your theory. It's a 
distinction well worth making even though most would agree on at least a desired 
convergence in the end.
CML: Good. Now combine the above with my last entry. The difference between a universal 
formulation of scientific laws and a nonuniversal one is that the former specifies a complete 
set of conditions for each law, whereas the latter specifies what may only be a partial set. 
Unfortunately, undecidability now asserts itself, and we have to admit that even when we 
have what we think is a consistent universal formulation, it may not be complete below a 
certain level of generality (i.e., above a certain level of detail). In this sense, the work of 
science is never quite done; the universe is still in the process of creation at any point in 
time, and certain axioms may not be expressed. But note also that the CTMU is being 
formulated as a general framework for the resolution of specific scientific issues, and does 
not pretend to be complete in the (forbidden) specific sense. To this extent, you can stop 
worrying that the CTMU, like some kind of crank theory, purports to “explain everything” in 
detail. It doesn’t.
...
> In the CTMU, as in Hume, reality coincides with perception (and also with its cognitive 
underpinnings as described by Kant). <
I don't think Hume ever would have conceded the parenthetical statement would he? Not 
the Hume whose _Concerning Human Understanding_ I read.  Is there a section I should 
re-read?
CML: No, you’re quite right. Hume said that mind was a mere illusion induced by 
perception. I’m saying he was wrong; mind, like objective reality, is a full-blown *aspect* of 
perception. That neither Hume nor Kant had the full story is why we need the CTMU.
> Indeed, science already effectively describes reality as a de facto blend of thought and 
perception, with theories using rational relationships to weld together our shared 
perceptions. <
RFV: That is good.

> Scientific theories can thus be regarded as "common generalizations" of those little 
impressionistic dots you mention. So when we set out to construct scientific theories - which 
is, after all, what we're doing here - we must first buy into the assumption that there are 
commonalities among our perceptions. And this isn't really a questionable assumption, 
because those perceptions (as opposed to our personal interpretations of them) can always 
be scientifically reconciled. This reflects the fact that reality is logically self-consistent, which 
in any case can be established by reasoning along the lines described above. <
Yes, although sometimes people like Dirac think it may not be:)
CML: Although some people think that quantum state superposition violates 2VL, it doesn’t, 
because no law of mutual displacement is in effect among potential states (only actual 
states). I think that’s what you mean here, anyway.
...
>>> If you're right, then Wheeler's title for his thesis ("Observer Participation thesis") would 
seem to indicate his agreement with me about what constitutes an "observer", namely 
anything able to participate in an irreversible change. <<<
>> That happens also to be mine:) It is a good concept that concedes what has to be 
conceded without selling out altogether. It also is in stark conflict with relativistic concepts -- 
whoops, let's stay on course here:) <<
> That's good. Then you're amenable to a formalism in which the universe perceives itself 
through the objects it contains. That's SCSPL, and that's the CTMU. <<
RFV: Perhaps amenable, but at any rate to the notion that it contains
irreversible "observership" changes:)
...
>>> It's even simpler than I put it above. Experience entails something that experiences and 
something that gets experienced. So if reality consists everywhere of experience, it is 
everywhere self-experiencing, and has everywhere the character of both mind (the 
subjective experiencer) and objective reality (that which gets experienced). <<<
>> A good notion, but I see this from my background as more of a demonstration of 
observer/controller equivalence duality. From absorber theory (Wheeler also) there is no 
significant distinction to be made between emitter and absorber. In tracking technology, the 
algorithm for tracking many targets to minimize uncertainty is essentially the same (an 
inverse) as that to situate sensors so as to maximize observability. <<
> Two different languages describing precisely the same thing. The main
difference: my language has been developed in the direction I've been
outlining, resulting in a global reality theory. <
RFV: Of at least secondary significance is the difference associated with the other dualities 
involving the inverses of fairly involved mathematical transformations:)

CML: Well, if you’re not talking about simple line/plane geometric duality, you’re probably 
talking about VàV*=Hom(V,R)àV**=Hom(V*,R) duality in multilinear algebra, which is what 
I’m guessing from your relativity background. That is, the way a basis B of an algebraically 
reflexive vector space V gives rise to indexed coordinate functionals comprising a dual 
basis B* of V*, and iterating this relationship once more takes you right back to V (i.e., to 
V**, which contains no further nontrivial structure beyond that of V). Yeah, that has an 
approximate parallel in the CTMU: to wit, V is analogous to syntax, V* is analogous to the 
set of mappings from syntax to perceptual reality, and V** is analogous to the set of 
metaperceptual mappings from V* back to syntax. It’s about self-reference, a basic feature 
of self-perceptual reality. 
Incidentally, that’s what I mean when I talk about how the theory of metalanguages is 
embedded in the structure of reality; the analysis of perception involves a metalanguage of 
object-level perceptual language, and reality employs such a 2VL-based metalanguage in 
order to enforce self-consistency. In fact, where L is the object language of single percepts 
(“perceptual objects”) arranged in time and empty space, and U is the perceptible universe 
itself, the entire perceptual map LßàU is distributed over reality, and so is the metalanguage 
L’(L) referring to it. If this looks a bit like the M:TßàU notation we employed above, that’s no 
accident.
[[The event (multi-particle) IED is the site within which consistency is preserved, control is 
effected, and every other metalinguistic function is executed. The IED “accesses” the 
conditionally-distributed metaL syntax.]]
>>> So the categorical imperative relating the subjective and objective aspects of reality 
everywhere coincides with experience; <<<
>> I have always seen the duality as action/observation -- both fairly
closely associated with a subjective view of reality overlaying a
physical level. Not worth persuing here probably:) <<
> Maybe so. One of the best ways to achieve understanding is to see the same things in as 
many different ways as possible. But remember; according to distributed solipsism, which is 
simply a correlate of yesterday's "blue shades" analogy, "subjective reality overlays physical 
reality" dualizes as "physical reality overlays subjective reality". The dual viewpoints are 
complementary and mutually essential, and this is the point. From this duality, plus a big 
load of logic, the CTMU follows. <
RFV: Well, actually the duality I was musing on was the action/observation one analogous 
to the tracking/sensor-assignment duality. There is for any set of observations an ideal 
action of sensor positioning that minimizes uncertainty.
CML: Sounds very interesting. And I think we’re talking about somewhat the same thing, in 
that reality theory is about finding an optimal syntactic configuration from which to consider 
multiple observations and interpretations of reality. In this case, “action” is cognitive and 
involves the selection of appropriate metalinguistic principles and parametrizations for 
evaluating object-level syntax (cognition) and semantics (perception). Now, how’s that for 
an earful? 

By the way, where do I find out more about these observer/tracker algorithms, preferably 
from a layman’s viewpoint?
...
> Hope you like this as much. <
RFV: I have, but to tell you the truth, I'm exhausted. I hope my recently withered IQ isn't 
embarrassing itself too badly.
CML: Not at all. You still seem pretty smart to me. If you just think about how points made 
so far might apply to your further questions, I think we’ll be fine.
Part VI
RFV: I believe I understand (and am sympathetic with your notion that T reflects all we 
know with regard to the operation of U and it makes little sense to blabber about that which 
is beyond such an understanding. I have very little difficulty with this. This is what
seems very scientific to me.
On the other hand, what I have extreme difficulty with is my insistence that T is not U in the 
sense of T being our theory of how U works and not the particulars of the night sky, a child's 
smile, etc.. To say that T is that, i. e., U, is more than I can acknowledge as of yet.
I am not going to worry too much about how consistent I'm being - you can identify if and 
where my positions seem inconsistent to you. But suffice it to say that I think I have less 
difficulty with your (subject, object)/(perceiving/perceived) dualism than the (T/U). The 
reason for this acceptability derives from my own acknowledgement of
transactions being all we can know of what is real in any case. And here we have observer 
and observed shaking hands as in absorption theory. So in the sense of perception being 
the operation on U, U is self-perceptual as I now take your meaning. 
CML: This is promising except for your continuing difficulty with the T/U dualism, which 
seemingly reflects your desire to have it both ways...to admit that mind equals reality, but 
reject the implications of this admission regarding the nature of theories. In fact, your T/U 
distinction entails the subj/obj, observer/percept, abstract/concrete, and solipsist/objectivist 
distinctions, which we have already shown are ultimately illusory. So the T/U distinction 
must be illusory as well. Your problem is that you insist on viewing T in the ordinary 
scientific way, as a temporary and unreal abstraction that will inevitably be superseded as 
“real” data accumulate. Unfortunately, that amounts to clinging a priori to the objectivist 
model. 
What you have to do is forget about T and pay attention to the limit of T, i.e. its syntax 
Syn(T)(the part of T that remains invariant as T evolves). For reasons already explained, 
Syn(T) is also the reality syntax Syn(U); Syn(T) = Syn(U). So T, as opposed to the mapping 
M:TàU, is almost invariably “true” on the syntactic level, i.e., has a model in Syn(U) 
(although perhaps not the one we have in mind). Remember, we’ve already shown that the 
abstract dimension of U is “real” in a general sense, and can therefore be described as “the 
abstract phase of U”. Call this phase U_ab. Now, U_ab permits the abstract existence of 

physically unrealizable relations, i.e., relations that cannot be consistently mapped to the 
physical phase U_phys as semantically interpreted. Your apparent assumption that 
everything in U_ab must be realized in U_phys is neither correct nor implied in anything 
we’ve discussed so far. Please try harder to lose it. 
RFV: If you mean this in the sense of Berkeley whereby our perceptions are all we know of 
"objective reality," I buy it. In his concept, there is some basis for these perceptions and 
there is little (if any - he thinks it would actually be a negative) advantage in accepting 
matter over "spirit." I have no real problem with that.
CML: It’s good that you have no problem with that, because as Berkeley pointed out in less 
sophisticated language than we’re presently using, the only alternative is to accept that 
matter is uncreated and eternal (which is totally opposed to what we actually know of 
temporally finite physical reality and turns out to be logically absurd to boot). What Berkeley 
called “spirit”, we call “telesis”. 
RFV: The "concrete phase" means something different to me perhaps. I see a needle on a 
tree. I perceive it exclusively as a needle and not as an abstract result of a theoretical 
process whereby it grew and the weight of gravity that brings the limb on which it hangs 
before my window. There are probably billions of similar needles in my purview but they are 
only there in the sense that I would miss them if they were mostly all gone. Perceptions are 
not very directly supported by theory is what I'm saying. They're just there. They are the 
given!
CML: Right...perceptions are “given”. But so is their consistency with other object-level 
perceptions, and so is the consistency of the higher-order relations educed from those 
perceptions. Without those relations, your “needle” percept is quite meaningless, and in 
fact, can’t even occur. That’s because these relations are responsible for your ability to 
distinguish that one little needle from everything else. In other words, the n-ary relations you 
“educe” from the set of perceptions including the pine needle, as included in the variable, 
evolving part of T, converge on the T-syntax Syn(T)=Syn(U), which underlies your ability to 
make the perception in the first place. Please think about this in light of my first entry above.
RFV: "T equals a global distributed formulation of the reality syntax" certainty implies a faith 
in the comprehensibility of the laws of nature - that I don't have much trouble with, but much 
more than this. It implies that the universe can not be without such a formalization. This 
implies that the formalizations we discover have to correlate with an actual a priori 
formulation. I have some difficulty with this - I think from a scientist's perspective. I tend to 
see the universe as just being there and about which we discover certain regularities by 
which we can describe it efficiently. Our descriptions are ours and have little if anything to 
do with it!
CML: Now hold on there. Neither faith nor comprehensibility is implied by the phrase you’re 
quoting, at least in the sense you seem to presume. All that’s implied is consistency. The 
logical consistency of perception implies all by itself that a “global distributed formulation” of 
Syn(U) exists. This formulation need not even be complete; beyond what’s necessary for 
consistency, it may contain all kinds of ambiguity. It can even be a set of consistency-
preserving syntactic automorphisms rather than a single formulation. But consistency is a 

sine qua non of existence, and U cannot exist – not now, and not ever – without a higher-
order logical relationship representing and effecting it. It’s important for you to grok this fact. 
No consistency in higher-order (inter – and intraperceptual) relations = contradictory object-
level perceptions = multiple dissociated realities. That’s not a negotiable point in reality 
theory.
RFV: I see what you're saying, but I see a humbler scope of such formulations. They do feel 
necessary in some sense, but at this stage of discovery it seems extremely arrogant to 
presume such a scope to our rather humble endeavors.
CML: Well, Fred, we’re doing reality theory here, and if your idea is that we shouldn’t use 
existential (as opposed to constructive) reasoning because we don’t yet know all the details 
of what we’re reasoning about, then you’re not prepared to reason about the nature of 
reality...and you never will be. You see, for reasons already touched on (like undecidability), 
we’re never going to know “all the details”. But we damned well know enough to work with 
on a general level right now, and that’s what we’re doing.
RFV: I do not have real perceptions that are "true" or "false" - they are merely perceptions, i. 
e., a photon (seeming) to impinge upon the eye, etc.. I must make conjectures with regard 
to these perceptions that then may be "true" or "false." Do you see this differently? If so, 
there is definitely something more to your perception of 2VL than meets my eye.
CML: This is incomprehensible. You have a set of perceptions {p_i}, i = 1,2,3…, and the 
truth value of that set is T (otherwise, the exceptions are illusions and not perceptions, 
which means that others in this reality can’t share them). Now, something either is in that 
set (T) or is not in that set (F), so 2VL rules. Note that we’re talking about plain old object-
level sense data here, not the (higher-order) interpretations or associations they might have 
triggered when you received them. The truth or falsity of these is another matter entirely.
> That's just the way it is; any two people who won't both admit that the perceptual basis of 
reality is 2VL have nothing to discuss.
RFV: Again, I think we have quite a bit to discuss, but I don't see the preceding statement 
as meaningful. I think I understand the necessity for 2VL in discussion perfectly well - 
although I may not - but I do not see immediate perceptions as either true nor false, i. e., 
their type is other than "logical!"
CML: Again, this is incomprehensible. Please re-read the last entry above. If perceptions 
were other than logical, then there would be no consistency among the perceptions of 
various observers. Goodbye, science. In fact, goodbye, reality. Again, this is not a 
negotiable point. Obviously, perceptions don’t consist *only* of logic; there is plenty of 
syntactic and semantic variation involved as well. But even that is ultimately governed by 
logic. The most we can do is differentiate between perceptual qualia and their logical 
correlates (despite the logical nature of the connection itself).
> But in that event, math and science would be a useless farce, because what one of us 
"perceives" to be true (and even tautologically true) may not be acceptable to the other.

RFV: Again, I concur other than on the use of the word "perceives." I would say "conceives" 
or "thinks" as distinct from a "perception."
CML: On one level, there is a distinction between conception and perception. On another, 
there isn’t. The set of possible conceptions in U_ab greatly exceeds the number of possible 
perceptions in U_phys. But every perception in U_phys must conform to a configuration of 
Syn(U) in U_ab, because that’s the basis of your “accepting syntax”...the structured set of 
mental states in which you are aware of “perceiving” something.
... CML: As I've already explained, any isomorphism to whose range and 
domain you can't assign different qualitative or quantitative coordinates at 
a given level of discourse is really an identity relation (at that level).
RFV: But and I see this as important, U has much more in it because of its actualities.
CML: Sure it does. If there are actual perceptible distinctions on a given level of discourse 
or conceptualization, then these will prevent an isomorphism from collapsing to an identity 
relation on that level whether or not you can efficiently formulate them. But we’ve already 
ascertained that there are no perceptible distinctions between mind and reality on a high 
level of discourse. So what you see as “important” actually means nothing to us here. 
> For example, the law of conservation of momentum, and its underlying arithmetic, is 
everywhere valid, but only active and perceptible in the presence of actual moving bodies. <
RFV: So the conservation laws of T tell us what will happen from an initial condition (if we 
accept determinism), but the U includes those conditions.
CML: There is always a level on which a theory T is deterministic. Even a probabilistic 
theory is deterministic in the sense that if it is valid, its probabilities must be realized over 
many trials. And as you know, the initial conditions provided by U are ambiguous on the 
quantum scale. This gives U a certain amount of freedom within the boundaries of its 
deterministic syntax.
> Similarly, physical reality is merely an evolving set of instantiations of a set of relations 
that are everywhere valid. These instantiations are metrically separated, but the metric itself 
is completely characterized by a distributed form that exists in every part of space that could 
ever be occupied by your brain (even if your brain has only managed to figure a little bit of it 
out by firing isomorphically to it). The distribution of this form throughout space distributes 
your mind over reality in the form of general mental syntax. 
RFV: A distributed form or syntax is not the sentence!
CML: No, nor is that implied by my paragraph. However, the sentence must conform to the 
syntax that distributes over the language in which that sentence is formulated. Otherwise 
the sentence will violate syntax and make no sense.
...CML: I think one of the problems so far is that you haven't really
absorbed the fact that the distinction between mental (abstract) and

objective (concrete physical) reality is a stratified distinction that
gradually disappears as we approach the top stratum (global
distribution).
RFV: Let's assume this is my problem. Then at some level physical reality has been 
abstracted such that it is in some sense isomorphic with the "mental (abstract) description. I 
would still have problem accepting that, even if this agreement could be reached, at lower 
levels the isomorphism is preserved. This is counter intuitive to me.
CML: What you seem to be saying here is that you don’t think object-level perceptions, 
which comprise the lower levels of the mind-reality isomorphism, need to be logically 
consistent with each other. Let me repeat, this is not a coherent philosophical or scientific 
position. An inconsistent reality is no reality at all, but a set of mutually disconnected 
independent realities. Any contrary assertion makes no sense whatsoever.
> Less general, more conditional features of reality appear to be localized within their 
specific instantiations, permitting internal-external distinctions. In this sense, cognitive (or 
computational) distribution relations are equivalent to topological inclusion relations (that's a 
CTMU duality principle).
RFV: * I think you think you have produced an argument why the isomorphism is preserved 
at the lower levels, but it escapes me. The new terms don't help me with this.
CML: To simplify, spacetime is “qualitative” in the sense that space is like a layered 
sequence of Venn diagrams (spacelike hypersurfaces to which cosmic time is normal) in 
which circles represent predicates defined on physical observables, and time is like logical 
substitution within those domains according to various rules of implication. The domains or 
Venn circles are called “IEDs” and are associated with events from which objects are 
interpolated (to illustratively adopt an oversimplistic graph-theoretic model, object worldlines 
amount to edges in a spacetime graph whose nodes are events). These “qualitative” IEDs 
expand at the rate c and then collapse into a new event within their mutual intersects. Inside 
the IED you have simulative computation or “cognition”; in the compact output event to 
which the IED collapses in cosmic time, you have the mutual perception (interaction) of two 
or more objects. The interior of the IED is wavelike, cognitive and simulative; the output 
event is particulate, informational and objective (that’s “wave-particle duality”, actually a 
temporal alternation between spatiotemporal and objective states of material being). In this 
way, spacetime is identified with the mathematical and physical laws that govern it. This 
spares us the absurd necessity of having a nonphysical “storage space” for the laws of 
physics (or by extension, a nonmathematical storage space for the laws of mathematics).
Please re-read the above paragraph over and over again until you understand it. Then 
define “conspansion” as follows: conspansion consists of two alternating phases of matter 
and energy, inner expansion as an IED (Inner Expansional Domain) at the rate c, and 
requantization as an event-component. Requantization is just the collapse or decoherence 
of the quantum wavefunction, as computed by the simulative computation going on in the 
(coherent) IED, with a reduction in scale that follows from the absence of an external metric 
against which to obtain an “objective measurement” of the size of the universe. This implies 
that the size of the universe is for all practical purposes invariant, and its apparent 

“expansion” (from an interior vantage) is actually a relative shrinkage of content (from a 
global vantage). Eddington knew about this global-local duality, even realizing that as 
objects shrink, time scales would have to shrink apace, i.e., time would have to accelerate 
or contract. But he didn’t know how to do the logical setup for the “shrinking atom” scenario. 
The CTMU does the setup, in the process solving problems Eddington could only have 
dreamt. 
Down below, you complain that I haven’t explained what “conspansion” is. Please make 
sure that you carefully consider what I’ve written here before repeating that complaint.
> If distribution is general or unconditional, on the other hand, it applies to all things, and the 
distinctions among these things exist within it in the sense that it resembles a "self-
executing program instruction" that internally "simulates" them. <
RFV: You seem to have no conception of the obstacles to accepting this.  First of all, you 
start out with "if," but you don't really mean "if." You mean that it is that way. But you 
combine "general" and "unconditional" as though they were synonyms. They are not to me 
in this context. I don't know how familiar you are with computer programming or "execute" 
type instructions or the various levels of programmed "simulations," but being familiar with 
all of them myself, the sentence is incomprehensible to me. Am I too hung up on their 
jargon meanings to allow them to be generalized as you seem to think you have done, or 
what? They seem like ill-chosen words to throw into this cauldron.
CML: We’ve already been through this, Fred. Just replace “if” with “where”, and consider 
that I’m distinguishing among ways of formulating laws. “General” means “representing a 
range of possible specifics”; these specifics are distinguished by the conditions applying to 
each. So when something is “general, it implicitly represents a whole range of possible 
conditions. Now, when something represents a whole range of possible conditions (a,b,c…), 
it’s “unconditional”; it doesn’t need an “if (a), then” or “if (b), then” or “if (c), then” condition in 
front of it to be predicated on a given object in the class of objects it represents. 
E.g., let the predicate “hard”, as used in “diamonds are hard”, represent the class of 
diamonds = (clear diamonds, blue diamonds, big diamonds, little diamonds,...) and so on. 
Then “hard” is general with respect to this class, and if we have a diamond d1 – any 
damned diamond at all – we can say “d1 is hard” without knowing what kind of diamond d1 
is. I.e., we don’t have to say “IF d1 is a blue diamond, THEN d1 is hard”; we know it’s hard 
regardless of specific type. That means that the general predicate “hard” is an 
*unconditional* predicate with respect to the class of diamonds. That’s all. Frankly, I have 
no idea what “professional jargon” you’re talking about here, but if what I’ve just said makes 
no sense in light of it, then my advice would be to lose it fast.
> In other words, general features of (simulative) spacetime include specific aspects as 
output, which is then uploaded to spacetime as further programming, and so on. 
Topological inclusion follows from computational inclusion because topology is 
computationally simulated.
RFV: Chris, these sentences mean absolutely nothing to me. For example, "general 
features of (simulative) spacetime" - all that comes to mind is spatial temporal location of 

events. What "general" and "simulative" add I cannot figure. Then: "uploaded to spacetime 
as special programming." Is there someone other than Chris langan that you envision 
capable of understanding that phrase? What is it that enables that person to understand it? 
The words are all in my vocabulary and the grammar is straight forward, so what's wrong? I 
guess you can say I'm too stupid, but then this six word quiz better have a pretty clever 
answer. What is it?
CML: With all due respect, let’s get something straight. Did you or did you not read that 
geomusser.doc file? Or did you just scan-and-forget? Because if you’d read it closely, you’d 
know that reality is treated in the CTMU as a “self-simulation” with a programmatic 
(simulative computation) phase and an output (perception) phase, and you’d know how 
these phases are related in the conspansive model of spacetime. That file explains how all 
this logic we’ve been discussing is physically implemented. Now, if you didn’t understand 
one word in that file, please refer again to the description of conspansion given above. 
Because I intend to keep writing in a way calculated to trigger meaningful associations for 
you in light of that knowledge.
RFV: Yes, I'm trying - both to you and me
If everything is defined as 2VL except for undecidabel statements, it is not a very useful 
concept. Electric currents are 2VL in a trivial sense, like your triangle may be pink! That is 
not an essential aspect of them.
CML: Nonsense. 2VL unequivocally governs the macroscopic (and in a more restricted 
sense, even the quantum) behavior of electrical currents. Nothing, nothing at all, is more 
“essential” to them. If that weren’t true, you couldn’t achieve a stable perception or 
description of what an electrical current is, much less its source, its direction of flow, or 
anything else about it in any given case. You seem to think that logic can somehow be 
conveniently in effect without being essential to whatever phenomenon you’re considering. 
But that’s utter nonsense, because in that case, you’d need to explain what it is that 
remotely enforces the effectiveness of logic with respect to that phenomenon, and you can’t 
do that without me then forcing you to explain why logic applies to your remote control 
mechanism, and so on ad infinitum in an absurd infinite regress. So the bottom line is this: 
either produce a finite explanation for such a mechanism, or accept the truth: logic is 
essential to everything we will ever perceive. Because from now on, I’m not going to argue 
about whether or not various aspects of reality coincide with and obey logic; they damned 
well do, period. They coincide with and obey it right up to the point at which the consistency 
of perceptual reality is effected. What they do beyond that, I don’t presently care. And at this 
point of the discussion, neither should you.
RFV: Naturally, one can do accounting like an accountant where every item must be put in 
twice, but its senseless. To say that E&M is a series of statements about which one says 
each statement is either true or false is ridiculous. Any valid theory is "true" by definition but 
that does not make the theory a 2VL theory! We've got to get past this.
CML: We sure do! Try this on for size: EM theory is not 2VL with respect to your personal 
knowledge of it; all you can personally do is formulate a set of subjective probabilities 
conforming to modal and probabilistic many-valued logics (which are themselves 2VL 

theories). But given any set of hypothetical nonprobabilistically-formulated statements 
regarding EM phenomena, each and every one of those statements is either true or false in 
point of fact (unless it’s formulated on the quantum level and obeys the superposition 
principle, which in any case must yield to T or F actualization). Now, that distinction having 
been made, consider that what we’re talking about is just the distinction between the 
evolving and invariant phases of T, i.e. T and Syn(T) respectively.
RFV: A theory describes the general behavioral rules not the contingencies of any particular 
configuration.
CML: No, but given any hypothetical set of contingencies, the theory (in conjunction with the 
2VL component of its syntax) will impose consistency constraints on them, and these 
constraints will ultimately be formulated in T-or-F terms. If the theory imposes no such 
constraints on sets of objective contingencies, then it’s as useless as tits on a boar. It has 
no predictive value whatsoever. So let’s not bother with it, OK?
...
> The phrase "generalized cognition", actually synonymous with time,
RFV: How in hell did we get here?:) I take a "synonym" to mean that a dictionary can help 
me, but it doesn't here.
CML: A synonym is a word or phrase that means the same thing as another word or 
phrase. Dictionaries are great, but we’re at the stage of defining terms here, and if they 
were already in a dictionary, we wouldn’t be bothering. We’d just be citing dictionary entries. 
Unfortunately, constructing a new theory is seldom that easy. Stay awake, Fred – this is a 
challenge for me too.
. . .CML: All you need is your memory, because we've already been through this.
RFV: * Your having told me something in the past, and my remembering or not, is not all I 
need to understand your concept of time/generalized
cognition.
CML: You’ve already agreed that mind is ultimately indistinguishable from reality. Minds 
evolve along a state-transition parameter called “cognition”; reality evolves along a state-
transition parameter called “time”. So on the level of generality of the mind = reality 
equation, “cognition” = “time”. I.e., given the high level of generality of the M=R equation, 
“generalized cognition equals time”. That’s it. Don’t keep trying to introduce artificial 
distinctions where there are none. Although I’m sure you could keep on doing that forever, it 
would make my task as an expositor impossible.
> The operative analogy is time : space :: cognition : information. 
RFV: I don't even accept this as a valid analogy, let alone move on from there.
CML: Here’s the problem, Fred. You’re not even thinking about how to interpret this 
analogy; you're just getting visibly impatient with it and putting the load back on me. But as 

long as you’ve already done that, consider once more the paragraph just above:
*** Minds evolve along a state-transition parameter called “cognition”; reality evolves along 
a state-transition parameter called “time”. So on the level of generality of the mind = reality 
equation, “cognition” = “time”. I.e., given the high level of generality of the M=R equation, 
“generalized cognition equals time”. *** 
Now, where the abstract equals the concrete, matter is abstract information. And so are 
relations among material objects. But relations among material objects can be factored into 
spatial and temporal components, the difference being that while the spatial components 
are perceived in parallel (i.e. simultaneously), the temporal components are perceived as a 
sequence of spatial components. But “time” consists of the temporal components, and 
according to what we just wrote, time = generalized cognition. So what is “space” a 
generalization of? What else but that which is transformed by ordinary cognition, namely 
information.
So we have the following analogy: 
time : cognition :: space : information, where “:” means “is a generalization of”.
This obviously transforms to 
time : space :: cognition : information
Please think about this for a while before taking me to task again. This analogy is every bit 
as clear an analogy on any IQ test you might take, given previous explanations.
>I.e., time = info-processing = generalized cognition (where information here refers to the 
abstract dimensions of real objects and processes, which, as we've already seen, coincide 
with them).
RFV: This does not compute! "Info-processing" requires "time," so in what sense can "time 
= info-processing?" What are the "abstract dimensions of real objects?" Spacetime 
coordinates, linear and angular momentum, etc.. are all that come to mind. Is that correct?
CML: A parameter, Fred, is a general predicate of all the specific things which are 
“parameterized” by it. Thus, the parameter called “time” is a generalization of any physical 
or mental process. Now, where reality is abstract, reality is information, and time is a 
generalization of all of the processes undergone by that information. So time = info-
processing. The “abstract dimensions of real objects” are the abstract parameters in which 
objects are distinguished.
> simply extends human cognition to generic information processing. <
RFV: Now that's easy enough, but I sense that you assume that I will be
accepting more than that.
CML: You’re right. Reread the above if necessary.

...RFV: How does/can "time" perform this function?
...CML: Time is not a "thing"; it's a function. . .a function of spacetime.
RFV: I could accept it being a dimension or direction of spacetime, but not a function. In 
what sense do you see it as a function of itself?
CML: Because, as just explained, it’s a generalization of all of the processes (functions) it 
parameterizes. Time is not merely a spatial dimension, even in relativity. It is the function by 
which space is transformed. Don’t be misled by the fact that for certain purposes, time can 
be conveniently represented as a spacelike dimension. It’s absolutely not, never was, and 
never will be. If it were, we could take a walk to ancient Greece and visit Plato. 
> Spacetime performs this function because it's full of syntax, and some of this syntax is 
grammatical as opposed to structural. Grammar governs the evolution or state-transition of 
the language of reality.
RFV: You are placing "syntax" where "sentence" should be it seems to me. Spacetime must 
contain what is real, not the form of what is real.
CML: Do you see what you’re doing here, Fred? You’re distinguishing between what’s 
“form” and what’s “real”, and then hanging your distinction on spacetime. The implication, of 
course, is that “form” gets conveniently crammed back inside the human head (or maybe 
some Platonic realm of ideals way out there in hyperspace), and “reality” gets to continue to 
hang out in “objective” spacetime. Bullshit. We did away with that distinction, what, fifty 
pages ago? And you agreed that it was done away with. Yet here you are, once again 
telling me that for some odd reason you can’t explain, “form” and “reality” can’t both reside 
in spacetime. I just don’t get it. Please try to get a grip on this.
> Thus, time is any grammatical implementation of the SCSPL reality-syntax. <
RFV: I think this blows the validity of analogies I'm trying to grasp.
CML: I think you better rethink that.
CML: Nomology must exist up to determinacy, or its absence would show up in the form of 
irresolvable perceptual paradox.
RFV: Could you explain why? And then could you relate this to the current nondeterministic 
theories of QM, etc.? But simply:)
CML: If nomology, i.e. laws of physics, were not at the very least associated with a 
deterministic set of constraints assuring the mutual consistency of perceptions, those 
perceptions would not be consistent. Beyond the degree of determinacy needed to ensure 
consistency, there exist various degrees of freedom. These degrees of freedom are what 
show up in “nondeterministic” theories like QM, etc. Indeed, all these theories are 
themselves deterministic up to the point needed to ensure consistency, their 

“nondeterminacy” kicking in only once that need has been met. Simple enough? In general, 
the universe is a balance between determinacy and freedom, and we can talk about either 
part without denying its complement. 
> A distributed abstract nomology to which perceptual reality must conform ensures the 
resolvability of paradox, which is just a way of saying "the self-consistency of reality".
RFV: Are you saying that to be self-consistent, theories of physics must be deterministic? 
Why? - in both senses of why does CTMU require it and why does it make sense from a 
physics point of view. I hope making a distinction here is acceptable in a conditional sort of 
wayJ
CML: Anything is “deterministic” with respect to which we can’t just as well flip a fair coin. 
That is, determinacy and randomness are complementary aspects of any theory, the 
deterministic side remaining invariant with respect to the incidental content that gets fed to 
the theory, and the nondeterministic side reflecting variety of objective content (and in QM, 
the behavioral randomness of single objects). Consistency is not an option, but a necessity 
in both physics and the CTMU. So it is deterministically enforced, either at the object level 
or among higher-order relations of objects. However, this does not preclude a 
complementary measure of freedom. The freedom is simply constrained not to violate the 
determinacy. If you roll a fair die, the freedom (nondeterminacy, randomness) of your toss 
does not violate the deterministic fact that the outcome must be one of the numbers from 1 
through 6! Freedom and determinacy consistently coexist.
...CML: Our respective electrons are distinct images of a single
distributed "syntactic photon" consisting of the rules that all photons
must obey regardless of where they are. The images reside in different
locations, and these differences are significant. But it's like Wheeler
said to Feynman: they're all one photon.
RFV: Do you really mean that leptons are generically speaking photons? Or what sense do 
you give the above in this regard. Why and in what sense are all photons one photon?
CML: They’re all one photon because they all have identical structures and behaviors up to 
the deterministic aspects of their common syntax, excluding degrees of freedom like 
frequency and location. They can all be regarded as “instantiations” of a single distributed 
abstraction. The distribution of syntax is not a replication and scattering, but a multiplexed 
projection of one underlying unity. (Actually, Wheeler was talking about electrons, not 
photons, but the idea’s approximately the same even with regard to exchange forces.) 
...CML: Because the isomorphism M is a perfect equivalence relation with respect to 
everything but degree of generality (the abstract is more general than the concrete because 
one abstraction can possess many
concrete actualizations).
RFV: I don't think I can accept that an abstraction "possesses" many concrete 
actualizations. In fact so far I think this is my biggest problem: The actual is significantly 
other than the generalization of its type.

CML: No it isn’t. Again, you’re reverting to your favorite distinction, that between the actual 
and the abstract, on an a priori basis. I’m simply not gonna let you do that any more. If you 
want to do it, the burden of proof is now on you to show where you’re getting your 
distinction and why it’s valid on your chosen level of discourse. Hint: you’re not going to be 
able to deliver. So why not just drop it right now and save yourself the trouble? Think about 
this for a while.
>On the other hand, "Unicorns do not exist" can be interpreted over an even larger real 
context, and to that extent is quantifiably truer in a concrete context.
RFV: "Truer," as in "bluer?" Is this 2VL?
CML: This is 2VL predicate logic. When one quantifier embeds or properly contains 
another, the associated predicate is “true for more objects” or “true over a larger range” or 
has “greater scope”. So if we feel like it, we can say that it’s “truer” in precisely this sense. 
Hey, you’re the one who brought up unicorns, and I’m just telling you what 2VL predicate 
logic has to say on the issue. If you like it, great. If not...oh, well!
...CML: Information consists of distinctions in which one thing is
syntactically constrained to differ from another. Because concrete
reality mirrors the abstract information coinciding with it, it too can
be regarded as information (we're information too, and the reason all
this info seems "concrete" is that there's a syntactic displacement
principle in effect).
RFV: ??? I don't get this!
CML: The “concreteness” of matter is associated with the fact that one piece of matter 
displaces another (to revert to the terminology up top, that’s inherent in Syn(U_phys)). This 
is not true of the abstractions to which pieces of matter correspond; if it were, the QM 
Superposition Principle wouldn’t work.
> So the question arises: what does this information become when all the binding 
constraints are removed? The answer is Unbound Telesis or UBT, the groundstate of being. 
This universe has only arisen within the UBT groundstate because it has constrained itself 
to do so; this is the sense in which it's "self-configuring".
RFV: The universe has "arisen" implies the supposed temporal origin of
reality. But from your system alone, why would there be an "arising" as against a stable 
omnipresent ubiquitous system?
CML: I’m using terms like “arising” mainly for your benefit, not mine. As I use it in this 
context, it describes a sequence of implications...a logical dependency relation. You just 
described my system to a T, provided you’re not making the error of clinging to an infinite 
Steady State model of reality in spite of mountains of contrary (Big Bang-style) evidence. In 
fact, my system is the only system that both conforms to your description and satisfies the 
cosmological evidence. There’s no other way to go. Hear me now, believe me later. 

> Cognition <---> time. <
...RFV: DON'T got it!!!!
...CML: That's OK, we've been through it at least thrice up there.
> The logic is so simple it hurts!
...RFV: I know, but I still ain't got it!:)
...CML: That's OK, we've been through it at least fource up there.
> Human cognition, assumed to be fundamentally unique by anthropocentrists everywhere, 
is just the temporal processing that goes on in a brain. [Of course, there may be more to it 
than that - and there is - but for present purposes, that's good enough. The "more" in 
question has to do with distributed solipsism, and the consequent fact that there is a sense 
in which sentient cognition is primary...that what goes on in reality, namely temporal 
processing of all kinds, really goes on in the minds of sentient observers.]
...RFV: This soothes my mind, but all of this is *not* just "time!"
...CML: Sure it is. It's the refined kind of time that passes in a brain.
RFV: This concept of time has still not been purchased by me even if we have now 
reiterated your position five or six times.
CML: Too bad. Like I say, I’m done with this one, Fred. The burden of proof is now on you 
to establish a distinction. Hint: you already agreed with me to discard the distinction, so I 
don’t even know what you’re talking about here.
[[If the universe is a mind, then it transforms spatial hypersurfaces in time. This is directly 
analogous to the way a human mind or any other automaton transforms states.]] 
> In particular, a form of duality that can be referred to as "spacetime-object", "wave-
particle" or just plain "conspansive" defines an alternation in which the mind solipsistically 
configures reality by a cumulative selective filtration of syntactic potential; during the inner-
expansion phase of conspansion, the mind builds a set of exhaustive potential future 
realities by superposition, and in the requantization phase, selects a future reality in which 
to re-embody itself as a material brain. ... 
**** You're going to have to give me the tutorial on conspansion I think and define your 
useage of some of these terms.
CML: It’s right up there. And may I say that it’s a pleasure to find in you such an avid 
student of the CTMU

God, the Universe, and Theories of Everything
CML:   What it comes down to is this: in order to make your hypothesis {re learning and 
resistance of neural pathways} meaningful, you must first come up with a model supporting 
a general definition of intelligence. Take it from me, that's a tall order. It can be done - in 
fact, I consider myself to have done it on an unpublished basis - but it requires some very 
fancy logic (interestingly, apropos of your interest in spirituality, the same logic has a 
philosophical extension with what might be considered spiritual implications).
CW:   Why don't you share it with us?
CML:   I intend to. I'm currently writing a book on it. The first paper I published about it was 
The Resolution of Newcomb's Paradox, in 1989. For now, think of it as an open-ended 
regression of virtual realities isomorphic to the theory of metalanguages ... a computer 
within a computer within a computer, and so on. Naturally, as various logical and 
mathematical constraints are added, the math gets progressively harder (and more 
powerful). Especially important are higher-dimensional algebra and the logic of formalized 
theories. Understanding the spiritual implications of the system in any detail requires some 
of the math. For now, suffice it to say that the system generates a "religion of logic" in which 
conventional religious concepts, previously nebulous and dependent on faith, become 
amenable to precise definition and logical analysis (this system, the CTMU, is legally 
recognized as the basis of a nonprofit religious-purpose corporation founded by me several 
years ago and currently dormant). What the CTMU says about intelligence is this: the term 
"intelligence", considered as a logical predicate, cannot be semantically confined. By the 
theory of definition, it must be generalized in a certain precise way and semantically 
distributed over the entire system. In other words, the conceivable universe is an intelligent 
entity...a facet of "God", so to speak.
JCC:   I found much interest in the discussion ... of this model purporting to introduce logical 
definition to religious concepts ... quite intriguing this is! There has been some word lately of 
researchers mapping a physical location in the brain for the "religious" experience. To 
believers, this would be our "com link" to the divine, while rationalists see it as the center 
where "oceanic" feelings are induced in the brain through autogenic and/or external 
chemistry. I'm most interested in the history of oracles, sibyls, and melissae (entranced 
mead priestesses) and hope to share a brief vignette of them... These transcendent aspects 
of mind/brain represent a common ground of the mystic, the poet, the rationalist, and the 
scientist, and it is my suspicion that attainment of rigorous understanding of such varied 
forms of consciousness will prove a breakthrough of the greatest impact for our species.
CW:   Re CTMU, are you familiar with TNS*** member Louis Mathe's book, The Missing 
Parameter?
CML:   Can't say that I am. Sounds interesting, though. What's the gist?
CW:   He proposes a six dimensional structure for the universe: 3 x space, time, 
mass/energy and complexity. He defines complexity in such a way that ultimately the 
infinitely complex "folds in" to the infinitely simple. In many ways his project is similar to your 
own with CTMU. An exposition of his ideas appeared in Vidya 111 & 112. They are 
available from the TNS archivist if you are interested.
CML:   I'll take a look if possible. However, even on the basis of your minimal description, I 

sense a difference of approach. For one thing, I begin at a level of structure prior to the 
concept of "dimensions". Whereas Mathe's "six dimensional structure" sounds like a new 
version of the standard Cartesian worldview based on analytic geometry, the CTMU has a 
more sophisticated structure designed for the resolution of paradoxes arising in the 
foundations of such fields as geometry. Interestingly, in order to really get beyond Cartesian 
geometry, it is necessary to get beyond Cartesian mind-body dualism. The CTMU is 
formulated on a level logically prior to both.
CW:   The intent is the same. You can get back issues of Vidya from Matt Urnezis, 3281 
Beachwood Dr., Lafayette, CA 94549 or I could copy them for you if I can find them.
CML:   It's always intriguing when a specific brain structure is newly associated with a 
particular kind of perception or cognition. However, association and identification are two 
different things. There is always a tendency for the scientific community to interpret such a 
discovery as a reductionist "proof" that the cognitive or perceptual modality in question is 
nothing but an illusory side-effect of some structural peculiarity of the brain...usually, a 
structural peculiarity whose origin can be traced to some unremarkable, and therefore 
"scientifically acceptable", aspect of natural selection. But often, no such inference is 
actually warranted.
There are many kinds and levels of knowledge. Unfortunately, when ordinary scientists 
speak of a "theory of everything", they refer to only a subset of human knowledge. This 
would be permissible if they could adduce an appropriate set of a priori constraints on the 
meaning of "everything". Unfortunately, they can't. So any plausible TOE candidate must 
either internally support the deduction of such constraints - i.e., tautologically limit its 
nominal domain of reference to its own narrow descriptive capacity - or permit the 
representation and interrelation of all conceivable kinds and levels of knowledge.
By relying only on the most general invariants of human cognition and perception, the 
CTMU - which, incidentally, stands for the Cognition (or Computation)-Theoretic Model of 
the Universe - avoids the risk of false tautology to which ordinary science is prone. At the 
same time, it permits the construction of logical relationships between science in the narrow 
sense, and arbitrary elements of a powerful logical extension of science. It thus relates to 
standard physical cosmology as the periodic table relates to earth, water, wind and fire, 
widening the descriptive aperture even as it enables a tightening of explanatory focus 
across the entire spectrum of human ideation.
As you might expect, some of the results are amazing.
JCC:   Those are clearly some extraordinary claims ... and if the extraordinary proofs are 
available to match them, I'd be the first to celebrate! You must pardon my reluctance 
though, as one who has spent the better part of a lifetime in reaching for freedom from the 
mindset of religion.
"Ordinary science", if that means the rigors of standard scientific methodology, is something 
which I would not be inclined to set aside with any great haste. Why? Because the 
skeptical, parsimonious view assures a better chance of attaining significant non-
tautological knowledge of the universe. Perhaps the mathematics would prove inaccessible 
to me, perhaps not, but I've learned to be wary of certain areas where the landscape 
includes metaphysics with the sagebrush and tumbleweed.
My hope is that my feeble skepticism will not too greatly retard human progress, but so be 
it. I'm still open to consider new facts, testable propositions, and results which rise clearly 
above the morass of semantics. Not to cause offense, but this wariness comes from having 
travelled much shaky ground, here be dragons and all that.
CML:   {to Julia} A touch of skepticism is always a good thing. Unfortunately, as usually 

applied, skepticism is itself tautological. For example, what makes you think that 
replicability, in the sense of the conventional scientific method, is a general feature of 
reality? By assuming as much, you've automatically chosen empiricism over rationalism in a 
universe defined by both. What makes you think that semantics is necessarily a "morass"? 
It could be that most people are merely confused about how to properly apply it. And what 
makes you think that metaphysics is any less amenable to solid formalization than any other 
branch of knowledge? In my humble view, these assumptions aren't so far removed from 
mathematical illiteracy.
Unless I miss my guess, you make such assumptions because it's easier to make them 
than to disprove them. I daresay the same is true of most people. I, however, happen to 
know that every one of these assumptions is false, and can prove it to any really intelligent 
person willing to pay sufficient attention.
Unfortunately, the high-IQ world is full of people who, after years of fruitless questing, 
regard themselves as having "done the religion thing", "done the logic thing", and so on, to 
such an extent that they can't bring themselves to suffer through the kind of proof we're 
talking about. Because they prefer to swallow their information in bite-sized chunks, the 
logical technicalities of proof are quite beside the point. Speaking from hard personal 
experience, I can assure you there are many jaded high-IQ types out there who feel that all 
they need do to refute an unwelcome proof is fail to read it carefully. When one begins with 
no idea who these people are, he can waste a very great deal of time trying to gain their 
understanding.
What you've responded to here are a couple of bite-sized chunks I tossed out for your 
casual delectation. I suspect that any more than that would have been inappropriate. Proof, 
on the other hand, is a serious pas de deux. So for the time being, and with heartfelt 
sympathy, I'm afraid I'll have to leave you stuck between faith and skepticism...i.e., no 
worse off than before.
By the way, I've restricted this reply to you. Please pass it around.
JP:   I found your recent discussion interesting, particularly Chris' comments on TOEs 
(theories of everything). I have always been a little disconcerted when I read about the top 
theoretical physicists (Weinberg, Hawking, etc.) claiming to have, or be close to, a TOE or 
even a GUT****. I like it even less when they go on TV and make these claims, possibly 
giving the public the impression that the main overall "problem" (i.e., understanding the 
Universe) has been solved. As you point out, "everything" has severe constraints on it. It is 
usually taken to mean a quantum field theory (or string theory) that explains the 
fundamental particles (quarks and leptons) and their interactions. The assumption is that 
given such a theory, everything else could then be computed. I have even seen assertions 
that research in "real" physics is essentially over, that it is merely a matter of filling in the 
blanks. Although particle physics is not my specialty, FWIW I think this is a pile of arrogant 
balderdash, a little like claiming that because we understand the basics of atomic and 
molecular structure (which we do), that fields like chemistry and biology are just a matter of 
plodding through the calculations.
From my point of view, the opposite is more nearly true. More understanding of basic 
phenomena leads to more questions and opportunities for both basic and applied research. 
And this is just in physics per se, never mind questions about consciousness, brain 
functions, etc. which may never be explained using basic physics, now matter how 
"complex" (one of the latest buzzwords). Maybe this is the point where Julia's mysticism (or 
the "paramythic"?) comes in; I haven't, and probably never will, figure it out.
Anyway, as of now it seems to be academic. There is no agreed-upon TOE, even in the 

restricted sense, and maybe never will be. As I understand it, the current leading contender 
is superstring theory, which cannot be verified experimentally without an accelerator the 
size of the galaxy, if at all. Of course, there's a certain beauty (even poetry?) about ten-
dimensional strings. And they also have a certain "practicality" -- who can argue about them 
with any authority? Having said all this, though, I certainly hope someone somewhere will 
always keep trying for a TOE. It would be the closest thing to a generalized "philosopher's 
stone," don't you think, Julia?
JCC:   I do feel that our quest for understanding is the real philosopher's stone because it is 
this which has brought visible transformation in the world. Humans walked on the moon by 
wishing for it, dreaming of it, imagining it for ages ... and painstakingly acquiring the 
knowledge and fashioning the means to journey there and return successfully. Imagination 
and speculation did not in themselves accomplish this dream, but opened the way to 
rational means. Inevitably many fanciful notions of the past were honored by relegation to 
the realm of fancy, but the lovely moonrise is no less romantic for that.
Should there be any doubt, this is what I meant by paramythia, namely that romantic 
consolation has its honored place. Indeed there is something in us which seeks a theory of 
everything; millions of people even now, as in former times, consider that they have a 
TOEhold in their deities, scriptural revelations, astrological insights, literary discernments, 
numerological systems, whatever. These are blessed opiates to many; far be it from me to 
gainsay them. In fact the burden of refutation or proof does not rest with me, not in the least! 
The burden falls upon the advocates, expositors, discoverers, scholastics, priests, and/or 
gurus who have made manifest a great brilliance on the horizon. Whether served up in large 
bites or small, truth advances well enough without my hasty embrace as true believer. Stuck 
between skepticism and faith? Actually not. Questing has not been fruitless when it has 
imparted caution as an adjunct of open-mindedness.
CML:   Actually, the burden {of proof} is on everyone. This is because the problems of 
religious hatred and ethical conflict cannot be localized, and no solution, no matter how 
potent, can work without being generally understood. Because of the volatility of the subject, 
it is uniquely susceptible to an epidemic of buck-passing ... a cowardly shell game in which 
the pea of truth could easily get lost in a frantic ideological shuffle among religious warriors, 
political opportunists, and academic theologians bent on preserving their neutrality and 
covering their rears.
I well understand Julia's caution. If she were not cautious, she would not be as intelligent as 
she obviously is. But in this context, responsibility and intelligence are one. False beliefs are 
corrected by reason; many of the beliefs that Julia refers to as "blessed opiates" are in fact 
social and psychological pathogens for which logic and intellect are the only cures. As 
intelligent human beings, therefore, we do not have the privilege of weighing truth against 
caution.
Academia, the self-professed guardian of truth and knowledge, has failed to produce a 
metaphysic of pure logic ... failed so miserably, in fact, that it shrinks in embarrassment from 
the very possibility. As funny as it might sound, it may be up to the high-IQ community to 
recover the fumble. Remember, the high-IQ community was founded by idealists who 
hoped that it would prove instrumental in solving humanity's problems. That it has failed to 
serve this ideal in the past is beside the point; if its members are as smart as they pretend 
to be, then their united intellects are an invaluable human resource that should not be 
wasted.
While Julia points out the inadequacy of faith as a TOE foundation, James points out the 
inadequacy of physics. Because physical reductionism is built into the very core of science, 

the sciences now face a crisis as serious as that of religion. The grammar of science is that 
of logic; no meaningful scientific or mathematical statement is anything but a fleshed-out 
logical formula. Yet, as any logician knows, a single irresolvable paradox of the form "A = 
not-A" destroys the information content of any theory containing it. Unfortunately, this is 
exactly the form to which the paradoxes of quantum mechanics - EPR nonlocality, for 
example - can be logically reduced. Since the reality of such paradoxes has been 
experimentally confirmed on multiple occasions, science is awakening to the unpleasant 
realization that its nipple is caught in a high-speed wringer.
However, all is not lost. The problem: certain self-imposed theoretical constraints of 
mainstream physics preclude the formulation of predicates powerful enough to credibly 
resolve certain physical paradoxes. The self-evident solution: a theoretical extension of 
physics logically designed to suspend this limitation. Because this extension lies "after" or 
"beyond physics" as currently defined, it constitutes "metaphysics" in formal Aristotelian 
sense. Yet, once this extension is adjoined to existing physical theory, it too will be called 
"science"!
The moral of the story: just as they have on many past occasions, science and metaphysics 
are destined to become one. So if one really cares about science, then one had better start 
caring about the conjunction of logic and metaphysics as well. Letting an excess of caution 
impair the learning process would be...well, unscientific! All I ask is that we keep open 
minds not just to new ideas, but to the ideals of hard work and responsibility without which 
truth cannot be expected to triumph.
JCC:   That's a fine reference to the very origin of the term metaphysics, the pages of 
Aristotle which came after his Physics (!) Well, Chris, you may well have achieved a 
noteworthy triumph in this CTMU. It's that the initial description made this work sound so 
"oracular" in nature, and that's what put me off a bit. In speculative fiction A.E. van Vogt 
played about with the concept of non-Aristotelian extensions of physics... fiction, admittedly, 
but based on intriguing foundations. Certainly it will be of interest to read a bit more of the 
CTMU: what it is, what it does, and how paradoxes are resolved in practice. In past 
centuries Newton and Pascal sought a grander vista but had neither the framework nor data 
to reach it. I'll keep an open mind, yes.
CW:   One thing is quite clear: there is no carte blanche for human survival. We really can 
screw everything up. But we should keep in mind that, while valuable in many ways, 
intelligence, beauty, physical strength are all relative traits and the ultimate point of human 
experience if there is one is not necessarily based on the endurance of achievements. For 
one thing, no human exists forever. For another, there may be many many experiential 
possibilities with quite different priorities. If "intelligence" is not limited in its application then 
this intractable social crisis is equally well an expression of it as is its opposite - perfect 
control and knowledge.
CML:   This, of course, is a kind of moral relativism devolving to an assumed absence of 
teleology. Fortunately, that assumption is bad. Intelligence is inseparable from purpose, and 
since the CTMU distributes intelligence over the universe, it does the same for purpose. 
Voila - a new brand of teleology that prefers increasing control and knowledge to a dysgenic 
deterioration of cognitive ability. You're right that humanity can "screw everything up". But if 
it does, it won't enjoy the luxury of a valid philosophical justification for its crime.
JCC:   Happily (maybe not for us, in particular) the Aion is (or has) a wealth of raw material 
encompassed in the vastness of space-time (Cosmos). Maybe something comes of it; my 
favorite hymn has always been "Que Sera, Sera!" {smile}

On Nagarjuna and the Heart Sutra
CW:   ... Much of what you are hinting at is reminiscent of the Buddhist philosopher 
Nagaryjuna. In his exposition of the middle path he demonstrated that experience itself 
cannot be limited to any preconceived postulate. These postulates are filters not sources. 
To put it another way: the higher order experiences (deriving a TOE) are not intrinsically 
more real than lower order experiences (stubbing your TOE).
JCC:   The reference is to Acharya Nagarjuna, fl. 150 C.E. who amplified the doctrines of 
sunyata (emptiness) through negation of opposites. Semantic questions appear to have a 
lot of bearing on issues of social engineering ...
CW:   We tend to have a very blinkered view of what is going on so our attempts at 
understanding why tend to be all over the map. However what I was saying was taking what 
you were saying generally (about CTMU) and applying it to what you were saying about 
evolutionary success. Whether you suffer or not invariably you learn.
CML:   Buddhism is an essentially nihilistic philosophy in which the evolution of self 
culminates in self-annihilation. This implies that the self is a manifestation of pure 
nothingness which deludes itself regarding its own existence. The scientific equivalent is 
known as ex nihilo cosmogony. This, however, is a metaphysical absurdity whose existence 
would collapse the logical structure of reality like a soap bubble in a vacuum. So much for 
Buddhism as currently defined. Fortunately, there now exists a philosophical scalpel - the 
CTMU - sharp enough to excise Buddhism's absurdities and thereby clear the way for a 
constructive reinterpretation of its more valuable insights. In the future, a reconstructed form 
of Buddhism will be merely one aspect of a single religion based on the power and 
infallibility of God's first and most unbreakable law, logic.
CW:   Actually you are wrong about that. See the chapter entitled Shunyata in Cutting 
Through Spiritual Materialism by Chogyam Trungpa. The middle path is essentially a 
refutation of nihilism. Nagaryjuna's philosophy is basically a commentary on the Heart 
Sutra. The Heart Sutra is interesting in that - being probably the central teaching of the 
Buddha - it wasn't actually taught by him. He just said "well said, well said" when his 
students propounded it.
CML:   Your point is well taken. However, Buddhist scholars are a bit like economists: 
presented with the same question, they all produce different answers. In fact, inasmuch as 
the history of Buddhist philosophy branches in a way similar to that of Western philosophy, 
one can no more talk about a unified Buddhist metaphysic than one can talk about a single 
Western philosophical outlook.
Etymologically, nirvana means "to blow out" or "to extinguish", as a flame deprived of fuel 
ceases to draw. Now, when a flame goes out, it ceases to exist. Modern scholars, aware 
that this imagery carries some rather disturbing implications, have taken pains to reform the 
concept, observing that it is not the flame itself, but only its finite boundary, that is subject to 
extinction. Unfortunately, this leads to certain problems... e.g., a conflict with anatta, the 
denial of metaphysical self. According to the doctrine of dependent coorigination - which, by 
the way, is somewhat analogous to the mathematical concept of recursion - the self can be 
analyzed as a relationship of independent personality components called "skandhas". Thus, 
when the self attains release from samsara and sheds its boundary, it merely falls apart, 
and the conceptual boundaries of its components are reasserted in its place! The loose 
ends and non sequiturs pile up mercilessly, leading one to the conclusion that perhaps the 

etymological definition of "nirvana" is what was originally meant after all.
Nagarjuna's philosophy (the Madhyamika school of Madhayana), far from solving such 
problems, complicates them. Nagarjuna held that knowledge can only be based on 
propositions derived from concepts and perceptions existing only for a given individual, that 
knowledge thus exists only relative to individuals, and that absolute knowledge is an 
impossibility. Understanding is attained when the relativity of knowledge is realized and all 
claims to absolute knowledge are abandoned, so that a direct awareness of reality, 
unconditioned by concepts, can be acquired.
Sadly, Nagarjuna, genius that he was, failed to pick up on a couple of crucial points. First, 
he was positing the statement "all knowledge is relative" as a piece of absolute knowledge. 
That's a self-contradiction. Second, terms like "direct awareness" and "reality" are 
themselves concepts. Deny their conceptuality, and they have no possible meaning and 
thus no meaningful existence. With every statement Nagarjuna makes, he digs deeper his 
hole of illogic, finally proving nothing beyond his own desperate need of that which he has 
attempted to jettison for the sake of "enlightenment".
Philosophies, like languages, are tied together by syntax and semantics. When these are 
corrupted by logical inconsistency, the philosophy becomes no less corrupt. One can 
immerse oneself in the loops and contradictions, hoping through the mortification of logic to 
intuit the overall shape of the maze in which he is lost; at present, this is the value of 
Buddhism. However, if the intuition is valid, then it must be amenable to consistent 
formulation. Buddhism has nothing resembling a coherent logical framework in which to 
achieve consistent self-formulation.
Again, the CTMU fills the breach. It says what Buddhist philosophers have been trying to 
say for millennia but lacked the logical sophistication to express. On the other hand, 
Western philosophy, while now incorporating powerful logical formalisms, lacks the holistic 
dimension of its Eastern counterpart. In fact, only a seamless weave of the strongest 
threads of Eastern and Western philosophy suffices to characterize of the structure of 
reality. That combination is here ... now.
CW:   My understanding of this is that N's argument relates to the last line of the sutra which 
says something like "we take this to be true because there is no deception in it." In other 
words what is true cannot be directly grasped but is realized by a process of elimination.
The word truth usually has to do with an observed relationship (eg gravitational attraction) 
that is invariant within a certain context. For example gravity doesn't always apply during 
dreams experiences but it does during waking experiences.
In a nutshell any relationship no matter how seemingly invariant is arbitrary in the sense that 
it is experienced. It is arbitrary because there is no context for experience itself. You can't 
compare awareness itself to anything. In this teaching awareness itself would be considered 
to be ultimate truth - true because it is the invariant of experience. But it seems more to be 
neither true nor false - you can't have a relationship (a truth) without invoking comparison. 
Relative truths are the invariants experienced contextually within awareness.
In some sense N's teaching on this topic has to do with impermanence but it is showing that 
the truth of impermanence is not necessarily a process of growth and decay but a basic 
sense of arbitrariness or nonexistence. In that way it is somewhat similar to the invention of 
calculus in that calculus discusses motion without reference to duration.
The aggregates or skandhas are not considered to be ultimate truth in this teaching. (See 
The Two Truths by Guy Newland for a synopsis.)
In some sense the experience of enlightenment must be something like the transition of 
matter to energy. Energy may have properties that are quite different from matter and might 

not "make sense" from the perspective of matter. Furthermore it is not a one-way transition: 
energy becomes matter and matter becomes energy under various circumstances. We can 
consider them as different forms of the same thing. The confusions of the relative truths and 
the wisdom of absolute truth have the same relationship being different forms of the same 
thing - awareness.
It is important to realize that these teachings are just that - they relate to a method that can 
actually be used to relate with mind. Taken outside the context of practice they become 
interesting philosophical speculations but are not necessarily useful. In the Heart Sutra they 
are talking about a very real experience available to anyone who is willing to do the work to 
divest themselves of self-deception. The experience came long before the analysis. So, 
although Buddhist philosophy might be exceedingly varied it all relates to a rather simple 
experience and is only valid in the context of that experience.
CML:   What you say here has merit. However, the self-deception of which one must divest 
oneself happens to include any notion that Buddhism as now formulated has overall logical 
integrity, or can serve as the basis for any logically consistent practice except error 
correction.
I note that you've given a semantical definition of word "truth", using the example of a non-a 
priori concept, gravitation. But semantics is ultimately based on syntax; there is a 
mathematical homomorphism between the logical component of cognitive syntax and any 
valid semantical construction. You then go on to say that truth can be grasped purely by 
elimination. But the syntactic meaning of "truth" is, in fact, set-theoretic inclusion in any set 
of noncontradictory propositions obeying this homomorphism. Whether or not a particular 
truth is achieved by elimination, it's still in the set, and the homomorphism criterion of this 
set prevents the separation of truth from logic. Concisely, it seems that "truth" is a well-
defined logical concept of the kind that our friend Nagarjuna threw out the window of 
enlightenment.
CW:   I'm not saying that at all. I think the correct term is "non-dwelling" which refers to an 
ongoing activity. I think, to sum up, what I am talking about is the possibility that there are 
experiences and there are transformations of experience that are not in and of themselves 
experiences. The idea that you could "experience enlightenment" is therefore contradictory. 
Enlightenment happens and as a result one experiences differently. The logical 
relationships intuited directly from experience change as a result of the transformation.
CML:   Yes, but the change cannot entail total cognitive discontinuity. Remember, a 
"transformation", whether cognitive or not, is a logical construct with logical ramifications, 
and thus answerable to logic.
Nagarjuna was right that much of what we consider "knowledge" is relative and can be 
transcended. Where he went wrong was in attempting to absolutize his own teachings 
regarding this fact...to present it as an "ultimate truth" when its range of validity is in fact 
restricted. Perhaps this was deliberate on his part...an allowance for the lack of cognitive 
sophistication of his students.
CW:   He didn't. "The wise do not dwell in the middle either."
CML:   You seem to be advocating a philosophical escape clause equivalent to "all crows 
are black ... except for those that aren't." That's tautological and therefore true. But this kind 
of truth can be glimpsed without, so to speak, "sitting at the feet of the Master".
In the final analysis, awareness cannot be regarded as unitary in any ordinary sense of the 
term. It has logical ingredients which must obey the laws of logic: a finite or infinite self 
(subject) that is aware (verb) of itself and/or its environment (direct object), or an open-
ended inductive regress based on this construction (the CTMU takes the latter route). Take 

that away, and "awareness" means nothing that can be meaningfully apprehended.
CW:   By taking the inductive regress you appear to agree with Nagaryjuna that awareness 
is fundamentally ungraspable. Would another approach be to consider these logical 
components as dimensions? One could talk about a unitary basis for "awareness" by the 
same method that the curvature of space is calculated in general relativity (if I recall 
correctly from Wheeler's book, A Journey Through Gravity and Spacetime). 
Nevertheless because of the logical limitations imposed one could only infer this unitary 
basis. It would also not be directly graspable.
CML:   That which is "inferred" by any means whatsoever, including an infinite regress, 
qualifies as one node of an inferential relationship and can thus to some extent be grasped. 
In the CTMU, the terminus of the metaphysical regress is called "unbound telesis" and can 
be simplistically understood as a universal, unitary substance from which spacetime is even 
now originating. The kind of "awareness" associated with this ultimate substance is very 
general and powerful indeed.
CW:   One question that came to mind as I was watching "A Brief History of Time" again last 
night. Is Truth dependant on time? I seem to be saying yes. You, no.
CML:   You're correct, but only partly so. I say that to be applied at the metaphysical level of 
truth, "time" must be redefined in a certain specific way. In the CTMU, time is not an 
ordinary linear dimension, but possesses a far more complex (and potent) logical 
characterization. Keep in mind that if you consider truth and time to be unconditionally 
interdependent, then any high-level semantical rupture in the truth concept also entails a 
rupture in time, and thus in temporalized consciousness. Without integrity of consciousness 
- which, as you know, is a primary desideratum of Buddhist philosophy - awareness of any 
kind is out of the question. Consequently, even the highest form of awareness must 
conserve the logical definition of truth.
CML:   Quite a few "weekend Buddhists" delude themselves that they have indeed reached 
such an awareness by clearing away all the "relative knowledge" in their muddled brains. 
The truth is that they have either 
(a) falsely intuited the existence of a mode of awareness that they did not really achieve, or 
(b) not properly formulated their intuition. 
(The reality is "b".) This forms the seed of a profoundly irrational outlook on life that would, if 
given a second opportunity, do to modern civilization exactly what it did to the civilization 
that gave it birth...namely, grease the skids to moral and social decay and oblivion.
CW:   I don't know where you came to these conclusions. I guess it is appropriate to start a 
religion if you feel you are privy to people's mental state, thoughts etc. Are you sure it was 
Buddhadharma itself that caused this decay? It is quite easy to talk about being a Buddhist 
(which I am not claiming for myself - mostly due to insufficient effort at this point on my part) 
and quite another story to really let go. That's all it is really, something comes up and you let 
it go. It isn't complicated, nor does it necessarily require a great deal of logical 
sophistication. Buddhadharma, is basically just a method that one can use. When this 
method has served its purpose nobody is expecting you to continue with it. Any notion to 
the contrary is a misunderstanding of intent. In any case I was wondering how your work 
connected with Nagarjuna and now I know. Thanks.
CML:   First, you don't have to be telepathic to read about Asian history. Second, I didn't 
say that Buddhism was a primary cause of social and moral decay, but only that it was a 
contributing factor. Third, if you just "let it go" every time "something comes up", social 
problems don't get solved and there is (at the very least) no philosophical barrier against 
social decay. And fourth, certain religions do not adequately specify when certain principles 

are to be applied (indeed, cultural evolution limits specificity in this regard). Instead, they 
just say "this is truth" and leave it at that. Unfortunately in the case of Buddhism, this left 
millions of people futilely trying to "read the intent" of an inscrutable patriarch who was 
notoriously ambiguous in life, and no longer around to explain himself in death. It's time for 
his primitive ideas to evolve in light of modern logic and mathematics...that is, CTMU-style.
Now that we've got that out of the way, allow me to say that you're clearly a highly intelligent 
person with more than the average amount of philosophical awareness. It's a privilege to 
enjoy these discussions with you, and may we enjoy many more.
Altered States and Psi Phenomena (Part One)
CW:   The Chemistry of Conscious States by Dr Alan Hobson has some interesting ideas 
relating to this as well. One problem with analyzing dreams is that when you become aware 
during them they change fundamentally. They could also be something similar to phantom 
limb pain in which an unused area of the brain remains activated when it would be better off 
inactive.
CML:   At some level, one is aware of all of his dreams. Otherwise, he wouldn't be 
"dreaming". I remember virtually all of my own dreams; to me, the dream state is merely a 
slightly altered state of consciousness. In fact, when I visited a neurologist as a teenager 
regarding an alleged propensity to enter trancelike states in high school classrooms, I was 
hooked up to an EEG and told to fall asleep. I couldn't. When the doctor read the trace, 
however, he was certain that I had been sleeping. I infer that the ability to manifest or retain 
consciousness in various brainstates varies among individuals.
CW:   That is quite unusual.
CML:   Cal, I notice that you're a fan of Bob Monroe's. Is your involvement strictly academic, 
or are you attempting to cultivate OBE's? That happens to be a longstanding interest of 
mine. If you've succeeded in having any OBE's, did you seem to experience any associated 
psi effects - telepathy, clairvoyance, precognition, or psychokinesis? If so, were you able to 
obtain any sort of real-world evidence to that effect?
As you know, Monroe is famous for discovering a technique called "Hemi-Sync". It was 
employed by US government remote viewers during the cold war. Have you tried those 
tapes? Personally, I never needed them - it used to be that as soon as I reclined, I'd have 
more trouble staying in my body than getting out - but I sometimes wondered whether they 
might have an amplificative effect.
JW:   I find it immensely intriguing that the subject of "out-of-body-experiences" is being 
broached.
Recently, on the Sci-Fi channel program called Sightings, there was a case of a remote 
viewer who did surveilance for the US military - I believe it was during the Vietnam War. On 
this show, it was mentioned that he could go "forward and backward in time"; he could 
basically view anything from Auschwitz ( hopefully correct spelling ) to the activities of the 
Soviets. It was intimated that not too many people can do remote viewing.
If the premise of this particular show is correct, then the idea of somehow being able to tap 
into the "universal memory" where all actions and thoughts have, and will take place is an 
incredible prospect, to say the least.
CML:   It's correct, all right. Check out the book Remote Viewers by Jim Schnabel (Dell, 
1997). The US Government remote viewing project was undertaken in conjunction with the 
SRI (Stanford Research Institute). Although it was eventually terminated - or blacked out - it 

resulted in the accumulation of a towering pile of absolutely unequivocal evidence for psi 
effects. So, for that matter, have the experiments of the Rhine Institute, PEAR, and other 
academic centers for the exploration of advanced mental capabilities. At this point, you can't 
find a reputable statistician who will deny it. Scientifically speaking, the shoe has changed 
feet.
I had my first conscious, spontaneous OBE at the age of twelve or so. It scared me rather 
badly - a feeling of being electrically stimulated from head to toe, utter paralysis, forcible 
displacement, and finally the ability to move at will (with my body still lying on the bed). 
When I finally regained corporeal coincidence, I got up and checked out various details I'd 
observed...details not previously known to me. What I'd seen in the OBE state was perfectly 
confirmed.
At this point, the sleep and OBE states are almost indistinguishable for me. Time dislocation 
is very common, mainly from present to future. I'm familiar with probability theory, and there 
is absolutely no chance of "random coincidence"; what I see generally happens. 
Unfortunately, that's not always the same as being able to see just what I want, or just what 
is personally useful to me. Then again, sometimes it is.
Despite all the data, the scientific community still lacks a sound theoretical framework on 
which to hang such phenomena. However, partially in order to clarify things for myself, I've 
constructed one. It's mathematically unique, and I suspect that at some point, it's going to 
be recognized as a pretty big deal. But until then, you might as well get comfortable with the 
idea that the mind is not bounded by the skull. Mind and reality are one, and it remains only 
to explain the logic that binds them - and us - together.
CLF:   Hmmm....Is this something that one can learn to do? Or is the capacity for remote 
viewing thought to be a special talent which only a few can develop? ... always wanted a 
"Time Machine", but would settle for the ability to do remote viewing!)
JCC:   Fascinating matters under discussion here! I have written of a few odd personal 
occurrences, some induced, some quite unsought. Did I slip out of my physical shell for an 
instant, bewildered at the glace of architecture that clearly was not 1969 Charles St. in 
Baltimore? More likely a daydream, I think, and others could only have my anecdotal word... 
but yet it's quite interesting to hear similar anecdotes. I had been sufficiently curious to try a 
sort of hypnotic regression with a therapist some dozen years ago. Very strange feelings 
and dialogue, but nothing much testable beyond a certain psychological sense of validity.
Perhaps the most interesting research work was a compilation of composite data on two 
thousand case histories by a Dr. Helen Wambach. I don't recall the title, but it was one of 
the two or three texts I've read which truly provide food for thought on the possibility of 
reincarnation, and by extension, some sort of mental or essential transcendence of the 
vastness of time, space, dimensionalities as conventionally recognized.
Occam's Razor cut away all but romantic confabulation, yet I do enjoy entertaining some 
speculation on what might be ... and am glad that our colleagues here do not feel discomfort 
in sharing encounters with the liminal, the numinous. I'll have stopped living when I cease to 
wonder, even though I'm disinclined to return to a state of belief "because it's impossible" 
{smile} The precise nature of beliefs and daydreams is somewhat elusive too, upon close 
examination.
JW:   That's certainly amazing, Chris. I had read about individuals being able to have OBEs, 
but have never had any of these experiences myself, as far as I can recollect.
I'd say the closest I have come to having an OBE was when I was also a child. 
Occasionally, I would be falling in my dreams, then suddenly wake up. I had this same odd 
tingling sensation you speak of. I hadn't thought of this for years. I have also had, and still 

do have feelings of "deja-vu", that I have been at a certain place before, doing this same 
action countless times. Yet, having never experienced it before.
With me, all this brings to mind a "vision" I suppose one could call it, I had a couple years 
ago. It was late at night, and was a particular time in which I very much felt "compelled" to 
write. I began to write a poem, and as I wrote it seemed that I had an intuitive 
"understanding" about the interconnectiveness of all things. I saw a pattern. It was an 
amazing experience.
Chris, I suppose your realization struck a synonymous chord within my own sphere of 
understanding.
CW:   As far as I know, the only concious OBE I've had was a spontaneous one after I 
bought Ultimate Journey. For some reason I was extremely excited when that book came 
out. Having read the Michael series I can understand why.
You can control the trip to some degree. I have used them. I have a problem that when I lie 
back I tend to swallow my tongue which makes it somewhat difficult for me.
Monroe had about a 50% success rate with his Gateway program (which still exists). It is 
about a week and costs about $1000 (including food and lodging). You might want to try the 
"discover" tape series first.
WHK:   Is this where the expression comes from, about being ... "beside oneself"... ?
WRW:   OK Julia, I admire the way you write. I am enough of a pragmatist to appreciate 
Occam's razor. But, I got the impression that you do not embrace as strong a belief in out of 
body experiences as say Chris. (Pardon me Chris. You and Julia have been the most likely 
to address various topics. I don't mean to "pit" you one against the other.) How, in a 
nutshell, would you classify, or truly consider, such phenonmena such out-of-body 
experience.
JCC:   A fair statement, owing to experience over the years that belief commonly translates 
to a viewpoint centered around advocacy, deriving comfort from the embrace of an 
abstraction. After seeing my own patterns, I found strong belief to be too costly for my 
tastes. Can't really afford first-class tickets on the comet, anyway. {smile}
Rather than eagerness to purchase the most powerful theory of everything in the shop and 
carry it home, I find plenty of comfort in browsing through the museum corridors, admiring 
the great art and artifice ... and feeling relieved not to have to pay for the security system on 
all that!
It is sufficient that an idea is creative, imaginative, and opens doors to further 
understanding. Or, to feel reasonably confident that a principle, calculation, or process can 
be relied upon to produce results useful or pleasant. No need for all the criteria to be fulfilled 
and wrapped up in a single package of perfect closure here and now. I wanted just that 
when I was twenty, but now the notion seems empty, superfluous, a terribly boring heaven 
of the sort satirized in Letters from the Earth.
Humanity has covered an eventful journey, to great extent mirrored in the course of every 
individual life. Perhaps there is such comfort and fulfillment in honoring the progress of the 
human story, foibles and triumphs alike, that strong belief in this or that seem not so very 
compelling after all.
I know not whether my various personal experiences qualify as OBEs, time- travel, or 
whatever label one cares to give. Fantasy or communion with a higher reality, or some 
fraction of both? Difficult, because I always loved the notion of time-travel, just as 
Charmaine did. We can find gods and followers in equal numbers, consulting our Cantor, of 
course. The dialogue, the questioning, the exploration, and the wonderment are to me far 
more interesting. If neither end nor beginning is found to this tapestry of life, is anything lost 

or lessened?
Quite possibly belief was not required even before the revelations from "Michael". That was 
the aspect I most enjoyed in the series, despite the haziness on how the sevenfold schema 
maps into other systems modelling the human psyche. Incidentally, I enjoyed following the 
Ansir for One link provided on Joel's pages. It produces a report about thirty pages in 
length, which I'd rate accurate in the general picture but incorrect on several specific points. 
Then again, I said that about my natal chart (both the tropical and the sidereal one!) How 
can I possibly become a true believer when the clear trend is less Libran and more Virgoan 
with each passing year? Maybe I read the chart upside-down!? {said with typically Gallic 
shrug} For your interest and delight, I'll include Ansir for One with the personality test links 
on the webpage.
CLF:   Remotely related to remote viewing, perhaps ... :
Even if there's not the remotest chance that I've remote-viewed any real places in today's 
world, I do slip off to distant galaxies or future times in some of my dreams; fairly frequently, 
I have night dreams which could be science fiction movies. Then, I'm both irritated and 
saddened by the buzzing of my alarm clock; I wish for nothing more than to return to that 
magically vivid, tantalizingly novel, futuristic world.
The psychic connection: In Summer 1996, I did two things which I'd never done before; I 
actually attended a Mensa Annual Gathering and had my aura read (by machine, of course) 
at that gathering. I placed my hand on a mysterious metal contraption; then, after several 
minutes, a photo appeared displaying a colored aura around my head (no, not my hand!); a 
computer printout then provided a "personalized" interpretation of what the various colors 
and their positions meant.
Of course, people want the proper ambience with an aura reading. Crystal balls. Tasselled 
lampshades in a dimly lit chamber. Beaded curtains through which ghosts may waft in and 
out at will. A chilly draft and occasional chilling thump. A completely automated aura-meter 
beneath brilliant flourescent bulbs in a convention hall is unlikely to induce profoundly 
mystical feelings. Knowing this, Psychics Incorporated did supply the requisite gypsy - that 
is, a plump, matronly woman with an appropriately slow and empathetic contralto voice, 
dyed black hair beneath a golden scarf, eyes seemingly omniscient merely because of their 
size, and thick makeup which made her face seem tanned. The great Madame Penelope 
(alias "Mrs. Smith" in real life) solemnly read the computer printout, then intoned that it (of 
course!) corresponded exactly with what she had psychically divined.
When I was a kid (now we're looking at ancient history), I once had to "play palmist" at a 
neighborhood fair; my mother, having seen me reading a book on the subject, volunteered 
me for the job. As would any normal, adult-fearing kid, I took my assignment seriously; it 
was time to put my rudimentary abilities in acting and story-telling to the test. I sat in the 
mandatory palmist's tent, admitting only one adult at a time to the hushed solemnity of the 
oracle's quarters; the grownups, only coming for entertainment or to "be charitable" anyway, 
entered my dim-lit chamber with mock seriousness. As they struggled to hold down their 
chuckles, I would massage each palm, ramble some nonsense about the life line and sun 
line (yes, I recalled some trivia from my reading), and muttered that they would surely be 
healthy and happy for a long while to come. Well, even if this wasn't true, it was what they 
liked to hear, and what they might even believe if the child palmist - aka.actress - 
aka.confabulator intoned it with enough conviction.
Of course, the adults all found comic relief in my palmist's tent. No, they didn't believe my 
ramblings - but that was merely because no one had supplied me with a convincing 
magician's robe (complete with stars and sequins), because no scented smoke peridically 

billowed out from mysterious holes in the ground, and because no cobras writhed to tunes 
played by snake charmers just outside the entrance. At my debut, I was a "disadvantaged" 
palmist; had someone merely supplied those cobras, that day might have been the start of a 
brilliant career as a pseudo-psychic.
Altered States and Psi PHenomena (Part Two)
CML:   Okay, folks. Before things get out of hand, I'd like to remind our participants that 
from the point of view of anyone who's had an OBE, it is NOT a matter of belief, but of 
personal experience. As I've pointed out previously in this forum, there in principle exist 
classes of experience which are not jointly observable, and therefore not "scientifically 
replicable". Nevertheless, if subjected to prolonged, unnbiased testing in a cognitive 
laboratory, certain people can prove, beyond any shadow of statistical doubt, that certain 
objective byproducts of their nonreplicable subjective experiences exceed normal causality. 
I happen to be one of those people, and I'm far from alone. Anyone who doubts it should 
look up the Stanford Cognitive Sciences (CSL) Laboratory, the Princeton Engineering 
Anomalies Research (PEAR) lab, or any of a number of immaculate, peer-reviewed 
research organizations that can produce enough methodologically pristine data to make a 
monkey out of the crustiest skeptic.
JCC:   Over the years I have conversed with several people who have described 
extraordinary personal experiences. Such happenings are not entirely beyond my own 
experiences, which are of course closer at hand. Rather, my doubts concern the 
significance attributed to such anomalous occurrences. We are all human animals and 
subject to our own confabulation, motivations, subjectivity, and suggestibility ... and all that 
presuming good faith. Remote viewing, or prosaically, excellent insight? Not for me to say 
whether Gudjieff, Nostradamus, Simon Magus, and their peers were supremely gifted or 
charlatans ... not mutually exclusive categories, really. In my youth, I'd leap on the 
bandwagon and ride on until too many contradictions and doubts accumulated, dispelling 
initial enthusiasm.
There's US$1 million waiting in the bank, courtesy of the Amazing Randi for certain people, 
then! A worthy grant for certain people to continue productive research! Who wouldn't want 
to see those funds collected for a phenomenal breakthrough? I would personally be thrilled 
and honored to receive 6% as a finder's fee {smile}
CML:   Come on now, Julia. Randi's offer is a game among con men. Randi, himself an ex-
charlatan, is a man on a very specific mission: unmasking stage performers who claim that 
their "psychic abilities" are genuine. If his motivations go beyond this, then he's either a 
dunce or a con man yet. How do we know this? Because he has placed unwarranted 
constraints on what he will accept as "proof" of psi effects...namely, that they be 
experimentally replicable at his own personal convenience. That's not a general a priori 
criterion; in fact, when it comes to strongly nondeterministic phenomena, it's not even 
realistic. If Randi were being intellectually honest, he'd have paid off a long time ago.
JCC:   I have taken the liberty of placing a link about this on the webpage, under "Food for 
Thought." Judge for yourself in the matter. I think that one million is a reasonable grant for 
verifying discoveries in this area. There is some risk on the other side, granted.
CML:   Once again, the proof already exists. I'm going to follow this response with another 

that contains a URL for the CSL at Stanford. There's a paper or two there that you ought to 
read. (I'll check out your Randi link, but with all due respect, I doubt I'll see anything new. 
The crusty old devil hasn't changed his tune since the last Ice Age.)
Julia is entitled to decide for herself what she will or will not believe. Like everyone else, 
she'll do what she has to do in any case. But when it comes to the field of parapsychology, 
belief is increasingly beside the point, and Julia's respect for science compels her to tread 
lightly.
JCC:   Always has been beside the point, in that science is ostensibly that which frees 
humanity from dependence upon belief, from accepting assertions upon faith in authority, 
without options.
CML:   Personally, I prefer logic and mathematics to belief. That's why, although I'd certainly 
*like* people to believe me when I say that I have the logical skeleton of a TOE, it's a 
disposable luxury. The fact is, I can prove what I say...prove it, that is, to anyone with a 
sufficiently advanced understanding of mathematical logic and formal metaphysics. For 
those of you who didn't know it, papers on the CTMU "TOE" have been published regularly 
in a peer-reviewed journal - that of the Mega Society - for around a decade now, and no 
one, including several PhD's in mathematics and philosophy, has ever put a dent in it (and 
you'd better believe it wasn't for lack of effort).
JCC:   OK, I confess eagerness to see some of these papers made available on the 
Internet. I've never yet held a copy of Noesis but have certainly heard good report of the 
Editor. {smile}
CML:   Because the whole TOE quest has been bungled time and time again in the course 
of history, belief naturally gravitates toward skepticism in the TOE context. But that's fine, 
because in the final analysis, belief has nothing whatsoever to do with it. Math and logic, on 
the other hand, have everything to do with it, and if one hasn't done his or her homework in 
these areas, one must defer to those who have.
JCC:   Of course, and I may feel much more in accord when I've read more "of it" than I 
have "about it", fair enough. But is there not a conflict in appealing to the authority of 
Academia when favorable while disparaging Academia when it's stodgy about embracing 
concepts challenging to its collective wisdom? There is some consolation in seeing that 
radical concepts sometimes find vindication over the years, all the more solidly when it does 
come. And I sense that advances toward truth (small "t") do withstand the gauntlet.
CML:   Now, hold on a second. Are you saying that I "appealed to the authority of 
Academia" by citing research that happens to have been conducted at various universities? 
Academia, considered as a mindless bureaucracy, can be sharply and easily distinguished 
from specific pieces of research conducted under its auspices. In fact, the single-
mindedness with which academic bureaucrats hog research funds makes it virtually 
inevitable that academia will host a certain number of valuable projects. What I actually 
"appealed" to was the "authority" of certain individual psychologists and statisticians. Don't 
get all specious on me here!
JCC:   Just seeking better exposition of "Academia". There is truth to what you have written 
in your article concerning the antagonism between the academic system and expression of 
genius. I would like to believe that the constellation of Academia is evolving away from its 
origins in the medieval Islamic and Christian seminaries. There is another side however, 
that of the balance of adequate peer review, conservative as it tends to be by nature.
In a sense the point is that the upward progress of knowledge has been advanced by 
cautious progress at the edges, maybe a bit in the fashion of Holland's reclamation. The 
scientific method, doubtless imperfect and tedious, has served tolerably well. Giant leaps 

may be for the Teslas; the lesser lights might arrive there too in time, by small steps. The 
world has ample room for both the conservative and the speculative, without bending one to 
the rules of the other.
CML:   Funny you should mention "peer review". This term connotes a few bad 
assumptions. For one thing, geniuses generally don't have any peers ... especially in 
academia. For another, because the term "peer" is defined with respect to specific 
academic fields, it would seem to be inapplicable to interdisciplinary insight. For yet another, 
peer review is the very process during which a great deal of original work gets - now how do 
I put this delicately? - bald-assedly ripped off by people whose positions in the system allow 
them to get away with it. And last but not least, some of us, despite having enough ability to 
send many academics running in tears to their mommies, do not ourselves qualify as 
"peers" in the academic context, and thus cannot expect a fair shake from academia. Taken 
together, these facts imply that the venerable process of peer review, for all of its benefits 
within academia per se, is a miserable, stinking failure with respect to insight originating 
outside the loop.
My web site will indeed contain CTMU material, but only as I convince myself that I've 
covered my intellectual property rights. To understand what I mean, consider the following 
pair of facts.
1. Under US proprietary law, only patents, trademarks and literary (etc.) copyrights are 
covered. Mathematics, being regarded as "the universal property of all mankind", is 
explicitly exempted. Once a mathematical theory hits the Internet, it is freely downloadable, 
and in the minds of self-interested academics, freely attributable as well.
2. When it comes to the proper allocation of credit for scholastic insight, academia is "self-
policing". I.e., from the viewpoint of anyone outside the academic loop, the fox (academia) 
is guarding the henhouse (Truth, with a large T). Unfortunately, this particular fox is 
notoriously inattentive to the complaints of smaller animals.
These facts form a syllogism culminating in 
3. Anyone not favorably connected in academia who publishes an original mathematical 
theory of any real importance on the Net needs his head examined, preferably by a 
mineralogist.
That's why I usually justify my statements locally, as I give them. If you require more, you 
have three choices:
(a) be patient;
(b) put me in touch with qualified academics willing to sign off on whatever I show them (i.e., 
promise proper attribution in writing); or
(c) provide me with a physical address and cover my publication and mailing costs to obtain 
back copies of Noesis, in which a lot of CTMU material has already been published.
JCC:   That's entirely reasonable, and I will then opt for (a) practice of patience, probably 
my strongest suit nowadays. I salute you for opening an exciting thread of discussion and 
look forward to the continued exposition and development of your work. I'm open to 
including more links on our webpage to ensure balanced presentation. Can any of you 
recommend a worthy site on OBEs and such?
CML:   The URL of the Laboratories for Fundamental Research is www.lfr.org ... It contains 
a link for the Cognitive Sciences Laboratory, at www.lfr.org/csl Of particular interest are 
links to papers by the UC-Davis statistician Jessica Utts (www-stat.ucdavis.edu/users/utts). 
These papers, along with many other links and pieces of information, should bring you up-
to-date on the eveidence for psi, as well as the cats-and-dogs tenor of the psi debate.
CW:   Chris mentioned the Monroe Institute's website which is probably the most useful. 

The URL is www.MonroeInstitute.org.
By the way, if you want to hear about the horrors of peer review look for Becker's The Body 
Electric (not in print) or Koestler's The Case of the Midwife Toad (don't know if its in print).
JCC:   I shall then be adding links supplied by Chris and Cal to the "Food for Thought" 
section of our webpage. These should offer some interesting reading, even if I can no 
longer count web-surfing as legitimate out-of-body experience. {smile} It has seemed to be 
thus on a few occasions!
CML:   Julia's right about the merits of creativity and personal fulfillment, and I'm confident 
that she has ample familiarity with both. I'm equally confident that she knows better than to 
confuse believability with demonstrability. She's too intelligent to make that mistake.
WRW:   Yes, soon mere belief can become worship and the entire point of the experience. I 
am someone from the "bible belt" and was raised in a protestant denomination. I question 
so many things and do so at the risk of being labeled a "heathen". I know that I am not, in 
my understanding. It just appears that the vehicle used to comprehend anything paranormal 
has been reduced to such a mundane level. Worship must come with acceptance or so it 
appears. Given this, I can see how that OBE could become another cult worship and soon it 
would be governed by certain man made rules with a priesthood and a hierarchy and so 
forth.
CML:   Good points. However, let's not miss the *key* point: the distinction between 
confabulation and reality depends on how we define "reality". Maybe mystics define it "too 
broadly". Then again, maybe scientists define it too narrowly. One thing, however, is for 
sure: we can't just perch on the fence, having the cake of intense subjective experience and 
eating the preemptive constraints of conventional science too. And don't let your youthful 
disappointments with stage performers turn you away from modern statistical research on 
psi. It's worth a look.
JCC:   Perhaps it comes down to questions of admissible methodology. My experiences 
can be regarded as subjective, associated with changes in serotonin levels or whatever (my 
speculation, only). Whether cause, effect, or neither, I don't know. A therapist's use of the 
Netherton technique produced (or induced) really neat anecdotal material. Not much there, 
actually, to examine objectively. We are then outside the bounds of science, which, yes, are 
"narrow". Mysticism does not have such bounds... one {can} draw some distinctions 
between subjective and objective happenings, it would seem, at least for present level of 
knowledge. My youthful tendencies have been to commit to belief systems all to readily, in 
many things not to good advantage. Probably no one else has acted thus. {smile} So, I 
changed horses quite often in midstream before learning to swim and asking what I was 
doing in this river in the first place!
CML:   Well spoken, for the most part. However, I'd like to draw attention to your phrase "for 
present level of knowledge". Whose level of knowledge are we talking about? The fact is 
that the distinction between subjective and objective reality already lags far behind even 
mainstream science itself. The two main theories of physics on the large and small scales, 
relativity and quantum mechanics, suffer from numerous paradoxes which cannot be 
resolved as long as this distinction stands in its current benighted form. The CTMU changes 
all that. The only problem is that academia, being chock full of smug, self-satisfied little 
specialists used to being patted on the back for penetrating tunnel vision, contains very few 
people even remotely capable of dilating their mental apertures enough to accommodate a 
revision. That's why we have a "crisis in physics", not to mention philosophy, psychology, 
and every other deep science you can name. In other words, although the average scientist 
clings to it like a barnacle, the distinction on which you're relying is inadequate to the issue 

we're discussing.
On Society and Socialization 
CW:   I was thinking recently that the modern era (18th century and later) is characterized 
by attempts to create group dynamics that mimic the function of an individual human mind. 
In other words a group that can think the same way an individual brain can.
JCC:   That's an interesting consideration, perhaps in view of the different thinking styles 
modelled by the Myers-Briggs types, for example... that optimum results might be obtained 
from a certain mix of individual intelligences. And here too is the value of new approaches 
to intelligence(s) plural as Gardner and Sternberg are picturing.
CW:   Which leads to the question: to what degree do individual neurons know intent? This 
actually isn't as ridiculous as it might sound. Roger Penrose wrote at least one book on this 
question, The Emperor's New Mind. But I think if we can witness the initial evolution of the 
ability to think we will find similar attempts on a neurological level as the great (or notorious) 
social movements of the modern era.
JCC:   I don't know. Might there be some higher-level governing centers localized within 
various brain structures? The key might involve finding ways to read the "assembly 
language", the physical representation of ideas and symbols in the brain.
CML:   On a brighter note, Cal, ever heard of Teilhard de Chardin? The idea of a millennial 
quickening of a collective mentality is not new, and does indeed have neurological parallels. 
In the most recent issue of Noesis, the idea that the upper-echelon high-IQ community 
spans such a context is explored on the basis of specific improbable coincidences that have 
occurred recently among two or three groups. (Rather strange that you just came up with 
the same idea, isn't it?)
CLF:   These subjects (the quickening of a collective mentality, awareness of synchronistic 
events, etc...) are also discussed in The Celestine Prophecy.
JCC:   {smile}
CML:   Again, while the complete proof would occupy more space than we currently have, 
what it comes down to is this: we live in a universe in which space and time can be 
described as generalized forms of information and cognition. Because information and 
cognition are recursively defined concepts which cannot be separated - for one possible 
proof of this microassertion, I refer you to the theories of relativity and quanta - there exists 
a mathematical description of spacetime in which it has enough "hidden info-cognitive 
structure" to support a generalized version of what we call intelligence. By undertaking a 
similar generalization of the concept of purpose - the term "purpose" is merely a generic 
answer to why? questions regarding events in the absence of deterministic laws - this 
collective intelligence can be endowed with volition. Ergo, a group mind.
In principle, the same "transcendental mechanism" functions among neurons (Penrose 
actually offered a specific quantum-level biomechanical mechanism through which this 
might function). [Note that the preceding paragraph contains several specific assertions that 
you can challenge by making contrary assertions.]
WHK:   Is this how the Borg started? - If so, resistance really IS futile!
LDL:   Excellent points ... However, in an effort to keep Colloquy congenial I have written 
my last on the subject of Chris's assertions. He has failed to offer sufficient proof for me and 
that is enough. I can not in good conscience acknowledge he has proven the existence of a 

universal intelligence. Unlike Chris, I make no claim to have proven there is no universal 
intelligence and simply state my belief and my opinion that, without evidence to the 
contrary, I suspect there is no such critter.
CW:   One point we can't really be sure about is whether or not this universe exists as an 
independant entity. Intelligence, when refering to an individual entity, seems to imply 
independance of cognition.
When we talk about hiq societies, "group" mind in some sense contradicts the notion of 
individual achievement upon which selection criteria are based. I would say in that regard 
that HIQ societies are made up of many people who would like to participate on a group 
level in some way but are somewhat confused about it.
When I was talking about societal processes in some way mimicking neural processes I 
was thinking specifically about the widespread use of democracy and to some degree 
socialism as a way of making decisions on a mass scale. IN some sense they represent two 
ends of a spectrum (one end being self-organization the other being self-analysis). An 
important factor in human intellectual development has been the availability of high quality 
protein in the diet which in some societies has become the norm rather than the exception. 
We are the fattest civilization in history are we not?
But I think group process is going on all the time but up until now it has been on more of an 
animalistic level sometimes organized {Triumph of the Will} and sometimes not {On the 
Waterfront}. There seems to be a necessity right now to go beyond that if we plan on having 
a half decent place to come back to if we are to come back. However, as with human 
evolution there is no way to actively plan this possibility in advance. We actually have no 
idea what this possibility would actually be like any more than your cat can understand 
chess. But, in the meantime perhaps we could endevour to be a little more patient with each 
other???
LDL:   Short Range: We are the fattest people in history because of a variety of reasons; 
certainly more available food, meat in particular, moreover meat often loaded with estrogen, 
and less strenuous lives, vocationally as well as recreationally. Estrogen seems to be some 
factor in the equation and there are many undetermined effects on what trickle doses of 
estrogen might do to males and females. Sperm production, I've read, in American males is 
down 50%. Population, however, is still increasing, but violent crimes have dropped. 
Cyclical, or other factors? We may be doing strange things to ourselves, all with the best 
intentions. Will they ultimately be for the good, or for ill? Any guesses?
Long Range: With the human genome project we may soon be able to create remarkable 
human beings as different from ourselves as we are from neanderthal ... are we the seed of 
our own destruction? Could we embrace our own artificially created children if it was for the 
ultimate good, and allow ourselves to quietly disappear from the scene? Perhaps we might 
be revered as the benevolent fathers of the next evolutionary step by those we create!
EM:   I see the debate rages on for membership in this august conclave. I once was truly 
clever, but God punished me for my intellectual arrogance, now I must limp along at the 
99.5 level, and strain to understand the arguments of those securely ensconced in the 
intellectual firmament. Sic transit gloria, alas, it was fun, but nothing lasts forever:):):)
I will be on television tomorrow. How's that for an attention grabber? Anyhow, I will be, all 
the same, a local station in Hazleton, PA, to talk about my newsletter. I am very pleased 
with it, my apprenticeship with The Baltimore Freethinker Journal has been turned to very 
good account. My book sells for $9.95, a complete set of all issues of the Anthracite History 
Journal is $15, and we have only produced eight of them so far. The next issue, June, will 
begin the 3rd year of publication. I did what I meant to do, establish a publication that would 

bring together all the diverse activities of people concerned with anthracite history, and 
provide a forum to express any views they might hold.
Actually, I just unintentionally touched on the meaning of life there, in case you didn't notice. 
Expression, expression, expression, that is what we live for, that is what we long for, that is 
what we bash each other in the head for. To gather in a small group and hold forth our 
ideas about anything is expression, quite as much as how we array ourselves to appear 
before our fellow humans. How we dress, walk, talk, dance, sing, write poems, write books, 
play basketball, fornicate, conjugate, and subjugate are all forms of human expression.
The fount of all this industry is the preservation and enhancement of the individual egos of 
the collective organism we participants choose to call humanity, the wellspring of course, is 
power, or our idea of power. Some of us think there is a higher power, or external power, or, 
if Hindu an all-pervading internal/external power. Others are pleased to relax, or languish, in 
the serene disapproval of those needing such powers.
I have always been puzzled by the true believer, but I do admit to being equally puzzled by 
the true non-believer, in that I see it as quite as much a leap (saltatory) of faith as that of the 
believer. I got a solicitation in the mail today from the American Humanist Association to re-
subscribe, and I would be presented with a handsome 2nd Humanist Manifesto. I detested 
the first one, why would I want a 2nd? I didn't care much for Karl Marx's, or Martin Luther's, 
either. If I want to be conventional, I could go back to the Evangelical Church and mutter the 
Apostle's Creed every once in a while, and be in familiar surroundings.
I read something surprising recently. About 300 years ago, the difference in per capita 
income between the wealthiest and poorest nations was not very large, perhaps five to one. 
This year, it is 400 to 1, Switzerland compared with Mozambique. I see this as placing an 
unbearable strain on human relations between nations. Even in our own nation, there is a 
vast disparity between the distribution of resources between the various classes. Few 
among the public realize how great is this disparity.
JPr:   I am not surprised, and I agree with you that it puts an enormous strain on human 
relations between nations. Moreover, it puts a significant strain on relations between groups 
within the boundaries of our own nation.
I noted this phenomenon with respect to school district funding disparities in 1979, when in 
my doctoral program I took School Finance (in Illinois). I commented in class that I felt the 
rich districts within Illinois appeared to be getting richer at a stronger pace than the poor 
districts appeared to be getting richer, thus increasing the disparity. He seemed astonished 
at the time, and commented to the class it would make a great dissertation topic for me to 
pursue. I had the good sense not to do it, since I would have been bored to pieces with that 
topic, but my comment proved to be true in the next several years for Illinois school funding. 
Many states have had lawsuits regarding funding equalization.
Now, that phenomenon is becoming so pronounced generally that it is obvious to people 
like you and me in relation to general economics, not just school funding (which of course is 
essentially general economics for communities!). I am told that this phenomenon occurred 
at the end of the last century and resulted in the Great Depression. Someone told me there 
is a book on the history of this at the turn of the century, but I can't remember who told me 
(my sister, I believe) or the name of the book.
The ultimate result of such a great disparity in resources is often war. That is probably why I 
am so socialistic at heart. I prefer sharing to war in re-distributing wealth. I don't mind paying 
taxes that are used to support people; I also support the concept of a military for national 
defense, but not at the expense of human lives back home where some welfare efforts are 
important.

Oh, dear...there it is, I'm on record as a ...liberal!
EM:   ... A subject I once wanted to use for a book was My Baltimore. Had I done it, or if I 
ever do it, the approach would be sociological, though not in the usual academic sense. I 
was looking at a zip code map of Baltimore about 10 years ago right after I had been 
studying a census map. At the same time I was working in the Baltimore County Health 
Department, a small part of my assignment at the time was to file hospital admission 
records for mental illness. On my own time, I was gathering information for a book on 
alcoholism. My wife then attended the Smith College annual book sale at the Towson 
Armory, an event certainly known to Julia, whereupon she brought home a miscellaneous 
collection of books including some recent issue Baltimore Blue Books, the social register for 
this city. I noticed a distinct pattern of addresses for the membership of that august body. I 
have just revealed I am not one of them, or I'd have had my own current issue, wouldn't 
I?:):):)
For the alcoholism book, I had the then current AA Directory of all meetings in the city, and 
it was an easy matter to get the NA Directory as well. I then went to the phone book, and 
made a zip code list of all churches. Then I got crime lists for the year, made a zip code list 
of that. I made a zip code list of everything else. I made a zip code list of Baltimore Mensa, I 
had the directory for that year. My interest was in who does what, when, how often, how 
many, and why by zip code in this city and contiguous counties as defined by zip code for 
Baltimore. There was a mechanical approach in all of this, the book would have a clear 
plastic overlay with the zip code boundaries defined, which could then be put over each 
page of information to reveal quickly and graphically the particular body of information on 
that page. Which zip code had the most Mensa members? Which zip code had none? It has 
been my observation in life that in general conversation, any statement that begins with the 
word "people" is usually erroneous or uninformed. My book would give a picture of this city, 
what groups of people in particular sections do more or less of, and who they are by various 
divisions and stratifications. Even in my small amount of research, there were some 
surprises ...
KB:   I find one of my Problems with social Situations (and thusly [perhaps wrongly] I avoid 
them or remain distant until approached) is "male camaraderie." "Hello you ugly so-and-so", 
"Boy you look like ****," etc... I have found two men Friends (more appropriately, soul-
mates) in my twenty-eight Years that I can talk with about anything. It's not that I can't 
communicate on various Levels--rather, I do not want to communicate at some Levels.
Unfortunately this Plane to wit I am suspiciously absent is where most, IMHO, dwell. 
Occasional good natured barbs are most welcome. When the barbs become the foci of the 
relationship I take my emotional leave--this of course being viewed as aloof, moody, erratic, 
ad naseum. Many call me "thin-skinned." Perhaps--however, as I'm sure many here must 
have had similar Experiences in earlier life (say, School) that has made them guarded (thin-
skinned). Well now, isn't that quite a little rant I've spent way too much time upon?
EM:   I do so love a heartfelt rant! It's not the content so much as the style and passion that 
moves me. I loathe the hearty whap on the back and the bone crushing handshake, 
followed by the shouted inquiry, "Whaddya think o' them O's?" during baseball season. The 
truth is I never think of "them O's", never did and never will. I found baseball dull in 
childhood, the few times I engaged in it.
JPr:   I'll second the motion! Educational experiences are some of the worst for kids who 
are bright, aren't they? Even the teachers barb! (educator for 25 years)
EM:   Julia {noted that another list} became inactive, with the participants retreating into lurk 
mode. Does this relate to new member Lisa's comment that people rarely want to really 

"talk"? If so, why, and what can be done about it? You know I have opinions and theories 
about everything in the universe, known and unknown, and this situation is no exception. 
When we "talk", we express ourselves. Everything we do, and don't do, is a form of 
expression. We try, usually, to arrange this expression so we will be well regarded by our 
fellow primates, the moreso if they share a peculiar intellectual abnormality. Once we have 
had the audacity of committing our words to print and the gaze of others, we run the risk of 
our skill in expression being examined and the merit of our content being weighed or 
debated ( mene, mene, tekel, upharsin?). If our expression meets with ill favor, we may find 
our ethical and moral qualities under scrutiny, as well as parentage, upbringing, and the 
validity of standardized test scores proffered. I respectfully submit this is the genesis and 
sustenance of the practise of lurking.
Cosmogony, Holography and Causality
Comment #1:
I notice several recent posts on this board asking about those aspects of ID theory having to 
do with cosmogony, holography, astronomy and other questions to which the biological 
aspect of ID theory is intimately related. 
Dr. Frank Tipler of Tulane has a theory which says that the universe is a function of "front-
loaded" design by the God of traditional monotheism. In essence, Dr. Tipler’s theory says 
that the evolution of the universe is following a pre-written script consisting of the laws of 
physics and implicitly containing the blueprint of life, and he offers what he and others 
consider to be strong physical evidence of such a relationship. I have a theory which goes 
somewhat farther, acknowledging the distributed or front-loaded aspect of design while 
making the universe a self-contained feedback system in which the design function can take 
specific local arguments and deliver specific localized output in the course of evolution. My 
theory, called the CTMU (short for Cognitive-Theoretic Model of the Universe), implies that 
a self-aware designer distributes over reality, or in theological terms, that "God configures 
Himself as reality." This, of course, is fully consistent with the idea that God is the ultimate 
reality, an insight that has been oft-made but seldom explored by theologians and 
philosophers. 
With regard to holography, the CTMU utilizes a concept called hology, which differs from 
holography in its dynamism; specifically, hology is a species of self-similarity involving a 
kind of space-time dualization in which certain general aspects of the structure of reality are 
internally self-distributed as rules of grammar. The associated language is called SCSPL 
(short for Self-Configuring Self-Processing Language), and it includes the laws of physics 
as a restricted "sublanguage". SCSPL can be characterized as a self-contained 
metaphysical extension of standard logic. While some of the informational and probabilistic 
aspects of this language have been generically addressed by the noted ID theorists Michael 
Behe and William Dembski, they have done so without committing themselves to specific 
cosmological and causal models (including those described by Dr. Tipler and myself). This 
has been convenient inasmuch as producing a true model of causality is a rather tall order. 

With certain notable exceptions, many of those involved in the ID-neoDarwinism 
controversy seem not to grasp the full connection between the ID hypothesis and systems 
of cosmology. They want to restrict the discussion to conventional laboratory biology, which 
places severe limits on the kinds of problem that can be addressed and the kinds of solution 
that can be verified. These limits can be regarded as the price of undertaking causal 
analysis and attempting to verify causal hypotheses without committing to a specific model 
of causality. Since both neo-Darwinism and ID amount to (biological) theories of causality, 
neither side of the debate is in full compliance with the logical entailments of its position.
While I don’t know the level of support enjoyed by Dr. Tipler’s work, mine is officially 
unsupported and pursued strictly as a labor of love. So as near as I can tell, even if one 
asks the right questions - and certain questions that have recently appeared on Brainstorms 
at least resemble the right questions - one is unlikely to hear much about cosmology from 
rank-and-file ID supporters, who typically seem uninterested in such matters and evidently 
do not understand all of the logical issues involved. In answer to such questions, I'd thus 
have to respond that even though work on the causal and cosmological foundations of 
Intelligent Design is only infrequently acknowledged, it has been ongoing for quite some 
time. 
However, this brings up another question in dire need of attention: why are so many 
biologists who formulate and express opinions on hypotheses and methods critically 
dependent on the fundamental nature of causality apparently so unconcerned with the 
fundamental nature of causality? This is a question to which I myself would one day like to 
see an answer.
Comment #2:
In response to David Garrett, Rex Kerr says that what David calls "the mental aspects of 
reality" may be ontologically secondary and "emergent". 
No, they may not, at least insofar as emergence is an orderly process requiring coordination 
by general, abstract syntactic principles. That is, where emergence entails syntax, and 
syntax is abstract (mental or trans-physical) in nature, mental reality is critical to the 
emergence of physical reality and not vice versa. The fact that mind then achieves physical 
instantiation is beside the point; syntax is still more general than, and therefore logically 
prior to, the physical objects and states which instantiate it.
In response to a statement by Zachary, Rex then writes "there are multiple inconsistent sets 
of axioms of logic".
Quote:
“Here's a potential counterexample.
Let P(x) be a proof that x is true, and let LC be the statement that logic is internally 
consistent. Then:
System One: NOT P(LC) is true
System Two: P(LC) is true

System One is the subject of Godel's Incompleteness theorem, which basically says that 
NOT P(LC) for first order predicate logic (and it's been extended to other areas). System 
Two is the system that must be assumed if one can start with rationality and know that 
rationality is justified.
So, assuming we admit the Law of the Excluded Middle (i.e. A OR NOT A is a tautology), 
then one of the following holds:
(1) First order predicate logic is inconsistent.
(2) The process of rationally justifying rationality is impossible.
(3) There are multiple inconsistent sets of axioms of logic.”
Godel never intended to show that logic must be either true or inconsistent; that would have 
been too obvious. Instead, he showed that a particular combination of logic and arithmetic is 
either complete or consistent, but not both. Because the consistency of logic has never 
been seriously in question, an undecidable (but logically correct) axiomatic system is merely 
incomplete, which means that certain true relationships cannot be verified within it. 
The reason that logic cannot be inconsistent is that when all the inessential chicken-
scratchings are stripped away to leave only the essential ones, consistency still equals 
truth, which equals the inclusion of the formal or factual image of an expression in a 
descriptive system or its universe. Because logic is tautological, i.e. universal and reflexive, 
that which is logically inconsistent is not only illogical but unreal. Any conceivable attack on 
the consistency of logic would effectively destroy its own logical integrity and thereby defeat 
itself, excluding itself from reality in the process. Because there inevitably comes a point at 
which any attempt to deny the consistency of logic undermines and invalidates itself, it can 
be taken for granted that logic is consistent. If it weren't, then all argumentation regarding it, 
along with the entirety of human thought, would be futile...and in that case, it would be 
pointless to argue for or against anything at all.
Because the laws of nature exist independently of concrete instances and are therefore 
abstract, reality must conform to a self-consistent logical syntax consisting of valid rules of 
sentential and semantic abstraction. The rules of perception must be isomorphically 
consistent with the rules of logic; otherwise, perceptions could not be mentally acquired, i.e. 
isomorphically mapped from the natural world into the mind of the perceiver. The same 
applies to cognition itself; if there were no consistent cognition-to-cognition identity 
mapping, then cognition could not acquire its own products, and what some have 
characterized as the "many-valued logic" of human mentation would infiltrate cognition at 
the syntactic level. Cognitive-perceptual reality would then disintegrate for lack of a stable 
syntactic identity in terms of which its rules of structure, evolution and recognition could be 
formulated.
Rex continues: "However, if you can't use logic to prove that logic is valid, then it may 
matter how you happened across it. For example, suppose you find a box and you point it at 
the sentence, "NOT (A AND B) IFF (NOT A OR NOT B))". It says "true". You do this a 
whole bunch and it always gets the right answer. Then you write down, "This box always 
gives me the truth value of a sentence." You point the box at it. It says "true". But are you 
justified in believing this answer?"

Actually, you are. This is because of another (related) kind of box called a "truth table", 
which effectively defines logical connectives and functors on truth and consistency and vice 
versa. A little reflection reveals that these functors (not, and, or, implication and 
biconditionality), along with the synonymy of truth and descriptive inclusion, are utterly basic 
to human thought. Try to avoid them, and you forsake any possibility of being able to think 
straight. The mental operatons they represent are simply too elementary and too universal. 
Try to define alternates, and you'll be forced to do so in terms of the originals.
The looplike tautological axioms of logic will always be consistent because, in order to 
implicate them in any sort of contradiction, one must add on a particular instance of LSAT in 
which expressions are no longer isolated. Instead, they are linked in common variables to 
which truth values are to be assigned. These tentative linkages and assignments do not 
occur in the system of propositional logic per se; compound expressions admitted to the 
formal system of logic must previously have been proven true under all possible truth value 
assignments, and thus identified as tautologies. In contrast, an arbitrary instance of LSAT 
has not been proven tautological or even solvable; its internal consistency depends on the 
logical evaluation of its (likely) non-tautological structure under all possible "test 
assumptions", or compound truth functions assigning truth values to all of its sentential 
variables in all possible models. When all possible predicative test assumptions fail, it is the 
particular LSAT configuration and not logic which collapses. 
As we have already noted with regard to the equivalence of truth and inclusion, this kind of 
inconsistency can mean one of just two things: either something that was assumed to be a 
part of the inconsistent LSAT system is not really a part of the system, in which case the 
system can be surgically altered and restored to logical consistency ("the theory only 
appeared to be logically inconsistent and can be saved"), or the system is irremediably 
inconsistent and cannot be surgically rehabilitated, in which case it is fundamentally illogical 
and all argumentation based on it is futile ("the theory must be discarded"). But in neither 
case is the truth of logic at stake, for logic stands above the illogical or extralogical structure 
responsible for the failure. It distributes over the formal sentential (syntactic) structures of 
individual expressions, independently of the attributive (semantic) links distinguishing the 
structure as a whole.
Although inferences depend on models - this is something which scientists and 
mathematicians must always consider - logic is a truly basic, syntax-level self-model that 
defines its own functions and connectives on its own consistency, and therein lies its 
tautological integrity. It defines truth on its own terms, and its terms on its own kind of truth. 
Ultimately, the only kind of consistency that matters is consistency with the tautological 
definitions of sentential functors - everything else is "to be determined" - and this is what 
logic is by definition. Tautology, as based on the universal descriptivity and cognitive-
perceptual necessity of logical functors, is its characteristic attribute, and this makes it 
unconditionally correct. Where predicate logic and model theory are properly regarded as 
extensions of propositional logic accommodating semantic and interpretative operations 
respectively, they are subject to the rules of propositional logic and do not support violations 
of it. 
The premises on which Rex bases his arguments are closely related. That cognitive and 
perceptual reality can be separated even on the logical level, that one can use logical 

functors and tautologies to show that logic can be inconsistent or that there can be 
"alternatives" to logic, and that the truth of a logical tautology is somehow subject to 
empirical confirmation all lean on each other. Because none of these premises stands up, 
the three of them fall as one. Such argumentation is utterly indebted to logical functors and 
tautologies for any claim it might have to validity, and logic by definition will not permit these 
functors and tautologies to be used to subvert it. Only a theory of reality reflecting this 
incontrovertible fact can yield any amount of certainty. 
Comment #3:
Good discussion. Since Rex Kerr has been (intelligently) questioning and critiquing the 
CTMU and its conceptual ingredients, I'll post a quick response to some of his points.
Rex asks "why logic as a basis for reality?" Because logic by definition comprises the rules 
of cognition (and thus of recognition), anything which does not satisfy it is neither directly 
nor implicatively recognizable as a part of reality. Moreover, logic has only one syntax - the 
syntax of human cognition, to which any other comprehensible syntax necessarily reduces 
by its necessary reliance on the basic logic functors in terms of which we recognize mental 
and physical reality. (If Rex's alternative axiomatization of logic is consistent, then it is so 
only by reduction to logic qua cognitive syntax.)
When Rex holds any claim of certainty regarding the logical consistency of reality to be 
nothing but "unjustified conjecture", he is saying that reality may be logically inconsistent. 
By the definition of logic, and the mutually-normative truth values and logical functors which 
comprise it, this amounts to saying that reality may violate the rules of cognition. But since 
the rules of cognition are what allow us to recognize reality, this amounts to a contradiction. 
Rex compares logic to impulse functions, which he characterizes as (possibly unreal) 
cognitive constructs that help us break reality down into manageable chunks. An impulse 
function is an input-to-output transducer defined to operate in a particular way suiting a 
particular explanatory context (or the rule or mapping executed by such a transducer). But 
functions in general are quite another story. When cognition is generalized to coincide with 
information processing as executed by any consistent function or transducer, it is anything 
but unreal. It is in fact the stuff of which reality is made. 
Rex asserts that claims of certainty regarding the CTMU are unreasonable. However, the 
CTMU is based on premises that have been clearly explained, and a case can be made that 
it is tautological. To effectively contest the certainty of the CTMU, one would have to 
invalidate one or more of its premises, or show that the CTMU does not necessarily follow 
therefrom. Merely questioning the certainty of logic won't work; the certainty of logic is 
tautological given its definitional equivalence to the rules of cognition through which we 
recognize reality. Again, that which does not conform to logic cannot be recognized as a 
part or aspect of reality, either directly or by implication. 
Regarding the conceptual difficulty of cognition, Rex dismisses arguments based on 
"cognition of the gaps". In the context of a self-contained mathematical structure supporting 
the concepts of attributive (semantic and syntactic) information, cognition may be 
generalized as information processing and/or generation. Beyond this, we don't need to 

know what it is or what it does, except that (1) it must obey a set of logical invariants 
comprising its identity and supporting its procedural integrity, and (2) it must display 
mathematical characteristics appropriate to its generic functionability within its supporting 
structure, e.g. telic coherence and reflexive (self + input) modeling. From these attributes 
alone, certain interesting consequences can be derived regarding it.
Rex says that any attempt to solve the problem of mind-matter dualism without nested 
(embedded) models is nonsensical. This is quite true. Up to isomorphism, embedment is an 
identity relation; when one structure A is embedded in another structure B, A coincides with 
B to the full extent of A. Descartes denied that res cogitans and res extensa are the same 
thing; somewhat paradoxically, he maintained that they are fundamentally different. In 
contrast, the CTMU fully acknowledges their identity and their mutual embedment. The 
CTMU does this through conspansion (or conspansive dualization), which involves an 
alternation based on dual modes of containment (or embedment).
Comment #4:
Rex says that I err in conflating or assuming an identity between epistemology and 
ontology. For scientific purposes, reality must be knowable as reality; otherwise, it is of no 
interest to us, and there is no way to discuss it or reason about it. So for scientific purposes, 
there is an epistemology-ontology-reality overlap...a place where they must be conflated. 
Deny this, and there go science and reason; they can't work. We acknowledge this overlap 
simply by considering and discussing reality, which we are interested in considering and 
discussing only to the extent that we can recognize it as reality. If there is something else 
out there that is so deep, dark and noumenal that it can never be recognized or logically 
inferred from our level of being, then it is of utterly no interest to science or philosophy. This 
is why the distinction between epistemology and ontology ultimately disappears in 
discussions about the nature of reality. On this level of reasoning, it is just as Descartes 
said: cogito ergo sum. To know (and be known) is to be. 
Rex takes issue with my comment that "logic has only one syntax - the syntax of human 
cognition", alluding to evidence that formal logic is learned and that the primitive cognitive 
operations humans perform are not the same as standard logical operations. But they are 
the same, for they require a model, and the test of any such model is that it meet logical 
standards of definition and consistency. For example, say that we model cognition on its 
physical vehicle, a biological neural network (or "brain"). Complexity notwithstanding, we 
can in principle construct an adjacency matrix for such a network and assign firing 
thresholds and timed value sequences to neurons and synapses, thus achieving a vectorial 
representation of cognition. We can allow for learning by letting the firing thresholds vary; 
we can even allow for neural plasticity and cognitive impairment by allowing the matrix to 
undergo gradual or catastrophic evolution in a higher-order space of matrices. Cognition 
(and recognition, the interface of perception and cognition) is then a flow of information 
through an evolving matrix, a matter of 2-valued transformations of synaptic vectors and 
matrices thereof. Achieving a biopsychological explanation of cognition means mapping 
psychological events to such a 2-valued model, effectively reducing them to 2-valued logic.
Rex maintains that absolute certainty is never a realistic concept, but that the logical 
consistency of reality is at least as certain as anything else...a working hypothesis that is 

"good enough" for our epistemology. In fact, we can go somewhat farther that this, 
observing that any epistemology or ontology which runs afoul of 2-valued logic is 
imponderable and therefore of zero interest to science and philosophy. While it is true that 
we can consider illogic as (technically) a primal part of reality, we can do so only by 
excluding it from the level of reality that is actualized, and that we recognize and 
comprehend, in terms of 2-valued information (including information regarding any sort of 
"fuzzy" measures or functors). In fact, illogic is actively excluded from reality as its wave 
function evolves; Schrodinger's cat remains mercilessly confined to its box.
Rex is right when he supposes that my statement "When cognition is generalized to 
coincide with information processing as executed by any consistent function or transducer, 
it is anything but unreal...It is in fact the stuff of which reality is made" is fully consistent with 
the CTMU. But one need scarcely understand the whole CTMU to acknowledge the truth of 
this statement. One need merely observe that since information is 2-valued, no reality which 
lacks a consistent 2-valued functional description can be coherently perceived, conceived or 
cognitively rendered and processed as information by scientists, philosophers and other 
cognitive entities. In other words, if cognition could not be generalized in such a way that 
reality can be described in terms of it, then reality would lack a coherent cognitive 
representation, in which case it would be unrecognizable and uninteresting to, and 
existentially unsupportive of, cognitive entities like scientists and philosophers. This de facto 
contradiction, which harks back to Rex's attempted separation of epistemology and 
ontology, means that if we want to formulate a theory of reality (or even coherently discuss 
it), we must accept my statement as true. This does indeed constitute evidence in favor of 
the CTMU. 
I fully sympathize with Rex when he says that he does not, on his own, have the time to 
look up the premises of the CTMU. Similarly, and I say this with all due respect, I do not 
have the time to give every time-challenged person a personal guided tour of the theory as 
though he were the first person to whom I had mentioned it, or even to provide detailed 
references. However, Rex does say that he takes exception to the CTMU, and it is hard for 
me to understand how he can do this without previously having familiarized himself with its 
basic premises. If this is in fact the problem, then Rex can try this paper in PCID. (Please 
see the section entitled "Some Additional Principles".) 
 
Comment #5:
Rex makes the following distinction: 
“What we have decided in the above quotes not that it [generalized cognition] is the "stuff of 
which reality is made", but rather that it is the "mechanism by which reality is understood". 
This is a very important distinction because we can make observations without knowing 
what the causes were, or that there was a cause. Just because we can understand A and 
we can understand B, it does not follow that the transition from A to B can also be 
understood...we have to keep in mind the limitations we are working under.”
"Making an observation" amounts to cognition. Registering a transition from a state or event 
A to another B is also an act of cognition, whether or not a causal relationship between A 
and B can be understood or verified (in which case it too is cognitively registered). Such 

instances of cognition are how we experience reality; as Hume and Kant and others have 
pointed out, we can at best infer the existence of an external world from the consistency of 
our thoughts and perceptions. But inference too is cognitive, and under no circumstances 
can we recognize or infer the existence of anything that cannot be consistently embedded in 
our processes of recognition and inference. As we have already observed, embedment is 
an identity relation (up to isomorphism), and this means that reality is cognitive to the extent 
that it is embedded in our cognitive processes. On the other hand, where reference 
amounts to cognitive embedment, we lack the power to even refer to that which cannot be 
embedded in our cognitive processes. It follows that reality is cognitive to whatever extent 
we can meaningfully refer to it.
It is true that our cognitive reality is structured in such a way that we can often reason as 
though an external world exists. But we can never forget that as far as experience and 
observation and experimentation are concerned, "out there" exists for us as an image on 
the surface of our senses. I understand that this is difficult to grasp after several centuries of 
nonstop "The world exists independently of us on every level of being and it is our task to 
learn what it has to teach us!"; such articles of faith make it very hard for us to see 
ourselves as anything more than purblind moths beating our wings against a diamond-
paned window with "Objective Reality" painted on it in heavy black letters. In some ways, 
this picture is accurate. But all we can say for sure of our cognitive limitations is that (1) to 
whatever extent they are real, they are limits on cognition and nothing else; and (2) if 
anything of a non-cognitive nature is hiding behind them, then it has no place whatsoever in 
our cognitive picture of reality. For cognitive purposes - that is, for all purposes scientific and 
philosophical - it does not exist.
Comment #6:
Rex acknowledges that anything that affects reality can at least be described as "something 
that affects reality", even if we can't say anything more about it because of 
cognitive/perceptual limitations. But we can say somewhat more about it, namely that it is 
recognizable as the source of a perceptible effect and is therefore cognitive to the extent of 
being referenced as a "cause". Rex goes on to critique the name "CTMU", opining that the 
U (for universe) should be replaced with a P (for perception). Perhaps Rex has a point - the 
universe of science is, after all, logically defined on our perceptions of it. However, one 
might equally well observe that it is high time for those who make an unwarranted 
distinction between reality and (generalized) cognition to face their fundamental equivalence 
despite whatever heavy conceptual baggage they may be in the habit of carrying along with 
them.
Rex then says that the "CTMP", as he seemingly prefers to call it, should not use logic as its 
primitive component, but rather the cognitive operations that our brains enact, observing 
that any attempt to convert these native cognitive operations to two-valued logic is inelegant 
and robs us of insight about the nature of reality. However, logic is in fact the syntax of 
cognition by definition. It is the most elegant, compact distillation of our cognitive operations 
that we have been able to achieve in several millennia - this is why we critically rely on it in 
technical fields like cognitive science - and it would be a shame to unnecessarily pitch it out 
the reality-theoretic window just because it is intentionally formulated on a higher level of 
abstraction and with greater concision and precision than various (admittedly interesting) 

details of neural architecture and dynamics. 
Rex points out that I have previously made comments about the relevance of the CTMU to 
evolution and intelligent design, professing that he finds it difficult to imagine any connection 
between such important matters and the PCID paper. To this, I really don't know what to 
say other than "different strokes for different folks". I hope that as time marches on, Rex will 
gradually learn to appreciate at least a few of the observations and insights that I have tried 
to bring to bear on our understanding of nature. 
Comment #7:
Rex, your critique of my terminology is semantic and arbitrary. You seem to want to reserve 
the word "cognition" for the entire set of operations performed by a mind, including ones 
that are illogical or erroneous, while I want to use it in a more variable and context-
dependent way. For example, although I acknowledge that cognition can be illogical, I 
reserve the right to (also) use it to mean "rational cognition" where this is what I intend to 
discuss. I am justified in doing this because the content of irrational cognition, as opposed 
to its mere existence, is irrelevant to the structure of reality as it is described in the CTMU. 
Why is it irrelevant? Because it violates the (logical) syntax of any language capable of 
expressing a meaningful theory of reality. 
I'll make two additional points. (1) In the CTMU, generalized cognition explicitly has 
syntactic and telic (pre-syntactic, trans-syntactic) levels. So if your objection also contains 
the idea that cognition can occur on multiple levels, this has already been covered. (2) Even 
though logic and cognition coincide in perceptual reality and its theoretical representation, I 
agree with you that any disjunction would naturally be of interest, and therefore urge you to 
continue your studies regarding it (such as they and it may be).
Some papers require more than one reading. If my paper is one of them, then so be it; all I 
could do was try to make the material accessible. But in any case, I have a piece of well-
meant advice for you: if you really intend to maintain that there exists a yawning gap 
between the CTMU and "a sensible view of epistemology", you had better read the paper as 
many times as necessary in order to understand what it really says about the epistemology-
ontology-reality relationship. 
Comment #8:
Hello, folks. I'd just like to make some comments about the CTMU and my responsibility to 
defend it. 
First, although I stand by the PCID paper down to the last jot and tittle, the CTMU is in some 
respects a work in progress, and I reserve the right to add to it or qualify it as I like and as 
time permits. In fact, I have several CTMU-related papers now in the process of creation or 
revision, and no one but me has anything to say about their content or timing. 
Secondly, none of the CTMU critics whom I have thus far seen has the understanding or the 
technical ability to speak authoritatively about my theory. I have found many criticisms of the 
CTMU to be motivated by incomprehension and serious misreadings and/or non-readings of 

the published material rather than any supposed conceptual or explanatory deficiency on 
my part, and while I do not claim that my presentation (or that of any other technical author) 
is beyond improvement, I lay a healthy measure of blame for such misunderstandings on 
the doorsteps of those who recurrently complain about them. 
Thirdly, given that I am trying to present a new model of reality, I'm amazed by the number 
of CTMU criticisms that obviously rely on the very cognitive structures in need of correction 
or augmentation. The very existence of ISCID (and for that matter of complexity theory) 
testifies to the explanatory inadequacy of certain conceptual structures and models, and 
endlessly denying or belaboring their more-than-obvious weaknesses would be nothing but 
a waste of time for those of us who already know them for what they are. 
Fourthly, while it may not be politically correct to observe that the viewpoints of some 
people are less evolved than those of others, there eventually comes a point at which 
certain viewpoints must be rationally identified as noise. As it happens, there are certain 
people regarding whom I have made this identification with what I consider to be a high 
degree of certainty, on the basis of not only their tendency to make technical errors but also 
their demeanor...for example, their readiness to lapse into ad hominem attacks and 
baseless, sometimes libelous accusations of plagiarism and other kinds of wrongdoing (e.g. 
"First of all, Syndiffeonesis is a concept whose originality has been disputed."). Nobody with 
a constructive agenda has the time for such people or their nonsense, and I am no 
exception. 
Speaking strictly for myself, when I see a thread or a forum being dominated by such a 
person through sheer volume and vehemence of input, I begin to question the wisdom of 
involving myself at all. While this is merely a personal observation on my part, it suggests 
that those who desire my auctorial input regarding the CTMU not encourage such people in 
their abuse, particularly after they have been suspended for their mischief, transparently 
permuted their handles and come back for more.
I'm usually very happy to respond to questions and critiques as time permits; to some 
extent, I consider this a theorist's responsibility. But I cannot reasonably be expected to do it 
in venues conspicuously ridden by those with established tendencies to deviate from 
matters of content. 
Thanks. 
 
Comment #9:
Since gedanken has unexpectedly taken to spelling my name correctly, I'll try to encourage 
his good behavior by posting a response.
The Copenhagen interpretation of QM is "falling out of favor with physicists" because it has 
been largely supplanted by the relative state formulation (the Many Worlds interpretation). It 
would be a terrible mistake to think that this is because MW is somehow on firmer ground. 
MW suffers from the same problem that CI does: no model, no supporting structure within 
the the spacetime manifold of neoclassical physics, i.e. General Relativity, and no overall 
explanation of its own (aside from an abstract mathematical criterion, "no messy wave 

function collapse"). The way to solve this problem is not by playing pick-and-choose among 
unsupported interpretations, but by providing a model enabling a simultaneous embedding 
of the key concepts of microscopic, mesoscopic and macroscopic physics.
Although nonlocality is indeed a problem in any manifold which fails to support it, this in no 
way rules out the existence of a manifold which does support it. As for the repugnance of CI 
to physical reductionism, i.e. the fact that physical reductionism doesn't work on the level of 
QM, there is absolutely no good reason to prefer stock physical reductionism over QM. As 
for the problematic definition of "observer", the only way to rule out such a definition "in 
principle" is with respect to a given class of models. But since QM, and thus physics in 
general, already lacks a consistent model, this is no objection at all. In any case, the 
problem is simply to produce a model supporting the concept of observation on all levels of 
generality and specificity. Papers already written, and papers now on the drawing boards, 
explain why the CTMU is that model.
The problem with describing all scientific theories as "useful fictions" is that there is a sense 
in which rational cognition itself is a "scientific theory". The thing about this particular theory, 
however, is that it cannot be changed; the paradigm can never be replaced from the 
viewpoints of cognitive entities like us. As I've repeatedly explained, this gives it a kind of 
"absoluteness". The fact that QM is extremely powerful and useful, and in substantial 
agreement with observation, suggests that if it is "logically inconsistent, incoherent (and) at 
some level fictional", this applies only in the absence of an adequate model. Since the 
CTMU can be shown to be an adequate model, it no longer applies (at least from where I'm 
sitting). In this model, the "approximate nature of the scientific (QM) result" is explained not 
merely by randomness or indeterminacy with respect to the localistic first-order Markovian 
restriction typical of inadequate models, but in terms of the self-configurative freedom of a 
self-generative system.
As for the apparent fact that MW does not require nonlocality, this is moot as long as MW 
lacks a consistent model of its own - something more substantial than a "multiverse" for 
which there is no better explanation than that which MW purports to offer for the universe 
itself. Unfortunately, it is impossible to meaningfully explain one entity by positing a larger 
unexplained entity allegedly supporting its existence. In contrast, the CTMU contains a new 
self-explanatory model of physical reality in which limited "nonlocality" is supported and 
wave functions can collapse. "Alternate universes", such as they may be, can then reside in 
the nomological syntax of the observable universe as abstract possibilities in no need of full 
realization. 
In the absence of any reason to accept MW, there is no pressing need to give a reason not 
to accept it...except, for example, that it lacks a model, that it "explains" the decoherence of 
the wave function only by positing the existence of an inexplicable "multiverse", and that it 
uses an abstract mathematical criterion ("no messy wave function collapse") to posit the 
existence of a concrete meta-universe. In fact, when pursued to its logical conclusion, the 
idea that abstract mathematical criteria have the power to shape concrete reality leads 
directly to the CTMU, do not Pass Go, do not collect $200. As for how the CTMU avoids 
"standing on some of the same mistakes that lead to (the) Copenhagen (interpretation)", it 
does so by presenting a self-explanatory model in which these "mistakes" are logically 
grounded and thus no longer qualify as mistakes. 

 
Comment #10:
Gedanken likens the CTMU to "an argument with great complexity proposing to explain a 
free energy or perpetual motion system...a worldview lead(ing) one to consider the number 
of angels dancing on the head of a pin...(which) might have a great deal of self referential 
self consistency." Except for the last part, this analogy fails. Gedanken then offers this 
erroneous view of the CTMU as a reason not to "read the argument over and over until one 
actually understands it," going on to report that he "immediately suspected that [CTMU] 
premises and/or logic had failed, possibly due to fuzzy claims distributed throughout the 
work, simply upon reading the conclusion."
Somewhat inexplicably, gedanken follows up with: "I shall indeed continue to study Mr. 
Langan’s presentation. It does have very interesting aspects. I certainly don’t claim to 
disagree with either all of its methods or conclusions." However, apparently lest anyone get 
too comfortable with this grudging concession, he points out that he "particularly disagree(s) 
with certain conclusions presented...on page 1 and most clearly on page 4, relating (to the) 
syntactic nature of the universe." Finally, regarding those aspects with which gedanken 
does happen to agree, he "questions if they have actually been demonstrated in any 
meaningful way by Mr. Langan."
Gedanken can question whatever he wants to until the cows come home. But until he 
identifies some specific flaw in my argumentation, this has nothing whatsoever to do with its 
validity. To underscore this fact, I will answer his challenge to "name a single aspect of 
physical reality that can be described with logical statements in which there is no question 
of error or degree of imperfection in the description." Here's one such statement: "Physical 
reality must conform to 2-valued logic in every observable respect (whether or not it maps 
consistently to any particular set of contingent expressions)." Here's another: "The overall 
structure of physical reality must conform to a new mathematical structure called SCSPL." 
Here's another: "All aspects of physical reality, including such trivial matters as cosmogony 
and causality, must conform to all of the logical implications of the preceding statements." 
There are others, but they're in the paper too.
Rex Kerr then pipes in with "an added thought on crisp two-valued logic where things are 
true or false with no in between". To boil Rex's argument down to what seem to be its bare 
essentials, he is saying that (1) contingent statements like "The man is in the room" cannot 
be identified as true, false or undecidable until all terms and concepts therein, e.g. "being in 
a room", are properly defined; (2) it is notoriously impossible to define such concepts; (3) 
therefore, nothing in reality can be unequivocally identified as true, false or undecidable; (4) 
so 2-valued logic fails as a basis for reality theory. 
Rex's argument begins to disintegrate at step 2. It is in fact possible to define descriptive 
predicates with respect to a given consistent model of reality. It gets worse at step 3; unlike 
the perceptual primitives and elementary logic functors in terms of which perception actually 
occurs, more or less fuzzy descriptions of perceptual states and observations do not equate 
to the observations themselves. This makes step 4 a simple non-sequitur.
Rex then compounds his error by asserting that "fuzzy logic and 2-valued logic are 

derivable from each other. Neither is needed first. If you have one, you can get the other." 
If Rex means that (A) MVL is derivable from 2VL, and (B) 2VL is derivable from MVL, that's 
fine. But statement B ignores the fact that MVL cannot arise without 2VL in the first place, 
and that 2VL thus retains priority. While one can distinguish among 2VL (yer-or-no) 
possibilities using 2VL alone, one cannot distinguish among the (many-valued) possibilities 
of MVL without 2VL; they either are or are not distinct from each other. If they are, then 2VL 
is presupposed; if they are not, then MVL collapses to 2VL anyway.
Comment #11:
OK, gedanken - here you go. "If event A has happened, then event not-A cannot have 
happened in the same place at the same time." Not specific enough? Then try this: "The 
sun rose in the east this morning. It follows that it did not rise in the west." 
Now, despite the fact that these examples meet your stated criterion - they are instances of 
the self-consistency of reality, i.e. its obedience to 2-valued logic - you'll probably maintain 
that they're trivial, nothing you didn't already know. But if you already knew about them, 
then why are you so intent on questioning the underlying logic? After all, in order to observe 
any such fact to be false, you'd need to do more than just talk about Many Worlds; you'd 
need to split yourself into gedanken 1 and gedanken 2 and observe the contradictory events 
in different universes. (The problem, of course, is reporting back to yourself afterwards.) 
Not to be repetitive, but some of the more nontrivial implications of this situation are 
discussed that PCID paper you seem so intent on dismissing. 
Comment #12:
What is this, gedanken - "Fun With Semantics" time?
Let's try this again. Picture yourself standing outside just before dawn wherever it is you 
happen to be and seeing the sun travel upward from the horizon. To describe this 
observational event, you must define your terms with respect to a model. Once you 
construct a model and formulate your description accordingly, you can formulate a negative 
description. Your model carries this negative description and its negative truth value into the 
corresponding negative perception, which can thus be ruled out. Why? Because the logical 
negation functor works equally well in the languages of description and perception. If not, 
then forget about making and describing observations, because you'll never get to first 
base. 
Suppose that in order to wriggle out of this reasoning, you make good on your threat to 
trudge up to the north pole and hole up in a snowdrift until the arctic winter creeps over you 
like an icy sunless hell and turns you into a popsicle. Nevertheless, when the sun finally 
makes it above the horizon and thaws out your peepers, you will again be able observe the 
event, construct a model, define your terms, describe your observation, negate your 
description, and rest assured that the result does not describe what you perceived.
Perhaps you mean to assert that no matter how hard we try, we are unable to formulate 

accurate models and descriptions. But in that case, we need merely restrict consideration to 
raw perception and realize that making an observation immediately rules out any 
inconsistent observation.
Comment #13:
Gedanken says that raw perception entails maximum uncertainty and unreliability. Since 
physical reality is perceptual, this amounts to saying that physical reality itself is unreliable, 
a scientifically meaningless assertion. Scientific observation is anything but imprecise; 
experiments are generally undertaken with very precise instruments and duly replicated by 
other experimenters, something which could not happen if gedanken's notions about reality 
were correct. (By the way, regarding that of which my theory does or does not take notice, 
I'll be the judge. When it comes to the CTMU, the accuracy displayed by Rex and gedanken 
is limited to what they actually understand of it. Unfortunately, that's not much.) 
Comment #14:
Gedanken: "When we are left with only raw perception, we are also left with the greatest 
uncertainty." [My italics; note that "greatest" means "maximal".]
Gedanken, a few short minutes later: "This of course is highly variable, thus hardly 
'maximal'"! 
Whatever you say, gedanken! (I guess this is what happens when logic and precision go 
bye-bye.) But please be a gentleman and stop parroting Rex Kerr's half-baked comment 
regarding the CTMU. The CTMU associates precision and stability with SCSPL syntax, 
allowing for imprecision on the semantic and interpretative levels of reference. I don't expect 
you to know what this means, but I don't expect you to pretend that you do either.
Now weren't you going to bed or something? 
Critical Questions
Comment #1:
Regarding the debate between proponents of Darwinism and ID, Geoff writes: "Until more 
evidence is found, what reasoning or logic dictates that one position is more sound than the 
other? Perhaps this information will never be found. If that’s the case, then it may suggest 
more appropriate directions for efforts to encourage people to consider probing questions 
about evolution." 
Since Geoff has mentioned logic, I'll provide a reminder that there is only one reasonably 
well-defined logic-based (as opposed to exclusively "biology-based", "information theory-
based" or "physics-based") theory of ID and/or evolution. Called the CTMU (short for 
Cognitive-Theoretic Model of the Universe), it uses the tautological basis of logic to make 
inferences about the natural world. (Browse recent threads and back issues of PCID for 

more information.) This theory logically relates function to structure and easily 
accommodates the oft-noted tautological dimension of Darwinian evolution with respect to 
first-order Markovian laws of nature, obviating the necessity to specifically "decode" 
evolutionary events (although this remains a viable possibility in certain cases).
Comment #2:
In acknowledging that "there is some considerable evidence that the notion of irreducible 
complexity does not completely (or even necessarily partly) exclude all evolutionary ideas," 
RBH invokes certain results in the field of genetic programming. Although I agree with 
RBH's basic assertion regarding the compatibility of IC and evolution, and even that 
computational simulations may support it in certain computational contexts, genetic 
programmers currently lack any coherent mapping from computational to naturalistic 
contexts. Thus, they are severely limited in the extent to which they can present any 
computational simulation as "evidence" of anything in the natural world (and I'm talking 
about a deeper level of correspondence than that discussed in recent criticisms of Lenski's 
work). 
To realize the evidentiary value of such a simulation, one would need at least a minimal 
computational model of nature ... e.g. the CAMU (Cellular Automaton Model of the 
Universe), which attempts to map some part of nature into a metaphysical cellular 
automaton. If, on the other hand, one prefers a self-contained language-based 
computational model, as is suggested by any likening of organisms to assembly language 
programs, then one is driven toward SCSPL and thus toward the CTMU. The CTMU follows 
from the fact that any comprehensive description of nature must account for the generation 
of computational syntax along with mere syntactic content, and must thus portray nature as 
telic and protocomputational rather than merely computational in essence. Nature has the 
capacity to self-generate and regenerate, whereas computational automata and even 
computation theory do not. This militates against the notion that nature is fully analogous to 
standard computation theory.
In other words, the reason that the Cellular Automaton Model and other so-called 
"computational models of reality" fail as models of evolution is that they cannot account for 
the evolution of computation itself, the array or machine or circuit in which computation 
occurs, or even the relationship between the array and its contents. This leads naturally to 
the topic of manifolds. A cellular array is a discrete manifold in which neighboring cells are 
separated and connected by boundaries. In language theory, the concept of a cellular array 
corresponds to structural syntax, but the correspondence breaks down where the 
grammatical or generative part of syntax requires more of a setting than a mere discrete 
manifold can provide. Similar objections apply to any manifold in which discrete fitness 
landscapes are regarded as "points". 
Thus, when RBH concludes that "a fair amount of the incredulity over what evolutionary 
processes can accomplish in unguided (by intelligence) selective contexts derives from a 
failure to appreciate the complexity (in the ordinary language sense) of the topographies of 
high-dimensioned natural fitness landscapes and the consequent availablity of multiple 
different pathways on those landscapes," it is important to remember that there is more to a 
model of nature than any mere fitness landscape or manifold thereof can provide 

(regardless of dimensionality). Obviously, the properties of such landscapes can ultimately 
explain nothing without an explanation for the existence of fitness landscapes themselves. 
Comment #3:
Argon responds "Well, the properties of the landscapes can often explain the immediate 
activities of objects within them. So I wouldn't say they explain nothing. They just don't 
explain everything." 
Good point, and I only have one point to make in return. What I wrote was "Obviously, the 
properties of such landscapes can ultimately explain nothing without an explanation for the 
existence of fitness landscapes themselves." I meant that an "ultimate explanation" differs 
from an "explanation" in that it must cite a model of causation and an ultimate cause 
therein, or to apply Argon's colorful phrasing, "the bottom turtle". In the present context, this 
comes down to the wherewithal of fitness landscapes themselves. 
RBH adds "But until and unless I see some fairly specific mappings that describe the 
correspondences leading from a general theory of everything to the operation of a GA on 
my machines, I'm comfortable not worrying overmuch about the cosmic level of analysis." 
The cosmic level of causation is the ultimate level of causation. So while I agree that "to 
criticize a theory or research approach because it doesn't account for everything including 
the proverbial kitchen sink is not real helpful," this doesn't apply to theories that claim to 
offer ultimate explanations of (e.g.) life. While one could argue that Darwinism doesn't 
explicitly claim to offer an ultimateexplanation of life, I get the distinct impression that many 
of its proponents think that no deeper explanation (e.g. ID) could possibly be required. It 
was for them that my criticism was intended.
While fitness landscapes fail to qualify as ultimate explanations, I readily concur that they 
can be useful at intermediate stages of explanation. I'd merely add that if they have 
explained more than the smaller turtles atop them, it is only because they have stood on the 
shoulders of giant turtles beneath. 
Comment #4:
Argon agrees that "the proximate explanations provided by fitness landscapes and 
experimental work are useful. In fact, the IC->ID hypothesis of Behe is specifically 
formulated at this level. Likewise Dembski's filters & stuff. These approaches are predicated 
on the ability to know the historical pathways (fitness landscapes) available to a system. If 
that mapping is suspect then the support for the hypotheses could be indeterminable."
Quite so. However, the mapping is what it is. And what it is is the CTMU, which implies ID 
even without attention to specific fitness landscapes (in fact, the CTMU is the only picture in 
which specific computational fitness landscapes can be coherently mapped to nature). I 
merely meant to point out that the richness of higher-dimensional fitness landscapes cannot 
be taken as "evidence" in favor of anti-ID hypotheses. Because the CTMU implies a form of 
ID, anti-ID hypotheses are anti-CTMU hypotheses. So on a rational (logical, pre-empirical) 
level, any attempt to use fitness landscapes as evidence for such a hypothesis would 
undermine the theory-to-universe mapping on which it critically depends. 

Comment #5:
Argon states that "Dembski and Behe have got some trouble with the CTMU because they 
are basing their hypotheses about ID on proximate causes." Even if this were so - and I 
don't agree that it is - it would be no big deal. After all, the proximate causes to which Argon 
refers reside directly on the route between the CTMU theory-universe mapping and nature. 
Argon continues: "That is, they will accept proximate explanations for the evolution of IC 
systems (if they exist) as evidence against ID." In that case - and I must question Argon's 
ability to speak authoritatively on what Professors Behe and Dembski would actually do if 
someone (impossibly) produced "empirical evidence" against ID - then perhaps we'd have a 
minor disagreement. However, we'd remain in substantial agreement regarding ID itself, as 
opposed to anybody's idea of what it takes to confirm or disconfirm it. Regarding Argon's 
assertion that "Dembski has specifically argued against theistic evolution and deistic 
evolution concepts as being 'no friend' of intelligent design," I suspect (assuming the 
accuracy of Argon's statement) that the reasons may have something to do with the extent 
to which these hypotheses require front-loading. The CTMU does not require that ID be 
exclusively front-loaded. 
Argon goes on: "So what is being discussed by Behe et al. at these levels is not whether ID 
is necessary for the fundamental underpinnings of the universe. Instead, their argument is 
about the manner by which intelligence is manifested in local situations such as with the 
emergence of some types of biological features ... Basically, their IDea of ID is not exactly 
your IDea of ID. They are 'proximate IDists'." Again, I doubt that anyone is in a position to 
speak definitively for individual ID theorists but the theorists themselves. In any case, there 
is no reason to believe that empirical ID research programs need be incompatible with 
CTMU-level mathematical reasoning. Such programs require a supporting model of 
causality, and that's non-negotiable. 
Argon asserts that "As it stands now the CTMU does not differentiate between potential, 
proximate mechanisms for the emergence of say, factor VIII in the mammalian clotting 
system." If Argon is insinuating that the CTMU is incapable of differentiating between front-
loading and proximate loading (of design), this is flatly incorrect. On the other hand, when 
Argon asserts that "empirical and modeling methods potentially can [distinguish between 
these alternatives] with a reasonable degree of confidence," they certainly cannot do so 
without sound rational underpinnings. To my knowledge, Behe and Dembski have not ruled 
out further exploration of these underpinnings. In fact, I'm quite sure that they have not. 
Comment #6:
Good post. A few comments may be in order. If a new functional system were introduced 
that could deterministically "bridge IC gaps", it could still be interpreted as an instance of 
front-loaded ID without precluding ad hoc ID. Causal explanations for IC phenomena can be 
formulated on different levels; the most important is the most basic and general, by which all 
other levels are explained. Argon seems to acknowledge both of these facts when he writes 
that "intelligence could be acting at a root level...by setting the original conditions of the 
universe". And precisely because CTMU reasoning begins on the root level, its conceptual 
apparatus is capable not only of distinguishing among proximate mechanisms, but of 

providing them with a fundamental model of causality. Without such a model, what does or 
does not constitute a "causal explanation" is moot. 
 
Comment #7:
Argon comments that "although CTMU may be to provide a fundamental model of causality, 
the actual connections to specific biological phenomena may not be readily determinable or 
predictable from the root theory." Although I appreciate the use of "may", whereby Argon 
seems to acknowledge the speculative character of his statement, it pays to be careful 
when making statements of this kind...especially to the authors of theories to which they 
refer. The author of a theory or model can usually be expected to know somewhat more 
than a layman about the potential for specificity, if only due to his sheer familiarity with the 
material. In particular, while it may not be superficially apparent how a global model of 
causality might (computably) generate specific predictions, its ability to do cannot be ruled 
out on the basis of a general introduction. Because the CTMU is in a unique position with 
respect to the derivation of logical truths about the natural world, a little caution may be 
appropriate when it comes to setting limits on its implicative power. 
 
Comment #8:
Certainly, one can understand RBH's desire for specificity. Specificity is what allows those 
occupied with concrete reality to deal directly with specific concrete objects. However, too 
much specificity is not always a good thing, particularly with regard to universal abstractions 
(or if one likes, abstract universals). For one thing, specificity is inversely related to 
generality and thus to scope. So if one is seeking a general mapping, e.g. the kind of cause-
to-effect mapping that would be associated with a fundamental (and therefore general) 
model of causality, excessive specificity is something that one wants to avoid. Since (for 
reasons I've already given) providing context-specific theories of physical and biological 
evolution with a fundamental model of causality is an urgent necessity for those seeking 
answers for questions about physical and biological origins, we must reason on the most 
general possible level. But that's fine, because specifics are generatively deduced from 
generalities. And that's really the whole point: the only way to maximize our deductive 
capacity with respect to natural causation is to identify and work from the proper generalities 
within an appropriate deductive framework. Since this is the motivation behind the CTMU, 
attempting to marginalize the CTMU on grounds of insufficient initial specificity is like 
discounting the utility of horses because they are not initially preceded by carts...especially 
when CTMU generalities can themselves be rationally deduced. 
Now let me attempt to clarify something else. I'm not in a position to say what constitutes an 
"appropriate stance" in the mind of anyone but myself. I don't know what anyone else 
understands or doesn't understand of the CTMU, or what constitutes "adequate phrasing" 
as far as anyone else is concerned, or even what differentiates terra firma from "thinner 
ground" in anyone else's worldview. But as far as I'm concerned, I've explained enough of 
the CTMU, and enough of the overall relationship of the CTMU to natural phenomena 
(including biological phenomena), to reject any suggestion that I'm unilaterally responsible 
for anyone else's total ignorance regarding them. I don't claim to be a perfect expositor; I 
have only my personal conviction that the lightbulb should by now have started to come on 
on for anyone paying a reasonable amount of attention to what I say. But there are two 

things of which I'm quite certain: (1) the CTMU is not merely a "promissory note", and (2) 
somewhat like RBH, I don't have the time to worry overmuch about people who persistently 
fall short of my pedagogical expectations. Suffice it to say that I'm in the process of 
elaborating on the general framework I've already established, and answering certain 
questions arising from the answers I've already given. 
That being said, I'll reiterate: without a fundamental model of causality, Darwinism has 
precisely nothing to offer that couldn't be had for the price of a statistics calculator (and a 
mix of data and conjectures that do not depend in any way on Darwinism). Some would no 
doubt respond that the same is true of ID, and it's no secret where they'd be coming from. 
But then again, at least there's someone on the ID side of the fence coherently addressing 
the issue. If someone would like to offer another model of causality than my own, then by all 
means, feel free. But bear in mind that if and when you do, it will be my turn to play 
Skeptical Inquisitor in The Case of the Counterfeit Promissory Notes. And bear in mind that 
if you can't, then you're not in as strong a position as you might like others to believe. 
 
CTMU and the Axiom of Choice
Comment #1:
Hello, Russell. 
You say that as a fan of the CTMU, you try to invent counterarguments. That’s fine. But as 
I’m sure you realize, it can be hard to formulate a coherent counterargument to an argument 
of which you have not yet formed an adequate understanding. For example, you write that 
“there is incredible conceptual difficulty with the UBT (Unbound Telesis) concept.” If this is 
true, then it is true only in the minds of people whose conceptual limitations prevent them 
from understanding it. 
Consider: in cybernetics and information theory, we speak of constraint as opposed to 
freedom. We quantify the constraints in “bits”, and the freedom in “degrees”. Immediately, 
this arithmetical quantification leads to the logical possibility of 0 constraint, i.e. 0 
information, and infinite (unbounded) freedom. It’s part of the price of forming a coherent 
mathematical model.
Constraint is not the only existential necessity routinely taken to zero. Consider energy. 
Because physical existence entails energy, there is no such thing as a zero-energy state in 
physical reality. Yet physicists sometimes talk about zero energy as though it has meaning, 
e.g. when some of them maintain that positive energy and negative energy sum to 0. If one 
maintains that this only works in an unreal idealization of physical reality (rather than in 
physical reality itself), then one is asserting in effect that these physicists are working from 
an unreal model, are not modeling reality, and are not doing physics. Although I could be 
mistaken, I have a hunch that this isn’t really what you mean to imply.
Now to clear up a couple of other loose ends in your posts. You write that “zero extension 
and zero duration *IS* constraint, big time constraint.” Well, yes and no. When (and if) I’ve 
used the phrase “zero extension and duration”, it has been in simplified reference to 

undefined extension and duration. The reference is as plain as the nose on your face; since 
“undefined” means “unmeasurable”, and since any meaningful definition of extension and 
duration implies at least some degree of measurability, nondefinition precludes extension 
and duration with respect to the undefined domain. (I’ve explained this to you before, and I 
just did so again. Please try to remember that I'm usually too busy to enjoy repeating 
myself.)
You also make reference to the axiom of choice, a postulate of standard set theory. As I’ve 
explained on multiple occasions, the CTMU utilizes another approach to set theory, and in 
this new (SCSPL) approach, the axiom of choice doesn’t necessarily mean what it used to 
mean (neither, for that matter, do terms like “set” and “inclusion”). Thus, when you wax 
philosophical about the Banach-Tarski paradox, your speculations have only limited 
relevance to the CTMU (and likewise for the relationship between the CTMU and the 
Lowenheim-Skolem theorem).
By the way, there’s another moral to this story: mathematics is philosophy. Anyone who has 
ever cracked a book about modern analytic-linguistic philosophy knows that it consists 
largely of mathematics of the most unforgiving sort...the kind of math that one can’t really 
get a handle on simply by browsing the Internet. Please try to keep this in mind the next 
time you want to belabor the supposed distinction between mathematical and philosophical 
proof. 
Thanks for your consideration.
Comment #2:
OK. I'm going to try very hard to display the proper spirit of intellectual charity here, and 
explain exactly how something that is undefined might as well be zero.
First, Parallel (with the apparent support of Pim) says that "Because zero is a defined 
quantity it should not be obvious to Mr. Rierson or anyone that zero extension and duration 
is a simplified reference to undefined extension and duration."
I find it hard to understand how Parallel can speak definitively about what anyone in 
particular, other than himself, finds obvious. As it happens, Mr. Rierson and I have already 
had extensive discussions on this particular point, and I think I'm within my rights to ask that 
I not be forced to repeat myself in other forums. In any case, the scarcity of my time forces 
me to insist on it.
Parallel goes on to say that "my argument based on unmeasurability is also erroneous." But 
it isn't, at least not in the context at hand (the CTMU). In scientific parlance, to say that the 
spatial or temporal distance between objects or events is unmeasurable due to lack of 
syntactic support amounts to saying that it "might as well" be zero. That is, the objects or 
events are either identical or coincident for all practical purposes. If one denies this, then 
one is obliged to demonstrate that they are in fact separated. Obviously, this would be hard 
to do without some kind of measurement or formal demonstration of necessity.
To attribute extension and duration to some part of nature, even approximately or in some 

infinitesimal or infinitary sense, one needs to be able to observe it or consistently define it 
within some formal system to which nature demonstrably conforms. But as any quantum 
theorist understands, observation is measurement. So to attribute extension or duration to 
some part of nature, one must have some kind of measurement, at least in principle, and if 
one does not, then one has no business ascribing nonzero values to spatial and temporal 
distinctions associated with extension and duration. 
Unfortunately, where something is undefined due to its lack of expressive syntax, it is totally 
unmeasurable even in principle. Therefore, there are no extensional or durational 
distinctions to be made, and this means that for practical and theoretical purposes, 
extension and duration are zero. (As Russell and some others are aware, I generally use 
terms like "virtual zero extension" and "zero infocognitive radius" instead of just plain "zero" 
to make this point; it's a subtle distinction, but it's been there all along.) 
As for Parallel's example - "n/0 is undefined and immeasurable, yet it does not therefore 
follow that it is zero" - it doesn't really fit the context at hand. However, it is relevant to the 
extent that a quotient of the form "n/0"might as well be zero for all practical purposes. That 
is, quotients of the form "n/0" have no real, identifiable formal or physical existence in 
empirical science or (standard) arithmetic. This, however, does not preclude a systemic 
extension in which such quotients are allowed.
There's an interesting aside here: intuitively, (nonzero n)/0 "diverges to infinity". To some 
extent, this mirrors the relationship between constraint and freedom, whereby taking 
constraint to 0 takes freedom to infinity.
As far as concerns 0 being a defined quantity, of course it's defined...in the syntax of 
arithmetic. Without an underlying conceptual syntax, it would be undefined...and the 
absence of syntax is what UBT is all about.
Comment #3:
First, Pim, it's rude to repeatedly misspell the names of those you address. Please stop it.
Second, your post says very clearly that you support the content of Parallel's post. So I 
guess the answer to your question might be, "I can tell what you support because I'm 
somebody who can read."
Third, please refrain from giving me personal advice. I don't think it's your place to do so, 
and in any case, I have no use for it. Thanks for your consideration.
Fourth, I don't particularly agree with your suggestion that we "move the discussion forward" 
on terms unfavorable to the author of the theory which forms the subject matter of this 
thread. In fact, I find it more than a little disingenuous. 
Evolutionary Intelligence
Comment #1:

There is much theoretical controversy regarding the nature of intelligence, largely because 
prevailing scientific models – having been designed to preclude subjective contamination in 
the gathering and evaluation of objective data - support the existence of neither intelligence 
nor any other subjective attribute. Thus, it has often been asked whether intelligence (as in 
"Intelligent Design") is really a measurable, empirically verifiable quantity.
The science of psychology, as practiced in laboratories, hospitals, clinics and educational 
institutions around the world, answers in the affirmative, and offers a very great deal of 
supporting data in the form of IQ test scores. IQ tests consist of problems to be solved, 
usually within time constraints. Correct solutions possess utility in the form of higher test 
scores and all utile correlates thereof. Despite questions regarding the validity of IQ testing 
in this era of political correctness, these measures and the data they provide exhibit a very 
high degree of statistical integrity.
Survival is a problem for every organism and species, and biological fitness is directly 
analogous to utility. That is, survival has positive utility for that which survives (while 
optimality is unnecessary for survival and may in any case be undecidable, survival itself is 
objective and observable). To this extent, "survival of the fittest" (and all of its more refined 
variants) refers to the solution of a problem, namely to solving for fitness as a condition of 
survival, and evolution is thus analogous to a problem-solving procedure.
This analogy is better than one might think. When it comes to evaluating aggregate data, 
the lab-coated, by-the-book scientists trained in clinical and experimental psychology 
typically do not concern themselves with the subjective nature of what their instruments 
measure; when dealing with populations, they take what a logician or systems theorist 
would call a "black box" approach to intelligence, concerning themselves solely with the 
numbers thereby generated. In effect, intelligence is operationally defined as that property 
of test subjects which best correlates with IQ test performance.
Although psychologists are encouraged to take account of subjective factors, they are 
limited by the physicalist models dominant throughout the sciences, and ultimately lack the 
means to make a clear subjective-objective distinction. In this respect, they resemble 
evolutionary biologists, from whom they differ only in their tacit acknowledgment of the 
subjective dimension of human reasoning and emotions (which each of them can in any 
case "observe internally"). As far as concerns their (highly replicable) data, there is no such 
distinction. 
If we take a black-box approach to evolution, this experimental paradigm can be applied to 
evolutionary biology up to isomorphism. That is, the environment presents biological entities 
with problems in the form of stressors, and nature responds by modifying the entities [in] 
ways that solve these problems:
Survival problem --> time-constrained solution process --> fitness solution --> observation 
and statistical evaluation 
So the question is, why can we not say that nature possesses something analogous to an 
"IQ", and given its admirable success rate in solving for occupation and survival in various 

ecological niches, that it is "objectively intelligent" in the same way that a high-scoring test 
subject is objectively intelligent?
Let’s take a brief look at the kinds of objection that a critic would be forced to make in order 
to discount this experimental paradigm, which is really just the statistical paradigm at work 
in a certain objectivized scientific context. 
For example, a critic might say: "But we can see human beings filling out the tests, and we 
know them to be intelligent!" However, the intelligence of test subjects is precisely what is to 
be determined by testing them; as far as the statistics are concerned, they might as well be 
AI machines. 
A critic might also say: "A logical extension of this viewpoint leads directly to the realm of 
the inanimate, and would thus lead to the absurd conclusion that inert physical structures 
and processes display intelligence!" But again, from a scientific perspective, whether or not 
nature is "intelligent" is the point to be determined. In any case, no reference to any 
subjective-objective distinction is possible without a detailed model of the distinction.
ID critics like to point out that ID lacks a model for intelligence. But so does the scientific 
mainstream, and yet this does not stop scientists from observing and measuring what they 
call "intelligence" by taking a black-box approach to it. Less formally, ask any scientist 
whether he or she possesses some degree of intelligence, and one will almost certainly 
receive a "yes" for one’s trouble; ask that same scientist whether he or she possesses or 
even knows of a detailed model of intelligence, and one will get a different answer entirely.
Is this a scientific argument to the effect that nature is intelligent? Or should we simply put 
an end to terms like intelligence and fields like clinical psychology?
Comment #2:
An effective procedure is one which admits of mechanical implementation with respect to an 
abstract machine model, e.g. a Turing machine. Complexity and utility are irrelevant; the 
issue is whether there exists a series of discrete steps, defined on the model in question, 
leading from some input to some output. In particular, the utility of the programmer is strictly 
a side issue. Solving problems on an IQ test goes beyond the realm of the merely effective 
because there is a value attached to test performance, and so for surviving in the wild. 
Indeed, a real “problem” always relates in some way to the utility of that by which it is 
recognized as a problem. If it is in no way a matter of utility – if nothing benefits in any way 
from its solution - then when you come right down to it, it’s really no problem at all.
On the other hand, any problem that deserves to be called a “problem” is related to its 
solution by a more or less complex transformation of its initial (unsolved) state into its final 
(solved) state, and this transformation is associated with a net gain of utility. This means 
that the result of the transformation, i.e. the solution of the problem, qualifies as a "goal" for 
the entity with respect to which this utility is defined, and utility specifies and drives the 
transformation. Survival fits this description because it possesses utility for those entities 
which achieve it, and achieving it often requires the complex and tightly-constrained 
adaptive transformation of an initial state consisting of a stressful environment-organism 

relationship.
The number and distribution of possible transformations of such a relationship, i.e. the 
contingency, determines the discriminativity of the problem, i.e. the extent to which it can be 
used to measure problem-solving ability or “intelligence”. So where utility corresponds to 
specification and complexity is defined as usual, the psychology of cognitive testing involves 
a sort of “specified complexity”, and because it is in certain ways analogous to biological 
evolution, we have a logical case for admitting the concept of intelligence, and to some 
extent the methodology of intelligence testing, to evolutionary biology. 
Integrity means "soundness", which naturally implies validity and reliability. So "statistical 
integrity" refers to the soundness of statistical methodology, including experimental design, 
data gathering and analysis, and statistical inference as applied in clinical and experimental 
psychology. Thus, when statistically significant relationships are detected by means of 
intelligence testing, they usually stand up to scrutiny with respect to methodology and 
prediction.
As for IQ being a relative measure, this applies to comparisons rather than to tests and test 
items. Whether or not a problem has been solved to a given level of accuracy within 
operative time constraints is an absolute (yes or no) determination, and likewise for one’s 
raw score on a test. One could simply answer questions at random over many trials and 
take the average result as a baseline score, working from the space of all possible 
attempted solutions within the given constraints. The result would be a determination of 
absolute rather than relative problem-solving ability (at least with respect to the test in 
question). 
The point, of course, is that although one can split hairs about where to set the points of 
reference and define the bounds of relevance, it would be difficult if not impossible to make 
a scientific case that the psychometric paradigm and its associated concepts have no place 
in evolutionary biology. In principle, intelligence and mental causation have as much 
bearing here as they have in psychology; the kind of biological data required, how to gather 
it, and how much of it to gather are secondary questions. Regardless of any difficulties 
attending the answers to these questions, there are no scientific grounds for excluding 
intelligence as a causal factor in evolution given the potential for an operational definition 
analogous to that employed in psychology.
Karl D. Stephen: Tegmark’s Parallel Universes: A Challenge to 
Intelligent Design?
Comment #1:
Max's arguments represent no threat whatsoever to ID theory, and I think Max would readily 
admit it. The reason is simple: ID theory is about biological origins, any scientific 
explanation of which requires an underlying model of causality conspicuously absent from 
Max's work. Although the MW (Many Worlds) interpretation of quantum mechanics purports 
to "explain" micro-causality by exhaustive (and therefore deterministic) decoherence of the 
wave function, it ultimately does nothing of the kind. 

Because the wave functions of material particles are embedded in that of the universe as a 
whole, one cannot explain quantum causality in physics without explaining the cosmogonic 
limit of causality, the ultimate origins problem which centers on the origin of the universe 
itself. But MW offers no advantages in this regard, for it explains the origin of the multiverse 
no better than standard cosmology explains that of the physical universe (i.e., not at all). At 
best, it merely pushes the cosmogony problem back a step and leaves it hanging just like 
before.
The thing that I find interesting is this. This is Max's second article in Scientific American; his 
first, about quantum mechanics, was coauthored with a man recognized by many as the 
world's foremost living physicist, John Wheeler. Wheeler was once the mentor of Hugh 
Everett, the inventor of MW, and was largely responsible for the somewhat bizarre progress 
that MW has made in the scientific community. (He was also Richard Feynman's mentor, 
and was likewise responsible for much of the progress made by the more respectable path 
integral formalism.) However, Wheeler himself is no longer a supporter of MW. He 
eventually repudiated MW due to its "excess metaphysical baggage", and has made it very 
clear in a number of papers that he prefers an approach to cosmogony and cosmology in 
which the universe is regarded as self-contained. He has even been wont to mention the 
forbidden G-word ("God"), distantly implying that a self-contained explanation would 
possess a theological dimension, and therefore a teleological dimension, and therefore an 
ID-friendly dimension.
Max, however, is evidently blazing his own trail, and it doesn't look like God has a whole lot 
to do with it. For Max, the upside is that the academic community and its friends in Big 
Publishing, including SciAm, will approve of his seeming respect for its atheistic 
sensibilities. The downside is that God won't be around to help Max explain the multiverse. 
Which just goes to show you, you can't have your cake and eat it too, and you can't explain 
even a single universe by positing the existence of many. 
Comment #2:
According to the standard model, the limit of nature is a cosmic singularity, and the limit of 
causality is whatever generated and sustains this singularity (standard cosmology merely 
purports to explain how this singularity, once its existence and underlying ontology were 
already given, blew up into the universe we see, beginning at 10^-43 seconds after the 
initial big bang). Many Worlds theory purports to account for these things by placing the 
universe - that is, the thing to be explained - inside a multiverse, and then fails to explain the 
multiverse. Yet, in order to explain a process that runs as part of an underlying process, the 
way local physical evolution (causality) runs as a part of global cosmic evolution, one must 
explain the underlying process right back to the beginning. 
Regarding your usage of terms like "associative ad hominem" and "straw man", I'm sorry if 
you take personal offense. If so, I excuse you from asking me any further questions. People 
are entitled to their opinions regarding social and commercial institutions; those who insist 
on identifying personally with the institutions in question have only themselves to blame if 
generalized critiques offend them.
Comment #3:

“At the end of the day, all science is about is describing and predicting the observations we 
make. Words like "cause", "mechanism" and "explanation", when used in science, just refer 
to descriptions of (some part of) the universe in terms of stuff that we consider familiar. 
From a scientific point of view, all that matters is whether an infinite space-time/multiverse/
MWI enables us to better describe and predict observations.”
"What science is about" is in the present context a matter of controversy, but it goes a little 
farther than merely describing and predicting observations. Or rather, there is more to 
describing and predicting observations than meets the casual eye. Technically, we might 
state this as follows: perceptual reality is not perceptually connected. Instead, it is 
connected to itself by virtue of its placement in the richer network of cognitive reality, in 
which reside such concepts as causality, mechanism and explanation. 
In theoretically (cognitively) connecting perceptual reality in an explanatory causal network, 
one can't always progress by short obvious steps; sometimes one must plunge into an 
ocean of non-testability in order to come up with a superior testable description on the far 
shore (think of this kind of insight as analogous to irreducible complexity, but often followed 
by a simplificative "refolding stage"). In other words, it is not always easy to distinguish 
(empirically fruitful) science from nonscience as science progresses; one must rely on logic 
and mathematics in the "blind spots" between islands of perceptibility.
Logic is clarificative. Consider the words "description" and "prediction". They denote a 
model-theoretic isomorphism between a linguistic construct (the description or prediction) 
and its "objective" content (a set of observations to be described or predicted, where 
observation is understood to have an unavoidable subjective component). Ideally, this 
description displays maximum compression; it describes present and future data as 
compactly as possible while sacrificing nothing in depth or scope.
But the real point is that science consists not just of the description or theory, but the theory-
observation correspondence...in logical terms, the model. Strictly speaking, one can't even 
take the existence of an objective, perceptually independent universe for granted, although 
we often do so on faith; our perceptions are all that we can directly know of that which we 
perceive. Further insight regarding the content of our perceptions requires some blend of 
inference, reportage and faith, and thus comes at second hand relative to the observations 
themselves.
The central object of science is thus a correspondence (similarity mapping, isomorphism) 
between cognitive-linguistic structures and the observations to which they refer, and by 
extension to the putatively material contents of these observations. Because this mapping 
must simultaneously accommodate both of the entities it relates, science is at once 
cognitive and perceptual in essence. So as far as science is concerned, the medium of 
reality is a blend of cognition and perception, or by extension, of mind and matter, the 
abstract and the concrete, the mental and the material. It flows smoothly between them 
rather than switching suddenly from one to the other across a gap; any such discontinuity 
would interdict the mapping at all levels, rendering science impossible even in principle.
In an ensemble theory, the universe studied by science is explained as one member of an 

ensemble of universes existing in a higher space. There is a sense in which the original MW 
theory is not an ensemble theory: all histories are assumed to exist within one and the same 
spacetime manifold. That is, Hugh Everett stopped short of postulating the kind of weird and 
wacky hyperspace now being held forth by MW-happy ensemble theorists. Why? There are 
technical reasons, but perhaps the main reason is that Everett, like his mentor John 
Wheeler, felt that explanations applying to the real world should reside within the real world 
itself. That way, they could be considered real, and therefore valid. 
Metaphysical ensemble theories like Max's apply a certain transformation that cannot be 
scientifically justified: they transform an explanatory or descriptive regress into a topological 
regress. Syntactic explanatory structures are converted into actual spaces of (potentially) 
infinite extent and giving way to an infinite diverging sequence of ever-higher spaces. 
Unfortunately, these spaces are not measurable or even metrizable within perceptual 
reality, and are therefore not scientific in the empirical sense. As cognitive-linguistic 
extrapolations from perceptual reality, they are recognizable as "scientific" only where 
reality is acknowledged as cognitive and linguistic, and where science is allowed to use 
purely rational methods (i.e. logic and mathematics) to study them. But what does this kind 
of science look like? How should cognitive-linguistic explanatory extrapolations be handled?
As it turns out, the model designed to handle such extrapolations accommodates ensemble 
theories without the unscientific transformation in question. In this model, they become 
syntactic potentials existing internally rather than externally to reality. The model in 
question, the CTMU (Cognitive-Theoretic Model of the Universe), models reality as an 
SCSPL (Self-Configuring Self-Processing Language). According to the structure of this 
model, any useful description or prediction that can be extracted from an ensemble theory 
can be developed within reality itself. As I've already pointed out, this is more consistent 
with the intentions of Wheeler and even of Everett than ordinary ensemble theory. And 
better still, it is tautological in a particular (model-theoretic) sense designed to embrace the 
empirical sciences.
The model in question was described in PCID a few months ago. Whereas Max puts a 
particular counterteleological spin on MW - his basic idea is that the universe is what it is 
because it is a kind of "ensemble average" of mathematically consistent possibilities, which 
is to some extent what it is in the CTMU as well - the CTMU recognizes this averaging 
process as teleological in nature and makes it responsive to the Telic Principle, a self-
contained generalization of the Anthropic Principle(s). The CTMU has been around as long 
or longer than Tegmarkian ensemble theory, and while it addresses virtually all of the same 
problems, it is 100% ID-consistent. For what it's worth, one can derive from it an enhanced 
version of quantum mechanics (more than Standish has apparently derived from a partial 
but related set of assumptions).
Regarding Pim's comments, Max only gives possible means to falsify his theory relative to a 
given set of assumptions which need not hold. The same is true of Ian Stewart's comment 
regarding what is or is not open to design. The CTMU, within which ensemble theory can 
(and must) be packed, is expressly “fractal” to the extent that self-similarity is a basic design 
feature. It follows that infinite divergence is not necessary for self-similarity. And since God 
can map to MW only as a unity (by a one-to-many mapping associated with the multiverse), 
Max presents no real obstacle to "identifying reliable Intelligent Design".

Jacob writes that he would be “interested in finding out from CML why he believes 
something sustains the cosmic singularity and what he thinks would be the consequence of 
that lack of sustenance to the cosmic singularity”. That which exists needs a medium of 
existence, and if it is singular, then it cannot provide itself with a medium (since doing so 
requires a distinction between medium and content which is precluded by singularity). 
Therefore, if the cosmic origin were not sustained by an existential medium, the universe 
would not exist...unless, that is, it somehow manages to sustain its own existence. But this 
leads directly to the CTMU (and not to ensemble theory).
As for why I’m "concerned about origins"...well, evolution is about origins. The Origin of 
Species. The origin of the cosmos, of matter, and of the laws of physics. The origins of 
events, AKA “causality”. Biological evolution is sustained by the underlying process of 
physical evolution, which is sustained in turn by cosmic evolution. So I think that we all need 
to be concerned about origins, and this includes the origin of any “multiverse” posited to 
explain why this universe exists as we find it.
Don't get me wrong - I like some of Max's insights. But it can be shown in minute logical 
detail why Max's infinitely-regressing model fails to qualify as a metaphysical framework. 
Although there are certain seldom-recognized logical demands on the title “metaphysical 
framework”, Max is not even attempting to meet them. On the other hand, ensemble theory 
implicitly claims to be more than merely physical, and it exhibits serious deficiencies with 
respect to this claim.
Comment #4:
“The level II parallel universe is not really a separate universe because it interacts with ours” 
means that in order to explain cosmic inflation, some cosmologists posit the existence of an 
underlying scalar energy field associated with an all-inclusive background space in which 
“bubble universes” like ours are supposed to inflate. The dependency of cosmic inflation on 
this background field contradicts any supposition of two-way independence.
In the present context, reality can be considered synonymous with “nature”, including 
abstract but non-imaginary features of nature, while perceptual reality is that part of reality 
directly accessible to perception. (Anyone who finds my usages of these terms ambiguous 
was already in trouble, because other authors seldom define them at all.) Perceptual reality 
is connected to itself by (e.g.) geometry and causality, which – as Hume, Kant et al have 
already pointed out – are not tangible physical quantities. Neither, for that matter, are space 
and time. 
Science always entails theory-observation correspondence up to cognitive-perceptual 
consistency. Beyond this point, science need not entail such a correspondence at every 
step, since the logicomathematical criteria for truth are more general and fundamental than 
empirical criteria and in this sense have priority. We know this because empirical truths 
must be logicomathematically consistent at every step, whereas logicomathematical truths 
need not be empirically instantiated at every step. Science must sometimes follow inobvious 
logicomathematical routes to get from one level of “empirical explanation” to another. 

You say that it is "very valid to posit parallel universes" even when they can’t be detected in 
the real universe that we actually observe. If so, then their validity is not empirical but 
logicomathematical in the above sense, and only if they turn out to be consistent according 
to the laws of logic and mathematics. This is a matter of supposition regarding the individual 
elements of Tegmarkian ensemble theory, the simplistic overall structure of which expressly 
lacks mathematical closure and cannot be regarded as consistent in a comprehensive 
sense. A higher level of reasoning indicates that there is more going on in the multiverse, 
logically and mathematically speaking, than ensemble theories can accommodate, 
particularly as regards the existence and dynamics of self-aware structures.
By the way, if there exists a singularity in which no laws of science hold, then it isn’t properly 
a subject for scientific investigation; it’s merely a terminal extrapolation of science. And 
regarding the "newness" of Max's claims, a number of cosmologists have gone beyond the 
standard cosmic singularity concept, including Stephen Hawking (and for that matter, me). 
So if such a claim is new, Max is a newcomer among the innovators. As I’ve said, the 
entirety of ensemble theory exists implicitly in the CTMU (along with a considerable amount 
of high-level structure that ensemble theory lacks). Since the CTMU is ID-consistent, 
ensemble theory is not a threat to ID. 
Comment #5:
“While the CTMU may be rumored to contain Tegmark's ideas and as I understand many 
other yet to be formulated ideas, I think that the real power of Tegmark's ideas lie in the fact 
that he has made the ideas accessible to laymen and scientists alike, that he has provided 
with supporting evidence as well as ways to in principle at least falsify his ideas. Which may 
explain why his ideas can be found in so many popular and scientific journals.”
Actually, Pim, many CTMU ideas have been pretty exactly formulated. I’m not saying my 
presentation has been flawless – I’m even now in the process of improving it – but it’s 
considerably better than you seem to allow. The problem, I think, has to do less with 
formulation than with conceptual difficulty. As I’ve already observed, Tegmarkian ensemble 
theory is extremely simplistic; one has a potentially infinite topological regress of spaces 
containing all possible (mathematically consistent) subspaces. Rather than simply say “big 
deal” and point out that this deceptive simplicity, along with Max’s stint at the Institute for 
Advanced Study and his well-known connection with J.A. Wheeler, may have even more to 
do with the popularity of his material than its alleged scientific validity, I’ll leave it at that. 
After all, Max may not yet have had the opportunity to defend his ideas against the 
implications of my own. 
You say that "the question of Tegmark's ideas being a threat or challenge to ID seems ill-
posed since I would argue that no scientific idea can in principle be a challenge to ID but it 
can be a challenge to hypotheses of ID or methods to infer ID as proposed by Dembski", 
and go on to opine that if the CMTU implicitly contains Tegmark's ideas, "then it is as much 
a 'threat' to ID as Tegmark's ideas." In my view, this is very much mistaken. Superficially, 
Max’s work seems to justify the attribution of any hypothetical instance of SCI to a 
deterministic averaging process over an exhaustive ensemble, and at the logical level, the 
SCI concept is a pretty straightforward consequence of ID. Trying to divorce the basic 
specified complexity concept from ID ultimately has the effect of making ID scientifically 

irrelevant. However, if you allow for the possibility that ID is scientifically valid as you may or 
may not understand it, then one would need a whole new approach to the truth concept to 
support the notion of a true but irrelevant hypothesis regarding origins.
You’ve made it clear that you like the frontier-town atmosphere created by the idea of 
parallel universes running amok in hyperspace. OK, I can respect that. But when carried to 
its ultimate conclusion, this concept leads unavoidably to the CTMU. Why? Because the 
multiverse is a logical unity, and its scientific investigation requires a model-theoretic 
approach centered on a distributed logical syntax. There really isn’t any escape; any 
rejection of this premise immediately robs the scientific enterprise of all validity with respect 
to the multiverse hypothesis, and does so in such a way that no mere prediction can 
redeem it (predictions require logically consistent supporting models). I’m doing what I can 
to try to put CTMU ideas out there in what I consider to be a simple and direct way - Max 
Tegmark gets immeasurably more support regarding his ideas than I do regarding mine - 
and would merely submit that those who find them opaque or trivial more carefully consider 
whether or not they themselves may play some small part in any supposed failure to 
communicate. 
Saying that “a theory which seems to 'implicitly' include all this and more but explicitly 
seems to fail to propose such scientific hypotheses may in the end be of little interest to 
science” is a bit shaky, since one can say precisely the same of the logic and mathematics 
on which science critically depends. Moreover, as I’ve already pointed out, my idea of 
science is a little less restrictive than the conventional view, which seems to be based on a 
rather tightly constrained version of model theory in which one side of what should be a 
symmetric correspondence – i.e., the observable, material side as opposed to the logical 
and conceptual side - is (in violation of logic) given vastly more weight than the other. And 
by the way, nobody said that Tegmark proposes a singularity in which no laws of science 
hold (although he doesn't seem to argue against it). That remark was addressed to the 
statements of the person to whom I was responding.
Here's a little hint as to what's going on here. It's from page 44 of the CTMU paper 
published in PCID:
“SCSPL incorporates the concepts of syntactic stratification and syntactic distribution. For 
example, because the laws of mathematics everywhere apply with respect to the laws of 
physics, the former distribute over the latter in the syntactic sense. Thus, where the laws of 
mathematics and physics are denoted by S1=LMS and S2 respectively, S1 distributes over 
S2, i.e. forms a syntactic covering for S2. Essentially, this means that the laws of 
mathematics are everywhere a required syntactic component of the language of physics.”
Tegmark's "level 4 multiverse" is just S1, the logicomathematical component of human 
cognitive-perceptual syntax. S2 merely consists of our own laws of physics, including the 
anthropic "fine-tuning" of universal constants. Max's thesis thus comes down to the idea that 
various S2 analogues can be formulated in S1, an idea already implicit in the fact that S1 
distributes over S2 as a more general component of TOE (SCSPL) syntax, and his 
terminology simply replaces S1 with "the (level 4) multiverse".
I need merely point out that whereas this is where Max's model terminates, the CTMU goes 

on to develop the idea in such a way as to deal with the problem of origins. That is, where 
Max's model ends, the CTMU proceeds to the comprehensive level of teleology and the 
logical foundations of ID.
Anyway, to direct the focus back toward the topic at hand, I’ll say it again – I agree that 
some of Max’s ideas are intriguing. But they do indeed fit into the CTMU framework in an 
obvious way, and the CTMU is ID-consistent. So, then, is the ultimate logical extension of 
Max’s ideas.
Comment #6:
“Does this mean that you disagree with the reviewer of Tegmark's work that suggests that 
Tegmark's multiple universes poses a challenge to ID?”
The suggestion that I must agree with Max simply because Max’s cosmological scenario 
can be interpreted within the CTMU seems a little hasty. In particular, I’d have to differ 
strongly with any assertion that Tegmarkian multiverse theory is complete as it stands. 
Superficially, Tegmark’s work can be seen as posing a challenge to ID. I’ve even pointed 
out why; Max is trying to understand the universe as a product of deterministic exhaustion 
and summation rather than design. Faced with troublesome "anthropic" coincidences, Max 
wants to use a selection principle to extrapolate and concretely reify probabilistic resources 
in such measure as to explain the status quo merely by applying the laws of probability to 
first-order Markovian combinations of physical state and laws of physics. He justifies the 
multiverse not by observing it, but by recognizing it as a necessity of forming a certain 
preferred kind of explanation for the universe...an explanation consisting solely of probability 
theory and distributed laws of physics. The possibility of design is simply ignored (but not 
explicitly denied).
In other words, Max wants to put the explanandum before the explanans and retrodictively 
construct an explanation conforming to certain logically unnecessary and scientifically 
unjustified constraints. He then wants to turn the tables yet again and "retrodictively predict" 
the explanandum from this explanation. It’s a circular exercise from start to finish. But 
whereas the CTMU recognizes this kind of circularity as a basic feature of the scientific 
process and explores the implied mathematical structure (supertautological SCSPL), thus 
potentially avoiding the concrete reification of inobservable quantities, Max doesn’t bother 
with this stage of theorization. He has wisely left that stage of reasoning to a metaphysical 
logician like me (where analytical metaphysics properly includes physics).
You mention Bill Dembski’s 3-way distinction between determinacy, nondeterminacy 
(chance) and design. In the CTMU, this distinction comes down to the 3-way distinction 
between determinacy, nondeterminacy and self-determinacy, the last being associated with 
telic recursion and the others being secondarily defined with respect to it. Telic recursion is 
just another term for "metacausation"; instead of simply outputting the next state of a 
system, it outputs higher-order relationships between state and law (or state and syntax). 
Regarding the distinction between origins and evolution, not too many people are clear on 
it. This distinction is based on the standard view of causality, in which there seems to be a 

clean distinction between the origin and application of causal principles, specifically first-
order Markovian laws of nature. In the CTMU, origins distribute over causes in a new kind of 
structure called a conspansive manifold, and are therefore not cleanly distinguishable from 
causality. Both are products of a higher-order process, telic recursion. To put it in simpler 
terms, evolution consists of events which originate in causes which originate in (teleological) 
metacauses. So in the CTMU, to talk about evolution is to talk about metacausal origins by 
ontogenic transitivity.
As for the question of whether or not I agree with Karl (Stephan) regarding the opposition of 
multiverse theory to ID theory, the answer would be yes and no. As I’ve already said, 
Tegmark’s work can be seen as posing a challenge to ID in its reliance on pure 
mathematical determinism and concomitant exclusion of teleology. However, Karl seems 
not to have read my paper, which provides a framework for disposing of this challenge. In 
fact, this challenge has already been met on a very advanced level right here in PCID.
Comment #7:
“Allegedly supporting that bold claim Mr. Langan cites a PCID paper of his published last 
year. However, Dr. Tegmark's work has been published years prior to Langan's paper. So I 
question Langan's assertion that Tegmark is the newcomer here. Additionally, I fail to see 
how the passage Langan quotes contains resemblance to Tegmark's work.”
...except that the first paper on the CTMU was published in 1989, in the journal of an 
exclusive group to which I belonged. Entitled "The Resolution of Newcomb's Paradox", it 
utilized a (then brand new) computational model of the universe based on nested virtual 
realities. The model was called NeST, short for Nested Simulation Tableau. Subsequently, 
other papers on the CTMU were published in that journal and elsewhere, some developing 
its cosmological implications. This can all be documented.
If what you mean is that priority automatically belongs to the first person who gets a given 
idea into Scientific American (or some other big-name periodical), I'd have to take exception 
to that, particularly since I had a brief correspondence with a SciAm editor named George 
Musser several years ago. I briefly described the theory and asked him whether he’d be 
interested in publishing an account of it. He wasn’t. But I can document the 
correspondence, so when it comes right down to it, his taste in cosmology articles and 
authors wouldn’t count for much in a priority dispute.
As far as concerns your inability to see how Tegmark’s work fits into the CTMU despite the 
fact that I just explained how it fits into the CTMU, I don’t think I need bear any responsibility 
for that. The paper is out there, and Max’s "level 4 multiverse" fits into it just as described. 
Of course, unlike multiverse theory, the CTMU manages to avoid any blanket physical 
reification of abstract structures. In fact, the mathematical structure of the CTMU goes so far 
beyond that attributed to the multiverse that there’s no need to pretend that conventional 
multiverse theory, if we can even call it that with a straight face, is in the same league.
By the way, I find your tone unnecessarily confrontational. Nobody is accusing Max of 
knowing anything about the CTMU before producing his work, and no one is accusing Max 
of plagiarism. I’ve even said that I like some of Max’s insights. What bothers me a little is 

the readiness with which some authors, perhaps including Max, assume that they’re 
completely in the know about what’s been going on in their fields of interest. Some people 
don’t run (or write) in the same circles, and often, this is because the circles in question are 
closed to them.
Last but not least, the issue here isn’t whether Max is entitled to credit for his work – of 
course he is! – but whether it represents a threat to ID. To explain why it doesn’t, I was 
forced to mention the CTMU, which just plain flat-out predates Max’s work (unless Max was 
writing on this subject back in the eighties). If anyone doesn’t like this, I can only 
sympathize. 
 
Comment #8:
The CTMU "includes" multiverse theory in the sense that multiverse theory can and must be 
interpreted in the CTMU. 
Remember, although it is tempting to think that Max is basically conducting physics and 
cosmology as usual (where "as usual" connotes materialism and determinism), he is 
actually working from more or less controversial premises explicitly including the following: 
(1) mathematical structures are formal systems; (2) at least one mathematical structure 
(formal system) is isomorphic to the physical universe; (3) mathematical and physical 
existence are basically the same.
Equating physical reality to a formal system leads inevitably to the conclusion that physical 
reality has formal, i.e. linguistic, structure. It is fundamentally linguistic in nature, and in 
consequence, physics and cosmology are directly answerable to formal (logical and 
language-theoretic) criteria. Concisely, reality is a kind of language.
SCSPL is simply the kind of language in question...the reality-structure implied by the 
hypothesis to which Max subscribes. Therefore, to whatever extent multiverse theory is 
valid, it belongs in the CTMU (I've already described exactly where). In fact, the CTMU 
constitutes a general answer to the very question that Max aims to solve: what is the 
mathematical structure to which the universe is isomorphic?
Since the full explanation is somewhat more technical than would be appropriate here, I'm 
working it into a paper-in-progress. 
Comment #9:
Parallel asks "if the CTMU contains Tegmarkian theory as Mr. Langan claims, then how 
could Tegmarkian theory contain what the CTMU lacks, such as the ability to be falsified?"
This question rests on the assumption that there is no theory that is both tautological and 
nontrivial in the a posteriori sense. To those who don't happen to share this assumption, the 
question is meaningless. On the other hand, were someone to insist that the question is 
meaningful anyway (and damn the torpedoes), then one would need to prove that this 
assumption is valid (and good luck). 

But there's an even worse problem connected with this question. First, multiverse theory 
can only be "falsified" relative to certain assumptions, the truth of which is not uniquely 
implied by the tests proposed. Meanwhile, because the CTMU is explicitly tied to such 
empirical details as the rate of apparent cosmic expansion, quantum nonlocality and so on, 
it has been logically and empirically validated. Multiverse theory, on the other hand, has no 
support at all. That is, even if multiverse theory supports the observation that "the universe 
is expanding at an accelerating rate", then one must go in search of a logical model, in the 
formal sense, of multiverse theory. Next stop: the CTMU. 
Parallel goes on to say that "the fist step in documenting something that's 'out there' is to 
provide the information necessary to facilitate documentation, such as the appropriate 
journal citations. How else can we as members of the public review the contents of your 
paper(s) in order to assess the veracity of your intellectual priority claim?"
The paper cited, The Resolution of Newcomb's Paradox, was published in Noesis: The 
Journal of the Noetic Society, number 44, December 1989-January 1990, pages 3-12. I 
don't know whether random members of the public can find older back issues of this journal, 
but I can, and if anyone with any real weight in such matters were to challenge me on this, 
I'd simply put myself to the trouble of producing them. And that would be that.
Jacob Aliet opines that the "CTMU is like a flute; it can be blown to any tune one desires". 
Well, not really, but I guess that one would need to read up on it a little, and understand 
what one has read, in order to figure out why. 
Jacob then asseverates that "any 'theory' that can explain causality without invoking an 
intelligent agent (especially when its resultant in something as elegant as our universe - 
from what Max calls "frogs eye view"), is a threat to ID." 
Unfortunately, quite apart from the question of intelligent agency, multiverse theory cannot 
"explain" anything in the strict logical sense, because although some people are in the habit 
of mistakenly calling it a "model" of reality, it does not meet the formal criteria applying to 
models. To really explain anything, it would need a model of its own, and as I've been 
saying, that could only be the CTMU (unless and until it runs afoul of the CTMU, is which 
case it is out the window on logical grounds alone).
Perhaps I should clarify something. In the 20th century, science-as-usual became involved 
an unseemly tussle with certain inconvenient facts of logic like undecidability, the 
Lowenheim-Skolem theorem and the Duhem-Quine thesis. In particular, the last of these 
says that any given set of empirical observations can be explained by multiple distinct 
theories. It follows that when we are inevitably confronted by such theories, we need to ask 
a question: which explanation is better substantiated on rational, logicomathematical 
grounds? In this context, the answer is clear: by the nature of its design and construction, 
the CTMU has a tremendous explanatory advantage over multiverse theory.
Now let's get back to multiverse theory and its "challenge" to ID. Although Max's recent 
SciAm paper is really just a survey - that's how Max sums it up in the synopsis - he makes 
some "predictions". Here they are:

Prediction 1: The mathematical structure describing our world is the most generic one that 
is consistent with our observations.
Prediction 2: Our future observations are the most generic ones that are consistent with our 
past observations.
Prediction 3: Our past observations are the most generic ones that are consistent with our 
existence.
I need merely point out that these aren't really what most scientists, e.g. biologists, would 
consider "predictions". Each involves the term "generic", which points to the fact that Max is 
talking about an ensemble average (where the cosmic ensemble to which he refers can 
only be indirectly observed by way of the average itself). But is multiverse theory the only 
way to explain such an averaging effect? Hardly; we can explain it as a syntactic function of 
SCSPL, and in fact must do so, for otherwise, those trying to function on this realm of 
discourse, including multiverse theorists, would lack a coherent language in which to 
formulate their hypotheses and inferences. Therefore, were these predictions to be found 
true, they would not confirm multiverse theory except insofar as it has a place in some 
larger theory providing multiverse theory with a supporting syntax, language and model. 
Hello, CTMU. 
I can't, of course, force anyone who is firmly attached to the idea of a physical, material 
multiverse extending to infinite distances in trans-physical space to change his mind. But I 
can observe that the notion of a multiverse that is at once physical and material, and yet 
trans-physical and trans-material, is plainly in need of reformulation ... the kind of 
reformulation that only the CTMU can provide. I can also observe that the CTMU not only 
supports the existence of an intelligent designer, but apodeictically implies it.
Since the particular brand of multiverse theory tendered by Max Tegmark logically implies 
the CTMU on formal (mathematical) grounds, prior to any question of empirical 
confirmation, and since the CTMU logically implies the existence of an intelligent designer, 
multiverse theory implies the existence of an intelligent designer.
And we really can't put it any more simply than that, can we?
Comment #10:
I think we may have a little misunderstanding regarding the manner in which the CTMU 
encompasses other theories. Since the CTMU is built on the TOE called "logic", let's use 
logic as an example. Logic is a required syntactic ingredient of any theory, which is just 
another way of saying that logic descriptively includes any correct theory. This doesn't 
mean that every theory is logical; it means that on the assumption of its correctness, it is 
expressible in terms of logic. Logic is thus more than an "unnecessary wrapper" for 
theories; theories fail without it. Theories thus have only three possible states: in verifiable 
conformance with logic; in tentative conformance with logic; in the garbage bin. The CTMU 
"includes" a theory like Tegmark's in exactly this sense. Like logic, the CTMU contains no 
assumptions; all of its principles, along with its explanatory power, are self-evident on the 

model-theoretic level of discourse (at least to one who is actually paying attention to it and 
knows the subject matter).
Some might argue that an infinite regress passes as a causal explanation of nature. But it 
doesn't, because an infinite regress never reaches the causal origin or "first cause" of 
nature, which is also the causal identity of nature. In contrast, the CTMU identifies nature as 
its own causal identity, recognizing it as its own cause, effect and causal agency. That's 
called "closure", and without it, one doesn't have a causal explanation (since a causal 
explanation is really what is being "enclosed" by the identity). This is deduced rather than 
"predicted". The CTMU does not allow ID to be falsified because ID turns out to be a 
condition of causal closure, which is in turn a condition of meaningful theorization on the 
ontological level.
Erik, perhaps bent on marketing multiverse theory rather than the CTMU, asks three 
questions. (1) "What are 'language theoretical-criteria'?" (2) "Exactly which logical and 
language-theoretical criteria do you have in mind?" (3) "Why is[n't] it a problem that 
Tegmark's hypothesis allows for the possibility that our universe is isomorphic to a 
mathematical structure that has higher cardinality than any language?"
1. Language-theoretic criteria are the definitive structural and dynamical features and 
constraints of languages and model-theoretic isomorphisms between languages and their 
universes, particularly in metaphysical contexts like multiverse theory.
2. Since the logical and language-theoretic criteria I have in mind were very clearly stated in 
the CTMU description that I published in PCID last year, I don't feel that I'm being rude to 
suggest, along with Jason, that those who want to know about them should go and look for 
them there. But just for the sake of civility, three of the most basic are consistency, 
comprehensiveness (true of a TOE by definition), and closure (equivalent to the analytic 
self-containment of reality with respect to relevance, which simply means that if something 
is sufficiently real to affect reality, then by definition, it is a part or aspect of reality itself). 
When such criteria are properly applied and developed in a model-theoretic context, the 
results are far from trivial. 
3. In conventional mathematical usage, there is no such thing as an isomorphism between 
sets or structures of different cardinality. If one likes, one can try pulling a trick or two 
related to the Skolem paradox and the Lowenheim-Skolem theorem, but frankly, that's more 
in line with the CTMU than with multiverse theory. So if Max "allows for this possibility" even 
while stating that reality is isomorphic to a formal system - and this seems to be a fairly 
accurate description of what Max is doing - then that would be a problem, all right. But it 
would be Max's problem no less than mine. 
By the way, I seem to be getting criticized for leading the thread away from Karl's paper and 
the impact of multiverse theory on ID. Since the CTMU is relevant to both of these topics, I 
don't know quite what that means; it's a bit like criticizing somebody for referring to 
arithmetic while trying to describe addition and multiplication. On the other hand, I have no 
problem with those who find something to like in the work of Max Tegmark. Multiverse 
theory is an interesting topic indeed, and I commend both Karl and Max for writing 
interesting and entertaining papers.

Comment #11:
Jacob says that "Maxes parallel universes is derived from empirical data." I don't know 
where Jacob got this idea, but it's dead wrong. Jacob also says that "all elephants inside a 
loaf of bread are pink". I believe he means to offer this as some kind of CTMU analogy...and 
again, that's wrong. 
Jacob says that "a statement can be logical but that in itself does not confer any veracity to 
the claims it makes." This is true on one level, but ceases to be true with respect to scientific 
applications of model theory (which involve semantic correspondences). The CTMU is 
expressly formulated at and above the model-theoretic level.
Jacob quotes me as follows: "The CTMU identifies itself with the structure of these 
operators and thus with the distributive syntax of its self-modeling SCSPL universe, 
including the reflexive grammar by which the universe refines itself from unbound telesis or 
UBT, a primordial realm of infocognitive potential free of informational constraint." From this, 
he infers that the CTMU "makes claims that are not self-evident". But that's not true, given 
that everything in the CTMU stems from necessary features of any possible (valid) TOE. 
Jacob goes on to say that "CML does not bother to offer any empirical evidence to back up 
his claims." That's not true either; take, for example, accelerating cosmic expansion. 
Nobody even comes close to explaining this in any non-CTMU way except by postulating 
strange, inexplicable forms of "dark energy" and so on, which of course merely begs the 
essential question(s).
Jacob asks "From the above quote alone - what is the evidence of the existence of the 
UBT?" The evidence is logical; it's one of the language-theoretic criteria we've been talking 
about. To put it as simply as possible, unbound telesis is simply the logical complement of 
constraint with respect to syntax-state relationships, and is a requisite of any attempt to 
meaningfully define or quantify constraints such as physical states and laws of nature. 
Jacob asks "from whence arose the UBT?" Since UBT is nil constraint, it doesn't need to 
have "arisen"; causes are necessary only in the presence of informational content (that's 
really the point). Jacob also asks "has the UBT been exhausted?" How can something that 
is unbound be exhausted, given that exhaustion is a function that would have to bind its 
argument? Jacob wonders "Why is the UBT free of informational constraint and how do we 
know this?" We know this by definition...specifically, by a definition logically required in 
order to form any self-contained description of nature and causality. Jacob goes on to opine 
that "the CTMU raises more questions than offer any explanations." If this is true, then it is 
true only within the minds of people who fail to understand it.
Jacob goes on to assert that the CTMU is of absolutely "no importance". Wrong again; the 
CTMU is that branch of logic in which formal theories, properly including any well-formed 
scientific theory, meet their universes of discourse, AKA nature. Jacob reiterates that the 
CTMU is "based entirel[y] on abstract and philosophical ideas" and is "very short on 
empirical data." Not really, given that the CTMU constitutes a necessary explanation of 
accelerating cosmic expansion, so-called "quantum nonlocality", and even the origin and 
evolution of the cosmos. Indeed, if one considers relativity theory and quantum mechanics 
to have been empirically confirmed, then the CTMU is confirmed right along with them, 

since it harbors basic explanations for some of the most essential and problematic structural 
elements of quantum mechanics and relativity. Jacob then elaborates on his 
misunderstanding of the CTMU by restating in so many words that it is of absolutely no 
importance...to which one can only respond, thanks for the psychological insight.
In addition, Jacob opines that "the [CTMU] neologisms are meant to hoodwink one into 
thinking they are reading/learning something significant. But when the chair stops rocking 
you are still sitting at the same place." Again, this is of clinical interest at best. But to drive 
home his "point", Jacob points out that "when one looks at Maxes article, there are no 
strange words. I only needed check up ergodicity. Thats another trivial difference." 
However, Max uses many neologisms including (but not necessarily limited to): multiverse, 
Level I multiverse, Level II multiverse, Level III multiverse, Level IV multiverse, bird 
perspective, frog perspective, ensemble theory (cosmological sense), chaotic inflation (not 
Max's neologism, but novel and misleading nonetheless), and so on. While I'd agree that 
one need not crack a standard dictionary to look for some of these, one is even more 
unlikely to find them in a standard dictionary if one chooses to do so. So if Jacob or anyone 
else really thinks that all of Max's neologisms are "based on whats known, now whats been 
modelled from a portentous imagination," then perhaps he needs to take a closer look at 
"multiverse theory". (Not that I deny Max and others the option to coin new words and 
phrases in developing their ideas; every word and phrase in every language on earth was 
originally coined by someone.)
By defending the CTMU from this benighted sort of criticism, am I deviating from the topic of 
this thread? No. First, I'm responding to the remarks of others in this thread. More 
importantly, the CTMU answers many of the questions that Max is trying to answer with 
multiverse theory, particularly as regards ID, and is ultimately implied by Max's own 
premises and "predictions", including his proposed (but measurement-challenged) 
theoretical extraction of the "generic" implications of conflating formal structures and 
physical reality. Again, if one wishes to discuss the operations of arithmetic on any but the 
lowest level of discourse, one had better be prepared to refer to "arithmetic" as the source 
of these operations...or at least prepared to let others refer to it in clarification.
A final suggestion: rather than undertaking these overblown, content-free criticisms of the 
CTMU, why don't Jacob and others simply admit that they haven't put themselves to the 
trouble of understanding the CTMU (which lack of effort is in any event as obvious as it can 
possibly be)? That way, should I choose to participate further in this thread, we can talk 
directly about the many weaknesses of multiverse theory.
Comment #12:
Erik, you conjecture that I "simply meant that the axioms of the mathematical structure, to 
which our universe is isomorphic, is a kind of language." Aside from the fact that this merely 
belabors the obvious, so far, so good. But then you go on: "In that case I have no further 
comments, since I neither think it is a remarkable nor an objectionable thing to say." SCSPL 
actually goes a bit farther than that, but since you show no special understanding of the 
implications of even the basic level of linguistic structure you're considering - and some of 
them are indeed remarkable - I find it hard to take your viewpoint very seriously.

You then confirm this judgment by admitting that you've "neither read the CTMU paper in 
the ISCID archive nor do (you) at present have any intention of doing so. It is too long, the 
frequency of funny philosophical terminology is too high, and (your) expectations of finding 
something of significant interest are too small for (you) to find it worth investing time on 
reading it."
Although I find this more than a little rude, I also find it unsurprising. So I'll settle for your 
admission that you have nary a clue about the CTMU or the (philosophical) level of 
discourse on which it and the problems it addresses are formulated, and leave it at that.
Comment #13:
Pim, you deplore my reluctance to take the viewpoints of certain people seriously, stating 
that my writings "may not be that accessible to the general and perhaps even much of the 
scientific population." Insofar as your point is constructive, it is well taken. But I'm afraid I 
see no percentage in squandering time on thankless attempts to educate people who show 
every sign of having closed minds and bad attitudes, especially when such people typically 
bring but little to the table of intellectual commerce. It simply wouldn't be a good investment 
of my time at this point. On the other hand, I think I've been more than generous in 
explaining my ideas to those who sincerely display interest. 
Regarding your own (failed) attempts to read and understand the CTMU, I wish I could help 
you with that. But when it comes right down to it, I've already more than established its 
superiority to Feynman's spoof example of a TOE, and I've already made it at least partially 
accessible to the general public (I know this because I run a list on which people regularly 
demonstrate their understanding of the theory). As for scientists who claim that they can 
understand nothing of it at all...well, that's something I've learned to take with a grain of salt. 
I'm a little nonplused when you call overblown, content-free criticisms of the CTMU "good 
questions" that "could benefit from a more detailed answer". I've been giving detailed 
answers to such questions right here in this thread, and I'm at a loss as to why you and 
certain others don't seem to understand word one. Thus, when you say that "the 
accessibility of an idea is the responsibility of the author", I think it's appropriate to point out 
that communication is always a two-way street. 
You go on to remark that "a good idea is only as good as the linguistic ability of the author 
which allows him to communicate these ideas in a manner which can create enthusiasm 
and curiosity in the reader" as though this somehow relieves the reader of all responsibility 
in the communication process. I naturally disagree. You then say you've read papers by 
Standish, Tegmark and Schmidhuber which are "far more detailed in their physics and 
mathematics" than my own. Well, I've looked at some of those papers too, and they're 
physically and mathematically vapid. In fact, they're virtually devoid of novel physical 
insight, and as for metaphysical insight, they fall somewhere between clueless and 
tentative. In any case, this particular debate isn't about flashing equations; it's about 
establishing a model in which equations can be interpreted to explanatory effect with regard 
to certain philosophical issues.
If you're really baffled by something I've said, just ask for clarification. If your question is 

sincere and politely phrased, you just might get an intelligible answer. On the other hand, if 
what we're talking about is some nebulous cloud of ineffable confusion regarding every 
single word I write, then please pardon my reluctance to invite more of the same.
Comment #14:
“Mr. Langan, I'm confused. First you argued that you could win in a "priority dispute" over 
ideas being attributed to Max Tegmark based on what you wrote in 1989 about "nested 
virtual realities".”
Yes, Mr. Parallel, I quite agree - your confusion is spectacular. Although it's you who is 
obviously trying to manufacture a "priority dispute" here, you keep on insisting that it's me. 
All I've done is make relevant statements of fact, e.g. that the CTMU predates recent work 
on "the multiverse", and that Max Tegmark's work can be interpreted in the CTMU in a 
certain specific way. You, on the other hand, keep generating ominous insinuations to the 
effect that I'm somehow accusing Max of plagiarism or theft of credit when I've repeatedly 
stated that I intend nothing of the kind. I resent this, and for the record, I request that you 
restrain yourself.
Not that it makes any further difference, but in a certain important respect, likening the 
CTMU to Arabian algebra and multiverse theory to Newtonian calculus is rather misleading 
on your part. Just as the calculus is mathematically more advanced than arithmetic, the 
CTMU is mathematically more advanced than multiverse theory; after all, unlike Max, I've 
actually described a large part of the mathematical structure of my theory. In this respect, 
the CTMU would play the role of calculus rather than arithmetic in your misguided example.
May we now lay this "priority dispute" nonsense belatedly to rest? Thanks for your 
cooperation. 
Comment #15:
“Mr. Langan, your response is nothing short of incredulous.”
First, Mr. Parallel, I think you may have wanted to say "incredible" instead of "incredulous" 
(despite my credible response, you're the one who's apparently incredulous). Second, I 
don't care what you think my response is. Third, you and you alone are choosing to interpret 
a date, 1989, and a fact, namely that multiverse theory can be interpreted in the CTMU, as 
a "priority dispute". Fourth, each time I said the bad words "plagiarism" and "priority 
dispute", it was because I was being harassed by you or somebody else and/or wanted to 
make it clear that I think Max is entitled to credit for his own work. Fifth, I can substantiate 
my claims with factual evidence, but as far as I can tell, you're neither polite nor important 
enough to merit the effort. Sixth, many people have websites, some of them extensive, on 
which they do not make all of their copyrighted work available to anyone for nothing but a 
mouse click. And lucky seventh, kindly get off my back before I'm forced to refer your case 
to a moderator. Thank you. 

On Progress, Readdressing Reality Theory, and Information in 
the Holographic Universe
Comment #1:
Stu asks: "why shouldn't ID start by simply following standard scientific methodology?" The 
short and punchy answer is that standard scientific methodology is inadequate for the 
verification of inferred scientific truth; one needs logicomathematical methodology for that. 
On the other hand, if all you're looking for is empirical confirmation, then you're not looking 
for truth, and in that case, you're not looking for the truth about biological origins. All you're 
looking for is another fallible exercise in causal induction from limited observations, and 
given the philosophical intransigence displayed on a daily basis by both sides of this 
debate, that won't solve anything.” 
It might sound strange to hear someone say that the scientific method is not about 
verification (as opposed to imperfect confirmation), but that's how it is. To understand why, 
you need a bit of advanced logic, and specifically model theory. The scientific method, 
being empirical, is based on an observational model of reality subject to weaknesses like 
the problem of induction and the Duhem-Quine thesis, due to which certainty cannot be 
inductively attained. How, then, is scientific truth to be ascertained? Logic provides the 
answer: by deduction from tautologically self-evident certainties. In fact, aside from direct 
apprehension, this is the only way there has ever been to "know" anything at all.
Unfortunately for those excessively enamored of the scientific method, deduction does not 
appeal to an observational model, but to another kind of model entirely...a substitutive 
model, or what a mathematical linguist would call a generative model in which the truth 
property is grammatically inherited from "origins" which are themselves either axiomatic or 
deduced. To know the truth about the origin of something (a theorem, the first organism, the 
universe), two conditions must be satisfied: (1) one must be able to deduce or directly 
apprehend the original event or terminal antecedent, and (2) truth must be heritable along 
the pathway from antecedent to consequent, cause to effect.
In science and philosophy alike, that which is relevant to perceptual reality is real. In other 
words, relevance to reality implicatively conveys reality, and reality is heritable under the 
unary operations "X affects Y" and "X is affected by Y" (and under the related n-ary 
operations). This is what makes reality a coherent, connected structure. It also makes 
reality perfectly self-contained up to relevance, and a perfectly self-contained system is a 
perfectly reflexive system with respect to all possible functions and processes, including 
generation and causation. Logically, this makes reality its own origin and its own cause. So 
we have managed to logically deduce the origin of reality, namely reality itself (this, by the 
way, is the basic position of not only the CTMU but naturalism; if naturalism espouses any 
other position, e.g. "there is no origin!", then it can be easily reduced to bunk). So much for 
criterion 1.
What about criterion 2? Specifically, where's the isomorphism between the generative 
model of logic and the observational model of science? That's where the CTMU comes into 
its own. The CTMU is the theory which relates the two kinds of model as complementary 
aspects of reality, and thus the theory which allows us to use logicomathematical 

methodology to discover scientific truth. In other words, causality can only be known by 
inference, and the only way to show that reality mirrors inference is to show how logic, the 
basis of inference, is built into the infrastructure of reality. 
Stu then suggests that we "build a detailed enough theory to allow experiments to 
differentiate ID from RM&NS." In light of my response to Stu's first question, this requires 
that we extend scientific methodology to embrace the native methods of logical and 
mathematical reasoning (which are already invoked but not fully respected by the scientific 
method). When we do this, causality becomes more than empirical induction in reverse; it 
becomes the (verification-friendly) generative grammar of reality. Again, this requires the 
CTMU.
Stu asserts that without a means of conducting experiments to differentiate ID from 
RM&NS, no theory of ID can ever be taken seriously, and that we therefore "need 
predictions rather than postdictions". In response, I'll merely point out that any logical theory 
which predicts or retrodicts any sort of life at all stands head and shoulders above neo-
Darwinism, which does neither. The reason that neo-Darwinism fails to account for life post 
hoc, ad hoc or propter hoc is that it lacks the wherewithal to generate an ultimate common 
origin from which life can "descend", and it lacks any generative model of causality in which 
"descent" bears the required resemblance to logical deduction. 
Thus far, the problem with this entire debate has been that neither side of it has a model of 
causality. Remember, to assert that it is true that anything causes or is the cause of 
anything else, you need a model supporting coupled definitions of truth and causality. If you 
lack such a model, then you cannot with any certainty predicate truth of hypotheses 
regarding causality. All you can do is talk about more or less tentative correlations within a 
limited experimental context. But how do you characterize the correlations? Without a solid 
model and definition of causality, you have no idea. 
Consider these simplified hypotheses: "Life is caused by random mutation and natural 
selection" and "Life is caused by intelligent design." In order to decide between these 
hypotheses, there are several terms whose definitions must be agreed on by both camps. 
The first of these is "is caused by". That is, we need a common definition of causation. But 
this means that we require a model of causation, and to achieve certainty, it must be 
logically demonstrative in character. No model, no certifiable decision in favor of either 
hypothesis, and it makes no difference how much "empirical evidence" is offered. The 
hypothesis to be verified must be well-defined, and if it is defined beneath the level at which 
certainty is attainable regarding it, then nothing gets decided. Worse yet, if one side insists 
that only the artificially restrictive scientific method can be used as a means of verification, 
and the other side concedes to this, then real progress can't even begin. 
Stu opines that "the CTMU sits at too high a level to help at this early stage...a bit like using 
General Relativity as the starting point for a lab experiment in momentum transfer." A 
thoughtful criticism. However, the more certainty one desires, the higher the level of model 
that one needs. On the other hand, if one settles for a provisional, low-level model up front, 
then one lacks certainty regarding the significance of one's findings, and again, nothing gets 
decided. Indeed, if one exclusively restricts oneself to an observational model, certain 
model-theoretic criteria can never be logically satisfied. 

Stu says that having a model is a good idea, but questions "whether its necessary to have a 
TOE before ID researchers can construct experiments that would allow falsification". As 
explained above, having a general model is an absolute necessity, at least at the stage 
where general (e.g. ID, non-ID) hypotheses are to be proven. If this model can be logically 
derived, then there is no point in pretending to rely on low-level experimentation which can 
prove nothing in its absence, particularly when the whole point of such experimentation 
would be to arrive at the model in question. 
Since natural selection is trivially observable in some contexts, there is no point in 
distinguishing an alternative model of evolution from neo-Darwinism in those contexts. The 
distinction need be made only with respect to higher-order fitness-survival relationships, 
possibly involving higher-order, irreducibly-(n>1)-ary fitness-critical combinations of fitness-
related attributes ("irreducible complexity"). Unfortunately for neo-Darwinism, the only way it 
can even begin to extend itself from the micro scale to the macro scale is by showing that 
every step in every evolutionary pathway offers some degree of utility to the evolving 
organism. But as we all know, it has displayed no actual ability to do that.
Instead, neo-Darwinism finds circumstantial correlations between fitness or survivability on 
one hand, and certain functions and their associated genes on the other, and then launches 
into grand pseudo-causal generalization on that basis. It is then asserted that fitness has 
not been merely tautologically equated to survival, but that survival has been "caused by" 
certain genes and their functions! This is an exercise in circularity; certain things that have 
been circumstantially correlated with survival are being identified as "causes" of survival, 
and this particular kind of causal identification is justified in neither logic nor statistics (as is 
thoroughly explained in any decent elementary introduction to correlation). Thus, while neo-
Darwinism claims to have refuted the putatively tautological (unfalsifiable) nature of the 
fitness=survival equation, it has done nothing of the kind. Nor can it ever do so given its 
definitive conceptual and methodological limitations. 
This leads directly to a burning question: why not simply accept the tautology and embrace 
the theory which acknowledges and explains it, namely the CTMU? After all, it's not as 
though the CTMU doesn't have a defensible lock on scientific truth and verification (as 
explained above and elsewhere).
Stu asks whether I could "drill down far enough to produce a sufficiently detailed sub-set of 
CTMU to guide the researchers." The answer is yes. But meanwhile, anyone who responds 
to the above explanations with "that's great, but it means nothing without a new 
experimental methodology for all of the materially-minded instrumentalists involved in the 
debate" is missing the point. The point, of course, is that when nip comes to tuck, materially-
minded instrumentalists have no real business "in the debate" at all. 
By definition, concrete instrumentalists are neither comfortable with nor cognizant of all the 
demands of correct high-level theorization, qualifying at best as competent hands-on 
technicians who help equally-competent theorists test and refine their abstractions (thus, 
the well-known and well-justified symbiotic distinction between theorists and 
experimentalists in the physical sciences). With all due respect to the ingenuity and 
scientific indispensability of good experimentalists, the debate over biological origins is over 

the heads of those unschooled in advanced logical abstraction. A new experimental 
methodology will come - I and others are working hard to make it happen - but the issue of 
biological origins can be logically and philosophically decided without its benefit.
The history of thought displays a well-established progression: in many areas of intellectual 
endeavor, what used to be called "philosophy" has given way to what is now called 
"science". When it comes to intellectual progress, this is often the natural order of things; 
the broad and general, arrived at by induction from more or less scattered details, gradually 
gives way to the narrow and specific. Funnelvision tapers somewhat jerkily into tunnelvision, 
and formerly submicroscopic details emerge in ever-sharper relief as the focus tightens and 
the field narrows. That is, the fundibular spatiotemporal image of human intellectual 
progress reflects an invariant quasi-volumetric relationship between scope and 
informational resolution or microscopy. 
So attractive is the higher resolution often associated with reductions of scope that many 
scientists have come to hate philosophy, the mother of science, with a terrible passion. Its 
scope is too wide, and the details it encompasses seem too fuzzy. Never will these 
scientists be driven from the paradise they have found at the constricted end of the 
funnel...the end boldly emblazoned with "Concrete Results Or Bust!" 
Unfortunately, conviction and enthusiasm are neither a road to entitlement nor a barrier 
against anomaly, and anomalies - e.g. biological machines for which Darwinian 
explanations cannot readily be educed - have a way of compromising resolution. This 
sometimes necessitates a mind-expanding trip back up the funnel, and this naturally gives 
rise to a certain amount of resistance on the parts of those determined to stay comfortably 
within the concrete realm. Not to put too fine a point on it, but that's what we seem to have 
going in this controversy, and amazingly, some of those advocating a major change of 
perspective seem bent on placating, or at least working around the unrealistic 
preconceptions, of those who explicitly prefer tunnelvision. 
Although this is not the way to achieve a final resolution of the dispute, it makes a sad kind 
of political sense, and in any case, "it is what it is". So I try to make the best of it, and do 
what I can to refine and elucidate the linkage between the logical and empirical sides of the 
issue. While the CTMU has already accomplished this on a high level of abstraction - at 
least for those who can understand it - I have no trouble admitting that there's still plenty of 
room for progress.
Comment #2:
Pim van Meurs asks: "Is our inability to deduce Darwinian explanations (a) sufficient reason 
to reject Darwinian explanations? What if we can formulate 'just so' stories'?" 
A just-so story is not an explanation. Provided that it meets certain logical criteria, it can at 
best be regarded as a "possible explanation". But as I've already pointed out above, neo-
Darwinism lacks both the wherewithal to generate an ultimate common origin from which life 
can descend, and a generative model of causality in which descent incorporates logical 
deduction. This means that Darwinian explanations alone cannot satisfy the logical criteria 
in question. Therefore, they do not actually qualify as "possible explanations".

Pim opines that since there may be "other ways (than stepwise utility) to show macro scale 
evolutionary pathways", my usage of "as we all know" may be hyperbolic. But it isn't. Strictly 
speaking, Darwinism implies gradualism because it needs to keep event probabilities in line 
with the probabilistic resources afforded by "nature", as restrictively and arbitrarily identified 
with material structures and first-order laws of physics. In order to get around this problem, 
it has been suggested that (e.g.) certain organisms may embody information regarding 
entire sequences of steps on the molecular level, and that this may keep probabilities high 
even when the steps are individually inutile. But even if this or any comparable scenario 
were deemed general, it would offer no means of exceeding the known limitations of causal 
induction. It would thus fail to decide the issue of biological origins in favor of Darwinism.
Pim then wonders what stepwise utility might mean for a theoretical approach which "tries to 
infer something" based on elimination. Obviously, the first order of business for such an 
approach would be an exhaustive characterization of the entire probability space from which 
possibilities are to be eliminated. This, however, would require a global model of reality, 
thoroughly indebt said approach to the CTMU, and thereby eliminate neo-Darwinism in the 
qualification round. 
Pim then asks how one would calculate certain desirable probabilities, thereby allowing one 
to eliminate pathways that are "largely untouchable due to their distant nature. After all," he 
continues, "(an) absence of details on such pathways hardly invalidates Neo-Darwinism nor 
lends credibility to alternatives that depend on invalidating all other alternatives." I can only 
observe that my post does not rely on mere eliminative reasoning.
Finally, Pim asks "How can CTMU help us resolve these scientific questions?", going on to 
report that while logical abstraction is fine in theory, he "wonders about its relevance ... 
when it is limited in practical application." In particular, Pim asks whether he, as a scientist, 
might place too much emphasis on practical considerations. I agree with Pim that on the 
concrete level, science requires a pragmatic approach. But questions regarding biological 
origins and fundamental evolutionary causation are not immediately practical in nature. As 
I've explained, they're logical in nature, and practice is secondary to logic. Similarly, while 
practical applications of the CTMU are in the works, they are secondary to the explanatory 
value of its logic. 
RBH adds: "With all due respect to advanced abstract logicians, I don't think I'll hold my 
breath until the decision about biological origins is made manifest to us lesser folk." This 
personal decision by RBH is beyond my control. However, it was not my intention to imply 
that RBH is a "lesser" being. I only meant to explain why making things materially manifest 
to concrete instrumentalists is not what it actually takes to decide the issue of biological 
origins.
RBH goes on: "This appears to be tantamount to the claim that experimental methodology, 
whether new or old, is superfluous." While experimentation is far from superfluous with 
respect to questions of a material nature, it is directed toward confirmation rather than 
verification. Unfortunately, this makes it inadequate to decide the issue of biological origins. 
RBH concludes: "If pure thought can give us the answers, why bother with messy 

experimental methodologies? And eschewing experimentation has the additional salutory 
effect of ensuring that one's theories will not be slain by inconvenient facts." In some 
contexts, these remarks would be well-justified. But in the context at hand, they are not. 
They fail to address the content of my post and are inconsistent with the spirit in which it 
was written. While experimentation is certainly a valid scientific enterprise, it is nonetheless 
inadequate to fully decide the issue of biological origins.
Let me make it clear that I have ample respect for experimentalists. Without them, science 
could not exist as we know it. But this does not make experimental methodology perfect or 
even adequate, and it does not place limitations on what can be demonstrated with logic 
alone. Although experiments confirming either of the major hypotheses regarding biological 
origins would no doubt sway open-minded people in one direction or the other, they would 
not actually decide the issue. Only logic can do that.
Comment #3:
Parallel writes "If the CTMU is 'the theory which allows us to use logicomathematical 
methodology to discover scientific truth', how have scientists using logic and mathematics 
managed to discover so many scientific truths without the CTMU?" The answer, once again, 
is that empirical science is not in the business of establishing "truths". It is in the business of 
tentatively confirming hypotheses. If anyone finds this distinction flawed or inordinately 
subtle, he or she should take the time to further investigate it.
Parallel continues "And if the CTMU is 'the theory which allows us to use 
logicomathematical methodology to discover scientific truth', how many scientific truths 
have been discovered with the CTMU?" This is just the flip side of parallel's initial mistake. 
The CTMU is primarily a theory of logic, not empirical science. This is what qualifies it to 
verify truths about the nature and origins of things. The scientific value of the CTMU resides 
largely in the fact that within its framework, certain logical truths can be regarded as 
scientific truths (as opposed to tentatively-confirmed scientific hypotheses).
As I've explained, science employs an observational model of reality. This model has a dual 
generative model, which is the same kind of model employed in logic and other branches of 
mathematics. The CTMU superimposes and links these two models, thus linking the two 
associated kinds of reasoning. Anyone who sincerely wants to know what conclusions, 
scientific or otherwise, may be reached on this basis should carefully read the introductory 
paper I published in PCID last autumn. 
Comment #4:
The CTMU is the theory that allows the discovery of "scientific truth" because, when science 
is defined strictly on the scientific method, there is no such thing as a "scientific truth". There 
are only directly apprehended truths, logically derived truths, and in contradistinction to 
these, tentatively-confirmed scientific hypotheses. This is why my answer "implies that 
science has discovered no (certified) scientific truths." 
Now, that's a stone cold fact. Science may have accidentally happened upon truths of one 
sort or another, but sadly, it can't certify that this is what they are. "Fantastic" or not, 

"semantic" or not, that's how it is. If you can show that this is not in fact the case, then be 
my guest. Otherwise, please address your criticisms to somebody else. When you come 
right down to it, this is Models & Formalized Theories 101 we're talking about here; if you 
haven't yet taken these courses, please fill the gaps in your knowledge before attempting to 
engage me in further discussion on this topic.
You go on to accuse me of "begging your question". I suggest that you read the final 
paragraph of my last response, and then carefully read the paper I mention. Don't care to 
do that? Then kindly stop accusing me of "begging questions" that are answered in the 
abstract at the head of the article, and please don't continue to pretend that I haven't 
already pointed out that logic comes before practice (those who refuse to apply logic before 
launching into practice are in clear violation of common sense and all but doomed to 
eventual failure). Because the logic I've presented here is designed to stand on its own, I 
don't need to provide you or anyone else with anything further in order to make my case. If I 
choose to elaborate in the future, well, that will be an undeserved extra. 
You also say that "the nature of things - e.g., of physical entities - is the proper domain of 
empirical science." Thanks for the opinion. But given that the scientific method is restrictive, 
and that it relies on an observational rather than a generative model of reality, can you 
establish the intellectual adequacy of your viewpoint? As I've already observed, you cannot.
You then assert that "scientists discover the nature of things with observation, logic, and 
mathematics." Tentatively, yes; with certainty, no. That is, they merely confirm hypotheses 
while making a lot of guesses, mistakes and oversights, which is unavoidable given the 
canons of empirical methodology, which mandate the slapdash observational application of 
mathematical formalisms for which observational models of reality lack support.
Given all of these mistakes on your part, do you really think that you're being fair to poor 
Albert Einstein by trying to weigh him in on your side of the scale? Please, a little respect for 
the dead.
Comment #5:
Parallel asserts that "scientific truths are per definition 'truths' discovered by science ... 
'scientific truth' inherently defines the variety of truth discovered by science upon which the 
given caveats of science are imposed." I see what parallel is trying to get at, but as I've 
already stated, it just doesn't wash. While my own usage of the phrase "scientific truth" was 
clearly predicated on the availability of a generative model of reality (the CTMU), parallel 
lacks such a model. So whatever parallel means by "scientific truth", it isn't the kind of truth 
recognized in logic (which is, after all, the formal language of truth). Needless to say, logical 
rules of derivation are not subject to what parallel calls "the (empirical) caveats of science". 
Compounding his error, parallel states that "the necessity for oxygen to sustain the vast 
majority of known lifeforms is just one example of a truth discovered by science." We can 
call this a "truth" only insofar as it is so simple, and involves so little inference, that it is for 
all practical purposes directly apprehended (I've already explicitly allowed that direct 
apprehension is a route to truth). On the other hand, if this conclusion requires a nontrivial 
amount of inference, then calling it a "truth" requires an isomorphism between the 

generative model of logical inference and an observational model containing oxidizing 
lifeforms. Again, parallel lacks such an isomorphism. Given this fact, it is almost overkill to 
point out the ambiguity and wriggle-room implicit in the phrase "vast majority". The same 
applies to parallel's reliance on the adjective "known", which indicates that his example is at 
least partially about subjective knowledge states. 
Next, Parallel says that "If the truth of Langan's CTMU is above the restrictive uncertainties 
of scientific empiricism, why does it purport to be a theory, e.g., the 'Cognitive Theoretic 
Model of the Universe'? By defining the CTMU as a 'theory', Langan implies that it is subject 
to the very empirical restraints [sic] he declares it to be above." Even after staring at this for 
minutes on end, I still can't make sense of it. The term "theory" has a strict mathematical 
definition based on the application of generative rules of inference to axioms and theorems. 
According to this definition, even logic itself is a theory. In any case, theories are not defined 
in such a way that they are necessarily subject to empirical constraints. In this respect, the 
term "theory" differs sharply from scientific methodology. This dissonance between scientific 
methodology and scientific theories, i.e. theories with scientific content, reflects the self-
conflicted nature of science.
Parallel then quotes Einstein to the effect that theories consist of assumptions to which logic 
has been applied, and that a "theory finds the justification for its existence in the fact that it 
correlates a large number of single observations ... it is just here that the 'truth' of the theory 
lies". Notice that Einstein places "truth" in quotes, ostensibly because he was aware of the 
distinction between the provisional "truth" of an assumption-ridden scientific theory and 
truth as it is logically evaluated with respect to syntax, semantics and interpretation. 
You know, this whole exchange has been truly amazing. First, I explain that scientific 
empiricism and instrumentalism fail to support the concepts of truth and verification due to 
their reliance on non-generative observational and utilitarian models (where truth is the 
abstract mathematical identity studied in logic). It's not as though this can be rationally 
disputed; it's a hard fact. Next, I explain that logic and mathematics work from a different 
(generative) kind of model more amenable to formal verification, and point to an alternative 
theory of reality (the CTMU) which conjoins the two kinds of model and thus opens logical 
routes to verification in fields formerly limited to confirmation. (Note that the failings of 
empirical and instrumentalist models exist independently of the CTMU.)
So what happens next? Incredibly, my points are immediately rejected on the grounds that 
they are not in line with empirical and instrumentalist criteria. And to ice the cake, it is 
summarily demanded that I demonstrate my ability to satisfy these criteria regardless of 
their inadequacy, which is exactly what I'd set out to explain in the first place.
Truly, this is a world of many wonders! (So what if some of them resemble broken records.) 
 
Comment #6:
Regarding possible kinds of truth, RBH asks a good question. "Suppose, just for the sake of 
argument, that the 'direct apprehensions' of two people differ on some matter. On what 
grounds can one decide between them? What is the certifying agency that says 'This one is 
true, that one illusory'?" That agency is the distributed cognitive-perceptual syntax of the 

self-spoken language of reality (or in CTMU terminology, of SCSPL). It can be shown that if 
reality is connected - if we are really united in an objective manifold of common perception - 
this level of syntax must exist. Remember, science is primarily concerned with the 
formulation of general laws of nature as required for prediction, and these are just structural 
and grammatical components of this distributed reality-syntax. 
Regarding the methodology associated with this consistency-enforcing, connectivity-
enforcing agency, RBH asks whether "the purpose of the new experimental methodology 
that you and others are working to develop, to decide among conflicting direct 
apprehensions? Or are all direct apprehensions of equal standing and no conflict is 
expected among them?" On the distributed level of SCSPL syntax - i.e., on the level of 
direct apprehension - no such conflict is possible. If there is a conflict, then by definitive 
deduction, we have either a deviation from syntax (illusion) or a simple misunderstanding. 
RBH goes on to ask: "If the issue of biological origins can be "logically and philosophically" 
decided without some new experimental methodology, why do we we need that 
methodology? And is there a distinction worth making between "logically and philosophically 
decided" and "experimentally decided"? That is, what is gained by having some new 
experimental methodology if Truth can be arrived at via pure thought? Why do experiments 
at all?" 
Some truths can be deduced by pure thought; others, particularly those involving some 
degree of freedom or self-determinacy, cannot. The new methodology will be designed to 
distinguish deterministic processes governed by distributed syntax from processes involving 
freedom and intelligence, which involve the spot-synthesis of "localized syntax". The trick is 
to distinguish localized from distributed syntax. The CTMU is the sole framework in which 
this distinction can be made.
Incidentally, what RBH might call "truth with a small t" is clearly of potential value, 
particularly in the absence of deduction. Instances of "truth", broadly including the products 
of scientific empiricism, are generally developed with respect to restricted contexts. Without 
contextual limitations, very little of a material nature can be even tentatively decided. 
Unfortunately, many proponents of neo-Darwinism recognize no such restrictions. In 
response to what seem to be extremely low probabilities for certain (irreducibly and 
specifiedly-complex) compound events, they invariably set out to de-restrict the context in 
order to multiply probabilistic resources and thereby raise the probabilities, even to the point 
of endorsing bizarre cosmologies in which realities proliferate beyond infinity (we've seen 
that maneuver on this very board). When it comes to explosively or implosively enlarging 
the sample space in order to provide themselves with probabilistic resources, they dwarf 
any nuclear blast! 
This maneuver is neither logically nor scientifically supportable, particularly in answering 
questions about origins and ultimate causation. By definition, an origin is something that can 
be accessed by (logically-fortified) causal regression; interminably delaying access by 
contextual expansion and outright combinatorial explosion is flatly inconsistent with 
answering the question. Foiling this endless-delay tactic in any final sense requires a 
generative-deductive theory of reality (the CTMU). In the CTMU, reality is deductively 
identified as its own origin (as explained above), which in turn implies that causality is 

generative. Working from these realizations, we can restrict the context in such a way as to 
meaningfully address the source and means of evolution. To avoid an open causal regress 
in which nothing is ever decided, we need CTMU-style logico-algebraic closure, and - here's 
the kicker - because this kind of closure is implicit in naturalism, the CTMU locks 
metaphysical and methodological naturalism into compliance using pure logic.
[As regards the latest remarks of parallel and Pim, I regret to say that I'm at a loss. I point 
out that a theory is a well-defined logico-algebraic structure with certain properties and 
requirements with which conventional scientific models and methods are incompatible; in 
response, they adduce a corrupt definition of "theory" that neatly embodies the very 
problem I've been trying to address. They stay squarely planted inside the materialistic half-
court of Cartesian dualism, and tirelessly keep their dialectical merry-go-round spinning, by 
conspicuously ignoring key distinctions recognized by competent theorists the world over. 
What can one possibly say in the face of such determination? Only an awed silence seems 
appropriate.] 
Comment #7:
Stu writes "One thing troubles me...I am a little nervous when Truth (capital T) is claimed." 
Mathematicians claim Truth all the time. Indeed, because the truth-Truth distinction is just 
one of certainty, i.e. probability, everybody who claims "truth" (t) implicitly claims some 
measure of "Truth" (T), or the attribute denoting inclusion in a formal system or recognizable 
class of facts or perceptions mutually related by an inferential schema or "scientific theory" 
(which is required to exhibit logical consistency and thus to tacitly incorporate the formal 
system of logic). That is, scientific truth t is just the assignment of a subunary, usually 
subjective probability to logical truth T; if t does not come down to a probabilistic stab at T 
and thus devolve to generative logical inference, then it is meaningless.
T is recognizable in a formal system or inferential schema because it is a deductively 
heritable attribute. Thus, if T is present in an antecedent expression known by deduction or 
direct apprehension to belong to the system or schema of interest, then it is present in any 
consequent expression generated by the proper application of the rules of substitution 
governing the evolution of said system or schema. Unfortunately, while every claim of t is 
probabilistically defined on a claim of T, the observational model of science fails to support 
T and thus fails to support t (as a probabilistic function of T), thus putting science in default 
of its own validative criteria. A scientific theory is supposed to be isomorphic to its universe; 
if it is required to be consistent and thus to implicitly incorporate logic, but its universe is 
observationally modeled in such a way that the generative mechanisms of logic are 
excluded, then the required isomorphism is lacking, and truth is out of the picture.
This problem has everything to do with the "mysterious" relationship between mathematics 
and science. Science needs to apply mathematics in the course of theorization, and indeed 
predicates of various assumptions and hypotheses a kind of "empirical truth" based on 
probability functions of mathematical truth. Yet, its underlying observational model fails to 
map itself to the generative model of mathematical inference, and thus fails to support any 
kind of truth at all! This ultimately renders it incapable of decisively answering questions of 
any kind, let alone fundamental questions about the nature of reality. This is what the CTMU 

aims to fix.
Stu continues: "CTMU is based upon a set of tautologies and logical argument. My problem 
is that any logical/mathematical proof is a computation, and thereby falls under the theory of 
computation, a science. From that it has been said that proof theory has been misclassified, 
and it should be viewed as a science. If a science then the truths reached are tentative, 
albeit at a higher level than typical science truths are viewed."
Those who want to classify proof theory as an empirical science are those who want to use 
computers to prove mathematical theorems, e.g. as computers were used by Appel and 
Haken to "prove" the four color theorem. The idea is that as long as computers are 
executing programs equivalent to formal systems, they can be trusted at some acceptable 
level of reliability to correctly determine the validity of mathematical conjectures. Obviously, 
this approach relies on the premise that machines are faithful to the formal systems in which 
they are programmed. So we see that it is really formal systems which come first; machines 
are only being relied on to model those systems in the course of long and tricky inferential 
procedures. 
In fact, computers themselves have emerged from a branch of mathematics (computation 
theory) dealing with abstract machine models amounting to generic formal systems. So no 
matter how you cut it, proof criteria remain logical rather than empirical. 
 
Comment #8:
Stu writes that he is "still unsure due to the fact that it was your physical brain that produced 
the theory." The brain works in a regular way, and because brains are physically embedded 
in nature, this implies that nature possesses a constant syntax acting as a source of 
regularity (a syntax consists of the "regular" parts of a language, in this case the language 
of nature). Because the physical world cannot exist without regular laws of nature reflecting 
the invariants of its structure, dynamics and composition, syntax has priority, and the 
physicality of the brain cannot be used as an argument against this syntax or its reflexive 
implications. In particular, since the laws of logic are the most basic (and general) rules in 
this syntax, physicality provides no basis for criticizing logic, and because the CTMU is a 
tautological extension of logic, physicality provides no basis for criticizing the CTMU. Logic 
distributes over physics, not vice versa. The CTMU, like the rest of mathematics, is abstract 
and in this sense beyond concrete physicality.
Stu continues: "In an analogy with the computer producing proofs, the same criteria applies 
i.e reliability, ability to execute formal systems etc. As it is not possible to exhaustively 
evaluate your source code and hardware (to get confidence of correct functioning), we can 
only try to evaluate the output."
Because the CTMU is to some extent its own output - consider that the syntax of logic is 
self-generating - those who are unable to discover whether it deviates from logic by direct 
evaluation are unlikely to succeed by evaluating its output. The output must be properly 
interpreted, and anyone unable to evaluate the theory on its face would probably be unable 
to correctly interpret its concrete implications. In any case, since it is impossible to evaluate 
anything without applying logical rules of evaluation, anyone evaluating these rules and 

finding them inadequate would necessarily be proving his own evaluation inadequate in the 
process. By finding your own rules of inference in error, you'd be undermining your own 
reasoning and thus sawing the floor out from under your own feet. Because the CTMU is a 
tautological extension of logic, questioning its integrity entails the same kind of risk. As for 
the possibility that an error was made in developing the logic, the CTMU is straightforward 
enough to be checked without the aid of a computer.
In any case, we need to keep our eyes on the ball. When one devises a validation function 
based on an empirical procedure designed to confirm that one or more formal systems have 
been correctly applied, the argument of the validation function is still an abstract formal 
system. Those who suggest that mathematical results should be checked by computer are 
relying on (mechanically modeled) formal systems - I've already explained this above - and 
those advocating that validation be predicated on multiple runs or samples is proposing to 
apply yet another formal system to boot, namely the mathematical theory of probability. 
Again, no matter how you slice it, abstract structures rule the roost. They're presupposed by 
the very concept of physicality. The CTMU is about the entailments of this fact. 
Stu then presumes that it is logically possible that an error has been made, and that the 
CTMU needs to be tested in some fashion before truth can be claimed. Good point; 
mathematical proofs, all of which lay claim to absolute truth (as implied by the term "proof"), 
get checked by other mathematicians all the time, and are sometimes found to be wanting. 
All I can say is, nobody's stopping anyone who knows how to do so from checking the 
CTMU. In any event, rational methods, and not empirical methods, are conventionally used 
to check mathematical reasoning. From a verificative standpoint, computers can do no more 
than confirm their programmers' logic.
Stu seems to acknowledge this when he writes: "In one sense no different to any claim of a 
mathematical or logical proof. But also against reality. Why? It needs an acid test in case 
the (potential) error is too subtle for anyone to spot (I am thinking here of Euclidian 
geometry, considered True for a long time before being shown to be incomplete). Just 
because CTMU may well turn out to be the best description of reality we have does not 
mean it can claim absolute truth. Or am I missing something obvious?"
"Absolute truth" - what you've been calling Truth with a capital T - denotes inclusion in an 
inferential language. That is, it denotes generation by the syntax, in particular the generative 
grammar, of the language in which inclusion is asserted. Absolute truth is claimed for the 
CTMU because the CTMU is identified by definition with cognitive-perceptual syntax and 
the relationship of that syntax to the language thereby generated, i.e. cognitive-perceptual 
(rational-empirical) reality. Regarding Euclidean geometry, the syntax status and absolute 
truth of which seems to be precluded by the putative independence of the parallel postulate, 
it is still syntactic in the sense that at the level on which it is formulated, it is the "local 
default geometry" of human cognition and perception. (Since even the spacetime manifold 
of General Relativity is flat in the microscopic limit, we cannot say that elliptical and 
hyperbolic geometry are vying with flat Euclidean geometry for conceptual priority. 
Geometrically, they can only be defined by reference to a flat tangent space representing 
their "average".) 
 
Comment #9:

Jacob Aliet writes that “One thing that makes this discussion circular is lack of clarity. Clarity 
about meaning of terms being used. It is possible that the participants have tacitly 
sanctioned this ambiguity because it gives them wriggle room. ... Each claims correctness 
and each accuses the other of incapacity and so on an so forth. Which isn't useful.”
Since I agree with Jacob regarding the undesirability of circular and ambiguous dialectic, I’m 
not going to engage in it. If Jacob honestly wants to learn about the CTMU (as opposed to 
preserving or expanding his own wriggle room by ignoring my previous explanations), he 
can politely and sincerely ask constructive questions about it like anyone else can. On the 
other hand, if he’s already made up his mind that the CTMU is "useless" and simply wants 
to hold needlessly forth to this effect, then what more is there to discuss?
As those familiar with a broad spectrum of scientific literature can attest, Jacob and anyone 
else who shares his mindset can easily be pointed to any of ten thousand mainstream 
papers that are so full of abstruse technical jargon and/or nebulous, tentative, undefined or 
model-free conceptualizations that any layman would be lucky to wade through two 
consecutive sentences without mishap, let alone calculate their ultimate utility. Because I’m 
neither a cynic nor a sadist, I won’t do that. But because I’m not a masochist either, I see no 
need to let Jacob get away with insinuating that I’m the only technical author whose writing 
he is unable, for whatever reason, to understand or evaluate.
I’ve said it before and I’ll say it again: communication is a two-way street. When one person 
takes another to task for being a poor expositor, he runs the risk of being called a poor 
listener in return. I can only hope that my critics will bear this in mind. Meanwhile, if Jacob 
thinks that something I’ve written above is unclear, then why doesn’t he ask for specific 
clarification? If his question is meaningful and non-repetitive, I’ll probably try to find the time 
to answer it. 
 
Comment #10:
Jacob Aliet states that "There is already a definition of 'scientific truth'." 
No, there is not, except perhaps in the minds of people who don't understand the actual 
relationship between truth and science. Therefore, I do not require any special authority 
beyond that of the actual relationship to properly define it. Jacob also takes me to task 
because I "have not demonstrated that there are higher truths that escape our observational 
models." Because the derivation of mathematical truth relies on an axiomatic rather than an 
observational class of models, no such demonstration is immediately required (though it 
could easily be produced at a cost of yet more of my time). The shift from truth to Truth has 
nothing to do with "degrees of certainty"; it has to do with a sudden shift from an 
observational model, with respect to which truth is not inferentially accessible, to a 
generative model, in which it is. Thus, when Jacob later assures us that he "never did state 
that I cannot understand your writing," I'm afraid that no such confession will be necessary.
Jacob continues: "In your paper, you do not explain how CTMU can improve the scientific 
models as far as biological origins are concerned. Is there somewhere you have explained 
the applicability of CTMU to science?" The relationship of the CTMU to science has been 

explained (with reference to the paper) right here in this thread, as well as in other recent 
threads on this board. Jacob goes on: "As I understand it, its a reality model - it makes lots 
of sense even though it leaves some fundamental questions unanswered such as why and 
how SCSPL self-refines from the UBT. But we have many reality models some religious, 
some not." The reality model we're discussing is logical, and the fundamental questions 
cited by Jacob have been answered therein. Specifically, "why?" is answered by the Telic 
Principle, and "how?" is answered by telic recursion (please refer to the paper if you find 
this confusing).
Jacob says that the sentence "Foiling this endless-delay tactic in any final sense requires a 
generative-deductive theory of reality (the CTMU)" has been "formulated without any basis 
other than convenience." No. it hasn't. A major point of my paper was to explain that this is 
a logical necessity, and I've done so with full reference to the mathematics of propositional, 
predicate and interpretational logic. Jacob's glib denial of this indisputable fact again 
suggests that if he actually read my paper, he did not understand what he was reading. 
While I sympathize, I must again observe that communication is a two-way street.
Finally, Jacob asks a question that echoes the repetitive critiques interminably generated by 
the indefatigable Russell Rierson: "Reality originates from itself or from unreality? And if 
reality originates from itself - doesnt that mean its pre-existent/eternal? The idea of 
'originates from' makes it temporal - in which case, where did the mother reality come 
from?" As I’ve already stated several times, reality originates autonomously from an 
ontological groundstate called UBT, which consists of pure telic potential that devolves, 
from a logical perspective, to a superposition of logic and nonlogic. I've already pointed out 
that this is the origin of superposition in quantum mechanics, which features the many-
valued “splitting” of 2-valued logic with respect to physical content.
And since we're now on the recurrent topic of UBT, let's kill two birds with one stone and 
have a look at Russell's post itself. Russell writes “Interesting...I found the error? in Chris 
Langan's CTMU, in that the unbound telesis (UBT) contains all positive properties yet it 
itself has no negation, which means that it itself is not a positive property, therefore it cannot 
contain all positive properties. Goodbye CTMU?” 
Despite my having addressed this issue in a previous thread, Russell is engaging in serial 
self-repetition. Again, he is trying to disprove the CTMU using the concept of UBT (unbound 
telesis), this time by way of the axiomatic method as employed by Kurt Godel.
As Godel himself understood but Russell evidently does not, axioms and the theorems 
derived from them must be interpreted in a model in order to be applied outside the 
axiomatic system in which they occur (in this context, application and interpretation are 
equivalent concepts). It follows that Godel’s reasoning can only be applied to those theories 
which share, and incorporate terms defined on, Godel’s theological model. Unfortunately, 
Godel was unfamiliar with the CTMU, and as a possible consequence of this, his model is 
not logically equivalent to the CTMU in all respects. In particular, Godel’s model fails to 
accommodate the concept of UBT. Does this mean that UBT is “not logical”, and thus has 
no place in a logical theory? Russell evidently believes this with all his might. But on this 
count, he is chronically guilty of oversimplification. For although UBT is indeed trans-logical, 
there can ultimately be no "logical theory” without it. This is because UBT is what provides 

SCSPL with a (primitive, pre-logical) "medium of existence". 
As we might have expected, Godel's choice of a model is reflected in his terminology, and 
specifically in his usage of the term “contain” (a term that has been considerably extended 
in meaning by the CTMU, where it devolves to superposition). As I’ve explained elsewhere 
at considerable length, UBT does not “contain” bound telesis in the usual sense; it fails to 
distributively incorporate any concept of extension or duration, and this rules out standard 
containment. It is simply a trans-logical extrapolation of SCSPL which attends the 
suspension of intrinsic SCSPL constraint, and naturally incorporates not only SCSPL but its 
logical self-negation. It follows that although negation cannot be applied in the usual manner 
to UBT, which implies that UBT is not what Godel would call a "positive property", it 
nevertheless “contains” other positive properties in the sense that it represents the 
suspension of their definitive constraints, and the superposition of the things thereby 
distinguished. 
Because SCSPL consists of a closed network of intrinsically-defined constraints (remember 
MAP?), the surface formulation of its mathematical structure does not explicitly require UBT. 
Most theorists in my position would probably take this as an excuse to skirt the issue and 
avoid controversy. But since UBT is an implicit logical necessity, I see no point in excluding 
it from the discussion. The main question regarding UBT, I think, is this: if UBT is essential 
to logic, then why does UBT appear paradoxical and therefore antilogical? UBT is not 
merely paradoxical, but “meta-paradoxical” by definition. What does this mean? Paradox is 
what results from self-referentially applying the negation functor of logic to logic itself within 
logical bounds, and avoiding paradox is precisely what gives logic its discernability and 
utility. But if avoiding paradox gives logic its utility, then logicneeds paradox in order to have 
utility (where the utility of logic tautologically resides in its power to exclude the negation of 
logic, i.e. paradox). This means that both logic and paradox exist in a mutually supportive 
capacity. But if so, then there is necessarily a medium of existence - a kind of “existential 
protomedium” or ontological groundstate - accommodating both logic and paradox. UBT is 
simply the name given to this protomedium, and it is why the CTMU refers to reality as a 
“self-resolving paradox”. 
Although Russell does not like the fact that UBT bears a disquieting resemblance to 
paradox, far better UBT than logic itself. If there were no medium in which logic could be 
negated - if there were no UBT - then logic would itself be indistinguishable from paradox, 
and in that case it and our world would fall apart. Does Russell want logic to fall apart? 
Maybe; maybe not. But I think it’s safe to say that Godel, a logician, did not want logic to fall 
apart, and this implies that he’d have been a fan of UBT.
Part of the problem here is that Russell does not accept the notion of null constraint; for 
him, there must always be some kind of constraint, and despite his inability to define it in 
juxtaposition to its complement, he takes the position that this initial constraint “just exists”. 
This, of course, disqualifies Russell from discussing (much less explaining) the genesis of 
the initial constraint, and in fact, it disqualifies him from even distinguishing the initial 
constraint from its complement. Sadly, failing to distinguish the initial constraint from its 
complement is to fail to define or distinguish the initial constraint, and this implies that 
Russell really has no initial constraint in mind after all. That is, while Russell seems to 
believe that there must be some sort of initial constraint, he cannot define this constraint by 

distinguishing it from its negation (if he could, then this would compel him to admit the 
existence of the complementary relationship between logic and nonlogic, and thus that both 
logic and nonlogic are superposed manifestations of something very much like UBT).
From the fact that UBT is a logically necessary self-extrapolation of logic, it follows that if 
Godel's theological model does not incorporate UBT, then Godel had no model of sufficient 
power to exhaustively characterize global reality. And if Godel lacked such a model, then 
his reasoning obviously cannot be used against the theory of global reality called "the 
CTMU". If I might be allowed a little wishful thinking, this suggests that Russell's "Goodbye 
CTMU" quip be replaced with a no-less-satisfying "Goodbye, Russell." But I suppose that 
would be asking too much.
In any event, I’m tired of repeating myself. I long ago reached the point of diminishing 
returns from even the most simplistic attempts to explain CTMU concepts to people who 
lack the ability or the willingness to absorb them (e.g. Pim, who I expect will forever fail to 
see any nontrivial difference between the CTMU and his own impenetrable, naively 
instrumentalist brand of naturalism). If ignoring repetitive criticisms is the only way for me to 
avoid a cyclical drain on my time, I’ll regretfully be forced to adopt such an approach. I hope 
that my present critics will try to remember this before sounding off again in their usual 
circular fashion. 
Comment #11:
Jacob Aliet writes "Langan states, in response to my request, that he doesnt need to 
demonstrate that there are any truths that observational models alone cannot access. He 
however claims that observational models are inadequate in determining actual Truth. It is 
clear that whereas Langan complains that there are truths that require reality models and 
logicomathematical methodology (where there is none), he cannot provide even one 
example of such truths. I will leave that issue at that."
Examples of laws that cannot be proven generally true in observational models include the 
laws of gravity and inertia, Coulomb's law, the invariance of the speed of light, and any 
other physical law that Jacob can think of.Proof: The solar system is locally situated within 
the universe. Therefore, it is impossible to observe the entire universe from the earth. 
Therefore, no empirically falsifiable law that compactly expresses observations made from 
the earth or its immediate environs is observably true throughout the entire universe, and 
counterexamples to the law may exist in other locations. Therefore, the truth of the law 
cannot be observationally certified. Therefore, if its truth is to be certified at all, 
logicomathematical methodology is required (Q.E.D.). It's called the problem of induction, 
and what it means is that the power of deductive reasoning is ultimately absent from 
observational models.
Not satisfied with leaving it at that, Jacob then writes "Langan states : 'The shift from truth to 
Truth has nothing to do with degrees of certainty.' Langan had earlier stated : '...the truth-
Truth distinction is just one of certainty.' ... Clearly, Langan is contradicting himself." Since 
the phrase "degrees of" is absent from my earlier statement, Jacob has erred again. The 
discrete logical distinction between certainty and uncertainty does not depend on 
incremental distinctions associated with "degree of confirmation", a concept notoriously 

riddled with intractable paradoxes from Hempel's Ravens to Goodman's Grue. Jacob then 
compounds his error by conflating the certainty-confirmation distinction with my earlier 
statement to the effect that so-called "scientific truth" comes down to a probabilistic stab at 
actual (logical) truth.
Jacob then asserts that "Langan does not explain the applicability of CTMU to science - a 
question me and Pim van Meurs have asked many times. He claims it has been explained 
in this very thread. I assume he is referring to the same contentious explanations that he 
offered earlier as answers." This is Jacob's third error. In a nutshell, I stated that the CTMU 
conjoins the generative model of logical deduction, which characterizes the CTMU itself, to 
the observational model of science (as detailed in the paper I published in PCID, where it is 
explained by certain duality principles including conspansive, topological-descriptive and 
constructive-filtrative duality). This explanation was factual rather than "contentious". This 
brings Jacob to 3 errors and counting.
Jacob then accuses me of "equating the telic principle to 'what God thinks is right'". I never 
made such a statement, and although something like this is implied by statements I actually 
made, Jacob has proven unable to work within the required implicative framework. Lest his 
incapacity fall short of a clean-cut error, Jacob then predicts that he will be told that "the 
Telic principle just is - the will of God." Unfortunately, this is subject to the problem of 
induction as outlined above, and I would not in fact have put it so starkly. This brings 
Jacob's error count to 4.
Not yet satisfied after having made four egregious errors, Jacob says that although it may 
be true that reality originates autonomously from an ontological groundstate called UBT, 
"there is no evidence that it is the case. It's just a hypothesis. I hope you are working on 
ways of supporting your ideas with empirical data." Of course I'm continuing to look for 
empirical data, and with some considerable success (since everywhere I look, I see 
existence rather than nonexistence). However, since UBT is essential to logic (as already 
explained), and since this constitutes powerful mathematical evidence that it exists, 
empirical confirmation is unnecessary...and Jacob has now chalked up error number 5.
Determined to break his own record, Jacob then writes that "being real itself means its 
bound...But if the UBT is real, then it is part of reality and reality then can't be said to 
originate from it." Although UBT is not bound by SCSPL, which accounts for the kind of 
reality that is central to science, Jacob makes seemingly deliberate errors of omission and 
misinterpretation. Nobody ever said that SCSPL reality is equivalent to the extrapolated 
level of reality ascribed to UBT; in fact, I've repeatedly said quite the opposite. 
Conservatively speaking, this brings Jacob's error tally to 6. He's batting a thousand and still 
going strong. 
Six little lead balloons are sufficient to load the six-gun with which Jacob evidently wants to 
blow both of his feet off, but it never hurts to load up a spare magazine. So Jacob persists 
with "you have failed to explain how having a reality model like the CTMU can help improve 
the scientific truth of the earths rotation around the sun..." As I've explained, while the 
earth's rotation around the sun can be directly observed as long as it is in progress, the 
problem of induction (as manifest in Goodman's paradox) prevents the associated orbital 
mechanics from being generalized to all of time, and that's round number 7 in Jacob's extra 

mag. The erroneous accusation of non-explanation pushes the total up to 8.
Jacob then accuses me of "avoiding the question of what methodology one would use in 
falsifying the God-based reality model I postulated." Again he is incorrect; as I've stated on 
numerous occasions, logical tautologies like the CTMU are empirically unfalsifiable by 
definition. Error number 9. 
Finally, Jacob states that I have "failed to define the terms I asked you to define. Terms you 
used in a novel and misleading manner. I guess we can repeat that communication is a two 
way street and leave them undefined." Since I have no idea what Jacob means, I guess 
we'll have to settle for 9 and leave it at that. (Then again, we can also give him a bonus 
point for his accusation of "contentiousness" in error number 3, elevating his score to a 
round and satisfying 10.)
So that about wraps it up. Thank you, Jacob, for wishing me "good luck in finding empirical 
support for the CTMU and its relevance to science". Although we don't really need empirical 
evidence for mathematical relationships, it's out there in profusion, and I therefore accord 
you a modicum of sincerity.
Now via con Dios (or not).
Comment #12:
Jacob Aliet writes "Langan states, in response to my request, that he doesnt need to 
demonstrate that there are any truths that observational models alone cannot access. He 
however claims that observational models are inadequate in determining actual Truth. It is 
clear that whereas Langan complains that there are truths that require reality models and 
logicomathematical methodology (where there is none), he cannot provide even one 
example of such truths. I will leave that issue at that."
Examples of laws that cannot be proven generally true in observational models include the 
laws of gravity and inertia, Coulomb's law, the invariance of the speed of light, and any 
other physical law that Jacob can think of.Proof: The solar system is locally situated within 
the universe. Therefore, it is impossible to observe the entire universe from the earth. 
Therefore, no empirically falsifiable law that compactly expresses observations made from 
the earth or its immediate environs is observably true throughout the entire universe, and 
counterexamples to the law may exist in other locations. Therefore, the truth of the law 
cannot be observationally certified. Therefore, if its truth is to be certified at all, 
logicomathematical methodology is required (Q.E.D.). It's called the problem of induction, 
and what it means is that the power of deductive reasoning is ultimately absent from 
observational models.
Not satisfied with leaving it at that, Jacob then writes "Langan states : 'The shift from truth to 
Truth has nothing to do with degrees of certainty.' Langan had earlier stated : '...the truth-
Truth distinction is just one of certainty.' ... Clearly, Langan is contradicting himself." Since 
the phrase "degrees of" is absent from my earlier statement, Jacob has erred again. The 
discrete logical distinction between certainty and uncertainty does not depend on 
incremental distinctions associated with "degree of confirmation", a concept notoriously 

riddled with intractable paradoxes from Hempel's Ravens to Goodman's Grue. Jacob then 
compounds his error by conflating the certainty-confirmation distinction with my earlier 
statement to the effect that so-called "scientific truth" comes down to a probabilistic stab at 
actual (logical) truth.
Jacob then asserts that "Langan does not explain the applicability of CTMU to science - a 
question me and Pim van Meurs have asked many times. He claims it has been explained 
in this very thread. I assume he is referring to the same contentious explanations that he 
offered earlier as answers." This is Jacob's third error. In a nutshell, I stated that the CTMU 
conjoins the generative model of logical deduction, which characterizes the CTMU itself, to 
the observational model of science (as detailed in the paper I published in PCID, where it is 
explained by certain duality principles including conspansive, topological-descriptive and 
constructive-filtrative duality). This explanation was factual rather than "contentious". This 
brings Jacob to 3 errors and counting.
Jacob then accuses me of "equating the telic principle to 'what God thinks is right'". I never 
made such a statement, and although something like this is implied by statements I actually 
made, Jacob has proven unable to work within the required implicative framework. Lest his 
incapacity fall short of a clean-cut error, Jacob then predicts that he will be told that "the 
Telic principle just is - the will of God." Unfortunately, this is subject to the problem of 
induction as outlined above, and I would not in fact have put it so starkly. This brings 
Jacob's error count to 4.
Not yet satisfied after having made four egregious errors, Jacob says that although it may 
be true that reality originates autonomously from an ontological groundstate called UBT, 
"there is no evidence that it is the case. It's just a hypothesis. I hope you are working on 
ways of supporting your ideas with empirical data." Of course I'm continuing to look for 
empirical data, and with some considerable success (since everywhere I look, I see 
existence rather than nonexistence). However, since UBT is essential to logic (as already 
explained), and since this constitutes powerful mathematical evidence that it exists, 
empirical confirmation is unnecessary...and Jacob has now chalked up error number 5.
Determined to break his own record, Jacob then writes that "being real itself means its 
bound...But if the UBT is real, then it is part of reality and reality then can't be said to 
originate from it." Although UBT is not bound by SCSPL, which accounts for the kind of 
reality that is central to science, Jacob makes seemingly deliberate errors of omission and 
misinterpretation. Nobody ever said that SCSPL reality is equivalent to the extrapolated 
level of reality ascribed to UBT; in fact, I've repeatedly said quite the opposite. 
Conservatively speaking, this brings Jacob's error tally to 6. He's batting a thousand and still 
going strong. 
Six little lead balloons are sufficient to load the six-gun with which Jacob evidently wants to 
blow both of his feet off, but it never hurts to load up a spare magazine. So Jacob persists 
with "you have failed to explain how having a reality model like the CTMU can help improve 
the scientific truth of the earths rotation around the sun..." As I've explained, while the 
earth's rotation around the sun can be directly observed as long as it is in progress, the 
problem of induction (as manifest in Goodman's paradox) prevents the associated orbital 
mechanics from being generalized to all of time, and that's round number 7 in Jacob's extra 

mag. The erroneous accusation of non-explanation pushes the total up to 8.
Jacob then accuses me of "avoiding the question of what methodology one would use in 
falsifying the God-based reality model I postulated." Again he is incorrect; as I've stated on 
numerous occasions, logical tautologies like the CTMU are empirically unfalsifiable by 
definition. Error number 9. 
Finally, Jacob states that I have "failed to define the terms I asked you to define. Terms you 
used in a novel and misleading manner. I guess we can repeat that communication is a two 
way street and leave them undefined." Since I have no idea what Jacob means, I guess 
we'll have to settle for 9 and leave it at that. (Then again, we can also give him a bonus 
point for his accusation of "contentiousness" in error number 3, elevating his score to a 
round and satisfying 10.)
So that about wraps it up. Thank you, Jacob, for wishing me "good luck in finding empirical 
support for the CTMU and its relevance to science". Although we don't really need empirical 
evidence for mathematical relationships, it's out there in profusion, and I therefore accord 
you a modicum of sincerity.
Now via con Dios (or not).
Comment #13:
Regarding the "ideological war" in which he purports to be involved, Jacob writes that "No 
evidence has been offered that there are higher truths that the scientific method cannot 
access without reality models. No example of any Truth has been given whatsoever." Yet, 
the problem of induction has been explained to Jacob several times, and not a glimmering 
of comprehension has been displayed in return.
Although Jacob might honestly think that I'm dancing around his questions, what I'm seeing 
is Jacob dancing around the answers. Many of us have seen the late-night commercials for 
Life Alert, in which an elderly person falls to the floor and cries "Help! I've fallen and I can't 
get up!" To me, Jacob's posts cry "Help! I'm locked in a bad model and I can't get out!" 
We've already established beyond all doubt Jacob's unmistakable propensity to err on the 
side of empiricism. To me, this is so utterly transparent that I consider any further 
discussion of these matters with Jacob himself to be futile. But if Jacob has some super-
logician, some twinkling star in the academic sky, who in his sheer bureaucratic 
competence can put the brakes to the CTMU (or just to my "bad argumentation"), then bring 
him on. Of course, in my personal opinion, one might as well stick an apple in his mouth 
and hand over plenty of salt, pepper, and a bag of charcoal briquets while one is at it, since 
by the time I'm finished with him, he'll probably feel like everyone at the luau has had a 
piece, so to speak. For this, he'll have only himself and Jacob to thank.
I almost get the impression that Jacob may have heard some of my braver and more 
tireless critics refer to the CTMU as "bafflegab" and "word salad", claiming that it contains 
elementary mistakes in logic and/or set theory and/or [fill in the blank]. However, Jacob 
should probably exercise a certain amount of caution in the weight he ascribes to such 

critiques and their authors. You see, I've already met a number of those people on this or 
that Internet bulletin board, and if the truth be told, they didn't fare quite as well as they 
pretend they did, nor I so badly. If Jacob doubts this, then he's free to get one of the "really 
smart" ones, the most capable thinker and debater, to challenge me under his real name in 
a neutral forum. I predict that Jacob will have trouble finding any takers. 
Although the stated excuse will probably run something like "superbrains is a waste of 
time," "nobody listens to that crap!", "the guy's ideas are nebulous and vapid," "he reminds 
me of my five-year-old", "we used to rap about that stuff over a bong!", "Langan 
understands nothing about real science" and so on ad nauseam, the real reason - a simple 
fear of defeat - will shed no glory at all on the reputation or self-image of the excusemaker. 
Those whom Jacob considers to be his behind-the-scenes supporters, those shadowy 
knights of the intellect, simply don't want to be humiliated in public, and they know or 
strongly suspect that it would happen just like clockwork. They'd rather watch Jacob be 
humiliated instead. I advise Jacob not to get sucked into the very trap they've learned, by 
means of hard knocks, to avoid. Remember, even if I were nothing but a big dummy myself, 
the CTMU is tautological, and that means I couldn't lose an argument over it if I tried. 
Just a word to the wise. (Then again, Jacob might think he's treed me like a fugitive racoon, 
in which case his frame of reference is so alien to mine that I'm unable to advise him at all. 
In that case, I humbly beg his pardon and wish him Godspeed on the golden road to 
wherever it is he may be going.) 
 
Comment #14:
Regarding the "ideological war" in which he purports to be involved, Jacob writes that "No 
evidence has been offered that there are higher truths that the scientific method cannot 
access without reality models. No example of any Truth has been given whatsoever." Yet, 
the problem of induction has been explained to Jacob several times, and not a glimmering 
of comprehension has been displayed in return.
Although Jacob might honestly think that I'm dancing around his questions, what I'm seeing 
is Jacob dancing around the answers. Many of us have seen the late-night commercials for 
Life Alert, in which an elderly person falls to the floor and cries "Help! I've fallen and I can't 
get up!" To me, Jacob's posts cry "Help! I'm locked in a bad model and I can't get out!" 
We've already established beyond all doubt Jacob's unmistakable propensity to err on the 
side of empiricism. To me, this is so utterly transparent that I consider any further 
discussion of these matters with Jacob himself to be futile. But if Jacob has some super-
logician, some twinkling star in the academic sky, who in his sheer bureaucratic 
competence can put the brakes to the CTMU (or just to my "bad argumentation"), then bring 
him on. Of course, in my personal opinion, one might as well stick an apple in his mouth 
and hand over plenty of salt, pepper, and a bag of charcoal briquets while one is at it, since 
by the time I'm finished with him, he'll probably feel like everyone at the luau has had a 
piece, so to speak. For this, he'll have only himself and Jacob to thank.
I almost get the impression that Jacob may have heard some of my braver and more 
tireless critics refer to the CTMU as "bafflegab" and "word salad", claiming that it contains 

elementary mistakes in logic and/or set theory and/or [fill in the blank]. However, Jacob 
should probably exercise a certain amount of caution in the weight he ascribes to such 
critiques and their authors. You see, I've already met a number of those people on this or 
that Internet bulletin board, and if the truth be told, they didn't fare quite as well as they 
pretend they did, nor I so badly. If Jacob doubts this, then he's free to get one of the "really 
smart" ones, the most capable thinker and debater, to challenge me under his real name in 
a neutral forum. I predict that Jacob will have trouble finding any takers. 
Although the stated excuse will probably run something like "superbrains is a waste of 
time," "nobody listens to that crap!", "the guy's ideas are nebulous and vapid," "he reminds 
me of my five-year-old", "we used to rap about that stuff over a bong!", "Langan 
understands nothing about real science" and so on ad nauseam, the real reason - a simple 
fear of defeat - will shed no glory at all on the reputation or self-image of the excusemaker. 
Those whom Jacob considers to be his behind-the-scenes supporters, those shadowy 
knights of the intellect, simply don't want to be humiliated in public, and they know or 
strongly suspect that it would happen just like clockwork. They'd rather watch Jacob be 
humiliated instead. I advise Jacob not to get sucked into the very trap they've learned, by 
means of hard knocks, to avoid. Remember, even if I were nothing but a big dummy myself, 
the CTMU is tautological, and that means I couldn't lose an argument over it if I tried. 
Just a word to the wise. (Then again, Jacob might think he's treed me like a fugitive racoon, 
in which case his frame of reference is so alien to mine that I'm unable to advise him at all. 
In that case, I humbly beg his pardon and wish him Godspeed on the golden road to 
wherever it is he may be going.) 
 
Comment #15:
Jacob Aliet, determined to keep his error streak snowballing furiously downhill, claims that I 
think he has "super-logicians egging him on". If so, I hasten to point out that this was meant 
as sarcasm; I have no reason to believe that Jacob has any behind-the-scenes contact with 
anyone who actually understands logic. In response to my suggestion regarding a neutral 
forum, Jacob then alludes to the ARN board. While I can't recall losing a debate at ARN - 
quite the opposite, in fact - I personally do not regard it as a "neutral forum". In fact, I think 
that although it presents itself as an "ID-friendly" board, perhaps with the best of intentions, 
its policies and choice of moderators are somewhat inconsistent with this claim (which 
doesn't necessarily mean that things can't or won't change in the future). Jacob then 
accuses me of "intellectual bullying"; if being correct makes one a bully, I suppose he may 
almost have a point. 
Jacob then says that somebody calling himself "Dayton" is perceived to have won an 
argument against me at ARN. I merely recall rejecting Dayton's peremptory claim that logic 
could reveal nothing about the geometry of the physical universe (something that Dayton, 
despite what he and others may like to think, can neither know nor prove). If Dayton still 
maintains that he possesses knowledge regarding this claim, then the burden of proof 
obviously remains entirely on Dayton, and this is a burden that to my own knowledge 
Dayton has never met. Consequently, Dayton's attempted vindicatation of doctrinaire 
empiricism with examples from geometry has gained no credibility since I first questioned it, 
and this makes Dayton and his erstwhile activities at ARN old and somewhat boring news 

from my perspective.
Jacob goes on: "One central question to the CTMU which many would like to know is how 
you intend to demonstrate corresponsence between the syntax in your reality model and 
real life phenomena. Because pushing around symbols and not mapping them to real life 
objects that they are supposed to map to really doesnt mean much if the correspondence 
cant be established." This was explained in the paper, but perhaps Jacob requires a bit of 
repetition and/or simplification. Proof: Suppose that there is some degree of 
noncorrespondence between cognitive syntax and perceptual content (observed 
phenomena). Then there exist items of perceptual content which do not correspond to or 
coincide with cognitive syntax. But if these items do not coincide with cognitive syntax, then 
they are unrecognizable, i.e. inobservable (since cognitive syntax is by definition the basis 
of recognition). But then these items are not included in perceptual reality (the set of 
observable phenomena), and we have a contradiction. Therefore, perceptual reality must 
coincide with cognitive syntax. This establishes the existence of the mapping to which 
Jacob refers, and we can leave the details for later.
Still unsatisfied, Jacob then makes a curious offer. "Subject to your willingness, I can 
organize a one-to-one discussion with one person on the CTMU. What we will need to 
establish is a clear topic of discussion and perharps specific areas of interest - maybe two - 
regarding CTMU. We can have a Formal Debate at infidels - look at some debates and the 
rules and procedures there. If its acceptable to you, let me know and I will start the 
groundwork. I am sure someone will pick the gauntlet. I suggest Infidels because it has that 
unique facility for Formal Debates - no room for cheerleaders. I hope the mod here doesn't 
take offense at this."
Is Jacob by any chance referring to "Internet Infidels"? If so, I'm afraid that this is not a 
neutral forum by any stretch of the imagination. It contains participants and moderators 
known for belittling and vituperating ID supporters, often by name and often to what seems 
to be the verge of libel. I believe it is sponsored by an organization which champions an 
ideology called "metaphysical naturalism". Strangely, this organization seems to define 
"metaphysical naturalism", a phrase which explicitly refers to a meaningful relationship 
between metaphysics and nature, in a way particularly friendly to materialism and even to 
atheism, which would seem to leave the term "metaphysical" clinging to the term 
"naturalism" like an abandoned spider web or the empty shell of a long-deceased barnacle. 
(This is only my opinion, of course; it is in no way associated with ISCID or its 
administrators or moderators.)
Finally, Jacob goes after Jason Young, in the process adding to his error streak. "Chris has 
just admitted that he has no empirical support for the CTMU [error] - and that he and others 
are still looking for some. That being the status quo, the CTMU is a logical construction that 
promises to be able to solve the problem of induction (for those who think its a problem). [If 
Jacob fails to recognize the problematic nature of induction, then this too is an error.] Until 
then, we are pushing symbols on paper and making conjectures [error] - any six year old 
can do that." [Error number 4 in this paragraph alone.]
I hope that the above proof of the existence of a cognitive-perceptual mapping, which is 
straightforwardly based on the definition of "recognition" with respect to any meaningful 

model thereof, proves helpful to Jacob, RBH and others who have had trouble with it in the 
past. In return, would it be too much to ask that Jacob find somebody else to hound for a 
change (besides me and others whose only crime is to display a little understanding of the 
CTMU)? Thank you.
Comment #16:
RBH claims that the proof I offered "is related to the potential for conflicts of 'direct 
apprehensions' question that I also asked and that was not clearly answered." He then 
elaborates: "According to Langan's theory, direct apprehension of the world is one of the 
two ways of accessing 'Truth-with-a-capital-T.' (The other is logical derivation.) It supposes 
that (human) perceptual content is in the end a veridical representation of reality: we all 
share the same perceptual content and there can be no real perceptual conflicts, only 
apparent conflicts."
My answer, which RBH subsequently repeats, was this: "It can be shown that if reality is 
connected - if we are really united in an objective manifold of common perception - this level 
of syntax [governing the relationship among the direct apprehensions of numerous subjects] 
must exist. Remember, science is primarily concerned with the formulation of general laws 
of nature as required for prediction, and these are just structural and grammatical 
components of this distributed reality-syntax." The clause "it can be shown" refers to the 
section of my paper dealing with syndiffeonesis, and that was clearly written. The direct 
apprehensions of multiple subjects tautologically require a joint medium possessing a 
unified description distributing over the medium as syntax. So as far as I'm concerned, RBH 
has been answered clearly enough, and without benefit of supposition.
Regarding "the agency for resolving potential conflicts among direct apprehensions", RBH 
quotes me to the effect that this agency is "the distributed cognitive-perceptual syntax of the 
self-spoken language of reality (or in CTMU terminology, of SCSPL)." This, says RBH, 
"embodies my reservations about the CTMU: it is (once again) a syntactic theory, a theory 
of symbols manipulated according to formal rules on the basis of the symbols' shapes ... 
(but) the general laws of science are not merely syntactic: they have content." 
As I've previously made clear, the CTMU is actually a distributively self-referential theory 
which model-theoretically relates cognitive-perceptual syntax and its formal embodiment to 
perceptual content, which appears to us in terms of attributes conventionally called 
"physical observables". I've also made it clear that physical observables can be regarded as 
SCSPL predicates semantically bound by, and defined in terms of, SCSPL syntax. The laws 
of science, insofar as they can be logically certified, can be regarded as a (mathematically 
secondary) level of SCSPL syntax - that's what makes them "laws of science (as opposed 
to mathematics)" - and their parameters and contents are perceptual arguments of syntax 
bound by syntax. 
The whole point of the proof which RBH dismisses is that if physical observables were not 
bound by SCSPL syntax, then they would be unrecognizable within the structure of SCSPL 
reality. In the theories of computation and cognition alike, to be "recognized" is to be bound 
by the accepting syntax of an acceptor (i.e., by the computational or cognitive protocols of 
an accepting automaton or entity). On the other hand, no binding = no recognition, which in 

the perceptual realm translates to "no perception". So In effect, any part of reality unbound 
by the syntax of reality would be unrecognizable to any part of reality itself, and would not 
be included in perceptual reality for real entities bound by SCSPL syntax. 
RBH ends with the assertion that "Langan's existence proof above (subject to reservations 
associated with whether conflicts among direct apprehensions can actually be resolved that 
way) does establish that some mapping must exist, but it is the details that Langan says can 
be left for later that are in fact critical to assessing whether SCSPL's syntax is in fact the 
underlying syntax of the real universe. Without those details, that is, absent an explicit 
mapping from the symbols and operators of the syntax to the entities, processes, and 
relationships of the (perceptual) world, the CTMU floats in thin air, unconnected to what it is 
intended to explain. Without those details it is uninformative. Until those "details" are taken 
care of, it has no interpretation in terms applicable to the content of the perceptual world 
and thus no explanations."
Now we've arrived at the crux of what seems to be RBH's confusion regarding the CTMU. 
As I've repeatedly made clear, syntax is stratified, and the CTMU works from a high level of 
reality-syntax comprising certain tautological aspects of the relationship between syntax and 
the language thereby generated. As regards the basic structure of reality, nothing else need 
be supposed, assumed or conjectured. In particular, minute details of syntax need not be 
given. In fact, the CTMU is perfectly happy to leave many such details to empirical 
methodology. But this does not change the fact that empirical methodology is based on a 
model of reality which is artificially restricted with respect to SCSPL (in which it logically 
requires embedment), and is accordingly limited in its power to reveal "truth".
Thus, when RBH asserts that "it is the actual [detailed] mapping...that assigns meanings 
(interpretations) to the symbols being manipulated by the syntactic rules," he is not entirely 
correct. A certain level of meaning resides in syntax itself, and in the tautological 
relationship of syntax to language. To RBH's flat denial that anything of interest can be 
deduced about nature on that basis, I can respond only that as far as I'm concerned, it can 
be and has been. So although I respect RBH's right to disagree, I'm afraid that I consider 
him to have come out on the short end of this exchange.
Comment #17:
Parallel asks "where is Mr. Langan's SCSPL syntax? Apart from talking about an SCSPL 
syntax and tossing in a few symbols in the process, where in Langan's paper is the actual 
logicomathematical syntax? It seems that he merely describes what this syntax does, 
asking readers to assume that it exists and is logical without ever proving that it actually 
exists. Why argue over something that has never even been shown to exist? Where's the 
math? Where are the formal proofs?"
SCSPL syntax cannot be finitely enumerated. Nevertheless, as I've explained repeatedly 
above, it can be treated as a well-defined constant in its own right (provided that its 
existence can be established by means of syndiffeonesis and/or other concepts, something 
which has already been accomplished in plain view). This constant can then be implicated 
in relationships involving other variables and constants, e.g. a constant representing SCSPL 
as a whole. Given this fact, the PCID paper contains all the math I need to make my case, 

at least for those capable of understanding the kinds of math employed. As far as concerns 
formal proofs - that is, proofs in which concepts have merely been replaced with symbols - it 
is abundantly clear that any such proof would be thoroughly lost on anyone who has already 
displayed an inability to keep track of the concepts represented by the symbols. 
Regarding my IQ, and for that matter parallel's IQ, they are not at issue here. Moreover, I 
have neither the intention nor the obligation to provide personal information to anonymous 
strangers, much less to an anonymous stranger who has gone to what I consider bizarre 
lengths to establish an adversarial relationship with me. Regarding the limitations of IQ 
tests, that's a controversial issue well beyond the topic of this thread, in the context of which 
it constitutes a transparent attempt at diversion. 
Comment #18:
Parallel says "So the SCSPL syntax cannot be finitely enumerated, which means it cannot 
be enumerated." Not exactly; it only means that SCSPL syntax cannot be completely 
enumerated. The distinction between denumerability and nondenumerability in mathematics 
is very clear, and it has nothing to do with finitude. (Is anybody else wondering why parallel 
seems to have so much trouble with simple math concepts?)
Parallel then asks "How can an infinite syntax be known or evaluated?" As supporting 
examples of how SCSPL syntax can be referred to and implicated in mathematical 
expressions without finite enumeration, consider N (the natural numbers), R (the real 
numbers), aleph nought (denumerable infinity), sqrt(2) (an irrational number which 
evaluates to a nonrepeating decimal that cannot be finitely specified), and pi (a 
transcendental number that is the root of no algebraic equation). I could, of course, go on at 
considerable length, but as we can see, the quality of the objection merits no such effort.
Next, parallel complains that "it seems the SCSPL syntax is an inexpressible concept that is 
inferred to exist by way of other concepts. This sounds more like divination than logic, more 
like myth than mathematics." As the examples above make clear, logic and mathematics 
have no problem with concepts that are not finitely expressible. 
Parallel then bellyaches that in response to his inane and prosecutorial "where's the beef?" 
line of questioning, I called him "stupid" and threatened him with a "knuckle sandwich". 
Strangely, I have no such recollection. 
Finally, ignoring what he was told the first time around, parallel persists with his demands 
for personal information. Once again, I'm afraid that parallel is neither polite nor important 
enough to warrant any degree of compliance with his summary demands. But just so 
nobody can accuse me of not being helpful and modest, I'll gladly admit to having an IQ of 
80, maybe even less, if parallel will direct us to a single Brainstorms post of his which is 
addressed to me (or anyone else) and even comes close to complying with the user 
guidelines established for this forum.
I hope that parallel finds these answers satisfactory. But if not, who cares?
Now let's move on to another cynosure of constructive criticism, the irrepressible Jacob 

Aliet. Although I'm almost as tired of Jacob's meaningless and repetitive banter as I am of 
parallel, I'll take just enough time to call attention the following excerpts: 
"This is the first time I am actually experiencing Langans monumental word-twisting ability. 
He took my words,twisted their meaning and then attacked them in their novel form. I do not 
know whether he does this for his personal amusement or just for the cheap thrill of 
annoying people. I never suggested ARN as a neutral board and I never accused Langan of 
intellectual bullying. I made it clear that I was stating other peoples opinions."
[Jacob is evidently referring to his previous post, which contains another shining example of 
his constructive brand of participation here: "Many have complained of your intellectual 
bullying and others that your ideas are too nebulous to hold a discussion on. Almost 
inexorably, it seems to me, your ego comes to the fore and the discussion degenerates. 
Geometry = Logic? Not exactly thread comes to mind. Some say Dayton kicked some ***.]
The last time I checked, Brainstorms was not supposed to be about instigating trouble and 
fomenting discord through the importation of interpersonal hostility from other forums. 
Unfortunately, Jacob's post only deteriorates from there, alternating spasmodically between 
personal denigration and the supposed need to specify, contra Godel's theorems and a 
number of other mathematical relationships, every single teensy-weensy rule of SCSPL 
syntax down to the last iota. 
I've said it before and I'll say it again: the general mathematical relationship of syntax to 
language does not presuppose the exhaustive enumeration of syntactic rules, but treats 
them in the aggregate. If Jacob finds this to be a problem, then the problem exists solely in 
the minds of Jacob and other wide-eyed neophytes uneducated in linguistic and 
grammatical algebra. And whether Jacob likes it or not, this is as cold, hard and irreversible 
as a fact can be.
As we all know, constructive and insightful criticism can help one to refine, clarify or correct 
one's ideas, and I've never held myself forth as an a priori exception. But sadly, this is not 
the kind of criticism we've been seeing from Jacob. There's an old saying attributed to Mark 
Twain: "It is better to be silent and thought a fool than to open one's mouth and remove all 
doubt." I can only suggest that Jacob consider whether it might, in some small way, apply to 
him.
Organisms Using GA vs. Organisms being built by GAs
Comment #1:
No, charlie d. (Why does this sentence ring so many bells for me?)
In physics - the root of "physics", "physis", means "nature" in Greek - energy is the capacity 
to do work, or action over time. Work is force times the displacement of a chunk of matter in 
the line of action.
Quantum mechanics is a big part of physics. But in the coherence phase of the wave 
function, there is no observed chunk of matter on which to perform work (to observe the 

matter would be to collapse the wave function). You can take it on faith that the matter is 
there in compact form, e.g. riding a "pilot wave", but you can't back it up scientifically. That's 
because according to quantum mechanics, its concurrent properties must commute, and 
due to uncertainty, this rules out half of the observables that we ordinarily require matter to 
possess. Anything analogous to energy in the coherence phase would be influencing 
probability, not matter; it is pregeometric in Wheeler's sense of the term. We can call it 
"meta-energy" if we like, but it doesn't fit the physical definition of "energy" per se.
Obviously, given the etymology of "physics", the inclusion of quantum mechanics in physics 
implies its inclusion in nature. Therefore, that which is implied by quantum mechanics, 
including meta-energy, is not "supernatural".
Regarding your exhaustive classification of ID theories, that too is incorrect. In my theory, 
nature is self-selecting and cannot be distinguished from God on any but the most 
superficial level.
It makes no difference to me, by the way, how many ID proponents (or non-proponents) 
have or have not thus far been perspicacious enough to invest in my theory. It suffices for 
you to know that I'm a fellow of ISCID, that my theory posits a form of "intelligent design", 
and that it defines nature in such a (comprehensive) way as to require no supernatural 
intervention of any kind.
Just a little reminder.
Chris
Comment #2:
Gedanken: “If one is dealing in “natural causes” then one can deal with empiricism. One can 
deal with observation of nature.”
Unfortunately, it cannot be scientifically proven that everything relevant to nature, all of 
which should of course properly be *included* in nature, can be *differentially observed* in 
nature. One can recite the scientific method like a litany, but that doesn’t help.
Empirical methodology fails to confirm higher-order descriptions (of the structure of reality) 
because of the observational requirement that structure must resolve to the object level. 
Because it would be improper to prejudicially limit "nature" to the object level when any 
meaningful description of nature contains higher-order properties and relationships not 
necessarily requiring object-level resolution – you can make such an assumption if you 
want, but being able to back it up scientifically is quite another matter - empirical 
methodology holds only a limited amount of water.
But that's really not so bad, because science already makes heavy use of rational 
methodology in the formulation of "empirical" hypotheses, and of course, in the formation of 
theories. So we can simply widen our definition of "science" and "nature" to include the 
mathematical sciences and mathematical truth. Scientific methodology would thus be 
extended to accommodate higher-order causation.

“If intelligence is not “beyond natural” or “non-natural” or “extra-natural” then we should also 
be able to analyze intelligence itself empirically.”
Only if we admit the domain of definition of "intelligence", which is by definition rationalistic 
(that’s what intelligence means) as opposed to empirical, as a legitimate part of "science". 
Sadly, the majority of scientists have not yet opted to do this. But until then, no amount of 
objection will prove that science must admit only the behavioral correlates of intelligence.
Personally, I’ve always found it a bit incredible that people as intelligent as scientists want to 
exclude from science the domain of definition of that which allows them to do science. But 
for some reason on which I’m apparently not qualified to comment, it appears that this 
remains the case.
Chris 
Comment #3:
quote:
”Precisely. Exactly what I meant: therefore, zero energy can do exactly zero physical work. 
Now, if you try to put yourself in the shoes of an omnipotent intelligence (I am sure that 
won't be so hard ), and you have to impart some information change to some physical 
being, there is simply no way to do that without doing some physical work (may that be 
mutating a single nucleotide to allow an adaptation to ensue, or assembling all-grown 
humans from mud and spit, or telling a bearded man on a mountain what your laws are). 
Unless of course, you use some supernatural method that is not subject to the restictions of 
the laws of physics.”
I'm afraid you're missing the point again, charlie. The "work" you're talking about requires 
something analogous to energy that acts on probabilities in coherent wavefunctions. It is not 
"physical energy" in the classical sense, and therefore is not necessarily apportioned by 
distributed laws of physics. Therefore, its effects may be localized and nonreplicable.
quote:
”This ultimate "free lunch", ironically, seems to be the only proposal for a design mechanism 
mainstream ID theorists like Dembski could come up with.”
This is not a "free lunch", charlie. The word you may be looking for is "teleology". (As for 
Bill, didn't he name his book "No Free Lunch"?)
quote:
”I guess one can also envision some sort of "cosmic" front loading in which it is the laws of 
physics themselves that determine everything else, but that's apparently a no-no to 
Dembski and most other IDists (it would be indistinguishable from naturalism, after all).”
Speaking just for myself, I usually base my assertions on logic, so I usually forego what you 
might consider the obligatory opinion polls among ID theorists. But I will say that in its 
logical structure, front-loading is quite distinguishable from philosophical naturalism, which 

offers no explanation whatsoever for the laws that are being teleologically front-loaded. If 
philosophical (or methodological) naturalists themselves can't see this due to their restricted 
definitions of "nature", that makes little difference to those with broader perspectives 
embracing logic and nature to their full extent.
quote:
”As for your theory, I can't say I understand it, but if you say it's ID, then I believe you. It's 
however unclear to me if you ever proposed any physical mechanism for your cosmic 
model; as far as I could tell, it was a purely logical construct. I should note also that 
membership in ISCID does not necessarily make one an Intelligent Design proponent. Ask 
Micah.”
Of course, I realize that you do not understand my theory. But as a matter of record, it was 
explicitly presented as a theory of intelligent causation before the latest public round of this 
controversy even began in earnest. And regarding physical as opposed to natural 
mechanisms, the scientific verifiability of such distinctions and restrictions on "mechanism" 
is precisely the issue at hand.
quote:
”Now, going back to the real substance of the issue being discussed. LGT, mutation and all 
the other ways that a designer could act are of course natural mechanisms, but that's not 
the point. The real problem is to explain how the designer implements them. For instance, I 
can intelligently mutate a gene in bacteria or even mice, but that's not supernatural, 
because I can give you a detailed protocol to do it that does not defy the laws of nature. If 
the designer does the same, ID proponents should spell out how.”
Maybe, but you certainly cannot give me a detailed protocol that verifiably does not defy the 
laws of probability in any finite model of nature. As for me, I've already spelled out a new 
class of mechanism. I've explained that a certain model of reality contains a kind of 
"mechanism" called "telic recursion" that involves pregeometric feedback between, and 
mutual adjustment of, (more or less localized) physical laws and states in the maximization 
of intrinsic utility. I've explained that it involves certain properties and processes of a new 
kind of causal manifold. And I've explained that telic-recursive mechanisms are natural 
mechanisms to exactly the extent that logic and mathematics are "natural".
As information about this model is published at ISCID and elsewhere, you'll have the same 
access to it as anyone else (should you wish to avail yourself of it). Meanwhile, I reserve the 
right to interject when uninformed parties assert its nonexistence.
Chris
T-Duality Universe
Comment #1:
Parallel claims that UBT paradoxes have been articulated. However, "paradox" is a rather 
exacting concept, and as I've already pointed out, it is to be carefully distinguished from 
items of confusion in the minds of people still struggling with basic CTMU concepts (on 

which I'm probably the final authority if only because I constructed the theory). What one 
would have to do to show that UBT generates a paradox is involve it in a statement of the 
form "x = not-x", prove that the CTMU assumes or logically implies this statement, and then 
show that the statement, and the predicate x around which it is formulated, cannot be 
consistently interpreted in any model of the context to which it refers. This kind of thing is 
always a tall order, and in the present instance, it would require considerably more 
understanding of the model under discussion than I've seen from certain of its critics.
In fairness to Parallel, he seems to attempt to specify a paradox when he opines that "the 
undefinable" cannot have well-defined properties such as “unbound”, “without restraint”, and 
“zero extension and duration”. But this attempt is a bit hard to figure, since if the property 
"undefinable" is well-enough defined to be contradicted by the properties “unbound”, 
“without restraint”, and “zero extension and duration” as Parallel maintains, then it is well-
enough defined to be described by them as well, particularly with respect to a model 
involving syntactic and presyntactic stages. Because Parallel does not take account of such 
a model, he can't be talking about the CTMU. What Parallel is talking about, only he knows 
for sure.
Parallel then goes on to ask "If defining the undefined is not paradoxical, pray tell what is?" 
To this question, one can only answer that trying to pin any discernable measure of extent 
or duration on the undefined would be paradoxical indeed. Where no extension or duration 
can be discerned (because they are undefined), none might as well exist. The situation is 
really very simple: where things are referentially mapped onto a syntax-free domain, no 
difference can possibly be discerned between them with respect to that domain (although 
they may be distinguishable in another domain with a syntax which supports spatial, 
temporal and other distinctions). The measure of difference between them is just the 
measure of distinguishing information or constraint (0), and because information is the 
currency of science, that's a scientific fact. 
Parallel then asserts that standard big bang theory "does not seem to generate such noisy 
paradox". Unfortunately, for all of its strengths, the Standard Model is as full of holes as a 
Swiss cheese factory. The only way it can appear free of paradox is if one refrains from 
looking at it too closely.
Parallel opines that there is no need for a final TOE in contemporary ID inquiry, holding that 
the current purpose of ID inquiry is to set criteria for the validation and falsification of ID 
hypotheses. For some of those in the ID movement, this is no doubt true, but in any case, 
establishing such a criterion is as much a matter of philosophy as of science. Truth and 
falsity, validation and falsification, are problematic concepts. One can take any of a number 
of approaches to these concepts, including those associated with verificationist, 
falsificationist, identity, correspondence, coherence, pragmatist, deflationary and other 
theories of truth, and come up with any number of positions regarding what makes a 
proposition valid or invalid. Again, what it comes down to is that one needs a global, 
fundamental model in order to fundamentally define (and answer questions about) concepts 
like truth, existence and causality, and to this model corresponds a TOE grounded in logic 
and philosophy. 
Parallel states that "if we find a complex machine (like DNA) and wish to determine if it was 

created by some intelligent designer, it is not incumbent upon us to explain where that 
designer came from or what he looks like or what he had for lunch." Looks and lunch menus 
aside, it is in fact quite important where the designer came from, for by direct causal 
regression, the ultimate source of that which is designed is identical to the source (of the 
source...of the source) of the designer. Without this level of inquiry, one cannot hope to 
understand the true character or extent of the designer's "intelligence", or even whether 
"design" is an appropriate concept. In the final analysis, such questions cannot be 
separated from questions regarding the designer's nature and origin.
Comment #2:
Some of the posts in this thread touch on the concept of "physical infinity". Aside from the 
question of whether infinite space is paradoxical a priori, physical counting processes are 
finitely limited. (To understand why, just ask yourself how long it would take you to count an 
infinite number of small physical objects.) Physics is constructive by nature; that which is 
logical but cannot be finitely constructed is metaphysical in the analytical sense. Thus, 
although infinite nestings of sets and spaces are sometimes assumed to exist in the realm 
of pure mathematics as products of transfinite induction, it has never been shown that this 
abstraction possesses a physical analogue.
Transfinite geometry preceded transfinite arithmetic by millennia. Long before Georg Cantor 
discovered transfinite arithmetic, the ancient Greeks were wondering about an infinite 
cosmos and the flight of arrows along infinitesimally converging series of intervals. General 
solutions for these problems (if not their corresponding notations) were offered considerably 
before Cantor, and Cantor's work arguably adds no physical or philosophical, as opposed to 
algebraic, insight. In any case, certain problems that existed before Cantor made the scene 
remain controversial to this day, and the physical sciences still lack anything resembling a 
blanket justification for the physical application of transfinite algebra (a criticism that does 
not necessarily apply to the CTMU). Therefore, it seems questionable to single out 
anybody's theory in particular, especially the CTMU, for special criticism because some 
particular person feels that he or she has not been satisfactorily edified regarding the 
troublesome problem of infinity.
To some extent, the situation can be viewed in terms of the distinction between perceptual 
reality, consisting of that which can be perceived or experienced, and cognitive reality, i.e. 
that which can be conceived. Perceptual reality is finite (constructive) with respect to 
perceptions involving countable processes and the spatial or objective components thereby 
enumerated. Physical constraints or laws, on the other hand, can be infinitary, involving 
infinite series, differential equations and renormalizations. But the whole point of non-finite 
quantities and processes is that they cannot be actualized in any completely finite, 
physically countable way. Thus, infinitary laws of nature are just what any decent logician 
knows they are: metaphysical ingredients of a cognitive metalanguage of perceptual reality. 
The CTMU merely develops the implications of this distinction.
In a qualified sense, I agree that we need a rigorous definition of the relationship between 
unbound and bound (actualized) potential. To some extent, there already is one; for 
example, the CTMU rigorously applies a strong self-containment criterion of which other 
theories fall abysmally short. However, the idea that everything can be reduced to 

"mechanics" is highly dubious, especially insofar as it seems to preclude any explanation of 
the genesis of mechanics itself. It is also highly dubious to speak of a "transformation" or 
temporal "transition" from unbound to bound "states", since prior to binding, nothing is well-
enough bound to coherently serve as a "state" or any other kind of transformational 
argument. A major point of CTMU cosmogony is to let the logical SCSPL universe serve as 
its own transformational argument, and to eliminate the need for an initial "transition" in the 
usual simplistic sense. The ontological potential described as UBT brings nothing into being 
that was not in a way "already there"; in the sense adumbrated by John Wheeler, it is 
pregeometric = pre-spatiotemporal = pre-temporal and not itself subject to temporal 
ordering operations, all of which presuppose syntactic coherence and are necessarily 
intrinsic to SCSPL systems. A salient feature of CTMU cosmogony is that the pre-temporal 
cosmic potential of any particular SCSPL universe fails to coherently distribute over UBT, 
and is therefore not distributively identifiable with UBT.
Regarding the assertion that the actual "cannot create itself because this transition is 
*primary* i.e. before any type of *temporal* feedback can occur, or timeline is created," this 
would seem to be a profoundly unwarranted statement. By definition, a primary 
transformation (in a sequence of evolutionary transformations) is merely a self-contained 
transformation that does not require external prompting or an external antecedent, and this 
criterion is satisfied by a reflexive transformation in 3-way coincidence with its domain and 
codomain. If the point of coincidence "was always there" in the form of an externally 
unactualized, unevolved atemporal potential, then no external transformation is required. 
With anyone who fails to see this, I really don't know what more there is to discuss. Suffice it 
to say that any such person is stuck with a set of intractable paradoxes that he can neither 
resolve nor constructively circumvent, and that such a person is obviously in no position to 
criticize anybody else's means of circumvention.
That being said, I couldn't help but notice the ongoing disagreement regarding UBT and n-
valued logic. UBT is not a logic; that is, it lacks any distributed syndiffeonic structure. It 
makes no difference how many values one tries to pin on it; nonlogic is nonlogic. Instead of 
a syndiffeonic interplay of freedom and constraint, there is only freedom. (We can, of 
course, define "nonlogic" by applying the negation functor of logic to logic itself, something 
that is logically necessary due to the self-referentiality of logic. Deny this, and logic 
becomes non-self-referential even in principle, and thus cannot be used to explain itself or 
anything that it characterizes.)
What about logical systems? For the expression of structure, 2-valued logic (2VL) is a 
necessary and sufficient criterion. In any structured (syndiffeonic) system, everything finally 
comes down to 2VL. We can come at this fact from below and from above. From below, we 
merely observe that because 0VL and 1VL do not permit nontrivial distinctions to be made 
among syntactic components, they do not admit of nontrivial, nonunary expressive syntax 
and have no power to differentially express structure. From above, on the other hand, we 
note that any many-valued logic, including infinite-valued logic, is a 2-valued theory - it must 
be for its formal ingredients and their referents to be distinguished from their complements 
and from each other - and thus boils down to 2VL. So 2VL is a necessary and sufficient 
element of logical syntax for systems with distributed internal structure. Infinite-valued logics 
can add nothing in the way of scope, but can only increase statistical resolution within the 
range of 2VL itself (and not individual resolution except in a probabilistically inductive 

sense).
For those unfamiliar with the CTMU concept of syndiffeonesis - and it has already been 
explained at some length - it captures the interplay of freedom and constraint in a relational 
context. Synesis is distributive, deterministic sameness with respect to a coherent 
antecedent constraint such as a definite combination of law and state (and corresponds to 
an algebraic identity), while diffeonesis comprises differences among possible 
consequential constraints. This reflects the fact that bound telesis is bound freedom, and 
that imposing a constraint on freedom, i.e. logically binding it and conspansively bounding it, 
cannot destroy the essential nature of that which is bound. This is why the wave function is 
the fundamental unit of reality in quantum mechanics; the multiplexing of possibilities in the 
wave function is just absolute freedom expressing itself (through symmetric diffeonesis) 
within the bounds of synetic constraints consisting of laws and states. The wave function 
expresses the freedom of UBT within a distributed mathematical constraint consisting of 
2VL and Hilbertian operator algebra. It is by virtue of syndiffeonesis that we can speak of 
the intrinsic structure of a medium. UBT lacks syndiffeonesis because it admits of no 
coherent distributed (syntactic) constraint; from the viewpoint of 2VL systems which take 
"singular" form with respect to it, freedom is all that it has in any distributive sense. 
I can't help but be a bit taken aback when people who talk authoritatively about quantum 
mechanics, uncertainty, infinite-dimensional Hilbert spaces, infinitely-nested spacetimes 
and other clear manifestations of absolute freedom rail against UBT. Where do they think all 
of that freedom comes from? From constraint? Constraint is deterministic. Even if we posit 
the existence of a primal constraint which drives a deterministic many-worlds (relative-state) 
multiplexing of state and thus "creates freedom", what accounts for the nondeterministic 
(telic or aleatory) mapping of our consciousness into one particular history among the 
infinite possible histories thereby "created"? Again, we are forced to confront an inevitable 
fact: freedom is built into the microscopic structure of reality, and because the wave function 
is a stratified affair ultimately embracing the entire cosmos (regardless of the number of 
ulterior spaces in which our immediate reality is nested), it is built into the macroscopic 
structure of reality as well. Quantum mechanics, syndiffeonesis and UBT go together; 
remove any one of them, and all that finally remains is a teetering pile of paradoxes waiting 
to collapse.
The bottom line, I suppose, is that insofar as mainstream theorists can't even come close to 
explaining cosmogony or difficult related concepts like infinity, they and their supporters 
have no business specifically targeting theorists who have chosen not to be limited by their 
scientifically obstructive conceptual handicaps. In a word, people in theoretical glass houses 
shouldn't throw stones, or delude themselves (or others) that the CTMU fails to shed light 
on problems that more popular theories fail to even address. The only way that someone 
could possibly make such a claim is if he understands virtually nothing about the CTMU. 
Comment #3:
Rex Kerr opines that since demonstrating the unique existence of the incomprehensible is 
probably not possible, "using the singular 'it' to refer to Tao-UBT should be taken as a 
linguistic convenience only and not indicative of anything about Tao-UBT." 

How true. Mathematically, it is indeed convenient that variables can be defined regardless 
of multiplicity of content, and can thus be referred to as “it”. But on a deeper level, the UBT 
issue is all about whether the negation functor of logic can be applied to logic itself to yield 
non-logic, which together with logic would approximate UBT. If not, then logic has no 
complement and must be taken as a primitive constraint. This pretty well captures the 
falsificationist viewpoint, according to which logic is tautological and therefore trivial, in light 
of which some people may find it shocking that (e.g.) their digital computers and all of the 
symbolic reasoning that went into their development and everything that they can be used 
to calculate and simulate are “trivial”.
But Rex's viewpoint leads to a couple of even more dire problems. (1) That which has no 
complement is indistinguishable from its complement and therefore contains zero 
information. But if logic has no informational value, then neither does logical consistency. 
And if logical consistency has no informational value, then consistent and inconsistent 
theories are of equal validity. Oops...goodbye, math & science! [Don’t you think that maybe 
it would be better to take something else other than logic as "that (primitive entity) which has 
no complement", and that maybe this trans-logical, pre-logical something should be taken 
as a mathematical criterion of meaningful scientific theorization?] (2) If logical functors 
cannot be applied to logic as a whole, then logic cannot serve as a metalanguage of logic. 
But then logic is in no way self-explanatory, and we’ve run into an explanatory brick wall 
due to which certain urgent questions, including questions at the very heart of the ID/neo-
Darwinism controversy, may be unanswerable. After all, if logic cannot in any sense be 
logically explained, then neither can that which is necessarily formulated in terms of 
logic...e.g., everything of interest to science.
Rex then guesses that the structure (of UBT) is more like that of a filamentous mold-like 
spot of comprehensibility, which by analogy one could imagine being imbedded in an infinite 
sea of incomprehensible cheese. “Ah”, he asks, “but is it Tao-UBT-cheese, or Tao-UBT-
bread?” All that one need add to this colorful and witty metaphor is a bottle of vino and a 
few paintings, and one has an art gallery buffet! 
Rex then owns to being amused by these discussions “since for the most part postulating 
extra incomprehensible stuff has no impact on anything we do or experience, since you 
could replace it with different stuff (or no stuff) and we'd never know the difference.” But of 
course, this says more about the limitations of current scientific inquiry than it does about 
the logical structure of scientific theories. In fact, the question of whether the universe is free 
or deterministically constrained is clearly relevant to questions like biological evolution and 
free will, and the relationship between freedom and constraint ultimately comes down to that 
between UBT and logic. 
Comment #4:
Rex Kerr notes that some questions may be unanswerable, including questions involving 
biological evolution and free will. While this may be a reasonable starting hypothesis - we 
do, after all, have undecidability, uncertainty and various kinds of model-theoretic ambiguity 
to consider - it requires rational or empirical support with respect to any given question. Until 
such support is given, it implies nothing about the structure of the universe, particularly with 
regard to which questions can or cannot be answered, easily or otherwise, by rational or 

empirical means. It is probably more constructive to concentrate on what the universe, 
broadly including logic and mathematics, can tell us, at least until we have the means to 
define hard epistemological limitations of the kind that Rex seems to anticipate. This, I think, 
is more in keeping with the pursuit of knowledge and the spirit of scientific inquiry.
Regarding Rex’s mold metaphor, he is in a better position than I to affix the mold labels, the 
cheese labels and the bread labels. It is, after all, his metaphor. However, since UBT and 
SCSPL are my concepts, I’m probably the one to say whether or not his metaphor holds 
water relative to their relationship. Unfortunately, I’m afraid that while I find the metaphor 
droll, I just don’t see much illustrative value in it. Nor, I daresay, would many Buddhists.
Rex conjectures that logic may not be completely explicable by logic, and that as comforting 
as the putative global and universal applicability of logic might be, it unnecessary for doing 
science. As I’ve already pointed out, this merely sets a limit on the kind of science that Rex 
is content to do. Although Rex states that “practically everything we use science for can be 
done with systems that are locally consistent and/or locally logical”, local logic and local 
consistency, even if they can be coherently defined, are inadequate with respect to 
answering questions of a global or fundamental nature, e.g. questions regarding existence, 
origins and causality. Thus, according to the limits that Rex has imposed on science, 
ordinary scientists would have no business attempting to address such questions, and 
should leave questions about the true nature of biological origins and evolution to theorists 
who are less limited in their approach. (Then again, I suspect that many scientists would 
prefer to question their limitative assumptions rather than suffer the epistemological 
consequences of failing to do so.)
Rex professes a lack of understanding as to why not having a complement is the same as 
being indistinguishable. The standard answer, of course, is that since information always 
restricts (or constrains) a potential by eliminating its alternatives therein, nothing to which 
informational value can be attached lacks a complement (in some probability space). For 
example, since observing that something exists is to rule out its nonexistence - existence 
and nonexistence are complementary states, provided that we conveniently classify 
nonexistence as a "state" - such observations distinguish existence from nonexistence and 
thus have positive informational value. On the other hand, that to which no information at all 
can be attached cannot be said to exist, and is thus indistinguishable. Because this applies 
to consistency and inconsistency, it also applies to logic and nonlogic.
Rex then presents the example of wavelength, saying that while it “doesn't have a 
complement, it's a perfectly usable quantity.” However, wavelength, like any other quantity, 
has meaning only with respect to a perceptual and cognitive syntax according to which it is 
defined and measured. First, it is not clear that the predicate “wavelength” cannot be 
considered to have a complement in this syntax, e.g. a complement consisting of all other 
predicates in that syntax (and so on). Secondly, what we’re really talking about here is the 
complement not merely of a predicate, but of an entire syntax, and we’ve already explained 
why such a complement is necessary. Specifically, we explained that if the syntax of logic 
were to have no complement, then consistency and inconsistency would be impossible to 
distinguish either locally or globally, and concepts like “truth” and “science” would be 
meaningless. Because the truth predicate is global by nature and cannot be locally 
confined, it gives way not to “localized truth” when cut off from global criteria, but merely to 

uncertainty. Conversely, anything about which an observer can be locally certain relies on 
global syntax, i.e. on mental syntactic structures that he or she cognitively and perceptually 
distributes over everything that he or she thinks and experiences. 
Rex then states that "'logic' and 'not-logic' is a contradiction, under logic, so if logic admits 
both 'logic' and 'not-logic' then logic is self-contradictory." Not if it treats nonlogic as 
something which is excluded by logic in any given model, for example a nondistributive 
lattice. He then observes that “there's no problem with having non-logical statements, just 
allowing the entire theory of logic and the theory of not-logic to simultaneously exist in the 
same model.” Although I see where Rex is coming from, logic and nonlogic can in fact exist 
in the same model, e.g. a nondistributive lattice, provided that nonlogic does not interfere 
with logic in that part of the model over which logical syntax in fact distributes, e.g. the 
Boolean parts of the lattice. That the non-Boolean parts of the lattice approximate poorly-
understood relationships among Boolean domains is irrelevant to the value of such "non-
logical" models, as we see from the fact that nondistributive lattices permit the 
representation of real noncommutative relationships in quantum mechanics.
Comment #5:
Russell Rierson says that "The (UBT) potential has no cognitive machinery to recognize its 
refinement from ... ‘itself’,..."
Since UBT is not to any extent refined except within an SCSPL domain, it needs cognitive 
machinery to recognize itself only within that domain. And that’s where the cognitive 
machinery is in fact present, right there in the self-refining SCSPL domain. Because SCSPL 
syntax fails to distribute over UBT, neither does the associated machinery, but it doesn’t 
have to. It need only coincide with the SCSPL which is undergoing self-configuration. Since 
I don’t know how I can make this any clearer, I request that if you still don’t understand it, 
you think harder about what has already been explained regarding it (including a bit more 
that I’ll write below). 
Russell continues: "...and the refinement can only infer the existence of this most 
fundamental *ontological groundstate* quantity. We know from discussion that the unbound 
potential is not equal to bound potential because they are defined as two separate things,..." 
No, UBT and SCSPL are not defined as "two separate things", any more than a chunk of ice 
floating in a pond is a "separate thing" from the water in the pond. The water in the pond is 
where the chunk of ice came from and what it is essentially composed of, but the crystalline 
lattice structure of the ice does not distribute over the pond, and the water in the pond is not 
distributively bound by this structure. The molecules in the liquid-phase H20 have more 
degrees of freedom than those in the ice; they are less constrained, and less bound. All that 
you need do in order to apply this analogy is to take it to its logical conclusion while 
generalizing your usual idea of containment, replacing ice with SCSPL, the pond and its 
molecules of liquid water with UBT, and the crystalline molecular lattice of the ice with an 
SCSPL logic lattice, and to relax your grip on the tidy little picture of a chunk of ice with 
extrinsic measure bobbing around "in" the water. Any metric imputed to UBT must be 
intrinsically derivable within SCSPL domains (e.g., by intrinsic mutual exclusion).

Russell concludes: "...yet the unbound potential is described as the "ontological ground 
state" of bound potential."
Yes, just as a pond full of water in which a chunk of ice forms by an intrinsic phase change 
can be regarded as the "ontological groundstate" of the chunk of ice. Other than the need 
for and the implications of intrinsic metrization, this is a very straightforward concept, and it 
is absolutely central to the development of a logical model spanning the existence of logic 
itself. Because this is as simple as it gets at the level of concreteness you seem to desire, 
your only choices are to reject the CTMU on what I've already shown to be illogical grounds, 
or surrender a little concreteness. It’s your choice.
Virtues of Scientists 
Comment #1:
“For example, here are some documents that probably should be added to prestigious 
physics journals. Don't you think so? (Or how would you decide whether these should be 
printed in prestigious physics journals?)”
Having taken a look at the page to which this was linked, I don't mind saying that I find it 
amusing. But as it happens, the joke isn't just on the author; it's also on prestigious physics 
journals.
The above quote links to a page containing an article asserting that motion through 
spacetime is impossible. While the author's tone leaves a bit to be desired – for example, 
he seems to want to dump poor Stephen Hawking right out of his wheelchair and make him 
apologize to the world for believing in time travel - he's got something resembling a valid 
point. Motion through spacetime is a problematic concept, and because time appears as a 
quasi-spatial dimension in standard continuum models of physical reality, physics is largely 
where the problem resides. Although conceptual workarounds exist, the author notes that 
they are far from satisfactory...and he's right. 
If the author were so-disposed, he could try to clean up his act, dress the problem up with 
some stylish jargon and get it into a prestigious physics journal as a topic for future 
exploration. But something tells me that no matter how much cleaning and dressing up he 
did, such a paper would probably be rejected. The problem is too fundamental and 
extensively ramified, and keeping the intractable conceptual deficits of prevalent theories 
out of prestigious physics journals falls under the mandate of standard ass-covering 
procedure...at least when the one raising his voice is just a small fry who hasn't earned the 
right to criticize or poach on territory staked out by "real" scientists. (On the other hand, 
while they seldom use it, dues-paying bigshots are typically given more leeway in such 
matters.)
What about publishing in a philosophy journal? Unfortunately, although the author might 
have better luck there, many scientists dismiss philosophy as irrelevant to science. This 
position is obviously nonsense; explicitly or not, science progresses entirely within the 

framework of analytic-linguistic philosophy and can under no circumstances afford to 
divorce it, least of all when addressing fundamental issues that may exceed the limitations 
of its own empirical methodology...issues like cosmology, the interpretation and/or 
extension of quantum theory, and the true nature of physical entities, attributes and 
processes including time, space, matter, energy and causality. 
It is not, of course, virtuous to arrogate problems of depth and/or scope beyond the capacity 
of one’s methods and conceptual models. Using science to attack such problems is 
allowable and even courageous, at least within empirical (perceptual) limits; tuning out or 
denying a hearing to anyone who attacks them using the purely analytical methods of 
philosophy and mathematics is somewhat less admirable, particularly when science would 
be impossible without these very methods. Where nature obeys laws of physics that are 
logical and mathematical in form, logic and mathematics would seem to be natural 
ingredients of the physical universe with even greater generality, and greater deductive 
potential, than the laws of physics.
Fortunately, not all scientists lay claim to more acreage or harder ground than they can 
fruitfully till. Many realize that the limits of empiricism are not necessarily those of the 
natural world, and remain open-minded toward the possibility of discovering truths about 
nature by rational means. I don't think it's too much of a stretch to say that this is a virtue to 
be emulated.
Comment #2:
Gedinken comments that:
“Referring to the article Nasty Little Truth About Spacetime Physics Chris Langham [sic] 
suggested that the author of the web page could ‘dress the problem up with some stylish 
jargon and get it into a prestigious physics journal as a topic for future exploration’.
Could I ask what the useful ‘problem’ was, in some detail, that was reasonable for a 
prestigious physics journal? The problem in my reading of Chris’ statement is that the claim 
can be made convincing when readers read this post here, but Chris’s post doesn’t have to 
actually put up anything really worth publishing in a physics journal to make his point sound 
good here in this thread. But can Chris actually describe the detail that was worth publishing 
in a prestigious physics journal, explaining in some convincing detail how it is actually an 
interesting physics problem?”
The problem to which I refer was clearly indicated. It’s the concept of progress along a time 
axis in a spacetime manifold. What the author is trying to get at in his own belligerent way is 
just that time has no place in geometric formalisms including the geometric formalism of 
physics. As long as time is represented in physical geometry as a spatial distance - and 
space is what geometry is all about - this problem will persist. So this comes down to a 
rather bad example of ideas that are so “cranky” as to be physically irrelevant, and it points 
(somewhat ironically) to the readiness of certain scientists to stonewall or laugh relevant but 
risky or troublesome material out of their journals. 
Comment #3:

“That's one potential route to respectability, and could serve both the scientific enterprise 
and the ID enterprise.”
It's hard to say that more communication between the neo-Darwinist and ID camps wouldn't 
be a good thing. (Of course, communication doesn't always work when one or both sides of 
a conflict already have their minds made up.)
However, as I understand the ID agenda, the scientific and ID "enterprises" are one and the 
same (at least when it is duly acknowledged that science embraces the mathematical as 
well as the empirical sciences). Any other assertion would imply that science has necessary 
inbuilt restrictions which exclude ID-type hypotheses, or that ID is somehow exclusory of 
science. Of course, this has not yet been established. Nor has it been established that it is 
properly up to ID to win the approval of those who believe in such restrictions. 
 
Comment #4:
“But I certainly agree that ID should aspire to such goals as Chris noted. Perhaps if we point 
out the logical inconsistencies and inconsistencies with observation then ID advocates can 
make repairs (in such places of failure thereof) so as to meet those restrictions of science 
and have greater probability of becoming publishable material.”
ID has been defined in a such a way as to satisfy valid scientific criteria. Any criterion that 
its definition fails to satisfy is of dubious validity and therefore less than scientific, e.g. the 
assumption that all truths about nature can be meaningfully reduced to first-order predicates 
transforming either randomly or by Markovian laws of physics. In principle, logic permits the 
consistent formulation of natural truths for which this is simply not the case, and that puts 
these hypotheses beyond the reach of standard scientific methodology (but not necessarily 
beyond the reach of mathematics). A case can be made that this problem resides in the 
unrealistic restrictiveness of the scientific method rather than with the hypotheses in 
question.
Regardless of the shortcomings of the "many ID presentations" to which you refer, the 
fundamental thesis of ID theory is perfectly consistent with the observational evidence. On 
the other hand, many scientific theories do not satisfy the mathematical requirements of 
logic. One of the logical criteria of which scientific theories tend to fall short is that of having 
a model. Although ID theory is often criticized for this reason, it is no less true of many other 
fields of science, including even the most fundamental. Those familiar with the problems 
bedeviling various branches of physics and cosmology know this quite well; for example, 
the seeming absence of a joint interpretation of classical and quantum mechanics leaves a 
gap, virtually devoid of known causal principles, between the microscopic and macroscopic 
scales of physical reality...scales that biological evolution happens to bridge. It follows that 
less fundamental branches of science, which rely on physics for their understandings of 
nature and causality, also lack consistent models. Among the things that scientists don't yet 
know is whether a consistent model of nature and causality will permit certain facts of 
nature to be mathematically deduced rather than merely approximated by empirical 
induction. 

By the way, mathematics is indeed a part of science. In its purest forms, its purpose is to 
characterize abstract structures that display sufficient internal consistency to be eligible for 
physical embodiment. Because it is thus more general than physics, it does not need 
physics to accomplish its ends. Physics, on the other hand, needs mathematics very, very 
badly, and so does every science for which physical reducibility is claimed (including 
evolutionary biology). This strongly suggests that mathematics represents the structure of 
nature more deeply and comprehensively than do more concrete branches of science. In 
any case, the line between mathematics and physics is notoriously difficult to draw...far 
more difficult than some of those who talk about it seem to impy.
I'm sure that most serious ID researchers have no objection to the demand that ID be held 
to scientific standards within the limitations of these standards. What I question is the 
arbitrary sanctification of these standards themselves. Many scientists seem not to realize 
that certain very basic conventional assumptions of science, far from being ironclad facts of 
nature, remain points of controversy. Some of these points of controversy reside squarely in 
the intersect of neo-Darwinism and ID theory. Under these circumstances, an open mind is 
perhaps the most indispensable scientific virtue of all. 
 
Comment #5:
Gedinken writes: 
“I’m not convinced of that. Some ID thesis may be testable. And there may be concepts that 
are consistent but not differentiated by observation. Simple consistency is not sufficient for a 
subject to be “scientific”. (Pink magical invisible pixies creating what we view might be 
“consistent” with observation, but hardly “scientific” unless some empirical observation can 
differentiate the cases of their pinkness, magic capabilities, invisibility, and/or existence 
whatsoever.)”
My point, of course, was that one can establish truth in nature by mathematical as well as 
scientific means. For example, in order to prove that 2+2=4 in nature, one doesn’t need to 
run an experiment in which one puts two marbles in an empty glass jar, peers inside, adds 
two more and then runs a final count. One can simply use arithmetic and count on nature 
(pun intended) to follow suit. Scientists have no good reason to assume that nothing of this 
nature is possible in the more advanced realms of logic and mathematics. So although 
some ID hypotheses may indeed be empirically testable, mathematical verification may be 
more appropriate for others.
By the way, not to be too judgmental, but I’m finding your picture of science a little naïve. 
First you tried to illustrate a point about the supposed feasibility of time travel by counting to 
10 and back again [counting integers in a denumerable (discontinuous) sequence is not 
enough to provide science with a serviceable model of time and causality]; now it’s pink 
magical invisible pixies. If you've adopted this style of expression because you believe that 
your thoughts are so complex that others can understand them only if you take them down 
to the kindergarten level, then I’d suggest that it may not be quite that bad. On lists like this 
one, you might not need to oversimplify to the extent of obscuring what might otherwise 
have been an interesting point.

You observe that "many ID enthusiasts fail to recognize that they are failing to differentiate 
the case they are making based simply on empirical observation and logic." Wisely, you use 
the term many rather than all. Thus, when you go on to say that "they point to a failure to 
accept an argument that fails on that basis as a bias inherent in science," you’re not talking 
about all ID theorists. Regarding your request for specifics rather than "a vague generalized 
discussion", I’m being as specific as I need to be. In particular, when it comes to providing 
ID with a supporting model, the details of individual cases are strictly secondary. This can 
be clearly seen in the fact that no matter how many details are offered by ID theorists, they 
are dismissed by neo-Darwinists for lack of an underlying causal model. So it seems that 
before these detailed arguments will be accepted, ID requires a basic supporting model of 
nature and causation. 
 
Comment #6:
The first thing that any scientist must understand about modeling is that some models are 
optional, while some are not. As Kant and others have reminded us, among those that are 
not are basic categories of cognition and perception. Because the arithmetical operation of 
addition corresponds to disjoint union of sets, and because the proposed experiment 
involves taking a disjoint union of sets - that's what adding objects to a set of other objects 
is necessarily about - addition qualifies as an irreducible category of perception with respect 
to it. So one need merely run the associated gedankenexperiment subjectively, and nature 
must objectively conform to the outcome.
In other words, to add marbles to a set of other marbles is to create a disjoint union of sets 
with cardinality the sum of their cardinalities, and one can do this entirely in one’s head. No 
physical experiment is required. If it were, then scientists would be so limited in their power 
to hypothesize and theorize that they could not function. Because necessary configurations 
of essential categories of cognition and perception are also necessary features of objective 
reality, scientists cannot rule out the mathematical (rational, subjective) derivation of 
nontrivial facts of nature. 
Unfortunately, certain possibilities of this kind are in fact ruled out by the empirical 
methodology of mainstream science. This exemplifies the danger of dismissing philosophy 
as "irrelevant to science". In my opinion, scientists who do not make this particular error of 
conceptualization are exhibiting a most admirable and important intellectual virtue. 
Comment #7:
Gedinken wrote:
“RBH gave a good example. Here is another: We have Euclidean geometry as one of the 
oldest mathematical proofs. Isn’t that good enough for us to know that Euclidean geometry 
is completely accurate for astronomical measurements and models?”
First, although RBH’s handling of my example is interesting and even somewhat instructive, 
I’ve already explained why it doesn’t vindicate your blanket approval of standard scientific 
methodology. Second, your current example differs from my example in a crucial way: it 
crosses the line into the domain of what would, for reasons of ignorance, currently be 

considered "optional" models. Thus, while the answer to your question is negative - at least 
at the current stage of the geometric modeling of nature – this, along with your 
oversimplified "disproof" of Darwinism, still fails to validate your position. So if you mean to 
compare your new examples to 2+2=4, I’m afraid that won't work. My point only required 
one valid example, and I gave it.
Note the italics in the above paragraph. Their purpose is to point out that once again, you 
have no way of knowing whether or not the proper geometry for astronomical applications 
cannot be mathematically deduced, particularly in the long run. The conventional working 
assumption, of course, is "no, it can’t!" But a working assumption is not necessarily a truth, 
and a scientist asked to prove this assumption wouldn’t even know where to begin. 
(Fortunately, this need not apply to everyone for all time.)
Comment #8:
“I think that the concept of numbers is induced, not deduced. [* * * * = * * + * *] The symbols 
2 and 4 were names given, not mathematical concepts from some sort of logic or proof.”
This appears typical of a viewpoint called nominalism, which ignores the necessity for 
something general enough to correlate and enforce consistency among instances of 
perception. Nominalism leads to an unanswerable question: what accounts for the 
consistency of percepts to which the same names are consistently given (and from which 
babies learn by induction)? Nominalism is often juxtaposed with realism, which holds that 
universals exist. Realism is in some sense true, for if universals did not exist, then the 
universe would have no means of enforcing its own universal self-consistency. As it 
happens, this has a technical formulation: if the universe were a language (and the program 
of scientific theorization attests that it can be so-represented for scientific purposes), then 
abstract truths like 2+2=4 would constitute its "syntax", and if categories of perception and 
cognition were not to match this syntax, then the consistency of the universe would not be 
reflected in perception and science would be impossible. On the other hand, if this kind of 
syntax were really learned by induction, then the mind of an observer would be free to 
construct alternative syntaxes. If this were possible with respect to arithmetic, then one 
could (e.g.) devise new integer sequences with different distributions of primes. If you think 
that this is feasible, then go ahead and try it...but don’t be too surprised if it turns out to be a 
bit harder than expected.
Since you continue to focus on the example of Euclidean geometry, a couple of 
observations may be in order. In classical mechanics, the geometry of the universe was 
assumed to be flat; Special Relativity merely updated the flatness concept to four 
dimensions. Then came General Relativity, which replaced the flat Minkowskian manifold of 
Special Relativity with the concept of curved spacetime. But this raised a question: with 
respect to what is spacetime curved? In answering this question, we find that the 
microscopic geometry of nature is still assumed to be flat; the flat Minkowskian manifold has 
simply become the local (tangent space) limit of a curved Riemannian manifold. In effect, 
flat "ideal geometry" remains that on which curved physical geometry is defined. To 
complicate things yet further, whereas geometry was once considered the basic stuff of 
which the universe is made, this notion has led to intractable theoretical problems and is no 
longer on firm footing. It now seems that geometry must be replaced with something more 

fundamental, raising the possibility that the replacement operation will determine certain 
geometric particulars of nature without benefit of observation.
Regarding Alix’s post, I’m not arguing that undecidable features of nature do not exist; in 
fact, their existence would imply the correctness of a certain model of nature and causality 
with which I happen to be associated (because undecidability is a formal property, and 
formal is roughly synonymous with linguistic, objectively undecidable elements in nature 
would imply that nature is linguistic in character). What I do argue is that among the 
features of nature that are not undecidable, some may be amenable to a priori investigation, 
and among those that are, some may be at least partially determined by volition. 
 
Brains As Models of Intelligence
Intelligence testing has, for some time been in disrepute. Critics have a number of 
complaints with it; it is "culturally biased", it "fails to account for creativity", there are "too 
many kinds of intelligence to measure on one kind of test", and so on. But advances in the 
theory of computation, by enabling a general mathematical description of the processes 
which occur within human brains, indicate that such criticisms may overlook more general 
parameters of intellectual ability... i.e., that there exist mathematical models of human 
mentation which allow at least a partial quantification of these parameters.
Neural networks are computative structures analogous in general topology and 
functionability to the human cortex. The elements of such networks act like generalized 
brain cells linked by excitative and inhibitive "synapses" and conditioned by "learning 
functions" acting on "sensory" input. Because they are computative machines, they obey 
the same principles as machine 
programs, digital computers, and cellular automata.
While there are certain aspects of human brains which distinguish them from generalised 
neural nets, it is instructive to assume a close analogy between them; specifically, that 
everything a brain can do has its formal counterpart in the model. Thus, an algorithmic 
description of mental processes, translated into the "language" of the model, generates 
patterns with definite consequences in the model's descriptive logic.
On one level, the mathematics is identical with that of vector spaces and obeys the 
principles of matrix algebra; on another, it is statistical. But just as vector algebra and 
statistics have deep connections with other branches of mathematics, the conventional 
analysis of neural nets can be elucidated by the application of extended formalisms. In light 
of certain trivial insights concerning the model - e.g., that it is capable of localized self-
simulation - this can lead to some enlightening conclusions on the structural and dynamical 
requirements of cerebral efficiency.
In principle, it is possible to measure deterministic intellectual potential by neurological 
analysis, provided we know precisely what to look at (some have tried, along these lines, to 
narrow it down to certain sets of mental reflexes, but things are not that simple). Of course, 
it is entirely possible that human intelligence has a nondeterministic component, as would 
be the case if humans possess anything remotely resemblihg "free will". But the potential for 

ratiocination should be measurable by means approaching anatomical description in 
objectivity.
It has long been a platitude that human beings use only a fraction of their brains at any 
given time. But it may be less important how many cells are firing than the patterns in which 
they fire; and the organization and density of neural populations are obviously crucial to the 
complexity and variety of procedural topologies, we are thus unlikely to find that everyone 
has been dealt the same intellectual hand in life, and this admits of intellectual distinctions 
among people. Eventually, a biochemical extension of the classical model should allow 
understanding and control of the roles that factors like emotion, motivation, and neural 
plasticity play in our mental growth and activities.
Because the commonly recognizable components of intelligence, such as deduction, 
association, and analogy, are all formalizable in terms of the model, its investigation bears 
strongly on the quest for true artificial intelliqence. This strengthens the hope that we may 
soon design machines which can enrich our lives and enable us to handle the ultracomplex 
systems which lie beyond our current abilities...even as we extend those abilities to achieve 
a better understanding of the universe within and without us. In a sense, many of our 
fondest dreams live in that one hope. Intelligent biological, ecological, and socio-economic 
simulators may one day construct algorithms for the indefinite maintenance of youth and 
good health, restore and stabilize the planet's frail ecosystems, and help us eradicate a 
multitude of societal ills. Many aspects of their structure and programming are already 
known. Whether these intelligent simulators are human, artificial, or bio-artificial hybrids, the 
study of intelligence is essential to their emergence and coexistence with us.
Due to the ultimate equivalence of all deterministic machines, my own study of neural nets 
has clarified aspects of cellular and conventional automata as well as deterministic systems 
in general. It also promises certain refinements in the usual methods intelligence testing.
On the Paradoxical Connection Between Money and Brains
The last issue of Noesis, by Chris Cole, presents the "Allais Paradox" in the context of 
"Bayesian regression". It is surmised that the idiosyncrasies of human mental "circuitry", 
particularly those involving hidden or false assumptions, may obstruct rational decision-
making, and that these quirks may somehow be responsible for variations in the "subjective 
value of money". Yet, what if the implied inconsistency exists not only in the minds of those 
to whom the paradox poses its alternatives, but within the minds of those who perceive that 
it does?
I have not had the time to acquire an extensive analysis of this paradox, and must rely on 
Chris's account of it. Accordingly, readers are referred to the last edition of the newsletter as 
preparatory reading for what follows (see editorial).
The paradox involves two alternatives, one of which may be claimed by a hypothetical 
human subject:
A) An 89% chance of an unknown amount of money x, 

a 10% chance of $1 million, and 
a 1% chance of $1 million;
OR
B) an 89% chance of x, 
a 10% chance of $2.5 million, and 
a 1% chance of nothing.
The subject is to choose between A and B, deterministically and from pure self-interest. The 
paradox involves a supposedly irrational tendency for subjects to switch from A to B as the 
value of x changes from $1 million to $0. This tendency would be evident in a statistic 
representing the outcomes of repeated trials involving different subjects, each of whom is 
queried for both test-values of X; call this statistic S. In addition, there is a supposed ten-
dency for S to become constant and rational as all the monetary amounts are reduced by 
several powers of ten. Let this tendency be reflected in the metastatistic M(S).
First, any paradox calls for an extension of the frame within which it is defined. Can such an 
extension be given in the present case? It can, provided the formulation of the paradox 
includes or implies terms whose own definitions generate the extension. Since the paradox 
involves a decision, it is decision-theoretic. It thus involves expectation and risk, certain 
aspects of which go beyond any so-called "rationality" which falsely absolutizes the decisive 
criteria.
Consider the "independence axiom" on which the stock version of rationality is defined: 
"The rational choice between two alternatives depends only on how those alternatives 
differ." Now, difference is an elementary kind of relation, and relations among variables 
cannot always be autonomously defined. There seems to be an assumption that the 
difference between A and B is context-free, or independent, even though it involves at least 
one context-sensitive criterion (risk). That, and not the putative irregularities in subjective 
decision-functions, is the strangest aspect of this paradox; how could a rational analyst 
ignore so basic a concept? It is almost as though the topic of money were acting as a neural 
noisemaker, a cue for hunger to exile logic. Concisely, the alternatives contain a joint 
variable (the unknown amount x) which conceals contextual information on which a critical 
decisive predicate (risk) is defined. Depending on the value given this variable, risk may or 
may not differ between the alternatives. The axiom cited above, whose formulation is 
consistent with this possibility, must obviously be reinterpreted in light of it.
Nature abhors absolute inconsistency; paradox, an inherently computational concept, is 
always a matter of computational deficiency ("always" requires some qualiflcation, applying 
to the inferior realm of natural temporalities). Paradoxes are useful whenever they allow us 
to locate and remedy erroneous or incomplete thought-patterns. But it is sometimes a little 
too easy for the analysts of paradox to locate these deficiencies not within themselves, but 
with those around whom the paradox is explicitly formulated.
Whether the paradox resides with the analysts or their subjects, it is computational. The 
question remains, do the implied mental deficiencies reflect human limitations as opposed 
to the broader limitations of computative devices, or can reasonably smart humans also 

learn to resolve this paradox? If so, we can eliminate one more doubt concerning the 
ultimate generic identity of human and mechanical computation, an identity first explored by 
Alan Turing in his celebrated paper on the criteria for mechanical simulation of human 
behavior (i.e., those of the "Turing Test"). If it can be shown that whether a universal 
deterministic machine can resolve a paradox (solve a problem by logical induction) in time < 
n depends partly on intrinsic factors (e.g., size) rather than only on its proper or improper 
programming by fallible humans - or that the minds of human programmers can themselves 
create and run the right program and thereby achieve understanding on the level of correct-
ly programmed universal machines comparable in general parameters like size and speed - 
the hunan-mechanical equivalency thesis will be strengthened, at least with respect to our 
(humanistically-flawed?) axioms of computation. The issue is not whether certain kinds of 
neural nets - e.g., those with the capacity for emotion - are inoptimal for solving some kinds 
of problems, but whether they can solve them at all.
Notice that nondeterministic machines, which are in principle immune to flaws in the axioms 
thought to underlie deterministic procedures, cannot with certainty be said to share human 
intellectual limitations, and that some nondeterministic machines actually emulate whatever 
"oracular" abilities humans may ultimately come to possess in the course of sensory and 
mental evolution. Moreover, we must consider that human brains are to some measure self-
programming, and must widen our analysis to define and account for all self-programming 
machines. But this is easy, since the class of self-programming machines is virtually 
identical with that of freely-programmable (universal) machines, given a means for the input 
and program—conversion of environmental data.
Current research implies that generic equivalency obtains within the natural realm. But if so, 
this is a truth of little use to us here, since Allais' paradox boils down to the mere bad habits 
(but not the absolute limitations) of human thinkers. To demonstrate this, it will suffice to 
resolve the paradox within our own neural parallel-processing networks, or "brains". The 
framework has been erected above; all that remains is to fill it in.
Let us begin by relating the economic and computational aspects of the paradox. Because 
the choice to be computed depends on collective economic criteria, which in turn depend in 
part on the computative characteristics of the individuals locally situated in the economy, 
neither aspect is independent of the other; they are tautologically united. The algorithm to 
be employed amounts to a binary deterministic game-theoretic function F with the pair (A,B) 
as its argument and with certain player-specific valuational (as opposed to expressly 
quantitative) parameters. In other words, the game varies to some extent with the 
"subjective" situation of the player. Being formulated on an essentially game-theoretical 
basis, F(A,B) incorporates a risk criterion. Risk, of course, is defined as the probability of 
inverse gain, or of loss (the evaluation of risk, being probabilistic, is one part of this matter 
to which Bayesian inference obviously applies, though economic reasoning in general is 
shot through with probabilistic regressions). Observe that since the subject is not allowed to 
play repeatedly, F cannot reduce risk to negative expectation, but must somehow 
incorporate the "worst case": a maximum loss of $1 million.
Assessments of risk can themselves be risky. We live in a country - and in a world - where 
observation demonstrates time and again that exposed money is a target for numerous 
grasping hands. The average person is encouraged by this to distinguish sharply between 

monetary exposure and nonexposure. A scheme's risk, one observes, is consistently 
understated to potential investors by those who want their money, and one can trust an 
evaluation of risk only as far as one trusts the evaluator. Money is not a thing which breeds 
trust among men.
It follows that we will observe a repolarization of F, as reflected in the statistic S(F), along 
with any change concerning the presence of (nonzero) risk. Allais paradox displays such a 
disjunction: when the unknown amount is 0, there is no risk. Yet when this amount is raised 
to $1 million, the probability of significant material loss becomes ".01" (or so says the poser, 
who may or may not have a fair coin in his hand). This is because the subject is effectively 
given the megabuck (via alternative A) and offered what amounts to a double-or-nothing bet 
on it (alternative B). Where the variable amount is 0, however, nothing has been given with 
any certainty, and risk cannot be materially defined. The concept of "material definition" is 
pivotal: in the same way that a bird in the hand is worth two in the bush, money possessed 
with certainty is qualitatively distinct from - and subjectively more valuable than - money for 
which one is merely hoping. Since the rational evaluation of risk must take account of 
distinctions of value for the subject, to whom money in the control of another is usually 
valueless, the aphorism takes on mathematical force.
Notice that while the distinction between zero and nonzero risk can be taken to involve 
psychological factors, these are strictly behavioral: conditioning (compare "programming") 
by negative reinforcement (compare "past input") within the local environment. Even digital 
computers are "psychological" to this extent. So the distinction may be safely understood as 
computational in the general sense, and no human peculiarities need be blamed.
On the other hand, what if we were to let the subject use his own "fair coin"? While trust 
ceases to matter, other factors remain. The actual material loss of $1 million equates to 
financial ruin for most people. But what is the exact point at which an unacceptable loss 
becomes acceptable? This distinction represents another potential irregularity, which we 
may for present purposes associate with the metastatistic M. That is, the test-values of x 
may be subject to various local relativizations affecting F, as well as to effects associated 
with the simultaneous reduction of all the amounts (a precise description of which exceeds 
the level of this presentation). To avoid taxing the reader's attention and/or facility with 
mathematics, this phase of the discussion will be kept as informal as possible.
It has been conjectured that arithmetic was originally a mercantile creation, born of trade 
and profit. If this is so, there may still exist a simplistic tendency to equate money with 
inventory against an infinite market (if twenty bolts of cloth are worth a talent of silver by 
decree of Assurbanipal, then how many talents will buy enough cloth to outfit two legions?). 
However, we now understand that economies seldom function so simply, and that money 
does not admit of linear valuation. Since human beings exist in economic settings which 
vary according to local criteria, they cannot rationally ignore those criteria in their valuation 
0f money. That is, they must "subjectivize" the value of money in accordance with their 
goals and current positions in the economy, by rules abstracted from their past experience 
and what they know of financial theory. The efficiency with which they do this, of course, 
again admits df analysis apart from considerations of detective psychology or the basic 
inferiority of neural mentation.

Similarly, the rate at which investment generates capital is not constant as the level of 
investment rises within a given financial setting. Expansion is seldom continuous; there are 
certain discrete monetary thresholds which must be reached before the successive phases 
of expansion can occur, and windows of opportunity are often labeled with particular antes. 
The economic and game-theoretic complexities which come into play can easily boggle any 
reasonable set of computative parameters...a fact well-known to those who attempt to 
construct computative models capable of any but the most general economic predictions. If 
the subject resorts to oversimplification - or even nondeterminism - in making his decision, 
we need not count it any great wonder. But one fact is obvious: one cannot always translate 
less money than one needs into enough to meet his goals, even if the "objective" difference 
in amounts appears relatively small, without information that is often difficult to acquire and 
fallible by dint of the regressive uncertainties that characterize open economies.
Thus, the subjective value of money reflects certain actualities within and without the 
immediate economic environment of the individual. A set amount is necessary to even 
become a "player" in certain social and financial circles, including those to which a given 
subject aspires. Because aspirations, which vary greatly among people, act like gauge 
modulators on the scale of monetary value, it is meaningless to speak of how alternatives 
differ "intrinsically" and without respect to outside influences on the scale of differences. In 
reality, money - and much else as well - has no measurable intrinsic value at all. This is a 
lesson dear to the hearts of economists, some of whom (e.g., the U. S. Federal Reservists) 
are actually in the practice of adjusting its value according to global criteria within which 
local variables differ (thus, regulation of the total supply of money can be used to control its 
average value over the space of regulation, but not the small-scale "subjective" variations 
which occur therein). If you must convince yourself of this lesson, try eating a hundred-
dollar bill the next time you get hungry, or thinking of any direct use for it in which it is 
irreplacable by something which costs far less than its face value. As the only such 
applications involve showing it to other people, its value depends on their subjectivizations 
of it.
Because the value of money is defined on economic criteria, and because economic criteria 
devolve locally to the subjective values of people, money on the local scale has subjective 
value only. It is defined on the psychology of need and happiness, and it comes as no 
surprise when subjective factors enter into judgments involving it. Were we to replace 
"money" with anything whose value obeys a constant function of quantity, the choices would 
remain rational (in the naive sense) up to the point of subjectivization. But as subjectivism is 
unavoidable in economic reasoning, no such monetary replacement can exist (a fact I 
demonstrate elsewhere along with more detailed findings on various intriguing logical and 
quantitative phenomena in economy-like systems).
Now we reach a seeming distinction between brains and universal machines. Any set of 
goals can be programmed into the latter, but the wants of men and women are more often 
determined by such quasi-congenital factors as talent and ability. Thus, humans are to 
some degree "pre-programmed", and to this extent non-universal. On the other hand, 
universal machines without any pre-programming are paralyzed by their functional 
generality; a random or nondeterministic machine produces output as a random function of 
input. This forces us to limit our comparison to functional machines. The distinction then 
comes to resemble that between ROM and programmed RAM, and thus becomes 

amenable to standard computational analysis. Concisely, human nature is ultimately a 
compression of nurture, or environmental conditioning, primed by the pure will to survive on 
the individual and collective levels. Since, even if we attempt to abstract all "nature" out of 
our machine, we are forced to unite it with a programmatic agency having as much nature 
as we do, the distinction is ultimately irrelevant to equivalency. Simply note that the 
physiomorphic distinctions among human brains, inasmuch as they are the products of 
natural selection, reflect past local variations within the system of selective principles and 
may be regarded as programmatic distinctions. Because we are considering only 
deterministic rational processes, we can for now ignore the effects of nondeterministic 
lacunae - which may be an essential part of what makes us human - in the selective 
process (and in any subjective substitute for F thereby generated).
So gain and risk are subjectively evaluated as functions of the punctuated and fluctuating 
monetary thresholds associated with particular systems of goals and computational 
parameters, both of the latter being determined as functions of location and conditioning on 
genetic and environmental levels. This admits of mathematical formalization, and we are 
temporarily spared a pronouncement of incorrigible eccentricity by our ability to follow this 
reasoning and thus override our respective mental quirks (and whatever limitations they 
seem to entail).
Money has stable value only as an averaging function on human scales of wants yet, 
because humans are diverse and variously-situated, individual scales need not conform to 
this average, out are instead part of what determines it. The problem resides in our 
tendency to try to predicate economic decisions on the value of money, rather than vice 
versa (which is how it actually is on the scale of this formulation). So - apart from the 
consideration of risk - it is not the "unusualness of the amounts" which confuses, but the 
subjective and polymorphic nature of money. Rationality must account for this, and 
reasoning which treats money as though it had absolute intrinsic value is flawed and 
therefore irrational. It thus falls before a higher reality in which Allais paradox straightens 
and vanishes like a well-shot arrow.
Note that the resolution possesses a certain undeniable beauty. Economies depend in part 
on the neural parameters of brains which evolved from a primal Darwinian "economy" of 
survival, which in turn evolved from the system of invariants by which the universe is 
constructed. By simple extension, we can think of this system of laws as the "anthropic" 
configuration of the "Mind of God". The implied model is a hellix linking generality with 
specificity, and closing ultimately upon itself in a teleological loop.
And thus can obscure paradoxes lead to profound insights. Precisely Because he is 
capable of such insights can man learn now to optimize economies in the promotion of 
global happiness, and apply reason and compassion where abuse now prevails. By defining 
the connection between human nature and free economics - the laws at once governing 
brains, economies, and other systems - we extend the narrow conceptual path leading 
towards human fulfillment. But more on this below.
On the Differences between People, Birds, and Bees

To the vast majority of us, the world is an inhospitable place. Life on earth is problematic, 
and human beings are not always up to solving its problems. Mankind can address this 
situation either by modifying the earth and blunting its hazards, by modifying himself so as 
to sidestep some of these hazards, or by elevating his problem-solving ability so that 
hazards can be predicted and finessed with minimal environmental tampering. As matters 
now stand, most such tampering - while often displaying sound logic from an extremely 
local vantage - is, in the wider sense, ecologically ill-conceived and ill-coordinated with other 
such efforts. That is, the problem of improving the lot of mankind and its companion 
lifeforms on earth does not admit of localistic solution: the problem does not break down 
into separate, parallel-soluble sub-problems.
Yet, in the name of "patriotism", "nationalism", "the right of peoples to self-determination", 
and other such anachronisms and oxymorons, globalistic solutative programs cannot be 
implemented. Nor, for that matter, has the problem of optimizing the human condition ever 
been coherently formulated; utilitarianism, communism, anarchism, and other such 
schemes are logically or socio-psychologically unsound or obsolete, and even democracy 
displays a range of inconsistencies. Nonetheless, man demonstrates a chronic need to 
wreak havoc, within and without his own kind, in internecine disputes over creeds and 
administrative algorithms he lacks the intelligence to evaluate. So well able to focus his 
mind on concise, locally-circumscribed problems, his intellect breaks apart into quibbling 
pieces when the content is enlarged beyond his ability to reason. At this point, he either 
fights, flees, or defers sullenly to others whose authority supposedly devolves to a superior 
ability to create and implement solutions to his problems ...that is, to higher intelligence.
Of course, while governments are often assumed to possess collective intelligence greater 
than that of the governed, they more often claim to derive their mandates from "the will of 
the people" - which, again, need not be a will governed by any concerted intelligence. At the 
bottom limit, governments are composed of brutal tyrants who rule by force and for the good 
of themselves only. Challenged from without, they uniformly broadcast a tired refrain of 
national sanctity which, because it harmonizes all too well with the self-justifications and 
ulterior motives of so many other governments, usually plays well enough. The situation is 
unconscionable, not only because it offends sensibilities, but because it is in no way 
consistent with any valid algorithm for world optimization. In fact. it is demonstrably anti-
solutative in the computation-theoretic sense, and the need to change it is unequivocal. But 
change will not come until men possess the intelligence to recognize their need...or until 
that need has smashed their baseless pride and spurious independence with hammers of 
catastrophe. Those hammers are ever gaining deadly mass in the form of overpopulation 
and its attending ills, the depletion of nonrenewable resources, environmental degradation, 
and an increasing ability to intentionally harm and destroy.
There are several obvious ways to deal with this. We might launch a new science of human 
intelligence whereby to upgrade and configure the components of the advanced human 
computer which must out-think the problems attending its increasing density upon a finite, 
overburdened substrate. Using related knowledge, we might design an artificial intellect to 
do this in our default. We might simply glean the best and brightest humans who currently 
exist, and try somehow to repose authority with them. Or, we could apply all these 
strategies at once, hoping for a lucky synergy among them. In any case, we require 
developmental formalisms superior to any currently in wide use (such formalisms exist 

already, and they boil down to the same essential conceptual systems).
Certain priorities tend to favor some of these strategies over others. Individual freedom, 
even distribution of power, and technological advancement appear to favor those involving 
the overall enhancement of human intelligence. This raises the following question: what is 
now being done to upgrade the intellectual potential of mankind? In most places, children 
are taught by rote and explanation, but not how to think; they are merely congratulated 
when they happen to show sparks of originality. In other places - especially in urban public 
schools - those of low motivation and ability are all but granted criminal dominance over 
other students, and even over faculty. From the computative angle, this is like trying to build 
a supercomputer by cranking out and parallelizing vast numbers of low-grade packet 
calculators.
In the same spirit, there has arisen a ludicrous tendency to vitiate the concept of intellectual 
distinctions by, for instance, pronouncing street rap a valid replacement for the 
technologically sufficient language from which it degenerated. One who thinks in street rap 
may be potentially as intelligent as another who thinks less constrictedly, but he is not 
functionally as intelligent, and will not fulfill his potential without remedial education. Main-
streaming such students is a disservice not only to them, but to those better equipped to 
handle advanced abstractions.
Moreover, simple principles of computation indicate that not all human brains are equal in 
potential. This may be a bitter pill for sociologists and anthropologists to swallow, since so 
many have become apologists for the hubris of modern, technologically-advanced cultures. 
But the size and internal structure of computative devices - neural as well as digital - bear 
heavily on power and efficiency, particularly in the limit. This is a fact which takes 
precedence over the media-propagated paranoia concerning certain "eugenic" atrocities of 
the mid-20th century...a little like the paralyzing fear of an agoraphobe in a burning house. 
The potential for abuse, being ubiquitous, does not constitute a rational argument against 
otherwise desirable changes.
Any species which has stripped and modified the context of its evolution bears a 
responsibility to control its further evolution, insofar as the latter is necessitated by the 
inability of that species to coordinate its activities in ways consistent with its survival. There 
is already enough data to establish a correlation between genes and intelligence; all that we 
need now is a refined knowledge of how to optimize brain structure through genetics and 
early programming without compromising other gene- and program-dependent aspects of 
individuality, viability and happiness (we need not presuppose a single gene or gene-set 
coding for high intelligence, which may be a matter of various more or less complex genetic 
combinations expressed in specific biochemical and other-parametrised environments 
during pre- and post-natal development).
A bit more on nature versus nurture in the cultivation of intelligence. Many people, in 
choosing mates, believe themselves motivated by the physical, emotional, and intellectual 
welfare of their future progeny. A cursory glance, however, reveals that most of these 
"gene-conscious" parents lack the data and intellectual power to successfully evaluate such 
matters...particularly the actual intelligence of their mates. For instance, despite the obvious 
abundance of wealthy mediocrities, it is easy to observe a widespread tendency to equate 

intellectual and financial assets. Sadly, that may be the only way that some people can put 
any value at all on intelligence. This shameful illusion is fostered by the all too frequent 
translation of money into social and political power, and by the absurd confidence of many 
successful businessmen in their dubious abilities to solve whatever problems they might 
later encounter as public servants.
A good deal of recent anthropological and sociobiological evidence suggests interesting, but 
disturbing, parallels between humans and more primitive species in terms of reproductive 
strategy. Primitive strategies may be acceptable in primitive species still subject to 
something vaguely approaching natural selection. Having suspended this with respect to 
ourselves, however, mimicking the reproductive behavior of insects, birds, and apes is - as 
well as being somewhat beneath us - simply not feasible. We have created a world in which 
intelligence is a necessity, and done so at a far greater pace than unassisted nature can 
duplicate, we have changed the rules of the game, and new games require new strategies. 
If men and women cannot change their strategies to fit their notions of family, love, and 
romance, they may have to change their feelings to suit their needs.
A strong case can be made that mankind's current path is dysgenic, particularly given the 
rather meretricious (or absent) reproductive priorities of the many. As strong a case exists 
that little time may remain before Malthusian pressures lead everywhere to the same 
changes already forced on overpopulated mainland China. Still another important point is 
that natural selection no longer prevents the physical and mental deterioration of our 
species by an accelerating accumulation of genetic defects. Compassion alone dictates the 
right of children to be born free of such defects, regardless of how little their parents may 
care to grant it. And what constitutes a "defect" may well depend on the rising level of 
complexity permeating a child's environment.
One is tempted to cite as counterexamples to the thesis of "reverse evolution" the many 
brilliant scientists and technicians now living on earth. One must further observe that there 
are many good-hearted people who do not rate as rocket scientists, and who in fact are 
congenitally defective relative to all reasonable standards of physical and mental fitness. 
One can even surmise that nothing short of mandatary sterilization could stop the vast 
majority of "suboptimal" breeders from continuing to create progeny who share their 
nonbeneficial characteristics. Indeed, it is difficult to argue with the truth in point of content. 
But to what extent do such truths militate against the desirability of some form of eugenics? 
Quite simply, they do not.
It is inarguable that certain plausible sets of assumptions about who we are and what we 
want imply the desirability of enlightened supervision over human genetic recombination 
and manipulation, while many of our problems undoubtedly come down to a nonintellectual 
clash of wills, a great many can be shown to result from a shortage of intellect among those 
empowered to make decisions on behalf of themselves and the rest of us, and of those 
officials and constituents who encourage short-sightedness by their implacable insistence 
on fast, specious solutions to problems whose full extents they are unable to fathom. 
Concisely, the decisions relegated to such people must be limited to those for which they 
are mentally equipped. But this implies the redistribution of power on the basis of 
intelligence, and thus a choice between eugenics on one hand and intellectual or electronic 
authoritarianism or elitism on the other.

Since the priorities of the many must often be computed ad hoc according to local criteria, 
civic responsibility can be defined only relative to some measure of computational ability. 
The capacity to transcend one's own situation and make decisions of societal as well as 
individual benefit thus involves intelligence. Intelligence cannot preclude evil, but can 
reduce its ability to take root and flourish. Democracy, in entrusting the common good to an 
enlightened majority, depends on the intelligence of all.
Consider the general requirements of democratic political systems. It is apparent that the 
efficiency of a democracy relies on the mental abilities of citizens to evaluate issues and 
candidates not only with respect to momentary personal advantage, but in light of global 
parameters; and that as population density rises and technology advances, these 
parameters become more numerous and interact with increasing complexity. The average 
citizen is already swamped with data he is powerless to integrate; consequently, he tends 
towards apathy, blatant self-interest, or gullibility, all of which compromise efficiency. 
Democracy and personal freedom worked well when a sparser population interfered less 
with itself and when issues were simpler. But as the world has evolved, the power of men to 
compute necessary decisions has not kept pace. Evidence of this is abundant; one cannot 
argue that the flaw resides exclusively in the system, for systems consist of, and are limited 
by, their components.
Governments "by the people" can be modeled as networks of human brains. Such "meta-
nets" have interesting properties. But any network can be computationally overloaded by 
input of sufficient complexity. Even if a meta-net can change its overall configuration to 
accelerate solution, it is limited by the power of its individual sub-nets. The main question 
here is, has this limit been approached or exceeded by existing social problems? The 
answer is yes, partly because many of the problems we face can be arbitrarily complexified 
by temporal extension. That is, just as a chess player telescopes his reasoning by 
predicating future moves on the future moves immediately preceding them, every solution 
we calculate becomes a condition of subsequent problems.
That pure laissez-faire democracy is inherently flawed follows from advanced logic. Nations 
have chosen democracy because it best conduces to personal freedom; all the alternatives 
exact what was considered too high a price tor whatever advantages they entailed. But the 
equation-string, freedom = responsibility = intelligence, is symmetric and transitive in 
today's world. To remove one term is to remove them all...and tyranny abhors a political 
vacuum.
To ensure the viability of a democracy, we must ensure the intelligence of the individuals 
comprising it. But this is a task beyond the reach of those by whose incompetence the 
system falters. Because we are not assured that the distributive majority of human beings 
have the capacity and motivation to learn how to think on the appropriate level of 
complexity, the correlation of genes and intelligence encourages either that a eugenics 
program be designed and administered by those of highest expertise, or that we develop a 
taste for societal deterioration and loss of freedom.
The fact is, freedom is conserved under changes in population density. More people means 
less of it. Accordingly, freedom of one kind can often be purchased only at the cost of 

another. These lessons have a mathematical certainty that is lost on most unintelligent 
people, and even on some who consider themselves mentally superior. Unfortunately, 
reality seldom yields to the convenience of those who choose to ignore it...and reality 
seems to dictate the formulation and implementation of certain judgments concerning just 
what attributes should be preserved under population reduction or favored under stasis. 
One such attribute will obviously be high intelligence.
This subject is a dependable trigger for hysteria. There are many "leaders" who consider 
eugenics synonymous with "racism", "discrimination", and even "genocide". Such ploys 
often work, and largely by virtue of the intellectual limitations of audiences and journalists. 
Even though such hysteria thus militates in favor of its apparent cause, few people have the 
courage to challenge it. But there is as little time for humoring the timorous as for mollifying 
the hysterical; ignoring the issue is unlikely to make it go away. There are those who believe 
that irresponsible breeding practices, and the stupidity which fosters them, cannot be 
stemmed without damage to our freedom. But freedom, and much else as well, cannot 
tolerate the geometric prolificacy of stupidity.
There is a widespread tendency, at any mention of this topic, to recall certain "lessons of 
history" involving practices endemic to wartime Germany. However, the philosophy behind 
such practices, Nietzsche s heroic version of existentialism, had an explicitly brutal aspect; 
with a pronounced xenophobic twist, it advocated domination of the weak by the strong. 
Had compassion not been anathema to the nazi creed, abuses could not have existed on 
so monstrous a scale. Obviously, the mistakes of the past need not be repeated. 
Compassion is perhaps the most important ingredient in any effort to improve the human 
species, especially with regard to humans yet unborn...each of whom would presumably 
wish to enter life with a complete physical, sensory, and mental apparatus.
Bearing this in mind, consider the emergency use of mandatory sterilization. Rights may be 
distinguished from privileges by the degree to which certain kinds of behavior restrict or 
interfere with the prerogatives of others. If you want to shout sedition from the rooftops, the 
first amendment grants you that right on the supposition that no one else need take you to 
heart (a risky assumption in societies where not everyone is mentally able to perceive the 
possible inconsistencies in your speech). If you want to convoke a coven of witches at 
midnight under a full moon, you are granted that right by virtue of the fact that no one need 
come who wishes not to. By this criterion, breeding is anything but a right; it cannot be done 
without affecting the child, and all who - directly or indirectly - must subsidize its life in the 
event of disability. At the extreme, witness the deplorable example of babies born to drug-
addicted mothers: they are prey to every ill that can rack their pitiful brains and bodies. Yet, 
such mothers - who have demonstrated a medical condition rendering them unfit to bear 
children - are treated as though tubal ligation amounted to death at the stake. The only 
argument in favor of the status quo relies on "conventional attitudes" towards childbearing, 
attitudes which have outlived the world which created them. With the advent of long-lasting, 
injectable contraceptives, such conventions will carry even less force than they do now.
Let us extend the theme. There is a modern tendency to claim that blindness, deafness, 
and other handicaps leave one able to experience and contribute to life as fully as anyone 
else, and that no one is morally fit to argue otherwise who does not share the handicap in 
question. This position was originally crafted to soften prejudice and bolster the self-esteem 

of the afflicted. But it has since been taken to imply that, where the condition is congenital, 
society has no business restraining those afflicted from passing it on to whatever progeny 
they might choose to have.
Fitness, the line goes, is a relative concept, subject to variations due to racial, cultural, 
religious, and personal criteria. Yet, global society is proceeding in a direction to which such 
criteria may be irrelevant or inimical. It thus makes little sense to adopt a deceptive 
tolerance which will not be shared by the world our descendants must inhabit. The mankind 
of tomorrow may not be disposed to forgive those whose passivity and self-indulgence 
saddled them with infirmities, for many of them may find that tomorrow's world does not 
always pardon the infirm.
Evidence is accumulating that certain psychological and behavioral tendencies are at least 
partially under genetic control: e.g., schizophrenia, substance dependency, and extreme 
aggression and antisocial behavior (violent criminality). Care must be taken to ensure that 
attempts to suppress such liabilities do not interfere with crucial components of genius. For 
example, in approaching great tasks or solving large and extremely difficult problems, 
something very like controlled aggression is often required; and certain more or less exotic 
mental processes, such as those involved in artistic creation, are somewhat dissociative. 
While it may never be possible to engineer genius to order, the genetic alchemy through 
which it emerges must be given a chance. Fortunately, such precautions are not 
incompatible with a substantial rise in average human intelligence.
Eugenics is often thought of as predicated on arbitrary standards of desirability or 
attractiveness. The question is then asked, who will decide what constitutes desirability? 
Yet, it is relatively easy to develop statistical profiles, given any set of restrictions on the 
sample, that would relieve any one person of such a responsibility. For example, where the 
vast majority of people perceive a given facial characteristic as grotesque, it could be rated 
a "defect" on the grounds that those having it could not escape frequent ostracism or pity on 
account of it. If it serves (or is inextricably linked with) no beneficial function, it could then be 
eliminated from the genome regardless of who claims a proprietary interest in its 
perpetuation. The matter of intelligence, even in this anti-testing climate, is clearer still.
Many people have been taught to view statistical reasoning with suspicion, particularly 
when applied to social minorities. Indeed, statistical formulae lack absolute force with 
respect to single individuals, who may be exceptions to the rule. Any general set of eugenic 
guidelines will inevitably affect different groups in different ways and to differing extents. But 
even though evaluating cases on an individual basis ensures the fair application of such 
guidelines, certain groups might perceive themselves as unfairly threatened. If necessary, 
eugenics could be pursued within such groups according to special guidelines, in the expec-
tation that the intelligence thereby gained will illuminate the need for submission to more 
general criteria. In any case, the need for genetic variability means that humans will 
continue to occupy a wide variety of shapes and styles.
Man's need to evolve still exists. The only way for civilization to evade it is by resorting to 
methods, which even now would seem pointless and Orwellian, whereby to forestall its 
collapse given the present low level of common intelligence among its members. In all 
likelihood, therefore, eugenics will prevail. But whether soon enough, and whether 

voluntarily or not, remains to be seen. Little enough already remains of the ecological 
majesty that was once the earth; mankind has too long traded the role of shepherd for that 
of parasite. Yet, to be a fit shepherd for the lesser of the earth, man must learn to be a 
shepherd unto himself.
It is easy for the "highly gifted" to remain aloof from such questions, either by ignoring them 
or by hiding within ethical structures too weak to bear the weight of correct answers. It 
would be interesting to know whether the Noetic Society could function concertedly as part 
of the solutative apparatus, or whether it is content to remain a vehicle for socializing and 
desultory puzzling. This question seems to demand an answer.
The Resolution of Economic Paradox: Toward a Theory of 
Economic Relativity
Step 1: The Wallet Game 
In this paper, we will attempt to draw some profound conclusions from some simple 
arithmetic.  These conclusions will involve the nature of value, rationality and mathematical 
expectation, and may help to explain why financial mathematicians increasingly tend to be 
hired from the ranks of physics students.  Specifically, they will show that value and 
expectation are not absolute quantities, but relative in a sense analogous to that 
encountered in modern physics, and that this relativity is independent of differential 
probabilities. 
First, we consider the paradox of Kraitchik.  Two people are offered a chance to bet on 
whose wallet contains the lesser amount of money; the one with less gets all of the money 
in both wallets. 
Both players reason as follows:
"If I lose, I lose just what I have, but if I win, I win more than I have.  Therefore, given that 
the win and loss probabilities are indifferent, my expectation is positive and I should bet."
First, note that the betting strategy is cyclic and cannot be completely executed; it applies 
iteratively to its own result, prescribing (if permitted) that the player bet over and over ad 
infinitum without ever realizing the promised "win".  Since, within or without any one game, 
the switching strategy is cyclical and of period 2 - that is, since two iterations result in a 
return to the player's initial state, whatever that state may be - only one switching operation 
is meaningful.  This is a signal that something is wrong with the strategy.
In fact, a paradox exists even where only one switch is allowed.  This paradox resides in the 
application of the above rationale to both players, implying a positive expectation for each in 
a 0-sum game in which the total amount of money is fixed.  The resolution of this paradox is 
effected by showing what's wrong with the rationale, and more precisely with its quantitative 
interpretation.  The problematic interpretation is based on the tacit assumption that the 
amount of money in the player's own wallet is "context-free" or fixed no matter how the 

game ends, but that the amount in his opponent's wallet is "context-sensitive" and varies 
according to the game's result.  Obviously, the opposite assumption is just as valid from a 
subjective viewpoint; one may consider the opponent's amount fixed and one's own amount 
as varying with the result of the game.  When symmetry is restored by separately 
interpreting the rationale in light of both assumptions - i.e., with respect to the two subjective 
"cases" associated with these assumptions - the mathematical expectation is seen to be 0.
Specifically, let the player's stake = x, his opponent's stake = y, "if I lose" = condition 1 (x1 > 
y1), and "if I win" = condition 2 (x2 < y2).  Then the problem is simply this: each player 
lopsidedly treats x as fixed and independent (and therefore unindexed), and y as mutable 
and dependent (and therefore in need of an index 1 or 2). That is, each player thinks
"If I lose, then I lose just what I have, but if I win, I win more than what I have", which 
becomes
"If y1 < x, then gain1 = -x, but if y2 > x, then gain2 = y2 = x+n, n>0."
In fact, y could just as well be the fixed quantity and x the conditionally dependent one.  In 
this case, the player would reason
"If x1 > y, then gain1 = -x1, but if x2 < y, then gain2 = y (< x1) = x1-n, n>0."
In the first subjective frame, the player's expectation is
(gain1 + gain2)/2 = [-x + (x+n)]/2 = n/2, n>0.
In the second subjective frame, it is
[-x1 + (x1-n)]/2 = -n/2, n>0.
Since n is arbitrary, the true expectation according to this essentially correct rationale is n/2 
+ -n/2 = 0.  Paradox resolved.  
Step 2: From the Wallet Game to the 2-Envelopes Problem
The 2-envelopes game may be described as follows.  A player is allowed to choose and 
hold one of two sealed envelopes containing unspecified amounts of money, then given a 
choice of whether or not to switch envelopes.  In either event, he gets to keep only one 
envelope; whereas the winner gets to keep both stakes in the wallet game, here he gets 
only the more valuable of the two (where choosing the more valuable envelope amounts to 
“winning the game”).  Obviously, switching amounts to betting that the other envelope 
contains more money than the one being held, while not switching amounts to betting that it 
contains an equal or lesser amount.  In most versions of the 2-envelopes game, there is an 
additional constraint: one unidentified envelope contains twice the amount in the other (the 
version of the game or problem which omits the constraint will be referred to as “minimal”). 
Since the total amount of money contained by the envelopes is fixed and immutable, the 
game is 0-sum with or without this constraint.

Now consider the 2-envelopes game with a slight modification: each envelope is assigned 
by the toss of a fair coin to one of two players, each of whom has the option to trade or not 
to trade.  In this modified version of the 2-envelopes game, the same rationale applies. 
Where x is the amount in the player's own envelope, the amount to be won by trading (x) is 
larger than that to be lost (x/2), and the player again reasons as follows: "If I lose, I lose half 
what I now have, but if I win, I win an amount equal to what I now have.  Therefore, given 
that the win and loss probabilities are indifferent, my expectation is positive and I should 
switch."  And again, the paradox arises from the applicability of this rationale to both players 
in a 0-sum game.  Since the only difference is a minor additional constraint - namely, that 
the winning envelope contains twice what the other one does - the paradox is resolved 
exactly as was that of Kraitchik.
Specifically, let the player's stake = x, his opponent's stake = y, "if I lose" = condition 1 [x1 = 
2(y1)], and "if I win" = condition 2 [x2 = (y2)/2].  Then the problem is again that each player 
lopsidedly treats x as fixed and independent (and therefore unindexed) and y as mutable 
and dependent (and therefore in need of an index 1 or 2).  Given that all variables must be 
expressed in terms of the objective total value |G| of the game, the half-or-double condition 
appears to require that the fixed variable in terms of which the value of the conditionally 
dependent variable is expressed has the value |G|/3; double this amount equals 2|G|/3 and 
half equals |G|/6, and both variables take these conditional values in turn.  That is, each 
player thinks
"If I lose, then I lose half what I have, but if I win, I win an amount equal to what I have", 
which becomes
"If 2(y1) = x, then gain1 = -y1 = -x/2, but if (y2)/2 = x, then gain2 = (y2)/2 = 4(y1)/2 = 2(y1) = 
x."
But in fact, y could just as well be the fixed quantity and x the conditionally dependent one. 
In this case the player reasons  
"If x1 = 2y, then gain1 = -y = -(x1)/2, but if x2 = y/2, then gain2 = y/2 = (x1)/4."
In the first subjective frame, the player's expectation is
(gain1 + gain2)/2 = (-x/2 + x)/2 = x/4.
In the second subjective frame, it is
[-(x1)/2 + (x1)/4]/2 = [-(x1)/4]/2 = -(x1)/8 = -x/4.
So the true expectation according to this (essentially correct) rationale is
x/4 + -x/4 = 0.
We can just as easily formulate gain in terms of y; in the first subjective frame, the player's 
expectation is

(gain1 + gain2)/2 = [-y1 + 2(y1)]/2 = (y1)/2,
while in the second, it is
[-y + y/2]/2 = (-y/2)/2 = -y/4 = -2(y1)/4 = -(y1)/2.
So the true expectation is
(y1)/2 + -(y1)/2 = 0.
Paradox resolved.  Observe that at no point did either player have to consider the viewpoint 
of the other; all each had to do was interpret his own straightforward rationale in terms of all 
arithmetical possibilities.  A simple correction for a simple oversight…but as we are about to 
see, still a bit too simple for comfort.
Step 3: Eliminating Incorrect Reasoning about the 2-Envelopes Problem
A further complication seems to arise when the primary player looks inside his envelope 
(E1) and sees a definite amount of money, say $x (x a positive real number).  Now is seems 
possible for the player to say, "Since the other envelope (E2) contains either $2x or $x/2, I 
stand to gain $x but only lose $x/2 by switching.  Since I stand to gain twice as much ($x) as 
I stand to lose ($x/2), the total ME for switching is positive (potential gain - potential loss = 
$x - $x/2 = $x/2) and I should switch."  In fact, since this applies to all possible nonzero 
values that the player might see in E1 (a value of $0 would not permit the 2x-or-x/2 
condition to be met and can thus be excluded), it is not even necessary to peek into E1! 
Unfortunately, this takes us back to a 0-information state regarding the contents of E1 and 
E2, seeming to violate the 0-information resolution already given in Step 2.
But this is merely the result of confusing the observed value $x with one of the two values in 
terms of which the "2x-or-x/2" condition is properly given.  Concisely, since there are just 2 
envelopes in the game, there are just 2 values to consider, one being twice the other (or 
inverting the "one" and the "other", half the other).  That is, |Emax| = 2|Emin| AND |Emin| = |
Emax|/2.  Notice that we're mentioning just two values, |Emin| and |Emax|, and that the 
specific number x has no particular correspondence to either of them; in fact, this 
correspondence is exactly what is revealed by the game's final result.  On the other hand, 
the player is formulating his strategy in terms of an intermediate third value, x, which has no 
definite relation to |Emin| or |Emax|.  That is, the values in terms of which the player is 
reasoning in the paradoxical formulation, $2x and $x/2, do not bear the same quantitative 
relationship to each other as do |Emax| and |Emin|; whereas |Emax| is two times |Emin|, 
$2x is four times $x/2!  This is the central fallacy in the reasoning of the last paragraph. 
To clarify, consider the following constraint(s) C1 and C2: 
|Emax| = 2|Emin| AND |Emin| = |Emax|/2.
Where |E1| = x, |E2| = 2x OR x/2. 

These two constraints are logically equivalent, but the equivalence is somewhat subtler than 
is usually supposed.  C2 contains not two but three values (x, 2x and x/2) corresponding to 
two variables, E1 and E2, bearing no specific relationship to the C1 variables Emax and 
Emin in the context of any single two-value game.  When x is held constant, the possibilities 
given for E2, namely 2x or x/2, define two separate games with two distinct total values, 3x 
and 1.5x, corresponding to the two possible expressions of y in terms of x, y = 2x and y = x/
2; thus, by holding x fixed and letting y vary, the player "splits the game" and thereby severs 
the algebraic continuity of ME.  Such a player is trying to trade not just envelopes, but 
games; in a misguided attempt to win a single game, he is really trying to choose one of two 
possible disjoint games over another.  But while envelopes can be manually switched, 
games cannot; one cannot up the chances that he will see a $20 bat rather than a $10 bird 
in his yard by activating a bird-repelling electronic bat caller, for although flipping the switch 
is a winning strategy at night, it is a losing one by day and has no causal bearing on 
whether the birds or the bats are flying.  ME is calculated not by supposing that the game 
one desires is the game he will turn out to be playing; instead, ME causally predicts the 
outcome of a single game in terms of the player's position within it, and must be calculated 
with respect to one game at a time. 
Is it possible for the player to increase his winnings by trying hopefully to play (and win) a 
higher-stakes game at the expense of losing a lower-stakes game?  Suppose that half the 
time, the more valuable of two disjoint equiprobable games is being played, and that when 
this is the case, switching will bring a win.  Since the win, being defined with respect to a 
more valuable game, has greater magnitude than the loss (which will also occur half the 
time), does it not still appear that switching is gainful?  At first glance, perhaps.  But as we 
have just seen, this appearance is based on an erroneous value spread that is twice what it 
should be; Emax is only twice as valuable as Emin, not four times as valuable.
Note that it makes no difference what the value of x might be.  Since the prescribed value 
distribution |Emax| = 2|Emin|) is symmetric and invariant with respect to x, so is any valid 
strategy based solely on that distribution.  The problem is that while the strategy must also 
be invariant with respect to y - y is, after all, an unknown quantity - it makes the mistake of 
adopting y as a critical parameter; as y "varies" with respect to x, so does the strategy's 
prescription for a "win".  Since the player is in effect betting on the value of y, the switching 
strategy presumes its own success; it is a random temporal loop that closes on a loss as 
often as a win (since the player begins with Emax half the time, switching - or for that 
matter, not switching - is a losing strategy half the time).  Since x tells the player nothing 
about the relative locations of the strategic variables |Emax| and |Emin|, it yields no 
strategic information and leaves him in an indifferent, 0-information mode.   
If the reader now regards the identification and correction of this fallacy as a no-brainer, he 
or she is absolutely correct.  This is what lends an element of humor to some of the 
convoluted resolutions that exist for the 2-envelopes paradox.  Here is one muddled (but not 
entirely wrongheaded) attempt gleaned from an Internet search: 
"The expected value of this gamble can not be taken.  The assumption that there is a 50% 
chance of doubling or cutting in half your money is not valid.  To take an expected value you 
must know absolutely what the possible outcomes are.  In this case the money in the other 

envelope is not random.  There is either a 100% chance of it being twice your envelope or a 
100% chance of it being half your envelope.   Here is another way of putting it: suppose the 
amount in envelope one is x and the amount in envelope two is y.  Assume you chose 
envelope x.  Then [where E = expectation] does E(y)=1.25*E(x)?  If so then E(x) must equal 
1.25*E(y) if your first choice were envelope y.  If E(x)=1.25*E(y) and E(y)=1.25*E(x) then 
the only values that satisfy both equations are x=0, y=0, which does not meet the condition 
that both envelopes contain a non-zero amount of money. So the argument for switching 
has been disproven by contradiction." 
This "resolution" correctly identifies one facet of the problem: there are two disjoint 
certainties (corresponding to two separate games).  But since these "certainties" have 
indifferent probabilities from the player's perspective, randomness still obtains; he has no 
idea which of the two possible games he is actually playing, and this forces him to reason in 
terms of two possibilities.  After all, it is the (subjective) knowledge state of the player that is 
important in formulating a strategy, not the deterministic (objective) actualities about which 
he is reasoning with uncertainty; by definition, key actualities underlying a given inductive-
probabilistic scenario are subjectively inaccessible.  So the error in the player's subjective 
rationale remains unidentified, and it appears that a mere "proof by contradiction" is 
insufficient to rehabilitate the theory of expectation and rationality.  A constructive resolution 
is evidently needed. 
There are any number of other possible "resolutions" boiling down to one obvious fact: 
whereas any valid strategy converts internal information about a game into mathematical 
expectation, the 2-envelopes game contains no such information for the "switch" and "do 
not switch" strategies to convert.  They are operators permuting indifferent values of 
indifferent probability, and are thus indifferent strategies.  But again, this does not explain 
why one of these strategies looks like a winner while the other looks like a loser. 
Rehabilitating ME requires not only an explanation of why and how this appearance exists 
(as just given), but a corrective adjustment of erroneous strategic reasoning.  Having 
already set the stage, we complete this requirement in Step 4. 
We must now mention a class of attempted resolutions which, in the absence of explicit 
information on the relative likelihood of |E1|, rely on the placement of a particular observed 
value in an implicit distribution “from which the values must have been taken”.  While it is 
true that the values in an objective, real-world game can be assumed to have come from an 
objective, real-world distribution, strategizing in a state of subjective uncertainty regarding 
one’s own position in that distribution is an exercise in inductive rather than statistical 
probability.  That is, it involves unknown sets rather than known ones, and probabilistic 
indifference rather than equality or randomness; it is a matter not of objective distributions, 
but of a subjective lack of information regarding them.   
Step 4: A Refinement of Step 2
The constructive resolution of Step 2 stipulates that the player has no information on the 
contents of either envelope; thus, insofar as no envelope can contain two different amounts 
of money simultaneously, the envelopes can be considered to "vary symmetrically" with 
respect to each other.  In Step 3, this stipulation is seemingly violated by the player's 

observation of a specific amount $x in the envelope he holds.  But as we showed, this 
observation (|E1| = $x) is decision-theoretically irrelevant to the question of whether the 
player holds Emax or Emin within the single game he is playing (as opposed to the set of 
two disjoint games predicated on a fixed value x).  In reality, there is still no information on 
the relative value of one specific envelope (E1 or E2) to the other (E2 or E1 respectively). 
So instead of treating the observed value x as suggested by C2 - instead of falling into the 
comfortable illusion that x affords real information on which to base a winning strategy - we 
are better off expressing x in terms of the variables given in C1: 
x = |Emax| OR |Emin|    
Now, where |G| is the total (unknown) value of the game, C1 implies that 
x = 2|G|/3 OR |G|/3 = |Emax| OR |Emin| 
Note that this does not imply any variation in |G|, but only in the player's knowledge of |G|, 
and specifically in his knowledge of whether his own envelope E1 corresponds to Emax or 
Emin. 
Now, if x = |G|/3, then 2x = 2|G|/3 and the player stands to gain  
2|G|/3 - |G|/3 = |G|/3,  
whereas if x = 2|G|/3, then the player stands to gain 
-x/2 = -|G|/3.   
Thus, the player's total ME is 
|G|/3 + -|G|/3 = 0. 
Note the resemblance between the Step 2 resolution and this one.  Whereas the former 
expressed the symmetrical variation between subjective frames E1 and E2 in terms of 
(then) unknown quantities x and y, the latter expresses it in terms of the correspondence 
between x = |E1| and the set {Emax,Emin}.  Whereas Step 2 let the player hold each frame 
constant in turn and vary the other with respect to it, our new resolution lets the player hold 
x constant under all conditions and vary the "game orientation" with respect to it; instead of 
circling subjectively to the "other side of the game", he remains fixed and "rotates the game" 
around his own position.  Thus, the fact that x has an observed and therefore subjectively 
constant value can be accommodated within the basic Step 1-Step 2 multi-frame resolution. 
Step 5: Some Preliminary Implications for Economic Theory 
The above reasoning has important implications.  These can be glimpsed by considering 
that when confronted by paradoxes devolving to a lopsided fixity of the player's subjective 
frame, we achieved resolutions by letting each frame vary in terms of the other.  The 
analogy with physics is obvious; when a "motionless" observer O1 perceives another O2 to 

be moving at constant velocity with respect to him, the "moving" observer can turn the 
tables, regarding himself as motionless and O1 as moving at constant velocity in the 
opposite direction.  This is called "Galilean Relativity".  But the story does not end there; 
relating subjective frames in terms of decision-theoretic invariants like the fixed total value |
G| of a game, and the 2x-or-x/2 constraint, invokes analogies with a more complex kind of 
relativity, the “special relativity” of Einstein.  The foregoing treatment suggests that value 
and expectation are relativistic in the full algebraic sense, not mere "absolutes" that can 
always be adequately characterized in terms of numeric constants.  
Let us expand on this a bit.  Economies relate subjective scales of value.  People 
experience need of, and therefore place subjective value on food, shelter, clothing and 
transportation.  Because these goods and services must be gathered, manufactured or 
performed by specialists, they tend to be concentrated rather than uniformly distributed 
among the populace.  This creates pressure for redistribution, and if redistribution is 
governed by individual (as opposed to collective) rationality, these items must be 
exchanged among individuals according to the law of supply and demand.  The medium of 
exchange is called an “economy”.  Because individual subjects place different values on 
different goods and services, values must be defined relative to subjective value scales, and 
economic transactions translate one subjective value scale into another.  The evolution of a 
consensus regarding the relative values of various goods and services permits the evolution 
of a convenient universal standard of exchange, “money”, in terms of which this consensus 
is expressed.  Nevertheless, value remains basically subjective in nature, and local 
deviations from standard valuations thus remain common.  Even as an economy freely 
evolves and economic forces converge, consolidate and split apart, transactions continue to 
be driven by individual need, and initiated or authorized at the subjective level.  
Bearing this in mind, look at a generic economic transaction in the context of a minimal 2-
envelope game in which each envelope is held by a competitive player and the 2x-or-x/2 
constraint is absent.  In almost all cases, the players in the transaction hold different stakes 
of which the objective values are probably different; one stake will almost certainly be more 
valuable than the other.  “Win” and “loss” may then be defined in the obvious way, the 
player emerging with the more valuable stake being the “winner” and the other being the 
“loser”.  And the same basic rationale – “if I lose, I lose just what I have, but if I win, I win 
more than I have” – applies.  The only difference is the kind and amount of information 
available; players in the envelope game have no specific information about the objective 
values of their stakes, while those involved in normal economic transactions have at least 
some information from which the relative values of stakes may be subjectively inferred. 
Were there really such a thing as absolute intrinsic value, the resolutions given for the 
Kraitchik and 2-envelopes paradoxes would be final.  Each player could reason strategically 
from an elevated multi-frame perspective to avoid the essential fallacy of these paradoxes. 
Unfortunately, economic uncertainty makes assessments of absolute value all but 
impossible; the Kraitchik rationale in effect becomes a subjective vote of confidence in one’s 
own opinions and projections, and all one can hope to do is allow for the dynamics of 
interacting subjective frames.  Although the Kraitchik and 2-envelopes paradoxes deal with 
games whose rules seem artificial, these rules turn out to be general; interframe differentials 
in subjective value account for the ubiquity and validity of the Kraitchik rationale in games 
which locally appear to be 0-sum, but need not be so in the wider contexts to which the 

players are subjectively linking them…contexts that ultimately merge in the global economy, 
precipitating cooperation and competition leading to expectative conflicts.  Indeed, 
relativism based on subjective value differentials expressed in a global “spacetime” of 
transactions or "economic events" is what allows a locally 0-sum game to be globally 
advantageous, contributing to an overall win-win scenario in which the economy undergoes 
real expansion.   
Once we suspend the 0-sum criterion that makes the Kraitchik rationale “fallacious”, its 
status changes from that of a fallacy to that of a true “law of economics”.  Being the 
distributed basis of collective demand-pull and cost-push inflationary scenarios – the former 
works in specialized subeconomies whose players compete for resources to produce a 
certain kind of salable item, while the latter pushes the resulting inflation outward across the 
subeconomic boundary - this law drives inflation; but since the creation of wealth is driven 
by subjective motivation, it is also what drives legitimate economic expansion.  Two real-
world conditions, ambiguity of value and value differentials between subjective frames, 
create relativistic scenarios whose expansive and inflationary effects diffuse throughout the 
economy via inflationary mechanisms whose subjective basis was previously not well-
understood.  In self-interestedly betting on themselves and the futures of their local 
subeconomies, players create the global economy and determine the global parameters of 
value.  Here, the Kraitchik and 2-envelopes paradoxes give way to an economic analogue 
of paradoxes involving sets that both determine and are determined by their elements, e.g. 
the paradoxes of Cantor and Russell. 
What, then, are the rules in terms of which frame-invariant economic calculations should be 
made, and these abstract economic paradoxes resolved in the real-world economy? 
Unfortunately, the answer – a general theory of economic relativity - will have to be the 
subject of a future paper.  
Letter from Chris Langan
Greetings. In addition to wishing all of you the very best of holidays, it is my honor to join 
with you in launching what promises to be a new era of mutual understanding and solidarity 
among the profoundly gifted. Formerly sequestered in isolated groups with little interest in 
cooperating or communicating with each other or the world at large, the cleverest minds on 
the planet have finally decided to link arms in one globe-spanning electronic network…a 
network whose tensile strength resides not only in good will and hope for the future, but 
mutual respect and an insatiable hunger for knowledge and intellectual adventure. Under 
the aegis of the Mega Foundation and the enthusiastic guidance of the finest minds in the 
Ultra-HIQ community, the Ultranet has the potential to focus many bright but divergent rays 
into a high-powered laser, and bring to critical mass what was until recently a far-strewn and 
all but invisible planetary resource.
At the dawning of the Third Millennium AD, we find ourselves at a point of convergence…a 
point at which hope and courage are essential to the future of our species. But hope and 
courage alone will not suffice. For the convergence is one not only of retrospection and 
expectation, but of joint self-determination, and this requires depth of insight as well. In a 

world of boundless possibility, the primary ingredient of self-determination is the ability to tell 
fruitful ground from barren, the strong bridge from the weak. And who possesses this ability 
in greater measure than the severely gifted? Here on the threshold of the future, we are no 
longer to be taken for granted, no longer a crudely harvested item of commerce to be 
thoughtlessly bought, sold and wasted by an indifferent establishment. Now we are poised 
to become the eyes and brains, and ultimately the lifeline, of humanity. Merely by giving 
each other our respect and cooperation, we can meet our responsibility to the world and 
demand its appreciation and support in return.
Remember, the right tail of a normal curve does not the world make, and the part of the 
world that fills the bell has no particular understanding or appreciation for those of us out on 
the rim. Left to its own devices, it will continue to underrate and undervalue the treasure we 
possess, and monotonously dictate its one-sided terms to those of us who possess it. 
Mindlessly substituting bureaucratic protocol for true intellectual ability, it will continue to 
pretend, with increasingly disastrous consequences, that intelligence is nothing more than 
an effortlessly acquired commodity, an article of equal opportunity commerce that can be 
dependably and profitably wrung out of the fabric of society at its one-sided convenience. It 
follows that the understanding and appreciation we deserve must initially come from within 
our own ranks, generated and conducted exclusively by the nodes and edges of a uniting 
plexus. Let this be our plexus, and let us do all that we can to help it meet not only our own 
needs - needs that begin with intellectual fellowship and nurture - but those of a world 
increasingly dependent on our abilities.
Since the dawn of civilization, those under the IQ curve's central bulge have proven unable 
to meet us on our own ground. Perhaps the most obvious reasons are that (1) being 
separated from us by a psychological gulf, they cannot, and (2) what nature has divided by 
sheer statistical rarity can be easily suborned and absorbed. The world does not necessarily 
scorn us as much as it misunderstands us, and it has simply taken the path of least 
resistance in dealing with us. But in any case, the result has been a decidedly unhealthy 
trend in the chaotic history of mankind. So let us fashion our own world - a world based on 
common aptitude, woven seamlessly of electronic thread and free of impenetrable dividers - 
and with much to gain and nothing to lose along the route, meet each other there instead.
It is in this spirit that I thank those of you who have already dedicated your time and energy 
to the cause we share, and invite all intelligent people to join us in our long and impatient 
reach for the stars. For if and when the stars are finally reached, it will be primarily through 
the efforts of minds like ours.
Christopher Michael Langan

Noesis Discussion #1
Member Ray Wise has apparently moved. His copy of the last issue of Noesis was returned 
with notice of expiration of forwarding instructions. If any other member has his new 
address, he (or she) should send it to me or to James Hajicek, who has considerately 
been providing address labels.
I recently got a letter from Ron Hoeflin. This letter, which was apparently written before he 
had finished reading issue #44, is partially concerned with matters of policy. My 
response will follow the section on metagames.
I must tentatively qualify a statement made in Noesis no. 44 concerning a policy attributed 
to Martin Gardner. On the chance that my source might have been inaccurate, I mailed 
a copy of that issue to him with a covering letter. His response suggests that the policy 
either never existed or has been revised; as to which, his letter leaves some doubt. 
Regarding the paper on Newcomb's paradox, he prefers to withhold his comments 
pending more thorough evaluation. In the mean time, he thoughtfully provided copies of 
his most recent remarks on the subject, as well as an extensive bibliography. This 
bibliography is twenty-seven entries long, and covers the period from 1969 to 1985. It 
indicates that this paradox, of which I understand some members had never heard prior 
to its mention in Noesis, has been a hot topic in philosophy for over twenty years. To let 
those members unfamiliar with the history of Newcomb's paradox get some idea of its 
notoriety, I have taken the liberty of including the bibliography in this issue (pp. 11-12).
Because I usually work from basic principles, very few of the entries in this list played any 
part in my thinking. However, one cannot help but notice that the theory of 
"metagames" figures in several of them. This theory, with which I am familiar enough, 
involves joint functions of subjective utility. There are certain situations, including the 
famous prisoner's dilemma, in which the maximization of collective utility is inconsistent 
with reasoning based on individual self-interest. Some n-player games can have more 
than one winner; ND's 2-player game can obviously have two, and in fact must have 
two if the outcome is not to be severely unfavorable to either player. Consider the 
Newcomb payoff matrix
                 | ND: predicts black box | predicts both boxes|
Subject:          |------------------------|--------------------|
takes black box   | $l,000,000 | correct   |   $0   | incorrect |
----------------------------------------------------------------|
takes both boxes  | $l,001,000 | incorrect | $l,000 | correct   |
                  |------------------------|--------------------|
and note that the upper left entry is the only joint optimum, given that ND places a high 
premium on his unbroken veracity.

Observe also that the expected utility argument, as presented in Noesis no. 44, favors 
different moves according to whether mm does or does not have significant confidence 
In Nil's predictive powers. If he does not - even in the face of all the evidence - then he 
might regard ND's move as virtually random with respect to his own choice, thus in 
effect denying any mechanism of dependency between what ND does and what he 
does. This may cause him to assign equal probabilistic coefficients to both of ND's 
possible moves, which causes the sum of the expectations in row two of the matrix to 
exceed that of row one (of course, this casts considerable doubt on ND's own 
rationality, given the arguments presented in The Resolution of Newcomb's Paradox).
If MN is thinking rationally even in his own light, he will at least see ND's perfect record as 
evidence that ND is a master of both human nature and decision theory. Specifically, 
MN will suppose that ND is somehow gauging his own level of intelligence and 
rationality, and - finding it high - will use it to "simulate" MN's calculation of strategy 
using abstract game theory. Thus, MN will reason to the conclusion that both he and ND 
will be applying game theory to the formation of strategy. While this is not the same 
mechanism of dependency contained in G, it is a median ism nonetheless.
Now, constructing the above matrix, MN will naturally observe that his no-confidence 
application of expected utility leads him initially to a move which is not a joint optimum 
(i.e., that in the lower left corner of the matrix). He also knows that ND, who may well 
suspect his lack of confidence, can apply the minimax principle to "cut his losses" given 
the likelihood of this move. Observe that minimax is a "metarule" relative to expected 
utility; it uses the viewpoints of both players to the advantage of one. Here, it causes 
ND to choose column two. Unfortunately for MN, the intersection of row two with column 
two occurs at an outcome decidedly unfavorable to him. Moreover, MN cannot at this 
point improve his lot by switching moves; the intersection of row one and column two is 
even worse than the entry below it. The game has reached a strategic equilibrium, 
which in its present guise seems more to him like a game-theoretic dead end.
What should MN do now? He is dead set against any revision of his disbelief in ND's 
prescience; furthermore, he considers it hypocritical and infra dignitatem to "pretend" 
that he believes when in fact he does not. What if he should pretend, but ND should fail 
to take account of it? Then ND stays with column two while he switches to row one, and 
both lose everything. Even if ND takes account of his is pretense and switches to 
column one, MNhas effectively confirmed his "prescience". This bothers MN, even 
though this kind of prescience is not quite as repugnant to him as a spin through G 
program-space. He has still been manipulated on the basis of his own perfectly 
predictable reasoning, and is quite within bounds to regard this as an unsatisfactory 
glorification of his supposed "free will". Therefore, he resolves to "pretend to pretend", 
switching back to row two after ND obligingly switches to column one. This seems 
rational in any case, since the switch is worth an extra $1000 to MN.

Notice that we have been led into another computative regression, this one even less 
tractable than the one we have previously defined as "G". On the basis of what MN 
considers to be perfectly rational assumptions about the nature of this game, game 
theory has led him in a circle that may repeat endlessly in a spider-and-fly parody of the 
Turing halting problem. In this twist on the old paradigm, each of two automata tries to 
determine the programming of the other from output simulated according to circular 
premises to which it is assumed to conform. Returning to the symbolism of the 
Resolution the state-transition functions dM, dMD, have become determinate but circular, 
whereas mM, mMD remain mutually uncertain, and in fact nondeterministic relative to this 
low-level version of standard game theory.
Enter the theory of metagames, which involves the construction of extended matrices by 
adjunction of "conditional strategies". The moves in such a matrix are "metamoves" 
which cover every possible contingency; each possible move by one's opponent is 
regarded as one cell of a linear array whose contents vary over all repetitive 
permutations of countermoves. E.g., take an arbitrary one-move, two-player game in 
which each player has possible moves x or y. The conditional strategy for player 1, "xx", 
means: "if 1 were to expect 2 to make move x, then 1 would make move x; if 1 were to 
expect 2 to make move y, then 1 would make move x." In this case, 1 has four 
conditional strategies xx, xy, yx, and yy. The next extension is formed analogously: 
given l's metamoves, 2's counter-strategy "xxxx" means: "if 2 were to expect 1 to 
choose xx, then 2 would choose x; if 2 were to expect 1 to choose xy, then 2 would 
choose x; similarly, yx1 —> x2; yy1 —> x2." Where the original matrix is regarded as step 
one in this progression of extensions, the metamatrix for step n ≥ 2 consists of 22n-1 x 
22n-2entries. This exponential blow-up will finally reveal all possible equilibria, including 
the mutually favorable ones. Further extension becomes unnecessary when new 
equilibria cease to appear in the matrix, or when the set of equilibria reaches "closure".
The first metamatrix for Newcomb's game is as follows (where move A means "take 
(predict) the black box only", move B means "take (predict) both boxes", the rows are 
MN's possible strategies, the columns are ND's conditional strategies, and the starred 
cell represents a mutually favorable equilibrium):
ND:    AA              AB           BA           BB
 ---------------------------------------------------
 |   correct    |*   correct  *| incorrect  |  incorrect |
A|  $1,000,000  |* $1,000,000 *|    $0      |    $0      |
 |--------------|--------------|------------|------------|
B|  incorrect   |   correct    | incorrect  |   correct  |
 |  $1,001,000  |    $1,000    | $1,001,000 |   $1,000   |
 ---------------------------------------------------------

On the (by no means solid) premise that an arbitrary subject has the wit, time and 
inclination to reason this far, the lopsided equilibrium described for the base matrix 
becomes inconsistent with reasoning motivated strictly by individual self-interest in light 
of mutual uncertainty. It is now apparent to MN that ND will choose conditional strategy 
AB in any case, since he thereby wins no matter what MN does. So MN must choose A. 
The intersection of A and AB is not only a joint optimum, but an equilibrium from which 
neither can make an advantageous switch. Individual self-interest is now logically 
consistent with joint utility.
If Newcomb's paradox were reducible to the superficial conflict between individual and joint 
calculations 0f utility, the theory of metagames would be the key to resolution. The 
theory has justly been praised as the logical equivalent of the golden rule, and even as 
a cure for the sneaky, self-interested game theory so long practiced by the cold warriors 
in top-secret military think-tanks. Unfortunately, there are at least three aspects of the 
problem which cast doubt on the solutative adequacy of metagames.
First, certain ingredients of the Newcomb formulation make it impossible to apply the theory 
retroactively to all of ND's former victories. The "plausible distribution" of players with 
respect to their moves indicates that their choices were reached by various rationales. 
These must necessarily have included rationales which did not take account of the 
theory, or which included it merely as one phase of a more complex train of thought. 
This latter possibility leads to the second inadequacy of metagames.
That is, the metagame scenario continues to involve the players in a computative 
regression dangerously like that of the classic Entscheidungsproblem, wherein each 
player is trying to compute the computative power and programming of the other. After 
all, there is nothing to prevent one player from capitalizing on what he presumes will he 
the other's good-faith adherence to the metagame collectivization of utility. If ND doubts 
MN’s reliability on this score, he will probably fixate on base column two (not much of a 
"solution" from MN's viewpoint). In many other such games, if each player suspects a 
"rat" in the other's programming, the regression can continue forever...provided, of 
course, that no effort is made to elevate one or the other from G0, the physical level, to 
G1, the level of programmatic constraint on G0, (note that if strategic collectivization 
were enforced by projection from the programmatic level, defection would be 
impossible, and this would indeed ensure a joint optimum).
This brings out the third weakness of metagames: they cannot account for the G0, evidence 
confirming ND’s G1-status. The Newcomb paradox is ultimately one of prediction, and 
prediction paradoxes differ radically from those confined to the same G-timetype. This 
is in fact the distinctive feature of Newcomb's paradox, and it is impossible to claim 
resolution without addressing it. But that is apparently the position in which some 
claimants find themselves. With due respect to their insights, the resolution in #44 
stands well above any based on game theory alone.
***

I recently received a handwritten letter from Ron Hoeflin, which contains permission to 
quote. I'll address its points in the order they were made, with the exception of some 
remarks on the paradox; these require a more detailed response. I must own to being a 
little confused by some parts of this correspondence. If any of what follows seems low 
on enthusiasm, please chalk it up to my confusion .
First, he prefers to keep the journal on a monthly schedule. So, I assume, does everyone 
else. Noesis #44 was dated "December-January" for a number of reasons. The most 
important of these was that it was not ready until late January; it had to lie completely 
rewritten after I decided to serialize the content of the original draft. Just as important, it 
contains the solution of a famous and complex problem - a "first" for Noesis - and may 
require more time for absorption by those members unfamiliar with its key concepts. It 
thus seems inadvisable to follow it up too quickly. As some readers may recall, this is 
not the first time a month has been lost; the August-September 1989 issue also 
covered such a gap, and several other issues have been delivered well behind 
schedule. Nevertheless, #45 will - at Mr. Hoeflin's request - be redundantly dated. The 
February issue will be completed as time permits.
Second, Mr. Hoeflin remarks that "the lack of a strong central editor tends to turn each issue 
Into a monologue on the part of the person responsible for that issue, which I do not 
consider entirely satisfactory". This is true, as far as it goes. But there are several other 
important considerations that it ignores. The most salient of these is probably the 
distinction between "strong" and "qualified". It is sometimes not easy for those who lack 
the right background to grasp the complex issues that might be expected to occupy the 
thoughts of extremely intelligent people. As a case in point, the debate over Newcomb's 
paradox - a matter that I could have settled definitively at the outset - went on for two 
years before being laid to rest. And this is by no means the only such example.
"Monologues", regardless of what anyone thinks of them per se, are not always avoidable 
or even undesirable. The history of science, mathematics, and philosophy is one 
monologue after another, more or less...the point being that an informed monologue 
outweighs any cacophony of inflexible opinions. Nor are monologues entirely the fault 
of the editors In whose issues they may have occurred. I have not heard of any past 
editor of Noesis failing to relay the opinion of any member who saw fit to offer one. The 
fact is, many of the members of this group have been quite unyielding in their reticence. 
More active members will no doubt forbear to criticize them. If they may in turn be 
forgiven an occasional monologue to fill the silence.
Mr. Hoeflin - who acceded, along with Mr. Erlandson, to my provisional tenure as editor - 
states that if said editor cannot closely adhere to a schedule of monthly publication, he 
"would prefer to find somebody who can".

Regarding the case with which a willing and qualified editor might be found, it has already 
become obvious to many members that the job is not without its onerous aspects. At 
the risk of seeming gauche, I will give one example: the total monetary cost to me of 
printing and mailing issue #44 was approximately $40. This says nothing of the cost to 
me in time for research and compilation. At least two central editors - Messrs. Hoeflin 
and Erlandson - have previously intimated the difficulty of producing a steady flow of 
interesting material in the absence of input from other members. As a favor to the 
Society, I am willing to assume responsibility as long as it intersects with certain other 
priorities, and only as long as I feel appreciated for my effort and expense. If there is 
any disagreement about this, I will be only too happy to defer to whatever 
counterarguments are presented, regardless of their soundness or motivation. Any 
blame may settle where invited.
Mr. Hoeflin states that "if I resume the editorship myself, your essays will be published as 
rapidly as space (and your own submissions of material) permit". This would not be 
unreasonable under ordinary circumstances. But as my own situation is unique, I 
should probably point out once again that my past submission of material was severely 
constrained by uncertainty as to space. I can see no reason to relax that constraint, 
given that the above statement seems to reflect no change in past editorial policy. The 
possible effects that this would have on the long-term quality of the journal can at best 
be hazarded from past data. I see no point in pretending that what I recently contributed 
is not in certain respects preferable to much of the material which preceded it. If the 
other members are also indifferent to the distinction, then perhaps this exercise is futile, 
and another editor should be found before I waste more of my time and theirs. Noesis is 
a forum of convenience, not of last resort.
Next, it is suggested that Noesis will probably never become a "prestigious scholarly 
journal" without benefit of non-member subscribers, and that the question of whether to 
allow them should perhaps be put to a vote, contingent on returning to one central 
editor. This is probably a good idea, on the condition that such subscribers understand 
and agree to certain considerations along the lines of those reprised in the next several 
paragraphs.
Mr. Hoeflin goes on to express the opinion that the editor is "vastly overconcerned about 
plagiarism of original ideas by other members". I find this hard to follow, having in fact 
expressed trust in the members in my opening comments. This is probably a reference 
to the four-page letter I wrote last year, when the name of the society had just been 
changed to "The Hoeflin Research Group". This letter described a kind of occupational 
hazard faced by professional scientists and academicians, and contained no 
aspersions on the members of this group. This may be readily confirmed by anyone 
willing to review pages 13-16 of the last issue. How this letter relates to the Society is 
clear enough. Papers can get lost or mislaid; copies of the journal can get misplaced or 
passed around, conceivably ending up in the hands of people who may not share the 
sensibilities or qualifications of the authors or those for whom they were intended. And 
"plagiarism" is only one aspect of a more complex problem. My apologies to anyone 
else who got the wrong impression.

In the same vein, it is subsequently remarked that "if others carry your ideas forward, that is 
legitimate…" To this, however, it is necessary to add "...so long as they do not in the 
process avail themselves of facilities to which you are denied access." I have already 
carried these ideas forward rather thoroughly in many ways, and see no reason why 
others should try to duplicate my own results unless they are doing so because they 
suspect that I am in no position to claim them publicly, and wish to take unfair 
advantage of their own connections or credentials. But again, I am more concerned 
about publish-or-perish types than I am about the members of this group. If this group 
were the problem, it would not have been privy to my results. And the intrinsic 
importance of these ideas, as well as the matter of how to further develop them, will 
probably not be entirely obvious to anyone who has not applied prolonged effort 
towards understanding them.
One more word on the matter of access. The system of sponsorship and review means 
essentially that if a previously unpublished author of scholarly articles wishes to have 
his work published in any of the majority of "important" journals, he will be required nut 
only to provide personal credentials - which I never sought and which I consider utterly 
irrelevant to any meaningful evaluation of ideas - but in many cases to subjoin his own 
name to that of a better-known and more "reliable" collaborator, irrespective of the 
actual contribution thereof. This is a kind of hypocrisy to which I owe nothing and feel 
no inward obligation.
Those with academic credentials long ago arrogated the right to certify "truth" partly on the 
basis of its sources. This practice, some aspects of which may be traced back to 
prehistoric rites of domination, reached its present enlightened form with the rise of the 
medieval guilds. Historically, its effects have ranged from mildly beneficial to ludicrous 
and reprehensible. With respect to me, this policy happens to be weighted towards the 
bad end of the scale. Credentials can never "take the high ground" relative to truth, for 
truth is the only ground on which they can stand. As the highest use of intelligence is in 
the quest for truth, this applies uniformly to any political credentials derived from the 
history of this or any other society. I sincerely hope that this explanation will be deemed 
satisfactory by others who might think my comments eccentric, of whom I request only 
the allowance that I might know my own situation better than they.
Openminded members wanting to understand my position might like to take a look at the 
bibliography provided by Mr. Gardner. It depicts the projection of Newcomb's paradox 
upon a background of credentials. The image does ample justice to the source in point 
of irreconcilability and chaos; the one thing that most of these entries have in common - 
besides inadequacy - is rigidity. Students of human nature know that people are never 
so adamant as when they attempt to defend weak opinions. One might thus suppose 
that many of these authors would not welcome an "outsider" whose own views make 
theirs look even weaker than they already do. Now, suppose that the problem's 
notoriety had caused polarization among the members of the relevant disciplines (a 
widespread phenomenon in ideological competitions). This would represent a 
considerable enlargement of the "unwelcome mat" on which the outsider stands.

Suppose one were standing on such a mat. If nothing else, this could entail a more exacting 
scrutiny of his "credentials". But if one had held such things intellectually useless, he 
might not have garnered enough of them to deflate too many stuffed shirts...whose all-
too-human accepting syntaxes are wont to exclude embarrassing truths from their 
computations. There may thus exist an atmosphere in which "credible" but overly 
ambitious theoreticians might feel encouraged to try a little quasi-plagiaristic sleight of 
pen. Of those who have bought so heavily into the system that this seems impossible to 
them. I must request that they spare me any lectures on its supporting dogma (I tend to 
be unsympathetic to dogma in general).
Academic credentials generally include things like advanced degrees, associates and 
references, past publications, former and current grants, fellowships, achievement 
awards, jobs in research facilities and universities, and so on. If laurels were strictly 
correlated with ability, the world of science and philosophy would topheavy with 
Newtons, Pasteurs, Gausses, and Kants. That this is not so is evident to anyone 
immune to hype. Unfortunately, to lack any such plumage can make one seem ragged 
and forlorn, a mere beggar at the gates of the mighty...and to no one so much as to the 
"mighty" themselves. If you've been to college, you know the score. Whoever has not 
seen it, has wanted not to see it.
Mr. Hoeflin points out that he has again changed the name of the group, this time to "The 
One-in-a-million Society". For my part, this seems rather less dignified than "The Noetic 
Society". However. in view or the fact that the new name has been given to The 
Encyclopedia of Associations and Omni Magazine, I will use both names in the 
heading.
Now we come to the part or Mr. Hoeflin's letter that requires a more detailed response. It 
was admittedly written before he had a chance to read the paper I wrote. I will quote it 
verbatim.
"I personally never heard of Newcomb's paradox until Chris Cole mentioned it. I don't feel 
sufficiently versed in probability theory to contribute any thing significant to the topic 
beyond what I briefly mentioned in one of my editorials - namely, that the problem 
seems to involve the interrelationship between action and knowledge, i.e., 'infinite' 
freedom of action versus 'infinite' knowledge. It may well be that, as in Heisenberg's 
Uncertainty Principle, the 'product' of these two factors is some constant, so that the 
greater the freedom, the less the knowledge must be, and the greater the knowledge 
the less the freedom must be. . . my own feeling was that the mechanism by which the 
Infinite Knower comes by his knowledge has to be spelled out better."
By any ordinary standards, these comments are quite well-put. However, my own 
background in philosophy leads me to remark that a great deal of philosophical 
terminology is either too nebulous, or overly complex and viciously circular in its 
abstrusities. One is often served a large portion of cotton candy or pretzels, when his 
appetite is geared to more substantial fare. Fortunately for those on stricter intellectual 
diets, the CTMU provides an appropriate conversion of terminology.

In the CTMU (Computation-Theoretic Model of the Universe), everything is given a 
computative construction. The necessity of this follows from the inability of the human 
mind to understand anything except by virtue of intellectual computation. Thus, terms 
like "knowledge", "freedom", and "action" must be made meaningful with respect to 
various classes of G-subautomata. In this case, we need go no farther than the 
elementary theory of cybernetics in order to clarify the relationships to which Mr. Hoeflin 
alludes.
Consider the cybernetic relationship between variety and constraint. "Variety" is that 
property of a set defined as the number of distinct elements it contains, regardless of 
their order of occurrence or degree of repetition. "Constraint" is a relation between two 
sets, defined as that by virtue of which the variety of a set under one set of conditions is 
less than the variety of a similar set under another set of conditions. Since, when one 
goes up, the other goes down, and vice versa, they are related by an informational 
invariant similar to that embodied in the Heisenberg uncertainty principle of quantum 
theory.
When the elements of a set are modeled as vectors, as they are in physics and the study of 
neural networks, they have components analogous to the dimensions of a Euclidian 
vectorspace. These may be used to define an upper limit on the variety of the set. 
Where the actual variety of the set is observed to be less than this limit, we may 
consider that number of components which, in the absence of constraint, would give the 
observed variety. This number of components is said to quantify the "degrees of 
freedom" of the set. Logically speaking, the addition to a vectorspace basis of an 
independent (orthogonal) dimension is the adjunction to the vectoral syntax of a base-
space-inderivable, universally-quantified range of predicates. This range is a new 
degree of freedom relative to the structure, receptivity, and behavior of (vectorally-
modeled) automata. With respect to the conserved total variety of an unconstrained set, 
its post-constraint degrees of freedom relate complementatively to degrees of control.
What is "knowledge", relative to a given acceptor F? Usually, something known is 
something recognized. Knowledge is thus defined on a combination of input, state 
transition, and acceptance. The input alphabet S of F has a variety equal to the number 
v = |S| of distinct symbols it contains. The set SN of input strings s, with variety ∞S0 (vI) 
(i = 0, 1, 2, ...), is not only limited with v, but is constrained by the M-automated 
algorithm d (q,s), each phase of which corresponds to one of a system of "goals" 
related within it. This system amounts to a hierarchy of constraints, each of which 
further reduces that variety of Sn which remains after the last phase of recognition. 
Knowledge may thus be categorized by levels of organization corresponding to the 
stratification of d. Randomly memorized trivia is low-level knowledge; highly organized, 
goal-relevant information is higher-level knowledge. But there exist higher kinds yet.

A simple paradigm is easy to provide. Suppose you have always lived in a labyrinthine 
enclosure, and that an algorithm exists by which you can always determine your 
location within it. The algorithm first tells you which half of the enclosure you are in. 
Once you know that, it tells you which half of that half you arc in, and so on until you 
know your exact position. At each step, knowledge is gained in the form of a further 
constraint on your whereabouts. Now, suppose that in the course of your explorations 
of this enclosure you discover something entirely new: a window. Looking through it, 
you see for the first time an "outer space" replete with planets, stars, and galaxies. You 
have gained a whole new range of knowledge; the primary constraint represented by 
the boundary of your enclosure has been suspended. Were you a logician or a 
topologist, you could even go on to construct "windows of abstraction" through which 
new dimensions could be glimpsed. The Newcomb transducer MN happens to have 
such a window glazed with empirical data - i.e., data from observations of the 
paranormal efficacy of Newcomb’s Demon.
Knowledge has one of two bearings on recognition. In its common form it is identical with 
constraint and narrows the variety of an algorithm's domain as information is gained. In 
the form taken by knowledge in The Resolution of Newcomb's Paradox, it represents a 
general extension of the accepting syntax, or equivalently, the suspension of a primary 
constraint. On one hand, we have knowledge as it is usually defined; on the other, we 
have a new parameter of knowledge amounting to a new degree of cognitive freedom. 
By the manner of its induction, this adjoined parameter is sufficient to effect the 
computative nesting of one transducer within another, and of both within an arbitrary 
mechanistic extension of standard reality.
Where "action" is considered synonymous with the output behavior of a transducer M, a 
vectoral representation of M's range of output promotes consideration of its degrees of 
behavioral freedom (relative to specific goals and conditions, freedom can often be 
absolutely quantified; here, it is instead considered as dependent on recognition). 
Algorithms, especially strategic ones, often mirror an external reality in which concrete 
objectives are defined. Such objectives, unlike victory in Newcomb's game, cannot 
always be achieved by means of a single action. In cases where action elicits a variable 
dynamic response critical to algorithmic progress, it can be considered a part of the 
algorithm itself. In this algorithmic externalization, the outer dynamic becomes part of 
the device in which the algorithm runs. Viewed in this light, "freedom of action" is seen 
to be constrained in the same way as computation. This means that where knowledge 
is restrictive, it does indeed limit (degrees of) freedom of action.

However, extensional knowledge (like that which our labyrinth-dweller found through the 
window) and transcendental knowledge (like his mental discovery of another 
dimension) do the opposite, and these kinds alone can resolve such paradoxical 
inconsistencies between logical and probabilistic dependence as that which figures in 
Newcomb' s paradox. So it is not strictly correct to identify the relationship between 
knowledge and freedom of action with that between constraint and variety. Rather, it is 
more accurate to say that the relationship of knowledge to freedom depends on the 
relevance of knowledge to constraint, where all of these terms are relativized to a 
coherent algorithm. Sometimes we have covariance like that mandated by the 
Heisenberg uncertainty principle, and sometimes not. One need not go too far afield to 
find cases in which knowledge constrains in order to promote a subsequent range of 
freedom, or where it frees and constrains at once.
The abstract organo-robotics of the Resolution aids precision.
depicts a feedback loop in which the outward results of M’s output become parametric over 
d and thus in effect over m. The past behavior of M thus governs its future behavior by 
introducing, relaxing, or eliminating constraints within d (where d is defined as self-
modificative on this basis). This may result in (M-recognized) degrees of freedom being 
lost or gained by TN.
Where knowledge constrains, frees, or transcends action, it is a predicate of action. Yet, 
where knowledge is actively computed, it is action…“action of higher type". It is thus 
pointless to speak of unstratified knowledge or action, "infinite" or not. Any attempt to 
do so will lead inevitably to logical contradictions by violations of type. Moreover, as 
"infinite" knowledge can signify a total absence of constraint in a partial set of 
parameters - the remaining parameters being constrained — infinity must itself be 
parameterized by specific G -subautomatonic predicates. The related complexities, 
though well in hand, exceed our present purposes and must therefore be deferred to a 
future discussion.
Copyright 1990 by C. M. Langan. All rights reserved.
***
(The following annotated bibliography was sent to me by Martin Gardner. In view of the 
limited circulation and nonprofit status of Noesis, I assume that he would not object to 
our use of it. His own comments on the paradox may be sampled in the Mathematical 
Games Department of the July, 1973 issue of Scientific American, as well as in his 
books Knotted Doughnuts and aha! Gotchca (W. H. Freeman), and The Whys of a 
Philosophical Scrivenesr (Morrow). I will try to address some of these comments in a 
later issue.)

Noesis Discussion #2
The Society has a new member, who qualified by his score on the Mega Test. He is:
George W. Dicks, Jr.
[Address and telephone number omitted.]
He requests that his regards be conveyed to the other members. Anthony J. Bruni has 
changed his address
[Old and new address omitted.]
The addresses and telephone numbers of members, incidentally, are for the use of other 
members in contacting them, and are not intended for dissemination outside the group. This 
reminder follows a letter I received from one of you regarding the nuisance potential of such 
"leaks". Some are obviously more susceptible than others to such problems; contact with 
nonmembers might even be welcomed in some instances. But please use as much 
discretion with regard to personal information on other members as you would expect of 
them.
I have received several more letters from Ron Hoeflin. Because they mainly concern 
Newcomb's paradox, I assume that he would not mind my using them here. His comments 
may exemplify the doubts of others regarding the resolution, so responding to them directly 
may be a good way to promote understanding. It is not my intention to humiliate anyone, 
and Mr. Hoeflin deserves to be recognized for his forthrightness in voicing his objections.
As form precedes content, I will deal first with the occasional need to use mathematical 
formalism in lieu of plain English. The Resolution of Newcorob's Paradox in fact contains 
minimal abstract symbolism, given the need to satisfy the formalistic requirements of such 
papers. The mathematical "chicken-scratchings" I subjected you to were necessary to show 
that I know how to map the abstract theory of computation onto Newcomb's paradox in such 
a way that theorems of computation can be applied to resolution. If anyone else was badly 
confused by this maneuver, please understand that my intention was quite the opposite. If it 
is any consolation, you were spared many other important symbolical formalities.
There would seem to remain some doubt as to whether deciphering such ideas can 
possibly be worth the trouble. Such an attitude is utterly inconsistent with their obvious 
importance. At the risk of seeming immodest, I can supportably assert that the Resolution is 
among the most widely ramified pieces of reasoning that you will ever read. This owes in 
part to what it says about the reasoning process itself, a subject of frequent error and abuse 
even in the spheres to which it is most critical. Unfortunately, the paper's extreme concision 
tends to place such a burden of construction on the neophyte that misconstruction remains 
rather too likely. That, more than anything, has forced this to become a protracted affair, 
even as it silences complaints based on the "low importance" of "impossible" situations like 
that of Newcomb's paradox.
"Impossibility" is a widely misused term. It is the nature of possibility that it has meaning 
only relative to given sets of conditions or axioms. Thus, it is impossible to find a line which 

passes through a given point p, but which never intersects another line not containing p, in 
an elliptic geometry. On the other hand, it is possible to find one such line in a Euclidian 
geometry, but impossible to find more than one. And it is possible to find an infinity of them 
in a hyperbolic geometry. These distinctions would have been derided as lunacy prior to the 
advent of Gauss, Bolyai, Lobachevski, and Riemann, who dared to consider systems which 
did not incorporate Euclid's fifth postulate. Physics is no less axiomatic than geometry, and 
in fact incorporates geometry by necessity in its spatial determinations. So, inasmuch as 
physics is complete relative only to finite collections of data, one can neverproclaim the 
impossibility of anything without implicitly relativizing his hypothesis to just those 
circumstances precluding possibility. The situation is really very cut and dried, and has 
made a fool of many a serious intellect.
What one often means when one says "impossible" is "counter-inuitive". The intuition tends 
to rebel against anything alien to the conditions under which it evolved. Since these 
conditions are necessarily limited, the intuition is bound to abreact to much of what the 
mind, and reality, can conceive. The path of science is littered with shellshocked intuitions 
and the outworn notions to which they clung. Casualties of the forced march of progress, 
they lie mummified in cerements of library dust, where their unquiet rustlings beckon the 
unwary to join them.
"Unreal", which is as tricky a term as "impossible", appears in the following remark by Mr. 
Hoeflin. "I did agree with the solution you gave, as far as I could understand it. What 
interests me is the psychological question of why such an unreal situation as Newcomb's 
Paradox presents would attract so much interest on the part of serious thinkers. I suppose it 
does because it's a sort of limiting case, and ultimate limits, wherever they may be found, 
e.g., the simple example of computing pi, tell us about the ultimate geography of our 
conceptual universe. As a... philosopher, I was personally more persuaded by your 
metaphors and analogies than by your mathematical symbolism."
This passage contains several key concepts. There is, of course, the distinction between 
analogy and symbolism mentioned above. It is important to recognize that analogy is itself 
an inherently mathematical concept. It is virtually synonymous with the term "morphism", 
which signifies the mapping of one (structured) set to another. The distinction expresses a 
superficial difference among sets: some consist of numbers or symbols, while others consist 
of "natural" words or even concrete objects. Symbols are used in part to compactify 
linguistically-extensive definitions so as not to sacrifice apprehension of "pure structure" to 
obscuring detail, and save much paper in the mathematical literature. Trees are getting 
scarce, and the fewer sacrificed in the name of science, the better.
However, it is all too common for conceptual orientators to be compactified right out of 
existence in the process, and these are often needed by those unfamiliar with the full 
meanings and derivations of the structures. For instance, were an intelligent but 
mathematically illiterate person to be handed an abstract "epsilon-delta" development of the 
infinitesimal calculus, or the purely symbolic description of an algebraic group, he or she 
would be unlikely to think immediately of instantaneous rates of change, or symmetries of 
physical rotations. Rather, such a person would probably become embroiled in a frustrating 
attempt to track the symbols through their mutual, empirically unmotivated definitions, 
eventually falling victim to a truth regarding the nature of human intelligence: it is 

inseparable from motivation. While this does not imply that motivation and intelligence are 
identical, it bears mention for its extreme importance in psychometrics.
The "psychology" behind the notoriety of Newcomb’s paradox is elucidated by considering 
how the human mind models the procedural counterpart of such dilemmas. This counterpart 
is reflected in the ability of some minds to correctly prioritize a problem even in the absence 
of a conscious ability to solve it. The problem is "recognized" as important by a neural 
"acceptor", the brain, in spite of the nonrecognition of certain requisites of solution. It follows 
that either the criteria of importance differ in kind from those of solution, or that the brain can 
detect inconsistencies and representational inadequacies among the internal models 
defining its operational dynamic even when it lacks the ability to resolve them. This ability is 
not at all surprising; without it, human science could never advance. After all, the need to 
advance must be computable to that which does the advancing. The matter of real interest 
is how so many minds can suppress this ability in favor of the inconsistent and inadequate 
models themselves. Newcomb's paradox was invented, and granted importance, precisely 
because it is a compact, meaningful formulation of the scientific imperative. Its resolution is 
thus a formulation of the conditions for scientific advancement.
The supposed "unreality" of Newcomb's paradox is central to the issue. As already noted, 
this predicate is conditional in nature. To see why, imagine for a moment that you belong to 
a primitive tribe isolated from civilization in the heart of the Amazon rain forest. Suppose 
that an explorer appears one day, only to be surrounded by your fellow natives with their 
poison-dart blowguns. Fearful lest he become a tsantsa (shrunken head) in your collection 
of trophies and fetishes - and rightly so - he sets about to impress you with his "magical" 
powers.
Glowering, he produces unfamiliar objects: a (black, box-shaped) shortwave radio, a pair of 
magnets, and a gun. Naturally, you are quite unfamiliar with their names and functions. With 
his fingers on the radio's controls, he appears to hold a conversation in some unintelligible 
language with a god or demon inside it. You draw closer; the demon seems to raise its 
voice in warning. Quickly, you draw back. Next, the stranger points the gun skyward. There 
is a terrible noise and a flash of light, but nothing else seems to have happened...until a 
large, moss-backed sloth crashes to earth from somewhere in the canopy above you. You 
are relieved when the visitor kneels, places one magnet on the ground, and holds the other 
mysteriously above it. Muttering incantations, he brings it within an inch or two of the 
ground, and suddenly...yet another miracle happens! The lower magnet leaps up and 
affixes itself to the one in his hand. Three times, your "laws of physics" have been violated; 
thrice, you have witnessed unfamiliar effects that seem to lack physical mechanisms. You 
are elated when the stranger, now your honored "visitor", points at himself and says "Andy!" 
(or is it "ND"?), presents you with the magic jumping-stones, and takes his leave through 
the mist.
The "magical", or seemingly nonlocal, effects you have witnessed have explanations far in 
your technological future. They involve radio-frequency electromagnetic radiation, explosive 
chemistry and faster-than-vision ballistics, and magnetic fields. "Andy" seems to be above 
the laws of physics as you understand them; if you possessed the computative paradigm, 
you would consider him a kind of "metaphysical programmer" of "impossible" 
physicaleffects. In the language of the CTMU, you would consider him a projection from G 

to G0, the physical stratum of G.
Suppose, in a sadly unrealistic manner, that the equatorial rain forest in which you live will 
never be cut, burned, and bulldozed by greedy industrialists or ignorant and procreatively 
incontinent peasants. It follows that your primitive society may continue to evolve in 
isolation, perhaps to eventually develop a formal, axiomatic version of physics. Your 
descendants might then play the same game with the "Andy" legend that a modern 
physicist would try to play with the "ND" data. This game is the game of science, and it 
involves the restriction of G to G0. This is the process by which the paranormal becomes 
normal, and the miraculous becomes physical. It is the one and only valid logical syntax of 
science. It is not to be confused with the "scientific method", which can work only with the 
assumption of "locality"; this assumption, like Euclid's fifth postulate, is suspended pending 
actual confirmation in the process of G-restriction.
The locality assumption is suspended because predicating truth on it or on any other 
assumption is unacceptably biased, pseudo-tautological, and logically insupportable. This 
has been suspected (if not explicitly stated) by quantum physicists from EPR onward. The 
CTMU was largely motivated by the possibility of quantum nonlocality, and is in fact the 
only valid framework in which quantum nonlocality can be considered. Any "other" such 
framework must be CTMU-isomorphic, incomplete, or invalid. But more on this later on.
Thus, the "limiting case" represented by Newcomb's paradox - the apparent violation of the 
known laws of physics - leads to the CTMU, which is indeed the "ultimate geometry of our 
conceptual universe". Mr. Hoeflin has guessed correctly up to this point. However, his 
thoughts - perhaps like those of other members - take a reactionary turn, conforming to 
what probably seem the safer and more conservative viewpoints of established 
philosophers like Max Black. This change is reflected in his next letter, which will be treated 
point by point.
Before beginning, however, I will point out that my responses to Mr. Hoeflin's criticisms are 
redundant in the sense that they are implicit in The Resolution of Newcomb's Paradox. The 
paper was intentionally constructed to contain as much information in as tight a space as 
possible. While this was also true of my previous letters to the editor, their ineffectuality 
seemed to indicate an incomprehension so bottomless that I came to doubt that they had 
been read at all. In fact, the transductive model of human nature - in which people 
"recognize" only that input which coincides with their internal priorities - predicts just this 
kind of "selective perception". I therefore offer the following neutral observations. Closed 
minds are no longer in the running for truth; they run only in self-perpetuating, outwardly 
contradictory circles which serve static, falsely-completed sets of mental invariants. 
Transducers which can neither recognize their errors, nor accept correction, are doomed to 
err forever...or until the outward inconsistency of their output results in their destruction. 
This is apodeictic with respect to human beings as well. For many of us, the utility of 
recognition is split between venality and ego gratification. You, however deservedly, are 
being given somewhat more credit.
Mr. Hoeflin begins as follows. "It occurs to me to make two new comments on Newcomb's 
paradox. (1) The paradox seems analogous to the issue of whether a number sequence in 
an I.Q. test has a 'right' next number or not." This is quite true. The completion of number 

sequences is a matter of mathematical induction; a rule is derived which relates each 
number to its predecessor(s) in the series, and then re-applied to derive the next number. If 
the rule is incorrectly derived, then it must by definition produce within the given sequence a 
number which differs from the one which is actually there. Say that only in the last place sM 
of the series S does the rule R deliver a number x different from that given, y. Then the rule 
has been correctly derived with respect to {S - y}, and y is "anomalous" with respect to the 
derivation. That is, what was "supposed to happen" at "time sM" , given the preceding data 
and the "physics" R derived from it, was x. What actually occurred was y!
This is a "paradox", and to resolve it, we must modify R. Call this modification, or 
"theoretical advance", R'. R' supersedes R because it agrees with, or "predicts", what 
actually occurred (y) instead of what you had erroneously expected to occur (x). Note that 
the most general possible parametric extension that enables R', being both necessary and 
maximally unbiased with respect to future data, is justified by all scientific standards. Note 
also that where the parametric extension itself defines a rule and thus a series, that series 
characterizes a valid metatheory, or "metaphysics", for future extensions of R'.
This is precisely what was done in the Resolution. By the above reasoning, G is the 
universal metaphysics within which all future advances in physics and other scientific 
theories must be sought. Every problem on the "Mega Test", from verbal analogies on 
upward, involves the induction of a simple or complex rule or algorithm. The induction of G 
can therefore be regarded as the solution of the hardest and most important I.Q. test 
problem imaginable. That its original solver is a member of this group reflects well not only 
on the other members, but on the impressive name and purpose of the organization.
Next, Mr. Hoeflin remarks: "Some would argue that there is no right answer because one 
can concoct an algorithm that would yield any next number." This is a good point. It follows 
that we must employ some rule which selects among these various algorithms...a 
"constraint" which reduces their “variety” to just one. But this constraint itself characterizes 
an algorithm with which infinite others also compete. The regression is interminable. The 
answer? If we wish to be finally, absolutely, and forever right, we must concentrate on 
developing a universal syntax for the construction of meaningful algorithms on arbitrary sets 
and categories of data. The CTMU is such a syntax; moreover, it is minimal, and to that 
extent elegant.
Next: "On the other hand, one might say that the 'right' answer is the one generated by the 
simplest, most elegant algorithm that can generate all the previous numbers in the series." 
This is just a formulation of Occam's Razor. It is a tenet of the philosophy of science that 
there is an infinite number of potential theoretical extensions which contain any given set of 
facts as a subtheory. These extensions can be ranked by the informational content of their 
minimal reductions. Occam's Razor selects one of them for minimality with respect to a set 
of data exhibiting closure under apparent dependency relationships. The word "apparent" 
implies that there exists at the appropriate point in time a means of observing or testing for 
such relationships. But we have already seen that it is both futile and unscientific to place 
arbitrary restrictions on incoming data, including the restrictions of measuring devices and 
experimental arrangements. So we must always relativize the validity of our "simplest, most 
elegant" algorithms to observational limitations. Where these limitations characterize the 
very theory (methodology) on which the algorithm itself is based, we have another "artificial 

tautology"; the theory is a generator of self-fulfilling predictions.
Notice, however, that if a set of data displays only a subset of measurable attributes - e.g., 
only "whiteness" insteod of the artist's palette of colors the human visual system is capable 
of detecting - we can safely assume that mensural limitations are not in effect, and can 
"Occamize" the data to prevent spurious attributes from skewing our theory and its 
predictions. This is crucial to the solution of the trivial but toothworn "marble problem", as 
originally formulated by Mr. Hoeflin. If you remain unconvinced by previous arguments 
concerning it, imagine that you are a zoologist doing a study of montane fauna. Following 
the tracks of a cougar into a cave, you see a pile of well-gnawed bones. You select ten of 
them at random and drop them into your specimen sack. Suddenly, you hear a 
bloodcurdling scream; kitty is coming. You shoulder the sack and bolt. Back at camp, you 
examine the bones and find that all ten are from mule deer. It now occurs to you to calculate 
the probability that the cougar's only large prey is mule deer. You estimate that the pile in 
the cave contained approximately two hundred bones; you have sampled ten at random, 
and all are muley bones. What, then, is the probability that all of the bones were from mule 
deer, given the assumption that some definite number of (inedible) bones probably came 
from each animal who was eaten?
The crucial question, for our purposes, is whether or not you should begin your calculation 
by assuming that the pile is just as likely to contain the bones of kangaroos, tapirs, pandas, 
chimps, and Airedales as it is to contain those of mule deer (after all, there might be a zoo 
or a kennel nearby). If this seems spacious, notice that ochre and fuchsia marbles are not 
correlated with white marbles any better than these exotic species are correlated with mule 
deer. Obviously, some other species may in fact be known to inhabit the area, and nonzero 
probabilities must be allowed for them. But where no prior correlation exists, Bayesian 
inference is not to be initialized as though it does.
As I have remarked in a letter or two, probability is defined so as to be meaningless except 
relative to given data and data formulations. The potential for (probabilistic) information is 
not strictly equivalent to information. Rather, where such information is inderivable from past 
data given the best formulation of those data, it is undecidable with respect to the data 
axiomatization, and inutile for the derivation of theorems therefrom. Thus, there is 
potentially a nonzero probability that there are tapir bones in the cat's pile, but this 
probability is currently undecidable and cannot be used in Bayesian inference. Similarly, 
there is potentially a nonzero probability that solar physics is wrong or incomplete, and can 
neither formulate nor predict actuation of the mechanism by which the sun is about to go 
nova and incinerate the planet earth. But we cannot use this probability to calculate the 
likelihood that "tomorrow will never come" until we revise or extend the applicable physics.
Let us enlarge on the distinction between statistics and causality. Causality is just the most 
concise exact description of a set of statistical correlations, and can be regarded as the out-
put of an algorithm on statistical input. This algorithm, "logical induction", includes Occam's 
Razor and its metasyntax. What enters as "probabilistic dependency" emerges as "logical 
dependency"; the many-valued logic of the former, wherein probabilities are defined as 
truthvalues, has become two-valued by the formation of distinguishing predicates (or 
conversion of truthvalues to quantifiers). So logical dependency, or causality, is the 
inductive transform of probabilistic dependency. This transformation, being largely based on 

past data, can be rendered inconsistent by new data.
It has been remarked by statistically-minded scientists that the statistical correlations of 
which causal axioms are the compressed versions are themselves immune to this sort of 
vitiation. This is true but impractical. Because they must be ascertained by whomever would 
put them to use, such correlations are no safer in their "scattered" form than they are under 
inductive compression; they are ascertainable only by means of devices and techniques 
which are themselves based on past data. The means and conventions of observation, 
experimentation, and measurement which characterize any scientific methodology are often 
designed primarily to detect already-familiar kinds of correlations. To this extent, such 
means and conventions can themselves be considered the embodiments of past 
correlations and their apparent conditions, such as metrical locality. This tautology is no less 
artificial in its statistical guise than in its "theoretical" one; it reaches its formal limit in the 
mental structures of transducers using science to compute "objective" reality. These 
transducers project their structures outwardly, along with whatever limitations those 
structures ultimately impose on their internal models and conceptual algorithms.
Obviously, the structure of a transducer is a more absolute kind of limit than the momentary 
empirical or theoretical limitations of its science. The purpose of G is to push that limit out 
as far as possible, and then to define it as scientifically and as logically as possible. The 
structure of G is thus as deep and complex as the structure of the mind itself. And, as was 
pointed out in The Resolution of Newcomb's Paradox, it is that which can never be 
transgressed by the minds of which it is both a model and an outward projection. It is 
ultimate and inescapable, the only possible theory of its kind and generality. Anyone who 
proposes to extend human understanding without it, or something trivially isomorphic to it, 
unwittingly displays either ignorance or delusion.
Mr. Hoeflin goes on: "Almost all science and philosophy seems to depend on this search for 
the 'simplest, most elegant solution' consistent with the known data, with the proviso that 
new, unexpected data may occasionally arise, making one's 'simplest' solution wrong or 
inexpedient." This is one possible expression of Godel's Theorem, which can thus be 
interpreted as predicting the inevitable possibility of unpredictable data. We already know, 
from our above observations on the criteria for sound induction, that theories cannot be 
allowed to become self-limiting. A metric is a set of spatial relationships qualifying as a 
theory. So, in order not to make of it an artificial tautology, we must reserve a potential, 
however unlikely, for metrical violations. Associating this metric with G0 implies the 
incompleteness of G0. Accordingly, G0 —> G, and the CTMU is born. This, incidentally, is 
the spin-free derivation of quantum nonlocality alluded to in Noesis #43. No chicken-
scratchings are necessary.
Member Chris Cole's "spinless" version of Bell's theorem was expressed in #43 as follows: 
"It is not possible to reconcile the existence of an objective external reality with the 
exclusion of nonlocal instantaneous action at a distance". The purely inductive argument 
above can be reinterpreted in a computational setting after Turing, whose general results on 
the "halting problem" can be specified to the effect that it is impossible for any of n G-
subautomata to compute the metrical distribution of the programming of any other(s) on a 
real, local-deterministic basis, reality being computative mechanism - and quanta, G 
subautomata - for all computative purposes. I.e., the "theories" formed by r-subautomata 

about each other are no more justified in their metrical prejudice than the prejudice itself. 
Where time is defined on the particular spacetime metric accepted by MN, this effectively 
lets ND violate MN's time-directionality constraint.
Now we come to an inference which almost seems to imply that the person drawing it has 
not read the paper on which it centers. The antecedent of this inference is: "As Whitehead is 
supposed to have said, 'seek simplicity - but distrust it'." The consequent: "Whitehead's 
dictum, applied to Newcomb's Paradox, suggests that it is essentially an unresolvable 
paradox."
Regarding the antecedent, it is clearly paradoxical to advise that one seek something which 
cannot be trusted. The hyphenation not only divides the sentence, it demarcates a pair of 
syntaxes. In one, it is demonstrable that simplicity is an inductive criterion. This, of course, 
is the derivational syntax of Occam's Razor. But we already know that this theorem, which 
can only be applied to limited data, ultimately has no loyalty to data limitations; it 
meretriciously re-applies itself to any potential extension of the original data. Obviously, this 
is the sense - or syntax - in which it is to be "distrusted". That is, the first syntax is designed 
to exclude spurious attributes in the theoretical explanation of a finite set of data. The 
second syntax, a metasyntax of the first, re-admits formerly spurious attributes to the 
explanation, subject to their actual appearance in the data. In other words, the metasyntax 
mandates the extension of the syntax in accordance with (experimental or observational) 
extensions of the data. Everything depends on the data, which must be guarded against 
prejudice.
As we have seen above, this is just what the CTMU metasyntax G is designed to do for the 
natural sciences. It is stratified at the ultimate boundaries of the deterministic subsyntaxes 
defined within it, where those ultimate limitations are spatial, temporal, or phenomenal 
(attributive or existential). Whitehead's dictum, in its terse and pithy way, advocates the 
CTMU perspective. Since we have established that this perspective enables the resolution 
of Newcomb's paradox by relaxation of the time-directional constraint embodied in the 
"principle of restricted dominance", this aphorism in fact "suggests" the means of resolution. 
So much for the consequent of Mr. Hoeflin's inference.
But Mr. Hoeflin - who, I repeat, deserves congratulation for voicing his opinions - did not at 
the time of his letter have the luxury of my refutation, and redraws his inference from 
another argument. "(2) My other point yields the same conclusion but by a different route. 
There is a well known 'paradox' concerning the concept of omnipotence: 'If God were 
omnipotent, he could create a stone too heavy for him to lift - ergo, if God is omnipotent, 
then he is not omnipotent.' I think Newcomb's Demon is a similarly impossible construct." 
(This argument recalls a point made by K. Raniere, who opined in Noesis #30 that 
Newcomb's paradox should be restated in terms of whether or not it is possible to "make an 
absolute determination on choice without taking away choice".)
There can be nothing more oxymoronic than a transducer which considers its Designer, 
Effector, and User ("DEUS") to be bound by its own accepting syntax, particularly when this 
syntax has been thoughtfully endowed with the capacity to formulate extensions of itself by 
logical self-negation. Yet, the history of philosophy has been blighted by a universal inability 
to isolate and dissect the flaws in such pro-atheistic arguments as this. It will be both an 

honor and a pleasure to show that human logic contains within itself the means to disable 
such weak, but morally destructive, negative circularities.
"Omnipotence" implies an arbitrary ability to define and remove constraint. It includes the 
power of self-constraint; omnipotence allows God to create a constraint which applies to 
Himself. This is a stratification: God is at once That which constrains, and That which is 
constrained. This is no more strange than a Cretan who makes statements reflecting 
constraints on Cretans, as did the Cretan Epimenides when he said, "All Cretans invariably 
lie". Note that this latter statement implies neither that Epimenides is not a Cretan, nor that 
he cannot voice judgments on his race, himself, or any attribute applying to him. What it 
does imply, according to an application of common sense, is that whatever information such 
self-referential statements afford concerning their authors roust be carefully checked for 
self-negating inconsistency. We can believe the pronouncement of Epimenides only if we 
are willing, by a simple CTMU relativization of information, to except him from it at the time 
he makes it.
Similarly, those who wish to put self-negating words into the mouth of God, as it were, must 
make an exception for which they have themselves set the conditions. Thus, God can 
provisionally deprive Himself of omnipotence, but only by "simultaneously" exercising His 
omnipotence. The meaning of simultaneity is not the same as usual here; instead, it 
signifies the fact that God - in the hypothetical manifestation we are considering - occupies 
the same two levels of G as does ND. His (programmatic) omnipotence and His 
(programmed) non-omnipotence, set thus within the paradox-resolvent framework of G, 
cease to be paradoxical. Of course, the relationship of God to G goes well beyond this 
manifestation.
We can bring the situation even closer to home. Imagine that you are a chain smoker, and 
that you have just resolved to quit. You establish a constraint to that effect, willing it. upon 
yourself with all your resolve. Having done this, you can now ask yourself: "Can I have a 
smoke, or can't I?" The two voices in your head, one saying "yes!", and the other "no!", are 
just a nicotine addict's version of the "omnipotence paradox". That is, if you can choose 
whether or not to smoke, then you can choose not to smoke. But if you have chosen not to 
smoke, then you cannot (choose to) smoke.
Notice that this does not cause you to vanish in a puff of self-contradiction. You have merely 
stratified your intentionality. To keep observing your renunciation of tobacco, it is sufficient 
that you iterate this regression whenever the constraint weakens enough to let you question 
it. This example differs from the "omnipotence paradox" primarily in that its constraint does 
not impose absolute physical limitations on the dependent phase of the protagonist; this 
permits confinement of its regressive re-affirmation "within" the G0 timetype. Still, it is 
nothing short of astonishing that so common a predicament has not elicited more empathy 
among those willing to sacrifice their faith in the name of "logic".
Mr. Hoeflin now speculates in a direction expressly forbidden by the traditional formulation 
of Newcomb's paradox. "Suppose that in making my decision about whether to take one or 
both boxes I use a purely random basis for decision such as the decay of an atomic 
nucleus." Conveniently, my own stronger formulation relaxed the original constraint against 
basing one's decision on outward, otherwise irrelevant criteria. This was accomplished by 

arbitrary extension of the transducer MN into its physical environment, a maneuver 
described in my discussion of knowledge in Noesis #45 (p. 10, top paragraph). Mr. Hoeflin's 
condition thus has the effect of rendering MN's strategic algorithm dn nondeterministic, and 
the dynamic augmentation of MN a nondeterministic automaton. These possibilities, of 
course, were accounted for in the Resolution.
Next: "If the demon could predict this choice, then he would be doing the impossible, since 
the decay of that nucleus in a given period of time is random rather than deterministic." To 
be fair, this could be put into an average college physics textbook as a subatomic twist on 
the venerable oracle of thermodynamics known as Maxwell's Demon, there to reflect the 
shallowness of such literature with respect to terms like "impossible", "random", and 
"deterministic". Physicists, who often hold philosophy in contempt for its "fuzzy" and 
"meaningless" references, frequently fall into the same traps of language as those whom 
they scorn. Although the above point was dealt with in issue no. 44, a brief refresher may be 
in order.
The "nondeterminacy" of the decay of an atomic nucleus means that the schedule by which 
this decay occurs is to some extent not a function of physically measurable variables. 
Dynamically, this means that the event has an "open neighborhood" effectively isolating it 
from known physical causes or dynamical mechanisms; the set of determinants of such 
events is not closed under known deterministic operations. This open neighborhood does 
just what it sounds like it does, exposing the event to ulterior determinants.
The apparent nondeterminacy of such events, as derived from the statistical formalism used 
to predict them, is an artifact of that formalism. To consider it any more than that is to make 
the theory of nuclear decay a source of self-fulfilling predictions. But we have shown above 
that this is a logically impermissible artificial tautology. Such tautologies attempt to restrict 
incoming data on the basis of limitations in theory, measurement, or past data, and are not 
good science. It follows that when you talk about the "randomness" or "determinacy" of 
phenomena, these predicates are tacitly relativized to your current knowledge and ability. 
To put is as succinctly as possible, uncertainty is just computationally-relativized 
nondeterminacy. As science is necessarily computed by experimentative and experimental 
automata, these automata cannot supplant uncertainty with "absolute" nondeterminacy. 
Because human scientists have G-subautomatonic models, this goes for them too.
G is designed as the universal mechanism of arbitrary "ulterior determinants". By definition, 
it is maximally unbiased with regard to these determinants and their effects; it even allows 
for what we would consider "nonphysical" determinants. Because it models the paradox-
resolvent stratification of our internal logic, it is immune to vitiation by paradox; it 
"regresses" in such a way as to provide arbitrary contextual extensions for paradox-
resolution. It features a niche for hypothetical creatures like the Maxwell and Newcomb 
Demons. This niche is safe from scientific encroachment, because it recedes as science 
advances. To put it another way, for every theoretical language expressing scientific 
constraints, there is a metalanguage in which we may express the potential for changes, 
suspensions, or augmentations of those constraints. G is the active, mechanistic 
counterpart of this (Tarskian) linguistic regression, ramified and explicated in the CTMU.
>From his premise, Mr. Hoeflin infers: "If the demon can make such a prediction 

successfully, then we would have to conclude that the universe is essentially deterministic 
at bottom rather than indelerministic." He then adds, "But we have no way of establishing 
such an assumption." This is a self-contained antinomy. If the demon is observed to make 
such a prediction successfully (and repeatedly), then we have "established" - by observing 
him - that there is a determinative mechanism allowing him to either predict or control the 
decay. Of course, this does not mean that the universe is "deterministic", but merely that 
there is an unknown (and possibly nonlocal) mechanism relating to the event...and that the 
demon can make use of it under the conditions of his trials.
Mr. Hoeflin concludes: "Ergo, the paradox proposes an essentially unsolvable problem, 
given the present state of our physical science." There is a difference between giving a 
precise, detailed solution for a problem, and showing that a solution must exist. The 
distinction is between the constructive and existential viewpoints. The simple yes-or-no, 
one-box-or-two decision problem central to Newcomb's paradox requires only that MN infer 
from empirical data that there can exist a solution to the secondary problem of how ND can 
predict or control the outcome of the game. To solve the decision problem, MN must show 
that the conditions for existence of a solution to the secondary problem are not "impossible" 
or "inconsistent" among themselves. G is the required framework, and we needed only use 
logic to find it. The benighted current state of our physical science bears only on the 
secondary problem. But it was ND, and not we, who had to solve the secondary problem, 
and our physics can only try to catch up with him.
Why, some might yet ask, do we need G at all? Can we not simply assume that a rational, 
conventional, "physical" explanation for the Newcomb data - e.g., physically-measurable, 
behavior-critical "N-waves" emitted and received by the human brain - is there to be found? 
This is in fact the "enlightened" side of the traditional approach to science. Whether 
unfortunately or not, this approach is rapidly converging on bankruptcy. Besides the many 
indominable arguments from logic and mathematics, the reasons include the very argument 
on which Mr. Hoeflin based his objections - the apparent nondeterminacy of quantum 
events like nuclear decay. Of particular importance is the apparent existence of nonlocal 
conservation laws which violate the locality criterion of physics.
Physics, as conceived by most of us, is totally dependent on the assumption that causes 
are physically linked to their effects. Nothing is thought to happen without adjacency and 
contact, where contact is matter-to-matter, field-to-field, or matter-to-field. Adjacency is 
totally determined by the metric, or set of spatial relationships, on which it is defined. So 
physics is inherently metrical in nature, and phenomena which violate the metrical 
adjacency criterion totally disable the associated physics. In this event, physics must be 
extended. G is the exclusive syntax of metametrical theoretical extension, where the 
"metametric" is an arbitrary relational extension of the limiting physical subtheory that we 
call the "physical metric". Because inductive criteria forbid us to rule out nonlocal 
correlations and mechanisms, G is not a theoretical "option", but a fully justified 
metaphysics. All those who care about science and understanding can breathe a sigh of 
relief that this metaphysics has finally been discovered, precisely defined, and explicated as 
the CTMU.
Physics has long fixated upon the geometrical properties of space in its deepest analyses of 
time and causality. The structure of matter, on the other hand, has been seen in terms of 

algebraic symmetries of components. But physics is process, and petroglyphic 
mathematical entities can yield only a limited amount of insight on their own. It is time to 
realize that dynamical processes are computed, and that the mysterious power granted the 
observer in relativity and quantum theory resides entirely in the computative essence of 
observation. The universe is automatonic in its parts and in its entirety. Though the CTMU 
has been ardently desired by the great of intellect since time immemorial, never before has 
the concept of machine been made powerful enough to derive it. Wait no longer. G has 
risen at last. Copyright 1990 by C.M. Langan
In Clarification of the CTMU and its Applications in Noesis
The following enumerated remarks are in response to those of Chris Cole and George 
Dicks in Noesis 57 (Sections 1 and II resp.).
SECTION 1
1) The CTMU is a theory of reality as mind, not "the universe as a computer". The latter 
analogy was given advisedly as a conceptual aid and as a means by which to apply 
mechanistic and computational formalisms to the model. It does not encapsulate the whole 
theory.
2) The components of the CTMU are not all new. The idea as a whole is new. Technically, 
there are no new ideas, just new combinations of old components. The CTMU is more 
deeply rooted in analytical philosophy and the foundations of mathematics than in practical 
computer science, which nonetheless plays a large part in it.
3) I know of the late physicist Richard Feynman. But who is Ed Fredkin, and where can his 
ideas be sampled? If Chris means to give him credit for the CTMU, where are the 
applications? Given the current mania for computer simulation of natural processes, it would 
be incredible if no writers or scientists had latched onto the general notion of a 
computational universe. However, we don't give those who make mathematical conjectures 
automatic credit for proving (hem as theorems, we don't give Michelson, Morley, or Lorentz 
credit for Special Relativity, and we don't give Jules Verne credit for the deep-sea 
submarine. Credit is reserved for those who display greater understanding of the ideas in 
question--for example, by application to problems like Newcomb's paradox, quantum wave-
function collapse, and logical and empirical induction. Those are the rules; I'm only following 
them.
4) Anyone who thinks I'm an "incautious thinker" has a responsibility to point out some 
specific flaw in my reasoning. No matter how "general" my reasoning is, it is either right or 
wrong, and if it is wrong, then it contains an identifiable flaw. This group has had a standing 
invitation to locate such a flaw, but has not done so. For example, if my reasoning is too 
general to solve the (very general, but very important) problems I claim to have solved with 
it, such a fact is demonstrable; my claim could be falsified.
5) The CTMU is a theory of the codeterminate relationship between reality and the intellect. 

The empirical evidence offered so far includes the many experiments now confirming EPR-
Bell quantum non-locality and Heisenberg uncertainty. These are hard, physical facts which 
were fundamentally unexplained prior to the CTMU. The logical evidence presented in favor 
of the CTMU has been nothing short of overwhelming. Resolutions of paradoxes are 
evidence for logical theories as surely as predictions and explanations of anomalies are 
evidence for physical theories. Because physical theories necessarily have logical 
structures, logic has priority. A single unresolved paradox of the general form x = -x 
invalidates the entire logical system containing it, and thus every semantical theory 
formulated within that system; it renders determinations of truth and falsity impossible for 
every statement the system and theory contain. In principle, I can thus claim every 
confirming instance of every logically consistent scientific theory as evidence of the CTMU, 
making it the best-confirmed theory in history. In point of fact. this is precisely what I've 
already done.
6) Even if CTMU were a definition rather than a theory, definitions are necessary 
components of theories. There is an inclusory relation, not a total distinction, between the 
two. In fact, the CTMU can be characterized as a THEORY of how the mind DEFINES and 
IS DEFINED by the universe. If you must, re-read Noesis 46 and 47.
7) That some details of CTMU were omitted in my presentation was quite deliberate. I used 
only what I had to use to solve the problems I attacked. The size and scheduling of Noesis 
imposed severe restrictions on me; even so, I was simultaneously accused of being overly 
general and excessively technical. This is a paradox that even I find hard to resolve. 8) I 
have a razor-fine dissection of the original marble problem (#26, Trial Test B, Insight 10) in 
the form of a dialogue so clear and amusing that even an average high schooler could 
understand it. However, the current editor-despite what I'm sure are his many virtues-shows 
an apparent preference for scatological humor over this kind of material, and threatens to 
"edit like crazy" anything not completely to his liking. That naturally precludes future 
extensive discussion of the marble problem, towards which he professes antipathy. This is 
unfortunate, since the dialogue is illuminating on Bayesian paradox in general. Had anyone 
expressed any real encouragement, I'd have printed and mailed it myself. But supporters 
are apparently few. So for now, I'll just reiterate my former assertion: there is one and only 
one way to solve the problem as presented. In a nutshell, the reasons are:
a) Observation is all you have to go on, and the l<=(r)<=10 marbles you observe display just 
one color. The number of possible colors is critical information, so you can't just guess it or 
make it up. It has to come from observation. The way you refer to the electromagnetic 
distinctions on which color is defined is also important, but only as an upper limit on what 
may be observed. I.e., if you can discern only n colors, you can observe no more than n. 
Letting your chromatic sensitivity or vocabulary play a greater pan than this implies that you 
are controlling the sample by observing it, a ridiculous assertion. You are not a "demon"... 
or are you?
b) Since you can't make up the number of colors, you're entitled to just two, the one you've 
observed and a generalized complement to symmetrize (remove imbalances in) the initial 
information necessary for Bayes' rule to work. This is the direct negation of the observed 
color: white —> nonwhite. Failure to add this complement imbalances the calculation and 
makes whiteness "axiomatic" for marbles on the basis of just one observation ... an 

absurdity, given that the formulation requests a "probability" and thus implies an exhaustive 
and disjunctive set of at least two nonzero colors. The idea of "color" does not exist for 
monochromatic observers.
c) You must not only symmetrize color (i.e., white-nonwhite), but also the possible 
distributions in terms of color alone. Otherwise you're imparting phony confirmation to 
information you can't legitimately claim ... the information embodied in any asymmetry 
among allowed colors or initial distributions. Thus, the white nonwhite ratio n/(10 - n) is 
equally probable for all n from 1 to 10.
Let me explain. The method by which marbles were colored and put into the box can be 
known to you only through the actual distribution. Unequal probabilities among possible 
(initial) distributions would imply that you havea priori information on this method and so on 
the actual distribution itself. But the problem formulation gives you no such information. If 
such information emerges over repealed experiments, the problem changes and Bayesian 
inference is relativized to new data. But until then, you're more limited.
These conditions--that only two colors may be used, and that all distributions in these two 
colors are initially assumed to be equiprobable--plug directly into Bayes's rule and yield a 
probability of .67 that all ten marbles in the box are white, given that sampling with 
replacement has produced ten white marbles in a row. This is a very specific determination 
which should be easy for all to follow. There is nothing "ambiguous" about it. But just to 
make sure, let's run the computation one more time.
BAYES'S RULE: p(a|b) = [p(b|ai)p(ai]/[p(b|ai)p(ai + … + p(b|an)p(an]
where p(a|b) means "the probability of a, given b", and the ai are exhaustive independent 
alternatives defining a random variable A.
Let A = {a0 or … or a10} be the actual distribution of marbles in the box, and let an be the 
possible distribution with n white and (10 - n) nonwhite marbles. The symmetry condition on 
possible distributions means that p(a1) = … = p(a10) = .1 (since at least one white marble 
has been observed, p(a0) = 0). Let b represent the evidential data, a sample-with-
replacement of 10 while marbles in a row.
Then
The original marble problem has now been solved more explicitly than any other problem on 
any Ronald Hoeflin test. So, unlike most of them, it can now be cautiously used to measure 
intelligence.

SECTION II
8) Regarding George Dicks's criticisms, I'm forced to reiterate a crucial distinction that 
makes all the difference in the world. He has solved Newcomb's PROBLEM; I resolved 
Newcomb's PARADOX. George is one of many who have solved the problem in essentially 
the same way. I didn't solve it at all except as a "side effect" of resolving the paradox 
between (he arguments that George and Chris employed. This can only be done in one 
way, as defined with great precision in Noesis 44. The Resolution of Newcomb's Paradox 
represents the first and only time that this has been done in print. While I've already 
congratulated George on writing a fine paper with some original features, his entire 
"rebuttal" is virtually irrelevant in view of the foregoing distinction. But I'm going to go over a 
couple of things I'm afraid he and Chris might have missed in Noesis 44-49, just to make 
sure that nobody backs himself into a corner he can't pitch his tent in.
9) Computative regression is the essence of Newcomb's paradox. The regression is defined 
by the conventional formulation which George changed to get his own version of the 
problem. Trying to avoid a computative regression in resolving Newcomb's paradox is like 
trying to avoid the idea of viruses while searching for an AIDS vaccine. It can't be done with 
any hope of success.
10) The arguments used to solve Newcomb's problem fall into two general classes, 
mathematical expectation and temporal directionality. The former class is probabilistic and 
statistical; the latter is intuitive. Both classes involve subjective utility or self-interest, and 
both claim an empirical basis. Chris Cole argues from physical intuition grounded in his 
experience of reality. George Dicks argues from numerical data. But it's easy to see that 
Bayes's rule, the proper use of which I outlined above, can easily be adopted to Newcomb's 
problem in lieu of the method he employed. The way I've used it to avoid "false axioms" in 
the marble problem applies as well to Newcomb's problem, and yields a numerical 
determination comparable to his. The point is, CTMU principles cover all numerical methods 
in this class while depending on none. The CTMU is determinant over all of them in kind.
11) "Empiricism works" is a statement with which I fully agree. However, it does not work by 
itself. Empirical induction is to theorization what observation is to recognition; one is of no 
use without the other. The CTMU is primarily a theory of induction. To claim that it was used 
to resolve Newcomb's paradox at the expense of empiricism is a non sequitur. The CTMD 
resolves the paradox because, unlike certainty theory and standard numerical statistics, it 
accounts for the means by which empirical data--like the perfect success of Newcomb's 
Demon and quantum nonlocality--can violate intuitive (and observationally-confirmed) 
assumptions about lime and space. These means involve the relationship of mind to reality, 
not just mind or reality alone.
"Anything less is just theology" is a statement which I find impenetrable. Your past 
experiences and disappointments with what has traditionally passed for theology have 
nothing whatever to do with the CTMU. which realizes Kant's dream of a "theology" 
derivable from the union of mind and reality. If you mean to challenge that, you'd better gear 
up for a whole new game. CTMU "theology" owes nothing to the history of religion, and 
whatever arguments you use against it had better not either. That includes any idea you 
might have that the theological implications of the CTMU arc "unverifiable" or mere "wishful 

thinking".
12) The Resolution of Newcomb's Paradox allows for any arrangement of chooser and 
predictor. These designations are meaningful only relative to each other. That particular 
arrangement in which the predictor dominates the chooser happens to be set by the 
canonical formulation. In Noesis 45, the theory of metagames was incorporated into the 
CTMU. This theory, by merging the best interests of the players, allows them to cooperate 
in nondominative situations where "choosers" and "predictors" are indistinguishable. This 
idea was further developed in Noesis 48. The bottom line is, if neither player is "predictive" 
relative to the other, then there exists a jointly accessible algorithm which lets them 
cooperate to achieve mutual benefits (the theory of metagames). This prevents what 
George calls a "chaotic exchange of move and counter-move," provided both players know 
of it. But if one is dominant, as the formulation stipulates, then the situation is identical to 
that of The Resolution. These possibilities are exhaustive and mutually exclusive, so no 
further explanation is necessary. George's attention to specific arguments is commendable, 
but unnecessary resolve the original version of Newcomb's paradox. It can't be used to 
justify a claim of superiority or priority, particularly when it completely ignores the theory of 
metagames.
If, despite the ubiquity of computative regressions in every aspect of our mental lives, you 
doubt their possibility, you still can't use your doubt as an objection to the CTMU or its 
resolution of Newcomb's paradox. If you deny it, then you also deny the validity of 
mathematical achievements like nonstandard analysis, which itself uses an "incredible" 
model-a nonstandard universe containing "infinitesimal" (nonzero but subfinite) quantities-to 
resolve inconsistencies in the infinitesimal calculus. The CTMU is on even firmer ground, 
and you will ultimately find it harder to avoid than death and taxation. It is a fait accompli.
My remarks are numbered. If you disagree with anything I've said, please refer to it by 
number and line. The exchange has thus far been circular, and I can't waste time on circular 
objections. You supposedly have I.Q.'s in the vicinity of 180 and vocabularies to match. If 
you find these comments "stultifying", "unreadable", or anything but crystal clear, don't 
blame me. I'm just the one who tentatively bought your PR.
This is not to say that I don't hold your intellects in high regard despite your lack of 
encouragement. I only wish thai some of you could forget about defending your special 
views long enough to recognize that you have witnessed a remarkable development. I 
expect only what is reasonable and seek nothing that is not mine by right. One supposed 
benefit if a high IQ is knowing when to cut your losses and reinvest in new concepts, and 
you've all had an adequate prospectus since early 1990.
Chris Langan
Chris Langan to Rick Rosner 
October 09, 1992

Dear Rick:
Since I'm sending you my letter to Ron. I might as well take the opportunity to rectify what 
you modestly call your "misunderstanding of Godel", involving the supposed inability of tau-
tological systems to generate "interesting results". Read closely; if you want to get to the 
bottom of the controversy over "metaphysics", it doesn't get any clearer than this.
First, let's take a look at the word tautology. Its meaning in the vernacular involves needless 
repetition or redundancy. But in logic, its meaning is more precise and more benign. It 
describes a statement which is analytic, or true solely by virtue of its logical form. This 
reaches its limit in 2-valued prepositional logic: e.g., A v ~A (law of the excluded middle); 
~(A & ~A) (law of noncontradiction). In this notation, variables are sentential; "A" stands for 
any complete formula or "predicate". Such tautologies are self-referential; we can let "A" 
stand for the whole tautology in which it appears (e.g., A v ~A --> (A v ~A) v ~(A v ~A)). 
Since logic is entirely developed by deductive substitution from initial tautologies - as is a 
geometry from its axioms - these tautologies form what you'd call a "reflexively true 
tautological framework". They are "highly resistant to outside contradiction" because, in 
order to be comprehensible, any such contradiction must be formulated in terms of 
propositional logic and therefore submit to the very tautological rules it purports to 
"contradict".
It is possible to take an outside perspective on 2-valued prepositional logic by extending the 
set of truthvalues on which it relies. This perspective is that of many-valued logic. However, 
if you want to be able to regard statements as being either true or false but not both at once, 
you cannot take this perspective. Even if we were to take an MVL perspective for theoretical 
purposes (as sometimes we must), we would have to "translate" our results into 2VL in 
order to make them consciously comprehensible.
So we have three definitions of "tautology". In order of strength:
1. The self-referential sentential tautologies of 2VL; 
2. Less general analytic statements like "daisies are flowers"; 
3. Any statement that is repetitive or redundant.
The extreme generality of propositional logic usually makes it inadequate as a theoretical 
formalism. Most scientific theories are sets of objectively-interpreted predicates making 
qualitative and quantitative attributions with respect to objectively-interpreted object 
variables. It would thus be useful to "relax" the prepositional definition of tautology so as to 
extend its applicability to predicate logic. This can be done in a self-referential way through 
the well-known distinction between autologous ("the word short is short") and heterologous 
("the word illegible is not illegible") predicates. Unfortunately, this distinction involves a 
nongeneral assumption that we cannot usually make: that predicates are being 
typographically interpreted, or that predicate logic is being used only in reference to how it is 
written. So we must suspend the self-reference criterion. This, of course, leaves us with 
definition (2) above.
The self-referentiality of sentential tautology owes to the fact that these tautologies can only 
be expressed as things of the kind to which they refer…i.e., logical formulae. But this is 

rarely the case. For example, we sometimes make general statements about contexts 
which contain neither the statements themselves nor their objective images. These 
statements then comprise a "metalanguage" formulated in a context which properly includes 
that to which they refer...i.e., in a semantically context including a theoretical object 
language and its object universe, or referent context. Examples of statements requiring 
metalinguistic formulation are those attributing truth or falsity to sets of semantically 
interpreted object-level expressions. In these and other such cases, we will be using the 
term "tautology" in reference to any universal generalization over the referent context ... 
something "repeated" for everything in that context, but not necessarily for itself.
But first, a preliminary note. A non-self-referential tautology always implies a restriction of its 
referent context with respect to reality as a whole. Otherwise, there would be no place left to 
formulate it in which it could avoid referring to itself. There is just one obvious context which 
cannot be restricted in this way: reality as a whole. This, of course, is the universe of any 
theory of metaphysics. Like propositional logic, a metaphysical theory must be formulated 
within the context to which it refers. So, given our "relaxed" definition of tautology, it will be 
understood that tautology becomes self-referential by necessity at the "metaphysical limit" 
of predicate-logical theorization.
Note also that the cognitive syntax of the human mind - the time-invariant aspect of human 
mental functionabi1ity - qualifies as a tautology in the same self-referential sense as does 
metaphysics. Whatever it considers - itself and everything else - it must consider within its 
own definitive constraints. In other words, it can consider its own structure and operation 
only within its own structure and by its own operation, and everything else (all which is 
outside or beyond it) only as an externalized potentialization of itself (i.e., as that which can 
be considered within it). If the phrase "itself and everything else" seems suspiciously close 
to the phrase "reality as a whole" - the "universe of metaphysics" mentioned above - then 
you already glimpse what must follow.
Any nominal tautology (or "tautological theoretical framework") is of one of two kinds. Either 
it is analytic over the entire domain of definition of its argument, or it isn't; either it covers its 
entire universe, or it doesn't. In the former case, it is a valid tautology with respect to the 
given application. In the latter it is not, and if it is nevertheless tautologically applied, we call 
it an artificial tautology or a pseudotautology. Artificial tautology is the worst bane of 
inductive and empirical reasoning; it pretends to yield a kind of information it cannot yield, 
and to describe things completely which it actually describes partially, not at all, sometimes, 
or never. Most supposed "metaphysical" theories are of this variety (e.g., the Pepper theory 
of metaphysics, whose root concept is tautological only with respect to behavioral 
psychology and not reality in general).
Artificial tautology is especially insidious when, in indeterminate contexts with undescribed 
contents, it becomes "self-implying" in a manner which parodies true logical analycity...e.g., 
when the rules of inference of the theory in which it is misplaced ignore the ordinal 
distinction between its "antecedent" and "consequent". As widespread examples, take such 
notorious prejudices as "those defendants who most cleverly deny guilt are always guilty". 
In any court holding this belief, no hapless innocent can be clever in his own defense 
without "proving himself guilty"! This statement's claim to generality resides in the supposed 
exhaustivity of the domain of definition of its antecedent (the universally-quantified set of 

defendants from which "the cleverest" are taken) and the universal quantification ("always") 
of its synthetic consequent. In empirical contexts, this is a blueprint for disaster.
Another example, paraphrased from Noesis 73: "Those theorists most certain of their 
theories are necessarily dogmatic (and insane and stupid to boot)." Even though 
demagogues, who prey on the ignorance and prejudice of their audiences, are marked by 
just this kind of cynical reliance on artificial tautology, the wider definition of "demagogue" - 
which involves "leadership" - prohibits us from saying that the author of this particular one is 
"necessarily a demagogue". All we can say on its basis alone is that he's doing a 
transparent and rather sorry impression of one.
However, it's just as clear that any valid tautology, by virtue of its applicability over specific 
distinctions within its universe, must be general in a sense often confused with 
uninformative. This confusion is only natural for those preoccupied with seeking various 
kinds of specific information. If we want to cure an ill, it isn't enough to know that we need "a 
medicine"; we must know the specific kind of medicine we need.
If the story ended here, we'd be in big trouble. To get specific information, we need to use 
deductive reasoning. But we can only do so by starting with generalities and "working 
inwards". This means that without general info, there can be no specific info. In other words, 
we can't call the fact that we need "a medicine" "worthless information", since without it, we 
can't even begin to find the specific medicine we need. Generalities - and the inductive 
reasoning which produces them - are absolute prerequisites of "interesting" deductive 
theories.
Generalities reflect a general truth: not all of them are created equal. Tautology, as the very 
broadest kind of generality, is the most necessary prerequisite for informative theories. 
Thus, if Godel had ever said anything like "tautological systems cannot generate interesting 
results", he'd either have to draw some fast qualifications, or we'd have to rip the officer's 
stripes from his "genius" uniform and bust him down to privatdocent. Systems consisting 
only of tautologies may be informationally impoverished, but that's only because we haven't 
yet developed their primary 
advantages: their tautological structures relative to their data-universes .
All informative systems must have tautological bases relative to their universes...i.e., must 
come from premises (or axiomatic sets of premises) that are true for all things under all 
circumstances describable within them. Any system which does not is founded on premises 
which exclude some aspect of its universe, and is useless for arguments involving it. Where 
this excluded aspect is unknown, we cannot identify the arguments for which the system is 
or is not useful. This, of course, eviscerates the entire system from an informational 
standpoint, on the other hand, if the excluded aspect is known, then adjoining this info to the 
system in a way allowing it to interact with info already there extends the system to cover it. 
and there must now exist a tautological basis of the system with respect to its whole 
universe.
Notice what this says about the plight of pre-CTMU theories. The validity of any of them 
must be relativized to those aspects of the universe for which its basic premises are 
tautological; whatever information it contains exists only for them. Wasn't it too bad that the 

info in particular pre-CTMU theories was inapplicable to the contexts of other such 
theories...i.e., that all specific-theories couldn't be combined to amplify information about the 
contexts of each of them in least within logical constraints up-plying to relationships among 
the universes themselves?
Information is not an absolute quantity. It exists relative to the contexts in which it is 
applicable. If you know that apples are edible, but you have no apples, then you have no 
useful information on how to feed yourself, hut if you have an endless supply of apples, you 
have quite a bit of info indeed. On the other hand, no number of apples can alone make 
"apples are edible" yield info on how to fix your TV. Unfortunately, standard information 
theory just wasn't equipped to deal with these and other aspects of its titular subject matter. 
While Shannon-style information was a sufficiently powerful concept to promote the 
development of modern communication and computation systems, it had its limitations. The 
CTMU was invented partially to rescue the world from these limitations by redefining 
information in a more powerful way.
As readers of Noesis will recall, this crucial redefinition begins with a mutual, recursive 
interdefinition of information and cognition within a "reified tautology" called a quantum 
transducer. The quantum transducer, beingparadoxiform by direct analogy with 
tautologically-based inference, models the way subjectively-tautological cognitive syntaxes 
transduce information in time. The universality of this model allows reality to be reduced to 
it, and thus to (cognitive) information. "Information" is the objective aspect of the quantum 
transducer for itself and for all others; it is cognition-for-cognition, equating generalistically 
to a cognitive identity relation on that part of reality to which it corresponds (i.e., the part 
containing all the transducers playing active and passive roles in it).
As you suggested in Noesis 73, my "certitude" regarding the CTMU rests on its tautological 
structure relative to all humanly-comprehensible reality (I seem to recall mentioning 
something to this effect during one of our two conversations)...and a few related "tricks" like 
paradox-distributivity. Formulating reality as a tautology was an obvious move. The reason 
no one succeeded before me is that doing so required a basic (and rather counterintuitive) 
restructuring of our perceptions and conceptions of reality.
A primary effect of this restructuring was to eliminate certain barriers existing among various 
submetaphysicaI disciplines. Every field of human inquiry contains valuable information, but 
it has always been difficult to transfer this information across interdisciplinary boundaries. 
Thus, the elimination of these boundaries - the construction of a "universal formalism" - 
opens various realms of inquiry to relevant but otherwise-inaccessible information once 
"hidden" in the alien formalisms of other realms. The "liberated" information is then free to 
combine in synergistic (or even chaotic) ways to reveal new insights.
I know you remain skeptical of certain implications of the CTMU, largely because you're 
unfamiliar with the logical and model-theoretic criteria for proof. Hut you must at least know 
that they involve conjunctions like type theory and probability theory, and the theories of 
physics, computation and decision. Furthermore, these conjunctions are used to solve 
problems which cannot otherwise be solved, at least with any amount of ease. Your 
skepticism notwithstanding, it is obvious that this kind of "informational chain-reaction" can 
he a powerful generator of insight.

There is one problem in particular that cannot be solved without a CTMU-style tautology 
and its attendant informational explosion: that of providing a general, logically consistent 
picture of the universe. This owes to the fact that the basis (root concept) of any correct 
theory of metaphysics must be tautological relative to all conceivable aspects of reality. 
Because the "metaphysical universe" is so all-encompassing that it exceeds the set of all 
self-inclusive sets, where "self-inclusion" is synonymous with the kind of self-description on 
which (prepositional) tautology is defined, it must reduce (or regress inductively) to the 
broadest and most powerful tautology the human mind can formulate.
There is only one such "universal tautology", and therefore only one correct basis for 
metaphysical theorization. To convince you of this, I offer the following informal and highly 
simplified "proof". For the purposes of this proof, think of "information" as that by which 
transducers distinguish among objects or ideas. The phrase "T excludes d" means that the 
theory T contains neither the info d nor a deductively heritable generalization of it. The point 
of exclusion is to excuse us from differentiating between two theories, one of which is either 
a notational variant or deductive evolution of the other. Such theories pass as virtually 
identical; "different" theories have different tautological bases.
SHORT FORM: Say that there are two true but different theories of metaphysics M and M’, 
one or each of which contains information inferentially excluded by the other. Call all such 
info "d". Since M, M’ are both true, and the distinction between two truths is itself a truth, d 
is true. Since metaphysics is comprehensive over reality by definition, it can exclude no real 
truth. But at least one of the pair M, M’ excludes at least a part of d. So at least one of the 
pair is not a theory of metaphysics, and the assumption that two such theories exist is self-
contradictory. This implies that there is at most one true theory of metaphysics.
Could there be no true theory of metaphysics? According to the above discussion, 
metaphysics reduces ultimately to the human cognitive syntax (or more accurately, its 
symmetric self-expansion). So "no true theory of metaphysics" would imply that human 
beings lack a cognitive syntax. If this were so, human cognition would he random and 
patternless. But it isn't. So there is one true theory of metaphysics, and this is by definition 
the CTMU.
It might be objected that the CTMU, being based by definition on the human cognitive 
syntax, already resides in each of our minds and thus represents no informational gain. But 
this syntax is not so easily formulated within itself, and equating metaphysical reality to it is 
neither obvious nor simple. As explained above, a net informational gain comes from 
freeing information once "locked up" (artificially isolated) within U*-pseudotautologies and 
the scientific and mathematical theories implicitly based on them.
Now that we have the essential picture, let's try for some detail. Let Ui, be that part of a 
generalized universe U* to which we refer as the physical universe, or the set of all things 
directly observable by Ui-observers. This is a recursive definition in which Ui is defined on 
Ui-observers and vice versa, and varies with choice of subscript. Subscripts correspond to 
cognitive equivalency classes within U*, or sets of observers sharing the same information-
transductive syntax. Ui consists of that part of U* specifically decidable to Ui-observers, and 
is mathematically equivalent to the cognitive class itself. Assume that the class Ui is human.

The term "metaphysics" is variously construed. In certain usages it encompasses alternate 
(or "parallel", or independent) realities with no physical meaning. In the Aristotelian sense - 
and ours it is the totality of theoretical potential relative to the physical universe. While there 
is nothing mutually antithetical about these constructions, metaphysics relates to physics 
only as an exhaustive domain of ultimately Ui-effective "hidden causality" undecidable by 
conventional scientific means. The real universe U* is an extension of Ui by adjunction of 
this domain.
U* is related to the physical universe by a form of connectedness loosely characterizable as 
"relevancy"; i.e., it is an extension of Ui generated by causal regression. From Ui, it appears 
as "causal potential" manifesting itself in Ui as "physical effects". For Ui, U* is unique. For 
suppose that Ui were contained in many realities corresponding to many Ui-distinguishable 
metaphysical tautologies. For the differences among them to "register" in the minds of Ui 
observers, they must be specific relative to the Ui cognitive syntax. As relatively specific 
tautologies are of lower order than the "tautological" Ui cognitive syntax itself, the universes 
to which they apply - i.e., the realms of Ui potential and Ui-relevant "alternate reality" they 
represent - must be partial and therefore properly included in U* (which is complete by defi-
nition and theoretically infinite). It follows that U* is unique up to indiscernability: if "other 
versions" of U* exist, they must be within it, inductively homomorphic to it and 
indistinguishable from it.
It would be easy at this juncture to point out that by "reifying" information as the quantum 
transducer, and distributing the quantum transducer over reality, we have removed the 
major distinction between U* and any theory describing it. Whereas only the latter was 
formerly regarded as "informational", so now is U*. The U*-decscriptive theory is now 
merely a sort of endomorphic "self-equivalency" of U* as perceived by Ui. We could 
conclude our proof on these grounds alone; if U* is informational and "unique" for Ui, then 
so is the metaphysical information to which Ui regards it as "equivalent". But we can make 
this even clearer.
A theory of metaphysics is formulated by inhabitants of the real universe it describes. 
Relative to (Ui , U*), it is a description of U* by the observational subsystem Ui of U*, or a 
U*-self-description based on a Ui-formulated U*-quantified tautology applying to the 
"metaphysical" extension U* of the jointly-observable reality (Ui) of the Ui cognitive 
equivalency class of U*. The circularity of this description reflects the necessary self-
referentiality of tautology at the metaphysical level.
Suppose that there exist Ui-discernible theories of metaphysics M and M’ on {Ui, U*}. The 
Ui-discernability" of M, M’ implies that they are Ui-informationally disjoint: (M ∪ M’) - (M ∩ 
M’) = [illegible] ∅. The "infometrical" form of this relationship is graphically expressed as
M---------(d)---------M’,
where the edge (dotted line d) represents syndiffeonesis (difference within a cognitive 
class)...i.e., information in the sense given above.
Now, the disjunctive information represented by the edged exists in M ∪ M’, which, by the 

self-referentiality of metaphysical tautology, implies that it exists in their common universe 
U*. So the edge d represents real information that must be included in the real universe U*. 
By our initial assumption that M and M’ are both theories of metaphysics and therefore 
tautological on U*, d must be included in both of them. But since d is defined asdisjoint 
information - whence the way it disjunctively separates M and M’ - this leads to a 
contradiction. I.e., the nonuniqueness of M and M’ violates the universality criterion of 
metaphysics.
Now let's see if we can recap all of this.
Aristotelian metaphysics is universal, containing in principle all Ui-relevant information (Ui-
potential) U*. A theory of metaphysics M is an open inferential system which, because 
necessarily universal, reduces to a Ui-recognizable tautology T on U* heritable in M via 
generalized rules of inference (where "generalized inference" is just logical substitution). As 
specific information equates inductively to ancestral generalisms, and U* is both unique and 
Ui-indiscernible from T, the identification M = T = U* is practically unconditional. Now 
suppose that there exist two Ui-distinguishable true metaphysical theories M and M’; i.e., 
two Ui-distinguishable Ui-tautologies T and T’. These can only be Ui-distinguishable by 
virtue of a nonempty Ui-informationa1 disjunction: i.e., disjoint information d = (T ∪ T’) - (T ∩ 
T’) > ∅ recognizable in/by Ui (where the information in T or T’ equals the scope (image) of 
its universal quantifier, and ∅ is the null set). This information d, being the distinction 
between two Ui-perceptible truths, exists in Ui and thus U*. But as it is disjoint information, 
one member of the pair (T, T’) does not contain it. So this member does not cover U*, is not 
a U* tautology, and thus is not a theory of metaphysics. On the other hand, M = Uj = 1, 2... Mj, 
where the jointly U*-exhaustive Mj are all "true", Ui-distinct, and M-nonexluded, does and is.
So the assumption fails, and there can be only one correct theory of metaphysics at the 
tautological level. This, by definition, is the CTMU. I.e., the CTMU takes this existential proof 
of metaphysical uniqueness and uses the implied system as the identity of a transductive 
algebra meeting the conditions for human cognition by its homomorphic relationship to the 
human cognitive syntax. So for the human cognitive equivalency-class, the universe is 
generalistically identical to the CTMU tautology.
Soi-disant "metaphysicians" have been debating the merits of so-called metaphysical 
theories for centuries, usually claiming to argue from "logical" standpoints. The only accord 
they have been able to reach is an "agreement to disagree". Sadly, this has left the 
uncloistered masses with a level of metaphysical understanding not far above that which 
guided them through the last Ice Age, and science without a clue as to the meaning of what 
it is doing. If this is not a monumental injustice to humanity, then humanity has vastly 
overestimated its own importance.
Fortunately, mankind does have a protector against the abuses of time and energy being 
perpetrated upon it even now by mainstream philosophy. With the coming of the CTMU, 
time has run out forever on this conspiracy of the blind: the blind, sighted at last, can newly 
behold reality through tears of shame and gratitude; and the rest of us, freed from the 
rotting conceptual bonds of traditional "wisdom", can finally anticipate the fulfillment of our 
collective intellectual identity.

As a start down that road, the information in this letter alone exceeds that of a standard 
Ph.D in "philosophy". Think of it as a primary gateway into logical self-awareness.
Regards, Chris
Reply to Chris Langan on Isomorphisms, Models and Objectivity
Includes Langan’s responses
To: fire@prometheussociety.org, mega@yahoogroups.com
From: Kevin Langdon <kevin.langdon@polymath-systems.com>
Date: Thu, 05 Jul 2001 00:15:32 -0700
Subject: [mega] Reply to Chris Langan on Isomorphisms, Models, & Objectivity
On Sun, 11 Apr 1999 18:06:16 -0400 (EDT), Chris wrote:
> Subject: Re: [MegaList] Reply to Chris Langan, 4/11/99 (Part 
> One)
>>> Okay, everybody, break out the Dom Perignon and raise 
>>> your glasses to Kevin Langdon, who seems to be setting a 
>>> new personal record for obtusity and obstinacy. His last 
>>> messages underline as never before what he has proven 
>>> countless times already - namely, that while he sometimes 
>>> barely has a point when it comes to statistical psychometrics, 
>>> he is almost always dead wrong when it comes to anything 
>>> else, and will never, ever admit it. But this time it's even 
>>> better. Now Kevin is challenging the entire scientific, 
>>> mathematical and philosophical world over the meaning
>>> of "isomorphism" in the context of formalized theories! He 
>>> can't win, of course...not today, not tomorrow, not when the 
>>> burnt-out sun is a dense ball of ash spinning like an icy top 
>>> in the blackness. But that's what makes it all so...remarkable!
>> isomorphism, n.: a one-to-one correspondence between 
>> two mathematical sets; esp. a homomorphism that is
>> one-to-one
>> homomorphism, n. a mapping of a mathematical group, 
>> ring, or vector space onto another in such a way that the
>> result obtained by applying an operation to elements of the 
>> domain is mapped onto the result obtained by applying the 
>> operation to their images in the range.
>> --*Webster's New Collegiate Dictionary*
>> [I have only included the mathematical definition
>> for each term., in accordance with Chris' usage]
> Points to notice.
> 1. Above, Kevin presents a partial definition of "isomorphism" 
> along with the technical definition of "homomorphism". 
> Additionally, he fails to notice that the given partial definition 
> of isomorphism tacitly implies that it is onto (surjective). [I 
> warned Kevin about relying on Webster's in this kind of 

> discussion.]
I certainly agree that an isomorphism, as defined in mathematics, 
is necessarily onto; no set elements are not included in its range. 
The key point is that an isomorphism is a mapping that preserves
all operations.
But over and above that, I understand you to be claiming a 
metaphysical relationship between a model and the real-world 
object which it represents, and that is what I do not admit to have 
been established. I also question the way you use terminology, 
which I sometimes find ambiguous.
> 2. Kevin's assertion that isomorphisms can be one-way makes 
> no difference whatsoever to the thesis under discussion.
I never said that, but I agree that the "isomorphism" discussion 
is not central to the real question at hand, which is whether 
Chris' logical arguments imply anything about the real world.
> 3. The proper algebraic definition of isomorphism goes 
> something like this: If <A,*>,<A',*'> are monoids, m:A-->A' 
> a map of A into A' such that m(a*b) = ma *' mb for each a,b 
> in A, then m is a homomorphism of the monoid A into the 
> monoid A'. If m:A-->A' is a homomorphism, then m is:
> a monomorphism if it is injective (one-to-one)
> an epimorphism if it is surjective (onto)
> an isomorphism if it is bijective (one-to-one onto).
> This is taken from the nearest abstract algebra text I could 
> grab, "Algebra" by Goldhaber and Erlich (Macmillan, 1970). 
> Notice that since reality has algebraic structure, consisting not 
> just of sets but of operations among their elements, we have 
> to talk in terms of algebra. So as we see, when Kevin says 
> "isomorphism", what he really means is "monomorphism" 
> (even if he claims that in some strange way, this means that 
> he means "isomorphism" after all).
This "strange way," as was the case with equating "theory" and
"model," is common usage among scientists and mathematicians.
It's not my fault if they're sometimes less than rigorous.
> Kevin's confusion comes down to this. In the context of 
> simple sets, we can get away with saying that an isomorphism 
> is "one-to-one" because, if this is applied to both sets, 
> surjection (onto) is logically implied. That is, if one has two 
> sets A and B and says that there is a 1-to-1 mapping between 
> them, one is saying that for every element of either set there is
> another (unique) element in the other. It follows that the sets 
> A and B contain an equal number of elements, and that the 
> mapping is surjective (onto) no matter what its direction.
It's true that I had not explicitly stated the full mathematical 
definition of an isomorphism. I took it as implicit context that 
we're talking about the whole of each set.
> Thus, Kevin has not only failed to properly track the logical 
> implications of his set-theoretic definition of "isomorphism" 

> in the context of set theory itself, but he has failed to account 
> for the additional structure that algebra brings to set theory. 
> Reality includes not just sets, but algebraic operations within 
> them. And that's the first installment of Kevin's math and 
> reality lesson for today.
>> What I said, in my message was quoted by Chris in reponse 
>> to his statement:
>> 
>> [Intervening material snipped.]
>>>>> An isomorphism is a bijective (one-to-one onto)
>>>>> correspondence or similarity mapping between, e.g., a theory 
>>>>> T and its object universe T(U). In the absence of a clear 
>>>>> difference relation between the range and domain of an 
>>>>> isomorphism, it can be contracted so that range and domain 
>>>>> are coincident.
>>>> An isomorphism is just a one-to-one correspondence. Chris is 
>>>> seems to think that any one-way mapping implies a two-way 
>>>> mapping, which isn't necessarily true.
>>> An isomorphism is surjective (onto) as well as injective (1-to-
>>> 1). It is virtually always reversible unless it is explicitly 
>>> stipulated, for usually artificial reasons, that it is only one-way.
No problem to here, now that these terms have been defined. I often
don't know whether to look up a word or phrase Chris has used 
because I don't know whether it's standard or Langanian language.
>>> When it comes to the topic at hand - the correspondence 
>>> between a valid theory and its object universe - the isomorphism 
>>> is always two-way. That's what the "valid" means. To reiterate, 
>>> the theory "selects" its own object universe (which may not be 
>>> as large as the one that the theorist originally had in mind).
>> I'm tired of saying that things are unclear on my own. Therefore, 
>> I'm going to indicate certain points regarding which an empirical
>> determination of general understanding or nonunderstanding 
>> seems desirable. This will be test point #1. I will also ask you at 
>> other test points whether you follow what Chris is saying.
> Once again, folks, Kevin is asking whether you agree with him 
> regarding the term "isomorphism" in the context of formalized 
> theories. So crack your knuckles and get going on those keyboards! 
> Kevin wants to identify you as a member of his cult, and I want to 
> know what level of rationality I'm dealing with on this list.
If I were interested in founding a cult I sure as hell wouldn't select 
my audience for intelligence.
I would appreciate readings from as many of you as possible on the
seven test points in my two messages of 4/11. It's not a question of 
whether you're a Langdonoid, a Langanoid, or a hemorrhoid, but of
where the members of Mega stand on the particular propositions in 
dispute.
<snip>
>>> It's for this reason that all logic and abstract algebra texts define 

>>> an isomorphism as follows: "An isomorphism is a 
>>> homomorphism that is bijective, or one-to-one onto (injective 
>>> and surjective)."
>> That vocabulary is a little different from that which was used 
>> when I was in college, but it should be clear that we're all 
>> talking about the same thing--a one-to-one correspondence 
>> between two mathematical objects. What Chris is saying is that 
>> there is a metaphysical relationship between a cognitive model 
>> and that which is modeled. I say he hasn't proven it.
> I took my definition from a 1970 abstract algebra text in wide use 
> in the early 70's, so (unless Kevin is a fossil) the terminology was 
> current among logicians and mathematicians near the time Kevin 
> was in college.
I'm a fossil. I was in college in the early 1960's.
> A model is an isomorphism M:T<-->U(T) (full definition) of a 
> theory T and its object universe U(T). The model M, because it 
> has been constructed to embody the theory T and possesses real, 
> concrete existence in the object universe U(T) of T, is an intersect 
> of T and U(T). This is not what Kevin calls a "metaphysical 
> relationship"; it is an identity relationship of scope limited to the 
> model itself. It's analytic...it follows from the proper definition of 
> "model" (which again, I've taken from Mathematical Logic and 
> Formalized Theories, Rogers, 1971).
> Give it up, Kevin. You can't win.
Win what? I say that this is unclear. Chris has been saying that his
theory *is metaphysical* and that it *goes to the heart of everything*.
Naturally I'm suspicious of drawing conclusions about "model" and 
"reality" that amount to anything more than "the model is constructed 
with the intent of duplicating the structure of the modeled 
phenomena." If that's what Chris means, I agree with him. If he 
means something more, I say it hasn't been established.
>>>>> In this sense, the abstract structure of a valid theory (as 
>>>>> opposed to the pattern of neural potential encoding it within 
>>>>> a brain) is virtually identical to its object universe.
>>>> What do you mean "virtually"?
>>> "Virtually": for all practical purposes.
>> How do you know what purposes will turn out to be practical?
> By reliance on logical tautologies that cannot be violated because 
> they are necessary rational ingredients of experience.
Here again, Chris is claiming that logic implies something about the 
real world. Although things that are logically impossible seem to 
be ruled out, for the most part, in the world we live in, it does not 
follow that logic constrains the world to be exactly one way.
<snip>
> MY FIRST LIST:
>>> Well, there it is. Kevin Langdon steadfastly maintains that
>>> 1. His definition of "isomorphism" is superior to that found in 
>>> the most advanced logic and algebra texts.

>>> 2. Isomorphisms are one-to-one (injective), but not necessarily 
>>> onto surjective).
>>>3. Isomorphisms only go in one direction.
> KEVIN'S REVISED LIST:
>> 1. "My" definition of "isomorphism" is isomorphic to that found 
>> in math books and in Webster's.
>> 2. I didn't say this.
>> 3. I specifically said that an isomorphism is a one-to-one 
>> correspondence, and *not* a unidirectional mapping, as is 
>> clear from my quoted remarks at the beginning of this message.
> MY SECOND, DE-REVISED LIST:
> 1. No, it isn't. [Webster's, maybe, but that isn't a math book.]
> 2. Yes, you did. And you still are.
> 3. You specifically said that I erred in supposing that an isomorphism was
> necessarily bijective. And you said that an isomorphism is one-to-one. 
> So what you meant was that an isomorphism is not necessarily onto.
KEVIN'S SECOND, REREVISED LIST:
1. I've responded to this above.
2. Where?
3. That isn't what I said, either. You have a way of stating something and 
grafting something else onto it. If I deny the "something else" that doesn't 
imply that I'm denying the simple truth embedded in the formulation.
What I denied was that you've proven anything about the real world.
On Sun, 11 Apr 1999 18:45:14 -0400 (EDT), Chris wrote:
> Subject: Re: [MegaList] Reply to Chris Langan, 4/11/99 (Part Two)
>>> Gird thy loins, my fellow Megarians, as duty calls us to beat 
>>> down the remainder of Kevin Langdon's brutal assault on the 
>>> lexicon of logic, philosophy and higher mathematics. 
>>> 
>>> On Fri, 9 Apr 1999, Kevin Langdon wrote:
>>>>>> I agree that scientific theories do not generally include models 
>>>>>> of the theoreticians themselves.
>>>>> And that's what makes them objective. The mental processes of 
>>>>> the theorist are deliberately excluded from scientific theories. 
>>>>> When you denied that such theories were objective for that 
>>>>> reason, you were wrong.
>>>> That certainly doesn't make them objective. When a pro wrestler is 
>>>> trash talking about how he's gonna put his opponent in the hospital, 
>>>> his model of the situation doesn't include the mental processes of 
>>>> the wrestler himself, but it would not be in accordance with standard 
>>>> usage to describe his statements as "objective."
>>> Phenomenal focus is a necessary but insufficient condition for
>>> objectivity (do you understand this distinction?)
>> Approximately, but I'm not sure I have a precise handle on what you 
>> mean by "phenomenal focus" or what other element you believe is 
>> necessary for "objectivity."

> Phenomenal focus means empirical versus rational focus. Phenomena 
> are "out there". Rational processes are usually thought to be "in here"
> ...unless you'd just like to agree with my original point, which is that 
> they're both "out there" (and for that matter, "in here").
No, that's the point that hasn't been established.
>>> Trash-talking pro wrestlers fail the objectivity test for another reason, 
>>> namely emotional and dramatic contamination. The "theories" 
>>> promulgated by these wrestlers are deliberately infused with dramatic 
>>> and emotional elements. This is a separate issue, and it has no bearing 
>>> on what most of us refer to as "scientific theories".
>> That sinks your boat. Your messages are full of histrionics, hyperbole, 
>> accusations, caricatures, gratuitous attribution of motives to others, and 
>> putting words into other people's mouths. (Perhaps Chris will tell us 
>> about the fish he sees from his "submarine.")
> Anybody but you could easily have separated the personal elements in 
> my messages from the theoretical content. Try to stay in focus.
The personal elements are very revealing. Try to expand your focus.
>>>>> What you meant was that scientific theories lack *absolute 
>>>>> certainty* due to the impossibility of perfect empirical confirmation 
>>>>> in an inductive context relative to a given set of axioms, theorems 
>>>>> and postulates. In other words, you don't properly understand the 
>>>>> meaning of "objectivity".
>>>> What you meant is that I don't use the term the same way you do.
>>> What I meant was that you don't use it correctly (see above comment).
>> My definition of objectivity accords a lot more closely with standard 
>> usage than Chris'. It's just seeing things as they are, the opposite of 
>> subjectivity, which takes them in relation to an aspect of oneself as if
>> they had no independent existence.
> Objectivity entails (a) the exclusion of emotions, feelings, and so on
> from one's perspective; (b) a focus on pure perceptions (as opposed to
> emotions, feelings, prejudice, etc.).
This is not true. Objectivity entails the inclusion of everything, while being 
attached to nothing.
> For the record, Kevin, I'm no longer interested in your opinions 
> regarding "standard usage". We've already seen where it got you in 
> the context of isomorphisms.
> This is a philosophical discussion, so we have to use sufficiently 
> advanced terminology, not what you can regurgitate out of Webster's.
"Advanced" terminology is unnecessary and confusing if "elementary" 
terms have not been defined and understood first--and "regurgitating" 
something out of a math book is on exactly the same level.
>>>>>>> You're confusing non-objectivity with any subunary degree 
>>>>>>> of confirmation. The concept of objectivity, which is
>>>>>>> defined by juxtaposition to subjectivity, is independent of 
>>>>>>> degree of confirmation relative to a given axiomatization of 
>>>>>>> a theory. [This is your first really bad mistake.]
>>>>>> Presumably, you are speaking of, e.g., certain mathematical 
>>>>>> propositions which are analytically true. You seem to be 

>>>>>> further asserting that there are such propositions regarding 
>>>>>> the physical world, but you have not established this to my 
>>>>>> satisfaction.
>>>>> This is the exact opposite of what was actually stated. Analytic 
>>>>> truth, which boils down to syntactic consistency, is not subject to 
>>>>> empirical confirmation. Empirical truth, on the other hand, is 
>>>>> always subject to confirmation.
...which implies, as I said, that logic does not imply anything about the 
real world.
<snip>
>>>>>>>> A model is a model. The real world is the real world. The 
>>>>>>>> map is not the territory.
>>>>> Are you spacing out on me here? I'm trying to explain what a 
>>>>> "model" is in the context of advanced logic and formalized 
>>>>> theories. By definition, according to every scientist on this 
>>>>> planet, a model is based on an isomorphism, and in fact an 
>>>>> intersection, between theory and universe. That constitutes 
>>>>> exactly what you say is missing: an identity relation between 
>>>>> "map" (which in your lexicon corresponds to theory) and
>>>>> "territory" (the real world).
>>>> According to *you*, not to "every scientist on the planet." A 
>>>> scientist *tries* to make his models correspond to the real 
>>>> world, but most scientists would not agree with what you 
>>>> wrote about an "identity relation."
>>> Every scientist on the planet, especially if he knows anything 
>>> about logic, defines objectivity, model and isomorphism just like 
>>> I do. The other ones, and I say this with all due respect, don't 
>>> have meaningful opinions on the matter.
>> Chris always assumes that those who disagree with him don't have
>> meaningful opinions.
>> 
>> Here's test point #4 (do you follow Chris' definitions?) and #5 
>> (do you agree with them?).
> Let me get this straight. Are you attempting to turn technical 
> semantics into a popularity contest to be decided by "Langdonoids 
> R Us"? The only thing you can "test" by doing that is the cohesion 
> of your cult, a topic in which I personally have no interest.
It's not a question of "popularity"; it's a question of confirming or 
disconfirming *Chris' remarks* about what has been proven in the 
eyes of others.
>>> But after all, they're not logicians; they're scientists. So they only 
>>> have to *do* science, not explain what it is.
>> I agree with Chris that many scientists have never thoroughly 
>> examined the roots of their discipline.
>>> First, you use "theory" and "model" as synonyms. That's what 
>>> you might call a "nonstandard" equation.
>> I've seen a lot of world-class scientists use "theory" and "model" 
>> interchangeably.

> Well, then, you've seen a lot of them use it sloppily. So have I...but
> again, what difference does that make? You've already admitted that 
> "many scientists have never thoroughly examined the roots of their 
> discipline". Some of the roots they neglect are logical.
But not *termino*logical. The purpose of language is to communicate;
"standard" usages are a means to an end. Sometimes less rigorous 
language does the job at hand better.
>>> But to answer your question, we're talking about functional 
>>> scientific theories here. And while a specific theory only 
>>> corresponds to reality up to some limited degree of confirmation, 
>>> its generalized cognitive substrate - e.g. logic and mathematics - 
>>> corresponds perfectly to reality in a selective way. That is, they 
>>> define the architecture and programming of the parallel distributed 
>>> processor providing causality with an objective mechanism.
>> Test point #6. Who understands the paragraph above?
> Most qualified members, as opposed to unqualified members, probably
> understand that paragraph. But if anybody doesn't, so what? It was 
> merely added as an abbreviated jolt to your memory, to refresh it 
> concerning what I'd already expanded on in previous messages. Why 
> are you so desperate to divert everyone's attention from the meat of 
> this discussion?
Here is another example of Chris' assertions that he knows what others 
are thinking. But, as usual, he's wrong. I'm just trying to find the beef.
>>>>> Of course, you might say something like this: "Isomorphism, 
>>>>> shmorphism! Nothing whatsoever is reposed in reality; we just 
>>>>> assume it has causal structure of its own and try to approximate 
>>>>> that structure using our always-makeshift theories." But then I 
>>>>> need merely point out that the objective causal structure in 
>>>>> question has a certain minimal description, and this is just the 
>>>>> structural intersect of all of these temporary theories: namely, 
>>>>> information transduction, which we have already equated to 
>>>>> generalized cognition. More specifically, it is distributed parallel
>>>>> computation, which is just what one gets by distributing any 
>>>>> dynamic theory over any homogeneous local medium. Without 
>>>>> this, objective causality would lack the means to function, and 
>>>>> reality would be an unpredictable Langdonoid chaos for ever after.
>>>> I am not denying that there are *laws* governing physical and 
>>>> psychic phenomena. But these laws must be discovered and verified
>>>> and this is an *empirical* matter.
>>> Did you, or did you not, just affirm the existence of physical and 
>>> psychic laws? Because if you did, then you must have been making 
>>> a rational statement, because you don't have the empirical data to 
>>> identify the laws in question. This means that you are projecting your 
>>> rational need for laws onto "objective reality". You're doing it, Kevin. 
>>> You're doing it right now, and out of the other side of your mouth, 
>>> you're denying that it can be done!
>> This is bizarre. I've observed that certain physical and psychic laws 
>> have been demonstrated and established very firmly by many 

>> experiments, including my own. There is no "projection" in this.
> You're bizarre, all right. First you said "I am not denying that there
> are *laws* governing physical and psychic phenomena." Presumably, 
> you meant that because the existence of laws is obviously a general 
> rational prerequisite for the existence of "phenomena". Then you said 
> "But these laws must be discovered and verified and this is an 
> *empirical* matter." Presumably, you meant that the general rational 
> necessity of laws can only be refined into specific laws through 
> observation, a point on which we quite agree.
What "rational necessity"? I don't presume that. The world *could* 
(logically) be random and without any systematic regularities, in which 
case there wouldn't be any scientists and they wouldn't be able to 
observe or confirm anything. I *observe* that there are regularities; 
it's an empirical conclusion.
> I was speaking, of course, about your apparent concession that laws
> are a rational condition for the existence of phenomena. I assumed you
> were capable of understanding this but had not followed the immediate
> implications. Perhaps I was mistaken. Would you now like to disown 
> or modify this concession?
I agree that if there are phenomena and observers who can see them, 
they must be governed by laws of some kind.
Although we don't know what forms other universes might assume, one 
class of theoretical physical models predicts that there will turn out to be 
many universes, each with its own set of laws.
Kevin Langdon
Superscholar Interview
Mr. Langan, on IQ tests you don’t just score in the 99th percentile (as members of 
Mensa, the high-IQ society, do), but more like in the 99.9999th percentile. How is this 
difference measured?
There are distinctions to be made between conventional IQ tests designed for the vast 
majority of test subjects, and experimental tests designed to pick up where those tests 
leave off (around IQ 160 or so). Due to the nature of these distinctions, the difference of 
which you speak can only be estimated, not directly measured.
When one exceeds the ceiling of a full-range standardized IQ test like the WAIS* (as I have, 
for example), one’s IQ is said to be “off the charts”. As it cannot be fully measured with a 
standardized IQ test, further refinement requires an experimental test with a higher ceiling. 
However, because off-the-charts IQ’s are so rare that they are unlikely to be found in the 
limited samples on which conventional IQ tests are directly normed and statistically 
calibrated, experimental high-ceiling tests designed for them can only be indirectly 

calibrated. Basically, one sees how test subjects have scored on other tests, establishes a 
correspondence, and extrapolates for the very highest scores. At best, this yields an 
“estimated IQ”. [*WAIS = Wechsler Adult Intelligence Scale]
The items in experimental high-ceiling IQ tests tend to be complex and quite difficult, and 
more time is needed to properly solve them. This affects the requirements of such tests and 
the manner in which they are taken, which in turn raises the question of exactly how they 
relate to standard IQ tests. The field of high-ceiling IQ testing is thus open to controversy, 
as is IQ testing in general.
This controversy is worthwhile in some respects, but misleading in others. IQ is a politically 
loaded topic on which misinformation abounds; purportedly “scientific” criticisms of the IQ 
construct and its associated statistics are often motivated less by science than by fear that 
they somehow threaten fairness and equality, or by the exploitation of such fear for hidden 
social or political agendas. Similarly, critical commentary about the IQ of a specific person is 
often a thinly disguised way of attacking intellectual content.
In my view, ideas and other intellectual productions are more interesting, more indicative of 
intelligence, and more productively debated than IQ alone.
Kids who score that high on IQ tests tend to be so far ahead of their peers and 
teachers that they’re often bored out of their minds in school and thus, ironically, 
don’t tend to be considered great students by their teachers. Is this how it was for 
you?
Much of the time, yes. I had more than one teacher who considered me a let-down, and 
sometimes for what must have seemed good reason.
For example, I sometimes fell asleep in class. I can remember trying to resist it, but I wasn’t 
always successful. I was even known to fall asleep during tests, sometimes before 
completing them. And by “asleep”, I do mean “asleep”. It was once reported to me by one of 
my teachers that she had amused the entire class by repeatedly snapping her fingers in 
front of my face and eliciting no reaction whatsoever.
In fairness, this wasn’t always due to boredom alone. I was often tired and exhausted by 
distractions. For example, what pugnacious little thugs would be waiting in ambush as I left 
the school grounds at the end of the day? How many friends and helpers would this or that 
bully bring with him to the after-school fight for which I had been reluctantly scheduled? 
Would my stepfather be in his typical punitive mood when I got home? And so on.
Sometimes, I had trouble paying attention even when I wasn’t asleep. I had a habit of 
partially withdrawing from the class discussion and writing down my own thoughts in my 
notebook; this made me appear to be attentively taking notes. However, when the teacher 
would sneak up on me from behind or demand to see what I was writing, the truth would 
out, and one can imagine the consequences.
As time passed, I would have to say that I grew increasingly resistant and unresponsive to 

the Pavlovian conditioning on which much educational methodology is based. I suspect that 
between home and school, there had been a certain amount of cumulative desensitization.
These problems eventually got me stationed nearly full-time in the school library, where I 
greatly preferred to be anyway. Later, I was finally excused from attendance except as 
required in order to collect and turn in my weekly assignments.
Please describe your pre-adolescent years growing up? How precocious were you? 
Did any teachers see your potential during that time? Were any effective at guiding 
you?
I was precocious in some ways, normal in others. I had three little brothers; except for the 
youngest, we were only a year or so apart in age. We played together like normal kids, 
ignoring differences of scholastic achievement.
Our family situation was somewhat extraordinary, and we felt the consequences acutely. 
We moved around a lot, and were usually the poorest family in town. My stepfather 
sometimes worked as a journalist, and when doing so, he could be brutally honest about 
things like small-town corruption. This often resulted in harsh feelings against him, and by 
extension, against all of us, inevitably leading to prejudice.
Teachers are supposed to be immune to this kind of prejudice. But as my brothers and I 
learned the hard way, not all of them live up to ideals. Add to that the social liabilities of 
often being “the new kids in town” and the tendency of schoolchildren to shun or mistreat 
out-group members, and the result was a mostly unpleasant K-12 experience.
A few of my teachers – perhaps as many as one in four – sometimes tried to make things a 
little less unpleasant and a little more rewarding for me. However, this sometimes backfired, 
as when they would arouse the resentment of other students by holding me up as a 
scholastic exemplar.
Adolescence is a time of big changes. Describe your high school years. In what ways 
were they better and worse than your first years? Presumably, you blew away all 
standardized tests. Were Harvard and Caltech beating down your doors to have you 
as a student? If not, why not? What happened after your high school years? When 
did your formal education end?
I’ll be candid. I quickly came to see high school as an extended, survival-of-the-fittest 
physical combat regimen punctuated by the occasional brief oasis. A not insignificant 
number of people know, or at least think they know, what it feels like to be among the least 
popular kids in high school; for a male student, it means that one either fights to defend 
oneself, or swallows an unlimited amount of disrespect and abuse from other kids, right 
down to mockery and physical assault. I usually opted to defend myself, thus precipitating a 
further decline in personal popularity while at least salvaging a modicum of respect.
As far as I was concerned, I was “ready for college” by the 10th grade (and arguably before 
that). But of course, that meant nothing without an advocate within the system to furnish 

advice and recommendations. As I was without a mentor and in no position to pursue my 
own interests, that was the end of the story until graduation.
I applied to two colleges, each of which offered me a full academic scholarship. One 
pressured me to major in ancient languages, but as I preferred mathematics and 
philosophy, I chose the other. Unfortunately, for various reasons I hadn’t foreseen, this was 
not a good choice. To make a long story short, I found myself with a case of culture shock 
and intellectual alienation, an “advisor” harder to track down than Bigfoot, and sharp 
personality conflicts with two or three of the worst and most self-absorbed instructors I could 
have imagined … and after my K-12 experiences, that’s really saying something!  I was 
even accused of participating in a riot at which I was never present, and given no 
opportunity to respond. All in all, I’d have to call it a disappointment.
Then, after taking a year or so off to work and save up a little tuition money, I tried to make 
a fresh go of it. This turned out to be yet another waste of time. Although I encountered 
unexpected physical difficulties that I tried hard to resolve, the university administrators – 
citing my problems at the first institution – refused to budge in my direction, leaving me no 
choice but to depart in mid-winter. At that point, having finally read the writing on the wall, I 
shook the dust from my work boots and took my intellectual destiny into my own hands as 
best I could. As Wittgenstein might have put it, I resolved to “go the hard way”.
On several occasions after that, I allowed myself to be persuaded by others to give the 
system yet another chance. On the first occasion, the college claimed to have “lost” my 
application after the enrollment deadline. On the last occasion, I applied to the PhD program 
at a foreign university only to have my application rejected without explanation despite 
glowing written recommendations from various highly credentialed people. I could go on.
Evidently, my reconciliation with academia was not to be. Experience has taught me that I 
was right to trust my intuition: there is nothing to be gained by pretending that academic 
involvement is necessary, or even always desirable, in the quest for truth and knowledge. In 
fact, owing to the academic penchant for orthodoxy and political correctness, it can be a 
hindrance.
For many years you earned your livelihood in non-intellectual pursuits. When 20/20 
did a story on you in the late 1990s, they described a long string of jobs you have 
held, including bouncer. What were some of these jobs? Which of them did you 
enjoy most?
I’ll skip the boyhood lawn work and paper routes.
My first real job was farm work…weeding potatoes (fifty cents per hour) when I was 13-14 
years old or so. The money went to my mother for groceries. The next, which occupied me 
until I was about 17, was working as a ranch hand on various ranches, where my regular 
duties included stacking hay, manual irrigation, and working with horses and cows (12-14 
hours of labor per day, 7 days a week come rain or shine, $200 per month before taxes plus 
a bunk in a covered wagon and all the eggs and stew I could eat).

My next employer was the US Forest Service, in which I served as a firefighter, lookout, and 
regional fire guard (something less than a full ranger, but with direct responsibility for a 
relatively large geographic area). This work was seasonal and spanned about four years. 
During the winters, I began working construction, from digging ditches to banging nails and 
pouring concrete on projects from small houses to ten-story grain elevators.
Only when I was in my twenties did I get around to nightclub security, i.e., being a bar 
bouncer, interspersed with yet more construction work. It was risky and often distasteful, but 
it let me save a little money and usually left me enough physical and mental energy to 
pursue my studies during off-hours.
There were other jobs as well – e.g., digging clams on the Great South Bay of Long Island, 
a bit of ocean lifeguarding, a few weeks spent installing and reconditioning tennis courts, 
and so on. And of course, when the weather was bad or times were tough, there were also 
a few stretches of unemployment.
In keeping with my days as a ranch hand, I now finally own a horse ranch, but at a time 
when the horse market is severely depressed. While this has kept my income not far above 
its previous low range, at least the lifestyle is clean and healthy.
As far as enjoyment is concerned, almost all of these jobs offered a little of that, though in 
most cases not enough to make up for the pain (the potato weeding and tennis court work 
were uniformly boring and hell on my back). It’s notoriously hard to work for someone else 
when one’s true calling beckons; scheduling conflicts are unavoidable and tend to be 
painful.
Now that I own a ranch, most of the scheduling is up to me. That alone makes it the best 
“job” I’ve ever had, all the more so because it calls on much of what I learned in other lines 
of work.
Why, despite your intellectual gifts, did you turn to non-intellectual pursuits for your 
livelihood?
I did so by necessity.
Through its jealous stranglehold on intellectual certification, academia has all but 
monopolized gainful intellectual activity; if one has no money, no connections, and no 
degree, one’s intellect is all but vocationally irrelevant. Given two people, one with an IQ of 
100 and a college degree and the other with a 200 IQ and no degree, all else equal, any job 
that involves much intellectual processing will go to the former in almost every case. This is 
because academia has managed to convince the world that intelligence equates to 
academic certification, when in fact, it is a largely innate capacity independent of academic 
training and credentials.
When I first parted ways with academia, I understood that this would hurt my chances to 
make a good living; I had no choice in the matter anyway. But in deciding to make an 
intellectual go of it on my own, I simply discounted the tremendous obstacles I would face in 

getting attention for my work. After these obstacles began to emerge, I moved on several 
occasions — always at the urging of others that I not “give up on the system” — to give 
academia another try. But even after I’d been featured in the national media and presented 
in a #1 NY Times best-seller as someone who would probably be well-suited for a faculty 
position at a university such as Harvard, my efforts were either rebuffed or ignored!
It seems that academia, rather than encouraging the participation of anyone in particular, 
finds comfort in the assumption that no matter whom it excludes and neglects, no good idea 
can possibly elude it. But of course, this assumption is absurd on its face. Academia is a 
function of individual minds which exist independently of it, not vice versa; individual 
academics serve its educational mandate, produce its scholarly output, and harbor its 
loftiest aspirations. Without them, academia is literally nothing.
This makes it all the sadder that by ignoring outsiders and thus keeping a closed shop while 
cravenly submitting to the diktats of their institutions and embracing “consensus” at the 
expense of intellectual freedom and honesty, many academics unwittingly reinforce the 
hubris of an overweening bureaucracy which sometimes places its own interests above 
truth itself. Clearly, this is not the way to do justice to the intellectual potential of humanity.
As I have no more time to waste on vain attempts to penetrate the academic bureaucracy, it 
now behooves me to seek freer channels of communication.
Despite earning your living largely through blue-collar jobs, you never stopped 
learning. Indeed, most of your education has come through self-study. Please 
describe the course of study that you set for yourself. What were you reading? How 
much were you reading? What fields of inquiry most interested you? Who were your 
conversation partners (literally and metaphorically)? What were you trying to 
accomplish through all this study?
When I was very young (up to about the age of 6), I concentrated on the books in my 
grandparents’ bookshelves, on subjects ranging from science to Egyptology and Asian 
culture (my grandfather was a shipping executive who made frequent trips to China, India, 
and other Asian countries). I remember being especially fond of the encyclopedias. Then I 
discovered fantasy and science fiction, reading extensively in those genres.
By the time I was 13 or14, I had moved on to authors requiring a bit more emotional 
maturity, e.g., Shakespeare, Tolstoy, and Dostoevsky, as well as philosophers like Charles 
Darwin, Bertrand Russell, and Albert Einstein (Einstein is widely regarded as a physicist, 
but his work goes so deeply into the fundamental nature of reality that he can also be read 
as a metaphysical philosopher). When I went away to college for nine months, I read a 
considerable amount of classical literature as part of the curriculum. This included 
Augustine, Anselm, and Aquinas, who provided me with an introduction to theology and 
metaphysics. Meanwhile, when I could afford it, I’d buy and pore over dense but influential 
works like Kant’s Critique of Pure Reason and Russell and Whitehead’s Principia 
Mathematica.
After that, my access to good reading material was often quite limited, and I was forced to 

become more opportunistic as a reader. Translation: often lacking access to a decent library 
or bookstore, I read what was available. Books that I went out of my way to acquire usually 
involved technical subjects like logic, mathematics, philosophy, physics, and biology.
As one might gather from this reading list, I was feeding the general hunger for knowledge 
typical of bright youngsters. But at all times, my underlying goal was to know deep reality, 
the world as it really is rather than as it is superficially presented to us by the senses. I had 
discovered early on that no matter how smart the people I questioned, they really had only 
the vaguest understanding of the world, and I found this particular kind of ignorance totally 
unacceptable. Why live this life we are given without knowing it for what it really is?
Thus, while other kids worried about being liked and looked forward to making money, 
acquiring lots of cool stuff, and having an all-around good time, I poked around by myself in 
the deep, dark crevasses of reality in search of meaning and flashes of enlightenment…
always a strange occupation for a young person, and becoming more so with each passing 
year.
In the last decade, you’ve been actively developing what you call a “Cognitive-
Theoretic Model of the Universe.”
Actually, I’ve had the essence of the CTMU (Cognitive-Theoretic Model of the Universe) for 
well over two decades, and began to publish essays on it in 1989.
Since then, I’ve been periodically amused to watch academia and/or the media get excited 
and wax philosophical over the rediscovery of what seem to isolated, vaguely-formulated 
scraps of it by “approved” sources.
While I certainly don’t want to downplay the insights of others, I’ve come to suspect that in 
the dog-eat-dog, publish-or-perish world of academia, few if any are really up to making a 
forest of the trees.
Can you sketch the CTMU — in plain English — for our readers?
The name literally says it all. The phrase “Cognitive-Theoretic Model of the Universe” 
contains three main ingredients: cognitive theory, model, and universe. Cognitive theory 
refers to a general language of cognition (the structural and transitional rules of cognition); 
universe refers to the content of that language, or that to which the language refers; and 
model refers to the mapping which carries the content into the language, thus creating 
information. The way in which the title brings these three ingredients together, or “contracts” 
their relationship to the point of merging, reflects their perfect coincidence in that to which 
the title implicitly refers, i.e., reality (the physical universe plus all that is required to support 
its perception and existence). Thus, the CTMU is a theory which says that reality is a self-
modeling universal language, or if one prefers, that the universe is a self-modeling 
language.
The operation of combining language, universe, and model to create a perfectly self-
contained metalanguage results in SCSPL, short for Self-Configuring Self-Processing 

Language. This language is “self-similar” in the sense that it is generated within a formal 
identity to which every part of it is mapped as content; its initial form, or grammatical “start 
symbol”, everywhere describes it on all scales. My use of grammatical terminology is 
intentional; in the CTMU, the conventional notion of physical causality is superseded by 
“telic causation”, which resembles generative grammar and approaches teleology as a 
natural limit. In telic causation, ordinary events are predicated on the generation of closed 
causal loops distributing over time and space. This loop-structure reflects the fact that time, 
and the spatial expansion of the cosmos as a function of time, flow in both directions – 
forward and backward, outward and inward – in a dual formulation of causality 
characterizing a new conceptualization of nature embodied in a new kind of medium or 
“manifold”.
That’s as simple as I can make it without getting more technical. Everything was 
transparently explained in the 56-page 2002 paper I published on the CTMU, which has 
been downloaded hundreds of thousands of times. But just in case this still doesn’t qualify 
as “plain English”, there’s an even easier way to understand it that is available to any reader 
familiar with the Bible, one of the most widely read and best-understood books ever written.
In the New Testament, John 1 begins as follows: “In the beginning was the Word, and the 
Word was with God, and the Word was God” (my italics). Much controversy has centered 
on this passage, as it seems to be saying that God is literally equivalent to logos, meaning 
“word”, “wisdom”, “reason”, or “truth”. Insofar as these meanings all refer to constructs or 
ingredients of language or to language itself, this amounts to the seemingly imponderable 
assertion that God, of Whom believers usually conceive as an all-powerful Entity or Being, 
somehow consists of language. The CTMU is precisely what it takes to validate this 
assertion while preserving the intuitive conception of God as the all-knowing Creator – or in 
non-theological terms, the “identity” or “generator” – of reality. Nothing but the CTMU can 
fully express this biblical “word-being duality” in a consistent logico-mathematical setting.
The CTMU is not just a theory; it is logical model theory applied to metaphysics, and as 
much a logical necessity as any branch of mathematics or philosophy. One can no more 
escape from it than from X=X or 1+1=2. But when it comes to something that packs this 
combination of scope and power, many people, including certified academics, committed 
atheists, and even some religious believers, are apparently afraid to stare X=X in the face.
Little wonder. After all, once one has beheld the metaphysical structure of reality, there is no 
longer any such thing as plausible deniability or defense by ignorance; it’s the end of 
innocence, so to speak. Understandably, many people find that a little scary.
What are you trying to accomplish with the CTMU?
As a general theory of reality – or if one prefers, the general framework of such a theory – 
the CTMU has potential applications in virtually every field of human inquiry and endeavor.
Human knowledge is a veritable Tower of Babel. Various theories of science, mathematics, 
and philosophy centering on various parts and aspects of reality are couched in diverse 
formalisms and vocabularies that often bear little resemblance to each other and exhibit no 

obvious connections. The situation is reminiscent of a disorderly range of mountains; one 
can get from one valley to another by climbing the mountains, but by the time one gets to 
the next valley, the last is no longer visible. Worse, the inhabitants speak a different tongue 
with no discernable connection to the languages spoken in other valleys.
Theoretical compartmentalization creates the impression that certain parts or aspects of 
reality are indefinitely related to each other or not related at all, causing rifts and false 
divisions to appear in our conceptual and perceptual topography, fracturing and fragmenting 
our worldview. Sometimes, this leads to scientific crises; for example, relativity theory is 
seemingly impossible to unite with quantum theory. Some rifts may even be seen as mutual 
irrelevancies; for example, science and theology are often considered to be separated by an 
unbridgeable gulf, and thus mutually irrelevant.
This is hardly an ideal situation. In a reality where the physical world is held accountable to 
empirical or mathematical science, any scientifically irrelevant theology is implicitly 
displaced. This makes theological systems untouchable by science and vice versa, 
depriving science of moral guidance and encouraging the revelatory creation of different 
metaphysical realities associated with conflicting promises and instructions involving the 
physical world. (These metaphysical realities include not just overtly religious frameworks, 
but the random materialism embraced by many scientists and followers of science.) The 
resulting disagreements cause, or provide pretexts for, real-world conflicts.
In order to unify and make sense of our knowledge, we must have a universal foundational 
language in which the special-purpose languages of science can be consistently expressed 
and interpreted. The fact that this foundational language controls the interpretation of 
physical theories demands that it be metaphysical; it must refer to science “from above”. 
Yet, in order to do its job, it must also be necessarily true, which requires that it be a 
mathematically verified ingredient of science.
In other words, the required metalanguage is that through which science, including both 
mathematics and empirical applications of mathematics, becomes self-referential and self-
normative…the “bootstrapping” of ordinary mathematical-scientific discourse to a higher 
verificative level of discourse spanning science in its entirety. This requirement leads 
directly to the CTMU and SCSPL, exactly as described in this interview and elsewhere.
Among the benefits of such a language are these: properly developed and applied, it can 
synergistically unite the various fields of science; it can remove false conceptual divisions, 
reconciling science with philosophy and theology, mathematics with physics, and physics 
with metaphysics; it can promote a general understanding of reality, so that people cannot 
be so easily cheated of meaning by those wishing to create an illusion of amorphous 
“relativism” in order to exploit the attending moral vacuum; and it can serve as the basis of 
an overarching worldview capable of modeling all lesser theories and creeds up to mutual 
consistency, thereby promoting intellectual accord and conducing to peace and harmony on 
earth.
Obviously, the CTMU is a cross-disciplinary project. On what disciplines does it 
draw?

Because an ultimate theory must accommodate every valid theory pertaining to every part 
or aspect of reality, it must be approached in the most general terms possible. It must also 
be formed from the top down rather than just from the bottom up, as it is easier to maintain 
initial coherence as specificative distinctions are added than to create it ad hoc by cobbling 
together distinct entities. This means that we must begin with a perfectly general theoretic 
identity and work inward.
One therefore begins with mathematical logic, all the way from the propositional and 
predicate calculi to lattices and model theory; arithmetic, abstract algebra, and elementary 
analysis; basic probability theory and statistics; foundational mathematics, including the 
theories of sets and categories; and of course, metaphysics and theology. One can then 
move on to the theories of computation and information; the algebraic and computational 
theories of language, generative (computational) grammar and the logical theory of 
metalanguages; geometry and the theory of manifolds; classical and quantum physics, 
including relativity theory and cosmology; the study of causality and evolution in fields like 
biology, neuroscience, and cognitive psychology; decision theory and economics, especially 
as they relate to the nature and maximization of utility and the stratification of utility 
functions and distributions; and so on up to (intellectual) exhaustion.
I’ve just named what some would view as an intractable multiplicity of disciplines, each 
splitting into branches, each of which is rich enough to occupy a dedicated specialist 
throughout his/her entire academic career. This naturally presents a problem for generalist-
interdisciplinarians, especially independent researchers without access to academic 
libraries or the academic grapevine. Outsiders are seldom invited to academic conferences 
and symposia designed to bring interested academics up to speed on the work of 
specialists; they may even lack knowledge of up-to-date search terms through which to filter 
recent academic literature for material relevant to their work. Accordingly, they may find it 
more expedient to address conceptual deficiencies and solve problems from scratch than to 
sift through vast piles of academic literature for what they need.
Take mathematics, for example. The scholarly output of the mathematical community has 
been nothing short of tremendous. This obviously has an upside: mathematicians who are 
“in the loop” can often find what they need in the prior work of other mathematicians, or at 
least determine that they’re in virgin territory unexplored by others. But it also has a 
downside: those who are not in the loop may literally find it easier to discover or rediscover 
the mathematics they need than to scour the literature for some intelligible indication that 
somebody else has already written about it, and then decipher and back-engineer the 
meaning of the complex vocabularies and symbologies employed by previous authors. 
Personally, I’ve found myself in this position on more than a few occasions.
Much the same applies to other fields of science, where it can be even more difficult to 
solve problems instead of looking up their solutions. So let’s just say that I’ve had my share 
of challenges along the way…and that I really appreciate the Internet!
Have you completed work on this model or is it still a work in progress? What, if 
anything, remains to be done?
The classical Laplacian-deterministic worldview is all but dead. As reality is affected by 

every possible kind of ambiguity, uncertainty, indeterminacy, and undecidability, no theory 
of reality can ever be complete. In principle, this makes any such theory a permanent work-
in-progress in which a very great deal always remains to be done. Exploration must 
continue.
However, a theory of reality can still be comprehensive, classifying knowledge in the large 
rather than determining it exhaustively. The CTMU already provides a self-contained 
framework for theorization about reality, represents a pronounced departure from 
established lines of inquiry, and was ready for prime time even as presented in my 2002 
overview The Cognitive-Theoretic Model of the Universe: A New Kind of Reality Theory.
Of course, the CTMU doesn’t end with the paper. I do something new to develop and refine 
it nearly every day, and it all adds up. As a metaphysical (ontological, cosmological, and 
epistemological) framework, the CTMU has no real competition, and can thus be developed 
without fear of having to start over from scratch.
What has been the model’s reception in the academic world? How are you 
publicizing this work?
“What reception?” indeed.
While it’s true that I sometimes see my own concepts, some of them decades old and 
ostensibly original with me, flow from the pens of properly-certified academics and 
regurgitated in the lofty ad hoc speculations of university philosophers, I have (to my 
knowledge) yet to be officially cited by any of them. While I’ve been told that some 
influential academics have privately admitted to finding my work “ambitious”, “interesting”, 
and even “admirable”, they would apparently need to be coaxed out of the closet before 
admitting it publicly. (After all, I’m not a member of the club.)
The CTMU is a profound departure from other theories of reality. To a typical academic 
snob, my status as a working man with only a year or so of college no doubt suggests that it 
cannot withstand expert analysis; given its unique structure, any flaws should be readily 
apparent to some qualified expert who can be attached by name to his arguments and thus 
held to reasonable standards of analysis and debate. But despite occasional incoherent 
sniping by anonymous Internet gadflies, no qualified individual has ever found fault with it. 
This is not only because it is (as some claim) “incomprehensible” or philosophically and 
theologically loaded, but because it is substantially correct.
Again, I’ll speak freely. It appears to me that the academic world is far too wrapped up in 
itself to make time for anything or anybody else. The situation is exacerbated by a tendency 
among academics to assume that if it wasn’t coughed up from the belly of the academic 
beast, it can’t be worth a glance. The prospects are especially dim for potentially “game-
changing” work that is perceived to run afoul of academic orthodoxy, threaten the economic 
status quo, or have difficult social or political ramifications.
As I’ve already mentioned, academia has all but monopolized gainful intellectual activity 
through its stranglehold on intellectual certification. The dependence of economic 

opportunity on academic certification is impossible to miss; it should be no less obvious that 
this dependency relationship extends to the intellectual advancement of mankind. Woe to 
any would-be contributor who parts ways with the academic machine, for intellectual 
commerce is governed by a “publish-or-perish” economy of publication and citation in 
academic journals, wherein the obstacles to publication and proper attribution are 
proportional to the obscurity of the author, the importance and controversiality of the topic, 
and the level and prominence of the periodical. As a result, important technical works by 
authors without academic credentials and affiliations are unlikely to be published or cited, 
and even if these authors beat the odds against them, they cannot benefit in any meaningful 
way from their effort.
There are at least two obvious reasons for this situation.
(1) Much like an intellectual trade union, academia reserves all of its benefits for members, 
affording nonmembers no standing, no incentives, and no avenue of recourse should others 
use their ideas without credit. By making sure that people cannot get along without 
academic credentials, academia assures itself of a continued influx of paying clients and 
thus feeds its pyramidal growth economy without necessarily delivering all of the 
educational services it owes the public.
(2) Professional academics consider it risky to associate with those who may be perceived 
as “unqualified”, preferring to cite well-credentialed, well-connected authors likely to reflect 
well on their academic reputations. This aversion to risk applies a fortiori to journal editors 
and reviewers, and even to the commercial publishing houses specializing in scholarly 
nonfiction. This greatly increases the likelihood that meaningful contributions by academic 
outsiders will not be published, and if they are, that they will be used without credit.
Unfortunately, this means that people without academic credentials have nothing to gain, 
but everything to lose, by attempting to publish in academic journals, and thus that 
academic journals do not qualify as a rational venue for their ideas. Just as one need not 
step off a succession of cliffs to understand what will happen if one steps off the next cliff, 
one need not repeatedly hurl oneself at the closed shop door of academia to know that one 
will simply bounce off, the sound of impact echoing without citation or appreciation if at all.
As far as publicity is concerned, I’m afraid that I’ve been derelict in the self-promotion 
department. As a matter of personality, I’m far more interested in generating and pursuing 
the implications of my ideas than in selling them to the media and the public. I’ve never 
been keen on the salesmanship aspect of anything I’ve done; there are too many 
accomplished salesmen out there who excel at attracting attention to themselves despite 
having nothing of real value to sell, and competing against them is nearly always a waste of 
time. This is especially true in the intellectual sphere. Unless the audience is able to follow 
complex argumentation while steadfastly resisting sophistry and rhetorical trickery, such 
competitions usually devolve to shouting or pissing matches.
Of course, despite my contempt for sales, I keep promising myself that in the future, I’ll do a 
better job of disseminating and promoting my work. I think that once the Mega Foundation 
Media Center is up and running, we’ll start gaining more headway in that department.

You founded the Mega Foundation to “create and implement programs that aid in the 
development of extremely gifted individuals and their ideas.” Why is it necessary to 
help these exceptionally gifted kids? Don’t they have a leg up on the rest of us 
already? How many deal with challenges similar to your own in growing up?
Owing to the shape of a bell curve, the education system is geared to the mean. 
Unfortunately, that kind of education is virtually calculated to bore and alienate gifted minds. 
But instead of making exceptions where it would do the most good, the educational 
bureaucracy often prefers not to be bothered.
In my case, for example, much of the schooling to which I was subjected was probably 
worse than nothing. It consisted not of real education, but of repetition and oppressive 
socialization (entirely superfluous given the dose of oppression I was getting away from 
school). Had I been left alone, preferably with access to a good library and a minimal 
amount of high-quality instruction, I would at least have been free to learn without useless 
distractions and gratuitous indoctrination. But alas, no such luck.
While my own background is rather exceptional, it is far from unique. Many young people 
are affected by one or more of the same general problems experienced by my brothers and 
me. A rising number of families have severe financial problems, forcing educational 
concerns to take a back seat to food, shelter, and clothing on the list of priorities. Even in 
well-off families, children can be starved of parental guidance due to stress, distraction, or 
irresponsibility. If a mind is truly a terrible thing to waste, then the waste is proportional to 
mental potential; one might therefore expect that the education system would be quick to 
help extremely bright youngsters who have it rough at home. But if so, one would be wrong 
a good part of the time.
Let’s try to break the problem down a bit. The education system is subject to a psychometric 
paradox: on one hand, it relies by necessity on the standardized testing of intellectual 
achievement and potential, including general intelligence or IQ, while on the other hand, it is 
committed to a warm and fuzzy but scientifically counterfactual form of egalitarianism which 
attributes all intellectual differences to environmental factors rather than biology, implying 
that the so-called “gifted” are just pampered brats who, unless their parents can afford 
private schooling, should atone for their undeserved good fortune by staying behind and 
enriching the classroom environments of less privileged students.
This approach may appear admirable, but its effects on our educational and intellectual 
standards, and all that depends on them, have already proven to be overwhelmingly 
negative. This clearly betrays an ulterior motive, suggesting that it has more to do with 
social engineering than education. There is an obvious difference between saying that poor 
students have all of the human dignity and basic rights of better students, and saying that 
there are no inherent educationally and socially relevant differences among students. The 
first statement makes sense, while the second does not.
The gifted population accounts for a very large part of the world’s intellectual resources. As 
such, they can obviously be put to better use than smoothing the ruffled feathers of average 
or below-average students and their parents by decorating classroom environments which 
prevent the gifted from learning at their natural pace. The higher we go on the scale of 

intellectual brilliance – and we’re not necessarily talking just about IQ – the less support is 
offered by the education system, yet the more likely are conceptual syntheses and grand 
intellectual achievements of the kind seldom produced by any group of markedly less 
intelligent people. In some cases, the education system is discouraging or blocking such 
achievements, and thus cheating humanity of their benefits.
The Mega Foundation hopes to provide a modicum of damage control by offering 
encouragement and fellowship to those who were accidentally left behind the door, or 
deliberately held back for the sake of expediency or “social justice”, by those running the 
education system.
When did you start the foundation, where is it located, and what programs are you 
currently offering through it? What are the three best things that could happen to 
help your work with the foundation? It is organized as a non-profit. Where can 
interested persons give donations to it?
The Mega Foundation was begun in 1999 and incorporated in Connecticut. In 2004, it was 
relocated and incorporated in Missouri. It was established for the benefit of gifted people of 
all ages, as well as for all of those who can benefit from their insight. Ultimately, this means 
humanity in general.
While our wider mission has not changed, most of our recent programs and activities have 
been geared for adults, particularly along lines of networking and fellowship. This is largely 
due to the fact that while there are many programs for gifted kids these days, there are few 
for gifted adults who have fallen through the cracks of the education system.
Despite many challenges, we’ve managed to acquire a building for use as a media center. 
For the last couple of years, my wife and I have steadily worked to repair and renovate it 
with limited funds, but have been held back by cost factors and a local scarcity of workmen 
qualified to repair and maintain this kind of facility (a massively-constructed 
decommissioned power station). At this stage of the project, many of our planned activities 
are temporarily on hold. We estimate that the renovations should be complete within two 
years, at which point we will expand our current networking capabilities and resume 
conferencing.
The long-term needs of the Foundation are pretty generic. As one might expect, lining up 
sources of funding and generating exposure and public interest would probably top the list.
For anyone who is interested, our email address is info@megacenter.org.
Looking over American education, K-8, high school, college, and graduate school, 
what say you? Does the U.S. education system make it possible for people like 
yourself to thrive? Should they?
From where I sit, the bottom line is really very simple. There are many people these days 
who are quite low on knowledge and ability, but sport impressive college degrees and great 
jobs, sometimes even in academia itself. Yet, there are others who are at least as intelligent 

as the average college professor and possessed of the will and ability to contribute to 
society, but without a degree and at best menially employed. Many intelligent people 
eventually reach an impasse with the education system despite their best efforts, but when 
they attempt to make do without its stamp of approval, their situation becomes well nigh 
impossible.
There is something very wrong here, and given the power of the system, it cannot deny a 
measure of responsibility for this imbalance and its harmful effects. So my answer would 
have to be yes, the education system should try harder to let the highly intelligent thrive 
within it. Where circumstances make this difficult, it should at least make remedial 
allowances for the exceptional individuals whom it has clearly failed. Through standardized 
testing alone, for example, it could make low-cost or no-cost degrees available to capable 
individuals who cannot afford tuition or benefit from its regimented mass-production style of 
instruction.
Unfortunately, there appear to be some rather unsavory reasons for academia’s reluctance 
to make such allowances. Historically, it has always been subject to pressure by powerful 
economic and political interests. This pressure is generally directed toward the creation of a 
self-reinforcing arrangement: those in power tell academia how they want students to think; 
academia produces a constant supply of certified experts guaranteed to tell those in power 
what they want to hear; and those in power, having placed these experts in advisory 
positions, encourage them to uphold whatever consensus appears to justify their actions 
and desires.
Obviously, far from wanting to stimulate and empower their future competition, the 
socioeconomic elite would rather mold potential competitors into docile workers and 
consumers. Just as obviously, this has nothing to do with maximizing the intellectual 
potential of individual human beings, especially those with psychological traits that could 
make them “problematical”. Woodrow Wilson, speaking as the President of Princeton 
University in 1909, put it like this: “We want one class of persons to have a liberal 
education, and we want another … a very much larger class of necessity … to forgo the 
privileges of a liberal education and fit themselves to perform specific difficult manual tasks.” 
This has been impressed on academia through economic and political pressure exerted 
over many decades by various well-funded and tightly-controlled nonprofit foundations, 
policy institutes, learned councils, and advisory committees.
In other words, while education is obviously a dire social necessity, the education system 
shares a peculiar distinction with the mass media: both are ideal means of indoctrination, 
mental and behavioral conditioning, and social manipulation, all of which are practiced by 
the wealthy and powerful out of sheer self-interest, and all of which are diametrically 
opposed to intellectual depth and objectivity. This exposes the education system to forms of 
interference which bias it against certain ideas and compromise its basic educational 
functionality. Unfortunately, it appears to be unable to defend itself against such 
interference; while subjecting nearly everything but itself to ruthless deconstruction, it 
remains perfectly blind to its own systematic abuse.
Largely thanks to such interference, the education system is now seriously flawed. Its 
problems are almost too numerous to list: it is bureaucratic and peremptory, profit-oriented 

in a pyramidal way, full of prejudice against traditional American culture and values, and 
addicted to various articles of PC nonsense which it prosecutes aggressively and with 
astonishing intolerance and sanctimony. It worships orthodoxy, punishes originality, and 
often rewards intellectual mediocrity as if it were the sacred torch of human brilliance. 
Though unable to justify its highly standardized worldview, it demands near-perfect 
intellectual conformity therewith, thus creating a suffocating atmosphere for students and 
teachers alike. One could easily go on.
Despite these failings, most people still see the education system as the universal incubator 
and caretaker of human knowledge, the cynosure of human intellectual progress, and a 
safe repository of the priceless intellectual resources of mankind, naively trusting in the 
integrity of honest and dedicated teachers and researchers to prevent outside forces from 
subverting its machinery for ulterior purposes. However, America’s steady decline in overall 
academic performance, and our current dismal socio-economic predicament – for both of 
which academia clearly bears a large measure of responsibility – show that this faith has 
been largely unwarranted.
While some of the responsibility can be “kicked upstairs” to the political realm and beyond, 
educators are still left holding the bag. It is time for them to worry more about education, 
and less about guarding the power structure and promoting its conceptions of political 
correctness and social justice at a net loss of our most crucial intellectual resources.
What’s good about the education system? What isn’t? What would you change?
What’s good about the education system is that it provides a bit of worthwhile instruction to 
those who need it while bringing people together to exchange information and share ideas. 
What’s bad about it is that it does so more inefficiently, inequitably, and unaffordably by the 
day, and mixes legitimate educational content with various questionable assumptions that 
turn up, unsurprisingly enough, in questionable social and economic policies.
If I could change this, I would. Unfortunately, it appears to be driven by large concentrations 
of money and power, and the kind of media and government cooperation that only big 
money can buy. Since I don’t have that kind of money, I’ll confine my answer to educational 
methods themselves.
First, let’s make a distinction between the bottom-up and top-down approaches to learning. 
Bottom-up learning starts with the details and basic skills and works toward the big picture; 
top-down learning starts with the big picture and uses its overall organization to motivate 
and clarify the details. Those who favor the standard bottom-up approach hold that in any 
subject, the requisite skills and details must be mastered before higher-level knowledge can 
be imparted; those who favor the top-down approach hold that the big picture provides 
crucial motivation and clarity regarding the requisite skills and details. Learning is best 
achieved by constructive feedback between these two approaches.
Unfortunately, this constructive feedback is seldom if ever properly achieved in standard 
curricula. Instead, one usually finds a cumulative sequence of tedious courses designed to 
teach facts and skills without concern for student interest or motivation. Generalization 

tends to be excessive; guiding principles are offered without sufficient justification, while in 
mathematics, methods and details are shorn of motivation and understanding for the sake 
of “abstraction” and “rigor”. Essentially, the student is expected to trot briskly on the 
academic treadmill like a donkey after a carrot on a stick, eyes on the prize of a prestigious 
degree and the material rewards that it seems, ever more deceptively, to promise. The all-
important flashes of insight craved by gifted minds are regarded as superfluous.
Thus, the standard curriculum may be likened to a play which keeps its audience engrossed 
with a series of portentous mysteries and cliffhangers, except that the portent is largely 
missing, and the mysteries appear disconnected from each other and therefore utterly 
trivial.
In technical fields, the problem is exacerbated by the way in which textbooks are often 
written. Background knowledge is assumed, rigor and abstraction are rigorously and 
abstractly pursued, and motivation and proper interpretation are left to the instructor, without 
whose helpful hints and background explanations the text is all but indecipherable. This 
prevents the text from being of much use to academic outsiders, and is seemingly 
calculated to make even the brightest and most eager autodidact throw up his hands and 
slog resignedly back to academia, hat and money in hand.
Obviously, this sort of thing is very supportive of the academic profit incentive and the long-
term financial security of academia and academics. After all, academia does not need to 
worry about engaging the student when attendance is either compulsory or coerced by 
threats of lifelong poverty and frustration for want of credentials.
Unfortunately, it is not nearly as beneficial for the student or for the world at large, which 
loses the insight of brilliant minds unable to make the academic connection.
Where to from here? What are your aspirations for the future? What projects are you 
working on now and what projects would you yet like to undertake? Any final 
thoughts you would like to share with our readers?
Distilled to a single sentence, my aspirations come down to making the world a better place.
I’ve already mentioned the Mega Foundation Media Center. Despite the fact that it’s still full 
of lumber, tools, and scaffolding, we’ve already got it outfitted with some sound and video 
equipment. Our idea is to produce educational films and related documentaries. We have 
some very promising projects on the drawing boards.
Some of these projects relate to a book I’ve been writing on mathematically proving the 
existence of God. Surprising as it may seem, this can certainly be done. In fact, save for a 
few crucial ingredients, it was nearly accomplished by (e.g.) Anselm of Canterbury in the 
11th century AD. (Sadly, neither Anselm nor his various followers and modern analysts 
were able to pin down all of the logic and ontology required to fill out and support his 
essential argument.)
Some people, reasoning from past failures, regard such a proof as impossible. But then 

again, many people had considered it impossible to solve the venerable chicken-or-egg 
problem, for which I presented a concise case-by-case solution around a decade ago. The 
chicken-or-egg problem and the existence of God both relate to the general issue of circular 
dependency, a connection to be explored in the book.
I would hope that in time – if we still have the time – my work along these lines could 
revolutionize theology. Some will no doubt warm to this prospect; others will not, including 
committed atheists, uneducable agnostics, and theists who insist on ascribing illogical 
“divine properties” to God on dogmatic grounds ultimately having little to do with core 
scripture. But no matter what anyone may say, truth, logic, and God are equivalent 
concepts. To cheat one of them is to cheat all of them.
I believe that we can afford to cheat none of them, and I’m quite sure that God would agree.
PROLOGUE TO BUFFOONERY
by C.M. Langan
Those of you still with the group know me only as the author of the CTMU, a complex theory 
describing the nature of reality.  Few of you have been sufficiently impressed with the theory 
to do more than criticize it in ways that suggest you have read nothing about it. While I have 
already used the CTMU for applications that would make the careers of any team of 
credentialed signatories -- granted, they would have to enlarge on certain aspects, but they 
would also be writing for a specialty journal in which such detail is welcome and appropriate 
-- I have been rewarded only by silence and unpopularity. This raises the question of why I 
continue to bother with your opinions.
The answer has several parts. First, it is against my nature to let people sell themselves 
short, and anyone who persistently disputes the given applications is ultimately going to 
look less intelligent than he now assumes he is.  Next, I have a stake in the  journal, having 
used it to introduce the theory. You all had a part in that, and -- despite your limited uptake 
-- must be treated accordingly.   Next, I keep hearing about the unlimited potential of this 
group to make a mark in the annals of intellectual history, if not to solve problems like P?
NP, the four-color map problem, and the several problems whose solutions   I've already 
published.   If I had no credence in this potential, I would never have used  Noesis for 
anything but mild entertainment, which is all that the majority of members seem to get out of 
it.  And last but not least, the CTMU is of great intrinsic importance.  In a sense truer than 
many of the weird cosmologies propagated in professional journals, it promises to be an 
"ultimate theory of everything".  If you still doubt it, I suggest that you finally put yourself to 
the trouble of reading the issues I edited...even if it means having to wake up and spill a 
little coffee on them.
Granted, I used a few too many big words.  Mea culpa.  But when trying to introduce any 
theory in a limited number of words, one must often resort to very dense language.  Even 
so, I kept the dense passages to a minimum and used as many illustrations as I could fit 
between them.  The end result is that for every objection made by any of you,  I can cite 
several passages from Noesis 44-49 in light of which it should not have been made.  This is 
a blanket observation which applies to every criticism offered thus far.  To object is fine, but 

you have to be able to listen to reason.
Let's take an example.  Those of you who read Noesis only casually are aware of the recent 
criticisms of Messrs. Cole and Dicks, both of whom object to "inadequacies" in the CTMU. 
 Consider those involving probability theory.  Chris says that probabilistic computations are 
impossible without very specific kinds of prior information.  George claims that probabilistic 
computations are necessary and should therefore be undertaken regardless of how little 
information is available.  A contradiction plainly exists between these viewpoints.  But rather 
than criticizing each other, they both criticize the CTMU, which is -- for reasons long since 
given --  the only formalism in which it is possible to reconcile their views. Probabilistic 
paradoxes  do  indeed  exist,  and  they  are  utterly  inimical  to  any  naive  (non-
metamathematical) approach to empirical induction.   Yet, it's me who gets caught in the 
crossfire, despite the CTMU resolvability of Chris's paradoxes and the CTMU formalization 
of George's style of induction! This situation is even more pronounced with respect to 
Newcomb's paradox.  If not for the CTMU, the fur would still be flying among the partisans 
of these two members.
Nonetheless, I have managed to use your objections in a way that maximizes their value. 
 Any creative researcher who qualifies as more than a drone faces two kinds of problem. 
 First, he must solve the problem his research is designed to address.  And then he must 
figure out how to finesse it past the majority of his self-serving, mutually negative, and 
deliberately obtuse "peers", who can always be expected to prefer their former viewpoints 
no  matter  how  weak,  inadequate,  or  paradox-ridden  they  may  have  been.   The   first 
problem  is  logical,  mathematical,  and  scientific.   The  second  is  psychological  and 
political...in short, human. Even the most talented and intelligent thinkers, who are easily up 
to the first phase of this process, are dismayed and disoriented by the next.  And I'm talking 
about people who are for the most part already accepted within their disciplines.  If they 
weren't -- as I am not -- many of them would fold up and start selling insurance. 
The premature objections thus far leveled at the CTMU and its applications in Noesis are 
valuable  in  that  they  indicate  those  parts  of  the  theory  which,  having  been  initially 
unpalatable to high-IQ readers, are likely to stick in the craws of the public at large.  They 
can thus be specifically targeted in the book I'm writing to explain the theory in detail.  In the 
language of the CTMU itself, they will let me focus on particular deficiencies in the acceptive 
and inferential syntaxes of the â-sub-automata for whom the book is being written.   To 
those of you who have at least contributed to this extent, I give thanks.  To the rest of you, I 
really didn't know what to give until the emergence of Rick Rosner suggested a humorous 
approach to the "debate".
Rick, who admittedly walked in late on all this, claims that he doesn't "believe in" my 
solution to the infamous marble problem or in The Resolution of Newcomb's Paradox. Rick 
is too intelligent to make this denial, given my previous explanations.  I'm therefore left to 
assume that he glossed over them, possibly because they don't conform to his idea of good 
reading.  This has compelled me to undertake a total revision of my stilted, pedantic, and 
long-winded writing style.   That is, where I'd been using learned allusions and 99 cent 
jawbreakers, I've taken to using colorful images, cheap jokes, and antediluvian cliches. 
 Alright, so maybe it is beneath the dignity of my ideas.  But hey, you've got to get your 
points across, and maybe this is the way to do it.   So Goodbye, English language, and 
Hello, clown language.

I resolved Newcomb's Paradox.  You quibbled.  I settled the crisis in physics.  You napped. 
 I clarified the process of logical and empirical induction.  You snored.  Nothing I could say 
made the slightest difference in your fixed opinions concerning the issues in question.   I 
have a name, am probably only human, and therefore could not possibly be smart enough 
to tell you anything.   My voice simply doesn't carry that far over the clouds.   But even 
though you tuned me clean out of your elevated awareness, there may yet be one whom 
you will not ignore.  One who is so smart, so endearing, and so outrageously entertaining 
that even the stones grow ears at the sound of his name.  I gave you six issues of Chris 
Langan, a character who failed abysmally to tickle your fancy.  Well, you ain't seen nothin' 
yet, so get ready and get set.   Because now, without further ado, from the steel and 
concrete warrens of the Big Apple, I bring you our new mascot, the unique, the amazing... 
Jojo Einstein, Street Clown. But be forewarned: what he's gonna do to the marble problem, 
he can do as easily to Newcomb's Paradox. 
Now heeeeeere's Jojo!
A DAY IN THE LIFE OF JOJO EINSTEIN, STREET CLOWN
By C.M.  Langan
Any resemblances to real persons living or dead, et cetera.  Jojo Einstein is fictional and his 
narrative need not reflect the views of the author, who nonetheless appreciates his 
apparent adulation.
It was autumn in Manhattan, that special season in the center of the universe.  Jojo the 
clown, despite his sensitivity to life's subtler charms, was focussed in another direction: his 
next meal.  Tourism was off now that the rubes had used up their summer vacations. 
 Conventioneers and business people were abundant as always, but yuppies were not 
Jojo's stock in trade.  He needed the kind of gullible backwoods yokel for whom the city was 
primarily a giant carnival, and summer was their time.  Jojo, curly red wig vibrating in the 
gentle breeze, needed a mark, and needed one bad. 
It was still warm enough for Fishman's Kosher Deli to have its doors open.   Aromatic 
tendrils  of  fresh  lox  and  knishes  wafted  into  his  nostrils,  bunching  as  they  traveled 
downward into clawed fists which mauled his tortured stomach mercilessly beneath the 
puffy, billowing cloth of his enormously polka-dotted jumpsuit.  His belly was as empty as 
the toes of his custom-cobbled size 33-Z floppy shoes, now so worn that he could feel every 
bump in the grimy pavement through their perforated, paper-thin soles.
Jojo stared disgustedly at the sign he had taped to the brown brick facade of the deli. 
 "MOMS!" it read through a colorful cloud of spray painted balloons, "How would your kids 
like to meet a REAL CIRCUS CLOWN? Available Now for Parties." This was followed by 
the telephone number of a combination flophouse-bordello in Hell's Kitchen, with "Please 
Leave Message" written beneath it.  As he peeled loose the sign, he thought spitefully about 
adding a little something -- like (applications welcome) -- next to the number.
A Wall Street cowboy, Stetson cocked, blundered into Jojo as he inserted the rolled-up sign 

into his suit.  "Hey hey, hey!", barked the enraged clown.  "That's right, you.  Ivan Milk-can! 
What's the matter," he yelled at the man's retreating back, "no time to talk? Okay then, step 
lively! Maybe you'll kick yourself in the  tuchis."   Jojo invisibly flexed his large muscles, 
reminding himself to get back into the gym as soon as he had worked out this little problem 
with  his  diet.   He  had  doubled  as  a  strongman  in  the  circus,  and  still  looked  like  a 
bodybuilder underneath his costume.  The street was a jungle, and a clown had to be able 
to defend himself. 
A mime slowed down as he approached, obviously sniffing around for a likely spot to do his 
routine.  "Get lost!" snarled Jojo, not yet ready to relinquish his street corner.  The mime 
scurried away, doing a furtive impression of the irascible clown.  If there was one thing that 
bugged Jojo, it was a street mime poaching on his rightful territory.   Pickings were slim 
enough, and these mimes were everywhere.  Street musicians came in a close second at 
the bottom of his hit parade, but he wasn't above charging them "rent" for the privilege of 
providing him with a soundtrack.  He was doing them a favor; even after his cut, they still 
made more than they could make without his visuals.  It all came down to one simple truth: 
like the lion of the savanna and the shark of the sea, the clown was at the pinnacle of the 
street-showbiz food chain.  And Jojo Einstein was the undisputed king of the street clowns. 
Jojo's eyes narrowed into a hungry, grease-painted squint as they scanned the midday 
crowd on Times Square.  Most of the faces were locked up tight, offering no chink through 
which a famished clown might access what remained of their naivete.  No emotion, only 
motion.  Then, over the traffic, he saw what he was looking for.  It was nothing especially 
obvious, but Jojo's street-trained eyes had seen enough.  What they saw was better than 
stupidity, better than gullibility. It was overconfidence. 
With surprising quickness, the canny clown shuffled directly over the curb and jaywalked 
into the melee of scratched and dented cars.   Brakes screeched as a taxi halted inches 
from him.  The cabby swore viciously, but Jojo -- who would ordinarily have leapt onto the 
hood and done his patented hot dog dance for the amusement of onlookers -- took only 
enough time to smear a small shaving cream pie over the windshield before jouncing 
onward.  He preferred to use Kreemi-Whip, but had eaten the last of that a while ago.  He 
had sucked it directly out of the can, and it had blown so many of his gaskets that his colon 
had sustained a third-degree sunburn. 
"Hey!" he shouted as he closed the gap on his quarry.  "Hey mister, I think you dropped 
this!"  "This" was a wallet, which Jojo held forth as the man looked back over his shoulder. 
 "It's yours", Jojo assured him as he ambled closer.   "See, it has your picture in it!"   He 
snapped the wallet open in the young man's face, releasing a salvo of small paper flowers. 
 The youth, trying to spit away a blossom which had stuck to his lips, surveyed Jojo with a 
mixture of surprise and annoyance. 
"Now that I've got your attention, sir", said Jojo, talking fast as he waddled comically around 
to face his victim, "I couldn't help noticing that you look like a fellow who isn't afraid to take a 
little risk every now and again.   A betting man, if you will!"   He had positioned himself 
directly in the mark's path, preventing escape.   The mark just stood there, his briefcase 
dangling at his side.  A logo, "World's Most Exclusive IQ Society", was tastefully inscribed 
on the mauve vinyl. There was the bait, and man, was it ever perfect!

"You have a very intelligent face.  Anyone ever tell you that?"  The egghead blinked.  "Quite 
intelligent!  Why, I'd bet your IQ is right through the roof!  Am I right?"
"Look here," said the whiz-kid.  "I've got a meeting in..."
"I won't take much of your time.   No siree, bob.   I just want you to take a little IQ test, 
nothing really for a guy like you.  You can even win a little money, one buck gets you a 
dollar, five gets you ten."
"Well, I really don't..."
"Aw, c'mon.  You're not afraid to use your smarts to make a little dough, are you?"  Jojo 
pulled a fist from his pocket, held it under the kid's nose, and opened it for a fraction of a 
second.  The kid glimpsed an innocent-looking pink polyhedron.  "This is a die, a fair die. 
 But instead of the usual six faces, this one has twelve.  A regular die's a cube.  This one's 
something different, I forget the name."  Jojo had to suppress a smile as he feigned a tip-of-
the-tongue word search.  "What do they call it, a, a..."
"A dodecahedron," the kid finished on cue, impaling himself firmly on the hook.  "One of the 
five Platonic solids." 
"Yeah!" said Jojo gratefully.   "That's it, a dodecahedron. Man, I sure wish I had your IQ. 
 This is gonna be so easy for you!"  He pulled a narrow open-ended tube from one of his 
many pockets and deposited the die inside it.  "See, the game is, you shake it once and 
look at the top number.  Then you bet that on the next shake, a different number shows up. 
 One buck gets you a dollar, five gets you ten."  Jojo didn't have a red cent, but this looked 
to be a lead-pipe cinch. 
The mark sighed resignedly, digging into his pants pocket.  He extracted several bills, the 
smallest of which was a five.  Jojo gingerly plucked out a fin.  "You won't regret it, sir, I can 
promise you that!"
"Okay", said the genius.  "It's a fair die.  That means that whatever face shows up on the 
first shake has a one-in-twelve chance of showing on the second shake.  I have an eleven-
in-twelve chance of winning.  Those are great odds, so let's go."  He accepted  the tube and 
gave it a perfunctory shake before looking inside.  A tiny portrait of Jojo grinned up at him, 
eyes crossed towards the number "1" where a nose should have been.  He showed it to the 
clown. 
"Why, it's me!" said Jojo delightedly. 
"So it is", said the kid, too confidently.   "But it's one-in-twelve that you'll survive the next 
shake with the same olfactor."  He shook again, this time more sincerely. 
"Ready to win?" said Jojo, barely able to suppress a burst of untimely laughter. 
"As a matter of fact, I am", said the mark.  He stopped shaking and peered into the tube. 
 His expression turned dark. 
"Lemme see", said Jojo, tilting the tube in his direction as the mark held onto it.   Sure 

enough, there he was again, still with a big red "1" in place of his red rubber proboscis. 
 "Aw, gee, sir, tough luck.  Looks like you got a bad break. Want to go again? Five gets you 
ten, ten gets you twenty."
"Something's fishy here", said the kid with a sour-grapes edge to his voice.  "You won't mind 
if I have a closer look at your 'fair die', will you?"  He put his briefcase down on the sidewalk 
and inverted the tube over his open hand.  The pink polyhedron spilled out.  On every face 
was a picture of Jojo, a "1" in place of his snout. 
"You're a crook", accused the kid.  "Give me back my five dollars, please."
"Oh, ho, ho!" chortled Jojo, royally amused.  "And why am I a crook, pray tell?" He wadded 
up the fiver, stretched his huge ruffled collar open with a finger, and dropped it down the 
opening. 
"You said this was a fair die.  It isn't."
"Really?" said Jojo with mock incredulity.  "How do you figure?"
The kid was indignant.  "The die isn't fair because every face has the same thing on it."
"Well, so it does!" agreed Jojo.  "But look at it, sir.  Doesn't each face have an equal chance 
to end up on top?"
The kid assumed a pained expression.  "So what? Each face is supposed to be different!"
"Is that right! And who says that?"  Jojo's mirth was rapidly approaching an uncontrollable 
level. 
The kid looked Jojo up and down, contempt smeared unprettily across his features.  Then 
he nodded with an I've-got-your-number air.  "I see.  Here, take your junk back.  I'm busy."
"Why, certainly, sir."  Jojo took the tube and die, remaining squarely in the kid's path.  "But 
are you sure I can't interest you in another kind of game, one a little less tricky?"
The mark, too angry for his own good, could not believe that the clown thought he could be 
taken twice in a row.  But that appeared to be the program.  With ice in his voice, he bit 
again.  "What other game?"
Jojo had already pulled a covered black box out of his suit.  He shook it; it rattled.  "There 
are ten marbles in this box.  You pull out ten marbles one at a time, each time replacing the 
marble in the box.  Then you tell me what colors are inside.  Then we look.  If you're right, 
you win.  Five gets you ten, ten gets you twenty."  The brain looked at the box, calculating. 
 Finally he spoke.  "Alright.  But I'm betting twenty dollars.  Agreed?"  He was all fired up, 
red hot for revenge. 
The clown had to struggle to free his spasming larynx from a violent urge to cackle. 
 "Whatever you like!"  He held out his hand.  The kid fished out a Jackson and slapped it 
into his palm.  Jojo raised the box to slightly above eye level and removed the lid.  The mark 
reached up and into the box.   When his hand emerged, a snow white marble glinted 

brilliantly between two fingers.
"White!" said Jojo enthusiastically.  "That's good luck".  The kid dropped the marble back 
into the box.   Jojo lifted the box straight over his head and did an imitation of Chubby 
Checker imitating a belly dancer, thus mixing its contents. "That was trial one", he informed 
the pigeon.  "You got nine more to go."
The second trial was identical.  "Jeez!" commented the clown.  "You're defyin' the laws of 
chance!"
Eight more times, the act was repeated.  Eight more times, a white marble was pulled from 
the box. 
"Well done, professor", congratulated Jojo.  "Now what do you say? What colors of marbles 
are in the box?"
The mark smiled disdainfully, clearly bent on showing off.  "I'll tell you.  But first, let me tell 
you how I calculated the answer."
"Be my guest", said Jojo, his star blue eyes twinkling merrily. 
"There are ten marbles in the box.  That means ten possible colors, one for each marble. 
 But that's only an upper limit, just like the number of faces on your dodecahedral die was 
an upper limit on the number of distinct symbols on the faces.   There isn't any specific 
dependency of one on the other."
"What?" said the clown, feigning puzzlement. 
"When you set an upper and lower limit on a random variable, you're only defining the range 
of the variable.  If you tell me the number of colors is between one and ten inclusive, you're 
not giving me one iota of information about wherethe number falls within that range.  The 
variable is still random."
"No kidding", wondered the clown. 
"If I were to use that range in my calculation of the specific color distribution of marbles in 
the box, I'd be trying to get a kind of information out of it that it doesn't have.  I'd be trying to 
convert general information directly into specific information.  Any conclusion I might draw in 
the process would thus be invalid -- the same way my former conclusion about the die was 
invalid."
"Man oh man!" marveled Jojo.  "What an IQ!"
Smugly, the genius continued.   "It follows that my only source of information about the 
specific number of colors, and the exact distribution of marbles among those colors, is 
observation.  That's pure empirical science!"
Jojo rolled his eyes and whistled softly in admiration. 
"The limit is important, but only when you try to violate it.  For instance, if I try to calculate 

the probability that ten solidly-colored marbles have eleven different colors, I get zero.  If I'm 
using Bayes' rule, the limit is automatically reflected in the zero probability of any hypothesis 
surpassing it.   That's how the limit figures in Bayesian inference...not as a quantifier of 
possibilities."
Jojo stared raptly, pretending to be hypnotized.  If the kid had kept on babbling about ten 
possible colors after picking ten whities in a row, then he, Jojo, ordinarily as compassionate 
as a piece of earthmoving equipment, might have had to break down and steer him to a 
shrink.  Before he could hurt himself. 
"I observed just one color, white, in ten trials.   Say I use Bayes' rule to calculate the 
probability of an all-white color distribution.  If I start out with the idea that the only possible 
color is white, I naturally get one hundred percent."
"Cowabunga!" exclaimed Jojo.   "That's not a probability.   That's a  certainty!"   "Precisely", 
said  the  high-IQ  wizard  knowingly.   "And  certainty  is  exactly  what  sampling  with 
replacement can't give you.  So it's wrong to confine the range of possible colors to white 
alone.  But Bayes' rule is still the right rule to use in a calculation of this kind.  So I keep the 
rule, but throw out the idea of using only one possible color."
Jojo nodded, hanging worshipfully on the mark's every word. 
"Any color besides white is clearly nonwhite.  Yet, I have no evidence -- no observation -- 
that would justify a subdivision of 'nonwhite' into specific nonwhite colors like red and green. 
 So by simple elimination, 'nonwhite' must be the ticket.  I now have two possible colors, 
white and nonwhite.   And the beauty of it is, the term 'nonwhite' consistently represents 
every possible color other than white!  Nothing is excluded.  And nothing is represented at a 
level of specificity unwarranted by observation." 
"Wow!", sighed Jojo.  The mark thought the clown was interested in what he had to say.  In 
fact,  Jojo  could  hear  nothing  over  the  silent  strains  of  "Under  the  Big  Top"  rising 
triumphantly in his elephantine nerf ears.  The ears were a little precaution he sometimes 
took when he thought it might turn cold; they doubled nicely as earmuffs. 
"Bayes'  rule  is  designed  to  update  probabilities.   It  combines  initial  information  with 
subsequent observation, and needs both to work.  I just made ten observations.  But I still 
need a set of initial probabilities to combine them with.  Since I'm looking for a distribution of 
ten marbles among two colors, I need an initial probability for each of the ten possible 
distributions I might find.  I knew nothing before I started sampling; I had no information on 
the  distribution.   But  it  takes  information  to  distinguish  anything  from  anything  else, 
probabilities  included.   Without  information,  there's  no  basis  for  calling  any  possible 
distribution 'more probable' than any other.  So I have to set all the initial probabilities equal. 
 From this and observation, it follows that the probability of an all-white distribution is 
approximately .67.  That makes it the most probable distribution.  So I estimate that all ten 
marbles in the box are white!" He waited expectantly. 
Jojo's mouth sadly parodied the gaping grin painted around it.  He lowered the box.  The 
mark  looked  inside.   As  Jojo  tilted  the  box  gently  in  the  mark's  direction,  a  row  of 
immaculate white marbles collected along the bottom edge. 

The kid looked up, confused.  "I count only nine marbles.  Where's the tenth?"
Jojo tapped the side of the box.  A solitary black marble rolled down to join the white ones. 
 The mark dropped his jaw, totally deflated. 
"You know, maybe it just isn't your day", consoled the clown, replacing the box in his 
garment and pocketing the twenty.  "I'm not even gonna ask you if you want to try again." 
 He could already taste the bagels, prosciutto, and ice cream he was going to be eating as 
soon as he could lose the sucker.  "But man, are you ever brilliant!  I'd ask you for your 
autograph, but I know you're busy."   Excess salivation was making it hard for him to 
enunciate.  Otherwise, he might have explained that the city had a couple too many bridges, 
and had appointed him exclusive cut-rate broker for one down Brooklyn way. 
"C'est la vie", sighed the genius, bending down for his briefcase.  "Wait.  Better make that 
solvitur ambulando."   Crestfallen but smiling ruefully, he plodded tiredly away to his just 
oblivion. 
As Jojo stuffed his face in the sit-down deli half a block away, he reflected on the score. 
 First, the mark had been a real dope to assume that there was some mathematical law 
relating the number of faces of a "fair die" to what's on them. The contents of faces are 
totally independent of the number of faces except at the very limit -- that is, on the verge of 
logical inconsistency -- and "total independence from" means "no information on".  When 
two variables are unrelated by definition, any such relation holding in a given case can only 
be confirmed by observation.  Dependency relations are subject to the same criterion as 
everything else in probability theory: no confirmee, no usee.
Then the kid had wised up with the marbles, which was almost the same problem with a 
ten-sided "die" and colors instead of numbers.  But he hadn't wised up enough.  Almost all 
clowns know some magic, and Jojo had been a close-up magician for years.  The sleight of 
hand by which he'd inserted the black marble had been no big deal.  Just goes to show you, 
he ruminated: there's a sucker born every minute, a fool is soon separated from his cash, 
and what's more, nobody ever lost any money underestimating the intelligence of the 
American public.  Man, what he could have done to that kid with a deck of cards!
A DAY IN THE LIFE OF JOJO EINSTEIN, STREET CLOWN
continued
By C. M. Langan
Refilling his maw, he thanked providence for small favors.  Why,  the kid might even have 
subjected him to some kind of Ph.D. thesis on the mysteries of Bayesian paradox!  Such 
paradoxes  apply  mainly  to  sets  of  differently-relativized  probabilities,  not  to  localized 
calculations like those involving the die and the box of marbles.  A given subject either has 
information or not; if so, he either has evidence of its relevance or not.  If not, it's useless. 
 Beyond  observation, he has only CTMU probability theory and telekinesis.  At least the kid 
had seemed to understand that probabilities are by definition relative: none of them remain 
valid in the event of a loss or extension of relevant data.  The only invariants are certainties, 

and even these finally obey the same logic as probabilities.  You can take the objectivity out 
of the subject, but you can't take the subjectivity out of objective reality.
Hey, maybe the kid's astronomical IQ had meant a little something after all.  At least the boy 
wonder had known better than to fabricate the initial probability distribution.  That, mused 
Jojo as his tongue batted a lump of Haagen-Dazs around his mouth, would have been like 
speculating on how many angels could do the lambada on the head of a pin.  Where every 
initial probability distribution has an equal claim to validity, there is no choice but to settle for 
their average.  Probability distributions are represented by curves plotted against the x and 
y axes, each curve bounding an equal area of total probability.  The average of the curves is 
a  flat line,  and  the area  beneath it is  evenly  distributed along the horizontal  axis  of 
independent possibilities, discrete or continuous.  The curve isn't bell-shaped, ball-shaped, 
or banana-shaped, and Jojo was grateful to have been spared the full treatment. 
Then  again,  even  though  the  term  "average"  reflects  the  symmetry  obtaining  among 
exclusive possibilities in the total absence of information, it sometimes needs qualification. 
 Say you have a Markovian system of co-dependent events. You may want to seek its 
"attractors" by interpolation and/or extrapolation (which can also be viewed as a kind of 
"averaging").  There might be a number of different ways to do this; which one is best can 
be known only by virtue of -- you guessed it -- informative data pertaining to the particular 
system you're observing.  Assuming that this system is deterministic, the attractor guiding it 
towards its successive equilibria might be "strange", chaotic, and fractally regressive. That 
can  complicate  things.   But  in  any  case,  the  amount  and  nature  of  your  information 
determines not whether you should symmetrize, but merely how.  No exceptions allowed; 
by definition, every probabilistic algorithm involves some kind of symmetry, even at the 
deterministic limit where  the "probability"  goes to unity!   The kid had also seemed to 
understand the importance of information, and the necessity to symmetrize when you run 
out of it.  That was impressive, if only because it isn't clear from the perspective of standard 
information theory.  Info is more than bits of transmitted signal; it's a dependency relation 
recognized by an acceptor, and its "transmission" is just lower-order recognition on the part 
of any acceptor defined to include the source, channel and receiver.   Information thus 
describes every possible kind of knowledge.  Since objective reality can be known only as 
knowledge, it must be treated as information. 
Over  the  centuries,  different  thinkers  have  come  up  with  some  pretty  funny  ideas 
concerning  what  reality  "really  is",  and  have  been  ridiculed  by  their  children  and 
grandchildren for what they thought.  But defining information on cognition, and cognition as 
an informative function of information, ensures that whatever else reality turns out to be -- 
ether, phlogiston, or tapioca pudding -- it can always be consistently treated as information 
for cognitive purposes.  Since cognition is what science is all about, what's good enough for 
one is good enough for the other.  So if the equation of reality and information is a "joke", 
reasoned Jojo, our chuckle-happy descendants will be the butts of it no less than we.  And 
anyway, they'll probably be too busy crying about the barren, overpopulated, tapped-out ball 
of toxic sludge they've inherited from us "comedians" to laugh about much of anything. 
The cognitive universality of information implies cognitive symmetry or stasis in its absence. 
 The counterpart of symmetry, asymmetry ("not of like measure"), obtains among variables 
which  differ; to be used in any computation, this difference must be expressed over the 
qualitative range of a set of distinct differentiative predicates, or the quantitative range of at 

least one differentiative predicate.   If you can specify no such differential scale, then no 
such  scale  exists  for  the  purpose  of  your  computation.  And  with  no  such  scale  to 
aparametrize the function assigning weights to the various possibilities, the function can't 
assign different weights.  That leaves you with non-asymmetry or symmetry.  As information 
arrives, this symmetry can be "broken".  But information alone can break it, and you're stuck 
with it until then. 
Sameness can sometimes be deduced, in which case it has informational value.  But as an 
effect of symmetrization, sameness means that no differentiative information is available. 
 Whenever there's no info on hypothetical arguments, the corresponding variables must 
remain open for computative purposes.  Symmetrizing the object variables themselves -- for 
instance, by assigning the same content to each of them -- entails the risk of creating false 
information; these identical values might be wrong.  But symmetrizing on a higher level of 
induction -- e.g., by equalizing the distributions of possible values of algorithmic parameters 
--  leaves  the  variables  open.   They  remain  objectively  neutral,  unbiased,  and  free  of 
potentially misleading false information.
This unbiased neutrality is a rock-hard inductive criterion.  If it orders crepes suzette, it had 
better not get lumpy oatmeal.  And what it craves is the kind of higher-order symmetrization 
just described.   The distinction between higher-order and lower-order symmetrization is 
subtle and easy to miss, which explains why many probability theorists don't understand 
how or why they should symmetrize.   But then again, your average ankle-biter doesn't 
understand why he shouldn't rape the cookie jar before every meal, either.   And that, 
concluded the clown, is pretty much the reason why some folks find the "principle of 
equivalence (or insufficient reason)"  hard to swallow,  and why  they become mired in 
paradoxes that can't be resolved without it.
Yeah, Jojo knew about Bayesian regression, a paradox generated by a pair of inconsistent 
assumptions.  One is that each one-place predicate occurring in an n-fold observation of r 
objects is equally likely; the other is that each n- or r-place predicate involving these one-
place predicates is equally likely.  In the first case, you're likely to observe each possible 
color the same number of times.  In the second, you're as likely to observe any subset of 
colors more or less often than other subsets.  The question is, which assumption should you 
make in the absence of definite information regarding either one?   Jojo, who had been 
kicked out of MIT for pie-facing the chancellor[1], didn't need a degree to unkink that one. All 
he had to know was higher-order predicate logic, courtesy of that sidesplitting vaudeville-era 
comedy team, Russell and Whitehead.   Their act,  Principia Mathematica, had played in 
London, Paris, and Reno, and Jojo had it on the original 78. 
See, the gist of it was, predicate logic is stratified.  Each successive order includes all those 
below it, but none of those above it.  What that means to probability theorists is as clear as 
benzene-laced mineral water:   it's impossible to express dependency relationships which 
violate this rule, known to aficionados as the theory of types.  It was designed to avoid the 
negative circularities called  paradoxes or  antinomies.   It manages to accomplish this by 
"breaking the loop" of language by which such paradoxes are formed.  But here's the sweet 
part: this same mechanism also prevents  artificial tautologies, circular inferences which 
justify themselves like so many philosophers at a teach-off.  Jojo chuckled inwardly: cogito 
ergo sum quod cogito quod sum quod cogito quod... I think, therefore I am what I think what 
I am what I think what...  Somewhere along the line, you've got to admit a little "objective 

reality" into the deal!   Or, like a snake who lost his coke bottles, you swallow your own 
metaphysical tail and roll away into the never-never land of fantasy and solipsism, a scaly 
hula-hoop of empty assumptions powered by a perpetual motion machine. 
On the other hand, it's just as stupid to think you're the Solomon who can meat cleaver a 
Haitian divorce for the subjective and objective parts of reality.  It takes two to tango, and 
this pair dances in a crazy-glue lockstep.   Neither one can even be  defined without the 
other.  Kant knew it, Russell knew it, and even that hilarious Austrian Godel knew it.  But 
quite a few other people didn't seem to know it.  Take, for example, the vast majority of 
practicing logicians, mathematicians, and empirical scientists.  Sure, a lot of them paid it the 
"correct" amount of lip service... apparently to apologize for not being able to make any 
sense out of it.   But it's real clear and real unequivocal, and nobody has anybody but 
himself to blame if he talks the talk before he can walk the walk.   The whiz kid, who'd 
started out like some kind of solipsist, had probably been as thoroughly sucked in by 
modern scientific pseudo-objectivism as all those Joe Average "dummies" he habitually 
surveyed from on high!
Even type theory itself was in some ways a bust.  It was meant to rid logic and mathematics 
of paradox, but did so only by shifting paradox into the higher realm of undecidability theory. 
 When Godel delivered that little punch line, Russell felt like the joke was on him.  That, 
lamented the clown as he smeared mustard on his next kosher pork delight, was a shame. 
 Had Jojo been around, he'd have reminded his good pal Bert of a cardinal rule of comedy: 
you have to learn to tune out the hecklers.  Hecklers frequently knock comedians for telling 
jokes over their heads, and science is full of critics ready to pounce on any theory they're 
too dumb, lazy, or preoccupied to understand.  Anybody who comes up with anything really 
brilliant in this world learns that from Jump Street. 
Type Theory can't eliminate paradox, because paradox is a condition of human mentality. 
 Every problem on every IQ test ever invented can be formulated as a paradox, and every 
solution is the resolution of a paradox.  Man is a problem-creating, problem-solving animal. 
 Eliminate paradox, and you eliminate human mentality.  It was amazing, the lengths that 
some of these goofs would go to in order to avoid a little common sense!  Russell could 
have yanked victory from the jaws of defeat merely by reformulating the purpose of his 
theory.   Instead of claiming that it  eliminated all logical paradox, he could simply have 
claimed that it placed certain logical restrictions on paradox-resolution...and therefore on 
solving  the  logico-mathematical  and  scientific  problems  that  can  be  expressed  in 
paradoxical terms.  And guess what, whiz-kids of the world, thought Jojo as he horked down 
a gigantic bolus of garlic and gluten: that just so happens to cover problems like my little 
box of marbles.   Patting his side, he was answered by the reassuring rattle of ten glass 
globules.  He silently blessed each and every one of their stony, money-making little hearts. 
Because paradox is a condition of subjective mentality, being the self-referential basis of 
temporal consciousness, it also applies to whatever is observed by the subject -- in other 
words, to so-called "objective reality".  And to ice the cupcake, it also applies to the subject's 
conception of his relationship to objective reality!  That was Godel's insight.  Unfortunately, 
old Kurt, in his zeal to yank the banana peel out from under old Bert, went light on the 
scientific ramifications of his undecidability gag.   This left the audience with a bunch of 
cracked and leaking coconuts where their wise and learned brains had been, and science 
was able to skate right along like nothing had happened.   But see, something  had gone 

down after all, and Jojo couldn't decide whether to laugh or die about it.  But so what? he 
thought as he wiped his de-gloved paws on the window curtains next to his table.  At least 
he knew he'd always be able to make a buck off it. 
But hey, money wasn't everything, even in this city.  There were other important things to 
remember.   Like the purposive connection between the theories of Types and Relativity. 
 See, Jojo wasn't the only famous Einstein who shared Russell's concern with logic.  Herr 
Albert had something very similar in mind when he adopted the principle of locality in order 
to avoid causal paradoxes in which super-luminal agencies make observations and then 
change the way they happen.  The basic idea was the same: effects cannot negate their 
causes.  Causes can negate certain effects by way of promoting others, but not vice versa. 
 And more to the point at hand, the output of a function or algorithm cannot post hoc tend to 
negate the function or its parameters...e.g., the initial distribution on which observations 
depend.  Where the output has already been observed, this forces the symmetrization of all 
the "causes" (e.g., chromatic likelihoods) from which it may have resulted.   Deduction 
eliminates  possibilities; but  whenever  an inductive  context  can  be arbitrarily extended 
relative  to  a  given  hypothesis,  the  inductive  process  must  respect  all possibilities  by 
obedience to the theory of types.  
In other words, what type theory said about Bayesian regression was this.  Bayes' rule is an 
algorithm, or complex logical function ascribing one of a set of predicates (probabilities) to a 
variable causal argument.  It was designed not only to derive a conditional probability, but to 
do so without contaminating relevant data with irrelevant assumptions.  The latter amounts 
to  the  avoidance  or  prior  resolution  of  any  paradoxical  inconsistency  between  such 
assumptions and the reality which is being measured: if you assume something that favors 
any particular outcome representing a fraction 1/n of possible outcomes, you will only be 
right 1/n of the time.  That amounts to a paradox (n-1)/n of the time, a situation that has to 
be avoided wherever possible.  
Type theory avoids it by  directionalizing the inclusory or attributive dependencies in the 
formulation.   A variable standing for a color is of higher type than one representing an 
object.   A  variable  standing  for  a  chromatic  distribution  is  of  higher  type  than  one 
representing a color.   And a variable standing for a probability distribution (of chromatic 
distributions of colors over objects) is of higher type than one representing a chromatic 
distribution.  Each one of these variables gets to determine those beneath it, but not with or 
above it.  So you can't take an assumption about a chromatic distribution -- say, that each 
color is or is not equally likely to be observed in sampling -- and try to determine the 
probability  distribution  from  it.   Since  Bayes'  rule  requires  initial  information  on  the 
probability distribution, you have to forget about the likelihoods of specific colors and start 
from the probability distribution itself.  That means that where you have no prior information 
on  the  probability  distribution,  you  have  to  symmetrize or  flatten  it.   Each  chromatic 
distribution is thus  equally likely,  and the corresponding range of individual  chromatic 
likelihoods is open and unbiased.  See, Ma?  No loops. 
Jojo recalled a related question involving the states of systems in probabilistic contexts. 
 Prior to extended observation, how do you know how to partition the set of possibilities you 
intend to symmetrize?  Take an urn containing two marbles.  What's the probability that the 
marbles have different colors?  Are the possible states enumerated combinatorially: xx, xy, 
yy?   Or permutatively: xx, xy, yx, yy?   Jojo chuckled, almost choking on a pickled egg. 

 Probabilities, states, and difference relations compute solely as  information.   In CTMU 
probability theory,  information  about states is subject to the same constraints  as  that 
expressed in terms of states.  There are 3 states in a combinatorial urn.  There are 4 states 
in a permutative urn.  Since 4 > 3, the permutative state-set has more information.  But just 
how has this extra permutative info been acquired?   The down and dirty truth of it is, it 
hasn't  been "acquired".  It's been fabricated.
Most probabilistic algorithms need at least combinatorial info to work.  If you're going to go 
without  observation  and  fabricate  information  about  states,  it  has  to  be  at  least 
combinatorial.   But you don't dare help yourself to  extraunconfirmed info!   So the urn's 
states are initially combinatorial, and p = 1/3.  In other words, states defined on individual 
objects  and  ordered  relations  among  objects  don't  get  to  determine  higher-order 
combinatorial predicates of whole sets.  Only observation is "high enough" to do that.  And 
that, concluded Jojo as he shoveled ice cream in a last-ditch attempt to quench the egg's 
fiery -- no, radioactive -- aftertaste, was that.  Glory Be to the CTMU unification of probability 
theory and higher-order predicate logic!  Trying to get over on the CTMU is sort of like trying 
to outsmart yourself: you lose if you win, and win if you lose.  The clown wheezed pitifully. 
 Where the heck could a mere egg-pickler be getting plutonium-239?
The necessity for combinatorial information in purely subjective computations is a demand 
of our cognitive syntax...the way we see the world and organize it mentally.  Such demands 
determine all the mathematical models we construct to represent and simulate reality. 
 Because they inhere in every such model, they are a basic ingredient in all calculations 
based on such models, "objective probabilities" included.  The fact is, since all probabilities 
have subjective components, there are no grounds for refusing to recognize subjective 
probabilities as "real".  Is the above probability highly subjective?  Yeah.  Is it only relatively 
valid?  You know it.  But while some bananas are strictly by Goodyear, a chimp has to eat. 
 And the only way to chew this particular bunch is to recognize that all probabilities regress, 
on data reduction, to statements about the semantical relationship of subjective cognitive 
syntaxes to that of "objective reality".  Such statements, if extremely general, nonetheless 
express a real relationship between two realities, you and your environment.  The only way 
they aren't real is if you aren't real.  And if you're reading this, you're as real as Cheetah, 
Bonzo and all their offspring combined. 
The Bayes algorithm is formulated in a certain metalanguage, and its variables occupy 
specific  levels  within  it.   Any  universe  to  which  the  rule  is  applicable  must  conform 
semantically to its internal stratification.   By virtue of type, the dependency relationships 
among Bayesian variables are asymmetric  and directional.   This also applies to more 
specialized  rules  like  the  Schrodinger  equation,  which  are  merely  empirical-slash-
subjective[2] evolutions of Bayes' rule under given kinds of empirical input.  In this sense, 
Bayes' rule is the universal first step in scientific inference, and associated at a very deep 
level  with  the  human  mental  syntax  itself.   That's  why  Bayesian  inference,  Bayesian 
paradox, and Bayesian regression are central issues in reality research, and why anybody 
who complains that they're bored or irritated with these topics belongs on Monkey Island 
with the rest of the big time mental heavyweights.  And that, mused Jojo, goes double for 
anybody unwilling to grasp the topic at even the general level on which he was thinking 
about it.  He was glad that he could keep his thoughts to himself instead of having to run a 
hard-sell on some bunch of self-styled "experts".

Jojo, who felt himself converging on an explosive disaster to rival even his Kreemi-Whip 
nightmare, reflected on the molasses-in-January speed with which the scientific community 
often seemed to assimilate earthshaking new abstractions. Hand them a gadget or a 
preformed mechanical principle, and they treat it like a three-alarm fire in a loaded bank 
vault.  But hand them a pure, beautiful abstraction, and you might as well indulge in a little 
suspended animation while you wait for all the "experts" to puzzle and grope their ways 
through it.  Even so, it's just a heartbeat next to the time it would take them to root it out 
themselves  from  beneath  the  tangled  circuitry  of  fact  and  supposition  that  they  call 
"knowledge".  It's a practical world, and the reformer of abstractions had better dress for a 
long,  cold  hike  to  acceptance.   Because  people  want  to  face  their  mistakes  and 
inadequacies about as much as a hundredth-time-virgin bride wants to show hubby that 
Property of Hell's Angels tattoo on her creamy derriere.  
See, everybody's an expert, and everybody has his or her own ideas about what is or is not 
"rational", "imaginable" (see Noesis 58, p. 17), "possible" (see Noesis 46), "probable", and 
so on for every subjective context.  This can apply even to hard, extensional contexts like 
truth and certainty.   But Jojo had noticed that whenever people bother to justify such 
opinions, they always fail to come up with anything remotely resembling a formal derivation. 
 That's because no categorical derivation is possible with respect to anything but the most 
general features of the human cognitive syntax.  Instead, they offer "plausible arguments" 
on which there "seems to be a consensus" among like-minded cognoscenti. Translation: 
mildly denatured crapola, with 99.9 percent certainty. 
Jojo, not content to settle for any narrow field of expertise, was an expert on experts, and 
knew that no expert could determine diddley except relative to his own axioms and rules of 
inference.   That naturally precludes any value judgment on anything involving  additional 
axioms, rules, or data.   Some things are easily recognizable as nonsense because they 
oppose known facts at identical levels of specificity: if somebody were to claim that the War 
of 1812 was fought in 1922, he'd plainly be a bag of gas.  But as the situation gets more 
complex, it rapidly gets very difficult to make this kind of determination.   If a critic isn't 
extremely careful, he risks becoming just another old curmudgeon with antiquated notions 
about what rules the universe is "required to obey".  
Take  Newcomb's  paradox,  a  generic anomaly designed  to  cut  off  the  ring  on  such 
lamebrain arguments.   Newcomb's paradox is just an arbitrary version of a situation that 
has arisen, and will no doubt continue to arise, countless times in the always-surprising 
annals of science: data is received which appears to contradict a currently accepted set of 
axioms and rules of inference.  That much  is obvious, since otherwise there would be no 
point to it.   But the clever way in which the formulation evokes the issues of free will, 
rationality, and megabucks seems to have short-circuited the faculties of many of those who 
tried to solve it.   They treated it as nothing but a real or phony get-rich-quick scheme 
instead of as a paradigm for empirical and theoretical induction!  Even when this big bad 
paradox was finally resolved by Jojo's personal amigo and sometimes-biographer Chris 
Langan, everybody played ostrich, 'possum, and downright deaf and dumb rather than 
admit they might have "overlooked" something.  Given the fact that some of them claimed 
ultrahigh  IQ's  and  had  a  lot  to  lose  by  imitating  the  furniture,  Jojo  found  this...well, 
unimaginable,  improbable, and  incredible. Buttrue.   Langan had belonged to a society of 
whiz-kids  when  he  resolved  Newcomb's  paradox  and  certain  other  notorious 
inconsistencies.  The poor guy had been led to believe that he would thus be assured of a 

fair and insightful hearing for his ideas.  But he'd been in for a minor disappointment.  Jojo 
recalled a few notable examples: one member had implied that Langan's ideas were less 
than original because one partial aspect of them had been "proposed" by somebody else 
who  apparently  had  a  limited  grasp  of  the  logico-mathematical  complexities  they 
entailed...many of which nobody but Langan seems to have even considered.  Another had 
adopted the computative terminology of Langan's Resolution to undermine the work itself, 
an  exercise  in  one-man  foot-shooting.   And  yet  another  had  denigrated  Langan's 
contributions because "others" had expressed disapproval, telling him that his request for 
an apology amounted to "killing the messenger"!  Instead of presenting their arguments in 
ways that invited a response, these critics all seemed to dismiss out of hand the idea that 
any suitable response would be possible.  The fact that none of them offered so much as 
one unqualified sentence in support of his concise and incisive analyses had sort of bugged 
Langan.  But it really shouldn't have.  Because if he'd known whiz-kids like Jojo knew whiz-
kids, he'd have expected nothing but the kind of meaningless and circular bickering he got. 
 And he'd have made a laff riot out of it, just like Jojo with his whiz-kid. 
Then again, Langan's critics didn't have to look so  bad.   Some of their remarks were 
relatively insightful.  The problem was, they offered their criticisms after Langan had already 
answered them!  For example, it was remarked -- after Langan had made it clear that the 
standard type-theoretic resolution of the Epimenides paradox could be transformed into a 
resolution based on CTMU many-valued logic upon real time violation -- that this paradox 
"would really lead to a (CTMU-invalidating) contradiction" if one were forced to assign one 
of only two truth values to every statement.  One of the main ideas of the CTMU, if Jojo 
wasn't badly mistaken, was to define a paradox-resolvent stratification of truth functions 
generating a potential infinity of truth values (top paragraph, page 9,  Noesis 44)!   Good 
thing Langan had a sense of humor. 
Part of the beauty of the CTMU was the way many-valued (modal) logic was so naturally 
applied to reality in the stratified computational model.  The two-valuedness of the human 
accepting syntax delineates only that part of global reality constrained by human conceptual 
and observational limitations.  For each level [gamma]n of [gamma], there is an nth truth 
value.   Truth values are related according to the  inductive and  deductive relationships 
holding within and among the levels of [gamma].   Langan had made this as clear as 
possible, given limitations in space and reader attention.  
Someone else had made the observation that using the CTMU to resolve Newcomb's 
paradox was "like using a B-2 for crop dusting"!  Newcomb's paradox, a conundrum relating 
physics, decision theory, and the philosophy of free will within a metaphysical matrix, had 
defied resolution for over twenty years by the time Langan wrapped it up.   The reason? 
Physics, decision theory, and the philosophy of free will is to crop dusting what botanical 
genetic engineering is tospreading fertilizer.  Along with the other matters to which it was 
closely related -- e.g., quantum reality -- Newcomb's paradox had all but smothered to 
death under a two-decade accumulation of "fertilizer" before Chris Langan rescued it with 
his CTMU.  It was even denied that quantum non-locality confirmed the CTMU, despite the 
equation of non-locality and metrical violation.  Langan had already explained that metrics, 
being parts of the computative syntaxes of dynamical transducers, could be computationally 
relativized to nested transducers in such a way as to resolve these violations while affording 
an elegant explanation of quantum wave function collapse.   Man oh man, the trials that 
Chris Langan had been made to endure!  Genius could be a long row to hoe for someone 

with no fat cat academic "mentors" willing to sponsor his radically superior ideas for a 
prestigious and potentially lucrative slice of the credit (funny, thought Jojo, how the sacred 
responsibility of such mentors to "protect the integrity of their disciplines" so often seemed 
to line up with the protection of their personal funding and reputations).  The clown suffered 
silently on behalf of his sadly misunderstood buddy, who had foolishly dared to weigh in on 
the scales of truth with no more than the certainty of being right.   Talk about your Don 
Quixotes!
Yet Jojo, with benefit of hindsight, knew that Langan's opponents had all been squashed 
from day one.  Langan's grasp of logic and reality was so advanced that it could be risky 
even to get an argumentative look on your face where he could see it, and his critics had 
argued without knowing the real extent of his ability.  For example, their journal Noesis had 
contained several citations of the notorious four-color problem of graph theory.  In one of 
these,  it  was  mentioned  that  a  pal  of  the  editor  had  been  working  to  shorten  the 
computerized proof of the four-color theorem.  First, proofs are informational and must be 
relativized to the cognitive automata by which the information can be processed.  The proof 
in question involved millions of logical operations directly unverifiable by any human, and 
thus proved zilch relative to human cognitive capabilities.  If the four-color conjecture had 
become a "theorem", it existed only for human-digitalcyborgs with split personalities whose 
two faces had no guarantee that they could trust each other.  Second, what Chris Langan 
knew about this kind of problem -- and about problems in general -- would ultimately send 
most four-color theorists and their machine programs straight into the academic fossil pit, 
there to R.I.P. on their tarred laurels.  
Jojo wondered how all those megamelons, who were aware of many of the paradoxes and 
inconsistencies riddling science and metaphysics, could by sound or silence reject the most 
general and necessary model in which "undecidable" mathematical and logico-philosophical 
conjectures become relativizable and solvable.  The situation had been truly bizarre.  But 
what goes around has a way of coming around, and the Noetic Society later met with a kind 
of poetic justice. Chris Langan, after being criticized half to death by his fellow members, 
had told them that they risked wearing "a pair of huge floppy shoes, a red rubber nose, and 
a baggy jumpsuit with outsized polka dots and a big round crinkly collar" for failing to pay 
adequate attention to his explanations.   Jojo stared at his riotous reflection in the deli 
window and laughed aloud.  All right, so none of you had a clown suit to wear, he thought. 
 That's okay.  At least somebody can dress the part.  And tell when he's playing it.
See, a funny kind of game had been played right in front of all their noses, but in such a way 
that most of them missed it.  Langan had been trying to introduce a new theory, the CTMU. 
 The  trick  he  had  to  perform:  cram  enough  detail into  an  intelligible but  maximally 
abbreviated account of his ideas.   He thought he was doing a pretty good job before a 
couple of members screamed like wounded peacocks, accusing him of "long-windedness" 
and other complimentary attributes.  All right, so they didn't care for his way with words. 
 Facing reality, he bowed out of the editorship like any duly-chastened "windbag" should 
have.   But then, from the soggy ash heap of rejection, a  whole new perception of the 
situation seems to have arisen.  In effect, the CTMU was criticized for being incomplete and 
poorly reasoned.  Get a load of that!  Langan was cut off at the mike, and then harangued 
for not finishing his speech (if not for being a loony, slipshod crackpot to boot). 
Now, if there was some sub-rosa agreement that this was the best way to proceed, Langan 

was not a signatory.  There was no undertone of cooperation.  These people were simply 
being  unfriendly to  him,  and  it  stank.   It  was  bad  enough  that  he  was  cut  off  from 
conventional avenues; that he had been forced to develop his ideas utterly without support, 
acknowledgment, or encouragement.  But to give him (or watch him being given) the bum's 
rush was something like contemptible, particularly after he had donated valid resolutions of 
major paradoxes that nobody else could handle.  If that's the way science is supposed to 
work, then Murphy's Law qualifies as a crucial axiom of the scientific method. While this 
would have come as no surprise to a long line of intellectual desperados -- guys like 
Copernicus, Galileo, and Galois -- it falls well short of the promotional hype advantageously 
used by the shiny, impeccably tailored "front men" of academia. 
According to them, the academic and industrial intelligentsia form a kind of "ecosystem" in 
which the "Darwinian competition" of ideas results in "survival of the fittest".   The only 
problem is,  these are fallible humans, not natural forces. The front men even manage to 
create an impression that where conventional processes fail to reveal "the fittest" ideas 
because these ideas are too far out and iconoclastic, there exist "fail safes" sure to prevent 
them from being lost.   Fail safes like the MacArthur Foundation, which at one time was 
making a lot of noise about searching for "the new Einstein" in hopes of supporting his 
research.  Unfortunately, in true catch-22 style, such organizations wouldn't know "the new 
Einstein" if he walked up and kicked them in the whoopie cushion.  Instead, they rely on the 
"expert opinions of established authorities" in various fields.  The result? While Langan was 
crunching paradoxes, they were fixating onclowns.  No, we're not just talking silly people; 
the MacArthur Prize was once actually awarded to a clown who looked just like Jojo.[3]  Jojo 
had wanted to kick  himself for missing out on that little opportunity, which would have 
catapulted him and his aging mother straight into a cushy condo in Westchester.  But at 
least he was in good company: if the MacArthur Foundation had existed in the time of 
Evariste Galois, the founder of the modern theory of algebraic equations, it would have 
"encouraged his genius" post mortem by at least half a century! 
Langan recognized all of this, and coped with the situation by doing whatever he had to do 
to support his  own research.   He never kissed up to anybody, and he never made the 
slightest compromise regarding what he understood to be logico-mathematical certainties. 
 Knowing that nobody would stick up for the truth if he wouldn't, he tightened up his belt and 
ate crow until the Noetic Society provided him with a convenient way to introduce it.  No 
doubt about it, printing his theory in Noesis didn't offer much in the way of amenities.  But it 
did give him something he might not have been able to get any other way.   That is, no 
matter who squawked about his presentation, who objected to his personality, or who might 
have adopted his insight without a word of thanks or credit, he had established his exclusive 
authorship  of  what  would  ultimately  prove  to  be  the  culmination  of  post-Aristotelian 
intellectual evolution.  It wasn't much, but at least it was something.  Langan appreciated the 
help, even if it had been like pulling teeth.  And he was  still willing to cooperate with his 
fellow eggheads.
Being a realist's realist, Jojo knew that the Noetic Society was no group of dunces.  But it 
was also no wellsprings of sympathy and understanding where Langan was concerned, and 
Jojo -- when not hard at work on some gobbler -- was a lover of fair play.  Chris Langan 
was walking the walk; while the experts exercised their jaws, he dogged truth and meaning 
with minimal concern for his own personal  welfare.  With so many other birdlings in the nest 
of science peeping their little beaks off for food and attention, Langan had been nobody's 

darling.  He was accustomed to paying the price for his "lone-wolf" attitude.  But what really 
got Jojo's goat was the way the Noetic Society, which in effect billed itself as a mutual 
appreciation society for lonely geniuses at the pinnacle of human intelligence, had repaid 
Langan's  insight  with  every  particle  of  personal,  political,  and  conceptually  irrelevant 
chicken manure it could generate.  Like he was some guano-starved bean patch.
If any Noetic Society members had appreciated what Langan gave them, they were so 
intimidated by the chicken-manure  hurricane he encountered that they scurried for cover 
like so many field mice, thus punctuating the hurricane with long spells of arctic weather. 
 After all, better to be a one-in-a-million hero than stand with a one-in-five-billion pariah who 
seemed to have finished last in the Noetic Society Mr. Popularity competition.  The amazing 
thing about it was, the way Chris Langan continued to voice respect for the intellects of his 
fellow members as they did their level best to frustrate him spitless while sabotaging the 
intellectual future of the human race.  How, long after others would have written them off, he 
refused to abandon them in the desert of ignorance.  Why, if the clown hadn't overeaten, he 
would have rushed straight back to the flophouse and composed volumes of soaring poetry 
to celebrate the perseverance of guys like Christopher Michael Chappelle-Langan!
Then the flashbulb of inspiration detonated like a loaded cigar in Jojo's teeming gourd, 
infusing him with a new and wonderful sense of destiny.  If that kid he'd hustled was really 
one of the smartest people in the world, then Jojo was in the wrong line of work.  Life as a 
clown  had been good.   But his new life would  be even better.   Armed with  years  of 
experience as a professional buffoon, Jojo was about to become...a politician!  He gazed 
serenely out the window.  It was autumn in Manhattan, and the Apple was a fat juicy pearl in 
the humongous gaping oyster that was the world.
Another Crank Comes to Visit: The Cognitive-Theoretic Model of 
the Universe
Comment #1: 
Good grief. And here I was hoping we could leave it on a semi-pleasant note.
Unfortunately, instead of retiring to write something vaguely constructive, Mark has now 
created a third rudely-titled "crank" page for me where previously there were only two, and 
without my original responses (in which I roughly explained what I was actually trying to 
convey). Instead, I’m now invited to start over from scratch in "defending my theory".
It’s an old game, and everybody knows it all too well. The target is supposed to enhance the 
reputation of the critic by pretending that the critic is legitimate while bumping up the critic’s 
hit count with his “defense”, despite the obvious lack of any willingness on the part of the 
critic to give an inch under any circumstances, even if somebody puts a blueberry-math pie 
in his face.
This, of course, leaves me with no rational alternative but to point out that Mark is not a 
legitimate critic. In fact, Mark is incompetent. Thus, instead of defending myself against 
Mark, the most appropriate course of action in the present instance is to invite Mark to 

defend himself.
Let me explain what I mean by “incompetent”.
There are a lot of ideas floating around out there. Some are good; others are bad; others 
aren't so attractive to the naked eye, but improve under magnification (many of the best 
ideas have come in this form, and sometimes the magnification process is not complete 
until long after publication).
A three-way partition can also be applied to Internet pundits, e.g. Mark, who entertain 
themselves and their readers by evaluating the ideas of others. Some are good at it, others 
are not so good, and others are a complete waste of time and bandwidth.
The value criteria for distinguishing among good and bad ideas are fairly cut and dried:
(1) Syntactic consistency: Is the idea well-formed and logically consistent? (Y/N)
(2) Semantic consistency: Is the idea consistently applied to its universe? (Y/N)
(3) Relevance: Is the idea relevant to its purported content or the problem to be solved? (Y/
N)
The competency criteria for distinguishing among evaluators, e.g. Mark, are equally 
obvious:
(1) Comprehension: The evaluator makes sure he fully understands the ideas he evaluates 
and refrains from attaching extraneous constructions, speculative interpretations, or 
inappropriate conceptual models (even in the face of uncertainty regarding the proper 
interpretation).
(2) Discernment: The evaluator possesses the willingness, the knowledge, and the 
intelligence to properly and thoroughly apply value criteria 1-3.
(3) Neutrality: The evaluator limits his judgments to value criteria 1-3, and withholds final 
judgment on ideas to which he is unable to apply criteria 1-3 with reasonable certainty (e.g., 
in fields outside his areas of expertise, or where he bumps up against his intellectual 
ceiling).
In scholarly discourse, evaluators are required to justify their judgments. Those who display 
inadequate comprehension, discernment, or neutrality in their judgments, having failed one 
or more competency criteria, are by definition incompetent. Among incompetent evaluators, 
the worst-of-breed are obviously those who chronically fail all three competency criteria.
With regard to my essay, Mark fails all three competency criteria. Indeed, he readily admits 
to it. This renders Mark incompetent, by his own admission, to do what he's trying to do 
here. Accordingly, he fails to qualify as a legitimate "debunker", "crank fighter" or whatever it 
is that he likes to call himself, instead constituting a mere pain in the neck and leaving me 
nothing sufficiently coherent to “defend” against.
In fact, it's a bit worse than that. This is because Mark sometimes seems to choose the 
ideas he attacks *because* he fails to comprehend them. In other words, it's not just that 
Mark randomly encounters ideas he's unfit to evaluate, and then does so anyway just to be 

a pain in the neck; it's that for Mark, personal incomprehension almost seems to be an 
irresistible evaluation-stimulus.
Of course, in keeping with his overall incompetence as an evaluator, Mark doesn't 
understand this. Instead, he pulls a cognitive switcheroo of which he is seemingly not 
consciously aware, automatically confusing his own incomprehension with 
incomprehensibility. In fact, "incomprehensibility" seems to be his main critique of my essay.
In other words, Mark has switched a judgment on his own subjective mental state 
(incomprehension) for a purportedly objective attribute of the idea he's trying to evaluate 
(incomprehensibility), thus making the distinction "good math | bad math" effectively 
equivalent to "math that Mark is capable of understanding, and therefore likes | math-like 
content that Mark is incapable of understanding, and therefore hates!”
Now, if Mark were as smart as he evidently thinks he is, he'd be less aggressive. He 
wouldn't immediately stick his neck out to upchuck all over ideas he doesn't understand. 
Instead, finding himself unable to locate obvious falsehoods in the target of his derision, 
he'd wait until he has more data on what's really going on with it. After all, that's what 
reasonable people do.
But Mark isn't always reasonable, or all that smart either, at least when he lets his 
characteristic irascibility get the better of him. In fact, as we've already established, he can 
be an incompetent little pain in the neck. In fact, he often appears to wallow in irrationality 
with what appears to be near-demonic relish.
Remember, the value and competency criteria listed above are objective in nature. This isn’t 
just an opinion; it’s a rock-solid indictment of Mark’s incompetence as an evaluator of ideas 
that he considers sufficiently “mathematical” to merit his special attention, but about which 
he actually can’t tell his ass from his elbow.
This doesn’t necessarily mean that nothing Mark says makes sense; some of what he says 
obviously does make sense. But some does not, and that’s where Mark tumbles into 
incompetency. Obviously, as the very first order of business here, Mark needs to mend his 
incompetent ways.
(I hope we’ve managed to avoid any problems with English comprehension this time 
around.)
Comment #2: 
OK, I think we’ve waited about long enough for Mark to defend himself from the charge of 
incompetence.
You know, it never looks good when the proprietor of a highly contentious web site hides 
behind his commentators. It tends to destroy the forum as an appropriate setting for serious 
intellectual discussions. So I trust that Mark has merely been busy, or better yet, that he 
recognizes the futility of trying to defend his indefensible behavior.
In any case, I’ll go ahead and pave the way to a final resolution of the situation by dispelling 
any remaining doubt that Mark is incompetent to evaluate the essay he’s been attacking 
here. Fortunately, an analysis of the first “substantive” paragraph of his critique will be 

sufficient for that purpose.
Here’s the paragraph:
“Right from the start, we can see the beginnings of how he’s going to use a supposedly set-
theoretic notion, in a very peculiar way. I don’t know anyone who seriously thinks that the 
universe is a set. Sets are a tool that we use to construct abstract models that describe 
things. The universe isn’t a set; it’s the universe. And yet a huge part of his argument is, 
ultimately, based on “disproving” the idea that the universe is a set, based on silly word-
games.”
Let’s have a look the above paragraph sentence by sentence.
Sentence 1: “Right from the start, we can see the beginnings of how he’s going to use a 
supposedly set-theoretic notion, in a very peculiar way.”
I don’t know what this means; it’s “geek” to me. It’s probably an error, but in the spirit of 
evaluative competence, I’ll withhold judgment.
Sentence 2: “I don’t know anyone who seriously thinks that the universe is a set.”
Error 1: That’s vanishingly unlikely. Materialists think that the universe is a set of material 
objects (e.g., atoms and subatomic particles in various combinations) on which all else can 
be secondarily defined and/or causally supervened. Any assertion by Mark that he doesn’t 
know at least one person subscribing to such a viewpoint is simply incredible, especially 
given the atheist-materialist circles in which he runs. (Mark describes himself as a “religious, 
theistic, reconstructionist Jew,” but that’s beside the point; merely that he attended a 
modern university is enough to tell us that he has rubbed elbows with many atheistic 
materialists.)
But materialism is almost beside the point; all we need here is the scientific method. With its 
unrelenting emphasis on observation of, and experimentation on, material objects including 
the measurement devices thereby affected, the scientific method demands that everything 
in science be related to observables and the objects to which they are attached, which, 
being individually discernable, qualify as elements of sets (with all appropriate distinctions 
applied; e.g., sets of physical objects or events are countable, while sets of points in a 
continuum are uncountable).
In search of counterexamples, one may be tempted to point to such things as time and 
process, “empty space”, various kinds of potential, forces, fields, waves, energy, causality, 
the spacetime manifold, quantum wave functions, “laws of nature”, “the mathematical 
structure of physical reality,” and so on as “non-material components of the universe”, but 
these are predicates whose physical relevance utterly depends on observation of the 
material content of the universe. To cut them loose from the elements of observational sets 
would be to deprive them of observational content and empty them of all physical meaning.
Sentence 3: “Sets are a tool that we use to construct abstract models that describe things. “
Error 2: More accurately, the concept “set” is a formal entity into which real content may be 
mapped by description or definition. To preclude content is to sever the mapping and render 
the “tool” descriptively useless.

Everything discernable (directly perceptible) within the physical universe, including the 
universe itself (as a coherent singleton), can be directly mapped into the set concept; only 
thusly are secondary concepts endowed with physical content. One ends up with sets, and 
elements of sets, to which various otherwise-empty concepts are attached. Unfortunately, in 
standard theory, this attachment is reminiscent of sessile mollusks which have glued 
themselves to foreign bodies, and this is a problem for set theory as a descriptive language 
for the universe (or as a foundational language of the mathematical formalisms applied to 
the universe by science), as it is subject to a crippling form of dualism which separates 
functions from the sets they relate. But while set concept is obviously necessary - these 
other concepts are rendered physically meaningless without it - this in no way implies its 
sufficiency on any scale of reference.
Sentence 4: “The universe isn’t a set; it’s the universe.”
Error 3: This is an instance of logical negation amounting to an absolute distinction between 
“set” and “the universe”. It asserts the nonexistence of structural overlap between “universe” 
and “set” on all levels of reference, thus precluding shared structure.
Let’s take a closer look. Mark isn’t just saying
4a. “The universe is *in part* a set, but ultimately *more than* just a set (of objects, events, 
etc.)”;
he’s saying
4b. “The universe is *not* a set, period.”
These statements are mathematically distinct. Mark’s statement, 4b, implies that the 
universe is nowhere a set, i.e., that neither it nor any of its contents can be mapped into a 
collection or aggregation of objects, elements, points, or any other discernable entities as 
content. But this is completely absurd.
Indeed, if the “set” concept is free of physical content, then this precludes the use of any 
measurement device for observation or experimentation, and in fact, reference to anything 
that is observationally discernable and quantifiable in number, as this would provide 
physical content for the “set” concept. Whoops, no more science!
Obviously, the universe IS a (structured) set, but not MERELY a set in the context of any 
established version of set theory. Its description requires a more powerful mathematical 
language incorporating the “set” concept within a more capacious formal entity (which, of 
course, was largely the point of my little essay, which was written back before “everybody 
knew” that standard set theory could not be rehabilitated as a foundational language). Hello, 
CTMU, and hello, SCSPL!
In short, the author of Sentence 4 (i.e., Mark) is either mathematically illiterate, or he’s trying 
a bit clumsily to agree with me in all essential respects, but doesn’t quite know it due to the 
depth of his own incomprehension.
Sentence 5: “And yet a huge part of his argument is, ultimately, based on ‘disproving’ the 
idea that the universe is a set, based on silly word-games.”

Error 4: This statement consists of two parts:
5a: “His argument is based on ‘disproving’ the idea that the universe is a set” (I’ll be 
charitable and assume that Mark knows what proof actually entails in mathematics);
5b: “This attempted disproof, and the argument based on it, are silly word games.”
Quibbles aside, statement 5a is close to accurate; I do, after all, maintain that the universe 
is not merely a set, but something with greater expressive capacity (properly including that 
inherent in the set concept itself). However, statement 5b amounts to an accusatory 
misconstruction of the writer’s personal incomprehension, for which there is no excuse.
And that’s just one little paragraph. Its completely erroneous character conclusively 
establishes that Mark’s critique fails value criteria 1-3 enumerated above, and that Mark 
himself fails all three adjoining competency criteria … which, somewhat to his credit, he 
freely admits.
Summary: Explaining the errors made by Mark at the very beginning of his critique requires 
more space than is occupied by Mark’s statements themselves. Mark actually generates 
errors at roughly the same rate, and arguably faster than the rate, at which he writes about 
the “errors” of others!
Even though this may not seem like serious business to some readers, it certainly is. If Mark 
does not desist in his nonsense, it may well turn out to be something he regrets for the rest 
of his life. This is not because he is merely wrong; we all live and learn. It is because Mark 
often lacks any clue regarding the wrong turns he has taken, and in order to distract himself 
from his frustration, habitually lashes out at the sources of his confusion like a vindictive 
child. Any failure of comprehension precipitates him into a fit of pique, at which point he 
disastrously (for him) attempts to damage the understanding and the reputations of others 
without just cause.
I’m sure it would be a relief for all concerned if this were the end of my participation here. So 
I hope that’s the case…all the more so because if it is not, then one way or another, things 
will only go further downhill for Mark.
Comment #3: 
Alright, then. At this point, I think it’s safe to say that Mark has no intention of trying to 
defend himself against the charge of evaluative incompetence.
In repeatedly failing to defend himself against the charge of evaluative incompetence, Mark 
has now exhausted his last chance to prove that he has the intellectual standing to criticize 
my work. (Lest this be misinterpreted, “intellectual standing” refers not to Mark’s 
intelligence, but merely to his highly deficient knowledge state, on which I think he could 
greatly improve by freeing himself from various irrational prejudices and unnecessary 
cognitive bottlenecks.)
As I’ve already observed, Mark generates errors faster than he writes. This makes it quite 
tedious to rebut him, as it is far easier for him to dash off a few paragraphs of ill-considered 
pseudomathematical gobbledygook than it is for me to explain all of his errors in detail. But 
just so as not to waste an opportunity for instruction, I’ll do it one more time anyway.

Mark: "What a load of ad-hominem ridden bullshit."
Comment: It was Mark who first resorted to personalized invective in this exchange. Anyone 
who doubts this need merely look at the titles of both of his Langan/CTMU critiques, 
including this one.
Mark: "The universe isn’t a set. A set is a mathematical construct defined axiomatically. 
That can sound like doublespeak, but it actually captures an extremely important distinction 
– one which you still don’t seem to understand."
Error 1: A set is not a “mathematical construct defined axiomatically”. That would be set 
*theory*. While set *theories* are indeed defined axiomatically, the set concept itself is 
defined in a very basic and general way, which is precisely why it supports multiple versions 
of set theory incorporating different axioms.
Everybody around here seems to like Wikipedia. Well, here’s how Wikipedia defines “set”: 
“A set is a collection of distinct objects, considered as an object in its own right. Sets are 
one of the most fundamental concepts in mathematics” (... sufficiently fundamental, in fact, 
to nucleate different axiomatic theories and play an indispensable role throughout 
mathematics).
This definition is qualified and restricted by various strains of set theory, but it remains 
essentially intact as theoretical context varies. More advanced versions of set theory 
improve on naïve set theory only by adding distinctions and restrictions; for example, NBG 
adds the concept of classes, while ZF proscribes self-inclusion. Such advanced theories do 
nothing to expand the expressive capacity of the basic "set" concept.
Error 2: Because the universe fulfills the definitive criteria of the “set” concept (and more), it 
is at least in part a (structured) set. Mark seems to believe that “set”, being a concept or 
formal entity, cannot possibly describe the universe; after all, the universe is not a mere 
concept, but something objective to which concepts are attached as descriptive “tools”. But 
to the extent that concepts truly describe their arguments, they are properties thereof. The 
entire function of the formal entities used in science and mathematics is to describe, i.e. 
serve as descriptive properties of, the universe.
Obviously, not all formal entities qualify as properties of the things to which they are 
attributed. E.g., the formula “abracadabra, X, shazam!” is just a nonsense formula in which 
X has been written without attention to fact, and can hardly be described as a “property of 
X”. However, when a form duly reflects the actual structure of X – e.g., when it is an 
isomorphism or a model of X which attributes to X the distinguishable, observationally 
replicable structure we actually observe when we look at X - it indeed defines a property of 
X, at least for scientific purposes. For example, where X is actually green (reflects green 
light), the form “X is green” ascribes the property “greenness” to X. Similarly, because the 
formal entity “set”, meaning “collection of distinct objects”, actually describes the structure of 
the universe - which is in fact a collection of distinct objects, plus additional structure – the 
property of “being a set” is a factual property of the universe, and this permits us to say “the 
universe is a set”.
Error 3: Previously, Mark was caught substituting CTMU "incomprehensibility" for his own 
personal incomprehension regarding the CTMU. Here he goes a step further, substituting 

*my* alleged incomprehension for the alleged CTMU "incomprehensibility" that he originally 
substituted for his own personal incomprehension of the CTMU. Instead of standing pat on 
his own personal confusion, he wanders ever farther afield, from his own mental state to an 
allegedly objective attribute of a theory to another mental state … this time, somebody 
else's. This is not how sound mathematical reasoning is conducted.
Mark: "In math, we build mathematical models of things in order to study and understand 
them. The mathematical model is an abstract description that’s useful for developing an 
understanding of the thing that it models – but the model is not the thing that it models. A 
set is a mathematical model that’s useful for describing many things. There are many things 
in the universe that can be modeled very well using set theory. But that’s entirely different 
from saying that the universe, or that anything in the universe is a set. A mathematical 
model is not the thing that it models."
Error: Mark is having a terrible problem distinguishing “set” from “set theory”. As I’ve just 
pointed out, “being a set” is in fact a property of the universe. That’s because “set” is 
defined as “a collection of distinct objects”, and the universe is in fact a collection of distinct 
objects (and more). The definition of “set” correctly, if only partially, describes the structure 
of the universe, and nothing can be separated from its structure. Remove it from its 
structure, and it becomes indistinguishable as an object and inaccessible to coherent 
reference.
I also get the impression that Mark is confused regarding the definition of “model”, which 
comes in two strengths. In logic, a model is a valid interpretative mapping, i.e., an 
interpretation of a formula A or class of formulae A* under which A, or every formula of the 
class A*, is true. The argument is generally a mathematical language with its own formal 
structure, while the image is anything that “makes the argument true” by virtue of identical 
structure “up to isomorphism” (of course, this situation is symmetrical; just as content 
instantiates a language, the language describes its content, and where validity is given, we 
have a “model” in either direction). The model includes both ends of the mapping, argument 
and image, in the form of shared structure.
So much for logic. In less formal contexts, the term “model” may be used in a less exacting 
way; the validity criterion of the mapping may be relaxed. The model can then be 
structurally non-identical to the argument, as when a scientist tentatively applies some 
mathematical description to a phenomenon without being sure that the description is 
correct, e.g., because inobvious features of the argument and/or image may be 
irreconcilable with the explicit part of the mapping. In this case, the model is not a legitimate 
property of the object thereby modeled, and can thus be separated from it without depriving 
the argument of structure. This is the sense in which Marks seems to be using the term.
Unfortunately, it is not the sense in which *I* usually employ the term, i.e. the logical sense, 
and it is my work that Mark has been criticizing.
Mark: ”There are also many things in the universe that cannot be modeled very well using 
set theory. (For example, try to put together a meaningful set-theoretic model of vacuum 
fluctuation and hawking radiation based on the set of particles in the universe. It really 
doesn’t fit well.)”
Comment: Although he seems unaware of it, Mark is not actually disagreeing with me here. 

Simply that the universe can be partially characterized as “a set of particles” does not imply 
the existence of a “meaningful set-theoretic model of vacuum fluctuation and hawking 
radiation”. In fact, this is a large part of what my essay actually says (a pity that Mark 
doesn't appear to understand this). Once again, a key distinction is that between “set” and 
“set theory”; although any version of the latter contains more formal structure than the 
unadorned “set” concept, it is insufficient to describe or model the universe in its entirety.
Mark: ”Does the existence of things in the universe which can’t me modeled nicely in set 
theory mean that set theory is something wrong? No. Because the universe isn’t a set. The 
universe and a mathematical model of the universe are very different things. There are 
many different possible mathematical models of the universe. Even taking set theory as a 
basis, there are numerous different set-theoretic mathematical models of the universe. The 
universe isn’t any of those mathematical models; those models are mathematical 
constructions that we use to try to understand it.”
Error: For the umpteenth time, “set” does not equal “set theory” (any version), and the 
property “being a set” isn’t something one can slap onto the universe, or not, at whim. It is 
synonymous with “being a collection of distinct objects”, which accurately reflects the 
observed structure of the universe. This makes it an actual property of the universe.
If the universe does not possess the property “being a set” – or if one prefers, “being valid 
content of the formal entity ‘set’” - then the set concept cannot be properly applied to the 
universe even as a tool. But then the natural and mathematical languages to which the set 
concept is fundamental cannot be properly applied to the universe either, and science is 
impossible. (This is called “reductio ad absurdum”; it consists in the derivation of an 
absurdity or contradiction from Mark’s initial premise that “the universe is not a set”.)
Mark: ”The lack of understanding of this distinction – the difference between a model and 
the thing that it models – runs throughout your writing.”
Error: In logic, a model is an interpretation (interpretative mapping) of a formula A or class 
of formulae A* under which A, or every formula of the class A*, is true. That is, it is a valid 
interpretative mapping. Because it is valid, it is an actual property of the thing modeled. Just 
as Mark can construct a verbal model of himself which factually represents his actual 
properties (“The person named Mark corresponds to the formal entity ‘a software engineer 
who is also a religious, theistic, Reconstructionist Jew’”), he can start with the model and 
then apply it to himself as a compound property: “Mark IS a software engineer AND a 
religious, theistic, Reconstructionist Jew.” The model reflects one or more actual properties 
displayed by Mark.
Mark: ”It’s part of why you try to talk about “syntax” in your model in a way that doesn’t 
make any sense to people who know what syntax means in math and logic. Because you 
muddle important distinctions. Syntax and semantics are very different things in a 
mathematical model. But if you insist that the mathematical model is indistinguishable from 
the thing that it models… then the syntax of an object is the object, the semantics of an 
object are the object, and therefore the syntax and semantics of the object are exactly the 
same thing – because they both are the object.”
Error: In logic, “syntax” describes the intrinsic structure of formulae and systems thereof, 
while ”semantics” additionally accounts for the meaning, interpretation, or descriptive 

content of formulae. Essentially, the syntax-semantics distinction is as simple as the form-
content distinction on which it is based. I've made it very clear in my writings that by 
"syntax", which I naturally define with respect to SCSPL, I mean “the formal (structural and 
grammatical) invariants of SCSPL”. By using a functional definition of syntax, one can avoid 
the necessity of enumerating its specific ingredients (functional definition is definition in 
terms of function; one specifies the definiendum in terms of its functionality in the overall 
system in which it exists, independently of content, at any desired level of generality).
Not, mind you, that I can’t enumerate the ingredients of SCSPL syntax at least in part, or 
that I haven’t actually done so. But a full extensional definition is not necessary for the 
purposes of this essay. Mark clearly has no business taking exception to my usage, as the 
CTMU is not his theory, but mine. If Mark wants to use his own preferred definition of syntax 
(and I can only imagine what that might be, if not the typographical structure of a 
programming language), then Mark needs to write his own theory.
There is one little respect in which Mark is right, however: the CTMU does indeed couple 
syntax and semantics in a new and profoundly different way, and has done so for the last 
couple of decades or more. Perhaps someday, Mark will come to understand what this 
means. But for now, it is 100% certain, by his own admission, that he does not.
Mark: “As I’ve frequently said on this blog: the worst math is no math. And that’s a pretty 
good description of your writing. There are lots of mathematical words, but they’re used in 
ways that just make no sense. They look impressive, but when you try to burrow down to 
get to their meaning, they don’t make sense. They muddle together fundamental concepts 
in nonsensical ways; they blur the distinctions between things that are necessarily distinct.”
Error 1: When Mark says that the mathematical words I use "do not make sense", he again 
oversteps his bounds. The most he is actually entitled to say is that they do not make sense 
*to him*. We have now ascertained that the reason for this is Mark’s severe 
incomprehension regarding my usage of certain terms, and in some cases, regarding their 
conventional meanings as well.
Error 2: Mark should not keep assuming that there is "no math" underlying the CTMU and 
my various descriptions of it, especially after he has been caught making errors of 
mathematical comprehension in connection with it. As Mark observes, there is plenty of 
mathematical terminology in this essay, and it has indeed been correctly and relevantly 
employed. Like it or not, that’s mathematical content. The most Mark can say is that he 
disputes this content, dismisses it as irrelevant or inconsequential, or doesn’t understand it 
(which, in any case, we already know beyond any shadow of doubt).
Error 3: Mark says that my essay blurs the distinction between certain fundamental 
concepts. In the present context, one may assume that he has two specific distinctions in 
mind: model | universe and syntax | semantics. But as we have already seen, it is Mark who 
does not understand these distinctions, at least in the context of the work he is criticizing.
Mark :"Worse, even if you ignore much of the muddled reasoning, you still can’t make this 
stuff work. If you actually take the word salad and try to render it as math, what you get is 
something very much like naive set theory. Unfortunately, naive set theory doesn’t work: it’s 
inconsistent. And your system, which by definition embeds itself, necessarily includes all of 
the inconsistencies of naive set theory."

Error 1: Again, Mark is attempting to impute the muddled character of his own mental state 
to the reasoning in my essay. This is evaluative incompetency plain and simple (see the 
value and competency criteria enumerated above). In view of his personal befuddlement, it 
is simply impossible for him to say whether or not the CTMU can “work”.
Error 2: Again, while I am employing the basic “set” concept in my reasoning, I am not 
employing “naïve set theory”. Nor am I employing any more advanced version of set theory; 
such versions improve on naïve set theory only by adjoining extra distinctions and 
restrictions that do nothing to expand the expressive capacity of the “set” concept, or any 
other concept general enough to suffice as an ultimate reductive entity.
Mark: “Of course, you won’t actually address any of these problems. You’ll just wave your 
hands around and insult me some more.”
Comment: It is not my responsibility to solve problems which are functions of Mark’s 
personal incomprehension rather than anything actually relevant to my work. And my goal 
here has not been to “insult” Mark, but merely to get to the bottom of his incomprehension 
and establish that he has no business flinging insults like “crank” around when he clearly 
doesn’t understand who or what he’s attacking.
Mark: “I remain uncertain of just how it is that doing that somehow defends the validity of 
your theory, but that’s probably just because I’m not as smart as you.”
I have not considered whether I’m “smarter than” Mark or vice versa. That’s because such a 
judgment would detract from the content of the discussion. Even though I do find him 
deficient in the kind of knowledge he’d actually need to properly read my work, I actually 
think he’s probably pretty smart, all considered. I just think that he’s verbally incontinent and 
incompetent as an evaluator of my work.
Bottom line: Philosophically and mathematically speaking, Mark is what one would call a 
“hardcore dualist”. This is because he makes a hard and uncompromising distinction 
between form (e.g., “set”; “model”) and content (e.g., “universe”). As we have seen, Mark 
cannot possibly justify this form of dualism, as it has the effect of separating the universe 
from the structural properties in terms of which we scientifically identify it and reason about 
it at any stage and on any level.
Mark is obviously a decent computer programmer; this is implied by the fact that he’s a 
senior software engineer for Google. But just as obviously, he is neither a mathematician 
nor a philosopher. Writing good code is not easy, and Mark deserves respect for his evident 
ability to do it. But he should either stow the “math” blog, or trim his sails and try to stay 
closer to home. He is simply not up to going toe-to-toe with all of those on whom he targets 
his uncontrollable resentment.
Regarding Mark’s commentators, thanks for your participation. Some of you have offered, 
amidst the noise, what almost seems to be intended as constructive and well-meant advice. 
To the extent that this is actually the case, your efforts are appreciated. I would merely 
advise you not to leap so readily to what seem to be your highly standardized conclusions 
regarding me, my level of knowledge, and the originality and profundity of my writing, lest 
you end up disappointed and embarrassed as a result.
If I pop in here again, it will be strictly as an undeserved favor. Good day to all of you.

Comment #4: 
Mark, when I look at your writing, it’s like trying to decode gibberish.
I’m not just saying that; I really mean it. You claim you can’t understand a word I write, but 
for me it’s the other way around. I’ve already tried repeatedly to tell you, your ditto-heads, 
and other commentators in plain English that my essay nowhere relies on naïve set theory, 
and in fact can be construed as a condemnation of naïve set theory for philosophical 
purposes. Yet here you go again, behaving as though I said the exact opposite. Your only 
possible justifications are (1) that I’m lying about not using naïve set theory; or (2) that I’m 
so asleep that I don’t know when I’m relying on naïve set theory and when I’m not. But to 
my way of looking at it, both of those claims are absurd. I feel like I walked into a seedy 
diner and ordered the “fresh garden salad” only to have the proprietor hand me a day-old 
corn dog with a couple of bites missing and a check that reads “1 salad plus tax (no credit).”
In fact, I’m reminded of a sad old joke. Somewhere in the Deep South of yore, a bus 
containing a Black gospel choir was on its way to a revival. Suddenly, a car stopped on the 
shoulder of the road pulled out directly in front of the bus. Panicked, the driver cranked the 
wheel as hard as he could, veering directly into the path of an oncoming semi. Unable to 
stop, the fast-moving semi ripped open the midsection of the bus, strewing the highway with 
dead or injured passengers, some moaning in pain. A minute or two later, an archetypal 
redneck and his woman drove up in a pickup truck. Seeing the carnage, the hillbilly 
stopped, got out of the truck, and sauntered among the bodies for a minute or two. Then he 
returned to the pickup, and without saying a word, resumed driving in the same direction as 
before. “But Billy-Bob,” said his incredulous damsel, “ain’t summa them people still alive?” 
The redneck snorted contemptuously and used his teeth to pop open a can of beer. 
“Wayull, Lurleen,” he drawled after a long and satisfying gulp, “some of ‘em *SAYud* they 
was. But you know how them @#$$%&s lie!” (Ring a bell?)
On a more serious note, I think I know what your problem is. As soon as I mentioned the 
words “self-including set” in the essay, a little warning buzzer went off in your head. What 
you should have said to yourself at that point was “He’s right – if the universe is a set, and if 
the universe actually implements self-inclusion in any way – after all, he has explicitly stated 
that when it comes to the universe, there’s ultimately nothing external to contain it or serve 
as a medium or background for it - then really what we’d have is something at least 
reminiscent of a self-inclusive set. But since that’s a violation of logic associated with naïve 
set theory, which everyone knows doesn’t work, this guy must be trying to describe, or at 
least pointing in the general direction of, a new way of eliminating the contradiction. So 
maybe I’d better try a little harder to get the message. Even if I can't quite get it right, at 
least I won't be guilty of getting it all wrong.”
Instead, what you evidently said to yourself was more like this: “Oh man, this fool is 
absolutely out to lunch! ‘Self-including set’ indeed – when I get done with him, this crank is 
going to be sorry he ever dared to open his mouth! No doubt about it - he’ll rue the day he 
ever heard the glorious name of Mark Chu-Carroll (which probably hasn’t happened yet, but 
thanks to my thousands of hits and my faithful legion of anti-crank warriors, soon will)! Why, 
with my superior math skills, this guy and everyone like him is totally at my mercy, cannon 
fodder for my unbelievably excellent anti-crank blog! Good Math (my opinion) trumps Bad 
Math (any conflicting opinion) every time, so everybody better hunker down and get ready 
for some more of that trademark supercilious Chu-Carroll wit! Oh, joy - life is fine when 

you’re a crank-fighting internet hero like me, myself, and I!!”
While you might see the latter self-dialogue as a bit over the top, your subsequent behavior 
shows that it accurately reflects your basic attitude. And whether or not you’re capable of 
recognizing it, this makes you incompetent to evaluate the essay you’ve been trying to 
evaluate, to the extent that at this point, I no longer think that you (and some of your fans) 
are capable of understanding anything that I say on the subject, mathematical or otherwise. 
So why not do yourself a favor, stop giving yourself so much undeserved credit as an all-
purpose authority on all things mathematical, and learn to withhold judgment on that which 
you don’t understand? You’ll probably live a longer and happier life if you do.
Just a piece of friendly advice to someone who seems desperately in need of it.
Comment #5: 
I'm actually too busy for this right now, but I see that given the apparent harshness of my 
first response in this thread - the primary purpose of which was to provide simple criteria by 
which conceptual value and evaluative competence can be objectively rated - a little 
background might be in order.
1. Mark originally critiqued my essay in February, 2008 at ScienceBlogs.
2. Just a week or two ago, somebody brought Mark's critique to my attention. Here’s the link 
provided in that email:
http://scienceblogs.com/goodmath/2008/02/two_for_one_crackpot_physics_a.php
3. Noting that Mark had titled his critique in an extremely deprecating manner, I thought it 
advisable to respond. However, I found that because Mark had moved his blog to 
Scientopia, it would be impossible to enter a response at ScienceBlogs. Therefore, I 
proceeded to the analogous entry at Mark's new blog site. Here's that link:
http://scientopia.org/blogs/goodmath/2008/02/21/two-for-one-crackpot-physics-and-
crackpot-set-theory/
4. I entered what I thought, under the circumstances, was a friendly and moderately 
informative response. I’m sure that my impatience was evident, but bear in mind that I had 
just discovered that Mark had been insulting me and my work with impunity for the last 
several years.
5. Mark responded in a highly aggressive and insulting way. So I entered another comment 
to make sure that everyone understood the problem. I hoped that this would be the end of 
the matter.
6. Unfortunately, it was not the end of the matter. Mark moved a slightly updated version of 
his critique to the current front page of his blog, under an arguably even more insulting 
heading, in an apparent effort to teach me the following lesson:
"Hey @#$%&, you don't mess with Mark Chu-Carroll, except at your peril!"
(The wording is my own; I'm merely trying to approximate what seemed to be the message 
that Mark was sending by renewing his critique under the heading he chose.)

If one thinks about it a little, this may at least begin to explain the harshness of my tone. 
While I understand that some people might find my language excessive, I’m afraid I can’t 
agree. In fact, I think that my language has been quite controlled under the circumstances.
Thanks for your attention.
Comment #6: 
There seems to be a little confusion here. The poster "Anonymous" is not me, and has not 
been authorized to speak for me. He is proceeding on his own initiative, using his own 
understanding of the theory, in which he has not been coached or personally instructed by 
me. (Of course, he is free to do what he has chosen to do. But thus far, he has not handled 
this discussion quite as I would have handled it.)
This forum belongs to Mark Chu-Carroll, and because Mark is forthrightly using his real 
name attached to his real credentials, his name and reputation are on the line (which is 
exactly as it should be). Almost without exception, the rest of you are trying to argue without 
answerability, and unsurprisingly, your argumentation is shoddy. In fact, most of it is so bad 
that it would be a complete waste of time for me to address it at all. I'm simply too busy for 
that.
If you insist on having me address any particular point you have made in even a cursory 
fashion, you need to have Mark clean it up for you and present it as a formal objection along 
with his personal endorsement. (None of this is negotiable; this way, Mark will pay the price 
for upholding whatever nonsense I'm forced to spend my valuable time dismantling.)
Alternatively, if you actually claim any qualifications in this field, you can provide such 
information as will allow you and your home institution to be unequivocally identified and 
thoroughly checked out by all concerned. That way, you and your institution can pick up the 
tab instead of Mark. (If you have no reputation or credentials and affiliations in this particular 
field, then please don't bother providing any information about yourself - if I put my own 
reputation at risk by arguing with you, then you must have one to put at risk as well, or no 
go. In situations like this one, such reciprocity is only fair.)
Thanks for your attention, and have a nice day.
Comment #7: 
What a surprise - another rare appearance by the one and only Mark Chu-Carroll. 
Unfortunately, it almost seems to have been instigated by someone who misleadingly 
assured Mark that he could win a serious argument with me on the topic of my own work 
when in fact, that’s quite out of the question.
Having already caught Mark in several glaring instances of mathematical incomprehension 
(see above), I suspect that the more technical my explanations, the deeper and more 
petulant his incomprehension will become, and the more impudent and unintelligible his 
retorts will be. So I'll try to keep my responses as simple as possible, albeit with the sad 
expectation that Mark won’t understand a word of them anyway.
This is a long reply. There are two reasons for that. First, Mark is being characteristically 
dense and thus forcing me to be repetitive. Secondly, as I remarked above, he generates 

errors faster than he writes – it seems like a paradox, but Mark is one of a kind - and he 
usually racks up more of them than Kellogg’s has corn flakes. (We’re talking about silly 
failures of linguistic and/or mathematical comprehension that most intelligent high school 
students would have the sense to avoid.) In fact, if this drags on, I’ll have no choice but to 
save time by concentrating less on being informative, and more on the specific personality 
issues which would seem to account for the fact that Mark is so very, very hard to inform.
Or maybe just ignore him.
Mark: “How do you define ‘profound’?”
I can't speak for anyone else, but "profound" usually means something like "leading to 
important or at least meaningful and far-reaching consequences or insights". Thus, the 
recognition of profundity is strongly dependent on the capacity of any given reader to 
recognize meaning and importance when he sees them. Where this capacity is low, 
profundity is wasted. It increasingly appears that Mark is a person on whom profundity may 
be wasted. (But here we go again.)
Mark: “It's a ‘theory’ [that] can't be tested, makes no predictions, and answers no 
meaningful questions. By an actual scientific definition of theory, it isn't even a theory.”
Error 1: There are many kinds of theory. Only some of them are "scientific" in the sense that 
they take the scientific method as an implicit meta-axiom (a higher-level axiom proscribing 
the recognition of empirical axioms, or axioms with empirical force). This is a severe 
theoretical liability; scientific theories are confined by definition to scientific methodology, 
which, as currently understood, prohibits them not only from being verified in the logical 
sense, but from exploring the nature and strength of their own connections to their 
universes, which precludes any form of what we might call “self-verification”. If and when 
Mark familiarizes himself a bit more with the logical side of model theory and its proper 
application to the content and methodology of science, perhaps he’ll be able to comment on 
the subject a bit more fruitfully.
Error 2: Just as there are different kinds of theory, there are different kinds and levels of 
verification. Making and testing predictions is arguably the only way to empirically confirm a 
scientific theory ... but not all theories are empirical, and scientific theories are not the only 
theories with empirical content. Theories can be formal or informal, mathematical 
(axiomatic) or scientific (in which case they are still at least partially mathematical), and if 
scientific, then descriptive, explanatory, or predictive in character. An explanatory theory 
can give a superficial explanation of its empirical content, or it can penetrate deeply enough 
into its subject matter to resolve associated paradoxes on the syntactic or semantic level (a 
powerful source of veracity in itself). It can even extend the interpretative framework to self-
verificative effect, as does the CTMU (unfortunately, this is probably well over the head of 
most readers of this forum – exceptions are allowed, but improbable).
Error 3: The CTMU is not a mere "scientific” theory. Philosophically, that's a good thing, 
because no scientific theory that does more than catalogue data can be validated by any 
means whatsoever, including empirical testing. At best, empirical testing yields only an 
imperfect “degree of confirmation”, and is subject to several kinds of inductive and 
interpretative ambiguity.

Although the appropriate testing procedure is not the same for a theory like the CTMU as it 
is for an ordinary scientific theory, the CTMU can in fact be tested. First, it can be tested for 
logical and semantic consistency by examining it for errors or internal contradictions. 
Unfortunately, one would need to understand it in order to do that, and the vast majority of 
its critics (including Mark) do not. Or one could try to test the theory by debating it, as Mark 
seems bent on doing. But thus far, he has not been debating the theory I wrote. He has 
instead been debating against another theory entirely, a straw-man theory which he 
*claims* that I wrote, but which I find completely unrecognizable ... as unrecognizable as he 
apparently finds the theory I actually wrote.
Of course, the fact that the CTMU is not strictly scientific, i.e. dependent on the scientific 
method for confirmation, does not in principle stop it from yielding predictions or 
explanations of empirical phenomena. But using it for such purposes is a bit tricky, not least 
because many of its predictions and explanations may be unrecognizable as such to a 
philosophically naïve, quasi-religious neo-Darwinian apologist like Mark sometimes appears 
to be.
Error 4: As errors 1-3 amply confirm, Mark is again indulging in what has now been 
revealed as a most unbecoming habit: mistaking his personal incomprehension for an 
actual property of someone else's theory. This is obviously something that he should try 
harder to control. Much harder.
Mark: “Seriously... CTMU is nothing but an elaborate word-game.”
Probable error: If Mark is using "word game" to mean "a verbal contest regarding matters of 
fact," then he is correct. But if he's using the term to mean "words chosen merely to create 
the illusion of victory in some basically meaningless, content-free debate or other 
communicative process," which is probably what he’s doing, then he is mistaken. (I’m just 
trying to do the right thing, and stop Mark from hysterically misleading others regarding my 
ideas out of sheer ignorance and resentment.)
Mark: “Just look at Chris's bullshit about sets and set theory. He bobs and weaves, but 
completely avoids defining what he means by the word set.”
Error: In fact, I explicitly agreed with Wikipedia's definition of "set". Mark evidently disagrees 
with that definition. The burden is now on Mark to explain why it is wrong, why the universe 
fails to instantiate it, or failing that, why it automatically implies reliance on some specific 
brand of set theory that Mark loves to hate.
Mark: "It's *not* sets from Cantor's naive set theory. It's *not* sets as defined in NBG. It's 
not sets as defined in ZF.”
Error: Ruling out these versions of set theory is pointless, because my usage of "set" is 
indifferent to any standard version of set theory. When Mark insists on shackling this 
definition to his least-favorite version, he renders the concept foundationally irrelevant, 
necessitating its replacement by a more basic and flexible kind of mathematical object and 
language.
Mark: “In fact, he gets annoyed when you talk about set theory because, he insists, he's 
talking about *sets*, not *set theory*.”

Mirabile dictu, a point of agreement! (Of course, we differ on its significance.)
Mark: ”That's an amazingly ridiculous thing to say from someone who claims to be 
discussing a scientific theory.”
Error: I do not claim to be discussing a “scientific theory". If I were to make such a claim 
regarding the CTMU, it would imply an extended definition of science achieved by 
eliminating the dualistic brick wall that sits between theory and observation in the standard 
formulation of the scientific method (something which people who sound like Mark typically 
have no idea how to do, and which they often assume to be impossible). Again, this in no 
way implies that the CTMU is devoid of empirical content. Its empirical content necessarily 
includes the entire perceptual universe, as does that of logic.
Mark: “Set theory is nothing but a very precise way of defining what we *mean* by the word 
set.”
Another point of agreement. Mark, in specifying "naive set theory" as the core ingredient of 
his personal erroneous interpretation of my essay, has been very clear that this is what *he* 
means that he thinks *I* mean when I use the word "set". But that's yesterday's news.
Error: Unfortunately, Mark's personal interpretation of my interpretation of concepts like 
"set", "set theory", and "the relationship between set theory and the general definition of a 
set" is out to lunch … a six- or seven-martini lunch, to push the idiom. As explained above, 
I'm using the term "set" in a very general way ... the way that, e.g., Wikipedia uses it. If Mark 
doesn't like this definition, then he needs to explain why it is inadequate for my purposes 
even when I'm not relying on it in my essay, and why I need to settle for one standard 
version of set theory or another even while explicitly rejecting set theory as an exclusive 
basis for the CTMU.
One almost gets the impression that in declaring the “set” concept meaningless except in 
conjunction with some standard version of set theory, Mark would also declare the 
“quantum gravity” concept meaningless except in conjunction with some existing and 
probably mistaken theory of quantum gravity. If Mark were to have his way, scientists would 
be unable to meaningfully address such unexplained phenomena without first adopting 
whatever half-baked theory might already exist regarding them (which itself can only have 
been formed in violation of that rule). It’s a catch-22, a conflation of definition and 
theorization that would stop science dead in its tracks.
Like so many of Mark’s ill-conceived and ill-informed opinions, it makes absolutely no sense 
(except in certain highly restricted formal contexts, none of which are presently operative).
Mark: “Chris rejects any attempt to provide a precise definition of ‘set’.”
Error: Wrong again. I explicitly deferred to Wikipedia's definition of "set", which, though 
general, is admirably precise in its generality. Again, if Mark doesn't like this definition, then 
he needs to explain why it's so awful, and why I should be concerned that he doesn't 
understand that I'm not relying on it (except to observe, as I did in my essay, that to the 
extent that the universe is a set, it is seemingly vulnerable to certain paradoxes associated 
with sets, and therefore in need of a foundational theory capable of resolving those 
paradoxes on a level deeper than conventional set theory allows).

Mark: “Why would he do that? Because his theory is transparent nonsense.”
Error: Mark has already been repeatedly called on the carpet for inserting his own hare-
brained speculations in place of the actual meaning of certain material which he absurdly 
pretends to have read. That carpet has just gone from threadbare to ratty. If it gets any 
thinner, it too will be “transparent”.
Mark: “But by refusing to define one of the fundamental base terms that he uses, he can 
weasel out of any actual criticism of the shoddy logic in his theory.”
Error: But I did define "set". (See how Mark is obstinately forcing me to repeat myself?) I 
defined it just the way it is defined by Wikipedia and its reputed mathematical experts, i.e., 
the mathematically trained subset of Wikipedia editors allegedly involved in editing and re-
editing its mathematical articles. Perhaps Mark should explain his beef with them.
If Mark doesn’t like Wikipedia anymore, then here’s how “set” was defined by Georg Cantor: 
“A set is a gathering together into a whole of definite, distinct objects of our perception and 
of our thought - which are called elements of the set.” (This is in Wikipedia too.) Note that 
Cantor, once having rendered this general theory-independent definition based on 
perception and cognition, was no longer in a position to insist that his own “naive” version of 
set theory be shoehorned into it. This theoretic independence is what protected the general 
definition from being completely discarded when certain aspects of his personal theory 
about it came under attack.
Need sets always be “well-defined”, and does this always imply embedment in some 
formalization of set theory? Obviously, a set should be well-defined in precisely the sense 
given by Cantor, as this is enough to render it perceptible or intelligible. Once this criterion 
has been met, however, any particular version of set theory is beside the point; the notion 
that one must be attached is merely an arbitrary formal criterion that has nothing immediate 
to do with the percept or concept in question. The point of proving a set to be “well-defined” 
is to establish the possibility of its existence; when something is perceived as a set, or 
mathematically conceived as an image or generalization of a perceived set, its existence is 
clearly given by perception and need not be formally established. That’s a very good thing, 
because there are several versions of set theory available, some of them self-consistent, 
and any given one of them may or may not be suitable for particular scientific or 
philosophical purposes. My purposes, for example.
As it happens (and not by accident), consistent versions of set theory can be interpreted in 
SCSPL. The problem is, SCSPL can’t be mapped into any standard version of set theory 
without omitting essential ingredients, and that’s unacceptable. This is why the CTMU 
cannot endorse any standard set theory as a foundational language. But does this stop the 
universe from being a set? Not if it is either perceptible or intelligible in the sense of 
Cantor’s definition. One thing’s for sure: if it is neither, then it is theoretically unidentifiable. 
And in that case, Mark is wasting not only his own time, but everybody’s time, by going 
around and around about it like a tape loop of a broken record in an echo chamber.
Mark: “Consider the recent discussions here of this. Is the universe a set?”
Yes. As I've already stated, the universe fulfills the general definition of "set" in numerous 
ways, and this indeed makes it a set (among other things with additional structure). 

Otherwise, its objects could not be discerned, or distinguished from other objects, or 
counted, or ordered, or acquired and acted on by any function of any kind, including the 
functions that give them properties through which they can be identified, discussed, and 
scientifically investigated. If something is “not a set”, then it can’t even be represented by a 
theoretical variable or constant (which is itself a set), in which case Mark has no business 
theorizing about it or even waving his arms and mindlessly perseverating about it.
Does this mean that a set is all that the universe is? Of course not, although one would 
never know it from Mark’s interminable fussing and fuming.
Mark: “Does the superset of the universe really exist?”
Yes, provided that Mark means “power set”. It exists in SCSPL syntax, which itself exists by 
logical necessity. One can’t even conceive of logic without applying a distributed “power-set 
template” to its symbols and expressions, and such templates clearly perform a syntactic 
function. However, because Mark evidently has a definition for "syntax" which differs from 
my own (and perhaps from most other peoples’ as well), but which must nevertheless be 
interpreted in a way appropriate to the specific theory under consideration, namely my 
theory and not Mark’s, and because Mark probably defines "existence" in a shallow and 
materialistic way that he hasn’t really thought out very well, he doesn't understand what this 
means.
Mark: “Is the superset of the universe part of the universe?“
Provided that the "superset" in question is the power set, the short answer is yes. More 
accurately, the power set is a distributed *aspect* of the universe by virtue of which objects 
and sets of objects are relationally connected to each other in the assignment and 
discrimination of attributes (the intensions of sets). Without it, the universe would not be 
identifiable, even to itself; its own functions could not acquire and distinguish their 
arguments. In fact, considered as an attributive component of identification taking a set as 
input and yielding a higher-order relational potential as output, it is reflexive and “inductively 
idempotent”; the power set is itself a set, and applied to itself, yields another (higher-order) 
power set, which is again a set, and so on up the ladder.
Of course, even the perceptual stratum of the universe is not totally perceptible from any 
local vantage. The universe, its subsets, and the perceptible connections among those 
subsets can be perceived only out to the cosmic horizon, and even then, our observations 
fail to resolve most of its smaller subsets (parts, aggregates, power-set constituents). But a 
distributed logical structure including the power set can still be inferred as an abstract but 
necessary extension of the perceptual universe which is essential to identification 
operations including that of perception itself.
The scientific import is obvious. Where the universe is defined, for scientific purposes, to 
contain the entire set of past and future observational and experimental data, plus all that 
may be inferred as requirements of perception, its power set is integral to it as a condition of 
its perception and scientific analysis, not to mention its intrinsic self-differentiation and 
coherence. Without its power set, its parts or subsets would be intrinsically indiscernible and 
indistinguishable, which would of course amount to an oxymoron; “parts” are distinguishable 
by definition, and therefore constitute a set with the discrete topology construed by 
relevance (any reference to which naturally invokes the power set) and the indiscrete 

topology construed by veracity (inclusion-exclusion). Without the power set function and its 
stratified relational potential, one not only can’t say what the parts and their mutual 
relationships are, one can’t even say what they’re *not* … and as any parts not relevant to 
the others are not “parts” as advertised, even referring to them generates contradictions and 
must therefore be avoided.
Mark: “Those questions *can't* be answered without actually defining set in a precise way. 
(For example, in NBG theory, the collection of subsets of an infinite set isn't a set, so the 
‘superset’ doesn't exist.)”
Error: What utter nonsense. Aside from the fact that NBG avoids supersets by the largely 
(but not entirely) semantical device of redefining certain sets as “classes”, one can simply 
move the entire discussion onto a new foundation, i.e., into a new foundational language, 
and explain how the set concept should be interpreted within it. The foundation I'm talking 
about is not NBG, or ZF, or naive set theory, but the CTMU and SCSPL. For the hundredth 
time, sets can be interpreted therein as collections of discernable, distinguishable objects 
and events (just as Cantor defines them), or if one prefers, as functions and functional 
arguments whose more involved properties are developed not in set theory, but in (you 
guessed it) SCSPL. That way, set-theoretic paradoxes, e.g. the power set paradox, can be 
precluded or resolved with (you guessed it again) SCSPL mechanisms instead of the 
mechanisms of any standard, foundationally inadequate version of set theory.
Until Mark comes to grips with this fact and desists in his asinine attempts to tell the author 
of the CTMU (me) what the CTMU says, his understanding of it will remain stunted. As 
everyone is by now aware, the more blighted and pathetic Mark’s (mis-)understanding of 
something, the stronger and more irresistible his compulsion to “spread the wealth” by 
adopting a deceptive tone of authority and brazenly misleading others to the effect that it 
somehow equates to his own confusion regarding it, when in fact, he has merely attempted 
to tie his personal confusion around its neck like a squawking, flapping, hyper-opinionated 
albatross. It is obvious to all but the most deluded of his partisans that this is a brand of folly 
in which he should not be encouraged, and that those who do so anyway are beneath 
contempt.
Now for a little sermon containing some useful advice for Mark and others who think the 
way he does. Mark is seemingly a reasonably intelligent person who appears to be 
interested in learning some math, but he has what amounts to a personality-driven learning 
disability: instead of taking the time to properly absorb some new bit of math he has found, 
he rushes to post it on his blog, complete with technical errors and errors of comprehension. 
Then he moves on to the next tantalizing bit of math and the next blog post. The unfortunate 
result is that he never properly absorbs and integrates what he thinks he is “learning”. Thus, 
when he encounters a paper (like my essay) which seems to involve some of the math he 
has supposedly “learned”, but which he doesn’t really understand at all, he blindly leaps to 
the conclusion that his confusion cannot possibly be due to any fault of his own. After all, 
having briefly lit upon that kind of math and then fluttered away like a fickle, flighty 
mathematical butterfly to visit another, he fancies himself an expert on it (as opposed to, 
say, a dabbler or a dilettante). So naturally, it’s not Mark who’s in a fog; it must be the other 
guy! And that makes the other guy irresistible cannon fodder for yet another entertaining 
salvo from the big guns of the most fearsome rubber-band-powered anti-crank destroyer in 
the blogosphere, the USS Good Math, Bad Math!

By thinking and behaving in this silly way, Mark encourages some of his commentators to 
assume that they are able to see technical problems with my work that I can’t see. This is 
almost always a mistake. I do in fact see the full range of what might be construed as 
technical problems with my work, but differ from my critics in that I usually see their 
solutions as well. Because the solutions are obvious to me, the problems begin to unravel 
before they can take up lodging in my theory, sparing me the trouble of noting their putative 
existence and agonizing over them and engaging in the kind of masochistic publish-or-
perish tail-chasing that they inspire in academics (and others) who don’t really understand 
them. After all, academics write about problems not only to offer definitive solutions for 
them, but to explore gaps in their own comprehension. Unfortunately, the precious, carefully 
cultivated orchids of academia often forget in the course of their well-referenced but 
ultimately omphaloskeptical self-explorations that they very much belong to an intellectual 
closed shop, and that their own cognitive gaps preclude definitive judgments on the 
cognitive adequacy of the weeds that grow wild and free beyond the sheltering walls of their 
ivory tower hothouses.
When Mark or one of his commentators summarily accuses me of ignorance or 
carelessness for appearing to ignore such “problems” in some piece of writing he has 
bumped into, thus prompting him to blow his top like Krakatau and do his trademark hotdog 
dance for the tourists, he does not merely seem to be trying to pass himself off as my 
intellectual equal. That alone wouldn’t bother me; I usually have no problem with 
assumptions of intellectual parity as long as people remain polite. Rather, Mark appears to 
be trying to pass himself off as my intellectual superior ... and believe it or not, I don’t have 
to let him get away with that if I’d rather make an issue of it.
In other words, if you are one of those who has been encouraging Mark in his folly, you are 
doing him a disservice. If you’re really his friend, then why not allow him to come to his 
senses, drop the pretense, hypocrisy, and incorrigible buffoonery, and spare himself the 
humiliation of being made to look less knowledgeable or intelligent than he obviously thinks 
he is? After all, if Mark learns to show a little respect, then others are more likely to return it. 
On the other hand, if he continues to pop off because a few diehard sycophants appear 
willing to cover for him and get his back even when the springs and cogs and gear oil spray 
out of his ears, then there’s always a risk that sooner or later, at a time to be determined by 
fate (and/or me), he’ll learn the unpleasant taste of crow. Raw crow, with the feathers and 
the mites.
To the few of you who seem to understand what’s actually going on, thanks for hanging in 
there. But again, you should probably try not to assume that you see technical issues with 
my theory that I don’t see, e.g. the problem of induction and the relevance of Gödel’s 
theorems. The CTMU contains ample allowance for both.
I recall putting a few pieces online in which the problem of induction and Godel’s theorem 
are mentioned. For example, regarding the latter, one piece was called “The Theory of 
Theories” and written in an easy, breezy style; the other was called “Self-Reference and 
Computational Complexity” (2001) and contained more mathematical detail. Unfortunately, 
it doesn’t seem to be available any more. As I recall, it began with an explanation of self-
reference and diagonalization in the Liar Paradox, introduced the theory of metalanguages, 
applied these concepts to Gödel’s proof of the undecidability theorem, moved on to 
computation theory and Turing’s work on incomputability re the Halting Problem, sketched a 

comparison between the diagonalization techniques involved in undecidability and 
incomputability, introduced computational tractability with attention to oracles and 
relativization, and finished off by discussing the analogue of linguistic self-reference in 
Boolean circuits with respect to their prospective application to P?NP. The CTMU wasn’t 
mentioned, but bear in mind that the CTMU is a self-referential system to which the basic 
principles of self-reference apply.
That paper was online for years. It’s probably languishing on a storage drive somewhere; if I 
find it, maybe I’ll slap it back up. Meanwhile, please rest assured that I’m aware of most if 
not all of the major technical issues bearing on my work.
Comment #8: 
Mark: “The reason that I've focused on the set/set theory thing is simple. If the basis of an 
argument is based on undefined, inconsistent, and/or invalid terms, the entire argument is 
undefined, inconsistent and/or invalid.”
Not if the reason they're "undefined" is that you, Mark Chu-Carroll, refuse to accept their 
definitions as given, and then refuse to explain why you're refusing to accept their 
definitions as given.
Mark: “In the case of sets, sets are a simple basic concept that can, very easily, become 
inconsistent. That's the whole point of set theory. Set theory is a system that produces a 
definition of sets that doesn't devolve into inconsistency.”
So then let's have a look at some of these inconsistencies. Carefully write an essay on them 
- the specific ones, not just the ones at which you've been frantically waving your arms – 
and post it on your site. If it’s good enough, maybe I’ll respond.
As I've remarked above, the formal well-definition of sets is unnecessary regarding sets that 
are directly perceived. Otherwise, the last time you perceived a set, you should have 
refused to follow through with the perception until the set announced that it had duly 
embedded itself in some consistent version of set theory.
Did you insist on that? (Why, sure you did!)
Mark: “The definition of set that you focus on, from wikipedia is, ultimately, the definition of 
sets from naive set theory. You can bitch and moan, bob and weave, whine and complain 
all you want - but if you use the naive set theory definition of sets, then your argument is 
built on naive set theory.”
You really don't have a clue, do you, Mark? Cantor's definition of "set" is not explicitly 
parameterized by Cantor's "naive" version of set theory. The theory is not a definiens of the 
definition; the definition has explicit definientia, namely, the well-established and patently 
consistent operations of discerning its elements and gathering them together. You're simply 
asserting otherwise without adequately explaining yourself.
Mark: “And naive set theory is inconsistent, and thus invalid.”
That's why I don't use it. As explained, over and over again.
Mark: “Your ‘theory’ starts with an argument about whether or not the universe is a set, and 

derive supposedly deep and profound conclusions from that argument. But you're argument 
is clearly based on a definition of ‘set’ that isn't valid.”
No, it isn't. Stop trying to tell the authors of the theories you criticize what they meant when 
they wrote their theories. It's ridiculous.
Mark: ”You cannot derive a valid argument from an invalid foundation. And all of your 
pointless verbiage doesn't change that. If you want to use sets in your argument, you need 
to use a definition of sets that isn't invalid. If you're not willing to do that, then your theory is 
nothing but an exercise in intellectual masturbation.”
So is this dialogue, as long as you refuse to pluck the scales from your eyes and open your 
mind a little.
You can’t BS your way out of the pickle you’ve gotten yourself into here. You may as well lie 
down and play dead.
Stay down, Mark. Don't even try to get up.
Comment #9: 
Just a couple of friendly observations.
First, I don’t fully trust bare external links posted on this site. I will probably neither click on 
them, nor paste them directly into a browser. To put it bluntly, I've had too many problems 
with the kind of person who tends to frequent this (skeptical / materialist-physicalist / "anti-
pseudoscience" / "debunking" / atheistic or anti-religious) kind of forum, especially for the 
purpose of criticizing me or my work in the way that we’ve seen here. Too many such 
people, confused but nonetheless committed to their beliefs, turn out to be more trouble 
than direct communication with them could ever be worth.
More generally, although I try to make reasonable exceptions, I have a hard time regarding 
those who share what appears to be Mark's basic mindset as trustworthy by those who 
share anything resembling my own perspective. In fact, I regard them as lacking any firm 
basis for ethical understanding or behavior, something for which I have a sad abundance of 
experiential confirmation. (Of course, this is a statistical judgment which says nothing 
personal about Mark or anyone else.)
Secondly, it's a small world ... for some of us, at least. I'm familiar with Robert Pirsig's work 
because, at one time, we shared certain acquaintances. It’s something that I’ve heard 
enough about and even find interesting - Robert is clearly a very bright man - but which I 
find a bit too nebulous to be very useful to me. On the other hand, some of his ideas make a 
great deal of sense as far as they go, so please don't rush to the conclusion that I dismiss 
his philosophy. It's just that comparing it to the CTMU would be like comparing a Ford 
Model T to the Starship Enterprise. Any associated knowledge-transfer would be pretty 
much one-way, from me outward. Such a transfer will probably occur one of these days, but 
on my own terms and in my own good time.
To the extent that anyone’s interest in my work is sincere, I very much appreciate it. But 
please try to remember that when you read something I've written about it, you’re probably 
reading a highly simplified version from which much of the detail has been regretfully 

omitted. Why has it been omitted? Because most people, even those who claim to know 
some mathematics, would merely be distracted by it, are possibly incapable of 
understanding it (as we've seen), and/or would take it as something to be misleadingly 
attacked out of context.
With all due respect, those who assume that such detail does not exist, or believe that 
something I've said about the CTMU is invalidated by something they think they know, have 
another think coming. Praemonitus, praemunitus.
Good Day. 
Comment #10: 
Just in case there's any residual confusion...
I certainly don't want to offend those of anonymous or pseudonymous persuasion, but 
again, I don't usually respond to ad hominem criticism, no matter how well-seasoned with 
psychobabble and faux-mathematical punditry. Nor do I participate in fishing expeditions 
wherein straw men angle for red herring under the coercive supervision of ax-wielding, ax-
grinding bellyachers. If you want to engage me (or anyone else worth engaging), you'll need 
to do much better. You'll also need to use your real names and include some verifiable 
autobiographical information with your nonsense...and even then, I'd better be impressed by 
what I read, or there will again be no response from me.
Igor - I hate to be rude, but I don't care about anyone's opinion of my intelligence. Having 
lived a relatively unsheltered life, I'm fine with having my intellect underestimated. I'm 
especially apathetic about informal Turing tests administered by college students and/or 
anonymous Internet gadflies on sites like this one. To put it mathematically, I estimate the 
mathematical expectation of interacting with people who express doubt regarding my 
intelligence as well under zero, that is, as negative. Obviously, any loss of negative utility is 
a winning proposition. So vaya con Dios, and no hard feelings.
For those who don't know anything about me, I live on a secluded ranch in the Midwest. I 
work the ranch myself. As I'm not a pampered ivory-tower academic with no comprehension 
of honest physical labor, I owe no one any favors, least of all academics, aspiring 
academics, academic clients, or academic groupies. Instead, I sometimes try to do favors 
for others, regardless of their academic backgrounds. The CTMU is one of those favors. If 
you don't want to read it, or suspect that it is faulty, then don't read it and don’t talk about it. 
My work is out there for those who wish to read it, not for those who don't. As far as I'm 
concerned, those who don't want to read it don't deserve it, and will ultimately regret their 
ignorance regarding it.
While we're on the topic of academia, I'm more than willing to concede that some 
professional academics are intelligent, well-motivated, and worthy of moderate respect. On 
the other hand, it’s quite well known that I’m not an admirer of academic bureaucracy. Lord 
knows, I've tried to accept academia as the serene and impartial temple of knowledge and 
intellectual excellence that it has always claimed to be, and dearly wish that I could do so in 
good conscience. But unfortunately, after a good deal of honest reflection, I find such claims 
to be hollow.
Most of the world's political con artists, high-level thieves, war criminals, scientific whores, 

and half-baked social engineers are academically trained and certified within an inch of their 
misbegotten lives as a condition of their "achievements". Given that academia is in the 
indoctrination business (among other businesses), it is only natural to associate their 
behavior with their training. It follows that at this point in history, academia causes at least 
as many problems as it solves. (Would I do away with it? Heavens, no. Would I try to return 
it to its former state of grace? Of course I would - it used to be better, and it could be better 
again.) Any intellectual metric critically relying on academic achievement, or any kind of 
"achievement" explicitly or implicitly requiring academic certification, is therefore 
unacceptable. Such metrics are confounded by far too much excess baggage to be useful.
Regarding academic journals, had I wanted to publish in one of them, I'd have tried to do so 
long ago. The fact of the matter is that because I don't fully trust academia at its present 
stage of quasi-corporate degeneracy, or for that matter the judgments of random 
academics, I don't fully trust academic journals. Consequently, I've never submitted a paper 
to such a journal. I've considered revising that policy of late, but it would have nothing to do 
with earning the grudging respect of academic snobs. Furthermore, if I were to do so, and 
some academic snob were to inform me that this has finally proven that I might have 
something vaguely worthwhile to say, I would immediately know that nothing worthwhile 
could possibly issue from the mouth or the mind of the snob in question.
There's a lesson here. Believe it or not, like it or not, learning and competency can 
sometimes be achieved for oneself. Given a certain threshold of intelligence and the will to 
learn, one does not always need academia for that purpose, at least where precautionary 
licensure or access to expensive training equipment is not legitimately required. The 
smartest academics of all time have left their material in the public domain, and nobody 
need plant an expensive and protracted kiss on the academic blarney stone to obtain 
access to it. Similarly, it has never been shown that academia is necessary in order to 
teach, invent, or discover. Virtually all major fields of science and philosophy originated with 
non-academics; indeed, even academia itself was originated by people who started their 
careers as non-academics.
I hope this sheds a bit of light on my personal views regarding some of the issues that have 
been raised here.
(Re the Lila Squad, perhaps I'll browse around a bit and check it out.)
Thanks.
Comment #11:
That’s a reasonable response, Igor.
Every now and then, I casually look around for an academic journal which shows some 
indication of not having been buried under a steaming pile of academic politics, orthodoxy, 
and networking. If I find one, perhaps I’ll feed something into the system on an experimental 
basis.
Of course, my expectations are minimal. The likely result, if one of my submissions were 
published in an academically-controlled periodical, would be something like this:
1. Some number of academics will read it, of whom a small fraction will be interested in it.

2. Those who are interested will suddenly recall that citing even the most talented amateur 
(academically uncertified author) is far riskier to one’s academic reputation than simply 
treating his insight as public property and integrating it into their own work without citation, a 
virtually risk-free proposition given that anyone without academic credentials is unprotected 
by academic privilege, beneath the radar of scholarly ethics, and in a pinch, easily written 
off as a case of “parallel discovery”.
3. Somewhere down the line, I will be taken to task for claiming “their” work as my own 
while lacking the educational credentials "obviously" necessary to have produced it.
In short, not only are academic journals not the open, universally accessible records of fair 
and impartial research that academia cracks them up to be, but for any non-academic, 
contributing to them is certainly risky and probably thankless.
As I say, I may try it anyway, just so I can point out that I was right all along. In any event, 
more detailed explanations of my work will be published even if I have to do it myself ... in 
which case the terms and the timing will be all my own.
Thanks for your perspective.
Comment #12: 
Thanks for the kind words, Jeremy.
Aside from Mark, I doubt that any of the hostile or derisive commentators here have any 
credentials to speak of. That's why they're posting anonymously - they have no educational 
bona fides of the kind they'd like others to believe they have, and are afraid of being 
identified as pretentious know-nothings. That's also why nothing they say makes the least 
bit of sense. It can't be pinned on them, so they couldn’t care less whether or not it hangs 
together.
It may help to think of such people as pressurized vessels of anger and desperation who 
crave answers, but finding none, seek revenge upon the world for manifesting itself to them 
as a barren intellectual desert. Tortured souls trying to ease their inner pain, they have been 
so perfectly frustrated in their efforts that at any moment, they could rupture like distended 
gallbladders. (Having been told not to expect a response from me, this bunch apparently 
fabricated an unrecognizable effigy on which to lavish their excess bile.)
Now, Mark is another story entirely. Mark, who has a name, a life, and a job, needs to be 
more careful. This may explain why he’s finally gotten something right: I do indeed think that 
the CTMU is the best thing since the spoken word. Otherwise, I wouldn’t bother to defend it 
from people like Mark.
You know, I've always celebrated the presence of so many brilliant people out there. But at 
the same time, I’ve been dismayed by the fact that they're not usually the ones who are 
running things or doing most of the talking. Those tend to be moral imbeciles with (at best) 
danger-zone intelligence. Those in charge of society tend to be smarter than average, but 
not quite smart enough to recognize their own intellectual limitations and deflate their egos 
and job descriptions to scale. They tend to have other important qualifications like 
narcissism, ruthlessness, greed, deceptiveness, innate disregard for truth, limitless self-
entitlement, and the burning desire to become “high achievers” (e.g., successful thieves and 

depots). It's really quite a waste.
But the smart ones are out there in force. Even if one isn't lucky enough to bump into them 
every day, the world is positively brimming with their more worthwhile productions, and the 
pages of intellectual history are replete with their inspirational examples. Nobody who 
understands this can deny the intelligence of mankind, even though much of it has been 
self-cancelling and subject to diversion, prostitution, or enslavement. Aside from academic 
closure and self-importance, this is largely why we see little mention of the CTMU from 
academic sources. Those academics smart enough to meaningfully criticize at it are, by 
definition, too smart to think they can succeed. Recognizing controversy when they smell it, 
they shy away from the heat and avoid rocking the academic boat, making not so much as 
a peep out of turn. Those who hate the CTMU are thus doomed to a tedious cycle of 
alternately griping about it and vainly hoping that it will go away.
I can assure everyone of that persuasion, including several of the above commentators, that 
the CTMU won't be going away. As time passes, it will almost certainly grow more 
conspicuous. But although its eventual ascendancy will be good news for humanity, it will 
be a problem for Mark. Recalling this thread, people will see that even after Mark was 
crushed like an overripe banana, he continued to triumphantly wag his fanny along with his 
pseudonymous amigos and mischaracterize the CTMU as a "delusion". On the strength of 
that damning revelation, Mark's score for intelligence and intellectual integrity will come in 
so low that he'd have to drain the oceans to read the dipstick. Poor Mark could find himself 
ridiculed, despised, washed up.
I would sincerely like to help Mark avoid this unenviable fate. You see, I don't (yet) think that 
Mark is stupid or evil, but merely a bit on the manic side. Thus, when he works himself into 
an uncontrollable pseudomathematical frenzy and takes wild pot shots at intellectual targets 
with his full-auto math-powered ray gun, he neglects to ascertain that he’s not at the focus 
of a bright spherical reflector … until it’s too late, and the odor of singed hair tells him that 
this is exactly where he has planted himself. Once he awakens to this smoldering 
realization, he’s too smoked up to recant and apologize.
Mark would be well-advised to review this thread until he comes to understand how he has 
actually fared in it. Then he should issue a public apology for his nasty, misleading thread 
titles and other assorted nonsense and repent with all his heart. Perhaps this will let him 
escape what will otherwise be his destiny, and avoid being cut adrift from the rest of 
humanity, and perhaps from his Maker, like a leaky, creaking old tub not worth the cost of 
salvage.
For Mark’s own sake, he should at least give it some serious thought.
Comment #13:
 It appears that the confusion persists.
This site purports to be about mathematics. Mathematical knowledge is stratified; there is a 
crucial distinction to be made between low-level mathematical learning and mathematical 
understanding. Mastery of mathematical details is one thing; understanding and integrating 
the details across many structures and formalisms is quite another. One cannot call oneself 
a mathematician, or criticize other mathematicians, until one understands and mentally 

bridges the distinction. (Of course, insofar as true mathematical knowledge incorporates 
both detail and comprehension, distinguishing levels of mathematical knowledge can be a 
bit misleading; the phrase "low-level mathematical knowledge" is a bit oxymoronic, actually 
denoting the mere rote memorization of mathematical boilerplate and cookie-cutter 
applications and procedures.)
Because the CTMU is profoundly disturbing to atheists, and because platitudes and low-
level mathematical minutiae can be parroted without benefit of real mathematical 
understanding, the CTMU sometimes attracts critics who temporarily succeed in passing 
themselves off as "mathematicians". They usually do this by regurgitating a disjointed 
assortment of mathematical details and generalisms which they perceive as undermining 
the CTMU and supporting their cases. After all, mathematical learning is easily simulated for 
a mathematically unsophisticated audience; one simply harvests a few weighty-sounding 
mathematical snippets and platitudes from the web and then anonymously parrots them for 
the rubes, winking, nodding, and smirking as they roll fluidly off the tongue.
But let’s get back to reality. To a competent reader – and in these parts, there are fewer of 
those than one might suppose - most CTMU critics resemble obnoxious schoolchildren on 
the rampage, popping off about this or that awful grownup in a way reminiscent of the movie 
series "Children of the Corn", or perhaps the old Star Trek episode "Miri", the sci-fi tale of a 
mirror-Earth in which most of society has been wiped out by a deadly plague. To make a 
short story shorter, Miri features ragtag death-squads of prepubescent rug rats who, 
unafraid of spankings and unwilling to toe the line for anyone displaying any degree of 
mental and emotional maturity, occupy themselves with tracking down and liquidating any 
grownups - derisively referred to as "grups" - of whom they get wind.
Now, I'm as fond of kids as anyone else. But Children of the Corn are quite another matter, 
and I eventually get bored with childish drivel. Such drivel becomes interesting only when 
one asks the right questions about it. For example, how deceptive and convincing can an 
evil little Corn-Child be when anonymously trying to convince the world that he/she is a real 
"mathematician", and that he/she knows for a cold hard fact that the CTMU is "nonsense" 
and its author a “crank”? How stupid can most people be in gullibly swallowing the faux-
authoritative critiques of such deceitful brats hook, line, and sinker? From a psychological or 
sociological standpoint, such questions can be intriguing.
In this very thread, we see several commentators about whom such questions might 
reasonably be asked. Behold their unmistakable tone of authority, the sweeping breadth of 
their pronouncements, the sheer boldness of their pretensions to mathematical 
understanding! Smell their oozing contempt for “cranks” and “crackpots”! Look at the clever 
way in which they rationalize the lopsided advantage of anonymity by cutting off the ring on 
anyone who might object, announcing that they’re hiding behind a pseudonym not because 
they’re craven little nerds, but for purposes of "safety"! (In fairness, this rationalization was 
offered by just one critic in this thread; suffice it to say that where an ordinary CTMU critic 
can be accurately described as "insufferable", this link-happy little fellow can be described 
as "insufferable, cubed".) Joseph Goebbels himself displayed no greater mastery of the 
bluff.
Too bad that at the end of the day, all of this masterful rhetoric merely gives rise to a very 
pointed question: why do all of these commentators seem to fear having their pants yanked 
down around their spindly ankles in front of the entire mathematical and/or philosophical 

world? Why doesn’t one of them screw up his courage, take a deep breath, and come clean 
about his impressive academic credentials, positions, and affiliations ... something he 
should really want to get out of the way as soon as possible, inasmuch as until he does, 
he's indistinguishable from a loud-talking two-bit grifter trying to pull a fast one? After all, 
Mark did it - why can’t they?
It’s a mystery.
Comment 14: 
A fine post, Jeremy … in fact, one of several. It always warms my heart to learn that 
someone has actually been listening.
Unfortunately, several others in this thread are not nearly as good at listening as you are. In 
fact, they’re quite bad at it. So as tiresome as this is getting to be, I’ll repeat myself to them 
yet again.
If you are a CTMU critic who wants to argue about the CTMU with me, then it will have to be 
official; otherwise, you’re just an undistinguished malcontent bent on wasting more of my 
time. Therefore, if a Q&A session is what you desire, you must forthrightly share with us 
your complete real name, your academic employer and/or other affiliations, and your 
various qualifications.
Anonymous, fish-scented claims that you (e.g.) "work on Wall Street" and attended some 
overpriced, philosophically irrelevant business academy ("I am a 2390 SAT-toting Wharton 
graduate with dual degrees in mathematics and economics ... and I currently work on Wall 
St") won't do the trick. Only in conjunction with complete personal data can your information 
be checked and your comments permanently attached to your “expert” reputation, such as it 
may be.
Of course, you should have no problem with this. You'd be expected to provide your name 
on any scholarly paper you might publish, or for any speech or interview in which you might 
hold forth as an expert. Why not show that you have at least a scintilla of courage and 
honesty so that we can get to the important matter of establishing who the cranks and 
crackpots really are around here? After all, the CTMU critics in this thread are the ones who 
insist on using such epithets.
If you are on the faculty of a major college or university and thus the expert critic you 
pretend to be, the worst thing you have to fear on this blog is a standing ovation. If you turn 
out to be at all genuine, you'll soon be swaggering about just like one of the big boys … as 
opposed, for example, to a wannabe big boy who got too big for his little britches, popped 
his suspenders, and traded in the swagger for a comical pants-around-the-ankles waddle in 
his ankle garters and his Wall Street boxers with the cowboys and the dollar signs.
Above all, don’t worry about anyone hitching a free ride on your precious CV. As far as I’m 
concerned, the negative commentators in this thread are a parasitic drain on my time, an 
unfortunate sidetrack I encountered in attempting to limit the damage done over the last 
three-plus years by Mark. Obviously, dropping the name of a CTMU critic, eminent or not, 
could score points only with somebody who dislikes the CTMU, and I clearly have nothing to 
gain by that. I don’t bother with CV's, I seldom drop names, and no hurt intended, but I 
consider people who dislike the CTMU and its author to be anything but worthwhile 

associates.
Why miss this golden opportunity to put your Wall Street money where your gigantic mouth 
is, and reveal yourself to the world as the big shot you evidently think you are? If you’re as 
good as advertised, then your CV can only shine brighter at my expense, and more power 
to you. If, on the other hand, you turn out to be just a jealous little mediocrity, which I 
presently believe to be the case, then you’ll pay the price in humiliation and lost credibility, 
which is only fair when you habitually run your belligerent mouth at the expense of others.
Fair’s fair. Either come clean, or put a cork in it and obligingly crawl back under the rock that 
hides you.
Thanks for paying attention this time.
Comment #15:
 Rubix: "I'm not going to risk reputation when you have not proven that you are capable of 
meeting me halfway."
I'm terribly sorry, but you're just not important enough to demand that anyone who actually 
knows what he's talking about "meet you halfway", much less someone who has been 
featured by several major periodicals and news networks.
Let me provide you with a hint. Before anybody but another nobody like you can "meet you 
halfway", you must first get to the halfway point yourself. That is, you must show everyone 
that you're not just another disgruntled, name-dropping ("Neil deGrasse Tyson", "Richard 
Dawkins") gadfly with an ax to grind.
This has been repeatedly and clearly explained to you, so that even someone like you can 
understand it.
"I remind you, again, that one of my majors was *mathematics.*"
Very well, then (although one would never know it from looking at the nonsense you post). 
Let's have the information that will allow us to couple your complete real name with your 
"math degree" and other assorted qualifications and credentials.
"People have no problems revealing their credentials when it comes to an honest debate."
Then put your money where your indefatigable mouth is already, and let's see what kind of 
"Tyson" or "Dawkins" you really are.
Otherwise, you might as well stuff a sock in it.
(Remember to remove the ankle garter.)
Comment #16: 
"So are you going to actually do us the honor of a fair debate, or are you just going to call us 
unimportant idiots?"
I'm afraid it's got to be the latter, at least for you personally.

Of course, you could always come clean.
Comment #17: 
“I also think that Langan knows how much nonsense his words are, but he is building a 
foundation and you can see he has his followers. It's probably very profitable.”
Not exactly. But thanks for once again drawing our attention to the fact that when 
anonymous fools are running around tirelessly accusing one of self-promotion and 
profiteering no matter how tastefully one refrains, it really makes no sense not to go ahead 
and "take it to the limit".
Fortunately, it's not too let to put that engine in gear.
"Langan is pulling a coattails-of-Dawkins where his aim is to supplement his name with 
credential-laden opponents."
Well that's rich, isn't it.
I come here to rebut somebody who was using my name and my work to provide his own 
blog with content (Mark), and some cheeky little ignoramus hiding behind a pseudonym 
rushes out yapping like an enraged rat terrier about my alleged desire for a transfusion of 
credentials from semi-retired professional atheist Richard Dawkins! (While Richard’s 
intellect apparently remains sharp enough, militant atheism is now such a central part of his 
shtick that he’s no longer of much value to any non-atheist as a scholarly associate.)
One can’t be sure, but it almost seems as though the rat terrier in question, between windy 
gusts of boasting ("Wall Street", "Wharton", "2390 SAT score", etc.), is sneakily trying to 
associate himself with poor Professor Dawkins by endlessly repeating his name, pretending 
to speak on his behalf, and presuming to shield him from potential opponents who haven't 
so much as mentioned him except to quote the loquacious little terrier himself.
In the name of decency, the rat terrier in question should stop his shameless name-
dropping … especially since, in all likelihood, Richard Dawkins wants nothing to do with 
him. The probable truth of it is, Dawkins wouldn't let some pushy little nonentity of that 
insufferable ilk scrub his toilet if he showed up on the doorstep dressed as a sexy little 
chambermaid with a plunger dangling from his neck and a brand new copy of "The God 
Delusion" lovingly clutched to his bosom.
Neither, for that matter, would I. As we all know from painful experience, such malcontents 
never stop talking, only get more annoying the longer they ramble, and are nearly 
impossible to get rid of without a jumbo can of pepper spray.
Professor Dawkins had best beware. This is not a pair of lips he wants glued to his instep.
Comment #17: 
"Demanding credential-laden opponents is a tactic frequently seen in the ID community."
But wasn't it explicitly decided some time ago that big-name atheists, evolutionists, and ID 
critics should avoid "lending credence" to even the most prominent ID proponents by 
debating them in public?

Sure it was...it's all over the web. Check it out for yourself.
"In some other interview you claimed intelligence requires it, but this is demonstrably false."
That would no doubt be a very interesting "demonstration". ; )
Comment #18: 
At least prominent ID proponents are willing to debate prominent ID critics, even though the 
latter are unwilling to return the favor. So if either side should be accused of being afraid to 
debate the other (under their real names with their expert reputations duly at stake), it's ID 
critics and not ID proponents.
Credit where credit is due, and blame where blame is deserved.
"...evidence that everyone must be forced to accept."
"Forced"? An interesting choice of language.
But throughout the sciences, "accepting evidence" is not nearly as much of a problem as 
accepting what it is said to be evidence *of*.
Of course, one would have to know a little about the philosophy of science in order to make 
proper sense of that.
Comment #19: 
Anonymous: “I hasten to add that I am in no way affiliated with him and that some of the 
things I say may not even correspond to his views. However, I think that I am at least 
slightly better informed about the CTMU than many of its critics.”
I agree on both counts – the above evaluation of my ideas and utterances using the 
Wikipedia Crankometer is pretty accurate in most respects (with a couple of question 
marks, both of which would seem to work in my favor).
Regarding “Rubix”, let’s back it up a little.
Rubix: “By 'forced', I mean it's evidence that is true for you, true for me, and true for 
everyone.”
Replicated observation provides hard scientific evidence of perceptual content. But as soon 
as one calls it “evidence” of anything but its own existence, one is exceeding the bounds of 
observation alone. This is why one can *prove* absolutely nothing on an empirical 
(scientific) basis but the bare existence of one's raw data; one inevitably runs into the 
problem of induction. All that one can do is probabilistically confirm a given (explanatory or 
predictive) theory within certain bounds to a certain depth.
The depth criterion is where materialists typically fall short; their applications of Occam’s 
razor implicitly exclude entire levels of explanation including ontology. Materialists often 
claim that ontology is scientifically irrelevant on the supposed grounds that for scientific 
purposes, matter and nature “just exist”. But as soon as one gives up on explanatory 
closure and stops short of explaining existence itself, that’s the end of one’s explanation, 

and to the extent that science is explanatory, the end of one’s scientific support. One is left 
standing not on science, but on bald assumptions, and this obviously limits the level of 
theorization for which one is equipped.
Unfortunately for materialists, they have no idea how to endow their scientific “explanations” 
with the necessary degrees of closure and thereby overcome this limitation. As only the 
CTMU can do that, it is a necessary ingredient of science. (So is everything that the CTMU 
logically implies, including you-know-Who.)
I need merely add that specific questions about how the CTMU pulls this off, assuming that 
they have not already been answered elsewhere, are to be asked only by honest, qualified, 
and highly credentialed people. That way, everything's on the up and up, and I don't have to 
worry about accidental misattribution.
Rubix: “You imply that the same same of evidence can result in multiple interpretations, 
which is true. But typically we limit our explanations to what is sufficient and consistent, 
hence Occam's Razor. If my explanation correctly explains/predicts things, it will win out 
over another explanation that adds superfluous frameworks that add no new information 
(especially in absence of evidence). Again, it's like arguing that a hot kettle of water boiling 
on a stove could be "evidence" that fairies are involved. We don't have any reason to 
believe that, especially when we have other more consistent explanations that don't require 
such things.”
I hate to have to break this to you, but rare, hidden, or nonexistent aspects of reality, e.g. 
“fairies”, are not necessarily either syntactically or semantically inconsistent, and cannot be 
conclusively ruled out except in specific bounded contexts. The most that one can say for 
sure is that fairies are unobserved and/or observationally unreplicated within a bounded set 
of data, and no mere application of Occam’s razor can resolve the issue. After all, Occam’s 
razor is merely a fallible rule of thumb.
One of the main technical problems with Occam’s razor is that it can’t be applied in a 
vacuum, but only within a supporting explanatory framework. If that framework is itself 
flawed or incomplete, then Occam’s razor can be worse than useless. This is a problem for 
materialism; the existence of matter itself can’t be explained within a purely materialistic 
framework, and this nullifies Occam’s razor as a rationale for materialism.
Uncertainty cannot be confined to the ontological level of explanation without simply 
assuming that ontology is unnecessary and severing it from reality. To improve on this 
assumption, one would need some kind of high-level mathematical proof. But of course, 
such a proof is well beyond the intellectual capacity of any materialist (which is why some 
materialists regard it as impossible).
Rubix: “Same goes for Intelligent Design. We can explain evolution without it. We can 
explain complexity without it. You might point to QM as an example of "evidence with 
multiple interpretations," but the difference is that they remain agnostic due to lack of 
deeper evidence and understanding, and they're *falsifiable*, unlike ID.”
Wrong again. Evolution and complexity cannot be falsified. Not only is natural selection 
equivalent to the logical and biological fact that organisms must conform to their natural 
environments – no possible observation statement can negate so inevitable an observation 

- but falsification and the human minds which employ it themselves reflect evolution and 
emergence. Thus, any instance of falsification affirms the mechanisms to be “falsified”. (Of 
course, one is free to claim that falsification, like reality itself, “just exists”. But not only does 
this imply a truncated ontology devolving to unjustified assumptions, it simultaneously 
implies a kind of mathematical Platonism which contradicts the truncation.) Falsification can 
be consistently accomplished only with respect to specific hypotheses.
Rubix is also wrong that interpretations of quantum mechanics can necessarily be falsified. 
That’s because QM may have multiple models permitting multiple consistent interpretations, 
at least down to a certain explanatory depth. To obtain the deepest possible interpretation 
of QM, one would need a comprehensive theory of deep reality on which to form its image 
… deeper and more general than QM itself, at least as QM is normally understood (as a set 
of mathematical concepts and procedures that predict the results of certain experiments 
involving microscopic phenomena without fully explaining them or being explained by them, 
particularly with regard to wave function collapse, entanglement, and nonlocality).
See? This is what I mean. Nearly every sentence that comes out of the mouth of a CTMU 
critic, or for that matter an ID critic, is either wrongheaded or simply wrong. Just as we 
established above with Mark, there aren’t enough hours in the day to keep up with their 
mistakes and misconceptions.
That’s why I insist on making them identify themselves and put their reputations on the line. 
Believe me, it’s the only way to make them think a little before hitting the send key, and get 
them to catch at least a few of their own mistakes so that I won’t have to spend eternity 
doing it for them.
Now enough.
Comment #20:
 Andrew Goldstein: "I have a few questions for Chris, though, that came to me as I read 
through the CTMU. Your theory would imply that renormalization, technically, introduces a 
supertautological structure in the quantum Hilbert space recursively."
That's not quite correct. Obviously, the supertautological (recursive) structure of the CTMU 
incorporates the Hilbert space structure of quantum mechanics. This follows from the 
inherent self-duality of the theory. However, speaking in terms of quantum ontology, you 
don't want to put the cart before the horse.
Rather than saying that the CTMU "induces supertautological structure" on (preexisting) 
Hilbert space "recursively", it is better to say that it recursively determines the structure of 
Hilbert space itself through its structural linkage to "classical" spacetime, i.e., via the 
inherent self-duality of the supertautology (I assume that this is what you mean by 
"renormalization").
In other words, the relationship between classical and quantum mechanics, or macroscopic 
and microscopic reality, is what gets "induced" by the supertautology on itself, and the first-
order theory of quantum mechanics, which of course includes the structure of Hilbert space, 
is a partial (and strictly bounded) image of that "process". (The quotes are a reminder that 
the process in question is pre-mechanical or pre-physical; it occurs atemporally on the level 
of SCSPL self-organization, i.e., syntaxification.) In the CTMU, such ontological priority 

relationships are quite important, as implied by those CTMU papers you must have read.
Unfortunately, rendering your subsequent "explanation", i.e., your questions about the 
CTMU even partially intelligible (assuming that this is strictly possible in a CTMU context, 
which I won’t bother to check) would require a detailed interpretation of quantum mechanics 
in the CTMU. With all due respect to your level of expertise in physics (whatever it may be), 
that would be a time-consuming affair requiring extensive background knowledge, and 
therefore inappropriate to any general interest forum.
But thanks for your interest.
And now, as long as I've responded to Andrew here, I suppose that I might as well slog 
through an explanation of why evolution is unfalsifiable, which is probably of more interest 
to, and certainly more easily understood by, most of those who visit this forum.
First, I’ll issue the usual pro forma reminder to Rubix and his fellow travelers that CTMU 
debates are for people who are both real and qualified. I'm the author of that theory, and 
those are my conditions. They are the very same rules that have been in place throughout 
the scholarly world from the very beginning of the intellectual enterprise. I see no reason 
why they should be changed for the CTMU, and I consider it dishonest of anyone to even 
suggest it.
That being reiterated, Rubix is again missing the point regarding evolution.
Regarding your examples of how evolution might be falsified – e.g., rabbit fossils embedded 
in the wrong geologic strata, etc. - falsification requires an observation (statement) 
expressing a direct counterexample to a theory specific enough to be *directly* counter-
exemplified. If the observation statement merely presents some sort of "evidence" of a 
counterexample, then one must justify the choice of that particular interpretation of the 
evidence over all other possible (non-falsifying) interpretations, and one is back to the 
problem of induction.
The more one tries to limit the interpretation to make the evidence direct and 
incontrovertible, the more localized it becomes, until finally, evolution as a whole is seen to 
be quite immune to it. Instead of being accepted as a counterexample, the “evidence” is 
then dismissed as a local anomaly with its own localized explanation that supports, or at 
least does not contravene, the overall theory that it was supposed to have "falsified". The 
evolutionary hypothesis is simply too broad to affected one way or the other by localized 
evidence tightly bound to context. A sea-change in the evidence would be required to turn 
the adherents of evolutionary theory away from it, and this isn't going to happen. There's 
already too high a pile of "confirming" evidence that isn't going away.
You’ve proposed some falsificative scenarios. Here are those which actually have 
falsificative bearing (as creation in the generic sense does not preclude evolution, that 
scenario is omitted):
“[1] Finding fossils in the wrong areas (e.g. the famous Haldane quote, "Fossil rabbits in the 
Precambrian!") or [2] a fossil record that shows no change over time, or [3] finding that 
mutation doesn't pass on genetic material properly or doesn't operate in aggregate, …”
These are all virtual impossibilities. [3] is either a tautology (mutation in the course of 

replication is precisely the failure of proper replication), a malapropism (mutations are not 
what "pass on" genetic material, but merely what modify the genetic material that gets 
passed on), or a blatant counterfactual that violates the foundations of biology and is 
therefore not an option (mutations in reproductive DNA are indeed passed on by 
reproduction). In none of these cases can it falsify evolution.
[1] and [2], on the other hand, would require a metaphysical explanation; they would imply 
that something must have exempted certain living entities from the pressures and/or 
limitations of physical causation. Claiming that evolution is falsifiable on that basis is thus 
like claiming that the statement "physical objects have mass" is empirically falsifiable simply 
by finding a massless physical object. The problem is that a physical object must have 
mass (or equivalently, energy) in order for one to find it using a physical detector; thus, 
although it briefly passes as sensible to talk about finding a “massless physical object”, it is 
actually nonsense. Similarly, any “evolution-falsifying” fossil is clearly embedded in the 
physical environment, and absent spontaneous generation or a wormhole connecting 
different geologic eras (etc.), must by definition have adapted to that environment in order to 
proliferate and produce fossils.
Next, we have the problem that in the large, standard evolution can be regarded as 
something like a principle of indifference which is perfectly transparent to physical 
causation. For its specific causal explanations, it points to the "laws of nature", especially 
the laws of physics as presently conceived (which is where it gets its putative affinity for 
materialism and randomism). This renders its causal dimension, summarized as "evolution 
is a natural process," quite immune to falsification. Anything that it cannot specifically 
explain is simply attributed to the action of unspecified, even hitherto undiscovered, natural 
laws. This kind of open-ended naturalistic attribution is rife throughout the sciences, and can 
be justified only under an extended definition of “nature” accommodating the unknown.
But perhaps this isn’t convincing enough, so let's take a closer look. Since evolution is a 
composite thesis, one must specify which part of it is up for falsification. These parts include 
the existence of selective environmental pressure, adaptation to selective pressure by 
individual organisms as a condition of survival, and reproductive heritability of the adaptive 
traits of those organisms. One thus assumes either that all adaptive traits are initially 
present in some line or combination of lines, up to and including the very source of 
"common descent", or that they can arise “emergently” through random mutations within the 
genetic code.
The first two ingredients are clearly tautologically related. On both the individual and 
intergenerational scales, adaptation is defined as a viable response to selective pressure, 
and selective pressure is defined as that which requires adaptation for survival. Similarly, 
reproduction with heritability of traits is not only a well-observed empirical fact, but 
something without which neither we, nor any other species, would exist in a remotely stable 
form.
Let's put it another way. Organisms either adapt to their environments, or their 
environments kill them and thereby obstruct their breeding and ultimately arrest the 
continuation of their lines (or in the language of the modern synthesis, the spread of their 
genes). It comes down to a very simple and incontrovertible fact: organisms, being 
physically embedded in their environments, are critically causally dependent on their 
environments, and when they change (evolve), they must do so in accommodation thereto. 

This means that the definitions of “selection” and “adaptation” are tautologically linked via 
the real, logically inevitable organism-environment relationships on which they are 
semantically based.
[Elsewhere, you write that "natural selection isn't an unfalsifiable tautology. Survival of the 
fittest is a gross oversimplification meant to dismiss evolution as a mere notion of ‘survivors 
surviving.’ Scientists don't even like using that definition precisely because it's an 
incomplete view.” Nevertheless, specific definitions of "fitness" and "adaptation" are 
necessarily coupled with survival - otherwise, fitness is obviously out of the question - and 
the overall structure of the theory of evolution is invariant with respect to them. Specific 
instances wander hither and yon, and specific explanations wax and wane, but the theory of 
evolution rolls ever onward.]
What if we now add a fourth component to the evolutionary hypothesis, namely, the 
stipulation that one species can arise from another by mutation, or from some combination 
of others by hybridization or some other form of gene-mixing? Evolutionary speciation 
comes close to being an "independent axiom" of evolutionary theory akin to the parallel 
postulate of Euclidean geometry; it cannot be derived from natural selection, adaptation, 
and mutable reproduction alone. Suffice it to say that it comes to rest on the details of the 
genotype-phenotype mapping, which are still rather poorly understood. It would therefore be 
difficult if not impossible to falsify.
In case all of this still leaves you cold, all that you really need remember is the bottom line, 
which you have already (in effect) acknowledged: in logic, falsification is really just inverse 
verification, and because empirical verification is impossible for explanatory and predictive 
scientific theories, so is empirical falsification. Both are out of the question for anything 
exceeding the bare data, and are thus limited to purely descriptive theories. Definite (as 
opposed to statistical or inductive) falsification is primarily for descriptive empirical theories 
only, via which it can secondarily preclude explanatory or predictive empirical theories 
based on false or irrelevant data. Aside from that, only in the non-empirical, logico-
mathematical realm can we employ the axiomatic method or metamathematical reasoning 
to effect falsification by disproof (derivation of the negation).
What does this mean? To falsify evolution under present circumstances, one would have to 
produce, in realtime, an organism that is able to survive independently of its environment, 
without any kind of adaptation whatsoever. But "an organism independent of its 
environment" is an oxymoron, because organisms are all physically embedded in their 
environments and therefore causally dependent on them. To deny this is to imply that 
physical embedment is somehow not what it seems, and that the "organism" in question is 
in such complete and intimate control of its environment that it need no longer go through 
the motions of behavioral adaptation, wasting no effort on eating, breathing, or eliminating. 
Such an organism, being so perfectly adapted to its environment that its survival is no 
longer dependent on the environment in any way, exists entirely on its own terms; its 
environment is a virtual extension of it and thus integral to it. In short, it is literally one with 
its world.
But of course, if we were ever to find such an "organism", we would regard it as more than a 
mere organism, and rightly so. This, of course, would take it out of the biological realm as it 
is currently understood, in which case biological evolution could hardly be falsified by it. 
Such a being would also very likely be credited with having effected the most profound and 

transcendent evolutionary adaptation of all time: taking complete control of its own 
evolution, or as it were, its own stasis. In this, it would reflect the theological aspect of the 
CTMU.
That’s really the final word on the supposed falsifiability of evolution, at least as far as logic 
is concerned, and why I've been on record as accepting evolution from the start. I merely 
hold that for various reasons, evolution is an incomplete explanation of what it purports to 
explain, and that God and divine creativity must be added to it in order to complete the 
explanation (stock "theistic evolution" is not implied; I have my own unique, and uniquely 
correct, perspective). Therefore, those who claim that I reject any form of evolution are 
boldfaced liars, and any commentator who fails to understand this needs to wrap his mind 
around it as quickly as possible.
This, by the way, is how I know that whenever I'm attacked by militant evolutionists, I'm 
really dealing with atheists deceptively trying to package their godlessness in a tautological 
wrapper and hitch it to a scientific locomotive that is moving forward as inexorably as a 
juggernaut. Such people always yearn for a good "debate", meaning something like a pack 
of ravenous velociraptors attacking, dismembering, and gobbling up anything that looks 
vaguely non-atheistic or non-materialistic in any way (their “natural prey”, to push the 
metaphor). It's in their very bones! So they gravitate to any mention of the CTMU like flies to 
honey, hoping for a free chance to publicly insult the intelligence of its author with total 
impunity even though he would likely crush them like empty aluminum beer cans in any 
ideologically neutral non-pseudonymous confrontation.
But sad to say, their Great Leaders, fearing to lose their illustrious seat cushions in honest 
debate, have now in effect forbidden them to "lend credence" to anyone who appears to 
embrace something other than random materialism as an explanation for the natural world 
by honestly debating them out in the open. Thus, militant atheistic evolutionists are reduced 
to obediently parroting the party line under pseudonyms, so that they cannot be accused by 
their superiors of breaking ranks (or, perhaps more importantly, so that they can’t be pinned 
down by name when they lose). What a quandary, especially when they injudiciously fixate 
on a target who insists on the use of real identities!
But in the end, it's your quandary and not mine. If you can't come clean about who you 
really are, then maybe it's time to roll out your hero Dawkins after all. At least he's not afraid 
to attach his real identity to his opinions. And if the old warhorse is simply too busy or too 
important to be trotted out and put through his paces by so “uneducated” a horseman as I, 
or simply doesn't want to "lend credence" to those who disagree with him, then at least he 
can be pilloried by name for refusing to engage well-known and verifiably intelligent people 
who just might hand him his rump on a platter. (Of course, you’re free to doubt such an 
outcome. But sadly for you, there's only one way to know for sure whether you’re right, and 
your side clearly isn't up to it.)
So much for your evolutionary (or atheistic-materialistic) nonsense. Now let’s move on to 
the rest of your nonsense.
First, Occam's razor has nothing whatsoever to do with certainty. Your sentence
"It keeps explanations to their bare minimum elements of certainty"

is thus meaningless. On one hand you have certainty, which is limited to the bare data (no 
explanatory or predictive theorization allowed), while on the other, you have a rather dull 
tool (Occam’s razor) which simply minimizes the number of concepts and variables in 
theories.
In order to apply Occam's razor with certainty, you'd have to apply it to the bare data, in 
which case it would merely tell you not to fabricate extra data or introduce irrelevant data 
without telling you the first thing about how to distinguish necessity from superfluity, 
relevance from irrelevance, or correct from incorrect explanation or prediction. It says 
nothing about explanatory or predictive sufficiency except that this alone, with no extra 
padding, is "all that you want" (but good luck finding it). Furthermore, it cannot anticipate 
anomalies, or future data, or help to identify any hidden variables that might render a theory 
more robust as the context expands and new kinds of data emerge. Its entire mathematical 
basis is simply that more concepts create more possibilities, and that absent any sort of 
“evidence” (no help there either), these additional possibilities are statistically more likely to 
be erroneous or irrelevant than correct.
Regarding Newcomb's Paradox, you write
"The real way to resolve the paradox is through rational decision theory. The paradox only 
exists if we frame it in a paradoxical way. It's like Zeno's Paradox, which isn't a real paradox 
but a framing of a problem that tries to circumvent calculus by invoking the same confusions 
about infinity that people experience in the 0.999... = 1 argument."
The paradoxical formation of the Newcomb scenario, of which you seem to dispute the 
existence, is usually credited to Robert Nozick, then of Harvard University, and based on 
rational decision theory. Thus, when you deny that Newcomb's paradox is actually a 
paradox, you're disagreeing with the late Professor Nozick. Accordingly, I suggest that you 
track down Professor Nozick, PhD, R.I.P., at the cemetery where he’s buried, 
pseudonymously insult him as is your wont, and then count the number of responses you 
get from him.
But since you'll almost certainly get zip from poor Nozick, and rightly so, I'll go ahead and 
tell you what he would probably tell you if he were so-disposed (which he wouldn’t be, 
because reasoning with you is obviously a perfect impossibility).
First, you need to get straight with the following: the fact that a paradox is resolvable, or has 
apparently been resolved, doesn't mean that it “wasn't a real paradox”. A paradox is a 
formalistic notion that can be thought of as equivalent to “X = not-X”, and as a property of a 
formalism or its interpretation, it is a logical function of the formalism-interpretation pair. In 
fact, even when (incorrectly) extended into the universe of the formalism, it remains a 
paradox. Moreover, the paradox has not been definitely removed from its originating 
formalism through revision or extension until the revision or extension in question has been 
shown to minutely resolve all possible forms of it.
Obviously, calling something the "resolution" of a paradox is not meaningful if it contains 
paradoxes of its own, as this merely transforms one set of paradoxes into another. For 
example, standard analysis incorporates the so-called paradox of infinitesimals, which are 
somewhat oxymoronically defined as nonzero but sub-finite (absolutely indetectable) 
quantities. Some people hold that this paradox has been adequately resolved; others 

disagree. Various efforts have been made to resolve it, a couple of the more famous being 
the traditional Cauchy-Weierstrass epsilon-delta formalism and nonstandard analysis. To 
my knowledge, none has succeeded to everyone's complete satisfaction; paradoxes or 
loose ends of one sort or another keep popping out. Thus, claiming that Zeno's paradoxes 
were never “real paradoxes” because Newcomb and Leibniz discovered calculus a couple 
of millennia down the line doesn't wash. Yes, the calculus definitely works, but the question 
of “why?” is still open to study.
As for "rational decision theory", that was part of Robert Nozick's original argument, and 
was explicitly included in both the original paper "The Resolution of Newcomb's Paradox" 
[Langan 1989] and the essay on that topic which I included in "The Art of Knowing" a 
decade or so later. Nozick's arguments were correct as far as they went, and he was also 
correct in noting the apparent existence of opposite but equally "rational" solutions to the 
central decision-theoretic problem. According to decision theory, it is “rational” neither to 
turn down money offered without risk nor to ignore objective data, and Nozick described 
how these aspects of rationality are placed in apparent conflict by the Newcomb scenario. 
In other words, Nozick showed that the scenario generates a decision-theoretic paradox 
within the standard causal framework applied throughout the sciences.
This was merely the starting point for my original 1989 paper, which extended the ordinary 
causal framework of the paradox in what was probably a new way. That is, it applied a 
novel technique in philosophy, situating a well-known philosophical problem (Newcomb’s 
paradox) in multiple levels of a stratified "hyper-automaton" running nested simulations. I 
called this the "nested simulation tableau" (NeST). If that was actually the first time this 
technique was used - and I suspect that it was - then it was a philosophical milestone. 
Although that paper was published in a low-circulation nonacademic journal, it may well 
have been the first example of the modern strain of cyberphilosophy called “Matrix 
philosophy". (Matrix philosophy, dearly beloved of sophisticated geeks and transhumanists 
everywhere, was developed well after the movie "The Matrix", which came out around a 
decade after my paper; the academics who now “own” the field not only didn’t invent it, but 
have since made quite a hash out of it.) The whole exercise was a specialized application of 
the CTMU, in which the paradox requires conceptual embedment. My later essay was 
incorporated in a string of essays in which it was partially integrated with modern research 
on the neuropsychology of volition.
But of course, all of this will be completely lost on you. Why? Because some people can’t 
tell their colon from a skunk burrow, and for the life of me, you often look like you might be 
one of them. Therefore, I dedicate the information contained in this response not to you, but 
to the brave contributors who continue to attempt to converse with you as though every 
word out of your mouth isn't a veritable fertilizer bomb of errors waiting to explode in a 
chaotic blizzard of fizzling conceptual confetti. (As for me, I’ve about had it with you.)
Now please stop wasting my time. Like our host Mark Chu-Carroll, you generate errors 
faster than you write … maybe even faster than Mark. But unlike Mark, you lack the 
courage to share your real identity with those whom you attack, and as you surely know 
deep down inside, that makes you … well, I naturally hesitate to say “a contemptible, belly-
crawling gutless wonder completely unworthy of respect or consideration of any kind, 
despite any phony rationale you may append to your intellectual cowardice”. So instead, I’ll 
leave it up to you and other readers to decide how well that shoe may fit.

But in any case, I hope I’ve made myself clear. God will be coming again soon to a theater 
near you, and no self-preserving child of the corn wants to get caught with his ankle garters 
and Wall Street boxers showing. Please consider yourself and your friends to have been 
warned, this time on a very logical, very precise basis on which not a single one of you will 
be getting over no matter how hard you try, full stop.
And now, having allowed this response to eat up half my morning - it’s a very rainy day here 
in the American Heartland - I must bid you all adieu.
Comment #21: 
Good Lord Almighty.
Rubix: “I won't even address how funny it is that Chris intentionally avoids answering 
Andrew's questions because he doesn't understand quantum mechanics, himself.”
For those who may be unaware of it, “understanding quantum mechanics” is a tall order 
which few if any can rightfully fill. (If Andrew could fill it, then he’d obviously be able to 
answer his own quantum-mechanical questions.) Even the people who discovered the field 
didn't claim to understand it; to this day, nobody quite knows how its founders managed to 
discover it.
As long as we're on the general topic, I may as well mention that quantum mechanics has 
no satisfactory or for that matter ontologically sound interpretation *outside* the CTMU, 
something that could not be said if SCSPL were “just a set”. But again, that’s involved 
enough to be well over the heads of most readers ... and as I’ve already explained, this 
forum is anything but an appropriate venue for such an explanation.
Sadly, there will be no public discussion, by me at least, of certain technical details of the 
CTMU until I’ve located or provided what I consider to be the right venue. For what it’s 
worth, I don’t expect this to be at all easy, especially with so many atheistic, materialistic 
forum-maggots crawling around deceitfully spewing disinformation and defaming any 
person or idea which runs afoul of their idiotic mindset.
Thus, I’m afraid that the cognitively impecunious Rubix, a self-styled Wall Street hero who 
never seems to have any intellectual cash to put where his alligator mouth happens to be, 
will have to prove his own points for a change. It’s simply not enough to overheat his motor 
mouth about the ideas of others when his comprehension dips so far into the red.
Rubix: “Sound ridiculous? So does your argument against evolution.”
I’m sorry, but I really can’t make sense of anything Rubix has written on this subject, 
including his droll bedtime story (in which I find myself and my ideas impossible to 
recognize). Perhaps Rubix should try using some mathematics, or some physics, or maybe 
just some logic. (I know he likes to accuse others of knowing nothing about these subjects, 
but that's a sure sign of envy and insecurity.)
Absent that, perhaps Rubix should go ahead and roll out his champion Richard Dawkins. 
Unlike Rubix, Dawkins is a prominent atheistic evolutionist to whom people actually pay a 
bit of attention. At least he wouldn’t make as many stupid mistakes, or cling to them like 
such a tick … that is, until he reached a level of reasoning above the ceiling of his exacting 

but theologically vacuous intellectual frame.
Rubix: “You don't seem to understand the difference between a tautology and a definition.”
Maybe that’s because sometimes, there isn’t any such difference. After all, propositional 
tautologies and their symbolic ingredients have mathematical definitions, and from a 
metalogical perspective, might even be described in themselves as “primitive definitions” 
which, though cognitively inevitable, collectively define truth in terms of logic functors and 
vice versa (via truth tables). Other kinds of definition are linked in this way as well.
I’ll come back to this below.
Rubix: “Even if you accept evolution, I'm not attacking you for that. I'm attacking you for 
invoking ID and trying to use crackpot pseudo-math and cranky physics to justify it.”
I recall mentioning ID in connection with ID *critics* like Rubix, but so help me, I can’t 
remember having relied on it. I also can’t recall using any “crackpot math and physics” in 
justification of it. Sadly, this makes me wonder anew whether Rubix is a pathological liar, an 
hysterical fruitcake, or a cynical shill holding forth on behalf of someone or something else.
Rubix: “Your argument against Occam's Razor is also offbase and I won't spend much time 
on it. It's not supposed to help you find hidden variables -- but it's not preventing you from 
looking for them. It's meant to keep hidden variable explanations out of the realm of "truth" 
until we have evidence for said variables. Otherwise, by definition, you are jumping to 
conclusions that may turn out to be wrong once the evidence rolls in, and you may be 
tacking on frameworks that actually explain none of the variance inherent in the results. 
That's all there is to it.”
Exactly as I said, at least in part. I can’t say I’m not pleased to see that there is at least one 
thing on which Rubix understands that he’s been cornered like an outhouse rat. 
(Unfortunately, the admission merely prompts him to claim that this is what he really meant 
all along. C’est la vie.)
Rubix: “Newcomb's Paradox is only a ‘paradox’ because it's framed in an unsound way.”
So it seems that Robert Nozick, PhD, an eminent academic philosopher late of Harvard 
University, along with Martin Gardner of Scientific American, were nothing but cranks and 
frauds who saw paradox where Rubix swears there was none. Too bad the great Rubix 
wasn’t there to clue them in!
Clearly, Rubix needs to quit his day job on “Wall Street”, get busy with something other than 
licensed thievery for a change, and enlighten the world about all of this as quickly as 
possible.
Rubix: “Also, leave it to a crank philosopher to take credit for things unjustly. You're aware 
that ideas for the Matrix philosophy have long predated you, right? Forget Nozick's 
Experience Machine or Plato's Allegory of the Cave or even the Buddhist Maya or 
Zhuangzi's Butterfly Dream or Descartes' Demon. Hell, forget Tron, which came out 7 years 
prior, or the Lawnmower Man (which began pre-production in 1988). Obviously your 
important paper which used the CTMU in 1989 was the first example of the Matrix 
philosophy. Give me a break, man.”

I hate to belabor the obvious, especially to such a preeminent philosophical genius. But 
there’s a very clear difference between Platonic philosophy (the Allegory of the Cave), 
cyberphilosophy (e.g., the Turing test, human intelligence, and consciousness), and Matrix 
philosophy (so-named because it was inspired by the movie of that title). There’s a common 
element, but no competent intellect would so thoughtlessly mangle the distinctions … e.g., 
whether computers and computation are explicitly involved, how the problem is mapped into 
the computational realm, whether there's a computational world-simulation (“Matrix 
philosophy”), and so on.
However, that’s almost beside the point, given that Rubix is evidently having a great deal of 
difficulty telling the difference between written philosophy and the fluffy, frothy Hollywood 
movies from which he actually seems to have gotten most of his mathematical and 
philosophical “knowledge”. (Does anyone else wonder if this might be saying something 
about the philosophical expertise of “Rubix of Wall Street”, celebrated latter-day nemesis of 
Zeno of Elea?)
Now, I certainly don’t want to give the wrong impression. There could very well be a prior 
paper out there which uses roughly the same technique as I used in “The Resolution of 
Newcomb’s Paradox” / 1989. It’s just that out here in the sticks, far away from the teeming 
intellectual bazaar overspreading the splendiferous ivory tower complex of Greater 
Academistan, I’ve never heard a whisper of it. Perhaps, if we’re lucky, Rubix can set the 
record straight by putting us onto this philosophical milestone.
But if not, then perhaps Rubix will soon be transferring credit for the Apollo space program 
from those self-aggrandizing goldbricks Eisenhower and Kennedy to the Tinsel Town 
scriptwriter who adapted Jules Verne’s “From the Earth to the Moon” for the silver screen.
Rubix: “I should add that your comments on Zeno's Paradox further shows your 
mathematical ignorance. We fully understand the ‘why’ behind the answer.”
Really! Then Rubix needs to either prove it once and for all, or refer us to the papers in 
which it is proven or the comprehensive anthologies in which it is decided. (By the way, who 
on earth is “we”? It is important, when one assumes an imperious tone of intellectual 
authority, to let everyone know for exactly whom one claims to be speaking.)
“Zeno’s paradox is not a real paradox,” indeed. What a rollicking barrel full of slap-happy 
tail-pulling Wall Street monkeys!
Rubix: “You also overemphasize your importance and influence … The crankometer is 
screaming.”
How typical. First, Rubix repeatedly swoops out of a muddy sky with unprovoked attacks on 
something he claims I wrote about Newcomb’s paradox, but which I don’t recognize. His 
remarks are characteristically disordered and weirdly off-topic, so I don’t respond. But as 
ever, he mindlessly persists, tossing off scrap after snippet of total nonsense which betrays 
his complete ignorance of the paradox and my analysis of it (I still don’t know what it is of 
mine that he actually read, if anything).
So finally, in a merciful attempt to correct his bizarre misconceptions, I write a few words 
about my first paper on the topic. And what’s the response? In sweet revenge for my brutal 
attempt to shoehorn a few hated and feared bits of real information into his empty skull, he 

sanctimoniously accuses me of “overemphasizing my importance and influence”!
It’s a bit like accusing someone who throws a lifeline to a drowning victim of “assault with a 
rope”, or someone who brushes away a biting fly of “extreme cruelty to animals”. In 
RubixWorld, this is what passes for constructive dialogue.
Yes, no doubt about it - the crankometer is screaming bloody murder. But in this case, it is 
screaming the pseudonyms of zonked-out atheist-materialist fanatics calling themselves by 
absurd nicknames like “Rubix” and “John Fringe” … who, unfortunately, can’t reason their 
way out of a wet paper bag together, but lack the self-control to tape their own sleeptalking 
mouths shut in order to avoid embarrassing themselves.
Speaking of “John Fringe”:
John Fringe: "So evolution is not falsifiable because the facts known after the theory was 
proposed (like all the genetic stuff) are in agreement with the theory?"
First, one doesn't need "all the genetic stuff" in order to parse the theory of evolution into its 
general components. Darwin formulated the theory of evolution without modern genetics 
while knowing full well that traits are passed from parent to offspring. Plants, insects, fish, 
reptiles, birds, mammals, even bacteria ... all have offspring that resemble the parents. 
That's what "reproduction" means; it’s all about the inheritance of traits, in contexts from 
biology to copying machines. (For someone who believes in dictionaries, Fringe doesn't 
seem to consult one very often. May I suggest that he try an *English* dictionary?)
More generally: the meaning of what I wrote about the unfalsifiability of Darwinian evolution, 
if Fringe actually bothered to read any of it before putting nimble monkey fingers to sweat-
sodden keyboard, is that the basic relationship between the conceptual components of the 
theory of evolution could have been logically deduced by a sufficiently adept theorist.
With all due respect, it is possible that Darwin, while surely no stranger to genius, may not 
have been that adept a theorist, and this may be why he was compelled to board the H.M.S. 
Beagle, sail to the Galapagos Islands, and painstakingly record his observations. But then 
again, Darwin’s voyage of discovery was probably a good idea even if he already 
understood the tautological nature of his thesis. After all, presenting a theory along with an 
application or two, not to mention a bit of adventure, makes it easier for most people to 
understand and can ease its acceptance. It’s a matter of salesmanship.
But one way or the other, one of the more unfortunate consequences of Darwin’s approach 
– among its many strong points, of course - is that today, most biologists are under the 
erroneous impression that evolution as a whole is falsifiable when in fact, it is not. Specific 
applications of evolution are falsifiable; there's no doubt about that. But the theory as a 
whole is unfalsifiable for exactly the reasons I gave, end of story.
Does this mean that the theory of evolution is "not science"? Of course not. In fact, the 
theory of evolution, being a tautology, is so broad that it spans science in its entirety, which 
is why we see applications of it popping up all over the place, from biology to computer 
science to cosmology. Tautologies apply to all kinds of science, not just those in which they 
were first employed.
What kind of tautology are we talking about? Again, it has to do with the physical 

embedment of an organism in its environment, on which its survival and reproduction thus 
critically depend. This is a “tautology” because there’s no way out of it; it is always true by 
the nature of the organism-environment relationship. While it is obviously not a propositional 
tautology of sentential logic, it is nevertheless a tautological formation of language, namely, 
a pair of terms – “organism” and “environment”, or “adaptation” and “selection”, or 
“evolution” and “nature”, pick any or all – which are recursively defined on each other, or 
equivalently, on the relationship between their definitions.
Of course, Fringe and Rubix will claim that this is not a tautology at all. But as long as 
Fringe believes so strongly in the use of dictionaries, why not have a look in the dictionary 
right now to see what it has to say about the word “tautology”? After all, if Fringe and Rubix 
were to consult the dictionary more often, perhaps they wouldn’t always leave everybody 
with the distinct impression that their thoughts are as jumbled and odiferous as the contents 
of a restaurant dumpster.
Remember how I referred Mark to the Wikipedia definition of “set”? Here’s what Wikipedia 
has to say about tautology: “Tautology may refer to: (1) Tautology (rhetoric), using different 
words to say the same thing even if the repetition does not provide clarity. [That’s a 
somewhat clumsy reference to generic linguistic redundancy which is not confined to 
rhetoric.] (2) Tautology also means a series of self-reinforcing statements that cannot be 
disproved because the statements depend on the assumption that they are already correct. 
[Pay close attention to this one, as it is the kind of tautology that describes the overall theory 
of evolution; "disproved" corresponds to "falsified".] (3) Tautology (logic), a technical notion 
in formal logic, universal unconditioned truth, always valid.” [That’s propositional tautology, 
to which the other kinds are clearly related.]
(Incidentally, I recall seeing that Mark has a rant on tautology posted to this site. Why 
doesn’t Fringe see if The Great Man himself will argue his side of the case? As matters now 
stand, it couldn’t possibly hurt.)
This brings us to the place where petulant airheads like Fringe and Rubix so often go 
completely overboard: they simultaneously subscribe to a total break between science and 
the tautological basis of veracity, and an absolute linkage between science and falsifiability. 
But really now, how twisted around can something get? As verification and falsification 
(verification of a negative) are just flip sides of the same logical coin, this is totally asinine, 
even if occasionally taught in universities by highly credentialed logical illiterates.
Science can be understood only with full allowance for the nature of tautology and the limits 
of falsification, and Fringe and Rubix clearly understand neither. They believe that the 
content of science is never tautological, yet always falsifiable; they exclude the basis of truth 
from science, yet include the possibility of falsehood. This is consistent only insofar as once 
one foolishly purges science of any trace of universal truth (i.e., tautology), one had *better* 
allow falsification, as it’s all that one can possibly hope for! But when the light finally comes 
on, it turns out to be inconsistent with logic and the spirit of science, according to which truth 
is really what it’s all about.
Of course, to those steeped in the ongoing Internet pogrom against the enemies of militant 
evolutionism, this may all sound too facile. How could it possibly be so easy to get the better 
of all those highly educated biologists and other scientists out there ... the ones who are 
always claiming that the theory of evolution as a whole can be falsified just like everything 

else in science?
It's really very simple. First, it’s quite true that specific applications of evolution are often 
falsifiable even though this turns out not to extend to the overall theory of evolution. But 
despite the widespread belief that scientists alone should have the last word on evolution, 
there are surprisingly many amateurs out there who seem to have better comprehension of 
its overall structure than the average scientist. Most scientists can recite the scientific 
method from start to finish, but are woefully unequipped to analyze and logically reduce it, 
just as (e.g.) an expert auto mechanic is unequipped to design or repair a catalytic 
converter or an engine computer. (I hate to sound disrespectful, but when it comes to this 
topic, give me a scientist or an academic of any kind, and unless he knows more than he 
picked up in science class alone, I'll hand you back his posterior seasoned with teriyaki and 
ginger.)
Last but not least - as I believe I’ve mentioned elsewhere, “pseudonymous CTMU critic” is 
virtually synonymous with “abject brainless piss ant” (no offense to anyone, but that’s been 
my experience over the last couple of decades, and I strongly believe in heeding past 
experience). Yet the nickname “John Fringe”, assuming that’s what it is, plainly evokes a 
conflation of the TV show "Fringe" and the actor (John Noble) who plays its central 
character, a brilliant and courageous if mentally skewed scientist.
Except for the skewed mentation, such a nickname doesn’t seem a good fit for our own Mr. 
Fringe. So why doesn’t he choose a new one more befitting his image?
Or perhaps he could simply grow a pair, drag himself in from the cold, and come clean 
along with his undercover partner in crime, Rubix.
ATTENTION Mark CC: You really need to stand up and take a little responsibility for what 
goes on in your own forum. On the heels of our exchange regarding your CTMU critique, 
these participants are making a fool of you. My sincere advice: exercise a little damage 
control, and take charge of your own side of the discussion you started at the top of this 
page.
After all, it’s in your own best interests. When somebody owns a house, and he knowingly 
lets it be used as ground zero for thieves, robbers, con men, and other assorted criminals 
and ne’er-do-wells, he must take legal and moral responsibility for his role as their facilitator. 
He may not get caught in the early stages of the operation, but in the end, he finds it 
impossible to hide.
Intellectually speaking, your present situation is directly analogous. Since you’ll ultimately 
have to take responsibility for enabling the brain-dead but nonetheless despicable antics of 
these imbeciles on your behalf, why not take the bull by the horns right here and now, and 
start fighting your own battles? After all, this isn’t about some dispute that randomly arose in 
your comments section; this is a dispute that you started several years ago, right up there at 
the top of the page!
I’m just trying to be helpful. These people you’ve got arguing for you are a disgrace, and 
wasting time on such gremlins is not what passes for intellectual stimulation in my neck of 
the woods. You might think that they’re shielding you, but the shield is as soft and full of 
holes as a slice of Gruyere, and you may one day be forced to wear it like a moldy 

breastplate.
Wouldn’t you rather do yourself proud, and sally forth to cover your own side of the debate?
Comment #22:
Thanks for the suggestion, flaneur. I do indeed have a global system of ethics that 
reconciles all levels of utility including the corporate level. Never fear, it's in the pipeline.
And now, I regret that must once again put my feet on a hard place, and my shoulder to the 
Sisyphean rock that is "Good Math, Bad Math".
Mark Chu-Carroll: “This shouldn't be hard for someone with an awesome intellect like yours 
to comprehend. But I'll go ahead and explain it to you anyway. No one here is arguing for 
me. The people who continue to argue with you are arguing for themselves, because they 
find it interesting or entertaining. As long as they continue to find it worthwhile, they're 
perfectly welcome to continue to do it.”
So you say. But of course, when people rely on pseudonyms, they could be anyone … even 
you.
Mark Chu-Carroll: “No one is shielding me. If I felt the need to defend myself, I would.”
Trust me, you have a need to defend yourself. I’ve caught you in all kinds of mathematical 
errors, several of them quite serious. I could catch you in many more. If I wished to do so, I 
could fuel an entire blog with your errors. (Fortunately for you, I probably won’t.)
Mark Chu-Carroll: “Personally, I just think that you're a waste of time. Why?”
That’s alright. You don’t have to tell me why. But something tells me that here it comes 
anyway.
Mark Chu-Carroll: “Most importantly, you're an obvious, dreadful, boring crank.”
A "crank"? Whatever you do, Mark, don’t drop that mirror – it’s seven years of bad luck.
I’m terribly sorry that you find me boring. It’s just that every time you’ve opened your mouth, 
errors have leaked out … billowing, gas-giant errors that positively cry out for correction. I 
find it fascinating that someone intelligent enough to possess your (reputed) programming 
skill set (there's that hated word again) is so deficient in metacognitive presence that he 
can’t catch a single one of them for the life of him.
Mark Chu-Carroll: “I lost interest in this discussion when your dodging on the definition of 
sets became so obvious. You're playing such a shallow, foolish game. Your "theory" relies 
on a supposed contradiction. But that contradiction is a classic example of the self-inclusion 
paradoxes that plague naive set theory.”
That’s a very interesting thesis. The problem with it is that while my work has never relied 
on naïve set theory in any way, the only contradiction you claim to have found in it stems 
from … naïve set theory! More puzzling still, no matter how many times this is pointed out to 
you, you can’t seem to wrap your mind around it.

Once I’d examined your anti-CTMU, anti-Langan tirade and noticed your complete 
misunderstanding of certain basic terms that you were ironically accusing *me* of not 
understanding (“set”, “set theory”, “model”, “syntax”, etc.), I became very curious. How, I 
asked myself, could somebody like this graduate from college in a math-related discipline? 
How can he issue a critique this incoherent while managing to convince himself that he’s 
actually saying something intelligible?
I still don’t know for sure how to answer these questions, and to be honest, it nags at me. 
People who can’t see directly in front of their faces are a dangerous chronic malady of the 
human race.
Mark Chu-Carroll: “But that contradiction is a classic example of the self-inclusion 
paradoxes that plague naive set theory. The entire argument is nothing but rubbish - if you 
build on an inconsistent basis, you get an inconsistent result. Naive set theory is 
inconsistent, and you use it to get an inconsistent result. All of your weaseling verbiage is 
irrelevant - you refuse to address that key point, and I think it's perfectly clear why.”
After the number of times I’ve told you that the CTMU has nothing whatsoever to do with 
naïve set theory, your refusal to come to grips with it is nothing short of pathological. You 
need professional help, Mark, and I’m not kidding about that. I happen to know a PhD 
neuropsychologist (topnotch degree, research background, hospital residency, the whole 
nine yards). Would you like me to have her explain to you how you can obtain treatment?
Mark Chu-Carroll: “In addition to that, I think that you're a troll. You argue that you don't 
have the time or interest to actually discuss your "theory" here - but you come back, time 
and time again, spending foolish amounts of time writing insulting diatribes about how the 
other commenters aren't worth your time. That would make no sense unless you're a troll.”
I beg to differ. There are two main reasons I’ve taken so much of my valuable time to 
respond to you and your friends.
First, I do it for the sake of those whom you’re misinforming about my work. When you tell 
lies about somebody and his ideas, as you’ve been doing here, you don’t have the luxury of 
complaining when he takes the time to set things straight. After all, busy people land on 
your site, glance at your self-promotional blurb (in which you almost make it appear that you 
know what you’re talking about), read the top of the page, and then glance at the most 
recent (usually uncomplimentary) comments at the bottom of the page. On the strength of 
what they see, some of them erroneously assume that your critique is some sort of expert 
consensus, as opposed to the aimless bloviation of just another opinionated part-time 
blogger whose “math expertise” is confined to writing boilerplate code, and who is weighted 
down by the bursting load of pseudonymous toadies in his pants. Many people get their 
information from the Internet these days, and the half-baked output of people like you can 
gather undeserved momentum.
Secondly, I do it because I have a hard time giving up on people. You know next to nothing 
about my ideas, yet have held forth on them very unkindly and very inexpertly. 
Unfortunately for you, there’s much more to them than you’re apparently capable of 
grasping, and I really do think that you may suffer for this somewhere down the line. Many 
people, including you, will probably at some point see you as a reprehensible character if 
not a complete idiot.

Remember, the average person wouldn’t dream of attacking somebody for no good reason 
the way you’ve attacked me and others in this blog. Even where the ideas you criticize are 
mistaken (as some may have been), well-meaning people simply don’t behave the way you 
do.
Mark Chu-Carroll: And you simultaneously argue that (a) the fact that you have absolutely 
no qualifications at all is irrelevant to any discussion of your "theory"; (b) that all academic 
and scientific credentialing is broken, stupid, and pointless; and (c) that you will not discuss 
your "theory" with anyone who lacks the credentials that you disdain.”
(a) I’m not entirely without qualifications. Like everyone else, I have my own particular 
combination of intelligence, intuition, talent, and knowledge. It’s just that I lack the 
qualifications written in fancy calligraphy on the very expensive tickets called “degrees”. 
(Some people assume that this is entirely my fault, but it’s not quite that simple.) In fact, I 
wouldn’t trade the qualifications I have for any number of doctorates in any number of 
academic fields from any number of universities. I can already squash most academics, if 
need be in their own fields; why trade that for being one of them?
(b) It’s hardly my fault that the academic credentialization process is broken. Nevertheless, 
it most certainly is. While academia would be anything but "pointless" if it were in good 
running order, it will indeed remain “broken” as long as it is not.
(c) Nobody but a fool, having been unfairly pigeonholed by someone like you as a “crank”, 
would be stupid enough to share his ideas with you in depth. Given possession of an 
original theory that he values, nobody but a fool would reveal it to pseudonymous vipers like 
the ones infesting your forum. Given that academia determines who get credit for 
intellectual productions, no rational person without academic credentials (or any dependent 
form of security) would be so foolish as to indiscriminately share his intellectual productions 
with credit-hungry, publish-or-perish academics for the mere privilege of being ignored, 
insulted, or perhaps even plagiarized by them.
In short, the reason I don’t discuss my work in depth with people like you is … you, and 
people like you. You’re the problem, and that’s something about which there can be no 
doubt in any rational mind.
Again, I’d never dream of attacking somebody out of the blue the way you did me. Decent 
people simply don’t do things that way … unless, of course, they’ve first been attacked by 
people like you, Rubix, Fringe, and so on. Even though anonymous critics and their 
ringleaders are a dime a dozen and notorious for spewing smelly mud like ruptured sewage 
lines, the bunch of you are simply beyond the pale. Obviously, any hostility associated with 
your squirting behavior falls on your heads alone.
Mark Chu-Carroll: “I simply can't take you seriously. Either you're one of the most self-
unaware nitwits that I've ever seen, or you're an intelligent but malicious troll. Either way, I 
find you boring.”
Let me explain something to you, Mark. When you attack somebody and his ideas on your 
blog like a rabid cur, venting your meanness and getting your jollies by using all kinds of 
ungentlemanly pejoratives, and the person you’ve attacked shows up to defend himself, you 
really shouldn’t call him a “troll” for doing so. If he then points out to you that your criticism is 

based on a theory (naïve set theory) that has nothing to do with the theory you’re attacking 
(the CTMU), you are morally obliged to amend your critique, retract your pejoratives, and 
apologize for your insults. If you refuse to shoulder your moral obligation in this regard, then 
*you* become the troll.
You, Mark Chu-Carroll, are definitely the primary troll here; the juvenile trolls only gathered 
because they smelled meat. You might as well live under a bridge, polish your scales, and 
gobble up unwary billygoats.
As for the kind of troll you are, that would be an ankle-biter, and as for your primary tactic, 
that would be “bite-and-run”. Think of a badger that sits at the mouth of its hole, waiting for a 
pair of ankles to appear. When a juicy ankle comes into view, you rush out with a terrible 
snarl and set to work with your sharp little fangs, biting the targeted ankle as hard as you 
can! But then, when the biting victim picks up a stick and raises it to swat you as you so 
richly deserve, you hotfoot it back into your burrow and disappear down the chute like 
yesterday’s dinner.
After all, it’s much less painful for your anonymous above-ground friends to take the beating 
in your stead; they’re not even real badgers using their real names, but just a gang of 
pseudonymous, waddling wraiths without a functioning synapse among them. They’re 
usually not even aware of being swatted, and when they are, the pain doesn’t last.
As I say, it’s all very cowardly and deceitful. You know, you very well could be right - 
perhaps you and your evil little hunchbacks really aren’t worth the bother after all. But 
unfortunately, there remains a bit of unfinished business involving your doltish partisan 
Rubix.
It has now become obvious that Rubix is using the old “shotgun technique” of semi-
pseudonymous Internet debating: spray disjointed nonsense as indiscriminately as possible, 
and hope that by some insane, one-in-a-million freak of chance, something will stick (after 
all, the other fellow is using his real name). Or hope that your opponent, realizing that he’s 
addressing a pseudonymous pea brain, will give up and go away.
But again, Rubix has foolishly waded into one of the many areas in which he is plainly as 
blind as a newborn blue-footed booby in the Galapagos fog.
Rubix: “You argued that evolution was unfalsifiable. I gave you clear examples of why this 
was false. You argued back saying "Well, any contradictory evidence you find will just get 
treated as a local anomaly or explained away as a way to keep evolution from being labeled 
as false." So, to directly give an analogy, I gave you a story of something that involved a 
falsifiable concept (gravity) and some apparently-contradictory evidence (floating ball) and a 
logical explanation as to why it doesn't necessarily mean we should falsify the theory based 
on that evidence (windy beaches, our knowledge of forces, etc).”
Well glory be - now we’ve finally got some physics to sink our teeth into.
Gravity is a well-confirmed experimental fact. So is evolution. Can they be treated on the 
same footing?
Let’s have a brief look. As I explained, evolution is tautological because under analysis, it is 
seen to be a system of linked definitions captured by the relationship of physical 

embedment (of an organism) in its causal environment. Could something like this possibly 
apply to gravity? Could gravity follow from the embedment of physical objects in the 
physical environment and their consequent dependency on physical causation, as the 
overall theory of evolution?
Well, not according to classical mechanics. Back when gravity was seen as a force, i.e., the 
gradient of a potential, inhabiting an invariant medium as content (rather than as a variable 
property of the medium itself), matter was seen to obey the force. But unfortunately, nobody 
knew why the force should have arisen within the medium in the first place. Hence, matter 
could not be said to “obey the medium”, but only the force *within* the medium, wherever it 
may have come from. In other words, no tautology.
But then along came Riemann, Lorentz, Hilbert, Einstein, and so on, and the *force within* 
the medium of (absolute) space now became a *property of* the medium, reformulated as 
“spacetime”. The property in question is spacetime curvature. Suddenly, gravity took up 
residence in the physical embedment relationship as compactly described by Einstein’s field 
equations (which relate matter to spacetime through curvature), and a tautology was born!
Granted, Einstein followed Darwin in not calling his theory or its summary equation a 
“tautology”. But take a close look at it. What does it really say? World-class physicist John 
Wheeler, one of my favorite scientists of all time, put it like this:
“Matter tells space how to curve, and space tells matter how to move.”
How are space and matter related in this quotation? Why, they’re *mutually defined*!
Def. 1: Space (the spacetime “environment”) is that which tells matter how to move.
Def. 2: Matter (physical objects, reducing to compact energy packets) is that which tells 
space how to curve.
Now, in what way is this not a pair of linked recursive definitions, each consisting of a 
definiendum of which the other is a definens? (Even if one wants to call them “descriptions” 
instead of “definitions”, they are primitive with respect to the General Theory of Relativity, 
which means they can’t be unwound without unwinding the entire theory.)
In fact, a pair of linked recursive definitions is exactly what we’re dealing with here, these 
being the definitions (or if one prefers, the GR descriptions) of matter and its overall 
physical environment, spacetime. That’s why, on the left side of Einstein’s equation, one 
sees a expression representing the metric of curved spacetime, and on the right side of the 
equation, an expression representing the distribution in spacetime of mass and energy.
In other words, Einstein’s gravitational field equation represents a relationship between 
content and medium, object and environment. But lo and behold, this is precisely what I 
pointed out to Rubix regarding the theory of evolution! Which of course raises the question: 
why on earth is goofy little Rubix trying to use a direct, well-structured example of what I 
pointed out to him as a rationale for rejecting what I pointed out to him?
No doubt about it, the prospects are growing dim for poor little Rubix of Wall Street. But 
being a compassionate man, I’m still trying to give him credit for at least a flyspeck of 
intelligence. So is there some way that floating experimental balls - of which the 

unregenerately pseudonymous Rubix has none of his own, sad to say - can falsify gravity 
without precipitating the tautological Theory of Relativity out the window, which no atheist-
materialist debate hack can possibly afford to do?
I’m afraid not.
But just so Rubix doesn’t run away in tears of hurt and confusion before he has a chance to 
see the light - which he will no doubt do anyway, as that’s what he always does - let’s recap.
(1) Einstein, prompted by certain implications of Maxwell’s equations (including c-
invariance), used recently discovered mathematical concepts, mainly a souped-up version 
of Riemannian geometry, to “remodel” essentially the same empirical data available to 
Galileo and Newton in such a way as to turn the half-baked physical embedment 
relationship of classical mechanics into the sophisticated recursive embedment relationship 
of the Special and General Theories of Relativity.
(2) This relationship, being susceptible to characterization as a linked pair of mutual 
recursive definitions (or primitive GR descriptions), has tautological structure. [See 
definitions 1 and 2 above.]
(3) From this tautological structure, gravity is reborn not as an empirical consequence of 
observation, but as a tautological consequence of the rational mechanics of the Special and 
General Theories of Relativity, and there is no way whatsoever to “falsify” gravity without 
dumping SR and GR like bad and shameful habits.
What does this mean? Well, for starters, it means that if Rubix insists on clinging to his lame 
attempt at a rebuttal, he finds himself in the difficult position of proclaiming himself an anti-
relativity crank. And unfortunately, there’s just no way that one can be conclusively 
identified an anti-relativity crank and get any respect at all as a good footsoldier for the 
proud cause of Scientific Materialistic-Atheistic Reality Theory, or S.M.A.R.T. (I’ve chosen 
the acronym to go along with the Dawkinsian meme “Brights”, which for obvious reasons 
never really caught on like it was supposed to, and also because anyone who believes in 
S.M.A.R.T. is an “ass”, which on the heels of “S.M.A.R.T.” yields an accurate 
characterization of how such people are often perceived when they disparage superior 
intellects.)
But what about the original data shared by Galileo, Newton, and Einstein alike, all those 
falling pizzas and head-bonking apples and bending light rays and precessing planets and 
so on … can’t all of that still be observationally falsified?
Only if one is prepared to get horribly stuck trying to explain all of the data which confirm 
Newtonian gravity, Einsteinian gravity, and their intersection in flat spacetime. Given that 
Einstein was forced to resort to a tautological theoretical structure in order to address the 
deficiencies of Newtonian (non-tautological) gravity, there’s simply no good reason to 
suppose that it can be accomplished in any theory without tautological structure. (Relativity 
is what I call an “analytic tautology”; it has its roots in empirical data, but once the 
tautological structure of the data emerges, science is irreversibly committed to it.)
That gravity cannot be satisfactorily explained in any other way can be demonstrated 
beyond the shadow of a doubt. But those arguments are so far over the heads of vapid 
CTMU-deniers Rubix, Fringe, and MarkCC that they might as well give up on math, stock 

up on face putty, and head for the Borscht Belt to stage a Three Stooges revival for geriatric 
vacationers (right up the highway from Wall Street). In any case, it is now painfully clear that 
far from having threatened the CTMU in any way, the entire combined contents of their 
intellectual toolkits are not worth the ink in the period at the end of a sentence in any CTMU 
document or written description.
Mercifully for my schedule, nothing else that Rubix has written merits a response. 
“Falsification metrics”, “no competent mathematician believes in Zeno’s (or Newcomb’s) 
Paradox”, “science fiction = street/cyberpunk Matrix philosophy”, ad hominem “crank 
indices” … it’s all complete bafflegab, proving beyond any shadow of doubt that Rubix, not 
to mention his sidekick Fringe and their spiritual sugardaddy Mark, are as full of it as hog 
farm manure wagons.
Fortunately, I have my own farm to worry about. And so, having once again squashed these 
insignificant gadflies like the bugs they are, I’m afraid I must return to the fields.
[Next installment: Zombie bugs!]
Comment #23:
"Nevermind the fact that I personally got into Harvard myself..."
Right on schedule: a zombie bug.
A bug that couldn't get into a good daycare center if his mommy were holding him in one 
hand, and a twenty thousand dollar check in the other.
Comment #24:
No matter how thoroughly Mark is ignored, he keeps coming back for more.
Mark: "But when it comes to me... I take rather a lot of flak from other science bloggers, 
because I'm a religious Jew. And I'm not just a religious Jew; I'm a religious Jew who's a 
philosophical follower of Martin Buber, and who spends time studying Jewish mysticism."
Then Mark should know better than to falsely objectify me and my work under his own 
baseless assumptions, casting us as the embodiment of all that he so passionately hates.
Mark: "Of course, Chris doesn't claim to be doing abstract mysticism or spirituality. He 
claims to be doing science. But as science, it fails miserably, dreadfully, pathetically."
Error 1: Actually, I claim to be doing metaphysics, which means that I'm doing something 
relevant to both science and spirituality. This has been clearly stated from the outset.
Error 2: Even if the CTMU were called "science" instead of "metascience" (or metaphysics), 
it would still be valid, hence no “failure”. Rather, it would occasion an extended redefinition 
of "science" accommodating higher levels of description and analysis.
Mark: "And because he insists on presenting it in dreadful pseudo-mathematical/scientific 
form, it fails as spiritual metaphysics."
Error: Every CTMU description ever published contains logically consistent and nontrivial 

mathematical expressions and/or mathematical concepts. However, perhaps because Mark 
is an instrumentalist to whom understanding alone is worthless, he cannot fully comprehend 
basic mathematical concepts and terminology. It seems that if something can't help Mark 
write code, time his oil changes, or count his lunch money, he doesn't recognize it as 
"math".
As an unabashed instrumentalist, Mark has no business pontificating on spirituality or for 
that matter metaphysics, of which we may now safely surmise that he understands precisely 
nothing at all. In fact, his understanding appears to run so far into the red that for spiritual 
purposes, he is at the total mercy of the demons of confusion and ignorance populating his 
mind.
Mark: "To be clear about that - Chris constantly bitches about how I've been harping on the 
set-theory aspect. But it's important, and it's very demonstrative of the kind of problems that 
pervade his presentation of the CTMU."
Error: Mark's interminable bellyaching about sets is indeed demonstrative, and it is indeed 
demonstrative of problems. But the problems thereby demonstrated are Mark's various 
learning disabilities, cognitive lacunae, and sheer pigheadedness.
Mark: "The word "set" is vague. It describes a "thing" which can contain other "things". But it 
doesn't say what that thing is, or how that thing behaves. That's where set theory comes in. 
Set theory isn't something vague and detached from discussion of sets. It's what *defines* 
what a set *means*."
Error 1: Cantor clearly, not "vaguely", defined the word "set" as the product (or composition) 
of a perceptual operation, "discerning", and a conceptual operation, "gathering together".
Cantor did *not* define a set as "a product of the above operations plus the operation 
'including itself (or its power set) as an element', which is perhaps why this dastardly 
operation is not considered in naive set theory, and why poor, maligned Cantor neglected to 
drive a wooden stake through its heart and stuff its mouth with garlic ... i.e., neglected to 
add what Mark considers the crucial (no pun intended) operation of "draculization", i.e., 
"adding all kinds of rules and qualifications, a consistent version of set theory as it were, so 
that I, MarkCC, don't get buried under the dreadful, bloodsucking paradoxes I've unwittingly 
generated by including that last operation there, namely, 'including itself or its power set as 
an element'".
[Note: Most mathematicians think that a proscriptive axiom (as in ZF) or redefinition and 
reclassification (as in NBG) must be introduced to standard set theory in order to preclude 
the self-inclusion operation "after the fact" of Boolean algebra, which would superficially 
seem to permit it. But in fact, there is another way that Mark has simply failed to consider - it 
flies right over his head - and it is explicitly locked into the core structure of the CTMU.]
Sadly, Mark fails to realize any of this. Instead, like a pseudomathematical Van Helsing, he 
leaps into the very same grave-hole that his predecessors dug by introducing the operation 
of self-inclusion. That is, Mark insists on defining "set" as follows:
"A set is the product of four (not just two) perceptual and conceptual operations: (1) 
discerning; (2) gathering together (so far so good); (3) ‘including itself or its power set as an 
element’ (not so good); and (4) draculization, i.e., beefing up the definition of ‘set’ so that set 

theorists are not permanently buried under the weighty load of set-theoretic paradoxes they 
have unwisely generated by explicitly including operation #3.”
Of course, Cantor was a much more profound thinker than Mark, and his neglect of the self-
inclusion and draculization operations was just fine, for as it turns out, the CTMU 
foundational language does not treat the universe as a mere self-including set. This, of 
course, was my point in mentioning such a construct in my essay; I was simply pointing out 
to Mark and other querulous Van Helsings that the CTMU does not rely on set-theoretic 
self-inclusion to effect explanatory closure (as it would have to do if the universe were 
merely “the largest set”).
Error 2: The word "set", as defined by Cantor, does indeed say what a set is and how a set 
behaves. What it is: a gathering together of discernable elements. How it behaves: it takes 
form when elements are discerned and gathered together in any quantity. This may not be 
enough information for Mark, but more is not required for a general description of the 
CTMU. Why not? Because in the CTMU, the universe is not treated as a mere “set” as 
defined by either Mark or (more generally) Cantor.
Error 3: No axiomatic version of set theory is included in Cantor’s definition of "set"; the 
definition is meaningful without it. Rather, the definition of "set" is itself a "minimal set 
theory" which can be viewed as a general subtheory in which multiple set theories intersect; 
it differs from standard set theory mainly in omitting the self-inclusion operation, and the 
further operation of compensating for that operation.
Whereas a set is a gathering together of discernable objects or elements, set theory is a 
framework for interpreting and ramifying this definition in any context with a model in that 
framework. However, the CTMU has no model in that framework. As I've previously 
observed, while it is possible to model consistent versions of set theory in SCSPL, SCSPL 
cannot be fully modeled in any standard version of set theory. This implies that no standard 
version of set theory is capable of expressing the full meaning of "set" in SCSPL. Only 
SCSPL itself, which supersedes standard axiomatic set theory, has that capability.
I’ll ignore the remainder of Mark’s first post, as responding to it in detail would merely 
occasion the further repetition of something that Mark cannot understand to save his life: 
that the CTMU does not treat the universe as a mere set, but as a more fundamental and 
expressively capacious entity immune to set-theoretic paradoxes.
Mark: “As much as I think that Chris's "theory" is a pile of ill-defined rubbish, I don't think 
that that's an entirely fair criticism. His theory is ill-defined enough that it's hard to be sure 
exactly what he even means by his set arguments. In it, he never really presents a full 
model of the universe as a set. He puts up just enough of a sketch to make it possible to 
knock down with a naive self-inclusion paradox.”
Error 1: Even though I haven't yet explicated the CTMU in full and excruciating detail, the 
CTMU is not “ill-defined”. It is merely “ill-understood by Mark and others like Mark,” many of 
whom have not even bothered to read the available material on it. In other words, it has 
nucleated a confusion-born pile of rubbish in the minds of Mark and his friends. Those piles 
of rubbish, being entirely subjective, are not my problem, except when people like Mark 
confuse them with objective properties of the CTMU (and even then, I have more important 
problems on my plate).

Error 2: The CTMU contains no “set arguments”; sets are invoked merely as a point of 
conceptual orientation and departure. To the extent that CTMU structure incorporates the 
“set” concept, Cantor’s definition of “set” is both meaningful and adequate.
Error 3: If Mark were given a thousand years, he couldn’t “knock down [the CTMU] with a 
naive self-inclusion paradox.” Attempting to interpret such paradoxes in the CTMU 
automatically invokes their resolutions as an interpretative requirement.
Mark: “But you could create a model of the universe as a set. In a model like that, you'd 
model the universe as a collection of stateful objects. Things like "position" would then 
become a part of the state of an object. That's doable in set theory: the "state" of an object 
is just a collection of relations, which are easy to define. It's certainly not simple, because 
the state of things changes over time - but that's inevitable, because the universe itself 
changes over time.”
So far, so good, and what a pleasant surprise! But alas, the pleasure is only temporary.
Mark: “But even if you go that far, that still points out a major problem in Chris's "theory". 
Because Chris tries to build his argument on the assumption that the universe is-a set.”
Error: In the CTMU, while the universe is indeed a set to the precise extent that it is 
discernable and amenable to conceptual aggregation, a set is not *all* that it is. This 
generates a paradox for poor Mark, who evidently thinks that if the universe is *in any way* 
a set – i.e., that if the universe contains discernable elements (objects, states, events, 
relations, operations, and/or so on) which can be conceptually aggregated under the 
heading “universe” - then (1) it must be *nothing but* a set as he obstinately insists on 
defining it (including operations #3 and #4 above), and (2) set theory must be its 
foundational language.
Unfortunately for Mark, the CTMU does not treat the universe as nothing but a set, and as a 
happy correlate of this fact, its foundational language is not a standard version of set theory, 
but SCSPL.
Mark: “Things like the position and velocity of an object are intrinsic, inter-related properties 
of the object - not things that are defined by an external universal relation.”
If Mark actually understood the full implications of his statement, his CTMU comprehension 
problems might go away. But of course, if pigs had wings and jet engines, they could fly to 
Paris for champagne and canapés.
And so, at the end of Mark's long and unrewarding day, the only valid complaint that he can 
formulate about set theory and the CTMU is that I haven't fully explained the CTMU / 
SCSPL interpretation of "set" in a way that tells Mark, who desperately resists any attempt 
to enlighten him, how to resolve set-theoretic paradoxes. (I could do that in just a few 
paragraphs, but this is not the right venue, and such an explanation would be a favor to 
Mark, who has not behaved in such a way toward me that I owe him any favors.)
In any case, I never promised to personally walk Mark through any particular aspect of the 
CTMU, and the validity of the CTMU can be established independently of any particular 
standard version of set theory. But if it makes him feel any better, this dialogue has at least 
made me aware of possible sources of incomprehension which bear extra clarification.

Rest assured, I'll be doing something about that.
Now good day to all.
Comment #25:
"He's done nothing but attack my background, Mark's background, etc."
It's really quite scandalous that Rubix of Wall Street, pseudonymous champion and 
guardian of Mark Chu-Carroll, is such a lying, character-assassinating little blabbermouth, if 
only because it reflects so poorly on Mark.
Rubix, by the way, doesn't have a "background". Backgrounds belong to people who 
operate under real identities, as I do. Rubix is too frightened to do that. Why is he 
frightened? It's hard to say, but we might as well begin with the fact that his last post, so full 
of venom and vitriol, fails to contain a single true statement. (Maybe there’s one buried in 
there somewhere, but I seriously doubt it, and I’m not about to waste my time looking.)
Lord knows, I've been hounded by more than my fair share of anonymous dirtbags over the 
years; it goes with the territory. One almost gets used to it. But I must say, little Rubix here 
pretty much takes the cake.
Now enough, already. Is Mark such a pathetic intellectual weakling that he can't even fight 
his own battles ... the ones he starts, like he started this one over three years ago?
What a shameful embarrassment for poor Mark!
Comment #26:
First, let me congratulate several of the above participants for the fine job they’re doing 
arguing on the CTMU side of things … that is, on the *right* side of things. It’s a pleasure to 
behold.
But even though it almost appears that I could safely turn my back on this discussion, I’ll go 
ahead and add my two cents just for the sake of thoroughness.
AI: "It doesn't matter whether or not you are saying the universe is "just" a set. You have 
repeatedly stated that you define a "set" to be "a collection of objects" (with no further 
qulaifiers). With this definition, it is posisble to prove anything. Anything at all. Thus, 
anything you show using this assumption is meaningless."
Not exactly. You first have to construct a paradox around that definition (which you seem to 
have relaxed in a way I don’t recall having authorized), using a language that supports the 
construction and thus renders itself inconsistent (until you slap a few convenient semantic, 
aka set-theoretic, constraints and definitions on it as an afterthought).
When Cantor's definition is mapped into SCSPL by the CTMU, such a construction is not 
supported. If you don't understand what that means without "further qualifiers", that is to 
say, without yet another perfectly transparent description of the language and the mapping 
in question, I can only say that I'm aware of the confusion and plan to address it...by my 
own means, on my own schedule. Meanwhile, as far as concerns Mark, I need merely keep 
him penned up in the little box of errors he unwisely insisted on building for himself. Given 

his pigheaded refusal to recant, this will require no mental effort at all.
AI: "You can call it whatever you want, but the fact the fact of the matter is that you need a 
stricter definition before you can proceed. Which, apparently, you have ... So I did a little 
research and found the paper you published in 2002. And in your SCSPL section, you... fail 
to really define it. You do the same frustrating, annoying bullshit that Marx does in that you 
claim your construct has these various properties but it is impossible for anyone else to 
actually figure out if your conlcusions make any sense because you have not made your 
assumptions or definitions clear."
I see where you're coming from. But even that terse little description I gave in the 2002 
paper you read, let alone its cumulative reinforcement by other things I've written, was 
enough to characterize SCSPL; one need merely understand the context of that 
characterization. If you don't understand the context, then either you have your homework 
cut out for you, or you can simply wait until I get around to doing some more explaining. All I 
ask is that I not be slurred, defamed, and called names like "crank" and "crackpot" while I'm 
trying to work.
Remember, I don't enjoy academic perks and protections, I don't have an uncle in the 
business of publishing scholarly nonfiction, and partially as a consequence of that, I’m often 
occupied with other matters. Academia has done a sterling job of freezing people like me 
completely out of the loop, even including "non-academic" publishers, and this is obviously 
something for which reasonable allowances are required of me...especially given what's at 
stake regarding this particular theory.
AI: "So, while it may be the case that your theory is consistent, there is no point in trying to 
convince the world of its truth until you fix basic writing problems that wouldn't fly in a decent 
high school."
So juvenile delinquent Rubix skedaddles back to Wall Street High just in time for dance 
class, and now I'm talking to another high school kid with a bone to pick? I guess that here 
at "Good Math, Bad Math", it just gets better and better all the time. (Seriously, I'd hope 
you're at least a cut or two above that.)
AI: "Once again, the point is NOT that the universe being a set is what leads to the 
contradiction (though this does have problems of its own). I realize the set of the Universe 
has different structure in CTMU, but unless your definition of "set" is stricter than Cantor's, 
you are reasoning from a contradiction. This part has nothing to do with the universe at all."
Again, that's not quite right. It's the *CTMU interpretation* of Cantor's definition, which the 
CTMU faithfully preserves, that obviates standard set-theoretic paradoxes.
AI: "And if the universe being a set both is unnecessary and takes time to prove, why is it 
even in the CTMU?"
As I said, set-theoretic paradox is invoked as a point of orientation and departure. When 
constructing a new theory, one has no choice but to use available concepts as best one 
can, and it is at least partially up to the reader to allow for this necessity.
AI: "Not that I think you can prove it, since you would have to come up with a better 
definition of "set" first."

...or a better *interpretation* of "set", in a better *language* than standard set theory.
AI: "One problem that immediately jumps out is that there are multiple models of 
computation and definitions of the various linguistic terms that Chris uses, but he doesn't 
address this."
To some extent, I do address it. Beyond that extent, it need not be addressed, because the 
context is limited by the structure of the theory, which automatically suppresses irrelevant 
constructions.
AI: "Another is the apparent determinism of the theory."
Determinism is a problem only where it is exclusive (every theory is to some extent 
deterministic). The CTMU explicitly accommodates the uncertainty, indeterminacy, and 
undecidability of reality.
AI: "And while these would not be crippling if Chris were using language as a MODEL for 
reality, he is saying reality IS a language."
Yes…but let’s not forget what kind of language I’m talking about. It is a language that is also 
a model, and in addition, a universe unto itself. This is why it is called “trialistic” in that 2002 
paper, and that’s why the very title of my theory conforms to this attribute.
AI: "That's a problem, as Mark pointed out long ago."
And as I pointed out long ago, the real problem is Mark, his incomprehension, his 
uncontrollable resentment, his rabid atheistic fan club, and his/their decidedly unscholarly 
habits of self-expression.
You know, I think there may be still a certain amount of misunderstanding about me, my 
situation, and what I'm trying to do. At one time, I too would have thought that somebody in 
the academic system would have made a minor exception or two for me in view of my 
extraordinary circumstances – offered a little encouragement, opened a door or two - but 
unfortunately, academics as a breed don't have it in them. When all is said and done, the 
vast majority are self-interested creatures of their own impenetrable labyrinthine 
bureaucracy. Sadly, this renders them susceptible to coercive orthodoxy and mass 
ideological stagnation.
Some academic apologists, determined to show that academia welcomes the occasional 
outsider, are fond of citing the example of Ramanujan, an almost preternaturally brilliant 
mathematician who was taken under the wing of an academic despite his lack of formal 
education. Unfortunately, that was an utter fluke, a function of that academic’s uncommon 
compassion, integrity, and improbable ability to recognize very advanced work for what it 
was.
In deciding how to deal with academia, I prefer to consider the whole story. This naturally 
includes my own personal experience, which has conclusively demonstrated to me that 
academia is little more than a glorified trade union … one which long ago announced its 
unkind intentions toward me by serially turning me away, thus in effect sentencing me to life 
without parole at hard labor. Given my experience, I have little choice but to regard it as 
something like an institutional equivalent of Judge Roy Bean with a perpetual hangover, 

minus the death penalty and fit to be tied. Obviously, trust becomes difficult.
In other words, here's the bad news regarding CTMU clarification: by an accident (or not) of 
fate or bureaucracy, I'm in complete charge of the scheduling. The good news: this means 
that it will happen exactly when and how I think best....which is only fair, since it's my theory, 
and I'm the one who's been putting up with nasty, hideously confused anti-CTMU gadflies 
for all of these years. I don't work for academia, I don't work for the banks who might have 
robbed me blind for a university education, I have no peers or colleagues who call me 
"brother" in their mobbed-up intellectual trade union, I’m not a darling of the rich “genius” 
foundations, and in consequence, I owe nothing to any of them.
In a way, that's a very good thing. In fact, that’s probably how it had to be. You see, the 
most profound insight belongs to no mere pusillanimous hack who buys his future at the 
cost of his soul. It's not just a myth: truth serves no master but God Himself, and holds in 
contempt those whom lesser masters have bought . After all, for the purposes of most 
earthbound masters, truth is a mere inconvenience.
I hope this helps explain why I haven’t been doing certain things the way others do them. In 
any case, the worst assumption one can make about me is that credentials and intellectual 
achievements always closely correspond to knowledge and ability. Make that ridiculous 
assumption with me, and you might as well call it a day, a year, and perhaps even a life.
Just another word to the wise.
Comment #27:
"Why do you include a rant about academia in every post?"
I don't. (Maybe you'd better scroll up and do some counting.)
Face it - if I or my work came with an academic imprimatur, certain commentators here 
would keep their patronizing advice to themselves, or at least feign a little respect when 
parting with it.
In view of the fact that I’m very tired of wasting my time on such people, whose bad 
attitudes appear in comments like “there is no point in trying to convince the world of its 
truth until you fix basic writing problems that wouldn't fly in a decent high school,” an 
occasional reminder is more than appropriate.
If such reminders offend anyone’s academic sensibilities, that’s a tragedy. But if you want to 
complain about it, I can only suggest that you dig out your graduation robe and mortarboard 
cap with the perky little tassel, and take it up with your bedroom mirror.
Comment #28:
AI: “That's fine... but until then, don't act like its other people's fault for not understanding. 
My comment about high school in no way implies I am stil in high school (I'm not), but it was 
supposed to make a point about the fundamental nature of your writing style flaws.”
I see your point – you don’t want to take the blame for not understanding certain material 
that you find difficult. But human nature being what it is, people sometimes blame others for 
their own incomprehension even when it *is* their own fault. From where I sit, it's not 

entirely clear that this is not what you’re doing, at least in part.
As far as my writing style is concerned, I’ll be sure to let you know if and when I need your 
advice. Until then, I think that the main problem with my writing is that it tends to overshoot 
the reading comprehension of some readers … sometimes even those with whom I’m trying 
to communicate. As I say, I plan to compensate for that, and enough said.
AI: “Then [using containment in two senses] is a serious problem. You can't use one word 
that is key to your entire theory, in two fundamentally different ways, without being clear 
about the distinction every time. I would recommend coming up with a new word to replace 
one of the usages in order to avoid falling into semantic pitfalls.”
Again, we seem to have a reading comprehension problem here. My dual usage of 
containment was spelled out quite plainly. The same goes for other words and concepts 
about which my critics sometimes complain.
While I plan to make certain points clearer for those who find them confusing, this doesn’t 
mean that I agree to take all of the heat for everyone’s else’s confusion. Metaphysics is a 
very abstract field, and those who can’t take the heat for their own confusion should 
consider staying out of the kitchen.
AI: “If you're going to invoke standard definitions (which you do), don't diverge from the 
standard usage. You try to apply the regular definition of, say, formal language to things 
(like the universe) which in no way conform to the standard or concept of langauge.”
Definitions have underlying models. In describing novel ideas, one must sometimes break 
away from the standard underlying models of certain terms or start over with new terms 
entirely. Unfortunately, I’ve found that when I do either one of these things, I’m immediately 
accused of having failed to properly do the other. Under no circumstances does the accuser 
ever take any personal responsibility for the cobwebs festooning his own cranial vault.
Accordingly, if you really want to understand my writing, I can offer you just two choices: 
you can wait for future installments, or you can concentrate a little harder and stop blaming 
others for what you don’t understand. Either accept my schedule and stop complaining 
about the explanations already given, or take at least partial responsibility for your own 
incomprehension.
AI: “You blame academia, but perhaps you should look at your own style first. This does not 
represent a bad attitude, but sincere advice. The only reason I'm actually putting any effort 
into this at all is because I think you might have something worthwhile, just expressed 
poorly.”
I don’t think it’s quite fair of you to accuse me of “blaming” academia for anything. That 
would be futile, as academia is unable to experience remorse or concertedly reform itself in 
any way. For better or for worse, it is what it is.
My point was merely that, being what it is, and relating to me as it does, academia has 
unavoidably impacted my explanatory schedule and the amount of CTMU material currently 
available. Aside from that, it bears no further discussion … unless, of course, someone 
*wants* to further discuss it, in which case there’s really no point in complaining about the 
topic.

Thanks for giving my theory the benefit of your doubt. Hopefully, we’ll get some real 
communication going at some point, and you may actually start to feel good about it.
Comment #29:
How utterly disreputable!
Having stared at these groaty, ad hominem tarballs of wisdom for the last minute or two, I 
can't say that they resemble anything but the sour grapes of two whining losers who haven’t 
won a point despite using every dirty rhetorical trick in the grubby, dog-eared atheist-
materialist playbook several times in succession.
Fringe is right. Perhaps now would be a good time for him and his badly whipped comrade-
in-arms to go home and lick their suppurating wounds.
As it is, poor Mark will need a mop and a bucket to clean up after them.
Comment #30:
(Of course, as we've already clearly established, little Rubix here wouldn't know science or 
mathematics from "crackpottery" if it leaped up and bit him in his mud-stained nose.)
Comment #31:
At the risk of belaboring the obvious, I suppose I should point out that the self-professed 
mistakes of a Sokal-happy piss ant are its very own.
When a deceitful piss ant lies about its name, excretes in a manner vaguely suggestive of 
quantum mechanics, and draws a polite dismissal from somebody on the kind presumption 
that it is sincere even if possibly confused, that’s it. The piss ant has no option to come back 
later and claim that its respondent was remiss for not nailing all six of its feet to the floor 
before it could stuff them in its mouth and chew them to stubs. Piss ants generate errors 
faster than they write, so when they write fast, they defy external correction.
While this can easily be illustrated using the graphs of generic functions, the math can be 
simplified even more: one piss ant excreting nonsense through one bunghole requires one 
cork. Even a piss ant can count that high (and of course, it works great for sock puppets 
too).
Comment #32:
Actually, my little finger knows more about quantum mechanics than can be found in the 
entire empty head of a gutless, insignificant gadfly like Rubix here.
But I suppose that little Rubix has no choice other than to persist in his nonsense , and will 
surprise no one by doing so until everyone is puking at the mere sound of him.
What a joke is slimy little Rubix of Wall Street!
Comment #33:
Mark C. Chu-Carroll: “I've been trying not to feed the trolls…”

It's good to hear that - autophagy looks bad on a Wall Street CV. But you should also try not 
to *be* the trolls.
Mark C. Chu-Carroll: “In other words, Chris, your response is ‘Since I can't actually point out 
anything that Rubix got wrong in his comment, I'll just fling pointless insults at him’.”
Rubix got *everything* wrong in his comment, Mark. Just like you, every time one of you 
opens your mouth.
Mark C. Chu-Carroll: “It's absolutely typical of you - content free ad-hominem, in a pathetic 
attempt to cover up your own dreadful ignorance.”
Mark, you've already been caught red-handed parading around with grade-school 
misconceptions regarding sets, models, set theory, and model theory. In fact, you've been 
caught pawning them off as "mathematical expertise". Sheer ignorance, "dreadful" 
ignorance, is your trademark.
But if you don't understand by now how badly you and Rubix have been thrashed here on 
fundamental points of mathematical comprehension, you're even farther gone than I already 
think you are.
In fact, that's the only plausible explanation for your strange refusal to stop biting my ankle, 
retire to your hole, and cut your mounting losses.
Comment #34: 
Jeremy: “Chris, I understand where you are coming from. You have been there, done that! 
Being an autodidact I myself use words the same way: categorically. "Syntax" has many 
roots and many outgrowths. To represent the word responsibly you have to apply it globally 
i.e., as a universal category.”
Very good, Jeremy. Given the (low) quality of criticism that my work often receives, I must 
say that I’m quite impressed with the comprehension displayed by you and at least one or 
two others contributing here.
As one might imagine, I could write pages on the meaning of “syntax” in the CTMU, and will 
at some point be doing so publicly. Fortunately, however, I don’t have to do so here: as I’ve 
already explained, the metaphysically contextualized functional definition of that term is 
sufficient for a general description of CTMU structure and dynamics.
I agree that touching base at some point might be a good idea. Remember that the Mega 
Foundation is a point of contact, and we’ll be gearing up some time soon. (It’s the tail end of 
haying season right now; antique farm equipment being what it is, I’m about hundred acres 
behind … and of course, there’s an upcoming book to finish.)
Meanwhile, keep up the fine work.
Comment #35:
Another grating anti-CTMU, anti-Langan screed from the indefatigable Fringe.
Fringe: "Basically there are four supporters of Langan here: Tim, Jeremy Jar, Anonymous, 

and Langan himself."
Even if that were true, what's the problem? The main opposition consists of Fringe and 
Mark Rubix-Carroll. (Even if the latter is not one person as he appears to be despite the 
pretty pictures, he might as well be in point of mathematical illiteracy and general 
perspective.)
Error: “He claims that [Cantor’s] definition [of “set”] is immune to naive set theory 
inconsistencies.”
Absolutely not. One is free to add operations 3 and 4 (self-inclusion and “draculization” as 
described above) in any standard version of set theory, minus the patches of course, to 
obtain inconsistencies. I merely say that when Cantor's definition is properly interpreted in 
SCSPL, these inconsistencies are avoided.
Error: “Then he uses that inconsistencies to infer things. For example, he claims there is a 
biggest set, a set bigger than any other. Then he speaks about its powerset, which is 
bigger. This paradox is directly related to his definition, which is inconsistent.”
As just observed, Cantor's definition of "set" is not intrinsically inconsistent, any more than 
its definitive operations (discernment and conceptual aggregation) are “inconsistent”. It is in 
fact perfectly consistent as long as it is not misinterpreted or interpreted in the wrong 
language. Moreover, because CTMU inferences are made in another language entirely, 
they cannot possibly incorporate those inconsistencies.
Error: “When pointed with this simple fact, his only answer is to deny reality and insulting 
people.”
Not only have I not "denied reality", but I have steadfastly tried to keep the discussion on 
course and focused on content - the alleged "errors" in the CTMU, and the errors of the 
owner of this blog, MarkCC - even as others have insulted *me*. If I've sometimes engaged 
in a bit of tit-for-tat, that was merely in reaction to those insults (up to and including those at 
the top of this page and at least one other page written my Mark, both of which are 
duplicated at Scienceblogs). After all, if those given to insults are not themselves insulted, 
they may never learn the liabilities of rudeness.
OK, that's it. Three strikes and Fringe is out like a light. And now for the preceding screed 
by Rubix Chu-Carroll.
Error: “I'll pull stuff directly from the CTMU paper itself to explicitly show why it's all 
nonsense.”
I beg your pardon, but it takes more than "pulling stuff from a paper" to "show that it is 
nonsense". We see plenty of pulling here, but no showing.
Error: “Langan quotes "mathematician" David Berlinski (another anti-evolution Discovery 
Institute crank) who says "DNA is just a material macromolecule and yet it supposedly 
explains everything -- this isn't realistic!"
I don't work for the DI. In fact, I've never even gotten a phone call or a letter from the DI, 
much less a check. I met Bill Dembski and a couple of other DI fellows at ISCID ... exactly 

once. (Bill’s a great guy.)
Error: “Chris somehow takes this quote to mean that the information in a DNA string is 
meaningless without something material to read it, and therefore information is meaningless 
without physical interaction of some sort.”
"Interaction", yes; "physical", not necessarily. Berlinski distinguishes between the 
information embodied in DNA, which relies on "causal pathways" – transformative physical 
interactions with other physical objects or "transducers" - to create effects, and the laws of 
nature themselves, which do not have the luxury of preexisting causal pathways or physical 
interactions to create the physical universe itself. He understandably regards this distinction 
as a matter for (meta-)scientific inquiry. The CTMU addresses this distinction, and the paper 
goes on to explain how. (Berlinski’s entire essay is on the DI website:
http://www.discovery.org/a/616 )
Error: “Chris then goes on to say that separating matter and information is like separating 
the mind from matter, resulting in a problematic duality because you've got matter 
competing with abstract information abstractly representing matter for primary status."
"Duality"? That should be "dualism". Dualism is part of the problem; duality (or triality) is part 
of the solution. Again, this is explained in the paper. I suppose that everyone occasionally 
gets something backwards, but one should try not to use such inversions as bases for one's 
allegedly substantive critiques.
(Strange – the same silly error was recently made by another CTMU critic at another blog, 
and he didn’t know anything about mathematics either. Maybe Rubix isn’t Mark after all! 
Come right down to it, the only thing we know for sure is that he’s not who or what he claims 
to be.)
Error: "I mean, jesus, it's pure bullshit that gets worse and worse as you read on. It's not 
that we're too dumb to understand it. It's because Chris overestimates himself and doesn't 
seem to understand why his arguments consist of ignorant rubbish that is founded on 
ridiculous premises."
As this sort of ad hominem garbage is of no informational value regarding the CTMU and is 
thus irrelevant to the stated purpose of Rubix’ screed, it was an error to include it.
Again, three - whoops, five - strikes, and that will about do it for this grubby little crackpot, 
whose remarks only get increasingly stupid and offensive as his screed progresses.
Good day.
Comment #36:
Clearly, Mr. Fringe understands not one thing I wrote.
Error: “What is he saying here? Mathematicians use sets to model things. So (non-sequitur) 
reality itself is a set.”
Reality is a set because, to the extent that it is subject to scientific inquiry and analysis, it 
consists of discernable entities subject to conceptual aggregation. One requires no 

particular mathematical model to infer this; one requires nothing more than the scientific 
method (admittedly a model of sorts, but I don’t think that’s what Fringe means).
Error: “It's the largest set of all (non-sequitur). Every set has a larger set, like its powerset. 
Contradiction. So he infers reality becomes more than a set (inference from inconsistent 
hypotheses not related to the contradicting hypotheses, but for a third one).”
The first three sentences almost make sense, except for the "non-sequitur". (Reality 
obviously coincides with the set of all things real, which is the largest real set) The last 
sentence appears to be saying that in order to infer that reality is more than a set, one must 
adopt inconsistent premises. That’s ridiculous.
Error: “You see. He assumes an absurd hypothesis (that there is a largest set), who he can 
because of his naive definition of set is loose enough, and he actually tries to infer 
something from that. I would call this to infer things from naive set theory inconsistencies.”
Wrong – I merely *referred* to a certain paradox involving the “largest set” concept in order 
to make a point regarding ontological closure. The paradox in question is explicitly 
precluded.
Having replaced himself at the plate after striking out the first time, Fringe has now doubled 
… that is, he has struck out twice.
I’d write “Batter up!”, but that would merely occasion the end of the inning.
Comment #37:
What a surprise - more Rubix.
Error: "Almost every idea in the CTMU is unoriginal."
Everything in the CTMU, including the wealth of ageless wisdom to which it gives renewed 
expression, is wired together in a new and original framework. (Its overall originality is at 
least partially why so many people, especially fools like Rubix, fail to understand it. That, 
and the math part.)
Error: "It's all just a messy, fatty, buttery rehash of Wheeler's ideas stuffed inside the moldy 
potato that is Intelligent Design."
Well, at least we’re up to something more recent than Spinoza and Young-Earth 
Creationism. (These too are favorite values of X in the timeworn and tattered CTMU 
criticism which reads "The CTMU is just a rehash of X".)
Error: "We don't even need to take the battle to the CTMU to show why Chris is unfit to 
claim himself a master of math, logic, and physics."
I don't recall making that claim, although I will say that an obvious numbskull like Rubix here 
should think long and hard before betting too much of his "Wall Street" lucre against it.
Error: "Just yesterday, he was responding to posts with the fury of rapid-fire but 
conveniently ran off like a wet cat (as expected) when I posed some basic QM questions to 
him to expose his ignorance."

I spent most of the afternoon yesterday on a 4-ton tractor, seeing to much more important 
matters than little Rubix here. After all, I have to grow food for my horses and cows, each 
one of whom possesses a bigger and better heart than Rubix. (Come to think of it, there’s at 
least one other major organ to which this may apply as well, three if we’re talking about just 
the studs and the bulls.)
Error: "Even though he has since returned to this blog to reply to other posts, he's 
completely ignored the simple QM questions."
How best to put it? It is not the place of a dimwitted little toad like Rubix to play quizmaster 
with the targets of his various accusations and insults, particularly when playing along with 
him would have at least three unhealthy effects:
(1) It would reinforce his already insufferable idiocy.
(2) It would allow him to divert everyone's attention, including his own, away from the many 
basic errors and misconceptions in which he has already been caught (and under which he 
still mindlessly labors).
(3) It would provide him with yet another opportunity to make mistakes, even after proving 
himself incapable of understanding anything much more complex than a game of 
hopscotch.
Error: "Furthermore, Newcomb's Paradox is a dead giveaway of his expertise."
Leave it to Rubix to keep pretending to be an authority on something after being beaten 
over his microcephalic head with it and then squashed underfoot like a stink bug.
And now, I guess I'd better announce that I've got to get back on the tractor, lest Rubix 
suppose that I'm fleeing from the slashes and thrusts of his rapier wit.
Comment #38:
I count two true statements in your first paragraph, neither of which implies the bogus 
statement that you made up. The lies you added don’t help.
Regarding quiz games, it appears that we may have another misunderstanding. When I 
address you, it is strictly for the purpose of wiping your slime trail off my theory. I have no 
interest in playing games with you or answering your questions. If you were to ask me what 
letter comes after A, I’d either ignore you or tell you to aspirate a bowl of alphabet soup and 
pick it out of your nose.
It’s nothing personal – it can’t be, because you’re not a real human being. You’re something 
like pond scum, only slimier, smellier, lighter upstairs, pathologically deceitful, and 
(therefore) too frightened to give your real name. This reduces you to nothing more than a 
minor nuisance to be corrected as I choose.
(No, I didn't click on your video.)
Comment #39:
1. Mark Chu-Rubix (misquoting me): "Nobody else is as smart as you."

I never said that. (I recall answering negatively when an interviewer once asked me whether 
I'd ever met anyone I found smarter than me, but that's a different statement entirely.)
2. Mark Chu-Rubix (quoting me): “You have more knowledge of quantum mechanics in your 
pinky than I do in my entire head.”
That's right - I wrote that. I’d write it exactly the same way again.
3. Mark Chu-Rubix (misquoting me): "Everyone else makes errors faster than they can 
write."
I never said that (except about Mark and Chubix). Another lie.
4. Mark Chu-Rubix quoting me): "Your theory is absolute knowledge."
Yes, I wrote that (in so many words).
5. Mark Chu-Rubix (misquoting me): "We're all throwing around mathematical 
misconceptions left and right."
I didn't say that (except about Mark and his puppets). It's another lie, for a total of 3 lies out 
of 5 attributions.
Attributions 1, 3, and 5, being lies, must be discarded. So the question is, do either of the 
true attributions 2 and 4 imply the truth of the alleged summary attribution "You claim 
yourself a master of math, logic, and physics"?
Statement 2: No. It's unnecessary to be a master of math, logic, and physics to have more 
knowledge of quantum mechanics in one's pinky than Mark/Chubix has in his entire head. 
Indeed, this could well be true of anything smarter than an organ grinder's monkey.
Statement 4: No. Absolute truth is a matter of logic, not personal mastery.
So not only did Mark Chu-Rubix lie 3 times out of 5; he drew an invalid conclusion from the 
only true attributions he made.
Premises: 60% lies
Conclusion: 100% erroneous (i.e., another lie)
If Chubix were Pinocchio, his nose would be twenty feet long and as crooked as a 
corkscrew. He's a liar, and Mark is (at the very least) his accessory.
But what else is new?
Comment #40:
I'm sorry, "Alexander M.", but it is evident from the tone and content of your questions that 
you haven't actually read my work.
That, combined with your pugnacious attitude and the fact that you are merely another 
anonymous poster whose remarks are not supported by your name and reputation, means 
that it is not worth my while to discuss the theory with you until you remedy every aspect of 

this situation.
But thank you for your interest.
Comment #41:
Alexander: "I am quite sorry for my pugnacious attitude,but then again: "How best to put it? 
It is not the place of a dimwitted little toad like Rubix to play quizmaster...". This kind of 
insults against other commenters caused me to respond like that."
Apparently, Alexander, you not only haven't read my work; you haven't read the thread. You 
need to read the entire thread. I'd still have a very long way to go before "insulting" Rubix as 
much as he deserves to be insulted. He's a despicable little creature ... a liar, an airhead, 
and quite vicious. (At this point in the discussion, I'm afraid that truth trumps polity.)
Perhaps you can find comfort in the fact that the dirty little bugger steadfastly refuses to 
provide the least bit of (true) personal information, owing to which the “insults” you deplore 
cannot be attached to a real person.
Alexander: "My name is Alexandros Marios Konzukov, physics student, no academic 
reputation whatsoever (although I must point out that you do not have any academic 
reputation either)."
Very well, then. In lieu of an "academic reputation", I'll settle for a *real* reputation of some 
kind. As I'm still unconvinced that "Alexandros Marios Konzukov" is your name, you can 
start by providing some bona fides. (Your information should be easily verifiable on the web 
or by telephone.)
Alexander: "My main point is that a successful theory has meaning only if it can make 
predictions, if it "gives back" more than you give it."
Firstly, this is incorrect. I've explained why at length; scroll up until you find my discussion of 
theoretical types and evaluative criteria. Secondly, the CTMU is not a scientific theory, but is 
explicitly formulated on the metaphysical level of discourse. Thirdly, it does "make 
predictions" (we need not consider them here, as I'm unwilling to divulge them in this 
venue). Fourthly, it does even better on the explanatory level...in fact, it is a scientific 
necessity. If you don't yet understand why, then you might try to read up on it a bit more; in 
any case, I don’t have the time to keep repeating myself.
Alexander: "However,the CTMU fails to meet this criterion, since it does not make any 
measurable predictions at all (all statements made are unfalsifiable)."
Firstly, the CTMU contains statements which are unfalsifiable precisely because they are 
logically verifiable. Secondly, any empirical statement which exceeds the raw data is 
technically unfalsifiable beyond syntactic and definitional consistency. (Empirical falsifiability 
is pretty much shot as a scientific theoretical criterion, especially in modern physics.)
Thus, any empirical theory that is not merely descriptive is empirically unfalsifiable. 
Whereas a purely descriptive empirical theory can be empirically falsified by observational 
nonreplication on identical percepts, all other empirical theories have degrees of 
interpretative freedom rendering 2-valued empirical falsification out of the question. (You 

must have failed to read that part of the thread as well. If you can't scroll up and read it, then 
I'm afraid I can't help you.)
Alexander: "It contains a lot of statements about the nature of reality, the universe, etc but 
no explanation about HOW it works."
You've just made a statement that is indeed falsifiable. The CTMU is very heavy on 
explanation; the problem is that some people are incapable of comprehending it in the 
forms thus far given. (The same applies to M-theory, but it has the advantage of a solid 
academic foothold.) In fact, many people are unable to comprehend the object of 
explanation or the nature of the explanation operator itself. Many of these people can’t even 
recognize their own incapacity.
Alexander: "The reason why I asked you to post your axioms and predictions is that in a 
logically sound theory,given its axioms there should be unique conclusions."
All in good time. The reason I'm present in this thread is not to accelerate the timing of 
those revelations, but to stop Mark Chu-Carroll and his sock puppets (yes, I know - nobody 
is Mark's sock puppet, why perish the thought!) from smearing the CTMU, and me, with 
false claims regarding supposed "math errors" that it contains. Look at the title of the 
critique at the top of the page - it's a piece of false and defamatory idiocy in burning need of 
correction. The nonsense just beneath it is arguably even worse for its absurd pretensions 
to mathematical expertise.
If you don't think that I’ve succeeded in correcting Mark's critique, then you need to read the 
entire thread ... in particular, the parts for which I have been responsible (although there’s 
some good writing by others as well, excluding the blogger and his clones and cronies).
Alexander: "This would make a first evaluation of your work much easier."
I agree. However, I'm afraid that first evaluations are sometimes a little challenging. Some 
people have no choice but to wait for the popularizations. If you can wait, they'll be coming.
If not, all I ask is that you refrain from misleadingly passing negative judgments on a theory 
that you admittedly do not understand.
Have a nice day.
Comment #42:
Mark Chu-Carroll: "By Chris's standards, you shouldn't be allowed to comment at all. After 
all, you've got a false-sounding name, and you haven't done anything to prove that you are 
who you say you are, or to show your qualifications to comment on the discussion. If that 
whole line of bullshit has any validity when used against critics of the CTMU, then it should 
be applied to defenders of the CTMU as well."
That's incorrect. Generally speaking, the typical defender of the CTMU is a model citizen if 
not an absolute saint next to the typical CTMU critic, who would evidently lie, cheat, steal, 
and pimp his sister, mother, and grandmother in order to get over on the theory, smear its 
author, and express his hatred of God and religion. (Think of somebody like Chubix, but 
whose incoherent babbling is far less voluminous.)

Mark Chu-Carroll: "Also, the CTMU most definitely *does* claim to be science. To pull a 
quote from the introduction to the CTMU, arguing why it's necessary: 'Because the content 
of reality is a matter of science as opposed to mere semantics, this issue can be resolved 
only by rational or empirical evidence, not by assumption alone.' Chris has always been 
rather emphatic about the idea that the CTMU is a *scientific* theory."
That all depends on what kind of science one is talking about - the empirical sciences, the 
mathematical sciences, or metascience (sometimes called "metaphysics"). The distinctions 
among these types of science are logical and philosophical in nature.
If one is talking about empirical science as distinguished from metascience, then the CTMU 
is not empirical science. This is because it relies not on empirical confirmation of specific 
observation statements involving particular perceptions, but on a level of rational discourse 
that can only be described as metamathematical and/or "meta-empirical". In this realm, 
"evidence" is primarily rational, just as it is in mathematics.
That is, the CTMU follows not merely from localized observations of specific perceptual 
content, theorization on which is subject to the problem of induction, but from the manifest 
existence of perception in general. (To "perceive perception" is to perceive the existence of 
science, i.e., to assert a particular kind of math-reality correspondence fundamental to the 
philosophy of science; it involves the high-level rational consideration of empirical 
correspondences.)
But of course, all of this defies the kind of sullen, bone-headed oversimplification of science 
to which Mark has repeatedly displayed a barnacle-like attachment.
Comment #43:
Chubix: "Anyways, all I will say is that Chris Langan is by **far** the best troll I've ever seen. 
For that, I have to give him credit. He follows the rules pretty darn well. He's uncrackable. 
I've wasted a great deal of time trying to get him to admit his scam, but he's just too 
persistent. I yield, Chris. You win."
Of course I do, Mark. This was written in stone from the beginning. In terms of mental 
capacity, we’re just not on the same level.
Of course, I keep telling myself that someone like you can reach down through the chin-high 
mire, grab his own bootstraps, pull with great force, and redeem himself. But when all is 
said and done, I can only surmise that this is not in the cards for you.
Chubix: "Chris' game is pretty simple. He enjoys being an ass and laughing at people who 
fall for his troll. He makes up an obviously bullshit theory, and then lets it sit there. He 
doesn't promote it. He won't simplify anything. He won't explain anything. He obviously can't 
do this because it ruins the troll."
It's not simple at all, Mark. That's why you've been unceremoniously squashed every time 
you've made any attempt to address the actual content of the discussion, and why you lack 
clue one about how to climb out of the hole you've dug around yourself. That's why you 
resort to lies, libel, and circumlocution - you couldn't successfully argue a point of content if 
your silly little life depended on it.

Mark Chubix-Carroll: "His intention: hope that someone credentialed will be dumb enough 
to fall into the trap and grant Chris status (ABC's 20/20, Errol Morris, and Malcolm Gladwell 
have all fallen for it). People have come to interview him, grant him exposure, get him into a 
book, allow him on a gameshow, etc. Further bolstering himself with absurd credentials 
(1600 SAT while napping, 500+ lb benchpresses, 190+ IQ, etc), he's a regular Aleksey 
Vayner."
Actually, I've never requested attention from any member of the press regarding me or the 
CTMU. They called me, never vice versa. The press did their own research, usually with 
care and accuracy, sometimes regarding details I'd rather not have shared with them. I've 
turned down several high-profile interview requests that any publicity-seeker would kill for. 
Nobody "allowed" me on their game show; “begged” would be more accurate, as I had to be 
asked three or four times after completely ignoring the initial request.
In other words, when the media asked me to share my information and/or participation with 
them, I (sometimes) agreed, end of story. There is no possible evidence you could ever 
bring to bear in favor of your contention that I somehow "lured them in"; no such evidence 
has ever existed.
But then again, a total lack of rational or empirical support never seems to stop you from 
saying what you say. It simply pours out of you in total contempt of logic and decency.
Incidentally, don’t bother wasting more time by denying that “Rubix” is your pseudonym. 
Rubix is the only participant here whom you have explicitly supported by name; his 
viewpoint, pattern of ignorance, and style of expression are strikingly similar to yours; and 
regarding his lies and repeated demonstrations of scientific and mathematical illiteracy, the 
ubiquitous question “Cui bono?” has only one possible answer: Mark Chu-Carroll, self-
styled "math expert".
Now why don't you take your pseudonym(s), your confusion, and your lies, not to mention 
your notorious self-confessed racism, and go to the devil? He's been waiting for you, and 
I’m told that patience is not his forte.
Comment #44:
What a surprise - more nasty ad hominem garbage from Mark Chubix-Carroll, unredeemed 
by a single constructive sentence.
Mark complains that I dodge questions, but everything he writes is a dodge ... a perfectly 
unmathematical content-free representation of his confused and highly negative emotional 
state, utterly devoid of intellectual value. Having failed to understand one word of 
explanation thus far given, he promises that more would be wasted on him as well.
Mark's post does, however, carry a bit of unwitting comedic value: it's funny when an 
unrepentant troll complains about trolling even while handing out free trolling advice and 
trolling me personally despite the fact that I hadn't even addressed him.
Mark seems to have a desperate need for attention ... *my* attention. First his sockpuppet 
Rubix promised to run away; then it came running back with another bellyful of insults and a 
pop quiz to puke up. (In fact, if Rubix were anyone but Mark, "troll" would be an inadequate 
description. "Stalker" would be better.)

Fortunately, because it is clearly Mark with whom we're dealing here - the circumstantial 
evidence is pretty overwhelming - we can chalk it all up to the fact that he doesn't have a 
real job any more. He's now a full-time troll...er, blogger...with infinite time on his idle and 
mischievous hands. Oh, joy!
The way I see it, the main problem here is that Chubix wants a freebie - he wants the 
freedom to fling insults and libel at me while never giving anybody a real name to which his 
insults can be returned. It's a dirty, rotten, low-down cheat, a polemical rip-off, a cowardly 
attempt to dance the ad hominem fandango on somebody else's face without paying a sou 
to the band.
So here's my offer to Mark: If he will admit to being Chubix (which almost everyone here 
realizes anyway), I'll talk a little quantum mechanics with him, and we can get to the bottom 
of yet another relative competency issue that has evidently been plaguing his mind.
How about it, Mark? Personally, I think it would be great if everyone could see what you 
actually know about quantum mechanics in a “deep” and topical kind of way, don’t you?
Comment #45:
Chubix: "We're not the same person."
Like hell you aren't. ;)
There's at least twice as much circumstantial evidence in favor of Mark being Chubix as one 
needs in order to make the identification with reasonable confidence. This includes the 
points I already mentioned; the fact that until his last post, he seemed to enjoy access to 
server data (knowing when I was logged into his site); he spews in the same emotional, 
content-free way that Mark spews at the top of this page (and elsewhere); Mark unwisely 
tolerates highly specific and potentially damaging accusations from Chubix that no sane, 
neutral, emotionally uninvolved forum moderator could afford to tolerate; Mark lives in the 
Greater NY area, raising the probability that he could easily have had a friend or relative 
take photos of Wall Street, and so on.
There's a margin of error, but it's now small enough to be considered negligible. As far as 
I'm concerned, Mark is Chubix, period. So the question is, will Mark confess? He might as 
well, because regardless of anything that he “and” Chubix may say or do, Mark will be 
picking up the tab for Chubix from here on out. Chubix’ errors are Mark’s errors; Chubix’ 
insults are Mark’s insults; Chubix’ libel is Mark’s libel.
All that Mark has to offer at this point is a little honesty. So can we at least get that out of 
him?
Comment #46:
Disemvoweled by MarkCC
Comment #47:
Let's start over, Mark.
Are you ready to confess to being Chubix?

Remember, you're not fooling anyone any more. The charade is already over; all you need 
do is demonstrate a little honesty by belatedly admitting it.
(No quantum mechanics, or Newcomb's paradox, until you decloak and come 100% clean. 
Do that, and maybe we can talk.)
Comment #48:
Mark, you're a full of it as a Thanksgiving turkey.
Bottom line: You're in the habit of trying to make yourself look intelligent by denigrating work 
you don't come close to understanding. This includes any work involving basic 
mathematical distinctions like set and set theory, model and universe, and syntax and 
semantics, which you totally misunderstand. Worse, when your incomprehension is 
revealed, you gather up all of your mistakes in a heavy-duty trash bag and lay them on the 
doorstep of your opponent, hoping that thanks to the various deceptive techniques you 
employ in your writing, nobody will notice the switcheroo. But everybody notices anyway.
In addition to being snide and/or downright nasty, your criticisms baselessly accuse your 
targets of ignorance regarding matters they understand far better than you do. When caught 
in fundamental mathematical errors, you manufacture pseudonymous trolls who issue lies, 
insults, and defamatory accusations to your black little heart's content. You're not interested 
in truth at all; you just want to emerge from the conflicts you initiate smelling like something 
other than sewer water, and you don't care what it takes to do that. You end up smelling like 
sewer water anyway.
You claim to be religious, but religious people typically don't behave as you do. You live not 
by your conscience, by the 3D's - doublespeak, deceit, and defamation. It's all you really 
know; the "math" is just a distraction. All in all, you’re a stain on the Internet, your mother, 
and the planet Earth.
Come clean, or go to hell. It's up to you. Either way, you’re no longer fooling anyone.
Abandon All Hope, Ye Who Enter This Thread
Comment #1:
Now, now, Dave. There's no reason to run on and on about how the comments on someone 
else’s blog ran on and on.
The long, or better yet, the short of it is just this: Mark Chu-Carroll, functioning in his self-
appointed capacity as a loud-mouthed hothead and self-styled expert on set theory, model 
theory, and other branches of mathematics about which he evidently knows next to nothing, 
got himself good and squashed, period.
Now, I understand that some would dispute this summary. However, facts remain facts.
Incidentally, I notice that this blog, the existence of which I was just today informed in an 
email, has what look like enthusiastic endorsements from several high-powered atheist-
materialists and others of controversial bent.
Are those for real? I mean, with people like that reading this blog, shouldn't you be careful? 
After all, one of them might lose his head, bumble out of his ivory tower hidey-hole in a fit of 
antireligious zeal, and subsequently awaken to find himself standing in bare-bottomed glory 

alongside the impetuous Mark. Not a good or comfortable place to be, one would think.
[PS: You know, I've been thinking of starting up a blog myself. Why continue to pump up the 
moribund blogs of critics by responding to them as a commentator? Which, of course, 
raises a couple of questions from the critical perspective. (1) Do you want to be famous? (2) 
If so, what do you want to be famous AS?]
Comment #2:
Yes ... well, it must be quite frustrating for a Columbia PhD in philosophy to have no idea 
what's going on with something like the CTMU. (I know it would be for me.)
Meanwhile, suffice it to say that the Chu-Carroll "train wreck" is what must ultimately 
happen when truth (e.g., the CTMU) meets a pack of group-thinking internet jackals who 
refuse to give it an inch, and worse, fight it with every dirty trick at their disposal.
Now, I certainly don't want to flog a dead horse here. But regarding your confessed inability 
to distinguish between certain facts (e.g., "Mark was crushed on several key points of 
mathematics") and non-facts (e.g., "Mark and his gaggle of vicious no-name sycophants 
held their own against Langan!"), you have only two constructive solutions: educate 
yourself, or defer to authority. (Standing pat would be nonconstructive.)
Of course, I don't fool myself that you'd ever seek an authoritative finding on this matter, as 
it might prove embarrassing to you and your friends. But to do so, you'd need to line up one 
or more reputable mathematicians (scientists, philosophers) willing to argue Mark's "points" 
under their own names in the full light of day (you probably wouldn't be able to complete this 
step, as no sane, reputable academic would risk it all for the sake of Mark's half-baked 
opinions). Then you would recruit a panel of moderators acceptable to both sides, prepare a 
forum, and let the dialogue begin. Finally, you would be edified by the spectacle of Mark's 
champions being methodically crushed like dimwitted bugs, mixing their juices with Mark's 
in a swirling vortex narrowing on a strategically located floor drain.
I predict that you will ignore this option on the pretext that your interest is strictly limited to 
the "entertainment value" of it all. But there's a problem with that: your ill-advised assertion, 
just above my own recent comment, that I "earned" a place on some list of "cranks".
Here’s a piece of well-meant advice for you, Dave. If you don't want to find your illustrious 
Columbia philosophy PhD rear end in a sling of metaphysical proportions, try to refrain from 
such glaring displays of bias in widely-read public fora. They can get you into trouble.
[Re your suggestion that I not moderate the comments in a blog of my own, I don't consider 
that practical - there are more anti-CTMU cockroaches out there in cyberspace than there 
are feet to stomp them, and if they were permitted to generate unlimited ad hominem noise, 
there would be no chance whatsoever of transmitting any worthwhile information or 
achieving a constructive outcome. But I do see where you're coming from, and would 
naturally welcome any comments of a constructive nature.]
Comment #3:
Explanation accepted, Dave. I’ll merely point out two salient facts.
(1) Mark did indeed “go after me”, peashooters blazing. I didn’t know him from Adam when 
he posted his initial critique under the title “Two For One: Crackpot Physics and Crackpot 
Set Theory”. By the time I took note of its existence, this pungent screed had been 
splattering the monitor screens of the world for three years on two separate websites, 
attracting dozens of derisive comments.

Then, when I finally got around to responding, the irrepressible little imp immediately 
interpreted this as an opportunity to slap up yet another screed with an even more insulting 
title.
(2) You say that you never meant to call me a “crank”. But if not, then in your sentence
"...that alone is enough to earn Chris a post in the ‘crank’ series, along with other such 
math-abusers like disprovers of Cantor's diagonal argument and people who deny that 
0.9999... = 1,”
it was overkill to follow my name with “other such math abusers...”. It’s a simple matter of 
style and politesse. When you go too far in your efforts to depict the rationalizations or 
mental state of someone who has been guilty of insulting behavior, you may at some point 
be perceived as supporting and gratuitously repeating the insults.
Which brings us to Abbas, who is listed as the Founding Editor of this (meta)blog and is 
apparently your fellow Columbia University philosophy graduate.
Abbas, may I ask that you exercise just a little of what normally passes for editorial 
responsibility in this kind of venue, and refrain from lumping me in with those who
"...seem to have rather pathetically mistaken our laughing at them and making fun of them 
for actually being interested in the nonsense they are spouting"?
While I respect your forthrightness, I presently have no reason to think that you actually 
care about science or philosophy at all, except as they impact your personal worldview, 
whatever that might be, and vocational activities - your “day job”, as you might put it from 
your command console up there in the Italian Alps. Obviously, that goes for my own writings 
in spades, call them what you will.
In any case, rather than display parallel bias, please feel free to let Dave answer for his own 
opinions (or not; I don’t care whether he responds again). I merely dropped by after the fact 
to comment, very calmly I might add, on remarks that you and he had already published. As 
most of us understand it, that’s the purpose of a "comments" section.
That being understood, I hope there are no hard feelings, and that springtime finds you 
floating on a fresh-scented carpet of Edelweiss.
Comment #4:
Tuuka, I'm afraid that you've made some mistakes here. The rather elementary nature of 
these mistakes suggests that your understanding of the CTMU is inadequate for the 
purpose of locating "errors" in it.
By the way, I've read all of Mark's CTMU threads, just as I warned him I would in case he 
and his friends and/or sockpuppets were foolish enough to compound certain reprehensible 
and possibly dangerous lines of misbehavior, so this isn't the first time I've seen your 
complaints. For what it's worth, I doubt that this forum is any more appropriate than Mark's 
for discussing them.
If I were you, I'd take this opportunity to let it drop, do a little more homework, and only then 
go back to the drawing boards for another try (if desired). I don't think you're a bad sort of 
fellow, but my patience has been considerably eroded of late, and I'd hate to have to 
categorize you as an opponent.
Meanwhile, no harm done.
Comment #5:
Very well then, Tuuka.

Your first and most elementary mistake is your evident assumption that you can undermine 
any purportedly “absolute” theory with relativization. If this were true, then it would apply 
even to the theory governing relativization, namely (higher-order) predicate logic. But it 
doesn’t. That's because any relativization is a form of predication, and predicate logic 
comprehensively governs all forms of predication.
The theory of predicate logic applies under all relativizations that might be predicated of any 
given argument. Because it stands above any particular relativization, it can be 
"unrelativizably used" without fear of generating contradictions. So we know immediately 
that certain predicates, e.g. predicate logic itself, can be “unrelativizably used”.
As the CTMU is a straightforward model-theoretic extension of predicate logic called a 
"supertautology", you can't relativize your way out of the CTMU either. (You go on to make 
several related mistakes with regard to specific CTMU properties, e.g. comprehensiveness 
and support for Wheeler's "Law Without Law", and principles, e.g. M=R and MAP; they fail 
on basically the same grounds.)
And now, regarding my responsibility to debate critics of my work, I'd like to make two things 
clear.
First, almost every debate in which I have ever participated on the CTMU has quickly filled 
up with anonymous gadflies attracted by the prospect of arguing with me, insulting me, 
spewing specious nonsense related to their childish pictures of the world, and sometimes 
falsely claiming priority or credit for my work. As this has occurred despite all assurances to 
the contrary, I no longer entertain such assurances.
Secondly, as I have found most Internet debates regarding my work to be thankless wastes 
of my time, I now try to restrict my Internet CTMU discussions to relatively well-known 
individuals in widely-read, well-documented venues in which there can be no confusion 
regarding who I am, who they are, what my work says, and who wrote my work.
By "well-known individuals", I mean people who must be careful what they say, because if 
they were to resort to lies or libel, or to display ignorance regarding things they should 
know, it would reflect poorly on their real-world reputations. That way, it's not just my 
reputation at stake; it's theirs as well. Thus, symmetry and fairness have at least a chance 
to prevail.
In case you find this policy hard to fathom, let's try a little thought experiment. Suppose for a 
moment that you think you see mistakes in the work of Richard Dawkins or Daniel Dennett, 
both of whom reportedly read this blog. Now suppose that you enter your corrections in 
some thread here in the expectation that they'll take note. Should you expect a response?
Of course, you'd get no response from either of them; they're too big and important for that. 
In fact, they're so big and important, and you so small and insignificant by comparison, that 
they'd be enhancing your personal reputation merely by publicly recognizing your existence, 
let alone validating you as a critic. (We know that this is true because it has been said in 
their names, and to their knowledge, with no disagreement from them and what appears to 
be their near-perfect compliance.)
Now ask yourself whether you're entitled to expect a response from somebody who has 
been on as many TV networks as Messrs. Dawkins and Dennett (from the US, UK, and 
Canada to Brazil and the People’s Republic of China), but in addition, actually has a 
coherent model of reality and thus actually stands a chance of backing up what he says.
As the answer would appear to be “no”, I’d appreciate it if you wouldn’t be so quick to 
assume that you have some sort of special claim on my time and knowledge. You don’t. 
When I attend to your critiques or respond to you in any way, I’m doing you something very 
much like a favor.

Please try to remember this in the future.
Comment #6:
Tuukka, you’ve made a few more mistakes here. To your credit, they are of a kind that you 
wouldn’t even know how to make if you knew nothing at all about your subject matter.
Despite the mistakes, I have to give you additional credit for two things.
First, you’re a straightforward sort of fellow with at least enough courage and intellectual 
honesty to use your full real name in public.
Secondly, even though you don’t fully understand the CTMU, you understand somewhat 
more of it than most, and you seem to acknowledge that the CTMU is in some way 
important. If this is accurate, then it indicates a certain amount of intelligence.
A bit of advice. You are now being harangued by an anonymous gadfly of the sort that 
never fails to show up in any CTMU-related discussion. These gadflies are entirely parasitic; 
they never contribute anything substantive, but at best, merely pretend to do so. They are 
always annoying, but some are worse than others. Some are cunning, street-smart little 
buggers in a polemical, content-free sort of way, and if one is ignorant or unwary, they can 
almost appear to be saying something of value (whereas in reality, this is virtually never the 
case).
Don’t play its game with it until it comes out of the shadows with real, verifiable information 
regarding itself, thus showing that it is willing to put its money where its mouth is. (If it does 
that, I'll be sure to swat it for you, a prospect of which it is no doubt aware.) If you allow it to 
do so, it will merely continue to practice self-concealment as it does its best to muddy the 
water, outputting progressively more asinine, basically ad hominem nonsense until all that 
remains is a Chu-Carroll “train wreck”, complete with terminal censorship.
I don’t know about you, but I’m getting very impatient with that sort of thing. If you value my 
conversation at all, take care to maintain a healthy distance from the fly. Trust me, it will 
only end up biting you if you feed it.
Comment #7:
OK then, “Jesse Mazer” (if that’s really your name, which you seem to admit is presently 
impossible to verify - I'll give you one more chance to address this problem before evicting 
you from my side of the discussion).
As a philosophical and mathematical amateur claiming to possess academic credentials in 
“physics”, you probably have no idea how academia works with respect to complete 
outsiders – not just people without academic credentials, mind you, but people who may 
have been actively excluded from academia by roughly the same exclusionary methods that 
HUAC used during the McCarthy Era (secret eyes-only folders and dossiers, behind-closed-
doors reports and discussions, and so on – if you ever heard your grade school teachers 
threaten students to the effect that some form of misbehavior would “go on your permanent 
record”, I refer to the college-level analogue).
Here, in my limited experience – and everyone’s experience is limited, is it not? – is how it 
works.
Academia filters its input by content and context. The CTMU is already widely known to 
have philosophical (and theological) content not in conformance with academic consensus. 
(Why is this "widely known"? Because I said as much on national television, more than 
once, and wrote as much in several places as well.) That alone makes it ineligible for 

explicit academic support, and thus renders academic publication all but irrelevant. 
Delusions to the contrary notwithstanding, academic journals do not publish work by 
nonacademics bearing negatively on academic consensus, or weighing decisively on issues 
regarding which academia is officially agnostic.
Obviously, empirical observation alone is enough to confirm this for any reasonable person. 
However, if anyone knows of an academic journal that handles such matters differently but 
is nevertheless well-respected by most academics, just spill the beans and I'll be happy to 
drop the editor an email. (I’m out of the academic loop, so I don’t exactly have such 
information at my fingertips.) While I personally don’t see an immediate need for a flurry of 
pro forma chicken-scratching, I’m getting a little tired of this nonsense, so just point out the 
journal, and if I find it suitable, I’ll be happy to provide plenty of mathematical squiggles in 
which even the most hard-boiled CTMU critics can finally soak their aching heads.
On a related note, you seem to think that the CTMU is free of mathematics. However, that 
the CTMU is “mathematical” is quite obvious from its inclusion of mathematical terminology. 
If you believe that the terminology was wrongly or vapidly used, or fails to specify a unique 
mathematical structure, then the burden is on you to explain how we can know this to be the 
case.
Of course, that will be a problem for you, as the symbolic rendition of any meaningful formal 
structure in mathematics is merely shorthand for a set of natural-language definitions, 
usually those of variables, constants, relational and operational symbols, and auxiliary 
typography, often accumulated through a compounding sequence of successively less 
primitive and more heavily-defined structures. Mathematically, symbolic expression offers 
nothing extra of a substantive nature. At best, it offers an efficient shorthand for purposes of 
computational convenience; at worst, it confuses and obfuscates from the perspective of 
those unfamiliar with the operative symbology (as can be readily attested by those who 
have read many advanced mathematics papers, including professional mathematicians who 
dare to venture outside their specialties).
I don't blame you for not knowing these things, "Jesse". I do, however, blame you for the 
derisive tone of your comments and your general (bad) attitude, which seems to imply that 
you've taken it all into account when in point of fact, you've obviously failed to do so.
I therefore request that you butt out of the discussion, as there's really no need for the kind 
of comments you've been making here (at least from my own perspective as a central 
participant in said discussion).
Fair enough? (Or would you rather pull a "Chu-Carroll" on us and display your spectacular 
ignorance regarding basic mathematical terminology, in which case I fail to see what's in it 
for anybody but you?)
Comment #8:
I'm terribly sorry, Carlos, but my work and I are hardly "tangential to the discussion". Maybe 
you'd better scroll back up to the top of the page, and then start reading.
By the way, as I've tried to make clear, merely being "a longstanding audience member at 
3QD" is not good enough to qualify for any sort of protracted discussion with me. That's 
because many of the miscreants who have disrupted past CTMU threads have been, or 
claimed to be, "longstanding audience members" of the fora in which these disruptions 
occurred.
I'm sorry if I my previous attempts to explain this were too incoherent or incomprehensible 
for you.

At any rate, now you know (I hope).
Comment #9:
Christian is right, Tuukka.
For present purposes, we can start by distinguishing two kinds of predicate: (1) predicates 
immediately required for the recognition of (perceptual or cognitive) reality, and (2) 
predicates which are induced from the immediate recognition of reality.
Any induced predicate requires some kind of scaffolding by which induction is supported; 
the only real way to define an induction operator and specify its products (induced 
predicates) is to provide that kind of support. Granted, the scientific establishment has long 
been skirting this requirement - until recently, it has had no choice in the matter - but logic is 
logic, and logic is something to which science must eventually answer.
I'll be more specific. You're trying to construct a predicate, "the successor of 0" (where 0 = 
0_i, as Christian observes), which itself requires a supporting framework or "model" in which 
it is properly interpreted. Unfortunately, you refuse to specify the value of the index, i.e., the 
model you intend. Then, while making no effort to remedy the situation, you try to construct 
an even more complex predicate involving supposed conflicts among the values of i with 
respect to the original predicate. And then, for the grand finale, you try to blame it all on the 
CTMU, which is so-derived as to accommodate all possible interpretative relativizations.
Conveniently enough, this leads us directly to "Law Without Law", which means:
"There is nothing prior or external to natural law itself from which natural Law could have 
arisen."
The CTMU and SCSPL describe the structure imposed on reality by this ontological, in fact 
logical, requirement. That is, in the context of perceptual (observational, scientific) reality, 
any naïve formulation of predicate logic must be made inductively idempotent, i.e., implicitly 
"self-extending" through the CTMU. Otherwise, global reality-theoretic applications of 
predicate logic are not supported.
As there's no way out, why not save yourself some trouble and reconcile yourself to the 
adamantine nucleus of 21st century reality theory? After all, it's been firmly ensconced in 
the popular culture for over a decade now, and was first announced over a decade before 
that.
(To blunt any further outbreaks of academic snobbery in the 3QD audience, I should 
probably add that to object to the CTMU on the grounds that it hasn't been published in 
what most academics consider a respectable academic journal is not only beside the point, 
it's terribly dishonest and hypocritical. That's because the CTMU-shaped hole in the 
academic literature is, at this point, academia's own fault.)
Comment #10:
You continue to make mistakes regarding my theory, Tuukka.
I don't want to have to deal "the hard way" with your erroneous CTMU pronouncements. But 
in return for my going the gentler route, could you please try to catch some of your own 
errors before I have to do it for you?
Thanks.
Comment #11:

Hamid: " 'Actually, I would love to see Richard Dawkins take you on.' - Dave M.
"So would I, because I think it's possible for Dawkins and Langan to find common 
ground...not on the issue of God or evolution, that really would not be a productive 
conversation at this point probably, and it would be a waste of both their times, and there 
are vested interests in keeping the conversation simplistic. Rather it should be on the topic 
of simulation in life and the universe."
Hamid, the notion that Richard Dawkins and I should get together in a spirit of reconciliation 
is touching.
However, although I'd be perfectly willing to participate in such an exercise (especially if I 
could do it over the net – this ranch is over a hundred miles from the nearest major airport), 
Dawkins would not. Being a darling of the academic establishment, he is in a position to 
hide himself away inside the ivory tower from those who disagree with him, especially 
nonacademics, and will almost certainly do so in my case. Why? Because his pet issue is 
the question of the existence of God, and he very well understands that on this particular 
question, it's not the Discovery Institute or William Lane Craig about whom he most needs 
to worry.
Against (e.g.) Craig, Dawkins would be up against a very capable debater who knows well 
how to blunt and deflect standard attacks on theism. Because Dawkins himself is a very 
capable debater who knows how to deliver such attacks with great wit and aplomb, the 
likely outcome would be a draw ... just as it usually is in such contests, with neither side 
having a clear and decisive advantage. But against me, Dawkins would be far more likely to 
emerge the clear loser. That's because I have a well-developed logical picture of reality, 
and this is just the kind of advantage that could break the symmetry.
Let me put this as succinctly as possible: in any fair, content-based debate against me 
regarding Dawkins' pet issue, the existence or nonexistence of God, he would lose. He 
would lose even if he joined forces with Daniel Dennett, Sam Harris, Larry Krauss, Steven 
Pinker, and a slew of other top (atheistic) academics skimmed from the cream of the 
university system. Why would they lose? Because, despite any and all assumptions to the 
contrary, they lack a well-developed logical understanding of reality, and any fair, content-
based debate is ultimately decided by logic.
Many people are under the illusion that Dawkins and company possess a logic-based 
picture of the world. But they certainly do not. If such a discussion were allowed to proceed 
without content-free polemics, rhetorical diversions, playing to audience bias, staged 
disruptions, and so on, this could be unequivocally demonstrated. In fact, it would be a shut-
out. In very short order, I'd be washing up while Dawkins and his friends pondered that 
which haunts their dreams: the new prospects for logical theism and world religious unity 
unleashed by their fondness for atheistic grandstanding.
No, I wouldn’t be looking for a debate between Dawkins and me. He's simply not up to it. 
Besides, it would amount to something very much like "cruelty to academics", and who 
wants to be guilty of that? ;-)
Comment #12:
Who are you, F.J.A.?
Comment #13:
Very well then, Frank.

For essentially logical reasons, reality incorporates variously-bounded levels of freedom. 
Accordingly, bad things can happen, even to good people. That's what "freedom" means; 
goodness or teleology, not being universally determined (at least on localized scales), can 
be counterinstantiated.
As a conduit of reality, man too is free. He therefore has the following choice: either 
cooperate intelligently to reduce and/or make the best of whatever pain intrudes into the 
world, or not. Unfortunately, man often compounds his pain through errors of commission or 
omission. To blame God for this is irrational and irresponsible. It is also to fail a certain kind 
of test inherent in the nature of existence.
Of course, there's more to it than that, but this will have to do for now.
I'm writing a book on this and other topics. If you want to see more of my theodical 
reasoning - for example, the full explanation of why the universe is logically required to 
incorporate freedom - you'll have to wait for the book, which I'll probably be publishing and 
distributing through a certain nonprofit foundation. The publication date has not yet been 
set, but should probably come within a year.
Comment #14:
Dave Maier: "Chris: I think you give Dawkins too much credit. Of course you're right that he 
wouldn't be up to the debate, but that doesn't mean he's avoiding it on purpose because he 
knows he'd lose. In fact I wouldn't be surprised if he hasn't even heard of you or the CTMU. 
Not everybody watches TV. tomas: Absolutely."
Perhaps you're right, Dave. But are you sure you really want to say so?
You see, if you're right, then Richard Dawkins would have to be quite stupid. This would be 
true for one or both of the following reasons:
(1) It is quite stupid to claim that you know whereof you speak regarding a given subject 
when you actually have nothing but your opinion to go on, especially if there's more to the 
subject in question than mere opinion after all (and regarding theology, Dawkins obviously 
can’t say whether there is or there isn't).
(2) It is quite stupid to claim that you know what you're talking about even while 
sequestering yourself away from most of the world in the Holy Ivory Tower (aka the 
universities), thus limiting your awareness to a closed system which filters and/or blocks out 
any idea not originating or already existing within it, even if said idea receives repeated 
airplay on (inter)national television and major periodicals and then lingers on the Internet 
throughout the entire next decade.
If I were Richard Dawkins (which I admittedly am not and would not wish to be), and I were 
saying some the things that Richard been saying with respect to theology and other 
branches of philosophy and science, I'd make very sure that I was as up-to-speed as I could 
be on these subjects … as opposed, for example, to waiting for one of my equally blindered 
academic associates to tap me on the shoulder and whisper "Pssst! Richard, there's 
somebody out there who the mass media says is a lot smarter than you, and who swears 
up and down that he can prove you're full of cow manure!"
Obviously, one doesn't get up to speed in one’s chosen field by assuming that academia is 
necessarily a one-stop shop on all avenues of intellectual commerce. There's simply no 
evidence for such an assumption – there can’t be, because academia is a profoundly 
circular enterprise - and making assumptions on no evidence is something that no self-
styled intellectual authority should ever do, especially with the intention of treating or 
(worse) promoting them as facts.

I suppose I should also mention that several years ago, there was an extremely aggressive 
and underhanded attack against me and the (former) Wikipedia CTMU article that was 
traced to the website of an organization called “The Brights’ Net”. If you need three guesses 
to figure out who ultimately controls this website (despite the boilerplate disclaimers), I’ll 
give them to you without hesitation. It may be circumstantial, but it’s highly suggestive 
nevertheless.
By the way, I once had a beard to go with the mustache. This raises an interesting question: 
does something that sounds like a threat when accompanied by a mustache sound as 
menacing when the mustache is augmented by a beard? Or do the benign public images of 
trusted figures like Daniel Dennett and Santa Claus weigh against such a likelihood?
One thing's for sure – as any academically certified philosopher is well aware, many 
important philosophical issues hang in the balance! ;-)
Comment #15:
Tuukka, I think that what Christian and others are trying get at here is that (1) technically, 
"undefined (or partially defined) predicate" is an oxymoron, and (2) nothing without a proper 
definition is meaningful, in a strict logical sense, to the people with whom you’re ostensibly 
trying to communicate.
You cannot specify a definition only partially and still claim to have a “well-defined 
predicate". That's because a definition, model, or predicate is a mapping into or onto a 
universe, and must therefore terminate on some specific set of points or relations within or 
on that universe.
Once you specify the full definition of "the number whose successor is 0_i", including all 
definientia thereof, there is no longer a semantic conflict of the kind you propose. Why not? 
Because you have now specified a numerical universe for your predicate, and thus 
eliminated any others with which it might have conflicted.
On the other hand, if you specify all (of the usual) such universes, they form a tower of 
mathematical fields in which your "predicate" must be precisely located (defined, modeled, 
“predicated”). To wit, where a number is an element of a certain field, you must specify a 
field in order to define “number”. That’s why mathematicians refer not to garden-variety 
numbers in their proofs and equations, but speak explicitly of natural numbers, integers, 
rationals, reals, complex numbers, quaternions, octonions, and so on.
On the other hand, if you choose to define "number" in a general way that applies to the 
elements of any possible number field, thus to capture the bare essence of what it means to 
be a "number", then you can't make a contradiction out of the resulting predicate. That's 
because it means precisely the same thing in every possible universe (and in your 
terminology, can be "nonrelativizably used"). Moreover, you can’t specify precisely what it 
means to be the “successor” of a number. All you can do is define 0 as a generic additive 
identity, which is not enough to define "the number whose successor is 0_i".
In either case, if you fail to specify a particular universe of numbers in which to model your 
predicates and any questions thereon formulated, then (1) you cannot obtain an answer to 
the question "is there a number of which 0 is the successor?"; (2) you cannot claim that the 
(missing) answer is self-contradictory; and (3) you have no contradiction to use against the 
CTMU.
I hope this is clear to you now.
Comment #16:

Let me recap recent events for you, Tuukka.
1. On your own initiative, you came onto this thread to inform me and the rest of the world 
that you had nailed me and my theory dead to rights on a certain novel but irrefutable point 
of logic.
2. Noting that you had made some errors, I tried to dissuade you.
3. You denied that you had erred and persisted in prosecuting the CTMU.
4. At your insistence, I pointed out your primary error of comprehension regarding your 
target.
5. You denied that you had erred and persisted in prosecuting the CTMU.
6. Somebody else pointed out an obvious but seemingly trivial error that you had made.
7. You denied that you had erred and persisted in prosecuting the CTMU. I was the 
raccoon, you were the coon-dog, and I was to be treed forthwith!
8. I again tried to dissuade you.
9. Others joined in, expanding on the seemingly trivial error of logic that you now seemed 
bent on compounding.
10. You again denied that you had erred and persisted in prosecuting the CTMU.
11. I explained precisely what your critics meant, how it ties into your own logical 
terminology involving "nonrelativized predicates", and (again) why your terminology cannot 
be used against the CTMU.
12. You now insist that the CTMU is, in effect, trivial and insignificant because it fails to 
accommodate "nonrelativizable predicates” (as already explained, the CTMU 
accommodates all valid predicates), and because you, personally, cannot see how it solves 
the “symbol-grounding problem” or “the problem of joy".
I am advising you once again to desist.
I have no trouble believing that you have your own theory of reality, and that it represents 
an interesting new perspective on certain philosophical issues. To the extent that it is 
actually unique, I sincerely wish you the best of luck with it. But now that you have made it 
very clear how much you actually understand of the CTMU and its relationship to logic, 
don't you think you'd better return the favor?
Please bear in mind that over the last couple of decades, many people - some having 
extraordinary IQ’s, and some claiming to be professional academics but strangely unwilling 
to come clean about their actual identities and qualifications – have tried to find "errors" in 
my work. Not one has succeeded. There is a very simple reason for this: my work is exactly 
what I say it is, and by the nature of its design, it is absolutely bulletproof. This is why it has 
been deliberately marginalized by the very successful, very profitable coalition of Academia, 
Inc. and Atheists-R-Us. But only a fool would take this marginalization as indicative of its 
true worth. Regardless of any misunderstandings that have been propagated regarding it, it 
is in every way valid, significant, and revolutionary.
I'm sure that you have arguments against this, Tuukka. (Arguments, if not all of the logic 
behind them, seem to come very easily to you.) However, we have now established that 
you lack the CTMU comprehension to justify detailed answers to your various CTMU 
critiques, none of which seem to be based on what the CTMU actually says or actually is.
If you like, you may explain your own theory of reality to anyone who is interested. But be 
careful, as just one of two things will prove to be true regarding it.
(1) It cannot be comprehensively interpreted in the CTMU, in which case it is logically 
invalid.
(2) It is valid and can thus be interpreted in the CTMU, in which case you should avoid 

casting any doubt on its fulfillment of this requirement by casting further aspersions on the 
CTMU.
I hope that we now understand each other at last. Once again, best of luck with your own 
theory, and thanks for the opportunity to learn a bit about it.
Comment #17:
So I guess that's it for Tuukka then. He bravely took his best shot, and missed by a mile.
Tuukka, I enthusiastically applaud your courage, if not your encyclopedic comprehension of 
logic and analytic philosophy, and wish you Godspeed on the road to whatever kind of 
insight may inspire you.
But wait a minute ... as far as philosophy and for that matter entertainment are concerned, 
this really isn't a very satisfactory outcome at all, is it.
After all, we know by reading the glowing references for 3 Quarks Daily that several highly 
qualified big-name academic atheists count themselves as readers (Dawkins, Dennett, 
Pinker, and so on - no surprise there, as it's an interesting and attractive site). But it just 
doesn't seem right that every single one of them would let the Tuukkas of the world go up 
against mean, God-believing intellectual bully Chris Langan all alone like this.
Are we really to believe that not a single big-name academic atheist reading this blog, while 
operating honestly and forthrightly under his real, illustrious, and fully verifiable name, has 
the compassion, intestinal fortitude, and sheer academia-certified brainpower to help poor 
Tuukka out of his jam?
Isn't one of these dedicated crusaders willing, for example, to defend Tuukka's belief that 
the CTMU is nothing but an incomprehensible "poem", or that it fails to nontrivially describe 
the structure of reality, or that it does not imply the existence of God?
Come on, now. Won't the atheistic bedrock of academia send out its best man so that we 
can see whether or not he, it, and its obedient cash cow, the commercial (technical 
nonfiction) publishing industry, are all that they crack themselves up to be on the subjects of 
God, religion, and reality theory?
Yes, yes, I know. Big-name atheistic academics are exceedingly busy "defending science" 
and debunking religion and all that. Why, it's their public duty!
But still, as almost anyone with half a sniffer is aware, it smells a bit off ... almost like 
something may be rotten in Denmark, so to speak.
Well, I guess we'll just have to see what happens, won't we.
Wikepedia Debate
All response #’s link back to this page. 
Response # 1
Response # 2
Response # 3
Response # 4
Response # 5
Response # 6

Response # 7
Response # 8
Response # 9
Response # 10
Response # 11
Response # 12
Response # 13
Response # 14
Response # 15
Response # 16
Response # 17
Response # 18
Response # 19
Response # 20
Response # 21
Byrgenwulf, instead of bashing each other over the head, why not address the nexus of 
your dispute by inviting Asmodeus or even Mr. Langan himself to a two-party formal debate 
over the CTMU? There are several potentially suitable forums, such as the formal debate 
forum at the Science & Philosophy Forums. Although going toe-to-toe, or more 
appropriately mind-to-mind, with the reported "smartest man in the universe" might be more 
than a little dangerous. :-) CaveBat 16:04, 17 August 2006 (UTC)
Thanks - an excellent idea. However, since the very inception of the dispute, I have made it 
clear to Asmodeus I am more than willing to do that (since he is adamant that I don't 
"understand" the CTMU): only he has consistently refused to indulge. I really appreciate the 
suggestion, though. Oh well. I suppose I do have other, more edifying things to do, anyway. 
Byrgenwulf 16:50, 17 August 2006 (UTC)
Response #1
 
 :  In fact, Byrgenwulf has already debated me (and DrL) regarding the CTMU. 
In the course of this debate, which he inaugurated voluntarily and entirely on his own terms, 
he made at least a dozen major errors regarding the target of his criticism. Since there 
seems to be a bit of confusion on this score, let me provide a concise summary.
General errors of classification
In so many words, Byrgenwulf - who identifies himself on his Talk page as Rumpelstiltskin, 
and claims to work professionally as a graduate instructor in the philosophy of physics - 
asserted that:
1. ...the CTMU is "pseudoscience" masquerading as "physics". [In fact, the CTMU was 
always clearly identified as metaphysics (i.e., philosophy), and this was reflected in the 
original categorization of its Wikipedia article. A philosophical system which does not rely on 
scientific methodology cannot be "pseudoscience".]
2. ...as a theory of everything, the CTMU must be a theory of physics. [This reflects a 
misunderstanding of the phrase; the belief that it can only describe a "unified field theory" in 
physics entails the erroneous assumption that physics is necessarily identical to everything. 
In fact, a theory of everything is properly classified as philosophical by default.]

3. ...the CTMU was not peer-reviewed. [The journal PCID, in which a paper on the CTMU 
was published in 2002, is explicitly peer-reviewed by its PhD editorial staff. Although some 
"ID critics" deprecate the quality of peer review at PCID, peer review is definitely occurring.]
4. ...the CTMU amounts to Intelligent Design theory. [The CTMU predates ID theory and 
nowhere relies on any part of it.]
5. ...the CTMU is not noteworthy. [In fact, the CTMU was discussed by such major media 
sources as ABC, the BBC, and Popular Science, and is thus unquestionably noteworthy by 
Wikipedia standards.]
Technical errors on Article Talk Page
In so many words, Byrgenwulf (Rumpelstiltskin) asserted that:
1. ...the CTMU is ruled out by Godel's theorems. [It is not, for it nowhere purports to derive 
formally undecidable truths.]
2. ...the CTMU fails to choose between completeness and consistency. [It clearly prefers 
consistency, explicitly substituting comprehensiveness for completeness.]
3. ...tautology is devoid of information. [Since an informative distinction can be made 
between consistent and inconsistent theories, and inconsistencies take the generic logical 
form "X and not(X)", thus violating the tautology "not(X and not(X))", there exists an 
informative distinction between tautologies and their negations in the theoretical 
context...i.e., between those inconsistent theories containing statements which imply "X and 
not(X)", and those consistent theories which do not. Impressions to the contrary 
notwithstanding, it is sometimes quite useful to know that a given theory is logically 
consistent; hence, tautology carries information, albeit of a higher order than the information 
contained in most scientific theories.]
Technical errors on AfD (or AfD Talk) Page
In so many words, Byrgenwulf (Rumpelstiltskin) asserted that:
1. ...if X is described as a model or theory, then because this description incorporates "the 
terminology of science", X must be evaluated as science. [As everyone familiar with logic is 
well aware, science has no monopoly on the terms theory and model.]
2. ...interpretations of QM are scientific in nature. [They are in fact philosophical, only 
becoming scientific if and when falsifiable observation statements are extracted from them.]
3. ...the CTMU purports to refute General Relativity. [It does nothing of the kind.]
4. ...scientific paradoxes can only be scientifically resolved. [Any scientific paradox can be 
reduced to a logical paradox of the generic form "X = ~X" (or "X and not(X)"), and thus 
requires a logical resolution which restores consistency by restoring the tautology "not(X 
and not(X))" to uniform applicability on all scales of inference. This logical resolution may, or 
may not, be "scientific" in the sense of falsifiability.]
In other words, out of twelve (12) debating points, Byrgenwulf (Rumpelstiltskin) lost all 
twelve. To put this in the colorful language of military history and chess, Rumpelstiltskin was 
"crushed". Consequently, he does not qualify for a rematch.
At such a time as a viable contender becomes available - a qualified, reputable authority 
whose defeat would do the CTMU and its author some good, and thus make the contest 
worthwhile - another debate can probably be arranged. Asmodeus 20:11, 19 August 2006 

(UTC)
Asmodeus, I hope you do not mind, I have replaced the bullets in your post here with 
numbers so I can more easily address the points you make.
1. In many of the publications pertaining to the CTMU, it is listed as a "replacement for 
the Big Bang" etc., while Langan is often referred to as a physicist or a cosmologist. So 
calling it a pseudophysical theory is not inappropriate. Additionally, the boundaries 
between physics and philosophy, as you no doubt know, are somewhat blurred when it 
comes to foundational problems in physics.
2. I never said that.
3. "Peer review" for the PCID leaves one Hell of a lot to be desired. Rather than the 
normal process, papers are uploaded to the ISCID website (like arXiv, really), and then 
should a fellow of the institute (of which Langan is one) "vet" the paper, it is published in 
the electronic journal. Moreover the ideological and systemic bias inherent in the ISCID 
itself automatically disqualifies theories representing certain viewpoints from occurring; 
something which, in principle, could never happen in a proper peer review process.
4. I never said that. The CTMU, however, by imparting a teleological side to reality which 
is expressed through the manifestation of forms as a result of cognitive/computational 
processes, sits very comfortably with ID - the totality of reality "intelligently designs" 
itself, one might say.
5. I stand by that - the CTMU is not noteworthy as a piece of scholarship. It might be 
noteworthy as a piece of popular trivia.
So that's that list of "errors" dealt with.
1. Prove this. Don't just say "this is a technical error". Actually prove it, using rigour and 
proper mathematical/logical techniques. I know that Goedel and I can prove that if 
arithmetic is a part of reality, and the CTMU hence must describe arithmetic, then it falls 
victim to the Goedel's theorem.
2. If the CTMU is to pride itself on being explicated at the level of rigour of mathematical 
logic, then it must rely on a formally defined description such as "completeness" as 
opposed to some numinously defined "comprehensiveness". After all, if it is a theory of 
everything, then being comprehensive (like, say, the works of the old Konigsberg Clock) 
doesn't cut it.
3. Manipulating logical tautologies cannot yield an increase in information held by the 
manipulator (that's what I actually said, not what Asmodeus quoted me as saying). All 
the information is already implicit in the tautologies. Please don't misquote and 
misrepresent me just to "score points".
That's that lot.
1. I did not just use "model" or "theory", because the CTMU also claims to be an 
interpretation of QM as well as offer a new "interpretation of relativity". Moreover, just as 
the boundaries as to whether or not intepretations of QM are physics or philosophy are 
by no means clear, and there are good arguments for either view, so there are many 
(technical!) definitions of "theory". Didn't Asmodeus know that?
2. Not necessarily. That depends on whether one accepts a pure Popperian stance on 

matters or not (personally, I like Quine: we can falsify all we like, but always come up 
with another explanation as to why an experiment failed). If physics is the application of 
rigorous mathematical methods to solve empirical problems about reality (which I think 
is a fair definition taking account of most things from Newton to qubits), then the problem 
of interpretation of quantum mechanics, inasmuch as it is a problem of assigning 
descriptions to empirical as well as mathematical objects, has a good claim to being 
physics. I am sure many professional physicists, as well as philosophers, would agree 
with me. The point is, the matter can be convincingly argued both ways (I can well 
understand it being classified as metaphysics), so it is not a "technical error". The "sum 
over futures" interpretation of QM, appearing only in a boast to the label of a diagram, 
would need further detailing by Langan in order to be properly evaluated, of course.
3. Well, either the CTMU must disagree in some respects with relativity, or at least the 
common interpretation of the formalism, or a host of other anomalies will arise, such as 
the alteration of physical constants. I remember drumming up an idea very similar to 
"conspansion" one night after supper, after trying to explain black holes to a twelve year 
old with the better part of a bottle of cabernet down my throat: thinking how, if the "size" 
of things became smaller, the distance between them would appear to increase, giving 
the appearance of expansion. However, this concept is so riddled with ambiguities (not 
unlike the idea of "how fast does time flow?") that it barely merits pursuing.
4. I never said that: Arthur Rubin did in the AfD, after I had just pointed out that the 
Hawking-Hartle model of cosmology was unveiled not in Hawking's pop science writings 
but in Physical Review B. It really isn't very difficult to report accurately on source 
material that is right in front of you, is it? I don't think it's an "error" anyway.
So much for the "technical errors". Asmodeus has not shown me to be wanting in a 
single one of them!
In short, then, Asmodeus has selectively misquoted me, in order to try to make me look 
like a fool (using my real name, as well, presumably to try to compound my expected 
embarassment). However, apart from demonstrating what might either be ignorance or 
just obstinancy on his part, Asmodeus has hardly made a damning argument...in fact, 
from what was written above, it seems he has either chosen not to, or is unable to:
represent accurately what is written in a source (a truly elementary ability, I should think)
take account of the vast literature on the logical and philosophical foundations of physics 
(the CTMU similarly fails in this regard)
demonstrate an understanding of the rigour and care which is necessary when 
conducting work of this nature
admit that the scholarly credentials of the CTMU are less than lacklustre.
Please Asmodeus, if you are intent on criticising me, do so properly or refrain from doing 
so: actually make arguments as to why what you say is correct, and what I say is wrong, 
because I can assure you that many of the points you make, whilst perhaps being 
defensible in an argument, are by no means "commonly established knowledge"; on the 
contrary, many of them (like "conspansion" and a teleologically driven self-cognising 
reality) are decidedly peculiar.
I see from your last paragraph that you are desparate to engage someone qualified, in 

the hope that by "defeating" them in a debate, the CTMU and its author might be 
"vindicated". If your performance is anything like as shabby and wishy-washy as that 
above, I fear for you, me being but a mere student and all, since it is hardly likely that 
someone more qualified than myself would let things like that slide any more than I 
would. You claim I "lost on all twelve points". While it is perfectly obvious that you listed 
just the things that you think I am incorrect on, I did not lose on a single one. Maybe 
Asmodeus (who may or may not be Langan) cannot even make 12 points properly, let 
alone defend them. Byrgenwulf 13:43, 20 August 2006 (UTC) (forgot to sign yesterday)
Response
 
  #2
 
 :  I'm sorry, Rumpelstiltskin, but you said everything I listed above - often at far 
greater length - and you were wrong every time. Since you're the one asking for a debate, 
the burden of proof regarding all of the above points is on you alone. Meet it in something 
approaching a credible way, and maybe I'll reconsider. But don't try to claim that you asked 
for a debate and never got one. You did, even though you didn't deserve it and I had no 
desire to waste my time on it. I'm simply telling you why you're not entitled to another, and 
why future requests from you along those lines will be summarily denied. Now have a nice 
day. Asmodeus 21:55, 19 August 2006 (UTC)
Asmodeus, you seem to have missed the point. I am not asking for a debate as an 
agreeable way to spend my time, because I can think of other, more enjoyable activities. 
The situation is this. I have made certain statements about the CTMU, which statements 
you hold are incorrect, and bring up at every available opportunity in some effort to 
discredit or belittle me. I do not agree that I am incorrect, and hence wish to be given the 
chance to defend myself (but made it clear that Wikipedia is not the place for that). 
Moreover, you have not in any way even shown that I am wrong: you have merely made 
it clear that your opinions and mine are at odds, by claiming that I am wrong.
The burden of proof, moreover, does not lie with me in many of the points made above. 
If you hold that I said something, it is up to you to prove that I did, in fact, say it. If I claim 
that you told me yesterday that you are Langan, I have to prove this if I wish to make 
use of the fact: I cannot tout it at length while I wait for you to prove otherwise (which 
you obviously cannot).
The whole issue of a "debate", then, arises solely because we are of different opinions 
on the validity of the CTMU, and some of my criticisms of it (and I have not even 
scratched the surface as to where it falls down). A "debate" would be a reasonable 
method to sort out who is right, surely? That way, if I am incorrect, you get to continue 
pointing this out. Whereas if I am correct, you stop badmouthing me, and I get to 
continue saying that the CTMU is wrong.
But until such time as you have actually proven me wrong, you really ought to cease 
insisting that I am. To take a very simple example, and one of the first that arose: 
completeness/consistency/comprehensiveness. The CTMU must decide whether it is, 
as Langan often claims, a theory of the level of rigour of mathematical logic. It must also 
decide whether it is able to explain absolutely every single phenomenon. In other words, 
whether every phenomenon is provable as an event ("theorem" - we can surely call the 
theorems derived from CTMU's axioms "phenomena" or "events") within the theory. If 
the theory cannot do this, it is not complete. If the CTMU wishes to claim completeness, 
it must prove that this cannot arise. Actually prove, not just state. Alternatively, if it is not 
complete, but rather "comprehensive", then first "comprehensiveness" must be formally 
defined, and then it must be shown that the CTMU meets the requirements imposed by 

the definition, and that these requirements are necessary and sufficient conditions for 
explaining the whole of reality. Until such time as this happens (and the CTMU paper 
does not do this, it is more concerned with explication than proof), I am entitled to point 
out that the CTMU has not yet done this. And voice my doubts that it can do it at all, 
since already with arithmetic we have a problem.
CTMU must surely explain arithmetic, and the lesson we learn from Goedel is that we 
can keep axiomatic systems of arithmetic consistent and complete only by the addition 
of extra axioms to take account of "Goedelian propositions", formally undecidable 
statements, but each new axiom added will generate a new Goedelian proposition as we 
read down the diagonal of the Goedel numbering generated by the theory. But, if the 
CTMU wishes to be based solely on tautological statements, then it must be shown that 
these additional axioms which would necessarily have to be included, if only for 
arithmetic (as one of the elements of reality CTMU must explain), are likewise 
tautological. Which on a very superficial level it appears they cannot be, since if they 
were tautological axioms, there would be nothing to gain by adding them to the list, as 
any theorem derivable with them would be derivable without them (I can add {P or not 
P}, the "fundamental tautology", to any proof without changing it materially at all). 
Therefore, I maintain, the CTMU fails on these grounds to meet the standards it claims 
for itself (a "falsification", one could say, inasmuch as an explicitly metaphysical system 
can be falsified in the true meaning of the word).
I can go into more detail on this and any other of the points above. And particularly 
relevant, perhaps, to the proceedings here is the issue of whether or not the CTMU 
makes physical (as opposed to purely metaphysical) claims and whether these claims 
are accurate and viable or not. But once again Asmodeus, I feel compelled to say that 
Wikipedia is not the place for this sort of discussion. Should you wish to pursue it, I am 
prepared to do so. But until such time as you conclusively demonstrate that I am 
incorrect, I must ask you to refrain from claiming that I am. It is both common courtesy, 
and the rational way to conduct a discussion.
But the CTMU seems to operate very similarly: make a whole host of claims and 
assertions, but never actually offer any proof. Use grandiose terms and claim inviolate 
authority, but never actually support this with concrete evidence. And I am sure that 
even Dembski himself can admit that the ISCID is not generally regarded by the 
community as being a powerhouse publisher of cutting-edge scholarly work, and that the 
peer-review system implemented there is somewhat different to that at other, more 
"orthodox" institutes.
Please seriously consider what I have written here, Asmodeus. And stop 
misrepresenting my competence and my intentions. To reiterate, I am not interested in a 
debate (which has not occurred, since I have refused to enter into one here) as a form of 
amusement or similar. Rather, I simply wish to vindicate the assertions I have made, 
none of which are incorrect, without disrupting the proceedings at Wikipedia with off-
topic blather. Although, I am not too concerned, since I have neither a need nor a desire 
to prove to Langan or any other CTMU buff that I know what I am talking about. 
Byrgenwulf 13:43, 20 August 2006 (UTC)
Response
 
  #3
 
 :  First, let me try once again to explain something. Informal criteria like 
uniqueness, plausibility, and authorship can attract attention to a new idea and thus render 
it notable. To become notable, a theory does not have to appear fully-formed and fully-

justified at the outset; it is enough that it be deemed interesting, and in practice, even this 
criterion is often suspended. Not that it need be suspended for the CTMU; had the CTMU 
been uninteresting or blatantly wrong, it would never have been deemed presentable by the 
major media sources which presented it. Conversely, the fact that it was presented by those 
sources is enough to imply that it is neither uninteresting nor implausible on its face. 
Although you are entitled to hold a contrary opinion, the fact remains that other people - not 
necessarily card-carrying academics, but members of the public and the mass media which 
serve them - evidently regard the CTMU as both interesting and plausible. For purposes of 
notability, this places the onus squarely on anyone who wants to prove that it is not. But as 
you observe, Wikipedia is not the place to do that. For the purposes of Wikipedia, it is 
enough that others have paid attention to the theory and thereby rendered it notable.
That being understood: in response to your assertion that you "haven't even scratched the 
surface where (the CTMU) falls down", I say that you haven't even scratched the surface of 
the CTMU. As regards your specific objections, they make no sense. For example, you 
insist on treating the CTMU as a standard deterministic derivational (substitution) system 
with a finite set of axioms and no provision for the generation of novel information; that's 
utterly mistaken, and an error of that magnitude tells me in no uncertain terms that you 
haven't read the material carefully enough to understand it or meaningfully criticize it. While 
some of the points you raise appear to reflect a certain amount of insight, this makes it all 
the harder to understand your evident belief that they have not been addressed. They have 
indeed been addressed, and neither you nor anyone else has come close to showing that 
this was done incorrectly. You can claim that it has not yet been done thoroughly enough for 
your taste, or in what you think are the right places, or that not enough people have publicly 
found fault with the CTMU, but not that the available material fails to point to any means of 
resolving the issues that it raises. If some have failed to recognize this information for what 
it is, then that's unfortunate. But in any case, the CTMU remains notable, and still shares 
the highest state of any ordinary theory: not yet disproved.
Even if the CTMU had been disproved, it wouldn't matter in the least. When you introduced 
the integrity of the CTMU into the editorial processes of Wikipedia as a deletional criterion, 
you violated Wikipedia's policy of not attempting to judge the validity of the ideas it presents. 
Because of Wikipedia's unfortunate tendency to let uninformed personal opinion override its 
own policies and guidelines, this violation was supported, at least by the militant, quick-on-
the-trigger sector of the Wikipedia community whose participation was misleadingly solicited 
for that purpose. You now seem to admit in a generic sort of way that it is improper to insert 
content debates and validity judgments into Wikipedia's editorial processes, but only after 
having exploited such tactics to get the CTMU article deleted. For this, you owe the 
Wikipedia community an abject apology. History is full of colorful characters who have 
cleverly gamed and manipulated various political systems in order to accomplish their ends; 
sometimes they did it for the common good, more often not. But even in the former case, 
their ends were seldom perceived as justifying their means. Similarly, we cannot regard 
your behavior as justifiable, particularly as your ends and means appear equally 
questionable.
You and I have had a mutual incivility problem. As you are no doubt aware, this sprang from 
your initial unfortunate use of pejorative terms like "pseudoscience" and "crank" to tar the 
CTMU and its author. While this makes you primarily responsible for all of the incivility 
which followed, I am nevertheless trying to be as fair and as civil to you as possible under 
the circumstances, and hope that you can manage to reciprocate. But even in that light, I 

see no absolutely no need to conceal the inalterable fact that your statements regarding the 
CTMU are erroneous, your errors self-evident (as explained in brackets), and your 
incorrectness proven beyond any shadow of doubt. These observations have nothing to do 
with incivility; they have to do with my inalienable right, and responsibility, to call a spade a 
spade. Asmodeus 22:28, 20 August 2006 (UTC)
OK. Tell you what, Asmodeus, here's an idea: a "ceasefire" of sorts. I still maintain that 
the criticisms I have raised here of the CTMU have not been addressed at a suitable 
level of detail: simply saying "you're wrong because you haven't read the paper 
properly" doesn't cut it. Incidentally, I understand how the CTMU is alleged to allow for 
the generation of novel information (part of this "self-determinacy", not so, and "unbound 
telesis" stuff - the ouroboros springs to mind...), but, in the profound words of your friend 
and mine, John Wheeler, "I am skeptical as Hell"; nay, more, I am certain it is wrong. 
But anyway, we both seem to be in agreement that this is not the place for this sort of 
discussion.
As far as the AfD goes, I certainly did not make use of doubts about content to get the 
article deleted: rather, I made use of policies such as notability, verifiability, vanity, 
reliable sources, etc. But, one of your major gripes seems to be that I notified members 
of WikiProject Pseudoscience about the proceedings (actually, I didn't: it was one 
User:Christopher Thomas who posted notification of the AfD to that board, but I had 
previously brought the fiasco to his attention - I think by asking for assistance in editing 
the CTMU from that "WikiProject" - so please don't go hounding him...I'll shoulder the 
abuse). You maintain that since the CTMU cannot be classified as pseudoscience, the 
proceedings were somehow "rigged". I maintain that there is a sense in which the CTMU 
can be tarred with the pseudoscientific brush. However, the only way to settle who is 
right here, would be an in-depth philosophical discussion of not only the CTMU but also 
the nature of the physics/pseudophysics/metaphysics/pseudophilosophy boundaries. 
Which really ought not to be happening here.
The bottom line is that the CTMU article was deleted not because the CTMU is 
pseudoscience, wrong, or otherwise malformed. It was deleted because the Wikipedia 
community deemed it to be an unnecessary addition to this encyclopaedia. Perhaps the 
people who comented in the AfD had a bias against pseudoscience: I wish there was 
more of that here, as Wikipedia is being abused by various factions to give 
encyclopaedic legitimacy to their kooky ideas. I am not by any means saying that 
orthodox scholarship is necessarily right simply because it is orthodox scholarship, but I 
am saying that of any given segment of society, it is the most likely to hold a position 
closest to the triad of virtues: Reason, Logic and Reality, which I feel very strongly an 
encyclopaedia should be imparting to its readers (after all, was this not Diderot's 
dream?).
I have been acting only, as you put it with reference to yourself, to exercise my 
inalienable right, and responsibility, to call a spade a spade. The only possible way in 
which I might have transgressed was in encyclopaedically labelling Langan a crank. I 
accept that is a little "below the belt", so to speak, and so I cordially ask Langan to 
accept my apology for that. All my other actions here, I maintain, have been perfectly 
acceptable, and in perfect accordance with fact.
Particularly, and since this issue has been cropping up in the Fringe Theories proposed-
policy/guideline/thingummy, I feel we need to draw a distinction here. You say that if the 

CTMU were blatantly wrong, the mass media would not have seen fit to comment on it. I 
disagree. Muscle and Fitness' editorial team is not qualified to decided whether or not a 
given theory is a viable replacement for the Big Bang. Nor is the BBC. What is notable is 
that there is some allegedly highly intelligent chap, who, as such an intelligent person is 
almost socially obliged to do, has a grand theory about the world. This is what the media 
has reported on, and this is what Wikipedia should contain in articles. Since the media 
has not seen fit to comment on the technical details of the theory, it being unimportant to 
their particular angle on the story, this means that the technical details of the theory 
have not yet had their notability established. Incidentally, an illustration of just how 
magnificently incompetent the media is to comment on the technical merits of a theory is 
found in , being "cat-mew"?) and "quantemonocality" or something like that (quantum 
non-locality?!?). If they cannot even transcribe the words correctly, how can they be 
considered a reliable indicator of the viability of the theory?
Now, I can understand, I think, Langan's frustration. He no doubt believes that he has 
come up with a truly valuable theory. While his insistence on it being 100% correct, and 
being "isomorphic" to all other correct theories, is far from justified or even rational, I 
believe, the CTMU is certainly not silliness on the level of the Time Cube. And, his ideas 
have been resoundingly ignored by orthodox scholarship, maybe even unfairly (and I do 
believe that Langan knows deep down that the ISCID is not exactly "up there"). 
Academic accreditation is not a failsafe arbiter of quality of thought, as the Sokal hoax 
and the Bogdanov affair amply demonstrate: but it is the best "gatekeeper" we have in 
the world at the moment, that can have some claim to objectivity, transcending individual 
opinions of a piece of work, no matter how educated, informed, or intelligent the holders 
of these opinions may be.
If I had my way, all articles relating to postmodernism would be removed from this 
encyclopaedia as well. However, since this has taken off as a thoroughly objectionable 
trend in academic circles, it would seem that it unfortunately meets notability 
requirements. I can even imagine Langan looking at that postmodern crap, and 
wondering why, if that can be included, his theory cannot - if academe can idolise the 
monstrous out-spewings of Derrida and Lacan, then surely it cannot be an institution to 
be taken seriously. When I was much younger, I was a great fan of Ayn Rand. She 
came up with a bon mot which goes something like "the uncontested absurdities of 
today become the unquestioned truths of tomorrow". I think that is very true, and regard 
it as the job of everyone with half a brain, and I can assure you I am possessed of a little 
more than that, to question today's absurdities. It is my opinion, which I feel I can amply 
justify with sound argument, that the CTMU is just one of those absurdities. Obviously, 
some people would disagree with me, and think my criticism of it to be the absurdity.
So, my idea of a ceasefire goes like this. I shall cease being so utterly disparaging of the 
CTMU and its creator. If I do that, then from what I understand of your complaints 
against me, you can stop being so utterly disparaging of me. This is not to say that we 
must agree with one another, or even "agree to disagree"; that's pathetic. No, I am still 
entitled to say that I think the CTMU is incorrect, and you are still entitled to say that you 
think I am wrong about that. But we can, I am sure, both do it without the invective 
rhetoric. The issue of the AfD/DRV is a little sticky, since you believe that I acted in bad 
faith, maliciously "gaming the system", as you put it. Now, I am not sure whether the 
"we" in your statement is a turn of speech, or an attempt to speak for the Wikipedia 
community as a whole. If the latter, I must say I do not think that that is an accurate 

positioning of yourself, but never mind. I can assure you that my actions were not 
malicious at all; and statements like "Byrgenwulf has been shown to have acted in 
violation of Wikipedia policy" are not accurate, either. I have not "been shown" to have 
done that, you are "of the opinion" that I have done that, an opinion I dispute most 
vehemently. So, while it may be an effective demagogic tactic (and something I have 
learnt very quickly in this last month is that the Wikipedia community operates 
something like a post-Soviet mob-frenzy, not something which I believe is very 
conducive to the processing of knowledge), it is nonetheless an argumentative fallacy, 
and one which is mightily incivil to boot.
Please tell me, using quotes from Wikipedia policy documents, exactly where and how 
you feel my actions were against policy. Do it here, in a civil fashion (as opposed to yet 
another shrill cry of "foul!" over the din and roar of those abominable deletion 
discussions), and I will do my best to address your concerns squarely and head on. 
Then we can work through the issue and attempt to find some manner of resolution. I 
am personally loathe to use formal, bureaucratic mechanisms of dispute resolution, 
preferring to deal with things myself. I trust that that is an attitude with which you can 
sympathise; and I am not, despite what you might think, an unreasonable person - I am 
quite willing to discuss things rationally and calmly...I think many of us have become 
somewhat worked up over the past few weeks. So, let's give it one more bash, on 
explicit terms of civility this time, in order to put this whole saga to rest; but if this fails, 
then perhaps seeking some sort of "higher counsel" might be in order, especially since 
my intuition tells me a resurrection of the CTMU article is being prepared. Byrgenwulf 
10:34, 21 August 2006 (UTC)
Response
 
  #4
 
 :  Please, Rumpelstiltskin. I didn't come here to explain to you, for the 
twentieth time, what you did wrong - how you initially vandalized the article with uncivil 
epithets like "pseudoscience" and "crank", gamed the system, turned the AfD and DR into 
content debates, misrepresented the topic of the article after being repeatedly corrected, 
and misleadingly solicited votes. We've wasted enough time on that already, and if you 
were sincere about doing the right thing, you'd have admitted it by now. I'm here because 
you claimed to have asked for a debate on the CTMU and been denied. That was just 
another misrepresentation.
Driven and comforted by your "certainty" that the CTMU is wrong, you succeeded, by hook 
and by crook, in recruiting enough support to delete the CTMU article from Wikipedia, thus 
depriving its readers of something that just might, if you are mistaken, be both true and 
important. Congratulations. But this raises a question: if by some chance you turn out to be 
wrong - if the target of your attack turns out to be correct - what will you yield in atonement? 
Remember, there's no such thing as a free lunch; sooner or later, everyone has to pay the 
piper and clear the ledger. Of what would you like your atonement to consist? Asmodeus 
17:00, 22 August 2006 (UTC)
Alright, Asmodeus, I tried. I still resolutely deny that any of your accusations above are 
true, but since you insist that they are, and refuse to produce any evidence other than 
your own opinion, I shall stop arguing about that, lest this discussion degenerate even 
further into a Punch-and-Judy style "Oh no I didn't"/"Oh yes he did" waste of everyone's 
time. Asmodeus has also refused to discuss the CTMU on its merits anywhere but in the 
deletion proceedings and on the article talk page, where I felt such a discussion would 
be out of place, and hence refused to be drawn into it, offering to discuss it elsewhere 

rather. But whatever. Arguments about arguments are generally best left to lawyers and 
continental philosophers.
As for my "atonement", should I ever be proven wrong...how about my soul? I was told I 
was going to Hell for my efforts to point out some basic facts of reason and reality to the 
inventor of "Symmetric Relativity"...and I am probably going there for other reasons 
anyway, so I figure signing it away to Langan (or his daemonic avatar Asmodeus) would 
be a just "atonement" for putting him down. And, well, the chances of this ever coming to 
pass are about the same as me having a soul anyway...especially since the CTMU 
would seem to imply that I actually do have a soul of some form (and given the 
insistence on falsification made by Asmodeus above, surely the CTMU can, of course, 
never be proven correct...it can only be disproven, and a rigorous disproof first of all 
requires a proof that at least makes a reasonable stab at being rigorous, something we 
have yet to see with the CTMU). What do you think? :P Byrgenwulf 21:35, 22 August 
2006 (UTC)
Response
 
  #5
 
 :  If you felt that it was inappropriate to discuss the CTMU in the deletion 
proceedings and on the article talk page, then why did you insist on doing so, thus forcing 
others to respond to your attacks? But since you plainly have no acceptable answer, let's 
move on. May we now surmise that if you are incorrect about the CTMU, Langan owns 
whatever remains of your soul (after you have finished corrupting and/or attempting to 
destroy it, as by your own admission, you seem to have somehow been doing)? The 
answer to this question is important, so please think hard about it before answering. 
Asmodeus 18:31, 23 August 2006 (UTC)
Asmodeus, I have no soul. I am not trying to destroy it, any more than I am trying to 
destroy Santa Claus. But certainly, if I have one, Langan is welcome to it, should he 
earn it. Byrgenwulf 19:53, 23 August 2006 (UTC)
Asmodeus, you say, "the CTMU remains notable, and still shares the highest state of 
any ordinary theory: not yet disproved". That implies that all debaters have operated 
under some agreed-to and well-defined criteria of proof and that all CTMU 
statements have passed those criteria. Yet the actual state of affairs is that 
Byrgenwulf has been trying to get you to commit to any criteria by which the CTMU 
can be evaluated, but you allow no means of evaluation. So can you please tell us, if 
not by physical observations or logico-mathematic deduction, how would anyone 
actually go about trying to disprove the CTMU? In short, what's your definition of 
"proof"? And can you provide a rigorous definition for "comprehensiveness"? 
CaveBat 00:24, 22 August 2006 (UTC)
Response
 
  #6
 
 :  People who write and/or edit articles in Wikipedia are not required to "prove" 
the theories about which they are writing. It suffices that the theories be notable...e.g., 
because they were mentioned and discussed in the mass media.
But just to be polite, I'll point out that there are many things in the world that are not 
deterministic, but still comprehensive with respect to what they purport to describe. 
Tautologies are comprehensive with respect to all possible meanings of their sentential 
variables. Probability distributions are comprehensive with respect to a set of possible 
events. Quantum wave functions are nonclassical and weird, but they're supposed to be 
comprehensive with respect to the possible states of a physical system. General relativity 
purports to be comprehensive with respect to the structure of the spacetime manifold 

(except on very small scales, but that's life). A lot of people think that science is 
comprehensive with respect to reality at large - that if science can't identify, explain or 
predict it, then it can't possibly be real. But this never gets proven. Why not? Because under 
the current definition of scientific methodology, it can't be.
The CTMU paper in PCID says that with respect to a theory of reality, comprehensiveness 
means nonexclusion of true statements regarding the universe (as opposed to the 
deterministic generation of all true statements about the universe - that would be 
completeness). Just as tautologies hold under all possible meanings of their sentential 
variables, a comprehensive theory of reality must hold regardless of any undecidables that 
happen to emerge as the universe evolves; it has the nature of a metaphysical tautology 
distributing over the physical part of reality and any additional structure thereby entailed 
(otherwise, reality couldn't enforce its own consistency, and there would be no telling what it 
contains). Proving that would be as easy as proving that a rose is a flower...essentially, it's 
true by definition.
The CTMU is based on principles that are supposed to be true of reality as defined on the 
model-theoretic level, i.e., on the level of the general correspondence between theories or 
languages and their universes, or equivalently, between the universe and a generic 
observer of the universe. Here's the good news: the paper gives analytic justifications for a 
number of such principles. If you want to criticize them, all you have to do is read the paper. 
But steel yourself, because here comes the bad news: you'll have to go someplace else to 
argue about it. See, Wikipedia isn't the place.
By the way, I notice that you don't have a user page or a talk page. Why not grab a footful of 
ceiling and stay a while? That way, if you want to start up a conversation, you won't always 
have to use somebody else's talk page. You can use your own instead. Asmodeus 17:00, 
22 August 2006 (UTC)
CaveBat, you cut somewhat to the quick with your comment above...I'm just a little 
worried though, for CTMU's sake (nah, not really). Because if "logico-mathematical 
deduction" is to be ruled out as a method for investigating it, and I agree with you that 
this would seem to be the case on some levels, then we are left with the problem of 
induction, which has plagued thinkers from Hume to Popper. But here have a 
dichotomy; Asmodeus makes much use of Popperian terminology and ideas...despite 
the fact that Popper showed quite convincingly just what a humdinger of a problem the 
problem of induction really is. And the CTMU doesn't seem to acknowledge the problem, 
let alone attempt to solve it, although arguably induction forms quite an integral part of 
the necessary machinery for the CTMU to work. It makes use of inductive-type 
reasoning which I think is extremely naive: good common sense, but unfortunately, as 
Asmodeus himself pointed out above, life doesn't always turn out to be as naively neat 
and smooth when probed at the deepest level. (It's quite loopy, actually, I daresay, 
although many people might want to string me up for that...)
Asmodeus, have you read what Quine had to say on the nature of the analytic/synthetic 
distinction? I ask not to be impertinent, but rather because what you wrote above 
doesn't seem to take account of it, or even give it a token nod...your argument (and 
indeed the CTMU) rests on the assumption that there can somehow be "logical 
primitives" (such as Mind=Reality) which can be meaningfully rendered into so-called 
"tautological" principles to generate the "super-tautology" that is the CTMU. You see, I 
understand what you're trying to say about the comprehensiveness thing...the theory 

needn't churn out predictions (or necessarily statements) like a Turing machine, but 
rather, any true proposition which does come to pass must be explainable from within 
the theory, and a posteriori be a necessary truth. Very well.
However, coming up with a theory along those lines means that the theorist will 
eventually find himself riding a dilemma: either he faces a problem a la Quine where it 
turns out that his axioms are not tautological, because they actuall entail a whole lot of 
other, hidden assumptions - as in the case of a rose being a flower: it certainly is 
analytically true, of course, but not tautologically true, because it depends on a suitable 
definition for both rose and flower (and hence an indefinitely large semantic network) for 
its truth-value; on the other horn, if definitions and so forth are to be excluded from the 
formal system, then all we shall have is a manipulation of trivial statements like {P or not 
P}. This would require Langan to show how the manipulation of these statements can 
result in, for example, us human beings typing and reading these words over the 
Internet (for the other bits of his theory to work, like panentheism, "unbound telesis", 
etc.). This is something Langan does not do in the CTMU paper, and something which is 
actually impossible to do in toto. Bootstrapping systems are fascinating, but ideas such 
as (Mind=Reality) are not tautological in themselves. They are only "tautological" by 
argument, or by assumption - both of which methods Langan uses to justify them - but 
this means that they are not true tautologies.
So I am afraid I still don't think you have offered an adequate explanation. Byrgenwulf 
21:35, 22 August 2006 (UTC)
Response
 
  #7
 
 :  Another day at the docks, another Red Herring Fish-Off. The rules: first, 
make a bogus statement about the CTMU designed to convince the neophytes that you 
know what it says; follow it up with a broad allusion to some authority figure of yore, e.g. 
W.V.O. Quine, which may or may not falsify the bogus statement that you made but sounds 
good anyway; observe that the CTMU has been falsified by (e.g.) Quine, no less, through 
the bogus statement in question; and then chortle silently in the knowledge that you've 
managed to pull off another rigged "debate" about the CTMU, even though your opponent 
has declined! Finally, lean back and collect the grand prize in the Fish-Off: eight more 
errors, bringing the grand total to an even twenty (12 + 8 = 20).
1. Rumpelstiltskin claims that "logico-mathematical deduction" has been ruled out as a 
method for investigating the CTMU. [Obviously not. There's plenty of deductive processing 
in the CTMU, at least for those who read (not just scan) the available material on it.]
2. Rumpelstiltskin claims that the CTMU does not acknowledge the problem of induction. 
[The CTMU refers to it directly...and offers a new approach to dealing with it. The problem 
of induction refers to the fact that in reasoning from specific local observations to all of 
reality, one must assume the uniformity of nature, which is really what one is attempting to 
infer by empirical induction. In other words, since one is assuming what one is trying to 
infer, one is reasoning "tautologically", in a circular fashion. This problem is ubiquitous in the 
empirical sciences, and it is why no general scientific inferences can be proven. The CTMU, 
however, is not mere empirical science. Instead of looking for patterns in empirical data, 
making up a theory to fit them, and then treating the ingredients of the theory as "universal 
laws", it looks at the general correspondence between theories and data in search of logical 
(model-theoretic) universals, and then deduces their consequences. It does not assume the 
uniformity of nature; it infers the uniformity of nature, explaining why certain fundamental 
properties of nature have to be universal on logical grounds. This is a substantially new 

approach to the problem, and it is one of the main strengths of the unique theory that 
Rumpelstiltskin hilariously claims to "understand".]
3. Rumpelstiltskin claims that the CTMU makes use of inductive reasoning. [Only in the 
logical, as opposed to the empirical or probabilistic, sense...the sense in which any variable 
or relation can be generalized to an antecedent by inverse substitution. As explained above, 
the CTMU elevates empirical induction to the model-theoretic level of reasoning, thus 
circumventing the problem of induction.]
4. Rumpelstiltskin claims that it is wrong to assume that there can somehow be "logical 
primitives" (such as Mind=Reality) which can be meaningfully rendered into so-called 
"tautological" principles to generate the CTMU supertautology. [The CTMU does not make 
assumptions; it works from the inarguable fact that the universe exists, and that we are 
observing it. The M=R principle simply asserts that mind and reality have a perceptual 
intersect, which can then be generically used as the syntax of a new sort of mathematical 
structure called an intrinsic language. The CTMU shows that reality must conform to this 
structure as a condition of its existence, and accordingly identifies the universal properties 
of this structure as universal properties of reality at large.]
5. Rumpelstiltskin claims that "a rose is a flower" is not tautological. ["A rose is a flower" 
amounts to a semantic tautology. To see this, note that "rose = flower" --> "flower = flower" 
--> "X = X" (a tautology). It is, however, more refined and arguably more informative than a 
propositional tautology, because less general. Like other tautologies, this semantic 
tautology takes its form from the self-identity of the universe as a whole: U = U. (The identity 
relation is a wonderful thing, especially when manifest in the logical self-similarity of its 
single reland.)]
6. Rumpelstiltskin says that the CTMU fails because it requires an "indefinitely large 
semantic network" for its semantic (definitional) tautologies. [The CTMU invokes a general 
principle called the MAP for this purpose. The MAP says that the network is closed. The 
PCID paper even contains a drawing which illustrates this form of closure.]
7. Rumpelstiltskin claims that Langan has failed in that he does not show how the 
manipulation of tautologies like "P or not-P" can result in the world we see around us, and 
that in fact, this is "actually impossible to do". [Langan does more than merely "manipulate 
tautologies" like P or ~P; he generalizes the tautology concept and thereby takes logic to its 
inductive limit: a protean ontological groundstate relating to the universe, from within the 
universe itself, as ontic potential. If one likes, one can regard this protean ur-tautology as a 
primitive form of "selfhood" which gives rise to tautologies and "logical primitives" alike. 
Logic and the universe exist; hence, this property must hold, and assumptions have nothing 
whatsoever to do with it.]
8. Rumpelstiltskin says that although bootstrapping systems are fascinating, ideas such as 
(Mind=Reality) are not tautological in themselves, further claiming that Langan erroneously 
uses arguments and assumptions to establish that they are tautological, and that for this 
reason, they are not true tautologies. [Once again, the CTMU does not rely on assumptions. 
If Rumpelstiltskin can't see this, it is no doubt because he does not understand how CTMU 
principles have been logically inferred.]
9. Rumpelstiltskin says that CaveBat has "still not offered an adequate explanation." [With 
this, I happen to agree. Good thing - otherwise, Rumpelstiltskin's error count would be 21 
instead of "just" 20!]

Twenty (20) errors and counting. Asmodeus 18:31, 23 August 2006 (UTC)
What utter nonsense. A Fish-Off it most definitely is. It's hard to catch all these Red 
Herrings, what with Asmodeus seemingly running a breeding farm with a direct channel 
of access to this little dockside quay. So let's see...
I never said that. I said that logico-mathematical deduction would appear to be ruled out 
on some levels. An example of this is that the CTMU is not a "theory" which produces 
"theorems" which may be deduced(!) from the assumptions, but rather simply requires 
the non-exclusion of truth. Please do try not to slant what I say, Asmodeus...I am, 
believe it or not, sharp enough to pick it up. Unless you're doing it unintentionally, in 
which case I apologise...I suppose Hanlon's razor might be a good tool here.
If the CTMU looks at the the "general correspondence" between theories and data, and 
then infers from whatever particulars to whatever generalisation, it is making use of 
induction. It does not overcome the problem, I am afraid, only attempt to sweep it under 
the carpet with the aid of flash-bangs and polysyllables to distract from what is 
happening. How are we to know that the principle {not(P and not P)} always holds? Only 
ever through empirical observation and the endless (and probably futile) vigil for a 
falsification of it.
Well, "reasoning", I would hope, ought to be logical (otherwise we might as well give up 
right now). It is obvious to anyone reading this that I was talking about logical induction 
there. I don't actually know what your point is.
You're tangling your meta-loops here. This observation, being, as you call it, a 
"perceptual intersect", is inherently a product of mind. Existence, for that matter, is a 
predicate the existence of which (sorry) cannot be assured outside of mind. Not that I 
am promoting idealism, but the fact is that these matters are far more subtle than the 
CTMU allows for. Moreover, at some point, we have to include logical primitives of some 
form.
"A rose is a flower" is an analytically true statement. While it may be a semantic 
tautology, it is not a logical tautology, and there is a huge difference. Moreover, M=R is 
only a semantic tautology (given an argument like "perceptual intersects" and things), 
not a logical tautology. If the CTMU deals in mere semantic tautologies, analytic truths, 
then it is somewhat meaningless. Which is the point I was making, as, I thought, was 
obvious.
The MAP amounts to an assumption (or "externally imposed requirement for 
consistency's sake"). Moreover, it is expressed in a meta-language, not the object-
language, of the theory. Try to untangle those meta-loops, Asmodeus. And drawings, I 
am afraid, do not impress me much when it is proof and rigour I am after.
"Generalising the tautology concept and taking it to its inductive limit", Asmodeus, is a 
little like homeopathy...the mistaken idea that things become more potent, the less 
relevant substance they have. Langan does not show how this can give rise to anything. 
He says it does. There is one Hell of a difference. I can say that integrating the Wheeler-
DeWitt equation yields a plate of fish and chips as an answer. I can probably even give 
a reasonably coherent argument to that effect. But it doesn't make it true. Once again, 
whatever gives you the idea that logic exists as anything apart from a human construct, 
an aesthetic imposition upon man's environment? No, a theory like the CTMU must 

explain why logic works as means we use to function in our world, but without the use of 
logic itself, otherwise, once again, we get our meta-loops all knotted up.
What a whopper. The CTMU's principle are logically inferred, are they? From what? 
Empirical observations? Rational cogitations? The pink, pink sky? The point is, the buck 
has to stop somewhere. Langan tries to stop the buck by imposing constraints like MAP. 
But this is itself an assumption, or, if you like, an "observation" (be it ever so rational and 
well-motivated, even seemingly "necessitated"). Finally, if the CTMU's principles are 
logically inferred, that means that there must be some sort of argument to arrive at them. 
You're contradicting yourself, Asmodeus, and painting yourself ever more tightly into 
that corner.
It is perfectly clear to any reasonable person I was addressing Asmodeus, not CaveBat 
there; and it is also perfectly clear that Asmodeus knows this. Er hrm.
In actuality, the CTMU amounts to little more than the realisation that "I can make a list 
of principles such that nothing in the world contradicts these principles". Which is 
completely trivial. Here's one: "Everything happens by God's will, including his own 
existence". Yes, yes, I know that the CTMU is "isomorphic" to that one, in a rather 
distorted sense of the word "isomorphic". But once again, the truth, and hence worth, of 
a statement like that is contingent upon the semantic meanings of the various words in 
it. It suffers from Russell's paradox and no end of other logical syndromes. It does not, 
and cannot, break out from its own rung on the ladder of "metalevels", and simply 
including an assumption (yes, that's right, an assumption) which stipulates this, does not 
solve the problem.
As for "falsifying" the CTMU with Quine, that old "authority figure of yore"...well, I don't 
know so much. You see, Popper intended the term specifically to refer to empirical 
theories. Now I know it's been tossed around a bit in the discussion here, but I do feel I 
ought to insist that it is used properly, especially given Asmodeus' insistence that the 
CTMU has no empirical content (or if he has not insisted on that yet, he probably ought 
to, since otherwise we start opening the floodgates of pseudoscience). But I do feel that 
some of the points Quine made adequately point out the naivety and flawed nature of 
the CTMU; and this is ironic considering that Quine did indeed write about half a century 
ago, and yet Langan fails, seemingly, to take account of this and and a host of other 
arguments (indeed, any arguments whatsoever, other than a sorry platoon of 
bedraggled straw men and Langan's very own virtuoso musings).
Asmodeus, this error counting thing is getting more than a little annoying, especially 
since it is perhaps more accurately a record of how many straws you have been trying to 
grasp at to save yourself from sinking. Unfortunately, this does not seem to be working. 
Byrgenwulf 19:53, 23 August 2006 (UTC)
Response
 
  #8
 
 :  The Fish-Off continues.
Why 1-8 remain erroneous:
1. There are several kinds of "theory". The CTMU is certainly a theory in the general sense 
that it is a descriptive or explanatory function T which takes the universe U as an argument: 
T = T(U). However, instead of employing logical deduction to derive theorems from axioms, 
it employs logical induction to derive the overall structure of reality from certain necessary 
properties thereof (which are themselves deduced from the facts of existence and 

perception). That is, it derives the unique structure capable of manifesting all of the required 
properties.
2,3. Logical induction does not have to assume the uniformity of nature; it can be taken for 
granted that nature is uniformly logical. For if nature were anywhere illogical, then it would 
be inconsistent, and could not be coherently perceived or conceived. But if something 
cannot be coherently perceived or conceived, then it cannot be recognized as reality, and 
has no place in a theory of reality. So for theoretical purposes, reality exhibits logical 
homogeneity, and logical induction thus escapes Hume's problem of empirical induction. 
(Q.E.D.)
4. The CTMU is in fact the only theory that coherently allows for the mental dimension of 
existence and perception. It does this by ascribing to both mind and reality a common 
generic syntax including such basic ingredients as logic and arithmetic. If not for this 
syntactic interface, mind and reality could not interact, and observation and science would 
be absolutely impossible. Since observation and science are in fact possible, the structure 
of reality must incorporate this syntax and its entailments and grammatical products, which 
implies SCSPL and the CTMU. (Q.E.D.)
5. Analytically true statements merely instantiate tautologies. Again, "rose = flower" --> 
"flower = flower" --> "X = X" (a tautology). Circular definitions mirror circular logic, which 
relates to them as syntax.
6. The MAP is not an assumption. The MAP is implied by SCSPL closure. For if reality is 
not ultimately closed, then entities outside reality can be incorporated in real structures and 
processes; but in that case they are real, and thus inside reality. This contradiction implies 
that reality is ultimately closed with respect to all real relations and operations, including the 
definition operation as applied to reality itself. Hence, reality is ultimately semantically 
closed with respect to its own definition, and the MAP must hold. (Q.E.D.)
7. Logic works as a means of real-world functioning because it comprises the rules of 
cognition, and we know reality only insofar as it is cognitively accessible to us. Equivalently, 
logic is included in the syntax of SCSPL, and thus distributes over reality. This is very well 
explained in the PCID paper.
8. Again, the MAP is not an assumption; it is implied by SCSPL closure, a necessary 
property of ultimate reality.
(9. If you wish to avoid confusion, then don't address comments intended for me to 
somebody else. If the comments of others merit your attention, then they deserve their own 
responses.)
Further errors (10-17):
10. Rumpelstiltskin claims that the CTMU actually amounts to "little more than...a list of 
principles such that nothing in the world contradicts these principles", and is thus 
"completely trivial". [On the contrary, certain nontrivial conclusions have been drawn from 
the principles in question. Of course, in order to locate them, one would have to read the 
available material.]
11. Rumpelstiltskin claims that although the CTMU is "isomorphic" to "Everything happens 
by God's will, including his own existence", the truth of this statement comes to rest on the 
meanings of the various words in it. [This trivially applies to any statement whatsoever. 
However, words typically refer to content, and the content of theology cannot be dismissed 

a priori as mere semantics. To prove that theology is empty of content, Rumpelstiltskin 
would have to prove the nonexistence of God. But he has not yet done this.]
12. Rumpelstiltskin claims that the CTMU, like the statement to which it is isomorphic, 
suffers from Russell's paradox and myriad other logical syndromes. [SCSPL exhibits 
conspansive duality, which describes a relationship between two complementary forms of 
inclusion (descriptive and topological inclusion) which can be used to resolve the set-
theoretic paradoxes. This duality is implied by the necessity of SCSPL closure with respect 
to composition and attribution (see #6 above), being associated with the SCSPL properties 
hology and triality.]
13. Rumpelstiltskin claims that the CTMU does not, and cannot, break out from its own rung 
on the ladder of "metalevels." [By definition, it is the top rung of that ladder and thus 
subsumes all lower rungs (metalanguages subsume object languages). So the CTMU 
applies to all "metalevels".]
14. Rumpelstiltskin claims that although the CTMU includes an assumption which stipulates 
such a "breakout", this does not solve the problem. [Again, the CTMU contains no 
assumptions. Repeating an error after it has been identified as an error amounts to a new 
error.]
15. Rumpelstiltskin claims that I have insisted that the CTMU has no empirical content...or 
that if I have not yet done so, I should do so immediately in order to avoid "opening the 
floodgates of pseudoscience". [I have not said that the CTMU is free of empirical content. In 
fact, it has some direct and rather a lot of indirect empirical confirmation (through its natural 
integration of certain empirically-confirmed theories in its overall model-theoretic 
framework). However, because it explicitly does not rely on empirical induction or empirical 
confirmation, it does not purport to be empirical science, and thus cannot be called 
"pseudoscience".]
16. Rumpelstiltskin claims that Langan fails to take account of the fact that Quine pointed 
out the naivety and flawed nature of the CTMU half a century ago, and moreover, fails to 
take account of anything but his own musings. [Quine pointed out nothing of the kind. 
Furthermore, it is not smart for somebody without an extremely high level of intelligence to 
assume that by taking a handful of college courses presided over by institutionalized hacks, 
he or she automatically becomes better able to spot problematical issues than someone 
identified by the mass media as the "smartest man in (America, the World, etc.)"...even if 
one thinks that the mass media may be mistaken. In fact, it is so unintelligent that it 
comprises yet another error.]
17. Rumpelstiltskin claims this his error count is really a record of how many straws I have 
grasped at to save myself from sinking. [Not quite true. I've merely been encouraging 
Rumpelstiltskin to dogpaddle vigorously enough to barely stay afloat in the debate which he 
has insisted on having.] This brings Rumpelstiltskin's total error count to 12 + 8 + 8 = 28 and 
counting. Good work, Rumpelstiltskin! (By the way - if you object to this error count, then 
stop making errors, and I won't have to count them.) Asmodeus 18:00, 24 August 2006 
(UTC)
Hmmmm...Asmodeus says that if I want this error count to stop, I ought to stop making 
errors. Only, I don't actually think I am making errors. I am making points with which 
Asmodeus disagrees. This does not equate with "error". For after all, even if Asmodeus 
were the smartest man in the world, this would not imply that he is factually correct 

100% of the time. Much of the time, perhaps, but not all the time. A truly smart man 
could admit that.
1. That is by and large what I was saying. I did not say that the CTMU necessarily rules 
out deduction completely as a form of reasoning; merely, that it does not operate along 
those lines to induce its own truths. So if I made an "error", then so did Asmodeus 
and/or Langan.
2,3. Quote Asmodeus: "Logical induction does not have to assume the uniformity of 
nature; it can be taken for granted that nature is uniformly logical". This is a paradox. 
The assumption of "uniformity of nature" is precisely taking for granted that "nature is 
uniformly logical". It is making exactly this assumption with which Hume, and Popper 
after him, had such a problem. And I disagree that "if something cannot be consistently 
perceived or conceived" then it cannot be recognised as "reality". Because this depends 
on our definition of reality. Now I understand that Langan's particular definition of reality 
has this requirement, this uniformity of nature principle, and that this is one of the ways 
in which the CTMU is rendered "tautological". But this is not a necessary and self-
evident truth, as it would need to be for the CTMU to work. A synthetic truth, not 
analytic. But even if analytically true, nonetheless only true on the semantic level.
4. I see your point about the "interface" between mind and reality, but disagree that the 
CTMU is the only theory that meets the requirements. For example, but stipulating that 
arithmetic and logic are aesthetic impositions upon the environment, we can explain how 
reality miraculously appears to behave according to their dictates...and organisms which 
evolve viable ways of describing reality are organisms which can better survive in it, to 
breed and pass their viable ways on. Of course, this leads to the Kantian dichotomy of 
das Ding fuer uns and das Ding an sich in most applications, but it needn't. If reality is 
defined precisely as the human construct which models the human environment, we get 
around this problem. But then, the CTMU is not entirely true anymore, since the 
definition of reality is different (this possible reality allowing for the "anomalies"), and 
hence the CTMU is not tautological in the truest sense.
5. Yes, analytically true statements instantiate semantic tautologies. But they are not 
necessarily tautologous: I could define, in my peculiar dialect of English, rose to be "that 
feeling you get when five minutes feels like an hour". Now "that feeling you get when five 
minutes feels like an hour, is a flower" might be a tautology in the dreary realm of the 
crappy poet, but not in the real world. That is the point I have been making. The CTMU 
is only "tautologous" (and not really, even then) contingent upon the meanings we 
assign to words. Thus dependent on an indefinitely large semantic network. And contact 
with the external environment to establish the ultimate meanings of words, unless we 
are to fall into the same pit as Derrida, and live perpetually trapped by our language.
6,8, 11. The MAP corresponds to what might be called "nomological closure". But, the 
problem here comes in with the meanings of words - as Asmodeus points out in his 
point 11. Is God, for example, to be included in "reality"? Well, it depends on our 
definitions of "God" and "reality", doesn't it? This is undoubtedly theology, and no, 
theology is not necessarily devoid of content (I never said it is), but once again, the 
definitions Langan provides are not necessarily and self-evidently true. They are merely 
assumptions which work.
7. Is logic the "rules of cognition" or "laws of thought"? I don't think so, at least not in the 

sense of "physical law", anyway. It might be a "law" in the legalsense, a set of 
prescriptions to which we ought to adhere should we wish to behave rationally, but the 
vast majority of people spend the vast majority of their lives behaving decidedly 
irrationally, so I don't think that logic is quite that engrained into reality. Of course, it 
could be argued that "thinking rationally" is simply "behaving in accordance with the 
dictates of reality": I daresay Langan might offer this definition up. But, unfortunately, 
this is once again simply a convenient definition of words, a semantic trick, by no means 
necessarily true...true by argument, perhaps, but the CTMU doesn't rely on arguments 
or assumptions, does it?
10. Well, given the definitions of words which have been employed, we find ourselves 
drawing conclusions like panpsychism and panentheism from the CTMU's 
principles...but drawing them out of the CTMU's axioms doesn't require as much effort 
as Langan exerted: they fall out quite simply and naturally, because they are once again 
"definition-crunching". Langan certainly dresses them up in impressive pomp and 
splendour, but arriving at them is no real work; this doesn't make the conclusions 
necessarily wrong, of course. Merely not something magnificent.
12. Yes, I read this in the paper. I don't buy it. Show me a formal, rigorous proof of it, 
and that the SPSCL does not fall victim to these "logical syndromes", and you'll have my 
serious attention. But, of course, a rigorous proof does allow for a possible rigorous 
disproof. Once again, integrating the Wheeler-DeWitt equation yields a plate of fish and 
chips.
13. If the CTMU is at the "top of the ladder", then there is real trouble. Because, despite 
the convenient closure rule to the SPSCL, a metalanguage will be necessary to avoid 
the logical syndromes. In the absence of a formal proof to the contrary, of course. But 
then, we still have another problem, irrespective. Because of the problem with definitions 
which I highlighted above, there is another choice to make. Either, to keep itself at the 
apex without need for reference to a higher rung, the CTMU would need to include the 
"indefinitely large semantic network". But then, language cannot relate to empirical 
reality, since this would have to happen on the next level up (I know the CTMU says this 
won't have to happen, but it is a simple fact of model theory that it will). Alternatively, the 
level at which this happens will be the highest, but then the theory is no longer 
exclusively "top-down", and has some tangled meta-loops. So one way or the other, it 
doesn't work.
14. I have shown how many of the CTMU's axioms rely on assumptions.
15. Here's an "empirical observation" upon which the CTMU relies: reality behaves in a 
logically consistent fashion. How do we disprove it? One day when my coffee cup is 
both green and red, and there is no "higher court" of explanation to which we may 
appeal for the reason why. Which brings us by a commodius vicus of recirculation back 
to Hume's problem of induction and environs. And Popper's continuation of this 
discourse, and, dareIsayit, the realms of pseudoscience.
16. I agree with the institutionalised hacks bit...I am sure that even Langan cannot have 
such a loathing for institutionalised hacks as I do: he, after all, was not forced to vomit 
Afrocentrist postmodern neo-Bolshevik propaganda back at his lecturers in order to get 
a bachelor's degree. Or study fluid dynamics. No, for most worthwhile things I know, I 
have only myself to thank. However, I feel it pertinent to point out that the media 

labelling anyone as anything is not any form of indicator that the epithet is merited or 
true. And I never said that Quine refuted the CTMU, since he was writing when Langan 
was still a foetus. But, some of Quine's thoughts do make some very good points, points 
which can be effectively deployed against the CTMU (like the analytic/synthetic 
distinction); and Langan could have been aware of this, and taken it into account, simply 
by reading such heady treatises for his own edification...that's what I did, after all.
17. Asmodeus, you're right about me dogpaddling...because I can't swim. But I have to 
keep myself afloat somehow, as the sinking ship SS Catmew frantically empties her 
bilge tanks here on my talk page in a last attempt to avoid hitting the bottom of the 
ocean. :P
In a sense, perhaps, one might characterise the CTMU as a baroque form of idealism, 
where the reality-being-the-realm-of-mind is simultaneously defined to be the Godhead. 
This is a possible by-product of mind and reality being the two sides of the same coin. It 
does have an advantage over "conventional" idealism in that it does not require the 
existence of discrete, anthropomorphic souls. Which is bad for Langan if by some 
bizarre stroke of perversity I am ever proven wrong, since he will have a time of it 
excising my soul from the fabric of reality to put to his personal use. But, he's still a long 
way away from having to worry about such things, so no matter. Byrgenwulf 20:31, 24 
August 2006 (UTC)
Response
 
  #9
 
 :  Instead of generating the usual torrent of fresh errors, Rumpelstiltskin has 
evidently chosen to compound some of the errors that he has already made (not that he has 
made no new errors; it's just that he has done so by elaborating on some of his 28 existing 
errors). In order:
1. Since the CTMU relies on a combination of deduction and logical induction, its logic can 
in fact be checked (by someone capable of following it).
2,3: One can take the logical uniformity of nature "for granted" by implication rather than 
assumption. Again, the implication goes like this: if nature were anywhere illogical, then it 
would be factually inconsistent, rendering true indistinguishable from false and reality from 
nonreality, thus precluding coherent perception and cognition within the range of the 
inconsistency. But that which cannot be coherently perceived or conceived cannot be 
recognized as reality, and has no place in a theory of reality. So for theoretical purposes, 
reality exhibits logical homogeneity, and logical induction thus escapes the pitfalls of 
empirical induction.
4. The CTMU allows for the mental dimension of existence and perception because it is 
supertautological and therefore subject to complete operational closure. This is true of no 
other theory.
5. "A rose is that feeling you get when five minutes feels like an hour" is ultimately an 
impredicative definition and therefore a tautology; the "is" amounts to an equals sign. So as 
Gertrude Stein might have put it, what we have here is "A = B" --> "A = A" (where A = "a 
rose" and B = "that feeling..."). This may not instantiate a tautology for those who prefer the 
usual meaning of rose, but it still conforms to tautological syntax under your definition.
6,8,11. Again, the MAP is implied by the analytic closure of reality with respect to definition.
7. The laws of logic can be broken, but only at the expense of validity. Therefore, logic rules 
valid theoretical cognition.

10. You don't seem to appreciate the CTMU emphasis on definition. That's a problem for 
any philosopher, but especially for a philosopher who lives in a self-defining universe...i.e., a 
universe not defined by something external to it, for example, an absent deity. (Incidentally, 
while various nontrivial implications emerge quite naturally from the CTMU, this does not 
imply that "no real work" went into the CTMU itself.)
12. A formal, rigorous proof may well exist. But I doubt that you could easily follow it, and in 
any case, giving it here would violate WP:OR. So for now, it will suffice to note that it is 
impossible to map Russell's paradox irresolvably into the CTMU. Specifically, whereas the 
concept of self-inclusion is ambiguous and therefore troublesome in set theory - nobody has 
ever actually seen a set coherently "include itself" in the usual confused sense of that 
phrase - it has a more detailed interpretation in the CTMU. Thus, unlike set theory, the 
CTMU need not rely on mere semantic barriers to protect it from the associated paradoxes.
13. The CTMU is on the top rung of the ladder of metalanguages not because it sits still and 
blocks anything trying to get past it, but because in climbing past it, any higher 
metalanguage is forced to either recapitulate its essential structure, or lose all linguistic 
functionality and fall back to the ontological groundstate. Because the CTMU contains only 
the fundamental requisites of existence, nothing less can function as the language of reality.
14. You have shown nothing of the kind...not even once.
15. On the day when any point on the surface of your coffee cup is "both green and red" --> 
"green and not-green" --> "X and not-X" (a paradox), true will equal false and reality will 
equal nonreality. Logical thought, along with rational operations like proof, disproof, and 
empirical induction, will on that day depart from you, and as your mind evaporates and 
wafts forever away, you will no doubt use your very last bit of it to wish that you had 
something as solid as "pseudoscience" on which to plant your feet. (Of course, that day 
may already have come for you.)
16. The CTMU explicitly accommodates the Duhem-Quine thesis and its underlying 
analytic-synthetic entanglements, along with other advanced logical and model-theoretic 
relationships. So if the work of Quine can be effectively deployed against the CTMU, then it 
can be effectively deployed against logic and model theory themselves. (By the way, if you 
understand the pedagogical deficiencies of academically institutionalized hacks, then why 
are you trying to become one yourself, instead of "going the hard way" and actually working 
for a living like Langan? Some might discern a measure of hypocrisy in your obvious 
preference for effete ivory-tower cultism over honest, productive physical labor of 
measurable benefit to the rest of mankind.)
17. I hate to be the one to break the news to you, but sinking theoretical ships takes more 
than a broadside of catch-and-release red herring tooting on noisemakers and wearing 
"Property of Admiral Quine" stickers on their tails. If you really think that the CTMU has 
been vanquished by them, then swimmer or not, you need to dogpaddle out the hatch and 
uncork your periscope (darned seaweed gets in everything).
By the way, Rumpelstiltskin, I'm not referring to your assertions as errors just because I'd 
like them to be. I'm calling them errors because they display a (usually profound) 
misunderstanding of either logic, or of the CTMU in particular. You don't necessarily make 
them all out of sheer stupidity; you make them because you want so very badly for the 
CTMU to be mistaken, and to prove your imagined intellectual superiority to Chris Langan. 
Unfortunately, the validity of the CTMU is out of your hands, and Chris Langan is not within 

range of your intellectual firepower. My sincere and well-meant advice: think before 
attacking your keyboard. You don't want to make yourself look even worse than you already 
do. Asmodeus 14:08, 26 August 2006 (UTC)
Asmodeus, "rose = flower" is not a tautology. To see this, consider a real tautology, 
"unmarried man = bachelor". Now do you see why your example of a tautology is not 
actually a tautology? Let me explain, while any rose is a flower, it is not the case that 
any flower is a rose (that's because the set of roses is a proper subset of the set of 
flowers). So "rose" is not exactly equal to "flower". But any unmarried man is a 
bachelor, and any bachelor is an unmarried man. No doubt some similar elementary 
confusion underlies your misuse of the equality operator in statements like "Mind = 
Reality".
Obviously Asmodeus should listen to and learn from Byrgenwulf rather than always 
retorting with empty theoretic braggadocio. It's like someone who keeps bragging 
about a giant fish they caught, but every time you ask to see the fish all you get is 
another round of grandiose boasting about the fish. Despite being dressed in terms 
of formal logic, there is no demonstration that the "CTMU" is actually a theory of 
formal logic. If it were, there would be some way to test it. CaveBat 17:00, 27 August 
2006 (UTC)
Let's start at the end - and these numbered points are so tiresome, being a bookkeeping 
device for an invalid tally anyway. I do not "want so very badly" for the CTMU to be 
mistaken. It is mistaken, and naive to boot, a labyrinth of smoke and mirrors the kernel 
of which is not really very complicated. Your "psychological reading" of me, Asmodeus, 
is grossly flawed (did DrL help with it?), because despite what you might think, you do 
not know enough about me to accurately ascertain my motives, my capability, or my 
knowledge. I know that you have been on a few Google expeditions to find out all about 
me, but only the tiniest fraction of information pertaining to me is available online.
Moreover, you say that "Chris Langan is not within range of [my] intellectual firepower". 
Why? Because he is allegedly the "smartest man in the world"? That's not an argument! 
And if Langan were truly the smartest man in the world, he would see that. You see, we 
have to look at the grounds on which Langan earned this "title". He "breaks" 
conventional IQ tests (Stanford-Binet, Wechsler, etc.). Great. So do I. So do thousands 
and thousands of other people. The first time Langan wrote the Mega Test, he scored 
the minimum 42 correct for admission to the Mega Society. Now I know he came up with 
some argument about efficiency: "why waste time getting all 49 correct, when 42 will do 
for your purposes?" I know the feeling: I have often walked out of exams etc. early, not 
having bothered answering everything, because I know that what I have done will 
suffice. So if there were 7 really tough questions on the test, then there's no need to 
break one's head over them, since Hoeflin's questions (yes, I've seen his tests online) 
are of the sort where one knows whether or not one has answered correctly. When 
Langan took the test a second time, he scored higher, and it is primarily on this score, 
and on the wow-gee-whiz factor of his "conventional" IQ scores (since it is questionable 
whether Hoeflin's tests even measure IQ), that earned Langan this title.
In one of his interviews (I forget which), Langan is tested by a professional 
psychometrician who pronounces that he has "never seen anything like this" in his 
career. We are invited to be impressed by this pronouncement. But let's perform a quick 

Fermi calculation to see what this actually means. Let's say the chap has been doing his 
work for 20 years, and works 200 days a year. Let's further imagine that he performs an 
average of 3 IQ tests a day (since he will no doubt also counsel, consult, administer 
other psychometric tests, maybe lecture, etc.). Multiply it all together, and we find he has 
tested about 12 000 people. But what this means is that Langan would sit simply at the 1 
in 12 000 level: i.e., his IQ is at least about 165 (if I'm not mistaken - I don't remember all 
the details offhand, and am not bothered to look them up).
165 is high, but not phenomenal. I have little doubt that Langan's IQ is score higher than 
this: he is obviously very intelligent, after all. However, it should also be noted that the 
more IQ tests one does, the higher one's score will become...there is a certain pattern to 
the questions, and the more familiar one is with the pattern (and don't get me wrong - 
this does require intelligence) the quicker and more effectively one will answer the tests. 
But my point is that the grounds for him being declared the "smartest man in the world" 
are very shaky, indeed. There is, in all likelihood, no one "smartest man in the world". 
The popular press does have a tendency to over-emphasise superlatives, as we all 
know: the fastest car, bloodiest war, richest person, etc. There is no objective or 
scientific accuracy to many of the claims they make.
So, Asmodeus, while it is no doubt very flattering to Langan's ego to have earnt this 
epithet, we have to realise that it is not as well-earnt as he may choose to make himself 
believe. And anyway, scientific and philosophical theories are not judged on the IQs of 
their creators. They are judged on their merits. And sometimes, even an ignorant child 
can point out that the emperor has no clothes.
Moving on, I feel that, perhaps, now is the time to point something else out. I have not 
been "attacking my keyboard" without the requisite degree of thought, as Asmodeus has 
indicated. This discussion has not required much thought, overall, but nonetheless. 
What I wish to point out is this. Asmodeus has been gleefully tallying up my so-called 
"errors". However, I made a deliberate, glaring error which anyone who is familiar with 
the relevant theory, and on their guard for any oversight, omission, or bluff, as 
Asmodeus has pedantically proven he is, would not miss. Above, early on in this 
discussion, I spoke of "...reading down the diagonal of the Goedel numbering...". Now, it 
is common error among amateur aficionados of Goedel to associate the diagonal lemma 
with Cantor's proof of the continuity of the real numbers. But in actuality, Goedel used 
nothing like that system, and even the term "diagonal lemma" was used of his theory 
after the paper was published, because of its conceptual (not formal) similarity to 
Cantor's diagonal argument. In other words, that statement was bollocks, but bollocks 
that a sufficiently alert and informed reader would have picked up on. Curiously, 
Asmodeus didn't. It has not been added to his triumphal "error count", nor even casually 
pointed out. Since he was obviously on the lookout for errors, I can only assume that he 
is not sufficiently familiar with the subject matter to have picked up on my little bait. Of 
course, it would only be natural for Asmodeus to respond with "well, you've made so 
many errors, I can't count all of them and still have time to eat breakfast". Unfortunately, 
this "error" was really just begging to be called, as it was something that would 
incontrafutably prove me to be ignorant; it could have settled the matter there and 
then...so that response just doesn't cut it, I'm afraid.
Now, while the terms in which I made the point were bollocks (specifically the use of the 
word "diagonal"), the point still holds, of course. I was careful in that regard. This 

miraculous SPSCL language is a windmill at which quixotic thinkers have been tilting 
since the sixteenth century. All Langan has done with the CTMU is write down a series 
of platitudes about reality, derive some "requirements" from them, and then say that this 
SPSCL is a language possessed of these requirements. Smoke and mirrors. Moreover, 
as I have indicated, the paper is riddled with category mistakes: for example, we are told 
at one point that "reality is a relation". Then, we are told that "reality is a language", 
which, of course, contains relations. A category mistake. And more smoke and mirrors. 
There seems to be a fundamental misunderstanding (or blurring of the lines) between 
syntax and semantics - and this conceptual smudge carries over to the SPSCL.
The CTMU is filled with technical terminology. But often, this technical vocabulary is 
abused, and the manner in which it is deployed indicates a fundamental 
misunderstanding of the real meanings of the terms. For example, Langan speaks at 
one point of duality. He writes a little about the duality principle in projective geometry - 
the interchanging of vertices and edges - but then goes on to link this with the duality of 
vector spaces. However, these are two distinct (unrelated) meanings of the word 
"duality". Somewhat like that postmodernist writing about relativity, who confounded 
cultural relativity with special relativity. Had Langan been drawing obvious - or profound 
- mathematical conclusions in that section, the story would be different. But no, the 
situation arises purely from a very simple error, and ignorance (or disrespect) for the 
very specific and formal meanings which adhere to mathematical terms.
In short, then, the CTMU is based on two principle methods: the making of 
straightforward observations, and bluster. Similarly with Asmodeus' statements here. He 
has shown in his responses above that he spectacularly misunderstands many of the 
points I am making. For example, he still has not answered my concerns in points 2 & 3, 
about the uniformity of nature, without use of a circular argument; nor has any new 
information been added since the point was first raised. He just keeps on defining 
"uniformity of nature" with the use of the negation of the problem of induction. This does 
not, under any circumstances, constitute a valid or insightful argument.
Similarly with the "rose", "flower", "that feeling..." point 5. All Asmodeus did was make 
my point again for me, except that he seemed to think that what he wrote is different to 
what I wrote, and points out my "error". No, Asmodeus. My point is this. We once again 
have to draw the line between syntax and semantics. A statement like (A = A) is a 
syntactic tautology: the logical form of the statement renders it true. "A rose is a flower" 
is a semantic tautology, an analytically true statement, which is only true by virtue of the 
definitions of the words. Thinking about it, it is not even a tautology. Because "A flower 
is a rose" is not a tautology. And yet Asmodeus - model theory, propositional logic, and 
SPSCL virtuoso - has failed to raise this.
I very much do understand just how much the CTMU relies on definitions of words, 
Asmodeus. That's part of my problem with it. The CTMU can theoretically hold only so 
long as words have the definitions they do. But this detracts immensely from the 
"absolute" nature Langan claims for the theory. An argument has to be provided as to 
why Langan's definition of "reality" is the right one. Which means that it is not a 
necessary and self-evident truth. It is an assumption, the plausibility of which is 
established through rational argument.
It is also a sweeping generalisation to classify every academic as an "effete 
institutionalised hack". Just as invalid as claiming that no-one outside a university can 

have an intellectual life. There are many hacks with professorial tenure at the moment; 
many people who do not deserve the position, and whose thinking is so trapped in the 
ruts stamped out by the herd that the amount of new knowledge they may generate is 
virtually nothing. But by far the most productive generators of knowledge, insight and 
wisdom in this world are academics as well. Contrary to popular opinion, it is real work; 
just, perhaps, more pleasurable and satisfying than many other jobs, since one who is 
given to thought and theorising will do so anyway, and being paid for it is an added 
bonus.
I would encourage you, Asmodeus, to get off your high horse. There is no reason for 
you to claim an epistemological or intellectual position of superiority here. On the 
contrary. It is not for you to comment on my ability; not until you have proven your own 
beyond a shadow of a doubt: which is far from the situation here. It is an effective tactic 
to attack instead of defend, as is well known. And equally, it is effective to obscure what 
is actually going on with smoke and mirrors, and torrents of bluster. After all, it is largely 
thanks to the latter that the CTMU has been seriously considered by anyone at all. But 
Asmodeus, you can stop doing it here, since it simply does not work on me. Or anyone 
else with a modicum of understanding. And I shall see to it that no-one else here on 
Wikipedia is taken in or manipulated by it either. Your high level of intelligence may 
allow you to take many people in with this sort of conduct, including, perhaps, yourself. 
But it is not infallible, and sooner or later one really does have to ditch the fallacies in 
favour of the facts, which in the present circumstances are not on your side. Byrgenwulf 
17:44, 27 August 2006 (UTC)
I hadn't noticed Cavebat's response...where he raises the same point as I did about 
roses, flowers, and tautologies. And bluster. Thank you, CaveBat. It is reassuring to 
know that it is not me who is deluded here. Byrgenwulf 17:48, 27 August 2006 (UTC)
Response #10:
29. But Rumpelstiltskin, you and CaveBat are deluded. There are several kinds, and several 
associated definitions, of tautology, and we've been discussing at least two of them here. 
One, often specifically referred to as a logical or propositional tautology, is an always-true 
relationship of sentential variables in propositional logic. All of the other kinds of tautology - 
including impredicative, circular, or redundant usages of language, e.g., analytic statements 
which are true by virtue of definition alone - are related to logical tautology in a certain way. 
This relationship is exactly as I have described it; logical tautologies are perfectly general 
and stand a priori, whereas less general analytic statements, which relate to logical 
tautologies as to syntax, are not and do not. (I suggest that CaveBat let Rumpelstiltskin sink 
or swim on his own here - I'd rather not have to start a parallel error count for CaveBat.)
30. I don't claim to know anything factual about you. I only know what you claim about 
yourself. In fact, I have my doubts regarding your claims; for example, you seem to 
resemble certain other personae associated with a quasi-legendary "uber-troll" about which 
I was once earnestly warned. But that's another issue entirely.
31. You're mistaken about the number of items on the Mega Test, the "ceiling" of which 
Langan seems to have broken twice (once each for two contradictory normings - as I seem 
to recall, Langan has publicly disowned the Mega Test and its weirdly-vacillating statistics). 
Incidentally, that Langan can break standard (i.e., real) IQ tests is a matter of public record; 
that you do, on the other hand, is not. If I were you, I'd have myself professionally tested 

before leaping to such an improbable conclusion. After all, if you could break the ceilings of 
standard IQ tests, then your IQ would be over four sigma above the mean, i.e., 1 in at least 
30,000. It's not that I think you lack any intelligence at all; it's just that since you're so critical 
of Langan, I know that you wouldn't want to be guilty of overrating your own IQ.
32. When a veteran neuropsychologist tests a subject and says that he has "never seen 
anything like it", this wording may not mean that the subject has merely scored at the top of 
the observed range. Such wording may instead mean that the subject is markedly dissimilar 
to other test subjects, e.g., because the test ceiling was broken in an especially decisive 
way. You have allowed for only the first possibility, and that's an error.
33. I have just demonstrated that you have indeed been attacking your keyboard without the 
requisite amount of thought. Either that, or you have been making errors deliberately, which 
is hardly a brilliant debate strategy.
34. You state that while Langan's media billing is no doubt very flattering to his ego, it is not 
well-earned. Actually, I believe that Langan has said that he prefers to stand on his 
intellectual productions rather than on test scores. You claim that his intellectual productions 
are trivial and hollow, and thus that he has earned very little credit indeed. However, as we 
have already seen, you are not qualified to make such a judgment. Making authoritative 
judgments for which one is plainly underqualified is an error. (By the way, I can't help but 
notice your use of the phrase "the emperor has no clothes" - gee, I wonder where I've heard 
that before! Beware the company you keep.)
35. You assert that I erred in not making note of your use of Cantorian terminology in 
reference to the work of Godel. In fact, Godel is often explicitly said by mathematicians to 
have employed diagonalization techniques. "Diagonalization" has come to denote an entire 
class of mathematical methods of which Cantorian diagonalization is merely one example. 
Congratulations - in order to catch me in a supposed error, you have gone to the absurd 
length of admitting an error which you actually never made. Now, that's an error and a half! 
(By the way, it did strike me that you'd gotten your wires crossed regarding the 
diagonalization methods of Godel and Cantor. But this is the sort of befuddlement I've come 
expect from you, and properly explaining that particular error would have taken too much 
time away from your many others. So rather than uncovering what looked like a rat's nest of 
technical confusion, I moved on. Even if you planted this "mistake" in the full knowledge that 
Godel employed diagonalization, you've still erred by trying to blame your opponent for 
ignoring the bait.)
36. You assert that the above error (35) was begging for notice, inasmuch it would have 
incontrovertibly proven your ignorance and thus settled the matter once and for all. But if 
calling you on obvious errors were enough to settle this matter, it would clearly have been 
settled a long time ago.
37. You state that SPSCL is a windmill at which quixotic thinkers have been tilting since the 
sixteenth century. That's incorrect; in fact, nothing like SCSPL was described anywhere 
near the sixteenth century. The most advanced conceptualizations of logic and language 
were still far too primitive.
38. You say that the PCID paper is riddled with category mistakes because it states both 
that "reality is a relation" and that "reality is a language" which contains relations. But this is 
not a "category mistake", because (1) language is indeed relational in multiple senses, and 
(2) the term "relation" is not defined in terms of level or category. In fact, as almost 

everybody but you seems to be well aware, there exist relations of relations of ...(keep on 
going)... of relations. That's why logicians distinguish orders of relation.
39. You say that the interchanging of vertices and edges in projective geometry (and graph 
theory, etc.) is unrelated to vector space duality. This is an error of which only a 
mathematical ignoramus could be guilty. To see this, consider that in category theory, 
regarded by many as the most general field of mathematics, morphisms are represented by 
arrows, i.e., directed line segments resembling edges, while objects, i.e. the points 
connected by the arrows, are thus represented by vertices. This is not an accident of 
notation; vertices and edges correspond to unary and binary relations or mappings 
respectively. That this general relationship leads to different forms of duality in different 
mathematical contexts - for example, that the dual space V* of scalar mappings is as much 
a vector space as V itself, which contains the vectors thereby mapped - is obvious, and it is 
just as obvious that Langan was referring to this ubiquitous mathematical relationship.
40. Apropos of the distinction between semantic and syntactic tautology (see 29 above), 
you state in so many words that the CTMU is mere semantics. But in fact, the CTMU is a 
theory of the entailments and implications of syntax. This syntax, including logic, arithmetic, 
and other fundamental components, is defined to consist only of distributive (absolute) 
ingredients of reality. This is exactly the basis of SCSPL, and exactly why the CTMU 
constitutes necessary truth.
41. You state that I have not addressed your points regarding the uniformity of nature 
without using circular argumentation, and have thus failed to bring any new information to 
the controversy, merely defining "uniformity of nature" by negation on the problem of 
induction. But I have not "negated" the problem of induction. In fact, I have observed that 
the problem speaks to the unavoidability of circular argumentation in induction, and that the 
best way to deal with this problem is to base the circularity on a priori syntax - in particular, 
logical tautology - rather than a posteriori observations and semantic word games.
42. Being paid to think and theorize is not just a "bonus" for the vast majority of academics; 
it accounts for the bulk of their motivation. An intelligent person who merely wishes to think 
and theorize for the joy of it can easily do so, and equip his or her mind for that purpose, 
without paying many years and tens of thousands of dollars to the university system for the 
privilege of kissing the asses of the institutionalized hacks who pull almost all of its strings.
43. You say that I have "no reason ... to claim an epistemological or intellectual position of 
superiority here." On the contrary, you have thus far given me at least 43 such 
reasons...well, 44. To wit:
44. In the present circumstances, all of the facts are inarguably on my side. (And thanks!)
That's sixteen (16) more errors, a new one-day record which brings your total error count to 
a high-caliber 44. Since you seem to be on a roll, would you like to try for an even 50? 
Asmodeus 23:22, 27 August 2006 (UTC)
Back to the numbering, are we? Very well, then...I am not going to address all of them, 
lest once again we get dragged into a pantomime, so I shall attempt to confine myself to 
the more "important" points.
29. Asmodeus, I am glad to see that you finally distinguish between different uses of the 
word "tautology". Only, it is my contention that the "tautologies" applied in the CTMU are 
of the weaker kind: they are not "perfectly general and a priori" true, as a logical, 

syntactic tautology (life is life) is. Which means, if they are not a priori true, then they are 
not inviolate, as Langan claims.
30. Since you bring it up, I am curious now...who is this "über-troll" of whom you speak? 
Grendel?
31. Asmodeus, 1 in 30 000 people simply means 1 of about 200 000 people on this 
planet (taking 6E9 people altogether). And yes, I have been professionally tested a 
number of times in my life, and know I am above the 4 s.d. level. I just don't attribute 
particular weight to IQ, that's all, and that's why I have not seen the need to blow my 
own trumpet and join all kinds of clubs and societies, rather preferring to get on with life.
34. In what way am I not "qualified" to make a judgment on Langan's theory? I am 
perfectly "qualified" to do so. Are you telling me that there's this guy with a high IQ but 
no qualification, who is entitled to come up with theories and demand serious 
recognition for them, but that the only people who can criticise his theories are tenured 
professors and/or those who have met with the explicit approval of his avatar 
Asmodeus? It doesn't work like that, I'm afraid.
35. Bollocks: I did not just refer to diagonalisation, I explicitly referred to "reading down a 
diagonal".
37. No, sixteenth century thinkers were not looking for something specifically called 
"SPSCL". But since that time, and even possibly before, thinkers have been trying to 
artificially construct a "perfect language" in which reality can be completely and perfectly 
expressed. The SPSCL claims to be such a language. And it is just as quixotic and 
naive as the attempts of those earlier thinkers.
38. This still doesn't address Langan's mistake. Because while language undoubtedly 
contains relations, yes, and there are various "orders" of relations, indeed, it is 
nonetheless incorrect to insist that something can be both a relation and a language at 
the same time. It's a bit like saying that something is both a wheel and a car. It doesn't 
work.
39. Now you're really parading your ignorance. You see, finding the dual of a graph 
involves swapping faces and vertices around. But, the dual of a vector space is still a 
vector space (not a vector). So, if we were to symbolically represent functions for finding 
duals of graphs, we might say D1:F--->V and D2:V--->F. We have two groups of objects, 
and the "dual functions" map objects from one group into the other group. But, with 
vector spaces, the "dual functions" D1:W--->W (where W is a vector space) and D2:V---
>V (where V is a vector) map objects from one group into other objects from the same 
group. The functions (morphisms) are not, cannot be, the same, and thus there is no 
single category theoretic conception of duality which covers both areas. I did not say 
that projective geometry and vector spaces are completely unrelated (of course not: one 
fascinating point of intersection is matroids). I said that the notion of duality is not the 
same. And I am right. Ask any competent mathematician. Also, vertices and edges do 
not correspond to "unary and binary relations" respectively. That's nonsense. I can 
assure you that I do not need lectures in category theory from you. Especially since 
anyone taking information from you is bound to come away confused and misinformed.
40. No, that's what Langan says the CTMU is. But that is not the actual state of affairs. A 
statement like "Mind=Reality" is not true by virtue of syntax. It is true by virtue of the 

meanings Langan gives the words "mind" and "reality". It is a semantic tautology which 
holds under certain definitions. Thus the CTMU does not constitute necessary truth. 
Perhaps I ought to point out here that just because Langan says something, does not 
mean it is true. It seems that some people, possibly Langan himself, do not understand 
this.
41. Maybe you don't understand the meanings of the terminology used here. The 
problem of induction refers to the fact that we cannot know with certainty whether or not 
the conclusions of inductive inferences will be true (because we have to tacitly assume a 
"uniformity of nature principle"); it is a "problem" because if our conclusions to otherwise 
rational thought processes are not true, then sometimes our reason is less than 100% 
"accurate and coherent". Your solution is simply to say that "if nature were not uniform, 
then our thoughts could not possibly be coherent; therefore, nature is uniform". It turns 
the problem into its own solution. That is circular and invalid. No matter how many so-
called "tautologies" go into making it up.
43, 44. If you truly believe that, Asmodeus, after all that has transpired, then I pity you.
94.23452345 Something just struck me...the notion of "conspansive duality" is defined 
using the "Venn-diagram like" structures formed when looking down light cones on 
Minkowski diagrams, orthogonally to the world line at that particular point. Only, why, 
Asmodeus, do we use Minkowski spacetime and not Galilean? Because of relativity, 
yes? Special relativity. But SR is based on two principles: the equivalence of observers, 
and the "law of light". Now, the equivalence of observers is a good, sound philosophical 
notion (but not "tautological"). But the "law of light" is based entirely on empirical 
considerations: the measurements conducted in Michelson-Morley type experiments, 
etc. Without this empirical observation, we should be using Galilean spacetime, not 
Minkowski. Light cones would have no meaning. Yes, we could introduce some other 
theoretical limit to the speed of transmission of information to resurrect light-cone like 
structures, but we have no (tautological!) a priori reason for doing so (and Langan 
doesn't even try giving one in his paper, since he's apparently ignorant of this problem). 
So conspansive duality, a vital component of the CTMU, actually relies on an empirical, 
scientific observation! So much for "supertautology"... Byrgenwulf 08:13, 28 August 
2006 (UTC)
97.4586758 I went back to the CTMU paper just to check on this last point. In fact, 
Langan, does introduce an excuse as to why we have light-cone structures, so that we 
don't have to rely on the empirical derivation of SR...we are told that "Conspansion is a 
global process which cannot be locally differentiated as to rate. Its rate is thus a globally 
invariant “time-space conversion factor”, and because all changes of local state must be 
expressed in terms of it, it is maximal. This invariant maximal rate of production is 
referred to as the rate of conspansion c and can be physically identified with the speed 
of light in vacuo". We are not given a proof as to why "conspansion cannot be locally 
differentiated as to rate", of course, but simply asked to accept this (rather circular 
statement, since the reason for the global/local dichotomy in the first place relies on the 
result). Also, it does not mathematically follow that "because because all changes of 
local state must be expressed in terms of it, it is maximal"...that is "let's pretend" maths. 
There is no logical reason why it should take any value at all. I can express the entirety 
of physical laws in terms of the golden ratio, for example, but this does not imply 
anything about the golden ratio itself. Isn't it also just so convenient that we can identify 

the "rate of conspansion" with the speed of light? Whyever would we want to do that? 
There is no reason to assume that the two are the same thing...just because the speed 
of light is an observed maximum. The whole argument is more circular than a circus 
ring. Byrgenwulf 12:23, 28 August 2006 (UTC)
101.54645654 Reading this paper again is like another can of worms. Asmodeus said 
above that "nobody has ever actually seen a set coherently "include itself" in the usual 
confused sense of that phrase - it has a more detailed interpretation in the CTMU". 
Funny, that, considering that Langan defines the SPSCL's global processor Γ to contain 
a set O of "active reflexive objects", which in turn contains Γ. A big deal is made of this 
fact, but in no way is any provision made which circumvents the usual problems of self-
inclusion. We are simply told, and asked to believe, that it is not a problem in the SPSCL 
because the SPSCL is an übermetalanguage which does not suffer from the mere 
logical afflictions which plague other, similar languages, blather-blather-blather... 
Byrgenwulf 12:57, 28 August 2006 (UTC)
Asmodeus, can you please explain exactly how your example of a tautology "rose = 
flower" (so by the commutativity of equality "flower = rose") is a tautology given that not 
all flowers are roses? Are you saying in your last responce to me that your example is 
tautological a priori even with the given empirical referents? CaveBat 18:23, 28 August 
2006 (UTC)
That which we call an error would by any other name. . . . Anville 18:26, 28 August 
2006 (UTC)
Response #11
 
 :  Ah, but there was a substitution involved...the replacement of a general 
symbol with a more specific symbol. We started out with "a rose is a flower": A = B. (Forget 
about Rumpelstiltskin's alternative definition "a rose is that feeling you get when five 
minutes feels like an hour"; it's beside the point.) Then we substituted "rose" for "flower" 
within the scope of the "=" sign, giving us A = A, i.e., "a rose is a rose"; that's what the 
Gertrude Stein allusion was about. "A rose is a rose" is an unequivocal tautology, and there 
is nothing further to squeak about. (Your question hardly merited an answer, but I didn't 
want to leave you hanging. ;-) Asmodeus 21:02, 28 August 2006 (UTC)
Asmodeus, you're painting yourself into a corner again. "A rose is a flower" is not a 
tautology. Because the "is" in that sentence does not analytically translate into an equals 
sign. It translates into subset: "roses form a subset of the set of flowers". That really is a 
frightfully elementary mistake, Asmodeus, and attempting to obscure it with blather 
doesn't help any. Language can be tricky, for some, I know, but really...funny, though, 
that in many cases the CTMU makes similar mistakes. Byrgenwulf 21:42, 28 August 
2006 (UTC)
Response
 
  #12
 
 :  It just gets better and better with you, doesn't it? Read carefully. "A rose is 
a flower" is definitely a tautological statement. How can you tell? Because "rose" is the 
subject, and "is a flower" contains only information that is already implicit in "rose". This is 
called redundancy, and it makes the statement a tautology...as much a tautology as "a rose 
is a rose" (because these two statements contain exactly the same amount of information 
regarding the subject, and this amount of information is already implicit in the subject 
alone). Not a propositional tautology, mind you, but a tautology nonetheless. (If you don't 
believe me, try a good online dictionary. I recommend OneLook.)

Now, say that we change the above statement to "the flower is a rose." The subject is now 
the noun phrase the flower, and the predicate is a rose tells us what kind of flower we are 
dealing with. Because the predicate carries information not present in the subject alone, it is 
not as tautological as the first statement. However, it may surprise you to learn that 
although it is less tautological than the first statement, a case can still be made that it is 
"somewhat tautological", because it tells us that the flower is a specific kind of flower, when 
we already knew that the flower must be some specific kind of flower by definition. That is, 
since rose implies flower, it carries a combination of new and redundant information when 
attributed to "the flower". So you see, this strange, oversimplistic idea that you can always 
draw a sharp line between tautological and nontautological statements does not hold true in 
the real world. It might help occasion a chorus-line hot dog dance with your knucklehead 
buddies from the Unofficial Anti-Pseudoscience Executive here at Wikipedia, where 
everybody's an expert who really really wants to be, but it won't fly with anyone who actually 
knows what he's talking about. (Your error count is now 45, and when I get around to your 
last string of howlers, it will of course go even higher.) Asmodeus 00:57, 29 August 2006 
(UTC)
Asmodeus, please mind your civility. Witty, subtle digs are one thing, but "knucklehead" 
really just is too straightforward, don't you think?
Anyway, you're still wrong. Here's why. You are saying that "a rose is a flower" is a 
tautology as a figure of speech. I prefer the term "reduncancy" for that concept. But even 
then, it is not completely redundant, since the information contained in the predicate is 
less than the information contained in the subject.
We could say "a rose is a flower with x and y characteristics". This follows the old 
Aristotelian conception of a definition as containing a genus and differentia. That way, 
we could interchange the subject and object ("a flower with x and y characteristics is a 
rose") and still have a true statement. You have had to "cheat", you see, in what you 
wrote above, by swapping an indefinite article with the definite article, in order to get "the 
flower is a rose" to be true. And "the flower is a rose" is not in any way a tautology...that 
is complete nonsense. There are no "degrees of tautology": it is an attribute which a 
proposition either does or does not have.
All of that aside, it most certainly is not the case that "a rose is a flower" is necessarily 
true. As we have seen, I can propose an alternative definition for "rose", which would 
render the statement false in certain circumstances. This is my point about the CTMU. 
Even if principles such as "Mind=Reality" are semantic tautologies, it does not make the 
theory necessarily true. Only a logical or syntactic tautology can do that, Asmodeus. 
Two distinct meanings of the word "tautology" (which as I say is why I prefer 
"redundancy" for the figure of speech). And the statement "Mind=Reality" is not 
necessarily true, in the sense that {P or not-P} is. It is only true as long as a rose is a 
flower. Which just does not cut it.
Finally, you see fit to tell me that in a tautology, the maximum amount of information is 
already encoded in the subject. Great: this is a point I made way back at the beginning 
of this discussion, on the CTMU's talk page, even, if I recall. Thus, if that is to hold, are 
we to then accept that the CTMU contains no more information than that encoded in its 
three "tautological axioms"? Byrgenwulf 06:45, 29 August 2006 (UTC)
Response
 
  #13
 
 :  You're talking with your mouth full. In case you don't know what I mean, 

consider the following exemplary dialogue.
Adult: "Junior, I've caught you with your hand in the cookie jar for the last time. You've been 
naughty and you're going to be punished."
Junior: "Wait a minute (crunch!). Why is it a 'cookie jar'? Why not a 'pickled herring jar' 
(munch, crumble)? The jar could hold anything. It is a 'cookie jar' only because you have 
defined it as such. Your accusation is based solely on a choice of definitions. It is a 
semantic ruse! (gulp, smack!) And please mind your civility!"
The sentence "A flower is a rose" is either simply false (inasmuch as a flower can be of any 
kind whatsoever), or it is redefinition of flower which ignores any previous definition. In the 
latter event, we begin with the undefined term flower and then define it by synonymy with 
(the so far empty term) rose. Since flower is initially undefined, we are merely equating two 
empty terms without increasing the information content of the definiendum. Due to the form 
of the statement, it is a semantic tautology, information-free and meaningless. To enable 
the addition of any information at all, we must assign informational content to rose and use 
the definite article "the" to identify the specific subject to which this information is being 
attributed. This is why the definite article the had to be added to the second statement, and 
it in no way impairs the conclusions drawn on that basis.
Similarly, "A rose is a flower" is true by definition. Were you to redefine "rose" - e.g., "A rose 
is X" - then the above reasoning would apply; either your definition is information free, or it 
contains information permitting its misapplication and subsequent falsification. In the former 
case, we clearly have an empty tautology...a mere synonymy of empty terms. And in the 
latter case, we still have a tautology, because in looking at any particular rose, we know 
immediately that it is an X because you have previously defined it as such. In recognizing it 
as a rose, we have automatically recognized it as an X; X is already built into our 
recognition routine, where it has been implanted in the form of the definition "A rose is X".
Until you can handle this sort of reasoning, you don't stand a snowball's chance in hell of 
understanding the first thing about the CTMU. So why bother arguing about it? Be a good 
student, take a few more elementary logic classes, listen very closely to everything that the 
hacks say to you, and then come back with a degree and another debate challenge for 
Langan. That way, Langan may at least be able to rationalize the ensuing waste of time as 
an opportunity to squash a real card-carrying "acadummy" (as he has been wont to label 
snooty professional academics). Asmodeus 15:30, 29 August 2006 (UTC)
No, Asmodeus, I can handle this sort of reasoning, I just refuse to pretend that it is valid 
reasoning, that's all. I know the CTMU's based on it, of course. That's why the CTMU is 
so grossly flawed.
You see, you have now outlined two options: either a statement is devoid of information, 
in which case you say it is tautological. Alternatively, a statement which predicates one 
variable of another using the verbal relation "is", which you insist must be analytically 
translated as "=", and has the potential to be falsified, you say is also a tautology. That's 
bollocks. A tautology cannot be falsified, Asmodeus. That's the point of it. I cannot falsify 
{P or not-P} without evaporating my sanity.
You are completely and utterly wrong, and are just digging a deeper pit for yourself by 
not backing down and admitting it. While certainly there is another, almost unrelated 
usage of the word "tautology", as a figure of speech indicating redundancy, if the CTMU 
is only based on this idea of tautology then it is not "necessarily and absolutely true", 

because it relies on definitions of words (which are subject to argument and change).
Furthermore, words can be very tricky, Asmodeus...you don't seem to realise the 
slipperiness and subtlety with which we English speakers use the verb "to be". The 
sentence "a rose is a flower" can actually be interpreted a number of ways. One 
possible way is to ignore articles, as you seem to have been doing, and interpret it as 
the equating of two sentential variables: ("a rose" = "a flower"). This is not only not a 
tautology, but it is incorrect, as well. Because the two words have different meanings. 
Or, we can interpret as "a rose is a type of flower", in which case what I said about 
subsets above applies. But there is no way that if (rose = A) and (flower = B) that the 
sentence can be translated as (A=B) and still be tautologically true given the standard 
meanings of the words involved (or any other specified meaning, unless the exact same 
meaning is given to both words).
The only way in which "a rose is a flower" can be truthfully rendered as a semantic 
tautology is if we say "the set of roses forms a subset of the set of flowers", because it is 
not a complete definition: it contains the genus only, with no differentia. That latter 
sentence (with the sets/subset wording), can be said to be analytically identical with a 
possible interpretation of our example sentence, and it is tautologically true. You see, 
because "X", in "a rose is X", is not actually "a flower" but "a species of flower", the bold 
bit being implicit (understood) in everyday speech. But you cannot seemingly 
acknowledge this.
We could also introduce a differentia to the statement, as I explained above, to read "a 
rose is a flower with x characteristics"...that statement is tautologically true, just like "a 
bachelor is an unmarried male", where "male" is the genus and unmarried the 
differentia. A quick test, which you might find helps, Asmodeus, is to see whether in any 
given sentence one can replace all instances of one word or phrase with the other and 
still have the same semantic meaning. Wherever I read "bachelor", I can drop in 
"unmarried male" and achieve the same net denotative effect (although connotations 
differ - even more so, as we shall see, in the case of the rose, which is positively laden 
with connotations in English literature). But I can never substitute "rose" with "flower" in 
a sentence and have an identical semantic message. Never.
I actually cannot believe that I have to explain this elementary analysis to someone as 
allegedly intelligent as yourself. It's pathetic. Maybe you should fork out some of your 
hard-earned money and take some night classes in philosophy and logic, since while 
you clearly seem to think you understand these things, you are making it transparently 
clear that you do not (I, on the other hand, have a degree, which included three years of 
formal logic: not that that is even necessary when correcting an error this elementary). 
Being able to deploy polysyllables as the correct part of speech in vaguely the right 
context does not equate with legitimately know what they mean. But Asmodeus and/or 
Langan seems to think that by using lots of them, they add legitimacy to what he writes. 
This does not work.
Asmodeus, I strongly suggest you think carefully about how you are embarassing 
yourself here, and act accordingly. I know it is difficult for some to concede a point 
without losing face, but I can assure you that things will only get worse if you continue 
pressing this. You are just plain wrong. Interesting, also, that you said above that 
semantic tautologies are "information-free and meaningless", when it is, of course, on 
alleged semantic tautologies that the CTMU is based - not that a statement like 

"Mind=Reality" is even a true semantic tautology, of course, but anyway... Byrgenwulf 
16:15, 29 August 2006 (UTC)
Response
 
  #14
 
 :  Good grief - you really don't get it at all, do you? No statement which is 
syntactically well-formed can be falsified in and of itself. In order for a well-formed statement 
to be falsified, it must convey information, and to do that, it must be interpreted. The 
interpretation can then be falsified. That's why we introduced the definite article to which 
you objected so strongly; it implies the interpretation of the subject in a specific referent, and 
it is this interpretation that is liable to falsification. Given your evident inability to distinguish 
between syntax and interpretation, nothing you've said here makes any sense at all!
As far as concerns the meaning of "is", are you sure you're not related to Bill Clinton? Your 
personal confusion about the term "is", which is indeed used to describe both attributive and 
identity relationships, counts for nothing here. The plain fact is, you don't understand what a 
tautology is, you don't understand the distinction between syntax and semantics, and you're 
blustering for the sake of diversion. It's not fooling anyone whose opinion counts for 
anything; anyone who is fooled by it is just as unqualified as you are.
Wow...talk about "embarassing"! I'm surprised you can even show your face at the front of a 
classroom (if that's actually what you do). Asmodeus 16:47, 29 August 2006 (UTC)
Are personal attacks and rudeness a common debating technique amoungst CTMU 
proponents? Jefffire 16:53, 29 August 2006 (UTC)
I see that the karma police have arrived. Just to clarify: first Rumpelstiltskin said that I was 
embarrassing myself. Then I pointed out that Rumpelstiltskin is the one who should be 
embarrassed. If I was uncivil, then Rumpelstiltskin was uncivil. Please be more evenhanded 
in your judgments - it's not nice to disruptively game the system. Asmodeus 16:59, 29 
August 2006 (UTC)
Because two wrong make a right... Jefffire 17:01, 29 August 2006 (UTC)
Asmodeus, cool it. I didn't even bother addressing sentences of the form "a/the flower is 
a rose" in my previous comment, so what you said about definite articles is irrelevant. I 
spoke exclusively about sentences like "a rose is a flower"...however, since you bring it 
up again, let me address it.
I never said that a sentence can be falsified "in itself"...when did I say that? But, some 
sentences have the potential to be falsified, while others do not. For example, "I am 
drinking coffee" may be falsified, but "either I am drinking coffee or I am not" is always 
true: a tautology.
"The flower is a rose" has the potential to be falsified, yes. Thus it is not a tautology. 
Even when said of some specific flower which is, in fact, a rose, the sentence is not 
"tautological". The sentence itself, both syntactically and semantically, is not 
tautological, because a tautology is true irrespective of the interpretation of its 
constituent terms: get yourself a first year logic textbook and look it up. So the blab 
about definite articles is irrelevant, a red herring.
All semantics is, in this context, is the web of relations between the various logical 
constants we call "words". What this means is that sentences that are not syntactically 
tautological (like {P or not-R}) have the potential to be tautological, if our semantic web 

tells us that {P=R}. But it is not the case that "a rose is a flower" falls into even this 
category, since d(rose) =/= d(flower), where d(x) is a function which returns the semantic 
denotation of a word (let's try to be as pretentious as the CTMU, shall we, and decorate 
our text with superfluous mathematical terminology).
Unless, you wish for the CTMU's "supertautology" to be not a logical tautology but the 
figure of speech, i.e. reduncancy. It is super-redundant, I'll grant you that, in that it 
manages to say exactly the same identical thing in a myriad of various ways without 
actually saying a lot of thing at all. But that doesn't make it a "necessary, absolute 
Truth", it just makes it tiresome.
The little bit about "not fooling anyone whose opinion counts for anything" is a textbook 
example of the "no true Scotsman" argumentative fallacy. And what is your obsession 
with my qualifications? Let us not forget that at the end of the day I am more qualified 
than Langan.
And there is no bluster in what I wrote: only the facts of this discussion. Nothing I wrote 
betrays a misunderstanding of the various terms...on the contrary, I was highlighting 
your own misconceptions, which are clearly so deeply rooted that you cannot see where 
you are wrong. But I can assure you that anyone vaguely proficient in this topic will 
agree with me on this...this flower/rose thing is so unbelievably simple it is absurd that 
we are having this discussion.
I think you have bankrupted yourself, Asmodeus, and that is why you are resorting to 
incivility again - and I wasn't the one who started it: first you obliquely referred to the 
members of Wikiproject:Pseudoscience as knuckleheads and apes, then you came up 
with a metaphor about me being a child with his hands in the cookie-jar, and then you 
polished your little rant off with an attack on my qualifications. Only then did I say that 
you were embarassing yourself - which is not really uncivil, but more an attempt to try to 
get you to see reason. Byrgenwulf 17:40, 29 August 2006 (UTC)
Asmodaus, you've said above that "A rose is a flower" translates to "rose = flower". 
Then you say that if we reduce that alleged tautology to empty terms "we clearly have 
an empty tautology...a mere synonymy of empty terms". Reducing your example to such 
a synonymy of empty terms yields: x = y. So you're saying x = y is a tautology. But 
neither x = y nor the accurate translation x e Y (where 'e' denotes set membership) and 
thus the predicate term Yx are tautologies because there are semantic substitutions that 
can make them false. That's one thing Byrgenwulf has been trying to explain to you. 
Real logical tautologies, like for example (p -> (q -> p)), remain true under any 
substitution. I think your error in assuming that x = y is a tautologous synonymy of empty 
terms explains your belief in statements like "Mind=Reality". CaveBat 17:51, 29 August 
2006 (UTC)
Response 
 
 #15 (to Rumpelstiltskin)
 
 :  If definite articles are irrelevant, then why did you 
complain about the one which appeared in "the flower is a rose"?
Clearly the had some sort of impact, or you wouldn't have objected to it (as we've already 
seen, it signals a very relevant operation, namely, interpretation.) On the other hand, if it 
was relevant then but is irrelevant now, then your present comments are diversionary, 
serving only to steer the discussion as far as possible from its original course. Now, why 
would I be interested in allowing you to do that?

You go on: "The sentence itself, both syntactically and semantically, is not tautological, 
because a tautology is true irrespective of the interpretation of its constituent terms."
Only propositional tautologies are "true irrespective of the interpretation of their constituent 
terms." That is, their object-level interpretation conveys no information. This is precisely why 
propositional tautologies are perfectly general, which is why they relate to arbitrary well-
formed expressions as syntactic generalizations...i.e., as syntax.
On the other hand, if a statement is capable of conveying information, then some of its 
possible interpretations must be falsifiable. Yet, even though I gave you a link to a good 
online dictionary site, you still maintain that a falsifiable statement - a statement which has 
the potential to convey information under some valid interpretation - cannot possibly be 
tautological. That's incorrect. It can be tautological to the precise extent that it is redundant, 
nevertheless conveying some additional amount of information in the bargain. That's 
because redundant and nonredundant information can appear in one and the same 
expression, and in real life, almost always do.
Logical tautologies distribute over well-formed expressions in the same way that the syntax 
of a language distributes over the expressions of that language: independently of meaning. 
The truth predicate can be attributed to an expression containing elementary logic functors 
or their various semantic derivatives only because tautologies define the relationship 
between elementary logic functors and the truth predicate itself. Where this relationship 
does not hold, semantic and/or syntactic truth is quite out of the question.
In other words, you still don't appear to understand the proper relationship between 
syntactic and semantic tautology, which reflects your evident reluctance to use an English 
dictionary. Unfortunately, your inability to distinguish and properly relate different kinds of 
linguistic circularity is in the present context a dialectical brick wall. Until you show some 
glimmering of understanding regarding these distinctions and relationships, discussing the 
CTMU with you is nothing but a waste of my time. At this point, all you're doing is rambling 
for the entertainment of you and your peanut gallery.
I don't have any more time to waste on your low-grade nonsense. I'm doing you a favor by 
debating you at all, and the bottom line is still just this: you have now made at least 45 
distinct errors on which I have bothered to specifically call you. Wait...46. To wit:
46. "Let us not forget that at the end of the day I am more qualified than Langan."
Why, sure you are! ;-) Asmodeus 19:26, 29 August 2006 (UTC)
Asmodeus, you seem to forget that a number of times I have explicitly addressed the 
other meaning of "tautology", as a figure of speech which denotes redundancy in a 
sentence. Only, the use of that figure of speech is not really an impressive feat.
But let me recap quickly. The CTMU is claimed to be based on tautologies. As 
Asmodeus points out, dictionaries give two main meanings for the word "tautology". So, 
either the CTMU can be based on propositional tautologies, or on tautologies-as-figures-
of-speech, i.e. redundacy. We have seen that the so-called tautological principles of the 
CTMU are not tautological in the logical sense, as they are not statements that are true 
regardless of the interpretation of their terms.
This means they might still be tautologies in the other sense, the figure of speech sense. 
But they're not. This other meaning of the word "tautology" indicates things like "he 
drove a huge, big car". But principles like "Mind=Reality" are not based on this other, 

rhetorical definition of the word tautology, either. And even if they were, these sorts of 
tautology do not guarantee necessary truth: he could have been driving a little, tiny car. 
Only a logical tautology can guarantee necessary truth. Therefore, the CTMU has no 
claim to being based on necessary truths. Therefore, given Langan's obvious confusion 
about the definitions of many, many terms, it is complete bollocks.
But just to appease you, here's a dictionary definition of "tautologous" (this dictionary 
defines "tautology" as the noun form of the word "tautologous", so the real information is 
to be found there). Also, Merriam-Webster is at least a vageuely trustworthy dictionary 
(while still not quite the Oxford). We are given two meanings, which I duplicate here 
verbatim (including links):
Involving or containing rhetorical tautology: REDUNDANT
True by virtue of its logical form alone.
where the definition of a rhetorical tautology, is, of course:
1. a: needless repetition of an idea, statement, or word.
So, we can already rule out number two: "a rose is a flower" is not a logical tautology, a 
statement true by virtue of its logical form alone. It is arguably not even a rhetorical 
tautology (i.e. tautology-as-figure-of-speech, as I have been calling it, or redundancy), 
but even if it were, this does not make it necessarily correct, as I have pointed out 
before.
The reason I complained about changing the indefinite article into a definite article is that 
it alters the meaning of the sentence: semantics. But, analytically, we could say it 
changes the logical form of the sentence as well: instead of a variable bound by the 
universal quantifier (a flower is a rose indicates that "for all x, x is a flower iff x is a rose), 
the definite article gives us an existential quantifier (there exists an x such that x is a 
flower iff x is a rose). The latter is true of some given objects, since some flowers are 
roses, but the former is never true, since it is predicated of all objects, and most flowers 
aren't roses. Also, in a logical tautology, swapping the positions of the variables, 
because it does not change the syntax of the sentence, results in a sentence which is 
still a tautology. To use CaveBat's example from Peirce, if we take (p--->(q--->p)) and do 
a swap, we have (q--->(p--->q)): another tautology (actually the same one, but 
whatever). So, we should be able to do this with roses and flowers if the sentence were 
tautologous. But in order to keep it true, when swapping them around, you needed to 
alter the semantic meaning of the sentence beyond just the two nouns: you had to 
change the quantifier.
Since we had already established by that stage that the sentence was not a tautology by 
virtue of its syntax, any attempt to slyly change the meaning of the sentence needed to 
be pointed out. As it happens, the manner in which you chose to make use of oh-so-
convenient change of meaning didn't actually matter, later on. But I couldn't have known 
that at the time, so I saw fit to point out the trick. Also, Asmodeus, something you have 
to realise is that while in artificial languages the distinction between syntax and 
semantics is usually clearcut, in a natural language such as English this is not always 
the case.
Being maximally charitable, let us imagine that principles such as "Mind=Reality" are 
tautologous in the rhetorical sense. What does this tell us about their truth value? 

Nothing. So of what benefit is it to the CTMU to claim that they are tautologies like this? 
None whatsoever. Analytic philosophy often proceeds by showing that one statement 
necessarily entails another. Perhaps that is what Langan meant by "tautology".
But not all analytic philosophy is necessarily true, either: it is only true contingent upon 
the truth of its premises. This simply is not good enough for something claiming to be 
the "ultimate theory": it has to be necessarily true, which can only be achieved by the 
manipulation of logical tautologies. But this isn't what happens in the CTMU.
Asmodeus, you are completely and utterly wrong. You do not even understand what 
"tautology" means, and blur the lines between the two meanings so completely that you 
seemingly fail to understand the implications of this for the CTMU (reminiscent of the 
first article on the CTMU, which you also tried to save from deletion, which said that the 
CTMU may be called the "unified field theory" because it blurs the lines between all 
scientific fields to the point where they may no longer be individually distinguished: as 
far as I'm concerned, anyone who can defend a piece of writing which makes 
statements like that, simply does not know anything about anything). And all the other 
so-called "errors" I have made, all 46 of them, are actually not my errors, but points 
where your personal understanding of words and concepts departs so radically from the 
meanings generally assigned to them that you regard someone who uses words in the 
generally accepted fashion as being "incorrect".
You indicate above, Asmodeus, that you no longer want to do this. If you want to jump 
ship now, that's fine. Probably the best option for you, actually, since you have so 
irrevocably proven your ignorance: better to slink away like a "freedom fighter" from 
Angola, who fires a few parting salvos from his AK47 over his shoulder (without even 
turning to face his target) as he runs from the engagement. So don't feel obliged to carry 
on. Just know, now, that I have a wonderful body of errors that you have made, and 
shown that your supposed "error count" is nothing of the sort. I note also that you still 
haven't addressed some of the points I made above, like Langan's abuse of terminology, 
the nonsensical, circular nature of the SPSCL, the hypocrisy inherent in many of your 
arguments, etc. Great tactic, that, to claim that what I have written is "low-grade 
nonsense", when in actuality everyone knows it is all valid, but you simply cannot argue 
with it anymore. As a reminder to ourselves, let's have another look at the guide from 
the Internet Infidels board on how to conduct a Langanian argument:
Enter a CTMU discussion. Treat fallacies as if they are irrelevant.
If those that proposed the fallacies insist, state that the CTMU is too big to be bothered 
with minor fallacies.
Insist that the CTMU is mathematical without offering supporting evidence. Engage 
people in show-off pseudo-intellectual discussions on Maths, metaphysics, (analytical) 
philosophy, linguistics and fringe-science.
Try to wear the posters down.
Try to wear them down. Shift arguments, throw in red herrings, treat vague statements 
as accurate. Mount semantic wars. Question the competence of posters.
Wear them down.
If all else fails, make snide remarks, aggrandize ones ability to crush the minute 

arguments raised, and make "motions" to leave the discussion which is at that point now 
allegedly too "inferior" to bother about.
Hang around/ lurk - (getting back the breath perhaps or looking for a crack to get a 
foothold since its evidently lost).
Go back to 1
So, I take it we are somewhere between steps 7 and 8? Byrgenwulf 07:33, 30 August 
2006 (UTC)
Bravo Byrgenwulf! Your points are so clearly explicated. You are both a skillful 
thinker and writer! No doubt Langan will not risk debating you. I hadn't realized how 
fundamentally flawed the CTMU is until following you and Asmodeus. Yet a web 
search shows that the CTMU is fairly widespread. Since you now have a better 
understanding of it than most or anyone, you might write up a critique and make it 
available on the web. Otherwise, most netizens who encounter it and lack higher 
education in areas it addresses are easily swayed by its high-falutin terminology in 
conjunction with the media's aggrandization of its author. There is, as far as I can 
see, no formal critique of this much-touted "theory". CaveBat 17:05, 30 August 2006 
(UTC)
Response 
 
 #16
 
 :  To make an interminable story short, and to simply ignore the worst and 
most irrelevant of your ongoing nonsense, you're simply asserting that the M=R principle is 
nontautological. Because this principle asserts a limited equivalence between two terms, M 
= mind and R = reality, you are denying that this relationship (mind = reality) is tautological 
in the sense of logical necessity.
First, a little background: this principle is really just a statement of empiricism. Empiricism 
says that reality consists of perceptions or perceptual events. Now, the only way that 
perception can occur is through a processor which is capable of it...i.e., a percipient. 
Therefore, empiricism is really the statement that reality consists of an irreducible 
combination of percipient and perception; because perceptions include not only percipients 
but percepts (perceived objects, elements of "objective reality" supposedly independent of 
perception), it follows that reality also consists of an irreducible combination of percipient 
and percept.
But while this happens to be a very respectable philosophical thesis, that alone does not 
validate it. The British empiricists and their followers never got around to explicating their 
thesis, which encompasses many of the roots of modern science, to the required extent. 
However, they could easily have done so simply by exploiting its tautological nature. Again, 
in order to avoid your incessant complaints and ridiculous definitional hairsplitting, we will 
for the nonce replace this contested term "tautology" with generic logical necessity.
Let's elaborate on that a little. A logical tautology like not(A and not-A) is characterized by 
the fact that it remains true no matter what the variable A happens to signify. In other words, 
its truth is logically necessary. Now take a look at "Mind = Reality". Both M and R are 
variables in the sense that their contents can vary, but they also have distinct constant 
aspects - one is always "mental" (like a percipient), while the other is always real (like a 
perception, e.g., an act of scientific observation). The question is thus, are percipients 
necessarily conflated with perceptions?

The answer is both intuitively and analytically obvious, and it is unequivocally yes. You can't 
have a perception without a percipient. Therefore, we have what I've chosen to call an 
"analytic" or "semantic tautology" (you can call it whatever you like, but the CTMU is 
Langan's theory, and I believe that my choice of terminology closely parallels Langan's 
own). So that's it, Q.E.D. Error number 47.
Now that we've put our fingers on your most recent error, let's have a closer look. Although 
M=R is what Langan would call a "semantic tautology", a distinction is normally made 
between percipients and percepts. While these too are conflated by M=R, they can also to 
some extent be distinguished. Therefore, they have a nonempty intersect and a disjunction. 
As the PCID paper makes abundantly clear, the M=R principle is simply the basis on which 
their nonempty intersection is taken, and their disjunction interpretatively accommodated. 
Langan identifies this nonempty intersection, which includes logic, as "SCSPL syntax". In 
particular, this syntax includes logical tautologies.
Mathematically, logic is a language, and algebraically, we already know what a language 
looks like. But how must the language concept be altered and extended in order to use the 
language of logic as the syntax of a "larger" language capable of accommodating the M=R 
disjunction, and of serving as the framework of an overall theory of reality? The CTMU and 
SCSPL comprise the answer to this question. Let your errors occasion your enlightenment, 
and you will become a better person for it.
Now for the backlog.
29. In the course of this discussion, I have always properly distinguished and related the 
different kinds of tautology. You, on the other hand, apparently failed to understand these 
distinctions, and therefore concluded that I must have been referring to logical tautology 
alone. That's your mistake, not mine.
30. No, the uber-troll is hardly as notable as Grendel. It's just a hyperactive Internet troll who 
uses many pseudonyms and often claims to be located in Africa. I was once informed that it 
had developed a fixation on Langan and the CTMU. For my own part, I have indeed been 
periodically annoyed by "Africans" inexpertly affecting various levels of English fluency. For 
what it's worth, you would be one of the more fluent variants.
31. You're apparently trying hard to become a privileged member of the biggest and most 
exclusive club of all: Academia, Inc. In all likelihood, you have spent and/or borrowed a lot 
of money to do so, and sucked up so much abuse and disinformation that very little of your 
personal dignity, and philosophical integrity, remains intact.
34. No, you're still unqualified, because you make too many elementary mistakes to be 
qualified. Of the 47 errors for which you've thus far been nailed, I stand by my 
determinations on all of them. Only a small fraction would prove you unqualified to pretend 
expertise about much of anything, much less a potentially important theory containing many 
new ideas.
36. Who cares about your misunderstanding of Godel, to which we all became reconciled 
prior to the CTMU AfD? The diagonalization reference was your mistake. You claim that you 
made this mistake deliberately. But in my opinion, you made it out of pure confusion, caught 
it some time later, and then tried to blame your opponent for not wasting further time to 
educate you on the distinction. This ploy evidently worked on the peanut gallery, but not 
everyone is that easy to fool.

37. You have already shown that you understand almost as little about SCSPL structure 
and dynamics as they did in the 16th century. Therefore, the naivete is all yours.
38. A language is an overall algebraic relationship of linguistic components which contains 
various kinds of relationship, including semantic relationships. This is inarguable, and it 
involves no misuse of terminology. If you don't believe me, go ask one of your instructors. 
Your wheel-and-car analogy doesn't fit, and therefore constitutes another error. [48 errors]
39. I'll agree that somebody is parading his ignorance, but it's not me. In the context of a 
given mathematical structure (e.g. Euclidean geometry), duality is characterized by the 
partial invariance of a property or relationship of that structure under a switch involving 
unary (point-like) and binary (edge-like) relations. For example, consider the following 
relationship: "Two points determine a line segment (as its endpoints)." Switching points for 
lines, we get "Two (intersecting) line segments determine a point (of intersection)." Behold 
an example of duality! Similarly, V and V* share vector space structure despite the fact that 
the elements of V* map the elements of V into R, changing linear entities (the vectors in V) 
into "endpoints" of dual-linear entities (the vectorial elements of V*). It thus appears that we 
have changed lines into points while leaving certain aspects of vector space structure intact, 
and this too fits the above definition (although, as you correctly point out, there are 
differences among specific realizations of the duality concept). That's the first error you've 
made here. Your second error: in (e.g.) graph theory, edges are precisely the binary 
adjacency relationships between unary vertices. Congratulations - you've now split one 
error into two, one for each of the two mathematical structures on which your confusion 
centers. [49 errors]
40. Mind = reality is in fact identical to syntax (see the beginning of this response); syntax is 
the distributed intersect of M and R, which are therefore identical within its range. The 
existence of this intersect is not an assumption; it follows directly from the generic fact of 
observation. This has been repeatedly explained to you, and your obstinate refusal to 
acknowledge it constitutes another error. [50 errors]
41. Once again, I'll agree that somebody doesn't understand the meaning of certain 
terms...but that would be you. The problem of induction refers to the fact that inductive 
inference entails the uniformity of nature, which is also what it seeks to establish. This kind 
of circularity is implicit in all inductive processes; induction is extrapolative by nature, and 
thus requires a basis of extrapolation. The CTMU recognizes the inexorability of this 
inductive circularity, but instead of basing it on localized observations or semantic 
operations which may not be general (and are therefore subject to the problem of 
induction), bases it on syntax, as implied by the generic fact of observation. That's the only 
way to deal with the problem of induction. Secondly, if nature were not uniform with respect 
to logical syntax, then the mind - which shares that syntax with reality (that's the M=R 
principle) - would lack a uniform grammar in terms of which to recognize (and scientifically 
observe) nature. You can't bargain your way out of this; if logic does not distribute over 
nature as syntax, then goodbye observation, and goodbye nature. (By the way, you are 
right about one thing: by the necessity of explanatory closure, the problem of reality must 
indeed be intrinsically transformed into its own solution. That is, closure ultimately demands 
circularity. If you can't accept this, then you should study a shallower sort of philosophy 
more distant from ontology.)
43,44. I've got to hand it to you, Rumpelstiltskin - there's nothing quite as poignant as (what 
sounds like) Christian sensibility from somebody who has disowned his soul.

45. (see error 39, on which you managed to base two distinct errors)
46. It all comes down to syndiffeonesis. In order to distinguish a ratewise difference 
between any pair of physical processes, we need a form of processing which distributes 
over (embeds, carries) both subprocesses, i.e., which is coherent with respect to them and 
thus transpires at one distributed rate. (Forget about parallel recognition of relative values 
without distributed processing, and remember that we're not talking about a "background 
process", but a distributed process which is occurring through localized grammatical 
processors.) Thus, we require an invariant metavelocity...a pregeometric simulation process 
that transpires at one uniform rate. This simple observation makes it unnecessary to rely on 
Maxwell's equations or the Michelson-Morley experiment to understand why an invariant 
"metavelocity" must hold (although one does need to go beyond this simple kind of 
reasoning to assign it an exact value). In the CTMU, this metavelocity is the invariant rate of 
conspansion c, and it corresponds to the speed of light in vacuo. [51 errors]
47. You claim that the statement "nobody has ever actually seen a set coherently include 
itself in the usual confused sense of that phrase - it has a more detailed interpretation in the 
CTMU" is "funny" because "Langan defines the SPSCL's global processor Γ to contain a set 
O of active reflexive objects, which in turn contains Γ." But we're talking about two distinct 
senses of inclusion or containment here, and as I've already pointed out, this dual kind of 
self-containment is not identical to the muddled, paradoxical sort of self-containment current 
in standard set theory. Unlike the CTMU, set theory offers no means or mechanism for self-
inclusion; as I say, nobody has ever seen a set include itself as an element, much less by 
way of the limited operations available within set theory. That's why set theory incorporates 
semantic barriers which simply preclude its expression. [52 errors - and once again, 
congratulations!]
Incidentally, you have already lost this debate many times over. If I "jump ship", it will only 
be due to the fact that you are wasting my time with your absurd and utterly tedious 
insistence that no matter how many errors you make, you are nevertheless entitled to keep 
making them. Having established to my complete satisfaction that your critiques remain 
fundamentally incorrect, I now regard the debate as over, and you as having been about as 
thoroughly squashed as an opponent can be. Accordingly, if you want this dialogue to 
continue in any form, you should take care how you address me. Otherwise, you will either 
receive no response, or a brief reminder that having made over 50 errors, you are no longer 
entitled to have your criticisms taken seriously.
Speaking of which...as previously agreed, Langan now owns that part of you which ties you 
to the SCSPL identity, and is not claimed by the identity itself (which claim Langan would 
regard as prior). He is, of course, free to reject anything too corrupt for salvage, and will 
probably do so. Since you do not recognize the existence of this part of yourself, which 
Langan has called the "soul", this will probably not cause you the slightest qualm. Now have 
a nice day. Asmodeus 16:37, 31 August 2006 (UTC)
Wikipedia is not here to provide a forum for discussion, but for the record, in my 
experience the universe cares very little for English language word playing. Take the 
debate to some forum elsewhere, anywhere. Jefffire 16:42, 31 August 2006 (UTC)
Unless one is performing an experiment or building a device, it's all "word games". The 
"word games" that I play rely heavily on logic, considered as a branch of mathematics. 
Rumpelstiltskin decided to contest certain logical issues on his Talk page, and that was his 

right. If you dispute this, then take it up with Rumpelstiltskin - it was his debate. Asmodeus 
17:23, 31 August 2006 (UTC)
Asmodeus, you just don't get it, do you? Mathematics, philosophy, physics and formal 
logic all make use of quite a bit of terminology. This terminology is not just there to make 
whoever is using it sound knowledgeable or impressive. It is there because each word 
carries with it a very specific meaning, and if one does not use these words in the 
manner in which they are generally used, one is bound to make mistakes in one's work.
Let's look at the M=R phenomenon a little more closely. What you are essentially saying 
is that there are perceptions, things which pick up on and process perceptions, which 
you call percipients, and things which are perceived, i.e. percepts. Fine: that is 
straightforward. You say, then, that there is a "non-empty intersection" between 
perceptions and percipients. First, a non-empty intersection, as you ought to know, is 
not expressed by an "equals" sign, which is usually used to denote identity. Next, the 
objects of reference of mind and reality are not the same. Let's denote by "m" the things 
which can be said to be a part of mind, and "r" the things that can be said to be a part of 
reality.
Now, we can devise a relation, let's call it "N", which maps states of "m" into states of 
"r"...a sort of "epistemic naturalisation function", we could say, since ultimately our minds 
are based on our brains, which are simply lumps of warm, wet matter. Any content held 
in our mind must, a little bit of common sense tells us, be expressible as a state of 
matter, unless we wish to embrace some sort of horrid dualism. An interesting question, 
as it turns out, is what kind of relation this "N" is: is it one-one, onto, etc.? Can multiple 
states of mind be represented by a single state of matter? Can a single state of mind be 
represented by multiple states of matter, etc.? Importantly, the answers to those 
questions are not trivial.
However, we must be careful here. You see, the objects in the domain of this relation 
are not in the same group as the objects in the codomain of the relation...because that is 
precisely what the relation does, is take objects from one group and map them into 
objects from another.
The moral of the little story above is this. Your argument, Asmodeus, about empiricism, 
percipients, perception, etc., tells us very little besides the fact that this relation must 
exist. It is even something with which I agree 100%. What your argument does not do, 
however, is show in exactly what way this is a "logical necessity" in the sense that (P or 
not-P) is. Nor does it show how this "intersection" between mind and reality is an 
inevitable semantic offshoot of the common meanings assigned to "mind" and "reality". 
Nor does it tell us anything about the nature of this relation. Nor does the CTMU paper 
do any of that.
You see, I am prepared to grant that this "non-empty intersection" is a sound 
philosophical position. But I cannot grant that it is a "tautology", either "semantic" or 
"syntactic". It is contingent on too many other assumptions to hold this distinction. 
Calling it "tautologous" amounts to the assertion of the triviality, or possibly even non-
existence, of the relation "N", which is simply an untenable position. After all, in a sense 
the SPSCL would appear to be an exploration of the properties of this relation, one 
might say.
So, I understand Langan's compulsion to claim for the SPSCL the status of "absolute 

and necessary truth". But simply because we "cannot have a perception without a 
percipient" does not mean that "mind=reality" tautologically (and I use "tautologically" 
here as a synonym for "as an irrefutable analytical consequence"). It is, in fact, a 
category mistake of the archetypical variety to do so.
I know that you are going to be unable to accept all of this, Asmodeus. Because clearly 
this bad habit of thought is so engrained in your consciousness that you won't be able to 
shake it without making a huge effort, and you see no reason to do so, because you 
believe it to be right. A vicious circle. But whatever: it is, ultimately, your choice.
Selected comments on the "backlog"
29. A quick survey of the above discussion tells me that it is the other way around.
30. That would probably be because I am not an "African" by birth.
34. Disagreeing with someone who is deluded as to their own level of understanding is 
only a mistake insofar as it is a waste of one's own time. It is not an error of fact or 
knowledge, but it is a mistake I have made repeatedly here on Wikipedia. 
Congratulations: this is the first mistake I have made that you have identified.
36. Bollocks and bluster to try to cover up your error of omission. For shame.
37. What's there to understand about SPSCL? It is a puff of smoke.
38. No, you're wrong. We must distinguish here between formal and natural languages. 
Since we're talking about a formal language (the SPSCL), I shall restrict my comment to 
that. A formal language is a set. This set may represent or even contain relations, yes, 
just as a car has wheels, but it is not a relation itself. Please make sure you understand 
the meanings of very basic words before trumpeting your ignorance, unless you actually 
enjoy making a prat of yourself.
39. Vertices in graph theory are not "unary relations". Unary relations are properties: in 
maths, they are often Boolean-valued expressions, i.e. having a true or false value. The 
arity of a relation refers to the number of arguments it has: a unary relation has one, 
binary relation two, etc. In graph theory, we can specify a graph in two main ways: 
adjacency and incidence. Either we treat the vertices as basic objects and use the 
edges as a means of specifying adjacency relations between these objects, or we treat 
edges as the basic objects and use the vertices as a means of specifying incidence 
relations between edges. In the former case, vertices are objects, not relations. In the 
latter case, vertices are relations, but are not necessarily unary: there can be any 
number of edges incident on a given vertex. And not even in all graphs are edges binary 
relations: in a hypergraph, for example, edges can be incident on any number of vertices 
(this latter point is a little too general, perhaps, since we can restrict ourselves to graphs 
proper, but the vertices gaff clinches it). So "vertices and edges corresponding to unary 
and binary relations respectively" (which is what Asmodeus said) is indeed bollocks. 
And, let us not forget that Asmodeus tried to explain that the notions of duality in 
projective geometry and vector spaces are the same thing by making an inspecific but 
verbose appeal to category theory, in which he confounded the commutative diagrams 
used in category theory with graphs (while it is conceivable that theorems from graph 
theory may help in deciding whether a given diagram commutes, chances are that the 
principles of category theory itself will be more helpful). About the only common strain 
that runs through all mathematical notions of duality, Asmodeus, is that the dual of (the 

dual of an object) is the object itself. But this has no perfectly general category theoretic 
expression, simply because not all the mathematical objects to which duality is applied 
form categories (category theory is not a licence to generalise freely: on the contrary, it 
gives us the set of rules for deciding whether or not a generalisation is appropriate, and 
there are some very specific requirements that a set of objects with their morphisms 
must meet in order to be called a "category"). I would also dispute that only a 
"mathematical ignoramus" could be ignorant of category theory: I am not a 
mathematician, but I happen to be extremely interested in category theory, and use it for 
my work...and I am certain that many actuaries, for example, who are not mathematical 
ignorami, do not even know what category theory is. Finally, appealing to abstract 
nonsense and other manifestations of pseudo-erudition does not work as a tactic 
against me, or any other vaguely competent person. I would have thought you'd realised 
this by now. If you are interested in learning more about category theory, since you 
seem to know so little and understand less, I can recommend this primer, which is 
written by a mutual acquaintance of ours, none other than the sentient software agent 
itself. Repeatedly having to explain very simple concepts from first principles is tiresome 
work, so I'll let a .pdf do it for me.
40. The "intersection of M and R" can only be coherently expressed as the intersection 
of N(m) and r (see my comment above), and so whether or not this is "syntax" (and you 
have a peculiar way of using the word "syntax"), the manner in which the CTMU treats it 
is lacking.
41. Asmodeus tells us that the only way to deal with the problem of induction is to base 
the solution on a "generic fact", i.e. observation. But this itself makes use of induction. 
So we still haven't broken out of the problem yet, and are no closer to finding a solution 
that is not based on turning the problem into a quintain (a verbose sort of straw-man). 
Admiral Quine once came up with a lovely expression: "our argument is not strictly 
circular, but rather takes, figuratively speaking, the form of a closed curve in space". I 
agree that any argument regarding ontology must be a closed curve in space (of course, 
because human minds have to explain their own existence), but this does not mean that 
every example of an obviously circular statement is par for the course. Really, now.
43, 44. I have reason to believe that you are acquainted with the quasi-legendary über-
troll Andrew Beckwith. Well, his online antics gave rise to the sarcastic expression 
"Andrew Beckwith pities you". Does this explain things a bit better?
46. This still does not explain why we should equate the "rate of conspansion" with the 
speed of light, other than that it is convenient to do so, since we know that the speed of 
light is invariant, just as the "rate of conspansion" is meant to be. Moreover, the speed of 
light is not a "metavelocity": it is a speed, plain and simple. Also, Langan calls the "rate 
of conspansion" a "time/space conversion factor". The speed of light is not that: it is the 
distance that light moves in a given time interval ("conspansive duality" be damned). 
Finally, SR has a very specific mathematical structure, giving rise to the light cones and 
hyperplanes of simultaneity and all that, which is derived from the Lorentz 
transformations. It is not proven in the CTMU that this "rate of conspansion", or 
"syndiffeonesis" gives rise to an identical formulation to the Lorentz transformations; 
also, the speed of light is not just any invariant, but Lorentz invariant: has this been 
proven for "conspansion"? It cannot, of course, be done, with appeal to "conspansive 
duality", but this would probably involve using the Lorentz transformations as a "given", 

which doesn't cut it. Most of Asmodeus' argument in his point 46, though, is just bluster, 
and if one looks at it carefully, one point does not actually follow from the next. And the 
statement "in order to distinguish a ratewise difference between any pair of physical 
processes, we need a form of processing which distributes over (embeds, carries) both 
subprocesses, i.e., which is coherent with respect to them and thus transpires at one 
distributed rate" is tantamount to the assertion of the "infocognitive" equivalent of an 
aether theory, mutatis mutandis (particularly when coupled with the "scaling" of 
"absolute size" of objects, in as much as that has any meaning at all, be it physical or 
metaphysical, other than a change in Planck's constant related to the "size" of the 
universe - this can only be avoided if Planck's constant is a ratio between the "size" of a 
quantum and the "size" of the universe, but this doesn't work because "size" here is not 
well defined, and an undefined concept cannot have properties, since the ascription of 
any property is tantamount to a definition of sorts: in short, this part of the CTMU paper, 
and Asmodeus' defence of it, is a stream of unmitigated conceptual confusion).
47. Utter nonsense: there is nothing in the SPSCL which guarantees that there will be 
no problems with self-inclusion, other than Langan's personal insistence on this, which 
counts for less than nothing. Also, we are not talking about two distinct notions of 
"inclusion" here: we are talking about membership of a set. O forms a subset of Г, and Г 
forms a subset of O: at least that is what the CTMU paper asserts. And Langan provides 
no means superior to ordinary set theory (in fact, it is not clear in exactly what way his 
musings depart from set theory in a beneficial way) for the resolution of statements such 
as this. Little "MU diagrams" don't help, really. A diagram doesn't prove anything.
Asmodeus, your "error count" is totally invalid. Indeed, I am well entitled to say that it is 
more accurately a count of the number of times you've embarassed yourself here. 
Sometime, if I can summon up the will to do so, I might bother counting your total 
number of errors, and add the number of errors in the CTMU paper to that, for good 
measure (and because I believe they are your errors as well: even if you are not the 
originator of the theory, you still err by propounding its validity). But, at a quick count, 
let's see: you claim that I have made 52 errors. So actually, you have made at least 52 
errors. However, claiming that those are my errors in each case is also an error, so that 
makes it at least 104. Over 100 errors, Asmodeus, is something of which even the most 
brazen self-proclaimed genius ought to be ashamed, especially when they are made 
against someone as obviously inferior as myself. So the score is Asmodeus:104; 
Byrgenwulf:1 (see point 34). Not too shabby, I suppose.
As for my "soul": whoever said that you have proven me wrong? Only you seem to have 
that opinion, and obviously that doesn't count, since you're biased. So don't get ahead of 
yourself. You still have a long way to go before you establish to my satisfaction that the 
CTMU is in any way correct. A good start might be to admit the mistakes you've made 
so far, and try a different tack, such as using sound reasoning to prove your case, and 
not verbosity and bloody-minded insistence on being correct. Now have a nice day.
CaveBat, I am flattered that you think I have been at all lucid here. And as for writing a 
detailed critique on the CTMU, it is something which I have thought about, and intend on 
doing at some time, once my current state of frenzied real-world obligation has died 
down a little. I agree that the number of people on Internet forums and so forth who 
appear to be taken in by it, simply because it sounds very impressive and the chap who 
invented it is allegedly so clever, is rather alarming: and these people need to be 

informed of the truth. I think the reason that there is no "formal critique" of this theory is 
simply that most people would rather make a positive contribution to their field than 
waste time on exorcising nonsense. But, I do see it as something of a social duty to 
educate the public and help dispel the mists of ignorance that tend to settle everywhere 
that people congregate, so I definitely shall write up a debunking at some stage.
As for the CTMU's inventor Himself discussing it with me, I have reason to believe that 
you have already borne witness to that happening. And seen him being soundly 
humiliated (to which you contributed quite handily, pressing home the point about 
rose/flower tautologies as you did). Ah well, c'est la vie, even "the smartest man in the 
world" cannot always be right, even if he cannot admit it himself.
Jefffire, I agree with you that this is hardly the best place to be playing word games 
about the CTMU. However, since Asmodeus came and deposited a list of alleged 
"errors", I felt obliged to respond, and things somewhat spiralled from there. Hopefully, 
though, Asmodeus is going to skulk away again, now that he has proven his position to 
be completely untenable. Then I can archive this mammoth talk page and put this matter 
to bed. Byrgenwulf 12:39, 2 September 2006 (UTC)
For sod's sake, if someone is daft enough to believe that rubbish in the first place 
then mere reason isn't going to convince them otherwise. Anyone with more than 
half a wit can see that the whole arguement rests on the foibles of the English 
language, essentially making the fallacy as the "You can't cross the same river 
twice" arguement. I don't donate to see the bandwidth taken up with this sort of thng. 
Leave the cultists with their smug sense of self satisfaction, it doesn't hurt you, and 
they're hardly likely to convice anyone who doesn't think personal insults are a valid 
debating tool. Archive this rubbish, and delete anything else added by Asmodeus 
with the edit summary "Whatever...". Jefffire 14:41, 2 September 2006 (UTC)
I agree emotionally with Jefffire's statement here. However, reading through the 
"debate" on this page has given me the same genre of pleasure as PZ Myers's 
slashings of creationism or the Panda's Thumb fisking of the book The Politically 
Incorrect Guide to Darwinism and Intelligent Design. The good people of the 
Thumb are currently in the midst of demonstrating that this book "is not only 
politically incorrect but incorrect in most other ways as well: scientifically, 
logically, historically, legally, academically, and morally"; what's more, they're 
doing the job with commendable clarity.
Empirically speaking, the debunkings of pseudo-intellectual nonsense have 
almost always been more interesting than the nonsense itself. It is for all practical 
purposes impossible to convince a "true believer" that he has been bamboozled 
or is bamboozling himself, but a solid piece of critical analysis can do wonders for 
the fence-sitters, the laity whose attention is captured by a self-appointed 
priesthood — and who only need to exercise a little critical thought to find 
themselves free again. If nothing else, debunking Velikovsky has created a few 
opportunities to teach real astronomy; in the case at hand, a few people might 
get a taste of Quine they would have otherwise lived without. Anville 15:12, 2 
September 2006 (UTC)

Response #17
 
 :  In view of the unkind way you continue to mislead your befuddled but 
nonetheless adoring peanut gallery, I suppose I'd better attend to some of your most recent 
mistakes.
1. "What you are essentially saying is that there are perceptions, things which pick up on 
and process perceptions, which you call percipients, and things which are perceived, i.e. 
percepts. Fine: that is straightforward."
Well, good!
2. "You say, then, that there is a "non-empty intersection" between perceptions and 
percipients. First, a non-empty intersection, as you ought to know, is not expressed by an 
"equals" sign, which is usually used to denote identity."
The "=" sign means identity within the range of the intersect (using the intersect symbol 
here would have conveyed a set-theoretic bias, and that might have been even more 
confusing to someone suffering from brittle terminology syndrome). Beyond the intersect, 
identity still applies on a reductive level; since the intersect functions as a medium, all of its 
content can be said to consist of it, and is identical in that sense. This was all made 
perfectly clear. You might not have been able to figure it out, but Langan did explain it, so 
any confusion on that score would be your own fault. [53 errors]
3. "Next, the objects of reference of mind and reality are not the same."
So reality has "objects of reference"? Aren't you trapping yourself in a misuse of language? 
If reality has "objects of reference", then it refers. But reference is a function of language 
and mind. Hence, your statement implies that reality shares certain key functions of 
language and mind. This implies that you accept at least one fundamental premise of the 
CTMU. (On the other hand, maybe you mean "the objects to which the given terms 
refer"...in which case, the objects directly acquired by reference both exist within the 
language processor employing those terms and are to that extent identically reducible to 
their common processing medium.) [54 errors]
4. "Let's denote by "m" the things which can be said to be a part of mind, and "r" the things 
that can be said to be a part of reality."
...but of course, with no prior assumption of any fundamental difference. Assuming such a 
difference at the outset would amount to circular reasoning, which you claim to reject.
5. "Now, we can devise a relation, let's call it "N", which maps states of "m" into states of 
"r"...a sort of "epistemic naturalisation function", we could say,..."
...and vice versa, just for the sake of symmetry. That is, rather than just N = N:m-->r, what 
we actually have is N = (N1:m-->r, N2:r-->m). Once again, asserting an asymmetry at this 
point would amount to circular reasoning, which you claim to reject. [Arguably an error, but 
I'll cut you some slack.]
6. "...since ultimately our minds are based on our brains, which are simply lumps of warm, 
wet matter."
...by your personal assumption, which you can't prove and which many others do not accept 
for very good reasons. One of those reasons may be that you have not yet adequately 
defined "matter". (For his own part, Langan defines "matter" as a particular kind of reflexive 
information processor, but I take it that you disagree.) [55 errors]
7. "Any content held in our mind must, a little bit of common sense tells us, be expressible 

as a state of matter, unless we wish to embrace some sort of horrid dualism."
You seem to be talking about "supervenience", and you don't want to be making any 
circular assumptions there either. On the other hand, if you mean that mental content must 
be associated or correlated with processors...well, the CTMU contains the implications of 
that statement. (By the way, to assume that materialism is the only escape from dualism is 
simply ridiculous. Even if the CTMU did not exist, there would be other forms of monism to 
consider.) [56 errors]
8. "An interesting question, as it turns out, is what kind of relation this "N" is: is it one-one, 
onto, etc.? Can multiple states of mind be represented by a single state of matter? Can a 
single state of mind be represented by multiple states of matter, etc.? Importantly, the 
answers to those questions are not trivial."
How right you are! These questions are indeed interesting, and the answers are indeed 
nontrivial. Clearly, given that empirical science is doing nothing to answer them, a larger 
conceptual framework is urgently required.
9. "However, we must be careful here. You see, the objects in the domain of this relation 
are not in the same group as the objects in the codomain of the relation...because that is 
precisely what the relation does, is take objects from one group and map them into objects 
from another."
But again, that's an assumption. You have no a priori knowledge that the domain and 
codomain are not merely different aspects of one and the same thing. In fact, they must be; 
relations, like their relands, require support by some sort of distributed storage and 
processing medium, and the distributed intersect of the relands is the only medium that can 
do the job. (A medium which is nondistributive on its most general level is disconnected, 
and cannot accommodate a structurally or dynamically connected reality as content.) [57 
errors]
10. "...this "non-empty intersection" is a sound philosophical position. But I cannot grant that 
it is a "tautology", either "semantic" or "syntactic". It is contingent on too many other 
assumptions to hold this distinction."
Then name the "assumptions" without making any of your own. You can start by eliminating 
all of the assumptions on which you have just been called. [Your presumption is arguably an 
error, but I'll let it pass.]
11. "Calling it "tautologous" amounts to the assertion of the triviality, or possibly even non-
existence, of the relation "N", which is simply an untenable position."
If you choose to define "tautology" in so restricted a way, then feel free to replace it with 
"logical necessity". The logical necessity of the relation "N", the extent of which is simply 
contracted to 0 in the CTMU, in no way makes it trivial or nonexistent. [58 errors]
12. "After all, in a sense the SPSCL would appear to be an exploration of the properties of 
this relation, one might say."
That's right. It is also an exploration of the logical entailments of this relation, including the 
nature and functionality of the storage and processing medium which coherently supports it.
Let me give you a piece of friendly advice. If you really are a would-be philosopher, and if 
your name is really "Rumpelstiltskin", then you should really start being more careful. As an 
aspiring academic, you are in no position to claim that you have read and understood a 

paper while trying to use the truth of its own assertions against it, even on the grounds that 
its author may have transgressed certain neat-and-tidy terminological boundaries to which 
you are personally attached. You're supposed to make an effort to work within the author's 
framework and comprehend the author's points; that's what it means to "understand" his 
work. After all, he may have had a reason for using the terms he has used, and you are not 
in a position to make summary judgments to the contrary.
People with reputations to protect understand this. It makes no difference to them that some 
pseudonymous "bright" has stood up to make a fool of himself, or that such a "bright" has 
managed to elicit approval from a similarly anonymous peanut gallery. People with actual 
standing in their professional communities have a right to avoid personal humiliation, and 
out of respect, you should stop trying to draw them into your puddle of quicksand by 
"recommending" their category theory primers, linking to their talk pages, and so forth. As 
far as they are concerned, you may well be nothing but a loud, opinionated liability bent on 
exposing them to personal embarrassment, and it is neither fair nor respectful of you to 
assume otherwise.
The rest of your critiques have already been dealt with, and you have brought to bear no 
new and exculpatory reasoning regarding them. Hence, they will be ignored. However, 
while you have already emerged the loser from the debate you started, your error count 
continues to rise. It now stands at 58.
[P.S. To the peanut gallery: Some of you seem to be getting impatient with this dialogue. If 
you don't want me to respond to Rumpelstiltskin's ongoing nonsense, then stay out of the 
discussion. As long as you insist on conveying the false impression that Rumpelstiltskin's 
errors have somehow enabled him to emerge victorious, I'll be forced to look out for the 
intellectual welfare of those few innocents who bother to read Wikipedia's member talk 
pages.] Asmodeus 16:42, 3 September 2006 (UTC)
Now try it in Chinese ;) . Jefffire 17:24, 3 September 2006 (UTC)
Response 
 
 #18 (to Jefffire)
 
 :  Be careful - although you seem to be somewhat less 
enamored than Rumpelstiltskin of pointless terminological hairsplitting and other 
circumnavigations of content, the irrationality of your remarks is just as obvious and can 
easily be debunked in any language. Above, you stated "the whole arguement [sic] rests on 
the foibles of the English language, essentially making the [same] fallacy as the 'You can't 
cross the same river twice' arguement." To employ your own phrasing, anyone "with more 
than half a wit" can't help but recognize this statement as pure manure. Again, please stay 
out of the discussion unless you want it to continue arbitrarily. Rumpelstiltskin is evidently 
encouraged by your sniping, and as long as it persists, will not be able to keep still. 
Asmodeus 18:28, 3 September 2006 (UTC)
But you're not even responding to the same editor, lol! Jefffire 18:33, 3 September 
2006 (UTC)
Hello Asmodeus. I am going to confine my comments here to the absolute minimum. 
First, a little bit of etiquette. I am not on first name terms with you. Therefore, you may 
address me as "Byrgenwulf", or, if you insist on using my RL identity, you may call me 
"Mr. O'Grady". But I would prefer it if you do not use my first name. Thanks.
Next, on terminology: normally, I am not quite so preoccupied about terminology. 
However, since both you and the CTMU toss so much of it about, and much of it is 

misused, I saw fit to raise this fact, and press it home. Words have meanings, and if we 
don't use them in the correct fashion, we cannot expect the results of our musings to be 
valid.
On assumptions. I never claimed that what I said contained no assumptions: on the 
other hand, the CTMU does make that claim. But, if I make a statement which is 
contrary to the CTMU, and my statement is an assumption, then its negation in the 
CTMU must also be an assumption, namely the negation of my own. Alright, maybe not 
an assumption: it may the conclusion to what may even be a convincing argument. But it 
is not always a self-evident fact.
On circularity. Reality does not have objects of reference. The word "reality", however, 
does. That is obviously what I meant, and once again, it is the failure to appreciate 
subtleties like that which renders the CTMU flawed. Lines are constantly blurred 
between object and theory, language and metalanguage, model and interpretation.
On reputation. I believe you are kidding yourself when you think that the reason the 
CTMU has not received serious criticism from "people with reputations to protect" is 
because they will damage their reputations by doing so. It is certainly true that personal 
insults are far more common currency in CTMU debates than in many other fields, and 
no sane person would expose themselves to that by choice, I don't think, but then no-
one takes your invective too seriously, anyway. Also, all I did was link to a .pdf file of a 
primer on category theory, so that you might read it and hopefully better understand 
what you're talking about. You, on the other hand, deposited an absolutely nauseating 
message on John Baez's talk page, a bizarre hybrid of nudge-nudge/wink-wink 
sycophancy and tongue-in-cheek condescension, the presumptuousness of which is 
tempered only by its extreme absurdity ("just a heads-up"). So I really don't need 
lessons in personal conduct from you.
I have little interest in entertaining you and your delusions any longer. We are obviously 
not going to agree here; but I have established beyond a shadow of a doubt that the 
CTMU is nothing but a polysyllabic farce. I understand it, and understand what Langan 
is trying to do. I just feel that it is pretentious, pointless, and fundamentally misguided. 
You obviously feel very strongly that the opposite is the case, but my advice to you is to 
go and tell someone who cares, and who will believe you. No-one here seems to. What 
you are doing now amounts to trolling, and any new inflammatory comments will be 
treated as such and removed. Let's remember that you were not invited here to discuss 
the CTMU: my invitation expressly stipulated that we do it elsewhere - although there is 
little point in that even, since I think we've said everything that needs to be said on this 
matter. If after 24 hours there are no new posts on this thread (not counting ongoing 
straw-clutching remarks from Asmodeus) I shall archive this mess. Byrgenwulf 19:26, 3 
September 2006 (UTC)
Response
 
  #19
 
 :  In repayment for your merciful brevity, I'll keep my answers brief as well. 
However, in order to avoid confusion, I'll append them to your comments.
1. First, a little bit of etiquette. I am not on first name terms with you. Therefore, you may 
address me as "Byrgenwulf", or, if you insist on using my RL identity, you may call me "Mr. 
O'Grady". But I would prefer it if you do not use my first name. Thanks.
I disagree. If we were not on a first-name basis, you could never have squandered so much 
of my time so freely. However, pending your next breach of etiquette, I'll respect your 

wishes anyway.
2. Next, on terminology: normally, I am not quite so preoccupied about terminology. 
However, since both you and the CTMU toss so much of it about, and much of it is misused, 
I saw fit to raise this fact, and press it home. Words have meanings, and if we don't use 
them in the correct fashion, we cannot expect the results of our musings to be valid.
In the CTMU context, the meaning of CTMU terminology corresponds to its usage by the 
author of the CTMU. Regarding your impression that some of this terminology might be 
totally disconnected from standard usage, I helpfully directed you to a good online 
dictionary. Evidently, you chose not to use it. This strongly suggests that you aren't really 
interested in anyone's usage but yours. However, since the CTMU is not your theory, it isn't 
your usage that counts.
3. On assumptions. I never claimed that what I said contained no assumptions: on the other 
hand, the CTMU does make that claim. But, if I make a statement which is contrary to the 
CTMU, and my statement is an assumption, then its negation in the CTMU must also be an 
assumption, namely the negation of my own. Alright, maybe not an assumption: it may the 
conclusion to what may even be a convincing argument. But it is not always a self-evident 
fact.
You're quite right - the negation of an assumption is not necessarily an assumption. After 
all, the original assumption may have contradicted a logical necessity (as in my opinion it 
did). Thanks for your frank if belated admission that the CTMU might not be a mere 
assumption. In view of that concession, I'm willing to grant that some may not see it as self-
evident.
4. On circularity. Reality does not have objects of reference. The word "reality", however, 
does. That is obviously what I meant, and once again, it is the failure to appreciate 
subtleties like that which renders the CTMU flawed. Lines are constantly blurred between 
object and theory, language and metalanguage, model and interpretation.
Reality is understood largely through words connected to each other by reference. The 
referential connections among words are supposed to reflect real connections among the 
corresponding parts of reality. Yet, you seem to feel that reference is "too linguistic" to 
describe the true nature of these real connections themselves. In other words, reference 
corresponds to reality, but reality does not correspond to reference. Unfortunately, because 
correspondence is symmetric, that's a contradiction, and it is your seeming blindness to this 
kind of subtlety that weakens your criticisms of the CTMU. (If you at least realize that there 
are no hard, fast lines to be drawn between object and theory - that they have an 
unavoidable overlap - then perhaps there is reason for hope after all.)
5. On reputation. I believe you are kidding yourself when you think that the reason the 
CTMU has not received serious criticism from "people with reputations to protect" is 
because they will damage their reputations by doing so. It is certainly true that personal 
insults are far more common currency in CTMU debates than in many other fields, and no 
sane person would expose themselves to that by choice, I don't think, but then no-one takes 
your invective too seriously, anyway.
It's not that recognized authorities would damage their reputations by insightfully, 
respectfully criticizing the CTMU. They would damage their reputations by criticizing it as 
you have, shallowly and derisively. Just imagine how bad you could be made to look if (a) 
you were to become a well-known philosopher, (b) Langan and the CTMU were 

unconditionally vindicated, and (c) this dialogue were to emerge verbatim, revealing not only 
that you were instrumental in finagling its deletion from Wikipedia, but that despite what 
amounted to intensive personal tutoring, it remained well over your head. If you think that 
this sort of exchange could have no impact on your reputation, I suggest that you rethink the 
possibilities. (By the way - as I'm sure you must have guessed, I place little value on your 
opinion of my communications with others, particularly when they don't involve you. In fact, I 
regard them as none of your business.)
6. I have little interest in entertaining you and your delusions any longer.
Never fear - I have just as little interest in humoring your pretensions of philosophical 
expertise and understanding.
7. We are obviously not going to agree here; but I have established beyond a shadow of a 
doubt that the CTMU is nothing but a polysyllabic farce. I understand it, and understand 
what Langan is trying to do.
Not really. But any time you're ready to devote the time and effort to get a handle on it and 
put your money where your mouth is, feel free. Until then, you don't know enough to 
threaten the CTMU or its author in the least. (I have as little regard for your pedantic 
fussiness as you profess for Langan's polysyllabics.)
8. I just feel that it is pretentious, pointless, and fundamentally misguided. You obviously 
feel very strongly that the opposite is the case, but my advice to you is to go and tell 
someone who cares, and who will believe you. No-one here seems to.
Given the level of understanding thus far displayed by you and your partisans regarding the 
CTMU, I find this neither surprising nor discouraging. However, I do find it a mystery how 
your blatant philosophical prejudice could pass for "NPOV" on this site. I'd say that 
Wikipedia has a problem with rabid philosophical bias that has not yet been fully 
recognized, and that you and your newfound friends are right in the middle of it.
9. What you are doing now amounts to trolling, and any new inflammatory comments will be 
treated as such and removed.
That's an unfair accusation. Ever since you complained that I'd refused to debate you (see 
the beginning of this thread), I've been trying to give you all the attention you deserve. I'm 
sorry that you're still disappointed, but I tried. (I do have the right to defend myself, so if you 
remove these comments, they will be replaced...if need be, in the appropriate archive.)
10. Let's remember that you were not invited here to discuss the CTMU: my invitation 
expressly stipulated that we do it elsewhere - although there is little point in that even, since 
I think we've said everything that needs to be said on this matter.
Let's be honest - you, and you alone, chose the timing and placement of this exchange 
without consulting me. You should hardly be surprised that your subsequent stipulations 
were ignored.
11. If after 24 hours there are no new posts on this thread (not counting ongoing straw-
clutching remarks from Asmodeus) I shall archive this mess.
If I were you, I'd want to hide this exchange too. Incidentally, you should probably start 
thinking about reworking your User Page - it's rather confrontational and could easily be 
viewed as a challenge to be answered right here. Asmodeus 07:06, 4 September 2006 
(UTC)

Oh hello, Asmodeus. I must hand it to you, that while these numbered paragraphs are a 
stylistic abomination reminiscent of a legal document, they do make it easier to address 
specific points. So on we go then!
2. Well, I certainly agree that the author of the CTMU has the right to use words 
however he pleases: as does anyone. However, Humpty-Dumpty diction is not really an 
efficient means of communication, and especially when dealing with matters as 
pernickety and precise as philosophy, science, and logic, one really does need to be 
accurate.
3. What I "admitted", Asmodeus, is that the CTMU is not a self-evident, necessary truth. 
That is all. And that's not really a startling observation, I shouldn't imagine.
4. I think that, as usual, you are deliberately missing the point. All that linguistic 
reference can do is give us a "calculus of perceptual relations". Nay, not even that, 
because language already distorts perception: we need to narrow down language and 
make it specific in order to be more accurate. And since language gives rise to a 
mammoth web of relations between words themselves, which web will differ from person 
to person in such a manner as two peoples' webs can never be completely reconciled, it 
does not matter that once in a while language tangentially touches upon something 
"real": we are still only hunting ghosts.
5. Actually, Asmodeus, I don't think that this little discussion makes me look that bad. 
But I shudder to think how humiliating it would be for the CTMU's originator if he were 
proven to be participating in this discussion under a pseudonym, insulted those who 
disagree with him, had a 22-year-old grad student tell him where he's gone wrong in his 
thinking (even if he can't accept this himself), and just generally conducted himself in a 
fashion which is a good deal less than becoming of someone of his alleged intellect and 
understanding. I also find the veiled threat in your comment quite laughable, and 
incredibly naïve: I can assure you, Asmodeus, that there is not much in the CTMU to be 
"unconditionally vindicated". What valid points it makes have been made before by other 
thinkers (almost all of whom go uncredited in the CTMU paper); and the pretentiousness 
of the author's other claims render even these few aspects of it risible and hollow. Also, 
Asmodeus, respect is something that has to be earnt if it is to have any value. Your 
behaviour here is not deserving of respect, I don't think. Let's, just for a second, pretend 
that you are right in your insinuations: I am a precocious little upstart, and you are an all-
wise, infinitely insightful sage. Even under these circumstances, your sneering manner 
and pompous airs would be wholly uncalled for, since I would hardly pose a threat to 
you, and, if you were in the right about the CTMU, your demagogic talents would have 
swayed the hordes at the deletion proceedings. But since the opposite is the case (I 
have merely been trying to bring a little bit of reason and logic to this benighted, 
anarchic repository of alleged knowledge, while you have been trying, unsuccessfully, to 
establish a place on it for your pet theory), your conduct reveals what I can only interpret 
as a deep-seated insecurity and fear of being found wrong; something you will not let 
happen under any circumstances whatsoever, and will go to extreme lengths to prevent. 
I mean you no personal harm, Asmodeus. However, I feel that I have a duty to those I 
teach and to everyone else who is trying to learn, to keep what could be a superb 
reference work free of misleading personal aggrandisement conducted at their expense.
6, 9, 10. Let's quickly review what has happened here, Asmodeus. A new user, 
CaveBat, suggested that I engage you in a debate on the CTMU. I told him that I had 

already made that offer to you, and been denied. You then came along and claimed that 
such a discussion had already happened, and deposited a list of alleged "errors" that I 
had made. Because, just like you do, I have a right to defend myself, I felt the need to 
point out that those weren't "errors" at all, merely a bulleted list of your own misinformed 
opinions on things. And from there on, this slinging match over various matters ensued. 
Now, I suggested recommencing our relationship on express terms of mutual civility: I 
would cease making snide remarks about the CTMU, and you would cease making 
snide remarks about me. But, seemingly unable to restrain yourself, you continued with 
your vexatious "error count", your denigration of my intellect and ability, and your 
disparagement of other users of this encyclopaedia (the APEs from the "WikiProject" 
boards). So, once again, this engagement escalated. In addition, another editor has 
expressed concern about the resources that this discussion is taking up...and apart from 
it sapping my own enthusiasm to contribute positively to this "encyclopaedia" lest my 
work is savaged and mauled by the hordes of self-promoting kooks with personal 
agendas to foist upon everyone else, having a 236kb talk page is gobbling up a sizeable 
chunk of my monthly broadbandwidth allotment (let us not forget that South Africa is a 
third world country with only one landline telecommunications operator, and we all know 
how monopolies affect products). And Wikipedia is still not the place for a discussion on 
the merits of the CTMU, Asmodeus. It is just that simple.
7. There is nothing wrong with pedantry if abuse of language has been allowed to run 
riot to such an extent that it interferes with the search for truth. In other words, the study 
of ontology and related matters is an exacting discipline. Ideally, it should be conducted 
as rigorously as any mathematical investigation. But, since we have been using natural 
language, we have to be doubly careful that our words do not lead us astray: and being 
loose with words can lead to problems and anomalies just as surely as division by zero 
in algebra can let us "prove" that 0 = 1. Hence I have felt it necessary to point out that 
the CTMU relies on similar tricks (whether or not they were intentionally put into the 
theory to deceive, or are merely oversight on the part of its creator).
8. Wikipedia has many problems, Asmodeus. One of its biggest, I fear, is that it panders 
to the majority in all things: the morality and wisdom of the herd is to be held as inviolate 
and sacrosanct; it matters not whether something is correct, but merely whether it meets 
ephemeral criteria of "notability", "verifiability", etc. (so much for being an "authoritative" 
reference); objectionable individuals' activities are not only facilitated by policies and 
"administrators", but the situation has gotten so bad that a shiny Wikipedia article is now 
seen by many "epistemic minorities" as being an arbiter of the validity and respectability 
of their position - a problem when, on the other hand, a shiny Wikipedia article is seen 
by the naïve majority as being a convenient and reliable source of information, and 
perhaps even, to use a word which seems to be taboo around here, the truth about a 
given subject; the lack of any proper authority or control leaves individual editors to fight 
their own battles with epistemic minorities - something which most of the more 
competent editors are well capable of doing, but which ties up time and resources which 
are better spent on other things (not only on the encyclopaedia), and turns what could 
be a satisfying hobby into a wretched gladiatorial arena. So, in short, the philosophical 
bias which smothers Wikipedia is nothing but the overall atmosphere of "egalitarianism 
and political correctness at all costs", since what are the costs anyway, but mere trifling 
adornments like "truth", "reason", "logic" and "reality"? All things an encyclopaedia must 
obviously forsake in favour of such desirable traits as "neutrality", "inclusivity" and a 

hefty dose of neo-Soviet "equality". Wikipedia is a labyrinth of troll-caves into which the 
unwitting but well-meaning contributor is ensnared on the pretence of "openness", a 
terrifying glimpse of what a postmodern epistemology is like on a global scale. So, the 
efforts of the twenty-odd members of WikiProjects Physics and Pseudoscience in 
combatting weird "theories" and trying to build up and maintain a reasonable corpus of 
accurate articles on worthwhile subjects are very much a drop in the ocean, Asmodeus. 
I am sorry that you are offended by such loathsome behaviour, but then one cannot, 
despite the boundless wisdom contained in Wikipedia's policies, please everyone all the 
time. Which brings me to:
11. My userpage is inflammatory, is it? Oh no! The comments attributed to you on that 
page did come out of your mouth, after all. If they sound a little weird a couple of weeks 
later (like my evident intent to "bite Langan's feet" or my "deceptive habit" of asking for 
verifiable sources for things), then maybe in future you should put a little more thought 
into what you write before hitting the "Save page" button. And let's face it, the little rant 
on your userpage is hardly "isomorphic" to the group-hug sentiment we are meant to 
embrace as Wikipedia editors, now is it? But I've taken the comments at the bottom out, 
since I am planning to revamp the page soon, anyway...although I am not sure you will 
like the new version either, but it will at least be a little less personal, more general. So 
let's see, shall we?
12. I shall be archiving this page sometime after midnight tonight (GMT), under the title 
"August 2006", with a link from my "main" talk page. This is not because I feel I have 
anything to hide (on the contrary) but rather for the reasons explained above. I trust 
there will be no objections to this, or revert wars to restore it (all comments up to this 
point will be kept and duly preserved as a record of discussions that occurred, bla bla). 
Byrgenwulf 09:23, 4 September 2006 (UTC)
Response #20:
2. As I've pointed out, all of the definitional errors thus far have been yours...100% of them. 
(See number 7 below.)
4. Language can distort reality, but it is also the only thing that permits its representation 
and supports complex reasoning about it. A distinction must therefore be made between its 
invariant (logically necessary) and variable (fallible, falsifiable) aspects. The CTMU does 
this by distinguishing between distributed syntax and nondistributed forms of expression, 
and between linguistic potential and actualization. The M=R identity refers primarily to 
syntactic potential. Syntax, including logic, is the same for everybody, including you.
5. I don't think Langan considers you qualified to ever become a source of embarrassment 
to him. On the other hand, your peanut gallery notwithstanding, this discussion makes you 
look absolutely God-awful, and I'm not kidding about that. You haven't won a single point 
despite all your lexical fussing and fuming, your tone has wandered aimlessly between 
snide and vicious, and you began the whole mess by transparently finagling the deletion of 
an article that describes an inarguable form of truth on grounds of "non-notability" after the 
mass media had announced and described it to millions of people across the world. You 
seem to have done this to advance your personal atheist-materialist convictions and 
assuage your inner terror that there might be others in the world who are vastly smarter 
than you (and there certainly are). All in all, it's a very bad show you've been putting on 
here. You'll probably come to understand this as you get older. But if not, then forget about 

philosophy - even if you manage to grovel and wangle your way to an advanced degree, 
you'll never find your way to the heart of the discipline. You will simply become part of the 
reason that academic philosophy is now widely regarded as an irrelevant, unregenerate 
waste of time. (By the way, if you're dead set on working in a philosophy-related field, the 
history of philosophy might be a better way for you to go - see number 7 below).
6,9,10. I "came along and claimed that such a discussion had already happened, and 
deposited a list of alleged errors that you had made", because it is a fact that you made 
those errors in your initial attack on the CTMU article, as part of a content-oriented assault 
of the kind that was never supposed to occur within any editorial or administrative 
procedure at Wikipedia. If Wikipedia is ever to improve, its flaws must be recognized. One 
of its flaws appears to be that it has become glutted with the same kind of vociferous, torch-
bearing atheist-materialist fanatic who frequents sites like The Brights, Panda's Thumb, 
Internet Infidels and Pharyngula, engaging in the same kind of pack behavior that we saw in 
the CTMU AfD/DR. As you are one of the torch-bearing atheist-materialist fanatics in 
question, I'm sure that you believe that Wikipedia and the public at large are well-served by 
your aggressive, rule-bending pursuit of atheist-materialist ideology. However, inasmuch as 
this sort of narrow-minded self-confidence is characteristic of zealots everywhere, it does 
not constitute a valid basis for editorial policy.
7. I'm sorry that Langan's choice of terminology "led you astray". You could easily have 
solved this by consulting a dictionary - you're not nearly as good with language as you 
seem to think you are, and need to consult basic references more frequently. In fact, I'll give 
you an additional pointer. Your main problem with language is really very simple: definitions 
have underlying models, many of which are based on assumption. When one is switching 
models, one must be more flexible, reinterpreting language with respect to the new model 
(either that, or one must coin many more neologisms that even Langan is accused of 
coining). You clearly want to lock terms into their conventional underlying models, thus 
destroying their utility for describing any system which departs from conventional 
background assumptions. This is the (usually insufferable) intellectual strategy of the 
pedant. That it is yours as well implies that your mind is better suited to the history of 
philosophy, where the focus remains on old models and ossified definitions, than to cutting-
edge philosophy itself. The earlier and better you come to understand this, the longer and 
more satisfying a career you are likely to enjoy. (I'm not kidding - this is sincere and well-
meant advice.)
8. See (6, 9, 10) above. Incidentally, you are not the sole (or even a credible) arbiter of 
"truth", "reason", "logic" or "reality". These still involve many open questions on which there 
are many notable viewpoints. In fact, the vast majority of people would say that as a self-
professed atheist-materialist "with a chip on his shoulder", you are not even close to being 
on the mark about those topics. It is thus more than a bit ironic that you bandy these terms 
about so freely. (Don't be too quick to assume that you're one of the "elites" that Jimmy 
Wales had in mind when making his notorious some-editors-are-better-than-others 
comments - you've been caught in way too many errors for that, and your ideological torch 
is flaming far too brightly.) As far as neutrality is concerned, give us all a break - you're just 
one of many Wikipedia editors who wouldn't know NPOV if it walked up and bit you in the 
nose.
11. My User Page deals with a more or less generic set of obvious problems here at 
Wikipedia, without mentioning names or theories. In stark contrast, your userpage is like a 
stalker's shrine. Leave it up if you like, but if you do, then anybody can waltz in here any 

time and respond to its contentious and disparaging remarks, referencing and linking this 
and other dialogues as extensively as they like. Either resign yourself to this, or tone down 
your vitriol and depersonalize your concerns. Within a week after coming here, you'd 
launched the attack that would result in the deletion of the CTMU article; this is now 
Wikipedia history. Relax, you don't need to specialize in the CTMU any more. Asmodeus 
17:06, 4 September 2006 (UTC)
Hmmm...Just a few more. As far as dictionary definitions go, Asmodeus, I think you'll 
find that they are on my side...and it is not that Langan's words led me astray: they led 
him astray. You see, your diagnosis of my "major problem with language" is that I do not 
know that "definitions have underlying models, many of which are based on 
assumption". I quite agree with your characterisation of definitions, but not with your 
diagnosis. You see, this was, in fact, a point I have been making about the CTMU since 
the beginning: principles like "Mind=Reality" are based on the assumptions which 
underlie words for such complicated entities. We have no a priori reason to assume that 
syntax helps us overcome these problems, either: on the surface it appears to do so, but 
then the same could be said of semantics. Syntax, after all, may appear to be the same 
for everyone, but that is no guarantee that it is "true" in any way: the existence of a 
common denominator in human cognition tells us nothing more than cognition must be a 
result of something humans have in common. Like a brain. And all this means is that in 
some sense our brains must allow us to grow up, mate, and produce more lumps of 
meat like us (whatever this means in terms outside of human minds); it does not mean 
that the contents of our brains have much at all to do with what is going on outside them. 
The existence of a concept like "free will" might be able to sway the argument towards 
saying that there is some parallel, but the CTMU does not deploy this as a premise, 
rather appealing to "self-determinacy" later on to address the free will question. I also 
realise full well that the CTMU "departs from conventional background assumptions". 
Nothing inherently wrong with that, of course, provided cogent arguments are made as 
to why the "replacement assumptions" are superior, and all terms are explicitly redefined 
accordingly: neither of which things the CTMU does, and the former, of course, also 
implies the expressly non-tautological character of the theory. Also, the change in 
"background assumptions" does not, for example, render Langan's distortion or overly 
loose usage of concepts like "duality" any more meaningful.
Many of my arguments here have been centred on definitions of terms. But, Asmodeus, 
this is only because you seem to think that a torrent of fancy words is a replacement for 
a thin sliver of logic, and I have seen fit to point out to you that you are not using the 
terms properly, which often betrays an underlying ignorance of their actual meanings. 
Since you have been so ardent in your insistence that I don't know what I am talking 
about, I thought that perhaps a few empirical counter-examples might cause you to see 
that the situation is actually the converse of what you think it is; but evidently you can't. 
Oh well.
And finally, a few words about my motivations, although I would have thought I had 
made them transparently clear by now. I am indeed an atheist, and I suppose my 
philosophical position lies a little more on the side of materialism than idealism, but I 
think that nominalist is a little closer to being accurate. I also, Asmodeus, have no 
problem admitting that there have been, are currently, and will be in the future, people 
far more intelligent than myself. So once again, your little pop psychology reading fails. 
The same could be said for your curious attempt to play Nostradamus while mixing in a 

good dose of the old bar-bouncer's argumentum ad baculum: I don't have any desire to 
take career advice from someone who has performed odd jobs and menial labour all his 
life. You say that as an atheist-materialist with a chip on my shoulder, the vast majority 
of people would not consider me a viable arbiter of truth, reason, etc. Truth, Asmodeus, 
is not a democratic property. Reason is not something that the vast majority of people 
possess in copious quantities. I have yet to be "caught" in a single substantial "error" by 
any credible authority (and Asmodeus, you are not, whatever you yourself might think, a 
credible authority); I also know that if someone does point out a genuine error to me, I 
am well capable of admitting it. Like any good chess player, I will surrender if captured. 
It's just that you, Asmodeus, have picked all the wrong points here! Byrgenwulf 18:47, 4 
September 2006 (UTC)
Respons
 
 e #21
 
 :  At first, I wasn't going to respond. But since you're still holding fast to all of 
your original errors and making new errors to boot, and since your supercilious presentation 
remains as obnoxious as ever, I've had second thoughts. It also occurs to me that you 
deserve to have this posted to the newly-purged facade of your talk page. However, for the 
sake of civility, I'll give you one last last chance to avoid that dire eventuality.
59. The dictionary may have been on your side in some other debate. But in this one, it has 
left you unceremoniously in the lurch.
60. The M=R Principle (of the CTMU) is based on the facts that (1) perception requires 
percipients, and (2) perception entails syntactic invariants which distribute over percipients 
and percepts alike. Your major problems with language are serious enough to have 
prevented you from absorbing these self-evident facts despite repetitive remedial 
instruction.
61. Syntax is a priori by definition. Tautologically speaking, it thus constitutes an a priori 
justification of its own validity.
62. If the contents of human brains had nothing in common with what goes on outside them, 
then the scientific description, explanation, and prediction of objective reality would be 
impossible. On the other hand, if they do have something in common, it qualifies as 
"distributed syntax" with respect to both mind and reality, making them identical within its 
scope, which is coterminous with spacetime connectivity.
63. In the CTMU, self-deterrminacy is free will.
64. The CTMU does indeed provide cogent arguments for its departures from conventional 
background assumptions, and most of them do indeed involve tautology (as defined in 
reputable English dictionaries).
65. Langan's use of the term "duality" is exact, not "loose". That is, all examples of duality 
given by Langan conform to the general mathematical meaning of duality. (This has already 
been explained in response to your absurd contention that specific kinds of mathematical 
duality lack a common basis despite their conventional description in terms of the general 
mathematical concept "duality".)
66. I have explained the logic behind my reasoning at every turn, and it clearly beats your 
own fancily-worded circumlocutions.
67. You have not produced a single valid counterexample to any assertion made by 
Langan, or me, regarding the CTMU. All of your attempts to do so have been easily 
disposed of. No amount of doubletalk can change that.

68. Reducing the human mind to a function of "lumps of warm, wet matter", and human 
beings to mere "lumps of meat", strongly implies that you are a materialist plain and simple. 
(Your professed "nominalism" is another matter entirely.)
69. It is a mistake, and an extremely unattractive one at that, to peer down your snout at 
honest, hard-working people "who perform odd jobs and menial labour all their lives". Every 
one of them is contributing more to humanity in a single week than somebody like you, a 
lawyerly, labor-shirking verbal fencer who falls far short of his scholarly pretensions, could 
hope to match in a lifetime.
70. Although you have now been caught in 69 glaring, substantial errors, you have yet to 
confess to a single one of them. This strongly implies that you are utterly incapable of 
admitting fault in any meaningful way. (In fact, a psychologist would call this level of 
incapacity "pathological".) Your claim to the contrary must therefore be counted as another 
error, bringing your error count to an impressive seventy (70).
But just to close on a positive note, we do seem to be in agreement on one point: truth is 
indeed not a democratic property. You might try to remember that the next time it occurs to 
you to manipulate the democratic processes of Wikipedia to vote down a theory so true, and 
so far over your head, that you'd be better off kissing the sky than confusedly ranting about 
it. Asmodeus 17:10, 7 September 2006 (UTC)
Please report any typos or errors to dylancatlow@comcast.net

