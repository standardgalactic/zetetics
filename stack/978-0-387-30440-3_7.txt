Abrupt Climate Change Modeling
A
1
A
Abrupt Climate Change Modeling
GERRIT LOHMANN
Alfred Wegener Institute for Polar and Marine Research,
Bremerhaven, Germany
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
A Mathematical Deﬁnition
Earth System Modeling and Analysis
Example: Glacial-Interglacial Transitions
Example: Cenozoic Climate Cooling
Examples: Transient Growth
Future Directions
Bibliography
Glossary
Atmosphere The atmosphere is involved in many pro-
cesses of abrupt climate change, providing a strong
non-linearity in the climate system and propagating
the inﬂuence of any climate forcing from one part of
the globe to another. Atmospheric temperature, com-
position, humidity, cloudiness, and wind determine
the Earth’s energy ﬂuxes. Wind aﬀects the ocean’s sur-
face circulation and upwelling patterns. Atmospheric
moisture transport determines the freshwater balance
for the oceans, overall water circulation, and the dy-
namics of glaciers.
Oceans Because water has enormous heat capacity,
oceans typically store 10–100 times more heat than
equivalent land surfaces. The oceans exert a profound
inﬂuence on climate through their ability to trans-
port heat from one location to another. Changes in
ocean circulation have been implicated in abrupt cli-
mate change of the past. Deglacial meltwater has fresh-
ened the North Atlantic and reduced the ability of the
water to sink, inducing long-term coolings.
Land surface The reﬂective capacity of the land can
change greatly, with snow or ice sheets reﬂecting up
to 90% of the sunlight while dense forests absorb
more than 90%. Changes in surface characteristics can
also aﬀect solar heating, cloud formation, rainfall, and
surface-water ﬂow to the oceans, thus feeding back
strongly on climate.
Cryosphere The portion of the Earth covered with ice
and snow, the cryosphere, greatly aﬀects temperature.
When sea ice forms, it increases the planetary re-
ﬂective capacity, thereby enhancing cooling. Sea ice
also insulates the atmosphere from the relatively warm
ocean, allowing winter air temperatures to steeply de-
cline and reduce the supply of moisture to the atmo-
sphere. Glaciers and snow cover on land can also pro-
vide abrupt-change mechanisms. The water frozen in
a glacier can melt if warmed suﬃciently, leading to
possibly rapid discharge, with consequent eﬀects on
sea level and ocean circulation. Meanwhile, snow-cov-
ered lands of all types maintain cold conditions be-
cause of their high reﬂectivity and because surface tem-
peratures cannot rise above freezing until the snow
completely melts.
External factors Phenomena external to the climate sys-
tem can also be agents of abrupt climate change.
For example, the orbital parameters of the Earth vary
over time, aﬀecting the latitudinal distribution of so-
lar energy. Furthermore, ﬂuctuations in solar output,
prompted by sunspot activity or the eﬀects of solar
wind, as well as volcanoes may cause climate ﬂuctu-
ations.
Climate time scales The climate system is a composite
system consisting of ﬁve major interactive compo-
nents: the atmosphere, the hydrosphere, including the
oceans, the cryosphere, the lithosphere, and the bio-
sphere. All subsystems are open and non-isolated,
as the atmosphere, hydrosphere, cryosphere and bio-
sphere act as cascading systems linked by complex

2
A
Abrupt Climate Change Modeling
feedback processes. Climate refers to the average con-
ditions in the Earth system that generally occur over
periods of time, usually several decades or longer. This
time scale is longer than the typical response time
of the atmosphere. Parts of the other components of
the Earth system (ice, ocean, continents) have much
slower response times (decadal to millennial).
Climate variables and forcing State variables are tem-
perature, rainfall, wind, ocean currents, and many
other variables in the Earth system. In our notation,
the variables are described by a ﬁnite set of real vari-
ables in a vector x(t) 2 Rn. The climate system is sub-
ject to two main external forcings F(x; t) that condi-
tion its behavior, solar radiation and the action of grav-
ity. Since F(x; t) has usually a spatial dependence, F is
also a vector 2 Rn. Solar radiation must be regarded
as the primary forcing mechanism, as it provides al-
most all the energy that drives the climate system. The
whole climate system can be regarded as continuously
evolving, as solar radiation changes on diurnal, sea-
sonal and longer time scales, with parts of the system
leading or lagging in time. Therefore, the subsystems
of the climate system are not always in equilibrium
with each other. Indeed, the climate system is a dissipa-
tive, highly non-linear system, with many instabilities.
Climate models are based on balances of energy, mo-
mentum, and mass, as well as radiation laws. There are
several model categories, full circulation models, low-
order models, and models of intermediate complexity.
Climate models simulate the interactions of the atmo-
sphere, oceans, land surface, and ice. They are used for
a variety of purposes from study of the dynamics of the
weather and climate system, past climate to projections
of future climate.
Global climate models or General circulation models
(GCMs) The balances of energy, momentum, and
mass are formulated in the framework of ﬂuid dy-
namics on the rotating Earth. GCMs discretize the
equations for ﬂuid motion and energy transfer and
integrate these forward in time. They also contain
parametrization for processes – such as convection –
that occur on scales too small to be resolved directly.
The dimension of the state vector is in the order of
n  105  108 depending on the resolution and com-
plexity of the model.
Model categories In addition to complex numerical cli-
mate models, it can be of great utility to reduce
the system to low-order, box, and conceptual mod-
els. This complementary approach has been success-
fully applied to a number of questions regarding feed-
back mechanisms and the basic dynamical behavior,
e. g. [48,84]. In some cases, e. g. the stochastic climate
model of Hasselmann [32], such models can provide
a null hypothesis for the complex system. The tran-
sition from highly complex dynamical equations to
a low-order description of climate is an important
topic of research. In his book “Dynamical Paleocli-
matology”, Saltzman [77] formulated a dynamical sys-
tem approach in order to diﬀerentiate between fast-re-
sponse and slow-response variables. As an alternative
to this method, one can try to derive phenomenologi-
cally based concepts of climate variability, e. g. [21,43].
In between the comprehensive models and conceptual
models, a wide class of “models of intermediate com-
plexity” were deﬁned [12].
Earth-system models of intermediate complexity
(EMICs) Depending on the nature of questions asked
and the pertinent time scales, diﬀerent types of models
are used. There are, on the one extreme, conceptual
models, and, on the other extreme, comprehensive
models (GCMs) operating at a high spatial and tem-
poral resolution. Models of intermediate complexity
bridge the gap [12]. These models are successful in de-
scribing the Earth system dynamics including a large
number of Earth system components. This approach
is especially useful when considering long time scales
where the complex models are computationally too
expensive, e. g. [47]. Improvements in the develop-
ment of coupled models of intermediate complexity
have led to a situation where modeling a glacial cycle,
even with prognostic atmospheric CO2 is becoming
possible.
Climate simulation A climate simulation is the output
of a computer program that attempts to simulate the
climate evolution under appropriate boundary condi-
tions. Simulations have become a useful part of climate
science to gain insight into the sensitivity of the system.
Climate variability pattern Climate variability is deﬁned
as changes in integral properties of the climate sys-
tem. True understanding of climate dynamics and pre-
diction of future changes will come only with an un-
derstanding of the Earth system as a whole, and over
past and present climate. Such understanding requires
identiﬁcation of the patterns of climate variability and
their relationships to known forcing. Examples for cli-
mate variability patterns are the North Atlantic Os-
cillation (NAO) or the El Niño-Southern Oscillation
(ENSO).
Abrupt climate change One can deﬁne abrupt climate
change in the time and frequency domain. (a) Time
domain: Abrupt climate change refers to a large shift
in climate that persists for years or longer, such

Abrupt Climate Change Modeling
A
3
as marked changes in average temperature, or al-
tered patterns of storms, ﬂoods, or droughts, over
a widespread area that takes place so rapidly that the
natural system has diﬃculty adapting to it. In the con-
text of past abrupt climate change, “rapidly” typically
means on the order of a decade. (b) Frequency domain:
An abrupt change means that the characteristic peri-
odicity changes. Also the phase relation between cer-
tain climate variables may change in a relatively short
time. For both types of changes examples will be pro-
vided.
Regime shifts are deﬁned as rapid transitions from one
state to another. In the marine environment, regimes
may last for several decades, and shifts often appear
to be associated with changes in the climate system. If
the shifts occur regularly, they are often referred to as
an oscillation (e. g., Atlantic Multi-decadal Oscillation,
Paciﬁc Decadal Oscillation). Similarly, one can deﬁne
a regime shift in the frequency domain.
Anthropogenic climate change Beginning with the in-
dustrial revolution in the 1850s and accelerating ever
since, the human consumption of fossil fuels has ele-
vated CO2 levels from a concentration of  280 ppm
to more than 380 ppm today. These increases are pro-
jected to reach more than 560 ppm before the end of
the 21st century. As an example, a concomitant shift of
ocean circulation would have serious consequences for
both agriculture and ﬁshing.
Multiple equilibria Fossil evidence and computer mod-
els demonstrate that the Earth’s complex and dynamic
climate system has more than one mode of operation.
Each mode produces diﬀerent climate patterns. The
evidence of models and data analysis shows that the
Earth’s climate system has sensitive thresholds. Pushed
past a threshold, the system can jump from one stable
operating mode to a completely diﬀerent one.
Long-term climate statistics Starting with a given initial
state, the solutions x(t) of the equations that govern
the dynamics of a non-linear system, such as the at-
mosphere, result in a set of long-term statistics. If all
initial states ultimately lead to the same set of statis-
tical properties, the system is ergodic or transitive. If,
instead, there are two or more diﬀerent sets of statisti-
cal properties, where some initial states lead to one set,
while the other initial states lead to another, the system
is called intransitive (one may call the diﬀerent states
regimes). If there are diﬀerent sets of statistics that
a system may assume in its evolution from diﬀerent
initial states through a long, but ﬁnite, period of time,
the system is called almost intransitive [50,51,53]. In
the transitive case, the equilibrium climate statistics
are both stable and unique. Long-term climate statis-
tics will give a good description of the climate. In the
almost intransitive case, the system in the course of
its evolution will show ﬁnite periods during which
distinctly diﬀerent climatic regimes prevail. The al-
most intransitive case arises because of internal feed-
backs, or instabilities involving the diﬀerent compo-
nents of the climatic system. The climatic record can
show rapid step-like shifts in climate variability that
occur over decades or less, including climatic extremes
(e. g. drought) that persist for decades.
Feedbacks A perturbation in a system with a negative
feedback mechanism will be reduced whereas in a sys-
tem with positive feedback mechanisms, the pertur-
bation will grow. Quite often, the system dynamics
can be reduced to a low-order description. Then, the
growth or decay of perturbations can be classiﬁed by
the systems’ eigenvalues or the pseudospectrum. Con-
sider the stochastic dynamical system
d
dt x(t) D f (x) C g(x) C F(x; t) ;
(1)
where  is a stochastic process. The functions f ; g de-
scribe the climate dynamics, in this case without ex-
plicit time dependence. The external forcing F(x; t)
is generally time-, variable-, and space-dependent. In
his theoretical approach, Hasselmann [32] formulated
a linear stochastic climate model
d
dt x(t) D Ax C  C F(t) ;
(2)
with system matrix A 2 Rnn, constant noise term ,
and stochastic process . Interestingly, many features
of the climate system can be well described by (2),
which is analogous to the Ornstein–Uhlenbeck process
in statistical physics [89]. In the climate system, linear
and non-linear feedbacks are essential for abrupt cli-
mate changes.
Paleoclimate Abrupt climate change is evident in model
results and in instrumental records of the climate sys-
tem. Much interest in the subject is motivated by the
evidence in archives of extreme changes. Proxy records
of paleoclimate are central to the subject of abrupt cli-
mate change. Available paleoclimate records provide
information on many environmental variables, such
as temperature, moisture, wind, currents, and isotopic
compositions.
Thermohaline circulation stems from the Greek words
“thermos” (heat) and “halos” (salt). The ocean is
driven to a large extent by surface heat and freshwater

4
A
Abrupt Climate Change Modeling
ﬂuxes. As the ocean is non-linear, it cannot be strictly
separated from the wind-driven circulation. The ex-
pressions thermohaline circulation (THC) and merid-
ional overturning circulation (MOC) in the ocean are
quite often used as synonyms although the latter in-
cludes all eﬀects (wind, thermal, haline forcing) and
describes the ocean transport in meridional direction.
Another related expression is the ocean conveyor belt.
This metaphor is motivated by the fact that the North
Atlantic is the source of the deep limb of a global ocean
circulation system [10]. If North Atlantic surface wa-
ters did not sink, the global ocean circulation would
cease, currents would weaken or be redirected. The re-
sulting reorganization would reconﬁgure climate pat-
terns, especially in the Atlantic Ocean. One fundamen-
tal aspect of this circulation is the balance of two pro-
cesses: cooling of the deep ocean at high latitudes, and
heating of deeper levels from the surface through ver-
tical mixing.
Definition of the Subject
The occurrence of abrupt change of climate at various time
scales has attracted a great deal of interest for its theoretical
and practical signiﬁcance [2,3,9]. To some extent, a deﬁni-
tion of what constitutes an abrupt climatic change depends
on the sampling interval of the data being examined [28].
For the instrumental period covering approximately the
last 100 years of annually or seasonally sampled data, an
abrupt change in a particular climate variable will be taken
to mean a statistically highly signiﬁcant diﬀerence between
adjacent 10-year sample means. In the paleoclimate con-
text (i. e. on long time scales), an abrupt climate change
can be in the order of decades to thousands of years. Since
the climate dynamics can be often projected onto a lim-
ited number of modes or patterns of climate variability
(e. g., [21,22]), the deﬁnition of abrupt climate change is
also related to spatio-temporal patterns.
The concept of abrupt change of climate is therefore
applied for diﬀerent time scales. For example, changes
in climatic regimes were described associated with sur-
face temperature, precipitation, atmospheric circulation in
North America during the 1920s and 1960s [19,75]. Some-
times, the term “climate jump” is used instead of “abrupt
climate change”, e. g. [92]. Flohn [25] expanded the con-
cept of abrupt climate change to include both singular
events and catastrophes such as the extreme El Niño of
1982/1983, as well as discontinuities in paleoclimate in-
dices taken from ice cores and other proxy data. In the
instrumental record covering the last 150 years, there is
a well-documented abrupt shift of sea surface tempera-
ture and atmospheric circulation features in the North-
ern Hemisphere in the mid-1970s, e. g. [22,67,88]. Some
of the best-known and best-studied widespread abrupt cli-
mate changes started and ended during the last deglacia-
tion, most pronounced at high latitudes.
In his classic studies of chaotic systems, Lorenz has
proposed a deterministic theory of climate change with his
concept of the "almost-intransitivity" of the highly non-
linear climate systems. In this set of equations, there exists
the possibility of multiple stable solutions to the governing
equations, even in the absence of any variations in exter-
nal forcing [51]. More complex models, e. g. [11,20] also
demonstrated this possibility. On the other hand, varia-
tions in external forcing, such as the changes of incoming
solar radiation, volcanic activity, deglacial meltwater, and
increases of greenhouse gas concentration have also been
proposed to account for abrupt changes in addition to
climate intransitivity [9,25,38,41,49]. A particular climate
change is linked to the widespread continental glaciation
of Antarctica during the Cenozoic (65 Ma to present) at
about 34 Ma, e. g. [93]. It should be noted that many facets
of regional climate change are abrupt changes although the
global means are rather smoothly changing.
Besides abrupt climate change as described in the time
domain, we can ﬁnd abrupt shifts in the frequency do-
main. A prominent example for an abrupt climate change
in the frequency domain is the mid-Pleistocene transi-
tion or revolution (MPR), which is the last major “event”
in a secular trend towards more intensive global glacia-
tion that characterizes the last few tens of millions of
years. The MPR is the term used to describe the transi-
tion between 41 ky (ky D 103 years) and 100 ky glacial-in-
terglacial cycles which occurred about one million years
ago (see a recent review in [61]). Evidence of this is pro-
vided by high-resolution oxygen isotope data from deep
sea cores, e. g. [45,83].
Another example is the possibility of greenhouse gas-
driven warming leading to a change in El Niño events.
Modeling studies indicate that a strong enhancement of
El Niño conditions in the future is not inconceivable [85].
Such a shift would have enormous consequences for both
the biosphere and humans. The apparent phase shifts dur-
ing the 1970s seems unique over this time period, and may
thus represent a real climate shift although the available
time series is probably too short to unequivocally prove
that the shift is signiﬁcant [90]. The inability to resolve
questions of this kind from short instrumental time series
provides one of the strongest arguments for extending the
instrumental record of climate variability with well-dated,
temporally ﬁnely resolved and rigorously calibrated proxy
data.

Abrupt Climate Change Modeling
A
5
Introduction
One view of climate change is that the Earth’s climate sys-
tem has changed gradually in response to both natural and
human-induced processes. Researchers became intrigued
by abrupt climate change when they discovered striking
evidence of large, abrupt, and widespread changes pre-
served in paleoclimatic archives, the history of Earth’s cli-
mate recorded in tree rings, ice cores, sediments, and other
sources. For example, tree rings show the frequency of
droughts, sediments reveal the number and type of organ-
isms present, and gas bubbles trapped in ice cores indicate
past atmospheric conditions.
The Earth’s climate system is characterized by change
on all time and space scales, and some of the changes are
abrupt even relative to the short time scales of relevance to
human societies. Paleoclimatic records show that abrupt
climate changes have aﬀected much or all of the Earth re-
peatedly over the last ice-age cycle as well as earlier – and
these changes sometimes have occurred in periods as short
as a few years, as documented in Greenland ice cores. Per-
turbations at northern high latitudes were spectacularly
large: some had temperature increases of up to 10–20ıC
and a local doubling of precipitation within decades.
In the frequency domain, abrupt climate shifts are due
to changes in the dominant oscillations (as in the case of
the MPR), or due to a shift in the phase between diﬀerent
climate signals. As an example, the phase between the In-
dian Monsoon and ENSO exhibits signiﬁcant shifts for the
past 100 years [59].
The period of regular instrumental records of global
climate is relatively short (100–200 years). Even so, this
record shows many climatic ﬂuctuations, some abrupt or
sudden, as well as slow drifts in climate. Climatic changes
become apparent on many temporal and spatial scales.
Most abrupt climate changes are regional in their spa-
tial extent. However, regional changes can have remote
impacts due to atmospheric and oceanic teleconnections.
Some of these shifts may be termed abrupt or sudden
in that they represent relatively rapid changes in other-
wise comparatively stable conditions, but they can also
be found superimposed on other much slower climatic
changes.
The deﬁnition of “abrupt” or “rapid” climate changes
is therefore necessarily subjective, since it depends in large
measure on the sample interval used in a particular study
and on the pattern of longer-term variation within which
the sudden shift is embedded. It is therefore useful to avoid
a too general approach, but instead to focus on diﬀerent
types of rapid transitions as they are detected and modeled
for diﬀerent time periods. Although distinctions between
types are somewhat arbitrary, together they cover a wide
range of shifts in dominant climate mode on time scales
ranging from the Cenozoic (the last 65 millions of years)
to the recent and future climate.
A Mathematical Definition
Time Domain
Abrupt climate change is characterized by a transition of
the climate system into a diﬀerent state (of temperature,
rainfall, and other variables) on a time scale that is faster
than variations in the neighborhood (in time). Abrupt cli-
mate change could be related to a forcing or internally gen-
erated. Consider x(t) 2 Rn as a multi-dimensional climate
state variable (temperature, rainfall, and other variables).
We deﬁne an abrupt climate shift of degree  and ampli-
tude B, if
d
dt xi(t)
can be approximated by a function
B


x2
i C 2
(3)
for one i 2 f1; : : : ; ng in a time interval [t1; t2]. The case
 ! 0 is called instantaneous climate shift, i. e. xi(t) can be
approximated by the Heaviside step function. The degree
of approximation can be speciﬁed by a proper norm.
An alternative way of deﬁning an abrupt climate shift
is through the identiﬁcation of probable breaks in a time
series (e. g., the surface temperature series). The formula-
tion of a two-phase regression (TPR) test, e. g. [55,79], de-
scribing a series x(t) is given by
x(t) D 1 C ˛1t C (t)
for t  c
(4)
x(t) D 2 C ˛2t C (t)
for t > c :
(5)
Under the null hypothesis of no changepoint, the two
phases of the regression should be statistically equivalent
and both the diﬀerence in means 1;2, and the diﬀerence
in slopes, ˛1;2, should be close to zero for each possible
changepoint c.
In a stochastic framework one may use an appropriate
stochastic diﬀerential equation (Langevin equation)
d
dt x(t) D f (x) C g(x) ;
(6)
where  is a stationary stochastic process and the functions
f ; g : Rn ! Rn describe the climate dynamics. Abrupt
climate change can be deﬁned as a transition of a short
period of time [t1; t2], where the probability of an event is
larger than a threshold. The properties of the random force

6
A
Abrupt Climate Change Modeling
are described through its distribution and its correlation
properties at diﬀerent times. In the Ornstein–Uhlenbeck
process  is assumed to have a Gaussian distribution of
zero average,
h(t)i D 0
(7)
and to be ı-correlated in time,
h(t)(t C )i D ı() :
(8)
The brackets indicate an average over realizations of the
random force. For a Gaussian process only the average and
second moment need to be speciﬁed since all higher mo-
ments can be expressed in terms of the ﬁrst two. Note that
the dependence of the correlation function on the time dif-
ference  assumes that  is a stationary process.
The probability density p(x; t) for the variable x(t)
in (6) obeys the Fokker–Planck equation
@t p D  @
@x
f (x)p C @
@x

g(x) @
@x fg(x)pg

:
(9)
Its stationary probability density of (6) is given by
pst(x) D @ exp

2
Z x
x0
f (y)  g(y)g0(y)
g(y)2
dy

; (10)
where @ is a normalization constant. g0(y) stands for the
derivative of g with respect to its argument. The extrema
xm of the steady state density obey the equation
f (xm)  g(xm)g0(xm) D 0
(11)
for g(xm) ¤ 0. Here is the crux of the noise-induced tran-
sition phenomenon: one notes that this equation is not
the same as the equation f (xm) D 0 that determines the
steady states of the system in the absence of multiplicative
noise. As a result, the most probable states of the noisy
system need not coincide with the deterministic station-
ary states. More importantly, new solutions may appear or
existing solutions may be destabilized by the noise. These
are the changes in the asymptotic behavior of the system
caused by the presence of the noise, e. g. [84].
Climate Variability and Climate Change
The temporal evolution of climate can be expressed in
terms of two basic modes: the forced variations which are
the response of the climate system to changes in the exter-
nal forcing F(x; t) (mostly called climate change), and the
free variations owing to internal instabilities and feedbacks
leading to non-linear interactions among the various com-
ponents of the climate system [68] (mostly called climate
variability). The external causes F(x; t), operate mostly by
causing variations in the amount of solar radiation re-
ceived by or absorbed by the Earth, and comprise varia-
tions in both astronomical (e. g. orbital parameters) and
terrestrial forcings (e. g. atmospheric composition, aerosol
loading). For example, the diurnal and seasonal variations
in climate are related to external astronomical forcings op-
erating via solar radiation, while ice ages are related to
changes in Earth orbital parameters. Volcanic eruptions
are one example of a terrestrial forcing which may intro-
duce abrupt modiﬁcations over a period of 2 or 3 years.
The more rapid the forcing, the more likely it is that it will
cause an abrupt change. The resulting evolution may be
written as
d
dt x(t) D f (x) C g(x) C F(x; t) :
(12)
F(x; t) is independent of x if the forcing does not depend
on climate (external forcing).
The internal free variations within the climate system
are associated with both positive and negative feedback
interactions between the atmosphere, oceans, cryosphere
and biosphere. These feedbacks lead to instabilities or os-
cillations of the system on all time scales, and can either
operate independently or reinforce external forcings. In-
vestigations of the properties of systems which are far from
equilibrium show that they have a number of unusual
properties. In particular, as the distance from equilibrium
increases, they can develop complex oscillations with both
chaotic and periodic characteristics. They also may show
bifurcation points where the system may switch between
various regimes. Under non-equilibrium conditions, lo-
cal events have repercussions throughout the whole sys-
tem. These long-range correlations are at ﬁrst small, but
increase with distance from equilibrium, and may become
essential at bifurcation points.
When applying (12), diﬀerent concepts of climate
change are in the literature. Quite often, the dynamics is
governed by the following stochastic diﬀerential equation
d
dt x(t) D  d
dx U(x) C  C F(t)
(13)
with potential
U(x) D a4x4 C a3x3 C a2x2 C a1x :
(14)
If the potential is quadratic and F(t) D 0, the Orstein–
Uhlenbeck process is retained. In contrast, a bistable non-
linear system with two minima in U(x) has been assumed
in which shifts between the two distinctly diﬀerent states
are triggered randomly by stochastic forcing, e. g. [7]. In

Abrupt Climate Change Modeling
A
7
such a system, climate variability and change in the poten-
tial can interact due to stochastic resonance [1,7]. Stochas-
tic resonance occurs when the signal-to-noise ratio of
a non-linear device is maximized for a moderate value of
noise intensity . It often occurs in bistable and excitable
systems with sub-threshold inputs. For lower noise inten-
sities, the signal does not cause the device to cross thresh-
old, so little signal is passed through it. For large noise in-
tensities, the output is dominated by the noise, also leading
to a low signal-to-noise ratio. For moderate intensities, the
noise allows the signal to reach threshold, but the noise in-
tensity is not so large as to swamp it.
Strictly speaking, stochastic resonance occurs in
bistable systems, when a small periodic force F(t) (which
is external) is applied together with a large wide-band
stochastic force  (which is internal). The system re-
sponse is driven by the combination of the two forces that
compete/cooperate to make the system switch between the
two stable states. The degree of order is related to the
amount of periodic function that it shows in the system
response. When the periodic force is chosen small enough
in order to not make the system response switch, the pres-
ence of a non-negligible noise is required for it to happen.
When the noise is small very few switches occur, mainly
at random with no signiﬁcant periodicity in the system re-
sponse. When the noise is very strong a large number of
switches occur for each period of the periodic force and
the system response does not show remarkable periodic-
ity. Quite surprisingly, between these two conditions, there
exists an optimal value of the noise that cooperatively con-
curs with the periodic forcing in order to make almost ex-
actly one switch per period (a maximum in the signal-to-
noise ratio).
Furthermore, non-linear oscillators have been pro-
posed where the timing of the deterministic external forc-
ing is crucial for generating oscillations [51,77,78]. Some
aspects of non-equilibrium systems can be found in the
climatic system. On the climatological scale, it exhibits
abrupt jumps in the long-term rate of temperature change,
which are often associated with changes in circulation pat-
terns.
Frequency Domain
In the frequency domain, there are diﬀerent ways to de-
scribe abrupt climate change. A stationary process exhibits
an autocovariance function of the form
Cov() D h(x(t C )  hxi)(x(t)  hxi)i
(15)
where hi denotes the expectation or the statistical mean.
Normalized to the variance (i. e. the autocovariance func-
tion at  D 0) one gets the autocorrelation function C():
C() D Cov()/ Cov(0) :
(16)
Many stochastic processes in nature exhibit short-range
correlations, which decay exponentially:
C()  exp(/0) ;
for  ! 1 :
(17)
These processes exhibit a typical time scale 0.
As the frequency domain counterpart of the autoco-
variance function of a stationary process, one can deﬁne
the spectrum as
S(!) D 2
Cov() ;
(18)
where the hat denotes the Fourier transformation. How-
ever, geophysical processes are furthermore often non-sta-
tionary. In this regard, the optimal method is continuous
wavelet analysis as it intrinsically adjusts the time resolu-
tion to the analyzed scale, e. g. [16,59].
Wavelet Spectra
A major question concerns the signiﬁ-
cance testing of wavelet spectra. Torrence and Compo [86]
formulated pointwise signiﬁcance tests against reasonable
background spectra. However, Maraun and Kurths [58]
pointed out a serious deﬁciency of pointwise signiﬁcance
testing: Given a realization of white noise, large patches
of spurious signiﬁcance are detected, making it – without
further insight – impossible to judge which features of an
estimated wavelet spectrum diﬀer from background noise
and which are just artifacts of multiple testing. Under
these conditions, a reliable corroboration of a given hy-
pothesis is impossible. This demonstrates the necessity to
study the signiﬁcance testing of continuous wavelet spec-
tra in terms of sensitivity and speciﬁcity. Given the set of
all patches with pointwise signiﬁcant values, areawise sig-
niﬁcant patches are deﬁned as the subset of additionally
areawise signiﬁcant wavelet spectral coeﬃcients given as
the union of all critical areas that completely lie inside the
patches of pointwise signiﬁcant values. Whereas the speci-
ﬁcity of the areawise test appears to be – almost indepen-
dently of the signal-to-noise ratio – close to one, that of
the pointwise test decreases for high background noise, as
more and more spurious patches appear [58].
Eigenvalues and Pseudospectrum
Another spectral
method characterizing the abruptness of climate change is
related to the resonance of the linear system (1). As we will
see later in the context of atmosphere and ocean instabil-
ities, an eigenvalue analysis is inappropriate in describing
the dynamics of the system (12). Inspection of many geo-

8
A
Abrupt Climate Change Modeling
physical systems shows that most of the systems fail the
normality condition
A A D A A ;
(19)
where  denotes the adjoint-complex operator. If a ma-
trix is far from normal, its eigenvalues (the spectrum)
have little to do with its temporal evolution [71,87]. More
about the dynamics can be learned by examining the pseu-
dospectrum of A in the complex plane. The -pseudospec-
trum of operator A is deﬁned by two equivalent formula-
tions:
(A) D fz 2 C : jj(zI  A)1jj  1g
D fz 2 C : [ smallest singular value of
(zI  A)]  g :
(20)
This set of values z in the complex plane are deﬁned by
contour lines of the resolvent (zI  A)1. The resolvent
determines the system’s response to a forcing as supplied
by external forcing F(x; t), stochastic forcing g(x), or
initial/boundary conditions. The pseudospectrum reﬂects
the robustness of the spectrum and provides information
about instability and resonance. One theorem is derived
from Laplace transformation stating that transient growth
is related to how far the -pseudospectrum extends into
the right half plane:
jj exp(A t)jj  1

sup
z2(A)
Real(z) :
(21)
In terms of climate theory, the pseudospectrum indicates
resonant ampliﬁcation. Maximal ampliﬁcation is at the
poles of (zI  A)1, characterized by the eigenfrequencies.
In a system satisfying (19), the system’s response is charac-
terized solely by the proximity to the eigenfrequencies. In
the non-normal case, the pseudospectrum shows large res-
onant ampliﬁcation for frequencies which are not eigen-
frequencies. This transient growth mechanism is impor-
tant for both initial value and forced problems.
Earth System Modeling and Analysis
Hierarchy of Models
Modeling is necessary to produce a useful understanding
of abrupt climate processes. Model analyses help to focus
research on possible causes of abrupt climate change, such
as human activities; on key areas where climatic thresh-
olds might be crossed; and on fundamental uncertainties
in climate-system dynamics. Improved understanding of
abrupt climatic changes that occurred in the past and that
are possible in the future can be gained through climate
models. A comprehensive modeling strategy designed to
address abrupt climate change includes vigorous use of
a hierarchy of models, from theory and conceptual mod-
els through models of intermediate complexity, to high-
resolution models of components of the climate system,
to fully coupled earth-system models. The simpler mod-
els are well-suited for use in developing new hypotheses
for abrupt climate change. Model-data comparisons are
needed to assess the quality of model predictions. It is im-
portant to note that the multiple long integrations of en-
hanced, fully coupled Earth system models required for
this research are not possible with the computer resources
available today, and thus, these resources are currently be-
ing enhanced.
Feedback
One particularly convincing example showing that the
feedbacks in the climate system are important is the dry-
ing of the Sahara about 5000 years before present which
is triggered by variations in the Earth’s orbit around the
sun. Numerous modeling studies, e. g. [31], suggest that
the abruptness of the onset and termination of the early to
mid-Holocene humid period across much of Africa north
of the equator depends on the presence of non-linear feed-
backs associated with both ocean circulation and changes
in surface hydrology and vegetation, e. g. [18]. Without in-
cluding these feedbacks alongside gradual insolation forc-
ing, it is impossible for existing models to come even close
to simulating the rapidity or the magnitude of climatic
change associated with the extension of wetlands and plant
cover in the Sahara/Sahel region prior to the onset of des-
iccation around 5000 years before present.
Climate Archives and Modeling
Systematic measurements of climate using modern instru-
ments have produced records covering the last 150 years.
In order to reconstruct past variations in the climate sys-
tem further back in time, scientists use natural archives
of climatic and environmental changes, such as ice cores,
tree rings, ocean and lake sediments, corals, and histori-
cal evidence. Scientists call these records proxies because,
although they are not usually direct measures of tempera-
ture or other climatic variables, they are aﬀected by tem-
perature, and using modern calibrations, the changes in
the proxy preserved in the fossil record can be interpreted
in terms of past climate.
Ice core data, coral data, ring width of a tree, or infor-
mation from marine sediments are examples of a proxy for
temperature, or in some cases rainfall, because the thick-

Abrupt Climate Change Modeling
A
9
ness of the ring can be statistically related to tempera-
ture and/or rainfall in the past. The most valuable prox-
ies are those that can be scaled to climate variables, and
those where the uncertainty in the proxy can be measured.
Proxies that cannot be quantiﬁed in terms of climate or
environment are less useful in studying abrupt climate
change because the magnitude of change cannot be de-
termined. Quite often, the interpretation of proxy data is
already a model of climate change since it involves con-
straints (dating, representativeness etc.). Uncertainties in
the proxies, and uncertainties in the dating, are the main
reasons that abrupt climate change is one of the more dif-
ﬁcult topics in the ﬁeld of paleoclimatology.
Example: Glacial-Interglacial Transitions
Astronomical Theory of Ice Ages
Over the past half million years, marine, polar ice core and
terrestrial records all highlight the sudden and dramatic
nature of glacial terminations, the shifts in global climate
that occurred as the world passed from dominantly glacial
to interglacial conditions, e. g. [23,69]. These climate tran-
sitions, although probably of relatively minor relevance to
the prediction of potential future rapid climate change, do
provide the most compelling evidence available in the his-
torical record for the role of greenhouse gas, oceanic and
biospheric feedbacks as non-linear ampliﬁers in the cli-
Abrupt Climate Change Modeling, Figure 1
Oxygen isotope record from a southern hemisphere ice core [23] showing the glacial-interglacial changes. Note the asymmetry: the
state is longer in the cold (glacials) phases than in the warm phases (interglacials)
mate system. It is such evidence of the dramatic eﬀect of
non-linear feedbacks that shows relatively minor changes
in climatic forcing may lead to abrupt climate response.
A salient feature of glacial-interglacial climate change
is furthermore its asymmetry (Fig. 1). Warmings are rapid,
usually followed by slower descent into colder climate.
Given the symmetry of orbital forcings F(t), the cause of
rapid warming at glacial “terminations” must lie in a cli-
mate feedback [37,65]. Clearly, the asymmetric feedback
is due to the albedo (reﬂectance) of ice and snow chang-
ing from high values under glacial climates to low values
under warm climates. The albedo feedback helps explain
the rapidity of deglaciations and their beginnings in spring
and summer. Increased absorption of sunlight caused by
lower albedo provides the energy for rapid ice melt. The
build-up of snow and ice takes much longer than melting.
Many simpliﬁed climate models consist of only a few
coupled ordinary diﬀerential equations controlled by care-
fully selected parameters. It is generally acknowledged that
the “best” models will be those that contain a minimum of
adjustable parameters [77] and are robust with respect to
changes in those parameters. Rial [72] formulated a logis-
tic-delayed and energy balance model to understand the
saw-tooth shape in the paleoclimate record: A fast warm-
ing-slow cooling is described by
d
dt x(t) D R

1  x(t  )
K(t)

x(t  )
(22)

10 A
Abrupt Climate Change Modeling
C d
dt T(t) D Q (1  ˛(x))  (A C BT)
(23)
with x(t) for the normalized ice extent,  time delay,
K(t) D 1 C e(t)T(t) carrying capacity, 1/R response time
of the ice sheet, T(t) global mean temperature, ˛(x) plan-
etary albedo, external parameter e(t), and R bifurcation
parameter. A; B; C; Q are constants for the energy bal-
ance of the climate. The equation is calibrated so that
for x(t) D 1 the albedo ˛(x) D 0:3 and T(t) D 15ıC.
With (23), saw-toothed waveforms and frequency modu-
lation can be understood [72]. The delayed equation yields
damped oscillations of x(t) about the carrying capacity for
small . If  becomes long compared to the natural re-
sponse time of the system, the oscillations will become
strong, and will grow in amplitude, period and duration.
As in the logistic equation for growth, here the product
R is a bifurcation parameter, which when crossing the
threshold value /2 makes the solutions undergo a Hopf
bifurcation and settle to a stable limit cycle with funda-
mental period  4 [73].
The astronomical theory of ice ages – also called Mi-
lankovitch theory [62] – gained the status of a paradigm
for explaining the multi-millennial variability. A key
element of this theory is that summer insolation at high
latitudes of the northern hemisphere determines glacial-
interglacial transitions connected with the waxing and
waning of large continental ice sheets, e. g. [33,37], the
dominant signal in the climate record for the last million
Abrupt Climate Change Modeling, Figure 2
Oxygen isotope record from a Greenland ice core record [64] usingan updated time scale for this record [23]. Green: Sea-level derived
rate of deglacial meltwater discharge [24] which is strong after deglacial warming
years. Climate conditions of glacials and interglacials are
very diﬀerent. During the Last Glacial Maximum, about
20,000 years before present, surface temperature in the
north Atlantic realm was 10–20ıC lower than today [13].
A recent study of Huybers and Wunsch [36] has shown
that the most simple system for the phase of ice vol-
ume x(t) is given by
x(t C 1) D x(t) C 
(24)
with  a Gaussian white noise process, but with mean
 D 1, and  D 2.  represents the unpredictable back-
ground weather and climate variability spanning all time
scales out to the glacial/interglacial. This highly simpliﬁed
model posits 1-ky steps in ice volume x(t). The non-zero
mean biases the Earth toward glaciation. Once x(t) reaches
a threshold, a termination is triggered, and ice-volume is
linearly reset to zero over 10 ky. The following threshold
condition for a termination makes it more likely for a ter-
mination of ice volume to occur when obliquity 	(t) is
large:
x(t)  T0  a	(t) :
(25)
	(t) has a frequency of about 41 ky, and is furthermore
normalized to zero mean with unit variance. The other pa-
rameters are: amplitude a D 15; T0 D 105. Furthermore,
the initial ice volume at 700 ky before present is set to
x(t D 700) D 30. Equation (24) resembles an order-one
autoregressive process, similar to (2), plus the threshold

Abrupt Climate Change Modeling
A
11
condition (25). Models like (24), (25) are not theories of
climate change, but rather attempts at eﬃcient kinematic
descriptions of the data, and diﬀerent mechanisms can
be consistent with the limited observational records. In
the next section, the process of deglaciation is modeled
in a three-dimensional model including the spatial dimen-
sion.
Deglaciation
The question is what causes the abrupt warming at
the onset of the Boelling as seen in the Greenland ice
cores (Fig. 2). There is a clear antiphasing seen in the
deglaciation interval between 20 and 10 ky ago: During
the ﬁrst half of this period, Antarctica steadily warmed,
but little change occurred in Greenland. Then, at the
time when Greenland’s climate underwent an abrupt
warming, the warming in Antarctica stopped. Knorr and
Lohmann [42], also summarizing numerous modeling
studies for deglaciation, describe how global warming
(which may be induced by greenhouse gases and feed-
backs) can induce a rapid intensiﬁcation of the ocean cir-
culation (Fig. 3). During the Boelling/Alleroed, a sudden
increase of the northward heat transport draws more heat
from the south, and leads to a strong warming in the north.
This “heat piracy” from the South Atlantic has been for-
mulated by Crowley [15]. A logical consequence of this
heat piracy is the Antarctic Cold Reversal (ACR) during
the Northern Hemisphere warm Boelling/Alleroed. This
particular example shows that an abrupt climate change
of the ocean circulation (with large climate impacts in the
North Atlantic) is related to a smooth global warming.
To understand the dynamical behavior of the system, the
concept of hysteresis is applied, using the global warm-
ing after the last ice ages as the control parameter [42].
The system exhibits multiple steady states (Fig. 4): a weak
glacial ocean circulation and a stronger circulation (which
is comparable in strength to the modern mode of opera-
tion). Deglacial warming induces a transition from a weak
glacial THC state to a stronger THC state, characterizing
the abrupt warming during the deglaciation.
Millennial Climate Variability
Within glacial periods, and especially well documented
during the last one, spanning from around 110 to 11.6 ky
ago, there are dramatic climate oscillations, including
high-latitude temperature changes approaching the same
magnitude as the glacial cycle itself, recorded in archives
from the polar ice caps, high to middle latitude marine
sediments, lake sediments and continental loess sections.
These oscillations are usually referred to as the Dans-
Abrupt Climate Change Modeling, Figure 3
Forcing and model response of the ocean overturning rate.
a The background climate conditions are linearly interpolated
between glacial (LGM), and modern (PD), conditions. Gradual
warming is stopped after 7000 model years, which is related
to  47% of the total warming. b Circulation strength (export
at 30ıS) versus time. The green curve B1 represents the exper-
iment without any deglacial freshwater release to the North
Atlantic. Experiments B2 (yellow curve), B3 (red curve), and B4
(black curve), exhibit different successions of deglacial meltwa-
ter pulse scenarios to the North Atlantic [42]
gaard–Oeschger Cycle and occur mostly on 1 to 2 ky time
scales, e. g. [6], although regional records of these tran-
sitions can show much more rapid change. The termi-
nation of the Younger Dryas cold event, for example, is
manifested in ice core records from central Greenland as
a near doubling of snow accumulation rate and a temper-
ature shift of around 10ıC occurring within a decade with
world-wide teleconnections. One hypothesis for explain-
ing these climatic transitions is that the ocean thermoha-
line circulation ﬂips between diﬀerent modes, with warm
intervals reﬂecting periods of strong deep water forma-
tion in the northern North Atlantic and vice versa [29]. As
an alternative approach, one can estimate the underlying
dynamics (13), (14) directly from data [43]. The method
is based on the unscented Kalman ﬁlter, a non-linear ex-
tension of the conventional Kalman ﬁlter. This technique
allows one to consistently estimate parameters in deter-
ministic and stochastic non-linear models. The optimiza-

12 A
Abrupt Climate Change Modeling
Abrupt Climate Change Modeling, Figure 4
Hysteresis loop of the ocean overturning strength (black curve) with respect to slowly varying climate background conditions. The
transition values are given in % of a full glacial-interglacial transition [42]
Abrupt Climate Change Modeling, Figure 5
Potential derived from the data (solid) together with probability densities of the model (dashed) and the data (dotted)

Abrupt Climate Change Modeling
A
13
tion yields for the coeﬃcients a4 D 0:13 ˙ 0:01; a3 D
0:27 ˙ 0:02; a2 D 0:36 ˙ 0:08, and a1 D 1:09 ˙ 0:23.
The dynamical noise level of the system  is estimated to
be 2.4. The potential is highly asymmetric and degener-
ate (that is, close to a bifurcation): there is one stable cold
stadial state and one indiﬀerently stable warm interstadial
state (Fig. 5). This seems to be related to the fact that the
warm intervals are relatively short-lasting.
Coming back to the ice cores and a potential link-
age of the hemispheres, Stocker and Johnson [81] pro-
posed a conceptual model linking the isotopic records
from Antarctica and Greenland. The basis is an energy bal-
ance with temperatures in the North and South Atlantic
Ocean, as well as a “southern heat reservoir”. It is assumed
that the change in heat storage of a “southern heat reser-
voir” TS is given by the temperature diﬀerence between the
reservoir TS and the Southern Ocean temperature T, with
a characteristic time scale :
d
dt TS(t) D 1
 [T  TS] :
(26)
TN denotes the time-dependent temperature anomaly of
the North Atlantic. The Southern Ocean temperature T is
assumed to be TN according to the bipolar seesaw (North
Atlantic cold $ South Atlantic warm). Using Laplace
transform, one can solve for TS
TS D  1

Z t
0
TN(tt0) exp(t0/)dt0CTS(0) exp(t/):
(27)
The reservoir temperature is therefore a convolution of the
northern temperature using the time scale  ranging from
100 to 4000 years. Equation (27) demonstrates that TS and
TN will have entirely diﬀerent time characteristics. Abrupt
changes in the north appear damped and integrated in
time in the southern reservoir. A sudden reduction in the
thermohaline circulation causes a cooling in the North At-
lantic and a warming in the South, a situation similar to the
Younger Dryas period [80], see also Fig. 2.
Example: Cenozoic Climate Cooling
Antarctic Glaciation
During the Cenozoic (65 million years ago (Ma) to
present), there was the widespread glaciation of the
Antarctic continent at about 34 Ma, e. g. [93]. Antarctic
glaciation is the ﬁrst part of a climate change from rel-
atively warm and certainly ice-free conditions to mas-
sive ice sheets in both, the southern and northern hemi-
spheres [44]. Opening of circum-Antarctic seaways is one
of the factors that have been ascribed as a cause for Antarc-
tic climate change so far [40,93]. Besides gateway open-
ings, the atmospheric carbon dioxide concentration is
another important factor aﬀecting the evolution of the
Cenozoic climate [17,93]. As a third component in the
long-term evolution of Antarctic glaciation, land topogra-
phy is able to insert certain thresholds for abrupt ice sheet
build-up. Whereas tectonics, land topography, and long-
term Cenozoic CO2-decrease act as preconditioning for
Antarctic land ice formation, the cyclicities of the Earth’s
orbital conﬁguration are superimposed on shorter time
scales and may have served as the ultimate trigger and
pacemaker for ice-sheet growth at the Eocene-Oligocene
boundary around 34 Ma [14].
DeConto and Pollard [17] varied Southern Ocean heat
transport to mimic gateway opening instead of an explicit
simulation of ocean dynamics. They found a predomi-
nating role of pCO2 in the onset of glaciation instead of
a dominating tectonic role for “thermal isolation”.
Mid-Pleistocene Revolution
Glaciation in the Northern Hemisphere lagged behind,
with the earliest recorded glaciation anywhere in the
Northern Hemisphere occurring between 10 and 6 Ma and
continuing through to the major increases in global ice
volume around 2–3 Ma [60]. A recent compilation of 57
globally distributed records [45] is shown in Fig. 6. Let
us focus now on the mid-Pleistocene transition or revolu-
tion (MPR), describing the transition from 41 ky to 100 ky
glacial-interglacial cycles.
Milankovitch [62] initially suggested that the critical
factor was total summer insolation at about 65ıN, be-
cause for an ice sheet to grow some additional ice must
survive each successive summer. In contrast, the South-
ern Hemisphere is limited in its response because the ex-
pansion of ice sheets is curtailed by the Southern Ocean
around Antarctica. The conventional view of glaciation is
thus that low summer insolation in the temperate North
Hemisphere allows ice to survive summer and thus start to
build up on the northern continents. If so, how then do we
account for the MPR? Despite the pronounced change in
Earth system response evidenced in paleoclimatic records,
the frequency and amplitude characteristics of the or-
bital parameters which force long-term global climate
change, e. g., eccentricity ( 100 ky), obliquity ( 41 ky)
and precession ( 21 and  19 ky), do not vary during
the MPR [8]. This suggests that the cause of change in re-
sponse at the MPR is internal rather than external to the
global climate system.

14 A
Abrupt Climate Change Modeling
Abrupt Climate Change Modeling, Figure 6
A compilation of 57 globally distributed records by Lisiecki and Raymo [45]: The ı18O record reflects mainly the climate variables
temperature and ice volume
Abrupt Climate Change Modeling, Figure 7
Lisiecki and Raymo [45]: The corresponding wavelet sample spectrum calculated using Morlet wavelet with !0 D 6. Thin and thick
lines surround pointwise and areawise significant patches, respectively
The result of a wavelet spectral analysis (Fig. 7) sug-
gests several abrupt climate changes in the frequency do-
main (shown as schematic arrows in the ﬁgure). These
abrupt climate shifts represent major reorganizations in
the climate system. Some of them are possibly linked to
the development of Northern Hemisphere ice volume.
The MPR marked a prolongation to and intensiﬁcation
of the  100 ky glacial-interglacial climate. Not only does

Abrupt Climate Change Modeling
A
15
the periodicity of glacial-interglacial cycles increase going
through the MPR, but there is also an increase in the am-
plitude of global ice volume variations.
It is likely that the MPR is a transition to a more intense
and prolonged glacial state, and associated subsequent
rapid deglaciation becomes possible. The ﬁrst occurrence
of continental-scale ice sheets, especially on Greenland, is
recorded as ice-rafted detritus released from drifting ice-
bergs into sediments of the mid- and high-latitude ocean.
After a transient precursor event at 3.2 Ma, signals of large-
scale glaciations suddenly started in the subpolar North
Atlantic in two steps, at 2.9 and 2.7 Ma, e. g. [5].
The ice volume increase may in part be attributed to
the prolonging of glacial periods and thus of ice accumu-
lation. The amplitude of ice volume variation is also ac-
centuated by the extreme warmth of many interglacial pe-
riods. Thus, a colder climate with larger ice sheets should
have the possibility of a greater sudden warming [45]. The
MPR therefore marks a dramatic sharpening of the con-
trast between warm and cold periods. Note however, that
the amount of energy at the 40 ka period is hardly changed
in the time after 1 Ma, and notably, one sees the addition
of energy at longer periods, without any signiﬁcant re-
duction in obliquity-band energy. After about 1 Ma, large
glacial-interglacial changes begin to occur on an approx-
imately 100 ka time scale (but not periodically) superim-
posed upon the variability which continues largely un-
changed [91]. Why did 100 ka glacial-interglacials also be-
come possible in addition to the ice volume variability?
Lowering of global CO2 below some critical threshold, or
changes in continental conﬁguration, or atmospheric cir-
culation patterns, or all together, are among the conceiv-
able possibilities, e. g. [70].
Examples: Transient Growth
The former examples show the power of the combination
of models, data analysis, and interpretation for abrupt cli-
mate change. In the next two examples, it is shown how
important the transient growth mechanism is for abrupt
climate change.
Conceptual Model of the Ocean Circulation
In this section, a category of the non-linear models follow-
ing the simple thermohaline model of Stommel [82] is an-
alyzed. The common assumption of these box models is
that the oceanic overturning rate ˚ can be expressed by
the meridional density diﬀerence:
˚ D c(˛
T  ˇ
S) ;
(28)
where ˛ and ˇ are the thermal and haline expansion co-
eﬃcients, c is a tunable parameter, and 
 denotes the
meridional diﬀerence operator applied to the variables
temperature T and salinity S, respectively. Stommel [82]
considered a two-box ocean model where the boxes are
connected by an overﬂow at the top and a capillary tube
at the bottom, such that the capillary ﬂow is directed from
the high-density vessel to the low-density vessel follow-
ing (28).
The equations for temperature T and salinity S are the
heat and salt budgets using an upstream scheme for the
advective transport and ﬂuxes with the atmosphere:
d
dt T D ˚
V 
T 
Foa
0cph
(29)
d
dt S D ˚
V 
S  S0
h (P  E) ;
(30)
where V is the volume of the box with depth h, and (P  E)
denotes the freshwater ﬂux (precipitation minus evapora-
tion plus runoﬀ). Foa is the heat ﬂux at the ocean-atmo-
sphere interface, S0 is a reference salinity, and 0cp denotes
the heat capacity of the ocean.
Denoting furthermore x 2 R2 for the anomalies of
(
T; 
S), Lohmann and Schneider [48] have shown the
evolution equation is of the following structure:
d
dt x D Ax C hbjxi x :
(31)
The brackets hji denote the Euclidean scalar product. This
evolution Equation (31) can be transferred to a
x(t) D
1
(t) exp(At)x0 ;
(32)
with a scaling function (t; x0). The models of Stom-
mel [82], and many others are of this type, and their dy-
namics are therefore exactly known.
It is worth knowing that (29), (30) is equivalent to the
multi-dimensional Malthus–Verhulst model (also known
as the logistic equation), which was originally proposed
to describe the evolution of a biological population. Let
x denote the number (or density) of individuals of a cer-
tain population. This number will change due to growth,
death, and competition. In the simplest version, birth and
death rates are assumed proportional to n, but account-
ing for limited resources and competition it is modiﬁed by
(1  x):
d
dt x(t) D a(1  x)x :
(33)
In climate, the logistic
equation is important
for
Lorenz’s [52] error growth model: where x(t) is the alge-
braic forecast error at time t and a is the linear growth rate.

16 A
Abrupt Climate Change Modeling
Abrupt Climate Change Modeling, Figure 8
The basin of attraction (white area) and the dynamics in the ther-
mohaline phase space. With initial conditions outside the gray
area, the trajectories converge asymptotically to the origin cor-
responding to the thermally driven solution of the THC. Due to
the fast thermal response during the first decade of relaxation,
the distance of the trajectories from zero can increase temporar-
ily
It is useful to analyze the dynamics in the phase space
spanned by temperature and salinity anomalies and in-
vestigate the model sensitivity under anomalous high lati-
tude forcing, induced as an initial perturbation. The lines
in Fig. 8 are phase space trajectories after perturbations of
diﬀerent magnitude have been injected into the North At-
lantic. We notice that for most trajectories, the distances
from zero (0; 0) increase temporarily, where the maximal
distance from zero is after a decade. After about 10 years
the trajectories in Fig. 8 point into a “mixed tempera-
ture/salinity direction”, denoted further as e1.
Figure 8 implies that the adjustment of the THC in-
volves two phases: A fast thermal response and a slower re-
sponse on the e1-direction. The vector e1 is identical with
the most unstable mode in the system. Because the scal-
ing function (t) acts upon both temperature and salin-
ity (32), the evolution of the non-linear model can be well
characterized by the eigenvectors of the matrix A, which is
discussed in the following.
In our system, the operator A of the box model is found
to be non-normal, and the eigenvectors are not orthogo-
nal. One eigenvalue (e2) is closely related to temperature
anomalies, whereas the other (e1) is a “mixed tempera-
ture/salinity eigenvector” (Fig. 9). The eigenvectors of the
Abrupt Climate Change Modeling, Figure 9
Eigenvectors e1; e2, and adjoint eigenvectors e
1 ; e
2 of the tan-
gent linear operator A. The dotted lines show the line of con-
stant density and the perpendicular
adjoint matrix A are denoted by e
1 and e
2 , respectively.
For the non-normal matrix A, the eigenvectors of A and
A do not coincide, but fulﬁll the “biorthogonality condi-
tion”:
e
1 ? e2
and
e
2 ? e1 :
(34)
Both eigenvalues 1;2 are real and negative. Because of
2 < 1, the ﬁrst term dominates for long time scales and
the second for short time scales. Using the biorthogonality
condition, we get furthermore the coeﬃcients
ci D
˝e
i jx0
˛
˝e
i jei
˛
for i D 1; 2 :
(35)
A perturbation is called “optimal”, if the initial er-
ror vector has minimal projection onto the subspace with
fastest decaying perturbations, or equivalently if the coef-
ﬁcient c1 is maximal. This is according to (35) equivalent
to x0 pointing into the direction of e
1 . This unit vector
e
1 is called the “biorthogonal” [66] to the most unstable
eigenvector e1 which we want to excite. In order to make
a geometrical picture for the mathematical considerations,
assume that the tail of the vector x0 is placed on the e1-
line and its tip on the e2-line. This vector is stretched max-
imally because the tail decays to zero quickly, whereas the
tip is hardly unchanged due to the larger eigenvalue 1.
The most unstable mode e1 and its biorthogonal e
1 dif-
fer greatly from each other, and the perturbation that op-
timally excites the mode bears little resemblance to the
mode itself.

Abrupt Climate Change Modeling
A
17
It is remarkable that the optimal initial perturbation
vector e
1 does not coincide with a perturbation in sea
surface density at high latitudes, which would reside on
the dotted line perpendicular to  D const in Fig. 9. Even
when using a space spanned by (˛T; ˇS) instead of (T; S),
to take into account the diﬀerent values for the thermal
and haline expansion coeﬃcients, vector e
1 is much more
dominated by the scaled salinity anomalies than the tem-
perature anomalies of the high latitudinal box.
Numerical simulations by Manabe and Stouﬀer [57]
showed, for the North Atlantic, that between two and four
times the preindustrial CO2 concentration, a threshold
value is passed and the thermohaline circulation ceases
completely. One other example of early Holocene rapid
climate change is the “8200-yr BP” cooling event recorded
in the North Atlantic region possibly induced by fresh-
water. One possible explanation for this dramatic regional
cooling is a shutdown in the formation of deep water in the
northern North Atlantic due to freshwater input caused by
catastrophic drainage of Laurentide lakes [4,46]. The theo-
retic considerations and these numerical experiments sug-
gest that formation of deep water in the North Atlantic is
highly sensitive to the freshwater forcing.
Resonance in an Atmospheric Circulation Model
An atmospheric general circulation model PUMA [26]
is applied to the problem. The model is based on the
multi-level spectral model described by Hoskins and Sim-
mons [35]. For our experiments we chose ﬁve vertical lev-
els and a T21 horizontal resolution. PUMA belongs to
the class of models of intermediate complexity [12]; it has
been used to understand principle feedbacks [56], and dy-
namics on long time scales [76]. For simplicity, the equa-
tions are scaled here such that they are dimensionless. The
model is linearized about a zonally symmetric mean state
providing for a realistic storm track at mid-latitudes [27].
In a simpliﬁed version of the model and calculating the lin-
ear model A with n D 214, one can derive the pseudospec-
trum. Figure 10 indicates resonances besides the poles (the
eigenvalues) indicated by crosses. The Im(z)-axis shows
the frequencies, the Re(z)-axis the damping/ampliﬁcation
of the modes. Important modes for the climate system
are those with 0:5 < Im(z) < 0:5 representing planetary
Rossby waves. The basic feature is that transient growth of
initially small perturbations can occur even if all the eigen-
modes decay exponentially. Mathematically, an arbitrary
matrix A can be decomposed as a sum
A D D C N
(36)
where A is diagonalizable, and N is nilpotent (there ex-
ists an integer q 2 N with Nq D 0), and D commutes
with N (i. e. DN D NA). This fact follows from the Jor-
dan–Chevalley decomposition theorem. This means we
can compute the exponential of (A t) by reducing to the
cases:
exp(At) D exp((D C N)t) D exp(Dt) exp(Nt)
(37)
where the exponential of Nt can be computed directly
from the series expansion, as the series terminates after
a ﬁnite number of terms. Basically, the number q 2 N is
related to the transient growth of the system (q D 1 means
no transient growth).
The resonant structures are due to the mode inter-
action: It is not possible to change one variable without
the others, because they are not orthogonal. Interestingly,
one can also compute the A model, showing the optimal
perturbation of a mode ei through its biorthogonal vec-
tor (35).
The analysis indicates that non-normality of the sys-
tem is a fundamental feature of the atmospheric dynamics.
This has consequences for the error growth dynamics, and
instability of the system, e. g. [48,66]. Similar features are
obtained in shear ﬂow systems [71,87] and other hydro-
dynamic applications. This transient growth mechanism
is important for both initial value and forced problems of
the climate system.
Future Directions
Until now, details of abrupt climate change are not well
known to accurately predict it. With better information,
the society could take more conﬁdent action to reduce the
potential impact of abrupt changes on agriculture, water
resources, and the built environment, among other im-
pacts. A better understanding of sea-ice and glacier stabil-
ity, land-surface processes, and atmospheric and oceanic
circulation patterns is needed. Moreover, to eﬀectively use
any additional knowledge of these and other physical pro-
cesses behind abrupt climate change, more sophisticated
ways of assessing their interactions must be developed, in-
cluding:
Better models. At present, the models used to assess
climate and its impacts cannot simulate the size, speed,
and extent of past abrupt changes, let alone predict fu-
ture abrupt changes. Eﬀorts are needed to improve how
the mechanisms driving abrupt climate change are repre-
sented in these models and to more rigorously test models
against the climate record.
More theory. There are concepts to ﬁnd the under-
lying dynamical system, to derive a theory from a high-
order to low-order description similar to what is done in

18 A
Abrupt Climate Change Modeling
Abrupt Climate Change Modeling, Figure 10
Contours of log10(1/). The figure displays resonant structures of the linearized atmospheric circulation model. The modes extend
to the right half plane and are connected through resonant structures, indicating the transient growth mechanism inherent in atmo-
spheric dynamics
statistical physics (Mori–Zwanzig approach [63,94], Mas-
ter equation), or in stochastic diﬀerential equations. A sys-
tematic reduction of the complex system into fewer de-
grees of freedom shall bring a deeper level of understand-
ing about the underlying physics. A systematic approach
was suggested by Saltzman [77]. Spectral and pseudo-spec-
tral concepts have not been used too much in climate
theory. There is a variety of phenomenological stochas-
tic models in which non-linearity and ﬂuctuations coexist,
and in which this coexistence leads to interesting phenom-
ena that would not arise without the complex interplay.
Paleoclimatic data. More climate information from
the distant past would go a long way toward strengthening
our understanding of abrupt climate changes and models
of past climate. In particular, an enhanced eﬀort is needed
to expand the geographic coverage, temporal resolution,
and variety of paleoclimatic data. Although the present cli-
mate has no direct analogon to the past [54], the dynam-
ical interpretation of data will improve the understanding
of thresholds and non-linearities in the Earth system.
Appropriate statistical tools. Because most statisti-
cal calculations at present are based on the assumption
that climate is not changing but is stationary, they have
limited value for non-stationary (changing) climates and
for climate-related variables that are often highly skewed
by rapid changes over time such as for abrupt-change
regimes. Available statistical tools themselves need to be
adapted or replaced with new approaches altogether to
better reﬂect the properties of abrupt climate change.
Synthesis. Physical, ecological, and human systems
are complex, non-linear, dynamic and imperfectly under-
stood. Present climate change is producing conditions out-
side the range of recent historical experience and observa-
tion, and it is unclear how the systems will interact with
and react to further climate changes. Hence, it is crucial to
be able to better understand and recognize abrupt climate
changes quickly. This capability will involve improved
monitoring of parameters that describe climatic, ecolog-
ical, and economic systems. Some of the desired data are
not uniquely associated with abrupt climate change and,
indeed, have broad applications. Other data take on par-
ticular importance because they concern properties or
regions implicated in postulated mechanisms of abrupt
climate change. Research to increase our understanding
of abrupt climate change should be designed speciﬁcally
within the context of the various mechanisms thought to
be involved. Focus is required to provide data for process
studies from key regions where triggers of abrupt climate
change are likely to occur, and to obtain reliable time series
of climate indicators that play crucial roles in the postu-

Abrupt Climate Change Modeling
A
19
lated mechanisms. Observations could enable early warn-
ing of the onset of abrupt climate change. New observa-
tional techniques and data-model comparisons will also be
required.
Bibliography
Primary Literature
1. Alley RB, Anandakrishnan S, Jung P (2001) Stochastic reso-
nance in the North Atlantic. Paleoceanogr 16:190–198
2. Alley RB, Marotzke J, Nordhaus W, Overpeck J, Peteet D, Pielke
R Jr, Pierrehumbert R, Rhines P, Stocker T, Talley L, Wallace
JM (2002) Abrupt Climate Change: Inevitable Surprises. US Na-
tional Academy of Sciences, National Research Council Com-
mittee on Abrupt Climate Change, National Academy Press,
Washington
3. Alverson K, Oldfield F (2000) Abrupt Climate Change. In: Joint
Newsletter of the Past Global Changes Project (PAGES) and the
Climate Variability and Predictability Project (CLIVAR), vol 8, no
1. Bern, pp 7–10
4. Barber DC, Dyke A, Hillaire-Marcel C, Jennings AE, Andrews
JT, Kerwin MW, Bilodeau G, McNeely R, Southon J, Morehead
MD, Gagnonk JM (1999) Forcing of the cold event of 8,200
years ago by catastrophic drainage of Laurentide lakes. Nature
400:344–348
5. Bartoli G, Sarnthein M, Weinelt M, Erlenkeuser H, Garbe-
Schönberg D, Lea DW (2005) Final closure of Panama and the
onset of northern hemisphere glaciation. Earth Planet Sci Lett
237:33–44
6. Bender M, Malaize B, Orchardo J, Sowers T, Jouzel J (1999)
Mechanisms of Global Climate Change. Clark P et al (eds) AGU
112:149–164
7. Benzi R, Parisi G, Sutera A, Vulpiani A (1982) Stochastic reso-
nance in climatic change. Tellus 34:10
8. Berger A, Loutre MF (1991) Insolation values for the climate of
the last 10 million years. Quat Sci Rev 10:297–317
9. Berger WH, Labeyrie LD (1987) Abrupt Climatic Change, Evi-
dence and Implications. NATO ASI Series, Series C, Mathemati-
cal and Physical Sciences, vol 216. D Reidel, Dordrecht, pp 425
10. Broecker WS et al (1985) Does the Ocean-atmosphere Sys-
tem Have More than One Stable Mode of Operation? Nature
315:21–26
11. Bryan F (1986) High Latitude Salinity Effects and Inter-hemi-
spheric Thermohaline Circulations. Nature 323:301–304
12. Claussen M, Mysak LA, Weaver AJ, Crucifix M, Fichefet T, Loutre
M-F, Weber SL, Alcamo J, Alexeev VA, Berger A, Calov R,
Ganopolski A, Goosse H, Lohmann G, Lunkeit F, Mokhov II,
Petoukhov V, Stone P, Wang Z (2002) Earth System Models of
Intermediate Complexity: Closing the Gap in the Spectrum of
Climate System Models. Clim Dyn 18:579–586
13. CLIMAP project members (1976) The surface of the ice age
Earth. Science 191:1131–1137
14. Coxall HK, Wilson PA, Pälike H, Lear CH, Backman J (2005) Rapid
stepwise onset of Antarctic glaciation and deeper calcite com-
pensation in the Pacific Ocean. Nature 433:53–57. doi:10.1038/
nature03135
15. Crowley TJ (1992) North Atlantic deep water cools the south-
ern hemisphere. Paleoceanogr 7:489–497
16. Daubechies I (1992) Ten Lectures on Wavelets. Society for In-
dustrial and Applied Mathematics (SIAM). CBMS-NSF Regional
Conference Series in Applied Mathematics, vol 61, Philadelphia
17. DeConto RM, Pollard D (2003) Rapid Cenozoic glaciation of
Antarctica induced by declining atmospheric CO2. Nature
421:245–249. doi:10.1038/nature01290
18. DeMenocal et al (2000) Abrupt onset and termination of the
African Humid Period: Rapid climate response to gradual inso-
lation forcing. Quat Sci Rev 19:347–361
19. Diaz HF, Quayle RG (1980) The climate of the United States
since 1895: spatial and temporal changes. Mon Wea Rev
108:149–226
20. Dijkstra HA, Te Raa L, Weijer W (2004) A systematic approach to
determine thresholds of the ocean’s thermohaline circulation.
Tellus 56A(4):362
21. Dima M, Lohmann G (2002) Fundamental and derived modes
of climate variability. Application to biennial and interannual
timescale. Tellus 56A:229–249
22. Dima M, Lohmann G (2007) A hemispheric mechanism for the
Atlantic Multidecadal Oscillation. J Clim 20:2706–2719
23. EPICA Community Members (2006) One-to-one coupling of
glacial climate variability in Greenland and Antarctica. Nature
444:195–198. doi:10.1038/nature05301
24. Fairbanks RG (1989) A 17,000-year glacio-eustatic sea level
record: influence of glacial melting rates on the Younger Dryas
event and deep-ocean circulation. Nature 342:637–642
25. Flohn H (1986) Singular events and catastrophes now and in
climatic history. Naturwissenschaften 73:136–149
26. Fraedrich K, Kirk E, Lunkeit F (1998) Portable University Model
of the Atmosphere. DKRZ Report 16, Hamburg
27. Frisius T, Lunkeit F, Fraedrich K, James IN (1998) Storm-track
organization and variability in a simplified atmospheric global
circulation model. Q J R Meteorol Soc 124:1019–1043
28. Fu C, Diaz HF, Dong D, Fletcher JO (1999) Changes in at-
mospheric circulation over northern hemisphere oceans as-
sociated with the rapid warming of the 1920s. Int J Climatol
19(6):581–606
29. Ganopolski A, Rahmstorf S (2001) Rapid changes of glacial
climate simulated in a coupled climate model. Nature 409:
153–158
30. Ganopolski
A,
Rahmstorf
S
(2002)
Abrupt
glacial
cli-
mate changes due to stochastic resonance. Phys Rev Let
88(3):038501
31. Ganopolski A, Kubatzki C, Claussen M, Brovkin V, Petoukhov V
(1998) The influence of vegetation-atmosphere-ocean interac-
tion on climate during the mid-Holocene. Science 280:1916
32. Hasselmann K (1976) Stochastic climate models, Part 1, Theory.
Tellus 28:289–485
33. Hays JD, Imbrie J, Shackleton NJ (1976) Variations in the Earth’s
Orbit: Pacemaker of the Ice Ages. Science 194:1121–1132
34. Henderson GM, Slowey NC (2000) Evidence from U-Th dat-
ing against Northern Hemisphere forcing of the penultimate
deglaciation. Nature 404:61–66
35. Hoskins BJ, Simmons AJ (1975) A multi-layer spectral model
and the semi-implicit method. Q J R Meteorol Soc 101:
1231–1250
36. Huybers P, Wunsch C (2005) Obliquity pacing of the late Pleis-
tocene glacial terminations. Nature 434:491–494. doi:10.1038/
nature03401
37. Imbrie J, Imbrie JZ (1980) Modeling the climatic response to
orbital variations. Science 207:943–953

20 A
Abrupt Climate Change Modeling
38. IPCC (2007) Summary for Policymakers. In: Climate change
2007: The Physical Science Basis. Contribution of Working
Group I to the Fourth Assessment Report of the Intergrovern-
mental Panel on Climate Change. Cambridge University Press,
Cambridge and New York
39. Iwashima T, Yamamoto R (1986) Time-space spectral model of
low order barotropic system with periodic forcing. J Meterol
Soc Jpn 64:183–196
40. Kennett JP, Houtz RE, Andrews PB, Edwards AE, Gostin VA, Ha-
jos M, Hampton M, Jenkins DG, Margolis SV, Ovenshine AT,
Perch-Nielsen K (1974) Development of the circum-Antarctic
current. Science 186:144–147
41. Knorr G, Lohmann G (2003) Southern Ocean Origin for the re-
sumption of Atlanticthermohaline circulationduring deglacia-
tion. Nature 424:532–536
42. Knorr G, Lohmann G (2007) Rapid transitions in the Atlantic
thermohaline circulation triggered by global warming and
meltwater during the last deglaciation. Geochem Geophys
Geosyst 8(12), Q12006:1–22. doi:10.1029/2007GC001604
43. Kwasniok F, Lohmann G (2008) Underlying Dynamics of Glacial
Millennial-Scale Climate Transitions Derived from Ice-Core
Data. Phys Rev E (accepted)
44. Lawver LA, Gahagan LM (2003) Evolution of Cenozoic
seaways in the circum-Antarctic region. Palaeogeography,
Palaeoclimatology, Palaeoecology 198:11–37. doi:10.1016/
S0031-0182(03)00392-4
45. Lisiecki LE, Raymo ME (2005) A Pliocene-Pleistocene stack of 57
globally distributed benthic O-18 records. Paleoceanography
20:PA1003. doi:10.1029/2004PA001071
46. Lohmann G (2003) Atmospheric and oceanic freshwater trans-
port during weak Atlantic overturning circulation. Tellus 55A:
438–449
47. Lohmann G, Gerdes R (1998) Sea ice effects on the Sensitivity of
the Thermohaline Circulation in simplified atmosphere-ocean-
sea ice models. J Climate 11:2789–2803
48. Lohmann G, Schneider J (1999) Dynamics and predictability of
Stommel’s box model: A phase space perspective with impli-
cations for decadal climate variability. Tellus 51A:326–336
49. Lohmann G, Schulz M (2000) Reconciling Boelling warmth
with peak deglacial meltwater discharge. Paleoceanography
15:537–540
50. Lorenz EN (1963) Deterministic nonperiodic flow. J Atmos Sci
20:130–141
51. Lorenz EN (1976) Nondeterministic theories of climatic
change. Quat Res 6:495–506
52. Lorenz EN (1982) Atmospheric predictability experiments with
a large numerical model. Tellus 34:505–513
53. Lorenz EN (1990) Can chaos and intransitivity lead to interan-
nual variability? Tellus 42A:378–389
54. Lorenz S, Lohmann G (2004) Acceleration technique for Mi-
lankovitch type forcing in a coupled atmosphere-ocean circu-
lation model: method and application for the Holocene. Cli-
mate Dyn 23(7–8):727–743. doi:10.1007/s00382-004-0469-y
55. Lund R, Reeves J (2002) Detection of undocumented change-
points: A revision of the two-phase regression model. J Climate
15:2547–2554
56. Lunkeit F, Bauer SE, Fraedrich K (1998) Storm tracks in a warmer
climate: Sensitivity studies with a simplified global circulation
model. Clim Dyn 14:813–826
57. Manabe S, Stouffer RJ (1993) Century-scale effects of increased
atmospheric CO2 on the ocean-atmosphere system. Nature
364:215–218
58. Maraun D, Kurths J (2004) Cross wavelet analysis. Significance
testing and pitfalls. Nonlin Proc Geoph 11:505–514
59. Maraun D, Kurths J (2005) Epochs of phase coherence between
El Niño/Southern Oscillation and Indian monsoon. Geophys
Res Lett 32:L15709. doi:10.1029/2005GL023225
60. Maslin MA, Li XS, Loutre MF, Berger A (1998) The contribution
of orbital forcing to the progressive intensification of Northern
Hemisphere Glaciation. Quat Sci Rev 17:411–426
61. Maslin MA, Ridgewell A (2005) Mid-Pleistocene Revolution and
the eccentricitymyth. Special Publication of the Geological So-
ciety of London 247:19–34
62. Milankovitch M (1941) Kanon der Erdbestrahlung. Royal Serb
Acad Spec Publ, Belgrad, 132, Sect. Math Nat Sci 33:484
63. Mori H (1965) A Continued-Fraction Representation of the
Time-Correlation Functions Prog Theor Phys 33:423–455. doi:
10.1143/PTP.34.399
64. North Greenland Ice Core Project members (2004) High-reso-
lution record of Northern Hemisphere climate extending into
the last interglacial period. Nature 431:147–151
65. Paillard D (1998) The timing of Pleistocene glaciations from
a simple multiple-state climate model. Nature 391:378–381
66. Palmer TN (1996) Predictability of the atmosphere and oceans:
From days to decades. In: Anderson DTA, Willebrand J (eds)
Large-scale transport processes in oceans and atmosphere.
NATO ASI Series 44. Springer, Berlin, pp 83–155
67. Parker DE, Jones PD, Folland CK, Bevan A (1994) Interdecadal
changes of surface temperature since the late nineteenth cen-
tury. J Geophys Res 99:14,373-14,399
68. Peixoto JP, Oort AH (1992) Physics of Climate. American Insti-
tute of Physics, New York, p 520
69. Petit JR, Jouzel J, Raynaud D, Barkov NI, Barnola JM, Basile I,
Bender M, Chappellaz J, Davis M, Delaygue G, Delmotte M,
Kotlyakov VM, Legrand M, Lipenkov VY, Lorius C, Pepin L, Ritz
C, Saltzman E, Stievenard M (1999) Climate and atmospheric
history of the past 420,000 years from the Vostok ice core,
Antarctica. Nature 399:429–436
70. Raymo M, Ganley K, Carter S, Oppo DW, McManus J (1998)
Millennial-scale climate instability during the early Pleistocene
epoch. Nature 392:699–701
71. Reddy SC, Schmidt P, Henningson D (1993) Pseudospectra of
the Orr-Sommerfeld operator. SIAM J Appl Math 53:15–47
72. Rial JA (1999) Pacemaking the Ice Ages by Frequency Modula-
tion of Earth’s Orbital Eccentricity. Science 285:564–568
73. Rial JA (2004) Abrupt Climate Change: Chaos and Order at Or-
bital and Millennial Scales. Glob Plan Change 41:95–109
74. Ridgwell AJ, Watson AJ, Raymo ME (1999) Is the spectral signa-
ture of the 100 Kyr glacial cycle consistent with a Milankovitch
origin? Paleoceanography 14:437–440
75. Rogers JC (1985) Atmospheric circulation changes associated
with the warming over the northern North Atlantic in the
1920s. J Climate Appl Meteorol 24:1303–1310
76. Romanova V, Lohmann G, Grosfeld K, Butzin M (2006) The rela-
tive role of oceanic heat transport and orography on glacial cli-
mate. Quat Sci Rev 25:832–845. doi:10.1016/j.quascirev.2005.
07.007
77. Saltzman (2002) Dynamical Paleoclimatology. Generalized
Theory of Global Climate Change. In: International Geophysics
Series, vol 80. Harcourt-Academic Press (Elsevier Science), San
Diego, p 354

Acceleration Mechanisms
A
21
78. Schulz M, Paul A, Timmermann A (2004) Glacial-Interglacial
Contrast in Climate Variability at Centennial-to-Millennial
Timescales: Observations and Conceptual Model. Quat Sci Rev
23:2219
79. Seidel DJ, Lanzante JR (2004) An assessment of three alter-
natives to linear trends for characterizing global atmospheric
temperature changes. J Geophy Res 109:D14108. doi:10.1029/
2003JD004414
80. Stocker TF (1998) The seesaw effect. Science 282:61–62
81. Stocker TF, Johnsen SJ (2003) A minimum thermodynamic
model for the bipolar seesaw. Paleoceanography 18(4):1087
82. Stommel H (1961) Thermohaline Convection with Two Stable
Regimes of Flow. Tellus 13:224–230
83. Tiedemann R, Sarnthein M, Shackleton NJ (1994) Astronomic
time scale for the Pliocene Atlantic ı18O and dust flux
records of Ocean Drilling Program site 659. Paleoceanography
9:19–638
84. Timmermann A, Lohmann G (2000) Noise-Induced Transitions
in a simplified model of the thermohaline circulation. J Phys
Oceanogr 30(8):1891–1900
85. Timmermann A, Oberhuber J, Bracher A, Esch M, Latif M,
Roeckner E (1999) Increased El Niño frequency in a cli-
mate model forced by future greenhouse warming. Nature
398:694–696
86. Torrence C, Compo G (1998) A practical guide to wavelet anal-
ysis. Bull Amer Meteor Soc 79:61–78
87. Trefethen LN, Trefethen AE, Reddy SC, Driscoll TA (1993) Hy-
drodynamic stability without eigenvalues. Science 261:578–
584
88. Trenberth KE (1990) Recent observed interdecadal climate
changes in the Northern Hemisphere. Bull Am Meteorol Soc
71:988–993
89. Uhlenbeck GE, Ornstein LS (1930) On the theory of Brownian
Motion. Phys Rev 36:823–841
90. Wunsch C (1999) The interpretation of short climate records,
with comments on the North Atlantic and Southern Oscilla-
tion. Bull Amer Meteor Soc 80:245–255
91. Wunsch C (2004) Quantitative estimate of the Milankovitch-
forced contribution to observed Quaternary climate change.
Quat Sci Rev 23(9–10):1001–1012
92. Yamamoto R, Iwashima T, Sanga NK (1985) Climatic jump:
a hypothesis in climate diagnosis. J Meteorol Soc Jpn
63:1157–1160
93. Zachos J, Pagani M, Sloan L, Thomas E, Billups K (2001) Trends,
Rhythms, and Aberrations in Global Climate 65 Ma to Present.
Science 292(5517):686–693
94. Zwanzig R (1980) Thermodynamic modeling of systems far
from equilibrium. In: Garrido L (ed) Lecture Notes in Physics
132, in Systems Far From Equilibrium. Springer, Berlin
Books and Reviews
Dijkstra HA (2005) Nonlinear Physical Oceanography, 2nd revised
and extended edition. Springer, New York, pp 537
Hansen J, Sato M, Kharecha P (2007) Climate change and trace
gases. Phil Trans R Soc A 365:1925–1954. doi:10.1098/rsta.
2007.2052
Lockwood JG (2001) Abrupt and sudden climate transitions and
fluctuations: a review. Int J Climat 21:1153–1179
Rial JA, Pielke RA Sr, Beniston M, Claussen M, Canadell J, Cox P, Held
H, N deNoblet-Ducudre, Prinn R, Reynolds J, Salas JD (2004)
Nonlinearities, Feedbacks and Critical Thresholds Within the
Earth’s Climate System. Clim Chang 65:11–38
Ruddiman WF (2001) Earth’s Climate. Past and Future. WH Free-
man, New York, p 465
Stocker TF (1999) Abrupt climate changes from the past to the
future-a review. Int J Earth Sci 88:365–374
Acceleration Mechanisms
DON B. MELROSE
School of Physics, University of Sydney, Sydney, Australia
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Stochastic Acceleration
Diﬀusive Shock Acceleration
DSA at Multiple Shocks
Applications of DSA
Resonant Scattering
Acceleration by Parallel Electric Fields
Other Acceleration Mechanisms
Future Directions
Appendix A: Quasilinear Equations
Bibliography
Glossary
Acceleration mechanism Any process, or sequence of
processes, that results in a small fraction of the par-
ticles in a plasma each gaining an energy greatly in ex-
cess of the thermal energy.
Heliosphere The region around the Sun populated by
plasma escaping from the Sun in the solar wind, and
extending to well beyond the orbits of the planets.
Magnetosphere The region around a planet or star pop-
ulated by plasma on magnetic ﬁeld lines tied to the
planet or star.
Resonance The gyroresonance condition (6) corresponds
to the wave frequency being an integral, s, multiple of
the particle gyrofrequency in the rest frame of the par-
ticle gyrocenter; s D 0 is also called the Cerenkov con-
dition.
Shock A fast magnetoacoustic shock is a discontinuity in
plasma ﬂow that propagates into the upstream plasma
faster than the Alfvén speed, compressing both the
plasma and the magnetic ﬁeld.

22 A
Acceleration Mechanisms
Solar corona The hot (> 106 K) outer atmosphere of the
Sun.
Solar ﬂare An explosion in the solar corona that leads to
copious acceleration of fast particles.
Definition of the Subject
A typical astrophysical or space plasma consists of ther-
mal plasma components, a magnetic ﬁeld, various non-
thermal distributions of fast particles and turbulence in-
volving various kinds of waves. An acceleration mecha-
nism is the process, or sequence of processes, by which
the fast particles gain their energy. There is no single, uni-
versal acceleration mechanism, and it is now recognized
that the data require a rich variety of diﬀerent accelera-
tion mechanisms operating under diﬀerent conditions in
diﬀerent space and astrophysical plasmas. Three generic
mechanisms are emphasized in this review: stochastic ac-
celeration, diﬀusive shock acceleration and acceleration by
parallel electric ﬁelds.
Introduction
The major observational discoveries in this ﬁeld were al-
most all unexpected and initially quite baﬄing. The ﬁrst
notable discovery was that of cosmic rays (CRs), by Vic-
tor Hess in 1912. Hess measured the rate of ion produc-
tion inside a hermetically sealed container in a balloon
ﬂight to an altitude of 5300 m, and concluded that “The re-
sults of my observation are best explained by the assump-
tion that a radiation of very great penetrating power enters
our atmosphere from above”. By the late 1920s CRs were
identiﬁed as charged particles, and not photons, and due
to their isotropy it was inferred that they come from the
Galaxy. Perhaps the earliest idea on the origin of Galac-
tic CRs (GCRs) has stood the test of time: in 1934 Baade
and Zwicky [8] proposed that GCRs are accelerated in
supernova explosions. Supernovae are the only adequate
source of the energy needed to replace GCRs, whose life-
time in the Galaxy is about 107 yr [34]. The observed mo-
mentum distribution of GCRs is a power law, f (p) / pb,
with more recent observations implying b D 4:7 below
a knee in the distribution at p  1015 eV/c with the spec-
trum steepening at higher momenta; Axford [6] referred
to these as GCR1 and GCR2, respectively.
The next major discoveries followed the development
of radio astronomy in the 1940s. This led to the identiﬁca-
tion of a wide class of sources radiating by a nonthermal
mechanism, identiﬁed in the late 1940s as synchrotron ra-
diation. The presence of highly relativistic electrons in syn-
chrotron sources raises the question of how they are accel-
erated. Subsequent development of X-ray and gamma-ray
astronomy placed severe constraints on the acceleration of
the relativistic electrons in synchrotron sources, due to the
short life times of electrons radiating at high frequencies.
Solar radio astronomy led to the identiﬁcation of several
diﬀerent types of radio bursts associated with solar ﬂares,
including type III bursts due to  10 keV electrons escap-
ing from the Sun, type II bursts associated with shocks that
result from the ﬂare and type I and type IV emission that
involve radio continua. All these phenomena require spe-
ciﬁc acceleration mechanisms to explain the presence of
the radio-emitting electrons.
Space physics began in the late 1950s with the dis-
covery of the Earth’s Van Allen radiation belts, raising
the question of how these energetic electrons and ions
trapped in the Earth’s magnetosphere are accelerated. Fur-
ther space exploration led to the identiﬁcation of ener-
getic particle distributions in essentially all space plasmas:
other planetary magnetospheres, the solar wind, and more
speciﬁcally associated with bow shocks and interplanetary
shocks. Observations from spacecraft provide in situ data
on particles and ﬁelds, in the Earth’s magnetosphere, in
other planetary magnetospheres, and throughout the he-
liosphere. In the case of the magnetosphere, Earth-based
data on precipitation of particles, notably in the auroral
zones, and on radio waves originating from the magne-
tosphere complement the spacecraft data on particles and
waves in situ.
Current-day ideas on acceleration mechanisms are
dominated by three generic mechanisms, plus a rich va-
riety of speciﬁc mechanisms of various kinds. Two of the
generic mechanisms had forerunners in two papers by
Fermi [30,31]. In the ﬁrst of these [30], Fermi suggested
that the acceleration of GCRs is due to reﬂections oﬀmov-
ing interstellar clouds. CRs gain energy in head-on colli-
sions with clouds, and lose energy in overtaking collisions;
a net average gain results from head-on collisions being
more probable than overtaking collisions. This model is
recognized as the ﬁrst example of the generic mechanism
of stochastic acceleration. In the second paper [31], Fermi
proposed acceleration between ‘closing jaws’ in which all
reﬂections are head-on and hence energy-increasing. This
idea is an essential ingredient in the astrophysically most
important of the generic mechanisms: diﬀusive shock ac-
celeration (DSA). The third generic mechanism is acceler-
ation by a parallel electric ﬁeld, Ek; in particular, there is
compelling evidence for auroral electrons resulting from
such acceleration. Both stochastic acceleration and DSA
require that the particles be scattered eﬃciently in order
to be accelerated, and an important step in the develop-
ment of the theory is that very eﬃcient pitch-angle scat-
tering of fast particles can result from gyroresonant inter-

Acceleration Mechanisms
A
23
actions with low-frequency waves. The following remarks
relate to the three generic mechanisms and to resonance
acceleration.
Stochastic Acceleration
The original version of Fermi mechanism [30], involving
reﬂection oﬀmagnetized interstellar clouds, is too slow to
account for GCRs, basically because such collisions are so
infrequent. The acceleration mechanism is more eﬃcient
for more frequent, smaller energy changes than for less
frequent, larger energy changes. This led to the recogni-
tion that a more eﬃcient Fermi-type acceleration can re-
sult from MHD turbulence [23,50,82,83,95]. One form of
the idea is that the compression and rarefaction of B, as-
sociated with the MHD turbulence, conserves the mag-
netic moment, p2
?/B, and when combined with a scatter-
ing process that maintains the isotropy of the particles,
this implies a ‘betatron’ acceleration, or ‘magnetic pump-
ing’ [15,88]. A closer analogy with Fermi acceleration is
due to the small fraction of the particles that reﬂect oﬀthe
compressions in B, which propagate at the Alfvén speed,
vA. Besides the betatron-type acceleration and the reﬂec-
tion oﬀmoving compressions, there is also a transit ac-
celeration [90] due to particles diﬀusing through the com-
pressions and rarefactions. All these energy changes con-
tribute to stochastic acceleration by MHD turbulence [57].
The mathematical description of Fermi-type accelera-
tion is in terms of isotropic diﬀusion in momentum space.
In the early literature stochastic acceleration was treated
using a Fokker–Planck approach that includes a term
that describes a systematic acceleration and a term that
describes a diﬀusion in energy. In was shown by Tver-
skoi [98] that for Fermi’s original mechanism this equa-
tion reduces to an isotropic diﬀusion in momentum space.
The same equation is derived for acceleration by isotropic
turbulence [40,57,93]. However, MHD turbulence is never
isotropic: it consists of a spectrum of MHD waves, in both
the (non-compressive) Alfvén mode and the (compres-
sive) fast magnetoacoustic mode. It is now assumed that
the isotropy is due to eﬃcient resonant scattering, with no
implication that the turbulence itself is isotropic.
Diﬀusive Shock Acceleration
Fermi [31] suggested that the acceleration can be eﬃcient
if all reﬂections are head-on. A simple example is when
two magnetized clouds approach each other, and a parti-
cle bounces back and forth reﬂecting oﬀthe approaching
clouds. A related example is for a shock propagating into
a closed magnetic loop, where a particle trapped in the
loop bounces back and forth being reﬂected head-on by
Acceleration Mechanisms, Figure 1
Diffusive shock acceleration is illustrated: the shaded vertical re-
gion is the shock, the circular blobs denote idealized scattering
centers, and the solid line with arrowsdenotes the path of an ide-
alized fast particle
the approaching shock [69,106]. However, such examples
are constrained by the fact that acceleration ceases when
the approaching structures meet. In 1977–8 several diﬀer-
ent authors [7,10,17,56] recognized that a single shock can
accelerate particles eﬃciently, provided that the particles
are scattered on either side of the shock, as illustrated in
Fig. 1. This mechanism is called diﬀusive shock accelera-
tion (DSA).
To understand why DSA occurs, ﬁrst note that the
ﬂuid velocity changes discontinuously across the shock.
Let the two sides of the shock be labeled 1 and 2. Con-
sider a particle on side 1 about to cross the shock and en-
ter side 2. The scattering centers on side 2 are convected
along with the plasma ﬂow (velocities u1; u2), and the par-
ticle sees them approaching it head on at ju1  u2j. Once
the particle crosses the shock and is scattered it gains en-
ergy due to the oncoming motion of the scatterers. After
being scattered a number of times on side 2, the particle
can diﬀuse back to the shock and cross back onto side 1.
On doing so, it sees the scattering centers on side 1 ap-
proaching it head on at ju1  u2j. Again it gains energy on
being scattered. DSA requires eﬃcient scattering, and this
can be achieved only by resonant scattering.
Resonant Scattering
Both stochastic acceleration and DSA require that the fast
particles be scattered such that their distribution remains
almost isotropic. The recognition that fast particles are
scattered very eﬃciently developed in two other context
before its signiﬁcance to acceleration mechanism become
fully appreciated. These developments essentially deﬁned
a new ﬁeld of “plasma astrophysics” that emerged in the
mid 1960s.

24 A
Acceleration Mechanisms
Eﬃcient scattering is neededto explain precipitation of
particles trapped in the magnetosphere. Under quiescent
conditions, ions and electrons precipitate steadily (albeit
in localized bursts) from the radiation belts, implying that
there is a continuous resupply of fast particles. The pre-
cipitation is interpreted in terms of pitch-angle scattering
of trapped particles into the loss cone. Pitch-angle scatter-
ing of ions [24,105] involves ion cyclotron waves, and the
scattering of electrons involves whistlers [27,52]. Eﬃcient
scattering is also needed to explain the data on GCRs. Both
their estimated residence time,  107 yr, in the Galactic
disc and their small observed anisotropy imply that GCRs
diﬀuse slowly through the Galaxy. The net ﬂow speed is
of order the Alfvén speed, vA  104c. The eﬃcient scat-
tering is attributed to resonant scattering by Alfvén (and
magnetoacoustic waves) in the interstellar medium.
Resonant scattering, also called pitch-angle scattering,
occurs when the gyroresonance condition is satisﬁed for
a fast particle interacting with a wave whose phase speed
is much less than the speed of the particle. To the parti-
cle, the wave appears as a spatially periodic oscillation in
the magnetic ﬁeld. The resonance condition corresponds
to the wavelength of the magnetic ﬂuctuation being equal
to the distance that the particle propagates along the ﬁeld
line in a gyroperiod. As a consequence, the Lorentz force,
qv  ıB, due to the magnetic ﬂuctuation, ıB, in the wave
is in the same direction at the same phase each gyroperiod.
This causes a systematic change in the momentum of the
particle, without any change in energy, corresponding to
a change in the pitch angle, ˛ D arctan(pk/p?). The sense
of the change remains the same until phase coherence is
lost. Such interactions cause random changes in ˛, as a re-
sult of which the particles diﬀuse in ˛. The resonance con-
dition for a given wave (given !; kk) deﬁnes a resonance
curve in velocity (vk–v?) space or in momentum (pk–p?)
space; this curve is a conic section in general and it reduces
to a circle in the limit of slow phase speed. Pitch-angle dif-
fusion corresponds to particles diﬀusing around the cir-
cumference of this circle.
The resonant interaction results in the waves either
growing or damping, depending on whether energy is
transferred from the particles to the waves, or vice versa.
For any given distribution of particles, all the particles that
can resonate with a given wave lie on the resonance curve.
If the gradient in the distribution function along this curve
is an increasing function of ", more particles lose energy
to the wave than gain energy from the wave, and the wave
grows. For any anisotropic distribution, there is a choice of
the relevant signs (harmonic number s D ˙1, kk/jkkj and
vk/jvkj) that implies growth of those waves that carry away
the excess momentum. The back reaction on the distribu-
tion of particles, called quasilinear relaxation, reduces the
anisotropy that causes the wave growth [74]. This implies
that the resonant waves needed to scatter the particles can
be generated by the anisotropic particles themselves.
In DSA the resonant waves can be generated by a more
subtle anisotropy. Upstream of the shock the density of
the fast particles decreases with distance from the shock,
and the spatial gradient implies a form of anisotropy
that can cause the resonant waves to grow. Analogous to
the scattering of streaming CRs, the growth of the reso-
nant waves decreases rapidly with increasing particle en-
ergy, and some other source of resonant waves is required
for higher energy particles. Scattering downstream of the
shock is less problematic, with several possible sources
of resonant waves, including waves generated in the up-
stream region and swept back across the shock.
Acceleration by Parallel Electric Fields
There is overwhelming evidence that acceleration by Ek
occurs in the Earth’s auroral zones. Acceleration by Ek
plays a central role in populating pulsar magnetospheres
with electron-positron pairs, and is a possible mechanism
for the “bulk energization” mechanism for energetic elec-
trons in solar ﬂares.
Acceleration by Ek is necessarily associated with a par-
allel current, Jk: the relevant source term from Maxwell’s
equations for the energy transfer is JkEk. Historically, the
existence of Jk in the auroral zones is perhaps the oldest
idea relevant to the theory of acceleration, having been
predicted by Kristian Birkeland in 1903. Understanding
acceleration by Ek remains problematical. One concep-
tual diﬃculty is that astrophysical plasmas are typically
described using ideal magnetohydrodynamics (MHD), in
which one has Ek D 0 by hypothesis: theories involving Ek
are necessarily outside the scope of MHD. A related dif-
ﬁculty is that Ek accelerates particles of oppositely signed
charge in opposite directions, setting up a current ﬂow that
is opposed by inductive eﬀects. Both inductive eﬀects and
the charge separation resulting from the current ﬂow tend
to suppress the Ek and hence the acceleration.
A generic model has emerged for the acceleration by
Ek of auroral electrons [103]. The model requires a me-
chanical driver, which sets up an EMF, and a closed circuit
around which the current ﬂows. The EMF is made avail-
able by magnetic reconnection, which acts like a switch
setting up the current ﬂow around the circuit. The en-
ergy released is transported away through a Poynting ﬂux,
propagating at vA. This energy is transferred, at least
in part, to energetic particles, in an acceleration region,
which acts as a load in the circuit. The available po-

Acceleration Mechanisms
A
25
tential localizes across the acceleration region, producing
the Ek.
Suggested astrophysical applications of circuit mod-
els tend to be viewed with suspicion, especially because
the physics involved is speciﬁcally excluded in the as-
sumptions of ideal MHD. Some non-ideal-MHD eﬀect is
needed to explain why Ek ¤ 0 develops. One such mech-
anism is resistivity, which is included in non-ideal or re-
sistive MHD, but the Ek allowed by classical resistivity is
too weak to play any signiﬁcant role in the fully ionized
plasmas of interest. Currently favored possibilities include
double layers and inertial Alfvén waves.
Despite its apparent simplicity, acceleration by Ek is
the least understood of the three generic mechanisms.
Stochastic Acceleration
Second-order Fermi acceleration [30] is now identiﬁed as
an archetypical form of stochastic acceleration, and the
term Fermi acceleration is sometimes used in a generic
sense to describe stochastic acceleration.
Diﬀusion in Momentum Space
The isotropic diﬀusive equation in momentum space that
describes stochastic acceleration is [57,98,99]
@hf i(p)
@t
D 1
p2
@
@p

p2Dpp(p) @
@p

hf i(p) ;
(1)
where hf i(p) is the particle distribution function, f (p), av-
eraged over pitch angle. The diﬀusion coeﬃcient is
Dpp(p) D A
cp2
4v

1  v2
A
v2
2
;
(2)
where A is the acceleration rate. The ﬁnal factor in (2)
does not appear in simpler treatments of Fermi-type accel-
eration; this factor may be ignored only in the limit of par-
ticle speeds much greater than the Alfvén speed, v 	 vA.
The meaning of A is most easily understood by estimating
the mean rate of acceleration [72,97]
d"
dt
	
D 1
p2
@
@p
vp2Dpp(p) ;
(3)
where " D (m2c4 C p2
kc2 C p2
?c2)1/2 is the energy. In the
limit v 	 vA (3) gives
d"
dt
	
 Apc ;
(4)
which reduces for highly relativistic particles to the famil-
iar form hd"/dti  A". The acceleration rate may be esti-
mated as
A D 
4 ¯!
ıB
B
2
;
(5)
where ıB is the magnetic amplitude in the waves and ¯! is
their mean frequency. A remarkable feature of (5) is that,
although eﬀective scattering is an essential ingredient in
the theory, the acceleration rate is independent of the de-
tails of the scattering. The parameters that appear in (5)
refer only to the MHD turbulence that is causing the ac-
celeration.
Treatment in Terms of Resonant Interactions
Further insight into Fermi-type acceleration was given by
Achterberg [1] who showed that it is equivalent to reso-
nant damping of magnetoacoustic waves in the presence of
eﬃcient pitch-angle scattering. The general gyroresonance
condition is
!  s˝  kkvk D 0 ;
(6)
where ! is the wave frequency, kk is the component of its
wave vector parallel to B, s D 0; ˙1: : : : is the harmonic
number, ˝ D jqjB/m is the relativistic gyrofrequency of
the particle with charge q, mass m, Lorentz factor  and
velocity vk parallel to B. The eﬀect of gyroresonant inter-
actions on the particles and the waves is described by the
quasilinear equations, which are written down in the Ap-
pendix. The particular resonant interaction of interest here
is at the Cerenkov resonance, s D 0. Resonance at s D 0
is possible for waves that have a component of their elec-
tric ﬁeld along B, and this is the case for magnetoacoustic
waves, but not for Alfvén waves (in ideal MHD). The reso-
nant interaction alone would cause the particles to diﬀuse
in pk, with p? remaining constant. This corresponds to
the special case
df (p)
dt
D
@
@pk

Dkk(p)@f (p)
@pk

(7)
of the general diﬀusion equation (33). In the absence of
pitch-angle scattering, the distribution of fast particles be-
comes increasingly anisotropic, favoring pk over p?, lead-
ing to a suppression of the acceleration. In the presence
of eﬃcient pitch-angle scattering, one averages (7) over
pitch-angle, leading to the isotropic diﬀusion equation (3),
with Dpp(p) D 1
2
R d2Dkk(p), where  D pk/p is the
cosine of the pitch angle. The resulting expression for
Dpp(p), given by (2) with (5), conﬁrms that this resonant
interaction is equivalent to Fermi acceleration.
Diffusive Shock Acceleration
Diﬀusive shock acceleration (DSA) is now recognized as
the most important acceleration mechanism in astrophys-
ical plasmas: its identiﬁcation was a major achievement in

26 A
Acceleration Mechanisms
the ﬁeld. Prior to the identiﬁcation of DSA, it was diﬃcult
to explain how acceleration can be as eﬃcient as it appears
to be, and why power-law energy spectra are so common.
DSA is so eﬃcient that one needs to look for self-regu-
lation processes that limit its eﬃciency, and DSA implies
power-law spectra of the form observed without any addi-
tional assumptions.
The treatment of DSA given below is a nonrelativis-
tic, single-particle theory. The assumption that collective
eﬀects of the fast particles can be neglected is not neces-
sarily valid: DSA is so eﬃcient that the fast particles can
become an important component of the upstream plasma.
Once the pressure associated with the fast particles be-
comes comparable with the thermal pressure, the structure
of the shock is modiﬁed by this pressure. This can result in
the stresses being transferred from the downstream to the
upstream plasma primarily through the fast particles, with
no discontinuity in the density of the thermal gas [67].
Such nonlinear eﬀects provide a constraint on DSA.
Diﬀusive Treatment of DSA
Consider a distribution of particles f (p; z) that is aver-
aged over pitch angle and is a function of distance z from
a shock in a frame in which the shock is at rest. It is
assumed that scattering causes the particles to diﬀuse in
the z direction with diﬀusion coeﬃcient (z). The parti-
cles are also assumed to be streaming with the streaming
speed u. The diﬀusion is described by
@f (p; z)
@t
C u @f (p; z)
@z
C ˙p@f (p; z)
@p
D @
@z

(z)@f (p; z)
@z

C Q(p; z)  fesc(p) ;
(8)
where Q(p; z) is a source term, where the sink term fesc(p)
takes account of escape of particles downstream from the
shock, and where the term involving ˙p D (1/3)p@u/@z
determines the energy changes. It is assumed that the
speed u changes abruptly across the shock:
u D
(
u1
for
z < 0
(upstream) ;
u2
for
z > 0
(downstream) ;
@u
@z D (u1  u2)ı(z) :
(9)
A stationary solution of (8) exists when both the source
and the sink term are neglected, such that the equation re-
duces to u @f /@z D (@/@z)( @f /@z); a general solution is
f (p; z) D A C B exp

u
Z
dz
1
(z)

;
(10)
with u constant on either side of the shock, and where
A; B are constants. In the upstream region, z < 0, one
has u/(z) ¤ 0, and the solution (10) diverges at z !
1 unless one has B D 0 for z < 0. Writing f˙(p) D
limz!˙1 f (p; z), one has
f (p; z)D
8
ˆˆ<
ˆˆ:
f(p) C [fC(p)  f(p)]exp
h
u1
R 1
0
dz
1
(z)
i
for z < 0 ;
fC(p)
for z > 0 :
(11)
On matching the discontinuity in the derivative of this
solution with the discontinuity due to the acceleration
term, one ﬁnds
fC(p) D bpb
Z p
0
dp0 p0(b1) f(p0) ;
b D
3u1
u2  u1
;
(12)
which determines the downstream solution in terms of the
upstream solution. That is, if the shock propagates into
a region where the distribution is f(p), then after the
shock has passed the distribution is fC(p).
For monoenergetic injection, f(p) / ı(p  p0) say,
(12) implies fC(p) / pb. The power law index, b, is
determined by the ratio u1/u2, which is determined by
the strength of the shock. In terms of the Mach number
M D u1/vA1, where vA1 is the Alfvén speed upstream of
the shock, and the ratio  of speciﬁc heats, one ﬁnds
b D
3r
r  1 D 3( C 1)M2
2(M2  1) ;
r D
( C 1)M2
2 C (  1)M2 ;
(13)
where r is the compression factor across the shock. For
 D 5/3 and M2 	 1, the maximum strength of the
shock is r D 4 and the minimum value of the spectral in-
dex is b D 4.
Alternative Treatment of DSA
An alternative treatment of DSA is as follows. Consider
a cycle in which a particle with momentum p crosses the
shock from downstream to upstream and back again. Let
(p) be the change in p in a cycle, and P(p) be the prob-
ability per cycle of the particle escaping far downstream.
Simple theory implies
(p) D 4(r  1)
3r
u1
v p ;
P(p) D 4u1
vr :
(14)
After one cycle, p ! p0 D p C (p) implies that the
number of particles per unit momentum changes ac-
cording to 4 p2 f (p)dp
!
4 p02 f (p0)dp0
D
[1 

Acceleration Mechanisms
A
27
P(p)]4 p2 f (p)dp. After integration, one imposes the re-
quirement that the downstream ﬂux of particles balances
the upstream ﬂux, and a result equivalent to (12) is de-
rived.
This model allows one to estimate the acceleration
time, ta, in terms of the cycle time tc. One has tc D (1/
u1 C2/u2) D (1 Cr2)/u1, where 1;2 are the scattering
mean free paths in the upstream and downstream regions,
respectively. For isotropic scattering, the scattering mean
free path, , is related to the spatial diﬀusion coeﬃcient,
, by  D v/3, where v is the speed of the particle. The
mean free path may be written as  D rg, where rg is the
gyroradius, and it is usually assumed that  is a constant
of order unity, called Bohm diﬀusion. The rate of momen-
tum gain is given by
dp
dt D p
ta

dp
dt

loss
;
ta D u2
1
c¯;
¯ D 3r(1 C r2)
4(r  1)
;
(15)
where ta is an acceleration time, ¯ is a mean scattering free
path, and where a loss term is included.
Injection and Preacceleration
DSA requires eﬃcient scattering, and this is attributed to
resonant scattering, Sect. “Resonant Scattering”. For ions,
scattering by Alfvén and magnetoacoustic waves requires
v 	 vA, and for nonrelativistic electrons, scattering by
whistler waves requires v 	 43vA. These conditions are
typically not satisﬁed for thermal particles, and one needs
to postulate an injection or preacceleration mechanism to
create a seed population of ions or electrons above the re-
spective thresholds for DSA to operate on them.
One possible type of injection mechanism involves col-
lisions populating a depleted thermal tail. The thresholds
(v  vA for ions or v 	 43vA for electrons) are some-
where in the tail of the thermal distribution of particles,
and the acceleration removes these particles, leaving a de-
pleted thermal tail. Collisions between thermal particles
cause a net ﬂux of particles in momentum space into the
high-energy tail, tending to restore a thermal distribution.
The rate at which nonthermal particles are created by this
process depends on the collision frequency and on the de-
tails of the acceleration mechanism [38]. However, this
process encounters two serious diﬃculties. First, it is too
slow to account for the required injection. Second, for ions
it is extremely sensitive to the charge and mass, implying
that the relative isotopic abundances of accelerated ions
should diﬀer by very large factors from the relative isotopic
abundances of the background plasma, and this is simply
not the case in general. Other preacceleration mechanisms
involve either electric ﬁelds associated with the shock it-
self, or waves associated with the shock. These are dis-
cussed brieﬂy below in connection with shock drift accel-
eration.
DSA at Relativistic Shocks
In the extension of DSA to highly relativistic shocks, the
assumption of near isotropy of the pitch angle distribu-
tion is not valid, and the simple analytic theory does not
generalize in a straightforward way. Numerical calcula-
tions show that the resulting spectrum is a power law, with
b D 4.2–4.3 [4,41].
DSA at Multiple Shocks
In many of the applications of DSA, it is unlikely that a sin-
gle shock is responsible for the acceleration [2]. Extension
of the theory to multiple shocks shows that DSA is related
to Fermi-type acceleration: there are energy gains associ-
ated with DSA at each shock, and energy losses due to de-
compression between shocks. It is straightforward to in-
clude synchrotron losses in such a model, and the com-
petition between acceleration and synchrotron losses can
possibly explain ﬂat or slightly inverted synchrotron spec-
tra in some sources.
Multiple Shock Model
Consider the following model for DSA at multiple shocks.
(a) All shocks are identical with compression ratio, r D
b/(b  3), and all have the same injection spectrum,
0(p), assumed to be monoenergetic, 0(p) / ı(p 
p0).
(b) The distribution downstream of the ﬁrst shock, f1(p),
results from acceleration of the injection spectrum,
0(p), at the shock, cf. (12):
f1(p) D bpb
Z p
0
dq qb10(q) ;
b D
3r
r  1 : (16)
(c) Between each shock a decompression occurs. Assum-
ing that scattering keeps the particles isotropic, the
distribution function after decompression is f 0
1(p) D
f1(p/R), with R D r1/3. Hence the injection spec-
trum into the second shock is
f 0
1(p) D b(p/R)b
Z p/R
0
dq qb10(q) :
(17)
(d) The injection spectrum at any subsequent shock con-
sists of the sum of 0(p) and the decompressed
spectrum resulting from acceleration at the previous
shock.

28 A
Acceleration Mechanisms
Acceleration Mechanisms, Figure 2
The distribution function f0
i (p; p0) for particles accelerated after
i D 1–3 shock. The dashedline indicates the asymptotic solution,
f1(p) given by (18)
The model implies that downstream of the nth shock (after
decompression) the distribution is
f 0
n(p) D
n
X
iD1
f 0
i (p; p0)
f 0
i (p; p0) D Kbi
p0

p
Ri p0
b (ln p/Ri p0)i1
(i  1)!
:
(18)
After an arbitrarily large number of shocks, the spectrum
approaches f1(p) / p3 at p > p0, as illustrated in Fig. 2.
This spectrum is harder, by one power of p, than the spec-
trum / p4 from a single strong shock. Moreover, the
spectrum f1(p) / p3 is approached irrespective of the
strength of the shocks, although the stronger the shocks
the faster it is approached. An interpretation is that the
combination of DSA and decompression leads to a Fermi-
like acceleration mechanism: the asymptotic solution for
Fermi-type acceleration for constant injection at p D p0
is a power law with b D 3 for p > p0. Such a distribution
corresponds to a ﬂat synchrotron spectrum: the intensity
of synchrotron radiation is a power law in frequency, ,
with I() / ˛, ˛ D (b  3)/2.
Inclusion of Synchrotron Losses
Synchrotron losses cause the momentum of the radiat-
ing electron to decrease at the rate dp/dt D C1B2p2,
with C1 a constant. Synchrotron losses limit DSA: there
is a momentum, p D pc, above which the synchrotron
loss rate exceeds the rate of acceleration, and accelera-
tion to p > pc is not possible. The average acceleration
rate over one cycle due to DSA is dp/dt D (p)/tc D C2p,
and with the inclusion of synchrotron losses this is re-
placed by dp/dt D C2p  C1B2p2 D C2p(1  p/pc) with
pc D C2/C1B2. It is straightforward to repeat the calcula-
tion of DSA at multiple shocks including the synchrotron
losses [75]. In Fig. 3 the logarithm of the distribution func-
tion is plotted as a function of log(p/p0). The synchrotron
cutoﬀmomentum is chosen to be pc D 103p0, and all
the shocks have r D 3:8. The distribution below the syn-
chrotron cutoﬀ, p < pc, due to the cumulative eﬀect of in-
jection at every shock, becomes harder than b D 3, such
that the slope has a peak (with b  2) just below pc. Such
a distribution corresponds to a weakly inverted spectrum
with a peak just below a sharp cutoﬀdue to synchrotron
losses.
Applications of DSA
There in an extensive literature on applications of DSA,
and only a few remarks on some of the applications are
made here. The applications are separated into astrophys-
ical, solar and space-physics, and the emphasis in each of
these in qualitatively diﬀerent. In astrophysical applica-
tions the emphasis is on the acceleration of highly relativis-
tic particles, particularly GCRs and synchrotron-emitting
electrons. In solar applications one needs to explain de-
tailed data on the properties of the solar energetic parti-
cles (SEPs) observed in space, and on the radio, X-ray and
gamma-ray signatures of them in the solar atmosphere. In
space-physics, the objective is to model the detailed distri-
butions of particles and ﬁelds measured in situ. The com-
ments here indicate these diﬀerent emphases.
Acceleration in Young Supernova Remnants
Recent observational and theoretical developments have
been combined in the currently favored suggestion that
GCRs are accelerated at the shocks where young su-
pernova remnants (SNRs) interact with the interstellar
medium. The observational development includes X-ray,
gamma ray and TeV photon data from young supernovae,
including the remnants of historical supernovae . 103 yr
old, that show edge-brightening, which is interpreted in
terms of synchrotron emission from relativistic electrons
accelerated at an outer shock [43,101,104]. The suggestion
is that GCRs are also accelerated at these shocks. The theo-
retical development is the suggestion [11,12] that the CRs
can cause the magnetic ﬁeld to be ampliﬁed by a very large
factor in association with these shocks.
Since the original suggestion by Baade and Zwicky, it
has been widely believed that the acceleration of GCRs is
due to shocks generated by supernovae, and DSA provides
an eﬀective mechanism by which this occurs. Hillas [43]
gave four arguments in favor of the location being the
shocks in young SNRs:

Acceleration Mechanisms
A
29
Acceleration Mechanisms, Figure 3
The cumulative effect of DSA plus synchrotron losses after many shocks with injection at each shock; a the distribution, b the slope
of the distribution for p > p0: r D 3:8, pc/p0 D 103, N D 1, 10, 30, 50
(a) DSA produces a power-law spectrum of about the
right slope.
(b) The model is consistent with the long-standing argu-
ments that the only adequate energy source for GCRs
is supernovae.
(c) The composition of GCR1 is consistent with ordinary
interstellar matter being injected into DSA, and it is
at least plausible that such composition-independent
injection occurs at such shocks.
(d) When coupled with self-generation of magnetic ﬁeld,
DSA can explain the energy at which the knee between
GCR1 and GCR2 occurs [44], cf. however [81].
Although the details are not complete, these argu-
ments provide a plausible overview as to how GCR1
are accelerated.
The ampliﬁcation of the magnetic ﬁeld is attributed to the
nonlinear development of a nonresonant plasma instabil-
ity driven by the CRs [11,12,86]. A strong, self-generated
magnetic ﬁeld allows DSA to accelerate particles to higher
energy than otherwise, due to the maximum energy being
proportional to B.
Acceleration in Astrophysical Jets
Astrophysical jets are associated with accretion discs,
around proto-stars and various classes of compact objects,
including the supermassive black holes in active galactic
nuclei (AGN). Radio jets associated with AGN can extend
to enormous distances from the AGN; these jets require
that the synchrotron-emitting electrons be accelerated in
situ [16]. The appearance (characterized by various knots)
of the jet in M87 [70] in radio and optical is remarkably
similar, whereas the much shorter synchrotron loss time
of the optically-emitting electrons leads one to expect the
optical emission to be much more sharply localized about
acceleration sites. A possible model for the acceleration in-
volves DSA at multiple shocks, which develop naturally
in the ﬂow, provided the acceleration is suﬃciently fast
to overcome the losses of the optically-emitting electrons.
The required scattering of the optically-emitting electrons
can plausibly be attributed to a Kolmogorov spectrum of
turbulence [76].
A subclass of radio sources associated with AGN have
ﬂat or inverted spectra, corresponding to power laws
I() / ˛ with ˛  0 or ˛ > 0 over at least a decade in
frequency. Synchrotron self-absorption was proposed for
such spectra [51]. A speciﬁc model can account for ﬂat
spectra [68], but it requires such special conditions that
it was referred to as a “cosmic conspiracy” [22]. A pos-
sible alternative explanation, for spectra with ˛ < 1/3,
is in terms of an acceleration mechanism that produces
a power-law electron spectrum with b D 3  2˛. DSA at
a single nonrelativistic shock produces a spectrum with
b > 4 corresponding to ˛ < 0:5. A relativistic shock can
produce a somewhat harder spectrum but cannot account
for ﬂat spectra. DSA at multiple shocks tends towards
b D 3, ˛ D 0, and provides a possible alternative expla-
nation for ﬂat spectra. The pile-up eﬀect [87] occurs nat-
urally when synchorotron losses are included in DSA at
multiple shocks [75], and can account for inverted spectra
with ˛  1/3.
Solar Energetic Particles
It has long been recognized that acceleration of fast par-
ticles occurs in the solar corona in connection with solar
ﬂares. There is a power-law distribution of ﬂares in en-
ergy or in area: the energy released in a ﬂare is approx-
imately proportional to the area that brightens in H˛.
All ﬂares produce fast nonrelativistic (10–20 keV) elec-
trons, observed through their radio (type III bursts) or

30 A
Acceleration Mechanisms
X-ray (bright points) emission. Large ﬂares also produce
energetic ions and relativistic electrons. In the early lit-
erature [108], it was assumed that the acceleration oc-
curs in two phases: the ﬁrst (impulsive) phase involving
nonrelativistic electrons (and perhaps ions), attributed to
some form of “bulk energization”, and the second phase
involves a slower acceleration of solar CRs, attributed to
the shocks [59], such as those that produce type II radio
bursts. However, this simple picture is not consistent with
other data: gamma-ray data show that the relativistic elec-
trons and energetic ions appear immediately after the on-
set of the ﬂare, without the predicted delay. Spacecraft data
on solar energetic particles (SEPs) reveal further phenom-
ena and correlations that are not consistent with the two-
phase model. Various diﬀerent acceleration mechanisms
have been proposed for SEPs [77].
A new paradigm has emerged for the interpretation of
SEPs observed in space: events are separated into ﬂare-as-
sociated SEPs and CME-associated SEPs [62]. In a CME
(coronal mass ejection) a previously magnetically bound
mass of corona plasma becomes detached, accelerates
away from the Sun, and drives a shock ahead of it. The
detachment of a CME involves some form of magnetic re-
connection in the corona. Earlier ideas on a tight correla-
tion between ﬂares and CMEs have not been conﬁrmed by
more recent data. Although there is some correlation, it is
not one-to-one, and ﬂares and CMEs are now regarded as
distinct phenomena. The acceleration of CME-associated
SEPs is plausibly due to DSA, but how the ﬂare-associated
SEPs are accelerated remains poorly understood.
Acceleration Mechanisms, Figure 4
A cartoon [33] illustrating one suggestion for how magnetic reconnection leads to acceleration of particles in a solar flare
There are anomalies in the ionic composition in ﬂare-
associated SEPs, the most notable of which concerns 3He.
The ratio of 3He to 4He in the solar photosphere is
5  104, but the ratio in ﬂare-associated SEPs is highly
variable, and can be greatly enhanced, even exceeding
unity in exceptional cases. A plausible explanation for
3He-rich events is preferential preacceleration feeding into
DSA. A speciﬁc preacceleration mechanism that selects
3He involves the generation of ion cyclotron waves be-
tween the proton and 4He cyclotron resonances, with these
waves damping by accelerating 3He [32]. The formulation
of a self-consistent model for the acceleration of ﬂare-as-
sociated SEPs is complicated by the paucity of direct signa-
tures of processes in the solar corona, and the diﬃculty in
reconciling the data on various emissions from energetic
particles precipitating into the photosphere with the SEP
data.
Bulk energization of electrons is not understood.
A large fraction of the energy released in a ﬂare goes into
bulk energization, requiring acceleration of virtually all the
electrons in the ﬂaring region of the corona. One model
that involves shock acceleration is illustrated in Fig. 4:
magnetic reconnection leads to an outﬂow of plasma from
the reconnection site, with the upﬂow associated with
a CME and the downﬂow running into an underlying
closed magnetic loop. In such a model, the electron heat-
ing is identiﬁed with shock acceleration where the down-
ﬂow is stopped by the underlying loop. There are also var-
ious non-shock-associated mechanisms, involving Ek, for
bulk energization.

Acceleration Mechanisms
A
31
Resonant Scattering
The theory of resonant scattering was developed initially
in two diﬀerent contexts: the diﬀusion of GCRs through
the ISM and the formation and stability of the Earth’s Van
Allen belts.
Streaming of GCRs
A simple model for CRs streaming along the ﬁeld lines at
vCR is
f (p; ˛) D f0(p)

1 C vCR
v
cos ˛

;
f0(p) D KCR
 p
p0
b
;
(19)
where the non-streaming part of the distribution function,
f0(p), is measured to be a power law above some minimum
p0, with KCR a normalization constant.
The anisotropic CRs can generate the resonant waves
that scatter them. Growth of Alfvén (A) and magnetoa-
coustic (M) waves may be described in terms of a nega-
tive absorption coeﬃcient. On evaluating the absorption
coeﬃcient using the general expression (32) and inserting
numerical values for CRs one ﬁnds
A;M(!; ) D 2:7  107 
ne
1 cm3
1/2  p
p0
1:6
 cos 
j cos j
vCR
c
 b
3
vA
c

;
(20)
where j cos j is approximated by unity. It follows from
(20) that the waves grow in the forward streaming direc-
tion (cos  > 0 for vCR > 0) for vCR > (b/3)vA. The scat-
tering of the CRs by the waves generated by the streaming
particles themselves causes vCR to reduce. The rate of re-
duction may be evaluated using the quasilinear equation
written down in the Appendix. One ﬁnds
dvCR
dt
D s

vCR    1
2
j cos j
cos 
b
3 vA

;
(21)
with  D (FC
A  F
A )/(FC
A C F
A ), where F˙
A
are the
ﬂuxes of Alfvén waves in the forward (C) and back-
ward () streaming directions, and with the scattering fre-
quency given by
s D 3
Z C1
1
d cos ˛ sin2 ˛ D˛˛ ;
(22)
with D˛˛ determined by (34).
The growth rate (20) is fast enough to account for
growth of the waves that resonate with lower-energy
GCRs, and the scattering time is fast enough to reduce
the streaming to close to (b/3)vA. However, the growth
time increases rapidly with p, with an approximate one-
to-one relation between the resonant wave number k and
the momentum of the resonant particles kp D jqjB. For
p 	 1013 eV/c the growth time becomes ineﬀective, and
self-generation of the resonant waves cannot account for
eﬃcient scattering.
Scattering of Higher Energy CRs
For higher-energy GCRs, eﬀective scattering requires an
external source of MHD waves. There is a turbulent spec-
trum in the ISM, and the observations [5] show it to be
a Kolmogorov-like spectrum
W(k) / k5/3 ;
(23)
where W(k) is the energy in the waves per unit wavenum-
ber k. Resonant scattering by a Kolmogorov spectrum im-
plies a scattering frequency that decreases only slowly,
s / p1/3, with increasing p. This scattering must dom-
inate scattering by self-generated waves at higher p. It is
likely that the distribution of turbulence is far from ho-
mogeneous, and that the most eﬀective scattering occurs
in localized regions of enhanced turbulence. How eﬀective
scattering of higher energy particles occurs is a poorly un-
derstood aspect of acceleration mechanisms.
Nonresonant Versus Resonant Growth
Plasma instabilities can occur in two forms: resonant and
nonresonant. A nonresonant instability involves an in-
trinsically growing wave, described by a complex solu-
tion of a real dispersion equation. A recent suggestion that
has stimulated considerable interests involves an instabil-
ity [11,12] that is a nonresonant counterpart of (20) [73].
The potential importance of this instability is twofold.
First, it can allow the lower-energy particles to generate
a wide spectrum of waves including those needed to scat-
ter higher-energy particles. Second, its nonlinear develop-
ment can lead to ampliﬁcation of the magnetic ﬁeld, which
ultimately causes the instability to saturate [11,12,86].
A similar idea has been proposed in connection with gen-
eration of magnetic ﬁelds through a Weibel instability as-
sociated with the relativistic shocks that generate gamma-
ray bursts [107]. Such nonresonant growth is an impor-
tant ingredient in the currently favored acceleration mech-
anism for GCR1 [43].

32 A
Acceleration Mechanisms
Formation of the Van Allen Belts
A simple model for the formation of the radiation belts is
based on adiabatic invariants, which are associated with
quasi-periodic motions. The ﬁrst adiabatic invariant is as-
sociated with the gyration about a ﬁeld line, and it im-
plies that M / p2
?/B is an invariant. After averaging over
the gyrations, a fast particle bounces back and forth along
a ﬁeld line between two reﬂection points. The adiabatic
invariant associated with this bounce motion, denoted J,
is the integral of pk along the ﬁeld lines between the re-
ﬂection points. After averaging over bounce motion, the
remaining motion is a periodic drift around the Earth.
There is a third adiabatic invariant, ˚, which corresponds
to the integral for the vector potential around the Earth.
For a dipole-like magnetic ﬁeld it is convenient to label
a ﬁeld line by the radial distance, L, to its midpoint. Then
one has M / p2
?L3, J / pkL, ˚ / 1/L. Violation of the
third adiabatic invariant, due to perturbations in the Earth
magnetic ﬁeld with a period equal to that of the drift mo-
tion, causes the particles to diﬀuse in L at constant M and J.
The steady state density that would result from such dif-
fusion is proportional to p2
?pk / 1/L4. Particles that en-
ter the magnetosphere from the solar wind tend to dif-
fuse inwards to build up this density. As the particles dif-
fuse to smaller L, their momentum increases according to
p / L3/4. The inward diﬀusion ceases at the point where
pitch-angle scattering (which violates conservation of M)
becomes eﬀective and causes the particles to be scattered
into the loss cone and to precipitate into the Earth’s neutral
atmosphere. Due to their loss-cone anisotropy, illustrated
schematically in Fig. 5, the anisotropic particles can gen-
erate the resonant waves needed to scatter them, with the
growth rate increasing rapidly with decreasing L. In this
way, resonant scattering provides the sink (at smaller L)
that allows the Van Allen belts to remain in a steady state
when the magnetosphere is quiescent. The absorption co-
eﬃcient for the waves tends to be maintained near zero, re-
ferred to as marginal stability, such that sporadic localized
bursts of wave growth lead to sporadic localized bursts of
particle precipitation. Such bursts of wave growth can be
triggered by artiﬁcially generated VLF signals [42].
Acceleration by Parallel Electric Fields
Acceleration by an electric ﬁeld parallel to the magnetic
ﬁeld, Ek, is the simplest conceivable acceleration mecha-
nism, but also the least understood. The argument for ac-
celeration by Ek is compelling in connection with aurorae,
is very strong in connection with pulsars, and is strong for
bulk energization in solar ﬂares.
Acceleration Mechanisms, Figure 5
An idealized loss-cone distribution is illustrated; ˛0 is the loss
cone angle, the shaded region is filled by isotropic thermal par-
ticles, and the circular arcs denote contours of constant f
Available Potential
It is sometimes helpful to note that all relevant accelera-
tion mechanisms can be attributed to an inductive electric
ﬁeld, and to identify the speciﬁc inductive ﬁeld. In partic-
ular, the maximum available potential, or EMF (electro-
motive force), determines the maximum energy to which
particles can be accelerated. There are two general use-
ful semi-quantitative estimates of the available potential:
one is the EMF associated with changing magnetic ﬂux
through a circuit; the other involves the power radiated by
a rotating magnet in vacuo.
EMF Due to a Relative Motions
The EMF in a simple
circuit, due to changing magnetic ﬂux through the cir-
cuit, is ˚ D d(BA)/dt, where A is the area of the cir-
cuit. Although ˚ can be due to a changing magnetic ﬁeld,
˚ D AdB/dt, in most cases of interest it is due to relative
motions, ˚ D BdA/dt D BLv, where L and v need to be
identiﬁed in a speciﬁc model. A ˚ D BLv appears along
a magnetic ﬂux tube line that connects two MHD regions
in relative motion to each other, with v the relative veloc-
ity perpendicular to the ﬂux tube, with a magnetic ﬁeld B
in one of the regions, with L the perpendicular dimension
of the ﬂux tube orthogonal to the relative velocity in this
region. This simple argument needs to be complemented
with an identiﬁcation of how the current closes, and how
this potential is distributed around the closed circuit. This
model provides a plausible estimate for the energy of auro-
ral electrons and for electrons accelerated in Jupiter’s mag-
netosphere. In the case of auroral particles, the velocity, v,
is between the Earth and the solar wind, and the accel-
eration occurs along the auroral ﬁeld lines that connect
these two regions. Parameters B and L across the mag-
netotail and v for the solar wind, give ˚ of several kV,

Acceleration Mechanisms
A
33
consistent with the observed auroral electrons of a several
keV. In an early model for the Io–Jupiter interaction, the
moon Io was assumed to drag the Jovian magnetic ﬁeld
lines threading it through the corotating magnetosphere,
leading to the estimate ˚ D BIoLIovIo, with BIo is the Jo-
vian magnetic ﬁeld at the orbit of Io, LIo is the diameter
of Io and vIo is the relative velocity of Io to the corotating
magnetic ﬁeld lines [37]. This gives ˚ of a few MV, con-
sistent with a copious supply of few-MeV electrons associ-
ated with Io’s motion through the Jovian magnetosphere.
Another application of this estimate of ˚ provides
a limit on the maximum energy to which particles can
be acceleration due to DSA. The relative ﬂow on oppo-
site sides of the shock implies an electric ﬁeld in the plane
of the shock front: one may identify this electric ﬁeld as
Bv, with v the relative ﬂow speed perpendicular to B. Then
˚ D BvL is the potential available across the lateral extent,
L, of the shock. The maximum energy to which a particle
can be accelerated through DSA is limited to < eBvL per
unit charge.
A point of historical interest is that the earliest rec-
ognized suggestion for acceleration of CRs involved an
inductive ﬁeld due to dB/dt: in 1933 Swann [94] pro-
posed that the acceleration occurs in starspots due to
the magnetic ﬁeld increasing from zero to B D 0:2 T in
a time  D 106 s in a region of size L D 3  108 m, giving
˚ D BL2/ D 2  1010 V. Although this model is not con-
sidered realistic, it is of interest that a similar value for ˚
results from ˚ D BLv with the same B; L and with L/ re-
placed by v D 3 km s1; these numbers are close to what
one would estimate for the potential diﬀerence between
ﬁeld lines on opposite sides of a rotating sunspot. Such
large potential drops are available in the solar atmosphere,
but the implications of this are poorly understood.
EMF Due to a Rotating Compact Object
In a simple
model for the electrodynamics of a rotating magnetized
compact object, such as a neutron star or a black hole, the
slowing down is estimated assuming that the rotational en-
ergy loss is due to magnetic dipole radiation. The power
radiated may be written as I˚, where the eﬀective dis-
placement current, I, and the available potential, ˚, are re-
lated by ˚ D Z0I, with Z0 D 0c D 377 ˝ the impedance
of the vacuum. Equating ˚2/Z0 to the rate of rotational
energy loss provides as estimate of ˚. Although the slow-
ing down for pulsars is not actually due to magnetic dipole
radiation, estimates based on this model underpin the ba-
sic interpretation of pulsars, including estimates of the
age, the surface magnetic ﬁeld and the available potential.
The ˚ may be interpreted as between the stellar surface
and the pulsar wind region outside the light cylinder, lead-
ing to an Ek along the polar-cap ﬁeld lines that connect
these regions.
Acceleration of Auroral Electrons
Magnetic reconnection in the Earth’s magnetotail, in a so-
called magnetospheric substorm, results in energetic elec-
trons precipitating into the upper atmosphere, producing
the visible aurora. A build up of the magnetic stresses prior
to a substorm results from reconnection, at the sunward
edge of the magnetosphere, between terrestrial ﬁeld lines
and with interplanetary ﬁeld lines, with the solar wind
transferring magnetic ﬂux from the front of the magneto-
sphere to the magnetotail. There is a build up of the cross-
tail current associated with the build up of the magnetic
ﬂux. A generic model for a substorm [103] involves relax-
ation of these stresses through reconnection, which creates
closed ﬁeld lines, and allows transfer of magnetic ﬂux to
the front of the magnetosphere as these reconnected ﬁeld
rotate. The reconnection also causes the cross-tail current
to be partly redirected around a circuit along the recon-
necting ﬁeld lines, so that the current closes by ﬂowing
along ﬁeld lines to the ionosphere, across the ﬁeld lines
due to the Pedersen conductivity there, and returning to
the magnetotail along a nearby ﬁeld lines. The magnetic
energy released is goes partly into an Alfvénic ﬂux towards
the Earth, and this energy ﬂux is partially converted into
energetic electrons in an acceleration region. The accel-
eration region is far removed from the reconnection site,
and is at a height > 103 km, well above the location of the
visible aurora, as indicated schematically Fig. 6. The accel-
eration is due to an Ek in the acceleration region, but the
mechanism that sets up the Ek is still not clear. Three ideas
are discussed brieﬂy here.
A parallel potential diﬀerence develops between the
ionosphere, ˚ion, and the magnetosphere, ˚mag, along
a current-carrying ﬁeld line. The current implies a ﬂow
of electrons relative to ions. There is a limited supply of
magnetospheric electrons to carry the current, due to most
electrons mirroring before they reach the ionosphere. The
potential diﬀerence develops to allow the current to ﬂow.
Let Rm be the mirror ratio, along a current-carrying ﬁeld
line. For 1 
 e˚k/Te 
 Rm, the so-called Knight rela-
tion [54,63,102] gives
Jk  K(˚ion  ˚mag) ;
K D
e2ne
(2)1/2meVe
;
(24)
where Te D meV2
e is the electron temperature. The Knight
relation (24) is useful for a number of purposes, but it is
not particularly useful in considering how the potential lo-
calizes to create the Ek in the acceleration region.

34 A
Acceleration Mechanisms
Acceleration Mechanisms, Figure 6
A cartoon [13] showing the regions in the Earth’s magnetosphere associated with the acceleration of auroral electrons. The gener-
ation region is where magnetic reconnection occurs, launching Alfvén waves, which accelerate electrons in an acceleration region
where the EMF from the generator is localized along field lines
In situ observations of Ek in the acceleration region
have been interpreted in a variety of ways. Localized struc-
tures sweeping across a spacecraft were initially called elec-
trostatic shocks [78] and also referred to as double layers
(DLs) [19,28]. The theory of DLs is well established for lab-
oratory plasmas, and models for DLs have been adapted to
space and astrophysical plasma applications [18,85]. DLs
are classiﬁed as weak or strong, depending on whether the
potential diﬀerence is of order or much greater than Te/e,
respectively. Some models for the formation and structure
of DLs involve two other ideas that have been discussed
widely in the literature: anomalous resistivity and electron
phase space holes [80]. However, the details of how lo-
calized DLs accelerate particles and produce the observed
ﬂuxes of energetic electrons is still not clear.
Another way in which an Ek can arise is through the
Alfvén waves that transport the energy released in the
magnetotail [92,103,109]. The condition Ek D 0 is strictly
valid for an Alfvén wave only in ideal MHD, and Ek ¤ 0
can arise due to electron inertia, due to ﬁnite thermal
gyroradii and due to (anomalous) resistivity. Low-fre-
quency Alfvén waves with large k? become either inertial
Alfvén waves (IAW) or kinetic Alfvén waves, depending
on whether inertia or thermal eﬀects dominate. The dis-
persion relation and the ratio of the parallel to the perpen-
dicular electric ﬁelds in an IAW depend on the skin depth,
e D c/!p:
!
jkkj D
vA
(1 C k2
?2e)1/2 ;
Ek
E?
D
kkk?2
e
1 C k2
?2e
:
(25)
The Ek associated with IAWs is one suggested mechanism
for auroral acceleration: the skin depth is several kilome-
ters, of the order, 10 km, of observed changes in Jk across
ﬁeld lines [103,109]. Acceleration by IAWs is a relatively
old idea [36], favored for electrons accelerated by Alfvén
waves generated by Io’s motion through the Jovian mag-
netosphere [35]. An important detail is that this Ek, being
a wave ﬁeld, reverses sign, accelerating electrons in oppo-
site directions, every half wave period.
Another relevant idea is that of an Alfvénic resonator.
Inhomogeneities along ﬁeld lines cause vA to change, and
this leads to reﬂection of Alfvén waves at the top of the
ionosphere. Suppose there is also reﬂection at some higher
region. This combination leads to localized trapping of
wave energy in the Alfvén resonator between the reﬂec-
tion points [29,64]. A self-consistent acceleration model
requires that the upper reﬂection site be where the Ek de-
velops and the electrons are accelerated, perhaps resulting
in a localized current instability and associated anomalous
resistivity that cause the Alfvén waves to be reﬂected.
An extreme example of auroral acceleration occurs in
inverted-V electrons. The upward current can essentially
depopulate a localized auroral ﬂux tube of all thermal elec-
trons, leading to a plasma cavity in which the inverted-
V electrons are observed, Fig. 7. The “inverted-V” refers
to the dynamic spectrum observed by a spacecraft as it
crosses the cavity: lowest energies near the edges and max-
imum energy near the center. Inverted-V electrons corre-
late with auroral kilometric radiation [39], which is due to
electron cyclotron maser emission.
One can conclude that despite signiﬁcant progress, the
details of the acceleration of auroral electrons still need
further investigation [20].
Acceleration in Pulsars Gaps
Pulsars are rapidly rotating neutron stars with superstrong
magnetic ﬁelds. There are three broad classes: ordinary,
recycled (or millisecond), and magnetars; only ordinary
pulsars are considered here. These have periods P  0:1–
1 s, surface magnetic ﬁelds B  107–109 T and are slow-

Acceleration Mechanisms
A
35
Acceleration Mechanisms, Figure 7
A density depletion in an auroral region of the Earth; inverted-V
electrons carry a large upward current in such regions [14]
ing down at a rate ˙P  1015. The age, P/2 ˙P, and the sur-
face magnetic ﬁeld, B / (P ˙P)1/2, are estimated assuming
the rotating magnetic dipole model in which the rotational
energy loss is due to magnetic dipole radiation. Equat-
ing the energy loss due to the observed slowing down to
˚2/Z0 gives an estimate for the potential available along
ﬁeld lines in the polar-cap region. For the Crab pulsar
the rotational energy loss is approximately 1031 W, im-
plying ˚  6  1016 V; the available potentials in most
other pulsars are somewhat smaller than this. In the most
widely favored model, the electric ﬁeld is assumed to cause
a charge-limited ﬂow from the stellar surface, provid-
ing the “Goldreich–Julian” charge density required by the
corotation electric ﬁeld. However, the Goldreich–Julian
charge density cannot be set up globally in this way, and
requires additional source of charge within the magne-
tosphere. The available potential is assumed to develop
across localized regions called “gaps” where acceleration of
“primary” particles, due to Ek ¤ 0, results in the emission
of gamma rays which trigger a pair cascade that produces
the necessary secondary (electron–positron) charges. This
additional source of charge is assumed to screen Ek out-
side the gaps. However, there is no fully consistent model
and no consensus on the details of the structure and lo-
cation of the gaps. The present author believes that exist-
ing models, based on a stationary distribution of plasma
in a corotating frame, are violently unstable to temporal
perturbations, resulting in large-amplitude oscillations in
Ek [61].
Although the details of acceleration by Ek are quite
diﬀerent in the pulsar case from the auroral case, the un-
derlying diﬃculties in understanding such acceleration are
similar. A static model for acceleration by Ek encounters
seemingly insurmountable diﬃculties, and an oscillating
model involves unfamiliar physics.
Acceleration of Nonrelativistic Solar Electrons
A large fraction of the energy released in a solar ﬂare goes
into very hot electrons: all the electrons in a relatively
large volume are accelerated in bulk to 10–20 keV. Sev-
eral models for bulk heating involving acceleration by Ek
have been proposed, but all have unsatisfactory features.
One idea is to invoke a runaway process [45]. Consider
the equation of motion of an electron in an electric ﬁeld
subject to Coulomb collisions with a collision frequency
e(v) D e(Ve/v)3, where Ve is the thermal speed of elec-
trons:
dv
dt D  eE
m  ev
Ve
v
3
:
(26)
It follows that electrons with speed
v
Ve
>
ED
E
1/2
;
ED D mVee
e
;
(27)
are freely accelerated; ED is called the Dreicer ﬁeld. This
so-called electron runaway sets up a charge separation that
should quickly screen Ek and suppress the acceleration. It
also causes Jk to change, and this is opposed by inductive
eﬀects. Runaway on its own is not a plausible acceleration
mechanism.
There are inconsistencies in estimates of the available
potential, ˚. Assuming that bulk energization is due to ac-
celeration by an Ek implies ˚  10–20 kV, that is of order
104 V. Two arguments suggest a much larger potential. On
the one hand, there is observational evidence that large
ﬂares tend to occur in ﬂux loops carrying large currents,
I & 1012 A. If one identiﬁes I with the observed current,
then to produce the power I˚ of order 1022 W released
in a large ﬂare requires ˚ of order 1010 V. On the other
hand, as indicated above, the potential available due to
photospheric motions is estimated above to be 109–1010 V.
This potential cannot be ignored when two magnetic loops
come in contact and reconnect: the potential diﬀerence be-
tween the reconnecting ﬁeld lines is expected to be of order
109–1010 V.
There are several problem that have yet to be clearly re-
solved. One problem is how the required Ek ¤ 0 develops.
The suggestions as to how this problem is resolved in the

36 A
Acceleration Mechanisms
auroral acceleration region need to be explored for the so-
lar application. (An exception is inertial Alfvén waves: the
skin depth being far too small to be relevant in the solar
corona.) The suggestion that double layers might play an
important role in solar ﬂares has a long history [85], as do
the suggestions involving anomalous resistivity [26] and
an Alfvénic resonator [47,48]. However, none of the exist-
ing models is satisfactory. Another problem is how to rec-
oncile the estimates ˚  104 V and 109–1010 V. It appears
that the available 109–1010 V must localize into > 105 re-
gions with Ek ¤ 0 and ˚  104 V, where the acceleration
occurs, such that the accelerated electrons that escape from
one region do not pass through any other. A third prob-
lem is that, like the auroral and pulsar cases, any model
for acceleration by a static Ek encounters serious diﬃcul-
ties. Although a large-scale oscillating Ek might plausibly
overcome these diﬃculties, there is no model for such os-
cillations.
Other Acceleration Mechanisms
The three most important acceleration mechanisms are
DSA, stochastic acceleration and acceleration by parallel
electric ﬁelds. Various other mechanisms are relevant in
speciﬁc applications. Examples mentioned above are the
acceleration of energetic particles in the Earth’s Van Allen
belts and various preacceleration mechanisms for DSA.
Some comments are made here on three other mecha-
nisms: shock drift acceleration, gyroresonant acceleration
and acceleration in current sheets.
Shock Drift Acceleration (SDA)
When a particle encounters a shock it is either transmit-
ted across the shock front or reﬂected from it, and in ei-
ther case it tends to gain energy [96]. Shock drift accelera-
tion (SDA) is attributed to the grad-B drift when the par-
ticle encounters the abrupt change in B: the scalar product
of drift velocity and acceleration by the convective elec-
tric ﬁeld, qu  B, is positive, implying energy gain. The
energy gained by a particle depends on how far it drifts
along the front. A simple model for DSA is based on p2
?/B
being conserved when the particle encounters the shock.
This conservation law is somewhat surprising (it cannot
be justiﬁed on the usual basis that p2
?/B is an adiabatic in-
variant), and it applies only approximately and only after
averaging over gyrophase [25,84].
In treating SDA it is convenient to make a Lorentz
transformation to the de Hoﬀmann–Teller frame. The
frame most widely used in discussing a shock is the shock-
normal frame, in which the shock front is stationary and
the upstream ﬂow velocity is normal to it. In this frame
Acceleration Mechanisms, Figure 8
In the de Hoffmann–Teller frame the fluid velocities are parallel
to the magnetic fields on either side of the shock
there is a convective electric ﬁeld, u  B, on both sides of
the shock, and the point of intersection of B with the shock
moves in the plane of the shock. In the de Hoﬀmann–
Teller frame this point of intersection is stationary, u and
B are parallel to each other on both sides of the shock,
and there is no electric ﬁeld, Fig. 8. The de Hoﬀmann–
Teller frame exists only for “subluminal” shocks for which
the velocity of intersection is less than c; in the “superlu-
minal” case, where the velocity of intersection is greater
than c, there exists a frame in which B is perpendicular to
the shock normal. In the de Hoﬀmann–Teller frame the
energy of a particle is unchanged when it is transmitted
across the shock, or is reﬂected from the shock. A simple
theory for SDA involves the energy being conserved in the
de Hoﬀmann–Teller frame on transmission or reﬂection,
with the implications of this being found by Lorentz trans-
forming to the shock-normal frame.
Let quantities in the de Hoﬀmann–Teller and shock-
normal frames be denoted with and without primes, re-
spectively. Let  1 be the angle between u1 and B1. Pro-
vided that the relative velocity between the two frames is
nonrelativistic, the angle  1 is the same in the two frames,
and the relative velocity is u0 D u1 tan  1. In the nonrel-
ativistic case, the pitch angles in the two frames are re-
lated by v0 cos ˛0 D v cos ˛ C u0, v0 sin ˛0 D v sin ˛. Re-
ﬂection is possible only for sin2 ˛0
1  B1/B2, and the maxi-
mum change in energy occurs for reﬂected particles at the
threshold, ˛0
1 D ˛c, sin2 ˛c D B1/B2. The maximum ra-
tio of the energy after reﬂection (ref) to before (inc) is for
a particle with v1 cos ˛1 D u0(1  cos ˛c), that is propa-
gating away from the shock, and is overtaken by the shock.
The maximum ratio is
 Eref
Einc

max
D 1 C (1  B1/B2)1/2
1  (1  B1/B2)1/2 ;
(28)

Acceleration Mechanisms
A
37
Acceleration Mechanisms, Figure 9
The foreshock may be separated into an electron foreshock,
nearer to the field line that is tangent to the bow shock, and an
ion foreshock, nearer to the bow shock; the dashed line indicates
the ill-defined boundary between these two regions, which ac-
tually merge continuously into each other
which is also the case in a relativistic treatment [53]. For
a strong shock, B2 ! 4B1, the ratio (28) is (2 C
p
3)/2 
p
3 D 13:93. However, the energy ratio decreases rapidly
away from the point in the momentum space of the inci-
dent particles where this maximum occurs [9].
SDA accounts well for relevant observations of par-
ticles associated with shocks in the heliosphere, in par-
ticular, the Earth’s bow shock and other planetary bow
shocks. SDA produces a foreshock region ahead of any
(convex) curved shock. The foreshock is the region ahead
of the shock and behind the ﬁeld line that is tangent to the
shock, as illustrated in Fig. 9. Fast particles reﬂected from
the shock propagate along the ﬁeld lines as they are swept
towards the shock by the upstream ﬂow. Electrons have
higher speeds than ions, so that they propagate further be-
fore encountering the shock, producing an electron fore-
shock region that is much larger than the ion foreshock
region. SDA should similarly populate a foreshock region
ahead of any curved shock.
Type II solar radio bursts are directly associated with
shocks in the solar corona, and extending into the solar
wind [79]. Acceleration by SDA is a favored mechanism
for the acceleration of electrons that produce type II ra-
dio bursts [46,55,60]. SDA is also a potential candidate for
preacceleration for DSA. Although both suggestions seem
plausible, additional ideas are required to account for the
details of the observations.
A modiﬁcation of DSA, called shock surﬁng acceler-
ation [100], involves the particles being trapped in the
shock front for much longer than the simple theory of SDA
implies. In other modiﬁcations, the assumption that the
shock is a simple discontinuity is relaxed. Various addi-
tional possible acceleration mechanisms arise: due to a po-
tential diﬀerence along ﬁeld lines [49]; various instabilities
that can lead to large-amplitude waves that can trap and
preaccelerate particles (“surfatron” acceleration [66]); and
nonstationarity that involves the shock continually break-
ing up and reforming [89].
Resonant Acceleration
MHD turbulence consists of a mixture of the Alfvén and
fast modes. Fermi-type stochastic acceleration involves
only the fast mode, and may be interpreted in terms of res-
onant damping at harmonic number s D 0. Resonances at
s ¤ 0 are possible and lead to dissipation of turbulence in
both modes, with the energy going into the particles. For
highly relativistic particles, resonances at high harmonics
lead to a diﬀusion in momentum space, somewhat analo-
gous to Fermi-type acceleration, but with a diﬀusion coef-
ﬁcient that depends on a higher power of p [58]. Although
this acceleration must occur, for say GCRs due to turbu-
lence in the interstellar medium, there is no strong case
for it playing an important role in speciﬁc applications.
Acceleration During Magnetic Reconnection
Magnetic energy release through magnetic reconnection
is known to be associated with powerful events, notably
magnetospheric substorms and solar ﬂares, that can lead
to copious acceleration of fast particles. There have been
various suggestions that reconnection can lead directly to
acceleration of fast particles. For example, for magnetic re-
connection in a current sheet, around the surface where
B changes sign, a test particle that enters the sheet typ-
ically emerges with a higher energy [91]. Although such
energy gains occur, they do not appear to constitute an
important acceleration mechanism. Both theory and ob-
servation suggest that the important acceleration associ-
ated with reconnection is indirect and spatially separated
from the reconnection, and is not a direct consequence of
the reconnection. The magnetic energy released in recon-
nection goes primarily into an Alfvénic ﬂux and into mass
motions accelerated by the magnetic tension in the recon-
nected ﬁeld lines.
A diﬀerent application of heating in current sheets has
been proposed in connection with pulsar winds [21,65].
Beyond the light cylinder there is a wind containing
wound up magnetic ﬁeld lines in a nearly azimuthal direc-
tion, with the sign of B reversing periodically with radial
distance. The wind accelerates particles at a termination
shock, where it encounters the surrounding synchrotron
nebula. An outstanding problem is that the energy ﬂux is

38 A
Acceleration Mechanisms
Poynting dominated near the light cylinder, but must be
kinetic-energy dominated before it reaches the termina-
tion shock. The suggestion is that magnetic energy dissi-
pation in the current sheets separating the regions of op-
posite B provide the necessary conversion of magnetic en-
ergy into particle energy [21,65]. An alternative suggestion
is that the number of current carriers becomes inadequate
leading to the development of an accelerating ﬁeld [71].
Future Directions
Three generic acceleration mechanisms are widely recog-
nized, and there are also numerous other speciﬁc mecha-
nisms that are important in speciﬁc applications. The pi-
oneering works of Fermi [30,31] anticipated two of the
three generic mechanisms: stochastic acceleration and dif-
fusive shock acceleration (DSA), respectively. The third
generic mechanism, acceleration by a parallel electric ﬁeld,
Ek, was suggested even earlier [94], but remains the least
adequately understood of the three.
The most important acceleration mechanism in astro-
physics is DSA. It is the accepted mechanism for the ac-
celeration of GCRs and for relativistic electrons in most
synchrotron sources. The favored location for the acceler-
ation of GCRs is at the shocks in young supernova rem-
nants [43]. Despite the basic theory being well established
there are essential details where our current understand-
ing is incomplete and where further progress is to be ex-
pected. The well-established theory is a test-particle model
for DSA at a single nonrelativistic shock. Several modiﬁ-
cations to the theory are known to be important: the gen-
eralizations to relativistic shocks, and to multiple shocks,
and the dynamical eﬀects of the accelerated particles on
the shock structure. There are several aspects of the re-
quired eﬃcient scattering in DSA that are inadequately
understood. One is the injection problem: resonant scat-
tering is essential and requires that nonrelativistic ions and
electrons have speeds v > vA and v > 43vA, respectively.
Preacceleration to above these thresholds is required be-
fore DSA can operate. For ions, the elemental abundances
of GCRs suggests an injection mechanism that is insen-
sitive to ionic species, whereas for ﬂare-associated solar
energetic particles there are extreme elemental anomalies
(notably 3He). For electrons the injection problem is more
severe, and less understood. Second is the long-standing
question of the ratio of relativistic electrons to ions [34].
In situ data on shocks in the heliosphere provide little ev-
idence for DSA producing relativistic electrons. It is spec-
ulated that the shocks in synchrotron sources have much
higher Mach numbers than those in the heliosphere, and
that such shocks are more eﬃcient in accelerating elec-
trons. A third aspect relates to resonant scattering of the
highest energy particles. Unlike lower-energy particles, the
resonant waves needed to scatter higher-energy particles
cannot be generated by the particles themselves, and other
sources of the required waves are speculative. A suggestion
that at least alleviates this diﬃculty is that the lower-energy
particles cause waves to grow through a nonresonant in-
stability, which may lead to ampliﬁcation of the magnetic
ﬁeld by a very large factor [11]: an increase in B increases
the maximum energy to which DSA can operate. Fourth,
spatial diﬀusion across ﬁeld lines is postulated to occur in
DSA at perpendicular shocks, but is incompletely under-
stood [3].
Despite its apparent simplicity, acceleration by Ek is
the least understood of the three generic mechanisms. An
important qualitative point is that Ek ¤ 0 is inconsistent
with MHD, and acceleration by Ek necessarily involves
concepts that are relatively unfamiliar in the context of
MHD. The best understood application is to acceleration
of auroral electrons associated with magnetospheric sub-
storms, but even in this case, how the Ek develops re-
mains uncertain. Existing models for acceleration by Ek
in ‘gaps’ in pulsars and in bulk energization of electrons
in solar ﬂares have serious ﬂaws. Some of the present dif-
ﬁculties are alleviated by assuming that the relevant Ek is
oscillating: speciﬁc models include inertial Alfvén waves
and an Alfvén resonator in the magnetosphere, and large-
amplitude oscillations in pulsar magnetospheres. Further
progress in understanding acceleration by Ek is likely to
come from more detailed in situ data on acceleration of
auroral electrons.
Compared with the rapid evolution in ideas in other
subﬁelds of astrophysics, the ideas involved in understand-
ing particle acceleration have developed only gradually
over many decades. Further progress is certain to occur,
but is likely to involve gradual acceptance and modiﬁca-
tion of the existing ideas outlined above, rather than radi-
cal new insights.
Appendix A: Quasilinear Equations
The quasilinear equations are written down here using
a semiclassical formalism. The waves are regarded as a col-
lection of wave quanta, with energy „! and momentum
„k. Waves in an arbitrary mode, labeled M, have disper-
sion relation ! D !M(k), polarization vector eM(k) and
ratio of electric to total energy, RM(k). Their distribution
is described by their occupation number NM(k). A reso-
nant interaction occurs when the gyroresonance condition
is satisﬁed:
!  s˝  kkvk D 0 ;
(A1)

Acceleration Mechanisms
A
39
where s is an integer, ˝ D jqjB/m is the relativistic gy-
rofrequency, and kk, vk are the components of k, v par-
allel to B. Resonances at s > 0 are said to be via the nor-
mal Doppler eﬀect, and those at s < 0 are said to be via
the anomalous Doppler eﬀect. Resonances at s  0 are
possible only for waves with refractive index greater than
unity. The eﬀect of a wave-particle interaction is described
by the probability of spontaneous emission, wM(p; k; s),
which is the probability per unit time that the particle emit
a wave quantum in the wave mode M in the elemental
range d3k/(2)3. For a particle of charge q and mass m,
the probability of spontaneous emission is given by
wM(p; k; s) D 2q2RM(k)
"0„!M(k)
ˇˇe
M(k)  V(k; p; s)
ˇˇ2
 ı

!M(k)  s˝  kkvk

;
V(k; p; s) D

v?
s
k?R Js(k?R) ;
 iv?J0
s(k?R) ; vkJs(k?R)

;
˝ D ˝0
 ;
˝0 D jqjB
m ;
R D v?
˝ D p?
jqjB :
(A2)
An advantage of the semiclassical formalism is that the
Einstein coeﬃcients imply that the probability of stimu-
lated emission and true absorption are given by multiply-
ing this probability by NM(k). This allows the eﬀect on the
occupation numbers of the waves and the particles to be
written down using a simple bookkeeping argument. For
the waves one ﬁnds
dNM(k)
dt
D
dNM(k)
dt

spont
 M(k)NM(k) ;
(A3)
where the two terms on the right hand side describe spon-
taneous emission and absorption, respectively, with the
absorption coeﬃcient given by
M(k) D 
1
X
sD1
Z
d3p wM(p; k; s) ˆDs f (p) ;
ˆDs D „
s˝
v?
@
@p?
C kk
@
@pk

D „!
v
 @
@p C cos ˛  nMˇ cos 
p sin ˛
@
@˛

:
(A4)
The two forms correspond to, respectively, cylindrical and
polar coordinates in momentum space.
The evolution of the distribution of particles due to the
resonant interaction includes a term, neglected here, that
describes the eﬀect of spontaneous emission, and a quasi-
linear diﬀusion equation that describes the eﬀect of the in-
duced processes. In cylindrical and spherical polar coordi-
nates, this equation is
df (p)
dt
D 1
p?
@
@p?


p?

D??(p) @
@p?
C D?k(p) @
@pk

f (p)

C
@
@pk

Dk?(p) @
@p?
C Dkk(p) @
@pk

f (p)

D
1
sin ˛
@
@˛


sin ˛

D˛˛(p) @
@˛ C D˛p(p) @
@p

f (p)

C 1
p2
@
@p

p2

Dp˛(p) @
@˛ C Dpp(p) @
@p

f (p)

;
(A5)
respectively, with the diﬀusion coeﬃcients in either set of
coordinates given by
DQQ0(p) D
1
X
sD1
Z
d3k
(2)3 wM(p; k; s) Q Q0 NM(k);
Q D ˆDsQ ;
p? D s˝
v?
;
pk D „kk ;
˛ D „(! cos ˛  kkv)
pv sin ˛
;
p D „!
v :
(A6)
Bibliography
Primary Literature
1. Achterberg A (1981) On the nature of small amplitude Fermi
acceleration. Astron Astrophys 97:259–264
2. Achterberg A (1990) Particle acceleration by an ensemble of
shocks. Astron Astrophys 231:251–258
3. Achterberg A, Ball L (1994) Particle acceleration at superlumi-
nal quasi-perpendicular shocks. Appl SN1978K SN1987A. As-
tron Astrophys 285:687–704
4. Achterberg A, Gallant YA, Kirk JG, Guthmann AW (2001) Par-
ticle acceleration by ultrarelativistic shocks: theory and simu-
lations. Mon Not Roy Astron Soc 328:393–408
5. Armstrong JW, Rickett BJ, Spangler SR (1995) Electron den-
sity power spectra in the local interstellar medium. J Atrophys
443:209–221
6. Axford WI (1994) The origins of high-energy cosmic rays. J As-
trophys Suppl 90:937–944
7. Axford WI, Leer E, Skadron G (1977) Acceleration of cosmic
rays by shock waves. 15th Internat Cosmic Ray Conf Papers
(Plovdiv) 11:132–137
8. Baade W, Zwicky F (1934) Cosmic rays from super-novae. Proc
Natl Acad Sci 20:259

40 A
Acceleration Mechanisms
9. Ball L, Melrose DB (2001) Shock drift acceleration of electrons.
Publ Astron Soc Aust 18:361–373
10. Bell AR (1978) The acceleration of cosmic rays in shock
fronts 1&2. Mon Not Roy Astron Soc 182:147–156; 182:443–
455
11. Bell AR (2004) Turbulent amplification of magnetic field and
diffusive shock acceleration of cosmic rays. Mon Not Roy As-
tron Soc 353:550–558
12. Bell AR, Lucek SG (2001) Cosmic ray acceleration to very high
energy trhough the non-linear amplification by cosmic rays
of the seed magnetic field. Mon Not Roy Astron Soc 321:433–
438
13. Bellan PM (1996) New model for ULF Pc5 pulsations: Alfvén
cones. Geophys Res Lett 23:1717–1720
14. Benson RF, Calvert W, Klumpar DM (1980) Simultaneous wave
and particle observations in the auroral kilometric radiation
source region. Geophys Res Lett 7:959–962
15. Berger JM, Newcomb WA, Dawson JM, Frieman EA, Kulsrud
RM, Lenard A (1958) Heating of a confined plasma by oscillat-
ing electromagnetic fields. Phys Fluids 1:301–307
16. Bicknell GV, Melrose DB (1982) In situ acceleration in extra-
galactic radio jets. J Astrophys 262:511–528
17. Blandford RD, Ostriker FR (1978) Particle acceleration by as-
trophysical shocks. J Astrophys 221:L29–L32
18. Block LP (1978) A double layer review. Astrophys Space Sci
55:59–83
19. Boström R, Gustafsson G, Holback B, Holmgren G, Koskinen
H (1988) Characteristics of solitary waves and weak double
layers in the magnetospheric plasma. Phys Rev Lett 61:82–85
20. Chaston CC, Bonnell JW, Carlson CW, Berthomier M, Peticolas
LM, Roth I, McFadden JP, Ergun RE, Strangeway RJ (2002) Elec-
tron acceleration in the ionospheric Alfven resonator. J Geo-
phys Res 107:SMP 41–1–16
21. Coroniti FV (1990) Magnetically striped relativistic magneto-
hydrodynamic winds. The Crab Nebula revisited. J Astrophys
349:538–545
22. Cotton WD, Wittels JJ, Shapiro II, Marcaide J, Owen FN, Span-
gler SR, Rius A, Angulo C, Clark TA, Knight CA (1980) The very
flat radio spectrum of 0735+178: A cosmic conspiracy? J As-
trophys 238:L123–L128
23. Davis L Jr (1956) Modified Fermi mechanism for the accelera-
tion of cosmic rays. Phys Rev 101:351–358
24. Dragt AJ (1961) Effect of hydromagnetic waves of the lifetime
of Van Allen radiation protons. J Geophys Res 66:1641–1649
25. Drury LO’C (1983) An introduction to the theory of diffusive
shock acceleration of energetic particles in tenuous plasma.
Rep Prog Phys 46:973–1027
26. Duijveman A, Hoyng P, Ionson JA (1981) Fast plasma heating
by anomalous and inertial resistivity effects in the solar atmo-
sphere. J Astrophys 245:721–735
27. Dungey JW (1963) Loss of Van Allen electrons due to
whistlers. Planet Space Sci 11:591–595
28. Ergun RE, Andersson L, Main D, Su Y-J, Newman DL, Goldman
MV, Carlson CW, Hull AJ, McFadden JP, Mozer FS (2004) Auro-
ral particle acceleration by strong double layers: the upward
current region. J Geophys Res 109:A12220
29. Ergun RE, Su Y-J, Andersson L, Bagenal F, Delemere PA, Lysak
RL, Strangeway RJ (2006) S bursts and the Jupiter ionospheric
Alfvn resonator. J Geophys Res 111:A06212
30. Fermi E (1949) On the origin of cosmic radiation. Phys Rev
75:1169–1174
31. Fermi E (1954) Galactic magnetic field and the origin of cos-
mic radiation. J Astrophys 119:1–6
32. Fisk LA (1978) 3He-rich flares: a possible explanation. J Astro-
phys 224:1048–1055
33. Forbes TG, Malherbe JM (1986) A shock condensation mech-
anism for loop prominences. J Astrophys 302:L67–L70
34. Ginzburg VL, Syrovatskii SI (1964) The origin of cosmic rays.
Pergamon Press, Oxford
35. Goertz CK (1980) Io’s interaction with the plasma torus. J Geo-
phys Res 85:2949–2956
36. Goertz CK, Boswell RW (1979) Magnetosphere-ionosphere
coupling. J Geophys Res 84:7239–7246
37. Goldreich P, Lynden-Bell D (1969) Io, a Jovian unipolar induc-
tor. J Astrophys 56:59–78
38. Gurevich AV (1960) On the amount of accelerated particles in
an ionized gas under various acceleration mechanisms. Sov
Phys JETP 11:1150–1157
39. Gurnett DA (1974) The earth as a radio source: terrestrial kilo-
metric radiation. J Geophys Res 79:4227–4238
40. Hall DE, Sturrock PA (1967) Diffusion, scattering, and acceler-
ation of particles by stochastic electromagnetic fields. Plasma
Phys 10:2620–2628
41. Heavens AF, Drury LO’C (1989) Relativistic shocks and particle
acceleration. Mon Not Roy Astron Soc 235:997–1009
42. Helliwell RA (1967) A theory of discrete VLF emissions from
the magnetosphere. J Geophys Res 72:4773–4790
43. Hillas AM (2006) Cosmic rays: recent progress and some cur-
rent questions. In: Klöckner H-R, Jarvis M, Rawlings S (eds)
Cosmology, galaxy formation and astroparticle physics on
the pathway to the SKA. astro-ph/0607109 (to be published)
44. Hillas AM (2006) The cosmic-ray knee and ensuing spectrum
seen as a consequence of Bell’s SNR shock acceleration pro-
cess. J Phys Conf Ser 47:168–177
45. Holman GD (1985) Acceleration of runaway electrons and
joule heating in solar flares. J Astrophys 293:584–594
46. Holman GD, Pesses ME (1983) Type II radio emission and the
shock drift acceleration of electrons. J Astrophys 267:837–843
47. Ionson JA (1982) Resonant electrodynamic heating of stellar
coronal loops: an LRC circuitanalog. J Astrophys 254:318–334
48. Ionson JA (1985) Coronal heating by resonant (AC) and non-
resonant (DC) mechanisms. Astron Astrophys 146:199–203
49. Jones FC, Ellison DC (1987) Noncoplanar magnetic fields,
shock potentials, and ion deflection. J Geophys Res 92:
11205–11207
50. Kaplan SA (1956) The theory of the acceleration of charged
particles by isotropic gas magnetic turbulent fields. Sov Phys
JETP 2:203–210
51. Kellermann KI, Pauliny-Toth IIK (1969) The spectra of opaque
radio sources. J Astrophys 155:L71–L78
52. Kennel CF, Petschek HE (1966) Limit on stably trapped parti-
cle fluxes. J Geophys Res 71:1–28
53. Kirk JG (1994) Particle Acceleration. In: Benz AO, Cour-
voisier TJ-L (eds) Saas-Fee Advanced Course 24, Plasma As-
trophysics. Springer, New York, pp 225–314
54. Knight S (1973) Parallel electric fields. Planet Space Sci
21:741–750
55. Knock SA, Cairns IH, Robinson PA, Kuncic Z (2001) Theory of
type II radio emission from the foreshock of an interplanetary
shock. J Geophys Res 106:25041–25052
56. Krimsky GF (1977) A regular mechanism for the acceleration

Acceleration Mechanisms
A
41
of charged particles on the front of a shock wave. Sov Phys
Doklady 234:1306–1308
57. Kulsrud R, Ferrari A (1971) The relativistic quasilinear theory
of particle acceleration by hydromagnetic turbulence. Astro-
phys Space Sci 12:302–318
58. Lacombe C (1977) Acceleration of particles and plasma heat-
ing by turbulent Alfvén waves in a radiogalaxy. Astron Astro-
phys 54:1–16
59. Lee MA, Ryan JM (1986) Time-dependent shock acceleration
of energetic solar particles. J Astrophys 303:829–842
60. Leroy MM, Mangeney A (1984) A theory of energization of
solar wind electrons by the Earth’s bow shock. Ann Geophys
2:449–455
61. Levinson A, Melrose D, Judge A, Luo Q (2005) Large-am-
plitude, pair-creating oscilllations in pulsar magnetospheres.
J Astrophys 631:456–465
62. Lin RP (2006) Particle acceleration by the Sun: electrons, hard
X-rays/gamma-rays. Space Sci Rev 124:233–248
63. Lyons LR (1980) Generation of large-scale regions of auroral
currents, electric potentials, and precipitation by the diver-
gence of convection electric fields. J Geophys Res 85:17–24
64. Lysak RL, Song Y (2005) Nonlocal interactions between elec-
trons and Alfvn waves on auroral field lines. J Geophys Res
110:A10S06
65. Lyubarsky YE, Kirk JG (2001) Reconnection in a striped pulsar
wind. J Astrophys 547:437–448
66. McClements KG, Dieckmann ME, Ynnerman A, Chapman
SC, Dendy RO (2001) Surfatron and stochastic accelera-
tion of electrons at supernova remant shocks. Pys Rev Lett
87:255002
67. Malkov MA, Drury LO’C (2001) Nonlinear theory of diffu-
sive acceleration of particles by shock waves. Rep Prog Phys
64:429–481
68. Marscher AP (1977) Structure of radio sources with remark-
ably flat spectra: PKS 0735+178. Astron J 82:781–784
69. McLean DJ, Sheridan KV, Stewart RT, Wild JP (1971) Regular
pulses from the Sun and a possible clue to the origin of solar
cosmic rays. Nature 234:140–142
70. Meisenheimer K, Röser H-J, Schlötelburg M (1996) Astron As-
trophys 307:61–79
71. Melatos A, Melrose DB (1996) Energy transport in a rotation-
modulated pulsar wind. Mon Not Roy Astron Soc 279:1168–
1190
72. Melrose DB (1968) The emission and absorption of waves by
charged particles in magnetized plasmas. Astrophys Space
Sci 2:171–235
73. Melrose DB (2005) Nonresonant Alfvén waves driven by cos-
mic rays. In: Li G, Zank GP, Russell CT (eds) The physics of colli-
sionless shocks. AIP Conference Proceedings #781, American
Institute of Physics, pp 135–140
74. Melrose DB, Wentzel DG (1970) The interaction of cosmic-ray
electrons with cosmic-ray protons. J Astrophys 161:457–476
75. Melrose D, Couch A (1997) Effect of synchrotron losses
on multiple diffusive shock acceleration. Autralian J Phys
14:251–257
76. Micono M, Zurlo N, Massaglia S, Ferrari A, Melrose DB (1999)
Diffusive shock acceleration in extragalactic jets. Astron As-
trophys 349:323–333
77. Miller JA, Cargill PJ, Emslie AG, Holman GD, Dennis BR, LaRosa
TN, Winglee RM, Benka SG, Tsuneta S (1997) Critical issues for
understanding particle acceleration in impulsive solar flares.
J Geophys Res 102:4631–14660
78. Mozer FS, Cattell CA, Hudson MK, Lysak RL, Temerin M,
Torbert RB (1980) Satellite measurements and theories of
low altitude auroral particle acceleration. Space Sci Rev 27:
155–213
79. Nelson GJ, Melrose DB (1985) Type II bursts. In: McLean DJ,
Labrum NR (eds) Solar Radiophysics, chapter 13. Cambridge
University Press, pp 333–359
80. Newman DL, Goldman MV, Ergun RE, Mangeney A (2001) For-
mation of double layers and electron holes in a current-driven
space plasma. Phys Rev Lett 87:255001
81. Parizot E, Marcowith A, Ballet J, Gallant YA (2006) Observa-
tional constraints on energetic particle diffusion in young su-
pernovae remnants: amplified magnetic field and maximum
energy. Astron Astrophys 453:387–395
82. Parker EN (1957) Acceleration of cosmic rays in solar flares.
Phys Rev 107:830–836
83. Parker EN, Tidman DA (1958) Suprathermal particles. Phys
Rev 111:1206–1211
84. Pesses ME (1981) On the conservation of the first adia-
batic invariant in perpendicular shocks. J Geophys Res 86:
150–152
85. Raadu MA (1989) The physics of double layers and their role
in astrophysics. Phys Rep 178:25–97
86. Reville B, Kirk JG, Duffy P (2006) A current-driven instability
in parallel, relativistic shocks. Plasma Phys Cont Fus 48:1741–
1747
87. Schlickeiser R (1984) An explanation of abrupt cutoffs in the
optical-infrared spectra of non-thermal sources. A new pile-
up mechanism for relativistic electron spectra. Astron Astro-
phys 136:227–236
88. Schlüter A (1957) Der Gyro-Relaxations-Effekt. Z Nat 12a:822–
825
89. Schmitz H, Chapman SC, Dendy RO (2002) Electron preaccel-
eration mechanisms in the foot region of high Alfvénic Mach
number shocks. J Astrophys 579:327–336
90. Shen CS (1965) Transit acceleration of charged particles in
an inhomogeneous electromagnetic field. J Astrophys 141:
1091–1104
91. Speiser TW (1965) Particle trajectories in model current
sheets, 1. Analytic solutions. J Geophys Res 70:1717–1788
92. Stasiewicz K, et al. (2000) Small scale Alfvénic structures in the
aurora. Space Sci Rev 92:423–533
93. Sturrock PA (1966) Stochastic acceleration. Phys Rev 141:186–
191
94. Swann WFG (1933) A mechanism of acquirement of cosmic-
ray energies by electrons. Phys Rev 43:217–220
95. Thompson WB (1955) On the acceleration of cosmic-ray par-
ticles by magneto-hydrodynamic waves. Proc Roy Soc Lond
233:402–406
96. Toptygin IN (1980) Acceleration of particles by shocks in
a cosmic plasma. Space Sci Rev 26:157–213
97. Tsytovich VN (1966) Statistical acceleration of particles in
a turbulent plasma. Sov Phys Uspeki 9:370–404
98. Tversko˘ı BA (1967) Contribution to the theory of Fermi statis-
tical acceleration. Sov Phys JETP 25:317–325
99. Tversko˘ı BA (1968) Theory of turbulent acceleration of
charged particles in a plasma. Sov Phys JETP 26:821–828
100. Ucer D, Shapiro VD (2001) Unlimited relativistic shock surfing
acceleration. Phys Rev Lett 87:075001

42 A
Adaptive Visual Servo Control
101. van der Swaluw E, Achterberg A (2004) Non-thermal X-ray
emission from young supernova remnants. Astron Astrophys
421:1021–1030
102. Vedin J, Rönnmark K (2004) A linear auroral current-voltage
relation in fluid theory. Ann Geophys 22:1719–1728
103. Vogt J (2002) Alfvén wave coupling in the auroral current cir-
cuit. Surveys Geophys 23:335–377
104. Völk HJ (2006) Shell-type supernova remnants. astro-ph/
0603502
105. Wentzel DG (1961) Hydromagnetic waves and the trapped ra-
diation Part 1. Breakdown of the adiabatic invariance. J Geo-
phys Res 66:359–369
106. Wentzel DG (1964) Motion across magnetic discontinuities
and Fermi acceleration of charged particles. J Astrophys
140:1013–1024
107. Wiersma J, Achterberg A (2004) Magnetic field genera-
tion in relativistic shocks. An early end of the exponential
Weibel instability in electron-proton plasmas. Astron Astro-
phys 428:365–371
108. Wild JP, Smerd SF, Weiss AA (1963) Solar bursts. Ann Rev As-
tron Astrophys 1:291–366
109. Wygant JR, Keiling A, Cattell CA, Lysak RL, Temerin M, Mozer
FS, Kletzing CA, Scudder JD, Streltsov V, Lotko W, Russell CT
(2002) Evidence for kinetic Alfén waves and parallel electron
energization at 4–6 RE altitudes in the plasma sheet boundary
layer. J Geophys Res 107:A900113
Books and Reviews
Alfvén H, Fälthammar C-G (1963) Cosmical Electrodynamics. Ox-
ford University Press, Oxford
Aschwanden MJ (2004) Physics of the solar corona. Springer, Berlin
Benz AO (1993) Plasma astrophysics: kinetic processes in solar and
stellar coronae. Kluwer Academic Publishers, Dordrecht
Berezinskii VS, Bulanov SV, Dogiel VA, Ginzburg VL, Ptuskin VS
(1990) Astrophysics of cosmic rays. North Holland, Amster-
dam
Dorman LI (2006) Cosmic ray interactions, propagation, and accel-
eration in space plasmas. Springer, New York
Drury LO’C (1983) An introduction to the theory of diffusive shock
acceleration of energetic particles in tenuous plasmas. Rep
Prog Phys 46:973–1027
McLean DJ, Labrum NR (eds) (1986) Solar Radiophysics. Cambridge
University Press, Cambridge
Melrose DB (1980) Plasma Astrophysics, vol I & II. Gordon, New York
Melrose DB (1986) Instabilities in Space and Laboratory Plasmas.
Cambridge University Press
Malkov MA, Drury LO’C (2001) Nonlinear theory of diffusive accel-
eration of particles by shock waves. Rep Prog Phys 64:429–
481
Michel FC (1991) Physics of neutron star magnetospheres. The Uni-
versity of Chicago Press, Chicago
Priest ER, Forbes T (2000) Magnetic reconnection–MHD theory and
applications. Cambridge University Press, Cambridge
Schlickeiser R (2002) Cosmic ray astrophysics. Springer, Berlin
Stone RG, Tsurutani BT (eds) (1985) Collisionless shocks in the helio-
sphere: a tutorial review. Geophysicial Monograph 34. Amer-
ican Geophysical Union, Washington DC
Sturrock PA (1980) Solar flares. Colorado Associated University
Press, Boulder
Svestka Z (1976) Solar Flares. D Reidel, Dordrecht
Vogt J (2002) Alfvén wave coupling in the auroral current circuit.
Surv Geophys 23:335–377
Adaptive Visual Servo Control
GUOQIANG HU1, NICHOLAS GANS2,
WARREN E. DIXON2
1 Department of Mechanical and Nuclear Engineering,
Kansas State University, Manhattan, USA
2 Department of Mechanical and Aerospace Engineering,
University of Florida, Gainesville, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Visual Servo Control Methods
Compensating for Projection Normalization
Visual Servo Control via an Uncalibrated Camera
Future Directions
Bibliography
Glossary
Camera-in-hand conﬁguration
The camera-in-hand conﬁguration refers to the case
when the camera is attached to a moving robotic sys-
tem (e. g., held by the robot end-eﬀector).
Camera-to-hand conﬁguration
The camera-to-hand conﬁguration refers to the case
when the camera is stationary and observing moving
targets (e. g., a ﬁxed camera observing a moving robot
end-eﬀector).
Euclidean reconstruction Euclidean
reconstruction
is
the act of reconstructing the Euclidean coordinates of
feature points based on the two-dimensional image
information obtained from a visual sensor.
Extrinsic calibration parameters The extrinsic calibra-
tion parameters are deﬁned as the relative position and
orientation of the camera reference frame to the world
reference frame (e. g., the frame aﬃxed to the robot
base). The parameters are represented by a rotation
matrix and a translation vector.
Feature point Diﬀerent computer vision algorithms have
been developed to search images for distinguishing
features in an image (e. g., lines, points, corners, tex-
tures). Given a three dimensional Euclidean point, its
projection to the two dimensional image plane is called
a feature point. Feature points are selected based on

Adaptive Visual Servo Control
A
43
contrast with surrounding pixels and the ability for the
point to be tracked from frame to frame. Sharp corners
(e. g., the windows of a building), or center of a ho-
mogenous region in the image (e. g., the center of car
headlights) are typical examples of feature points.
Homography The geometric concept of homography is
a one-to-one and on-to transformation or mapping
between two sets of points. In computer vision, ho-
mography refers to the mapping between points in two
Euclidean-planes (Euclidean homography), or to the
mapping between points in two images (projective ho-
mography).
Image Jacobian The image-Jacobian is also called the in-
teraction matrix, feature sensitivity matrix, or feature
Jacobian. It is a transformation matrix that relates the
change of the pixel coordinates of the feature points on
the image to the velocity of the camera.
Intrinsic calibration parameters The intrinsic calibra-
tion parameters map the coordinates from the nor-
malized Euclidean-space to the image-space. For the
pinhole camera, this invertible mapping includes the
image center, camera scale factors, and camera magni-
ﬁcation factor.
Pose The position and orientation of an object is referred
to as the pose. The pose has a translation component
that is an element of R3 (i. e., Euclidean-space), and
the rotation component is an element of the special or-
thogonal group SO(3), though local mappings of ro-
tation that exist in R3 (Euler angles, angle/axis) or
R4 (unit quaternions). Pose of an object has speciﬁc
meaning when describing the relative position and ori-
entation of one object to another or the position and
orientation of an object at diﬀerent time instances.
Pose is typically used to describe the position and ori-
entation of one reference frame with respect to another
frame.
Projection normalization The Euclidean coordinates of
a point are projected onto a two-dimensional image
via the projection normalization. For a given camera,
a linear projective matrix can be used to map points in
the Euclidean-space to the corresponding pixel coor-
dinates on the image-plane. This relationship is not bi-
jective because there exist inﬁnite number of Euclidean
points that have the same normalized coordinates, and
the depth information is lost during the normalization.
Unit quaternion The quaternion is a four dimensional
vector which is composed of real numbers (i. e.,
q 2 R4). The unit quaternion is a quaternion subject
to a nonlinear constraint that the norm is equal to 1.
The unit quaternion can be used as a globally nonsin-
gular representation of a rotation matrix.
Visual servo control system Control systems that use in-
formation acquired from an imaging source (e. g.,
a digital camera) in the feedback control loop are de-
ﬁned as visual servo control systems. In addition to
providing feedback relating the local position and ori-
entation (i. e., pose) of the camera with respect to some
target, an image sensor can also be used to relate lo-
cal sensor information to an inertial reference frame
for global control tasks. Visual servoing requires mul-
tidisciplinary expertise to integrate a vision system
with the controller for tasks including: selecting the
proper imaging hardware; extracting and processing
images at rates amenable to closed-loop control; im-
age analysis and feature point extraction/tracking; re-
covering/estimating necessary state information from
an image; and operating within the limits of the sensor
range (i. e., ﬁeld-of-view).
Lyapunov function A Lyapunov function is a contin-
uously diﬀerentiable positive deﬁnite function with
a negative deﬁnite or semi-deﬁnite time-derivative.
Definition of the Subject
Control systems that use information acquired from an
imaging source in the feedback loop are deﬁned as visual
servo control systems. Visual servo control has developed
into a large subset of robotics literature because of the en-
abling capabilities it can provide for autonomy. The use of
an image sensor for feedback is motivated by autonomous
task execution in an unstructured or unknown environ-
ments. In addition to providing feedback relating the lo-
cal position and orientation (i. e., pose) of the camera with
respect to some target, an image sensor can also be used
to relate local sensor information to an inertial reference
frame for global control tasks.
However, the use of image-based feedback adds com-
plexity and new challenges for the control system design.
Beyond expertise in design of mechanical control systems,
visual servoing requires multidisciplinary expertise to in-
tegrate a vision system with the controller for tasks includ-
ing: selecting the proper imaging hardware; extracting and
processing images at rates amenable to closed-loop con-
trol; image analysis and feature point extraction/tracking;
recovering/estimating necessary state information from an
image; and operating within the limits of the sensor range
(i. e., ﬁeld-of-view). While each of the aforementioned
tasks are active topics of research interest, the scope of this
chapter is focused on issues associated with using recon-
structed and estimated state information from an image to
develop a stable closed-loop error system. That is, the top-
ics in this chapter are based on the assumption that images

44 A
Adaptive Visual Servo Control
can be acquired, analyzed, and the resulting data can be
provided to the controller without restricting the control
rates.
Introduction
The diﬀerent visual servo control methods can be divided
into three main categories including: image-based, posi-
tion-based, and approaches that use of blend of image and
position-based approaches. Image-based visual servo con-
trol (e. g., [1,2,3,4,5]) consists of a feedback signal that is
composed of pure image-space information (i. e., the con-
trol objective is deﬁned in terms of an image pixel error).
This approach is considered to be more robust to cam-
era calibration and robot kinematic errors and is more
likely to keep the relevant image features in the ﬁeld-of-
view than position-based methods because the feedback
is directly obtained from the image without the need to
transfer the image-space measurement to another space.
A drawback of image-based visual servo control is that
since the controller is implemented in the robot joint
space, an image-Jacobian is required to relate the deriva-
tive of the image-space measurements to the camera’s lin-
ear and angular velocities. However, the image-Jacobian
typically contains singularities and local minima [6], and
the controller stability analysis is diﬃcult to obtain in
the presence of calibration uncertainty [7]. Another draw-
back of image-based methods is that since the controller is
based on image-feedback, the robot could be commanded
to some conﬁguration that is not physically possible. This
issue is described as Chaumette’s conundrum (see [6,8]).
Position-based visual servo control (e. g., [1,5,9,10,11])
uses reconstructed Euclidean information in the feedback
loop. For this approach, the image-Jacobian singularity
and local minima problems are avoided, and physically re-
alizable trajectories are generated. However, the approach
is susceptible to inaccuracies in the task-space reconstruc-
tion if the transformation is corrupted (e. g., uncertain
camera calibration). Also, since the controller does not
directly use the image features in the feedback, the com-
manded robot trajectory may cause the feature points to
leave the ﬁeld-of-view. A review of these two approaches
is provided in [5,12,13].
The third class of visual servo controllers use some
image-space information combined with some recon-
structed information as a means to combine the advan-
tages of these two approaches while avoiding their dis-
advantages (e. g., [8,14,15,16,17,18,19,20,21,22,23]). One
particular approach was coined 2.5D visual servo con-
trol in [15,16,17,18] because this class of controllers ex-
ploits some two dimensional image feedback and recon-
structed three-dimensional feedback. This class of con-
trollers is also called homography-based visual servo con-
trol in [19,20,21,22] because of the underlying reliance of
the construction and decomposition of a homography.
A key issue that is pervasive to all visual servo control
methods is that the image-space is a two-dimensional pro-
jection of the three-dimensional Euclidean-space. To com-
pensate for the lack of depth information from the two-di-
mensional image data, some control methods exploit addi-
tional sensors (e. g., laser and sound ranging technologies)
along with sensor fusion methods or the use of additional
cameras in a stereo conﬁguration that triangulate on cor-
responding images. However, the practical drawbacks of
incorporating additional sensors include: increased cost,
increased complexity, decreased reliability, and increased
processing burden. Geometric knowledge of the target or
scene can be used to estimate depth, but this information
is not available for many unstructured tasks.
Motivated
by
these
practical
constraints,
direct
approximation and estimation techniques (cf. [4,5])
and dynamic adaptive compensation (cf. [19,20,21,22,
24,25,26,27,28]) have been developed. For example, an
adaptive kinematic controller has been developed in [24]
to achieve a uniformly ultimately bounded regulation
control objective provided conditions on the translation
velocity and bounds on the uncertain depth parameters
are satisﬁed. In [24], a three dimensional depth estimation
procedure has been developed that exploits a prediction
error provided a positive deﬁnite condition on the in-
teraction matrix is satisﬁed. In [20,26,27], homography-
based visual servo controllers have been developed to
asymptotically regulate a manipulator end-eﬀector and
a mobile robot, respectively, by developing an adaptive
update law that actively compensates for an unknown
depth parameter. In [19,21], adaptive homography-based
visual servo controllers have been developed to achieve
asymptotic tracking control of a manipulator end-eﬀector
and a mobile robot, respectively. In [22,28], adaptive ho-
mography-based visual servo controllers via a quaternion
formulation have been developed to achieve asymptotic
regulation and tracking control of a manipulator end-
eﬀector, respectively.
In addition to normalizing the three-dimensional co-
ordinates, the projection from the Euclidean-space to
the image-space also involves a transformation matrix
that may contain uncertainties. Speciﬁcally, a camera
model (e. g., the pinhole model) is often required to re-
late pixel coordinates from an image to the (normalized)
Euclidean coordinates. The camera model is typically as-
sumed to be exactly known (i. e., the intrinsic calibra-
tion parameters are assumed to be known); however, de-

Adaptive Visual Servo Control
A
45
spite the availability of several popular calibration meth-
ods (cf. [29,30,31,32,33,34,35]), camera calibration can be
time-consuming, requires some level of expertise, and has
inherent inaccuracies. If the calibration parameters are not
exactly known, performance degradation and potential
unpredictable response from the visual servo controller
may occur.
Motivated by the desire to incorporate robustness
to camera calibration, diﬀerent control approaches that
do not depend on exact camera calibration have been
proposed (cf. [18,36,37,38,39,40,41,42,43,44,45,46,47,48,
49,50,51]). Eﬀorts such as [36,37,38,39,40] have investi-
gated the development of methods to estimate the im-
age and robot manipulator Jacobians. These methods are
composed of some form of recursive Jacobian estimation
law and a control law. Speciﬁcally, a visual servo con-
troller is developed in [36] based on a weighted recur-
sive least-squares update law to estimate the image Jaco-
bian. In [37], a Broyden–Jacobian estimator is applied and
a nonlinear least-square optimization method is used for
the visual servo control development. In [38], the authors
used a nullspace-biased Newton-step visual servo strategy
with a Broyden Jacobian estimation for online singular-
ity detection and avoidance in an uncalibrated visual servo
control problem. In [39,40] a recursive least-squares algo-
rithm is implemented for Jacobian estimation, and a dy-
namic Gauss–Newton method is used to minimize the
squared error in the image plane.
In [41,42,43,44,45,46,47], robust and adaptive visual
servo controllers have been developed. The development
of traditional adaptive control methods to compensate
for the uncertainty in the transformation matrix is in-
hibited because of the time-varying uncertainty injected
in the transformation from the normalization of the Eu-
clidean coordinates. As a result, initial adaptive control re-
sults were limited to scenarios where the optic axis of the
camera was assumed to be perpendicular with the plane
formed by the feature points (i. e., the time-varying uncer-
tainty is reduced to a constant uncertainty). Other adap-
tive methods were developed that could compensate for
the uncertain calibration parameters assuming an addi-
tional sensor (e. g., ultrasonic sensors, laser-based sensors,
additional cameras) could be used to measure the depth
information.
More recent approaches exploit geometric relation-
ships between multiple spatiotemporal views of an ob-
ject to transform the time-varying uncertainty into known
time-varying terms multiplied by an unknown con-
stant [18,48,49,50,51,52]. In [48], an on-line calibration al-
gorithm was developed for position-based visual servoing.
In [49], an adaptive controller was developed for image-
based dynamic control of a robot manipulator. One prob-
lem with methods based on the image-Jacobian is that the
estimated image-Jacobian may contain singularities. The
development in [49] exploits an additional potential force
function to drive the estimated parameters away from the
values that result in a singular Jacobian matrix. In [52],
an adaptive homography-based controller was proposed
to address problems of uncertainty in the intrinsic cam-
era calibration parameters and lack of depth measure-
ments. In Subsect. “Adaptive Homography–Based Visual
Servo Control Approach”, a combined high-gain control
approach and an adaptive control strategy are proposed to
regulate the robot end-eﬀector to a desired pose asymptot-
ically with considering this time-varying scaling factor.
Robust control approaches based on static best-guess
estimation of the calibration matrix have been developed
to solve the uncalibrated visual servo regulation problem
(cf. [18,44,50,51]). Speciﬁcally, under a set of assumptions
on the rotation and calibration matrix, a kinematic con-
troller was developed in [44] that utilizes a constant, best-
guess estimate of the calibration parameters to achieve
local set-point regulation for the six degree-of-freedom
visual servo control problem. Homography-based visual
servoing methods using best-guess estimation are used
in [18,50,51] to achieve asymptotic or exponential regula-
tion with respect to both camera and hand-eye calibration
errors for the six degree-of-freedom problem.
This chapter is organized to provide the basic under-
lying principles of both historic and recent visual servo
control methods with a progression to open research is-
sues due to the loss of information through the image
projection and the disturbances due to uncertainty in the
camera calibration. Section “Visual Servo Control Meth-
ods” provides an overview of the basic methodologies.
This section includes topics such as: geometric relation-
ships and Euclidean reconstruction using the homogra-
phy in Subsect. “Geometric Relationships”; and image-
based visual servo control, position-based visual servo
control, and hybrid visual servo control approaches in
Subsects. “Image-based Visual Servo Control”–“Homog-
raphy–Based Visual Servo Control”. In Sect. “Compen-
sating for Projection Normalization”, an adaptive tech-
nique that actively compensates for unknown depth pa-
rameters in visual servo control is presented. Speciﬁcally,
this adaptive compensation approach has been combined
with the homography-based visual servo control scheme
and image-based visual servo control scheme to generate
asymptotic results in Subsects. “Adaptive Depth Compen-
sation in Homography–Based Visual Servo Control” and
“Adaptive Depth Compensation in Image–Based Visual
Servo Control”, respectively. In Sect. “Visual Servo Con-

46 A
Adaptive Visual Servo Control
trol via an Uncalibrated Camera”, several visual control
approaches that do not require exact camera calibration
knowledge are presented. For example, in Subsect. “Jaco-
bian Estimation Approach”, a visual servo controller that
is based on a Jacobian matrix estimator is described. In
Subsect. “Robust Control Approach”, a robust controller
based on static best-guess estimation of the calibration ma-
trix is developed to solve the regulation problem. In Sub-
sect. “Adaptive Image–Based Visual Servo Control Ap-
proach”, an adaptive controller for image-based dynamic
regulation control of a robot manipulator is described
based on a depth-independent interaction matrix. In Sub-
sect. “Adaptive Homography–Based Visual Servo Control
Approach”, a combined high-gain control approach and
an adaptive control strategy are used to regulate the robot
end-eﬀector to a desired pose based on the homography
technique.
Visual Servo Control Methods
The development in this chapter is focused on the de-
velopment of visual servo controllers for robotic systems.
To develop the controllers, the subsequent development
is based on the assumption that images can be acquired,
analyzed, and the resulting data can be provided to the
controller without restricting the control rates. The sen-
sor data from image-based feedback are feature points.
Feature points are pixels in an image that can be iden-
tiﬁed and tracked between images so that the motion of
the camera/robot can be discerned from the image. Im-
age processing techniques can be used to select copla-
nar and non-collinear feature points within an image. For
simplicity, the development in this chapter is based on
the assumption that four stationary coplanar and non-
collinear feature points [53] denoted by Oi8i D 1, 2, 3, 4
can be determined from a feature point tracking algorithm
(e. g., Kanade–Lucas–Tomasi (KLT) algorithm discussed
in [54,55]).
The plane deﬁned by the four feature points is denoted
by  as depicted in Fig. 1. The assumption that feature
points lie on a plane is not restrictive and does not reduce
the generality of the results. For example, if four coplanar
target points are not available then the subsequent devel-
opment can also exploit methods such as the virtual par-
allax algorithm (e. g., see [16,56]) to create virtual planes
from non-planar feature points.
Geometric Relationships
The camera-in-hand conﬁguration is depicted in Fig. 1. In
Fig. 1, F denotes a coordinate frame that is considered to
be aﬃxed to the single current camera viewing the object,
and a stationary coordinate frame F denotes a constant
(a priori determined) desired camera position and orienta-
tion that is deﬁned as a desired image. The Euclidean coor-
dinates of the feature points Oi expressed in the frames F
and F are denoted by ¯mi(t) 2 R3 and ¯m
i 2 R3, respec-
tively, as
¯mi , xi(t)
yi(t)
zi(t)T ;
¯m
i , x
i
y
i
z
i
T ;
(1)
where xi(t), yi(t), zi(t) 2 R denote the Euclidean coor-
dinates of the ith feature point, and x
i , y
i , z
i 2 R de-
note the Euclidean coordinates of the corresponding fea-
ture points in the desired/reference image. From standard
Euclidean geometry, relationships between ¯mi(t) and ¯m
i
can be determined as
¯mi D x f C R ¯m
i ;
(2)
where R(t) 2 SO(3) denotes the orientation of F with re-
spect to F, and x f (t) 2 R3 denotes the translation vec-
tor from F to F expressed in the coordinate frame F.
The normalized Euclidean coordinate vectors, denoted by
mi(t) 2 R3 and m
i 2 R3, are deﬁned as
mi D mxi
myi
1T ,
h
xi
zi
yi
zi
1
iT
;
(3)
m
i D
h
m
xi
m
yi
1
iT
,
h x
i
z
i
y
i
z
i
1
iT
:
(4)
Based on the Euclidean geometry in Eq. (2), the relation-
ships between mi(t) and m
i can be determined as [53]
mi D
z
i
zi
„ƒ‚…

R C x f
d nT
„
ƒ‚
…
m
i
˛i
H
;
(5)
where ˛i(t) 2 R is a scaling term, and H(t) 2 R33 de-
notes the Euclidean homography. The Euclidean homog-
raphy is composed of: a scaled translation vector which is
equal to the translation vector x f (t) divided by the dis-
tance d 2 R from the origin of F to the plane ; the
rotation between F and F denoted by R(t); and n 2 R3
denoting a constant unit normal to the plane .
Each feature point Oi on  also has a pixel coordinate
pi(t) 2 R3 and p
i 2 R3 expressed in the image coordi-
nate frame for the current image and the desired image
denoted by
pi ,

ui
vi
1
T ;
p
i ,

u
i
v
i
1
T ;
(6)

Adaptive Visual Servo Control
A
47
Adaptive Visual Servo Control, Figure 1
Coordinate frame relationships between a camera viewing a planar patch at different spatiotemporal instances
where ui(t), vi(t), u
i , v
i 2 R. The pixel coordinates pi(t)
and p
i are related to the normalized task-space coor-
dinates mi(t) and m
i by the following global invertible
transformation (i. e., the pinhole camera model)
pi D Ami ;
p
i D Am
i ;
(7)
where A 2 R33 is a constant, upper triangular, and in-
vertible intrinsic camera calibration matrix that is explic-
itly deﬁned as [57]
A ,
2
64
˛
˛ cot '
u0
0
ˇ
sin '
v0
0
0
1
3
75 D
2
4
a11
a12
a13
0
a22
a23
0
0
1
3
5 ; (8)
where a11, a12, a13, a22, a23 2 R. In Eq. (8), u0; v0 2 R
denote the pixel coordinates of the principal point (i. e.,
the image center that is deﬁned as the frame buﬀer co-
ordinates of the intersection of the optical axis with the
image plane), ˛; ˇ 2 R represent the product of the cam-
era scaling factors and the focal length, and ' 2 R is the
skew angle between the camera axes. Based on the physi-
cal meaning, the diagonal calibration elements are positive
(i. e., a11; a22 > 0). Based on Eq. (7), the Euclidean rela-
tionship in Eq. (5) can be expressed in terms of the image
coordinates as
pi D
˛i

AHA1
„ ƒ‚ … p
i
G
;
(9)
where G(t) 2 R33 denotes the projective homography.
Based on the feature point pairs p
i ; pi(t) with
i D 1; 2; 3; 4, the projective homography can be deter-
mined up to a scalar multiple (i. e., G(t)
g33(t) where g33(t) 2 R
denotes the bottom right element of G(t)) [19]. Various
methods can then be applied (e. g., see [58,59]) to decom-
pose the Euclidean homography to obtain the rotation ma-
trix R(t), scaled translation
x f (t)
d , normal vector n, and the
depth ratio ˛i(t).
Image-Based Visual Servo Control
As described previously, image-based visual servo con-
trol uses pure image-space information (i. e., pixel coor-
dinates) in the control development. The image error for
the feature point Oi between the current and desired poses

48 A
Adaptive Visual Servo Control
is deﬁned as
eii(t) D
ui(t)  u
i
vi(t)  v
vi

2 R2 :
The derivative of eii(t) can be written as (e. g., see [3,4,5])
˙eii D Lii
vc
!c

;
(10)
where Lii(mi; zi) 2 R2n6 is deﬁned as
Lii D
a11 a12
0
a22


 1
zi
0
mxi
zi
mximyi


1 C m2
xi

myi
0
 1
zi
myi
zi
1 C m2
yi
mximyi
mxi

;
and vc(t); !c(t) 2 R3 are linear and angular velocities of
the camera, respectively, expressed in the camera coordi-
nate frame F. If the image error is deﬁned based on the
normalized pixel coordinates as
eii(t) D
"
mxi(t)  m
xi
myi(t)  m
yi
#
2 R2
then the derivative of eii(t) can be written as (e. g.,
see [13])
˙eii D
 1
zi
0
mxi
zi
mximyi


1 C m2
xi

myi
0
 1
zi
myi
zi
1 C m2
yi
mximyi
mxi


vc
!c

:
(11)
The image error for n (n  3) feature points is deﬁned as
ei(t) D eT
i1(t)
eT
i2(t)
  
eT
in(t)T 2 R2n :
Based on Eq. (10), the open-loop error system for ei(t) can
be written as
˙ei D Li
vc
!c

;
(12)
where the matrix
Li(t) D

LT
i1(t)
LT
i2(t)
  
LT
in(t)
T 2 R2n6
is called the image Jacobian, interaction matrix, feature
sensitivity matrix, or feature Jacobian (cf. [2,3,4,5]). When
the image Jacobian matrix is nonsingular at the desired
pose (i. e., ei(t) D 0), and the depth information zi(t) is
available, a locally exponentially result (i. e., ei(t) ! 0 ex-
ponentially) can be obtained by using the controller
vc
!c

D kiLC
i ei ;
(13)
where ki 2 R is a positive control gain, and LC
i (t) is the
pseudo inverse of Li(t). See [13] for details regarding the
stability analysis.
In the controller Eq. (13), zi(t) is unknown. Direct
approximation and estimation techniques (cf. [4,5]) have
been developed to estimate zi(t). For example, an approx-
imate estimate of Li(t) using constant z
i at the desired
pose has been used in [4]. Besides the direct approxima-
tion and estimation approaches, dynamic adaptive com-
pensation approaches (cf. [19,20,21,22,24,25,26,27,28])
have also been developed recently to compensate for this
unknown depth information, which will be represented in
Sect. “Compensating for Projection Normalization”.
Position–Based Visual Servo Control
The feedback loop in position-based visual servo control
(e. g., [1,5,9,10,11]) uses Euclidean pose information that is
reconstructed from the image information. Various meth-
ods can be applied (cf. [5,60,61]) to compute the pose esti-
mate based on the image information and a priori geomet-
ric knowledge of the environment. The resulting outcome
of these methods are the translation vector x f (t) and rota-
tion matrix R(t) between the current and desired poses of
the robot end-eﬀector. The rotation matrix can be further
decomposed as a rotation axis u(t) 2 R3 and a rotation an-
gle (t) 2 R.
The pose error ep(t) 2 R6 can be deﬁned as
ep(t) D
x f
u

;
where the corresponding open-loop error system can then
be determined as
˙ep D Lp
vc
!c

;
(14)
where Lp(t) 2 R66 is deﬁned as
Lp D
R
0
0
L!

:
(15)
In Eq. (15), L!(u(t); (t)) 2 R33 is deﬁned as [15]
L! D I3  
2 [u] C
0
@1 
sinc()
sinc2 

2

1
A [u]2
 :
(16)

Adaptive Visual Servo Control
A
49
Based on the open-loop error system in Eq. (14), an exam-
ple position-based visual servo controller can be designed
as vc
!c

D kpL1
p ep
(17)
provided that Lp(t) is nonsingular, where kp 2 R is a pos-
itive control gain. The controller in Eq. (17) yields an
asymptotically stable result in the sense that ep(t) asymp-
totically converges to zero. See [13] for details regarding
the stability analysis.
A key issue with position-based visual servo control
is the need to reconstruct the Euclidean information to
ﬁnd the pose estimate from the image. Given detailed ge-
ometric knowledge of the scene (such as a CAD model)
approaches such as [5,60,61] can deliver pose estimates.
When geometric knowledge is not available, homogra-
phy-based Euclidean reconstruction is also a good option.
Given a set of feature points as stated in Subsect. “Geo-
metric Relationships” and based on Eqs. (5) and (7), the
homography H(t) between the current pose and desired
pose can be obtained if the camera calibration matrix A
is known. Various methods can then be applied (e. g.,
see [58,59]) to decompose the homography to obtain the
rotation matrices R(t) and the scaled translation vector
x f (t)
d . If
x f (t)
d
is used in the control development, then ei-
ther d should be estimated directly or adaptive control
techniques would be required to compensate for/identify
the unknown scalar.
Homography–Based Visual Servo Control
The 2.5D visual servo control approach developed in [15,
16,17,18] decomposes the six-DOF motion into transla-
tion and rotation components and uses separate trans-
lation and rotation controllers to achieve the asymptotic
regulation. This class of controllers is also called homog-
raphy-based visual servo control in [19,20,21,22] because
of the underlying reliance of the construction and decom-
position of a homography.
The translation error signal ev(t) 2 R3 can be chosen
as a combination of image information (or normalized Eu-
clidean information) and reconstructed Euclidean infor-
mation as
ev D
h
mxi  m
xi
myi  m
yi
ln

zi
z
i
iT
;
(18)
where the current and desired Euclidean coordinates are
related to the chosen feature point Oi. The rotation error
signal e!(t) 2 R3 is typically chosen in terms of the recon-
structed rotation parameterization as
e! D u :
The corresponding translation and rotation error system
can be written as
˙ev D Lvvc C Lv!!c
(19)
˙e! D L!!c ;
(20)
where L!(t) was deﬁned in Eq. (16), and Lv(t); Lv!(t) 2
R33 are deﬁned as
Lv D  ˛i
z
i
2
4
1
0
mxi
0
1
myi
0
0
1
3
5 ;
Lv! D
2
4
mximyi
1  m2
xi
myi
1 C m2
yi
mxi myi
mxi
myi
mxi
0
3
5 :
(21)
The rotation and translation controllers can be designed
as
!c D kL1
! e! D ke!
vc D L1
v (kev  kLv!e!)
(22)
to yield an asymptotically stable result.
There are some other control approaches that com-
bine image-space and reconstructed Euclidean informa-
tion in the error system such as [8,23]. The partitioned
visual servo control approach proposed in [8] decouples
the z-axis motions (including both the translation and ro-
tation components) from the other degrees of freedom and
derives separate controllers for these z-axis motions. The
switched visual servo control approach proposed in [23]
partitions the control along the time axis, instead of the
usual partitioning along spatial degrees of freedom. This
system uses both position-based and image-based con-
trollers, and a high-level decision maker invokes the ap-
propriate controller based on the values of a set of Lya-
punov functions.
Simulation Comparisons
The controllers given in Eqs. (13), (17), and (22) are im-
plemented based on the same initial conditions. The errors
asymptotically converge for all three controllers. The dif-
ferent image-space and Euclidean trajectories in the simu-
lation are shown in Fig. 2.
Subplots a–c of Fig. 2 are two-dimensional image-
space trajectories of the four feature points generated by
the controllers Eqs. (13), (17), and (22), respectively. The
trajectories start from the initial positions and asymptot-
ically converge to the desired positions. A comparison of
the results indicate that the image-based visual servo con-
trol and homography-based visual servo control have bet-
ter performance in the image-space (see subplots a and c).

50 A
Adaptive Visual Servo Control
Adaptive Visual Servo Control, Figure 2
Simulation results for image-based, position-based, and combined visual servo control approaches
Also, since the position-based visual servo controller does
not use image-features in the feedback, the feature points
may leave the camera’s ﬁeld-of-view (see subplot b).
Subplots d–f of Fig. 2 are the three-dimensional Eu-
clidean trajectories of the camera generated by the con-
trollers Eqs. (13), (17), and (22), respectively. Since the po-
sition-based visual servo control uses the Euclidean infor-
mation directly in the control loop, it has the best perfor-
mance in the Euclidean trajectory (subplot e), followed by
the homography-based controller (subplot f). The image-
based visual servo controller in subplot d generates the
longest Euclidean trajectory. Since image-based visual
servo controller doesn’t guarantee the Euclidean perfor-
mance, it may generate a physically unimplementable tra-
jectory.
Compensating for Projection Normalization
A key issue that impacts visual servo control is that the
image-space is a two-dimensional projection of the three-
dimensional Euclidean-space. To compensate for the lack
of depth information from the two-dimensional image
data, some researchers have focused on the use of alter-
nate sensors (e. g., laser and sound ranging technologies).
Other researchers have explored the use of a camera-based
vision system in conjunction with other sensors along with
sensor fusion methods or the use of additional cameras in
a stereo conﬁguration that triangulate on corresponding
images. However, the practical drawbacks of incorporat-
ing additional sensors include: increased cost, increased
complexity, decreased reliability, and increased processing
burden. Recently, some researchers have developed adap-
tive control methods that can actively adapt for the un-
known depth parameter (cf. [19,20,21,22,24,25,26,27,28])
along with methods based on direct estimation and ap-
proximation (cf. [4,5]).
Adaptive Depth Compensation
in Homography–Based Visual Servo Control
As in [19,20,21,22,26,27,28], homography-based visual
servo controllers have been developed to achieve asymp-
totic regulation or tracking control of a manipulator end-
eﬀector or a mobile robot, by developing an adaptive up-
date law that actively compensates for an unknown depth
parameter. The open-loop error system and control devel-
opment are discussed in this section.

Adaptive Visual Servo Control
A
51
Open–Loop Error System
The translation error signal
ev(t) is deﬁned as in Subsect “Homography–Based Visual
Servo Control”. However, the rotation error signal is de-
ﬁned as a unit quaternion q(t) , q0(t)
qT
v (t)T 2 R4
where q0(t) 2 R and qv(t) 2 R3 (e. g., see [22,28]). The
corresponding translation and rotation error systems are
determined as
z
i ˙ev D ˛i ¯Lvvc C z
i Lv!!c
(23)
and
˙q0
˙qv

D 1
2

qT
v
q0I3 C q
v

!c ;
(24)
where the notation q
v (t) denotes the skew-symmetric
form of the vector qv(t) (e. g., see [22,28]), and ¯Lv(t) 2
R33 is deﬁned as
¯Lv D
2
4
1
0
mxi
0
1
myi
0
0
1
3
5 :
(25)
The structure of Eq. (25) indicates that ¯Lv(t) is measurable
and can be used in the controller development directly.
Controller and Adaptive Law Design
The rotation
and translation controllers can be developed as (e. g.,
see [15,19,22,28])
!c D K!qv ;
(26)
vc D 1
˛i
¯L1
v
Kvev C ˆz
i Lv!!c
 ;
(27)
where K!; Kv 2 R33 denote two diagonal matrices of
positive constant control gains. In Eq. (27), the parameter
estimate ˆz
i (t) 2 R is developed for the unknown constant
z
i , and is deﬁned as (e. g., see [19,22,28])
˙ˆz
i D ieT
v Lv!!c ;
(28)
where i 2 R denotes a positive constant adaptation gain.
Based on Eqs. (23)–(28), the closed-loop error system is
obtained as
˙q0 D 1
2qT
v K!qv ;
(29)
˙qv D 1
2

q0I3 C q
v

K!qv ;
(30)
z
i ˙ev D Kv ev C ˜ziLv!!c ;
(31)
where ˜zi(t) 2 R denotes the following parameter estima-
tion error:
˜zi D z
i  ˆz
i :
(32)
Closed–Loop Stability Analysis
The controller given in
Eqs. (26) and (27) along with the adaptive update law in
Eq. (28) ensures asymptotic translation and rotation regu-
lation in the sense that
kqv(t)k ! 0
and
kev(t)k ! 0
as
t ! 1 : (33)
To prove the stability result, a Lyapunov function candi-
date V(t) 2 R can be deﬁned as
V D z
i
2 eT
v ev C qT
v qv C (1  q0)2 C 1
2i
˜z2
i :
(34)
The time-derivative of V(t) can be determined as
˙V D eT
v (Kv ev C ˜ziLv!!c)  qT
v

q0I3 C q
v

K!qv
 (1  q0)qT
v K!qv  ˜zi eT
v Lv!!c ;
(35)
where Eqs. (29)-(32) were utilized. The following nega-
tive semi-deﬁnite expression is obtained after simplifying
Eq. (35):
˙V D eT
v Kvev  qT
v K!qv :
(36)
Barbalat’s Lemma [62] can then be used to prove the result
given in Eq. (33).
Based on the controller in Eq. (26) and (27) and the
adaptive law in Eq. (28), the resulting asymptotic transla-
tion and rotation errors are plotted in Fig. 3 and Fig. 4, re-
spectively. The image-space trajectory is shown in Fig. 5,
and also in Fig. 6 in a three-dimensional format, where
the vertical axis is time. The parameter estimate for z
i is
shown in Fig. 7.
Adaptive Depth Compensation
in Image–Based Visual Servo Control
This section provides an example of how an image-based
visual servo controller can be developed that adaptively
compensates for the uncertain depth. In this section, only
three feature points are required to be tracked in the image
to develop the error system (i. e., to obtain an image-Jaco-
bian with at least equal rows than columns), but at least
four feature points are required to solve for the unknown
depth ratio ˛i(t).
Open–Loop Error System
The image error ei(t) deﬁned
in Subsect. “Image-based Visual Servo Control” can be

52 A
Adaptive Visual Servo Control
Adaptive Visual Servo Control, Figure 3
Unitless translation error ev(t)
Adaptive Visual Servo Control, Figure 4
Quaternion rotation error q(t)
rewritten as
ei(t) D
2
66666664
mx1(t)  m
x1
my1(t)  m
y1
mx2(t)  m
x2
my2(t)  m
y2
mx3(t)  m
x3
my3(t)  m
y3
3
77777775
,
2
6666664
ex1
ey1
ex2
ey2
ex3
ey3
3
7777775
;
(37)
when three feature points are used for notation simplicity.
The deﬁnition of image error ei(t) is expandable to more
than three feature points. The time derivative of ei(t) is
given by
˙ei(t) D Li
vc
!c

;
(38)
Adaptive Visual Servo Control, Figure 5
Image-space error in pixels between p(t) and p. In the figure,
“O” denotes the initial positions of the 4 feature points in the
image, and “*” denotes the corresponding final positions of the
feature points
Adaptive Visual Servo Control, Figure 6
Image-space error in pixels between p(t) and p shown in a 3D
graph. In the figure, “O” denotes the initial positions of the 4 fea-
ture points in the image, and “*” denotes the corresponding final
positions of the feature points
where the image Jacobian Li(t) is given by
Li D
2
66666666664
˛1
z
1
0
˛1mx1
z
1
mx1my1
1 C m2
x1
my1
0
˛1
z
1
˛1my1
z
1
1  m2
y1
mx1my1
mx1
˛2
z
2
0
˛2mx2
z
2
mx2my2
1 C m2
x2
my2
0
˛2
z
2
˛2my2
z
2
1  m2
y2
mx2my2
mx2
˛3
z
3
0
˛3mx3
z
3
mx3my3
1 C m2
x3
my3
0
˛3
z
3
˛3my3
z
3
1  m2
y3
mx3my3
mx3
3
77777777775
;
(39)

Adaptive Visual Servo Control
A
53
Adaptive Visual Servo Control, Figure 7
Adaptive on-line estimate of z
i
where the depth ratio ˛i(t); i D 1; 2; 3 is deﬁned in (5). Let
 D 1
2
3
T 2 R3 denote the unknown constant
parameter vector as
 D
h 1
z
1
1
z
2
1
z
3
iT
:
(40)
Even though the error system (38) is not linearly parame-
terizable in , an adaptive law can still be developed based
on the following property in Paragraph “Controller and
Adaptive Law Design”.
Controller and Adaptive Law Design
Based on an
open-loop error system as in Eq. (38), the controller is de-
veloped as
vc
!c

D ki ˆLT
i ei ;
(41)
where ki 2 R is the control gain,
ˆ(t) D  ˆ1(t) ˆ2(t) ˆ3(t)
T 2 R3
is the adaptive estimation of the unknown constant pa-
rameter vector deﬁned in Eq. (40), and ˆLi(t) 2 R66 is an
estimation of the image Jacobian matrix as
ˆLi D
2
66666664
˛1 ˆ1
0
˛1mx1 ˆ1
mx1my1
1 C m2
x1
my1
0
˛1 ˆ1
˛1my1 ˆ1
1  m2
y1
mx1my1
mx1
˛2 ˆ2
0
˛2mx2 ˆ2
mx2my2
1 C m2
x2
my2
0
˛2 ˆ2
˛2my2 ˆ2
1  m2
y2
mx2my2
mx2
˛3 ˆ3
0
˛3mx3 ˆ3
mx3my3
1 C m2
x3
my3
0
˛3 ˆ3
˛3my3 ˆ3
1  m2
y3
mx3my3
mx3
3
77777775
:
(42)
The adaptive law for ˆ(t) in Eq. (42) is designed as
˙ˆ D  eM ˆLi M ˆLT
i ei ;
(43)
where  2 R33 is a positive diagonal matrix, eM(t) 2
R36 and ˆLi M(t) 2 R66 are deﬁned as
eM D
2
4
ex1
ey1
0
0
0
0
0
0
ex2
ey2
0
0
0
0
0
0
ex3
ey3
3
5
ˆLi M D
2
6666664
˛1
0
mx1˛1
0
0
0
0
˛1
my1˛1
0
0
0
˛2
0
mx2˛2
0
0
0
0
˛2
my2˛2
0
0
0
˛3
0
mx3˛3
0
0
0
0
˛3
my3˛3
0
0
0
3
7777775
:
The adaptive law is motivated by the following two prop-
erties:
Li  ˆLi D ˜Li D ˚M ˆLi M ;
(44)
eT
i ˚M ˆLi M D ˜TeM ˆLi M :
(45)
In Eqs. (44) and (45), the adaptive estimation error ˜(t) 2
R3 and ˚M(t) 2 R66 are deﬁned as
˜ D  ˜1
˜2
˜3
T
D

1  ˆ1
2  ˆ2
3  ˆ3
T ;
˚M D diag
˚ ˜1; ˜1; ˜2; ˜2; ˜3; ˜3

:
Under the controller Eq. (41) and the adaptive law
Eq. (43), the closed-loop error system can be written as
˙ei D kiLi ˆLT
i ei D ki ˆLi ˆLT
i ei  ki ˜Li ˆLT
i ei
D ki ˆLi ˆLT
i ei  ki˚M ˆLi M ˆLT
i ei : (46)
Closed–Loop Stability Analysis
The controller given in
Eq. (41) and the adaptive update law in Eq. (43) ensures
asymptotic regulation in the sense that
kei(t)k ! 0
as
t ! 1 :
(47)
To prove the stability result, a Lyapunov function candi-
date V(t) 2 R can be deﬁned as
V D 1
2 eT
i ei C 1
2 ki ˜ 1 ˜ :
The time-derivative of V(t) can be determined as
˙V D eT
i ˙ei  ki ˜ 1 ˙ˆ
D eT
i
ki ˆLi ˆLT
i ei  ki˚M ˆLi M ˆLT
i ei

 ki ˜ 1  eM ˆLi Mei

D ki eT
i ˆLi ˆLT
i ei  kieT
i ˚M ˆLi M ˆLT
i ei C ki ˜eM ˆLi M ˆLT
i ei

54 A
Adaptive Visual Servo Control
Based on Eqs. (44) and (45)
eT
i ˚M ˆLi M ˆLT
i D ˜TeM ˆLi M ˆLT
i ;
and ˙V(t) can be further simpliﬁed as
˙V D kieT
i ˆLi ˆLT
i ei  ki ˜eM ˆLi M ˆLT
i ei C ki ˜eM ˆLi M ˆLT
i ei
D kieT
i ˆLi ˆLT
i ei:
Barbalat’s Lemma [62] can then be used to conclude the
result given in Eq. (47).
Visual Servo Control via an Uncalibrated Camera
Motivated by the desire to incorporate robustness to cam-
era calibration, diﬀerent control approaches (e. g., Jaco-
bian estimation-based least-square optimization, robust
control, and adaptive control) that do not depend on ex-
act camera calibration have been proposed (cf. [18,36,37,
38,39,40,41,42,43,44,45,46,47,48,49,50,51,63]).
Jacobian Estimation Approach
The uncalibrated visual servo controllers that are based on
a Jacobian matrix estimator have been developed in [36,37,
38,39,40]. The camera calibration matrix and kinematic
model is not required with the Jacobian matrix estima-
tion. This type of visual servo control scheme is composed
of a recursive Jacobian estimation law and a control law.
Speciﬁcally, in [36], a weighted recursive least-square up-
date law is used for Jacobian estimation, and a visual
servo controller is developed via the Lyapunov approach.
In [37], a Broyden Jacobian estimator is applied for Ja-
cobian estimation and a nonlinear least-square optimiza-
tion method is used for the visual servo controller de-
velopment. In [39,40] a recursive least-squares algorithm
is implemented for Jacobian estimation, and a dynamic
Gauss–Newton method is used to minimize the squared
error in the image plane to get the controller.
Let the robot end-eﬀector features y() 2 Rm be
a function of the robot joint angle vector (t) 2 Rn, and
let the target features y(t) 2 Rm be a function of time.
The tracking error in the image-plane can then be deﬁned
as
f (; t) D y()  y(t) 2 Rm :
The trajectory (t) that causes the end-eﬀector to follow
the target is established by minimizing the squared image
error
F(; t) D 1
2 f T(; t)f (; t) :
By using the dynamic Gauss–Newton method, the joint
angle vector update law (controller) can be designed
as [40]
kC1 D k  ˆJT
k ˆJk
1 ˆJT
k

fk C @fk
@t ht

;
(48)
where ht is the increment of t. In Eq. (48), the esti-
mated Jacobian ˆJ(t), can be determined by multiple re-
cursive least-square Jacobian estimation algorithms (e. g.,
Broyden method [37], dynamic Broyden method [40],
or an exponentially weighted recursive least-square algo-
rithm [40]).
Robust Control Approach
Robust control approaches based on static best-guess es-
timation of the calibration matrix have been developed
to solve the uncalibrated visual servo regulation problem
(cf. [18,44,50,51]). Speciﬁcally, a kinematic controller was
developed in [44] that utilizes a constant, best-guess es-
timate of the calibration parameters to achieve local set-
point regulation for the six-DOF problem; although, sev-
eral conditions on the rotation and calibration matrix are
required. Model-free homography-based visual servoing
methods using best-guess estimation are used in [18,50] to
achieve exponential or asymptotic regulation with respect
to both camera and hand-eye calibration errors for the six-
DOF problem. A quaternion-based robust controller is de-
veloped in [51] that achieved a regulation control objec-
tive.
State Estimation
Let ˆA 2 R33 be a constant best-guess
estimate of the camera calibration matrix A. The normal-
ized coordinate estimates, denoted by ˆmi(t); ˆm
i 2 R3, can
be expressed as
ˆmi D ˆA1pi D ˜Ami ;
(49)
ˆm
i D ˆA1p
i D ˜Am
i ;
(50)
where the upper triangular calibration error matrix
˜A 2 R33 is deﬁned as
˜A D ˆA1A :
(51)
Since mi(t) and m
i can not be exactly determined, the es-
timates in Eqs. (49) and (50) can be substituted into Eq. (5)
to obtain the following relationship:
ˆmi D ˛i ˆH ˆm
i ;
(52)
where ˆH(t) 2 R33 denotes the estimated Euclidean ho-
mography [18] deﬁned as
ˆH D ˜AH ˜A1 :
(53)

Adaptive Visual Servo Control
A
55
Since ˆmi(t) and ˆm
i can be determined from Eqs. (49) and
(50), a set of twelve linear equations can be developed from
the four image point pairs, and Eq. (52) can be used to
solve for ˆH(t). If additional information (e. g., at least four
vanishing points) is provided (see [64] for a description
of how to determine vanishing points in an image), var-
ious techniques (e. g., see [58,59]) can be used to decom-
pose ˆH(t) to obtain the estimated rotation and translation
components as
ˆH D ˜AR ˜A1 C ˜AxhnT ˜A1 D ˆR C ˆxh ˆnT ;
(54)
where ˆR(t) 2 R33 and ˆxh(t); ˆn 2 R3 denote the esti-
mate of R(t), xh(t) and n, respectively, as [18]
ˆR D ˜AR ˜A1 ;
(55)
ˆxh D  ˜Axh ;
(56)
ˆn D 1

˜ATn ;
(57)
where  2 R denotes the positive constant
 D
 ˜ATn :
(58)
Based on Eq. (55), the rotation matrix estimate ˆR(t) is
similar to R(t). By exploiting the properties of similar ma-
trices (i. e., similar matrices have the same trace and eigen-
values), the following estimates can be determined as [18]
ˆ D  ;
ˆu D  ˜Au ;
where ˆ(t) 2 R and ˆu(t) 2 R3 denote the estimates of (t)
and u(t), respectively, and  2 R is deﬁned as
 D
1
 ˜Au
 :
Alternatively, if the rotation matrix is decomposed as
a unit quaternion. The quaternion estimate
ˆq(t) ,
ˆq0(t)
ˆqT
v (t)
T 2 R4
can be related to the actual quaternion q(t) as [51]
ˆq0 D q0 ;
ˆqv D ˙ ˜Aqv ;
(59)
where
 D kqvk
 ˜Aqv
 :
(60)
Since the sign ambiguity in Eq. (59) does not aﬀect the
control development and stability analysis [51], only the
positive sign in Eq. (59) needs to be considered in the fol-
lowing control development and stability analysis.
Control Development and Stability Analysis
The rota-
tion open-loop error system can be developed by taking
the time derivative of q(t) as
 ˙q0
˙qv

D 1
2

qT
v
q0I3 C q
v

!c :
(61)
Based on the open-loop error system in (61) and the sub-
sequent stability analysis, the angular velocity controller is
designed as
!c D k! ˆqv;
(62)
where k! 2 R denotes a positive control gain. Substitut-
ing Eq. (62) into Eq. (61), the rotation closed-loop error
system can be developed as
˙q0 D 1
2 k!qT
v ˆqv
(63)
˙qv D 1
2 k!
q0I3 C q
v
 ˆqv :
(64)
The open-loop translation error system can be deter-
mined as [50]
˙e D  1
z
i
vc  !
c e C m
i
 !c ;
(65)
where the translation error e(t) 2 R3 is deﬁned as
e D zi
z
i
mi  m
i :
The translation controller is designed as (see also [18,50])
vc D kv ˆe ;
(66)
where kv 2 R denotes a positive control gain, and ˆe(t) 2
R3 is deﬁned as
ˆe D zi
z
i
ˆmi  ˆm
i :
(67)
In Eq. (67), ˆmi(t) and ˆm
i can be computed from Eq. (49)
and (50), respectively, and the ratio (zi(t))/z
i can be com-
puted from the decomposition of the estimated Euclidean
homography in Eq. (52).
After substituting Eqs. (62) and (66) into Eq. (65), the
resulting closed-loop translation error system can be de-
termined as
˙e D

kv
1
z
i
˜A C

k! ˆqv


e  k!

m
i
 ˆqv :
(68)
The controller given in Eqs. (62) and (66) ensures asymp-
totic regulation in the sense that
kqv(t)k ! 0 ;
ke(t)k ! 0
as
t ! 1
(69)

56 A
Adaptive Visual Servo Control
Adaptive Visual Servo Control, Figure 8
Unitless translation error between m1(t) and m
1
provided kv is selected suﬃciently large, and the following
inequalities are satisﬁed (cf. [18,50,51])
min
 1
2
 ˜A C ˜AT
 0;
max
 1
2
 ˜A C ˜AT
 1;
where 0; 1 2 R are positive constants, and minfg and
maxfg denote the minimal and maximal eigenvalues of
1/2( ˜A C ˜AT), respectively.
To prove the stability result, a Lyapunov function can-
didate V(t) 2 R can be deﬁned as
V D qT
v qv C (1  q0)2 C eTe :
(70)
The time derivative of Eq. (70) can be proven to be nega-
tive semi-deﬁnite as [51]
˙V   k!0
Kv1
kqvk2  0
z
i
kek2 ;
(71)
where Kv1 is a positive large enough number. Using the
signal chasing argument in [51], Barbalat’s Lemma [62]
can then be used to prove the result given in Eq. (69).
Based on the controller in Eqs. (62) and (66), the result-
ing asymptotic translation and rotation errors are plotted
in Fig. 8 and Fig. 9, respectively.
Adaptive Image–Based
Visual Servo Control Approach
In [49], an adaptive controller for image-based dynamic
regulation control of a robot manipulator is proposed us-
ing a ﬁxed camera whose intrinsic and extrinsic parame-
ters are not known. A novel depth-independent interac-
tion matrix (cf. [49,65,66]) is proposed therein to map the
Adaptive Visual Servo Control, Figure 9
Quaternion rotation error q(t)
visual signals onto the joints of the robot manipulator. Due
to the depth-independent interaction matrix, the closed-
loop dynamics can be linearly parameterized in the con-
stant unknown camera parameters. An adaptive law can
then be developed to estimate these parameters online.
Open–Loop Error System
Denote the homogeneous
coordinate of the feature point expressed in the robot base
frame as
¯x(t) D

x(t)
y(t)
z(t)
1
T 2 R4 ;
and denote its coordinate in the camera frame as
c ¯x(t) D cx(t)
c y(t)
cz(t)T 2 R3 :
The current and desired positions of the feature point on
the image plane are respectively deﬁned as
p(t) D u(t)
v(t)
1T 2 R3
and
pd D ud
vd
1T 2 R3 :
The image error between the current and desired posi-
tions of the feature point on the image plane, denoted as
ei(t) 2 R3, is deﬁned as
ei D p  pd D

u
v
1
T 

ud
vd
1
T :
(72)
Under the camera perspective projection model, p(t) can
be related to ¯x(t) as
p D 1
cz M ¯x(t) ;
(73)
where M 2 R34 is a constant perspective projection ma-
trix that depends only on the intrinsic and extrinsic cal-
ibration parameters. Based on Eq. (73), the depth of the

Adaptive Visual Servo Control
A
57
feature point in the camera frame is given by
cz(t) D mT
3 ¯x(t) ;
(74)
where m3 2 R14 is the third row of M. Based on
Eqs. (72)–(74), the derivative of image error ˙ei(t) is related
to ˙¯x(t) as
˙ei D ˙p D 1
cz

M  pmT
3
 ˙¯x :
(75)
The robot kinematics are given by
˙¯x D J(q)˙q ;
(76)
where q(t) 2 Rn denotes the joint angle of the manipu-
lator, and J(q) 2 R4n denotes the manipulator Jacobian.
Based on Eqs. (75) and (76),
˙ei D 1
cz

M  pmT
3

J(q)˙q :
(77)
The robot dynamics are given by
H(q)¨q C
1
2
˙H(q) C C(q; ˙q)

˙q C g(q) D  :
(78)
In Eq. (78), H(q) 2 Rnn is the positive-deﬁnite and sym-
metric inertia matrix, C(q; ˙q) 2 Rnn is a skew-symmet-
ric matrix, g(q) 2 Rn denotes the gravitational force, and
(t) 2 Rn is the joint input of the robot manipulator.
Controller and Adaptive Law Design
As stated in Prop-
erty 2 of [49], the perspective projection matrix M can only
be determined up to a scale, so it is reasonable to ﬁx one
component of M and treat the others as unknown constant
parameters. Without loss of generality, let m34 be ﬁxed,
and deﬁne the unknown parameter vector as
 D
[m11 m12 m13 m14 m21 m22 m23 m24 m31 m32 m33]T
2 R11 :
To design the adaptive law, the estimated projection error
of the feature point is deﬁned as
e(tj; t) D p(tj) ˆmT
3 (t)¯x(tj)  ˆM(t)¯x(tj)
D W ¯x(tj); p(tj) ˜
(79)
at time instant tj. In Eq. (79), W
¯x(tj); p(tj)

2 R311
is a constant regression matrix and the estimation error
˜(t) 2 R11 is deﬁned as ˜(t) D   ˆ(t) with ˆ(t) 2 R11
equal to the estimation of . The controller can then be
designed as [49]
 D g(q)  K1 ˙q  JT(q)

ˆMT  ˆm3pT C 1
2 ˆm3ei

Bei ;
(80)
where K1 2 Rnn and B 2 R33 are positive-deﬁnite gain
matrices. The adaptive law is designed as
˙ˆ D  1
YT(q; p)˙q
C ˙ m
jD1WT ¯x(tj); p(tj) K3e(tj; t)

;
(81)
where  2 R1111 and K3 2 R33 are positive-deﬁnite
gain matrices, and the regression matrix Y(q; p) is deter-
mined as
 JT(q)
  ˆMT  ˆm3pT  MT C m3pT
C 1
2 ( ˆm3  m3) eT
i

Bei D Y(q; p) ˜ :
As stated in [49], if the ﬁve positions (corresponding to
ﬁve time instant tj; j D 1; 2; : : : ; 5) of the feature point for
the parameter adaptation are so selected that it is impossi-
ble to ﬁnd three collinear projections on the image plane,
under the control of the proposed controller Eq. (80) and
the adaptive algorithm Eq. (81) for parameter estimation,
then the image error of the feature point is convergent to
zero, i. e., kei(t)k ! 0 as t ! 1 (see [49] for the closed-
loop stability analysis).
Adaptive Homography–Based Visual Servo Control
Approach
Problem Statement and Assumptions
In this section,
a combined high-gain control approach and an adaptive
control strategy are used to regulate the robot end-eﬀec-
tor to a desired pose asymptotically. A homography-based
visual servo control approach is used to address the six-
DOF regulation problem. A high-gain controller is devel-
oped to asymptotically stabilize the rotation error system
by exploiting the upper triangular form of the rotation er-
ror system and the fact that the diagonal elements of the
camera calibration matrix are positive. The translation er-
ror system can be linearly parameterized in terms of some
unknown constant parameters which are determined by
the unknown depth information and the camera calibra-
tion parameters. An adaptive controller is developed to
stabilize the translation error by compensating for the un-
known depth information and intrinsic camera calibration
parameters. A Lyapunov-based analysis is used to examine
the stability of the developed controller.
The following development is based on two mild as-
sumptions.
Assumption 1: The bounds of a11 and a22 (deﬁned in Sub-
sect. “Geometric Relationships”) are assumed to be known
as
a11 < a11 < a11 ;
a22 < a22 < a22:
(82)

58 A
Adaptive Visual Servo Control
The absolute values of a12, a13, and a23 are upper bounded
as
ja12j < a12 ;
ja13j < a13 ;
ja23j < a23 :
(83)
In (82) and (83), a11, a11, a22, a22, a12, a13 and a23 are
positive constants.
Assumption 2: The reference plane is within the camera
ﬁeld-of-view and not at inﬁnity. That is, there exist posi-
tive constants zi and zi such that
zi < zi(t) < zi:
(84)
Euclidean Reconstruction Using
Vanishing Points
Based on Eqs. (5)–(7), the homography relationship based
on measurable pixel coordinates is
pi D ˛iAHA1p
i :
(85)
Since A is unknown, standard homography computation
and decomposition algorithms can’t be applied to extract
the rotation and translation from the homography. If some
additional information can be applied (e. g., four vanishing
points), the rotation matrix can be obtained. For the van-
ishing points, d D 1, so that
H D R C x f
d nT D R:
(86)
Based on Eq. (86), the relationship in Eq. (85) can be ex-
pressed as
pi D ˛i ¯Rp
i ;
(87)
where ¯R(t) 2 R33 is deﬁned as
¯R D ARA1 :
(88)
For the four vanishing points, twelve linear equations can
be obtained based on Eq. (87). After normalizing ¯R(t) by
one nonzero element (e. g., ¯R33(t) 2 R which is assumed
to be the third row third column element of ¯R(t) without
loss of generality) twelve equations can be used to solve for
twelve unknowns. The twelve unknowns are given by the
eight unknown elements of the normalized ¯R(t), denoted
by ¯Rn(t) 2 R33 deﬁned as
¯Rn ,
¯R
¯R33
;
(89)
and the four unknowns are given by ¯R33(t)˛i(t). From the
deﬁnition of ¯Rn(t) in Eq. (89), the fact that
det( ¯R) D det(A) det(R) det(A1) D 1
(90)
can be used to conclude that
¯R3
33 det( ¯Rn) D 1 ;
(91)
and hence,
¯R D
¯Rn
3p
det( ¯Rn)
:
(92)
After ¯R(t) is obtained, the original four feature points on
the reference plane can be used to determine the depth ra-
tio ˛i(t).
Open–Loop Error System
Rotation Error System
If the rotation matrix R(t) intro-
duced in (5) were known, then the corresponding unit
quaternion q(t) , q0(t)
qT
v (t)T can be calculated us-
ing the numerically robust method presented in [28]
and [67] based on the corresponding relationships
R(q) D

q2
0  qT
v qv

I3 C 2qvqT
v  2q0q
v ;
(93)
where I3 is the 3  3 identity matrix, and the notation
q
v (t) denotes the skew-symmetric form of the vector
qv(t). Given R(t), the quaternion q(t) can also be written
as
q0 D 1
2
p
1 C tr(R) ;
qv D 1
2u
p
3  tr(R) ;
(94)
where u(t) 2 R3 is a unit eigenvector of R(t) with respect
to the eigenvalue 1. The open-loop rotation error system
for q(t) can be obtained as [68]
 ˙q0
˙qv

D 1
2

qT
v
q0I3 C q
v

!c ;
(95)
where !c(t) 2 R3 deﬁnes the angular velocity of the cam-
era expressed in F.
The quaternion q(t) given in Eq. (93)–(95) is not mea-
surable since R(t) is unknown. However, since ¯R(t) can be
determined as described in Eq. (92), the same algorithm as
shown in Eq. (94) can be used to determine a correspond-
ing measurable quaternion
¯q0(t); ¯qT
v (t)
T as
¯q0 D 1
2
p
1 C tr( ¯R) ;
¯qv D 1
2 ¯u
p
3  tr( ¯R) ;
(96)
where ¯u(t) 2 R3 is a unit eigenvector of ¯R(t) with re-
spect to the eigenvalue 1. Based on Eq. (88), tr( ¯R) D
tr(ARA1) D tr(R), where tr() denotes the trace of a ma-
trix. Since R(t) and ¯R(t) are similar matrices, the relation-
ship between q0(t); qT
v (t)T and ¯q0(t); ¯qT
v (t)T can be de-
termined as
¯q0 D q0 ;
¯qv D kqvk
kAqvkAqv , Aqv ;
(97)

Adaptive Visual Servo Control
A
59
where (t) 2 R is a positive, unknown, time-varying
scalar.
The square of (t) is
2 D
qT
v qv
(Aqv)TAqv
D
qT
v qv
qTv ATAqv
:
(98)
Since A is of full rank, the symmetric matrix ATA is pos-
itive deﬁnite. Hence, the Rayleigh–Ritz theorem can be
used to conclude that
min(ATA)qT
v qv  qT
v ATAqv  max(ATA)qT
v qv : (99)
From Eqs. (98) and (99), it can be concluded that
1
max(ATA)  2 D
qT
v qv
(Aqv)TAqv

1
min(ATA) :
s
1
max(ATA)   
s
1
min(ATA) :
(100)
Based on Eq. (100), there exist positive bounding constants
,  2 R that satisfy the following inequalities:
 < (t) <  :
(101)
The inverse of the relationship between ¯qv(t) and qv(t) in
Eq. (97) can also be developed as
qv D 1
 A1 ¯qv
D 1

2
64
1
a11 ¯qv1 
a12
a11a22 ¯qv2 

a13
a11  a12a23
a11a22

¯qv3
1
a22 ¯qv2  a23
a22 ¯qv3
¯qv3
3
75 :
(102)
Translation Error System
The translation error, denoted
by e(t) 2 R3, is deﬁned as
e(t) D pe(t)  p
e ;
(103)
where pe(t), p
e 2 R3 are deﬁned as
pe D

ui
vi
 ln(˛i)
T ;
p
e D

u
i
v
i
0
T ;
(104)
where i 2 f1;    ; 4g. The translation error e(t) is measur-
able since the ﬁrst two elements are image coordinates,
and ˛i(t) is obtained from the homography decomposi-
tion. The open-loop translation error system can be ob-
tained by taking the time derivative of e(t) and multiplying
the resulting expression by z
i as
z
i ˙e D ˛iAevc C z
i Ae

A1pi
 !c ;
(105)
where vc(t) 2 R3 deﬁnes the linear velocity of the camera
expressed in F, and Ae(t) 2 R33 is deﬁned as
Ae D
2
4
a11
a12
a13  ui
0
a22
a23  vi
0
0
1
3
5 :
Rotation Controller Development and Stability Analy-
sis
Based on the relationship in Eq. (97), the open-loop
error system in Eq. (95), and the subsequent stability anal-
ysis, the rotation controller is designed as
!c1 D k!1 ¯qv1 D (k!11 C 2)¯qv1
!c2 D k!2 ¯qv2 D (k!21 C k!22 C 1)¯qv2
!c3 D k!3 ¯qv3 D (k!31 C k!32 C k!33)¯qv3 ;
(106)
where k!i 2 R; i D 1; 2; 3 and k!i j 2 R; i; j D 1; 2; 3;
j < i, are positive constants. The expressed form of the
controller in Eq. (106) is motivated by the use of com-
pleting the squares in the subsequent stability analysis. In
Eq. (106), the damping control gains k!21; k!31; k!32 are
selected according to the following suﬃcient conditions to
facilitate the subsequent stability analysis
k!21 > 1
4 k2
!1

2
a12
a11a22
;
k!31 > 1
4 k2
!1
1
a11
 
a12a23
a22
C a13
!2
;
k!32 > 1
4 k2
!2

2
a23
a22
;
(107)
where a11, a11, a22, a22, a12, a13 and a23 are deﬁned
in Eqs. (82) and (83), and k!11, k!22, k!33 are feedback
gains that can be selected to adjust the performance of the
rotation control system.
Provided the suﬃcient gain conditions given in
Eq. (107) are satisﬁed, the controller in Eq. (106) ensures
asymptotic regulation of the rotation error in the sense
that
kqv(t)k ! 0 ;
as
t ! 1 :
(108)
To prove the stability result, a Lyapunov function candi-
date V1(t) 2 R can be deﬁned as
V1 , qT
v qv C (1  q0)2 :
(109)
Based on the open-loop error system in Eq. (95), the time-
derivative of V1(t) can be determined as
˙V1 D 2qT
v ˙qv  2(1  q0)˙q0
D qT
v !c D qv1!c1 C qv2!c2 C qv3!c3 :
(110)

60 A
Adaptive Visual Servo Control
After substituting Eq. (102) for
qv(t), substituting
Eq. (106) for !c(t), and completing the squares, the ex-
pression in Eq. (110) can be written as
˙V1 <  1


k!11
1
a11
¯q2
v1 C k!22
1
a22
¯q2
v2 C k!33 ¯q2
v3

:
(111)
Barbalat’s lemma [62] can now be used to conclude that
kqv(t)k ! 0 as t ! 1.
Translation Control Development and Stability Anal-
ysis
After some algebraic manipulation, the translation
error system in Eq. (105) can be rewritten as
z
i
a11
˙e1 D ˛ivc1 C Y1(˛i; ui; vi; !c; vc2; vc3)1
z
i
a22
˙e2 D ˛ivc2 C Y2(˛i; ui; vi; !c; vc3)2
z
i ˙e3 D ˛ivc3 C Y3(˛i; ui; vi; !c)3 ;
(112)
where 1 2 Rn1, 2 2 Rn2, and 3 2 Rn3 are vectors of
constant unknown parameters, and Y1() 2 R1n1, Y2() 2
R1n2, and Y3() 2 R1n3 are known regressor vectors.
The control strategy used in [52] is to design vc3(t)
to stabilize e3(t), and then design vc2(t) to stabilize e2(t)
given vc3(t), and then design vc1(t) to stabilize e1(t) given
vc3(t) and vc2(t). Following this design strategy, the trans-
lation controller vc(t) is designed as [52]
vc3 D 1
˛i
(kv3e3 C Y3(˛i; ui; vi; !c) ˆ3)
vc2 D 1
˛i
(kv2e2 C Y2(˛i; ui; vi; !c; vc3) ˆ2)
vc1 D 1
˛i
(kv1e1 C Y1(˛i; ui; vi; !c; vc2; vc3) ˆ1) ;
(113)
where the depth ratio ˛i(t) > 08t. In (113), ˆ1(t)
2
Rn1; ˆ2(t) 2 Rn2; ˆ3(t) 2 Rn3 denote adaptive estimates
that are designed according to the following adaptive up-
date laws to cancel the respective terms in the subsequent
stability analysis
˙ˆ1 D 1YT
1 e1 ;
˙ˆ2 D 2YT
2 e2 ;
˙ˆ3 D 3YT
3 e3 ; (114)
where 1 2 Rn1n1; 2 2 Rn2n2; 3 2 Rn3n3 are diago-
nal matrices of positive constant adaptation gains. Based
on Eqs. (112) and (113), the closed-loop translation error
system is
z
i
a11
˙e1 D kv1e1 C Y1(˛i; ui; vi; !c; vc2; vc3) ˜1
z
i
a22
˙e2 D kv2e2 C Y2(˛i; ui; vi; !c; vc3) ˜2
z
i ˙e3 D kv3e3 C Y3(˛i; ui; vi; !c) ˜3 ;
(115)
where ˜1(t) 2 Rn1; ˜2(t) 2 Rn2; ˜3(t) 2 Rn3 denote the
intrinsic calibration parameter mismatch deﬁned as
˜1(t) D 1  ˆ1(t) ;
˜2(t) D 2  ˆ2(t) ;
˜3(t) D 3  ˆ3(t) :
The controller given in Eq. (113) along with the adap-
tive update law in Eq. (114) ensures asymptotic regulation
of the translation error system in the sense that
ke(t)k ! 0
as
t ! 1 :
To prove the stability result, a Lyapunov function candi-
date V2(t) 2 R can be deﬁned as
V2 D 1
2
z
i
a11
e2
1 C 1
2
z
i
a22
e2
2 C 1
2z
i e2
3 C 1
2
˜T
1  1
1
˜1
C 1
2
˜T
2  1
2
˜2 C 1
2
˜T
3  1
3
˜3 :
(116)
After taking the time derivative of Eq. (116) and sub-
stituting for the closed-loop error system developed in
Eq. (115), the following simpliﬁed expression can be ob-
tained:
˙V2 D kv1e2
1  kv2e2
2  kv3e2
3 :
(117)
Barbalat’s lemma [62] can now be used to show that
e1(t); e2(t); e3(t) ! 0 as t ! 1.
The controller given in Eqs. (106) and (113) along with
the adaptive update law in Eq. (114) ensures asymptotic
translation and rotation regulation in the sense that
kqv(t)k ! 0
and
ke(t)k ! 0
as
t ! 1 ;
provided the control gains satisfy the suﬃcient conditions
given in Eq. (107). Based on the controller in Eqs. (106)
and (113) and the adaptive law in Eq. (114), the resulting
asymptotic translation and rotation errors are plotted in
Figs. 10 and 11, respectively. The image-space trajectory is
shown in Fig. 12, and it is also shown in Fig. 13 in a three
dimensional format.

Adaptive Visual Servo Control
A
61
Adaptive Visual Servo Control, Figure 10
Unitless translation error e(t)
Adaptive Visual Servo Control, Figure 11
Quaternion rotation error q(t)
Future Directions
Image-based visual servo control is problematic because it
may yield impossible Euclidean trajectories for the robotic
system, and the image-Jacobian can contain singularities.
Position-based visual servo control also has deﬁcits in that
the feedback signal does not exploit the image information
directly, so the resulting image-trajectory may take the fea-
ture points out of the ﬁeld-of-view. Homography-based
visual servo control combines the positive aspects of the
former methods; however, this approach requires the ho-
mography construction and decomposition. The homog-
raphy construction is sensitive to image noise and there
exists a sign ambiguity in the homography decomposition.
Open problems that are areas of future research include
Adaptive Visual Servo Control, Figure 12
Image-space error in pixels between p(t) and p. In the figure,
“O” denotes the initial positions of the 4 feature points in the
image, and “*” denotes the corresponding final positions of the
feature points
Adaptive Visual Servo Control, Figure 13
Image-space error in pixels between p(t) and p shown in a 3D
graph. In the figure, “O” denotes the initial positions of the 4 fea-
ture points in the image, and “*” denotes the corresponding final
positions of the feature points
new methods to combine image-based and reconstructed
information in the feedback loop without the homogra-
phy decomposition, perhaps using methods that directly
servo on the homography feedback without requiring the
homography decomposition.
The success of visual servo control for traditional
robotic systems provides an impetus for other au-
tonomous systems such as the navigation and control of
unmanned air vehicles (UAV). For new applications, new
hurdles may arise. For example, the inclusion of the dy-
namics in traditional robotics applications is a straight-

62 A
Adaptive Visual Servo Control
forward task through the use of integrator backstepping.
However, when a UAV is maneuvering (including roll,
pitch, yaw motions) to a desired pose, the feature points
may be required to leave the ﬁeld-of-view because of the
unique ﬂight dynamics. Other examples include systems
that are hyper-redundant, such as robotic elephant trunks,
where advanced path-planning must be incorporated with
the controller to account for obstacle avoidance and main-
taining a line-of-sight to a target.
Bibliography
1. Weiss LE, Sanderson AC, Neuman CP (1987) Dynamic sensor-
based control of robots with visual feedback. IEEE J Robot Au-
tom RA-3:404–417
2. Feddema J, Mitchell O (1989) Vision-guided servoing with
feature-based trajectory generation. IEEE Trans Robot Autom
5:691–700
3. Hashimoto K, Kimoto T, Ebine T, Kimura H (1991) Manipula-
tor control with image-based visual servo. Proc IEEE Int Conf
Robot Autom, pp 2267–2272
4. Espiau B, Chaumette F, Rives P (1992) A new approach to visual
servoing in robotics. IEEE Trans Robot Autom 8:313–326
5. Hutchinson S, Hager G, Corke P (1996) A tutorial on visual servo
control. IEEE Trans Robot Autom 12:651–670
6. Chaumette F (1998) Potential problems of stability and con-
vergence in image-based and position-based visual servoing.
In: Kriegman D, Hager G, Morse A (eds), The confluence of vi-
sion and control LNCIS series, vol 237. Springer, Berlin, pp 66–
78
7. Espiau B (1993) Effect of camera calibration errors on visual ser-
voing in robotics. 3rd Int Symp. Exp Robot, pp 182–192
8. Corke P, Hutchinson S (2001) A new partitioned approach to
image-based visual servo control. IEEE Trans Robot Autom
17:507–515
9. Wilson WJ, Hulls CW, Bell GS (1996) Relative end-effector con-
trol using cartesian position based visual servoing. IEEE Trans
Robot Autom 12:684–696
10. Martinet P, Gallice J, Khadraoui D (1996) Vision based control
law using 3D visual features. Proc World Autom Cong, pp 497–
502
11. Daucher N, Dhome M, Lapresté J, Rives G (1997) Speed com-
mand of a robotic system by monocular pose estimate. Proc
Int Conf Intell Robot Syst, pp 55–62
12. Hashimoto K (2003) A review on vision-based control of robot
manipulators. Adv Robot 17:969–991
13. Chaumette F, Hutchinson S (2006) Visual servo control part i:
Basic approaches. IEEE Robot Autom Mag 13:82–90
14. Deguchi K (1998) Optimal motion control for image-based vi-
sual servoing by decoupling translation and rotation. Proc Int
Conf Intell Robot Syst, pp 705–711
15. Malis E, Chaumette F, Bodet S (1999) 2 1/2D visual servoing.
IEEE Trans Robot Autom 15:238–250
16. Malis E, Chaumette F (2000) 2 1/2D visual servoing with respect
to unknown objects through a new estimation scheme of cam-
era displacement. Int J Comput Vis 37:79–97
17. Chaumette F, Malis E (2000) 2 1/2D visual servoing: a possi-
ble solution to improve image-based and position-based vi-
sual servoings. Proc IEEE Int Conf Robot Autom, pp 630–635
18. Malis E, Chaumette F (2002) Theoretical improvements in the
stability analysis of a new class of model-free visual servoing
methods. IEEE Trans Robot Autom 18:176–186
19. Chen J, Dawson DM, Dixon WE, Behal A (2005) Adaptive ho-
mography-based visual servo tracking for a fixed camera con-
figuration with a camera-in-hand extension. IEEE Trans Control
Syst Technol 13:814–825
20. Fang Y, Dixon WE, Dawson DM, Chawda P (2005) Homog-
raphy-based visual servoing of wheeled mobile robots. IEEE
Trans Syst Man Cybern – Part B: Cybern 35:1041–1050
21. Chen J, Dixon WE, Dawson DM, McIntyre M (2006) Homogra-
phy-based visual servo tracking control of a wheeled mobile
robot. IEEE Trans Robot 22:406–415
22. Hu G, Gupta S, Fitz-coy N, Dixon WE (2006) Lyapunov-based
visual servo trackingcontrol via a quaternion formulation. Proc
IEEE Conf Decision Contr, pp 3861–3866
23. Gans N, Hutchinson S (2007) Stable visual servoing through hy-
brid switched-system control. IEEE Trans Robot 23:530–540
24. Conticelli F, Allotta B (2001) Nonlinear controllability and sta-
bility analysis of adaptive image-based systems. IEEE Trans
Robot Autom 17:208–214
25. Conticelli F, Allota B (2001) Discrete-time robot visual feedback
in 3-D positioning tasks with depth adaptation. IEEE/ASME
Trans Mechatron 6:356–363
26. Fang Y, Behal A, Dixon WE, Dawson DM (2002) Adaptive 2.5D
visual servoing of kinematically redundant robot manipula-
tors. Proc IEEE Conf Decision Contr, pp 2860–2865
27. Fang Y, Dawson DM, Dixon WE, de Queiroz MS (2002) 2.5D vi-
sual servoing of wheeled mobile robots. Proc IEEE Conf Deci-
sion Contr, pp 2866–2871
28. Hu G, Dixon WE, Gupta S, Fitz-coy N (2006) A quaternion for-
mulation for homography-based visual servo control. Proc IEEE
Int Conf Robot Autom, pp 2391–2396
29. Tsai RY (1987) A versatile camera calibration technique for
high-accuracy 3D machine vision metrology using off-the-
shelf tv cameras and lenses. IEEE J Robot Autom 3:323–344
30. Tsai RY (1989) Synopsis of recent progress on camera calibra-
tion for 3D machine vision. MIT Press, Cambridge
31. Robert L (1996) Camera calibration without feature extraction.
Comput Vis Image Underst 63:314–325
32. Heikkila J, Silven O (1997) A four-step camera calibration pro-
cedure with implicit image correction. Proc IEEE Conf Comput
Vis Pattern Recogn, pp 1106–1112
33. Clarke TA, Fryer JG (1998) The development of camera calibra-
tion methods and models. Photogrammetric Rec 16:51–66
34. Sturm PF, Maybank SJ (1999) On plane-based camera calibra-
tion: A general algorithm, singularities, applications. Proc IEEE
Conf Comput Vis Pattern Recogn, pp 432–437
35. Zhang Z (1999) Flexible camera calibration by viewing a plane
from unknown orientations. Proc IEEE Int Conf Comput Vis,
pp 666–673
36. Hosoda K, Asada M (1994) Versatile visual servoing without
knowledge of true jacobian. Proc IEEE/RSJ Int Conf Intell Robot
Syst, pp 186–193
37. Jagersand M, Fuentes O, Nelson R (1997) Experimental evalua-
tion of uncalibratedvisual servoing for precision manipulation.
Proc Int Conf Robot Autom, pp 2874–2880
38. Shahamiri M, Jagersand M (2005) Uncalibrated visual servoing
using a biased newton method for on-line singularity detec-
tion and avoidance. Proc IEEE/RSJ Int Conf Intell Robot Syst,
pp 3953–3958

Additive Cellular Automata
A
63
39. Piepmeier JA, Lipkin H (2003) Uncalibrated eye-in-hand visual
servoing. Int J Robot Res 22:805–819
40. Piepmeier JA, McMurray GV, Lipkin H (2004) Uncalibrated dy-
namic visual servoing. IEEE Trans Robot Autom 24:143–147
41. Kelly R (1996) Robust asymptotically stable visual servoing of
planar manipulator. IEEE Trans Robot Autom 12:759–766
42. Bishop B, Spong MW (1997) Adaptive calibration and control
of 2D monocular visual servo system. Proc IFAC Symp Robot
Contr, pp 525–530
43. Hsu L, Aquino PLS (1999) Adaptive visual tracking with uncer-
tain manipulator dynamics and uncalibrated camera. Proc IEEE
Conf Decision Contr, pp 1248–1253
44. Taylor CJ, Ostrowski JP (2000) Robust vision-based pose con-
trol. Proc IEEE Int Conf Robot Autom, pp 2734–2740
45. Zergeroglu E, Dawson DM, de Queiroz M, Behal A (2001) Vi-
sion-based nonlinear tracking controllers in the presence of
parametric uncertainty. IEEE/ASME Trans Mechatron 6:322–
337
46. Dixon WE, Dawson DM, Zergeroglu E, Behal A (2001) Adaptive
tracking control of a wheeled mobile robot via an uncalibrated
camera system. IEEE Trans Syst Man Cybern – Part B: Cybern
31:341–352
47. Astolfi A, Hsu L, Netto M, Ortega R (2002) Two solutions to
the adaptive visual servoing problem. IEEE Trans Robot Autom
18:387–392
48. Ruf A, Tonko M, Horaud R, Nagel H-H (1997) Visual track-
ing of an end-effector by adaptive kinematic prediction. Proc
IEEE/RSJ Int Conf Intell Robot Syst, pp 893–898
49. Liu Y, Wang H, Wang C, Lam K (2006) Uncalibrated visual ser-
voing of robots using a depth-independent interaction matrix.
IEEE Trans Robot 22:804–817
50. Fang Y, Dixon WE, Dawson DM, Chen J (2003) An exponential
class of model-free visual servoing controllers in the presence
of uncertain camera calibration. Proc IEEE Conf Decision Contr,
pp 5390–5395
51. Hu G, Gans N, Dixon WE (2007) Quaternion-based visual servo
control in the presence of camera calibration error. Proc IEEE
Multi-Conf Syst Contr, pp 1492–1497
52. Chen J, Behal A, Dawson DM, Dixon WE (2003) Adaptive visual
servoing in the presence of intrinsic calibration uncertainty.
Proc IEEE Conf Decision Contr, pp 5396–5401
53. Faugeras O (1993) Three-dimensional computer vision: A geo-
metric viewpoint. MIT Press, Cambridge
54. Shi J, Tomasi C (1994) Good features to track. Proc IEEE Conf
Comput Vis Pattern Recogn, pp 593–600
55. Tomasi C, Kanade T (1991) Detection and tracking of point fea-
tures. Technical report. Carnegie Mellon University, Pittsburgh
56. Boufama B, Mohr R (1995) Epipole and fundamental matrix es-
timation using virtual parallax. Proc IEEE Int Conf Comput Vis,
pp (1030)–1036
57. Hartley R, Zisserman A (2000) Multiple view geometry in com-
puter vision. Cambridge University Press, New York
58. Faugeras O, Lustman F (1988) Motion and structure from mo-
tion in a piecewise planar environment. Int J Pattern Recogn
Artif Intell 2:485–508
59. Zhang Z, Hanson AR (1995) Scaled euclidean 3D reconstruc-
tion based on externally uncalibrated cameras. IEEE Symp
Comput Vis, pp 37–42
60. DeMenthon D, Davis L (1995) Model-based object pose in 25
lines of code. Int J Comput Vis 15:123–141
61. Quan L, Lan Z-D (1999) Linear n-point camera pose determina-
tion. IEEE Trans Pattern Anal Machine Intell 21:774–780
62. Slotine JJ, Li W (1991) Applied nonlinear control. Prentice Hall,
Inc., Englewood Cliff
63. Zachi A, Hsu L, Ortega R, Lizarralde F (2006) Dynamic control
of uncertain manipulators through immersion and invariance
adaptive visual servoing. Int J Robot Res 25:1149–1159
64. Almansa A, Desolneux A, Vamech S (2003) Vanishing point de-
tection without any a priori information. IEEE Trans Pattern
Analysis and Machine Intelligence 25:502–507
65. Liu Y, Wang H, Lam K (2005) Dynamic visual servoing of robots
in uncalibratedenvironments. Proc IEEE Int Conf Robot Autom,
pp 3142–3148
66. Liu Y, Wang H, Zhou D (2006) Dynamic tracking of manipula-
tors using visual feedback from an uncalibrated fixed camera.
Proc IEEE Int Conf Robot Autom, pp 4124–4129
67. Shuster M (1993) A survey of attitude representations. J Astron
Sci 41:439–518
68. Dixon WE, Behal A, Dawson DM, Nagarkatti S (2003) Nonlinear
control of engineering systems: A Lyapunov–based approach.
Birkhäuser, Boston
Additive Cellular Automata
BURTON VOORHEES
Center for Science, Athabasca University,
Athabasca, Canada
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Notation and Formal Deﬁnitions
Additive Cellular Automata in One Dimension
d-Dimensional Rules
Future Directions
Bibliography
Glossary
Cellular automata Cellular automata are dynamical sys-
tems that are discrete in space, time, and value. A state
of a cellular automaton is a spatial array of discrete
cells, each containing a value chosen from a ﬁnite al-
phabet. The state space for a cellular automaton is the
set of all such conﬁgurations.
Alphabet of a cellular automaton The alphabet of a cel-
lular automaton is the set of symbols or values that
can appear in each cell. The alphabet contains a dis-
tinguished symbol called the null or quiescent symbol,
usually indicated by 0, which satisﬁes the condition of
an additive identity: 0 C x D x.

64 A
Additive Cellular Automata
Cellular automata rule The rule, or update rule of a cel-
lular automaton describes how any given state is trans-
formed into its successor state. The update rule of a cel-
lular automaton is described by a rule table, which de-
ﬁnes a local neighborhood mapping, or equivalently as
a global update mapping.
Additive cellular automata An additive cellular automa-
ton is a cellular automaton whose update rule satisﬁes
the condition that its action on the sum of two states is
equal to the sum of its actions on the two states sepa-
rately.
Linear cellular automata A linear cellular automaton is
a cellular automaton whose update rule satisﬁes the
condition that its action on the sum of two states sep-
arately equals action on the sum of the two states plus
its action on the state in which all cells contain the qui-
escent symbol. Note that some researchers reverse the
deﬁnitions of additivity and linearity.
Neighborhood The neighborhood of a given cell is the set
of cells that contribute to the update of value in that cell
under the speciﬁed update rule.
Rule table The rule table of a cellular automaton is a list-
ing of all neighborhoods together with the symbol that
each neighborhood maps to under the local update
rule.
Local maps of a cellular automaton The local mapping
for a cellular automaton is a map from the set of all
neighborhoods of a cell to the automaton alphabet.
State transition diagram The state transition diagram
(STD) of a cellular automaton is a directed graph with
each vertex labeled by a possible state and an edge di-
rected from a vertex x to a vertex y if and only if the
state labeling vertex x maps to the state labeling vertex
y under application of the automaton update rule.
Transient states A transient state of a cellular automaton
is a state that can at most appear only once in the evo-
lution of the automaton rule.
Cyclic states A cyclic state of a cellular automaton is
a state lying on a cycle of the automaton update rule,
hence it is periodically revisited in the evolution of the
rule.
Basins of attraction The basins of attraction of a cellular
automaton are the equivalences classes of cyclic states
together with their associated transient states, with two
states being equivalent if they lie on the same cycle of
the update rule.
Predecessor state A state x is the predecessor of a state y if
and only if x maps to y under application of the cellular
automaton update rule. More speciﬁcally, a state x is an
nth order predecessor of a state y if it maps to y under
n applications of the update rule.
Garden-of-Eden A Garden-of-Eden state is a state that
has no predecessor. It can be present only as an initial
condition.
Surjectivity A mapping is surjective (or onto) if every
state has a predecessor.
Injectivity A mapping is injective (one-to-one) if every
state in its domain maps to a unique state in its range.
That is, if states x and y both map to a state z then
x D y.
Reversibility A mapping X is reversible if and only if
a second mapping X1 exists such that if X(x) D y
then X1(y) D x. For ﬁnite state spaces reversibility
and injectivity are identical.
Definition of the Subject
Cellular automata are discrete dynamical systems in which
an extended array of symbols from a ﬁnite alphabet is it-
eratively updated according to a speciﬁed local rule. Orig-
inally developed by John von Neumann [1,2] in 1948, fol-
lowing suggestions from Stanislaw Ulam, for the purpose
of showing that self-replicating automata could be con-
structed. Von Neumann’s construction followed a compli-
cated set of reproduction rules but later work showed that
self-reproducing automata could be constructed with only
simple update rules, e. g. [3]. More generally, cellular au-
tomata are of interest because they show that highly com-
plex patterns can arise from the application of very sim-
ple update rules. While conceptually simple, they provide
a robust modeling class for application in a variety of dis-
ciplines, e. g. [4], as well as fertile grounds for theoretical
research. Additive cellular automata are the simplest class
of cellular automata. They have been extensively studied
from both theoretical and practical perspectives.
Introduction
A wide variety of cellular automata applications, in a num-
ber of diﬀering disciplines, has appeared in the past ﬁfty
years, see, e. g. [5,6,7,8]. Among other things, cellular au-
tomata have been used to model growth and aggrega-
tion processes [9,10,11,12]; discrete reaction-diﬀusion sys-
tems [13,14,15,16,17]; spin exchange systems [18,19]; bi-
ological pattern formation [20,21]; disease processes and
transmission [22,23,24,25,26]; DNA sequences, and gene
interactions [27,28]; spiral galaxies [29]; social interac-
tion networks [30]; and forest ﬁres [31,32]. They have
been used for language and pattern recognition [33,34,
35,36,37,38,39]; image processing [40,41]; as parallel
computers [42,43,44,45,46,47]; parallel multipliers [48];
sorters [49]; and prime number sieves [50].

Additive Cellular Automata
A
65
In recent years, cellular automata have become impor-
tant for VLSI logic circuit design [51]. Circuit designers
need “simple, regular, modular, and cascadable logic cir-
cuit structure to realize a complex function” and cellular
automata, which show a signiﬁcant advantage over lin-
ear feedback shift registers, the traditional circuit building
block, satisfy this need (see [52] for an extensive survey).
Cellular automata, in particular additive cellular automata,
are of value for producing high-quality pseudorandom
sequences [53,54,55,56,57]; for pseudoexhaustive and de-
terministic pattern generation [58,59,60,61,62,63]; for sig-
nature analysis [64,65,66]; error correcting codes [67,68];
pseudoassociative memory [69]; and cryptography [70].
In this discussion, attention focuses on the subclass of
additive cellular automata. These are the simplest cellular
automata, characterized by the property that the action of
the update rule on the sum of two states is equal to the
sum of the rule acting on each state separately. Hybrid
additive rules (i. e., with diﬀerent cells evolving according
to diﬀerent additive rules) have proved particularly use-
ful for generation of pseudorandom and pseudoexhaustive
sequences, signature analysis, and other circuit design ap-
plications, e. g. [52,71,72].
The remainder of this article is organized as fol-
lows: Sect. “Notation and Formal Deﬁnitions” introduces
deﬁnitions and notational conventions. In Sect. “Addi-
tive Cellular Automata in One Dimension”, considera-
tion is restricted to one-dimensional rules. The inﬂuence
of boundary conditions on the evolution of one-dimen-
sional rules, conditions for rule additivity, generation of
fractal space-time outputs, equivalent forms of rule rep-
resentation, injectivity and reversibility, transient lengths,
and cycle periods are discussed using several approaches.
Taking X as the global operator for an additive cellu-
lar automata, a method for analytic solution of equations
of the form X() D ˇ is described. Section “d-Dimen-
sional Rules” describes work on d-dimensional rules de-
ﬁned on tori. The discrete baker transformation is deﬁned
and used to generalize one-dimensional results on tran-
sient lengths, cycle periods, and similarity of state tran-
sition diagrams. Extensive references to the literature are
provided throughout and a set of general references is pro-
vided at the end of the bibliography.
Notation and Formal Definitions
Let S(L) D fsig be the set of lattice sites of a d-dimensional
lattice L with nr equal to the number of lattice sites on di-
mension r. Denote by A a ﬁnite symbols set with jAj D p
(usually prime). An A-conﬁguration on L is a surjective
map v : A 7! S(L) that assigns a symbol from A to each
site in S(L). In this way, every A-conﬁguration deﬁnes
a size n1      nd, d-dimensional matrix  of symbols
drawn from A. Denote the set of all A-conﬁgurations on
L by E(A; L).
Each si 2 S(L) is labeled by an integer vector Ei D
(i1; : : : ; id) where ir is the number of sites along the rth
dimension separating si from the assigned origin in L.
The shift operator on the rth dimension of L is the map
r : L 7! L deﬁned by
r(si) D sj ;
Ej D (i1; : : : ; ir  1; : : : ; id) :
(1)
Equivalently, the shift maps the value at site Ei to the value
at site Ej.
Let (si; t) D (i1; : : : ; id; t) 2 A be the entry of 
corresponding to site si at iteration t for any discrete dy-
namical system having E(A; L) as state space. Given a ﬁ-
nite set of integer d-tuples N D f(k1; : : : ; kd)g, deﬁne the
N -neighborhood of a site si 2 S(L) as
N(si) D
n
sj
ˇˇˇEj D Ei C Ek; Ek 2 N
o
(2)
A neighborhood conﬁguration is a surjective map v :
A 7! N(s0). Denote the set of all neighborhood conﬁg-
urations by EN (A).
The rule table for a cellular automata acting on the
state space E(A; L) with standard neighborhood N(s0) is
deﬁned by a map x : EN (A) 7! A (note that this map
need not be surjective or injective). The value of x for
a given neighborhood conﬁguration is called the (value
of the) rule component of that conﬁguration. The map
x : EN (A) 7! A induces a global map X : E(A; L) 7!
E(A; L) as follows: For any given element (t)
2
E(A; L), the set C(si) D
˚
(sj; t)
ˇˇsj 2 N(si)

is a neigh-
borhood conﬁguration for the site si, hence the map
(si; t)
7!
x(C(si)) for all si produces a new sym-
bol (si; t C 1). The site si is called the mapping site.
When taken over all mapping sites, this produces a matrix
(t C 1) that is the representation of X((t)). A cellular
automaton is indicated by reference to its rule table or to
the global map deﬁned by this rule table.
A cellular automaton with global map X is additive if
and only if, for all pairs of states  and ˇ,
X( C ˇ) D X() C X(ˇ)
(3)
Addition of states is carried out site-wise mod(p) on the
matrix representations of  and ˇ; for example, for a one-
dimensional six-site lattice with p D 3 the sum of 120112
and 021212 is 111021.
The deﬁnition for additivity given in [52] diﬀers
slightly from this standard deﬁnition. There, a binary val-
ued cellular automaton is called “linear” if its local rule

66 A
Additive Cellular Automata
only involves the XOR operation and “additive” if it in-
volves XOR and/or XNOR. A rule involving XNOR can
be written as the binary complement of a rule involving
only XOR. In terms of the global operator of the rule, this
means that it has the form 1 C X where X satisﬁes Eq. (3)
and 1 represents the rule that maps every site to 1. Thus,
(1 C X)( C ˇ) equals 1 : : : 1 C X( C ˇ) while
(1 C X)() C (1 C X)(ˇ)
D 1 : : : 1 C 1 : : : 1 C X() C X(ˇ)
D X() C X(ˇ) mod(2) :
In what follows, an additive rule is deﬁned strictly as one
obeying Eq. (3), corresponding to rules that are “linear”
in [52].
Much of the formal study of cellular automata has fo-
cused on the properties and forms of representation of the
map X : E(A; L) 7! E(A; L). The structure of the state
transition diagram (STD(X)) of this map is of particular
interest.
Example 1 (Continuous Transformations of the Shift Dy-
namical System) Let L be isomorphic to the set of inte-
gers Z. Then E(A; Z) is the set of inﬁnite sequences with
entries from A. With the product topology induced by the
discrete topology on A and  as the left shift map, the sys-
tem (E(A; Z); ) is the shift dynamical system on A. The
set of cellular automata maps X : E(A; Z) 7! E(A; Z)
constitutes the class of continuous shift-commuting trans-
formations of (E(A; Z); ), a fundamental result of Hed-
lund [73].
Example 2 (Elementary Cellular Automata) Let L be iso-
morphic to L with A D f0; 1g and N D f1; 0; 1g. The
neighborhood of site si is fsi1; si; siC1g and EN (A) D
f000; 001; 010; 011; 100; 101; 110; 111g. In this one-di-
mensional case, the rule table can be written as xi
D
x(i0i1i2) where i0i1i2 is the binary form of the index i.
Listing this gives the standard form for the rule table of
an elementary cellular automata.
000
001
010
011
100
101
110
111
x0
x1
x2
x3
x4
x5
x6
x7
The standard labeling scheme for elementary cellular au-
tomata was introduced by Wolfram [74], who observed
that the rule table for elementary rules deﬁnes the binary
number P7
1D0 xi2i and used this number to label the cor-
responding rule.
Example 3 (The Game of Life) This simple 2-dimensional
cellular automata was invented by John Conway to illus-
trate a self-reproducing system. It was ﬁrst presented in
1970 by Martin Gardner [75,76]. The game takes place on
a square lattice, either inﬁnite or toridal. The neighbor-
hood of a cell consists of the eight cells surrounding it. The
alphabet is {0,1}: a 1 in a cell indicates that cell is alive, a 0
indicates that it is dead. The update rules are: (a) If a cell
contains a 0, it remains 0 unless exactly three of its neigh-
bors contain a 1; (b) If a cell contains a 1 then it remains a 1
if and only if two or three of its neighbors are 1. This cel-
lular automata produces a number of interesting patterns
including a variety of ﬁxed points (still life); oscillators (pe-
riod 2); and moving patterns (gliders, spaceships); as well
as more exotic patterns such as glider guns which generate
a stream of glider patterns.
Additive Cellular Automata in One Dimension
Much of the work on cellular automata has focused on
rules in one dimension (d D 1). This section reviews some
of this work.
Boundary Conditions and Additivity
In the case of one-dimensional cellular automata, the lat-
tice L can be isomorphic to the integers; to the non-
negative integers; to the ﬁnite set f0; : : : ; n  1g 2 Z; or
to the integers modulo an integer n. In the ﬁrst case, there
are no boundary conditions; in the remaining three cases,
diﬀerent boundary conditions apply. If L is isomorphic to
Zn, the integers mod(n), the boundary conditions are pe-
riodic and the lattice is circular (it is a p-adic necklace).
This is called a cylindrical cellular automata [77] because
evolution of the rule can be represented as taking place on
a cylinder. If the lattice is isomorphic to f0; : : : ; n  1g,
null, or Dirchlet boundary conditions are set [78,79,80].
That is, the symbol assigned to all sites in L outside of this
set is the null symbol. When the lattice is isomorphic to
the non-negative integers ZC, null boundary conditions
are set at the left boundary. In these latter two cases, the
neighborhood structure assumed may inﬂuence the need
for null conditions.
Example 4 (Elementary Rule 90) Let ı represent the global
map for the elementary cellular automata rule 90, with rule
table
000
001
010
011
100
101
110
111
0
1
0
1
1
0
1
0
For a binary string  in Z or Zn the action of rule 90 is
deﬁned by [ı()]i D i1 C iC1 mod(2), where all in-
dices are taken mod(n) in the case of Zn. In the remaining

Additive Cellular Automata
A
67
cases,
[ı()]i D
8
ˆ<
ˆ:
1
i D 0
i1 C iC1
0 < i < n  1
n2
i D n  1
null conditions
[ı()]i D
(
1
i D 0
i1 C iC1
0 < i
half-inﬁnite conditions
(4)
Note that L and Zn are representations of the intervals
[1; 1] and [0; 1] respectively. Cellular automata rules are
not quite functions on these intervals, however, since they
are generally double valued on rational points having dis-
tinct representations as binary strings [81]. For example,
both 01 and 10 in ZC, where underlining indicates inﬁ-
nite repetition, are numerically 1/2 but ı(01) D 110 D 3/4
while ı(10) D 010 D 1/4.
The state space E(f0; 1g; L) for binary valued one-di-
mensional cellular automata is just the set of binary se-
quences over the speciﬁed one-dimensional lattice. For the
cases of Z and Zn all such rules commute with the shift
operator . When null boundary conditions are involved,
however, commutativity fails at the boundary sites. For
example, let X be the global operator for an elementary
cellular automata operating on strings  D 0 : : : n1
of length n with null boundary conditions. Noting that
1 D 1 mod(2), the commutator [X; ] has components

[X; ] ()

i D

X() C X()

i
D
8
ˆ<
ˆ:
X(012) C X(012) mod(2)
i D 0
0
0 < i < n  1
X(n100)
i D n  1
(5)
The relation [X; ] D 0 can be preserved for rules de-
ﬁned on ZC by altering the neighborhood structure. The
case N D f1; 0; 1g is called nearest neighbor because the
value introduced at site i at the next iteration of a rule is de-
termined by the values at site i and its immediately neigh-
boring sites i  1 and i C 1. Taking N D f0; 1; 2g yields
left justiﬁed neighborhoods. This eliminates the need for
null boundary conditions at the left boundary in ZC.
Changes in the neighborhood structure of this sort are
equivalent to changes in the mapping site. It is important
to recognize that such changes can signiﬁcantly alter the
topological structure of the state transition diagram (STD)
for a rule. If X represents the global map for a rule with
nearest neighbor neighborhoods, then the global map for
the same rule with left justiﬁed neighborhoods is X and
the presence of the shift operator can change cycle peri-
ods. For example, the maximum cycle period for nearest
neighbor rule 90 acting on Z6 is 2 while the maximum cy-
cle period for this same rule with left justiﬁed neighbor-
hoods is 3.
1000107!0101007!100010
nearest neighbor case
1000107!1010007!0010107!100010
left justiﬁed case
The additivity condition of Eq. (3) has an expression
in terms of rule table components. For A D f0; 1g this is
given by:
Theorem 1 ([81])
A k-site rule X : E(f0; 1g; L) 7! E(f0; 1g; L) with rule
components xi is additive if and only if for i D i0 : : : ik1
and j D j0 : : : jk1, with (i C j)r D ir C jr mod(2), it is
true that xi C xj D xiCj mod(2).
Corollary 1 ([81])
A k-site rule X : E(f0; 1g; L) 7! E(f0; 1g; L) is additive if
and only if for all i D i0 : : : ik1
xi D
k1
X
rD0
ikr1x2r mod(2) :
(6)
By Eq. (6), the k rule components x2r, 0  r  k  1, de-
termine the set of k-site additive rules. Hence only 2k of
the possible 22k k-site rules are additive, including the 0
rule that maps all sites to 0.
Additive Cellular Automata and Fractals
There is a direct connection between the space-time out-
put patterns of additive cellular automata and self-similar
fractal patterns [82,83,84,85,86,87,88]. The simplest exam-
ples are elementary rules 102 and 90. When acting on
a doubly inﬁnite sequence with the initial state 010, iter-
ation of these rules yields the space-time output indicated
in Fig. 1. In the case of rule 60, this output is the mod(2)
Pascal triangle while for rule 90 it consists of every other
row of this triangle [89].
The pattern generated by rule 60 (or, equivalently, by
rule 102) rescales to yield the fractal known as the Sir-
pinski gasket [90,91]. Direct connections between cellu-
lar automata outputs and the fractal generation schemes
of matrix substitution systems and hierarchical iterated
function systems are shown in [92,93,94,95,96,97,98,99].
In [100,101] the dimension spectrum associated to the

68 A
Additive Cellular Automata
Additive Cellular Automata, Figure 1
Space-time output of rules 60 and 90 from initial state with a single 1
space-time output of additive cellular automata is shown
to be equal to the singularity spectrum of an associated
multifractal.
Forms of Representation
Unless otherwise noted, the lattice in this section will be
Zn with periodic boundary conditions. Several forms of
representation for additive rules with periodic boundary
conditions appear in the literature. Rules have been repre-
sented as dipolynomials over ﬁnite ﬁelds [102]; as recur-
sion relations [103,104]; as circulant matrices [105,106];
and as polynomials in roots of unity or in powers of the
left shift [81].
The global operator for a k-site additive rule with
neighborhood structure N D fr; : : : ; k  r  1g can be
written as
X D r
k1
X
sD0
as s
as 2 A :
(7)
In the dipolynomial representation, a state  D 0 : : :
n1 deﬁnes a dipolynomial (x) while the rule X is rep-
resented by the dipolynomial equivalent of Eq. (7):
(x) D
n1
X
sD0
sxs ;
as 2 A
X D xr
k1
X
sD0
asxs ;
as 2 A
(8)
The action of X on a state  is obtained by multiplica-
tion of the corresponding dipolynomials: X(x)(x), with
all products reduced mod(n) [102].
The rule X is also representable in terms of powers of
the shift operator, with the expression
X D
k1
X
sD0
as s
as 2 A
(9)
where the coeﬃcients in this equation diﬀer from those in
Eq. (7). For example, rule 90 acting on Zn is represented
by 1 C  in the form of Eq. (7), and by  C 5 in the
form of Eq. (9) because 1 D 5 mod(6).
When a rule X acting on Zn is represented as in Eq. (9)
the string (a0; : : : ; an1) is directly connected to the rep-
resentation of X as a circulant matrix.

Additive Cellular Automata
A
69
A right circulant matrix is a matrix in which each suc-
cessive row is obtained from the row immediatly above by
shifting that row one unit to the right, with the ﬁnal row
entry shifted to the front. Thus,
circ(a0; a1; a2; : : : ; an2; an1)
D
0
BBB@
a0
a1
a2
  
an2
an1
an1
a0
a1
  
an3
an2
:::
a1
a2
a3
  
an1
a0
1
CCCA
If  2 E(A; Zn) is written as a column vector, the op-
eration of X on  is given as multiplication by the right
circulant matrix circ(a0; : : : ; an1) with all terms reduced
mod(p). The value of this representation is that proper-
ties of circulant matrices are well known and this provides
signiﬁcant information about the cellular automata rule.
This approach has a natural extension to the case of null
boundary conditions [107,108], although the matrix in-
volved is no longer a complete circulant. For example, the
nearest neighbor form of rule 90 acting on Z6 with peri-
odic boundary conditions has matrix representation
0
BBBBBB@
0
1
0
0
0
1
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
1
0
0
0
1
0
1
CCCCCCA
(10a)
while the matrix representation for null boundary condi-
tions is
0
BBBBBB@
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
0
0
0
0
1
0
1
CCCCCCA
(10b)
The circulant representation of the left shift  on Zn is
 D
0
BBBBB@
0
1
0
0
0
  
0
0
0
0
1
0
0
  
0
0
:::
0
0
0
0
0
  
0
1
1
0
0
0
0
  
0
0
1
CCCCCA
(10c)
The following lemmas [109] show the direct connection to
Eq. (9) and to Hedlund’s condition:
Lemma 1 ([109])
An n  n matrix A is circulant if and
only if [A; ] D 0.
Lemma 2 ([109])
An n  n matrix A is circulant if and
only if A D PA() for some polynomial PA of degree less
than or equal to n. Further, if A D circ(a0; : : : ; an1) then
PA() D
n1
X
sD0
as s
(11)
Other properties of circulant matrices provide links to the
representation of rules over Zn by polynomials in the nth
roots of unity. Let ! D e2	i/n be the ﬁrst complex nth root
of unity.
Deﬁnition 1 The Fourier matrix of order n is the matrix
Fn D
1
pn
0
BBBBBBB@
1
1
1
: : :
1
1
1
!n1
!n2
: : :
!2
!
1
!n2
!n4
: : :
!4
!2
:::
1
!2
!4
: : :
!n4
!n2
1
!
!2
: : :
!n2
!n1
1
CCCCCCCA
(12)
Denote the Hermitian conjugate (transpose, complex con-
jugate) of this by F
n .
Lemma 3 ([109])
a) The Fourier matrix is unitary: FnF
n D F
n Fn D I.
b) The eigenvalues of Fn are ˙1 and ˙i with multiplicities
depending on n.
c) The characteristic polynomials of F
n are
(  1)2 (  i) ( C 1)

4  1
 n
4 1
n D 0 mod(4)
(  1)
4  1(n1)/4
n D 1 mod(4)
2  1 4  1(n2)/4
n D 2 mod(4)
(  i)
2  1 4  1(n3)/4
n D 3 mod(4)
(13)
Every n  n circulant matrix A is diagonalized by Fn. Fur-
ther, if PA is the polynomial deﬁned by Eq. (11) then
FnAF
n D (A) D diag PA(1); PA(!); : : : ; PA(!n1)
(14)
hence the rth eigenvalue of A is PA(!r). Deﬁne the n  n
matrix r D diag(0; : : : ; 0; 1; 0; : : : ; 0) with the 1 in the
rth position and set ˘r D F
n rFn. The matrices ˘r are
Hermitian and satisfy the conditions
˘r˘s D
(
0
r ¤ s
˘s
r D s
n1
X
rD0
˘r D I

70 A
Additive Cellular Automata
Thus, they are orthogonal, idempotent, and form a resolu-
tion of unity. Hence, they are a complete set of projection
matrices.
Lemma 4 ([109]) Let A D circ(a0; : : : ; an1). Then
A D
n1
X
rD0
PA(!r)˘r
(15)
Representation of an additive rule in terms of complex
polynomials yields an interesting result on injectivity.
A rule X : E(A; Z) 7! E(A; Z) is surjective if every con-
ﬁguration has a predecessor. If the predecessor of a conﬁg-
uration having a predecessor is unique, the rule is injective.
A rule that is both surjective and injective is bijective. For
cellular automata, injectivity is equivalent to reversibility.
If a rule X is reversible, there is an inverse rule; that is, a re-
versible rule X1 such that if  2 E(A; Z) and X() D ,
then X1() D  [110,111,112,113,114,115]. The question
of whether or not a cellular automata rule is surjective or
injective is decidable only in dimension one [112]. Injec-
tive additive rules are also called group rules [52]. Those
with maximum period cycles provide an eﬀective means
of generating pseudorandom sequences. For a binary val-
ued rule operating on strings of length n and all divisors
of n, the maximal possible cycle period is 2n  1 (i. e., ev-
ery state but the 0 state is on the cycle). If a rule operating
on strings of length n has periodic boundary conditions,
the shift operator produces cycles of length n. As a result,
no additive rule acting on strings with periodic boundary
conditions can produce cycles of maximal period. If null
boundary conditions are used, however, maximal period
cycles can appear [79].
For a given cellular automata rule, the set of states hav-
ing no predecessor is called the Garden-of-Eden (GoE). All
additive rules acting on E(A; Z) are surjective but this is
not true of rules acting on E(A; Zn). For additive rules
acting on strings of length n with periodic boundary con-
ditions there will be conﬁgurations having no predeces-
sors. Nevertheless, these Garden-of-Eden states are not in-
trinsic to the rule since they do have predecessors when the
state space E(A; Zn) is embedded in E(A; Z), as the fol-
lowing example shows.
Example 5 Let X : E(f0; 1g; Z) 7! E(f0; 1g; Z) be deﬁned
by the rule table
000
001
010
011
100
101
110
111
0
0
1
1
1
1
0
0
For X : E(f0; 1g; Z3) 7! E(f0; 1g; Z3) the states f001; 010;
100; 111g have no predecessors. It is easy to show, how-
ever, that these states do have predecessors in Z6 when Z3
and Z6 are embedded in Z:
f000111; 111000g 7! 001;
f001110; 110001g 7! 010;
f011100; 100011g 7! 100;
f01; 10g 7! 1
While all additive rules are surjective in this sense, not all
additive rules are injective. The condition for injectivity is
easily seen using the representation in terms of nth roots
of unity. From Eq. (14) a rule will be injective on E(A; Zn)
if and only if its diagonalized matrix representation is in-
vertible, hence there can be no zero eigenvalues.
Lemma 5 ([81,116])
Let X : E(A; Zn) 7! E(A; Zn) be
an additive rule represented by A D circ(a0; : : : ; an1). X
is injective on Zn  Z if and only if PA(!s) ¤ 0 mod(p) for
0  s  n  1.
If the rule in Lemma 5 is to be injective on Z then it must
be so on Zn for all n. Thus the complex polynomial PA(z)
must be irreducible with respect to all nth roots of unity,
for all n. This leads to the next theorem:
Theorem 2 ([81,116])
Let X : E(A; Z) 7! E(A; Z) be
an additive rule represented as in Eq. (9). X will be injec-
tive if and only if
lim
"!0
1
2i
I
c(")
P0
A(z)
PA(z) dz D 0
(16)
where P0
A(z) is the derivative of PA(z) and c(") is a contour
consisting of circles of radius 1 C " and 1  ".
This follows since the integral in Eq. (16) counts the num-
ber of zeros minus the number of poles of PA(z) contained
within the contour. Since there are no poles, in the limit
this counts the number of zeros on the unit circle.
Corollary 2 ([81]) Let X : E(f0; 1g; Zn) 7! E(f0; 1g; Zn)
be deﬁned by X D Pk1
sD0  s.
1. If k is even, X is not injective.
2. If k is odd, X will be injective for all n such that n ¤ mk
for any m.
Table 1 lists all rules that are injective on E(f0; 1g; Zn) for
at least some n, with k  5.
Transient Lengths and Cycle Periods
For any cellular automata acting on a ﬁnite state space,
every state eventually maps to a ﬁxed point or cycle. If

Additive Cellular Automata
A
71
Additive Cellular Automata, Table 1
Injective Rules for k 6 5
X
k
injectivity condition

4
5 none

3
4 none

2
3 none

2 none
I
1 none

2 C 
3 C 
4
5 n ¤ 3m

 C 
3 C 
4
5 none

 C 
2 C 
4
5 none
I C 
3 C 
4
5 none
I C 
2 C 
4
5 n ¤3m
I C 
 C 
4
5 none

 C 
2 C 
3
4 n ¤3m
I C 
2 C 
3
4 none
I C 
 C 
3
4 none
I C 
 C 
2
3 n ¤ 3m
I C 
 C 
2 C 
3 C 
4
5 n ¤ 5m
a rule is injective, it is reversible and every state is a ﬁxed
point, or is on a cycle. If not injective, there will be states
without predecessors, Garden-of-Eden states. As indicate,
however, if a rule is additive its Garden-of-Eden states are
spurious in the sense that they do have predecessors if the
state space is enlarged.
The following theorem lists several signiﬁcant prop-
erties of cellular automata rules acting on E(A; Z) or
E(A; ZC) with left justiﬁed neighborhoods.
Theorem 3 ([81])
Let X be a k-site cellular automata
rule acting on E(A; Z) or on E(A; ZC) with left justiﬁed
neighborhoods. Then the following statements are equiva-
lent: (a) X is surjective, (b) X has an empty Garden-of-
Eden, (c) Every ﬁnite sequence 0 : : : n1 has exactly pk1
pre-images and every state  has at most pk1 predecessors,
(d) X maps eventually periodic states to eventually periodic
states and non-periodic states to non-periodic states, (e) as
a map of the interval [0,1] X maps rationals to rationals
and irrationals to irrationals.
If
X : E(A; Zn) 7! E(A; Zn) is a k-site rule with
jAj D p and either periodic or null boundary condi-
tions, the state transition diagram, STD(X) is a graph
with pn vertices labeled by the set of p-adic numbers
fi0; : : : ; in1j0  ir  p  1g. An edge is directed from
the vertex i0; : : : ; in1 to the vertex j0; : : : ; jn1 if and
only if X(i0; : : : ; in1) D j0; : : : ; jn1. Each state  maps
to a unique state X() so STD(X) consists of a set of trees
rooted on ﬁxed points or cycles. States at the top of trees
are Garden-of-Eden states.
If h(X; n) is the maximum tree height, states at heights
h  h(X; n) cannot appear after h(X; n)  h C 1 itera-
tions and after h(X; n) iterations only ﬁxed points and
states on cycles remain. Thus, iteration of a non-injective
rule on E(A; Zn) decreases the number of available states
with a corresponding reduction in entropy. On the other
hand, non-injective additive rules acting on E(A; ZC)
do not reduce entropy [117] even though the do so on
E(A; Zn) for all n. The explanation for this apparent
paradox is that the Garden-of-Eden states that appear in
E(A; Zn) are artifacts of the ﬁnite length of states in this
space. When embedded in ZC, states in Zn correspond to
periodic conﬁgurations, hence to rational numbers in [0,1]
and the set of all rationals has measure 0 in the reals.
Parameters of interest for characterizing state transi-
tion diagrams of rules acting on E(A; Zn) are the maxi-
mum tree height h(X; n) and the cycle periods cs(X; n).
Theorem 4 ([102])
1. Trees rooted at all vertices on cycles or at ﬁxed points of
the STD for additive cellular automata are isomorphic to
the tree rooted at the ﬁxed point 0.
2. The periods of all cycles of an additive rule acting on Zn
are divisors of the period for cycles obtained by starting
from an initial state containing only a single 1.
3. Let c(m) be the maximum cycle period for an addi-
tive cellular automaton acting on Zm and take n D 2m.
Then c(n) divides 2c(m).
4. Let n D 2km, m odd. The maximum cycle period
c(n) for an additive rule acting on
Zn
satisﬁes
c(n)j2ord(n;m)2m where ord(n; m) D minfrj 2r
D
2m mod(m)g
In most cases, the maximum cycle period
equals
2ord(n;m)  2m or, if the rule is symmetric, 2sord(n;m)  2m.
In [104,118], Jen shows that when this is not the case, it
is a number theoretic consequence of an anomalous shift
that reduces the maximum cycle period. As indicated, the
choice of mapping site can inﬂuence cycle periods and the
eﬀect of anomalous shifts is analogous. This is an immedi-
ate result of the next theorem.
Theorem 5 ([77,118]) Let X : E(A; Zn) 7! E(A; Zn) be
the global mapping for a cellular automaton rule. A state 
is on a cycle of X if and only if there exist integers r and s
such that Xr() D s().
In some cases r D c(n) and s D n so the theorem is not
as strong as it might ﬁrst appear. If Xr() D s() for
all states on cycles of maximum period, however, then
c(n) D rt where t D minfj j js D 0 mod(n)g. A change of
mapping site is equivalent to multiplication by a power

72 A
Additive Cellular Automata
of the shift . If  is on a cycle of maximum period
then ( kX)r() D  ks() and the cycle period is rq with
q D minfj j j(k  s) D 0 mod(n)g. In general, there is no
requirement that q D t.
Comprehensive results on cycle periods and maxi-
mum transient lengths in the case of null or periodic
boundary conditions were ﬁrst obtained by Elspasa [119]
in work characterizing the cycle sets in the state tran-
sition diagrams of linear machines. Since then a num-
ber of researchers have independently derived similar re-
sults [120,121,122,123,124,125,126]. Let A D circ(a0; : : : ;
an1) and let  be a state in E(A; Zn). The minimal
annihilating polynomial of  is the monic polynomial
P(z) such that P(A) D 0 mod(p). This polynomial ex-
ists since A always satisﬁes its characteristic equation. Let
P(z) D zkP(z) with P(0) ¤ 0. The order of P(z),
ord(P(z)), is deﬁned as the smallest natural number c
such that P(z)j(zc  1). The following theorem is given
in [121,122]. Alternate versions appear in [123,124,126].
Theorem 6 Let  2 E(A; Zn) with minimal annihilating
polynomial P(z) D zkP(z). Then Ak belongs to a cycle
with period c D ord(P(z)).
Since the minimal annihilating polynomial always divides
the minimal polynomial, which, for additive cellular au-
tomata represented by circulant matrices, or by the corre-
sponding null boundary condition matrix, is the same as
the characteristic polynomial of that matrix, all cycle pe-
riods and maximum transient lengths can be found from
the characteristic polynomial. Hence, the maximum cy-
cle period is the order of the characteristic polynomial
since there always exists a state whose minimal annihilat-
ing polynomial is the minimal polynomial.
The questions of reachability, and conditions for states
to be on cycles is also addressed using the formulation
in terms of roots of unity. If an additive rule acting on
E(A; Zn) is represented by a circulant matrix A and
a state  is represented by  D Pn1
sD0 s!s, write
PA(!) D
rY
jD1
j(!)
sY
kD1
˝k(!) D (!)˝(!)
(17)
where (!) is a product of the irreducible factors of PA(!)
representing injective rules, and ˝(!) is a product of the
irreducible factors representing non-injective rules. Diag-
onalization of A yields (A) D ()(˝) and 1()
exists since (!) represents injective (hence reversible)
rules. Let  denote the nullity of (˝).
Theorem 7 ([81,127])
A state (!) is reachable by an
additive cellular automata rule with circulant matrix rep-
resentation A if and only if (˝)j(!). The fraction of
reachable conﬁgurations is 2. Further, if d() is the in
degree of the STD vertex labeled by  then
d() D
(
0
 is a Garden-of-Eden state
2
otherwise
(18)
Example 6a (Rule 90 acting on E(f0; 1g; Zn) with pe-
riodic boundary conditions)
The circulant matrix for
nearest neighbor rule 90 acting on Zn with periodic
boundary conditions is A(ı) D circ(0; 1; 0; : : : ; 0; 1). The
characteristic polynomial of this matrix satisﬁes the re-
currence relation QnC1(x) D xQn(x) C Qn1(x) mod(2).
For n D 2sm with m odd, the characteristic polynomial of
A(ı) has the form
Qn(x) D x2sr2sC1
m
(x)
r2kC1(x) D xr2k1(x) C r2k3(x)
(19)
and the minimal annihilating polynomial is [108]
xrm(x)
s D 0
x2s1r2s
m(x)
s > 0
(20)
where rm(0) ¤ 0. For n D 6 (s D 1, m D 3), the circu-
lant matrix representing rule 90 is given by Eq. (10a).
The characteristic polynomial of this matrix, with co-
eﬃcients reduced mod(2), is x2(x4 C 1) D x2(x C 1)4.
Thus, the minimal polynomial is x(x C 1)2 D x(x2 C 1)
and r3(x) D (x C 1). The minimum integer c such that
(x2 C 1) divides (xc C 1) is just c D 2, showing that the
maximum cycle period is 2.
Example 6b (Rule 90 Represented by Roots of Unity, Act-
ing on Z6)
The Theorem 5 condition Xr() D s()
for a state  to be on a cycle can be written as
r(X)Fn() D ns()Fn(). This gives the conditions
for  to be on a cycle as a set of linear equations [81].
For n D 6, elementary rule 90 has the form ı D  C 5,
or in terms of the sixth root of unity, ı(!) D ! C !5. Us-
ing !3 D 1 with all sums taken mod(2)
(ı) D diag

0; ! C !5; !2 C !4; 0; !2 C !4; ! C !5
2(ı) D diag

0; !2 C !4; !2 C !4; 0; !2 C !4; !2 C !4
3(ı) D diag

0; ! C !5; !2 C !4; 0; !2 C !4; ! C !5
(21)
hence 3(ı) D (ı) or equivalently, (ı)(2(ı) C I)
D 0 mod(2). Thus, the maximum tree height is one and
the maximum cycle period is two. In addition,  D 2 so
that 1/4 of the total of 64 states will be on cycles. Further,

Additive Cellular Automata
A
73
for n D 6 (observing that 1 D 1 mod(2))
F6()
D
1
p
6
0
BBBBBB@
0 C 1 C 2 C 3 C 4 C 5
0 C 1!5 C 2!4 C 3!3 C 4!2 C 5!
0 C 1!4 C 2!2 C 3 C 4!4 C 5!2
0 C 1!3 C 2 C 3!3 C 4 C 5!3
0 C 1!2 C 2!4 C 3 C 4!2 C 5!4
0 C 1! C 2!2 C 3!3 C 4!4 C 5!5
1
CCCCCCA
(22)
Since !2 C !4 D !3 D 1 D 1 mod(2), 2(ı) D diag(0;
1;1; 0; 1; 1) and the condition 2(ı)F6() D F6() re-
quires that [F6()]0 D [F6()]3 D 0 which reduces to
0C2C4 D 1C3C5 D 0 or equivalently, 4 D
0C2 and 5 D 1C3. Hence a state  will be on a cy-
cle if and only if it has the form  D (0; 1; 2; 3; 0 C
2; 1 C 3).
Computing Predecessor States
A problem of general interest for cellular automata
is
computation
of
predecessor
states.
For
a
rule
X : E(A; L) 7! E(A; L) with a state ˇ given this requires
solution of the equation X() D ˇ. It is always possible
to construct solutions for this equation, or to show that
none exist by a method of backward reconstruction based
on the rule table.
Example 7 (Rule 60 Acting on Z4 With Periodic Bound-
ary Conditions)
Rule 60 is a 2-site rule, deﬁned by
(00; 11) 7! 0; (01; 10) 7! 1. Given the state 0110 the pre-
decessors of this state can be computed as follows:
1. The initial 0 in 0110 can arise from either 00 or 11.
2. Starting with a 00, the next symbol in 0110 is a 1 and
this can arise from a 01 or a 10, but this must also con-
nect to the original 00 so only 01 is allowed, giving 001.
Starting from a 11, on the other hand, the same reason-
ing requires 110.
3. The third symbol in 0110 is also a 1. To be consistent
with 001 requires that 10 be selected, and to be consis-
tent with 110 requires that 01 be selected, thus giving
the two partially constructed possibilities as 0010 and
1101.
4. Finally, the fourth symbol must be a 0. This requires
that the predecessor string conclude with either 00 or
11. Since the strings are in Z4 with periodic boundary
conditions, the ﬁnal symbol in the predecessor string
must also be the ﬁrst symbol in that string. Thus, both
0010 and 1101 are seen to be predecessors of 0110.
Other ways of computing predecessor states for ﬁnite
strings is through the construction of a rule matrix [81]
or the use of de Bruijn diagrams [81,113]. Backward re-
construction, the rule matrix, and use of a de Bruijn dia-
gram are valid methods for computing predecessor states
for all one-dimensional rules. For additive rules, how-
ever, there is an analytic means for computing prede-
cessor states, starting from left justiﬁed neighborhoods
deﬁned on E(A; Zn) or E(A; ZC) [81,128]. This can
be illustrated for rules deﬁned on E(f0; 1g; ZC). This
method also works for rules deﬁned on E(f0; 1g; Zn) if
it is embedded in E(f0; 1g; ZC) as the subset of half-
inﬁnite periodic sequences with periods that divide n.
Deﬁne operators B : E(f0; 1g; ZC) 7! E(f0; 1g; ZC) and
1 : E(f0; 1g; ZC) 7! E(f0; 1g; ZC) by
B()
s D
s
X
iD0
i mod(2)

1()

s D
(
0
s D 0
s1
s > 0
(23)
Theorem 8 ([81,128])
Let D D I C  be the global op-
erator for elementary rule 60 acting on E(f0; 1g; ZC) and
deﬁne the state ˛(s) in ZC by
h
˛(s)i
i D
(
0
i ¤ s
1
i D s
1. The general solution of D() D ˇ is
 D a0B(˛(0)) C B1(ˇ) :
2. The general solution of Dk() D ˇ is
 D
k1
X
sD0
asBsC1(˛(s)) C Bkk(ˇ) ;
where the coeﬃcients as provide initial conditions.
The general technique for computing predecessors can be
illustrated with the case of rule 150, expressed in left justi-
ﬁed form as X D IC C2. To solve (IC C2)() D ˇ
deﬁne four sequences:
(0)
i
D 2i;
(1)
i
D 2iC1
ˇ(0)
i
D ˇ2i;
ˇ(1)
i
D ˇ2iC1
i D 0; 1; 2; : : :
(24)
The equation (I C  C 2)() D ˇ reduces to the pair of
coupled equations
(0)
i
C (0)
iC1 D (1)
i
C ˇ(0)
i
(1)
i
C (1)
iC1 D (0)
iC1 C ˇ(0)
i
(25)

74 A
Additive Cellular Automata
and these can be written as
D

(0)
D (1) C ˇ(0)
D

(1)
D 

(0)
C ˇ(1)
(26)
From Theorem 8, Eq. (26) can be formally solved to
obtain
(0) D a0B ˛(0)
 C B1 
(1) C ˇ(0)
(1) D a1B ˛(0)
 C B1 
(0) C ˇ(1)
(27)
Substituting the second equation of (27) into the ﬁrst,
making use of the identity 2((0)) D 1((0) C (0)
0
˛(0)), rearranging terms and absorbing the term (0)
0 ˛(0)
into the constant parameter, results in the equation

I C B21 
(0)
D B

a0˛(0) C a1B

˛(1)
C B1 
ˇ(0)
C B22 
ˇ(1)
(28)
Theorem 9 ([81,128])
The operator (I C B21) is in-
vertible with (I C B21)1 D I C C(2;1) with

C(2;1) ()

i D
8
<
:
0
i D 0
P
l
(i1)
3
m
sD0
(i3s2 C i3s1)
i > 0
(29)
where all sums are taken mod(2), r D 0 for r < 0, and dxe
indicates the greatest integer less than or equal to x.
The solution for (0) is substituted into the second equa-
tion of (27) yielding a solution for (1). These are recom-
bined to get the general solution for . This technique of
reducing a single equation to a set of coupled equations
involving simpler additive rules works in general although
the form for partitioning of sequences is speciﬁc to the par-
ticular case. Computation of predecessors involves inver-
sion of operators of the form I C Brs. The general form
for the inverse of this operator is I C C(r;s) where C(r;s)
is the lower triangular matrix that is the solution of the
equation
jCr
X
mDj

r C 1
m  j C 1

[C(r;s)]im C [C(r;s)]i;jCs D ıi;jCs
(30)
d-Dimensional Rules
Both [102,105] discuss the extension from one-dimen-
sional to d-dimensional rules deﬁned on tori. In [102] this
discussion uses a formalism of multinomials deﬁned over
ﬁnite ﬁelds. In [105], the one-dimensional analysis based
on circulant matrices is generalized. The matrix formulism
of state transitions is retained by deﬁning a d-fold “circu-
lant of circulants,” which is not, of itself, necessarily a cir-
culant. Computation of the the non-zero eigenvalues of
this matrix yields results on transient lengths and cycle pe-
riods.
More recently, an extensive analysis of additive rules
deﬁned on multi-dimensional tori has appeared [129].
A d-dimensional integer vector En D (n1; : : : ; nd) deﬁnes
a discrete toridal lattice L En. Every d-dimensional ma-
trix of size En with entries in A, jAj D p (prime), deﬁnes
an additive rule acting on E(A; L(En)) as follows: Let T
and (t) be elements of E(A; L(En)) with X the rule de-
ﬁned by T and (t) a state at time t. The state transition
deﬁned by X is (t C 1) D X((t)) and this is given by

(t C 1)

i1:::id D
X
k1;:::;kd
[C (T )]k1:::kd
i1:::id

(t)

k1:::kd
[C (T )]k1:::kd
i1:::id D Tj1:::jd
js D ks  is mod(ns)
(31)
The matrix C(T ) is the d-dimensional generalization of
a circulant matrix with T as the equivalent of its ﬁrst row.
For example, if d D 1 and p D 2 with T D (0; 1; 0; 0; 0; 1)
this deﬁnes the additive rule  C 5 (rule 90) and the ma-
trix C(T ) is given in Eq. (10a).
Let S and T be elements of E(A; L(En)) and de-
ﬁne the binary operation  : E(A; L(En))  E(A; L(En))
7! E(A; L(En)) by
[ (S; T )]i1:::id D
X
k1; : : : ; kd
0  ks < ns
Sk1:::kd Ti1k1:::idkd
(32)
with all sums taken mod(p).
Lemma 6 ([129]) Let S and T be as above, with general-
ized circulant matrices C(S), C(T ). The product C(S)C(T )
deﬁned by

C(S)C(T )j1:::jd
i1:::id
D
X
k1:::kd

C(S)k1:::kd
i1:::id

C(T )j1:::jd
k1:::kd mod(p)
(33)
is also a generalizedcirculant and C(S)C(T )DC( (S; T )).
An important tool for analysis of additive rules on multi-
dimensional tori is the discrete baker transformation. This

Additive Cellular Automata
A
75
is a discrete version of the baker transformation (Bernoulli
shift) for continuous one dimensional dynamical sys-
tems [130]. The discrete baker transformation is the op-
eration Bp : E(A; L(En)) 7! E(A; L(En)) deﬁned by
Bp  T 
i1:::id D
X
isDpks mod(ns)
Tk1:::kd mod(p)
(34)
with the empty sum set to 0. In the one dimensional case
with p D 2 and T D (a0; : : : ; an1) this becomes
[B2  T ]i D
X
k: iD2k mod(n)
ak mod(2)
(35)
Example 8 If d D 1, p D 2 and n D 7 then B2(a0; a1; a2;
a3; a4; a5; a6) D (a0; a4; a1; a5; a2; a6; a3). On the other
hand, if n D 6 then B2(a0; a1; a2; a3; a4; a5) D (a0 C a3;
0; a1 C a4; 0; a2 C a5; 0).
In one dimension, if X is the rule deﬁned by
T D (a0; : : : ; an1) then BpX D X2.
Example 9 If d D 2 and p D 3 with n1 D 5, n2 D 6 and
T D
2
6666664
a00
a01
a02
a03
a04
a10
a11
a12
a13
a14
a20
a21
a22
a23
a24
a30
a31
a32
a33
a34
a40
a41
a42
a43
a44
a50
a51
a52
a53
a54
3
7777775
then
B2  T
D
2
6666664
a00 C a20 C a40
a02 C a22 C a42
a04 C a24 C a44
0
0
0
0
0
0
a30 C a10 C a50
a32 C a12 C a52
a34 C a14 C a54
0
0
0
0
0
0
a01 C a21 C a41
a03 C a23 C a43
0
0
0
0
a31 C a11 C a51
a33 C a13 C a53
0
0
0
0
3
7777775
with all sums mod(3).
The
discrete
baker
transformation
exponentially
speeds up rule evolution.
Theorem 10 ([129])
If p is prime then [C (T )]pr D C

Br
p  T

.
Thus, if the matrix T deﬁnes a rule X : E(A; L(En))
7! E(A; L(En)) then X pr is obtained from the matrix
Br
p  T .
The operator Bp is a permutation if every sum in
Eq. (34) contains at most one non-zero summand. In Ex-
ample 7, B2 is a permutation for n D 7 but is not for n D 6.
Lemma 7 ([129])
Set  D maxfexpnsj1  s  dg. That
is,  is the largest integer k such that for some s, pk divides
ns. Then for all r >  and any T 2 E(A; L(En)), Bp is a per-
mutation on Br
p  T .
Theorem 11 ([129]) Let q be prime and let ordmq be the
order of q in m when this is deﬁned and 1 otherwise. Write
ns D pks ms
and
set
c D lcm

ordm1 p; : : : ; ordmd p

.
Then, for any rule X deﬁned by a matrix T 2 E(A; L(En))
the following are true:
1. BCc
p
 T D B
p  T
2. The period of any cycle in STD(X) divides p(pc  1)
3. The heignt of trees in STD(X) does not exceed p.
The baker transformation is a linear transformation on
the space of d-dimensional matrices with entries from A.
Since each element of this space deﬁnes an additive cellular
automata rule, the vertices of the state transition diagram
for the baker transformation can be labeled by these rules,
and this is exhaustive.
Deﬁnitions:
1. An oriented graph G D (V; E) is a set V of vertices to-
gether with an edge set E  V  V. If (v; w) 2 E then
there is an edge directed from vertex v to vertex w.
2. An oriented graph G1 D (V1; E1) reduces to an ori-
ented graph G2 D (V2; E2) modulo p (G1 <p G2) if (a)
V1 D V2 D V, and (b) (v; w) 2 E if and only if there is
an oriented chain of length p from v to w in G2.
Example 10
Figure 2 shows the reduction modulo 3 of
a connected graph with 12 vertices to a graph with three
connected components.
The baker diagram for the space E(A; L(En)) has
an edge directed from rule X to rule Y if and only if
Y D BpX.
Lemma 8 ([129])
If (X; Y) is an edge in STD(Bp) then
STD(Y) <p STD(X).
Since E(A; L(En)) is ﬁnite, iteration of Bp on E(A; L(En))
must
eventually reach
a
ﬁxed
point
or
cycle.
If
T 2 E(A; L(En)) set (T ) equal to the sum mod(p) of
all entries in T . If X is the rule deﬁned by T take (X)
as the maximum height of trees in STD(X), (X) as the
number of vertices in STD(X) lying on cycles, and s(X)
as the number of cycles of length s contained in STD(X).

76 A
Additive Cellular Automata
Additive Cellular Automata, Figure 2
Example of Graph Reduction Modulo 3 for a Graph with 12 Ver-
tices (from [129])
Theorem 12 ([129])
1. If STD(X) <p STD(Y) then
a) (X) 
l
(Y)
p
m
b) If j is the largest number such that jp j(Y) > 0 then
p j(X) D 0
c) (X) D (Y).
2. Let X, Y be two rules belonging to a cycle R of STD(Bp)
with period lR and let fc1; : : : ; ckg be the set of all cycle
periods in STD(X). Then
a) STD(X) and STD(Y) are isomorphic as graphs
b) (X)  1 and p 6 jcs for all s
c) For all s, csj(plR  1), lR
D
lcm(ordc1 p; : : : ;
ordck p), lRjc
d) In addition, if X is deﬁned by the matrix T and
(T ) D 0 then the number of connected components
of STD(X) is divisible by p.
Future Directions
Much work remains on both the theoretical analysis and
the applications of cellular automata. While much of this
work will utilize non-additive automata, there are still
many open questions on additive cellular automata as well.
Several theoretical questions revolve around the issue of
surjectivity. While all additive cellular automata are surjec-
tive (bracketing the spurious Garden-of-Eden states that
exist for rules deﬁned on ﬁnite state spaces) and for gen-
eral rules surjectivity in dimensions higher than one is un-
decidable, it is desirable to have a simple general criteria
for surjectivity in one dimension. It would also be use-
ful to know if any surjective cellular automaton (additive
or non-additive) is capable of universal computation. If
there are surjective rules that are universal computers this
would connect to work in mathematical logic on the com-
putational limits of formal systems [133]. This connec-
tion arises because surjective rules only exhibit Garden-of-
Eden states when acting on ﬁnite state spaces. The appear-
ance of predecessor states when the state space is enlarged
seems analogous to the increase in computational power
of a formal system when it is enlarged by the addition of
a new axiom.
Section “d-Dimensional Rules” introduced the dis-
crete baker transformation of additive cellular automata
rules. This transformation is a linear operator on the
space of additive rules and further research into its prop-
erties can contribute to a deeper understanding of the
structure of additive rules. In addition, there appears to
be a connection between certain universal numbers that
can be deﬁned from this transformation and the well-
known Mersenne and Fermat numbers. Elucidation of this
connection would provide a signiﬁcant link to number-
theoretic aspects of cellular automata.
When applications are considered, [52] indicates
many avenues of continued development for additive cel-
lular automata. In addition, many of the references in the
general bibliography point to directions of current cellular
automata research in a number of areas. As indicated in
the introduction, applications in physics (crystal growth,
hydrodynamics, reaction-diﬀusion systems, astronomy),
medicine and biology (pattern formation, genetic inter-
action networks, disease modeling, ecosystem modeling),
pattern recognition and image processing, and computa-
tion (random number generation, language and pattern
recognition, test pattern generation for VLSI chips, sig-
nature analysis, error correcting codes, cryptography) are
only a small portion of the cellular automata applications
that continue to be studied.
Bibliography
This article provides a brief survey of some of the sig-
niﬁcant theoretical results on additive cellular automata,
together with references to applications of both cellular
automata in general and additive cellular automata in par-
ticular. For historical information, references [1,2,3,5,7]
are recommended. References [4,6,8,9] provide a general
background in the use of cellular automata in modeling, as
well as a number of examples. Speciﬁc exemplary cases of
applications are found in [10,11,12,13,14,15,16,17,18,19,
20,21,22,23,24,25,26,27,28,29,30,31,32,33,34,35,36,37,38,
39,40,41,42,43,44,45,46,47,48,49,50,51,52,53,54,55,56,57,
58,59,60,61,62,63,64,65,66,67,68,69,70,71,72]. [52], which
deals extensively with the use of additive cellular automata
in computing applications and VLSI chip design, is of
particular value. [82,83,84,85,86,87,88,89,90,91,92,93,94,
95,96,97,98,99,100,101] provide a good background in

Additive Cellular Automata
A
77
the relation between cellular automata and fractal pat-
terns. [81,102,103,104,105,106,107,108,109,110,111,112,
113,114,115,116,117,118,119,120,121,122,123,124,125,
126,127,128,129] deal with the theoretical analysis of ad-
ditive cellular automata. A good survey of work in cellular
automata up to the mid-1990s is [131]. An important
computational survey of cellular automata dynamics is
given in [132].
Primary Literature
1. von Neumann J (1963) The general and logical theory of au-
tomata. In: Taub A (ed) J. von Neumann Collected Works,
vol 5. Pergamon, NY, pp 288–328
2. von Neumann J, Burk AW (ed) (1966) Theory of Self-
Reproducing Automata. University of Illinois Press, Urbana
3. Arbib M (1966) Simple self-reproducing universal automata.
Inf Control 9:177–189
4. Toffoli T, Margolis N (1987) Cellular Automata Machines:
A New Environment for Modeling. MIT Press, Cambridge
5. Codd EF (1968) Cellular Automata. Academic Press, NY
6. Duff MJB, Preston K, Jr. (1984) Modern Cellular Automata:
Theory and Applications. Plenum, NY
7. Sarkar P (2000) A brief history of cellular automata. ACM Com-
put Syst 32(1):80–107
8. Chopard B, Droz M (1998) Cellular Automata Modelling of
Physical Systems. Cambridge University Press, Cambridge
9. Lindenmayer A, Rozenberg G (1976) Automata, Languages,
Development. North Holland, Amsterdam
10. Mackay AL (1976) Crystal Symmetry. Phys Bull 27:495–497
11. Langer JS (1980) Instabilities and pattern formation in crystal
growth. Rev Mod Phys 52:1
12. Lin F, Goldenfeld N (1990) Generic features of late-stage crys-
tal growth. Phys Rev A 42:895–903
13. Greenberg JM, Hastings SP (1978) Spatial patterns for discrete
models of diffusion in excitable media. SIAM J Appl Math
34(3):515–523
14. Greenberg JM, Hassard BD, Hastings SP (1978) Pattern forma-
tion and periodic structures in systems modeled by reaction-
diffusion equations. Bull Am Math Soc 84:1296–1327
15. Madore BF, Freedman WL (1983) Computer simulations of the
Belousov-Zhabotinsky reaction. Science 222:615–616
16. Adamatzky A, Costello BDL, Asai T (2005) Reaction-Diffusion
Computers. Elsevier, London
17. Oono Y, Kohmoto M (1985) A discrete model for chemical tur-
bulance. Phys Rev Lett 55:2927–2931
18. Falk H (1986) Comments on a simple cellular automata in spin
representation. Physica D 20:447–449
19. CanningA, Droz M (1991) A comparison of spin exchange and
cellular automata models for diffusion-controlled reactions.
In: Gutowitz HA (ed) Cellular Automata: Theory and Experi-
ment. MIT Press, Cambridge, pp 285–292
20. Vitanni P (1973) Sexually reproducing cellular automata.
Math Biosci 18:23–54
21. Young D (1984) A local activator-inhibitor model of verte-
brate skin patterns. Math Biosci 72:51–58
22. Dutching W, Vogelsaenger T (1985) Recent progress in mod-
eling and simulation of three dimensional tumor growth and
treatment. Biosystems 18(1):79–104
23. Moreira J, Deutsch A (2002) Cellular automata models of
tumor development: A critical review. Adv Complex Syst
5(2&3):247–269
24. Sieburg HB, McCutchan JA, Clay OK, Cabalerro L, Ostlund JJ
(1991) Simulation on HIV infection in artificial immune sys-
tems. In: Gutowitz HA (ed) Cellular Automata: Theory and Ex-
periment. MIT Press, Cambridge, pp 208–227
25. Santos RMZD, Continho S (2001) Dynamics of HIAV approach:
A cellular automata approach. Phys Rev Lett 87(16):102–104
26. Beauchemin C, Samuel J, Tuszynski J (2005) A simple cellular
automaton model for influenza A viral infection. J Theor Biol
232(2):223–234
27. Burks C, Farmer D (1984) Towards modeling DNA sequences
as automata. Physica D 10:157–167
28. Moore JH, Hahn LW (2002) Cellular automata and genetic
algorithms for parallel problem solving in human genetics.
In: Merelo JJ, Panagiotis A, Beyer H-G (eds) Lecture Notes in
Computer Science. Springer, Berlin, pp 821–830
29. Gerola H, Seiden P (1978) Stochastic star formation and spiral
structure of galaxies. Astrophys J 223:129–135
30. Flache A, Hegselmann R (1998) Understanding complex so-
cial dynamics: A plead for cellular automata based modeling.
J Artif Soc Social Simul 1(3):1
31. Chen K, Bak P, Jensen MH (1990) A deterministic critical
forest-fire model. Phys Lett A 149:207–210
32. Drossel B, Schwabl F (1992) Self-organized critical forest-fire
model. Phys Rev Lett 69:1629–1632
33. Smith III AR (1972) Real-time language recognition by one-
dimensional cellular automata. J Comput Syst Sci 6:233–253
34. Sommerhalder R, van Westrhenen SC (1983) Parallel lan-
guage recognition in constant time by cellular automata.
Acta Inform 19:397–407
35. Ibarra OH, Palis MA, Kim SM (1985) Fast parallel language
recognition by cellular automata. Theor Comput Sci 41:
231–246
36. Morita K, Ueno S (1994) Parallel generation and parsing of ar-
ray languages using reversible cellular automata. Int J Pattern
Recognit Artif Intell 8:543–561
37. Jen E (1986) Invariant strings and pattern recognizing prop-
erties of 1D CA. J Stat Phys 43:243–265
38. Raghavan R (1993) Cellular automata in pattern recognition.
Inf Sci 70:145–177
39. Chattopadhyay S, Adhikari S, Sengupta S, Pal M (2000)
Highly regular, modular, and cascadable design of cellu-
lar automata-based pattern classifier. IEEE Trans VLSI Syst
8(6):724–735
40. Rosenfeld A (1979) Picture Languages. Academic Press, NY
41. Sternberg SR (1980) Language and architecture for paral-
lel image processing. In: Gelesma ES, Kanal LN (eds) Pattern
Recognition in Practice. North-Holland, Amsterdam, p 35
42. Hopcroft JE, Ullman JD (1972) Introduction to Automata The-
ory, Language, and Computation. Addison-Wesley, Reading
43. Cole SN (1969) Real time computation by n-dimensional
iterative arrays of finite state machines. IEEE Trans Comput
C-18:349
44. Benjamin SC, Johnson NF (1997) A possible nanometer-scale
computing device based on an additive cellular automata.
Appl Phys Lett 70(17):2321–2323
45. Carter F (1984) The molecular device computer: point of
departure for large scale cellular automata. Physica D 10:
175–194

78 A
Additive Cellular Automata
46. Hillis WD (1984) The connection machine: A computer archi-
tecture based on cellular automata. Physica D 10:213–228
47. Manning FB (1977) An approach to highly integrated
computer-maintained cellular arrays. IEEE Trans Comput
C-26:536
48. Atrubin AJ (1965) A one-dimensional real time iterative mul-
tiplier. IEEE Trans Comput EC-14:394
49. Nishio H (1981) Real time sorting of binary numbers by a 1-
dimensional cellular automata. Kyoto University Technical
Report
50. Fischer PC (1965) Generation of primes by a one-dimensional
real time iterative array. J Assoc Comput Machin 12:388
51. Pries W, Thanailakis A, Card HC (1986) Group properties of
cellular automata and VLSI applications. IEEE Trans Comput
C-35:1013–1024
52. Chaudhuri PP, Chowdhury DR, Nandi S, Chattopadhyay S
(1997) Additive Cellular Automata: Theory and Applications,
vol 1. IEEE Computer Society Press, Los Alamitos
53. Bardell PH, McAnney WH (1986) Pseudo-random arrays for
built-in tests. IEEE Trans Comput C-35(7):653–658
54. Hortensius PD, McLeod RD, Card HC (1989) Parallel random
number generation for VLSI systems using cellular automata.
IEEE Trans Comput 38(10):1466–1473
55. Tsalides P, York TA, Thanailakis A (1991) Pseudorandom num-
ber generation for VLSI systems using cellular automata. IEEE
Proc E: Comput Digit Technol 138:241–249
56. Matsumoto M (1998) Simple cellular automata as pseudoran-
dom m-sequence generators for built-in self-test. ACM Trans
Modeling Comput Simul (TOMACS) 8(1):31–42
57. Tomassini M, Sipper M, Perrenoud M (2000) On the gener-
ation of high-quality random numbers by two-dimensional
cellular automata. IEEE Trans Comput 49(10):1146–1151
58. Das AK, Chaudhuri PP (1989) An efficient on-chip determin-
istic test pattern generation scheme. Euromicro J, Micropro-
cess, Microprogramm 26:195–204
59. Serra M (1990) Algebraic analysis and algorithms for linear
cellular automata over GF(2) and their applications to digital
circuit testing. Congressus Numerantium 75:127–139
60. Das AK, Chaudhuri PP (1993) Vector space theoretical anal-
ysis of additive cellular automata and its applications for
pseudo-exhaustive test pattern generation. IEEE Trans Com-
put 42:340–352
61. Tziones P, Tsalides P, Thanailakis A (1994) A new cellular
automaton-based nearest neighbor pattern classifier and
its VLSI implementation. IEEE Trans VLSI Implement 2(3):
343–353
62. Mrugalski G, Rajski J, Tyszer J (2000) Cellular automata-
based test pattern generators with phase shifters. IEEE Trans
Comput-Aided Des 19(8):878–893
63. Sikdar BK, Ganguly N, Chaudhuri PP (2002) Design of hierar-
chical cellular automata for on-chip test pattern generator.
IEEE Trans Comput Assist Des 21(12):1530–1539
64. Hortensius PD, McLeod RD, Card HC (1990) Cellular automata
based signature analysis for built-in self-test. IEEE Trans Com-
put C-39:1273–1283
65. Serra M, Slater T, Muzio JC, Miller DM (1990) Analysis of one di-
mensional cellular automata and their aliasing probabilities.
IEEE Trans Comput-Aided Des 9:767–778
66. Dasgupta P, Chattopadhyay S, Sengupta I (2001) Theory and
application of nongroup cellular automata for message au-
thentification. J Syst Architecture 47(7):383–404
67. Chowdhury DR, Basu S, Gupta IS, Chaudhuri PP (1994) Design
of CAECC cellular automata based error correcting code. IEEE
Trans Comput 43:759–764
68. Chowdhury DR, Gupta IS, Chaudhuri PP (1995) Cellular au-
tomata based byte error correcting code. IEEE Trans Comput
44:371–382
69. Chowdhury DR, Gupta IS, Chaudhuri PP (1995) A low-cost
high-capacity associative memory design using cellular au-
tomata. IEEE Trans Comput 44:1260–1264
70. Nandi S, Kar BK, Chaudhuri PP (1994) Theory and applica-
tion of cellular automata in cryptography. IEEE Trans Comput
43(12):1346–1357
71. Cattell K, Muzio JC (1996) Synthesis of one-dimensional lin-
ear hybrid cellular automata. IEEE Trans Comput-Aided Des
15:325–335
72. Cattell K, Zhang S, Serra M, Zmuzio JC (1999) 2-by-n hybrid
cellular automata with regular configuration: Theory and ap-
plications. IEEE Trans Comput 48(3):285–295
73. Hedlund GA (1969) Endomorphisms and automorphisms of
the shift dynamical system. Math Syst Theory 4:320–375
74. Wolfram S (1983) Statistical mechanics of cellular automata.
Rev Mod Phys 55:601–644
75. Gardner M (1970) The fantastic combinations of John Con-
way’s new solitaire game ‘life’. Sci Am 223:120–123
76. Gardner M (1971) On cellular automata self-reproduction, the
Garden of Eden and the game of ‘life’. Sci Am 224:112–117
77. Jen E (1988) Cylindrical cellular automata. Commun Math
Phys 118:569–590
78. Tadaki S, Matsufuji S (1993) Periodicity in one-dimensional fi-
nite linear cellular automata. Prog Theor Phys 89(2):325–331
79. Nandi S, Pal ChaudhuriP (1996) Analysis of periodic and inter-
mediate boundary 90/150 cellular automata. IEEE Trans Com-
put 45(1):1–12
80. Chin W, Cortzen B, Goldman J (2001) Linear cellular automata
with boundary conditions. Linear Algebra Appl 322:193–206
81. Voorhees BH (1996) Computational Analysis of One Dimen-
sional Cellular Automata. World Scientific, Singapore
82. Willson S (1984) Cellular automata can generate fractals. Dis-
cret Appl Math 8:91–99
83. Willson S (1984) Growth rates and fractional dimension in cel-
lular automata. Physica D 10:69–74
84. Peitgen H-O, Richter PH (1986) The Beauty of Fractals: Images
of Complex Dynamical Systems. Springer, NY
85. Willson S (1987) The equality of fractional dimension for cer-
tain cellular automata. Physica D 24:179–189
86. Willson S (1987) Computing fractional dimension of additive
cellular automata. Physica D 24:190–206
87. Culik II K, Dube S (1989) Fractals and recurrent behavior of
cellular automata. Complex Syst 3:253–267
88. Willson S (1992) Calculating growth rates and moments for
additive cellular automata. Discret Appl Math 35:47–65
89. Voorhees B (1988) Cellular automata, Pascal’s triangle, and
generation of order. Physica D 31:135–140
90. von Haeseler F, Peitgen H-O, Skordev G (1992) Pascal’s trian-
gle, dynamical systems and attractors. Ergod Theory Dyn Syst
12:479–486
91. Allouche JP, von Haeseler F, Peitgen H-O, Skordev G (1996)
Linear cellular automata, finite automata and Pascal’s trian-
gle. Discret Appl Math 66:1–22
92. von Haeseler F, Peitgen H-O, Skordev G (1992) Linear cellu-
lar automata, substitutions, hierarchical iterated function sys-

Additive Cellular Automata
A
79
tems. In: Encarnacao JL, Peitgen H-O, Sakas G, Englert G (eds)
Fractal Geometry and Computer Graphics. Springer, NY
93. von Haeseler F, Peitgen H-O, Skordev G (1993) Cellular au-
tomata, matrix substitution, and fractals. Ann Math Artif Intell
8(3,4):345–362
94. von Haeseler F, Peitgen H-O, Skordev G (1995) Global anal-
ysis of self-similar features of cellular automata: selected ex-
amples. Physica D 86:64–80
95. Barbé A, von Haeseler F, Peitgen H-O, Skordev G (1995)
Course-graining invariant patterns of one-dimensional two-
state linear cellular automata. Int J Bifurc Chaos 5:1611–1631
96. von Haeseler F, Peitgen H-O, Skordev G (2001) Self-similar
structures of rescaled evolution sets of cellular automata I. Int
J Bifurc Chaos 11(4):913–926
97. Nagler J, Claussen JC (2005) 1/f˛ spectra in elementary cellu-
lar automata and fractal signals. Phys Rev E 71:067103
98. von Haeseler F, Peitgen H-O, Skordev G (2001) Self-similar
structures of rescaled evolution sets of cellular automata II.
Int J Bifurc Chaos 11(4):927–941
99. Barbé A, von Haeseler F, Peitgen H-O, Skordev G (2003)
Rescaled evolution sets of linear cellular automata on a cylin-
der. Int J Bifurc Chaos 13(4):815–842
100. Takahashi S (1990) Cellular automata and multifractals:
Dimension spectra of linear cellular automata. Physica D 45:
36–48
101. Takahashi S (1992) Self-similarity of linear cellular automata.
J Comput Syst Sci 44(1):114–140
102. Martin O, Odlyzko A, Wolfram S (1984) Algebraic properties of
cellular automata. Commun Math Phys 93:219–258
103. Jen E (1986) Global properties of cellular automata. J Stat Phys
43(1/2):219–242
104. Jen E (1988) Linear cellular automata and recurring se-
quences in finite fields. Commun Math Phys 119:13–28
105. Guan P, He Y (1986) Exact results for deterministic cellular au-
tomata with additive rules. J Stat Phys 43(3/4):463–478
106. Das AK, Sanyal A, Chaudhuri PP (1992) On characterization of
cellular automata with matrix algebra. Inf Sci 61:251–277
107. Tadaki S (1994) Orbits in one-dimensional finite linear cellular
automata. Phys Rev E 49(2):1168–1173
108. Voorhees B (2008) Representations of rule 90 and related
rules for periodic, null, and half-infinite boundary conditions.
J Cell Autom 3(1):1–25
109. Davis PJ (1979) Circulant Matrices. Wiley-Interscience, NY
110. Culik II K (1987) On invertible cellular automata. Complex Syst
1:1035–1044
111. Toffoli T, Margolis N (1990) Invertible cellular automata: A re-
view. Physica D 45:229–253
112. Kari J (1990) Reversibility of 2D cellular is undecidable. Phys-
ica D 45:379–385
113. Sutner K (1991) De Bruijn graphs and linear cellular automata.
Complex Syst 5(1):19–30
114. Morita K (1994) Reversible cellular automata. J Inf Process Soc
Japan 35:315–321
115. Moraal H (2000) Graph-theoretical characterization of invert-
ible cellular automata. Physica D 141:1–18
116. Voorhees B (1994) A note on injectivity of additive cellular au-
tomata. Complex Syst 8(3):151–159
117. Lind DA (1984) Applications of ergodic theory and sofic sys-
tems to cellular automata. Physica D 10:36–44
118. Jen E (1989) Limit cycles in one-dimensional cellular au-
tomata. In: Stein DL (ed) Lectures in the Sciences of Complex-
ity. Addison Wesley, Reading
119. Elspas B (1959) The theory of autonomous linear sequential
networks. TRE Trans Circuits CT-6:45–60
120. Stevens JG, Rosensweig RE, Cerkanowicz AE (1993) Transient
and cyclic behavior of cellular automata with null boundary
conditions. J Stat Phys 73(1,2):159–174
121. Stevens JG (1999) On the construction of state diagrams for
cellular automata with additive rules. Inf Sci 115:43–59
122. Thomas DM, Stevens JG, Letteiri S (2006) Characteristic
and minimal polynomials of linear cellular automata. Rocky
Mountain J Math 36(3):1077–1092
123. Sutner K (1988) On 
-automata. Complex Syst 2(1):1–28
124. Sutner K (1988) Additive automata on graphs. Complex Syst
2:649–661
125. Sutner K (2000) Sigma-automata and Chebyschev polynomi-
als. Theor Comput Sci 230:49–73
126. Sutner K (2001) Decomposition of additive CA. Complex Syst
13(2):245–270
127. Lidl R, Pilz G (1984) Applied Abstract Algebra. Springer, NY
128. Voorhees B (1993) Predecessors of cellular automata states: I.
Additive automata. Physica D 68:283–292
129. Bulitko V, Voorhees B, Bulitko V (2006) Discrete baker trans-
formation for linear cellular automata analysis. J Cell Autom
1:40–70
130. Arnold V, Aviz A (1968) Ergodic Problems of Classical Mechan-
ics. Benjamin, NY
131. Illichinsky A (2001) Cellular Automata: A Discrete Universe.
World Scientific, Singapore
132. Wuensche A, Lesser M (1992) The Global Dynamics of Cellular
Automata. Addison Wesley, Reading
133. Chaitin G (2006) Meta Math! Vintage, NY
Books and Reviews
Wolfram S (1994) Cellular Automata and Complexity. Addison Wes-
ley, Reading
Wolfram S (2002) A New Kind of Science. Wolfram Media,
Champaign
Legendi T (1987) Parallel Processing by Cellular Automata and Ar-
rays. Kluwer, Dordrecht
Rietman E (1989) Exploring the Geometry of Nature: Computer
Modeling of Chaos, Fractals, Cellular Automata, and Neural
Networks. McGraw-Hill, NY
Manneville P, Boccara N, Vishniac GY, Bidaux R (1990) Cellu-
lar Automata and Modeling of Complex Physical Systems.
Springer, NY
Goles E, Martinez S (eds) (1992) Statistical Physics, Automata Net-
works and Dynamical Systems. Kluwer, Dordrecht
Boccara N, Goles E, Martenez S (1993) Cellular Automata and Coop-
erative Systems. Kluwer, Dordrecht
Goles E, Martinez S (1994) Cellular Automata, Dynamical Systems
and Neural Networks. Kluwer, Dordrecht
Adamatzky A (1994) Identification of Cellular Automata. Taylor &
Francis, London
Perdang JM, Lejeune A (eds) (1994) Cellular Automata: Prospects in
Astronomy and Astrophysics. World Scientific, Singapore
Goles E, Martenez S (1996) Dynamics of Complex Interacting Sys-
tems. Kluwer, Dordrecht
Sipper M (1997) Evolution of Parallel Cellular Machines: The Cellular
Programming Approach. Springer, NY

80 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
Delorme M, Mazoyer J (eds) (1998) Cellular Automata: A Parallel
Model. Kluwer, Dordrecht
Goles E, Martinez S (eds) (1999) Cellular Automata and Complex
Systems. Kluwer, Dordrecht
Crutchfield JP, Hanson JE (1999) Computational Mechanics of Cel-
lular Processes. University of California Press, Berkeley
Wolf-Gladrow DA (2000) Lattice-Gas Cellular Automata and Lattice
Boltzmann Models. Springer, NY
Lafe O (2000) Cellular Automata Transformations: Theory and Ap-
plications in Multimedia Compression, Encryption and Mod-
eling. Kluwer, Dordrecht
Yang T (2001) Cellular Image Processing. Nova, NY
Griffeath D, Moore C (eds) (2003) New Constructions in Cellular Au-
tomata. Oxford University Press, NY
Deutsch A, Dormann S (2004) Cellular Automata Modeling of Bio-
logical Pattern Formation: Characterization, Applications, and
Analysis. Springer, Berlin
Amos M (2004) Cellular Computing. Oxford University Press, NY
Rothman DH, Zaleski S (2004) Lattice-Gas Cellular Automata: Sim-
ple Models of Complex Hydrodynamics. Cambridge Univer-
sity Press, Cambridge
Batty M (2005) Cities and Complexity: Understanding Cities with
Cellular Automata, Agent-Based Models, and Fractals. MIT
Press, Cambridge
Kier LB, Seybold PG, Cheng C-K (2005) Cellular Automata Model-
ing of Chemical Systems: A Testbook and Laboratory Manual.
Springer, NY
Schiff JL (2007) Cellular Automata: A Discrete View. Wiley-
Interscience, NY
Gros C (2007) Complex and Adaptive Dynamical Systems: A Primer.
Springer, NY
Bandini S, Moroni L (eds) (1996) ACRI ‘96: Proceedings of the 2nd
International Conference on Cellular Automata for Research
and Industry. Springer, NY
Bandini S, Serra R, Liverani FS (eds) (1998) Cellular Automata: Re-
search Towards Industry, ACRI ‘98: Proceedings of the 3rd
International Conference on Cellular Automata for Research
and Industry. Springer, NY
Bandini S, Worsch T (eds) (2000) Theory and Practical Issues on
Cellular Automata: Proceedings of the 4th International Con-
ference on Cellular Automata for Research and Industry.
Springer, NY
Tomassini M, Chopard B (eds) (2002) Cellular Automata: Proceed-
ings of the 5th International Conference on Cellular Au-
tomata for Research and Industry. Springer, NY
Sloot PMA, Chopard B, Hoekstra AG (eds) (2004) Cellular Automata:
Proceedings of the 6th International Conference on Cellular
Automata for Research and Industry. Springer, NY
El Yacoubi S, Chopard B, Bandini S (eds) (2006) Cellular Automata:
Proceedings of the 7th International Conference on Cellular
Automata for Research and Industry. Springer, NY
Websites
http://cell-auto.com/links/ Gives many links to other sites on cellu-
lar automata
http://www.theory.org/complexity/cdpt/html/node4.html
Pro-
vides reviews of theoretical aspects of cellular automata
http://www.ddlab.com An excellent site; it provides access to the
Discrete Dynamics Lab program, a valuable asset in work on
cellular automata and random Boolean networks
http://cellular.ci.ulsa.mx Provides access to a number of worth-
while unpublished papers and a number of useful references
Adomian Decomposition Method
Applied to Non-linear Evolution
Equations in Soliton Theory
ABDUL-MAJID WAZWAZ
Department of Mathematics, Saint Xavier University,
Chicago, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Adomian Decomposition Method
and Adomian Polynomials
Modiﬁed Decomposition Method
and Noise Terms Phenomenon
Solitons, Peakons, Kinks, and Compactons
Solitons of the KdV Equation
Kinks of the Burgers Equation
Peakons of the Camassa–Holm Equation
Compactons of the K(n,n) Equation
Future Directions
Bibliography
Glossary
Solitons Solitons appear as a result of a balance between
a weakly nonlinear convection and a linear dispersion.
The solitons are localized highly stable waves that re-
tains its identity: shape and speed, upon interaction,
and resemble particle like behavior. In the case of a col-
lision, solitons undergo a phase shift.
Types of solitons Solitary waves, which are localized trav-
eling waves, are asymptotically zero at large distances
and appear in many structures such as solitons, kinks,
peakons, cuspons, and compactons, among others.
Solitons appear as a bell-shaped sech proﬁle. Kink
waves rise or descend from one asymptotic state to
another. Peakons are peaked solitary-wave solutions.
Cuspons exhibit cusps at their crests. In the peakon
structure, the traveling-wave solutions are smooth ex-
cept for a peak at a corner of its crest. Peakons are the
points at which spatial derivative changes sign so that
peakons have a ﬁnite jump in the ﬁrst derivative of the
solution u(x; t). Unlike peakons, where the derivatives

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
81
at the peak diﬀer only by a sign, the derivatives at the
jump of a cuspon diverge.
Compactons are solitons with compact spatial support
such that each compacton is a soliton conﬁned to a ﬁ-
nite core or a soliton without exponential tails. Com-
pactons are generated due to the delicate interaction
between the eﬀect of the genuine nonlinear convection
and the genuinely nonlinear dispersion.
Adomian method The Adomian decomposition method
approaches linear and nonlinear, and homogeneous
and inhomogeneous diﬀerential and integral equations
in a uniﬁed way. The method provides the solution
in a rapidly convergent series with terms that are ele-
gantly determined in a recursive manner. The method
can be used to obtain closed-form solutions, if such so-
lutions exist. A truncated number of the obtained se-
ries can be used for numerical purposes. The method
was modiﬁed to accelerate the computational process.
The noise terms phenomenon, which may appear for
inhomogeneous cases, can give the exact solution in
two iterations only.
Definition of the Subject
Nonlinear phenomena play a signiﬁcant role in many
branches of applied sciences such as applied mathemat-
ics, physics, biology, chemistry, astronomy, plasma, and
ﬂuid dynamics. Nonlinear dispersive equations that gov-
ern these phenomena have the genuine soliton property.
Solitons are pulses that propagate without any change
of its identity, i. e., shape and speed, during their travel
through a nonlinear dispersive medium [1,5,34]. Solitons
resemble properties of a particle, hence the suﬃx on is
used [19,20].
Solitons exist in many scientiﬁc branches, such as
optical ﬁber photonics, ﬁber lasers, plasmas, molecular
systems, laser pulses propagating in solids, liquid crys-
tals, nonlinear optics, cosmology, and condensed-matter
physics. Based on its importance in many ﬁelds, a huge
amount of research work has been conducted during the
last four decades to make more progress in understand-
ing the soliton phenomenon. A variety of very powerful
algorithms has been used to achieve this goal. The Ado-
mian decomposition method introduced in [2,6,21,22,23,
24,25,26], which will be used in this work, is one of the
reliable methods that has been used recently.
Introduction
The aim of this work is to apply the Adomian decom-
position method to derive speciﬁc types of soliton so-
lutions. Solitons were discovered experimentally by John
Scott Russell in 1844. Korteweg and de Vries in 1895 in-
vestigated analytically the soliton concept [10], where they
derived the pioneer equation of solitons, well known as
the KdV equation, that models the height of the surface of
shallow water in the presence of solitary waves. Moreover,
Zabusky and Kruskal [35] investigated this phenomenon
analytically in 1965. Since 1965, a huge number of re-
search works have been conducted on nonlinear disper-
sive and dissipative equations. The aim of these works
has been to study thoroughly the characteristics of soliton
solutions and to study various types of solitons that ap-
pear as a result of these equations. Several reliable meth-
ods were used in the literature to handle nonlinear dis-
persive equations. Hirota’s bilinear method [8,9] has been
used for single- and multiple-soliton solutions. The in-
verse scattering method [1] has been widely used. For
single-soliton solutions, several methods, such as the tanh
method [13,14,15], the pseudospectral method, and the
truncated Painlevé expansion, are used.
However, in this work, the decomposition method, in-
troduced by Adomian in 1984, will be applied to derive the
desired types of soliton solutions. The method approaches
all problems in a uniﬁed way and in a straightforward
manner. The method computes the solution in a fast con-
vergent series with components that are elegantly deter-
mined. Unlike other methods, the initial or boundary con-
ditions are necessary for the use of the Adomian method.
Adomian Decomposition Method
and Adomian Polynomials
The Adomian decomposition method, developed by
George Adomian in 1984, has been receiving much atten-
tion in applied mathematics in general and in the area of
initial value and boundary value problems in particular.
Moreover, it is also used in the area of series solutions for
numerical purposes. The method is powerful and eﬀec-
tive and can be used in linear or nonlinear, ordinary or
partial diﬀerential equations, and integral equations. The
decomposition method demonstrates fast convergence of
the solution and provides numerical approximation with
a high level of accuracy. The method handles applied prob-
lems directly and in a straightforward manner without us-
ing linearization, perturbation, or any other restrictive as-
sumption that might change the physical behavior of the
physical model under investigation.
The method is eﬀectively addressed and thoroughly
used by many researchers in the literature [1,21,22,23,24,
25,26]. It is important to indicate that well-known meth-
ods, such as Backlund transformation, inverse scattering
method, Hirota’s bilinear formalism, the tanh method, and

82 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
many others, can handle problems without using initial or
boundary value conditions. For the Adomian decomposi-
tion method, the initial or boundary conditions are neces-
sary to conduct the determination of the components re-
cursively. However, some of the standard methods require
huge calculations, whereas the Adomian method mini-
mizes the volume of computational work.
The Adomian decomposition method consists in de-
composing the unknown function u(x; t) of any equation
into a sum of an inﬁnite number of components given by
the decomposition series
u(x; t) D
1
X
nD0
un(x; t) ;
(1)
where the components un(x; t); n  0 are to be deter-
mined in a recursive manner. The decomposition method
concerns itself with the determination of the components
u0(x; t); u1(x; t); u2(x; t); : : : individually. The determina-
tion of these components can be obtained through a re-
cursive relation that usually involves evaluation of simple
integrals.
We now give a clear overview of Adomian decompo-
sition method. Consider the linear diﬀerential equation
written in an operator form by
Lu C Ru D f ;
(2)
where L is the lower-order derivative, which is assumed
to be invertible, R is a linear diﬀerential operator of order
greater than L, and f is a source term. We next apply the
inverse operator L1 to both sides of Eq. (2) and use the
given initial or boundary condition to get
u(x; t) D g  L1(Ru) ;
(3)
where the function g represents the terms arising from in-
tegrating the source term f and from using the given con-
ditions; all are assumed to be prescribed. Substituting the
inﬁnite series of components
u(x; t) D
1
X
nD0
un(x; t)
(4)
into both sides of (3) yields
1
X
nD0
un D g  L1
 
R
 1
X
nD0
un
!
:
(5)
The decomposition method suggests that the zeroth com-
ponent u0 is usually deﬁned by all terms not included un-
der the inverse operator L1, which arise from the initial
data and from integrating the inhomogeneous term. This
in turn gives the formal recursive relation
u0(x; t) D g ;
ukC1(x; t) D L1 (R(uk)) ;
k  0 ;
(6)
or, equivalently,
u0(x; t) D g ;
u1(x; t) D L1 (R(u0)) ;
u2(x; t) D L1 (R(u1)) ;
u3(x; t) D L1 (R(u2)) ;
::: :
(7)
The diﬀerential equation under consideration is now re-
duced to integrals that can be easily evaluated. Having de-
termined these components u0(x; t); u1(x; t); u2(x; t); : : :,
we then substitute the obtained components into (4) to
obtain the solution in a series form. The determined se-
ries may converge very rapidly to a closed-form solution,
if an exact solution exists. For concrete problems, where
a closed-form solution is not obtainable, a truncated num-
ber of terms is usually used for numerical purposes. Few
terms of the truncated series give an approximation with
a high degree of accuracy. The convergence concept of the
decomposition series is investigated thoroughly in the lit-
erature.
Several signiﬁcant studies were conducted to compare
the performance of the Adomian method with other meth-
ods, such as Picard’s method, Taylor series method, ﬁnite
diﬀerences method, perturbation techniques, and others.
The conclusions emphasized the fact that the Adomian
method has many advantages and requires less computa-
tional work compared to existing techniques.
The Adomian method and many others applied this
method to many deterministic and stochastic problems.
However, the Adomian method, like some other methods,
suﬀers if the zeroth component u0(x; t) D 0 and makes
the integrand of the right side in (7) u1(x; t) D 0. If the
right side integrand in (7) does not vanish, if it is of a form
such as eu0 or ln(˛ C u0); ˛ > 0, then the method works
eﬀectively.
As stated before, the Adomian method decomposes
the unknown function u(x; t) into an inﬁnite number of
components. However, for nonlinear functions of u(x; t)
such as u2; u3; ln(1 C u); cos u; eu; uux, etc., a special rep-
resentation for nonlinear terms was developed by Ado-
mian and others. Adomian introduced a formal algorithm
to establish the proper representation for all forms of non-
linear functions of u(x; t). This representation of nonlin-

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
83
ear terms is necessary to apply the Adomian method prop-
erly. Several alternative algorithms have been introduced
in the literature by researchers to calculate Adomian poly-
nomials. However, the Adomian algorithm remains the
most commonly applied because it is simple and practical;
therefore, it will be used in this work.
Adomian assumed that the nonlinear term F(u) can be
expressed by an inﬁnite series of the so-called Adomian
polynomials An given in the form
F(u) D
1
X
nD0
An(u0; u1; u2; : : : ; un) ;
(8)
where An can be evaluated for all forms of nonlinearity.
Adomian polynomials An for the nonlinear term F(u)
can be evaluated using the following expression:
An D 1
n!
dn
dn
"
F
 n
X
iD0
i ui
!#
0
; n D 0; 1; 2;    : (9)
The general formula (9) can be simpliﬁed as follows. As-
suming that the nonlinear function is F(u), using (9), Ado-
mian polynomials are given by
A0 D F(u0) ;
A1 D u1F0(u0) ;
A2 D u2F0(u0) C 1
2!u2
1 F00(u0) ;
A3 D u3F0(u0) C u1u2F00(u0) C 1
3!u3
1F000(u0) ;
A4 D u4F0(u0) C
 1
2!u2
2 C u1u3

F00(u0)
C 1
2!u2
1u2F000(u0) C 1
4!u4
1F(iv)(u0) ;
A5 D u5F0(u0) C (u2u3 C u1u4)F00(u0)
C
 1
2!u1u2
2 C 1
2!u2
1u3

F000(u0)
C 1
3!u3
1u2F(iv)(u0) C 1
5!u5
1F(v)(u0) :
(10)
Other polynomials can be generated in a similar manner.
It is clear that A0 depends only on u0, A1 depends only on
u0 and u1, A2 depends only on u0; u1, and u2, and so on.
For F(u) D u2, we ﬁnd
A0 D F(u0) D u2
0 ;
A1 D u1F0(u0) D 2u0u1 ;
A2 D u2F0(u0) C 1
2!u2
1F00(u0) D 2u0u2 C u2
1 ;
A3 D u3F0(u0) C u1u2F00(u0) C 1
3!u3
1F000(u0)
D 2u0u3 C 2u1u2 :
(11)
Modified Decomposition Method
and Noise Terms Phenomenon
A reliable modiﬁcation of the Adomian decomposition
method [22] was developed by Wazwaz in 1999. The mod-
iﬁcation will further accelerate the convergence of the se-
ries solution. As presented earlier, the standard decompo-
sition method admits the use of the recursive relation
u0 D g ;
ukC1 D  L1(Ruk) ;
k  0 :
(12)
The modiﬁed decomposition method introduces a slight
variation to the recursive relation (12) that will lead to the
determination of the components of u in a faster and easier
way. For speciﬁc cases, the function g can be set as the sum
of two partial functions, g1 and g2. In other words, we can
set
g D g1 C g2 :
(13)
This assumption gives a slight qualitative change in the
formation of the recursive relation (12). To reduce the size
of calculations, we identify the zeroth component u0 by
one part of g, that is, g1 or g2. The other part of g can be
added to the component u1 among other terms. In other
words, the modiﬁed recursive relation can be deﬁned as
u0 D g1 ;
u1 D g2  L1(Ru0) ;
ukC1 D  L1(Ruk) ;
k  1 :
(14)
The change occurred in the formation of the ﬁrst two com-
ponents u0 and u1 only [22,24,25]. Although this variation
in the formation of u0 and u1 is slight, it plays a major role
in accelerating the convergence of the solution and in min-
imizing the size of calculations. It is interesting to point
out that by selecting the parts g1 and g2 properly, the exact
solution u(x; t) may be obtained by using very few itera-
tions, and sometimes by evaluating only two components.
Moreover, if g consists of one term only, the standard de-
composition method should be employed in this case.
Another useful feature of the Adomian method is the
noise terms phenomenon. The noise terms may appear
for inhomogeneous problems only. This phenomenon was
addressed by Adomian in 1994. In 1997, Wazwaz investi-
gated the necessary conditions for the appearance of noise
terms in the decomposition series. Noise terms are deﬁned
as identical terms with opposite signs [21] that arise in the
components u0 and u1 particularly, and in other compo-
nents as well. By canceling the noise terms between u0 and
u1, even though u1 contains other terms, the remaining

84 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
noncanceled terms of u0 may give the exact solution of the
equation. Therefore, it is necessary to verify that the non-
canceled terms of u0 satisfy the equation. The noise terms,
if they exist in the components u0 and u1, will provide the
solution in a closed form with only two successive itera-
tions.
It was formally proved by Wazwaz in 1997 that a nec-
essary condition for the appearance of the noise terms is
required. The conclusion made is that the zeroth compo-
nent u0 must contain the exact solution u, among other
terms [21]. Moreover, it was shown that the nonhomo-
geneity condition does not always guarantee the appear-
ance of the noise terms.
Solitons, Peakons, Kinks, and Compactons
There are many types of solitary waves. Solitons, which
are localized traveling waves, are asymptotically zero at
large distances [1,5,7,21,22,23,24,25,26,27,28,29,30,31,32,
33]. Solitons appear as a bell-shaped sech proﬁle. Soli-
ton solution u() results as a balance between nonlin-
earity and dispersion, where u(); u0(); u00();    ! 0 as
 ! ˙1, where  D x  ct, and c is the speed of the wave
prorogation. The soliton solution either decays exponen-
tially as in the KdV equation, or it converges to a con-
stant at inﬁnity such as the kinks of the sine-Gordon equa-
tion. This means that the soliton solutions appear as sech˛
or arctan(e˛(xct)). Moreover, one soliton interacts with
other solitons preserving its permanent form.
Another type of solitary wave is the kink wave,
which rises or descends from one asymptotic state to an-
other [18]. The Burgers equation and the sine-Gordon
equation are examples of nonlinear wave equations that
exhibit kink solutions. It is to be noted that the kink
u() converges to ˙˛, where ˛ is a constant. However,
u0(); u00();    ! 0 as  ! ˙1.
Peakons are peaked solitary-wave solutions and are
another type of solitary-wave solution. In this struc-
ture, the traveling wave solutions are smooth except for
a peak at a corner of its crest. Peakons are the points at
which a spatial derivative changes signs so that peakons
have a ﬁnite jump in the ﬁrst derivative of the solution
u(x; t) [3,4,11,12,32]. This means that the ﬁrst derivative
of u(x; t) has identical values with opposite signs around
the peak.
A signiﬁcant type of soliton is the compacton, which
is a soliton with compact spatial support such that each
compacton is a soliton conﬁned to a ﬁnite core or a soli-
ton without exponential tails [16,17]. Compactons were
formally derived by Rosenau and Hyman in 1993 where
a special kind of the KdV equation was used to derive this
new discovery. Unlike a soliton that narrows as the ampli-
tude increases, a compacton’s width is independent of the
amplitude [16,17,27,28,29,30]. Classical solitons are ana-
lytic solutions, whereas compactons are nonanalytic solu-
tions [16,17]. As will be shown by a graph below, com-
pactons are solitons that are free of exponential wings or
tails. Compactons arise as a result of the delicate interac-
tion between genuine nonlinear terms and the genuinely
nonlinear dispersion, as is the case with the K(n,n) equa-
tion.
Solitons of the KdV Equation
In 1895, Korteweg, together with his Ph.D student de
Vriesf, derived analytically a nonlinear partial diﬀerential
equation, well known now by the KdV equation given in
its simplest form by
ut C 6uux C uxxx D 0; u(x; 0) D
2c2ecx
(1 C ecx)2 :
(15)
The KdV equation is the simplest equation embodying
both nonlinearity and dispersion [5,34]. This equation has
served as a model for the development of solitary-wave
theory. The KdV equation is a completely integrable bi-
Hamiltonian equation. The KdV equation is used to model
the height of the surface of shallow water in the presence
of solitary waves. The nonlinearity represented by uux
tends to localize the wave, while the linear dispersion rep-
resented by uxxx spreads it out. The balance between the
weak nonlinearity and the linear dispersion gives solitons
that consist of single humped waves. The equilibrium be-
tween the nonlinearity uux and the dispersion uxxx of the
KdV equation is stable.
In 1965, Zabusky and Kruskal [35] investigated nu-
merically the nonlinear interaction of a large solitary
wave overtaking a smaller one and discovered that soli-
tary waves underwent nonlinear interactions following the
KdV equation. Further, the waves emerged from this inter-
action retaining their original shape and amplitude, and
therefore conserved energy and mass. The only eﬀect of
the interaction was a phase shift.
To apply the Adomian decomposition method, we ﬁrst
write the KdV Eq. (15) in an operator form:
Ltu D uxxx  6uux ;
(16)
where the diﬀerential operator Lt is
Lt D @
@t ;
(17)

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
85
assuming that the integral operator L1
t
exists and may be
regarded as a onefold deﬁnite integral deﬁned by
L1
t (:) D
Z t
0
(:)dt :
(18)
This means that
L1
t
Lt u(x; t) D u(x; t)  u(x; 0) :
(19)
Applying L1
t
to both sides of (16) and using the initial
condition we ﬁnd
u(x; t) D
2c2ecx
(1 C ecx)2 C L1
t (uxxx  6uux) :
(20)
Notice that the right-hand side contains a linear term uxxx
and a nonlinear term uux. Accordingly, Adomian polyno-
mials for this nonlinear term are given by
A0 D F(u0) D u0u0x ;
A1 D 1
2 Lx(2u0u1) D u0x u1 C u0u1x ;
A2 D 1
2 Lx(2u0u2 C u2
1) D u0xu2 C u1x u1 C u2x u0 ;
A3 D 1
2 Lx(2u0u3 C 2u1u2)
D u0x u3 C u1x u2 C u2x u1 C u3x u0 :
(21)
Recall that the Adomian decomposition method suggests
that the linear function u may be represented by a decom-
position series
u D
1
X
nD0
un ;
(22)
whereas nonlinear term F(u) can be expressed by an inﬁ-
nite series of the so-called Adomian polynomials An given
in the form
F(u) D
1
X
nD0
An(u0; u1; u2; : : : ; un) :
(23)
Using the decomposition identiﬁcation for the linear and
nonlinear terms in (20) yields
1
X
nD0
un(x; t) D
2c2ecx
(1 C ecx)2  L1
t
 1
X
nD0
un(x; t)
!
xxx
 6L1
t
 1
X
nD0
An
!
:
(24)
The Adomian method allows for the use of the recursive
relation
u0(x; t) D
2c2ecx
(1 C ecx)2 ;
ukC1(x; t) D  L1
t
ukxxx
  6L1
t (Ak) ;
k  0 :
(25)
The components un; n  0 can be elegantly calculated
by
u0(x; t) D
2c2ecx
(1 C ecx)2 ;
u1(x; t) D  L1
t
u0xxx
  6L1
t (A0)
D 2c5ecx(ecx  1)
(1 C ecx)3
t ;
u2(x; t) D  L1
t

u1xxx

 6L1
t (A1)
D c8ecx(e2cx  4ecx C 1)
(1 C ecx)4
t2 ;
u3(x; t) D  L1
t

u2xxx

 6L1
t (A2)
D c11ecx(e3cx  11e2cx C 11ecx  1)
3(1 C ecx)5
t3 ;
u4(x; t) D  L1
t
u3xxx
  6L1
t (A3)
D c14ecx(e4cx 26e3cx C66e2cx 26ecx C1)
12(1 C ecx)6
t4 ;
(26)
and so on. The series solution is thus given by
u(x; t) D
2c2ecx
(1 C ecx)2


1 C c3(ecx  1)
(1 C ecx) t C c6(e2cx  4ecx C 1)
2(1 C ecx)2
t2
C c9(e3cx  11e2cx C 11ecx  1)
6(1 C ecx)3
t3 C   

;
(27)
and in a closed form by
u(x; t) D
2c2ec(xc2t)
(1 C ec(xc2t))2 :
(28)
The last equation emphasizes the fact that the dispersion
relation is ! D c3. Moreover, the exact solution (28) can
be rewritten as
u(x; t) D c2
2 sech2 h c
2

x  c2t
i
:
(29)
This in turn gives the bell-shaped soliton solution of the
KdV equation. A typical graph of a bell-shaped soliton is
given in Fig. 1.
The graph shows that solitons are characterized by ex-
ponential wings or tails. The graph also conﬁrms that soli-
tons become asymptotically zero at large distances.
Kinks of the Burgers Equation
Burgers (1895–1981) introduced one of the fundamental
model equations in ﬂuid mechanics [18] that demonstrates

86 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
Adomian Decomposition Method Applied to Non-linear Evolu-
tion Equations in Soliton Theory, Figure 1
Figure 1 shows the soliton graph u(x; t) D sech2(x  ct); c D
1; 5  x; t  5
coupling between nonlinear advection uux and linear dif-
fusion uxx. The Burgers equation appears in gas dynamics
and traﬃc ﬂow. Burgers introduced this equation to cap-
ture some of the features of turbulent ﬂuid in a channel
caused by the interaction of the opposite eﬀects of con-
vection and diﬀusion. The standard form of the Burgers
equation is given by
ut C uux D uxx ;
u(x; 0) D
2c
(1 C e
cx
 )
;
t > 0 ;
 ¤ 0 ;
(30)
where u(x; t) is the velocity and  is a constant that de-
ﬁnes the kinematic viscosity. If the viscosity  D 0, then
the equation is called an inviscid Burgers equation. The in-
viscid Burgers equation will not be examined in this work.
It is the goal of this work to apply the Adomian de-
composition method to the Burgers equation; therefore we
write (30) in an operator form
Ltu D uxx  uux ;
(31)
where the diﬀerential operator Lt is
Lt D @
@t ;
(32)
and as a result
L1
t (:) D
Z t
0
(:)dt :
(33)
This means that
L1
t
Lt u(x; t) D u(x; t)  u(x; 0) :
(34)
Applying L1
t
to both sides of (31) and using the initial
condition we ﬁnd
u(x; t) D
2c
(1 C e
cx
 )
C L1
t (uxx  uux) :
(35)
Notice that the right-hand side contains a linear term and
a nonlinear term uux. The Adomian polynomials for this
nonlinear term uux are the same as in the KdV equation.
Using the decomposition identiﬁcation for the linear
and nonlinear terms in (35) yields
1
X
nD0
un(x; t) D
2c
(1 C e
cx
 )
C L1
t
 

1
X
nD0
un(x; t)
!
xx
 L1
t
 1
X
nD0
An
!
:
(36)
The Adomian method allows for the use of the recursive
relation
u0(x; t) D
2c
(1 C e
cx
n )
;
ukC1(x; t) D L1
t

ukxxx

 L1
t (Ak) ;
k  0 :
(37)
The components un; n  0 can be elegantly calculated by
u0(x; t) D
2c
(1 C e
cx
 )
;
u1(x; t) D L1
t

u0xxx

 L1
t (A0)
D
2c3e
cx

(1 C e
cx
 )2 t ;
u2(x; t) D L1
t

u1xxx

 L1
t (A1)
D c5e
cx
 (e
cx
  1)
2(1 C e
cx
 )3 t2 ;
u3(x; t) D L1
t
u2xxx
  L1
t (A2)
D c7e
cx
 (e
2cx
  4e
cx
 C 1)
33(1 C e
cx
 )4
t3 ;
u4(x; t) D L1
t

u3xxx

 L1
t (A3)
D c9e
cx
 (e
3cx
  11e
2cx
 C 11e
cx
  1)
124(1 C e
cx
 )5
t4 ;
(38)

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
87
Adomian Decomposition Method Applied to Non-linear Evolu-
tion Equations in Soliton Theory, Figure 2
Figure 2 shows the kink graph u(x; t) D tanh(x  ct); c D 1;
10  x; t  10
and so on. The series solution is thus given by
u(x; t) D
2ce
cx

(1 C e
cx
 )

 
e cx
 C
c2
(1 C e
cx
 )
t C
c4(e
cx
  1)
22(1 C e
cx
 )2 t2
C c6(e
2cx
  4e
cx
 C 1)
63(1 C e
cx
 )3
t3 C   
!
;
(39)
and in a closed form by
u(x; t) D
2c

1 C e
c
 (xct) ;
 ¤ 0 ;
(40)
or equivalently
u(x; t) D c

1  tanh
h c
2 (x  ct)
i
:
(41)
Figure 2 shows a kink graph.
The graph shows that the kink converges to ˙1 as
 ! ˙1.
Peakons of the Camassa–Holm Equation
Camassa and Holm [4] derived in 1993 a completely inte-
grable wave equation
ut C 2kux  uxxt C 3uux D 2uxuxx C uuxxx
(42)
by retaining two terms that are usually neglected in the
small-amplitude shallow-water limit [3]. The constant k
is related to the critical shallow-water wave speed. This
equation models the unidirectional propagation of water
waves in shallow water. The CH Eq. (42) diﬀers from the
well-known regularized long wave (RLW) equation only
through the two terms on the right-hand side of (42).
Moreover, this equation has an integrable bi-Hamiltonian
structure and arises in the context of diﬀerential geometry,
where it can be seen as a reexpression for geodesic ﬂow
on an inﬁnite-dimensional Lie group. The CH equation
admits a second-order isospectral problem and allows for
peaked solitary-wave solutions, called peakons.
In this work, we will apply the Adomian method to the
CH equation
ut uxxt C3uux D 2uxuxx Cuuxxx ;
u(x; 0) D cejxj;
(43)
or equivalently
ut D uxxt 3uux C2uxuxx Cuuxxx ;
u(x; 0) D cejxj;
(44)
where we set k D 0.
Proceeding as before, the CH Eq. (44) in an operator
form is as follows:
Ltu D uxxt3uux C2uxuxx Cuuxxx ;
u(x; 0) D cejxj;
(45)
where the diﬀerential operator Lt is as deﬁned above. It is
important to point out that the CH equation includes three
nonlinear terms; therefore, we derive the following three
sets of Adomian polynomials for the terms uux; uxuxx,
and uuxxx:
A0 D u0u0x ;
A1 D u0x u1 C u0u1x ;
A2 D u0x u2 C u1x u1 C u2x u0 ;
(46)
B0 D u0x u0xx ;
B1 D u0xx u1x C u0x u1xx ;
B2 D u0xx u2x C u1x u1xx C u0x u2xx ;
(47)
and
C0 D u0u0xxx ;
C1 D u0xxx u1 C u0u1xxx ;
C2 D u0xxx u2 C u1u1xxx C u0u2xxx :
(48)

88 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
Applying the inverse operator L1
t
to both sides of (45)
and using the initial condition we ﬁnd
u(x; t) D cejxjCL1
t
(uxxt  3uux C 2uxuxx C uuxxx) :
(49)
Case
1.
For
x > 0,
the
initial
condition
will
be
u(x; 0) D ex. Using the decomposition series (1) and
Adomian polynomials in (49) yields
1
X
nD0
un(x; t) D cex C L1
t
 1
X
nD0
un(x; t)
!
xxt
C L1
t
 
3
1
X
nD0
An C 2
1
X
nD0
Bn C
1
X
nD0
Cn
!
:
(50)
The Adomian scheme allows the use of the recursive
relation
u0(x; t) D cex ;
ukC1(x; t) D L1
t
(uk(x; t))xxt
C L1
t
(3 Ak C 2Bk C Ck) ;
k  0 :
(51)
The components un; n  0 can be elegantly computed by
u0(x; t) D cex ;
u1(x; t) D L1
t
(u0(x; t))xxt C L1
t
(3 A0 C 2B0 C C0)
D c2ex t ;
u2(x; t) D L1
t
(u1(x; t))xxt C L1
t
(3 A1 C 2B1 C C1)
D 1
2! c3ex t2 ;
u3(x; t) D L1
t
(u2(x; t))xxt C L1
t
(3 A2 C 2B2 C C2)
D 1
3! c4ex t3 ;
u4(x; t) D L1
t
(u3(x; t))xxt C L1
t
(3 A3 C 2B3 C C3)
D 1
4! c4ex t4 ;
(52)
and so on. The series solution for x > 0 is given by
u(x; t) D cex


1 C ct C 1
2!(ct)2 C 1
3!(ct)3 C 1
4!(ct)4 C   

;
(53)
and in a closed form by
u(x; t) D ce(xct) :
(54)
Case 2. For x < 0, the initial condition will be u(x; 0) D
ex. Proceeding as before, we obtain
u0(x; t) D cex ;
u1(x; t) D L1
t
(u0(x; t))xxt C L1
t
(3 A0 C 2B0 C C0)
D  c2ex t ;
u2(x; t) D L1
t
(u1(x; t))xxt C L1
t
(3 A1 C 2B1 C C1)
D 1
2! c3ex t2 ;
u3(x; t) D L1
t
(u2(x; t))xxt C L1
t
(3 A2 C 2B2 C C2)
D  1
3!c4ex t3 ;
u4(x; t) D L1
t
(u3(x; t))xxt C L1
t
(3 A3 C 2B3 C C3)
D 1
4! c4ex t4 ;
(55)
and so on. The series solution for x < 0 is given by
u(x; t) D cex


1  ct C 1
2!(ct)2  1
3!(ct)3 C 1
4!(ct)4 C   

;
(56)
and in a closed form by
u(x; t) D ce(xct) :
(57)
Combining the results for both cases gives the peakon so-
lution
u(x; t) D cejxctj :
(58)
Figure 3 shows a peakon graph.
The graph shows that a peakon with a peak at a corner
is generated with equal ﬁrst derivatives at both sides of the
peak, but with opposite signs.
Compactons of the K(n,n) Equation
The K(n,n) equation [16,17] was introduced by Rosenau
and Hyman in 1993. This equation was investigated exper-
imentally and analytically. The K(m,n) equation is a gen-
uinely nonlinear dispersive equation, a special type of the
KdV equation, named K(m,n), of the form
ut C (um)x C (un)xxx D 0 ;
n > 1 :
(59)
Compactons, which are solitons with compact support
or strict localization of solitary waves, have been investi-
gated thoroughly in the literature. The delicate interaction
between the eﬀect of the genuine nonlinear convection
(un)x and the genuinely nonlinear dispersion of (un)xxx
generates solitary waves with exact compact support that
are called compactons. It was also discovered that solitary

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
89
Adomian Decomposition Method Applied to Non-linear Evolu-
tion Equations in Soliton Theory, Figure 3
Figure 3 shows the peakon graph u(x; t) D ej(xct)j; c D 1;
2  x; t  2
waves may compactify under the inﬂuence of nonlinear
dispersion, which is capable of causing deep qualitative
changes in the nature of genuinely nonlinear phenom-
ena [16,17]. Unlike solitons that narrow as the amplitude
increases, a compacton’s width is independent of the am-
plitude. Compactons such as drops do not possess inﬁnite
wings; hence they interact among themselves only across
short distances.
Compactons are nonanalytic solutions, whereas classi-
cal solitons are analytic solutions. The points of nonana-
lyticity at the edge of a compacton correspond to points of
genuine nonlinearity for the diﬀerential equation and in-
troduce singularities in the associated dynamical system
for the traveling waves [16,17,27,28,29,30]. Compactons
were proved to collide elastically and vanish outside a ﬁ-
nite core region. This discovery was studied thoroughly by
many researchers who were involved with identical non-
linear dispersive equations. It is to be noted that solutions
were obtained only for cases where m D n. However, for
m ¤ n, solutions have not yet been determined.
Without loss of generality, we will examine the special
K(3,3) initial value problem
ut C(u3)x C(u3)xxx D 0;
u(x; 0) D
p
6c
2
cos
1
3 x

:
(60)
We ﬁrst write the K(3,3) Eq. (60) in an operator form:
Ltu D (u3)x (u3)xxx ;
u(x; 0) D
p
6c
2
cos
1
3 x

;
(61)
where the diﬀerential operator Lt is as deﬁned above. The
diﬀerential operator Lt is deﬁned by
Lt D @
@t :
(62)
Applying L1
t
to both sides of (61) and using the initial
condition we ﬁnd
u(x; t) D
p
6c
2
cos
1
3 x

 L1
t
(u3)x C(u3)xxx
: (63)
Notice that the right-hand side contains two nonlinear
terms (u3)x and (u3)xxx. Accordingly, the Adomian poly-
nomials for these terms are given by
A0 D (u3
0)x ;
A1 D (3u2
0u1)x ;
A2 D (3u2
0u2 C 3u0u2
1)x ;
A3 D (3u2
0u3 C 6u0u1u2 C u3
1)x ;
(64)
and
B0 D (u3
0)xxx ;
B1 D (3u2
0u1)xxx ;
B2 D (3u2
0u2 C 3u0u2
1)xxx ;
B3 D (3u2
0u3 C 6u0u1u2 C u3
1)xxx ;
(65)
respectively.
Proceeding as before, Eq. (63) becomes
1
X
nD0
un(x; t) D
p
6c
2
cos
1
3 x

L1
t
 1
X
nD0
An C Bn
!
:
(66)
This gives the recursive relation
u0(x; t) D
p
6c
2
cos
1
3 x

;
ukC1(x; t) D  L1
t

L1
t (Ak C Bk)

;
k  0 :
(67)

90 A
Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
The components un; n  0 can be recursively determined
as
u0(x; t) D
p
6c
2
cos
1
3 x

;
u1(x; t) D c
p
6c
6
sin
1
3x

t ;
u2(x; t) D  c2p
6c
36
cos
1
3 x

t2 ;
u3(x; t) D  c3p
6c
324
sin
1
3 x

t3 ;
u4(x; t) D c4p
6c
3888 cos
1
3 x

t4 ;
u5(x; t) D c5p
6c
58320 sin
1
3x

t5 ;
(68)
and so on. The series solution is thus given by
u(x; t)
D
p
6c
2
cos
1
3 x
  
1  1
2!
ct
3
2
C 1
4!
ct
3
4
C   
!
C
p
6c
2
sin
1
3 x
  
ct
3  1
3!
ct
3
3
C 1
5!
ct
5
5
C  
!
:
(69)
This is equivalent to
u(x; t)
D
p
6c
2

cos
1
3 x

cos
1
3 ct

Csin
1
3 x

sin
1
3 ct

;
(70)
or equivalently
u(x; t) D
8
<
:
n p
6c
2
cos  1
3(x  ct)o
;
j x  ct j 3	
2 ;
0
otherwise :
(71)
This in turn gives the compacton solution of the K(3,3)
equation. Figure 4 shows a compacton conﬁned to a ﬁnite
core without exponential wings.
The graph shows a compacton: a soliton free of expo-
nential wings.
It is interesting to point out that the generalized so-
lution of the K(n,n) equation, n > 1, is given by the
Adomian Decomposition Method Applied to Non-linear Evolu-
tion Equations in Soliton Theory, Figure 4
Figure 4 shows the compacton graph u(x; t) D cos
1
2 (x  ct);
c D 1; 0  x; t  1
compactons
u(x; t)
D
8
<
:
n
2cn
(nC1) cos2  n1
2n (x  ct)
o
1
n1 ;
j x  ct j  	
2 ;
0
otherwise ;
(72)
and
u3(x; t)
D
8
<
:
n
2cn
(nC1) sin2  n1
2n (x  ct)o
1
n1 ;
j x  ct j 	
 ;
0
otherwise :
(73)
Future Directions
The most signiﬁcant advantage of the Adomian method is
that it attacks any problem without any need for a trans-
formation formula or any restrictive assumption that may
change the physical behavior of the solution. For singu-
lar problems, it was possible to overcome the singular-
ity phenomenon and to attain practically a series solu-
tion in a standard way. Moreover, for problems on un-
bounded domains, combining the obtained series solution

Adomian Decomposition Method Applied to Non-linear Evolution Equations in Soliton Theory
A
91
by the Adomian method with Padé approximants pro-
vides a promising tool to handle boundary value problems.
The Padé approximants, which often show superior per-
formance over series approximations, provide a promising
tool for use in applied ﬁelds.
As stated above, unlike other methods such as Hirota
and the inverse scattering method, where solutions can be
obtained without using prescribed conditions, the Ado-
mian method requires such conditions. Moreover, these
conditions must be of a form that does not provide zero
for the integrand of the ﬁrst component u1(x; t). Such
a case should be addressed to enhance the performance
of the method. Another aspect that should be addressed
is Adomian polynomials. The existing techniques require
tedious work to evaluate it. Most importantly, the Ado-
mian method guarantees only one solution for nonlinear
problems. This is an issue that should be investigated to
improve the performance of the Adomian method com-
pared to other methods.
On the other hand, it is important to further examine
the N-soliton solutions by simpliﬁed forms, such as the
form given by Hereman and Nuseir in [7]. The bilinear
form of Hirota is not that easy to use and is not always
attainable for nonlinear models.
Bibliography
Primary Literature
1. Ablowitz MJ, Clarkson PA (1991) Solitons, nonlinear evolution
equations and inverse scattering. Cambridge University Press,
Cambridge
2. Adomian G (1984) A new approach to nonlinear partial differ-
ential equations. J Math Anal Appl 102:420–434
3. Boyd PJ (1997) Peakons and cashoidal waves: travelling wave
solutions of the Camassa–Holm equation. Appl Math Comput
81(2–3):173–187
4. Camassa R, Holm D (1993) An integrable shallow water equa-
tion with peaked solitons. Phys Rev Lett 71(11):1661–1664
5. Drazin PG, Johnson RS (1996) Solitons: an introduction. Cam-
bridge University Press, Cambridge
6. Helal MA, Mehanna MS (2006) A comparison between two dif-
ferent methods for solving KdV-Burgers equation. Chaos Soli-
tons Fractals 28:320–326
7. Hereman W, Nuseir A (1997) Symbolic methods to construct
exact solutions of nonlinear partial differential equations. Math
Comp Simul 43:13–27
8. Hirota R (1971) Exact solutions of the Korteweg–de Vries
equation for multiple collisions of solitons. Phys Rev Lett
27(18):1192–1194
9. Hirota R (1972) Exact solutions of the modified Korteweg–
de Vries equation for multiple collisions of solitons. J Phys Soc
Jpn 33(5):1456–1458
10. Korteweg DJ, de Vries G (1895) On the change of form of long
waves advancing in a rectangular canal and on a new type of
long stationary waves. Philos Mag 5th Ser 36:422–443
11. Lenells J (2005) Travelling wave solutions of the Camassa–
Holm equation. J Differ Equ 217:393–430
12. Liu Z, Wang R, Jing Z (2004) Peaked wave solutions of Ca-
massa–Holm equation. Chaos Solitons Fractals 19:77–92
13. Malfliet W (1992) Solitary wave solutions of nonlinear wave
equations. Am J Phys 60(7):650–654
14. Malfliet W, Hereman W (1996) The tanh method: I. Exact so-
lutions of nonlinear evolution and wave equations. Phys Scr
54:563–568
15. Malfliet W, Hereman W (1996) The tanh method: II. Perturba-
tion technique for conservative systems. Phys Scr 54:569–575
16. Rosenau P (1998) On a class of nonlinear dispersive-dissipative
interactions. Phys D 230(5/6):535–546
17. Rosenau P, Hyman J (1993) Compactons: solitons with finite
wavelengths. Phys Rev Lett 70(5):564–567
18. Veksler A, Zarmi Y (2005) Wave interactions and the analysis of
the perturbed Burgers equation. Phys D 211:57–73
19. Wadati M (1972) The exact solution of the modified Korteweg–
de Vries equation. J Phys Soc Jpn 32:1681–1687
20. Wadati M (2001) Introduction to solitons. Pramana J Phys
57(5/6):841–847
21. Wazwaz AM (1997) Necessary conditions for the appearance of
noise terms in decomposition solution series. Appl Math Com-
put 81:265–274
22. Wazwaz AM (1998) A reliable modification of Adomian’s de-
composition method. Appl Math Comput 92:1–7
23. Wazwaz AM (1999) A comparison between the Adomian de-
composition method and the Taylor series method in the se-
ries solutions. Appl Math Comput 102:77–86
24. Wazwaz AM (2000) A new algorithm for calculating Ado-
mian polynomials for nonlinear operators. Appl Math Comput
111(1):33–51
25. Wazwaz AM (2001) Exact specific solutions with solitary pat-
terns for the nonlinear dispersive K(m,n) equations. Chaos,
Solitons and Fractals 13(1):161–170
26. Wazwaz AM (2002) General solutions with solitary patterns
for the defocusing branch of the nonlinear dispersive K(n,n)
equations in higher dimensional spaces. Appl Math Comput
133(2/3):229–244
27. Wazwaz AM (2003) An analytic study of compactons struc-
tures in a class of nonlinear dispersive equations. Math Com-
put Simul 63(1):35–44
28. Wazwaz AM (2003) Compactons in a class of nonlinear disper-
sive equations. Math Comput Model l37(3/4):333–341
29. Wazwaz AM (2004) The tanh method for travelling wave solu-
tions of nonlinear equations. Appl Math Comput 154(3):713–
723
30. Wazwaz AM (2005) Compact and noncompact structures for
variants of the KdV equation. Int J Appl Math 18(2):213–221
31. Wazwaz AM (2006) New kinds of solitons and periodic solu-
tions to the generalized KdV equation. Numer Methods Partial
Differ Equ 23(2):247–255
32. Wazwaz AM (2006) Peakons, kinks, compactons and soli-
tary patterns solutions for a family of Camassa–Holm equa-
tions by using new hyperbolic schemes. Appl Math Comput
182(1):412–424
33. Wazwaz AM (2007) The extended tanh method for new soli-
tons solutions for many forms of the fifth-order KdV equations.
Appl Math Comput 184(2):1002–1014
34. Whitham GB (1999) Linear and nonlinear waves. Wiley, New
York

92 A
Agent Based Computational Economics
35. Zabusky NJ, Kruskal MD (1965) Interaction of solitons in a col-
lisionless plasma and the recurrence of initial states. Phys Rev
Lett 15:240–243
Books and Reviews
Adomian G (1994) Solving Frontier Problems of Physics: The De-
composition Method. Kluwer, Boston
Burgers JM (1974) The nonlinear diffusion equation. Reidel, Dor-
drecht
Conte R, Magri F, Musette M, Satsuma J, Winternitz P (2003) Lec-
ture Notes in Physics: Direct and Inverse methods in Nonlinear
Evolution Equations. Springer, Berlin
Hirota R (2004) The Direct Method in Soliton Theory. Cambridge
University Press, Cambridge
Johnson RS (1997) A Modern Introduction to the Mathematical
Theory of Water Waves. Cambridge University Press, Cam-
bridge
Kosmann-Schwarzbach Y, Grammaticos B, Tamizhmani KM (2004)
Lecture Notes in Physics: Integrability of Nonlinear Systems.
Springer, Berlin
Wazwaz AM (1997) A First Course in Integral Equations. World Sci-
entific, Singapore
Wazwaz AM (2002) Partial Differential Equations: Methods and Ap-
plications. Balkema, The Netherlands
Agent Based
Computational Economics
MOSHE LEVY
The Hebrew University, Jerusalem, Israel
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Some of the Pioneering Studies
Illustration with the LLS Model
Summary and Future Directions
Bibliography
Glossary
Agent-based simulation A simulation of a system of
multiple interacting agents (sometimes also known as
“microscopic simulation”). The “micro” rules govern-
ing the actions of the agents are known, and so are
their rules of interaction. Starting with some initial
conditions, the dynamics of the system are investigated
by simulating the state of the system through discrete
time steps. This approach can be employed to study
general properties of the system, which are not sensi-
tive to the initial conditions, or the dynamics of a spe-
ciﬁc system with fairly well-known initial conditions,
e. g. the impact of the baby boomers’ retirement on the
US stock market.
Bounded-rationality Most economic models describe
agents as being fully rational – given the information at
their disposal they act in the optimal way which max-
imizes their objective (or utility) function. This opti-
mization may be technically very complicated, requir-
ing economic, mathematical and statistical sophistica-
tion. In contrast, bounded rational agents are limited
in their ability to optimize. This limitation may be due
to limited computational power, errors, or various psy-
chological biases which have been experimentally doc-
umented.
Market anomalies Empirically documented phenomena
that are diﬃcult to explain within the standard ratio-
nal representative agent economic framework. Some
of these phenomena are the over-reaction and under-
reaction of prices to news, the auto-correlation of stock
returns, various calendar and day-of-the-week eﬀects,
and the excess volatility of stock returns.
Representative agent A standard modeling technique in
economics, by which an entire class of agents (e. g. in-
vestors) are modeled by a single “representative” agent.
If agents are completely homogeneous, it is obvious
that the representative agent method is perfectly legit-
imate. However, when agents are heterogeneous, the
representative agent approach can lead to a multitude
of problems (see [16]).
Definition of the Subject
Mainstream economic models typically make the assump-
tion that an entire group of agents, e. g. “investors”, can
be modeled with a single “rational representative agent”.
While this assumption has proven extremely useful in ad-
vancing the science of economics by yielding analytically
tractable models, it is clear that the assumption is not re-
alistic: people are diﬀerent one from the other in their
tastes, beliefs, and sophistication, and as many psychologi-
cal studies have shown, they often deviate from rationality
in systematic ways.
Agent Based Computational Economics is a frame-
work allowing economics to expand beyond the realm
of the “rational representative agent”. By modeling and
simulating the behavior of each agent and the interac-
tion among agents, agent based simulation allows us to
investigate the dynamics of complex economic systems
with many heterogeneous and not necessarily fully ratio-
nal agents.
The agent based simulation approach allows econ-
omists to investigate systems that can not be studied with

Agent Based Computational Economics
A
93
the conventional methods. Thus, the following key ques-
tions can be addressed: How do heterogeneity and system-
atic deviations from rationality aﬀect markets? Can these
elements explain empirically observed phenomena which
are considered “anomalies” in the standard economics lit-
erature? How robust are the results obtained with the an-
alytical models? By addressing these questions the agent
based simulation approach complements the traditional
analytical analysis, and is gradually becoming a standard
tool in economic analysis.
Introduction
For solving the dynamics of two bodies (e. g. stars) with
some initial locations and velocities and some law of at-
traction (e. g. gravitation) there is a well-known analytical
solution. However, for a similar system with three bodies
there is no known analytical solution. Of course, this does
not mean that physicists can’t investigate and predict the
behavior of such systems. Knowing the state of the system
(i. e. the location, velocity, and acceleration of each body)
at time t, allows us to calculate the state of the system an
instant later, at time t C t. Thus, starting with the ini-
tial conditions we can predict the dynamics of the system
by simply simulating the “behavior” of each element in the
system over time.
This powerful and fruitful approach, sometimes called
“Microscopic Simulation”, has been adopted by many
other branches of science. Its application in economics is
best known as “Agent Based Simulation” or “Agent Based
Computation”. The advantages of this approach are clear –
they allow the researcher to go where no analytical mod-
els can go. Yet, despite of the advantages, perhaps sur-
prisingly, the agent based approach was not adopted very
quickly by economists. Perhaps the main reason for this is
that a particular simulation only describes the dynamics of
a system with a particular set of parameters and initial con-
ditions. With other parameters and initial conditions the
dynamics may be diﬀerent. So economists may ask: what is
the value of conducting simulations if we get very diﬀerent
results with diﬀerent parameter values? While in physics
the parameters (like the gravitational constant) may be
known with great accuracy, in economics the parameters
(like the risk-aversion coeﬃcient, or for that matter the
entire decision-making rule) are typically estimated with
substantial error. This is a strong point. Indeed, we would
argue that the “art” of agent based simulations is the ability
to understand the general dynamics of the system and to
draw general conclusions from a ﬁnite number of simula-
tions. Of course, one simulation is suﬃcient as a counter-
example to show that a certain result does not hold, but
many more simulations are required in order to convince
of an alternative general regularity.
This manuscript is intended as an introduction to
agent-based computational economics. An introduction
to this ﬁeld has two goals: (i) to explain and to demonstrate
the agent-based methodology in economics, stressing the
advantages and disadvantages of this approach relative to
the alternative purely analytical methodology, and (ii) to
review studies published in this area. The emphasis in this
paper will be on the ﬁrst goal. While Sect. “Some of the Pi-
oneering Studies” does provide a brief review of some of
the cornerstone studies in this area, more comprehensive
reviews can be found in [19,24,32,39,40], on which part of
Sect. “Some of the Pioneering Studies” is based. A com-
prehensive review of the many papers employing agent
based computational models in economics will go far be-
yond the scope of this article. To achieve goal (i) above,
in Sect. “Illustration with the LLS Model” we will focus on
one particular model of the stock market in some detail.
Section “Summary and Future Directions” concludes with
some thoughts about the future of the ﬁeld.
Some of the Pioneering Studies
Schelling’s Segregation Model
Schelling’s [36] classical segregation model is one of the
earliest models of population dynamics. Schelling’s model
is not intended as a realistic tool for studying the actual
dynamics of speciﬁc communities as it ignores economic,
real-estate and cultural factors. Rather, the aim of this very
simpliﬁed model is to explain the emergence of macro-
scopic single-race neighborhoods even when individuals
are not racists. More precisely, Schelling found that the
collective eﬀect of neighborhood racial segregation results
even from individual behavior that presents only a very
mild preference for same-color neighbors. For instance,
even the minimal requirement by each individual of hav-
ing (at least) one neighbor belonging to ones’ own race
leads to the segregation eﬀect.
The agent based simulation starts with a square mesh,
or lattice, (representing a town) which is composed of cells
(representing houses). On these cells reside agents which
are either “blue” or “green” (the diﬀerent races). The cru-
cial parameter is the minimal percentage of same-color
neighbors that each agent requires. Each agent, in his turn,
examines the color of all his neighbors. If the percentage of
neighbors belonging to his own group is above the “mini-
mal percentage”, the agent does nothing. If the percentage
of neighbors of his own color is less then the minimal per-
centage, the agent moves to the closest unoccupied cell.
The agent then examines the color of the neighbors of the

94 A
Agent Based Computational Economics
new location and acts accordingly (moves if the number of
neighbors of his own color is below the minimal percent-
age and stays there otherwise). This goes on until the agent
is ﬁnally located at a cite in which the minimal percentage
condition holds. After a while, however, it might happen
that following the moves of the other agents, the mini-
mal percentage condition ceases to be fulﬁlled and then
the agent starts moving again until he ﬁnds an appropri-
ate cell. As mentioned above, the main result is that even
for very mild individual preferences for same-color neigh-
bors, after some time the entire system displays a very high
level of segregation.
A more modern, developed and sophisticated rein-
carnation of these ideas is the Sugarscape environment
described by Epstein and Axtell [6]. The model consid-
ers a population of moving, feeding, pairing, procreating,
trading, warring agents and displays various qualitative
collective events which their populations incur. By em-
ploying agent based simulation one can study the macro-
scopic results induced by the agents’ individual behavior.
The Kim and Markowitz Portfolio Insurers Model
Harry Markowitz is very well known for being one of the
founders of modern portfolio theory, a contribution for
which he has received the Nobel Prize in economics. It is
less well known, however, that Markowitz is also one of
the pioneers in employing agent based simulations in eco-
nomics.
During the October 1987 crash markets all over the
globe plummeted by more than 20% within a few days.
The surprising fact about this crash is that it appeared to be
spontaneous – it was not triggered by any obvious event.
Following the 1987 crash researchers started to look for
endogenous market features, rather than external forces,
as sources of price variation. The Kim-Markowitz [15]
model explains the 1987 crash as resulting from investors’
“Constant Proportion Portfolio Insurance” (CPPI) pol-
icy. Kim and Markowitz proposed that market instabilities
arise as a consequence of the individual insurers’ eﬀorts to
cut their losses by selling once the stock prices are going
down.
The Kim Markowitz agent based model involves two
groups of individual investors: rebalancers and insurers
(CPPI investors). The rebalancers are aiming to keep
a constant composition of their portfolio, while the insur-
ers make the appropriate operations to insure that their
eventual losses will not exceed a certain fraction of the in-
vestment per time period.
The rebalancers act to keep a portfolio structure with
(for instance) half of their wealth in cash and half in stocks.
If the stock price rises, then the stocks weight in the port-
folio will increase and the rebalancers will sell shares until
the shares again constitute 50% of the portfolio. If the stock
price decreases, then the value of the shares in the port-
folio decreases, and the rebalancers will buy shares until
the stock again constitutes 50% of the portfolio. Thus, the
rebalancers have a stabilizing inﬂuence on the market by
selling when the market rises and buying when the market
falls.
A typical CPPI investor has as his/her main objective
not to lose more than (for instance) 25% of his initial
wealth during a quarter, which consists of 65 trading days.
Thus, he aims to insure that at each cycle 75% of the initial
wealth is out of reasonable risk. To this eﬀect, he assumes
that the current value of the stock will not fall in one day by
more than a factor of 2. The result is that he always keeps
in stock twice the diﬀerence between the present wealth
and 75% of the initial wealth (which he had at the begin-
ning of the 65 days investing period). This determines the
amount the CPPI agent is bidding or oﬀering at each stage.
Obviously, after a price fall, the amount he wants to keep
in stocks will fall and the CPPI investor will sell and fur-
ther destabilize the market. After an increase in the prices
(and personal wealth) the amount the CPPI agent wants to
keep in shares will increase: he will buy, and may support
a price bubble.
The simulations reveal that even a relatively small frac-
tion of CPPI investors (i. e. less than 50%) is enough to
destabilize the market, and crashes and booms are ob-
served. Hence, the claim of Kim and Markowitz that the
CPPI policy may be responsible for the 1987 crash is sup-
ported by the agent based simulations. Various variants of
this model were studied intensively by Egenter, Lux and
Stauﬀer [5] who ﬁnd that the price time evolution becomes
unrealistically periodic for a large number of investors (the
periodicity seems related with the ﬁxed 65 days quarter
and is signiﬁcantly diminished if the 65 day period begins
on a diﬀerent date for each investor).
The Arthur, Holland, Lebaron, Palmer
and Tayler Stock Market Model
Palmer, Arthur, Holland, Lebaron and Tayler [30]
and Arthur, Holland, Lebaron, Palmer and Tayler [3]
(AHLPT) construct an agent based simulation model that
is focused on the concept of co-evolution. Each investor
adapts his/her investment strategy such as to maximally
exploit the market dynamics generated by the invest-
ment strategies of all others investors. This leads to an
ever-evolving market, driven endogenously by the ever-
changing strategies of the investors.

Agent Based Computational Economics
A
95
The main objective of AHLPT is to prove that mar-
ket ﬂuctuations may be induced by this endogenous co-
evolution, rather than by exogenous events. Moreover,
AHLPT study the various regimes of the system: the
regime in which rational fundamentalist strategies are
dominating vs. the regime in which investors start devel-
oping strategies based on technical trading. In the techni-
cal trading regime, if some of the investors follow funda-
mentalist strategies, they may be punished rather than re-
warded by the market. AHLPT also study the relation be-
tween the various strategies (fundamentals vs. technical)
and the volatility properties of the market (clustering, ex-
cess volatility, volume-volatility correlations, etc.).
In the ﬁrst paper quoted above, the authors simulated
a single stock and further limited the bid/oﬀer decision to
a ternary choice of: i) bid to buy one share, ii) oﬀer to sell
one share, or: iii) do nothing. Each agent had a collection
of rules which described how he should behave (i, ii or iii)
in various market conditions. If the current market condi-
tions were not covered by any of the rules, the default was
to do nothing. If more than one rule applied in a certain
market condition, the rule to act upon was chosen prob-
abilistically according to the “strengths” of the applicable
rules. The “strength” of each rule was determined accord-
ing to the rule’s past performance: rules that “worked” be-
came “stronger”. Thus, if a certain rule performed well, it
became more likely to be used again.
The price is updated proportionally to the relative ex-
cess of oﬀers over demands. In [3], the rules were used to
predict future prices. The price prediction was then trans-
formed into a buy/sell order through the use of a Constant
Absolute Risk Aversion (CARA) utility function. The use
of CARA utility leads to demands which do not depend on
the investor’s wealth.
The heart of the AHLPT dynamics are the trading
rules. In particular, the authors diﬀerentiate between “fun-
damental” rules and “technical” rules, and study their rel-
ative strength in various market regimes. For instance,
a “fundamental” rule may require a market conditions of
the type:
dividend/current price > 0:04
in order to be applied. A “technical” rule may be triggered
if the market fulﬁlls a condition of the type:
current price > 10-period moving-average of past prices:
The rules undergo genetic dynamics: the weakest rules are
substituted periodically by copies of the strongest rules
and all the rules undergo random mutations (or even ver-
sions of “sexual” crossovers: new rules are formed by com-
bining parts from 2 diﬀerent rules). The genetic dynamics
of the trading rules represent investors’ learning: new rules
represent new trading strategies. Investors examine new
strategies, and adopt those which tend to work best. The
main results of this model are:
For a Few Agents, a Small Number of Rules,
and Small Dividend Changes
 The price converges towards an equilibrium price
which is close to the fundamental value.
 Trading volume is low.
 There are no bubbles, crashes or anomalies.
 Agents follow homogeneous simple fundamentalist
rules.
For a Large Number of Agents
and a Large Number of Rules
 There is no convergence to an equilibrium price, and
the dynamics are complex.
 The price displays occasional large deviations from the
fundamental value (bubbles and crashes).
 Some of these deviations are triggered by the emer-
gence of collectively self-fulﬁlling agent price-predic-
tion rules.
 The agents become heterogeneous (adopt very diﬀerent
rules).
 Trading volumes ﬂuctuate (large volumes correspond
to bubbles and crashes).
 The rules evolve over time to more and more complex
patterns, organized in hierarchies (rules, exceptions to
rules, exceptions to exceptions, and so on ...).
 The successful rules are time dependent: a rule which is
successful at a given time may perform poorly if rein-
troduced after many cycles of market co-evolution.
The Lux and Lux and Marchesi Model
Lux [27] and Lux and Marchesi [28] propose a model to
endogenously explain the heavy tail distribution of returns
and the clustering of volatility. Both of these phenomena
emerge in the Lux model as soon as one assumes that
in addition to the fundamentalists there are also chartists
in the model. Lux and Marchesi [28] further divide the
chartists into optimists (buyers) and pessimists (sellers).
The market ﬂuctuations are driven and ampliﬁed by the
ﬂuctuations in the various populations: chartists convert-
ing into fundamentalists, pessimists into optimists, etc.
In the Lux and Marchesi model the stock’s fundamen-
tal value is exogenously determined. The ﬂuctuations of
the fundamental value are inputted exogenously as a white

96 A
Agent Based Computational Economics
noise process in the logarithm of the value. The market
price is determined by investors’ demands and by the mar-
ket clearance condition.
Lux and Marchesi consider three types of traders:
 Fundamentalists observe the fundamental value of the
stock. They anticipate that the price will eventuallycon-
verge to the fundamental value, and their demand for
shares is proportional to the diﬀerence between the
market price and the fundamental value.
 Chartists look more at the present trends in the market
price rather than at fundamental economic values; the
chartists are divided into
 Optimists (they buy a ﬁxed amount of shares per unit
time)
 Pessimists (they sell shares).
Transitions between these three groups (optimists, pes-
simists, fundamentalists) happen with probabilities de-
pending on the market dynamics and on the present num-
bers of traders in each of the three classes:
 The transition probabilities of chartists depend on
the majority opinion (through an “opinion index” mea-
suring the relative number of optimists minus the rel-
ative number of pessimists) and on the actual price
trend (the current time derivative of the current mar-
ket price), which determines the relative proﬁt of the
various strategies.
 The fundamentalists decide to turn into chartists if
the proﬁts of the later become signiﬁcantly larger than
their own, and vice versa (the detailed formulae used
by Lux and Marchesi are inspired from the exponential
transition probabilities governing statistical mechanics
physical systems).
The main results of the model are:
 No long-term deviations between the current mar-
ket price and the fundamental price are observed.
 The deviations from the fundamental price, which
do occur, are unsystematic.
 In spite of the fact that the variations of the funda-
mental price are normally distributed, the variations
of the market price (the market returns) are not. In
particular the returns exhibit a frequency of extreme
events which is higher than expected for a normal
distribution. The authors emphasize the ampliﬁca-
tion role of the market that transforms the input
normal distribution of the fundamental value varia-
tions into a leptokurtotic (heavy tailed) distribution
of price variation, which is encountered in the actual
ﬁnancial data.
 clustering of volatility.
The authors explain the volatility clustering (and as a con-
sequence, the leptokurticity) by the following mechanism.
In periods of high volatility, the fundamental information
is not very useful to insure proﬁts, and a large fraction of
the agents become chartists. The opposite is true in quiet
periods when the actual price is very close to the funda-
mental value. The two regimes are separated by a thresh-
old in the number of chartist agents. Once this threshold
is approached (from below) large ﬂuctuations take place
which further increase the number of chartists. This desta-
bilization is eventually dampened by the energetic inter-
vention of the fundamentalists when the price deviates too
much from the fundamental value. The authors compare
this temporal instability with the on-oﬀintermittence en-
countered in certain physical systems. According to Egen-
ter et al. [5], the fraction of chartists in the Lux Marchesi
model goes to zero as the total number of traders goes to
inﬁnity, when the rest of the parameters are kept constant.
Illustration with the LLS Model
The purpose of this section is to give a more detailed
“hands on” example of the agent based approach, and
to discuss some of the practical dilemmas arising when
implementing this approach, by focusing on one speciﬁc
model. We will focus on the so called LLS Model of the
stock market (for more detail, and various versions of the
model, see [11,17,22,23,24,25]. This section is based on the
presentation of the LLS Model in Chap. 7 of [24]).
Background
Real life investors diﬀer in their investment behavior from
the investment behavior of the idealized representative
rational investor assumed in most economic and ﬁnan-
cial models. Investors diﬀer one from the other in their
preferences, their investment horizon, the information at
their disposal, and their interpretation of this information.
No ﬁnancial economist seriously doubts these observa-
tions. However, modeling the empirically and experimen-
tally documented investor behavior and the heterogeneity
of investors is very diﬃcult and in most cases practically
impossible to do within an analytic framework. For in-
stance, the empirical and experimental evidence suggests
that most investors are characterized by Constant Relative
Risk Aversion (CRRA), which implies a power (myopic)
utility function (see Eq. (2) below). However, for a gen-
eral distribution of returns it is impossible to obtain an
analytic solution for the portfolio optimization problem
of investors with these preferences. Extrapolation of fu-
ture returns from past returns, biased probability weight-
ing, and partial deviations from rationality are also all ex-

Agent Based Computational Economics
A
97
perimentally documented but diﬃcult to incorporate in an
analytical setting. One is then usually forced to make the
assumptions of rationality and homogeneity (at least in
some dimension) and to make unrealistic assumptions re-
garding investors’ preferences, in order to obtain a model
with a tractable solution. The hope in these circumstances
is that the model will capture the essence of the system
under investigation, and will serve as a useful benchmark,
even though some of the underlying assumptions are ad-
mittedly false.
Most homogeneous rational agent models lead to the
following predictions: no trading volume, zero autocorre-
lation of returns, and price volatility which is equal to or
lower than the volatility of the “fundamental value” of the
stock (deﬁned as the present value of all future dividends,
see [37]). However, the empirical evidence is very diﬀer-
ent:
 Trading volume can be extremely heavy [1,14].
 Stock returns exhibit short-run momentum (positive
autocorrelation) and long-run mean reversion (nega-
tive autocorrelation) [7,13,21,31].
 Stock returns are excessively volatile relative to the div-
idends [37].
As most standard rational-representative-agent models
cannot explain these empirical ﬁndings, these phenomena
are known as “anomalies” or “puzzles”. Can these “anoma-
lies” be due to elements of investors’ behavior which are
unmodeled in the standard rational-representative-agent
models, such as the experimentally documented devia-
tions of investors’ behavior from rationality and/or the
heterogeneity of investors? The agent based simulation ap-
proach oﬀers us a tool to investigate this question. The
strength of the agent based simulation approach is that
since it is not restricted to the scope of analytical methods,
one is able to investigate virtually any imaginable investor
behavior and market structure. Thus, one can study mod-
els which incorporate the experimental ﬁndings regarding
the behavior of investors, and evaluate the eﬀects of var-
ious behavioral elements on market dynamics and asset
pricing.
The LLS model incorporates some of the main empir-
ical ﬁndings regarding investor behavior, and we employ
this model in order to study the eﬀect of each element of
investor behavior on asset pricing and market dynamics.
We start out with a benchmark model in which all of the
investors are rational, informed and identical, and then,
one by one, we add elements of heterogeneity and devia-
tions from rationality to the model in order to study their
eﬀects on the market dynamics.
In the benchmark model all investors are Rational,
Informed and Identical (RII investors). This is, in eﬀect,
a “representative agent” model. The RII investors are in-
formed about the dividend process, and they rationally act
to maximize their expected utility. The RII investors make
investment decisions based on the present value of future
cash ﬂows. They are essentially fundamentalists who eval-
uate the stock’s fundamental value and try to ﬁnd bar-
gains in the market. The benchmark model in which all
investors are RII yields results which are typical of most
rational-representative-agent models: in this model prices
follow a random walk, there is no excess volatility of the
prices relative to the volatility of the dividend process, and
since all agents are identical, there is no trading volume.
After describing the properties of the benchmark
model, we investigate the eﬀects of introducing various el-
ements of investor behavior which are found in laboratory
experiments but are absent in most standard models. We
do so by adding to the model a minority of investors who
do not operate like the RII investors. These investors are
Eﬃcient Market Believers (EMB from now on). The EMBs
are investors who believe that the price of the stock re-
ﬂects all of the currently available information about the
stock. As a consequence, they do not try to time the mar-
ket or to buy bargain stocks. Rather, their investment deci-
sion is reduced to the optimal diversiﬁcation problem. For
this portfolio optimization, the ex-ante return distribution
is required. However, since the ex-ante distribution is un-
known, the EMB investors use the ex-post distribution in
order to estimate the ex-ante distribution. It has been doc-
umented that in fact, many investors form their expecta-
tions regarding the future return distribution based on the
distribution of past returns.
There are various ways to incorporate the investment
decisions of the EMBs. This stems from the fact that there
are diﬀerent ways to estimate the ex-ante distribution from
the ex-post distribution. How far back should one look at
the historical returns? Should more emphasis be given to
more recent returns? Should some “outlier” observations
be ﬁltered out? etc. Of course, there are no clear answers
to these questions, and diﬀerent investors may have diﬀer-
ent ways of forming their estimation of the ex-ante return
distribution (even though they are looking at the same se-
ries of historical returns). Moreover, some investors may
use the objective ex-post probabilities when constructing
their estimation of the ex-ante distribution, whereas oth-
ers may use biased subjective probability weights. In order
to build the analysis step-by-step we start by analyzing the
case in which the EMB population is homogeneous, and
then introduce various forms of heterogeneity into this
population.

98 A
Agent Based Computational Economics
An important issue in market modeling is that of the
degree of investors’ rationality. Most models in economics
and ﬁnance assume that people are fully rational. This as-
sumption usually manifests itself as the maximization of
an expected utility function by the individual. However,
numerous experimental studies have shown that people
deviate from rational decision-making [41,42,43,44,45].
Some studies model deviations from the behavior of the
rational agent by introducing a sub-group of liquidity,
or “noise”, traders. These are traders that buy and sell
stocks for reasons that are not directly related to the future
payoﬀs of the ﬁnancial asset - their motivation to trade
arises from outside of the market (for example, a “noise
trader’s” daughter unexpectedly announces her plans to
marry, and the trader sells stocks because of this unex-
pected need for cash). The exogenous reasons for trading
are assumed random, and thus lead to random or “noise”
trading (see [10]). The LLS model takes a diﬀerent ap-
proach to the modeling of noise trading. Rather than di-
viding investors into the extreme categories of “fully ratio-
nal” and “noise traders”, the LLS model assumes that most
investors try to act as rationally as they can, but are inﬂu-
enced by a multitude of factors causing them to deviate to
some extent from the behavior that would have been op-
timal from their point of view. Namely, all investors are
characterized by a utility function and act to maximize
their expected utility; however, some investors may devi-
ate to some extent from the optimal choice which maxi-
mizes their expected utility. These deviations from the op-
timal choice may be due to irrationality, ineﬃciency, liq-
uidity constraints, or a combination of all of the above.
In the framework of the LLS model we examine the
eﬀects of the EMBs’ deviations from rationality and their
heterogeneity, relative to the benchmark model in which
investors are informed, rational and homogeneous. We
ﬁnd that the behavioral elements which are empirically
documented, namely, extrapolation from past returns, de-
viation from rationality, and heterogeneity among in-
vestors, lead to all of the following empirically docu-
mented “puzzles”:
 Excess volatility
 Short-term momentum
 Longer-term return mean-reversion
 Heavy trading volume
 Positive correlation between volume and contempora-
neous absolute returns
 Positive correlation between volume and lagged abso-
lute returns
The fact that all these anomalies or “puzzles”, which
are hard to explain with standard rational-representative-
agent models, are generated naturally by a simple model
which incorporates the experimental ﬁndings regarding
investor behavior and the heterogeneity of investors, leads
one to suspect that these behavioral elements and the di-
versity of investors are a crucial part of the workings of the
market, and as such they cannot be “assumed away”. As
the experimentally documented bounded-rational behav-
ior and heterogeneity are in many cases impossible to an-
alyze analytically, agent based simulation presents a very
promising tool for investigating market models incorpo-
rating these elements.
The LLS Model
The stock market consists of two investment alternatives:
a stock (or index of stocks) and a bond. The bond is as-
sumed to be a riskless asset, and the stock is a risky asset.
The stock serves as a proxy for the market portfolio (e. g.,
the Standard & Poors 500 index). The extension from one
risky asset to many risky assets is possible; however, one
stock (the index) is suﬃcient for our present analysis be-
cause we restrict ourselves to global market phenomena
and do not wish to deal with asset allocation across several
risky assets. Investors are allowed to revise their portfolio
at given time points, i. e. we discuss a discrete time model.
The bond is assumed to be a riskless investment yield-
ing a constant return at the end of each time period. The
bond is in inﬁnite supply and investors can buy from it as
much as they wish at a given rate of r f . The stock is in ﬁ-
nite supply. There are N outstanding shares of the stock.
The return on the stock is composed of two elements:
a) Capital Gain: If an investor holds a stock, any rise (fall)
in the price of the stock contributes to an increase (de-
crease) in the investor’s wealth.
b) Dividends: The company earns income and distributes
dividends at the end of each time period. We denote
the dividend per share paid at time t by Dt. We assume
that the dividend is a stochastic variable following
a multiplicative random walk, i. e., ˜Dt D Dt1(1 C ˜z),
where ˜z is a random variable with some probability
density function f (z) in the range [z1; z2]. (In order to
allow for a dividend cut as well as a dividend increase
we typically choose: z1 < 0; z2 > 0).
The total return on the stock in period t, which we denote
by Rt is given by:
˜Rt D
˜Pt C ˜Dt
Pt1
;
(1)
where ˜Pt is the stock price at time t.
All investors in the model are characterized by a von
Neuman-Morgenstern utility function. We assume that all

Agent Based Computational Economics
A
99
investors have a power utility function of the form:
U(W) D W1˛
1  ˛ ;
(2)
where ˛ is the risk aversion parameter. This form of utility
function implies Constant Relative Risk Aversion (CRRA).
We employ the power utility function (Eq. (2)) because
the empirical evidence suggests that relative risk aversion
is approximately constant (see, for example [8,9,18,20]),
and the power utility function is the unique utility func-
tion which satisﬁes the CRRA condition. Another impli-
cation of CRRA is that the optimal investment choice is
independent of the investment horizon [33,34]. In other
words, regardless of investors’ actual investment horizon,
they choose their optimal portfolio as though they are in-
vesting for a single period. The myopia property of the
power utility function simpliﬁes our analysis, as it allows
us to assume that investors maximize their one-period-
ahead expected utility.
We model two diﬀerent types of investors: Rational,
Informed, Identical (RII) investors, and Eﬃcient Market
Believers (EMB). These two investor types are described
below.
Rational Informed Identical (RII) Investors
RII in-
vestors evaluate the “fundamental value” of the stock as
the discounted stream of all future dividends, and thus can
also be thought of as “fundamentalists”. They believe that
the stock price may deviate from the fundamental value
in the short run, but if it does, it will eventually converge
to the fundamental value. The RII investors act according
to the assumption of asymptotic convergence: if the stock
price is low relative to the fundamental value they buy in
anticipation that the underpricing will be corrected, and
vice versa. We make the simplifying assumption that the
RII investors believe that the convergence of the price to
the fundamental value will occur in the next period, how-
ever, our results hold for the more general case where the
convergence is assumed to occur some T periods ahead,
with T > 1.
In order to estimate next period’s return distribution,
the RII investors need to estimate the distribution of next
period’s price, ˜PtC1, and of next period’s dividend, ˜DtC1.
Since they know the dividend process, the RII investors
know that ˜DtC1 D Dt(1 C ˜z) where ˜z is distributed ac-
cording to f (z) in the range [z1; z2]. The RII investors em-
ploy Gordon’s dividend stream model in order to calculate
the fundamental value of the stock:
P f
tC1 D EtC1[ ˜DtC2]
k  g
;
(3)
where the superscript f stands for the fundamental value,
EtC1[ ˜DtC2] is the dividend corresponding to time t C 2
as expected at time t C 1, k is the discount factor or the
expected rate of return demanded by the market for the
stock, and g is the expected growth rate of the dividend,
i. e., g D E(˜z) D
R z2
z1 f (z)zdz.
The RII investors believe that the stock price may tem-
porarily deviate from the fundamental value; however,
they also believe that the price will eventually converge to
the fundamental value. For simpliﬁcation we assume that
the RII investors believe that the convergence to the fun-
damental value will take place next period. Thus, the RII
investors estimate PtC1 as:
PtC1 D P f
tC1 :
The expectation at time t C 1 of ˜DtC2 depends on the re-
alized dividend observed at t C 1:
EtC1[ ˜DtC2] D DtC1(1 C g) :
Thus, the RII investors believe that the price at t C 1 will
be given by:
PtC1 D P f
tC1 D DtC1(1 C g)
k  g
:
At time t, Dt is known, but DtC1 is not; therefore P f
tC1 is
also not known with certainty at time t. However, given
Dt, the RII investors know the distribution of ˜DtC1:
˜DtC1 D Dt(1 C ˜z);
where ˜z is distributed according to the known f (z). The
realization of ˜DtC1 determines P f
tC1. Thus, at time t, RII
investors believe that PtC1 is a random variable given by:
˜PtC1 D ˜P f
tC1 D Dt(1 C ˜z)(1 C g)
k  g
:
Notice that the RII investors face uncertainty regarding
next period’s price. In our model we assume that the RII
investors are certain about the dividend growth rate g,
the discount factor k, and the fact that the price will con-
verge to the fundamental value next period. In this frame-
work the only source of uncertainty regarding next pe-
riod’s price stems from the uncertainty regarding next
period’s dividend realization. More generally, the RII in-
vestors’ uncertainty can result from uncertainty regarding
any one of the above factors, or a combination of several of
these factors. Any mix of these uncertainties is possible to
investigate in the agent based simulation framework, but
very hard, if not impossible, to incorporate in an analytic

100 A
Agent Based Computational Economics
framework. As a consequence of the uncertainty regarding
next period’s price and of their risk aversion, the RII in-
vestors do not buy an inﬁnite number of shares even if they
perceive the stock as underpriced. Rather, they estimate
the stock’s next period’s return distribution, and ﬁnd the
optimal mix of the stock and the bond which maximizes
their expected utility. The RII investors estimate next pe-
riod’s return on the stock as:
˜RtC1 D
˜PtC1 C ˜DtC1
Pt
D
Dt(1C˜z)(1Cg)
kg
C Dt(1 C ˜z)
Pt
; (4)
where ˜z, the next year growth in the dividend, is the source
of uncertainty. The demands of the RII investors for the
stock depend on the price of the stock. For any hypothet-
ical price Ph investors calculate the proportion of their
wealth x they should invest in the stock in order to maxi-
mize their expected utility. The RII investor i believes that
if she invests a proportion x of her wealth in the stock at
time t, then at time t C 1 her wealth will be:
˜W i
tC1 D W i
h[(1  x)(1 C r f ) C x ˜RtC1];
(5)
where ˜RtC1 is the return on the stock, as given by Eq. (1),
and W i
h is the wealth of investor i at time t given that the
stock price at time t is Ph.
If the price in period t is the hypothetical price Ph, the
t C 1 expected utility of investor i is the following function
of her investment proportion in the stock, x:
EU( ˜W i
tC1) D EU

W i
h

(1  x)(1 C r f ) C x ˜RtC1

: (6)
Substituting ˜RtC1 from Eq. (4), using the power utility
function (Eq. (2)), and substituting the hypothetical price
Ph for Pt, the expected utility becomes the following func-
tion of x:
EU( ˜W i
tC1) D (W i
h)1˛
1  ˛
z2
Z
z1
2
4(1  x)(1 C r f )
Cx
0
@
Dt(1Cz)(1Cg)
kg
C Dt(1 C z)
Ph
1
A
3
5
1˛
f (z)dz ;
(7)
where the integration is over all possible values of z. In
the agent based simulation framework, this expression for
the expected utility, and the optimal investment propor-
tion x, can be solved numerically for any general choice
of distribution f (z): For the sake of simplicity we restrict
the present analysis to the case where ˜z is distributed uni-
formly in the range [z1; z2]. This simpliﬁcation leads to the
following expression for the expected utility:
EU( ˜W i
tC1)
D
(W i
h)1˛
(1  ˛)(2  ˛)
1
(z2  z1)
 k  g
k C 1
 Ph
xDt
( 
(1  x)(1 C r f ) C x
Ph
 k C 1
k  g

Dt(1 C z2)
(2˛)


(1  x)(1 C r f ) C x
Ph
 k C 1
k  g

Dt(1 C z1)
(2˛))
(8)
For any hypothetical price Ph, each investor (numerically)
ﬁnds the optimal proportion xh which maximizes his/her
expected utility given by Eq. (8). Notice that the optimal
proportion, xh, is independent of the wealth, W i
h. Thus,
if all RII investors have the same degree of risk aversion,
˛, they will have the same optimal investment propor-
tion in the stock, regardless of their wealth. The number
of shares demanded by investor i at the hypothetical price
Ph is given by:
N i
h(Ph) D xi
h(Ph)W i
h(Ph)
Ph
:
(9)
Eﬃcient Market Believers (EMB)
The second type of
investors in the LLS model are EMBs. The EMBs believe in
market eﬃciency - they believe that the stock price accu-
rately reﬂects the stock’s fundamental value. Thus, they do
not try to time the market or to look for “bargain” stocks.
Rather, their investment decision is reduced to the opti-
mal diversiﬁcation between the stock and the bond. This
diversiﬁcation decision requires the ex-ante return distri-
bution for the stock, but as the ex-ante distribution is not
available, the EMBs assume that the process generating the
returns is fairly stable, and they employ the ex-post distri-
bution of stock returns in order to estimate the ex-ante re-
turn distribution.
Diﬀerent EMB investors may disagree on the optimal
number of ex-post return observations that should be em-
ployed in order to estimate the ex-ante return distribu-
tion. There is a trade-oﬀbetween using more observations
for better statistical inference, and using a smaller num-
ber of only more recent observations, which are probably
more representative of the ex-ante distribution. As in real-
ity, there is no “recipe” for the optimal number of obser-
vations to use. EMB investor i believes that the mi most
recent returns on the stock are the best estimate of the ex-
ante distribution. Investors create an estimation of the ex-
ante return distribution by assigning an equal probability

Agent Based Computational Economics
A
101
to each of the mi most recent return observations:
Probi( ˜RtC1 D Rtj) D 1
mi
for j D 1; : : :; mi
(10)
The expected utility of EMB investor i is given by:
EU(W i
tC1)
D (W i
h)1˛
(1  ˛)
1
mi
mi
X
jD1

(1  x)(1 C r f ) C xRtj
1˛;
(11)
where the summation is over the set of mi most recent ex-
post returns, x is the proportion of wealth invested in the
stock, and as before W i
h is the wealth of investor i at time t
given that the stock price at time t is Ph. Notice that W i
h
does not change the optimal diversiﬁcation policy, i. e., x.
Given a set of mi past returns, the optimal portfolio for the
EMB investor i is an investment of a proportion xi in the
stock and (1- xi) in the bond, where xi is the propor-
tion which maximizes the above expected utility (Eq. (11))
for investor i. Notice that xi generally cannot be solved
for analytically. However, in the agent based simulation
framework this does not constitute a problem, as one can
ﬁnd xi numerically.
Deviations from Rationality
Investors who are eﬃcient
market believers, and are rational, choose the investment
proportion x which maximizes their expected utility.
However, many empirical studies have shown that the be-
havior of investors is driven not only by rational expected
utility maximization but by a multitude of other factors
(see, for example, [34,41,42,43,44]). Deviations from the
optimal rational investment proportion can be due to
the cost of resources which are required for the portfolio
optimization: time, access to information, computational
power, etc., or due to exogenous events (for example, an
investor plans to revise his portfolio, but gets distracted
because his car breaks down). We assume that the diﬀer-
ent factors causing the investor to deviate from the opti-
mal investment proportion x are random and uncorre-
lated with each other. By the central limit theorem, the ag-
gregate eﬀect of a large number of random uncorrelated
inﬂuences is a normally distributed random inﬂuence, or
“noise”. Hence, we model the eﬀect of all the factors caus-
ing the investor to deviate from his optimal portfolio by
adding a normally distributed random variable to the op-
timal investment proportion. To be more speciﬁc, we as-
sume:
xi D xi C ˜"i;
(12)
where ˜"i is a random variable drawn from a truncated nor-
mal distribution with mean zero and standard deviation .
Notice that noise is investor-speciﬁc, thus, ˜"i is drawn sep-
arately and independently for each investor.
The noise can be added to the decision-making of
the RII investors, the EMB investors, or to both. The re-
sults are not much diﬀerent with these various approaches.
Since the RII investors are taken as the benchmark of
rationality, in this chapter we add the noise only to the
decision-making of the EMB investors.
Market Clearance
The number of shares demanded by
each investor is a monotonically decreasing function of the
hypothetical price Ph (see [24]). As the total number of
outstanding shares is N, the price of the stock at time t is
given by the market clearance condition: Pt is the unique
price at which the total demand for shares is equal to the
total supply, N:
X
i
N i
h(Pt) D
X
i
xh(Pt)W i
h(Pt)
Pt
D N ;
(13)
where the summation is over all the investors in the mar-
ket, RII investors as well as EMB investors.
Agent Based Simulation
The market dynamics begin
with a set of initial conditions which consist of an initial
stock price P0, an initial dividend D0, the wealth and num-
ber of shares held by each investor at time t D 0, and an
initial “history” of stock returns. As will become evident,
the general results do not depend on the initial conditions.
At the ﬁrst period (t D 1), interest is paid on the bond, and
the time 1 dividend ˜D1 D D0(1 C ˜z) is realized and paid
out. Then investors submit their demand orders, N i
h(Ph),
and the market clearing price P1 is determined. After the
clearing price is set, the new wealth and number of shares
held by each investor are calculated. This completes one
time period. This process is repeated over and over, as the
market dynamics develop.
We would like to stress that even the simpliﬁed bench-
mark model, with only RII investors, is impossible to solve
analytically. The reason for this is that the optimal in-
vestment proportion, xh(Ph), cannot be calculated analyt-
ically. This problem is very general and it is encountered
with almost any choice of utility function and distribu-
tion of returns. One important exception is the case of
a negative exponential utility function and normally dis-
tributed returns. Indeed, many models make these two as-
sumptions for the sake of tractability. The problem with
the assumption of negative exponential utility is that it im-
plies Constant Absolute Risk Aversion (CARA), which is

102 A
Agent Based Computational Economics
very unrealistic, as it implies that investors choose to invest
the same dollar amount in a risky prospect independent of
their wealth. This is not only in sharp contradiction to the
empirical evidence, but also excludes the investigation of
the two-way interaction between wealth and price dynam-
ics, which is crucial to the understanding of the market.
Thus, one contribution of the agent based simulation
approach is that it allows investigation of models with re-
alistic assumptions regarding investors’ preferences. How-
ever, the main contribution of this method is that it per-
mits us to investigate models which are much more com-
plex (and realistic) than the benchmark model, in which
all investors are RII. With the agent based simulation ap-
proach one can study models incorporating the empiri-
cally and experimentally documented investors’ behavior,
and the heterogeneity of investors.
Results of the LLS Model
We begin by describing the benchmark case where all in-
vestors are rational and identical. Then we introduce to the
market EMB investors and investigate their aﬀects on the
market dynamics.
Benchmark Case: Fully Rational and Identical Agents
In this benchmark model all investors are RII: rational,
informed and identical. Thus, it is not surprising that the
benchmark model generates market dynamics which are
typical of homogeneous rational agent models:
No Volume
All investors in the model are identical; they
therefore always agree on the optimal proportion to in-
vest in the stock. As a consequence, all the investors always
achieve the same return on their portfolio. This means that
at any time period the ratio between the wealth of any two
investors is equal to the ratio of their initial wealths, i. e.:
W i
t
W j
t
D W i
0
W j
0
:
(14)
As the wealth of investors is always in the same propor-
tion, and as they always invest the same fraction of their
wealth in the stock, the number of shares held by diﬀerent
investors is also always in the same proportion:
N i
t
N j
t
D
xtW i
t
Pt
xtW j
t
Pt
D W i
t
W j
t
D W i
0
W j
0
:
(15)
Since the total supply of shares is constant, this implies that
each investor always holds the same number of shares, and
there is no trading volume (the number of shares held may
vary from one investor to the other as a consequence of
diﬀerent initial endowments).
Log-Prices Follow a Random Walk
In the benchmark
model all investors believe that next period’s price will
converge to the fundamental value given by the discounted
dividend model (Eq. (3)). Therefore, the actual stock price
is always close to the fundamental value. The ﬂuctuations
in the stock price are driven by ﬂuctuations in the fun-
damental value, which in turn are driven by the ﬂuctuat-
ing dividend realizations. As the dividend ﬂuctuations are
(by assumption) uncorrelated over time, one would ex-
pect that the price ﬂuctuations will also be uncorrelated.
To verify this intuitive result, we examine the return auto-
correlations in simulations of the benchmark model.
Let us turn to the simulation of the model. We ﬁrst
describe the parameters and initial conditions used in the
simulation, and then report the results. We simulate the
benchmark model with the following parameters:
 Number of investors = 1000
 Risk aversion parameter ˛= 1.5. This value roughly
conforms with the estimate of the risk aversion param-
eter found empirically and experimentally.
 Number of shares = 10,000.
 We take the time period to be a quarter, and accord-
ingly we choose:
 Riskless interest rate r f D 0:01.
 Required rate of return on stock k = 0.04.
 Maximal one-period dividend decrease z1 = -0.07.
 Maximal one-period dividend growth z2 = 0.10.

˜z is uniformly distributed between these values. Thus,
the average dividend growth rate is g D (z1 C z2)/2 D
0:015.
Initial Conditions: Each investor is endowed at time t D 0
with a total wealth of $1000, which is composed of 10
shares worth an initial price of $50 per share, and $500
in cash. The initial quarterly dividend is set at $0.5 (for an
annual dividend yield of about 4%). As will soon become
evident, the dynamics are not sensitive to the particular
choice of initial conditions.
Figure 1 shows the price dynamics in a typical simu-
lation with these parameters (simulations with the same
parameters diﬀer one from the other because of the dif-
ferent random dividend realizations). Notice that the ver-
tical axis in this ﬁgure is logarithmic. Thus, the roughly
constant slope implies an approximately exponential price
growth, or an approximately constant average return.
The prices in this simulation seem to ﬂuctuate ran-
domly around the trend. However, Fig. 1 shows only one
simulation. In order to have a more rigorous analysis we

Agent Based Computational Economics
A
103
Agent Based Computational Economics, Figure 1
Price Dynamics in the Benchmark Model
perform many independent simulations, and employ sta-
tistical tools. Namely, for each simulation we calculate the
autocorrelation of returns. We perform a univariate re-
gression of the return in time t on the return on time t  j:
Rt D ˛j C ˇjRtj C " ;
where Rt is the return in period t, and j is the lag. The
autocorrelation of returns for lag j is deﬁned as:
j D cov(Rt; Rtj)
ˆ2(R)
;
and it is estimated by ˆˇ. We calculate the autocorrelation
for diﬀerent lags, j D 1; : : :40. Figure 2 shows the average
autocorrelation as a function of the lag, calculated over 100
independent simulations. It is evident both from the ﬁgure
that the returns are uncorrelated in the benchmark model,
conforming with the random-walk hypothesis.
No Excess Volatility
Since the RII investors believe that
the stock price will converge to the fundamental value next
period, in the benchmark model prices are always close
to the fundamental value given by the discounted divi-
dend stream. Thus, we do not expect prices to be more
volatile than the value of the discounted dividend stream.
For a formal test of excess volatility we follow the tech-
nique in [37]. For each time period we calculate the actual
price Pt, and the fundamental value of discounted divi-
dend stream, P f
t , as in Eq. (3). Since prices follow an up-
ward trend, in order to have a meaningful measure of the
volatility, we must detrend these price series. Following
Shiller, we run the regression:
ln Pt D bt C c C "t ;
(16)
in order to ﬁnd the average exponential price growth rate
(where b and c are constants). Then, we deﬁne the de-
trended price as: pt D Pt/eˆbt. Similarly, we deﬁne the de-
trended value of the discounted dividend stream p f
t , and
compare (pt) with (p f
t ). For 100 1000-period simula-
tions we ﬁnd an average (pt) of 22.4, and an average
(p f
t ) of 22.9. As expected, the actual price and the fun-
damental value have almost the same volatility.
To summarize the results obtained for the benchmark
model, we ﬁnd that when all investors are assumed to be
rational, informed and identical, we obtain results which
are typical of rational-representative-agentmodels: no vol-
ume, no return autocorrelations, and no excess volatility.
We next turn to examine the eﬀect of introducing into the
market EMB investors, which model empirically and ex-
perimentally documented elements of investors’ behavior.
The Introduction of a Small Minority of EMB Investors
In this section we will show that the introduction of a small
minority of heterogeneous EMB investors generates many
of the empirically observed market “anomalies” which
are absent in the benchmark model, and indeed, in most
other rational-representative-agent models. We take this
as strong evidence that the “non-rational” elements of in-
vestor behavior which are documented in experimental
studies, and the heterogeneity of investors, both of which

104 A
Agent Based Computational Economics
Agent Based Computational Economics, Figure 2
Return Autocorrelation in Benchmark Model
are incorporated in the LLS model, are crucial to under-
standing the dynamics of the market.
In presenting the results of the LLS model with EMB
investors we take an incremental approach. We begin
by describing the results of a model with a small sub-
population of homogeneous EMB believers. This model
produces the above mentioned market “anomalies”; how-
ever, it produces unrealistic cyclic market dynamics. Thus,
this model is presented both for analyzing the source of
the “anomalies” in a simpliﬁed setting, and as a reference
point with which to compare the dynamics of the model
with a heterogeneous EMB believer population.
We investigate the eﬀects of investors’ heterogeneity
by ﬁrst analyzing the case in which there are two types
of EMBs. The two types diﬀer in the method they use
to estimate the ex-ante return distribution. Namely, the
ﬁrst type looks at the set of the last m1 ex-post returns,
whereas the second type looks at the set of the last m2 ex-
post returns. It turns out that the dynamics in this case
are much more complicated than a simple “average” be-
tween the case where all EMB investors have m1 and the
case where all EMB investors have m2. Rather, there is
a complex non-linear interaction between the two EMB
sub-populations. This implies that the heterogeneity of in-
vestors is a very important element determining the mar-
ket dynamics, an element which is completely absent in
representative-agent models.
Finally, we present the case where there is an entire
spectrum of EMB investors diﬀering in the number of ex-
post observations they take into account when estimating
the ex-ante distribution. This general case generates very
realistic-looking market dynamics with all of the above
mentioned market anomalies.
Homogeneous Sub-Population of EMBs
When a very
small sub-population of EMB investors is introduced to
the benchmark LLS model, the market dynamics change
dramatically. Figure 3 depicts a typical price path in a sim-
ulation of a market with 95% RII investors and 5% EMB
investors. The EMB investors have m D 10 (i. e., they es-
timate the ex-ante return distribution by observing the set
of the last 10 ex-post returns). , the standard deviation
of the random noise aﬀecting the EMBs’ decision mak-
ing is taken as 0.2. All investors, RII and EMB alike, have
the same risk aversion parameter ˛ D 1:5 (as before). In
the ﬁrst 150 trading periods the price dynamics look very
similar to the typical dynamics of the benchmark model.
However, after the ﬁrst 150 or so periods the price dynam-
ics change. From this point onwards the market is char-
acterized by periodic booms and crashes. Of course, Fig. 3
describes only one simulation. However, as will become
evident shortly, diﬀerent simulations with the same pa-
rameters may diﬀer in detail, but the pattern is general: at
some stage (not necessarily after 150 periods) the EMB in-
vestors induce cyclic price behavior. It is quite astonishing
that such a small minority of only 5% of the investors can
have such a dramatic impact on the market.
In order to understand the periodic booms and crashes
let us focus on the behavior of the EMB investors. After
every trade, the EMB investors revise their estimation of
the ex-ante return distribution, because the set of ex-post
returns they employ to estimate the ex-ante distribution
changes. Namely, investors add the latest return generated
by the stock to this set and delete the oldest return from
this set. As a result of this update in the estimation of the
ex-ante distribution, the optimal investment proportion
x changes, and EMB investors revise their portfolios at

Agent Based Computational Economics
A
105
Agent Based Computational Economics, Figure 3
5% of Investors are Efficient Market Believers, 95% Rational Informed Investors
next period’s trade. During the ﬁrst 150 or so periods, the
informed investors control the dynamics and the returns
ﬂuctuate randomly (as in the benchmark model). As a con-
sequence, the investment proportion of the EMB investors
also ﬂuctuates irregularly. Thus, during the ﬁrst 150 pe-
riods the EMB investors do not eﬀect the dynamics much.
However, at point a the dynamics change qualitatively (see
Fig. 3). At this point, a relatively high dividend is realized,
and as a consequence, a relatively high return is generated.
This high return leads the EMB investors to increase their
investment proportion in the stock at the next trading pe-
riod. This increased demand of the EMB investors is large
enough to eﬀect next period’s price, and thus a second high
return is generated. Now the EMB investors look at a set
of ex-post returns with two high returns, and they increase
their investment proportion even further. Thus, a positive
feedback loop is created.
Notice that as the price goes up, the informed investors
realize that the stock is overvalued relative to the funda-
mental value P f and they decrease their holdings in the
stock. However, this eﬀect does not stop the price increase
and break the feedback loop because the EMB investors
continue to buy shares aggressively. The positive feedback
loop pushes the stock price further and further up to point
b, at which the EMBs are invested 100% in the stock. At
point b the positive feedback loop “runs out of gas”. How-
ever, the stock price remains at the high level because the
EMB investors remain fully invested in the stock (the set
of past m=10 returns includes at this stage the very high
returns generated during the “boom” – segment a–b in
Fig. 3).
When the price is at the high level (segment b–c), the
dividend yield is low, and as a consequence, the returns
are generally low. As time goes by and we move from
point b towards point c, the set of m D 10 last returns
gets ﬁlled with low returns. Despite this fact, the extremely
high returns generated in the boom are also still in this set,
and they are high enough to keep the EMB investors fully
invested. However, 10 periods after the boom, these ex-
tremely high returns are pushed out of the set of relevant
ex-post returns. When this occurs, at point c, the EMB in-
vestors face a set of low returns, and they cut their invest-
ment proportion in the stock sharply. This causes a dra-
matic crash (segment c–d). Once the stock price goes back
down to the “fundamental” value, the informed investors
come back into the picture. They buy back the stock and
stop the crash.
The EMB investors stay away from the stock as long
as the ex-post return set includes the terrible return of
the crash. At this stage the informed investors regain con-
trol of the dynamics and the stock price remains close to
its fundamental value. 10 periods after the crash the ex-
tremely negative return of the crash is excluded from the
ex-post return set, and the EMB investors start increasing
their investment proportion in the stock (point e). This
drives the stock price up, and a new boom-crash cycle is
initiated. This cycle repeats itself over and over almost pe-
riodically.
Figure 3 depicts the price dynamics of a single simu-
lation. One may therefore wonder how general the results
discussed above are. Figure 4 shows two more simulations
with the same parameters but diﬀerent dividend realiza-

106 A
Agent Based Computational Economics
Agent Based Computational Economics, Figure 4
Two More Simulations – same Parameters as Fig. 3, Different Divident Realizations
tions. It is evident from this ﬁgure that although the sim-
ulations vary in detail (because of the diﬀerent dividend
realizations), the overall price pattern with periodic boom-
crash cycles is robust.
Although these dynamics are very unrealistic in terms
of the periodicity, and therefore the predictability of the
price, they do shed light on the mechanism generating
many of the empirically observed market phenomena. In
the next section, when we relax the assumption that the
EMB population is homogeneous with respect to m, the
price is no longer cyclic or predictable, yet the mechanisms
generating the market phenomena are the same as in this
homogeneous EMB population case. The homogeneous
EMB population case generates the following market phe-
nomena:
Heavy Trading Volume
As explained above, shares
change hands continuously between the RII investors and
the EMB investors. When a “boom” starts the RII investors
observe higher ex-post returns and become more opti-
mistic, while the RII investor view the stock as becom-
ing overpriced and become more pessimistic. Thus, at this
stage the EMBs buy most of the shares from the RIIs.
When the stock crashes, the opposite is true: the EMBs are
very pessimistic, but the RII investors buy the stock once
it falls back to the fundamental value. Thus, there is sub-
stantial trading volume in this market. The average trading
volume in a typical simulation is about 1000 shares per pe-
riod, which are 10% of the total outstanding shares.
Autocorrelation of Returns
The cyclic behavior of the
price yields a very deﬁnite return autocorrelation pat-
tern. The autocorrelation pattern is depicted graphically
in Fig. 5. The autocorrelation pattern is directly linked to
the length of the price cycle, which in turn are determined
by m. Since the moving window of ex-post returns used to
estimate the ex-ante distribution is m D 10 periods long,
the price cycles are typically a little longer than 20 periods
long: a cycle consists of the positive feedback loop (seg-
ment a–b in Fig. 3) which is about 2–3 periods long, the
upper plateau (segment b–c in Fig. 3) which is about 10
periods long, the crash that occurs during one or two pe-
riods, and the lower plateau (segment d–e in Fig. 3) which
is again about 10 periods long, for a total of about 23–
25 periods. Thus, we expect positive autocorrelation for
lags of about 23–25 periods, because this is the lag be-
tween one point and the corresponding point in the next
(or previous) cycle. We also expect negative autocorrela-
tion for lags of about 10–12 periods, because this is the
lag between a boom and the following (or previous) crash,
and vice versa. This is precisely the pattern we observe in
Fig. 5.
Excess Volatility
The EMB investors induce large devia-
tions of the price from the fundamental value. Thus, price
ﬂuctuations are caused not only by dividend ﬂuctuations
(as the standard theory suggests) but also by the endoge-
nous market dynamics driven by the EMB investors. This
“extra” source of ﬂuctuations causes the price to be more
volatile than the fundamental value P f .
Indeed, for 100 1000-period independent simulations
with 5% EMB investors we ﬁnd an average (pt) of 46.4,
and an average (p f
t ) of 30.6; i. e., we have excess volatility
of about 50%.

Agent Based Computational Economics
A
107
Agent Based Computational Economics, Figure 5
Return Autocorrelation 5%, Efficient Market Believers, m D 10
As a ﬁrst step in analyzing the eﬀects of heterogene-
ity of the EMB population, in the next section we examine
the case of two types of EMB investors. We later analyze
a model in which there is a full spectrum of EMB investors.
Two Types of EMBs
One justiﬁcation for using a repre-
sentative agent in economic modeling is that although in-
vestors are heterogeneous in reality, one can model their
collective behavior with one representative or “average”
investor. In this section we show that this is generally not
true. Many aspects of the dynamics result from the non-
linear interaction between diﬀerent investor types. To il-
lustrate this point, in this section we analyze a very simple
case in which there are only two types of EMB investors:
one with m D 5 and the other with m D 15. Each of these
two types consists of 2% of the investor population, and
the remaining 96% are informed investors. The represen-
tative agent logic may tempt us to think that the resulting
market dynamics would be similar to that of one “average”
investor, i. e. an investor with m D 10. Figure 6 shows that
this is clearly not the case. Rather than seeing periodic cy-
cles of about 23–25 periods (which correspond to the av-
erage m of 10, as in Fig. 3), we see an irregular pattern. As
before, the dynamics are ﬁrst dictated by the informed in-
vestors. Then, at point a, the EMB investors with m D 15
induce cycles which are about 30 periods long. At point b
there is a transition to shorter cycles induced by the m D 5
population, and at point c there is another transition back
to longer cycles. What is going on?
These complex dynamics result from the non-linear
interaction between the diﬀerent sub-populations. The
transitions from one price pattern to another can be
partly understood by looking at the wealth of each sub-
population. Figure 7 shows the proportion of the total
wealth held by each of the two EMB populations (the re-
maining proportion is held by the informed investors). As
seen in Fig. 7, the cycles which start at point a are dictated
by the m D 15 rather than the m D 5 population, because
at this stage the m D 15 population controls more of the
wealth than the m D 5 population. However, after 3 cycles
(at point b) the picture is reversed. At this point the m D 5
population is more powerful than the m D 15 population,
and there is a transition to shorter boom-crash cycles. At
point c the wealth of the two sub-populations is again al-
most equal, and there is another transition to longer cycles.
Thus, the complex price dynamics can be partly under-
stood from the wealth dynamics. But how are the wealth
dynamics determined? Why does the m D 5 population
become wealthier at point b, and why does it lose most of
this advantage at point c? It is obvious that the wealth dy-
namics are inﬂuenced by the price dynamics, thus there is
a complicated two-way interaction between the two. Al-
though this interaction is generally very complex, some
principle ideas about the mutual inﬂuence between the
wealth and price patterns can be formulated. For exam-
ple, a population that becomes dominant and dictates the
price dynamics, typically starts under-performing, because
it aﬀects the price with its actions. This means pushing
the price up when buying, and therefore buying high, and
pushing the price down when selling. However, a more de-
tailed analysis must consider the speciﬁc investment strat-
egy employed by each population. For a more compre-
hensive analysis of the interaction between heterogeneous
EMB populations see [25].
The two EMB population model generates the same
market phenomena as did the homogeneous population

108 A
Agent Based Computational Economics
Agent Based Computational Economics, Figure 6
2% EMB m D 5, 2% EMB m D 15;96% RII
Agent Based Computational Economics, Figure 7
Proportion of the total wealth held by the two EMB populations
case: heavy trading volume, return autocorrelations, and
excess volatility. Although the price pattern is much less
regular in the two-EMB-population case, there still seems
to be a great deal of predictability about the prices. More-
over, the booms and crashes generated by this model are
unrealistically dramatic and frequent. In the next sec-
tion we analyze a model with a continuous spectrum of
EMB investors. We show that this fuller heterogeneity
of investors leads to very realistic price and volume pat-
terns.
Full Spectrum of EMB Investors
Up to this point we
have analyzed markets with at most three diﬀerent sub-
populations (one RII population and two EMB popula-
tions). The market dynamics we found displayed the em-
pirically observed market anomalies, but they were unreal-
istic in the magnitude, frequency, and semi-predictability
of booms and crashes. In reality, we would expect not only
two or three investor types, but rather an entire spectrum
of investors. In this section we consider a model with a full
spectrum of diﬀerent EMB investors. It turns out that more
is diﬀerent. When there is an entire range of investors, the
price dynamics become realistic: booms and crashes are
not periodic or predictable, and they are also less frequent
and dramatic. At the same time, we still obtain all of the
market anomalies described before.

Agent Based Computational Economics
A
109
In this model each investor has a diﬀerent number of
ex-post observations which he utilizes to estimate the ex-
ante distribution. Namely, investor i looks at the set of the
mi most recent returns on the stock, and we assume that
mi is distributed in the population according to a trun-
cated normal distribution with average ¯m and standard
deviation m (as m  0 is meaningless, the distribution is
truncated at m D 0).
Figure 8 shows the price pattern of a typical simula-
tion of this model. In this simulation 90% of the investors
are RII, and the remaining 10% are heterogeneous EMB
investors with ¯m D 40, and m D 10. The price pattern
seems very realistic with “smoother” and more irregular
cycles. Crashes are dramatic, but infrequent and unpre-
dictable.
The heterogeneous EMB population model generates
the following empirically observed market phenomena:
Return Autocorrelation: Momentum and Mean-Reversion
In the heterogeneous EMB population model trends are
generated by the same positive feedback mechanism that
generated cycles in the homogeneous case: high (low) re-
turns tend to make the EMB investors more (less) aggres-
sive, this generates more high (low) returns, etc. The dif-
ference between the two cases is that in the heterogeneous
case there is a very complicated interaction between all
the diﬀerent investor sub-populations and as a result there
are no distinct regular cycles, but rather, smoother and
more irregular trends. There is no single cycle length –
the dynamics are a combination of many diﬀerent cycles.
This makes the autocorrelation pattern also smoother and
more continuous. The return autocorrelations in the het-
erogeneous model are shown in Fig. 9. This autocorrela-
tion pattern conforms with the empirical ﬁndings. In the
short-run (lags 1–4) the autocorrelation is positive – this
is the empirically documented phenomena known as mo-
mentum: in the short-run, high returns tend to be fol-
lowed by more high returns, and low returns tend to be fol-
lowed by more low returns. In the longer-run (lags 5–13)
the autocorrelation is negative, which is known as mean-
reversion. For even longer lags the autocorrelation even-
tually tends to zero. The short-run momentum, longer-
run mean-reversion, and eventual diminishing autocorre-
lation creates the general “U-shape” which is found in em-
pirical studies [7,13,31] and which is seen in Fig. 9.
Excess Volatility
The price level is generally determined
by the fundamental value of the stock. However, as in the
homogeneous EMB population case, the EMB investors
occasionally induce temporary departures of the price
away from the fundamental value. These temporary de-
partures from the fundamental value make the price more
volatile than the fundamental value. Following Shiller’s
methodology we deﬁne the detrended price, p, and fun-
damental value, p f . Averaging over 100 independent sim-
ulations we ﬁnd (p) D 27:1 and (p f ) D 19:2, which is
an excess volatility of 41% .
Heavy Volume
As investors in our model have diﬀerent
information (the informed investors know the dividend
process, while the EMB investors do not), and diﬀerent
ways of interpreting the information (EMB investors with
diﬀerent memory spans have diﬀerent estimations regard-
ing the ex-ante return distribution), there is a high level of
trading volume in this model. The average trading volume
in this model is about 1700 shares per period (17% of the
total outstanding shares). As explained below, the volume
is positively correlated with contemporaneous and lagged
absolute returns.
Volume is Positively Correlated with Contemporaneous
and Lagged Absolute Returns
Investors revise their port-
folios as a result of changes in their beliefs regarding the
future return distribution. The changes in the beliefs can
be due to a change in the current price, to a new divi-
dend realization (in the case of the informed investors),
or to a new observation of an ex-post return (in the case
of the EMB investors). If all investors change their be-
liefs in the same direction (for example, if everybody be-
comes more optimistic), the stock price can change sub-
stantially with almost no volume – everybody would like
to increase the proportion of the stock in his portfolio, this
will push the price up, but a very small number of shares
will change hands. This scenario would lead to zero or per-
haps even negative correlation between the magnitude of
the price change (or return) and the volume. However, the
typical scenario in the LLS model is diﬀerent. Typically,
when a positive feedback trend is induced by the EMB
investors, the opinions of the informed investors and the
EMB investors change in opposite directions. The EMB in-
vestors see a trend of rising prices as a positive indication
about the ex-ante return distribution, while the informed
investors believe that the higher the price level is above the
fundamental value, the more overpriced the stock is, and
the harder it will eventually fall. The exact opposite holds
for a trend of falling prices. Thus, price trends are typi-
cally interpreted diﬀerently by the two investor types, and
therefore induce heavy trading volume. The more pro-
nounced the trend, the more likely it is to lead to heavy
volume, and at the same time, to large price changes which
are due to the positive feedback trading on behalf of the
EMB investors.

110 A
Agent Based Computational Economics
Agent Based Computational Economics, Figure 8
Spectrum of Heterogeneous EMB Investors (10% EMB Investors, 90% RII Investors)
Agent Based Computational Economics, Figure 9
Return Autocorrelation – Heterogeneous EMB Population
This explains not only the positive correlation between
volume and contemporaneous absolute rates of return, but
also the positive correlation between volume and lagged
absolute rates of return. The reason is that the behavior of
the EMB investors induces short-term positive return au-
tocorrelation, or momentum (see above). That is, a large
absolute return this period is associated not only with high
volume this period, but also with a large absolute return
next period, and therefore with high volume next period.
In other words, when there is a substantial price increase
(decrease), EMB investors become more (less) aggressive
and the opposite happens to the informed traders. As we
have seen before, when a positive feedback loop is started,
the EMB investors are more dominant in determining the
price, and therefore another large price increase (decrease)
is expected next period. This large price change is likely to
be associated with heavy trading volume as the opinions
of the two populations diverge. Furthermore, this large in-
crease (decrease) is expected to make the EMB investors
even more optimistic (pessimistic) leading to another large
price increase (decrease) and heavy volume next period.
In order to verify this relationship quantitatively, we
regress volume on contemporaneous and lagged absolute
rates of return for 100 independent simulations. We run
the regressions:
Vt D ˛ C ˇC jRt  1j C "t ;
and
Vt D ˛ C ˇL jRt1  1j C "t ;
(17)

Agent Based Computational Economics
A
111
where Vt is the volume at time t and Rt is the total return
on the stock at time t, and the subscripts C and L stand for
contemporaneous and lagged. We ﬁnd an average value of
870 for ˆˇC with an average t-value of 5.0 and an average
value of 886 for ˆˇL with an average t-value of 5.1.
Discussion of the LLS Results
The LLS model is an
Agent Based Simulation model of the stock market which
incorporates some of the fundamental experimental ﬁnd-
ings regarding the behavior of investors. The main non-
standard assumption of the model is that there is a small
minority of investors in the market who are uninformed
about the dividend process and who believe in market ef-
ﬁciency. The investment decision of these investors is re-
duced to the optimal diversiﬁcation between the stock and
the bond.
The LLS model generates many of the empirically doc-
umented market phenomena which are hard to explain
in the analytical rational-representative-agent framework.
These phenomena are:
 Short term momentum;
 Longer term mean reversion;
 Excess volatility;
 Heavy trading volume;
 Positive correlation between volume and contempora-
neous absolute returns;
 Positive correlation between volume and lagged abso-
lute returns;
 Endogenous market crashes.
The fact that so many “puzzles” are explained with a sim-
ple model built on a small number of empirically docu-
mented behavioral elements leads us to suspect that these
behavioral elements are very important in understanding
the workings of the market. This is especially true in light
of the observations that a very small minority of the non-
standard bounded-rational investors can have a dramatic
inﬂuence on the market, and that these investors are not
wiped out by the majority of rational investors.
Summary and Future Directions
Standard economic models typically describe a world of
homogeneous rational agents. This approach is the foun-
dation of most of our present day knowledge in economic
theory. With the Agent Based Simulation approach we can
investigate a much more complex and “messy” world with
diﬀerent agent types, who employ diﬀerent strategies to try
to survive and prosper in a market with structural uncer-
tainty. Agents can learn over time, from their own experi-
ence and from their observation about the performance of
other agents. They co-evolve over time and as they do so,
the market dynamics change continuously. This is a world
view closer to biology, than it is to the “clean” realm of
physical laws which classical economics has aspired to.
The Agent Based approach should not and can not re-
place the standard analytical economic approach. Rather,
these two methodologies support and complement each
other: When an analytical model is developed, it should
become standard practice to examine the robustness of
the model’s results with agent based simulations. Simi-
larly, when results emerge from agent based simulation,
one should try to understand their origin and their gener-
ality, not only by running many simulations, but also by
trying to capture the essence of the results in a simpliﬁed
analytical setting (if possible).
Although the ﬁrst steps in economic agent based sim-
ulations were made decades ago, economics has been slow
and cautious to adopt this new methodology. Only in re-
cent years has this ﬁeld begun to bloom. It is my belief and
hope that the agent based approach will prove as fruitful
in economics as it has been in so many other branches of
science.
Bibliography
Primary Literature
1. Admati A, Pfleiderer P (1988) A theory of intraday patterns: Vol-
ume and price variability. Rev Financ Stud 1:3–40
2. Arthur WB (1994) Inductive reasoning and bounded rationality
(The El Farol problem). Am Econ Rev 84:406–411
3. Arthur WB, Holland JH, Lebaron B, Palmer RG, Tayler P (1997)
Asset pricing under endogenous expectations in an artificial
stock market. In: Arthur WB, Durlauf S, Lane D (eds) The econ-
omy as an evolving complex system II. Addison-Wesley, Red-
wood City
4. Brock WA, Hommes CA (1998) Heterogeneous beliefs and
routes to chaos in a simple asset pricing model. J Econ Dyn
Control 22:1235–1274
5. Egenter E, Lux T, Stauffer D (1999) Finite size effects in
Monte Carlo Simulations of two stock market models. Physica
A 268:250–256
6. Epstein JM, Axtell RL (1996) Complex adaptive systems. In:
Growing artificial societies: Social science from the bottom up.
MIT Press, Washington DC
7. Fama E, French K (1988) Permanent and temporary compo-
nents of stock prices. J Political Econ 96:246–273
8. Friend I, Blume ME (1975) The demand for risky assets. Am
Econ Rev 65:900–922
9. Gordon J, Paradis GE, Rorke CH (1972) Experimental evidence
on alternative portfolio decision rules. Am Econ Rev 62(1):107–
118
10. Grossman S, Stiglitz J (1980) On the impossibility of informa-
tionally efficient markets. Am Econ Rev 70:393–408
11. Hellthaler T (1995) The influence of investor number on a mi-
croscopic market. Int J Mod Phys C 6:845–852

112 A
Agent Based Modeling and Artificial Life
12. Hommes CH (2002) Modeling the stylized facts in finance
through simple nonlinear adaptive systems. PNAS 99:7221–
7228
13. Jegadeesh N, Titman S (1993) Returns to buying winners and
selling losers: Implications for stock market efficiency. J Fi-
nance 48:65–91
14. Karpoff J (1987) The relationship between price changes and
trading volume: A survey. J Financ Quantitative Anal 22:109–
126
15. Kim GW, Markowitz HM (1989) Investment rules, margin, and
market volatility. J Portf Manag 16:45–52
16. Kirman AP (1992) Whom or what does the representative
agent represent? J Econ Perspectiv 6:117–136
17. Kohl R (1997) The influence of the number of different stocks
on the Levy, Levy Solomon model. Int J Mod Phys C 8:1309–
1316
18. Kroll Y, Levy H, Rapoport A (1988) Experimental tests of the
separation theorem and the capital asset pricing model. Am
Econ Rev 78:500–519
19. LeBaron B (2000) Agent-based computational finance: Sug-
gested readings and early research. J Econ Dyn Control
24:679–702
20. Levy H (1994) Absolute and relative risk aversion: An experi-
mental study. J Risk Uncertain 8:289–307
21. Levy H, Lim KC (1998) The economic significance of the cross-
sectional autoregressive model: Further analysis. Rev Quant Fi-
nance Acc 11:37–51
22. Levy M, Levy H (1996) The danger of assuming homogeneous
expectations. Financ Analyst J 52:65–70
23. Levy M, Levy H, Solomon S (1994) A microscopic model of the
stock market: Cycles, booms, and crashes. Econs Lett 45:103–
111
24. Levy M, Levy H, Solomon S (2000) Microscopic simulation of
financial markets. Academic Press, San Diego
25. Levy M, Persky N, Solomon, S (1996) The complex dyn of a sim-
ple stock market model. Int J High Speed Comput 8:93–113
26. Lux T (1995) Herd behaviour, bubbles and crashes. Econ J
105:881
27. Lux T (1998) The socio-economic dynamics of speculative bub-
bles: Interacting agents, chaos, and the fat tails of returns dis-
tributions. J Econ Behav Organ 33:143–165
28. Lux T, Marchesi M (1999) Volatility clustering in financial mar-
kets: A micro-simulation of interacting agents. Nature 397:498
29. Orcutt GH, Caldwell SB, Wertheimer R (1976) Policy exploration
through microanalytic simulation. The Urban Institute, Wash-
ington DC
30. Palmer RG, Arthur WB, Holland JH, LeBaron B, Tayler P (1994)
Artificial economic life: A simple model of a stock market. Phys-
ica D 75:264–274
31. Poterba JM, Summers LH (1988) Mean reversion in stock re-
turns: Evidence and implications. J Financial Econs 22:27–59
32. Samanidou E, Zschischang E, Stauffer D, Lux T (2007) Agent-
based models of financial markets. Rep Prog Phys 70:409–450
33. Samuelson PA (1989) The judgement of economic science on
rational portfolio management: Timing and long horizon ef-
fects. J Portfolio Manag 16:4–12
34. Samuelson PA (1994) The long term case for equities and how
it can be oversold. J Portf Management 21:15–24
35. Sargent T (1993) Bounded rationality and macroeconomics.
Oxford University Press, Oxford
36. SchellingTC (1978) Micro motives and macro behavior. Norton
& Company, New York
37. Shiller RJ (1981) Do stock prices move too much to be justi-
fied by subsequent changes in dividends? Am Econ Rev 71:
421–436
38. Stauffer D, de Oliveira PMC, Bernardes AT (1999) Monte
Carlo Simulation of volatility correlation in microscopic market
model. Int J Theor Appl Finance 2:83–94
39. Tesfatsion L (2002) Agent-based computational economics:
Growing economies from the bottom up. Artif Life 8:55–82
40. Tesfatsion L (2001) Special issue on agent-based computa-
tional economics. J Econ Dyn Control 25:281–293
41. Thaler R (ed) (1993) Advances in behavioral finance. Russel
Sage Foundation, New York
42. Thaler R (1994) Quasi rational economics. Russel Sage Founda-
tion, New York
43. Tversky A, Kahneman D (1981) The framing of decisions and
the psychology of choice. Science 211:453–480
44. Tversky A, Kahneman D (1986) Rational choice and the framing
of decision. J Bus 59(4):251–278
45. Tversky A, Kahneman D (1992) Advances in prospect the-
ory: Cumulative representation of uncertainty. J Risk Uncertain
5:297–323
Books and Reviews
Anderson PW, Arrow J, Pines D (eds) (1988) The economy as an
evolving complex system. Addison-Wesley, Redwood City
Axelrod R (1997) The complexity of cooperation: Agent-based
models of conflict and cooperation. Princeton University Press,
Princeton
Moss de Oliveira S, de Oliveira H, Stauffer D (1999) Evolution,
money, war and computers. BG Teubner, Stuttgart-Leipzig
Solomon S (1995) The microscopic representation of complex
macroscopic phenomena. In: Stauffer D (ed) Annual Rev Com-
put Phys II. World Scientific, Singapore
Agent Based Modeling
and Artificial Life
CHARLES M. MACAL
Center for Complex Adaptive Agent Systems Simulation
(CAS2), Decision and Information Sciences Division,
Argonne National Laboratory, Argonne, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Artiﬁcial Life
Alife in Agent-Based Modeling
Future Directions
Bibliography

Agent Based Modeling and Artificial Life
A
113
Glossary
Adaptation The process by which organisms (agents)
change their behavior or by which populations of
agents change their collective behaviors with respect to
their environment.
Agent-based modeling (ABM) A modeling and simula-
tion approach applied to a complex system or complex
adaptive system, in which the model is comprised of
a large number of interacting elements (agents).
Ant colony optimization (ACO) A heuristic optimiza-
tion technique motivated by collective decision pro-
cesses followed by ants in foraging for food.
Artiﬁcial chemistry Chemistry based on the information
content and transformation possibilities of molecules.
Artiﬁcial life (ALife) A ﬁeld that investigates life’s essen-
tial qualities primarily from an information content
perspective.
Artiﬁcial neural network (ANN) A heuristic optimiza-
tion and simulated learning technique motivated by
the neuronal structure of the brain.
Autocatalytic set A closed set of chemical reactions that
is self-sustaining.
Autonomous The characteristic of being capable of mak-
ing independent decisions over a range of situations.
Avida An advanced artiﬁcial life computer program de-
veloped by Adami [1] and others that models popula-
tions of artiﬁcial organisms and the essential features
of life such as interaction and replication.
Biologically-inspired computational algorithm
Any kind of algorithm that is based on biological
metaphors or analogies.
Cellular automata (CA) A mathematical construct and
technique that models a system in discrete time and
discrete space in which the state of a cell depends on
transition rules and the states of neighboring cells.
Coevolution A process by which many entities adapt and
evolve their behaviors as a result of mutually eﬀective
interactions.
Complex system A system comprised of a large number
of strongly interacting components (agents).
Complex adaptive system (CAS) A system comprised of
a large number of strongly interacting components
(agents) that adapt at the individual (agent) level or
collectively at the population level.
Decentralized control A feature of a system in which
the control mechanisms are distributed over multiple
parts of the system.
Digital organism An entity that is represented by its es-
sential information-theoretic elements (genomes) and
implemented as a computational algorithm or model.
Downward causation The process by which a higher-or-
der emergent structure takes on its own emergent be-
haviors and these behaviors exert inﬂuence on the con-
stituent agents of the emergent structure.
Dynamic network analysis Network modeling and anal-
ysis in which the structure of the network, i. e., nodes
(agents) and links (agent interactions) is endogenous
to the model.
Echo An artiﬁcial life computer program developed by
Holland [36] that models populations of complex
adaptive systems and the essential features of adapta-
tion in nature.
Emergence The process by which order is produced in
nature.
Entropy A measure of order, related to the information
needed to specify a system and its state.
Evolution (artiﬁcial) The process by which a set of in-
structions is transmitted and changes over successive
generations.
Evolutionary game A repeated game in which agents
adapt their strategies in recognition of the fact that
they will face their opponents in the future.
Evolution strategies A heuristic optimization technique
motivated by the genetic operation of selection and
mutation.
Evolutionary algorithm Any algorithm motivated by
the genetic operations including selection, mutation,
crossover, etc.
Evolutionary computing A ﬁeld of computing based on
the use of evolutionary algorithms.
Finite state machine A mathematical model consisting
of entities with a ﬁnite (usually small) number of pos-
sible discrete states.
Game of life, life A cellular automaton developed by
Conway [8] that illustrates a maximally complex sys-
tem based on simple rules.
Generative social science Social
science
investigation
with the goal of understanding how social processes
emerge out of social interaction.
Genetic algorithm (GA) A speciﬁc kind of evolutionary
algorithm motivated by the genetic operations of se-
lection, mutation, and crossover.
Genetic programming (GP) A speciﬁc kind of evolu-
tionary algorithm that manipulates symbols according
to prescribed rules, motivated by the genetic opera-
tions of selection, mutation, and crossover.
Genotype A set of instructions for a developmental pro-
cess that creates a complex structure, as in a genotype
for transmitting genetic information and seeding a de-
velopmental process leading to a phenotype.
Hypercycle A closed set of functional relations that is

114 A
Agent Based Modeling and Artificial Life
self-sustaining, as in an autocatalytic chemical reaction
network.
Individual-based model An
approach
originating
in
ecology to model populations of agents that empha-
sizes the need to represent diversity among individu-
als.
Langton’s ant An example of a very simple computation-
al program that computes patterns of arbitrary com-
plexity after an initial series of simple structures.
Langton’s loop An example of a very simple computa-
tional program that computes replicas of its structures
according to simple rules applied locally as in cellular
automata.
Learning classiﬁer system (LCS) A speciﬁc algorithmic
framework for implementing an adaptive system by
varying the weights applied to behavioral rules speci-
ﬁed for individual agents.
Lindenmeyer system (L-system)
A formal grammar, which is a set of rules for rewriting
strings of symbols.
Machine learning A ﬁeld of inquiry consisting of algo-
rithms for recognizing patterns in data (e. g., data
mining) through various computerized learning tech-
niques.
Mind–body problem A ﬁeld of inquiry that addresses
how human consciousness arises out of material pro-
cesses, and whether consciousness is the result of a log-
ical-deductive or algorithmic process.
Meme A term coined by Dawkins [17] to refer to the
minimal encoding of cultural information, similar to
the genome’s role in transmitting genetic informa-
tion.
Particle swarm optimization An optimization technique
similar to ant colony optimization, based on indepen-
dent particles (agents) that search a landscape to opti-
mize a single objective or goal.
Phenotype The result of an instance of a genotype inter-
acting with its environment through a developmental
process.
Reaction-diﬀusion system A system that includes mech-
anisms for both attraction and transformation (e. g., of
agents) as well as repulsion and diﬀusion.
Recursively generated object An object that is generated
by the repeated application of simple rules.
Self-organization A process by which structure and or-
ganization arise from within the endogenous instruc-
tions and processes inherent in an entity.
Self-replication The process by which an agent (e. g., or-
ganism, machine, etc.) creates a copy of itself that con-
tains instructions for both the agent’s operation and its
replication.
Social agent-based modeling Agent-based modeling ap-
plied to social systems, generally applied to people and
human populations, but also animals.
Social network analysis (SNA) A
collection
of
tech-
niques and approaches for analyzing networks of social
relationships.
Stigmergy The practice of agents using the environment
as a tool for communication with other agents to sup-
plement direct agent-to-agent communication.
Swarm An early agent-based modeling toolkit designed
to model artiﬁcial life applications.
Swarm intelligence Collective intelligence based on the
actions of a set of interacting agents behaving accord-
ing to a set of prescribed simple rules.
Symbolic processing A computational technique that
consists of processing symbols rather than strictly nu-
merical data.
Sugarscape An abstract agent-based model of artiﬁcial
societies developed by Epstein and Axtell [26] to in-
vestigate the emergence of social processes.
Tierra An early artiﬁcial life computer program devel-
oped by Ray [59] that models populations of artiﬁcial
organisms and the essential features of life such as in-
teraction and replication.
Universal Turing machine (UTM) An abstract represen-
tation of the capabilities of any computable system.
Update rule A rule or transformation directive for chang-
ing or updating the state of an entity or agent, as for ex-
ample updating the state of an agent in an agent-based
model or updating the state of an L-system.
Definition of the Subject
Agent-based modeling began as the computational arm
of artiﬁcial life some 20 years ago. Artiﬁcial life is con-
cerned with the emergence of order in nature. How do sys-
tems self-organize themselves and spontaneously achieve
a higher-ordered state? Agent-based modeling then, is
concerned with exploring and understanding the pro-
cesses that lead to the emergence of order through compu-
tational means. The essential features of artiﬁcial life mod-
els are translated into computational algorithms through
agent-based modeling. With its historical roots in artiﬁ-
cial life, agent-based modeling has become a distinctive
form of modeling and simulation. Agent-based modeling
is a bottom-up approach to modeling complex systems by
explicitly representing the behaviors of large numbers of
agents and the processes by which they interact. These es-
sential features are all that is needed to produce at least
rudimentary forms of emergent behavior at the systems
level. To understand the current state of agent-based mod-

Agent Based Modeling and Artificial Life
A
115
eling and where the ﬁeld aspires to be in the future, it is
necessary to understand the origins of agent-based model-
ing in artiﬁcial life.
Introduction
The ﬁeld of Artiﬁcial Life, or “ALife,” is intimately con-
nected to Agent-Based Modeling, or “ABM.” Although
one can easily enumerate some of life’s distinctive proper-
ties, such as reproduction, respiration, adaptation, emer-
gence, etc., a precise deﬁnition of life remains elusive.
Artiﬁcial Life had its inception as a coherent and sus-
tainable ﬁeld of investigation at a workshop in the late
1980s [43]. This workshop drew together specialists from
diverse ﬁelds who had been working on related problems
in diﬀerent guises, using diﬀerent vocabularies suited to
their ﬁelds.
At about the same time, the introduction of the per-
sonal computer suddenly made computing accessible,
convenient, inexpensive and compelling as an experimen-
tal tool. The future seemed to have almost unlimited pos-
sibilities for the development of ALife computer programs
to explore life and its possibilities. Thus several ALife soft-
ware programs emerged that sought to encapsulate the es-
sential elements of life through incorporation of ALife-re-
lated algorithms into easily usable software packages that
could be widely distributed. Computational programs for
modeling populations of digital organisms, such as Tierra,
Avida, and Echo, were developed along with more general
purpose agent-based simulators such as Swarm.
Yet, the purpose of ALife was never restricted to un-
derstanding or re-creating life as it exists today. According
to Langton:
Artiﬁcial systems which exhibit lifelike behaviors
are worthy of investigation on their own right,
whether or not we think that the processes that they
mimic have played a role in the development or me-
chanics of life as we know it to be. Such systems can
help us expand our understanding of life as it could
be. (p. xvi in [43])
The ﬁeld of ALife addresses life-like properties of systems
at an abstract level by focusing on the information con-
tent of such systems independent of the medium in which
they exist, whether it be biological, chemical, physical or in
silico. This means that computation, modeling, and simu-
lation play a central role in ALife investigations.
The relationship between ALife and ABM is complex.
A case can be made that the emergence of ALife as a ﬁeld
was essential to the creation of agent-based modeling.
Computational tools were both required and became pos-
sible in the 1980s for developing sophisticated models of
digital organisms and general purpose artiﬁcial life simu-
lators. Likewise, a case can be made that the possibility for
creating agent-based models was essential to making ALife
a promising and productive endeavor. ABM made it pos-
sible to understand the logical outcomes and implications
of ALife models and life-like processes. Traditional analyt-
ical means, although valuable in establishing baseline in-
formation, were limited in their capabilities to include es-
sential features of ALife. Many threads of ALife are still
intertwined with developments in ABM and vice verse.
Agent-based models demonstrate the emergence of life-
like features using ALife frameworks; ALife algorithms are
widely used in agent-based models to represent agent be-
haviors. These threads are explored in this article. In ALife
terminology, one could say that ALife and ABM have co-
evolved to their present states. In all likelihood they will
continue to do so.
This chapter covers in a necessarily brief and, per-
haps superﬁcial, but broad way, these relationships be-
tween ABM and ALife and extrapolates to future possibili-
ties. This chapter is organized as follows. Section “Artiﬁcial
Life” introduces Artiﬁcial Life, its essential elements and
its relationship to computing and agent-based modeling.
Section “Alife in Agent-Based Modeling” describes several
examples of ABM applications spanning many scales. Sec-
tion “Future Directions” concludes with future directions
for ABM and ALife. A bibliography is included for further
reading.
Artificial Life
Artiﬁcial Life was initially motivated by the need to model
biological systems and brought with it the need for com-
putation. The ﬁeld of ALife has always been multi-dis-
ciplinary and continues to encompass a broad research
agenda covering a variety of topics from a number of dis-
ciplines, including:
 Essential elements of life and artiﬁcial life,
 Origins of life and self-organization,
 Evolutionary dynamics,
 Replication and development processes,
 Learning and evolution,
 Emergence,
 Computation of living systems,
 Simulation systems for studying ALife, and
 Many others.
Each of these topics has threads leading into agent-based
modeling.

116 A
Agent Based Modeling and Artificial Life
The Essence of ALife
The essence of Artiﬁcial Life is summed up by Langton
(p. xxii in [43]) with a list of essential characteristics:
 Lifelike behavior on the part of man-made systems
 Semi-autonomous entities whose local interactions
with one another are governed by a set of simple rules
 Populations, rather than individuals
 Simple rather than complex speciﬁcations
 Local rather than global control
 Bottom-up rather than top-down modeling
 Emergent rather than pre-speciﬁed behaviors.
Langton observes that complex high-level dynamics and
structures often emerge (in living and artiﬁcial systems),
developing over time out of the local interactions among
low-level primitives. Agent-based modeling has grown up
around the need to model the essentials of ALife.
Self-Replication and Cellular Automata
Artiﬁcial Life
traces its beginnings to the work of John von Neumann
in the 1940s and investigations into the theoretical possi-
bilities for developing a self-replicating machine [64]. Such
a self-replicating machine not only carries instructions for
its operations, but also for its replication. The issue con-
cerned how to replicate such a machine that contained
the instructions for its operation along with the instruc-
tions for its replication. Did a machine to replicate such
a machine need to contain both the instructions for the
machine’s operation and replication, as well as instruc-
tions for replicating the instructions on how to replicate
the original machine? (see Fig. 1). Von Neumann used
the abstract mathematical construct of cellular automata,
originally conceived in discussions with Stanislaw Ulam,
to prove that such a machine could be designed, at least in
theory. Von Neumann was never able to build such a ma-
chine due to the lack of sophisticated computers that ex-
isted at the time.
Cellular automata (CA) have been central to the de-
velopment of computing Artiﬁcial Life models. Virtually
all of the early agent-based models that required agents to
be spatially located were in the form of von Neumann’s
original cellular automata. A cellular automata is a ﬁnite-
state machine in which time and space are treated as dis-
crete rather than continuous, as would be the case, for
example in diﬀerential equation models. A typical CA is
a two-dimensional grid or lattice consisting of cells. Each
cell assumes one of a ﬁnite number of states at any time.
A cell’s neighborhood is the set of cells surrounding a cell,
typically, a ﬁve-cell neighborhood (von Neumann neigh-
borhood) or a nine-cell neighborhood (Moore neighbor-
hood), as in Fig. 2.
A set of simple state transition rules determines the
value of each cell based on the cell’s state and the states
of neighboring cells. Every cell is updated at each time
according to the transition rules. Each cell is identical in
terms of its update rules. Cells diﬀer only in their initial
states. A CA is deterministic in the sense that the same
state for a cell and its set of neighbors always results in the
same updated state for the cell. Typically, CAs are set up
with periodic boundary conditions, meaning that the set
of cells on one edge of the grid boundary are the neighbor
cells to the cells on the opposite edge of the grid boundary.
The space of the CA grid forms a surface on a toroid, or
donut-shape, so there is no boundary per se. It is straight-
forward to extend the notion of cellular automata to two,
three, or more dimensions.
Von Neumann solved the self-replication problem by
developing a cellular automaton in which each cell had 29
possible states and ﬁve neighbors (including the updated
cell itself). In the von Neumann neighborhood, neighbor
cells are in the north, south, east and west directions from
the updated cell.
The Game Of Life
Conway’s Game of Life, or Life, de-
veloped in the 1970s, is an important example of a CA [8,
31,57]. The simplest way to illustrate some of the basic
ideas of agent-based modeling is through a CA. The Game
of Life is a two-state, nine-neighbor cellular automata with
three rules that determine the state (either On, i. e., shaded,
or Oﬀ, i. e., white) of each cell:
1. A cell will be On in the next generation if exactly three
of its eight neighboring cells are currently On.
2. A cell will retain its current state if exactly two of its
neighbors are On.
3. A cell will be Oﬀotherwise.
Initially, a small set of On cells is randomly distributed
over the grid. The three rules are then applied repeatedly
to all cells in the grid.
After several updates of all cells on the grid, distinc-
tive patterns emerge, and in some cases these patterns can
sustain themselves indeﬁnitely throughout the simulation
(Fig. 3). The state of each cell is based only on the current
state of the cell and the cells touching it in its immediate
neighborhood. The nine-neighbor per neighborhood as-
sumption built into Life determines the scope of the locally
available information for each cell to update its state.
Conway showed that, at least in theory, the structures
and patterns that can result during a Life computation
are complex enough to be the basis for a fully functional

Agent Based Modeling and Artificial Life
A
117
Agent Based Modeling and Artificial Life, Figure 1
Von Neumann’s Self-Replication Problem
Agent Based Modeling and Artificial Life, Figure 2
Cellular Automata Neighborhoods
computer that is complex enough to spontaneously gen-
erate self-replicating structures (see the section below on
universal computation). Two observations are important
about the Life rules:
 As simple as the state transition rules are, by using only
local information, structures of arbitrarily high com-
plexity can emerge in a CA.
 The speciﬁc patterns that emerge are extremely sensi-
tive to the speciﬁc rules used. For example, changing
Rule 1 above to “A cell will be On in the next gener-
ation if exactly four of its eight neighboring cells are
currently On” results in the development of completely
diﬀerent patterns.
 The Game of Life provides insights into the role of in-
formation in fundamental life processes.
Cellular Automata Classes
Wolfram investigated the
possibilities for complexity in cellular automata across the

118 A
Agent Based Modeling and Artificial Life
Agent Based Modeling and Artificial Life, Figure 3
Game of Life
full range of transition rules and initial states, using one-
dimensional cellular automata [70]. He categorized four
distinct classes for the resulting patterns produced by a CA
as it is solved repeatedly over time. These are:
 Class I: homogeneous state,
 Class II: simple stable or periodic structure,
 Class III: chaotic (non-repeating) pattern, and
 Class IV: complex patterns of localized structures.
The most interesting of these is Class IV cellular automata,
in which very complex patterns of non-repeating local-
ized structures emerge that are often long-lived. Wolfram
showed that these Class IV structures were also com-
plex enough to support universal computation [72]. Lang-
ton [45] coined the term “life at the edge of chaos” to de-
scribe the idea that Class IV systems are situated in a thin
region between Class II and Class III systems. Agent-based
models often yield Class I, II, and III behaviors.
Other experiments with CAs investigated the simplest
representations that could replicate themselves and pro-
duce emergent structures. Langton’s Loop is a self-repli-
cating two-dimensional cellular automaton, much sim-
pler than von Neumann’s [41]. Although not complex
enough to be a universal computer, Langton’s Loop was
the simplest known structure that could reproduce itself.
Langton’s Ant is a two-dimensional CA with a simple set
of rules, but complicated emergent behavior. Following
a simple set of rules for moving from cell to cell, a sim-
ulated ant displays unexpectedly complex behavior. After
an initial period of chaotic movements in the vicinity of its
initial location, the ant begins to build a recurrent pattern
of regular structures that repeats itself indeﬁnitely [42].
Langton’s Ant has behaviors complex enough to be a uni-
versal computer.
Genotype/Phenotype
Distinction
Biologists
distin-
guish between the genotype and the phenotype as hallmarks
of biological systems. The genotype is the template – the
set of instructions, the speciﬁcation, the blueprint – for an
organism. DNA is the genotype for living organisms, for
example. A DNA strand contains the complete instruc-
tions for the replication and development of the organism.
The phenotype is the organism – the machine, the prod-
uct, the result – that develops from the instructions in the
genotype (Fig. 4).
Morphogenesis is the developmental process by which
the phenotype develops in accord with the genotype,
through interactions with and resources obtained from
its environment. In a famous paper, Turing [66] modeled
the dynamics of morphogenesis, and more generally, the
problem of how patterns self-organize spontaneously in
nature. Turing used diﬀerential equations to model a sim-
ple set of reaction-diﬀusion chemical reactions. Turing
demonstrated that only a few assumptions were necessary
to bring about the emergence of wave patterns and gradi-
ents of chemical concentration, suggestive of morphologi-
cal patterns that commonly occur in nature. Reaction-dif-
fusion systems are characterized by the simultaneous pro-
cesses of attraction and repulsion, and are the basis for the
agent behavioral rules (attraction and repulsion) in many
social agent-based models.
More recently, Bonabeau extended Turing’s treatment
of morphogenesis to a theory of pattern formation based
on agent-based modeling. Bonabeau [12] states the rea-
son for relying on ABM: “because pattern-forming sys-

Agent Based Modeling and Artificial Life
A
119
Agent Based Modeling and Artificial Life, Figure 4
Genotype and Phenotype Relations
tems based on agents are (relatively) more easily amenable
to experimental observations.”
Information Processes
One approach to building systems
from a genotype speciﬁcation is based on the methodology
of recursively generated objects. Such recursive systems are
compact in their speciﬁcation, and their repeated applica-
tion can result in complex structures, as demonstrated by
cellular automata.
Recursive systems are logic systems in which strings
of symbols are recursively rewritten based a minimum set
of instructions. Recursive systems, or term replacement
systems, as they have been called, can result in complex
structures. Examples of recursive systems include cellular
automata, as described above, and Lindenmayer systems,
called L-systems [46]. An L-system consists of a formal
grammar, which is a set of rules for rewriting strings of
symbols. L-systems have been used extensively for model-
ing living systems, for example, plant growth and develop-
ment, producing highly realistic renderings of plants, with
intricate morphologies and branching structures.
Wolfram [71] used symbolic recursion as a basis for
developing Mathematica, the computational mathemat-
ics system based on symbolic processing and term replace-
ment. Unlike numeric programming languages, a sym-
bolic programming language allows a variable to be a ba-
sic object and does not require a variable to be assigned
a value before it is used in a program.
Any agent-based model is essentially a recursive sys-
tem. Time is simulated by the repeated application of the
agent updating rules. The genotype is the set of rules for
the agent behaviors. The phenotype is set of the patterns
and structures that emerge from the computation. As in
cellular automata and recursive systems, extremely com-
plex structures emerge in agent-based models that are of-
ten unpredictable from examination of the agent rules.
Emergence
One of the primary motivations for the ﬁeld
of ALife is to understand emergent processes, that is, the
processes by which life emerges from its constituent el-
ements. Langton writes: “The ‘key’ concept in ALife, is
emergent behavior.” (p. 2 in [44]). Complex systems ex-
hibit patterns of emergence that are not predictable from
inspection of the individual elements. Emergence is de-
scribed as unexpected, unpredictable or otherwise surpris-
ing. That is, the modeled system exhibits behaviors that are
not explicitly built into the model. Unpredictability is due
to the non-linear eﬀects that result from the interactions of
entities having simple behaviors. Emergence by these def-
initions is something of a subjective process.
In biological systems, emergence is a central issue
whether it be the emergence of the phenotype from the
genotype, the emergence of protein complexes from ge-
nomic information networks [39], or the emergence of
consciousness from networks of millions of brain cells.
One of the motivations for agent-based modeling is to
explore the emergent behaviors exhibited by the simulated
system. In general, agent-based models often exhibit pat-
terns and relationships that emerge from agents interac-
tions. An example is the observed formation of groups of

120 A
Agent Based Modeling and Artificial Life
agents that collectively act in coherent and coordinated
patterns. Complex adaptive systems, widely investigated
by Holland in his agent-based model Echo [37], are often
structured in hierarchies of emergent structures. Emer-
gent structures can collectively form higher-order struc-
tures, using the lower-level structures as building blocks.
An emergent structure itself can take on new emergent
behaviors. These structures in turn aﬀect the agents from
which the structure has emerged in a process called down-
ward causation [32]. For example, in the real world people
organize and identify with groups, institutions, nations,
etc. They create norms, laws, and protocols that in turn
act on the individuals comprising the group.
From the perspective of agent-based modeling, emer-
gence has some interesting challenges for modeling:
 How does one operationally deﬁne emergence with re-
spect to agent-based modeling?
 How does one automatically identify and measure the
emergence of entities in a model?
 How do agents that comprise an emergent entity per-
ceived by an observer recognize that they are part of
that entity?
Artiﬁcial Chemistry
Artiﬁcial chemistry is a subﬁeld
of ALife. One of the original goals of Artiﬁcial chem-
istry was to understand how life could originate from
pre-biotic chemical processes. Artiﬁcial chemistry studies
self-organization in chemical reaction networks by sim-
ulating chemical reactions between artiﬁcial molecules.
Artiﬁcial chemistry speciﬁes well-understood chemical re-
actions and other information such as reaction rates, rel-
ative molecular concentrations, probabilities of reaction,
etc. These form a network of possibilities. The artiﬁcial
substances and the networks of chemical reactions that
emerge from the possibilities are studied through compu-
tation. Reactions are speciﬁed as recursive algebras and ac-
tivated as term replacement systems [30].
Hypercycles
The emergence of autocatalytic sets, or hy-
percycles, has been a prime focus of artiﬁcial chem-
istry [22]. A hypercycle is a self-contained system of
molecules and a self-replicating, and thereby self-sus-
taining, cyclic linkage of chemical reactions. Hypercycles
evolve through a process by which self-replicating entities
compete for selection.
The hypercycle model illustrates how an ALife process
can be adopted to the agent-based modeling domain. In-
spired by the hypercycle model, Padgett [54] developed an
agent-based model of the co-evolution of economic pro-
duction and economic ﬁrms, focusing on skills. Padgett
used the model to establish three principles of social or-
ganization that provide foundations for the evolution of
technological complexity:
 Structured topology (how interaction networks form)
 Altruistic learning (how cooperation and exchange
emerges), and
 Stigmergy (how agent communication is facilitated by
using the environment as a means of information ex-
change among agents).
Digital Organisms The widespread availability of per-
sonal computers spurred the development of ALife pro-
grams used to study evolutionary processes in silico. Tierra
was the ﬁrst system devised in which computer programs
were successfully able to evolve and adapt [59]. Avida ex-
tended Tierra to account for the spatial distribution of
organisms and other features [52,68]. Echo is a simula-
tion framework for implementing models to investigate
mechanisms that regulate diversity and information-pro-
cessing in complex adaptive systems (CAS), systems com-
prised of many interacting adaptive agents [36,37]. In im-
plementations of Echo, populations evolve interaction net-
works, resembling species communities in ecological sys-
tems, which regulate the ﬂow of resources.
Systems such as Tierra, Avida, and Echo simulate
populations of digital organisms, based on the geno-
type/phenotype schema. They employ computational al-
gorithms to mutate and evolve populations of organisms
living in a simulated computer environment. Organisms
are represented as strings of symbols, or agent attributes,
in computer memory. The environment provides them
with resources (computation time) they need to survive,
compete, and reproduce. Digital organisms interact in var-
ious ways and develop strategies to ensure survival in re-
source-limited environments.
Digital organisms are extended to agent-based mod-
eling by implementing individual-based models of food
webs in a system called DOVE [69]. Agent-based models
allow a more complete representation of agent behaviors
and their evolutionary adaptability at both the individual
and population levels.
ALife and Computing
Creating life-like forms through computation is central to
Artiﬁcial Life. Is it possible to create life through computa-
tion? The capabilities and limitations of computation con-
strain the types of artiﬁcial life that can be created. The
history of ALife has close ties with important events in the
history of computation.

Agent Based Modeling and Artificial Life
A
121
Alan Turing [65] investigated the limitations of com-
putation by developing an abstract and idealized com-
puter, called a Universal Turing Machine (UTM). A UTM
has an inﬁnite tape (memory) and is therefore an idealiza-
tion of any actual computer that may be realized. A UTM
is capable of computing anything that is computable, that
is, anything that can be derived via a logical, deductive se-
ries of statements. Are the algorithms used in today’s com-
puters, and in ALife calculations and agent-based models
in particular, as powerful as universal computers?
Any system that can eﬀectively simulate a small set of
logical operations (such as AND and NOT) can eﬀectively
produce any possible computation. Simple rule systems in
cellular automata were shown to be equivalent to universal
computers [67,72], and in principal able to compute any-
thing that is computable – perhaps, even life!
Some have argued that life, in particular human con-
sciousness, is not the result of a logical-deductive or algo-
rithmic process and therefore not computable by a Uni-
versal Turing Machine. This problem is more generally
referred to as the mind-body problem [48]. Dreyfus [20]
argues against the assumption often made in the ﬁeld of
artiﬁcial intelligence that human minds function like gen-
eral purpose symbol manipulation machines. Penrose [56]
argues that the rational processes of the human mind tran-
scend formal logic systems. In a somewhat diﬀerent view,
biological naturalism contends [63] that human behavior
might be able to be simulated, but human consciousness is
outside the bounds of computation.
Such philosophical debates are as relevant to agent-
based modeling as they are to artiﬁcial intelligence, for
they are the basis of answering the question of what kind of
systems and processes agent-based models will ultimately
be able, or unable, to simulate.
Artiﬁcial Life Algorithms
ALife use several biologically-inspired computational algo-
rithms [53]. Bioinspired algorithms include those based
on Darwinian evolution, such as evolutionary algorithms,
those based on neural structures, such as neural networks,
and those based on decentralized decision making be-
haviors observed in nature. These algorithms are com-
monly used to model adaptation and learning in agent-
based modeling or to optimize the behaviors of whole sys-
tems.
Evolutionary Computing
Evolutionary computing in-
cludes a family of related algorithms and programming
solution techniques inspired by evolutionary processes, es-
pecially the genetic processes of DNA replication and cell
division [21]. These techniques are known as evolutionary
algorithms and include the following [7]:
 Genetic algorithms [34,35,36,38,51]
 Evolution strategies [60]
 Learning classiﬁer systems [38]
 Genetic programming [40]
 Evolutionary programming [28]
Genetic algorithms (GA) model the dynamic processes by
which populations of individuals evolve to improved levels
of ﬁtness for their particular environment over repeated
generations. GAs illustrate how evolutionary algorithms
process a population and apply the genetic operations of
mutation and crossover (see Fig. 5). Each behavior is rep-
resented as a chromosome consisting of a series of sym-
bols, for example, as a series of 0s and 1s. The encod-
ing process establishing correspondence between behav-
iors and their chromosomal representations is part of the
modeling process.
The general steps in a genetic algorithm are as follows:
1. Initialization: Generate an initial population of individ-
uals. The individuals are unique and include speciﬁc
encoding of attributes in chromosomes that represents
the characteristics of the individuals.
2. Evaluation: Calculate the ﬁtness of all individuals ac-
cording to a speciﬁed ﬁtness function.
3. Checking: If any of the individuals has achieved an ac-
ceptable level of ﬁtness, stop, the problem is solved.
Otherwise, continue with selection.
4. Selection: Select the best pair of individuals in the pop-
ulation for reproduction according to their high ﬁtness
levels.
5. Crossover: Combine the chromosomes for the two best
individuals through a crossover operation and produce
a pair of oﬀspring.
6. Mutation: Randomly mutate the chromosomes for the
oﬀspring.
7. Replacement: Replace the least ﬁt individuals in the
population with the oﬀspring.
8. Continue at Step 2
Steps 5 and 6 above, the operations of crossover and muta-
tion, comprise the set of genetic operators inspired by na-
ture. This series of steps for a GA comprise a basic frame-
work rather than a speciﬁc implementation. Actual GA
implementations include numerous variations and alter-
native implementations in several of the GA steps.
Evolution strategies (ES) are similar to genetic algo-
rithms but rely on mutation as its primary genetic oper-
ator.

122 A
Agent Based Modeling and Artificial Life
Agent Based Modeling and Artificial Life, Figure 5
Genetic Algorithm
Learning classiﬁer systems (LCS) build on genetic algo-
rithms and adaptively assign relative weights to sensor-ac-
tion sets that result in the most positive outcomes relative
to a goal.
Genetic programming (GP) has similar features to ge-
netic algorithms, but instead of using 0s and 1s or other
symbols for comprising chromosomes, GPs combine log-
ical operations and directives in a tree structure. In eﬀect,
chromosomes in GPs represent whole computer programs
that perform a variety of functions with varying degrees
of success and eﬃciencies. GP chromosomes are evaluated
against ﬁtness or performance measures and recombined.
Better performing chromosomes are maintained and ex-
pand their representation in the population. For example,
an application of a GP is to evolve a better-performing rule
set that represents an agent’s behavior.
Evolutionary programming (EP) is a similar technique
to genetic programming, but relies on mutation as its pri-
mary genetic operator.
Biologically Inspired Computing
Artiﬁcial neural net-
works (ANN) are another type of commonly used bio-

Agent Based Modeling and Artificial Life
A
123
logically inspired algorithm [50]. An artiﬁcial neural net-
work uses mathematical models based on the structures
observed in neural systems. An artiﬁcial neuron contains
a stimulus-response model of neuron activation based on
thresholds of stimulation. In modeling terms, neural net-
works are equivalent to nonlinear, statistical data model-
ing techniques. Artiﬁcial neural networks can be used to
model complex relationships between inputs and outputs
and to ﬁnd patterns in data that are dynamically chang-
ing. An ANN is adaptive in that changes in its structure
are based on external or internal information that ﬂows
through the network. The adaptive capability makes ANN
an important technique in agent-based models.
Swarm intelligence refers to problem solving tech-
niques, usually applied to solving optimization problems
that are based on decentralized problem solving strategies
that have been observed in nature. These include:
 Ant colony optimization [19].
 Particle swarm optimization [16].
Swarm intelligence algorithms simulate the movement
and interactions of large numbers of ants or particles over
a search space. In terms of agent-based modeling, the ants
or particles are the agents, and the search space is the en-
vironment. Agents have position and state as attributes. In
the case of particle swarm optimization, agents also have
velocity.
Ant colony optimization (ACO) mimics techniques
that ants use to forage and ﬁnd food eﬃciently [13,24].
The general idea of ant colony optimization algorithms is
as follows:
1. In a typical ant colony, ants search randomly until one
of them ﬁnds food.
2. Then they return to their colony and lay down a chem-
ical pheromone trail along the way.
3. When other ants ﬁnd such a pheromone trail, they are
more likely to follow the trail rather than to continue to
search randomly.
4. As other ants ﬁnd the same food source, they return
to the nest, reinforcing the original pheromone trail as
they return.
5. As more and more ants ﬁnd the food source, the ants
eventually lay down a strong pheromone trail to the
point that virtually all the ants are directed to the food
source.
6. As the food source is depleted, fewer ants are able to
ﬁnd the food and fewer ants lay down a reinforcing
pheromone trail; the pheromone naturally evaporates
and eventually no ants proceed to the food source, as
the ants shift their attention to searching for new food
sources.
In an ant colony optimization computational model, the
optimization problem is represented as a graph, with
nodes representing places and links representing possible
paths. An ant colony algorithm mimics ant behavior with
simulated ants moving from node to node in the graph,
laying down pheromone trails, etc. The process by which
ants communicate indirectly by using the environment as
an intermediary is known as stigmergy [13], and is com-
monly used in agent-based modeling.
Particle swarm optimization (PSO) is another decen-
tralized problem solving technique in which a swarm of
particles is simulated as it moves over a search space in
search of a global optimum. A particle stores its best po-
sition found so far in its memory and is aware of the best
positions obtained by its neighboring particles. The veloc-
ity of each particle adapts over time based on the locations
of the best global and local solutions obtained so far, incor-
porating a degree of stochastic variation in the updating of
the particle positions at each iteration.
Artiﬁcial Life Algorithms and Agent-Based Model-
ing
Biologically-inspired algorithms are often used with
agent-based models. For example, an agent’s behavior and
its capacity to learn from experience or to adapt to chang-
ing conditions can be modeled abstractly through the use
of genetic algorithms or neural networks. In the case of
a GA, a chromosome eﬀectively represents a single agent
action (output) given a speciﬁc condition or environmen-
tal stimulus (input). Behaviors that are acted on and enable
the agent to respond better to environmental challenges
are reinforced and acquire a greater share of the chromo-
some pool. Behaviors that fail to improve the organism’s
ﬁtness diminish in their representation in the population.
Evolutionary programming can be used to directly
evolve programs that represent agent behaviors. For ex-
ample, Manson [49] develops a bounded rationality model
using evolutionary programming to solve an agent multi-
criteria optimization problem.
Artiﬁcial neural networks have also been applied to
modeling adaptive agent behaviors, in which an agent de-
rives a statistical relationship between the environmental
conditions it faces, its history, and its actions, based on
feedback on the success or failures of its actions and the ac-
tions of others. For example, an agent may need to develop
a strategy for bidding in a market, based on the success of
its own and other’s previous bids and outcomes.
Finally, swarm intelligence approaches are agent-
based in their basic structure, as described above. They can

124 A
Agent Based Modeling and Artificial Life
also be used for system optimization through the selection
of appropriate parameters for agent behaviors.
ALife Summary
Based on the previous discussion, the essential features of
an ALife program can be summarized as follows:
 Population: A population of organisms or individuals is
considered. The population may be diversiﬁed, and in-
dividuals may vary their characteristics, behaviors, and
accumulated resources, in both time and space.
 Interaction: Interaction requires sensing of the imme-
diate locale, or neighborhood, on the part of an individ-
ual. An organism can simply become “aware” of other
organisms in its vicinity or it may have a richer set of
interactions with them. The individual also interacts
with its (non-agent) environment in its immediate lo-
cale. This requirement introduces spatial aspects into
the problem, as organisms must negotiate the search
for resources through time and space.
 Sustainment and renewal: Sustainment and renewal re-
quires the acquisition of resources. An organism needs
to sense, ﬁnd, ingest, and metabolize resources, or
nourishment as an energy source for processing into
other forms of nutrients. Resources may be provided by
the environment, i. e., outside of the agents themselves,
or by other agents. The need for sustainment leads to
competition for resources among organisms. Competi-
tion could also be a precursor to cooperation and more
complex emergent social structures if this proves to be
a more eﬀective strategy for survival.
 Self-reproduction and replacement: Organisms repro-
duce by following instructions at least partially embed-
ded within themselves and interacting with the envi-
ronment and other agents. Passing on traits to the next
generation implies a requirement for trait transmis-
sion. Trait transmission requires encoding an organ-
ism’s traits in a reduced form, that is, a form that con-
tains less than the total information representing the
entire organism. It also requires a process for trans-
forming the organism’s traits into a viable set of pos-
sible new traits for a new organism. Mutation and
crossover operators enter into such a process. Organ-
isms also leave the population and are replaced by
other organisms, possibly with diﬀerent traits. The or-
ganisms can be transformed through changes in their
attributes and behaviors, as in, for example, learning
or aging. The populations of organisms can be trans-
formed through the introduction of new organisms and
replacement, as in evolutionary adaptation.
As we will see in the section that follows, many of the es-
sential aspects of ALife have been incorporated into the
development of agent-based models.
Alife in Agent-Based Modeling
This section brieﬂy touches on the ways in which ALife has
motivated agent-based modeling. The form of agent-based
models, in terms of their structure and appearance, is di-
rectly based on early models from the ﬁeld of ALife. Sev-
eral application disciplines in agent-based modeling have
been spawned and infused by ALife concepts. Two are cov-
ered here. These are how agent-based modeling is applied
to social and biological systems.
Agent-Based Modeling Topologies
Agent-based modeling owes much to artiﬁcial life in both
form and substance. Modeling a population of heteroge-
neous agents with a diverse set of characteristics is a hall-
mark of agent-based modeling. The agent perspective is
unique among simulation approaches, unlike the process
perspective or the state-variable approach taken by other
simulation approaches.
As we have seen, agents interact with a small set of
neighbor agents in a local area. Agent neighborhoods are
deﬁned by how agents are connected, the agent interac-
tion topology. Cellular automata represent agent neigh-
borhoods by using a grid in which the agents exist in the
cells, one agent per cell, or as the nodes of the lattice of
the grid. The cells immediatelysurrounding an agent com-
prise the agent’s neighborhood and the agents that reside
in the neighborhood cells comprise the neighbors. Many
agent-based models have been based on this cellular au-
tomata spatial representation. The transition from a cellu-
lar automata, such as the game of Life, to an agent-based
model is accomplished by allowing agents to be distinct
from the cells on which they reside and allowing the agents
to move from cell to cell across the grid. Agents move ac-
cording to the dictates of their behaviors, interacting with
other agents that happen to be in their local neighbor-
hoods along the way.
Agent interaction topologies have been extended be-
yond cellular automata to include networks, either pre-
deﬁned and static as in the case of autocatalytic chemi-
cal networks, or endogenous and dynamic, according to
the results of agent interactions that occur in the model.
Networks allow an agent’s neighborhood to be deﬁned
more generally and ﬂexibly, and in the case of social
agents, more accurately describe social agents’ interac-
tion patterns. In addition to cellular automata grids and
networks, agent interaction topologies have also been ex-

Agent Based Modeling and Artificial Life
A
125
tended across a variety of domains. In summary, agent in-
teraction topologies include:
 Cellular automata grids (agents are cells or are within
in cells) or lattices (agents are grid points),
 Networks, in which agents of vertices and agent rela-
tionships are edges,
 Continuous space, in one, two or three dimensions;
 Aspatial random interactions, in which pairs of agents
are randomly selected; and
 Geographical Information Systems (GIS), in which
agents move over geographically-deﬁned patches, re-
laxing the one-agent per cell restriction.
Social Agent-Based Modeling
Early social agent-based models were based on ALife’s cel-
lular automata approach. In applications of agent-based
modeling to social processes, agents represent people or
groups of people, and agent relationships represent pro-
cesses of social interaction [33].
Social Agents
Sakoda [61] formulated one of the ﬁrst so-
cial agent-based models, the Checkerboard Model, which
had some of the key features of a cellular automaton. Fol-
lowing a similar approach, Schelling developed a model
Agent Based Modeling and Artificial Life, Figure 6
Schelling Housing Segregation Model
of housing segregation in which agents represent home-
owners and neighbors, and agent interactions represent
agents’ perceptions of their neighbors [62]. Schelling stud-
ied housing segregation patterns and posed the question
of whether it is it possible to get highly segregated settle-
ment patterns even if most individuals are, in fact, “color-
blind?” The Schelling model demonstrated that segregated
housing areas can develop spontaneously in the sense that
system-level patterns can emerge that are not necessarily
implied by or consistent with the objectives of the individ-
ual agents (Fig. 6). In the model, agents operated according
to a ﬁxed set of rules and were not adaptive.
Identifying the social interaction mechanisms for how
cooperative behavior emerges among individuals and
groups has been addressed using agent-based modeling
and evolutionary game theory. Evolutionary game theory
accounts for how the repeated interactions of players in
a game-theoretic framework aﬀect the development and
evolution of the players’ strategies. Axelrod showed, using
a cellular automata approach, in which agents on the grid
employed a variety of diﬀerent strategies, that a simple Tit-
For-Tat strategy of reciprocal behavior toward individu-
als is enough to establish sustainable cooperative behav-
ior [4,5]. In addition, Axelrod investigated strategies that
were self-sustaining and robust in that they reduced the
possibility of invasion by agents having other strategies.

126 A
Agent Based Modeling and Artificial Life
Agent Based Modeling and Artificial Life, Figure 7
Sugarscape Artificial Society Simulation in the Repast Agent-Based Modeling Toolkit
Epstein and Axtell introduced the notion of an exter-
nal environment that agents interact with in addition to
other agents. In their groundbreaking Sugarscape model
of artiﬁcial societies, agents interacted with their environ-
ment depending on their location in the grid [26]. This
allowed agents to access environmental variables, extract
resources, etc., based on location. In numerous computa-
tional experiments, Sugarscape agents emerged with a va-
riety of characteristics and behaviors, highly suggestive
of a realistic, although rudimentary and abstract, society
(Fig. 7). They observed emergent processes that they in-
terpreted as death, disease, trade, wealth, sex and repro-
duction, culture, conﬂict and war, as well as externalities
such as pollution. As agents interacted with their neigh-
bors as they moved around the grid, the interactions re-
sulted in a contact network, that is, a network consisting
of nodes and links. The nodes are agents and the links in-
dicate the agents that have been neighbors at some point in
the course of their movements over the grid. Contact net-
works were the basis for studying contagion and epidemics
in the Sugarscape model. Understanding the agent rules
that govern how networks are structured and grow, how
quickly information is communicated through networks,
and the kinds of relationships that networks embody are
important aspects of modeling agents.
Culture and Generative Social Science
Dawkins, who
has written extensively on aspects of Darwinian evolution,
coined the term meme as the smallest element of culture
that is transmissible between individuals, similar to the no-
tion of the gene as being the primary unit of transmitting
genetic information [17]. Several social agent-based mod-
els are based on a meme representation of culture as shared
or collective agent attributes.
In the broadest terms, social agent-based simulation
is concerned with social interaction and social processes.
Emergence enters into social simulation through genera-
tive social science whose goal is to model social processes
as emergent processes and their emergence as the result of
social interactions. Epstein has argued that social processes
are not fully understood unless one is able to theorize how
they work at a deep level and have social processes emerge
as part of a computational model [25]. More recent work
has treated culture as a ﬂuid and dynamic process subject

Agent Based Modeling and Artificial Life
A
127
to interpretation of individual agents, more complex than
the genotype/phenotype framework would suggest.
ALife and Biology
ALife research has motivated many agent-based computa-
tional models of biological systems, and at all scales, rang-
ing from the cellular level, or even the subcellular molecu-
lar level, as the basic unit of agency, to complex organisms
embedded in larger structures such as food webs or com-
plex ecosystems.
From Cellular Automata to Cells
Cellular automata are
a natural application to modeling cellular systems [2,27].
One approach uses the cellular automata grid and cells
to model structures of stationary cells comprising a tissue
matrix. Each cell is a tissue agent. Mobile cells consisting
of pathogens and antibodies are also modeled as agents.
Mobile agents diﬀuse through tissue and interact with tis-
sue and other co-located mobile cells. This approach is
the basis for agent-based models of the immune system.
Celada and Seiden [15] used bit-strings to model the cell
receptors in a cellular automaton model of the immune
system called IMMSIM. This approach was extended to
a more general agent-based model in and implemented to
maximize the number of cells that could be modeled in
the CIMMSIM and ParImm systems [9]. The Basic Im-
mune Simulator uses a general agent-based framework
Agent Based Modeling and Artificial Life, Figure 8
AgentCell Multi-Scale Agent-Based Model of Bacterial Chemotaxis
(the Repast agent-based modeling toolkit) to model the in-
teractions between the cells of the innate and adaptive im-
mune system [29]. These approaches for modeling the im-
mune system have inspired several agent-based models of
intrusion detection for computer networks (see for exam-
ple, [6]), and have found use in modeling the development
and spread of cancer [58].
At the more macroscopic level, agent-based epidemic
models have been developed using network topologies.
These models include people and some representation of
pathogens as individual agents for natural [11] and poten-
tially man-made [14] epidemics.
Modeling bacteria and their associated behaviors in
their natural environments is another direction of agent-
based modeling. Expanding beyond the basic cellular au-
tomata structure into continuous space and network to-
pologies, Emonet, et al. [23] developed AgentCell, a mul-
ti-scale agent-based model of E. Coli bacteria motil-
ity (Fig. 8). In this multi-scale agent-based simulation,
molecules within a cell are modeled as individual agents.
The molecular reactions comprising the signal transduc-
tion network for chemotaxis are modeled using an em-
bedded stochastic simulator, StochSim [46]. This multi-
scale approach allows the motile (macroscopic) behavior
of colonies of bacteria to be modeled as a direct result of
the modeled micro-level processes of protein production
within the cells, which are based on individual molecular
interactions.

128 A
Agent Based Modeling and Artificial Life
Artiﬁcial Ecologies
Early models of ecosystems used ap-
proaches adapted from physical modeling, especially mod-
els of idealized gases based on statistical mechanics. More
recently, individual-based models have been developed to
represent the full range of individual diversity by explic-
itly modeling individual attributes or behaviors, and ag-
gregating across individuals for an entire population [18].
Agent-based approaches model a diverse set of agents and
their interactions based on their relationships, incorporat-
ing adaptive behaviors as appropriate. For example, food
webs represent the complex, hierarchical network of agent
relationships in local ecosystems [55]. Agents are individ-
uals or species representatives. Adaptation and learning
for agents in such food webs can be modeled to explore
diversity, relative population sizes, and resiliency to envi-
ronmental insult.
Adaptation and Learning in Agent-Based Models
Bi-
ologists consider adaptation to be an essential part of the
process of evolutionary change. Adaptation occurs at two
levels: the individual level and the population level. In par-
allel with these notions, agents in an ABM adapt by chang-
ing their individual behaviors or by changing their propor-
tional representation in the population. Agents adapt their
behaviors at the individual level through learning from ex-
perience in their modeled environment.
With respect to agent-based modeling, theories of
learning by individual agents or collectives of agents, as
well as algorithms for how to model learning, become
important. Machine learning is a ﬁeld consisting of al-
gorithms for recognizing patterns in data (such as data
mining) through techniques such as supervised learning,
unsupervised learning and reinforcement learning [3,10].
Genetic algorithms [34] and related techniques such as
learning classiﬁer systems [38] are commonly used to rep-
resent agent learning in agent-based models. In ABM ap-
plications, agents learn through interactions with the sim-
ulated environment in which they are embedded as the
simulation precedes through time, and agents modify their
behaviors accordingly.
Agents may also adapt collectively at the population
level. Those agents having behavioral rules better suited to
their environments survive and thrive, and those agents
not so well suited are gradually eliminated from the popu-
lation.
Future Directions
Agent-based modeling continues to be inspired by ALife –
in the fundamental questions it is trying to answer, in the
algorithms that it employs to model agent behaviors and
solve agent-based models, and in the computational ar-
chitectures that are employed to implement agent-based
models. The future of the ﬁelds of both ALife and ABM
will continue to be intertwined in essential ways in the
coming years.
Computational advances will continue at an ever-in-
creasing pace, opening new vistas for computational pos-
sibilities in terms of expanding the scale of models that are
possible. Computational advances will take several forms,
including advances in computer hardware including new
chip designs, multi-core processors, and advanced inte-
grated hardware architectures. Software that take advan-
tage of these designs and in particular computational al-
gorithms and modeling techniques and approaches will
continue to provide opportunities for advancing the scale
of applications and allow more features to be included in
agent-based models as well as ALife applications. These
will be opportunities for advancing applications of ABM
to ALife in both the realms of scientiﬁc research and in
policy analysis.
Real-world optimization problems routinely solved by
business and industry will continue to be solved by ALife-
inspired algorithms. The use of ALife-inspired agent-
based algorithms for solving optimization problems will
become more widespread because of their natural imple-
mentation and ability to handle ill-deﬁned problems.
Emergence is a key theme of ALife. ABM oﬀers the ca-
pability to model the emergence of order in a variety of
complex and complex adaptive systems. Inspired by ALife,
identifying the fundamental mechanisms responsible for
higher order emergence and exploring these with agent-
based modeling will be an important and promising re-
search area.
Advancing social sciences beyond the genotype/
phenotype framework to address the generative nature of
social systems in their full complexity is a requirement
for advancing computational social models. Recent work
has treated culture as a ﬂuid and dynamic process subject
to interpretation of individual agents, more complex in
many ways than that provided by the genotype/phenotype
framework.
Agent-based modeling will continue to be the avenue
for exploring new constructs in ALife. If true artiﬁcial life
is ever developed in silico, it will most likely be done using
the methods and tools of agent-based modeling.
Bibliography
Primary Literature
1. Adami C (1998) Introduction to Artificial Life. TELOS, Santa
Clara

Agent Based Modeling and Artificial Life
A
129
2. Alber MS, Kiskowski MA, Glazier JA, Jiang Y (2003) On Cellu-
lar Automaton Approaches to Modeling Biological Cells. In:
Rosenthal J, Gilliam DS (eds) Mathematical Systems Theory in
Biology, Communication, and Finance, IMA Volume. Springer,
New York, pp 1–39
3. Alpaydın E (2004) Introduction to Machine Learning. MIT Press,
Cambridge
4. Axelrod R (1984) The Evolution of Cooperation. Basic Books,
New York
5. Axelrod R (1997) The Complexity of Cooperation: Agent-Based
Models of Competition and Collaboration. Princeton Univer-
sity Press, Princeton
6. Azzedine B, Renato BM, Kathia RLJ, Joao Bosco MS, Mirela
SMAN (2007) An Agent Based and Biological Inspired Real-
Time Intrusion Detection and Security Model for Computer
Network Operations. Comp Commun 30(13):2649–2660
7. Back T (1996) Evolutionary Algorithms in Theory and Practice:
Evolution Strategies, Evolutionary Programming, Genetic Al-
gorithms. Oxford University Press, New York
8. Berlekamp ER, Conway JH, Guy RK (2003) Winning Ways for
Your Mathematical Plays, 2nd edn. AK Peters, Natick
9. Bernaschi M, Castiglione F (2001) Design and Implementation
of an Immune System Simulator, Computers in Biology and
Medicine 31(5):303–331
10. Bishop CM (2007) Pattern Recognition and Machine Learning.
Springer, New York
11. Bobashev GV, Goedecke DM, Yu F, Epstein JM (2007) A Hybrid
Epidemic Model: Combining the Advantages of Agent-Based
and Equation-Based Approaches. In: Henderson SG, Biller B,
Hsieh M-H, Shortle J, Tew JD, Barton RR (eds) Proc. 2007 Winter
Simulation Conference, Washington, pp 1532–1537
12. Bonabeau E (1997) From Classical Models of Morphogenesis to
Agent-Based Models of Pattern Formation. Artif Life 3:191–211
13. Bonabeau E, Dorigo M, Theraulaz G (1999) Swarm Intelligence:
From Natural to Artificial Systems. Oxford University Press,
New York
14. Carley KM, Fridsma DB, Casman E, Yahja A, Altman N, Chen
LC, Kaminsky B, Nave D (2006) Biowar: Scalable Agent-Based
Model of Bioattacks. IEEE Trans Syst Man Cybern Part A: Syst
Hum 36(2):252–265
15. Celada F, Seiden PE (1992) A Computer Model of Cellular Inter-
actions in the Immune System. Immunol Today 13(2):56–62
16. Clerc M (2006) Particle Swarm Optimization. ISTE Publishing
Company, London
17. Dawkins R (1989) The Selfish Gene, 2nd edn. Oxford University
Press, Oxford
18. DeAngelis DL, Gross LJ (eds) (1992) Individual-Based Mod-
els and Approaches in Ecology: Populations, Communities
and Ecosystems. Proceedings of a Symposium/Workshop,
Knoxville, 16–19 May 1990. Chapman & Hall, New York. ISBN
0-412-03171-X
19. Dorigo M, Stützle T (2004) Ant Colony Optimization. MIT Press,
Cambridge
20. Dreyfus HL (1979) What Computers Can’t Do: The Limits of Ar-
tificial Intelligence. Harper & Row, New York
21. Eiben AE, Smith JE (2007) Introduction to Evolutionary Com-
puting, 2nd edn. Springer, New York
22. Eigen M, Schuster P (1979) The Hypercycle: A Principle of Nat-
ural Self-Organization. Springer, Berlin
23. Emonet T, Macal CM, North MJ, Wickersham CE, Cluzel P (2005)
AgentCell: A Digital Single-Cell Assay for Bacterial Chemotaxis.
Bioinformatics 21(11):2714–2721
24. Engelbrecht AP (2006) Fundamentals of Computational
Swarm Intelligence. Wiley, Hoboken
25. Epstein JM (2007) Generative Social Science: Studies in Agent-
Based Computational Modeling. Princeton University Press,
Princeton
26. Epstein JM, Axtell R (1996) Growing Artificial Societies: Social
Science from the Bottom Up. MIT Press, Cambridge
27. Ermentrout GB, Edelstein-Keshet L (1993) Cellular Automata
Approaches to Biological Modeling. J Theor Biol 160(1):97–133
28. Fogel LJ, Owens AJ, Walsh MJ (1966) Artificial Intelligence
through Simulated Evolution. Wiley, Hoboken
29. Folcik VA, An GC, Orosz CG (2007) The Basic Immune Simula-
tor: An Agent-Based Model to Study the Interactions between
Innate and Adaptive Immunity. Theor Biol Med Model 4(39):1–
18. http://www.tbiomed.com/content/4/1/39
30. Fontana W (1992) Algorithmic Chemistry. In: Langton CG, Tay-
lor C, Farmer JD, Rasmussen S (eds) Artificial Life II: Proceed-
ings of the Workshop on Artificial Life, Santa Fe, February 1990,
Santa Fe Institute Studies in the Sciences of the Complexity,
vol X. Addison-Wesley, Reading, pp 159–209
31. Gardner M (1970) The Fantastic Combinations of John Con-
way’s New Solitaire Game Life. Sci Am 223:120–123
32. Gilbert N (2002) Varieties of Emergence. In: Macal C, Sallach
D (eds) Proceedings of the Agent 2002 Conference on Social
Agents: Ecology, Exchange and Evolution, Chicago, 11–12 Oct
2002, pp 1–11. Availableon CD and at www.agent2007.anl.gov
33. Gilbert N, Troitzsch KG (1999) Simulation for the Social Scien-
tist. Open University Press, Buckingham
34. Goldberg DE (1989) Genetic Algorithms in Search, Optimiza-
tion, and Machine Learning. Addison-Wesley, Reading
35. Goldberg DE (1994) Genetic and Evolutionary Algorithms
Come of Age. Commun ACM 37(3):113–119
36. Holland JH (1975) Adaptation in Natural and Artificial Systems.
University of Michigan, Ann Arbor
37. Holland J (1995) Hidden Order: How Adaptation Builds Com-
plexity. Addison-Wesley, Reading
38. Holland JH, Booker LB, Colombetti M, Dorigo M, Goldberg DE,
Forrest S, Riolo RL, Smith RE, Lanzi PL, Stolzmann W, Wilson SW
(2000) What Is a Learning Classifier System? In: Lanzi PL, Stolz-
mann W, Wilson SW (eds) Learning Classifier Systems, from
Foundations to Applications. Springer, London, pp 3–32
39. Kauffman SA (1993) The Origins of Order: Self-Organization
and Selection in Evolution. Oxford University Press, Oxford
40. Koza JR (1992) Genetic Programming: On the Programming
of Computers by Means of Natural Selection. MIT Press, Cam-
bridge. 840 pp
41. Langton CG (1984) Self-Reproduction in Cellular Automata.
Physica D 10:135–144
42. Langton CG (1986) Studying Artificial Life with Cellular Au-
tomata. Physica D 22:120–149
43. Langton CG (1989) Preface. In: Langton CG (ed) Artificial Life:
Proceedings of an Interdisciplinary Workshop on the Synthe-
sis and Simulation of Living Systems. Los Alamos, Sept 1987,
Addison-Wesley, Reading, pp xv-xxvi
44. Langton CG (1989) Artificial Life. In: Langton CG (ed) Artificial
Life: The Proceedings of an Interdisciplinary Workshop on the
Synthesis and Simulation of Living Systems. Los Alamos, Sept
1987, Santa Fe Institute Studies in the Sciences of Complexity,
vol VI. Addison-Wesley, Reading, pp 1–47

130 A
Agent Based Modeling and Artificial Life
45. Langton CG (1992) Life at the Edge of Chaos. In: Langton CG,
Taylor C, Farmer JD, Rasmussen S (eds) Artificial Life II: Pro-
ceedings of the Workshop on Artificial Life. Santa Fe, Feb 1990,
Santa Fe Institute Studies in the Sciences of the Complexity,
vol X. Addison-Wesley, Reading, pp 41–91
46. Le Novere N, Shimizu TS (2001) Stochsim:Modelling of
Stochastic Biomolecular Processes. Bioinformatics 17(6):575–
576
47. Lindenmeyer A (1968) Mathematical Models for Cellular Inter-
action in Development. J Theor Biol 18:280–315
48. Lucas JR (1961) Minds, Machines and Godel. Philosophy
36(137):112–127
49. Manson SM (2006) Bounded Rationality in Agent-Based Mod-
els: Experimentswith EvolutionaryPrograms. Intern J Geogr Inf
Sci 20(9):991–1012
50. Mehrotra K, Mohan CK, Ranka S (1996) Elements of Artificial
Neural Networks. MIT Press, Cambridge
51. Mitchell M, Forrest S (1994) Genetic Algorithms and Artificial
Life. Artif Life 1(3):267–289
52. Ofria C, Wilke CO (2004) Avida: A Software Platform for
Research in Computational Evolutionary Biology. Artif Life
10(2):191–229
53. Olariu S, Zomaya AY (eds) (2006) Handbook of Bioinspired Al-
gorithms and Applications. Chapman, Boca Raton, pp 679
54. Padgett JF, Lee D, Collier N (2003) Economic Production as
Chemistry. Ind Corp Chang 12(4):843–877
55. Peacor SD, Riolo RL, Pascual M (2006) Plasticity and Species
Coexistence: Modeling Food Webs as Complex Adaptive Sys-
tems. In: Pascual M, Dunne JA (eds) Ecological Networks: Link-
ing Structure to Dynamics in Food Webs. Oxford University
Press, New York, pp 245–270
56. Penrose R (1989) The Emperor’s New Mind: Concerning Com-
puters, Minds, and the Laws of Physics. Oxford University Press,
Oxford
57. Poundstone W (1985) The Recursive Universe. Contemporary
Books, Chicago. 252 pp
58. Preziosi L (ed) (2003) Cancer Modelling and Simulation. Chap-
man, Boca Raton
59. Ray TS (1991) An Approach to the Synthesis of Life (Tierra Sim-
ulator). In: Langton CG, Taylor C, Farmer JD, Rasmussen S (eds)
Artificial Life Ii: Proceedings of the Workshop on Artificial Life.
Wesley, Redwood City, pp 371–408
60. Rechenberg I (1973) Evolutionsstrategie: Optimierung Tech-
nischer Systeme Nach Prinzipien Der Biologischen Evolution.
Frommann-Holzboog, Stuttgart
61. Sakoda JM (1971) The Checkerboard Model of Social Interac-
tion. J Math Soc 1:119–132
62. Schelling TC (1971) Dynamic Models of Segregation. J Math
Soc 1:143–186
63. Searle JR (1990) Is the Brain a Digital Computer? Presidential
Address to the American Philosophical Association
64. Taub AH (ed) (1961) John Von Neumann: Collected Works.
vol V: Design of Computers, Theory of Automata and Numer-
ical Analysis (Delivered at the Hixon Symposium, Pasadena,
Sept 1948). Pergamon Press, Oxford
65. Turing AM (1938) On Computable Numbers with an Appli-
cation to the Entscheidungsproblem. Process Lond Math Soc
2(42):230–265
66. Turing AM (1952) The Chemical Basis of Morphogenesis. Philos
Trans Royal Soc B 237:37–72
67. von Neumann J (1966) Theory of Self-Reproducing Automata,
edited by Burks AW. University of Illinois Press, Champaign
68. Wilke CO, Adami C (2002) The Biology of Digital Organisms.
Trends Ecol Evol 17(11):528–532
69. Wilke CO, Chow SS (2006) Exploring the Evolution of Ecosys-
tems with Digital Organisms. In: Pascual M, Dunne JA (eds) Eco-
logical Networks: Linking Structure to Dynamics in Food Webs.
Oxford University Press, New York, pp 271–286
70. Wolfram S (1984) Universality and Complexity in Cellular Au-
tomata. Physica D:1–35
71. Wolfram S (1999) The Mathematica Book, 4th edn. Wolfram
Media/Cambridge University Press, Champaign
72. Wolfram S (2002) A New Kind of Science. Wolfram Media,
Champaign
Books and Reviews
Artificial
Life
(journal)
web
page
(2008)
http://www.
mitpressjournals.org/loi/artl. Accessed 8 March 2008
Banks ER (1971) Information Processing and Transmission in Cel-
lular Automata. Ph. D. dissertation, Massachusetts Institute of
Technology
Batty M (2007) Cities and Complexity: Understanding Cities with
Cellular Automata, Agent-Based Models, and Fractals. MIT
Press, Cambridge
Bedau MA (2002) The Scientific and Philosophical Scope of Artificial
Life, Leonardo 35:395–400
Bedau MA (2003) Artificial Life: Organization, Adaptation and Com-
plexity from the Bottom Up. TRENDS Cognit Sci 7(11):505–512
Copeland BJ (2004) The Essential Turing. Oxford University Press,
Oxford. 613 pp
Ganguly N, Sikdar BK, Deutsch A, Canright G, Chaudhuri PP
(2008) A Survey on Cellular Automata. www.cs.unibo.it/bison/
publications/CAsurvey.pdf
Griffeath D, Moore C (eds) (2003) New Constructions in Cellular Au-
tomata. Santa Fe Institute Studies in the Sciences of Complex-
ity Proceedings. Oxford University Press, New York, 360 pp
Gutowitz H (ed) (1991) Cellular Automata: Theory and Experiment.
Special Issue of Physica D 499 pp
Hraber T, Jones PT, Forrest S (1997) The Ecology of Echo. Artif Life
3:165–190
International Society for Artificial Life web page (2008) www.alife.
org. Accessed 8 March 2008
Jacob C (2001) Illustrating Evolutionary Computation with Mathe-
matica. Academic Press, San Diego. 578 pp
Michael CF, Fred WG, Jay A (2005) Simulation Optimization: A Re-
view, New Developments, and Applications. In: Proceedings of
the 37th conference on Winter simulation, Orlando
Miller JH, Page SE (2007) Complex Adaptive Systems: An Introduc-
tion to Computational Models of Social Life. Princeton Univer-
sity Press, Princeton
North MJ, Macal CM (2007) Managing Business Complexity: Discov-
ering Strategic Solutions with Agent-Based Modeling and Sim-
ulation. Oxford University Press, New York
Pascual M, Dunne JA (eds) (2006) Ecological Networks: Linking
Structure to Dynamics in Food Webs. Santa Fe Institute Stud-
ies in the Sciences of the Complexity. Oxford University Press,
New York
Simon H (2001) The Sciences of the Artificial. MIT Press, Cambridge
Sims K (1991) Artificial Evolution for Computer Graphics. ACM SIG-
GRAPH ’91, Las Vegas, July 1991, pp 319–328

Agent Based Modeling and Computer Languages
A
131
Sims K (1994) Evolving 3D Morphology and Behavior by Competi-
tion. Artificial Life IV, pp 28–39
Terzopoulos D (1999) Artificial Life for Computer Graphics. Com-
mun ACM 42(8):33–42
Toffoli T, Margolus N (1987) Cellular Automata Machines: A New
Environment for Modeling. MIT Press, Cambridge, 200 pp
Tu X, Terzopoulos D, Artificial Fishes: Physics, Locomotion, Per-
ception, Behavior. In: Proceedings of SIGGRAPH‘94, 24–29 July
1994, Orlando, pp 43–50
Weisbuch G (1991) Complex Systems Dynamics: An Introduction to
Automata Networks, translated from French by Ryckebusch S.
Addison-Wesley, Redwood City
Wiener N (1948) Cybernetics, or Control and Communication in the
Animal and the Machine. Wiley, New York
Wooldridge M (2000) Reasoning About Rational Agents. MIT Press,
Cambridge
Agent Based Modeling
and Computer Languages
MICHAEL J. NORTH, CHARLES M. MACAL
Argonne National Laboratory, Decision and Information
Sciences Division, Center for Complex Adaptive Agent
Systems Simulation (CAS2), Argonne, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Types of Computer Languages
Requirements of Computer Languages
for Agent-Based Modeling
Example Computer Languages Useful
for Agent-Based Modeling
Future Directions
Bibliography
Glossary
Agent An agent is a self-directed component in an agent-
based model.
Agent-based model An agent-based model is a simula-
tion made up of a set of agents and an agent interaction
environment.
Annotations Annotations are a Java feature for including
metadata in compiled code.
Attributes Attributes are a C# feature for including meta-
data in compiled code.
Aspects Aspects are a way to implement dispersed but re-
current tasks in one location.
Bytecode Bytecode is compiled Java binary code.
Common intermediate language Common
Intermedi-
ate Language (CIL) is compiled binary code for the
Microsoft .NET Framework. CIL was originally called
Microsoft Intermediate Language (MSIL).
Computational algebra systems Computational algebra
systems (CAS) are computational mathematics sys-
tems that calculate using symbolic expressions.
Computational mathematics systems Computational
Mathematics Systems (CMS) are software programs
that allow users to apply powerful mathematical algo-
rithms to solve problems through a convenient and
interactive user interface. CMS typically supply a wide
range of built-in functions and algorithms.
Class A class is the object-oriented inheritable binding of
procedures and data that provides the basis for creat-
ing objects.
Computer language A computer language is a method of
noting directives for computers. Computer program-
ming languages, or more simply programming lan-
guages, are an important category of computer lan-
guages.
Computer programming language Please see glossary
entry for “Programming language”.
C# C# (Archer [1]) is an object-oriented programming
language that was developed and is maintained by Mi-
crosoft. C# is one of many languages that can be used
to generate Microsoft .NET Framework code. This
code is run using a ‘virtual machine’ that potentially
gives it a consistent execution environment on diﬀer-
ent computer platforms.
C++ C++ is a widely used object-oriented program-
ming language that was created by Bjarne Stroustrup
(Stroustrup [39]) at AT&T. C++ is widely noted for
both its object-oriented structure and its ability to be
easily compiled into native machine code.
Design pattern Design patterns form a “common vocab-
ulary” describing tried and true solutions for com-
monly faced software design problems (Coplien [6]).
Domain-speciﬁc language Domain-speciﬁc
languages
(DSL’s) are computer languages that are highly cus-
tomized to support a well deﬁned application area or
‘domain’. DSL’s commonly include a substantial num-
ber of keywords that are nouns and verbs in the area of
application as well as overall structures and execution
patterns that correspond closely with the application
area.
Declarative language According to Watson [47] a “de-
clarative language (or non-procedural language) in-
volves the speciﬁcation of a set of rules deﬁning the so-
lution to the problem; it is then up to the computer to
determine how to reach a solution consistent with the
given rules”.

132 A
Agent Based Modeling and Computer Languages
Dynamic method invocation Dynamic method invoca-
tion, combined with reﬂection, is a Java and C# ap-
proach to higher-order programming.
Encapsulation Encapsulation is the containment of de-
tails inside a module.
Field A ﬁeld is a piece of object-oriented data.
Functional language According to Watson [47] “in func-
tional languages (sometimes called applicative lan-
guages) the fundamental operation is function appli-
cation”.
Function pointers Function pointers are part of C++’s
approach to higher-order programming. Runtime
Type Identiﬁcation is another component of this ap-
proach.
Generics Generics are a Java and C# feature for general-
izing classes.
Goto statement A goto statement is an unconditional
jump in a code execution ﬂow.
Headless A headless program executes without the use
of a video monitor. This is generally done to rapidly
execute models while logging results to ﬁles or
databases.
Higher-order programming
According to Reynolds [35], higher-order program-
ming involves the use of “procedures or labels ... as
data” such that they “can be used as arguments to
procedures, as results of functions, or as values of
assignable variables”.
Imperative language According to Watson [47] in im-
perative languages “there is a fundamental underly-
ing dependence on the assignment operation and on
variables implemented as computer memory locations,
whose contents can be read and altered”.
Inheritance Inheritance is the ability of an object-ori-
ented class to assume the methods and data of another
class called the parent class.
Java Java (Foxwell [12]) is a widely used object-oriented
programming language that was developed and is
maintained by Sun Microsystems. Java is known for its
widespread cross-platform availability on many diﬀer-
ent types of hardware and operating systems. This ca-
pability comes from Java’s use of a ‘virtual machine’
that allows code to have a consistent execution envi-
ronment on many diﬀerent computer platforms.
Logic programming language
According to Watson [47] “in a logic programming
language, the programmer needs only to supply the
problem speciﬁcation in some formal form, as it is the
responsibility of the language system to infer a method
of solution”.
Macro language Macro languages are simple domain-
speciﬁc languages that are used to write script for tools
such as spreadsheets.
Mathematica Mathematica is a commercial software pro-
gram for computational mathematics. Information on
Mathematica can be found at http://www.wolfram.
com/.
Method A method is an object-oriented procedure.
Methodological individualism A reductionist approach
to social science originally developed by Max Weber
that focuses on the interaction of well deﬁned and
separate individuals (Heath [20]). Alternatives theo-
ries usually focus on more holistic views of interaction
(Heath [20]).
MATLAB The MATrix Laboratory (MATLAB) is a com-
mercial software program for computational math-
ematics. Information on MATLAB can be found at
http://www.mathworks.com/.
Mobile agents Mobile agents are light-weight software
proxies that roam the world-wide web and perform
various functions programmed by their owners such
as gathering information from web sites.
Module According to Stevens et al. [38], “the term mod-
ule is used to refer to a set of one or more contigu-
ous program statements having a name by which other
parts of the system can invoke it and preferably having
its own distinct set of variable names”.
NetLogo NetLogo (Wilensky [48]) is an agent-based
modeling and simulation platform that uses a domain-
speciﬁc language to deﬁne models. NetLogo mod-
els are built using a metaphor of turtles as agents
and patches as environmental components (Wilen-
sky [48]). NetLogo itself is Java-based. NetLogo is free
for use in education and research. More information
on NetLogo and downloads can be found at http://ccl.
northwestern.edu/netlogo/.
Non-procedural language Please see glossary entry for
“Declarative language”.
Object An object is the instantiation of a class to produce
executable instances.
Object-oriented language Object-oriented languages are
structured languages that have special features for
binding data with procedures; inheritance; encapsu-
lation; and polymorphism. Careful abstraction that
omits unimportant details is an important design prin-
ciple associated with the use of object-oriented lan-
guages.
Objective-C Objective-C is an object-oriented language
that extends the C language.
Observer The observer is a NetLogo agent that has a view
of an entire model. There is exactly one observer in ev-
ery NetLogo model.

Agent Based Modeling and Computer Languages
A
133
Patch A patch is a NetLogo agent with a ﬁxed location on
a master grid.
Polymorphism Polymorphism is the ability of an object-
oriented class to respond to multiple related messages,
often method calls with the same name but diﬀerent
parameters.
Procedural language According to Watson [47] “proce-
dural languages ... are those in which the action of the
program is deﬁned by a series of operations deﬁned by
the programmer”.
Programming language A programming language is
a computer language that allows any computable ac-
tivity to be expressed.
Record A record is an independently addressable collec-
tion of data items.
Reﬂection Reﬂection, combined with dynamic method
invocation, is a Java and C# approach to higher-order
programming.
Repast The Recursive Porous Agent Simulation Toolkit
(Repast) is a free and open source family of agent-
based modeling and simulation platforms (ROAD
[44]). Information on Repast and free downloads can
be found at http://repast.sourceforge.net/.
Repast simphony Repast Simphony (Repast S) is the
newest member of the Repast family of free and open
source agent-based modeling and simulation plat-
forms (North et al. [32], North et al. [30]). The Java-
based Repast S system includes advanced features for
specifying, executing, and analyzing agent-based sim-
ulations.
Repast simphony score Repast Simphony Score is an
XML metadata ﬁle format that describes the compo-
nents (e. g., agents and spaces) allowed in a Repast
Simphony simulation.
Runtime type identiﬁcation Runtime Type Identiﬁca-
tion (RTTI) is part of C++’s approach to higher-order
programming. Function pointers are another compo-
nent of this approach.
Structured language Structured languages are languages
that divide programs into separate modules each of
which has one controlled entry point, a limited num-
ber of exit points, and no internal jumps (Dijk-
stra [10]).
Swarm Swarm (Swarm Development Group [40]) is a free
and open source agent-based modeling and simula-
tion platform maintained by the Swarm Development
Group. The core Swarm system uses Objective-C.
A Java-based “Java Swarm” wrapper for the Objective-
C core is also available. Information on Swarm and
free downloads can be found at http://www.swarm.
org/.
Templates Templates are a C++ feature for generalizing
classes.
Turtle A turtle is a mobile NetLogo agent.
Virtual machine A virtual machine is a software environ-
ment that allows user code to have a consistent exe-
cution environment on many diﬀerent computer plat-
forms.
Unstructured language Unstructured languages are lan-
guages that rely on step-by-step solutions such that
the solutions can contain arbitrary jumps between
steps.
Definition of the Subject
Agent-based modeling is a bottom-up approach to repre-
senting and investigating complex systems. Agent-based
models can be implemented either computationally (e. g.,
through computer simulation) or non-computationally
(e. g., with participatory simulation). The close match be-
tween the capabilities of computational platforms and the
requirements of agent-based modeling make these plat-
forms a natural choice for many agent-based models.
Of course, realizing the potential beneﬁts of this natural
match necessitates the use of computer languages to ex-
press the designs of agent-based models. A wide range
of computer programming languages can play this role
including both domain-speciﬁc and general purpose lan-
guages. The domain-speciﬁc languages include business-
oriented languages (e. g., spreadsheet programming tools);
science and engineering languages (e. g., Mathematica);
and dedicated agent-based modeling languages (e. g., Net-
Logo). The general purpose languages can be used directly
(e. g., Java programming) or within agent-based modeling
toolkits (e. g., Repast). The choice that is most appropriate
for each modeling project depends on both the require-
ments of that project and the resources available to imple-
ment it.
Introduction
The term agent-based modeling (ABM) refers to the com-
putational modeling of a system as comprised of a number
of independent, interacting entities, which are referred to
as ‘agents’. Generally, an agent-based system is made up of
agents that interact, adapt, and sustain themselves while
interacting with other agents and adapting to a chang-
ing environment. The fundamental feature of an agent
is its autonomy, the capability of the agent to act inde-
pendently without the need for direction from external
sources. Agents have behaviors that make them active
rather than passive entities. Agent behaviors allow agents
to take in information from their environment, which in-

134 A
Agent Based Modeling and Computer Languages
cludes their interactions with other agents, process the in-
formation and make some decision about their next ac-
tion, and take the action. Jennings [21] provides a rigorous
computer science view of agency emphasizing the essential
characteristic of autonomous behavior.
Beyond the essential characteristic of autonomy, there
is no universal agreement on the precise deﬁnition of the
term “agent”, as used in agent-based modeling. Some con-
sider any type of independent component, whether it be
a software model or a software model of an extant indi-
vidual, to be an agent [4]. An independent component’s
behaviors can be modeled as consisting of anything from
simple reactive decision rules to multi-dimensional behav-
ior complexes based on adaptive artiﬁcial intelligence (AI)
techniques.
Other authors insist that a component’s behavior must
be adaptive in order for the entity to be considered an
agent. The agent label is reserved for components that can
adapt to their environment, by learning from the successes
and failures with their interactions with other agents, and
change their behaviors in response. Casti [5] argues that
agents should contain both base-level rules for behavior
as well as a higher-level set of “rules to change the rules”.
The base-level rules provide responses to the environ-
ment while the “rules to change the rules” provide adap-
tation [5].
From a practical modeling standpoint, agent charac-
teristics can be summarized as follows:
 Agents are identiﬁable as self-contained individuals.
An agent has a set of characteristics and rules govern-
ing its behaviors.
 Agents are autonomous and self-directed. An agent can
function independently in its environment and in its
interactions with other agents, at least over a limited
range of situations that are of interest.
 An agent is situated, living in an environment with
which it interacts along with other agents. Agents have
the ability to recognize and distinguish the traits of
other agents. Agents also have protocols for interaction
with other agents, such as for communication, and the
capability to respond to the environment.
 An agent may be goal-directed, having targets to
achieve with respect to its behaviors. This allows an
agent to compare the outcome of its behavior to its
goals. An agent’s goals need not be comprehensive or
well-deﬁned. For example, an agent does not necessar-
ily have formally stated objectives it is trying to maxi-
mize.
 An agent might have the ability to learn and adapt
its behaviors based on its experiences. An agent might
have rules that modify its behavior over time. Gener-
ally, learning and adaptation at the agent level requires
some form of memory to be built into the agents be-
haviors.
Often, in an agent-based model, the population of agents
varies over time, as agents are born and die. Another
form of adaptation can occur at the agent population level.
Agents that are ﬁt are better able to sustain themselves and
possibly reproduce as time in the simulation progresses,
while agents that have characteristics less suited to their
continued survival are excluded from the population.
Another basic assumption of agent-based modeling is
that agents have access only to local information. Agents
obtain information about the rest of the world only
through their interactions with the limited number of
agents around them at any one time, and from their in-
teractions with a local patch of the environment in which
they are situated.
These aspects of how agent-based modeling treats
agents highlight the fact that the full range of agent di-
versity can be incorporated into an agent-based model.
Agents are diverse and heterogeneous as well as dynamic
in their attributes and behavioral rules. There is no need
to make agents homogeneous through aggregating agents
into groups or by identifying the ‘average’ agent as repre-
sentative of the entire population. Behavioral rules vary in
their sophistication, how much information is considered
in the agent decisions (i. e., cognitive ‘load’), the agent’s
internal models of the external world including the pos-
sible reactions or behaviors of other agents, and the ex-
tent of memory of past events the agent retains and uses
in its decisions. Agents can also vary by the resources they
have manage to accumulate during the simulation, which
may be due to some advantage that results from speciﬁc
attributes. The only limit on the number of agents in an
agent-based model is imposed by the computational re-
sources required to run the model.
As a point of clariﬁcation, agent-based modeling is also
known by other names. ABS (agent-based systems), IBM
(individual-based modeling), and MAS (multi-agent sys-
tems) are widely-used acronyms, but ‘ABM’ will be used
throughout this discussion. The term ‘agent’ has connota-
tions other than how it is used in ABM. For example, ABM
agents are diﬀerent from the typical agents found in mo-
bile agent systems. ‘Mobile agents’ are light-weight soft-
ware proxies that roam the world-wide web and perform
various functioned programmed by their owners such as
gathering information from web sites. To this extent, mo-
bile agents are autonomous and share this characteristic
with agents in ABM.

Agent Based Modeling and Computer Languages
A
135
Types of Computer Languages
A ‘computer language’ is a method of noting directives for
computers. ‘Computer programming languages,’ or ‘pro-
gramming languages,’ are an important category of com-
puter languages. A programming language is a computer
language that allows any computable activity to be ex-
pressed. This article focuses on computer programming
languages rather than the more general computer lan-
guages since virtually all agent-based modeling systems
require the power of programming languages. This arti-
cle sometimes uses the simpler term ‘computer languages’
when referring to computer programming languages. Ac-
cording to Watson [47]:
Programming languages are used to describe algo-
rithms, that is, sequences of steps that lead to the
solution of problems ... A programming language
can be considered to be a ‘notation’ that can be used
to specify algorithms with precision.
Watson [47] goes on to say that “programming lan-
guages can be roughly divided into four groups: impera-
tive languages, functional languages, logic programming
languages, and others”. Watson [47] states that in impera-
tive languages “there is a fundamental underlying depen-
dence on the assignment operation and on variables im-
plemented as computer memory locations, whose contents
can be read and altered”. However, “in functional lan-
guages (sometimes called applicative languages) the fun-
damental operation is function application” [47]. Watson
cites LISP as an example. Watson [47] continues by noting
that “in a logic programming language, the programmer
needs only to supply the problem speciﬁcation in some
formal form, as it is the responsibility of the language sys-
tem to infer a method of solution”.
A useful feature of most functional languages, many
logic programming languages, and some imperative
languages is higher-order programming. According to
Reynolds [35]:
In analogy with mathematical logic, we will say that
a programming language is higher-order if proce-
dures or labels can occur as data, i. e., if these entities
can be used as arguments to procedures, as results of
functions, or as values of assignable variables. A lan-
guage that is not higher-order will be called ﬁrst-or-
der.
Watson [47] oﬀers that “another way of grouping pro-
gramming languages is to classify them as procedural or
declarative languages”. Elaborating, Watson [47] states
that:
Procedural languages ... are those in which the ac-
tion of the program is deﬁned by a series of opera-
tions deﬁned by the programmer. To solve a prob-
lem, the programmer has to specify a series of steps
(or statements) which are executed in sequence.
On the other hand, Watson [47] notes that:
Programming in a declarative language (or non-
procedural language) involves the speciﬁcation of
a set of rules deﬁning the solution to the problem;
it is then up to the computer to determine how
to reach a solution consistent with the given rules
... The language Prolog falls into this category, al-
though it retains some procedural aspects. Another
widespread non-procedural system is the spread-
sheet program.
Imperative and functional languages are usually pro-
cedural while logic programming languages are gener-
ally declarative. This distinction is important since it im-
plies that most imperative and functional languages re-
quire users to deﬁne how each operation is to be com-
pleted while logic programming languages only require
users to deﬁne what is to be achieved. However, when
faced with multiple possible solutions with diﬀerent exe-
cution speeds and memory requirements, imperative and
functional languages oﬀer the potential for users to explic-
itly choose more eﬃcient implementations over less eﬃ-
cient ones. Logic programming languages generally need
to infer which solution is best from the problem descrip-
tion and may or may not choose the most eﬃcient imple-
mentation. Naturally, this potential strength of imperative
and functional languages may also be cast as a weakness.
With imperative and functional language users need to
correctly choose a good implementation among any com-
peting candidates that may be available.
Similarly to Watson [47], Van Roy and Haridi [45] de-
ﬁne several common computational models namely those
that are object-oriented, those that are logic-based, and
those that are functional. Object-oriented languages are
procedural languages that bind procedures (i. e., ‘encapsu-
lated methods’) to their corresponding data (i. e., ‘ﬁelds’)
in nested hierarchies (i. e., ‘inheritance’ graphs) such that
the resulting ‘classes’ can be instantiated to produce exe-
cutable instances (i. e., ‘objects’) that respond to multiple
related messages (i. e., ‘polymorphism’). Logic-based lan-
guages correspond to Watson’s [47] logic programming
languages. Similarly, Van Roy and Haridi [45] functional
languages correspond to those of Watson [47].
Two additional types of languages can be added to
Van Roy and Haridi’s [45] list of three. These are unstruc-

136 A
Agent Based Modeling and Computer Languages
tured and structured languages [10]. Both unstructured
and structured languages are procedural languages.
Unstructured languages are languages that rely on
step-by-step solutions such that the solutions can contain
arbitrary jumps between steps [10]. BASIC, COBOL, FOR-
TRAN, and C are examples of unstructured languages. The
arbitrary jumps are often implemented using ‘goto’ state-
ments. Unstructured languages where famously criticized
by Edsger Dijkstra in his classic paper “Go To Statement
Considered Harmful” [10]. This and related criticism lead
to the introduction of structured languages.
Structured languages are languages that divide pro-
grams into separate modules each of which has one con-
trolled entry point, a limited number of exit points, and
no internal jumps [10]. Following Stevens et al. [38] “the
term module is used to refer to a set of one or more con-
tiguous program statements having a name by which other
parts of the system can invoke it and preferably having its
own distinct set of variable names”. Structured language
modules, often called procedures, are generally intended
to be small. As such, large numbers of them are usually re-
quired to solve complex problems. Standard Pascal is an
example of structured, but not object-oriented, language.
As stated earlier, C is technically an unstructured language
(i. e., it allows jumps within procedures and ‘long jumps’
between procedures), but it is used so often in a struc-
tured way that many people think of it as a structured lan-
guage.
The quality of modularization in structured language
code is often considered to be a function of coupling and
cohesion [38]. Coupling is the tie between modules such
that the proper functioning of one module depends on the
functioning of another module. Cohesion is the ties within
a module such that proper functioning of one line of code
in a module depends on the functioning of another one
line of code in the same module. The goal for modules is
maximizing cohesion while minimizing coupling.
Object-oriented languages are a subset of structured
languages. Object-oriented methods and classes are struc-
tured programming modules that have special features for
binding data, inheritance, and polymorphism. The previ-
ously introduced concepts of coupling and cohesion ap-
ply to classes, objects, methods, and ﬁelds the same way
that they apply to generic structured language modules.
Objective-C, C++, C#, and Java are all examples of object-
oriented languages. As with C, the languages Objective-C,
C++, and C# oﬀer goto statements but they have object-
oriented features and are generally used in a structured
way. Java is an interesting case in that the word ‘goto’ is
reserved as a keyword in the language speciﬁcation, but it
is not intended to be implemented.
It is possible to develop agent-based models using any
of the programming languages discussed above namely,
unstructured languages, structured languages, object-ori-
ented languages, logic-based languages, and functional
languages. Speciﬁc examples are provided later in this arti-
cle. However, certain features of programming languages
are particularly well suited for supporting the require-
ments of agent-based modeling and simulation.
Requirements of Computer Languages
for Agent-Based Modeling
The requirements of computer languages for agent-based
modeling and simulation include the following:
 There is a need to create well deﬁned modules that cor-
respond to agents. These modules should bind together
agent state data and agent behaviors into integrated in-
dependently addressable constructs. Ideally these mod-
ules will be ﬂexible enough to change structure over
time and to optionally allow fuzzy boundaries to im-
plement models that go beyond methodological indi-
vidualism [20].
 There is a need to create well deﬁned containers that
correspond to agent environments. Ideally these con-
tainers will be recursively nestable or will otherwise
support sophisticated deﬁnitions of containment.
 There is a need to create well deﬁned spatial relation-
ships within agent environments. These relationships
should include notions of abstract space (e. g., lattices),
physical space (e. g., maps), and connectedness (e. g.,
networks).
 There is a need to easily setup model conﬁgurations
such as the number of agents; the relationships between
agents; the environmental details; and the results to be
collected.
 There is a need to conveniently collect and analyze
model results.
Each of the kinds of programming languages namely,
unstructured languages, structured languages, object-ori-
ented languages, logic-based languages, and functional
languages can address these requirements.
Unstructured languages generally support procedure
deﬁnitions which can be used to implement agent behav-
iors. They also sometimes support the collection of diverse
data into independently addressableconstructs in the form
of data structures often called ‘records’. However, they
generally lack support for binding procedures to individ-
ual data items or records of data items. This lack of support
for creating integrated constructs also typically limits the

Agent Based Modeling and Computer Languages
A
137
language-level support for agent containers. Native sup-
port for implementing spatial environments is similarly
limited by the inability to directly bind procedures to data.
As discussed in the previous section, unstructured lan-
guages oﬀer statements to implement execution jumps.
The use of jumps within and between procedures tends
to reduce module cohesion and increase module coupling
compared to structured code. The result is reduced code
maintainability and extensibility compared to structured
solutions. This is a substantial disadvantage of unstruc-
tured languages.
In contrast, many have argued that, at least theo-
retically, unstructured languages can achieve the highest
execution speed and lowest memory usage of the lan-
guage options since nearly everything is left to the applica-
tion programmers. In practice, programmers implement-
ing agent-based models in unstructured languages usually
need to write their own tools to form agents by correlating
data with the corresponding procedures. Ironically, these
tools are often similar in design, implementation, and per-
formance to some of the structured and object-oriented
features discussed later.
Unstructured languages generally do not provide spe-
cial support for application data conﬁguration, program
output collection, or program results analysis. As such,
these tasks usually need to be manually implemented by
model developers.
In terms of agent-based modeling, structured lan-
guages are similar to unstructured languages in that they
do not provide tools to directly integrate data and proce-
dures into independently addressable constructs. There-
fore, structured language support for agents, agent envi-
ronments, and agent spatial relationships is similar to that
provided by unstructured languages. However, the lack of
jump statements in structured languages tends to increase
program maintainability and extensibility compared to
unstructured languages. This generally gives structured
languages a substantial advantage over unstructured lan-
guages for implementing agent-based models.
Object-oriented languages build on the maintainabil-
ity and extensibility advantages of structured languages by
adding the ability to bind data to procedures. This bind-
ing in the form of classes provides a natural way to imple-
ment agents. In fact, object-oriented languages have their
roots in Ole-Johan Dahl and Kristen Nygaard’s Simula
simulation language [7,8,45]! According to Dahl and Ny-
gaard [7]:
SIMULA (SIMULation LAnguage) is a language de-
signed to facilitate formal description of the lay-
out and rules of operation of systems with discrete
events (changes of state). The language is a true
extension of ALGOL 60 [2], i. e., it contains AL-
GOL 60 as a subset. As a programming language,
apart from simulation, SIMULA has extensive list
processing facilities and introduces an extended co-
routine concept in a high-level language.
Dahl and Nygaard go on to state the importance of
speciﬁc languages for simulation [7] as follows:
Simulation is now a widely used tool for analysis of
a variety of phenomena: nerve networks, commu-
nication systems, traﬃc ﬂow, production systems,
administrative systems, social systems, etc. Because
of the necessary list processing, complex data struc-
tures and program sequencing demands, simulation
programs are comparatively diﬃcult to write in ma-
chine language or in ALGOL or FORTRAN. This
alone calls for the introduction of simulation lan-
guages.
However, still more important is the need for a set of
basic concepts in terms of which it is possible to ap-
proach, understand and describe all the apparently
very diﬀerent phenomena listed above. A simulation
language should be built around such a set of basic
concepts and allow a formal description which may
generate a computer program. The language should
point out similarities and diﬀerences between sys-
tems and force the research worker to consider all
relevant aspects of the systems. System descriptions
should be easy to read and print and hence useful
for communication.
Again, according to Dahl and Nygaard [8]:
SIMULA I (1962–65) and Simula 67 (1967) are the
two ﬁrst object-oriented languages. Simula 67 in-
troduced most of the key concepts of object-ori-
ented programming: both objects and classes, sub-
classes (usually referred to as inheritance) and vir-
tual procedures, combined with safe referencing
and mechanisms for bringing into a program collec-
tions of program structures described under a com-
mon class heading (preﬁxed blocks).
The Simula languages were developed at the Norwe-
gian Computing Center, Oslo, Norway by Ole-Johan Dahl
and Kristen Nygaard. Nygaard’s work in Operational Re-
search in the 1950s and early 1960s created the need for
precise tools for the description and simulation of com-

138 A
Agent Based Modeling and Computer Languages
plex man-machine systems. In 1961 the idea emerged for
developing a language that both could be used for sys-
tem description (for people) and for system prescription
(as a computer program through a compiler). Such a lan-
guage had to contain an algorithmic language, and Dahl’s
knowledge of compilers became essential ... When the in-
heritance mechanism was invented in 1967, Simula 67 was
developed as a general programming language that also
could be specialized for many domains, including system
simulation.
Generally, object-oriented classes are used to deﬁne
agent templates and instantiated objects are used to im-
plement speciﬁc agents. Agent environment templates
and spatial relationships patterns are also typically imple-
mented using classes. Recursive environment nesting as
well as abstract spaces, physical spaces, and connected-
ness can all be represented in relatively straightforward
ways. Instantiated objects are used to implement speciﬁc
agent environments and spatial relationships in individ-
ual models. Within these models, model conﬁgurations
are also commonly implemented as objects instantiated
from one or more classes. However, as with unstructured
and structured languages, object-oriented languages gen-
erally do not provide special support for application data
conﬁguration, program output collection, or program re-
sults analysis. As such, these tasks usually need to be
manually implemented by model developers. Regardless
of this, the ability to bind data and procedures provides
such a straightforward method for implementing agents
that most agent-based models are written using object-ori-
ented languages.
It should be noted that traditional object-oriented
languages do not provide a means to modify class and
object structures once a program begins to execute. Newer
‘dynamic’ object-oriented languages such as Groovy [22]
oﬀer this capability. This potentially allows agents to gain
and lose data items and methods during the execution of
a model based on the ﬂow of events in a simulation. This in
turn oﬀers the possibility of implementing modules with
fuzzy boundaries that are ﬂexible enough to change struc-
ture over time.
As discussed in the previous section, logic-based lan-
guages oﬀer an alternative to the progression formed by
unstructured, structured, and object-oriented languages.
Logic-based languages can provide a form of direct sup-
port for binding data (e. g., asserted propositions) with ac-
tions (e. g., logical predicates), sometimes including the
use of higher-order programming. In principle, each agent
can be implemented as a complex predicate with multiple
nested sub-terms. The sub-terms, which may contain un-
resolved variables, can then be activated and resolved as
needed during model execution. Agent templates which
are analogous to object-oriented classes can be imple-
mented using the same approach but with a larger num-
ber of unresolved variables. Agent environments and the
resulting relationships between agents can be formed in
a similar way. Since each of these constructs can be modi-
ﬁed at any time, the resulting system can change structure
over time and may even allow fuzzy boundaries. In prac-
tice this approach is rarely, if ever, used. As with the previ-
ously discussed approaches, logic-based languages usually
do not provide special support for application data con-
ﬁguration, program output collection, or program results
analysis so these usually need to be manually developed.
Functional languages oﬀer yet another alternative to
the previously discussed languages. Like logic-based and
object-oriented languages, functional languages often pro-
vide a form of direct support for binding data with behav-
iors. This support often leverages the fact that most func-
tional languages support higher-order programming. As
a result, the data is usually in the form of nested lists of val-
ues and functions while the behaviors themselves are im-
plemented in the form of functions. Agent templates (i. e.,
‘classes’), agent environments, and agent relationships can
be implemented similarly. Each of the lists can be dynami-
cally changed during a simulation run so the model struc-
ture can evolve and can potentially have fuzzy boundaries.
Unlike the other languages discussed so far, a major class
of functional languages, namely those designed for com-
putational mathematics usually include sophisticated sup-
port for program output collection and results analysis.
An example is Mathematica (Wolfram [49]). If the appli-
cation data is conﬁgured in mathematically regular ways
then these systems may also provide support for applica-
tion data setup.
Example Computer Languages Useful
for Agent-Based Modeling
Domain-Speciﬁc Languages
Domain-speciﬁc languages (DSL’s) are computer lan-
guages that are highly customized to support a well de-
ﬁned application area or ‘domain’. DSL’s commonly in-
clude a substantial number of keywords that are nouns
and verbs in the area of application as well as overall struc-
tures and execution patterns that correspond closely with
the application area. DSL’s are intended to allow users to
write in a language that is closely aligned with their area of
expertise.
DSL’s often gain their focus by losing generality. For
many DSL’s there are activities that can be programmed
in most computer languages that cannot be programmed

Agent Based Modeling and Computer Languages
A
139
in the given DSL. This is consciously done to simplify the
DSL’s design and make it easier to learn and use. If a DSL
is properly designed then the loss of generality is often in-
consequential for most uses since the excluded activities
are chosen to be outside the normal range of application.
However, even the best designed DSL’s can occasionally be
restrictive when the bounds of the language are encoun-
tered. Some DSL’s provide special extension points that
allow their users to program in a more general language
such as C or Java when the limits of the DSL are reached.
This feature is extremely useful, but requires more sophis-
tication on the part of the user in that they need to know
and simultaneously use both the DSL and the general lan-
guage.
DSL’s have the potential to implement speciﬁc features
to support ‘design patterns’ within a given domain. Design
patterns form a “common vocabulary” describing tried
and true solutions for commonly faced software design
problems (Coplien [6]). Software design patterns were
popularized by Gamma et al. [13]. North and Macal [29]
describe three design patterns for agent-based modeling
itself.
In principle, DSL’s can be unstructured, structured,
object-oriented, logic-based, or functional. In practice,
DSL’s are often structured languages or object-oriented
languages and occasionally are functional languages.
Commonly used ABM DSL’s include business-oriented
languages (e. g., spreadsheet programming tools); science
and engineering languages (e. g., Mathematica); and dedi-
cated agent-based modeling languages (e. g., NetLogo).
Business Languages
Some of the most widely used busi-
ness computer languages are those available in spread-
sheet packages. Spreadsheets are usually programmed us-
ing a ‘macro language’. As discussed further in North and
Macal [29], any modern spreadsheet program can be used
to do basic agent-based modeling. The most common con-
vention is to associate each row of a primary spreadsheet
worksheet with an agent and use consecutive columns
to store agent properties. Secondary worksheets are then
used to represent the agent environment and to provide
temporary storage for intermediate calculations. A simple
loop is usually used to scan down the list of agents and to
allow each one to execute in turn. The beginning and end
of the scanning loop are generally used for special setup
activities before and special cleanup activities after each
round. An example agent spreadsheet from North and
Macal [29] is shown in Fig. 1and Fig. 2. Agent spreadsheets
have both strengths and weaknesses compared to the other
ABM tools. Agent spreadsheets tend to be easy to build
but they also tend to have limited capabilities. This bal-
ance makes spreadsheets ideal for agent-based model ex-
ploration, scoping, and prototyping. Simple agent models
can be implemented on the desktop using environments
outside of spreadsheets as well.
Science and Engineering Languages
Science and en-
gineering languages embodied in commercial products
such as Mathematica, MATLAB, Maple, and others can
be used as a basis for developing agent-based models.
Such systems usually have a large user base, are read-
ily available on desktop platforms, and are widely inte-
grated into academic training programs. They can be used
as rapid prototype development tools or as components
of large-scale modeling systems. Science and engineer-
ing languages have been applied to agent-based modeling.
Their advantages include a fully integrated development
environment, their interpreted (as opposed to compiled)
nature provides immediate feedback to users during the
development process, and a packaged user interface. Inte-
grated tools provide support for data import and graphi-
cal display. Macal [25] describes the use of Mathematica
and MATLAB in agent-based simulation and Macal and
Howe [26] detail investigations into linking Mathematica
and MATLAB to the Repast ABM toolkit to make use of
Repast’s simulation scheduling algorithms. In the follow-
ing sections we focus on MATLAB and Mathematica as
representative examples of science and engineering lan-
guages.
MATLAB and Mathematica are both examples of
Computational Mathematics Systems (CMS). CMS allow
users to apply powerful mathematical algorithms to solve
problems through a convenient and interactive user inter-
face. CMS typically supply a wide range of built-in func-
tions and algorithms. MATLAB, Mathematica, and Maple
are examples of commercially available CMS whose ori-
gins go back to the late 1980s. CMS are structured in two
main parts: (1) the user interface that allows dynamic user
interaction, and (2) the underlying computational engine,
or kernel, that performs the computations according to
the user’s instructions. Unlike conventional programming
languages, CMS are interpreted instead of compiled, so
there is immediate feedback to the user, but some perfor-
mance penalty is paid. The underlying computational en-
gine is written in the C programming language for these
systems, but C coding is unseen by the user. The most
recent releases of CMS are fully integrated systems, com-
bining capabilities for data input and export, graphical dis-
play, and the capability to link to external programs writ-
ten in conventional languages such as C or Java using
inter-process communication protocols. The powerful
features of CMS, their convenience of use, the need to

140 A
Agent Based Modeling and Computer Languages
Agent Based Modeling and Computer Languages, Figure 1
An example agent spreadsheet [29]
Agent Based Modeling and Computer Languages, Figure 2
An example agent spreadsheet code [29]
learn only a limited number of instructions on the part of
the user, and the immediate feedback provided to users are
features of CMS that make them good candidates for de-
veloping agent-based simulations.
A further distinction can be made among CMS. A sub-
set of CMS are what is called Computational Algebra Sys-
tems (CAS). CAS are computational mathematics systems
that calculate using symbolic expressions. CAS owe their
origins to the LISP programming language, which was the
earliest functional programming language [24]. Macsyma
(www.scientek.com/macsyma) and Scheme [37] (www.
swiss.ai.mit.edu/projects/scheme) are often mentioned as
important implementations leading to present day CAS.
Typical uses of CAS are equation solving, symbolic in-
tegration and diﬀerentiation, exact calculations in linear
algebra, simpliﬁcation of mathematical expressions, and
variable precision arithmetic. Computational mathematics
systems consist of numeric processing systems or symbolic
processing systems, or possibly a combination of both. Es-
pecially when algebraic and numeric capabilities are com-
bined into a multi-paradigm programming environment,
new modeling possibilities open up for developing sophis-
ticated agent-based simulations with minimal coding.
Mathematica
Mathematica is a commercially available
numeric processing system with enormous integrated nu-
merical processing capability (http://www.wolfram.com).
Beyond numeric processing, Mathematica is a fully func-
tional programming language. Unlike MATLAB, Mathe-
matica is a symbolic processing system that uses term re-
placement as its primary operation. Symbolic processing

Agent Based Modeling and Computer Languages
A
141
means that variables can be used before they have values
assigned to them; in contrast a numeric processing lan-
guage requires that every variable have a value assigned
to it before it is used in the program. In this respect, al-
though Mathematica and MATLAB may appear similar
and share many capabilities, Mathematica is fundamen-
tally much diﬀerent than MATLAB, with a much diﬀerent
style of programming and ultimately with a diﬀerent set of
capabilities applicable to agent-based modeling.
Mathematica’s symbolic processing capabilities allow
one to program in multiple programming styles, either as
alternatives or in combination, such as functional pro-
gramming, logic programming, procedural program-
ming, and even object-oriented programming styles. Like
MATLAB, Mathematica is also an interpreted language,
with the kernel of Mathematica running in the back-
ground in C. In terms of data types, everything is an ex-
pression in Mathematica. An expression is a data type with
a head and a list of arguments in which even the head of
the expression is part of the expression’s arguments.
The Mathematica user interface consists of a what is
referred to as a notebook (Fig. 3). A Mathematica note-
book is a fully integratable development environment
and a complete publication environment. The Mathemat-
ica Application Programming Interface (API) allows pro-
grams written in C, FORTRAN, or Java to interact with
Mathematica. The API has facilities for dynamically call-
ing routines from Mathematica as well as calling Mathe-
matica as a computational engine.
Figure 3 shows Mathematica desktop notebook envi-
ronment. A Mathematica notebook is displayed in its own
window. Within a notebook, each item is contained in
a cell. The notebook cell structure has underlying coding
that is accessible to the user.
In Mathematica, a network representation consists of
combining lists of lists, or more generally expressions of
expressions, to various depths. For example, in Mathemat-
ica, an agent can be represented explicitly as an expression
that includes a head named agent, a sequence of agent at-
tributes, and a list of the agent’s neighbors. Agent data and
methods are linked together by the use of what are called
up values.
Example references for agent-based simulation using
Mathematica include Gaylord and Davis [15], Gaylord
and Nishidate [16], and Gaylord and Wellin [17]. Gaylord
and D’Andria [14] describe applications in social agent-
based modeling.
MATLAB
The MATrix LABoratory (MATLAB) is a nu-
meric processing system with enormous integrated nu-
merical processing capability (http://www.mathworks.
Agent Based Modeling and Computer Languages, Figure 3
Example Mathematica cellular automata model
com). It uses a scripting-language approach to program-
ming. MATLAB is a high-level matrix/array language with
control ﬂow, functions, data structures, input/output, and
object-oriented programming features. The user interface
consists of the MATLAB Desktop, which is a fully inte-
grated and mature development environment. There is an
application programming interface (API). The MATLAB
API allows programs written in C, Fortran, or Java to in-
teract with MATLAB. There are facilities for calling rou-
tines from MATLAB (dynamic linking) as well as routines
for calling MATLAB as a computational engine, as well as
for reading and writing specialized MATLAB ﬁles.
Figure 4 shows the MATLAB Desktop environment il-
lustrating the Game of Life, which is a standard MATLAB
demonstration. The desktop consist of four standard win-
dows: a command window, which contains a command
line, the primary way of interacting with MATLAB, the
workspace, which indicates the values of all the variables
currently existing in the session, a command history win-
dow that tracks the entered command, and the current di-
rectory window. Other windows allow text editing of pro-
grams and graphical output display.
When it comes to agent-based simulation, as in most
types of coding, the most important indicator of the power
of a language for modeling is the extent of and the sophis-
tication of the allowed data types and data structures. As
Sedgewick [36] observes:

142 A
Agent Based Modeling and Computer Languages
Agent Based Modeling and Computer Languages, Figure 4
Example MATLAB cellular automata model
For many applications, the choice of the proper data
structure is really the only major decision involved
in the implementation; once the choice has been
made only very simple algorithms are needed [36].
The ﬂexibility of data types plays an important role in
developing large-scale, extensible models for agent-based
simulation. In MATLAB the primary data type is the dou-
ble array, which is essentially a two-dimensional numeric
matrix. Other data types include logical arrays, cell arrays,
structures, and character arrays.
For agent-based simulations that deﬁne agent relation-
ships based on networks, connectivity of the links deﬁnes
the scope of agent interaction and locally available infor-
mation. Extensions to modeling social networks require
the use of more complex data structures than the ma-
trix structure commonly used for grid representations. Ex-
tensions from grid topologies to network topologies are
straightforward in MATLAB and similarly in Mathemat-
ica. In MATLAB, a network representation consists of
combining cell arrays or structures in various ways.
The MATLAB desktop environment showing the
Game of Life demonstration appears in Fig. 4. The Game
of Life is a cellular automaton invented by mathemati-
cian John Conway that involves live and dead cells in
cellular automata grid. In MATLAB, the agent environ-
ment is a sparse matrix that is initially set to all zeros.
Whether cells stay alive, die, or generate new cells de-
pends upon how many of their eight possible neighbors
are alive. By using sparse matrices, the calculations re-
quired become very simple. Pressing the “Start” button
automatically seeds this universe with several small ran-
dom communities and initiates a series of cell updates. Af-
ter a short period of simulation, the initial random distri-
bution of live (i. e., highlighted) cells develops into sets of
sustainable patterns that endure for generations.
Several agent-based models using MATLAB have been
published in addition to the Game of Life. These in-
clude a model of political institutions in modern Italy [3],
a model of pair interactions and attitudes [34], a bar-
gaining model to simulate negotiations between water
users [43], and a model of sentiment and social mito-
sis based on Heider’s Balance Theory [18,46]. The latter
model uses Euler, a MATLAB-like language. Thorngate
argues for the use of MATLAB as an important tool to
teach simulation programming techniques [42].
Dedicated Agent-Based Modeling Languages
Dedi-
cated agent-based modeling languages are DSL’s that are
designed to speciﬁcally support agent-based modeling.
Several such languages currently exist. These languages are
functionally diﬀerentiated by the underlying assumptions
their designers made about the structures of agent-based
models. The designers of some of these languages assume
quite a lot about the situations being modeled and use this
information to provide users with pre-completed or tem-
plate components. The designers of other languages make
comparatively fewer assumptions and encourage users to

Agent Based Modeling and Computer Languages
A
143
Agent Based Modeling and Computer Languages, Figure 5
Example NetLogo ant colony model [48]
implement a wider range of models. However, more work
is often needed to build models in these systems. This ar-
ticle will discuss two selected examples, namely NetLogo
and the visual interface for Repast Simphony.
NetLogo
NetLogo is an education-focused ABM envi-
ronment (Wilensky [48]). The NetLogo language uses
a modiﬁed version of the Logo programming language
(Harvey [19]). NetLogo itself is Java-based and is free
for use in education and research. More information
on NetLogo and downloads can be found at http://ccl.
northwestern.edu/netlogo/.
NetLogo is designed to provide a basic computational
laboratory for teaching complex adaptive systems con-
cepts. NetLogo was originally developed to support teach-
ing, but it can be used to develop a wider range of ap-
plications. NetLogo provides a graphical environment to
create programs that control graphic ‘turtles’ that reside
in a world of ‘patches’ that is monitored by an ‘observer’.
NetLogo’s DSL is limited to its turtle and patch paradigm.
However, NetLogo models can be extended using Java to
provide for more general programming capabilities. An
example NetLogo model of an ant colony [48] (center)
feeding on three food sources (upper left corner, lower
left corner, and middle right) is shown in Fig. 5. Example
code [48] from this model is shown in Fig. 6.
Repast Simphony Visual Interface
The Recursive Porous
Agent Simulation Toolkit (Repast) is a free and open
Agent Based Modeling and Computer Languages, Figure 6
Example NetLogo code from the ant colony model [48]
source family of agent-based modeling and simulation
platforms (ROAD [44]). Information on Repast and free
downloads can be found at http://repast.sourceforge.net/.
Repast Simphony (Repast S) is the newest member of the
Repast family [30,32]. The Java-based Repast S system in-
cludes advanced features for specifying, executing, and an-
alyzing agent-based simulations. Repast Simphony oﬀers
several methods for specifying agents and agent environ-
ments including visual speciﬁcation, speciﬁcation with the
dynamic object-oriented Groovy language [22], and spec-
iﬁcation with Java. In principle, Repast S’s visual DSL can
be used for any kind of programming, but models beyond
a certain level of complexity are better implemented in
Groovy or Java. As discussed later, Groovy and Java are
general purpose languages. All of Repast S’s languages can
be ﬂuidly combined in a single model. An example Repast
S ﬂocking model is shown in Fig. 7 [33]. The visual speciﬁ-
cation approach uses a tree to deﬁne model contents (Fig. 8
middle panel with “model.score” header) and a ﬂowchart
to deﬁne agent behaviors (Fig. 8 right side panel with
“SearchExampleAgent.agent” header). In all cases, the user
has a choice of a visual rich point-and-click interface or
a ‘headless’ batch interface to execute models.
General Languages
Unlike DSL’s, general languages are
designed to take on any programming challenge. How-
ever, in order to meet this challenge they are usually more
complex than DSL’s. This tends to make them more dif-
ﬁcult to learn and use. Lahtinen et al. [23] documents
some of the challenges users face in learning general pur-
pose programming languages. Despite these issues, gen-
eral purpose programming languages are essential for al-

144 A
Agent Based Modeling and Computer Languages
Agent Based Modeling and Computer Languages, Figure 7
Example Repast Simphony flocking model [33]
Agent Based Modeling and Computer Languages, Figure 8
Example Repast Simphony visual behavior from the flocking model [33]
lowing users to access the full capabilities of modern com-
puters. Naturally, there are a huge number of general pur-
pose programming languages. This article considers these
options from two perspectives. First, general language
toolkits are discussed. These toolkits provide libraries of
functions to be used in a general purpose host language.
Second, the use of three raw general purpose languages,
namely Java, C#, and C++, is discussed.
General Language Toolkits
As previously stated, general
language toolkits are libraries that are intended be used
in a general purpose host language. These toolkits usually
provide model developers with software for functions such
as simulation time scheduling, results visualization, results
logging, and model execution as well as domain-speciﬁc
tools [31]. Users of raw general purpose languages have to
write all of the needed features by themselves by hand.

Agent Based Modeling and Computer Languages
A
145
A wide range of general language toolkits currently ex-
ist. This article will discuss two selected examples, namely
Swarm and the Groovy and Java interfaces for Repast Sim-
phony.
Swarm
Swarm [28] is a free and open source agent-
based modeling library. Swarm seeks to create a shared
simulation platform for agent modeling and to facilitate
the development of a wide range of models. Users build
simulations by incorporating Swarm library components
into their own programs. Information on Swarm and free
downloads can be found at http://www.swarm.org/ from
Marcus Daniels [9]:
Swarm is a set of libraries that facilitate implemen-
tation of agent-based models. Swarm’s inspiration
comes from the ﬁeld of Artiﬁcial Life. Artiﬁcial
Life is an approach to studying biological systems
that attempts to infer mechanism from biological
phenomena, using the elaboration, reﬁnement, and
generalization of these mechanisms to identify uni-
fying dynamical properties of biological systems ...
To help ﬁll this need, Chris Langton initiated the
Swarm project in 1994 at the Santa Fe Institute. The
ﬁrst version was available by 1996, and since then
it has evolved to serve not only researchers in biol-
ogy, but also anthropology, computer science, de-
fense, ecology, economics, geography, industry, and
political science.
The Swarm simulation system has two fundamental
components. The core component runs general-purpose
simulation code written in Objective-C, Tcl/Tk, and Java.
This component handles most of the behind the scenes
details. The external wrapper components run user-spe-
ciﬁc simulation code written in either Objective-C or Java.
These components handle most of the center stage work.
An example Swarm supply chain model is shown in Fig. 9.
Repast Simphony Java and Groovy
As previously dis-
cussed, Repast is a free and open source family of agent-
based modeling and simulation platforms (ROAD [44]).
Information on Repast and free downloads can be found
at http://repast.sourceforge.net/. Repast S is the newest
member of the Repast family [30,32]. The Java-based
Repast S system includes advanced features for specifying,
executing, and analyzing agent-based simulations. An ex-
ample Repast S ﬂocking model is shown in Fig. 7 [33].
Repast Simphony oﬀers several intermixable methods
for specifying agents and agent environments including
visual speciﬁcation, speciﬁcation with the dynamic ob-
ject-oriented Groovy language [22], and speciﬁcation with
Agent Based Modeling and Computer Languages, Figure 9
Example Swarm supply chain model
Agent Based Modeling and Computer Languages, Figure 10
Example Repast Simphony Groovy code from the flocking model
in Fig. 7 [33]
Java. The Groovy approach uses the dynamic object-ori-
ented Groovy language as shown in Fig. 10. The Java ap-
proach for an example predator-prey model is shown in
Fig. 11.
Java
Java [12] is a widely used object-oriented program-
ming language that was developed and is maintained by
Sun Microsystems. Java is known for its widespread ‘cross-
platform’ availability on many diﬀerent types of hard-

146 A
Agent Based Modeling and Computer Languages
Agent Based Modeling and Computer Languages, Figure 11
Example Repast Simphony Java code for a predator-prey model [41]
ware and operating systems. This capability comes from
Java’s use of a ‘virtual machine’ that allows binary code
or ‘bytecode’ to have a consistent execution environment
on many diﬀerent computer platforms. A large number of
tools are available for Java program development includ-
ing the powerful Eclipse development environment [11]
and many supporting libraries. Java uses reﬂection and
dynamic method invocation to implement a variant of
higher-order programming. Reﬂection is used for runtime
class structure examination while dynamic method invo-
cation is used to call newly referenced methods at run-
time. Java’s object-orientation, cross platform availabil-
ity, reﬂection, and dynamic method invocation along with
newer features such as annotations for including meta-
data in compiled code, generics for generalizing class,
and aspects to implement dispersed but recurrent tasks
make it a good choice for agent-based model develop-
ment.
C#
C# [1] is an object-oriented programming language
that was developed and is maintained by Microsoft. C#
is one of many languages that can be used to generate
Microsoft .NET Framework code or Common Interme-
diate Language (CIL). Like Java bytecode, CIL is run us-
ing a ‘virtual machine’ that potentially gives it a consistent
execution environment on diﬀerent computer platforms.
A growing number of tools are emerging to support C# de-
velopment. C#, and the Microsoft .NET Framework more
generally, are in principle cross platform, but in practice
they are mainly executed under Microsoft Windows.
The Microsoft .NET Framework provides for the com-
pilation into CIL of many diﬀerent languages such as C#,
Managed C++, and Managed Visual Basic to name just
a few. Once these languages are compiled to CIL, the re-
sulting modules are fully interoperable. This allows users
to conveniently develop integrated software using a mix-
ture of diﬀerent languages. Like Java, C# supports re-
ﬂection and dynamic method invocation for higher-or-
der programming. C#’s object-orientation, multi-lingual
integration, generics, attributes for including metadata in
compiled code, aspects, reﬂection, and dynamic method
invocation make it well suited for agent-based model de-
velopment, particularly on the Microsoft Windows plat-
form.
C++
C++ is a widely used object-oriented programming
language that was created by Bjarne Stroustrup (Strous-
trup [39]) at AT&T. C++ is widely noted for both its ob-
ject-oriented structure and its ability to be easily compiled
into native machine code. C++ gives users substantial ac-
cess to the underlying computer but also requires substan-
tial programming skills.
Most C++ compilers are actually more properly con-
sidered C/C++ compilers since they can compile non-ob-
ject-oriented C code as well as object-oriented C++ code.
This allows sophisticated users to the opportunity highly
optimize selected areas of model code. However, this also
opens the possibility of introducing diﬃcult to resolve er-
rors and hard to maintain code. It is also more diﬃcult
to port C++ code from one computer architecture to an-

Agent Based Modeling and Computer Languages
A
147
other than it is for virtual machine-based languages such
as Java.
C++ can use a combination of Runtime Type Identiﬁ-
cation (RTTI) and function pointers to implement higher-
order programming. Similar to the Java approach, C++
RTTI can be used for runtime class structure examina-
tion while function pointers can be used to call newly
referenced methods at runtime. C++’s object-orientation,
RTTI, function pointers, and low-level machine access
make it a reasonable choice for the development of ex-
tremely large or complicated agent-based models.
Future Directions
Future developments in computer languages could have
enormous implications for the development of agent-
based modeling. Some of the challenges of agent-based
modeling for the future include (1) scaling up models to
handle large numbers of agents running on distributed
heterogeneous processors across the grid, (2) handling the
large amounts of data generated by agent models and mak-
ing sense out of it, and (3) developing user-friendly in-
terfaces and modular components in a collaborative en-
vironment that can be used by domain experts with lit-
tle or no knowledge of standard computer coding tech-
niques. Visual and natural language development environ-
ments that can be used by non-programmers are continu-
ing to advance but remain to be proven at reducing the
programming burden. There are a variety of next steps for
the development of computer languages for agent-based
modeling including the further development of DSL’s; in-
creasing visual modeling capabilities; and the develop-
ment of languages and language features that better sup-
port pattern-based development. DSL’s are likely to be-
come increasing available as agent-based modeling grows
into a wider range of domains. More agent-based model-
ing systems are developing visual interfaces for specifying
model structures and agent behaviors. Many of these vi-
sual environments are themselves DSL’s. The continued
success of agent-based modeling will likely yield an in-
creasing number of design patterns. Supporting and even
automating implementations of these patterns may form
a natural source for new language features. Many of these
new features are likely to be implemented within DSL’s.
Bibliography
1. Archer T (2001) Inside C#. Microsoft Press, Redmond
2. Backus J, Bauer F, Green J, Katz C, McCarthy J, Naur P, Perlis A,
Rutishauser H, Samuelson K, Vauquois B, Wegstein J, van Wi-
jngaarden A, Woodger M (1963) Revised Report on the Algo-
rithmic Language ALGOL 60. In: Naur P (ed) Communications
of the ACM, vol 6. ACM, New York, pp 1–17
3. Bhavnani R (2003) Adaptive agents, political institutions and
civic traditions in modern italy. J Artif Soc Soc Simul 6(4). Avail-
able at http://jasss.soc.surrey.ac.uk/6/4/1.html
4. Bonabeau E (2001) Agent-based modeling: methods and tech-
niques for simulating human systems. Proc Natl Acad Sci
99(3):7280–7287
5. Casti J (1997) Would-be worlds: how simulation is changing
the world of science. Wiley, New York
6. Coplien J (2001) Software patterns home page. Available as
http://hillside.net/patterns/
7. Dahl O-J, Nygaard K (1966) SIMULA – an ALGOL-based simula-
tion language. Commun ACM 9:671–678
8. Dahl O-J, Nygaard K (2001) How object-oriented pro-
gramming started. Available at http://heim.ifi.uio.no/~kristen/
FORSKNINGSDOK_MAPPE/F_OO_start.html
9. Daniels M (1999) Integrating simulation technologies with
swarm. In: Proc of the agent 1999 workshop on agent simu-
lation: applications, models, and tools. Argonne National Lab-
oratory, Argonne
10. Dijkstra E (1968) Go to statement considered harmful. Com-
mun ACM 11(3):147–148
11. Eclipse (2008) Eclipse home page. Available at http://www.
eclipse.org/
12. Foxwell H (1999) Java 2 software development kit. Linux J
13. Gamma E, Helm R, Johnson R, Vlissides J (1995) Design pat-
terns: elements of reusable object-oriented software. Addison-
Wesley, Wokingham
14. Gaylord R, D’Andria L (1998) Simulatingsociety: a mathematica
toolkit for modeling socioeconomic behavior. Springer/TELOS,
New York
15. Gaylord R, Davis J (1999) Modeling nonspatial social interac-
tions. Math Edu Res 8(2):1–4
16. Gaylord R, Nishidate K (1994) Modeling nature: cellular au-
tomata simulations with Mathematica. Springer, New York
17. Gaylord R, Wellin P (1995) Computer simulations with Math-
ematica: explorations in complex physical and biological sys-
tems. Springer/TELOS, New York
18. Guetzkow H, Kotler P, Schultz R (eds) (1972) Simulation in so-
cial and administrative science. Prentice Hall, Englewood Cliffs
19. Harvey B (1997) Computer science logo style. MIT Press. Boston
20. Heath J (2005) Methodological individualism. In: Zalta E (ed)
Stanford encyclopedia of philosophy. Stanford University,
Stanford, Aviable at http://plato.stanford.edu
21. Jennings N (2000) On agent-based software engineering. Artif
Intell 117:277–296
22. Koenig D, Glover A, King P, Laforge G, Skeet J (2007) Groovy in
action. Manning Publications, Greenwhich
23. Lahtinen E, Ala-Mutka K, Jarvinen H-M (2005) A study of the
difficulties of novice programmers. In: Proc of the 10th annual
SIGCSE conference on innovation and technology in computer
science education. Caparica, Portugal. ACM
24. McCarthy J (1960) Recursive functions of symbolic expressions
and their computation by machine I. J ACM 3:184–195
25. Macal C (2004) Agent-based modeling and social simulation
with Mathematica and MATLAB. In: Macal C, Sallach D, North
M (eds) Proc of the agent 2004 conference on social dynamics:
interaction, reflexivity and emergence. Argonne National Lab-
oratory, Argonne
26. Macal C, Howe T (2005) Linking repast to computational math-
ematics systems: Mathematica and MATLAB. In: Macal C, Sal-
lach D, North M (eds) Proc of the agent 2005 conference

148 A
Agent Based Modeling, Large Scale Simulations
on generative social processes, models, and mechanisms. Ar-
gonne National Laboratory, Argonne
27. Macal C, North M (2007) Tutorial on Agent-based Modeling
and Simulation: Desktop ABMS. In: Henderson SG, Biller B,
Hsieh MH, Shortle J, Tew JD, Barton RR (eds) Proceedings
of the 2007 Winter Simulation Conference, December 9-12,
2007, pp 95–106, http://www.informs-sim.org/wsc07papers/
011.pdf
28. Minar N, Burkhart R, Langton C, Askenazi M (1996) The swarm
simulation system: a toolkit for building multi-agent sim-
ulations. Available at http://alumni.media.mit.edu/~nelson/
research/swarm/
29. North M, Macal C (2007) Managing business complexity: dis-
covering strategic solutions with agent-based modeling and
simulation. Oxford, New York
30. North M, Howe T, Collier N, Vos J (2005) Repast simphony run-
time system. In: Macal C, North M, Sallach D (eds) Proc of the
agent 2005 conference on generative social processes, mod-
els, and mechanisms. Argonne National Laboratory, Argonne
31. North M, Collier N, Vos R (2006) Experiences creating three
implementations of the repast agent modeling toolkit. ACM
Trans Model Comput Simul 16(1):1–25. ACM. New York
32. North M, Tatara E, Collier N, Ozik J (2007) Visual agent-based
model development with repast simphony. In: Macal C, North
M, Sallach D (eds) Proc of the agent 2007 conference on com-
plex interaction and social emergence. Argonne National Lab-
oratory, Argonne
33. North M, Howe T, Collier N, Tatara E, Ozik J, Macal C (2008)
Search and emergence in agent-based models. In: Agent-
based societies: Societal and cultural interactions. IGI Global
Publishing, New York
34. Pearson D, Boudarel M-R (2001) Pair interactions: real and per-
ceived attitudes. J Artif Soc Soc Simul 4(4). Available at http://
www.soc.surrey.ac.uk/JASSS/4/4/4.html
35. Reynolds J (1998) Definitional Interpreters for Higher-Order
Programming Languages. In: Higher-Order and Symbolic
Computation. Kluwer, Boston, pp 363–397
36. Sedgewick R (1988) Algorithms, 2nd edn. Addison-Wesley,
Reading, pp 657
37. Springer G, Freeman D (1989) Scheme and the art of program-
ming. McGraw-Hill, New York
38. Stevens W, Meyers G, Constantine L (1974) Structured design.
IBM Syst J 2
39. Stroustrup B (2008) Bjarne Stroustrup’s FAQ. Available at
http://www.research.att.com/~bs/bs_faq.html#invention
40. Swarm Development Group (2008) SDG home page. Available
at http://www.swarm.org/
41. Tatara E, North M, Howe T, Collier N, Vos J (2006) An Introduc-
tion to Repast Simphony Modeling using a simple Predator-
Prey Example. In: Proceedings of the Agent 2006 Conference
on Social Agents: Results and Prospects. Argonne National
Laboratory, Argonne
42. Thorngate W (2000) Teaching social simulation with MATLAB.
J Artif Soc Soc Simul 3(1). Available at http://www.soc.surrey.
ac.uk/JASSS/3/1/forum/1.html
43. Thoyer S,Morardet S, Rio P, Simon L, Goodhue R, Rausser G
(2001) A bargaining model to simulate negotiations between
water users. J Artif Soc Soc Simul 4(2). Available at http://www.
soc.surrey.ac.uk/JASSS/4/2/6.html
44. ROAD (2008) Repast home page. Available at http://repast.
sourceforge.net/
45. van Roy P, Haridi S (2004) Concepts, techniques, and models of
computer programming. MIT Press, Cambridge
46. Wang Z, Thorngate W (2003) Sentiment and social mitosis: im-
plications of Heider’s balance theory. J Artif Soc Soc Simul 6(3).
Available at http://jasss.soc.surrey.ac.uk/6/3/2.html
47. Watson D (1989) High-level languages and their compilers. Ad-
dison-Wesley, Wokingham
48. Wilensky U (1999) NetLogo. Center for Connected Learn-
ing and Computer-Based Modeling, Northwestern University,
Evanston, IL. http://ccl.northwestern.edu/netlogo/
49. Wolfram Research (2008) Mathematica home page. Available
at http://www.wolfram.com/
Agent Based Modeling,
Large Scale Simulations
HAZEL R. PARRY
Central Science Laboratory, York, UK
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Large Scale Agent Based Models:
Guidelines for Development
Parallel Computing
Example
Future Directions
Acknowledgments
Bibliography
Glossary
Agent A popular deﬁnition of an agent, particularly in
AI research, is that of Wooldridge [45], pp. 29: “an
agent is a computer system that is situated in some en-
vironment, and that is capable of autonomous action
in this environment in order to meet its design ob-
jectives”. In particular, it is the autonomy, ﬂexibility,
inter-agent communication, reactivity and proactive-
ness of the agents that distinguishes the paradigm and
gives power to agent-based models and multi-agent
simulation [15,21]. Multi-agent systems (MAS) com-
prise of numerous agents, which are given rules by
which they act and interact with one another to achieve
a set of goals.
Block mapping A method of partitioning an array of el-
ements between nodes of a distributed system, where
the array elements are partitioned as evenly as possi-
ble into blocks of consecutive elements and assigned
to processors. The size of the blocks approximates to

Agent Based Modeling, Large Scale Simulations
A
149
the number of array elements divided by the number
of processors.
Complexity Complexity and complex systems pertain to
ideas of randomness and irregularity in a system,
where individual-scale interactions may result in either
very complex or surprisingly simple patterns of behav-
ior at the larger scale. Complex agent-based systems
are therefore usually made up of agents interacting in
a non-linear fashion. The agents are capable of gen-
erating emergent behavioral patterns, of deciding be-
tween rules and of relying upon data across a variety
of scales. The concept allows for studies of interaction
between hierarchical levels rather than ﬁxed levels of
analysis.
Cyclic mapping A method of partitioning an array of el-
ements between nodes of a distributed system, where
the array elements are partitioned by cycling through
each node and assigning individual elements of the ar-
ray to each node in turn.
Grid Computer ‘Grids’ are comprised of a large num-
ber of disparate computers (often desktop PCs) that
are treated as a virtual cluster when linked to one an-
other via a distributed communication infrastructure
(such as the internet or an intranet). Grids facilitate
sharing of computing, application, data and storage
resources. Grid computing crosses geographic and
institutional boundaries, lacks central control, and is
dynamic as nodes are added or removed in an unco-
ordinated manner. BOINC computing is a form of dis-
tributed computing is where idle time on CPUs may
be used to process information (http://boinc.berkeley.
edu/).
Ising-type model Ising-type models have been primarily
used in the physical sciences. They simulate behav-
ior in which individual elements (e. g., atoms, animals,
social behavior, etc.) modify their behavior so as to
conform to the behavior of other individuals in their
vicinity. Conway’s Game of Life is a Ising-type model,
where cells are in one of two states: dead or alive. In bi-
ology, the technique is used to model neural networks
and ﬂocking birds, for example.
Message passing (MP) Message passing (MP) is the prin-
ciple way by which parallel clusters of machines are
programmed. It is a widely-used, powerful and general
method of enabling distribution and creating eﬃcient
programs [30]. Key advantages of using MP architec-
tures are an ability to scale to many processors, ﬂexibil-
ity, ‘future-prooﬁng’ of programs and portability [29].
Message passing interface (MPI) A computing standard
that is used for programming parallel systems. It is im-
plemented as a library of code that may be used to en-
able message passing in a parallel computing system.
Such libraries have largely been developed in C and
Fortran, but are also used with other languages such as
Java (MPIJava http://www.hpjava.org). It enables de-
velopers of parallel software to write parallel programs
that are both portable and eﬃcient.
Multiple instruction multiple data (MIMD)
Parallelization where diﬀerent algorithms are applied
to diﬀerent data items on diﬀerent processors.
Parallel computer architecture A parallel computer ar-
chitecture consists of a number of identical units that
contain CPUs (Central Processing Units) which func-
tion as ordinary serial computers. These units, called
nodes, are connected to one another (Fig. 1). They
may transfer information and data between one an-
other (e. g. via MPI) and simultaneously perform cal-
culations on diﬀerent data.
Single instruction multiple data (SIMD)
SIMD
tech-
niques exploit data level parallelism: when a large mass
of data of a uniform type needs the same instruc-
tion performed on it. An example is a vector or array
processor. An application that may take advantage of
SIMD is one where the same value is being added (or
subtracted) to a large number of data points.
Vector computer/vector processor Vector
computers
contain a CPU designed to run mathematical opera-
tions on multiple data elements simultaneously (rather
than sequentially). This form of processing is essen-
tially a SIMD approach. The Cray Y-MP and the Con-
vex C3880 are two examples of vector processors used
for supercomputing in the 1980s and 1990s. Today,
most recent commodity CPU designs include some
vector processing instructions.
Definition of the Subject
‘Large scale’ simulations in the context of agent-based
modelling are not only simulations that are large in terms
of the size of the simulation (number of agents simulated),
but they are also complex. Complexity is inherent in agent-
based models, as they are usually composed of dynamic,
heterogeneous, interacting agents. Large scale agent-based
models have also be referred to as ‘Massively Multi-agent
Systems (MMAS)’ [18]. MMAS is deﬁned as “ ‘beyond
resource limitation”: the number of agents exceeds lo-
cal computer resources, or the situations are too com-
plex to design/program given human cognitive resource
limits’ [18], Preface. Therefore, for agent-based modelling
‘large scale’ is not simply a size problem, it is also a prob-
lem of managing complexity to ensure scalability of the
agent model.

150 A
Agent Based Modeling, Large Scale Simulations
Agent Based Modeling, Large Scale Simulations, Figure 1
A network with interconnected separate memory and processors (after [30], pp. 19)
Multi-agent simulation models increase in scale as
the modeller requires many agents to investigate whole
system behavior, or the modeller wishes to fully exam-
ine the response of a single agent in a realistic context.
Two key problems may be introduced by increasing the
scale of a multi-agent system: (1) Computational resources
limit the simulation time and/or data storage capacity and
(2) Agent model analysis may become more diﬃcult. Dif-
ﬁculty in analyzing the model may be due to the model
system having a large number of complex components or
to memory for model output storage being restricted by
computer resources.
Introduction
Many systems that are now simulated using agent-based
models are systems where agent numbers are large and
potentially complex. These large scale models are con-
structed under a number of diverse scientiﬁc disciplines,
with diﬀering histories of agent simulation and method-
ologies emerging with which to deal with large scale simu-
lations: for example molecular physics, social science (e. g.
crowd simulation, city growth), telecommunications, ecol-
ogy and military research. The primary methodology to
emerge is parallel computing, where an agent model is dis-
tributed across a number of CPUs to increase the memory
and processing power available to the simulation. How-
ever, there are a range of potential methods with which
to simulate large numbers of agents. Some suggestions are
listed in Table 1.
The simplest solution to enable larger scale agent sim-
ulation is usually to improve the computer hardware that
is used and run the model on a server or invest in a more
powerful PC. However, this option may be too costly or
may not provide enough scaling so other options may then
be considered. Another simple solutions may be to reduce
the number of agents or to revert to a simpler modelling
approach such as a population model, but both of these
solutions would signiﬁcantly alter the model and the phi-
losophy behind the model, probably not addressing the re-
search question that the model was initially constructed
for. There are a number of unique advantages and insights
that may be gained from an agent-based approach. Agent
simulations are often constructed to enable the analysis of
emergent spatial patterns and individual life histories. The
realism of an agent-based approach may be lost by using
a simpler modelling technique.
The structure of agent simulations (often with asyn-
chronous updating and heterogeneous data types) would
mean that running a simulation on a vector computer may
make little diﬀerence to the simulation performance. This
is because an agent model typically has few elements that
could take advantage of SIMD: rarely the same value will
be added (or subtracted) to a large number of data points.
Vector processors are less successful when a program does
not have a regular structure, and they don’t scale to ar-
bitrarily large problems (the upper limit on the speed of
a vector program will be some multiple of the speed of the
CPU [30]).
Another relatively simple option is to implement an
aggregation of the individual agents into ‘super-agents’,
such as the ‘super-individual’ approach of Scheﬀer et
al. [35]. The basic concept of this approach is shown in
Fig. 2. These ‘super-agents’ are formed from individual
agents that share the same characteristics, such as age and
sex. However, it may not be possible to group agents in
a simulation in this way and, importantly, this method has
been proven ineﬀective in a spatial context [31,32].
The most challenging solution, to reprogram the
model in parallel, is a popular solution due to the short-
comings of the other approaches outlined above. A paral-
lel solution may also have some monetary cost and require

Agent Based Modeling, Large Scale Simulations
A
151
Agent Based Modeling, Large Scale Simulations, Table 1
Potential solutions to implement when faced with a large number of agents to model
Solution
Pro
Con
Reduce the number of agents in
order for model to run
No reprogramming of model
Unrealistic population.
Alters model behavior
Revert to a population based
modelling approach
Could potentially handle any number of
individuals
Lose insights from agent approach.
Unsuitable for research questions.
Construction of entirely new model
(non-agent-based)
Invest in an extremely powerful
computer
No reprogramming of model
High cost
Run the model on a vector
computer
Potentially more efficient as more calcu-
lations may be performed in a given time
This approach only works more efficiently with SIMD,
probably unsuitable for agent-based models
Super-individuals [35]
Relatively simple solution, keeping model
formulation the same
Reprogramming of model.
Inappropriate in a spatial context [31,32]
Invest in a powerful computer
network and reprogram the
model in parallel
Makes available high levels of memory
and processing power
High cost.
Advanced computing skills required for restructuring
of model
Agent Based Modeling, Large Scale Simulations, Figure 2
‘Super-agents’: Grouping of individuals into single objects that represent the collective
advanced computing skills to implement, but it can poten-
tially greatly increase the scale of the agent simulation. Ex-
tremely large scale object-oriented simulations that sim-
ulate individual particles on massively parallel computer
systems have been successfully developed in the physical
sciences of ﬂuid dynamics, meteorology and materials sci-
ence. In the early 1990s, work in the ﬁeld of molecular-
dynamics (MD) simulations proved parallel platforms to
be highly successful in enabling large-scale MD simula-
tion of up to 131 million particles [26]. Today the same
code has been tested and used to simulate up to 320
billion atoms on the BlueGene/L architecture containing
131,072 IBM PowerPC440 processors [22]. These simu-
lations include calculations based upon the short-range
interaction between the individual atoms, thus in some
ways approximate to agent simulations although in other
ways molecular-dynamic simulations lack the complexity
of most agent-based models (see Table 2).
There are signiﬁcant decisions to be made when con-
sidering the application of a computing solution such as

152 A
Agent Based Modeling, Large Scale Simulations
Agent Based Modeling, Large Scale Simulations, Table 2
Key elements of a ‘bottom-up’ simulation that may affect the way in which it may scale. Agent simulations tend to be complex (to
the right of the table), though may have some elements that are less complex, such as local or fixed interactions
Element
Least complex
! Most complex
Spatial structure
Aspatial or Lattice of cells (1d, 2d or 3d +)
Continuous space
Internal state
Simple representation (boolean true or
false)
Complex representation (many states from an
enumerable set) or fuzzy variable values
Agent heterogeneity
No
Yes
Interactions
Local and fixed (within a neighborhood)
Multiple different ranges and stochastic
Synchrony of model updates
Synchronous update
Not synchronous: asynchrony due to state-transition rules
parallel programming to solve the problem of large num-
bers of agents. In addition to the issue of reprogramming
the model to run on a parallel computer architecture, it
is also necessary to consider the additional complexity of
agents (as opposed to atoms), so that programming mod-
els and tools facilitate the deployment, management and
control of agents in the distributed simulation [10]. For ex-
ample, distributed execution resources and timelines must
be managed, full encapsulation of agents must be enforced,
and tight control over message-based multi-agent inter-
actions is necessary [10]. Agent models can vary in com-
plexity, but most tend to be complex especially in the key
model elements of spatial structure and agent heterogene-
ity. Table 2 gives an indication of the relative complexity of
model elements found in models that focus on individual
interactions (which encompasses both multi-agent models
and less complex, ‘Ising’-type models).
The following sections detail guidelines for the devel-
opment of a large scale agent-based model, highlighting in
particular the challenges faced in writing large scale, high
performance Agent based Modelling (ABM) simulations
and giving a suggested development protocol. Following
this, an example is given of the parallelization of a simple
agent-based model, showing some of the advantages but
also the pitfalls of this most popular solution. Key chal-
lenges, including diﬃculties that may arise in the analysis
of agent-based models at a large scale, are highlighted. Al-
ternative solutions are then discussed and some conclu-
sions are drawn on the way in which large scale agent-
based simulation may develop in coming years.
Large Scale Agent Based Models:
Guidelines for Development
Key Considerations
There is no such thing as a standard agent-based model,
or even a coherent methodology for agent simulation
development (although recent literature in a number of
ﬁelds sets out some design protocols, e. g. Gilbert [11] and
Grimm et al. [12]). Thus, there can be no standard method
to develop a large scale agent-based model. However, there
are certain things to consider when planning to scale up
a model. Some key questions to ask about the model are as
follows:
1. What program design do you already have and what is
the limitation of this design?
(a) What is it the memory footprint for any existing
implementation?
(b) What are your current run times?
2. What are your scaling requirements?
(a) How much do you need to scale now?
(b) How far do you need to scale eventually?
(c) How soon do you need to do it?
3. How simple is your model and how is it structured?
4. What are your agent complexities?
5. What are your output requirements?
The ﬁrst question is to identify the limitations in the pro-
gram design that you are using and to focus on the pri-
mary ‘bottlenecks’ in the model. These limitations will ei-
ther be due to memory or speed (or perhaps both). There-
fore it will be necessary to identify the memory footprint
for your existing model, and analyze run times, identifying
where the most time is taken or memory used by the sim-
ulation. It is primarily processor power that controls the
speed of the simulation. Runtime will also increase mas-
sively once Random Access Memory (RAM) is used up,
as most operating systems will resort to virtual memory
(i. e. hard drive space), thus a slower mechanism with me-
chanical parts rather than solid-state technology engages.
At this stage, it may be such that simple adjustments to the
code may improve the scalability of the model. However,
if the code is eﬃcient, other solutions will then need to be
sought.
The second question is how much scaling is actually
necessary for the model. It may be such that a simple or
interim solution (e. g. upgrading computer hardware) may
be acceptable whilst only moderate scaling is required,

Agent Based Modeling, Large Scale Simulations
A
153
but longer term requirements should also be considered –
a hardware upgrade may be a quick ﬁx but if the model
may eventually be used for much larger simulations it is
necessary to plan for the largest scaling that will potentially
be required.
The third question, relating to model simplicity and
structure, is key to deciding a methodology that can be
used to scale a model up. A number of factors will aﬀect
whether a model will be easy to distribute in parallel, for
example. These include whether the model iterates at each
time step or is event driven, whether it is aspatial or spatial
and the level/type of agent interaction (both with one an-
other and with the environment). More detail on the im-
plications of these factors is given in Sect. “Parallel Com-
puting”.
Agent complexity, in addition to model structure, may
limit the options available for scaling up a model. For ex-
ample, a possible scaling solution may be to group individ-
ual agents together as ‘super-individuals’ [35]. However, if
agents are too complex it may not be possible to determine
a simple grouping system (such as by age), as agent be-
havior may be inﬂuenced heavily by numerous other state
variables.
Output requirements are also important to consider.
These may already be limiting the model, in terms of
memory for data storage. Even if they are not currently
limiting the model in this way, once the model is scaled
up output data storage needs may be an issue, for exam-
ple, if the histories of individual agents need to be stored.
In addition, the way that output data is handled by the
model may be altered if the model structure is altered (e. g.
if agents are grouped together output will be at an aggre-
gate level). Thus, an important consideration is to ensure
that output data is comparable to the original model and
that it is feasible to output once the model structure is al-
tered.
A Protocol
In relation to the key considerations highlighted above,
a simple protocol for developing a large scale agent-based
simulation can be deﬁned as follows:
1. Optimize existing code.
2. Clearly identify scaling requirements (both for now and
in the future).
3. Consider simple solutions ﬁrst (e. g. a hardware up-
grade).
4. Consider more challenging solutions.
5. Evaluate the suitability of the chosen scaling solution
on a simpliﬁed version of the model before implement-
ing on the full model.
The main scaling solution to implement (e. g. from Ta-
ble 1) is deﬁned by the requirements of the model. Imple-
mentation of more challenging solutions should be done
in stages, where perhaps a simpliﬁed version of the model
is implemented on a larger scale. Agent simulation devel-
opment should originate with a local, ﬂexible ‘prototype’
and then as the model development progresses and sta-
bilizes larger scale implementations can be experimented
with [10]. This is necessary for a parallel implementation
of a model, for example, as a simpliﬁed model enables and
assessment of whether it is likely to provide the desired
improvements in model eﬃciency. This is particularly the
case for improvements in model speed, as this depends on
improved processing performance that is not easily calcu-
lated in advance.
Parallel Computing
Increasing the capacity of an individual computer in terms
of memory and processing power has limited ability to
perform large scale agent simulations, particularly due to
the time the machine would take to run the model using
a single processor. However, by using multiple processors
and a mix of distributed and shared memory working si-
multaneously, the scale of the problem for each individual
computer is much reduced. Subsequently, simulations can
run in a fraction of the time that would be taken to perform
the same complex, memory intensive, operations. This is
the essence of parallel computing. ‘Parallel computing’ en-
compasses a wide range of computer architectures, from
a HPC (High performance computing) Linux box, to ded-
icated multi-processor/multi-core systems (such as a Be-
owulf cluster), super clusters, local computer clusters or
Grids and public computing facilities (e. g. Grid comput-
ers, such as the White Rose Grid, UK http://www.wrgrid.
org.uk/). The common factor is that these systems consist
of a number of interconnected ‘nodes’ (processing units),
that may perform simultaneous calculations on diﬀerent
data. These calculations may be the same or diﬀerent,
depending whether a ‘Single Instruction Multiple Data’
(SIMD) or ‘Multiple Instruction Multiple data’ (MIMD)
approach is implemented.
In terms of MAS, parallel computing has been used to
develop large scale agent simulations in a number of dis-
ciplines. These range from ecology, e. g. [1,17,27,40,41,42,
43] and biology, e. g. [5,7] to social science, e. g. [38] and
computer science, e. g. [34], including artiﬁcial intelligence
and robotics, e. g. [3,4].
Several key challenges arise when implementing an
agent model in parallel, which may aﬀect the increase in
performance achieved. These include load balancing be-

154 A
Agent Based Modeling, Large Scale Simulations
tween nodes, synchronizing events to ensure causality,
monitoring of the distributed simulation state, managing
communication between nodes and dynamic resource al-
location [39]. Good load balancing and inter-node com-
munication with event synchronisation are central to the
development of an eﬃcient parallel simulation, and are
further discussed below.
Load Balancing
In order to ensure the most eﬃcient use of memory and
processing resources in a parallel computing system the
data load must be balanced between processors and the
work load equally distributed. If this is not the case then
one computer may be idle as others are working, result-
ing in time delays and ineﬃcient use of the system’s ca-
pacity. There are a number of ways in which data can be
‘mapped’ to diﬀerent nodes and the most appropriate de-
pends on the model structure. Further details and exam-
ples are given in Pacheco [30], including ‘block mapping’
and ‘cyclic mapping’. An example of ‘block mapping’ load
balancing is given below, in Sect. “Example”.
In many simulations the computational demands on
the nodes may alter over time, as the intensity of the
agents’ or environment’s processing requirements varies
on each node over time. In this case, dynamic load bal-
ancing techniques can be adopted to further improve the
parallel model performance. For example, Jang [19] and
Jang and Agha [20], use a form of dynamic load balancing
with object migration they term “Adaptive Actor Archi-
tecture”. Each agent platform monitors the workload of its
computer node and the communication patterns of agents
executing on it in order to redistribute agents according to
their communication localities as agent platforms become
overloaded. However, this approach does introduce addi-
tional processing overheads, so is only worth implement-
ing for large scale agent simulations where some agents
communicate with one another more intensely than other
agents (communication locality is important) or commu-
nication patterns are continuously changing so static agent
allocation is not eﬃcient.
Communication Between Nodes
It is important to minimize inter-node communication
when constructing a parallel agent simulation, as this may
slow the simulation down signiﬁcantly if the programmer
is not careful [37,38]. The structure of the model itself
largely determines the way in which data should be split
and information transferred between nodes to maximize
eﬃciency. Agent simulations generally by deﬁnition act
spatially within an environment. Thus, an important ﬁrst
consideration is whether to split the agents or the environ-
ment between nodes. The decision as to whether to split
the agents between processors or elements of the environ-
ment such as grid cells largely depends upon the complex-
ity of the environment, the mobility of the agents, and the
number of interactions between the agents. If the environ-
ment is relatively simple (thus information on the whole
environment may be stored on all nodes), it is probably
most eﬃcient to distribute the agents. This is particularly
the case if the agents are highly mobile, as a key prob-
lem when dividing the environment between processors
is the transfer of agents or information between proces-
sors. However, if there are complex, spatially deﬁned in-
teractions between agents, splitting agents between nodes
may be problematic, as agents may be interacting with
other agents that are spatially local in the context of the
whole simulation but are residing on diﬀerent processors.
Therefore conversely, if the agents are not very mobile but
have complex, local interactions and/or the agents reside
in a complex environment, it is probably best to split the
environment between nodes [25]. Further eﬃciency may
be achieved by clustering the agents which communicate
heavily with each other [37].
In models where there is high mobility and high inter-
action it is often possible, especially for ecological models,
to ﬁnd a statistical commonality that can be used as a re-
placement for more detailed interaction. For example, as
will be shown in our example, if the number of local agent
interactions is the only important aspect of the interac-
tions, a density map of the agents, transferred to a central
node, aggregated and redistributed, might allow agents to
be divided between nodes without the issue of having to do
detailed inter-agent communication between nodes a large
number of times.
The way in which the simulation iterates may inﬂu-
ence the approach taken when parallelizing the model.
The model may update synchronously at a given time step
or asynchronously (usually because the system is event-
driven). In addition, agents may update asynchronously
but the nodes may be synchronized at each time step or key
model stage. Asynchronous updating may be a problem
if there is communication between nodes, as some nodes
may have to wait for others to ﬁnish processes before com-
munication takes place and further processing is possi-
ble, resulting in blocking (see below). Communication be-
tween nodes then becomes highly complex [44]. It is im-
portant that messages communicating between agents are
received in the correct order, however a common prob-
lem in distributed simulations is ensuring that this is so
as other factors, such as latency in message transmis-
sion across the network, may aﬀect communication [44].

Agent Based Modeling, Large Scale Simulations
A
155
A number of time management mechanisms exist that
may be implemented to manage message passing in order
to ensure eﬀective node to node communication, e. g. [8].
Blocking and Deadlocking
Deadlock occurs when two or more processes are wait-
ing for communication from one of the other processes.
When programming a parallel simulation it is important
to avoid deadlock to ensure the simulation completes. The
simplest example is when two processors are programmed
to receive from the other processor before that processor
has sent. This may be simply resolved by changing the or-
der that tasks are executed, or to use ‘non-blocking’ mes-
sage passing. Where blocking is used, processing on nodes
waits until a message is transmitted. However, when ‘non-
blocking’ is used, processing continues even if the message
hasn’t been transmitted yet. The use of a non-blocking
MPI may reduce computing times, and work can be per-
formed while communication is in progress.
Example
To demonstrate some of the beneﬁts and pitfalls of par-
allel programming for a large scale agent-based model,
a simple example is given here. This summarizes a simpli-
ﬁed agent-based model of aphid population dynamics in
agricultural landscapes of the UK, which was parallelized
to cope with millions of agents, as described in detail in
Parry [31], Parry and Evans [32] and Parry et al. [33].
A key problem with the original, non-parallel, aphid
simulation was that it was hindered by memory require-
ments, which were far larger than could be accommodated
at any individual processing element. This is a common
computing problem [6]. The data storage required for each
aphid object in a landscape scale simulation quickly ex-
ceeded the storage capacity of a PC with up to 2097 MB
of RAM. The combined or ‘virtual shared’ memory of sev-
eral computers was used to cope with the amount of data
needed, using a Single Instruction Multiple-Data approach
(SIMD).
Message-passing techniques were used to transfer in-
formation between processors, to distribute the agents in
the simulation across a Beowulf cluster (a 30-node dis-
tributed memory parallel computer). A Message-passing
Interface (MPI) for Java was used, MPIJava (http://www.
hpjava.org). ‘MPIJava wraps around the open-source’
open-source native MPI ‘LAM’ (http://www.lam-mpi.
org/). Further details on the methods used to incorporate
the MPI into the model are given in Parry [31] and Parry
et al. [33].
Eﬀective parallelization minimizes the passing of in-
formation between nodes, as it is processor intensive. In
the example model, only the environment object and in-
formation on the number of agents to create on each node
are passed from a single control node to each of the other
nodes in the cluster, and only density information is re-
turned to the control node for redistribution and display.
The control node manages the progress of the model, acts
as a central communication point for the model and han-
dles any code that may not be distributed to all nodes
(such as libraries from an agent toolkit or a GUI). Struc-
turing a model without a control node is possible, or the
control node may also be used to process data, depending
on the requirements of the simulation. Transfer of den-
sity values, rather than agents, signiﬁcantly reduced the
computational overheads for message passing between the
nodes. The model was simple enough that speciﬁc inter-
agent communication between nodes was not necessary.
Even distribution of data between nodes was achieved
by splitting immigrant agents evenly across the system,
with each node containing information on the environ-
ment and local densities passed from the control node. The
number of immigrants to be added to each node was cal-
culated by a form of ‘block mapping’, pp. 35 in [30], which
partitioned the number of immigrants into blocks which
were then assigned to each node. So, if there were three
nodes (n D 3) and thirteen immigrants (i D 13), the im-
migrants mapped to each node would be as follows:
i0; i1; i2; i3 ! n1
i4; i5; i6; i7 ! n2
i8; i9; i10; i11; i12 ! n3 :
As thirteen does not divide evenly by three, the thirteenth
agent is added to the ﬁnal node.
Beneﬁts
Simulation runtime and memory availability was greatly
improved by implementing the simple aphid model in par-
allel across a large number of nodes. The greatest improve-
ment in simulation runtime and memory availability was
seen when the simulation was run across the maximum
number of nodes (25) (Figs. 3 and 4). The largest improve-
ment in speed given by the parallel model in compari-
son to the non-parallel model is when more than 500,000
agents are run across twenty-ﬁve nodes, although the par-
allel model is slower by comparison for lower numbers.
This means that additional processing power is required
in the parallel simulation compared to the original model,
such that only when very large numbers of agents are run
does it become more eﬃcient.

156 A
Agent Based Modeling, Large Scale Simulations
Agent Based Modeling, Large Scale Simulations, Figure 3
Plot of the mean maximum memory used (per node) against number of agents for the model: comparison between simulations using
2, 5 and 25 nodes and the non-parallel model (single processor)
Agent Based Modeling, Large Scale Simulations, Figure 4
Plot of the percentage speed up (per node) from the non-parallel model against number of agents for the model: comparison be-
tween simulations using 2, 5 and 25 nodes and the non-parallel model (single processor)
Pitfalls
Although there are clear beneﬁts of distributing the exam-
ple simulation across a large number of nodes, the results
highlight that the parallel approach is not always more
eﬃcient than the original single processor implementa-
tion of a model. In the example, the two node simulation
used more memory on the worker node than the non-
parallel model when the simulation had 100,000 agents
or above. This is due to additional memory requirements
introduced by message passing and extra calculations re-
quired in the parallel implementation (which are less sig-
niﬁcant when more nodes are used as these requirements
remain relatively constant).
The results also highlight that adding more proces-
sors does not necessarily increase the model speed. The
example model shows that for simulations run on two
nodes (one control node, one worker node) the simula-
tion takes longer to run in parallel compared to the non-
parallel model. Message passing time delay and the modi-
ﬁed structure of the code are responsible. The greatest im-
provement in speed is when more than 500,000 agents are
run across twenty-ﬁve nodes, however when lower num-
bers of nodes are used the relationship between the num-

Agent Based Modeling, Large Scale Simulations
A
157
ber of nodes and speed is complex: for 100,000 agents
ﬁve nodes are faster than the non-parallel model, but for
500,000 the non-parallel model is faster. Overall, these re-
sults suggest that when memory is suﬃcient on a single
processor, it is unlikely to ever be eﬃcient to parallelize
the code, as when the number of individuals was low the
parallel simulation took longer and was less eﬃcient than
the non-parallel model run on a single node.
This demonstrates that in order to eﬀectively paral-
lelize an agent model, the balance between the advan-
tage of increasing the memory availability and the cost of
communication between nodes must be assessed. By fol-
lowing an iterative development process as suggested in
Sect. “A Protocol”, the threshold below which paralleliza-
tion is not eﬃcient and whether this option is suitable for
the model should become apparent in the early stages of
model development. Here, the simpliﬁed study conﬁrmed
the value of further development to build a parallel version
of the full model.
Future Directions
Complexity and Model Analysis
In addition to the processing and data handling issues
faced by large scale agent-based simulations, as agent sim-
ulations increase in scale and complexity, model analysis
may become more diﬃcult and the system may become
intractable. There is no clearly deﬁned way of dealing with
increased diﬃculties of model analysis introduced by the
greater complexity of large scale agent-based models, in
the same way that there is no clear way to deal with the
complexity inherent in most agent-based systems. How-
ever, some guidelines to model analysis have recently been
developed for agent simulations, e. g. [12], and some sug-
gestions are put forward on ways in which agent simu-
lation complexity may be described, for example by the
use of a model ‘overview’ , ‘design concepts’ and ‘details’
(ODD) protocol for agent model description. In particu-
lar, the protocol requires that design concepts are linked to
general concepts identiﬁed in the ﬁeld of Complex Adap-
tive Systems [13], including emergence and stochasticity.
To address issues of complexity in a large scale agent
simulation, one possibility is to include an additional layer
of structuring, ‘organizational design’ , into the system.
This is where the peer group of an agent, it’s roles and re-
sponsibilities are assigned in the model and made explicit,
pp. 121 in [16]. In general agent simulations already have
an implicit organization structure; Horling and Lesser [16]
argue that explicit organizational design highlights hidden
ineﬃciencies in the model and allows the model to take
full advantages of the resources available.
Grid Computing
For large scale agent-based models, many researchers have
reached the limitations of ordinary PCs. However, there
are ‘critical tensions’ [10] between agent simulations built
on ordinary PCs and heterogeneous, distributed paral-
lel programming approaches. The architecture of a dis-
tributed system is very diﬀerent to that of an ordinary
PC, thus to transfer a simulation to a computer cluster
additional system properties must be taken into account,
including management of the distribution of the simula-
tion and concurrency [10]. This is particularly apparent
when parallelization is attempted on heterogeneous, non-
dedicated systems such as a public Grid. The Grid may of-
fer a number of advantagesfor large scale agent-based sim-
ulation, such as collaboration between modellers, access
to resources and geographically distributed datasets [47].
However, in such systems, issues of infrastructure reliabil-
ity, functional completeness and the state of documenta-
tion for some kinds of environments exist [10]. In order to
use such a system the ‘fundamental’ issue of partial failures
must be addressed (e. g. with a dynamic agent replication
strategy [14]).
Dissemination of Techniques
For parallel computing as a solution to large scale
agent-based simulation, there is an interesting and use-
ful future challenge to develop user friendly, high per-
formance, versatile hardware architectures and software
systems. Many developers of agent simulations are not
computer scientists by training, and still rely upon nu-
merous agent toolkits for simulation development (e. g.
Swarm and Repast). Automatic distribution of agents to
whatever resources are available would be a great tool
for many agent software developers. Therefore, perhaps
the greatest challenge would be to develop a system
that would allow for paralellisation to be performed in
an agent simulation automatically, where agents may be
written in a high-level language and could be automati-
cally partitioned to nodes in a network. One example of
an attempt to achieve this is Graphcode (http://parallel.
hpc.unsw.edu.au/rks/graphcode/). Based upon MPI, it
maps agent-based models onto parallel computers, where
agents are written based upon their graph topography
to minimize communication overhead. Another example
is HLA_GRID_Repast [47], ‘a system for executing large
scale distributed simulations of agent-based systems over
the Grid’ , for users of the popular Repast agent toolkit.
HLA_GRID_Repast is a middleware layer which enables
the execution of a federation of multiple interacting in-
stances of Repast models across a grid network with a High

158 A
Agent Based Modeling, Large Scale Simulations
Level Architecture (HLA). This is a ‘centralized coordina-
tion approach’ to distributing an agent simulation across
a network [39]. Examples of algorithms designed to en-
able dynamic distribution of agent simulations are given
in Scheutz and Schermerhorn [36].
Although parallel computing is often the most eﬀec-
tive way of handling large scale agent-based simulations,
there are still some signiﬁcant obstacles to the use of par-
allel computing for MAS. As shown with the simple ex-
ample given here, this may not always be the most eﬀec-
tive solution depending upon the increase in scale needed
and the model complexity. Other possible methods were
suggested in Sect. “Introduction”, but these may also be
unsuitable. Another option could be to deconstruct the
model and simplify only certain elements of the model
using either parallel computing or one of the other solu-
tions suggested in Sect. “Introduction”. Such a ‘hybrid’ ap-
proach is demonstrated by Zhang and Lui [46], who com-
bine equation-based approaches and multi-agent simula-
tion with a Cellular Automata to simulate the complex
interactions in the process of human immune response
to HIV. The result is a model where equations are used
to represent within-site processes of HIV infection, and
agent-based simulation is used to represent the diﬀusion
of the virus between sites.
It is therefore important to consider primarily the vari-
ous ways in which the model may be altered, hybridized or
simpliﬁed yet still address the core research questions, be-
fore investing money in hardware or investing time in the
development of complex computational solutions. Making
the transition from a serial application to a parallel version
is a process that requires a fair degree of formalism and
program restructuring, so is not to be entered into lightly
without exploring the other options and the needs of the
simulation ﬁrst.
Overall, it is clear that disparate work is being done in
a number of disciplines to facilitate large scale agent-based
simulation, and knowledge is developing rapidly. Some of
this work is innovative and highly advanced, yet inaccessi-
ble to researchers in other disciplines who may unaware of
key developments outside of their ﬁeld. This chapter syn-
thesizes and evaluates large scale agent simulation to date,
providing a reference for a wide range of agent simulation
developers.
Acknowledgments
Many thanks to Andrew Evans (Multi-Agent Systems and
Simulation Research Group, University of Leeds, UK) and
Phil Northing (Central Science Laboratory, UK) for their
advice on this chapter.
Bibliography
Primary Literature
1. Abbott CA, Berry MW, Comiskey EJ, Gross LJ, Luh H-K (1997)
Parallel Individual-Based Modeling of Everglades Deer Ecol-
ogy. IEEE Comput Sci Eng 4(4):60–78
2. Anderson J (2000) A Generic Distributed Simulation System
for Intelligent Agent Design and Evaluation. In: Proceedings
of the AI, Simulation & Planning In High Autonomy Systems,
Arizona
3. Bokma A, Slade A, Kerridge S, Johnson K (1994) Engineer-
ing large-scale agent- based systems with consensus. Robot
Comput-Integr Manuf 11(2):81–91
4. Bouzid M, Chevrier V, Vialle S, Charpillet F (2001) Parallel sim-
ulation of a stochastic agent/environment interaction model.
Integr Comput-Aided Eng 8(3):189–203
5. Castiglione F, Bernaschi M, Succi S (1997) Simulating the Im-
mune Response on a Distributed Parallel Computer. Int J Mod
Phys C 8(3):527–545
6. Chalmers A, Tidmus J (1996) Practical Parallel Processing:
An Introduction to Problem Solving in Parallel. International
Thomson Computer Press, London
7. Da-Jun T, Tang F, Lee TA, Sarda D, Krishnan A, Goryachev A
(2004) Parallel computing platform for the agent-based mod-
eling of multicellular biological systems. In: Parallel and Dis-
tributed Computing: Applications and Technologies. Lecture
Notes in Computer Science, vol 3320, pp 5–8
8. Fujimoto RM (1998) Time management in the high level archi-
tecture. Simul 71:388–400
9. Gasser L, Kakugawa K (2002) MACE3J: Fast flexible distributed
simulation of large, large-grain multi-agent systems. In: Pro-
ceedings of AAMAS
10. Gasser L, Kakugawa K, Chee B, Esteva M (2005) Smooth scal-
ing ahead: progressive MAS simulation from single PCs to
Grids. In: Davidsson P, Logan B, Takadama K (eds) Multi-agent
and multi-agent-based simulation. Joint Workshop MABS 2004
New York, NY, USA, July 19, 2004. Springer, Berlin
11. Gilbert N (2007) Agent-based Models (Quantitative applica-
tions in the social siences). SAGE, London
12. Grimm V, Berger U, Bastiansen F, Eliassen S, Ginot V, Giske
J, Goss-Custard J, Grand T, Heinz S, Huse G, Huth A, Jepsen
JU, Jorgensen C, Mooij WM, Muller B, Pe’er G, Piou C, Rails-
back SF, Robbins AM, Robbins MM, Rossmanith E, Ruger N,
Strand E, Souissi S, Stillman RA, Vabo R, Visser U, DeAngelis DL
(2006) A standard protocol for describing individual-based and
agent-based models. Ecol Model 198(1–2):115–126
13. Grimm V, Railsback SF (2005) Individual-based Modeling and
Ecology. Princeton Series in Theoretical and Computational Bi-
ology. Princeton University Press, Princeton, 480 pp
14. Guessoum Z, Briot J-P, Faci N (2005) Towards fault-tolerant
massively multiagent system. In: Ishida T, Gasser L, Nakashima
H (eds) Massively Multi-Agent Systems I: First Interna-
tional Workshop MMAS 2004, Kyoto, Japan, December 2004.
Springer, Berlin
15. Heppenstall AJ (2004) Application of Hybrid Intelligent Agents
to Modelling a Dynamic, Locally Interacting Retail Market. Ph D
thesis, University of Leeds, UK
16. Horling B, Lesser V (2005) Quantitative organizational mod-
els for large-scale agent systems. In: Ishida T, Gasser L,
Nakashima H (eds) Massively Multi-Agent Systems I: First Inter-

Agent Based Modeling, Large Scale Simulations
A
159
national Workshop MMAS 2004, Kyoto, Japan, December 2004.
Springer, Berlin
17. Immanuel A, Berry MW, Gross LJ, Palmer M, Wang D (2005)
A parallel implementation of ALFISH: simulating hydrological
compartmentalization effects on fish dynamics in the Florida
Everglades. Simul Model Pract Theory 13:55–76
18. Ishida T, Gasser L, Nakashima H (eds) (2005) Massively Multi-
Agent Systems I. First International Workshop, MMAS 2004, Ky-
oto, Japan. Springer, Berlin
19. Jang MW (2006) Agent framework services to reduce agent
communication overhead in large-scale agent-based simula-
tions. Simul Model Pract Theory 14(6):679–694
20. Jang MW, Agha G (2005) Adaptive agent allocation for
massively multi-agent applications. In: Ishida T, Gasser L,
Nakashima H (eds) Massively Multi-Agent Systems I: First Inter-
national Workshop MMAS 2004, Kyoto, Japan, December 2004.
Springer, Berlin
21. Jennings NR (2000) On agent-based software engineering. Ar-
tif Intell 117:277–296
22. Kadau K, Germann TC, Lomdahl PS (2006) Molecular dynamics
comes of age: 320 billion atom simulation on BlueGene/L. Int J
Mod Phys C 17(12):1755
23. Lees M, Logan B, Oguara T, Theodoropoulos G (2003) Simulat-
ing Agent-Based Systems with HLA: The case of SIM_AGENT –
Part II. In: Proceedings of the 2003 European Simulation Inter-
operability Workshop
24. Lees M, Logan B, Theodoropoulos G (2002) Simulating Agent-
Based Systems with HLA: The case of SIM_AGENT. In: Proceed-
ings of the 2002 European Simulation Interoperability Work-
shop, pp 285–293
25. Logan B, Theodoropolous G (2001) The Distributed Simulation
of Multi-Agent Systems. Proc IEEE 89(2):174–185
26. Lomdahl PS, Beazley DM, Tamayo P, Gronbechjensen N (1993)
Multimillion particle molecular-dynamics on the CM-5. Int J
Mod Phys C: Phys Comput 4(6):1075–1084
27. Lorek H, Sonnenschein M (1995) Using parallel computers to
simulate individual-oriented models in ecology: a case study.
In: Proceedings: ESM ’95 European Simulation Multiconfer-
ence, Prague, June 1995
28. Luke S, Cioffi-Revilla C, Panait L, Sullivan K (2004) MASON:
A New Multi-Agent Simulation Toolkit. In: Proceedings of the
2004 SwarmFest Workshop
29. Openshaw S, Turton I (2000) High performance computing
and the art of parallel programming: an introduction for ge-
ographers, social scientists, engineers. Routledge, London
30. Pacheco PS (1997) Parallel Programming with MPI. Morgan
Kauffman Publishers, San Francisco
31. Parry HR (2006) Effects of Land Management upon Species
Population Dynamics: A Spatially Explicit, Individual-based
Model. Ph D thesis, University of Leeds
32. Parry HR, Evans AJ (in press) A comparative analysis of par-
allel processing and super-individual methods for improving
the computational performance of a large individual-based
model. Ecological Modelling
33. Parry HR, Evans AJ, Heppenstall AJ (2006) Millions of Agents:
Parallel Simulations with the Repast Agent-Based Toolkit. In:
Trappl R (ed) Cybernetics and Systems 2006, Proceedings
of the 18th European Meeting on Cybernetics and Systems
Research
34. Popov K, Vlassov V, Rafea M, Holmgren F, Brand P, Haridi S
(2003) Parallel agent-based simulation on a cluster of work-
stations. In: EURO-PAR 2003 Parallel Processing, vol 2790,
pp 470–480
35. Scheffer M, Baveco JM, DeAngelis DL, Rose KA, van Nes EH
(1995) Super-Individuals: a simple solution for modelling large
populations on an individual basis. Ecol Model 80:161–170
36. Scheutz M, Schermerhorn P (2006) Adaptive Algorithms for the
Dynamic Distribution and Parallel Execution of Agent-Based
Models. J Parallel Distrib Comput 66(8):1037–1051
37. Takahashi T, Mizuta H (2006) Efficient agent-based simula-
tion framework for multi-node supercomputers. In: Perrone LF,
Wieland FP, Liu J, Lawson BG, Nicol DM, Fujimoto RM (eds) Pro-
ceedings of the 2006 Winter Simulation Conference
38. Takeuchi I (2005) A massively multi-agent simulation system
for disaster mitigation. In: Ishida T, Gasser L, Nakashima H (eds)
Massively Multi-Agent Systems I: First International Workshop
MMAS 2004, Kyoto, Japan, December 2004. Springer, Berlin
39. Timm IJ, Pawlaszczyk D (2005) Large Scale Multiagent Simula-
tion on the Grid. In: Veit D, Schnizler B, Eymann T (eds) Proceed-
ings of the Workshop on Agent-based grid Economics (AGE
2005) at the IEEE International Symposium on Cluster Comput-
ing and the Grid (CCGRID). Cardiff University, Cardiff
40. Wang D, Berry MW, Carr EA, Gross LJ (2006a) A parallel fish
landscape model for ecosystem modeling. Simul 82(7):451–
465
41. Wang D, Berry MW, Gross LJ (2006b) On parallelization
of a spatially-explicit structured ecological model for inte-
grated ecosystem simulation. Int J High Perform Comput Appl
20(4):571–581
42. Wang D, Carr E, Gross LJ, Berry MW (2005a) Toward ecosystem
modeling on computing grids. Comput Sci Eng 7:44–52
43. Wang D, Gross L, Carr E, Berry M (2004) Design and implemen-
tation of a Parallel Fish Model for South Florida. In: Proceedings
of the 37th Annual Hawaii International Conference on System
Sciences (HICSS’04)
44. Wang F, Turner SJ, Wang L (2005b) Agent Communication in
Distributed Simulations. In: Davidsson P, Logan B, Takadama
K (eds) Multi-agent and multi-agent-based simulation. Joint
Workshop MABS 2004 New York, NY, USA, July 19, 2004.
Springer, Berlin
45. Wooldridge M (1999) Intelligent agents. In: Weiss G (ed) Mul-
tiagent Systems: A Modern Approach to Distributed Artificial
Intelligence. MIT Press, Cambridge, pp 27–78
46. Zhang S, Lui J (2005) A massively multi-agent system for dis-
covering HIV-immune interaction dynamics. In: Ishida T, Gasser
L, Nakashima H (eds) Massively Multi-Agent Systems I: First In-
ternational Workshop MMAS 2004, Kyoto, Japan, December
2004. Springer, Berlin
47. Zhang Y, Thedoropoulos G, Minson R, Turner SJ, Cai W, Xie Y,
Logan B (2005) Grid-aware Large Scale Distributed Simulation
of Agent-based Systems. In: 2004 European Simulation Inter-
operability Workshop (EuroSIW 2005), 05E-SIW-047, Toulouse,
France
Books and Reviews
Agent libraries and toolkits with distributed or parallel features:

Distributed GenSim: Supports distributed parallel execu-
tion [2].

Ecolab: http://ecolab.sourceforge.net/. EcoLab models may
also use the Graphcode library to implement a distributed net-
work of agents over an MPI-based computer cluster.

160 A
Agent Based Modeling, Mathematical Formalism for

Graphcode system
http://parallel.hpc.unsw.edu.au/rks/graphcode/.

MACE3J: http://www.isrl.uiuc.edu/amag/mace/ an experimen-
tal agent platform supporting deployment of agent simula-
tions across a variety of system architectures [9,10].

MASON http://cs.gmu.edu/~eclab/projects/mason/.
MASON was ‘not intended to include parallelization of a single
simulation across multiple networked processors’ [28]. How-
ever, it does provide two kinds of simple parallelization:
1.
Any given step in the simulation can be broken into paral-
lel sub-steps each performed simultaneously.
2.
A simulation step can run asynchronously in the back-
ground independent of the simulation.

Repast
http://repast.sourceforge.net
and
HLA_GRID_
Repast [47]. The Repast toolkit has in-built capabilities for
performing batch simulation runs.

SimAgent:
http://www.cs.bham.ac.uk/research/projects/poplog/
packages/simagent.html. Two developments support dis-
tributed versions of SimAgent:
1.
The use of HLA to distribute SimAgent [23,24]
2.
The
SWAGES
package:
http://www.nd.edu/~airolab/
software/index.html. This allows SimAgent to be dis-
tributed over different computers and interfaced with
other packages.
Message Passing Interfaces (MPI):

Background and tutorials
http://www-unix.mcs.anl.gov/mpi/

MPICH2 http://www-unix.mcs.anl.gov/mpi/mpich/

MPI forum http://www.mpi-forum.org/

MPIJava http://www.hpjava.org

OpenMP http://www.openmp.org

OpenMPI http://www.open-mpi.org/
Parallel computing and distributed agent simulation websites:

Further references and websites http://www.cs.rit.edu/~ncs/
parallel.html.

Introduction to parallel programming
http://www.mhpcc.edu/training/workshop/parallel_intro/
MAIN.html

http://www.agents.cs.nott.ac.uk/research/simulation/
simulators/
(implementations
of
HLA_GRID_Repast
and
distributed SimAgent).

Globus Grid computing resources
http://www.globus.org/.

Beowulf computer clusters http://www.beowulf.org/
Agent Based Modeling,
Mathematical Formalism for
REINHARD LAUBENBACHER1, ABDUL S. JARRAH1,
HENNING S. MORTVEIT1, S.S. RAVI2
1 Virginia Bioinformatics Institute,
Virginia Polytechnic Institute and State University,
Virginia, USA
2 Department of Computer Science,
University at Albany – State University of New York,
New York, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Existing Mathematical Frameworks
Finite Dynamical Systems
Finite Dynamical Systems
as Theoretical and Computational Tools
Mathematical Results on Finite Dynamical Systems
Future Directions
Bibliography
Glossary
Agent-based simulation An agent-based simulation of
a complex system is a computer model that consists of
a collection of agents/variables that can take on a typ-
ically ﬁnite collection of states. The state of an agent
at a given point in time is determined through a col-
lection of rules that describe the agent’s interaction
with other agents. These rules may be deterministic
or stochastic. The agent’s state depends on the agent’s
previous state and the state of a collection of other
agents with whom it interacts.
Mathematical framework A mathematical framework
for agent-based simulation consists of a collection of
mathematical objects that are considered mathemati-
cal abstractions of agent-based simulations. This col-
lection of objects should be general enough to cap-
ture the key features of most simulations, yet speciﬁc
enough to allow the development of a mathematical
theory with meaningful results and algorithms.
Finite dynamical system A ﬁnite dynamical system is
a time-discrete dynamical system on a ﬁnite state set.
That is, it is a mapping from a Cartesian product of
ﬁnitely many copies of a ﬁnite set to itself. This ﬁnite
set is often considered to be a ﬁeld. The dynamics is
generated by iteration of the mapping.
Definition of the Subject
Agent-based simulations are generative or computational
approaches used for analyzing “complex systems.” What
is a “system?” Examples of systems include a collection
of molecules in a container, the population in an urban
area, and the brokers in a stock market. The entities or
agents in these three systems would be molecules, indi-
viduals and stock brokers, respectively. The agents in such
systems interact in the sense that molecules collide, indi-
viduals come into contact with other individuals and bro-

Agent Based Modeling, Mathematical Formalism for
A
161
kers trade shares. Such systems, often called multiagent
systems, are not necessarily complex. The label “complex”
is typically attached to a system if the number of agents is
large, if the agent interactions are involved, or if there is
a large degree of heterogeneity in agent character or their
interactions.
This is of course not an attempt to deﬁne a complex
system. Currently there is no generally agreed upon deﬁni-
tion of complex systems. It is not the goal of this article to
provide such a deﬁnition – for our purposes it will be suﬃ-
cient to think of a complex system as a collection of agents
interacting in some manner that involves one or more of
the complexity components just mentioned, that is, with
a large number of agents, heterogeneity in agent charac-
ter and interactions and possibly stochastic aspects to all
these parts. The global properties of complex systems, such
as their global dynamics, emerge from the totality of local
interactions between individual agents over time. While
these local interactions are well understood in many cases,
little is known about the emerging global behavior arising
through interaction. Thus, it is typically diﬃcult to con-
struct global mathematical models such as systems of or-
dinary or partial diﬀerential equations, whose properties
one could then analyze. Agent-based simulations are one
way to create computational models of complex systems
that take their place.
An agent-based simulation, sometimes also called an
individual-based or interaction-based simulation (which
we prefer), of a complex system is in essence a computer
program that realizes some (possibly approximate) model
of the system to be studied, incorporating the agents and
their rules of interaction. The simulation might be deter-
ministic (i. e., the evolution of agent-states is governed by
deterministic rules) or stochastic. The typical way in which
such simulations are used is to initialize the computer pro-
gram with a particular assignment of agent states and to
run it for some time. The output is a temporal sequence of
states for all agents, which is then used to draw conclusions
about the complex system one is trying to understand. In
other words, the computer program is the model of the
complex system, and by running the program repeatedly,
one expects to obtain an understanding of the characteris-
tics of the complex system.
There are two main drawbacks to this approach. First,
it is diﬃcult to validate the model. Simulations for most
systems involve quite complex software constructs that
pose challenges to code validation. Second, there are essen-
tially no rigorous tools available for an analysis of model
properties and dynamics. There is also no widely applica-
ble formalism for the comparison of models. For instance,
if one agent-based simulation is a simpliﬁcation of an-
other, then one would like to be able to relate their dynam-
ics in a rigorous fashion. We are currently lacking a math-
ematically rich formal framework that models agent-based
simulations. This framework should have at its core a class
of mathematical objects to which one can map agent-based
simulations. The objects should have a suﬃciently general
mathematical structure to capture key features of agent-
based simulations and, at the same time, should be rich
enough to allow the derivation of substantial mathematical
results. This chapter presents one such framework, namely
the class of time-discrete dynamical systems over ﬁnite state
sets.
The building blocks of these systems consist of a collec-
tion of variables (mapping to agents), a graph that captures
the dependence relations of agents on other agents, a local
update function for each agent that encapsulates the rules
by which the state of each agent evolves over time, and an
update discipline for the variables (e. g. parallel or sequen-
tial). We will show that this class of mathematical objects is
appropriate for the representation of agent-based simula-
tions and, therefore, complex systems, and is rich enough
to pose and answer relevant mathematical questions. This
class is suﬃciently rich to be of mathematical interest in
its own right and much work remains to be done in study-
ing it. We also remark that many other frameworks such
as probabilistic Boolean networks [80] ﬁt inside the frame-
work described here.
Introduction
Computer simulations have become an integral part of to-
day’s research and analysis methodologies. The ever-in-
creasing demands arising from the complexity and sheer
size of the phenomena studied constantly push com-
putational boundaries, challenge existing computational
methodologies, and motivate the development of new
theories to improve our understanding of the potential
and limitations of computer simulation. Interaction-based
simulations are being used to simulate a variety of biolog-
ical systems such as ecosystems and the immune system,
social systems such as urban populations and markets, and
infrastructure systems such as communication networks
and power grids.
To model or describe a given system, one typically has
several choices in the construction and design of agent-
based models and representations. When agents are cho-
sen to be simple, the simulation may not capture the be-
havior of the real system. On the other hand, the use of
highly sophisticated agents can quickly lead to complex
behavior and dynamics. Also, use of sophisticated agents
may lead to a system that scales poorly. That is, a linear

162 A
Agent Based Modeling, Mathematical Formalism for
increase in the number of agents in the system may re-
quire a non-linear (e. g., quadratic, cubic, or exponential)
increase in the computational resources needed for the
simulation.
Two common methods, namely discrete event simula-
tion and time-stepped simulation, are often used to imple-
ment agent-based models [1,45,67]. In the discrete event
simulation method, each event that occurs in the system is
assigned a time of occurrence. The collection of events is
kept in increasing order of their occurrence times. (Note
that an event occurring at a certain time may give rise to
events which occur later.) When all the events that oc-
cur at a particular time instant have been carried out, the
simulation clock is advanced to the next time instant in
the order. Thus, the diﬀerences between successive val-
ues of the simulation clock may not be uniform. Discrete
event simulation is typically used in contexts such as queu-
ing systems [58]. In the time-stepped method of simula-
tion, the simulation clock is always advanced by the same
amount. For each value of the simulation clock, the states
of the system components are computed using equations
that model the system. This method of simulation is com-
monly used for studying, e. g., ﬂuid ﬂows or chemical re-
actions. The choice of model (discrete event versus time-
stepped) is typically guided by an analysis of the computa-
tional speed they can oﬀer, which in turn depends on the
nature of the system being modeled, see, e. g., [37].
Tool-kits for general purpose agent-based simulations
include Swarm [29,57] and Repast [68]. Such tool-kits
allow one to specify more complex agents and interac-
tions than would be possible using, e. g., ordinary diﬀeren-
tial equations models. In general, it is diﬃcult to develop
a software package that is capable of supporting the sim-
ulation of a wide variety of physical, biological, and social
systems.
Standard or classical approaches to modeling are often
based on continuous techniques and frameworks such as
ordinary diﬀerential equations (ODEs) and partial diﬀer-
ential equations (PDEs). For example, there are PDE based
models for studying traﬃc ﬂow [38,47,85]. These can ac-
curately model the emergence of traﬃc jams for simple
road/intersection conﬁgurations through, for example, the
formation of shocks. However, these models fail to scale to
the size and the speciﬁcations required to accurately rep-
resent large urban areas. Even if they hypothetically were
to scale to the required size, the answers they provide (e. g.
car density on a road as a function of position and time)
cannot answer questions pertaining to speciﬁc travelers
or cars. Questions of this type can be naturally described
and answered through agent-based models. An example
of such a system is TRANSIMS (see Subsect. “TRANSIMS
(Transportation, Analysis, Simulation System)”), where an
agent-based simulation scheme is implemented through
a cellular automaton model. Another well-known exam-
ple of the change in modeling paradigms from continuous
to discrete is given by lattice gas automata [32] in the con-
text of ﬂuid dynamics. Stochastic elements are inherent in
many systems, and this usually is reﬂected in the resulting
models used to describe them. A stochastic framework is
a natural approach in the modeling of, for example, noise
over a channel in a simulation of telecommunication net-
works [6]. In an economic market or a game theoretic set-
ting with competing players, a player may sometimes de-
cide to provide incorrect information. The state of such
a player may therefore be viewed and modeled by a ran-
dom variable. A player may make certain probabilistic as-
sumptions about other players’ environment. In biologi-
cal systems, certain features and properties may only be
known up to the level of probability distributions. It is
only natural to incorporate this stochasticity into models
of such systems.
Since applications of stochastic discrete models are
common, it is desirable to obtain a better understanding of
these simulations both from an application point of view
(reliability, validation) and from a mathematical point of
view. However, an important disadvantage of agent-based
models is that there are few mathematical tools available at
this time for the analysis of their dynamics.
Examples of Agent-Based Simulations
In order to provide the reader with some concrete exam-
ples that can also be used later on to illustrate theoretical
concepts we describe here three examples of agent-based
descriptions of complex systems, ranging from traﬃc net-
works to the immune system and voting schemes.
TRANSIMS (Transportation, Analysis, Simulation Sys-
tem)
TRANSIMS is a large-scale computer simulation
of traﬃc on a road network [64,66,76]. The simulation
works at the resolution level of individual travelers, and
has been used to study large US metropolitan areas such
as Portland, OR, Washington D.C. and Dallas/Fort Worth.
A TRANSIMS-based analysis of an urban area requires (i)
a population, (ii) a location-based activity plan for each
individual for the duration of the analysis period and (iii)
a network representation of all transportation pathways of
the given area. The data required for (i) and (ii) are gener-
ated based on, e. g., extensive surveys and other informa-
tion sources. The network representation is typically very
close to a complete description of the real transportation
network of the given urban area.

Agent Based Modeling, Mathematical Formalism for
A
163
TRANSIMS consists of two main modules: the router
and the cellular automaton based micro-simulator. The
router maps each activity plan for each individual (ob-
tained typically from activity surveys) into a travel route.
The micro-simulator executes the travel routes and sends
each individual through the transportation network so
that its activity plan is carried out. This is done in such
a way that all constraints imposed on individuals from
traﬃc driving rules, road signals, fellow travelers, and pub-
lic transportation schedules are respected. The time scale
is typically 1 s.
The micro-simulator is the part of TRANSIMS respon-
sible for the detailed traﬃc dynamics. Its implementation
is based on cellular automata which are described in more
detail in Subsect. “Cellular Automata”. Here, for simplic-
ity, we focus on the situation where each individual travels
by car. The road network representation is in terms of links
(e. g. road segments) and nodes (e. g. intersections). The
network description is turned into a cell-network descrip-
tion by discretizing each lane of every link into cells. A cell
corresponds to a 7.5 m lane segment, and can have up to
four neighbor cells (front, back, left and right).
The vehicle dynamics is speciﬁed as follows. Vehicles
travel with discrete velocities 0, 1, 2, 3, 4 or 5 which are
constant between time steps. Each update time step brings
the simulation one time unit forward. If the time unit is
one second then the maximum speed of vmax D 5 cells per
time unit corresponds to an actual speed of 5  7:5 m/s D
37:5 m/s which is 135 km/h or approximately 83.9 mph.
Ignoring intersection dynamics, the micro-simulator
executes three functions for each vehicle in every up-
date: (a) lane-changing, (b) acceleration and (c) move-
ment. These functions can be implemented through four
cellular automata, one each for lane change decision and
execution, one for acceleration and one for movement.
For instance, the acceleration automaton works as follows.
A vehicle in TRANSIMS can increase its speed by at most
1 cell per second, but if the road ahead is blocked, the ve-
hicle can come to a complete stop in the same time. The
function that is applied to each cell that has a car in it uses
the gap ahead and the maximal speed to determine if the
car will increase or decrease its velocity. Additionally, a car
may have its velocity decreased one unit as determined by
a certain deceleration probability. The random decelera-
tion is an important element of producing realistic traﬃc
ﬂow. A major advantage of this representation is that it
leads to very light-weight agents, a feature that is critical
for achieving eﬃcient scaling.
CImmSim
Next we discuss an interaction-based simu-
lation that models certain aspects of the human immune
system. Comprised of a large number of interacting cells
whose motion is constrained by the body’s anatomy, the
immune system lends itself very well to agent-based sim-
ulation. In particular, these models can take into account
three-dimensional anatomical variations as well as small-
scale variability in cell distributions. For instance, while
the number of T-cells in the human body is astronomical,
the number of antigen-speciﬁc T-cells, for a speciﬁc anti-
gen, can be quite small, thereby creating many spatial in-
homogeneities. Also, little is known about the global struc-
ture of the system to be modeled.
The ﬁrst discrete model to incorporate a useful level
of complexity was ImmSim [22,23], developed by Seiden
and Celada as a stochastic cellular automaton. The system
includes B-cells, T-cells, antigen presenting cells (APCs),
antibodies, antigens, and antibody-antigen complexes. Re-
ceptors on cells are represented by bit strings, and an-
tibodies use bit strings to represent their epitopes and
peptides. Speciﬁcity and aﬃnity are deﬁned by using bit
string similarity. The bit string approach was initially in-
troduced in [31]. The model is implemented on a reg-
ular two-dimensional grid, which can be thought of as
a slice of a lymph node, for instance. It has been used
to study various phenomena, including the optimal num-
ber of human leukocyte antigens in human beings [22],
the autoimmunity and T-lymphocyte selection in the thy-
mus [60], antibody selection and hyper-mutation [24],
and the dependence of the selection and maturation of
the immune response on the antigen-to-receptor’s aﬃn-
ity [15]. The computational limitations of the Seiden-
Celada model have been overcome by a modiﬁed model,
CimmSim [20], implemented on a parallel architecture. Its
complexity is several orders of magnitude larger than that
of its predecessor. It has been used to model hypersensitiv-
ity to chemotherapy [19] and the selection of escape mu-
tants from immune recognition during HIV infection [14].
In [21] the CimmSim framework was applied to the study
of mechanisms that contribute to the persistence of infec-
tion with the Epstein–Barr virus.
A Voting Game
The following example describes a hy-
pothetical voting scheme. The voting system is con-
structed from a collection of voters. For simplicity, it is as-
sumed that only two candidates, represented by 0 and 1,
contest in the election. There are N voters represented by
the set fv1; v2; : : : ; vNg. Each voter has a candidate prefer-
ence or a state. We denote the state of voter vi by xi. More-
over, each voter knows the preferences or states of some of
his or her friends (fellow voters). This friendship relation is
captured by the dependency graph which we describe later
in Subsect. “Deﬁnitions, Background, and Examples”. In-

164 A
Agent Based Modeling, Mathematical Formalism for
formally, the dependency graph has as vertices the voters
with an edge between each pair of voters that are friends.
Starting from an initial conﬁguration of preferences,
the voters cast their votes in some order. The candidate
that receives the most votes is the winner. A number of
rules can be formulated to decide how each voter chooses
a candidate. We will provide examples of such rules later,
and as will be seen, the outcome of the election is governed
by the order in which the votes are cast as well as the struc-
ture of the dependency graph.
Existing Mathematical Frameworks
The ﬁeld of agent-based simulation currently places heavy
emphasis on implementation and computation rather
than on the derivation of formal results. Computation is
no doubt a very useful way to discover potentially in-
teresting behavior and phenomena. However, unless the
simulation has been set up very carefully, its outcome
does not formally validate or guarantee the observed phe-
nomenon. It could simply be caused by an artifact of the
system model, an implementation error, or some other
uncertainty.
A ﬁrst step in a theory of agent-based simulation is
the introduction of a formal framework that on the one
hand is precise and computationally powerful, and, on the
other hand, is natural in the sense that it can be used to
eﬀectively describe large classes of both deterministic and
stochastic systems. Apart from providing a common basis
and a language for describing the model using a sound for-
malism, such a framework has many advantages. At a ﬁrst
level, it helps to clarify the key structure in a system. Do-
main speciﬁc knowledge is crucial to deriving good mod-
els of complex systems, but domain speciﬁcity is often
confounded by domain conventions and terminology that
eventually obfuscate the real structure.
A formal, context independent framework also makes
it easier to take advantage of existing general theory and
algorithms. Having a model formulated in such a frame-
work also makes it easier to establish results. Additionally,
expressing the model using a general framework is more
likely to produce results that are widely applicable. This
type of framework also supports implementation and val-
idation. Modeling languages like UML [16] serve a similar
purpose, but tend to focus solely on software implementa-
tion and validation issues, and very little on mathematical
or computational analysis.
Cellular Automata
In this section we discuss several existing frameworks
for describing agent-based simulations. Cellular automata
(CA) were introduced by Ulam and von Neumann [84]
as biologically motivated models of computation. Early
research addressed questions about the computational
power of these devices. Since then their properties have
been studied in the context of dynamical systems [40], lan-
guage theory [52], and ergodic theory [51] to mention just
a few areas. Cellular automata were popularized by Con-
way [35] (Game of Life) and by Wolfram [55,86,88]. Cel-
lular automata (both deterministic and stochastic) have
been used as models for phenomena ranging from lattice
gases [32] and ﬂows in porous media [77] to traﬃc analy-
sis [33,63,65].
A cellular automaton is typically deﬁned over a regu-
lar grid. An example is a two-dimensional grid such as Z2.
Each grid point (i; j) is referred to as a site or node. Each
site has a state xi;j(t) which is often taken to be binary.
Here t denotes the time step. Furthermore, there is a no-
tion of a neighborhood for each site. The neighborhood N
of a site is the collection of sites that can inﬂuence the fu-
ture state of the given site. Based on its current state xi;j(t)
and the current states of the sites in its neighborhood N,
a function fi;j is used to compute the next state xi;j(t C 1)
of the site (i; j). Speciﬁcally, we have
xi;j(t C 1) D fi;j(¯xi;j(t)) ;
(1)
where ¯xi;j(t) denotes the tuple consisting of all the states
xi0;j0(t) with (i0; j0) 2 N. The tuple consisting of the states
of all the sites is the CA conﬁguration and is denoted
x(t) D (xi;j(t))i;j. Equation (1) is used to map the con-
ﬁguration x(t) to x(t C 1). The cellular automaton map or
dynamical system, is the map ˚ that sends x(t) to x(t C 1).
A central part of CA research is to understand how
conﬁgurations evolve under iteration of the map ˚ and
what types of dynamical behavior can be generated. A gen-
eral introduction to CA can be found in [43].
Hopﬁeld Networks
Hopﬁeld networks were proposed as a simple model of as-
sociative memories [42]. A discrete Hopﬁeld neural net-
work consists of an undirected graph Y(V; E). At any
time t, each node vi 2 V has a state xi(t) 2 fC1; 1g. Fur-
ther, each node vi 2 V has an associated threshold i 2 R.
Each edge fvi; vjg 2 E has an associated weight wi;j 2 R.
For each node vi, the neighborhood Ni of vi includes vi and
the set of nodes that are adjacent to vi in Y. Formally,
Ni D fvig [ fvj 2 V : fvi; vjg 2 Eg :
States of nodes are updated as follows. At time t, node vi
computes the function f i deﬁned by

Agent Based Modeling, Mathematical Formalism for
A
165
fi(t) D sgn
0
@i C
X
v j2Ni
wi;j xj(t)
1
A ;
where sgn is the map from R to fC1; 1g, deﬁned by
sgn(x) D
(
1 ;
if x  0
and
1 ;
otherwise :
Now, the state of vi at time t C 1 is
xi(t C 1) D fi(t) :
Many references on Hopﬁeld networks (see for exam-
ple [42,78]) assume that the underlying undirected graph
is complete; that is, there is an edge between every pair of
nodes. In the deﬁnition presented above, the graph need
not be complete. However, this does not cause any diﬃ-
culties since the missing edges can be assigned weight 0. As
a consequence, such edges will not play any role in deter-
mining the dynamics of the system. Both synchronous and
asynchronous update models of Hopﬁeld neural networks
have been considered in the literature. For theoretical re-
sults concerning Hopﬁeld networks see [69,70] and the
references cited therein. Reference [78] presents a num-
ber of applications of neural networks. In [54], a Hopﬁeld
model is used to study polarization in dynamic networks.
Communicating Finite State Machines
The model of communicating ﬁnite state machines
(CFSM) was proposed to analyze protocols used in com-
puter networks. In some of the literature, this model is also
referred to as “concurrent transition systems” [36].
In the CFSM model, each agent is a process execut-
ing on some node of a distributed computing system. Al-
though there are minor diﬀerences among the various
CFSM models proposed in the literature [17,36], the basic
set-up models each process as a ﬁnite state machine (FSM).
Thus, each agent is in a certain state at any time instant t.
For each pair of agents, there is a bidirectional channel
through which they can communicate. The state of an
agent at time t C 1 is a function of the current state and
the input (if any) on one or more of the channels. When
an agent (FSM) undergoes a transition from one state to
another, it may also choose to send a message to another
agent or receive a message from an agent. In general, such
systems can be synchronous or asynchronous. As can be
seen, CFSMs are a natural formalism for studying proto-
cols used in computer networks. The CFSM model has
been used extensively to prove properties (e. g. deadlock
freedom, fairness) of a number of protocols used in prac-
tice (see [17,36] and the references cited therein).
Other frameworks include interacting particle sys-
tems [50], and Petri nets [59]. There is a vast literature on
both, but space limitations prevent a discussion here.
Finite Dynamical Systems
Another, quite general, modeling framework that has been
proposed is that of ﬁnite dynamical systems, both syn-
chronous and asynchronous. Here the proposed mathe-
matical object representing an agent-based simulation is
a time-discrete dynamical system on a ﬁnite state set. The
description of the systems is modeled after the key com-
ponents of an agent-based simulation, namely agents, the
dependency graph, local update functions, and an update
order. This makes a mapping to agent-based simulations
natural. In the remainder of this chapter we will show
that ﬁnite dynamical systems satisfy our criteria for a good
mathematical framework in that they are general enough
to serve as a broad computing tool and mathematically
rich enough to allow the derivation of formal results.
Deﬁnitions, Background, and Examples
Let x1; : : : ; xn be a collection of variables, which take val-
ues in a ﬁnite set X. (As will be seen, the variables represent
the entities in the system being modeled and the elements
of X represent their states.) Each variable xi has associated
to it a “local update function” fi : Xn ! X, where “local”
refers to the fact that f i takes inputs from the variables in
the “neighborhood” of xi, in a sense to be made precise be-
low. By abuse of notation we also let f i denote the function
Xn ! Xn which changes the ith coordinate and leaves
the other coordinates unchanged. This allows for the se-
quential composition of the local update functions. These
functions assemble to a dynamical system
˚ D (f1; : : : ; fn): Xn ! Xn ;
with the dynamics generated by iteration of ˚. As an ex-
ample, if X D f0; 1g with the standard Boolean operators
AND and OR, then ˚ is a Boolean network.
The assembly of ˚ from the local functions f i can be
done in one of several ways. One can update each of the
variables simultaneously, that is,
˚(x1; : : : ; xn) D (f1(x1; : : : ; xn); : : : ; fn(x1; : : : ; xn)) :
In this case one obtains a parallel dynamical system.
Alternatively, one can choose to update the states
of the variables according to some ﬁxed update order,
for example, a permutation (1; 2; : : : ; n) of the set
f1; : : : ; ng. More generally, one could use a word on the set

166 A
Agent Based Modeling, Mathematical Formalism for
f1; : : : ; ng, that is,  D (1; : : : ; t) where t is the length
of the word. The function composition
˚	 D f	t ı f	t1 ı    ı f	1 ;
(2)
is called a sequential dynamical system (SDS) and, as be-
fore, the dynamics of ˚	 is generated by iteration. The
case when  is a permutation on f1; : : : ; ng has been stud-
ied extensively [2,3,5,9]. It is clear that using a diﬀerent
permutation or word  may result in a diﬀerent dynam-
ical system ˚
. Using a word rather than a permutation
allows one to capture the case where some vertices have
states that are updated more frequently than others.
Remark 1
Notice that for a ﬁxed , the function ˚	 is
a parallel dynamical system: once the update order  is
chosen and the local update functions are composed ac-
cording to , that is, the function ˚	 has been computed,
then ˚	(x1; : : : ; xn) D g(x1; : : : ; xn) where g is a parallel
update dynamical system. However, the maps gi are not
local functions.
The dynamics of ˚ is usually represented as a directed
graph on the vertex set Xn, called the phase space of ˚.
There is a directed edge from v 2 Xn to w 2 Xn if and
only if ˚(v) D w. A second graph that is usually associ-
ated with a ﬁnite dynamical system is its dependency graph
Y(V; E). In general, this is a directed graph, and its ver-
tex set is V D f1; : : : ; ng. There is a directed edge from i
to j if and only if xi appears in the function f j. In many
situations, the interaction relationship between pairs of
variables is symmetric; that is, variable xi appears in f j if
and only if xj appears in f i. In such cases, the dependency
Agent Based Modeling, Mathematical Formalism for, Figure 1
The phase space of the parallel system ˚ (a) and dependency graph of the Boolean functions from Example 2 (b)
Agent Based Modeling, Mathematical Formalism for, Figure 2
The phase spaces from Example 3: ˚ (a) and ˚id (b)
graph can be thought of as an undirected graph. We re-
call that the dependency graphs mentioned in the context
of the voting game (Subsect. “A Voting Game”) and Hop-
ﬁeld networks (Subsect. “Hopﬁeld Networks”) are undi-
rected graphs. The dependency graph plays an important
role in the study of ﬁnite dynamical systems and is some-
times listed explicitly as part of the speciﬁcation of ˚.
Example 2 Let X D f0; 1g (the Boolean case). Suppose we
have four variables and the local Boolean update functions
are
f1 D x1 C x2 C x3 C x4;
f2 D x1 C x2;
f3 D x1 C x3;
f4 D x1 C x4 ;
where “+” represents sum modulo 2. The dynamics of
the function ˚ D (f1; : : : ; f4): X4 ! X4 is the directed
graph in Fig. 1a while the dependency graph is in Fig. 1b.
Example 3 Consider the local functions in the Example 2
above and let  D (2; 1; 3; 4). Then
˚	 D f4 ı f3 ı f1 ı f2 : X4 ! X4 :
The phase space of ˚	 is the directed graph in Fig. 2a,
while the phase space of ˚, where  D id is in Fig. 2b.
Notice that the phase space of any function is a directed
graph in which every vertex has out-degree one; this is
a characteristic property of deterministic functions.

Agent Based Modeling, Mathematical Formalism for
A
167
Making use of Boolean arithmetic is a powerful tool in
studying Boolean networks, which is not available in gen-
eral. In order to have available an enhanced set of tools
it is often natural to make an additional assumption re-
garding X, namely that it is a ﬁnite number system, a ﬁnite
ﬁeld [49]. This amounts to the assumption that there are
“addition” and “multiplication” operations deﬁned on X
that satisfy the same rules as ordinary addition and mul-
tiplication of numbers. Examples include Zp, the integers
modulo a prime p. This assumption can be thought of as
the discrete analog of imposing a coordinate system on an
aﬃne space.
When X is a ﬁnite ﬁeld, it is easy to show that for
any local function g, there exists a polynomial h such that
g(x1; : : : ; xn) D h(x1; : : : ; xn) for all (x1; : : : ; xn) 2 Xn.
To be precise, suppose X is a ﬁnite ﬁeld with q elements.
Then
g(x1; : : : ; xn)
D
X
(c1;:::;cn)2Xn
g(c1; : : : ; cn)
n
Y
iD1
(1  (xi  ci)q1) :
(3)
This observation has many useful consequences, since
polynomial functions have been studied extensively and
many analytical tools are available.
Notice that cellular automata and Boolean networks,
both parallel and sequential, are special classes of polyno-
mial dynamical systems. In fact, it is straightforward to see
that
x ^ y D x  y
x _ y D x C y C xy
and
:x D x C 1 :
(4)
Therefore, the modeling framework of ﬁnite dynami-
cal systems includes that of cellular automata, discussed
earlier. Also, since a Hopﬁeld network is a function
Xn ! Xn, which can be represented through its local
constituent functions, it follows that Hopﬁeld networks
also are special cases of ﬁnite dynamical systems.
Stochastic Finite Dynamical Systems
The deterministic framework of ﬁnite dynamical systems
can be made stochastic in several diﬀerent ways, mak-
ing one or more of the system’s deﬁning data stochastic.
For example, one could use one or both of the following
criteria.
 Assume that each variable has a nonempty set of lo-
cal functions assigned to it, together with a probabil-
ity distribution on this set, and each time a variable is
updated, one of these local functions is chosen at ran-
dom to update its state. We call such systems proba-
bilistic ﬁnite dynamical systems (PFDS), a generaliza-
tion of probabilistic Boolean networks [81].
 Fix a subset of permutations T  Sn together with
a probability distribution. When it is time for the sys-
tem to update its state, a permutation  2 T is chosen
at random and the agents are updated sequentially us-
ing . We call such systems stochastic ﬁnite dynamical
systems (SFDS).
Remark 4 By Remark, each system ˚	 is a parallel system.
Hence a SFDS is nothing but a set of parallel dynamical
systems f˚	 :  2 Tg, together with a probability distri-
bution. When it is time for the system to update its state,
a system ˚	 is chosen at random and used for the next
iteration.
To describe the phase space of a stochastic ﬁnite dynami-
cal system, a general method is as follows. Let ˝ be a ﬁnite
collection of systems ˚1; : : : ; ˚t, where ˚i : Xn ! Xn
for all i, and consider the probabilities p1; : : : ; pt which
sum to 1. We obtain the stochastic phase space
˝ D p11 C p22 C    C ptt ;
(5)
where  i is the phase space of ˚i. The associated proba-
bility space is F D (˝; 2˝; ), where the probability mea-
sure  is induced by the probabilities pi. It is clear that the
stochastic phase space can be viewed as a Markov chain
over the state space Xn. The adjacency matrix of ˝ di-
rectly encodes the Markov transition matrix. This is of
course not new, and has been done in, e. g., [28,81,83]. But
we emphasize the point that even though SFDS give rise
to Markov chains our study of SFDS is greatly facilitated by
the rich additional structure available in these systems. To
understand the eﬀect of structural components such as the
topology of the dependency graph or the stochastic nature
of the update, it is important to study them not as Markov
chains but as SFDS.
Example 5 Consider ˚	 and ˚ from Example 3 and let
	 and  be their phases spaces as shown in Fig. 2. Let
p1 D p2 D 1/2. The phase space (1/2)	 C (1/2) of the
stochastic sequential dynamical system obtained from ˚	
and ˚ (with equal probabilities) is presented in Fig. 3.
Agent-Based Simulations as Finite Dynamical Systems
In the following we describe the generic structure of the
systems typically modeled and studied through agent-
based simulations. The central notion is naturally that of
an agent.

168 A
Agent Based Modeling, Mathematical Formalism for
Agent Based Modeling, Mathematical Formalism for, Figure 3
The stochastic phase space for Example 5 induced by the two deterministic phase spaces of ˚ and ˚ from Fig. 2. For simplicity
the weights of the edges have been omitted
Each agent carries a state that may encode its prefer-
ences, internal conﬁguration, perception of its environ-
ment, and so on. In the case of TRANSIMS, for instance,
the agents are the cells making up the road network. The
cell state contains information about whether or not the
cell is occupied by a vehicle as well as the velocity of the ve-
hicle. One may assume that each cell takes on states from
the same set of possible states, which may be chosen to
support the structure of a ﬁnite ﬁeld.
The agents interact with each other, but typically an
agent only interacts with a small subset of agents, its neigh-
bors. Through such an interaction an agent may decide to
change its state based on the states (or other aspects) of the
agents with which it interacts. We will refer to the process
where an agent modiﬁes its state through interaction as
an agent update. The precise way in which an agent mod-
iﬁes its state is governed by the nature of the particular
agent. In TRANSIMS the neighbors of a cell are the adja-
cent road network cells. From this adjacency relation one
obtains a dependency graph of the agents. The local up-
date function for a given agent can be obtained from the
rules governing traﬃc ﬂow between cells.
The updates of all the agents may be scheduled in
diﬀerent ways. Classical approaches include synchronous,
asynchronous and event-driven schemes. The choice will
depend on system properties or particular considerations
about the simulation implementation.
In the case of CImmSim, the situation is somewhat
more complicated. Here the agents are also the spatial
units of the system, each representing a small volume of
lymph tissue. The total volume is represented as a 2-di-
mensional CA, in which every agent has 4 neighbors, so
that the dependency graph is a regular 2-dimensional grid.
The state of each agent is a collection of counts for the
various immune cells and pathogens that are present in
this particular agent (volume). Movement between cells is
implemented as diﬀusion. Immune cells can interact with
each other and with pathogens while they reside in the
same volume. Thus, the local update function for a given
cell of the simulation is made up of the two components
of movement between cells and interactions within a cell.
For instance, a B cell could interact with the Epstein–Barr
virus in a given volume and perform a transition from un-
infected to infected by the next time step. Interactions as
well as movement are stochastic, resulting in a stochastic
ﬁnite dynamical system. The update order is parallel.
Example 6 (The voting game (Subsect. “A Voting Game”))
The following scenario is constructed to illustrate how
implementation choices for the system components have
a clear and direct bearing on the dynamics and simulation
outcomes.
Let the voter dependency graph be the star graph on
5 vertices with center vertex a and surrounding vertices b,
c, d and e. Furthermore, assume that everybody votes op-
portunistically using the majority rule: the vote cast by
an individual is equal to the preference of the majority of
his/her friends with the person’s own preference included.
For simplicity, assume candidate 1 is preferred in the case
of a tie.
If the initial preference is xa D 1 and xb D xc D
xd D xe D 0 then if voter a goes ﬁrst he/she will vote
for candidate 0 since that is the choice of the majority of
the neighbor voters. However, if b and c go ﬁrst they only
know a’s preference. Voter b therefore casts his/her vote
for candidate 1 as does c. Note that this is a tie situation
with an equal number of preferences for candidate 1 (a)
and for candidate 2 (b). If voter a goes next, then the sit-
uation has changed: the preference of b and c has already
changed to 1. Consequently, voter a picks candidate 1. At
the end of the day candidate 1 is the election winner, and
the choice of update order has tipped the election!

Agent Based Modeling, Mathematical Formalism for
A
169
This example is of course constructed to illustrate our
point. However, in real systems it can be much more dif-
ﬁcult to detect the presence of such sensitivities and their
implications. A solid mathematical framework can be very
helpful in detecting such eﬀects.
Finite Dynamical Systems
as Theoretical and Computational Tools
If ﬁnite dynamical systems are to be useful as a model-
ing paradigm for agent-based simulations it is necessary
that they can serve as a fairly universal model of com-
putation. We discuss here how such dynamical systems
can mimic Turing Machines (TMs), a standard univer-
sal model for computation. For a more thorough exposi-
tion, we refer the reader to the series of papers by Bar-
rett et al. [7,8,10,11,12]. To make the discussion reasonably
self-contained, we provide a brief and informal discussion
of the TM model. Additional information on TMs can be
found in any standard text on the theory of computation
(e. g. [82]).
A Computational View of Finite Dynamical Systems:
Deﬁnitions
In order to understand the relationship of ﬁnite dynami-
cal systems to TMs, it is important to view such systems
from a computational stand point. Recall that a ﬁnite dy-
namical system ˚ : Xn ! Xn, where X is a ﬁnite set, has
an underlying dependency graph Y(V; E). From a compu-
tational point of view, the nodes of the dependency graph
(the agents in the simulation) are thought of as devices that
compute appropriate functions. For simplicity, we will as-
sume in this section that the dependency graph is undi-
rected, that is, all dependency relations are symmetric. At
any time, the state value of each node vi 2 V is from the
speciﬁed domain X. The inputs to f i are the current state
of vi and the states of the neighbors of vi as speciﬁed by Y.
The output of f i, which is also a member of X, becomes
the state of vi at the next time instant. The discussion
in this section will focus on sequentially updated systems
(SDS), but all results discussed apply to parallel systems as
well.
Each step of the computation carried out by an SDS
can be thought as consisting of n “mini steps;” in each
mini step, the value of the local transition function at
a node is computed and the state of the node is changed
to the computed value. Given an SDS ˚, a conﬁguration
C of ˚ is a vector (c1; c2; : : : ; cn) 2 Xn. It can be seen
that each computational step of an SDS causes a transition
from one conﬁguration to another.
Conﬁguration Reachability Problem for SDSs
Based on the computational view, a number of diﬀer-
ent problems can be deﬁned for SDSs (see for exam-
ple, [4,10,12]). To illustrate how SDSs can model TM com-
putations, we will focus on one such problem, namely the
conﬁguration reachability (CR) problem: Given an SDS ˚,
an initial conﬁguration C and another conﬁguration C0,
will ˚, starting from C, ever reach conﬁguration C0? The
problem can also be expressed in terms of the phase space
of ˚. Since conﬁgurations such as C and C0 are repre-
sented by nodes in the phase space, the CR problem boils
down to the question of whether there is a directed path in
the phase space from C to C0. This abstract problem can
be mapped to several problems in the simulation of multi-
agent systems. Consider for example the TRANSIMS con-
text. Here, the initial conﬁguration C may represent the
state of the system early in the day (when the traﬃc is very
light) and C0 may represent an “undesirable” state of the
system (such as heavy traﬃc congestion). Similarly, in the
context of modeling an infectious disease, C may represent
the initial onset of the disease (when only a small number
of people are infected) and C0 may represent a situation
where a large percentage of the population is infected. The
purpose of studying computational problems such as CR
is to determine whether one can eﬃciently predict the oc-
currence of certain events in the system from a descrip-
tion of the system. If computational analysis shows that
the system can indeed reach undesirable conﬁgurations as
it evolves, then one can try to identify steps needed to deal
with such situations.
Turing Machines: A Brief Overview
A Turing machine (TM) is a simple and commonly used
model for general purpose computational devices. Since
our goal is to point out how SDSs can also serve as com-
putational devices, we will present an informal overview
of the TM model. Readers interested in a more formal de-
scription may consult [82].
A TM consists of a set Q of states, a one-way inﬁnite
input tape and a read/write head that can read and modify
symbols on the input tape. The input tape is divided into
cells and each cell contains a symbol from a special ﬁnite
alphabet. An input consisting of n symbols is written on
the leftmost n cells of the input tape. (The other cells are
assumed to contain a special symbol called blank.) One of
the states in Q, denoted by qs, is the designated start state.
Q also includes two other special states, denoted by qa (the
accepting state) and qr (the rejecting state). At any time, the
machine is in one of the states in Q. The transition function
for the TM speciﬁes for each combination of the current

170 A
Agent Based Modeling, Mathematical Formalism for
state and the current symbol under the head, a new state,
a new symbol for the current cell (which is under the head)
and a movement (i. e., left or right by one cell) for the head.
The machine starts in state qs with the head on the ﬁrst cell
of the input tape. Each step of the machine is carried out
in accordance with the transition function. If the machine
ever reaches either the accepting or the rejecting state, it
halts with the corresponding decision; otherwise, the ma-
chine runs forever.
A conﬁguration of a TM consists of its current state, the
current tape contents and the position of the head. Note
that the transition function of a TM speciﬁes how a new
conﬁguration is obtained from the current conﬁguration.
The above description is for the basic TM model (also
called single tape TM model). For convenience in describ-
ing some computations, several variants of the above basic
model have been proposed. For example, in a multi-tape
TM, there are one or more work tapes in addition to the in-
put tape. The work tapes can be used to store intermediate
results. Each work tape has its own read/write head and the
deﬁnitions of conﬁguration and transition function can be
suitably modiﬁed to accommodate work tapes. While such
an enhancement to the basic TM model makes it easier to
carry out certain computations, it does not add to the ma-
chine’s computational power. In other words, any compu-
tation that can be carried out using the enhanced model
can also be carried out using the basic model.
As in the case of dynamical systems, one can deﬁne
a conﬁguration reachability (CR) problem for TMs: Given
a TM M, an initial conﬁguration IM and another con-
ﬁguration CM, will the TM starting from IM ever reach
CM? We refer to the CR problem in the context of TMs
as CR-TM. In fact, it is this problem for TMs that cap-
tures the essence of what can be eﬀectively computed. In
particular, by choosing the state component of CM to be
one of the halting states (qa or qr), the problem of deter-
mining whether a function is computable is transformed
into an appropriate CR-TM problem. By imposing appro-
priate restrictions on the resources used by a TM (e. g.
the number of steps, the number of cells on the work
tapes), one obtains diﬀerent versions of the CR-TM prob-
lem which characterize diﬀerent computational complex-
ity classes [82].
How SDSs Can Mimic Turing Machines
The above discussion points out an important similarity
between SDSs and TMs. Under both of these models, each
computational step causes a transition from one conﬁg-
uration to another. It is this similarity that allows one to
construct a discrete dynamical system ˚ that can simulate
a TM. Typically, each step of a TM is simulated by a short
sequence of successive iterations ˚. As part of the con-
struction, one also identiﬁes a suitable mapping between
the conﬁgurations of the TM being simulated and those of
the dynamical system. This is done in such a way that the
answer to CR-TM problem is “yes” if and only if the an-
swer to the CR problem for the dynamical system is also
“yes.”
To illustrate the basic ideas, we will informally sketch
a construction from [10]. For simplicity, this construction
produces an SDS ˚ that simulates a restricted version of
TMs; the restriction being that for any input containing n
symbols, the number of work tape cells that the machine
may use is bounded by a linear function of n. Such a TM
is called a linear bounded automaton (LBA) [82]. Let M
denote the given LBA and let n denote the length of the
input to M. The domain X for the SDS ˚ is chosen to
be a ﬁnite set based on the allowed symbols in the input
to the TM. The dependency graph is chosen to be a sim-
ple path on n nodes, where each node serves as a repre-
sentative for a cell on the input tape. The initial and ﬁ-
nal conﬁgurations C and C0 for ˚ are constructed from
the corresponding conﬁgurations of M. The local transi-
tion function for each node of the SDS is constructed from
the given transition function for M in such a way that each
step of M corresponds to exactly one step of ˚. Thus, there
is a simple bijection between the sequence of conﬁgura-
tions that M goes through during its computation and the
sequence of states that ˚ goes through as it evolves. Us-
ing this bijection, it is shown in [10] that the answer to
the CR-TM problem is “yes” if and only if ˚ reaches C0
starting from C. Reference [10] also presents a number of
sophisticated constructions where the resulting dynamical
system is very simple; for example, it is shown that an LBA
can be simulated by an SDS in which X is the Boolean
ﬁeld, the dependency graph is d-regular for some con-
stant d and the local transition functions at all the nodes
are identical. Such results point out that one does not need
complicated dynamical systems to model TM computa-
tions.
References [7,8,10] present constructions that show
how more general models of TMs can also be simulated
by appropriate SDSs. As one would expect, these con-
structions lead to SDSs with more complex local transition
functions.
Barrett et al. [11] have also considered stochastic SDS
(SSDS), where the local transition function at each node
is stochastic. For each combination of inputs, a stochas-
tic local transition function speciﬁes a probability distri-
bution over the domain of state values. It is shown in [11]
that SSDSs can eﬀectively simulate computations carried

Agent Based Modeling, Mathematical Formalism for
A
171
out by probabilistic TMs (i. e., TMs whose transition func-
tions are stochastic).
TRANSIMS Related Questions
Section “TRANSIMS (Transportation, Analysis, Simula-
tion System)” gave an overview of some aspects of the
TRANSIMS model. The micro-simulator module is speci-
ﬁed as a functional composition of four cellular automata
of the form 4 ı 3 ı 2 ı 1. (We only described 3
which corresponds to velocity updates.) Such a formula-
tion has several advantages. First, it has created an abstrac-
tion of the essence of the system in a precise mathematical
way without burying it in contextual domain details. An
immediate advantage of this speciﬁcation is that it makes
the whole implementation process more straightforward
and transparent. While the local update functions for the
cells are typically quite simple, for any realistic study of an
urban area the problem size would typically require a se-
quential implementation, raising a number of issues that
are best addressed within a mathematical framework like
the one considered here.
Mathematical Results on Finite Dynamical Systems
In this section we outline a collection of mathematical re-
sults about ﬁnite dynamical systems that is representative
of the available knowledge. The majority of these results
are about deterministic systems, as the theory of stochas-
tic systems of this type is still in its infancy. We will ﬁrst
consider synchronously updated systems.
Throughout this section, we make the assumption that
the state set X carries the algebraic structure of a ﬁnite
ﬁeld. Accordingly, we use the notation k instead of X. It
is a standard result that in this case the number q of el-
ements in k has the form q D pt for some prime p and
t  1. The reader may keep the simplest case k D f0; 1g in
mind, in which case we are eﬀectively considering Boolean
networks.
Recall Eq. (3). That is, any function g : kn ! k can
be represented by a multivariate polynomial with coeﬃ-
cients in k. If we require that the exponent of each variable
be less than q, then this representation is unique. In par-
ticular Eq. (4) implies that every Boolean function can be
represented uniquely as a polynomial function.
Parallel Update Systems
Certain classes of ﬁnite dynamical systems have been
studied extensively, in particular cellular automata and
Boolean networks where the state set is f0; 1g. Many of
these studies have been experimental in nature, however.
Some more general mathematical results about cellular au-
tomata can be found in the papers of Wolfram and col-
laborators [87]. The results there focus primarily on one-
dimensional Boolean cellular automata with a particular
ﬁxed initial state. Here we collect a sampling of more gen-
eral results.
We ﬁrst consider linear and aﬃne systems
˚ D (f1; : : : ; fn): kn ! kn :
That is, we consider systems for which the coordinate
functions f i are linear, resp. aﬃne, polynomials. (In the
Boolean case this includes functions constructed from
XOR (sum modulo 2) and negation.) When each f i is a lin-
ear polynomial of the form fi(x1; : : : ; xn) D ai1x1 C
   C ainxn, the map ˚ is nothing but a linear transfor-
mation on kn over k, and, by using the standard basis, ˚
has the matrix representation
˚
0
B@
2
64
x1
:::
xn
3
75
1
CA D
2
64
a11
  
a1n
:::
:::
:::
an1
  
ann
3
75
2
64
x1
:::
xn
3
75 ;
where ai j 2 k for all i; j.
Linear ﬁnite dynamical systems were ﬁrst studied by
Elspas [30]. His motivation came from studying feedback
shift register networks and their applications to radar and
communication systems and automatic error correction
circuits. For linear systems over ﬁnite ﬁelds of prime car-
dinality, that is, q is a prime number, Elspas showed that
the exact number and length of each limit cycle can be de-
termined from the elementary divisors of the matrix A. Re-
cently, Hernandez [41] re-discovered Elspas’ results and
generalized them to arbitrary ﬁnite ﬁelds. Furthermore,
he showed that the structure of the tree of transients at
each node of each limit cycle is the same, and can be com-
pletely determined from the nilpotent elementary divisors
of the form xa. For aﬃne Boolean networks (that is, ﬁnite
dynamical systems over the Boolean ﬁeld with two ele-
ments, whose local functions are linear polynomials which
might have constant terms), a method to analyze their cy-
cle length has been developed in [56]. After embedding
the matrix of the transition function, which is of dimen-
sion n  (n C 1), into a square matrix of dimension n C 1,
the problem is then reduced to the linear case. A fast algo-
rithm based on [41] has been implemented in [44], using
the symbolic computation package Macaulay2.
It is not surprising that the phase space structure of ˚
should depend on invariants of the matrix A D (ai j). The
rational canonical form of A is a block-diagonal matrix
and one can recover the structure of the phase space of A

172 A
Agent Based Modeling, Mathematical Formalism for
from that of the blocks in the rational form of A. Each
block represents either an invertible or a nilpotent linear
transformation. Consider an invertible block B. If (x) is
the minimal polynomial of B, then there exists s such that
(x) divides xs  1. Hence Bs  I D 0 which implies that
Bsv D v. That is, every state vector v in the phase space
of B is in a cycle whose length is a divisor of s.
Deﬁnition 7 For any polynomial (x) in k[x], the order
of (x) is the least integer s such that (x) divides xs  1.
The cycle structure of the phase space of ˚ can be com-
pletely determined from the orders of the irreducible fac-
tors of the minimal polynomial of ˚. The computation
of these orders involves in particular the factorization of
numbers of the form qr  1, which makes the computa-
tion of the order of a polynomial potentially quite costly.
The nilpotent blocks in the decomposition of A determine
the tree structure at the nodes of the limit cycles. It turns
out that all trees at all periodic nodes are identical. This
generalizes a result in [55] for additive cellular automata
over the ﬁeld with two elements.
While the fact that the structure of the phase space
of a linear system can be determined from the invari-
ants associated with its matrix may not be unexpected,
it is a beautiful example of how the right mathematical
viewpoint provides powerful tools to completely solve the
problem of relating the structure of the local functions to
the resulting (or emerging) dynamics. Linear and aﬃne
systems have been studied extensively in several diﬀerent
contexts and from several diﬀerent points of view, in par-
ticular the case of cellular automata. For instance, additive
cellular automata over more general rings as state sets have
been studied, e. g., in [25]. Further results on additive CAs
can also be found there. One important focus in [25] is
on the problem of ﬁnding CAs with limit cycles of maxi-
mal length for the purpose of constructing pseudo random
number generators.
Unfortunately, the situation is more complicated for
nonlinear systems. For the special class of Boolean syn-
chronous systems whose local update functions consist of
monomials, there is a polynomial time algorithm that de-
termines whether a given monomial system has only ﬁxed
points as periodic points [26]. This question was moti-
vated by applications to the modeling of biochemical net-
works. The criterion is given in terms of invariants of the
dependency graph Y. For a strongly connected directed
graph Y (i. e., there is a directed path between any pair of
vertices), its loop number is the greatest common divisor
of all directed loops at a particular vertex. (This number is
independent of the vertex chosen.)
Theorem 8 ([26])
A Boolean monomial system has only
ﬁxed points as periodic points if and only if the loop num-
ber of every strongly connected component of its dependency
graph is equal to 1.
In [27] it is shown that the problem for general ﬁnite ﬁelds
can be reduced to that of a Boolean system and a linear
system over rings of the form Z/prZ, p prime. Boolean
monomial systems have been studied before in the cellu-
lar automaton context [13].
Sequential Update Systems
The update order in a sequential dynamical system has
been studied using combinatorial and algebraic tech-
niques. A natural question to ask here is how the system
depends on the update schedule. In [3,5,61,72] this was
answered on several levels for the special case where the
update schedule is a permutation. We describe these re-
sults in some detail. Results about the more general case
of update orders described by words on the indices of the
local update functions can be found in [34].
Given local update functions fi : kn ! k and permu-
tation update orders ; , a natural question is when the
two resulting SDS ˚
 and ˚	 are identical and, more gen-
erally, how many diﬀerent systems one obtains by varying
the update order over all permutations. Both questions can
be answered in great generality. The answer involves in-
variants of two graphs, namely the acyclic orientations of
the dependency graph Y of the local update functions and
the update graph of Y. The update graph U(Y) of Y is the
graph whose vertex set consists of all permutations of the
vertex set of Y [72]. There is an (undirected) edge between
two permutations  D (1; : : : ; n) and  D (1; : : : ; n)
if they diﬀer by a transposition of two adjacent entries i
and iC1 such that there is no edge in Y between i and
iC1.
The update graph encodes the fact that one can com-
mute two local update functions f i and f j without aﬀecting
the end result ˚ if i and j are not connected by an edge
in Y. That is,    fi ı fj    D    fj ı fi    if and only if i
and j are not connected by an edge in Y.
All permutations belonging to the same connected
component in U(Y) give identical SDS maps. The num-
ber of (connected) components in U(Y) is therefore an
upper bound for the number of functionally inequivalent
SDS that can be generated by just changing the update or-
der. It is convenient to introduce an equivalence relation
Y on SY by  Y  if  and  belong to the same con-
nected component in the graph U(Y). It is then clear that
if  Y  then corresponding sequential dynamical sys-
tems are identical as maps. This can also be characterized

Agent Based Modeling, Mathematical Formalism for
A
173
in terms of acyclic orientations of the graph Y: each com-
ponent in the update graph induces a unique acyclic ori-
entation of the graph Y. Moreover, we have the following
result:
Proposition 9 ([72]) There is a bijection
FY : SY/ Y! Acyc(Y) ;
where SY/ Y denotes the set of equivalence classes of Y
and Acyc(Y) denotes the set of acyclic orientations of Y.
This upper bound on the number of functionally diﬀerent
systems has been shown in [72] to be sharp for Boolean
systems, in the sense that for a given Y one construct this
number of diﬀerent systems, using appropriate combina-
tions of NOR functions.
For two permutations  and  it is easy to determine if
they give identical SDS maps: one can just compare their
induced acyclic orientations. The number of acyclic ori-
entations of the graph Y tells how many functionally dif-
ferent SDS maps one can obtain for a ﬁxed graph and
ﬁxed vertex functions. The work of Cartier and Foata [18]
on partially commutative monoids studies a similar ques-
tion, but their work is not concerned with ﬁnite dynamical
systems.
Note that permutation update orders have been stud-
ied sporadically in the context of cellular automata on cir-
cle graphs [71] but not in a systematic way, typically using
the order (1; 2; : : : ; n) or the even-odd/odd-even orders.
As a side note, we remark that this work also conﬁrms our
ﬁndings that switching from a parallel update order to a se-
quential order turns the complex behavior found in Wol-
fram’s “class III and IV” automata into much more regular
or mundane dynamics, see e. g. [79].
The work on functional equivalence was extended to
dynamical equivalence (topological conjugation) in [5,61].
The automorphism group of the graph Y can be made
to act on the components of the update graph U(Y) and
therefore also on the acyclic orientations of Y. All permu-
tations contained in components of an orbit under Aut(Y)
give rise to dynamically equivalent sequential dynamical
systems, that is, to isomorphic phase spaces. However,
here one needs some more technical assumptions, i. e., the
local functions must be symmetric and induced, see [9].
This of course also leads to a bound for the number of dy-
namically inequivalent systems that can be generated by
varying the update order alone. Again, this was ﬁrst done
for permutation update orders. The theory was extended
to words over the vertex set of Y in [34,75].
The structure of the graph Y inﬂuences the dynamics
of the system. As an example, graph invariants such as the
independent sets of Y turn out to be in a bijective cor-
respondence with the invariant set of sequential systems
over the Boolean ﬁeld k D f0; 1g that have nort : kt ! k
given by nort(x1; : : : ; xt) D (1 C x1)    (1 C xt) as local
functions [73]. This can be extended to other classes such
as those with order independent invariant sets as in [39].
We have already seen how the automorphisms of a graph
give rise to equivalence [61]. Also, if the graph Y has non-
trivial covering maps we can derive simpliﬁed or reduced
(in an appropriate sense) versions of the original SDS over
the image graphs of Y, see e. g. [62,74].
Parallel and sequential dynamical systems diﬀer when
it comes to invertibility. Whereas it is generally compu-
tationally intractable to determine if a CA over Zd is in-
vertible for d  2 [46], it is straightforward to determine
this for a sequential dynamical system [61]. For example,
it turns out that the only invertible Boolean sequential dy-
namical systems with symmetric local functions are the
ones where the local functions are either the parity func-
tion or the logical complement of the parity function [5].
Some classes of sequential dynamical systems such as
the ones induced by the nor-function have desirable stabil-
ity properties [39]. These systems have minimal invariant
sets (i. e. periodic states) that do not depend on the update
order. Additionally, these invariant sets are stable with re-
spect to conﬁguration perturbations.
If a state c is perturbed to a state c0 that is not periodic
this state will evolve to a periodic state c00 in one step; that
is, the system will quickly return to the invariant set. How-
ever, the states c and c00 may not necessarily be in the same
periodic orbit.
The Category of Sequential Dynamical Systems
As a general principle, in order to study a given class of
mathematical objects it is useful to study transformations
between them. In order to provide a good basis for a math-
ematical analysis the objects and transformations together
should form a category, that is, the class of transforma-
tions between two objects should satisfy certain reason-
able properties (see, e. g., [53]). Several proposed deﬁni-
tions of a transformation of SDS have been published,
notably in [48] and [74]. One possible interpretation of
a transformation of SDS from the point of view of agent-
based simulation is that the transformation represents the
approximation of one simulation by another or the em-
bedding/projection of one simulation into/onto another.
These concepts have obvious utility when considering dif-
ferent simulations of the same complex system.
One can take diﬀerent points of view in deﬁning
a transformation of SDS. One approach is to require that

174 A
Agent Based Modeling, Mathematical Formalism for
a transformation is compatible with the deﬁning structural
elements of an SDS, that is, with the dependency graph, the
local update functions, and the update schedule. If this is
done properly, then one should expect to be able to prove
that the resulting transformation induces a transformation
at the level of phase spaces. That is, transformations be-
tween SDS should preserve the local and global dynamic
behavior. This implies that transformations between SDS
lead to transformations between the associated global up-
date functions.
Since the point of view of SDS is that global dynamics
emerges from system properties that are deﬁned locally,
the notion of SDS transformation should focus on the local
structure. This is the point of view taken in [48]. The deﬁ-
nition given there is rather technical and the details are be-
yond the scope of this article. The basic idea is as follows.
Let ˚	 D f	(n) ı    ı f	(1) and ˚
 D g
(m) ı    ı g
(1)
with the dependency graphs Y	 and Y, respectively.
A transformation F : ˚	 ! ˚
 is determined by
 a graph mapping ' : Y	 ! Y (reverse direction);
 a family of maps k(v) ! kv with v 2 Y	;
 an order preserving map  7!  of update schedules.
These maps are required to satisfy the property that they
“locally” assemble to a coherent transformation. Using this
deﬁnition of transformation, it is shown (Theorem 2.6
in [48]) that the class of SDS forms a category. One of the
requirements, for instance, is that the composition of two
transformations is again a transformation. Furthermore, it
is shown (Theorem 3.2 in [48]) that a transformation of
SDS induces a map of directed graphs on the phase spaces
of the two systems. That is, a transformation of the lo-
cal structural elements of SDS induces a transformation of
global dynamics. One of the results proven in [48] is that
every SDS can be decomposed uniquely into a direct prod-
uct (in the categorical sense) of indecomposable SDS.
Another possible point of view is that a transformation
F : (˚	 : kn ! kn) ! (˚ : km ! km)
is a function F : kn ! km such that F ı ˚	 D ˚ ı F,
without requiring speciﬁc structural properties. This is the
approach in [74]. This deﬁnition also results in a category,
and a collection of mathematical results. Whatever deﬁni-
tion chosen, much work remains to be done in studying
these categories and their properties.
Future Directions
Agent-based computer simulation is an important method
for modeling many complex systems, whose global dy-
namics emerges from the interaction of many local enti-
ties. Sometimes this is the only feasible approach, espe-
cially when available information is not enough to con-
struct global dynamic models. The size of many realis-
tic systems, however, leads to computer models that are
themselves highly complex, even if they are constructed
from simple software entities. As a result it becomes chal-
lenging to carry out veriﬁcation, validation, and analysis
of the models, since these consist in essence of complex
computer programs. This chapter argues that the appro-
priate approach is to provide a formal mathematical foun-
dation by introducing a class of mathematical objects to
which one can map agent-based simulations. These objects
should capture the key features of an agent-based simu-
lation and should be mathematically rich enough to al-
low the derivation of general results and techniques. The
mathematical setting of dynamical systems is a natural
choice for this purpose.
The class of ﬁnite dynamical systems over a state set X
which carries the structure of a ﬁnite ﬁeld satisﬁes all
these criteria. Parallel, sequential, and stochastic versions
of these are rich enough to serve as the mathematical basis
for models of a broad range of complex systems. While ﬁ-
nite dynamical systems have been studied extensively from
an experimental point of view, their mathematical theory
should be considered to be in its infancy, providing a fruit-
ful area of research at the interface of mathematics, com-
puter science, and complex systems theory.
Bibliography
Primary Literature
1. Bagrodia RL (1998) Parallel languages for discrete-event simu-
lation models. IEEE Comput Sci Eng 5(2):27–38
2. Barrett CL, Reidys CM (1999) Elements of a theory of simula-
tion I: Sequential CA over random graphs. Appl Math Comput
98:241–259
3. Barrett CL, Mortveit HS, Reidys CM (2000) Elements of a the-
ory of simulation II: Sequential dynamical systems. Appl Math
Comput 107(2–3):121–136
4. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,
Stearns RE, Tosic P (2001) Garden of eden and fixed point con-
figurations in sequential dynamical systems. In: Proc Interna-
tional Conference on Combinatorics, Computation and Geom-
etry DM-CCG’2001. pp 95–110
5. Barrett CL, Mortveit HS, Reidys CM (2001) Elements of a the-
ory of simulation III: Equivalence of SDS. Appl Math Comput
122:325–340
6. Barrett CL, Marathe MV, Smith JP, Ravi SS (2002) A mobility
and traffic generation framework for modeling and simulat-
ing ad hoc communication networks. In: SAC’02 Madrid, ACM,
pp 122–126
7. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,
Stearns RE (2003) On some special classes of sequential dy-
namical systems. Ann Comb 7(4):381–408
8. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,

Agent Based Modeling, Mathematical Formalism for
A
175
Stearns RE (2003) Reachability problems for sequential dy-
namical systems with threshold functions. Theor Comput Sci
295(1–3):41–64
9. Barrett CL, Mortveit HS, Reidys CM (2003) Elements of a the-
ory of computer simulation. IV. sequential dynamical systems:
fixed points, invertibility and equivalence. Appl Math Comput
134(1):153–171
10. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,
Stearns RE (2006) Complexity of reachability problems for fi-
nite discrete sequential dynamical systems. J Comput Syst Sci
72:1317–1345
11. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,
Stearns RE, Thakur M (2007) Computational aspects of analyz-
ing social network dynamics. In: Proc International Joint Con-
ference on Artificial Intelligence IJCAI 2007. pp 2268–2273
12. Barrett CL, Hunt III HB, Marathe MV, Ravi SS, Rosenkrantz DJ,
Stearns RE, Thakur M (2007) Predecessor existence problems
for finite discrete dynamical systems. Theor Comput Sci 1-2:
3–37
13. Bartlett R, Garzon M (1993) Monomial cellular automata. Com-
plex Syst 7(5):367–388
14. Bernaschi M, Castiglione F (2002) Selection of escape mutants
from immune recognition during hiv infection. Immunol Cell
Biol 80:307–313
15. Bernaschi M, Succi S, Castiglione F (2000) Large-scale cellular
automata simulations of the immune system response. Phys
Rev E 61:1851–1854
16. Booch G, Rumbaugh J, Jacobson I (2005) Unified Modeling
Language User Guide, 2nd edn. Addison-Wesley, Reading
17. Brand D, Zafiropulo P (1983) On communicating finite-state
machines. J ACM 30:323–342
18. Cartier P, Foata D (1969) Problemes combinatoires de com-
mutation et reárrangements. Lecture Notes in Mathematics,
vol 85. Springer, Berlin
19. Castiglione F, Agur Z (2003) Analyzing hypersensitivity to
chemotherapy in a cellular automata model of the immune
system. In: Preziosi L (ed) Cancer modeling and simulation.
Chapman and Hall/CRC, London
20. Castiglione F, Bernaschi M, Succi S (1997) Simulating the Im-
mune Response on a Distributed Parallel Computer. Int J Mod
Phys C 8:527–545; 10.1142/S0129183197000424
21. Castiglione F, Duca K, Jarrah A, Laubenbacher R, Hochberg D,
Thorley-Lawson D (2007) Simulating Epstein–Barr virus infec-
tion with C-ImmSim. Bioinformatics 23(11):1371–1377
22. Celada F, Seiden P (1992) A computer model of cellular inter-
actions in the immune syste. Immunol Today 13(2):56–62
23. Celada F, Seiden P (1992) A model for simulating cognate
recognition and response in the immune system. J Theor Biol
158:235–270
24. Celada F, Seiden P (1996) Affinity maturation and hypermuta-
tion in a simulation of the humoral immune response. Eur J Im-
munol 26(6):1350–1358
25. Chaudhuri PP (1997) Additive Cellular Automata. Theory and
Applications, vol 1. IEEE Comput Soc Press
26. Colón-Reyes O, Laubenbacher R, Pareigis B (2004) Boolean
monomial dynamical systems. Ann Comb 8:425–439
27. Colón-Reyes O, Jarrah A, Laubenbacher R, Sturmfels B (2006)
Monomial dynamical systems over finite fields. Complex Syst
16(4):333–342
28. Dawson D (1974) Synchronous and asynchronous reversible
Markov systems. Canad Math Bull 17(5):633–649
29. Ebeling W, Schweitzer F (2001) Swarms of particle agents with
harmonic interactions. Theor Biosci 120–3/4:207–224
30. Elspas B (1959) The theory of autonomous linear sequential
networks. IRE Trans Circuit Theor, pp 45–60
31. Farmer J, Packard N, Perelson A (1986) The immune system,
adaptation, and machine learning. Phys D 2(1–3):187–204
32. Frish U, Hasslacher B, Pomeau Y (1986) Lattice-gas automata
for the Navier–Stokes equations. Phys Rev Lett 56:1505–1508
33. Fuk´s H (2004) Probabilistic cellular automata with conserved
quantities. Nonlinearity 17:159–173
34. Garcia LD, Jarrah AS, Laubenbacher R (2006) Sequential dy-
namical systems over words. Appl Math Comput 174(1):
500–510
35. Gardner M (1970) The fantastic combinations of John Con-
way’s new solitaire game “life”. Sci Am 223:120–123
36. Gouda M, Chang C (1986) Proving liveness for networks of
communicating finite-state machines. ACM Trans Program
Lang Syst 8:154–182
37. Guo Y, Gong W, Towsley D (2000) Time-stepped hybrid simu-
lation (tshs) for large scale networks. In: INFOCOM 2000. Nine-
teenth Annual Joint Conference of the IEEE Computer and
Communications Societies. Proc. IEEE, vol 2. pp 441–450
38. Gupta A, Katiyar V (2005) Analyses of shock waves and jams in
traffic flow. J Phys A 38:4069–4083
39. Hansson AÅ, Mortveit HS, Reidys CM (2005) On asynchronous
cellular automata. Adv Complex Syst 8(4):521–538
40. Hedlund G (1969) Endomorphisms and automorphisms of the
shift dynamical system. Math Syst Theory 3:320–375
41. Hernández-Toledo A (2005) Linear finite dynamical systems.
Commun Algebra 33(9):2977–2989
42. Hopfield J (1982) Neural networks and physical systems with
emergent collective computational properties. Proc National
Academy of Sciences of the USA 79:2554–2588
43. Ilachinsky A (2001) Cellular Automata: A Discrete Universe.
World Scientific, Singapore
44. Jarrah A, Laubenbacher R, Stillman M, Vera-Licona P (2007) An
efficient algorithm for the phase space structure of linear dy-
namical systems over finite fields. (submitted)
45. Jefferson DR (1985) Virtual time. ACM Trans Program Lang Syst
7(3):404–425
46. Kari J (2005) Theory of cellular automata: A survey. Theor Com-
put Sci 334:3–33
47. Keyfitz BL (2004) Hold that light! modeling of traffic flow by
differential equations. Stud Math Libr 26:127–153
48. Laubenbacher R, Pareigis B (2003) Decomposition and sim-
ulation of sequential dynamical systems. Adv Appl Math 30:
655–678
49. Lidl R, Niederreiter H (1997) Finite Fields. Cambridge University
Press, Cambridge
50. Liggett TM (2005) Interacting particle systems. Classics in
Mathematics. Springer, Berlin. Reprint of the 1985 original
51. Lind DA (1984) Applications of ergodic theory and sofic sys-
tems to cellular automata. Physica D 10D:36–44
52. Lindgren K, Moore C, Nordahl M (1998) Complexity of two-
dimensional patterns. J Statistical Phys 91(5–6):909–951
53. Mac Lane S (1998) Category Theory for the Working Mathe-
matician, 2nd edn. No 5. in GTM. Springer, New York
54. Macy MW, Kitts JA, Flache A (2003) Polarization in Dynamic
Networks: A Hopfield Model of Emergent Structure. In: Dy-
namic social network modeling and analysis. The National
Academies Press, Washington D.C., pp 162–173

176 A
Agent Based Modeling and Neoclassical Economics: A Critical Perspective
55. Martin O, Odlyzko A, Wolfram S (1984) Algebraic properties of
cellular automata. Commun Math Phys 93:219–258
56. Milligan D, Wilson M (1993) The behavior of affine boolean se-
quential networks. Connect Sci 5(2):153–167
57. Minar N, Burkhart R, Langton C, Manor A (1996) The
swarm simulation system: A toolkit for building multi-agent
simulations. Santa Fe Institute preprint series. http://www.
santafe.edu/research/publications/wpabstract/199606042 Ac-
cessed 11 Aug 2008
58. Misra J (1986) Distributed discrete-event simulation. ACM
Comput Surv 18(1):39–65
59. Moncion T, Hutzler G, Amar P (2006) Verification of bio-
chemical agent-based models using petri nets. In: Robert
T (ed) International Symposium on Agent Based Modeling
and Simulation, ABModSim’06, Austrian Society for Cyber-
netics Studies, pp 695–700; http://www.ibisc.univ-evry.fr/pub/
basilic/OUT/Publications/2006/MHA06
60. Morpurgo D, Serentha R, Seiden P, Celada F (1995) Mod-
elling thymic functions in a cellular automaton. Int Immunol 7:
505–516
61. Mortveit HS, Reidys CM (2001) Discrete, sequential dynamical
systems. Discret Math 226:281–295
62. Mortveit HS, Reidys CM (2004) Reduction of discrete dynamical
systems over graphs. Adv Complex Syst 7(1):1–20
63. Nagel K, Schreckenberg M (1992) A cellular automaton model
for freeway traffic. J Phys I 2:2221–2229
64. Nagel K, Wagner P (2006) Traffic Flow: Approaches to Mod-
elling and Control. Wiley
65. Nagel K, Schreckenberg M, Schadschneider A, Ito N (1995)
Discrete stochastic models for traffic flow. Phys Rev E 51:
2939–2949
66. Nagel K, Rickert M, Barrett CL (1997) Large-scale traffic simu-
lation. Lecture notes in computer science, vol 1215. Springer,
Berlin, pp 380–402
67. Nance RE (1993) A history of discrete event simulation pro-
gramming languages. ACM SIGPLAN Notices 28:149–175
68. North MJ, Collier NT, Vos JR (2006) Experiences creating three
implementations of the repast agent modeling toolkit. ACM
Trans Modeling Comput Simulation 16:1–25
69. Orponen P (1994) Computational complexity of neural net-
works: A survey. Nordic J Comput 1:94–110
70. Orponen P (1996) The computational power of discrete hop-
field networks with hidden units. Neural Comput 8:403–415
71. Park JK, Steiglitz K, Thruston WP (1986) Soliton-like behavior in
automata. Physica D 19D:423–432
72. Reidys C (1998) Acyclic Orientations of Random Graphs. Adv
Appl Math 21:181–192
73. Reidys CM (2001) On acyclic orientations and sequential dy-
namical systems. Adv Appl Math 27:790–804
74. Reidys CM (2005) On Certain Morphisms of Sequential Dynam-
ical Systems. Discret Math 296(2–3):245–257
75. Reidys CM (2006) Sequential dynamical systems over words.
Ann Combinatorics 10(4):481–498
76. Rickert M, Nagel K, Schreckenberg M, Latour A (1996) Two
lane traffic simulations using cellular automata. Physica A 231:
534–550
77. Rothman DH (1988) Cellular-automaton fluids: A model for
flow in porous media. Geophysics 53:509–518
78. Russell S, Norwig P (2003) Artificial Intelligence: A Modern Ap-
proach. Prentice-Hall, Upper Saddle River
79. Schönfisch B, de Roos A (1999) Synchronous and asyn-
chronous updating in cellular automata. BioSystems 51:
123–143
80. Shmulevich I, Dougherty ER, Kim S, Zhang W (2002) Probabilis-
tic boolean networks: A rule-based uncertainty model for gene
regulatory networks. Bioinformatics 18(2):261–274
81. Shmulevich I, Dougherty ER, Zhang W (2002) From boolean to
probabilistic boolean networks as models of genetic regula-
tory networks. Proc IEEE 90(11):1778–1792
82. Sipser M (1997) Introduction to the Theory of Computation.
PWS Publishing Co, Boston
83. Vasershtein L (1969) Markov processes over denumerable
products of spaces describing large system of automata. Probl
Peredachi Informatsii 5(3):64–72
84. von Neumann J, Burks AW (ed) (1966) Theory of Self-Reproduc-
ing Automata. University of Illinois Press, Champaign
85. Whitham G (1999) Linear and Nonlinear Waves, reprint edition
edn. Pure and Applied Mathematics: A Wiley-Interscience Se-
ries of Texts, Monographs and Tracts, Wiley-Interscience, New
York
86. Wolfram S (1983) Statistical mechanics of cellular automata.
Rev Mod Phys 55:601–644
87. Wolfram S (1986) Theory and Applications of Cellular Au-
tomata, Advanced Series on Complex Systems, vol 1. World
Scientific Publishing Company, Singapore
88. Wolfram S (2002) A New Kind of Science. Wolfram Media,
Champaign
Books and Reviews
Hopcroft JE, Motwani R, Ullman JD (2000) Automata Theory, Lan-
guages and Computation. Addison Wesley, Reading
Kozen DC (1997) Automata and Computability. Springer, New York
Wooldridge M (2002) Introduction to Multiagent Systems. Wiley,
Chichester
Agent Based Modeling and
Neoclassical Economics:
A Critical Perspective*
SCOTT MOSS
Centre for Policy Modeling, Manchester Metropolitan
University Business School, Manchester, UK
Article Outline
Introduction
Economic Modeling Approaches
Methodological Issues
Conditions for Complexity
Complexity and the Role of Evidence
Future Directions
Bibliography
The remarks about neoclassical economics are drawn from my
inaugural lecture [28]

Agent Based Modeling and Neoclassical Economics: A Critical Perspective
A
177
Introduction
Agent Based Modeling and Neoclassical Economics based
modeling naturally generates complexity whereas neoclas-
sical economics is incompatible in principle with complex-
ity. The reasons that preclude complexity in neoclassical
economic models also ensure that neoclassical economics
cannot describe any society ever observed or that could
ever be observed.
The meaning of complexity has been developed,
mainly by physicists, to cover unpredictable, episodic
volatility and also particular network topologies. In both
cases there are nodes representing the components of
a system and there are links among the components that
can represent interactions amongst those components.
Unpredictable, episodic volatility can result from particu-
lar forms of behavior by components and the interactions
amongst those components. I am not aware of any investi-
gations into relationships between that type of complexity
and network topology.
The point to be made here is that the core assumptions
and the methodology of conventional neoclassical eco-
nomics preclude the emergence of episodic volatility and
render social network topology inconsequential. When
elaborated with heterogeneous agents, network topologies
might have some eﬀects on the outputs from computa-
tional neoclassical economic models – but, again, I am not
aware of any systematic investigations into this possibility.
Economic Modeling Approaches
All deﬁnitions of complexity take for granted that there
will be some speciﬁcations of individual components, that
in general each component will interact with some other
components and there will be some macro level phenom-
ena that could not be described or understood except on
the basis of the components and their interactions. The
Agent Based Modeling and Neoclassical Economics: A Critical
Perspective, Figure 1
Constraints on model designs
purpose of this section is to categorize the ways in which
economists, agent based modelers and complexity scien-
tists approach this micro-macro issue.
There are several strands in the economics and social
sciences literatures for building macro analyzes explicitly
on micro foundations. The main strands are computable
general equilibrium (CGE), agent based computational
economics (ACE), agent based social simulation (ABSS)
and complexity science (CS) including econophysics and
sociophysics. These strands are not all mutually exclusive
although there are some conﬂicting elements among sev-
eral of them.
Computable General Equilibrium
CGE is the most theoretically constrained of the four
strands under consideration. As with general equilibrium
theory, it is predicated on the assumptions that house-
holds maximize utility and ﬁrms maximize proﬁts and that
markets clear. The computational load associated with ex-
plicit representation of every household and ﬁrm leads to
the adoption of representative agents intended to capture
the behavior of a group such as all households or ﬁrms
in a particular industrial sector. Some CGE models rep-
resent technology with input-output tables; others with
marginalist production functions.
Agent Based Computational Economics
An objection to the representative agent device is raised
in the ACE literature where the eﬀects of agent hetero-
geneity are explored. In these models, households can dif-
fer in their utility functions (or at least the parameters of
those functions) or agents can adopt diﬀerent game theo-
retic strategies and ﬁrms can employ diﬀerent production
functions. The theoretical core of ACE is not essentially
diﬀerent from that of CGE, both relying on conventional
economic theory.
Agent Based Social Simulation
Models reported in the ABSS literature are by and large
not driven by traditional theoretical concerns. There is
a very wide range in the degree of empirical content: many
models being developed to explore “stylized facts”, others
driven by qualitative case studies. The latter are often vali-
dated against both qualitative micro level data provided by
stakeholders and against macro level statistical data.
Complexity Science
Because neoclassical economic theory excludes social em-
beddedness, the social complexity research that could be

178 A
Agent Based Modeling and Neoclassical Economics: A Critical Perspective
relevant to a consideration of neoclassical economics must
be concerned with unpredictable, episodic turbulence. The
CS literature on ﬁnancial markets was seminal and re-
mains well known. The interest in ﬁnancial markets goes
back to Mandelbrot [25] who used ﬁnancial market data
both because it exhibits “outliers” in the relative change
series and because of the ﬁneness of the time grain of
the price and volume data. A seminal article by Palmer
et al. [35] reported a simulation model in which individ-
uals were represented by an early form of software agent
and which produced time series marked by the occasional
episodes of volatility of the sort observed in real ﬁnancial
market data. However, similar unpredictable episodes of
turbulence and volatility have emerged in models of the
early post-Soviet Russian economy [31], domestic water
consumption [8,15] and models of transactions in inter-
mediated markets [29]. Fine grain data exhibiting the same
patterns of volatility were found subsequent to the original
publication of each model.
Relationships Among the Four Approaches
The common thread between CGE and ACE is their com-
mon reliance on longstanding economic concepts of util-
ity, continuous production functions, proﬁt maximiza-
tion, the use of game theory et sic hoc omnes. The com-
mon thread between ABSS and complexity science is the
importance of social interaction and speciﬁcations of in-
dividual behavior that are either more ad hoc or based on
detailed qualitative evidence for speciﬁc cases.
Complex, as distinct from rational, agents’ behavior is
“sticky”: it takes non-trivial events or social pressures to
make them change. This is the social meaning of metasta-
bility. They are also socially embedded in the sense that
they interact densely with other agents and are inﬂuenced
by some of those other agents, generally speaking others
who are most like themselves and who they have reason
to like and respect [10]. The social diﬀerence between in-
ﬂuence and imitation is the social equivalent of the physi-
cal diﬀerence between dissipative and non-dissipative pro-
cesses. Of course, such inﬂuence is meaningless unless the
agents diﬀer in some way – they must be heterogeneous.
Methodological Issues
Neoclassical economic theory has no empirically based
micro foundation. It has agents of two types: households
that maximize utility and ﬁrms that maximize proﬁts.
Time and expectations are allowed to inﬂuence these max-
imization processes by substituting “expected utility” or
“expected proﬁts” for the realized magnitudes. In such
models, agents (households or ﬁrms) act as if they know
with certainty a population distribution of possible out-
comes from their actions. In the terminology introduced
by Knight [23], risk pertains when the agent knows the fre-
quency distribution of past outcomes that, as in actuarial
contexts, are expected with conﬁdence to pertain in the fu-
ture. When no such frequency distribution is known, then
uncertainty prevails. In the sense of Knight (though the
terminology gets muddled in the economics literature),
the assumption that agents maximize expected utility or
expected proﬁts is tenable in conditions of risk but not
in conditions of uncertainty. Moreover, it has long been
known (with Nobel prizes awarded to Allais [4] and to
Daniel Kahneman of Kahneman and Tversky [21] for the
demonstrations) that individuals do not act as if they were
maximizers of utility or expected utility. Nor is there any
evidence that enterprises actually maximize proﬁts. Many
economists acknowledge that rationality is bounded and
that we lack the cognitive capacity to absorb the required
amount of information and then to process that infor-
mation in order to identify some optimal decision or ac-
tion. This has given rise to a variety of schools of eco-
nomic thought such as evolutionary economics (Nelson
and Winter [32] is the seminal work here) and Keynesian
economics [22] being amongst the best known.
There is evidently a recognizable (and often recog-
nized) divide between the behavioral assumptions of neo-
classical economics on the one hand and, on the other
hand, common observation, experimental observation
(cf. [5]) and a host of business histories (the work of Chan-
dler [12,13] and Penrose [37] being surely the most inﬂu-
ential). The evidence shows that the assumptions of neo-
classical economics are inaccurate descriptions of the be-
havior the theories and models are purported to represent.
There are two classes of defense for these descriptively in-
accurate assumptions. On is the as-if defense and the other
is the for-simplicity defense. These are considered in turn.
The as-if defense was enunciated in several forms by
Samuelson [39], Friedman [17] and Alchian [3] in the
years around 1950. The details of the diﬀerences between
Samuelson and Friedman are not germane here. Both ar-
gued that their purpose was to model aggregate economic
entities such as markets or national economies and de-
scriptively inaccurate assumptions at micro level are per-
missible provided that the models are descriptively accu-
rate at macro level. Alchian’s contribution was to propose
a mechanism. He asserted that, at least in the case of ﬁrms,
those that were more proﬁtably would be more likely to
survive than ﬁrms that were less proﬁtable. Consequently,
over time, more and more ﬁrms would approach more
closely to the maximum of proﬁts available to them so that,
even if they did not actually seek to maximize proﬁts, the

Agent Based Modeling and Neoclassical Economics: A Critical Perspective
A
179
surviving population of ﬁrms would be those that implic-
itly did actually maximize proﬁts.
The as-if defense is in practice an equilibrium argu-
ment. Neoclassical economic models are solved for the
simultaneous maximization of utility and proﬁts by all
agents – or, at least it is proved that such a solution ex-
ists. In such a conﬁguration, no agent has any incentive
to change its behavior so the equilibrium presumed to be
stable in the small (that is, once reached it is maintained).
There is no proof that any general equilibrium model with
an arbitrary number of agents is stable in the large (that
is, that any feasible solution is an attractor of the system
as a whole). The Alchian form of the as-if defense does
not take into account any eﬀects of agent interaction or
inﬂuence of any agent by any other agent. In empirical –
that is to say, econometric – testing of neoclassical models,
extreme events and observations are dismissed as outliers
and removed from the data set being used for the testing
or else their eﬀect is encapsulated by specially constructed
dummy variables.
The for-simplicity defense rests on the presumption
that simpler models are always to be preferred to more
complicated models and the achievement of simplicity jus-
tiﬁes making assumptions about behavior and environ-
ment that are not justiﬁed by evidence. The author has for
many years now justiﬁed this claim by choosing any ar-
bitrary leading economics journal and searching the most
recent issue for an assumption made “for simplicity”. On
every occasion, the assumption made “for simplicity” has
turned out to be an assumption that changed the nature
of an empirical problem being addressed so that it con-
formed to the requirements (such as convexity or absence
of externalities) of the mathematical technique being ap-
plied to the analysis. Seven of the eleven papers in, at the
time of writing, the most recent (November 2007) issue of
the Quarterly Journal of Economics appealed to the value
of simplicity or, in one case, tractability to justify assump-
tions or speciﬁcations that were not justiﬁed empirically.
The direct quotations are:
Tractability obviously dictated the use of a simple
summary statistic of the distribution of legislator
ideologies (see p. 1418 in [14]).
The fact, established below, that deriving the prop-
erties of the seats-votes relationship requires consid-
eration only of the properties of the univariate dis-
tribution of  as opposed to those of the bivariate
distribution of  and  considerably simpliﬁes the
analysis (see p. 1480 in [9]).
We make three key assumptions to simplify the
analysis. First, we assume that all jobs last indef-
initely once found (i. e., there is no subsequent
job destruction). Second, anticipating our empiri-
cal ﬁndings, we assume that wages are exogenously
ﬁxed, eliminating reservation-wage choices. Third,
we assume that utility is separable in consumption
and search eﬀort (see p. 1516 in [11]).
A more conventional timing assumption in search
models without savings is that search in period t
leads to a job that begins in period t C 1. Assuming
that search in period t leads to a job in period t itself
simpliﬁes the analytic expressions : : : (see p. 1517
in [11]).
For simplicity, we’ll assume that ˇSAT

Xi;s

; ˇTEST

Xi;s

and ˇOTH

Xi;s

are linear in Xi;s and can
thus be written .... We will further assume that
the random utility component is independent and
identically distributed (i. i. d.) from a type 1 extreme
value distribution (see p. 1616 in [19]).
For simplicity, all households represent two-earner
married couples of the same age (see p. 1683
in [33]).
For simplicity, the model assumes that the high-
est 35 years of earnings correspond to the ages be-
tween 25 and 59 (see p. 1685 in [33]).
We follow Auerbach and Kotlikoﬀ(1987) by mea-
suring eﬃciency gains from social security privati-
zation using an LSRA that compensates households
that would otherwise lose from reform. To be clear,
the LSRA is not being proposed as an actual govern-
ment institution. Instead, it is simply a hypothetical
mechanism that allows us to measure the standard
Hicksian eﬃciency gains in general equilibrium as-
sociated with privatization (see p. 1687 in [33]).
Assume for simplicity that these batch sizes are
ﬁxed for each product class .... Given these ﬁxed
batch sizes for the two classes of product, the ﬁrm
maximizes proﬁts by deciding how many produc-
tion runs : : : [to] undertake : : : (see pp. 1731–1732
in [7]).
We adopt a number of simplifying assumptions to
focus on the main implications of this framework.
First, we assume that the relationship between the
ﬁrm and each manager is short-term. Second, when
xi;k D zi;k, the manager obtains a private beneﬁt.
We assume that managers are credit-constrained
and cannot compensate principals for these private
beneﬁts and that these private beneﬁts are suﬃ-
ciently large so that it is not proﬁtable for the princi-
pal to utilize incentive contracts to induce managers

180 A
Agent Based Modeling and Neoclassical Economics: A Critical Perspective
to take the right action. These assumptions imply
that delegation will lead to the implementation of
the action that is preferred by the manager : : : (see
p. 1769 in [1]).
All but the ﬁrst of these quotations are from theoreti-
cal papers and the “simpliﬁcations” enable the authors
to produce equilibrium solutions to their models. No
one has ever knowingly observed an equilibrium and in
a world where not everything is convex due to (for exam-
ple) economies of large scale production and where com-
putational capacities limit cognitive abilities, in principle
no equilibrium ever will be observed. Indeed, Radner [38]
showed that a necessary condition for general equilibrium
to exist is that all agents have unlimited computational ca-
pacities if trading takes place at a sequence of dates. In the
core general equilibrium model, all transactions are agreed
at a single moment for all time. The “simpliﬁcations” re-
quired to produce equilibrium models cannot therefore
be justiﬁed on the basis of relevance to empirical obser-
vation. They also ensure that the models cannot capture
complexity.
Conditions for Complexity
The social and behavioral conditions for complexity man-
ifest as unpredictable, episodic volatility appears to be the
following:
 Individuals behave in routine ways unless some non-
trivial event or events or social pressure from other in-
dividuals induce them to change their behavior.
 Individuals interact with other individuals.
 Individuals inﬂuence but do not generally imitate one
another.
 Interactions amongst individuals and individual be-
havior are not dominated by events that do not arise
from that interaction and behavior.
These conditions were ﬁrst noticed as a general pheno-
menon in physical models and articulated by Jensen [20]
as metastability, dense interaction, dissipation, and cold-
ness of the system, respectively. The phenomenon of un-
predictable, clustered volatility in social models had pre-
viously been noticed as had its similarity to self organized
criticality as described by statistical physicists starting with
Bak et al. [6].
Complexity and Social Volatility
Volatility in social statistical time series and power law dis-
tributed cross sectional data have long been observed by
statisticians and social scientists. Vilfredo Pareto [36] dis-
covered that the personal distribution of income is power
law distributed, a ﬁnding which has been replicated widely
across countries and time. The same phenomenon is now
well known to characterize word use [43] city sizes [44],
ﬁrm sizes (including market shares) [41], distributions
of links between internet sites [2] and a host of other
cross sectional distributions. Where ﬁrm sizes and mar-
ket shares are concerned, there have been strands in the
industrial economics literature reporting models yielding
that result. However, the observed results have not been
explained by models in which households maximize util-
ity and ﬁrms maximize proﬁts. As Simon and Bonini [41]
point out, some variant to Gibrat’s Law (or the law of pro-
portional eﬀect), which states that the growth rate of indi-
viduals (say ﬁrms) is not correlated with individual size,
will generate one highly skewed distribution or another
and the particular distribution can be reﬁned by an appro-
priate choice of the representation of the law.
These desired results also emerged from a series of
models based on plausible or empirically based speciﬁca-
tions of individual behavior and social interaction in agent
based social simulation models. In capturing stakehold-
ers’ perceptions of the behavior and social interactions of
relevant classes of individuals and also in relying on well
validated propositions from social psychology and cogni-
tive science, models were implemented that produced the
sort of skewed distributions that we observe in practice.
Episodic volatility followed from the metastability and so-
cial embeddedness of agents. In the nature of this process,
most changes are relatively small in magnitude but a few
changes are very large. This results in fat-tailed distribu-
tions of relative changes in variable values at macro level
and also in cross sectional data as described by Gibrat’s
Law. In practice, the large relative changes tend to be
bunched together in unpredictable episodes of volatility.
While these results arise naturally in evidence-driven
ABSS models, they are not easily reconciled with neoclas-
sical economic theory. As Krugman [24] (quoted by Eeck-
hout [16]) had it, “We have to say that the rank-size rule
is a major embarrassment for economic theory: one of
the strongest statistical relationships we know, lacking any
clear basis in theory”.
Complexity and Social Network Topology
Social network topologies can obviously have no meaning
in a model comprised by representative agents. In ACE,
ABSS and CS models, there will always be a network of
social interaction. However, the nature of the interaction
can be very diﬀerent across the diﬀerent approaches.

Agent Based Modeling and Neoclassical Economics: A Critical Perspective
A
181
In ACE models, it is common to ﬁnd social inter-
action taking the form of games – typically the Prison-
ers’ Dilemma. A nice example of such a model is Tesfat-
sion’s labor market model using McFadzean and Tesfat-
sions’s [26,42] Trading Network Game. In Tesfatsions’s
labour market model, there is a ﬁxed number of work-
ers and a ﬁxed number of employers identical in their
total oﬀers of labour and of employment, respectively.
Each worker (resp. employer) ascribes a value of utility
to an employment arrangement with any employer (resp.
worker). The utility starts out at a some exogenous value
and is then increased or reduced depending on the expe-
rience at each trading date. The experience is the combi-
nation of cooperation and defection by each party to the
employment relation at each time step. The social network
in this model
... is represented in the form of a directed graph
in which the vertices V(E) of the graph represent
the work suppliers and employers, the edges of the
graph (directed arrows) represent work oﬀers di-
rected from work suppliers to employers, and the
edge weight on any edge denotes the number of
accepted work oﬀers (contracts) between the work
supplier and employer connected by the edge (see
p. 431 in [42]).
The topology of this network depends on the outcomes of
sequences of prisoners’ dilemma games determining the
utilities of workers and employers to one another. Every
worker can see every employer and conversely so that the
directed links between agents are limited by the number
of work contracts into which each agent can engage. After
some arbitrary number of time steps, the strategies of the
agents are represented as genes and a genetic algorithm
is applied so that, over a whole simulation, the elements
of the most successful defect/cooperate strategies become
more dominant. Since these strategies determine the out-
comes of the prisoners’ dilemma games, the social network
continues to evolve with utility enhancing strategies be-
coming more dominant.
In a recent (at the time of writing) issue of The Journal
of Economic Dynamics and Control, Page and Tassier [34]
modeled the development of chain stores across markets.
A ﬁrm was deﬁned by its product. Each product was as-
signed an “intrinsic quality” represented by an integer
drawn at random from a distribution  q, and a set of
I “hedonic attributes” represented by I positive integers
in a range from 0 to some arbitrary, user selected num-
ber A. Consumers are represented by utility functions that
are positively related to “quality” and negatively related
to the diﬀerence between some desired set of hedonic at-
tributes and the hedonic attributes of the product. There
are a number (set by the model user) of discrete markets.
Page and Tassier then run a variety of simulations that
allow for ﬁrms to replicate themselves across markets or,
through lack of demand, to leave markets.
These two models seem to be representative of a wide
class of ACE models. In the ﬁrst place, agents are deﬁned
by utility functions or game theoretic strategies so that the
behavior of any individual agent is either ﬁxed or responds
smoothly to inﬁnitesimal changes in prices, incomes or
whatever other arguments might populate its utility func-
tion. In either event, agents cannot be metastable and fol-
low behavioral routines until (but only until) some signif-
icant stimulus causes them to change their behavioral re-
sponses. In the second place, agents’ preferences and re-
sponses are not inﬂuenced by the preferences or actions
of any other agents like themselves. That is, their behavior
as determined by their utility functions or game theoretic
strategies will respond to market signals or the actions of
the other agent in their game but not to communications
with or observations of any other agents. These agents are
not, in the words of Granovetter [18], socially embedded
especially since it is rare in a neoclassical model for there
to be more than two players in a game and unheard-of for
there to be more than three (cf. [27]).
Whilst we cannot state with authority that the con-
ditions of metastability, social inﬂuence and the consis-
tency principle are necessary for complexity to emerge at
macro level from micro level behavior, these conditions
have characterized the social simulation models that have
produced the episodic and unpredictable volatility associ-
ated with complexity. The absence of social embeddedness
in the neoclassical ACE models must also explain their lack
of any representation of social (as distinct from merely
economic) networks.
Complexity and the Role of Evidence
An interesting and fairly typical feature of papers report-
ing neoclassical models – both theoretical and computa-
tional with agents – is that they motivate the modeling ex-
ercise by appeal to some empirical, macro level economic
phenomenon and then ignore evidence about the micro
level behavior that might bring about such phenomena.
This practice can be seen in both of the ACE examples de-
scribed in Sect. “Conditions for Complexity”.
Tesfatsion [42] motivates her model on more theoret-
ical grounds than do Page and Tassier [34]. She wrote:
Understanding the relationship between market
structure, market behavior, and market power in

182 A
Agent Based Modeling and Neoclassical Economics: A Critical Perspective
markets with multiple agents engaged in repeated
strategic interactions has been a major focus of an-
alytical, empirical, and human-subject experimental
researchers in industrial organization since the early
1970s. To date, however, deﬁnitive conclusions have
been diﬃcult to obtain.
She goes on to cite “a uniﬁed theoretical treatment of
oligopoly decision-making”, an article on empirical ﬁnd-
ings with respect to market power that looks only at indus-
try level statistical measures, and some work with experi-
mental subjects. No references are made, either by Tesfat-
sion or those she cites, to any case studies of the “repeated
strategic interactions” in which the “multiple agents” en-
gage.
Page and Tassier give more historical detail. Their mo-
tivation turns on:
Chain stores and franchises dominate the Ameri-
can economic landscape. A drive through any mod-
erately sized city reveals remarkable conformity in
restaurants, stores, and service centers. Anyone who
so desires can eat at Applebee’s, shop at Wal-Mart,
and grab a Starbuck’s latte grande while getting her
car brakes done at Midas (see p. 3428 in [34]).
For example, in many markets, Lowe’s and Home
Depot capture a signiﬁcant portion of the home
improvement market. These big box stores drove
many small independent hardware stores and lum-
ber yards out of business. The residual demand
resides in niches that can be ﬁlled by hardware
chains specializing in upscale home furnishings like
Restoration Hardware. ... Often, when Lowe’s en-
ters a market, it creates a niche for Restoration
Hardware as well. And, as both Lowe’s and Restora-
tion Hardware enter more and more markets, they
in turn create additional common niches that can
be ﬁlled by even more chains. Thus, chains beget
chains (see p. 3429 in [34]).
So this article claims a clear and direct historical basis. And
yet
... To capture the increasing correlation in niches
formally, we introduce two new concepts, the niche
landscape and the diﬀerential niche landscape. The
former plots the quality required to enter a mar-
ket at a given set of hedonic attributes. The latter
plots the diﬀerences in two niche landscapes. In the
presence of chains, diﬀerential niche landscapes be-
come ﬂat, i. e. the niche landscapes become corre-
lated across markets (see p. 3429 in [34]).
The representation of the actors in this framework has
been discussed above. At no stage is the agent design dis-
cussed in relation to any empirical accounts of the behav-
ior and motivations of the managers of Wal-Mart, Star-
bucks, Lowes, Restoration Hardware or any other enter-
prise or any consumer.
This is, of course, the way of neoclassical economics
and it has extended to ACE research as well. What is per-
haps more unsettling is that it has also extended to the bas-
tions of complexity science – the econophysicists.
There is a long literature now on complexity and ﬁ-
nancial markets and also on complexity an the formation
of opinions – opinion dynamics. There are at least two
reasons for the popularity amongst physicists of ﬁnancial
market modeling. First, there are long series of very ﬁne
grain data. Second, the data exhibits the unpredictable,
episodic volatility associated with complexity. The popu-
larity of opinion dynamics cannot be based on the quality
of the data – even at macro level – because that quality is
much more coarse grain and inconsistent over time than
ﬁnancial market data. Nonetheless, the two literatures are
marked by the heavy presence and inﬂuence of physicists
and by the lack of connection between their agent designs
and any available evidence about the behavior of traders in
ﬁnancial markets or voters or others acting on or express-
ing their opinions.
A good example from the opinion dynamics litera-
ture – chosen at random from The European Physical Jour-
nal B – is by Schweitzer and Hoyst [40], “Modeling collec-
tive opinion formation by means of active Brownian par-
ticles”. The motivation for their article is
The formation of public opinion is among the chal-
lenging problems in social science, because it reveals
a complex dynamics, which may depend on diﬀer-
ent internal and external inﬂuences. We mention
the inﬂuence of political leaders, the biasing eﬀect
of mass media, as well as individual features, such as
persuasion or support for other opinions.
We immediately have complexity and social relevance to
motivate an article on social dynamics in a physics jour-
nal. However, there is no empirical justiﬁcation for mod-
eling individuals who form opinions as active Brownian
particles. The apparent complexity in the outcomes of so-
cial processes of opinion formation can be produced by
the non-linear feedbacks of ﬁelds of active Brownian par-
ticles. Whether individuals actually behave in this way is
not addressed by Schweitzer and Hoyst or, as far as I
know, by any contributor to the opinion dynamics liter-
ature.

Agent Based Modeling and Neoclassical Economics: A Critical Perspective
A
183
Much the same can be said of the econophysics liter-
ature on ﬁnancial markets. The clustered volatility associ-
ated with complexity is readily produced by physical mod-
els with characteristics of metastability, dissipation and
dense patterns of interaction. What the econophysicists
fail to address is the question of whether their particular
formulations – and active Brownian particle is just one of
many examples – are descriptively accurate representations
of the individual actors whose behavior they are seeking to
analyze.
In this regard, the econophysicists are not better sci-
entists than neoclassical economists. It can be said in fa-
vor of neoclassical (including ACE) economists that they
are at least following in a long tradition when they ig-
nore the relationship between what people actually do and
how agents are modeled. In the long history of the phys-
ical sciences, however, observation and evidence at micro
and macro level and all levels in between has dominated
theory (cf. [30]). There are some at least in the ABSS re-
search community who would prefer our colleagues with
backgrounds in the physical scientists to follow their own
methodological tradition in this regard and not that of the
neoclassical economists.
Future Directions
Complexity science is not a niche research interest in the
social sciences. Societies are complex and all social science
should be complexity science. However, any social science
that excludes social interaction and inertia or routine nec-
essarily suppresses complexity. As noted here, the adop-
tion of utility theory and representative agents by neo-
classical economists (and other social scientists inﬂuenced
by them) amounts to the exclusion of behavioral inertia
and social interaction, respectively. To drop both utility
and representative agents and to build analyzes bottom up
from a sound basis in evidence would produce a better –
very likely, a good – body of economic analysis. But the
transition from present convention would be enormous –
a transition that experience shows to be beyond the ca-
pacity of current and previous generations of mainstream
economists. Not only would they have to abandon theo-
ries that drive and constrain their research but also their
whole epistemological and wider methodological stance.
They would have to accept that prediction and forecasting
cannot be core methodological objectives and that theo-
ries are built by abstracting from detailed evidence based
social simulation models the designs and outputs from
which have been validated by stakeholders in a range of
contexts. This would be a future direction guided by good
science.
Bibliography
1. Acemoglu D, Aghion P, Lelarge C, Van Reenen J, Zili-
botti F (2007) Technology, information, and the decentral-
ization of the firm. Q J Econ 122(4):1759–1799. http://www.
mitpressjournals.org/doi/abs/10.1162/qjec.2007.122.4.1759
2. Adamic LA, Huberman BA (1999) Power-law distribution of the
World Wide Web. Science 287(5461):2115
3. Alchian AA (1950) Uncertainty, evolution and economic the-
ory. J Political Econ 58(2):211–221
4. Allais M (1953) Le comportement de l’homme rationnel devant
le risque: Critiques des postulats et axiomes de l’ecole ameri-
caine. Econometrica 21(4):503–546
5. Anderson JR (1993) Rules of the mind. Lawrence Erlbaum As-
sociates, Hillsdale
6. Bak P, Tang C, Weisenfeld K (1987) Self organized criticality: An
explanation of 1/f noise. Phys Rev Lett 59(4):381–384
7. Bartel A, Ichniowski C, Shaw K (2007) How does information
technology affect productivity? Plant-level comparisons of
product innovation, process improvement, and worker skills.
Q J Econ 122(4):1721–1758. http://www.mitpressjournals.org/
doi/abs/10.1162/qjec.2007.122.4.1721
8. Barthelemy O (2006) Untangling scenario components with
agent based modeling: An example of social simulations of
water demand forecasts. Ph D thesis, Manchester Metropolitan
University
9. Besley T, Preston I (2007) Electoral bias and policy choice: The-
ory and evidence. Q J Econ 122(4):1473–1510. http://www.
mitpressjournals.org/doi/abs/10.1162/qjec.2007.122.4.1473
10. Brown R (1965) Social psychology. The Free Press, New York
11. Card D, Chetty R, Weber A (2007) Cash-on-hand and com-
peting models of intertemporal behavior: New evidence
from the labor market. Q J Econ 122(4):1511–1560. http://
www.mitpressjournals.org/doi/abs/10.1162/qjec.2007.122.4.
1511
12. Chandler AD (1962) Strategy and structure: Chapters in the
history of the american industrial enterprise. MIT Press, Cam-
bridge
13. Chandler AD (1977) The visible hand: The managerial revolu-
tion in american business. Harvard University Press
14. Coate S, Brian K (2007) Socially optimal districting: A theoreti-
cal and empirical exploration. Q J Econ 122(4):1409–1471
15. Downing TE, Moss S, Pahl Wostl C (2000) Understanding cli-
mate policy using participatory agent based social simulation.
In: Moss S, Davidsson P (eds) Multi agent based social simula-
tion. Lecture Notes in Artificial Intelligence, vol 1979. Springer,
Berlin, pp 198–213
16. Eeckhout J (2004) Gibrat’s law for (all) cities. Am Econ Rev
94(5):1429–1451
17. Friedman M (1953) The methodology of positive economics.
In: Essays on Positive Economics. University of Chicago Press,
Chicago
18. Granovetter M (1985) Economic action and social structure:
The problem of embeddedness. Am J Sociol 91(3):481–510
19. Jacob BA, Lefgren L (2007) What do parents value in educa-
tion? An empirical investigation of parents’ revealed prefer-
ences for teachers. Q J Econ 122(4):1603–1637. http://www.
mitpressjournals.org/doi/abs/10.1162/qjec.2007.122.4.1603
20. Jensen H (1998) Self-organized criticality: Emergent complex
behavior in physical and biological systems. Cambridge Uni-
versity Press, Cambridge

184 A
Agent Based Modeling and Simulation
21. Kahneman D, Tversky A (1979) Prospect theory: An analysis of
decision under risk. Econometrica 47(2):263–292
22. Keynes JM (1935) The general theory of employment, interest
and money. Macmillan, London
23. Knight FH (1921) Risk, uncertainty and profit. Houghton-Mifflin
24. Krugman P (1995) Development, geography and economic
theory. MIT Press, Cambridge
25. Mandelbrot B (1963) The variation of certain speculative prices.
J Bus 36(4):394–419
26. McFadzean D, Tesfatsion L (1999) A c++ platform for the evo-
lution of trade networks. Comput Econ 14:109–134
27. Moss S (2001) Game theory: Limitations and an alternative.
J Artif Soc and Soc Simul 4(2)
28. Moss S (1999) Relevance, realism and rigour: A third way
for social and economic research. Technical Report 99-56,
Centre for Policy Modeling, Manchester Metropolitan Uni-
versity
29. Moss S (2002) Policy analysis from first principles. Proc US Nat
Acad Sci 99(Suppl. 3):7267–7274
30. Moss S, Edmonds B (2005) Towards good social science. J Artif
Soc and Soc Simul 8(4). ISSN 1460-7425. http://jasss.soc.surrey.
ac.uk/8/4/13.html
31. Moss S, Kuznetsova O (1996) Modeling the process of mar-
ket emergence. In: Owsinski JW, Nahorski Z (eds) Model-
ing and analyzing economies in transition. MODEST, Warsaw,
pp 125–138
32. Nelson RR, Winter SG (1982) An evolutionary theory of eco-
nomic change. Harvard University Press, Cambridge
33. Nishiyama S, Smetters K (2007) Does social security priva-
tization produce efficiency gains? Q J Econ 122(4):1677–
1719. http://www.mitpressjournals.org/doi/abs/10.1162/qjec.
2007.122.4.1677
34. Page SE, Tassier T (2007) Why chains beget chains: An ecolog-
ical model of firm entry and exit and the evolution of market
similarity. J Econ Dyn Control 31(10):3427–3458
35. Palmer R, Arthur WB, Holland JH, LeBaron B, Taylor P (1993) Ar-
tificial economic life: A simple model for a stock market. Phys-
ica D 75:264–274
36. Pareto V (1896–1897) Cours d’économie politique professé
à l’Université de Lausanne. Rouge, Lausanne
37. Penrose ET (1959) The theory of the growth of the firm. Wiley,
New York
38. Radner R (1968) Competitive equilibrium under uncertainty.
Econometrica 36(1):31–58
39. Samuelson PA (1949) Foundations of economic analysis. Har-
vard University Press
40. Schweitzer F, Hoyst JA (2000) Modeling collective opinion for-
mation by means of active Brownian particles. Europ Phys J B –
Condens Matter Complex Syst 15(4):723–732
41. Simon HA, Bonini CP (1958) The size distribution of busi-
ness firms. Am Econ Rev 48(4):607–617. ISSN 0002-8282
http://links.jstor.org/sici?sici=0002-8282%28195809%2948%
3A4%3C607%3ATS%DOBF%3E2.0.CO%3B2-3
42. Tesfatsion L (2001) Structure, behavior, and market power in
an evolutionary labor market with adaptive search. J Econ Dyn
Control 25(3–4):419–457
43. Zipf GK (1935) The psycho-biology of language. Houghton Mif-
flin, Boston
44. Zipf GK (1949) Human behavior and the principle of least ef-
fort. Addison-Wesley, Cambridge
Agent Based Modeling
and Simulation
STEFANIA BANDINI, SARA MANZONI,
GIUSEPPE VIZZARI
Complex Systems and Artiﬁcial Intelligence Research
Center, University of Milan-Bicocca, Milan, Italy
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Agent-Based Models for Simulation
Platforms for Agent-Based Simulation
Future Directions
Bibliography
Glossary
Agent The deﬁnition of the term agent is controversial
even inside the restricted community of computer sci-
entists dealing with research on agent models and
technologies [25]. A weak deﬁnition, that could be
suited to describe the extremely heterogeneous ap-
proaches in the agent-based simulation context, is “an
autonomous entity, having the ability to decide the ac-
tions to be carried out in the environment and interac-
tions to be established with other agents, according to
its perceptions and internal state”.
Agent architecture
The term agent architecture [53]
refers to the internal structure that is responsible of ef-
fectively selecting the actions to be carried out, accord-
ing to the perceptions and internal state of an agent.
Diﬀerent architectures have been proposed in order to
obtain speciﬁc agent behaviors and they are generally
classiﬁed into deliberative and reactive (respectively,
hysteretic and tropistic, according to the classiﬁcation
reported in [29]).
Autonomy The term autonomy has diﬀerent meanings,
for it represents (in addition to the control of an agent
over its own internal state) diﬀerent aspects of the pos-
sibility of an agent to decide about its own actions. For
instance, it may represent the possibility of an agent to
decide (i) about the timing of an action, (ii) whether or
not to fulﬁll a request, (iii) to act without the need of
an external trigger event (also called pro-activeness or
proactivity) or even (iv) basing on its personal expe-
rience instead of hard-wired knowledge [53]. It must
be noted that diﬀerent agent models do not generally
embody all the above notions of autonomy.

Agent Based Modeling and Simulation
A
185
Interaction “An interaction occurs when two or more
agents are brought into a dynamic relationship
through a set of reciprocal actions” [22].
Environment “The environment is a ﬁrst-class abstrac-
tion that provides the surrounding conditions for
agents to exist and that mediates both the interaction
among agents and the access to resources” [66].
Platform for agent-based simulation a software frame-
work speciﬁcally aimed at supporting the realization
of agent-based simulation systems; this kind of frame-
work often provides abstractions and mechanisms for
the deﬁnition of agents and their environments, to
support their interaction, but also additional function-
alities like the management of the simulation (e. g. set-
up, conﬁguration, turn management), its visualization,
monitoring and the acquisition of data about the sim-
ulated dynamics.
Definition of the Subject
Agent-Based Modeling and Simulation – an approach
to the modeling and simulation of a system in which the
overall behavior is determined by the local action and in-
teraction of a set of agents situated in an environment. Ev-
ery agent chooses the action to be carried out on the ba-
sis of its own behavioral speciﬁcation, internal state and
perception of the environment. The environment, besides
enabling perceptions, can regulate agents’ interactions and
constraint their actions.
Introduction
Computer simulation represents a way to exploit a com-
putational model to evaluate designs and plans without ac-
tually bringing them into existence in the real world (e. g.
architectural designs, road networks and traﬃc lights), but
Agent Based Modeling and Simulation, Figure 1
A general schema describing the usage of simulation as a predictive or explanatory instrument
also to evaluate theories and models of complex systems
(e. g. biological or social systems) by envisioning the eﬀect
of the modeling choices, with the aim of gaining insight
of their functioning. The use of these “synthetic environ-
ments” is sometimes necessary because the simulated sys-
tem cannot actually be observed, since it is actually being
designed or also for ethical or practical reasons. A gen-
eral schema (based on several elaborations, such as those
described in [19,31]) describing the role of simulation as
a predictive or explanatory instrument is shown in Fig. 1.
Several situations are characterized by the presence of
autonomous entities whose actions and interactions deter-
mine (in a non-trivial way) the evolution of the overall sys-
tem. Agent-based models are particularly suited to repre-
sent these situations, and to support the study and analysis
of topics like decentralized decision making, local-global
interactions, self-organization, emergence, eﬀects of het-
erogeneity in the simulated system. The interest in this
relatively recent approach to modeling and simulation is
demonstrated by the number of scientiﬁc events focused in
this topic (see, to make some examples rooted in the com-
puter science context, the Multi Agent Based Simulation
workshop series [17,35,45,55,56,57], the IMA workshop
on agent-based modeling1 and the Agent-Based Modeling
and Simulation symposium [7]). Agent-based models and
multi-agent systems (MAS) have been adopted to simulate
complex systems in very diﬀerent contexts, ranging from
social and economical simulation (see, e. g., [18]) to logis-
tics optimization (see, e. g., [64]), from biological systems
(see, e. g., [8]) to traﬃc (see, e. g., [4,12,60]) and crowd sim-
ulation (see, e. g., [11]).
This heterogeneity in the application domains also re-
ﬂects the fact that, especially in this context of agent fo-
cused research, inﬂuences come from most diﬀerent re-
1http://www.ima.umn.edu/complex/fall/agent.html

186 A
Agent Based Modeling and Simulation
search areas. Several traﬃc and crowd agent models, to
make a relevant example, are deeply inﬂuenced by physics,
and the related models provide agents that are modeled as
particles subject to forces generated by the environment
as well as by other agents (i. e. active walker models, such
as [37]). Other approaches to crowd modeling and simu-
lation build on experiences with Cellular Automata (CA)
approaches (see, e. g., [54]) but provide a more clear sepa-
ration between the environment and the entities that in-
habit, act and interact in it (see, e. g., [6,38]). This line
of research leads to the deﬁnition of models for situated
MASs, a type of model that was also deﬁned and success-
fully applied in the context of (reactive) robotics and con-
trol systems [61,62]. Models and simulators deﬁned and
developed in the context of social sciences [31] and econ-
omy [50] are instead based on diﬀerent theories (often
non-classical ones) of human behavior in order to gain fur-
ther insight on it and help building and validating new the-
ories.
The common standpoint of all the above-mentioned
approaches and of many other ones that describe them-
selves as agent-based is the fact that the analytical unit of
the system is represented by the individual agent, acting
and interacting with other entities in a shared environ-
ment: the overall system dynamic is not deﬁned in terms
of a global function, but rather the result of individuals’ ac-
Agent Based Modeling and Simulation, Figure 2
An abstract reference model to analyze, describe and discuss different models, concrete simulation experiences, platforms legiti-
mately claiming to adopt an agent-based approach
tions and interactions. On the other hand, it must also be
noted that in most of the introduced application domains,
the environment plays a prominent role because:
 It deeply inﬂuences the behaviors of the simulated enti-
ties, in terms of perceptions and allowed actions for the
agents.
 The aim of the simulation is to observe some aggregate
level behavior (e. g. the density of a certain type of agent
in an area of the environment, the average length of
a given path for mobile agents, the generation of clus-
ters of agents), that can actually only be observed in the
environment.
Besides these common elements, the above introduced
approaches often dramatically diﬀer in the way agents
are described, both in terms of properties and behavior.
A similar consideration can be done for their environ-
ment.
Considering the above introduced considerations, the
aim of this article is not to present a speciﬁc technical
contribution but rather to introduce an abstract refer-
ence model that can be applied to analyze, describe and
discuss diﬀerent models, concrete simulation experiences,
platforms that, from diﬀerent points of view, legitimately
claim to adopt an agent-based approach. The reference
model is illustrated in Fig. 2.

Agent Based Modeling and Simulation
A
187
In particular, the following are the main elements of
this reference model:
 Agents encompassing a possibly heterogeneous behav-
ioral speciﬁcation;
 Their environment, supplying agents their perceptions
and enabling their actions;
 Mechanisms of interaction among agents.
Agent interaction, in fact, can be considered as a spe-
ciﬁc type of action having a central role in agent-based
models. In the following Section, these elements will be
analyzed, with reference to the literature on agent-based
models and technologies but also considering the concrete
experiences and results obtained by researches in other ar-
eas, from biology, to urban planning, to social sciences,
in an attempt to enhance the mutual awareness of the
points of contact of the respective eﬀorts. Section “Plat-
forms for Agent-Based Simulation” brieﬂy discusses the
available platforms supporting the rapid prototyping or
development of agent-based simulations. A discussion on
the future directions for this multi-disciplinary research
area will end the article.
Agent-Based Models for Simulation
A model is an abstract and simpliﬁed representation of
a given part of the world, either existing or planned (a tar-
get system, in the terminology adopted for Fig. 1). Mod-
els are commonly deﬁned in order to study and explain
observed phenomena or to forecast future phenomena.
Agent-based models for simulation, as previously men-
tioned, are characterized by the presence of agents per-
forming some kind of behavior in a shared environment.
The notion of agent, however, is controversial even inside
the restricted community of computer scientists dealing
with research on agent models and technologies [25]. The
most commonly adopted deﬁnition of agent [68] speciﬁes
a set of properties that must characterize an entity to ef-
fectively call it an agent, and in particular autonomy (the
possibility to operate without intervention by humans, and
a certain degree of control over its own state), social ability
(the possibility to interact employing some kind of agent
communication language, a notion that will be analyzed
more in details in Subsect. “Agent Interaction”), reactiv-
ity (the possibility to perceive an environment in which it
is situated and respond to perceived changes) and pro-ac-
tiveness (the possibility to take the initiative, starting some
activity according to internal goals rather than as a reac-
tion to an external stimulus). This deﬁnition, considered
by the authors a weak deﬁnition of agency, is generally
already too restrictive to describe as agents most of the
entities populating agent-based models for simulation in
diﬀerent ﬁelds. Even if the distance between the context
of research on intelligent agents and agent-based simula-
tion, that is often more focused on the resulting behav-
ior of the local action and interaction of relatively simple
agents, cannot be neglected, the aim of this section is to
present some relevant results of research on agent models
and technologies in computer science and put them in re-
lation with current research on agent-based simulation in
other research areas.
As previously suggested, agent-based models (ABMs)
can be considered models of complex systems and the
ABM approach considers that simple and complex phe-
nomena can be the result of interactions between au-
tonomous and independent entities (i. e. agents) which
operate within communities in accordance with diﬀer-
ent modes of interaction. Thus, agents and ABMs should
not be considered simply as a technology [43,69] but also
a modeling approach that can be exploited to represent
some system properties that are not simply described as
properties or functionalities of single system components
but sometimes emerge from collective behaviors [22]. The
study of such emerging behaviors is a typical activity in
complex systems modeling [10] and agent-based models
are growingly employed for the study of complex systems
(see, e. g., [3,36,65] satellite workshops of the 2007 edition
of the European Conference on Complex Systems).
Agent Behavior Speciﬁcation
This section will ﬁrst of all discuss the possible notions of
actions available to an agent, then it will discuss the way
an action actually chooses the actions to be carried out,
introducing the notion of agent architecture.
Agent Actions – Actions are the elements at the basis of
agent behavior. Agent actions can cause modiﬁcations in
their environment or in other agents that constitutes the
ABM. Diﬀerent modeling solutions can be provided in or-
der to describe agent actions: as transformation of a global
state, as response to inﬂuences, as computing processes, as
local modiﬁcation, as physical displacement, and as com-
mand (more details about the reported methods to repre-
sent agent actions can be found in [22]).
 Functional transformation of states is based on concepts
of states and state transformation and constitutes the
most classical approach of artiﬁcial intelligence to ac-
tion representation (mainly used in the planning and
multi-agent planning contexts). According to this ap-
proach, agent actions are deﬁned as operators whose
eﬀect is a change in the state of the world [24,29,30].

188 A
Agent Based Modeling and Simulation
 Modeling actions as local modiﬁcation provides an ap-
proach opposite to the one based on transformation
of a global state. Agents perceive their local environ-
ment (the only part of the environment that they can
access) and according to their perceptions, they modify
their internal state. Automata networks [32] and cellu-
lar automata [67] are examples of this type of models.
They are dynamical systems whose behavior is deﬁned
in terms of local relationships and local transforma-
tions; in turn, these changes inﬂuence the overall sys-
tem state. Within the ABMs context cellular automata
are often exploited to represent the dynamic behavior
of agent environment [58] or to simulate population
dynamics [21] in artiﬁcial life [1,41].
 Modeling actions as response to inﬂuences [23] extends
the previous approach introducing elements to con-
sider the eﬀects of agent interactions and the simulta-
neous execution of actions in an ABM. Agent actions
are conditioned and represent a reaction to other agent
actions or to environment modiﬁcations.
 Agents can also be considered as computational pro-
cesses (in the vein of the actor model [2]). A comput-
ing system, and thus a ABM, can be considered as a set
of activities (i. e. processes) that are executed sequen-
tially or in parallel. This modeling approach in par-
ticular focuses on single computing entities, their be-
haviors and interactions. The most studied and well
known methods to represent processes are ﬁnite state
automata, Petri nets and their variants [46].
 Approaches derived from physics can also be found in
order to represent agent actions. In these cases, ac-
tions mainly concern movements and their applica-
tions are in the robotics contexts (i. e. reactive and situ-
ated agents) [42]. One of the most used notion derived
from physics is the one of ﬁeld (e. g. gravitational, elec-
trical, magnetic). Agents are attracted or repelled by
given objects or environment areas that emit ﬁelds to
indicate their position.
 According to cybernetics and control system theory,
actions can be represented as system commands that
regulate and control agent behavior. In this way, ac-
tions are complex tasks that the agent executes in order
to fulﬁll given goals and that take into account the envi-
ronment reactions and correctives to previous actions.
Agent Architecture – The term architecture [53] refers to
the model of agent internal structure that is responsible of
eﬀectively selecting the actions to be carried out, accord-
ing to the perceptions and internal state of an agent. Dif-
ferent architectures have been proposed in order to obtain
speciﬁc agent behaviors and they are generally classiﬁed
into deliberative and reactive (respectively, hysteretic and
tropistic according to the classiﬁcation reported in [29]).
Reactive agents are elementary (and often memory-
less) agents with a deﬁned position in the environment.
Reactive agents perform their actions as a consequence of
the perception of stimuli coming either from other agents
or from the environment; generally, the behavioral spec-
iﬁcation of this kind of agent is a set of condition-action
rules, with the addition of a selection strategy for choosing
an action to be carried out whenever more rules could be
activated. In this case, the motivation for an action derives
from a triggering event detected in the environment; these
agents cannot be pro-active.
Deliberative or cognitive agents, instead, are character-
ized by a more complex action selection mechanism, and
their behavior is based on so called mental states, on facts
representing agent knowledge about the environment and,
possibly, also on memories of past experiences. Delibera-
tive agents, for every possible sequence of perceptions, try
to select a sequence of actions, allowing them to achieve
a given goal. Deliberative models, usually deﬁned within
the planning context, provide a symbolic and explicit rep-
resentation of the world within agents and their decisions
are based on logic reasoning and symbol manipulation.
The BDI model (belief, desire, intention [51,52]) is per-
haps the most widespread model for deliberative agents.
The internal state of agents is composed of three “data
structures” concerning agent beliefs, desires and inten-
tions. Beliefs represent agent information about its sur-
rounding world, desires are the agent goals, while inten-
tions represent the desire an agent has eﬀectively selected,
that it has to some extend committed.
Hybrid architectures can also be deﬁned by combin-
ing the previous ones. Agents can have a layered architec-
ture, where deliberative layers are based on a symbolic rep-
resentation of the surrounding world, generate plans and
take decisions, while reactive layers perform actions as ef-
fect of the perception of external stimuli. Both vertical and
horizontal architectures have been proposed in order to
structure layers [13]. In horizontal architecture no priori-
ties are associated to layers and the results of the diﬀerent
layers must be combined to obtain agent’s behavior. When
layers are instead arranged in a vertical structure, reactive
layers have higher priority over deliberative ones, that are
activated only when no reactive behavior is triggered by
the perception of an external stimulus.
A MAS can be composed of cognitive agents (gener-
ally a relatively low number of deliberative agents), each
one possessing its own knowledge-model determining its
behavior and its interactions with other agents and the en-
vironment. By contrast, there could be MAS made only by

Agent Based Modeling and Simulation
A
189
reactive agents. This type of system is based on the idea
that it is not necessary for a single agent to be individu-
ally intelligent for the system to demonstrate complex (in-
telligent) behaviors. Systems of reactive agents are usually
more robust and fault tolerant than other agent-based sys-
tems (e. g. an agent may be lost without any catastrophic
eﬀect for the system). Other beneﬁts of reactive MAS in-
clude ﬂexibility and adaptability in contrast to the inﬂexi-
bility that sometimes characterizes systems of deliberative
agents [14]. Finally, a system might also present an hetero-
geneous composition of reactive and deliberative agents.
Environment
Weyns et al. in [66] provide a deﬁnition of the notion of
environment for MASs (and thus of an environment for an
ABM), and also discuss the core responsibilities that can
be ascribed to it. In particular, in the speciﬁc context of
simulation the environment is typically responsible for the
following:
 Reﬂecting/reifying/managing the structure of the phys-
ical/social arrangement of the overall system;
 Embedding, supporting regulated access to objects and
parts of the system that are not modeled as agents;
 Supporting agent perception and situated action (it
must be noted that agent interaction should be consid-
ered a particular kind of action);
 Maintain internal dynamics (e. g. spontaneous growth
of resources, dissipation signals emitted by agents);
 Deﬁne/enforce rules.
In order to exemplify this schema, we will now con-
sider agent-based models and simulators that are based on
a physical approach; the latter generally consider agents
as particles subject to and generating forces. In this case,
the environment comprises laws regulating these inﬂu-
ences and relevant elements of the simulated system that
are not agents (e. g. point of reference that generate at-
traction/repulsion forces). It is the environment that de-
termines the overall dynamics, combining the eﬀects that
inﬂuence each agent and applying them generally in dis-
crete time steps. In this cycle, it captures all the above in-
troduced responsibilities, and the role of agents is min-
imal (according to some deﬁnitions they should not be
called agents at all), and running a simulation is essen-
tially reduced to computing iteratively a set equations (see,
e. g., [4,37]). In situated ABM approaches agents have
a higher degree of autonomy and control over their ac-
tions, since they evaluate their perceptions and choose
their actions according to their behavioral speciﬁcation.
The environment retains a very relevant role, since it pro-
vides agents with their perceptions that are generated ac-
cording to the current structure of the system and to the
arrangement of agents situated in it. Socioeconomic mod-
els and simulations provide various approaches to the rep-
resentation of the simulated system, but are generally sim-
ilar to situated ABMs.
It is now necessary to make a clariﬁcation on how the
notion of environment in the context of MAS-based sim-
ulation can be turned into a software architecture. Klügl
et al. [40] argue that the notion of environment in multi-
agent simulation is actually made up of two conceptu-
ally diﬀerent elements: the simulated environment and
the simulation environment. The former is a part of the
computational model that represents the reality or the ab-
straction that is the object of the simulation activity. The
simulation environment, on the other hand, is a software
infrastructure for executing the simulation. In this frame-
work, to make an explicit decoupling between these levels
is a prerequisite for good engineering practice. It must be
noted that also a diﬀerent work [33], non-speciﬁcally de-
veloped in the context of agent-based simulation, provided
a model for the deployment environment, that is the spe-
ciﬁc part of the software infrastructure that manages the
interactions among agents.
Another recent work is focused on clarifying the no-
tion of ABM environment and describes a three layered
model for situated ABM environments [63]. This work ar-
gues that environmental abstractions (as well as those re-
lated to agents) crosscut all the system levels, from applica-
tion speciﬁc ones, to the execution platform, to the phys-
ical infrastructure. There are thus application speciﬁc as-
pects of agents’ environment that must be supported by
the software infrastructure supporting the execution of the
ABM, and in particular the ABM framework (MAS frame-
work in the ﬁgure). Figure 3 compares the two above-de-
scribed schemas.
The fact that the environment actually crosscuts all
system levels in a deployment model represents a prob-
lem making diﬃcult the separation between simulated en-
vironment and simulation infrastructure. In fact, the mod-
eling choices can have a deep inﬂuence on the design of
the underlying ABM framework and, vice versa, design
choices on the simulation infrastructure make it suitable
for some ABM and environment models but not usable for
other ones. As a result, general ABM framework support-
ing simulation actually exist, but they cannot oﬀer a spe-
ciﬁc form of support to the modeler, although they can of-
fer basic mechanisms and abstractions.
SeSAm, for instance, oﬀers a general simulation in-
frastructure but relies on plugins [40]. Those plugins, for
example, could be used to deﬁne and manage the spatial

190 A
Agent Based Modeling and Simulation
Agent Based Modeling and Simulation, Figure 3
A schema introduced in [40] to show differences and relationships between simulated and simulation environment (a), and a three
layer deployment model for situated MAS introduced in [63] highlighting the crosscutting abstractions agent and environment (b)
features of the simulated environment, including the as-
sociated basic functions supporting agent movement and
perception in that kind of environment. With reference to
Fig. 3b, such a plugin would be associated to the applica-
tion environment module, in the ABM application layer.
However, these aspects represent just some of the features
of the simulated environment, that can actually comprise
rules and laws that extend their inﬂuence over the agents
and the outcomes of their attempts to act in the environ-
ment.
Agent Interaction
Interaction is a key aspect in ABM. There is a plethora of
deﬁnitions for the concept of agent and most of them em-
phasize the fact that this kind of entity should be able to
interact with their environment and with other entities in
order to solve problems or simply reach their goals accord-
ing to coordination, cooperation or competition schemes.
The essence of a ABM is the fact that the global system dy-
namics emerges from the local behaviors and interactions
among its composing parts. Strictly speaking, for some
kind of ABM the global dynamics is just the sum of lo-
cal behaviors and interactions, so we cannot always speak
of emergent behavior when we talk about a ABM. How-
ever the assumptions that underlie the design of an inter-
action model (or the choice of an existing one for the de-
sign and implementation of a speciﬁc application) are so
important that they have a deep impact on the deﬁnition of
agent themselves (e. g. an interpreter of a speciﬁc language,
a perceiver of signals). Therefore it is almost an obvious
consequence that interaction mechanisms have a huge im-
pact on the modeling, design and development of appli-
cations based on a speciﬁc kind of ABM, which, in turn,
is based on a particular interaction model. It is thus not
a surprise that a signiﬁcative part of the research that was
carried out in the agent area was focused on this aspect.
This section presents a conceptual taxonomy of cur-
rently known/available agent interaction models, trying to
deﬁne advantages and issues related to them, both from
a conceptual and a technical point of view.
There are many possible dimensions and aspects of
agent interaction models that can be chosen and adopted
in order to deﬁne a possible taxonomy. The ﬁrst aspect
that is here considered to classify agent interaction mod-
els is related to the fact that agents communicate di-

Agent Based Modeling and Simulation
A
191
Agent Based Modeling and Simulation, Figure 4
The proposed taxonomy of agent interaction models
rectly (for instance exchanging messages), and thus the
model does not include an abstraction of the actual com-
munication channel, or there are some media interposed
among the communication partners which is explicitly in-
cluded in the interaction model. While the former ap-
proach, with speciﬁc reference to Agent Communication
Language (ACL)-based models, is the most widely adopted
in the agent area it has its drawbacks, and most of them are
related to the issue of agents acquaintance. The way ACL-
based agent interaction models deal with this issue is the
subject of another dimension of the taxonomy, providing
direct a priori acquaintance among agents, the adoption of
middle agents for information discovery and the develop-
ment of more complex acquaintance models to tackle is-
sues related to the representation and maintenance of ac-
quaintance information but also to robustness and scala-
bility of the agent infrastructure. However there are other
agent interaction models providing an indirect commu-
nication among them. Some of these approaches provide
the creation and exploitation of artifacts that represent
a medium for agents’ interaction. Other indirect interac-
tion models are more focused on modeling agent environ-
ment as the place where agent interactions take place, thus
inﬂuencing interactions and agent behavior.
Direct Interaction Models
The ﬁrst and most widely
adopted kind of agent interaction model provide a direct
information exchange between communication partners.
This approach ignores the details related to the commu-
nication channel that allows the interaction, and does not
include it as an element of the abstract interaction model.
Generally the related mechanisms provide a point-to-
point message-passing protocol regulating the exchange of
messages among agents. There are various aspects of the
communicative act that must be modeled (ranging from
low-level technical considerations on message format to
conceptual issues related to the formal semantics of mes-
sages and conversations), but generally this approach pro-
vides the deﬁnition of suitable languages to cover these
aspects. While this approach is generally well-understood
and can be implemented in a very eﬀective way (especially
as it is substantially based on the vast experience of com-
puter networks protocols), in the agent context, it requires
speciﬁc architectural and conceptual solutions to tackle is-
sues related to the agent acquaintance/discovery and on-
tological issues.
Intuitively an Agent Communication Language (ACL)
provides agents with a means of exchanging informa-
tion and knowledge. This vague deﬁnition inherently in-
cludes the point of view on the conception of the term
agent, which assumes that an agent is an intelligent au-
tonomous entity whose features include some sort of so-
cial ability [68]. According to some approaches this kind
of feature is the one that ultimately deﬁnes the essence
of agency [28]. Leaving aside the discussion on the def-
inition and conception of agency, this section will focus
on what the expression “social ability” eﬀectively means.
To do so we will brieﬂy compare these ACL share with
those approaches that allow the exchange of information
among distributed components (e. g. in legacy systems2)
some basic issues: in particular, the deﬁnition of a com-
munication channel allowing the reliable exchange of mes-
sages over a computer network (i. e. the lower level aspects
of the communication). What distinguishes ACLs from
such systems are the objects of discourse and their seman-
tic complexity; in particular there are two aspects which
distributed computing protocols and architectures do not
have to deal with:
2With this expression we mean pieces of software which are not
designed to interact with agents and agent based systems.

192 A
Agent Based Modeling and Simulation
Agent Based Modeling and Simulation, Figure 5
Layers and concerns of an agent communication language
 Autonomy of interacting components: modern sys-
tems’ components (even though they can be quite com-
plex and can be considered as self-suﬃcient with refer-
ence to supplying a speciﬁc service) have a lower degree
of autonomy than the one that is generally associated to
agents.
 The information conveyed in messages does not gen-
erally require a comprehensive ontological approach,
as structures and categories can be considered to be
shared by system components.
Regarding the autonomy, while traditional software com-
ponents oﬀer services and generally perform the required
actions as a reaction to the external requests, agents may
decide not to carry out a task that was required by some
other system entity. Moreover generally agents are consid-
ered temporally continuous and proactive, while this is not
generally true for common software components.
For what concerns the second point, generally compo-
nents have speciﬁc interfaces which assume an agreement
on a set of shared data structures. The semantics of the re-
lated information, and the semantics of messages/method
invocation/service requests, is generally given on some
kind of (more or less formally speciﬁed) modeling lan-
guage, but is tightly related to component implementation.
For agent interaction a more explicit and comprehensive
view on domain concepts must be speciﬁed. In order to be
able to eﬀectively exchange knowledge, agents must share
an ontology (see, e. g., [34]), that is a representation of a set
of categories of objects, concepts, entities, properties and
relations among them. In other words, the same concept,
object or entity must have a uniform meaning and set of
properties across the whole system.
Indirect Interaction Models
From a strictly technical
point of view, agent communication is generally indirect
even in direct agent interaction models. In fact most of
these approaches adopt some kind of communication in-
frastructure supplying a reliable end-to-end message pass-
ing mechanism. Nonetheless, the adoption of a conceptu-
Agent Based Modeling and Simulation, Figure 6
A conceptual diagram for a typical blackboard architecture, in-
cluding two sample primitives of the Linda coordination model,
that is, the output of a tuple into the blackboard (the out opera-
tion) and the non-destructive input of an agent from the black-
board (the read operation)
ally direct agent interaction model brings speciﬁc issues
that were previously introduced. The remaining of this
section will focus on models providing the presence of an
intermediate entity mediating (allowing and regulating)
agent interaction. This communication abstraction is not
merely a low-level implementation detail, but a ﬁrst-class
concept of the model.
Agent interaction models which provide indirect
mechanisms of communication will be classiﬁed into ar-
tifact mediated and spatially grounded models. The dis-
tinction is based on the inspiration and metaphor on
which these models are rooted. The former provide the
design and implementation of an artifact which emu-
lates concrete objects of agents’ environment whose goal
is the communication of autonomous entities. Spatially
grounded agent interaction models bring the metaphor of
modeling agent environment to the extreme, recognizing
that there are situations in which spatial features and in-
formation represent a key factor and cannot be neglected
in analyzing and modeling a system.
Both of these approaches provide interaction mecha-
nisms that are deeply diﬀerent from point-to-point mes-
sage exchange among entities. In fact, the media which en-
able the interaction intrinsically represent a context inﬂu-
encing agent communication.
In the real world a number of physical agents interact
sharing resources, by having a competitive access to them
(e. g. cars in streets and crossroads), but also collaborat-
ing in order to perform tasks which could not be carried
out by single entities alone, due to insuﬃcient competen-
cies or abilities (e. g. people that carry a very heavy bur-
den together). Very often, in order to regulate the inter-
actions related to these resources, we build concrete arti-
facts, such as traﬃc lights on the streets, or neatly placed

Agent Based Modeling and Simulation
A
193
handles on large heavy boxes. Exploiting this metaphor,
some approaches to agent interaction tend to model and
implement abstractions allowing the cooperation of enti-
ties through a shared resource, whose access is regulated
according to precisely deﬁned rules. Blackboard-based ar-
chitectures are the ﬁrst examples of this kind of models.
A blackboard is a shared data repository that enables co-
operating software modules to communicate in an indirect
and anonymous way [20]. In particular the concept of tu-
ple space, ﬁrst introduced in Linda [26], represents a per-
vasive modiﬁcation to the basic blackboard model.
Linda [26] coordination language probably represents
the most relevant blackboard-based model. It is based on
the concept of tuple space, that is an associative blackboard
allowing agents to share and retrieve data (i. e. tuples)
through some data-matching mechanism (such as pattern-
matching or uniﬁcation) integrated within the blackboard.
Linda also deﬁnes a very simple language deﬁning mecha-
nisms for accessing the tuple space.
The rationale of this approach is to keep separated
computation and coordination contexts as much as pos-
sible [27], by providing speciﬁc abstractions for agent in-
teraction. With respect to direct interaction models, part
of the burden of coordination is in fact moved from the
agent to the infrastructure. The evolution of this approach
has basically followed two directions: the extension of the
coordination language and infrastructure in order to in-
crease its expressiveness or usability, and the modeling and
implementation of distributed tuple spaces [16,48,49].
While the previously described indirect approaches
deﬁne artifacts for agent interaction taking inspiration
from actual concrete object of the real world, other ap-
proaches bring the metaphor of agent environment to the
extreme by taking into account its spatial features.
In these approaches agents are situated in an environ-
ment whose spatial features are represented possibly in an
explicit way and have an inﬂuence on their perception, in-
teraction and thus on their behavior. The concept of per-
ception, which is really abstract and metaphoric in direct
interaction models and has little to do with the physi-
cal world (agents essentially perceive their state of mind,
which includes the eﬀect of received messages, like new
facts in their knowledge base), here is related to a more
direct modeling of what is often referred to as “local
point of view”. In fact these approaches provide the im-
plementation of an infrastructure for agent communica-
tion which allows them to perceive the state of the en-
vironment in their position (and possibly in nearby lo-
cations). They can also cause local modiﬁcations to the
state of the environment, generally through the emission
of signals, emulating some kind of physical phenomenon
Agent Based Modeling and Simulation, Figure 7
A sample schema exemplifying an environment mediated form
of interaction in which the spatial structure of the environment
has a central role in determining agents’ perceptions and their
possibility to interact
(e. g. pheromones [15], or ﬁelds [5,44]) or also by sim-
ply observing the actions of other agents and reacting to
this perception, in a “behavioral implicit communication”
schema [59].
In all these cases, however, the structuring function of
the environment is central, since it actually deﬁnes what
can be perceived by an agent in its current position and
how it can actually modify the environment, to which ex-
tent its actions can be noted by other agents and thus in-
teract with them.
Platforms for Agent-Based Simulation
Considering the pervasive diﬀusion and adoption of
agent-based approaches to modeling and simulation, it is
not surprising the fact that there is a growing interest in
software frameworks speciﬁcally aimed at supporting the
realization of agent-based simulation systems.
This kind of framework often provides abstractions
and mechanisms for the deﬁnition of agents and their en-

194 A
Agent Based Modeling and Simulation
Agent Based Modeling and Simulation, Figure 8
A screenshot of a Netlogo simulation applet, (a), and a Repast simulation model, (b)
vironments, to support their interaction, but also addi-
tional functionalities like the management of the simula-
tion (e. g. set-up, conﬁguration, turn management), its vi-
sualization, monitoring and the acquisition of data about
the simulated dynamics. It is not the aim of this article to
provide a detailed review of the current state of the art in
this sector, but rather sketch some classes of instruments
that have been used to support the realization of agent-
based simulations and provide a set of references to rele-
vant examples of platforms facilitating the development of
agent-based simulations.
A ﬁrst category of these platforms instruments pro-
vides general purpose frameworks in which agents mainly
represent passive abstractions, sort of data structures that
are manipulated by an overall simulation process. A rel-
evant example of such tools is NetLogo3, a dialect of the
Logo language speciﬁcally aimed at modeling phenomena
characterized by a decentralized, interconnected nature.
NetLogo does not even adopt the term agent to denote
individuals, but it rather calls them turtles; a typical sim-
ulation consists in a cycle choosing and performing an ac-
tion for every turtle, considering its current situation and
state. It must be noted that, considering some of the previ-
ously mentioned deﬁnitions of autonomous agent a turtle
should not be considered an agent, due to the almost ab-
sent autonomy of these entities. The choice of a very sim-
ple programming language that does not require a back-
ground on informatics, the possibility to deploy in a very
simple way simulations as Java applets, and the availability
of simple yet eﬀective visualization tools, made NetLogo
extremely popular.
3http://ccl.northwestern.edu/netlogo/
A second category of platforms provides frameworks
that are developed with a similar rationale, providing for
very similar support tools, but these instruments are based
on general purpose programming languages (generally ob-
ject oriented). Repast4 [47] represents a successful repre-
sentative of this category, being a widely employed agent-
based simulation platform based on the Java language.
The object-oriented nature of the underlying program-
ming language supports the deﬁnition of computational
elements that make these agents more autonomous, closer
to the common deﬁnitions of agents, supporting the en-
capsulation of state (and state manipulation mechanisms),
actions and action choice mechanism in agent’s class. The
choice of adopting a general purpose programming lan-
guage, on one hand, makes the adoption of these instru-
ments harder for modelers without a background in infor-
matics but, on the other, it simpliﬁes the integration with
external and existing libraries. Repast, in its current ver-
sion, can be easily connected to instruments for statistical
analysis, data visualization, reporting and also geographic
information systems.
While the above-mentioned functionalities are surely
important in simplifying the development of an eﬀective
simulator, and even if in principle it is possible to adapt
frameworks belonging to the previously described cate-
gories, it must be noted that their neutrality with respect
to the speciﬁc adopted agent model leads to a necessary
preliminary phase of adaptation of the platform to the
speciﬁc features of the model that is being implemented.
If the latter deﬁnes speciﬁc abstractions and mechanisms
for agents, their decision-making activities, their environ-
4http://repast.sourceforge.net/

Agent Based Modeling and Simulation
A
195
ment and the way they interact, then the modeler must
in general develop proper computational supports to be
able to fruitfully employ the platform. These platforms, in
fact, are not endowed with speciﬁc supports to the realiza-
tion of agent deliberation mechanisms or infrastructures
for interaction models, either direct or indirect (even if it
must be noted that all the above platforms generally pro-
vide some form of support to agent environment deﬁni-
tion, such as grid-like or graph structures).
A third category of platforms represent an attempt to
provide a higher level linguistic support trying to reduce
the distance between agent-based models and their imple-
mentations. The latest version of Repast, for instance, is
characterized by the presence of a high level interface for
“point-and-click” deﬁnition of agent’s behaviors, that is
based on a set of primitives for the speciﬁcation of agent’s
actions. SimSesam5 [39] deﬁnes a set of primitive func-
tions as basic elements for describing agents’ behaviors,
and it also provides visual tools supporting model imple-
mentation. At the extreme border of this category, we can
mention eﬀorts that are speciﬁcally aimed at supporting
the development of simulations based on a precise agent
model, approach and sometimes even for a speciﬁc area of
application, such as the one described in [9,64].
Future Directions
Agent-based modeling and simulation is a relatively young
yet already widely diﬀused approach to the analysis, mod-
eling and simulation of complex systems. The heterogene-
ity of the approaches, modeling styles and applications
that legitimately claim to be “agent-based”, as well as the
fact that diﬀerent disciplines are involved in the related re-
search eﬀorts, they are all factors that hindered the deﬁni-
tion of a generally recognized view of the ﬁeld. A higher
level framework of this kind of activity would be desirable
in order to relate diﬀerent eﬀorts by means of a shared
schema. Moreover it could represent the ﬁrst step in ef-
fectively facing some of the epistemological issues related
to this approach to the modeling and analysis of complex
systems. The future directions in this broad research area
are thus naturally aimed at obtaining vertical analytical re-
sults on speciﬁc application domains, but they must also
include eﬀorts aimed at “building bridges” between the
single disciplinary results in the attempt to reach a more
general and shared understanding of how these bottom-up
modeling approaches can be eﬀectively employed to study,
explain and (maybe) predict the overall behavior of com-
plex systems.
5http://www.simsesam.de/
Bibliography
1. Adami C (1998) Introduction to Artificial Life. Springer, New
York
2. Agha G (1986) Actors: A Model of Concurrent Computation in
Distributed Systems. MIT press, Cambridge
3. Alfi V, Galla T, Marsili M, Pietronero L (eds) (2007) Interacting
Agents, Complexity and Inter-Disciplinary Applications (IACIA)
4. Balmer M, Nagel K (2006) Shape morphing of intersection
layouts using curb side oriented driver simulation. In: van
Leeuwen JP, Timmermans HJ (eds) Innovations in Design & De-
cision Support Systems in Architecture and Urban Planning.
Springer, Dordrecht, pp 167–183
5. Bandini S, Manzoni S, Simone C (2002) Heterogeneous agents
situated in heterogeneous spaces. Appl Artific Intell 16:831–
852
6. Bandini S, Manzoni S, Vizzari G (2004) Situated cellular agents:
a model to simulate crowding dynamics. IEICE Transactions on
Information and Systems: Special Issues on Cellular Automata
E87-D, pp 669–676
7. Bandini S, Petta P, Vizzari G (eds) (2006) International Sympo-
sium on Agent Based Modeling and Simulation (ABModSim
2006), vol Cybernetics and Systems. Austrian Society for Cy-
bernetic Studies (2006) 18th European Meeting on Cybernetics
and Systems Research (EMCSR 2006)
8. Bandini S, Celada F, Manzoni S, Puzone R, Vizzari G (2006) Mod-
elling the immune system with situated agents. In: Apolloni
B, Marinaro M, Nicosia G, Tagliaferri R (eds) Proceedings of
WIRN/NAIS 2005. Lecture Notes in Computer Science, vol 3931.
Springer, Berlin, pp 231–243
9. Bandini S, Federici ML, Vizzari G (2007) Situated cellular agents
approach to crowd modeling and simulation. Cybernet Syst
38:729–753
10. Bar-Yam Y (1997) Dynamics of Complex Systems. Addison-
Wesley, Reading
11. Batty M (2001) Agent based pedestrian modeling. Env Plan B:
Plan Des 28:321–326
12. Bazzan ALC, Wahle J, Klügl F (1999) Agents in traffic mod-
elling – from reactive to social behaviour. In: Burgard W,
Christaller T, Cremers AB (eds) KI-99: Advances in Artificial In-
telligence, 23rd Annual German Conference on Artificial Intelli-
gence, Bonn, Germany, 13–15 September 1999. Lecture Notes
in Computer Science, vol 1701. Springer, Berlin, pp 303–306
13. Brooks RA (1986) A robust layered control system for a mobile
robot. IEEE J Robot Autom 2:14–23
14. Brooks RA (1990) Elephants don’t play chess. Robot Autonom
Syst 6:3–15
15. Brueckner S (2000) An analytic approach to pheromone-based
coordination. In: ICMAS IEEE Comp Soc, pp 369–370
16. Cabri G, Leonardi L, Zambonelli F (2000) MARS: a pro-
grammable coordination architecture for mobile agents. IEEE
Inter Comp 4:26–35
17. Davidsson P, Logan B, Takadama K (eds) (2005) Multi-Agent
and Multi-Agent-Based Simulation, Joint Workshop MABS
(2004), New York, 19 July 2004, Revised Selected Papers. In:
Davidsson P, Logan B, Takadama K (eds) MABS. Lecture Notes
in Computer Science, vol 3415. Springer, Berlin
18. Dosi G, Fagiolo G, Roventini A (2006) An evolutionary model of
endogenous business cycles. Comput Econ 27:3–34
19. Edmonds B (2001) The use of models – making MABS more in-
formative. In: Multi-Agent-Based Simulation, Second Interna-

196 A
Agent Based Modeling and Simulation
tional Workshop MABS (2000), Boston MA, USA, July (2000),
Revised and Additional Papers. Lecture Notes in Computer Sci-
ence, vol 1979. Springer, Berlin, pp 15–32
20. Englemore RS, Morgan T (eds) (1988) Blackboard Systems.
Addison-Wesley, Reading
21. Epstein JM, Axtell R (1996) Growing Artificial Societies. MIT
Press, Boston
22. Ferber J (1999) Multi–Agent Systems. Addison-Wesley, London
23. Ferber J, Muller J (1996) Influences and reaction: A model of
situated multiagent systems. In: Proceedings of the 2nd Inter-
national Conference on Multiagent Systems
24. Fikes RE, Nilsson NJ (1971) STRIPS: a new approach to the ap-
plication of theorem proving to problem solving. Artif Intell
2:189–208
25. Franklin S, Graesser A (1997) Is it an agent, or just a program?:
A taxonomy for autonomous agents. In: Müller JP, Wooldridge
M, Jennings NR (eds) Intelligent Agents III, Agent Theories,
Architectures, and Languages ECAI ’96 Workshop (ATAL), Bu-
dapest, 12–13 August 1996. Lecture Notes in Computer Sci-
ence, vol 1193. Springer, Berlin, pp 21–36
26. Gelernter D (1985) Generative communication in Linda. ACM
Trans Program Lang Syst 7:80–112
27. Gelernter D, Carriero N (1992) Coordination languages and
their significance. Commun ACM 35:97–107
28. Genesereth MR, Ketchpel SP (1994) Software agents. Commun
ACM 37(7):48–53
29. Genesereth MR, Nilsson N (1987) Logical Foundations of Artifi-
cial Intelligence. Morgan Kaufmann, San Mateo
30. Georgeff M (1984) A theory of action in multi–agent planning.
In: Proceedings of the AAAI84, pp 121–125
31. Gilbert N, Troitzsch KG (2005) Simulation for the Social Scientist
2nd edn. Open University Press, Maidenhead
32. Goles E, Martinez S (1990) Neural and automata networks, dy-
namical behavior and applications. Kluwer, Norwell
33. Gouaich A, Michel F, Guiraud Y (2005) MIC: A deployment en-
vironment for autonomous agents. In: Environments for Multi-
Agent Systems, First International Workshop (E4MAS 2004).
Lecture Notes in Computer Science, vol 3374. Springer, Berlin,
pp 109–126
34. Gruber TR (1995) Toward principles for the design of ontolo-
gies used for knowledge sharing. Int J Hum Comp Stud 43:907–
928
35. Hales D, Edmonds B, Norling E, Rouchier J (eds) (2003) Multi-
Agent-Based Simulation III, 4th International Workshop MABS
(2003), Melbourne, 14 July 2003. Revised Papers. In: Hales D,
Edmonds B, Norling E, Rouchier J (eds) MABS. Lecture Notes in
Computer Science, vol 2927. Springer, Berlin
36. Hassas S, Serugendo GDM, Phan D (eds) (2007) Multi-
Agents for modelling Complex Systems (MA4CS). http://
bat710.univ-lyon1.fr/~farmetta/MA4CS07
37. Helbing D, Schweitzer F, Keltsch J, Molnár P (1997) Active
walker model for the formation of human and animal trail sys-
tems. Phys Rev E 56:2527–2539
38. Henein CM, White T (2005) Agent-based modelling of forces
in crowds. In: Davidsson P, Logan B, Takadama K (eds) Multi-
Agent and Multi-Agent-Based Simulation, Joint Workshop
MABS (2004), New York 19 July 2004. Revised Selected Papers.
Lecture Notes in Computer Science, vol 3415. Springer, Berlin,
pp 173–184
39. Klügl F, Herrler R, Oechslein C (2003) From simulated to real
environments: How to use sesam for software development.
In: Schillo M, Klusch M, Müller JP, Tianfield H (eds) MATES.
Lecture Notes in Computer Science, vol 2831. Springer, Berlin,
pp 13–24
40. Klügl F, Fehler M, Herrler R (2005) About the role of the environ-
ment in multi-agent simulations. In: Weyns D, Parunak HVD,
Michel F (eds) Environments for Multi-Agent Systems, First In-
ternational Workshop E4MAS (2004), New York 19 July 2004.
Revised Selected Papers. vol 3374, pp 127–149
41. Langton C (1995) Artificial life: An overview. MIT Press, Cam-
bridge
42. Latombe JC (1991) Robot Motion Planning. Kluwer, Boston
43. Luck M, McBurney P, Sheory O, Willmott S (eds) (2005)
Agent Technology: Computing as Interaction. University of
Southampton, Southampton
44. Mamei M, Zambonelli F, Leonardi L (2002) Co-fields: towards
a unifying approach to the engineering of swarm intelligent
systems. In: Engineering Societies in the Agents World III: Third
International Workshop (ESAW 2002). Lecture Notes in Artifi-
cial Intelligence, vol 2577. Springer, Berlin, pp 68–81
45. Moss S, Davidsson P (eds) (2001) Multi-Agent-Based Simu-
lation, Second International Workshop MABS (2000), Boston,
July, (2000), Revised and Additional Papers. Lecture Notes in
Computer Science, vol 1979. Springer, Berlin
46. Murata T (1989) Petri nets: properties, analysis and applica-
tions. Proc IEEE 77:541–580
47. North MJ, Collier NT, Vos JR (2006) Experiences creating three
implementations of the repast agent modeling toolkit. ACM
Trans Model Comp Sim 16:1–25
48. Omicini A, Zambonelli F (1999) Coordination for Internet
application development. Autono Agents Multi-Agent Syst
2:251–269 Special Issue: Coordination Mechanisms for Web
Agents
49. Picco GP, Murphy AL, Roman GC (1999) Lime: Linda meets
mobility. In: Proceedings of the 21st International Conference
on Software Engineering (ICSE 99) ACM Press, Los Angeles,
pp 368–377
50. Pyka A, Fagiolo G (2007) Agent-Based Modelling: A Methodol-
ogy for Neo-Schumpeterian Economics. In: Hanusch H, Pyka A
(eds) Elgar Companion to Neo-Schumpeterian Economics. Ed-
ward Elgar Publishing, pp 467–487
51. Rao A, Georgeff M (1991) Modeling rational agents within
a BDI-architecture. In: Proc Knowledge Representation and
Reasoning (KR&R 1991)
52. Rao A, Georgeff M (1995) BDI agents: from theory to prac-
tice. In: Proceedings of the International Conference on Multi-
Agent Systems
53. Russel S, Norvig P (1995) Artificial Intelligence: A Modern Ap-
proach. Prentice Hall, Upper Saddle River
54. Schadschneider A, Kirchner A, Nishinari K (2002) CA approach
to collective phenomena in pedestrian dynamics. In: Ban-
dini S, Chopard B, Tomassini M (eds) Cellular Automata, 5th
International Conference on Cellular Automata for Research
and Industry ACRI 2002. Lecture Notes in Computer Science,
vol 2493. Springer, Berlin, pp 239–248
55. Sichman JS, Antunes L (eds) (2006) Multi-Agent-Based Simu-
lation VI, International Workshop MABS (2005), Utrecht, The
Netherlands, 25 July 2005, Revised and Invited Papers. In: Sich-
man JS, Antunes L (eds) MABS. Lecture Notes in Computer Sci-
ence, vol 3891. Springer, Berlin
56. Sichman JS, Conte R, Gilbert N (eds) (1998) Multi-Agent Sys-
tems and Agent-Based Simulation, First International Work-

Agent Based Modeling and Simulation, Introduction to
A
197
shop MABS ’98, Paris, France, 4–6 July 1998. Proceedings. In:
Sichman JS, Conte R, Gilbert N (eds): MABS. Lecture Notes in
Computer Science, vol 1534. Springer, Berlin
57. Sichman JS, Bousquet F, Davidsson P (eds) (2003) Multi-Agent-
Based Simulation, Third International Workshop MABS (2002),
Bologna, Italy, 15–16 July 2002, Revised Papers. In: Sichman JS,
Bousquet F, Davidsson P (eds) MABS. Lecture Notes in Com-
puter Science, vol 2581. Springer, Berlin
58. Torrens P (2002) Cellular automata and multi-agent systems as
planning support tools. In: Geertman S, Stillwell J (eds) Plan-
ning Support Systems in Practice. Springer, London, pp 205–
222
59. Tummolini L, Castelfranchi C, Ricci A, Viroli M, Omicini A (2004)
“Exhibitionists” and “voyeurs” do it better: A shared environ-
ment approach for flexible coordination with tacit messages.
In: Weyns D, Parunak HVD, Michel F (eds) 1st International
Workshop on Environments for MultiAgent Systems (E4MAS
2004), pp 97–111
60. Wahle J, Schreckenberg M (2001) A multi-agent system for on-
line simulations based on real-world traffic data. In: Annual
Hawaii International Conference on System Sciences (HICSS-
34), IEEE Computer Society
61. Weyns D, Holvoet T (2006) From reactive robots to situated
multi-agent systems: a historical perspective on the role of en-
vironment in multi-agent systems. In: Dikenelli O, Gleizes MP,
Ricci A (eds) Engineering Societies in the Agents World VI, 6th
International Workshop ESAW (2005). Lecture Notes in Com-
puter Science, vol 3963. Springer, Berlin, pp 63–88
62. Weyns D, Schelfthout K, Holvoet T, Lefever T (2005) Decentral-
ized control of E’GV transportation systems. In: AAMAS Indus-
trial Applications. ACM Press, Utrecht, pp 67–74
63. Weyns D, Vizzari G, Holvoet T (2006) Environments for situ-
ated multi-agent systems: beyond infrastructure. In: Weyns D,
Parunak HVD, Michel F (eds) Environments for Multi-Agent
Systems II, Second International Workshop E4MAS (2005),
Utrecht, 25 July 2005. Selected Revised and Invited Papers.
Lecture Notes in Computer Science, vol 3830. Springer, Berlin,
pp 1–17
64. Weyns D, Boucké N, Holvoet T (2006) Gradient field-based task
assignment in an AGV transportation system. In: AAMAS ’06:
Proceedings of the fifth international joint conference on Au-
tonomous agents and multiagent systems. ACM Press, Hako-
date, pp 842–849
65. Weyns D, Brueckner SA, Demazeau Y (eds) (2008) Engineer-
ing Environment-Mediated Multi-Agent Systems: International
Workshop, EEMMAS 2007, Dresden, Germany, October 2007.
Selected Revised and Invited Papers. Lecture Notes in Com-
puter Science, vol 5049. Springer, Berlin
66. Weyns D, Omicini A, Odell J (2007) Environment as a first class
abstraction in multiagent systems. Auton Agents Multi-Agent
Syst 14:5–30
67. Wolfram S (1986) Theory and Applications of Cellular Au-
tomata. World Press, Singapore
68. Wooldridge MJ, Jennings NR (1995) Intelligent agents: theory
and practice. Knowl Eng Rev 10:115–152
69. Zambonelli F, Parunak HVD (2003) Signs of a revolution in
computer science and software engineering. In: Petta P, Tolks-
dorf R, Zambonelli F (eds) Engineering Societies in the Agents
World III, Third International Workshop, ESAW 2002, Madrid,
Spain, September 2002, Revised Papers. Lecture Notes in Com-
puter Science, vol 2577. Springer, Berlin, pp 13–28
Agent Based Modeling and
Simulation, Introduction to
FILIPPO CASTIGLIONE
Institute for Computing Applications (IAC) –
National Research Council (CNR), Rome, Italy
Agent-based modeling (ABM) is a relatively new com-
putational modeling paradigm that is markedly useful in
studying complex systems composed of a large number of
interacting entities with many degrees of freedom. Other
names for ABM are individual-based modeling (IBM) or
multi-agent systems (MAS). Physicists often use the term
micro-simulation or interaction-based computing.
The basic idea of ABM is to construct the computa-
tional counterpart of a conceptual model of a system under
study on the basis of discrete entities (agents) with some
properties and behavioral rules, and then to simulate them
in a computer to mimic the real phenomena.
The deﬁnition of agent is somewhat controversial as
witnessed by the fact that the models found in the lit-
erature adopt an extremely heterogeneous rationale. The
agent is an autonomous entity having its own internal state
reﬂecting its perception of the environment and interact-
ing with other entities according to more or less sophisti-
cated rules. In practice, the term agent is used to indicate
entities ranging all the way from simple pieces of software
to “conscious” entities with learning capabilities. For ex-
ample, there are “helper” agents for web retrieval, robotic
agents to explore inhospitable environments, buyer/seller
agents in an economy, and so on. Roughly speaking, an
entity is an “agent” if it has some degree of autonomy, that
is, if it is distinguishable from its environment by some
kind of spatial, temporal, or functional attribute: an agent
must be identiﬁable. Moreover, it is usually required that
an agent must have some autonomy of action and that it
must be able to engage in tasks in an environment without
direct external control.
From simple agents, which interact locally with simple
rules of behavior, merely responding beﬁttingly to envi-
ronmental cues, and not necessarily striving for an over-
all goal, we observe a synergy which leads to a higher-level
whole with much more intricate behavior than the compo-
nent agents (holism, meaning all, entire, total). Agents can
be identiﬁed on the basis of a set of properties that must
characterize an entity, and in particular, autonomy (the ca-
pability of operating without intervention by humans, and
a certain degree of control over its own state); social abil-
ity (the capability of interacting by employing some kind
of agent communication language); reactivity (the ability

198 A
Agent Based Modeling and Simulation, Introduction to
to perceive an environment in which it is situated and re-
spond to perceived changes); and pro-activeness (the abil-
ity to take the initiative, starting some activity according
to internal goals rather than as a reaction to an external
stimulus). Moreover, it is also conceptually important to
deﬁnite what the agent “environment” in an ABM is.
In general, given the relative immaturity of this mod-
eling paradigm and the broad spectrum of disciplines in
which it is applied, a clear cut and widely accepted deﬁni-
tion of high level concepts of agents, environment, inter-
actions and so on, is still lacking. Therefore a real ABM
ontology is needed to address the epistemological issues
related to the agent-based paradigm of modeling of com-
plex systems in order to attempt to reach a more general
comprehension of emergent properties which, though as-
cribed to the deﬁnition of a speciﬁc application domain,
are also universal (see ▷Agent Based Modeling and Sim-
ulation).
Historically, the ﬁrst simple conceptual form of agent-
based models was developed in the late 1940s, and it took
the advent of the computer to show its modeling power.
This is the Von Neumann machine, a theoretical machine
capable of reproduction. The device von Neumann pro-
posed would follow precisely detailed instructions to pro-
duce an identical copy of itself. The concept was then im-
proved by Stanislaw Ulam. He suggested that the machine
be built on paper, as a collection of cells on a grid. This
idea inspired von Neumann to create the ﬁrst of the mod-
els later termed cellular automata (CA). John Conway then
constructed the well-known “Game of Life”. Unlike the
von Neumann’s machine, Conway’s Game of Life operated
by simple rules in a virtual world in the form of a two-di-
mensional checkerboard.
Conway’s Game of Life has become a paradigmatic
example of models concerned with the emergence of or-
der in nature. How do systems self-organize themselves
and spontaneously achieve a higher-ordered state? These
and other questions have been deeply adressed in the ﬁrst
workshop on Artiﬁcial Life held in the late 1980s in Santa
Fe. This workshop shaped the ALife ﬁeld of research.
Agent-based modeling is historically connected to ALife
because it has become a distinctive form of modeling and
simulation in this ﬁeld. In fact, the essential features of AL-
ife models are translated into computational algorithms
throug agent-based modeling (see ▷Agent Based Mod-
eling and Artiﬁcial Life).
Agent-based models can be seen as the natural exten-
sion of the CA-like models, which have been very success-
ful in the past decades in shedding light on various phys-
ical phenomena. One important characteristic of ABMs
which distinguishes them from cellular automata, is the
potential asynchrony of the interactions among agents and
between agents and their environments. In ABMs, agents
typically do not simultaneously perform actions at con-
stant time-steps, as in CAs or boolean networks. Rather,
their actions follow discrete-event cues or a sequential
schedule of interactions. The discrete-event setup allows
for the cohabitation of agents with diﬀerent environmen-
tal experiences. Also ABMs are not necessarily grid-based
nor do agents “tile” the environment.
Physics investigation is based on building models of
reality. It is a common experience that, even using simple
“building blocks”, one usually obtains systems whose be-
havior is quite complex. This is the reason why CA-like,
and therefore agent-based models, have been used exten-
sively among physicists to investigate experimentally (that
is, on a computer) the essential ingredients of a complex
phenomenon. Rather than being derived from some fun-
damental law of physics, these essential ingredients con-
stitute artiﬁcial worlds. Therefore, there exists a pathway
from Newton’s laws to CA and ABM simulations in clas-
sical physics that has not yet expressed all its potential (see
▷Interaction Based Computing in Physics).
CA-like models also proved very successful in theoret-
ical biology to describe the aggregation of cells or microor-
ganisms in normal or pathological conditions (see ▷Cel-
lular Automaton Modeling of Tumor Invasion).
Returning to the concept of agent in the ABM par-
adigm, an agent may represent a particle, a ﬁnancial trader,
a cell in a living organism, a predator in a complex ecosys-
tem, a power plant, an atom belonging to a certain mate-
rial, a buyer in a closed economy, customers in a market
model, forest trees, cars in large traﬃc vehicle system, etc.
Once the level of description of the system under study
has been deﬁned, the identiﬁcation of such entities is quite
straightforward. For example, if one looks at the world
economy, then the correct choice of agents are nations,
rather than individual companies. On the other hand, if
one is interested in looking at the dynamics of a stock, then
the entities determining the price evolution are the buyers
and sellers.
This example points to a ﬁeld where ABM provides
a very interesting and valuable instrument of research.
Indeed, mainstream economic models typically make the
assumption that an entire group of agents, for example,
“investors”, can be modeled with a single “rational rep-
resentative agent”. While this assumption has proven ex-
tremely useful in advancing the science of economics by
yielding analytically tractable models, it is clear that the
assumption is not realistic: people diﬀer in their tastes, be-
liefs, and sophistication, and as many psychological stud-
ies have shown, they often deviate from rationality in

Agent Based Modeling and Simulation, Introduction to
A
199
systematic ways. Agent-based computational economics
(ACE) is a framework allowing economics to expand be-
yond the realm of the “rational representative agent”. By
modeling and simulating the behavior of each agent and
interactions among agents, agent-based simulation allows
us to investigate the dynamics of complex economic sys-
tems with many heterogeneous (and not necessarily fully
rational) agents. Agent-based computational economics
complements the traditional analytical approach and is
gradually becoming a standard tool in economic analysis
(see ▷Agent Based Computational Economics).
Because the paradigm of agent-based modeling and
simulation can handle richness of detail in the agent’s de-
scription and behavior, this methodology is very appealing
for the study and simulation of social systems, where the
behavior and the heterogeneity of the interacting compo-
nents are not safely reducible to some stylized or simple
mechanism. Social phenomena simulation in the area of
agent-based modeling and simulation, concerns the emu-
lation of the individual behavior of a group of social en-
tities, typically including their cognition, actions and in-
teraction. This ﬁeld of research aims at “growing” artiﬁcial
society following a bottom-up approach.
Historically, the birth of the agent-based model as
a model for social systems can be primarily attributed to
a computer scientist, Craig Reynolds. He tried to model
the reality of lively biological agents, known as artiﬁcial
life, a term coined by Christopher Langton. In 1996 Joshua
M. Epstein and Robert Axtell developed the ﬁrst large scale
agent model, the Sugarscape, to simulate and explore the
role of social phenomenon such as seasonal migrations,
pollution, sexual reproduction, combat, transmission of
disease and even culture (see ▷Social Phenomena Sim-
ulation).
In the ﬁeld of artiﬁcial intelligence, the collective be-
havior of agents that without central control, collectively
carry out tasks normally requiring some form of “intel-
ligence”, constitutes the central concept in the ﬁeld of
swarm intelligence. The term “swarm intelligence” ﬁrst ap-
peared in 1989. As the use of the term swarm intelligence
has increased, its meaning has broadened to a point in
which it is often understood to encompass almost any type
of collective behavior. Technologically, the importance of
“swarms” is mainly based on potential advantages over
centralized systems. The potential advantages are: econ-
omy (the swarm units are simple, hence, in principle, mass
producible, modularizable, interchangeable, and dispos-
able; reliability (due to the redundancy of the components;
destruction/death of some units has negligible eﬀect on the
accomplishment of the task, as the swarm adapts to the
loss of few units); ability to perform tasks beyond those of
centralized systems, for example, escaping enemy detec-
tion. From this initial perspective on potential advantages,
the actual application of swarm intelligence has extended
to many areas, and inspired potential future applications
in defense and space technologies, (for example, control
of groups of unmanned vehicles in land, water, or air),
ﬂexible manufacturing systems, and advanced computer
technologies (bio-computing), medical technologies, and
telecommunications (see ▷Swarm Intelligence).
Similarly, robotics has adopted the ABM paradigm
to study, by means of simulation, the crucial features of
adaptation and cooperation in the pursuit of a global
goal. Adaptive behavior concerns the study of how organ-
isms develop their behavioral and cognitive skills through
a synthetic methodology, consisting of designing artiﬁ-
cial agents which are able to adapt to their environment
autonomously. These studies are important both from
a modeling point of view (that is, for better understand-
ing intelligence and adaptation in natural beings) and
from an engineering point of view (that is, for develop-
ing artifacts displaying eﬀective behavioral and cognitive
skills) (see ▷Embodied and Situated Agents, Adaptive
Behavior in).
What makes ABM a novel and interesting paradigm
of modeling is the idea that agents are individually repre-
sented and “monitored” in the computer’s memory. One
can, at any time during the simulation, ask a question such
as “what is the age distribution of the agents?”, or “how
many stocks have accumulated buyers following that spe-
ciﬁc strategy?”, or “what is the average velocity of the par-
ticles?”. “Large scale” simulations in the context of agent-
based modeling are not only simulations that are large in
terms of size (number of agents simulated), but are also
complex. Complexity is inherent in agent-based models,
as they are usually composed of dynamic, heterogeneous,
interacting agents. Large-scale agent-based models have
also been referred to as “Massively Multi-agent Systems
(MMAS)”. MMAS is deﬁned as “beyond resource limi-
tation”: the number of agents exceeds local computer re-
sources, or the situations are too complex to design and
program given human cognitive resource limits. There-
fore, for agent-based modeling, “large scale” is not simply
a size problem, it is also a problem of managing complexity
to ensure scalability of the agent model. Agent-based mod-
els increase in scale as the modeler requires many agents
to investigate whole system behavior, or as the modeler
wishes to fully examine the response of a single agent in
a realistic context. There are two key problems that have
to be tackled as the scale of a multi-agent system increases:
computational resources limit the simulation time and/or
data storage capacity; and agent model analysis may be-

200 A
Agent Based Models in Economics and Complexity
come more diﬃcult. Diﬃculty in analyzing the model may
be due to the model system having a large number of com-
plex components or due to memory for model output stor-
age being restricted by computer resources (see ▷Agent
Based Modeling, Large Scale Simulations).
For implementation of agent-based models, both do-
main-speciﬁc and general-purpose languages are routinely
used. Domain-speciﬁc languages include business-ori-
ented languages (for example, spreadsheet programming
tools); science and engineering languages (such as Math-
ematica); and dedicated agent-based modeling languages
(for example, NetLogo). General-purpose languages can
be used directly (as in the case of Java programming)
or within agent-based modeling toolkits (for example,
Repast). The choice that is most appropriate for a given
modeling project depends on both the requirements of
that project and the resources available to implement it
(see ▷Agent Based Modeling and Computer Languages).
Interestingly, ABM is not being used exclusively in sci-
ence. In fact, the entertainment industry has promoted its
own interest in the ABM technology. As graphics technol-
ogy has improved in recent years, more and more impor-
tance has been placed on the behavior of virtual characters
in applications set in virtual worlds in areas such as games,
movies and simulations. The behavior of virtual charac-
ters should be believable in order to create the illusion that
these virtual worlds are populated with living characters.
This has led to the application of agent-based modeling to
the control of these virtual characters. There are a number
of advantages of using agent-based modeling techniques
which include the fact that they remove the requirement
for hand controlling all agents in a virtual environment,
and allow agents in games to respond to unexpected ac-
tions by players (see ▷Computer Graphics and Games,
Agent Based Modeling in).
Since it is diﬃcult to formally analyze complex multi-
agent systems, they are mainly studied through computer
simulations. While computer simulations can be very use-
ful, results obtained through simulations do not formally
validate the observed behavior. It is widely recognized that
building a sound and widely applicable theory for ABM
systems will require an inter-disciplinary approach and
the development of new mathematical and computational
concepts. In other words, there is a compelling need for
a mathematical framework which one can use to represent
ABM systems and formally establish their properties. For-
tunately, some known mathematical frameworks already
exist that can be used to formally describe multi-agent sys-
tems, for example, that of ﬁnite dynamical systems (both
deterministic and stochastic). A sampling of the results
from this ﬁeld of mathematics shows that they can be used
to carry out rigorous studies of the properties of multi-
agent systems and, in general, that they can also serve as
universal models for computation. Moreover, special cases
of dynamical systems (sequential dynamical systems) can
be structured in accordance with the theory of categories
and therefore provide the basis for a formal theory to de-
scribe ABM behavior (see ▷Agent Based Modeling, Math-
ematical Formalism for).
On the same line of thought, agents and interaction
can be studied from the perspective of logic and computer
science. In particular, ideas about logical dynamics, games
semantics and geometry of interaction, which have been de-
veloped over the past two decades, lead towards a struc-
tural theory of agents and interaction. This provides a ba-
sis for powerful logical methods such as compositionality,
types and high-order calculi, which have proved so fruitful
in computer science, to be applied in the domain of ABM
and simulation (see ▷Logic and Geometry of Agents in
Agent-Based Modeling).
The appeal of ABM methodology in science increases
manifestly with advances in computational power of mod-
ern computers. However, it is important to bear in mind
that increasing the complexity of a model does not neces-
sarily bring more understanding of the fundamental laws
governing the overall dynamics. Actually, beyond a cer-
tain level of model complexity, the model loses its ability
to explain or predict reality, thus reducing model build-
ing to a mere surrogate of reality where things may hap-
pen with a surprising adherence to reality, but we are un-
able to explain why this happens. Therefore, model con-
struction must proceed incrementally, step by step, possi-
bly validating the model at each stage of development be-
fore adding more details. ABM technology is very power-
ful but, if badly used, could reduce science to a mere exer-
cise consisting of mimicking reality.
Agent Based Models in Economics
and Complexity
MAURO GALLEGATI1, MATTEO G. RICHIARDI1,2
1 Università Politecnica delle Marche, Ancona, Italy
2 Collegio Carlo Alberto – LABORatorio R. Revelli,
Moncalieri, Italy
Article Outline
Glossary
Deﬁnition of the Subject
Introduction

Agent Based Models in Economics and Complexity
A
201
Some Limits of the Mainstream Approach
The Economics of Complexity
Additional Features of Agent-Based Models
An Ante Litteram Agent-Based Model:
Thomas Schelling’s Segregation Model
The Development of Agent-Based Modeling
A Recursive System Representation
of Agent-Based Models
Analysis of Model Behavior
Validation and Estimation
The Role of Economic Policy
Future Directions
Bibliography
Glossary
Abduction also called inference to the best explanation,
abduction is a method of reasoning in which one looks
for the hypothesis that would best explain the relevant
evidence.
Agents entities of a model that (i) are perceived as a unit
from the outside, (ii) have the ability to act, and pos-
sibly to react to external stimuli and interact with the
environment and other agents.
Agent-based computational economics (ACE)
is
the
computational study of economic processes modeled
as dynamic systems of interacting agent.
Agent-based models (ABM) are models where (i) there
is a multitude of objects that interact with each other
and with the environment; (ii) the objects are au-
tonomous, i. e. there is no central, or top-down con-
trol over their behavior; and (iii) the outcome of their
interaction is numerically computed.
Complexity there are more than 45 existing deﬁnitions
of complexity (Seth Lloyd, as reported on p. 303
in [97]). However, they can be grouped in just two
broad classes: a computational view and a descriptive
view. Computational (or algorithmic) complexity is
a measure of the amount of information necessary to
compute a system; descriptive complexity refers to the
amount of information necessary to describe a system.
We refer to this second view, and deﬁne complex sys-
tems as systems characterized by emergent properties
(see emergence).
Deduction the logical derivation of conclusions from
given premises.
Economics is the science about the intended and unin-
tended consequences of individual actions, in an en-
vironment characterized by scarce resources that both
requires and forces to interaction.
Emergence the spontaneous formation of self-organized
structures at diﬀerent layers of a hierarchical system
conﬁguration.
Evolution in biology, is a change in the inherited traits of
a population from one generation to the next. In social
sciences it is intended as an endogenous change over
time in the behavior of the population, originated by
competitive pressure and/or learning.
Heterogeneity non-degenerate distribution of character-
istics in a population of agents.
Induction the intuition of general patterns from the ob-
servation of statistical regularities.
Interaction a situation when the actions or the supposed
actions of one agent may aﬀect those of other agents
within a reference group.
Out-of-equilibrium a situation when the behavior of
a system, in terms of individual strategies or aggregate
outcomes, is not stable.
Definition of the Subject
A crucial aspect of the complexity approach is how inter-
acting elements produce aggregate patterns that those ele-
ments in turn react to. This leads to the emergence of ag-
gregate properties and structures that cannot be guessed
by looking only at individual behavior.
It has been argued [144] that complexity is ubiquitous
in economic problems (although this is rarely acknowl-
edged in economic modeling), since (i) the economy is
inherently characterized by the interaction of individuals,
and (ii) these individuals have cognitive abilities, e. g. they
form expectations on aggregate outcomes and base their
behavior upon them: “Imagine how hard physics would
be if electrons could think”, is how the Nobel prize winner
Murray Gell–Mann, a physicist, has put it (as reported by
Page [131]).
Explicitly considering how heterogeneous elements
dynamically develop their behavior through interaction is
a hard task analytically, the equilibrium analysis of main-
stream (neoclassical) economics being a shortcut that in
many cases is at risk of throwing the baby out with the bath
water, so to speak. On the other hand, numerical computa-
tion of the dynamics of the process started to be a feasible
alternative only when computer power became widely ac-
cessible. The computational study of heterogeneous inter-
action agents is called agent-based modeling (ABM). In-
terestingly, among its ﬁrst applications a prominent role
was given to economic models [4], although it was quickly
found of value in other disciplines too (from sociology
to ecology, from biology to medicine). The goal of this
chapter is to motivate the use of the complexity approach

202 A
Agent Based Models in Economics and Complexity
and agent-based modeling in economics, by discussing
the weaknesses of the traditional paradigm of mainstream
economics, and then explain what ABM is and which re-
search and policy questions it can help to analyze.
Introduction
Economics is in troubled waters. Although there exists
a mainstream approach, its internal coherence and abil-
ity to explain the empirical evidence are increasingly ques-
tioned. The causes of the present state of aﬀairs go back
to the middle of the eighteenth century, when some of the
Western economies were transformed by the technolog-
ical progress which lead to the industrial revolution. This
was one century after the Newtonian revolution in physics:
from the small apple to the enormous planets, all objects
seemed to obey the simple natural law of gravitation. It
was therefore natural for a new ﬁgure of social scientist,
the economist, to borrow the method (mathematics) of
the most successful hard science, physics, allowing for the
mutation of political economy into economics. It was (and
still is) the mechanical physics of the seventeenth century,
which ruled economics. In the ﬁnal chapter of his Gen-
eral Theory, Keynes wrote of politicians as slaves of late
economists: in their turn, they are slaves of late physicists
of the seventeenth century (see also [125]).
From then on, economics lived its own evolution based
on the classical physics assumptions (reductionism, de-
terminism and mechanicism). Quite remarkably, the ap-
proach of statistical physics, which deeply aﬀected physical
science at the turn of the nineteenth century by emphasiz-
ing the diﬀerence between micro and macro, was adopted
by Keynes around the mid 1930s. However, after decades
of extraordinary success it was rejected by the neoclassical
school around the mid 1970s, which framed the discipline
into the old approach and ignored, by deﬁnition, any in-
terdependencies among agents and diﬀerence between in-
dividual and aggregate behavior (being agents, electrons,
nations or planets).
The ideas of natural laws and equilibrium have been
transplanted into economics sic et simpliciter. As a con-
sequence of the adoption of the classical mechanics
paradigm, the diﬀerence between micro and macro was
analyzed under a reductionist approach. In such a setting,
aggregation is simply the process of summing up market
outcomes of individual entities to obtain economy-wide
totals. This means that there is no diﬀerence between mi-
cro and macro: the dynamics of the whole is nothing but
a summation of the dynamics of its components (in term
of physics, the motion of a planet can be described by the
dynamics of the atoms composing it). This approach does
not take into consideration that there might be two-way
interdependencies between the agents and the aggregate
properties of the system: interacting elements produce ag-
gregate patterns that those elements in turn react to. What
macroeconomists typically fail to realize is that the cor-
rect procedure of aggregation is not a sum: this is when
emergence enters the drama. With the term emergence we
mean the arising of complex structures from simple indi-
vidual rules [147,153,171]. Empirical evidence, as well as
experimental tests, shows that aggregation generates regu-
larities, i. e. simple individual rules, when aggregated, pro-
duce statistical regularities or well-shaped aggregate func-
tions: regularities emerge from individual chaos [106]. The
concept of equilibrium is quite a dramatic example. In
many economic models equilibrium is described as a state
in which (individual and aggregate) demand equals sup-
ply. The notion of statistical equilibrium, in which the
aggregate equilibrium is compatible with individual dis-
equilibrium, is outside the box of tools of the mainstream
economist. The same is true for the notion of evolutionary
equilibrium (at an aggregate level) developed in biology.
The equilibrium of a system no longer requires that every
single element be in equilibrium by itself, but rather that
the statistical distributions describing aggregate phenom-
ena be stable, i. e. in “[...] a state of macroscopic equilib-
rium maintained by a large number of transitions in op-
posite directions” (p. 356 in [64]).
According to this view, an individual organism is in
equilibrium only when it is dead. A consequence of the
idea that macroscopic phenomena can emerge is that re-
ductionism is wrong.
Ironically, since it can be argued, as we will do in
the section below, that economics strongly needs this
methodological twist [144], ABM has received lees at-
tention in economics than in other sciences ([110]; but
[82] is a counter-example). The aim of this chapter is
not to provide a review of applications of the complex-
ity theory to economics (the interested reader is referred
to [15,26,60,124,140,142]), but rather to describe the de-
velopment of the Agent-Based Modeling (ABM) approach
to complexity.
The chapter is structured as follows: after reviewing
some limits of mainstream economics (Sect. “Additional
Features of Agent-Based Models”), Sects. “The Economics
of Complexity” and “Additional Features of Agent-Based
Models” describe how the complexity perspective diﬀers
from the traditional one, and how many problems of the
mainstream approach can be overcome by ABM. As an ex-
ample, we present a prototypical example of ABM, based
on the work of Thomas Schelling on the dynamics of seg-
regation. After dedicating some sections to, respectively,

Agent Based Models in Economics and Complexity
A
203
a skeleton history of ABM, a recursive system represen-
tation of these models, a discussion on how ABM can be
interpreted, estimated and validated, we ﬁnally discus how
the complexity approach can be used to guide policy inter-
vention and analysis. A ﬁnal section discusses the achieve-
ments of the ABM agenda.
Some Limits of the Mainstream Approach
The research program launched by the neoclassical school
states that macroeconomics should be explicitly grounded
on microfoundations. This is how Robert Lucas put it:
“The most interesting recent developments in macroeco-
nomic theory seem to me describable as the reincorpo-
ration of aggregative problems [...] within the general
framework of ‘microeconomic’ theory. If these develop-
ments succeed, the term ‘macroeconomic’ will be simply
disappear from use and the modiﬁer ‘micro’ will become
superﬂuous. We will simply speak, as did Smith, Marshall
and Walras, of economic theory” (pp. 107–108 in [115]).
According to the mainstream, this implies that economic
phenomena at a macroscopic level should be explained
as a summation of the activities undertaken by individ-
ual decision makers. This procedure of microfoundation
is very diﬀerent from that now used in physics. The latter
starts from the micro-dynamics of the single particle, as
expressed by the Liouville equation and, through the mas-
ter equation, ends up with the macroscopic equations. In
the aggregation process, the dynamics of the agents lose
their degree of freedom and behave coherently in the ag-
gregate. In mainstream economics, while the procedure is
formally the same (from micro to macro), it is assumed
that the dynamics of the agents are those of the aggregate.
The reduction of the degree of freedom, which is char-
acteristic of the aggregation problem in physics, is there-
fore ruled out: a rational agent with complete information
chooses to implement the individually optimal behavior,
without additional constraints. There are three main pil-
lars of this approach: (i) the precepts of the rational choice-
theoretic tradition; (ii) the equilibrium concept of the Wal-
rasian analysis; and (iii) the reductionist approach of clas-
sical physics. In the following, we will show that assump-
tions (i)–(ii), which constitute the necessary conditions for
reducing macro to micro, are logically ﬂawed (and empir-
ically unfounded), while rejection of (iii) opens the road to
complexity.
Mainstream economics is axiomatic and based on un-
realistic (or unveriﬁable) assumptions. According to the
supporters of this view, such an abstraction is necessary
since the real world is complicated: rather than compro-
mising the epistemic worth of economics, such assump-
tions are essential for economic knowledge. However, this
argument does not invalidate the criticism of unrealis-
tic assumptions [136]. While it requires internal coher-
ence, so that theorems can be logically deduced from a set
of assumptions, it abstracts from external coherence be-
tween theoretical statements and empirical evidence. Of
course, this implies an important epistemological detach-
ment from falsiﬁable sciences like physics. In setting the
methodological stage for the dynamic stochastic general
equilibrium (DSGE) macroeconomic theory, Lucas and
Sargent declared:
“An economy following a multivariate stochastic
process is now routinely described as being in equi-
librium, by which is meant nothing more that at
each point in time (a) markets clears and (b) agents
act in their own self-interest. This development,
which stemmed mainly from the work of Arrow
[...] and Debreu [...], implies that simply to look
at any economic time series and conclude that it
is a disequilibrium phenomenon is a meaningless
observation. [...] The key elements of these mod-
els are that agents are rational, reacting to policy
changes in a way which is in their best interests pri-
vately, and that the impulses which trigger business
ﬂuctuations are mainly unanticipated shocks.” (p. 7
in [116]).
The self-regulating order of Adam Smith [153] is trans-
formed into a competitive general equilibrium (GE) in the
form elaborated in the 1870s by Walras, that is a conﬁgura-
tion of (fully ﬂexible) prices and plans of action such that,
at those prices, all agents can carry out their chosen plans
and, consequently, markets clear. In a continuous eﬀort of
generalization and analytical sophistication, modern (neo-
classical) economists interested in building microfounda-
tions for macroeconomics soon recurred to the reﬁnement
proposed in the 1950s by Arrow and Debreu [14], who
showed that also individual intertemporal (on an inﬁnite
horizon) optimization yields a GE, as soon as the econ-
omy is equipped with perfect price foresight for each fu-
ture state of nature and a complete set of Arrow-securities
markets [11], all open at time zero and closed simultane-
ously. Whenever these conditions hold true, the GE is an
allocation that maximizes a properly deﬁned social welfare
function, or the equilibrium is Pareto-eﬃcient (First Wel-
fare Theorem).
The literature has pointed out several logical incon-
sistencies of the mainstream approach. Davis [44] identi-
ﬁes three impossibility results, which determine the break-
down of the mainstream, i. e. neoclassical, economics:

204 A
Agent Based Models in Economics and Complexity
(i) Arrow’s 1951 theorem showing that neoclassical theory
is unable to explain social choice [10]; (ii) the Cambridge
capital debate pointing out that mainstream is contradic-
tory with respect to the concept of aggregate capital [40];
and (iii) the Sonnenschein–Mantel–Debreu results show-
ing that the standard comparative static reasoning is inap-
plicable in general equilibrium models. In particular, a few
points are worth remembering here.
1. The GE is neither unique nor locally stable under gen-
eral conditions. This negative result, which refers to the
work of Sonnenschein [155], Debreu [46] and Man-
tel [119], can be summarized along the following lines.
Let the aggregate excess demand function F(p) – ob-
tained from aggregating among individual excess de-
mands f (p) – be a mapping from the price simplex ˘
to the commodity space PN. A GE is deﬁned as a price
vector p such that F(p) D 0. It turns out that the only
conditions that F() inherits from f () are continuity,
homogeneity of degree zero and the Walras’ law (i. e.,
the total value of excess demand is zero). These assure
the existence, but neither the uniqueness nor the local
stability of p, unless preferences generating individ-
ual demand functions are restricted to very implausible
cases.
2. The existence of a GE is proved via the Brower’s ﬁx
point theorem, i. e. by ﬁnding a continuous function
g(): ˘ ! ˘ so that any ﬁxed point for g() is also
an equilibrium price vector F(p) D 0. Suppose that
we are interested in ﬁnding an algorithm which, start-
ing from an arbitrary price vector p, chooses price se-
quences to check for p and halts when it ﬁnds it.
In other terms, to ﬁnd the GE price vector F(p) D 0
means that halting conﬁgurations are decidable. As this
violates the undecidability of the halting problem for
Turing machines, from a recursion theoretic viewpoint
the GE solution is incomputable [138,167]. Notice that
the same problem applies, in spite of its name, to the
class of computable GE models [169].
3. By construction, in a GE all transactions are undertaken
at the same equilibrium price vector. Economic theory
has worked out two mechanisms capable of reaching
this outcome. First, one can assume that buyers and
sellers adjust, costless, their optimal supplies and de-
mands to prices called out by a (explicit or implicit) ﬁc-
titious auctioneer, who continues to do his job until he
ﬁnds a price vector which clears all markets. Only then
transactions take place (Walras’ assumption). Alterna-
tively, buyers and sellers sign provisional contracts and
are allowed to freely (i. e., without any cost) recontract
until a price vector is found which makes individual
plans fully compatible. Once again, transactions occur
only after the equilibrium price vector has been es-
tablished (Edgeworth’s assumption). Regardless of the
mechanism one adopts, the GE model is one in which
the formation of prices precedes the process of ex-
change, instead of being the result of it, through a taton-
nement process occurring in a meta-time. Real markets
work the other way round and operates in real time, so
that the GE model cannot be considered a scientiﬁc ex-
planation of real economic phenomena [9].
4. It has been widely recognized since Debreu [45], that
integrating money in the theory of value represented
by the GE model is at best problematic. No economic
agent can individually decide to monetize alone; mon-
etary trade should be the equilibrium outcome of mar-
ket interactions among optimizing agents. The use of
money – that is, a common medium of exchange and
a store of value – implies that one party to a transaction
gives up something valuable (for instance, his endow-
ment or production) for something inherently useless
(a ﬁduciary token for which he has no immediate use)
in the hope of advantageously re-trading it in the fu-
ture. Given that in a GE model actual transactions take
place only after a price vector coordinating all trading
plans has been freely found, money can be consistently
introduced into the picture only if the logical keystone
of the absence of transaction costs is abandoned. By the
same token, since credit makes sense only if agents can
sign contracts in which one side promises future deliv-
ery of goods or services to the other side, in equilibrium
markets for debt are meaningless, and bankruptcy can
be safely ignored. Finally, as the very notion of a GE
implies that all transactions occur only when individual
plans are mutually compatible, and this has to be true
also in the labor market, the empirically observed phe-
nomenon of involuntary unemployment and the mi-
crofoundation program put forth by Lucas and Sargent
are logically inconsistent.
5. The very absence of money and credit is a consequence
of the fact that in GE there is no time. The only role as-
signed to time in a GE model is, in fact, that of dating
commodities. Products, technologies and preferences
are exogenously given and ﬁxed from the outset. The
convenient implication of banning out-of-equilibrium
transactions is simply that of getting rid of any disturb-
ing inﬂuence of intermediary modiﬁcations of endow-
ments – and therefore of individual excess demands –
on the ﬁnal equilibrium outcome. The introduction of
non-Walrasian elements into the GE microfoundations
program – such as ﬁxed or sticky prices, imperfect com-
petition and incomplete markets leading to temporary

Agent Based Models in Economics and Complexity
A
205
equilibrium models – yields interesting Keynesian fea-
tures such as the breaking of the Say’s law and scope
for a monetary theory of production, a rationale for ﬁ-
nancial institutions and a more persuasive treatment
of informational frictions. As argued in Vriend [165],
however, all these approaches preserve a Walrasian per-
spective in that models are invariably closed by a GE
solution concept which, implicitly or (more often) not,
implies the existence of a ﬁctitious auctioneer who pro-
cesses information, calculates equilibrium prices and
quantities, and regulates transactions. As a result, if
the Walrasian auctioneer is removed the decentralized
economy becomes dynamically incomplete, as we are
not left with any mechanism determining how quanti-
ties and prices are set and how exchanges occur.
The ﬂaws of the solution adopted by mainstream macroe-
conomists to overcome the problems of uniqueness and
stability of equilibrium on the one hand, and of analytical-
tractability on the other one – i. e. the usage of a represen-
tative agent (RA) whose choices summarize those of the
whole population of agents – are so pervasive that we dis-
cuss them hereafter.
6. Although the RA framework has a long history, it is
standard to build the microfoundation procedure on
it only after Lucas’ critique paper [114]. Mainstream
models are characterized by an explicitly stated op-
timization problem of the RA, while the derived in-
dividual demand or supply curves are used to obtain
the aggregate demand or supply curves. Even when
the models allow for heterogeneity, interaction is gen-
erally absent (the so-called weak interaction hypoth-
esis [139]). The use of RA models should allow one
to avoid the Lucas critique, to provide microfounda-
tions to macroeconomics, and, ça va sans dire, to build
Walrasian general equilibrium models. Since models
with many heterogeneous interacting agents are com-
plicated and no closed form solution is often available
(aggregation of heterogenous interacting agents is ana-
lyzed in [5,6,7,53,78]), economists assume the existence
of an RA: a simpliﬁcation that makes it easier to solve
for the competitive equilibrium allocation, since direct
interaction is ruled out by deﬁnitions. Unfortunately, as
Hildenbrand and Kirman [95] noted:
“There are no assumptions on isolated individu-
als, which will give us the properties of aggregate
behavior. We are reduced to making assump-
tions at the aggregate level, which cannot be jus-
tiﬁed, by the usual individualistic assumptions.
This problem is usually avoided in the macroeco-
nomic literature by assuming that the economy
behaves like an individual. Such an assumption
cannot be justiﬁed in the context of the standard
model”.
The equilibria of general equilibrium models with a RA
are characterized by a complete absence of trade and ex-
change, which is a counterfactual idea. Kirman [99], Gal-
legati [76] and Caballero [36] show that RA models ig-
nore valid aggregation concerns, by neglecting interaction
and emergence, hence committing fallacy of composition
(what in philosophy is called fallacy of division, i. e. to at-
tribute properties to a diﬀerent level than where the prop-
erty is observed: game theory oﬀers a good case in point
with the concept of Nash equilibrium, by assuming that
social regularities come from the agent level equilibrium).
Those authors provide examples in which the RA does not
represent the individuals in the economy so that the re-
duction of a group of heterogeneous agents to an RA is
not just an analytical convenience, but it is both unjusti-
ﬁed and leads to conclusions which are usually mislead-
ing and often wrong ([99]; see also [98]). A further result,
which is a proof of the logical fallacy in bridging the mi-
cro to the macro is the impossibility theorem of Arrow: it
shows that an ensemble of people, which has to collectively
take a decision, cannot show the same rationality of an in-
dividual [123]. Moreover, the standard econometric tools
are based upon the assumption of an RA. If the economic
system is populated by heterogeneous (not necessarily in-
teracting) agents, then the problem of the microfounda-
tion of macroeconometrics becomes a central topic, since
some issues (e. g., co-integration, Granger-causality, im-
pulse-response function of structural VAR) lose their sig-
niﬁcance [69].
All in all, we might say that the failure of the RA frame-
work, points out the vacuum of the mainstream micro-
foundation literature, which ignores interactions: no box
of tools is available to connect the micro and the macro
levels, beside the RA whose existence is at odds with the
empirical evidence [30,158] and the equilibrium theory as
well [99].
The Economics of Complexity
According to the mainstream approach there is no direct
interaction among economic units (for a pioneeristic and
neglected contribution see [68]; see also [101]). In the most
extreme case, any individual strategy is excluded (princi-
ple of excluded strategy, according to Schumpeter [149])
and agents are homogeneous. Small departures from the

206 A
Agent Based Models in Economics and Complexity
perfect information hypothesis are incoherent with the
Arrow–Debreu general equilibrium model, as shown by
Grossman and Stiglitz [88], since they open the chance
of having direct links among agents [156]. In particular, if
prices convey information about the quality there cannot
be an equilibrium price as determined by the demand-sup-
ply schedule, since demand curves depend on the proba-
bility distribution of the supply (p. 98 in [87]).
What characterizes a complex system is the notion
of emergence, that is the spontaneous formation of self-
organized structures at diﬀerent layers of a hierarchical
system conﬁguration [43]. Rather, mainstream economics
conceptualizes economic systems as consisting of several
identical and isolated components, each one being a copy
of a RA. The aggregate solution can thus be obtained by
means of a simple summation of the choices made by each
optimizing agent. The RA device, of course, is a way of
avoiding the problem of aggregation by eliminating het-
erogeneity. But heterogeneity is still there. If the macroe-
conomist takes it seriously, he/she has to derive aggregate
quantities and their relationships from the analysis of the
micro-behavior of diﬀerent agents. This is exactly the key
point of the aggregation problem: starting from the micro-
equations describing/representing the (optimal) choices of
the economic units, what can we say about the macro-
equations? Do they have the same functional form of the
micro-equations (the analogy principle)? If not, how is the
macro-theory derived?
The complexity approach to economics discards the
GE approach to the microfoundation program, as well
as its RA shorthand version. Instead of asking to deduc-
tively prove the existence of an equilibrium price vector
p such that F(p) D 0, it aims at explicitly constructing it
by means of an algorithm or a rule. From an epistemolog-
ical perspective, this implies a shift from the realm of clas-
sical to that of constructive theorizing [168]. Clearly, the
act of computationally constructing a coordinated state –
instead of imposing it via the Walrasian auctioneer – for
a decentralized economic system requires complete de-
scription of goal-directed economic agents and their in-
teraction structure.
Agent-based modeling represents an eﬀective imple-
mentation of this research agenda ([60,124], see also [24,
67,81,175]). ABM is a methodology that allows one to
construct, based on simple rules of behavior and interac-
tion, models with heterogeneous agents, where the result-
ing aggregate dynamics and empirical regularities are not
known a priori and are not deducible from individual be-
havior. It is characterized by three main tenets: (i) there
is a multitude of objects that interact with each other and
with the environment; (ii) the objects are autonomous, i. e.
there is no central, or top-down control over their behav-
ior; and (iii) the outcome of their interaction is numeri-
cally computed. Since the objects are autonomous, they are
called agents ([3,4]; see also the repository of ACE-related
material maintained by Leigh Tesfatsion at http://www.
econ.iastate.edu/tesfatsi/ace.htm): “Agent-based Compu-
tational Economics is the computational study of eco-
nomic processes modeled as dynamic systems of interact-
ing agent” [161].
Agents can be anything from cells to biological entities,
from individuals to social groups like families or ﬁrms.
Agents can be composed by other agents: the only require-
ment being that they are perceived as a unit from the out-
side, and that they do something, i. e. they have the ability
to act, and possibly to react to external stimuli and interact
with the environment and other agents. The environment,
which may include physical entities (infrastructures, ge-
ographical locations, etc.) and institutions (markets, reg-
ulatory systems, etc.), can also be modeled in terms of
agents (e. g. a central bank, the order book of a stock ex-
change, etc.), whenever the conditions outlined above are
met. When not, it should be thought of simply as a set of
variables (say, temperature or business conﬁdence).
The methodological issues are the real litmus paper
of the competing approaches. According to one of the
most quoted economic papers, Friedman [71], the ulti-
mate goal of a positive science is to develop hypotheses
that yield valid and meaningful predictions about actual
phenomena. Not a word on predictions at the meso-level
or on the realism of the hypotheses. Even the Occam rule
is systematically ignored: e. g. to get a downward slop-
ing aggregate demand curve, mainstream economics has
to assume indiﬀerence curves which are: (i) deﬁned only
in the positive quadrant of commodity-bundle quantities;
(ii) negatively sloped; (iii) complete; (iv) transitive, and
(v) strictly convex, while ABM has to assume only the ex-
istence of reservation prices. Moreover, to properly aggre-
gate from microbehavior, i. e. to get a well shaped aggre-
gate demand from the individual ones, it has to be as-
sumed that the propensity to consume out of income has
to be homogeneous for all the agents (homothetic Engel
curves) and that distribution is independent from relative
prices. This methodology resembles the scientiﬁc proce-
dure of the aruspexes, who predicted the future by reading
the animals’ bowels. The ABM methodology is bottom-up
and focuses on the interaction between many heteroge-
nous interacting agents, which might produce a statistical
equilibrium, rather than a natural one as the mainstream
approach assumes. The bottom-up approach models in-
dividual behavior according to simple behavioral rules;
agents are allowed to have local interaction and to change

Agent Based Models in Economics and Complexity
A
207
the individual rule (through adaptation) as well as the in-
teraction nodes. By aggregating, some statistical regularity
emerges, which cannot be inferred from individual behav-
ior (self emerging regularities): this emergent behavior feeds
back to the individual level (downward causation) thus es-
tablishing a macrofoundation of micro. As a consequence,
each and every proposition may be falsiﬁed at micro, meso
and macro levels. This approach opposes the axiomatic
theory of economics, where the optimization procedure is
the standard for a scientiﬁc, i. e. not ad-hoc, modeling pro-
cedure.
The agent-based methodology can also be viewed as
a way to reconcile the two opposing philosophical perspec-
tives of methodological individualism and holism. Hav-
ing agents as the unit of analysis, ABM is deeply rooted
in methodological individualism, a philosophical method
aimed at explaining and understanding broad society-
wide developments as the aggregation of decisions by indi-
viduals [13,172]. Methodological individualism suggests –
in its most extreme (and erroneous) version – that a sys-
tem can be understood by analyzing separately its con-
stituents, the reductionist approach that the whole is noth-
ing but the sum of its parts [51,127]. However, the ability
to reduce everything to simple fundamental objects and
laws does not imply the ability to start from those objects
and laws and reconstruct the universe. In other terms, re-
ductionism does not imply constructionism [2].
The Austrian school of economics championed the
use of methodological individualism in economics in the
twentieth century, of which Friederich von Hayek has
been one of the main exponents. The legacy of Hayek to
ABM and the complex system approach has been recog-
nized [166]. Methodological individualism is also consid-
ered an essential part of modern neoclassical economics,
with its analysis of collective action in terms of rational,
utility-maximizing individuals: should the microfounda-
tions in terms of individual rational behavior be aban-
doned, the Lucas Critique [114] would kick in. However,
it is hard to recognize the imprinting of methodological
individualism in the RA paradigm, which claims that the
whole society can be analyzed in terms of the behavior
of a single, representative, individual and forgets to ap-
ply to it the Lucas critique. On the other hand, focusing
on aggregate phenomena arising from the bottom up [61]
from the interaction of many diﬀerent agents, ABM also
adopts a holistic approach when it claims that these phe-
nomena cannot be studied without looking at the entire
context in which they are embedded. Indeed, holism is
the idea that all the properties of a given system cannot
be determined or explained by the sum of its component
parts alone. Instead, the system as a whole determines in
an important way that the parts behave. The general prin-
ciple of holism was concisely summarized by Aristotle in
his Metaphysics: “The whole is more than the sum of its
parts”, a manifesto of the complexity approach. However,
ABM (and more in general complexity theory) should not
be confused with general systems theory, an holistic ap-
proach developed in the 1950s and 1960s that in its most
radical form argued that everything aﬀects everything else:
according to systems theory, phenomena that appear to
have simple causes, such as unemployment, actually have
a variety of complex causes – complex in the sense that
the causes are interrelated, nonlinear, and diﬃcult to de-
termine [133]. Conversely, the complexity approach looks
for simple rules that underpin complexity, an agenda that
has been entirely transferred to ABM.
Also, ABM can be thought of as a bridge be-
tween methodological individualism and methodological
holism. In agent-based models aggregate outcomes (the
whole, e. g. the unemployment rate) are computed as the
sum of individual characteristics (its parts, e. g. individ-
ual employment status). However, aggregate behavior can
often be recognized as distinct from the behavior of the
comprising agents, leading to the discovery of emergent
properties. In this sense, the whole is more than – and dif-
ferent from – the sum of its parts. It might even be the
case that the whole appears to act as if it followed a dis-
tinct logic, with its own goals and means, as in the example
of a cartel of ﬁrms that act in order to inﬂuence the mar-
ket price of a good. From the outside, the whole appears
no diﬀerent from a new agent type (e. g. a family, a ﬁrm).
A new entity is born; the computational experiment has
been successful in growing artiﬁcial societies from the bot-
tom up [61].
This bottom-up approach to complexity consists in de-
ducing the macroscopic objects (macros) and their phe-
nomenological complex ad-hoc laws in terms of a multi-
tude of elementary microscopic objects (micros) interact-
ing by simple fundamental laws [154], and ABM provides
a technique that allows one to systematically follow the
birth of these complex macroscopic phenomenology. The
macros at a speciﬁc scale can become the micros at the next
scale.
Depending on the scope of the analysis, it is generally
convenient to stop at some scale in the way down to recon-
struct aggregate, top-level dynamics from the bottom up.
When applied to economics, only a few levels (e. g. a micro,
a meso and a macro level) are in general suﬃcient to pro-
vide a thorough understanding of the system. Deﬁning the
elementary units of analysis amounts to ﬁxing the limits
for the reductionist approach, which is not aprioristically
discarded but rather integrated in the analysis. These units

208 A
Agent Based Models in Economics and Complexity
are in fact characterized by an inner structure that does not
depend on the environment in which they are embedded.
They can thus be analyzed separately.
The need for the ABM approach at any given scale
is often linked to the existence of some underlying auto-
catalytic process at a lower level. Autocatalytic processes
are dynamic processes with positive feedbacks, where the
growth of some quantity is to some extent self-perpetuat-
ing, as in the case when it is proportional to its initial value.
The importance of positive feedbacks has been recognized
in the literature on increasing returns, in particular with
respect to the possibility of multiple equilibria [151], since
the time of Marshall. However, the traditional analysis is
static, and does not address how an equilibrium out of
several might be selected. Looking at the problem from
a dynamic stochastic process perspective, selection is ex-
plained in terms of one set of small historical events mag-
niﬁed by increasing returns.
Moreover, the existence of an autocatalytic process im-
plies that looking at the average, or most probable, behav-
ior of the constituent units is non representative of the dy-
namics of the system: autocatalyticity insures that the be-
havior of the entire system is dominated by the elements
with the highest auto-catalytic growth rate rather than by
the typical or average element [154]. In presence of auto-
catalytic processes, even a small amount of individual het-
erogeneity invalidates any description of the behavior of
the system in terms of its average element: the real world is
controlled as much by the tails of distributions as by means
or averages. We need to free ourselves from average think-
ing [3].
The fact that autocatalytic dynamics are scale invari-
ant (i. e. after a transformation that multiplies all the vari-
ables by a common factor) is a key to understanding the
emergence of scale invariant distributions of these vari-
ables (e. g. power laws), at an aggregate level. The relevance
of scale free distributions in economics (e. g. of ﬁrm size,
wealth, income, etc.) is now extensively recognized (Brock,
1999), and has been the subject of through investigation in
the econophysics literature [120].
Additional Features of Agent-Based Models
We have so far introduced the three fundamental char-
acteristics of ABM: there are agents that play the role
of actors, there is no script or Deus ex-machina and the
story is played live, i. e. it is computed. Following Ep-
stein [58,59,60], we can further characterize the method-
ology, by enumerating a number of features that, although
not necessary to deﬁne an agent-based model, are often
present. These are:
Heterogeneity
While in analytical models there is a big advantage in re-
ducing the ways in which individuals diﬀer, the computa-
tional burden of ABM does not change at all if diﬀerent
values of the parameters (e. g. preferences, endowments,
location, social contacts, abilities etc.) are speciﬁed for dif-
ferent individuals. Normally, a distribution for each rel-
evant parameter is chosen, and this simply implies that
a few parameters (those governing the distribution) are
added to the model.
Explicit Space
This can be seen as speciﬁcation of the previous point: in-
dividuals often diﬀer in the physical place where they are
located, and/or in the neighbors with whom they can or
have to interact (which deﬁne the network structure of the
model, see below).
Local Interaction
Again, this can be seen as a speciﬁcation of the network
structure connecting the agents. Analytical models often
assume either global interaction (as in Walrasian markets),
or very simple local interaction. ABM allow for much
richer speciﬁcations. No direct interaction (only through
prices) is allowed in the GE, while direct interaction (local
and stochastic, usually [101]) is the rule for the complexity
approach: ﬁgures 1a-c give a graphical representation of
Walrasian, random and scale-free interaction respectively.
Note that the empirical evidence supports the third case:
hubs and power laws are the rule in the real world [38,52].
Actually, some neoclassical economists asked for an
analysis of how social relations aﬀect the allocation of re-
sources (e. g., [12,107,134]). They went almost completely
unheard, however, until the upsurge in the early 1990s of
a brand new body of work aimed at understanding and
modeling the social context of economic decisions, usu-
ally labeled new social economics or social interaction eco-
nomics [56]. Models of social interactions (Manski [118]
oﬀers an operational classiﬁcation of the channels through
which the actions of one agent may aﬀect those of other
agents within a reference group) are generally able to pro-
duce several properties, such as multiple equilibria [34];
non-ergodicity and phase transition [54]; equilibrium strat-
iﬁcation in social and/or spatial dimension [27,83]; the ex-
istence of a social multiplier of behaviors [84]. The key
idea consists in recognizing that the social relationships
in which individual economic agents are embedded can
have a large impact on economic decisions. In fact, the
social context impacts on individual economic decisions

Agent Based Models in Economics and Complexity
A
209
Agent Based Models in Economics and Complexity, Figure 1
a a Walrasian GE representation; b a random graph; c a scale free graph in which several hubs can be identified
through several mechanisms. First, social norms, cultural
processes and economic institutions may inﬂuence mo-
tivations, values, and tastes and, ultimately, make pref-
erences endogenous [31]. Second, even if we admit that
individuals are endowed with exogenously given prefer-
ences, the pervasiveness of information asymmetries in
real-world economies implies that economic agents vol-
untarily share values, notions of acceptable behavior and
socially based enforcement mechanisms in order to re-
duce uncertainty and favor coordination [50]. Third, the
welfare of individuals may depend on some social charac-
teristics like honor, popularity, stigma or status [41]. Fi-
nally, interactions not mediated by enforceable contracts
may occur because of pure technological externalities in
network industries [152] or indirect eﬀects transmitted
through prices (pecuniary externalities) in non-competi-
tive markets [28], which may lead to coordination failures
due to strategic complementarities [42].
Bounded Rationality
Interestingly, while in analytical models it is generally eas-
ier to implement some form of optimal behavior rather
than solving models where individuals follow “reasonable”
rules of thumb, or learn either by looking at what hap-
pened to others or what happened to them in the past,
for ABM the opposite is true. However, it can be argued
that real individuals also face the same diﬃculties in de-
termining and following the optimal behavior, and are
characterized by some sort of bounded rationality: “There
are two components of this: bounded information and
bounded computing power. Agents have neither global in-
formation nor inﬁnite computational capacity. Although
they are typically purposive, they are not global optimizers;
they use simple rules based on local information” (p. 1588
in [59]).
The requirement on full rationality is indeed very
strong, since it requires an inﬁnite computational capac-
ity (the ability of processing tons of data in a inﬁnitesimal
amount of time) and all the information. Moreover, ac-
cording to the mainstream approach, information is com-
plete and free for all the agents. Note that one of the as-
sumptions in the Walrasian approach is that each agent
has only private information: this is equivalent to say that
strategic behavior about information collection and dis-
semination is ruled out and the collection of the whole
set of the information is left to the market via the auc-
tioneer (or a benevolent dictator [25]). Indeed, one could
read the rational expectation “revolution” as the tentative
to decentralize the price setting procedure by defenestrat-
ing the auctioneer. Limited information is taken into ac-
count, but the constraints have to aﬀect every agent in the
same way (the so-called Lucas’ islands hypothesis) and the
Greenwald–Stiglitz theorem [86] states that in this case
the equilibrium is not even Pareto-constrained. If infor-
mation is asymmetric or private, agents have to be hetero-
geneous and direct interaction has to be considered: this
destroys the mainstream model and generates coordina-
tion failures.
On the contrary, agent-based models are build upon
the hypothesis that agents have limited information. Once
again, the ABM approach is much more parsimonious,
since it only requires that the agents do not commit sys-
tematic errors. Moreover, given the limited information
setting, the economic environment might change aﬀect-
ing, and being aﬀected by, agents’ behavior: individuals
learn through experience and by interacting with other
agents.
Non-equilibrium Dynamics
As we will explain in more details below, ABM are recur-
sive models, in which the state of the system at time t C 1
is computed starting from the state at time t. Hence,
they allow the investigation of what happens all along the
route, not only at the start and at the end of the jour-
ney. This point is, we believe, the most important. Brian
Arthur (p. 1552 in [16]) oﬀers an eﬀective statement of

210 A
Agent Based Models in Economics and Complexity
its relevance for economic theory: “Standard neoclassi-
cal economics asks what agents’ actions, strategies, or ex-
pectations are in equilibrium with (consistent with) the
outcome or pattern these behaviors aggregatively create.
Agent-based computational economics enables us to ask
a wider question: how agents’ actions, strategies or expec-
tations might react to – might endogenously change with –
the pattern they create. [...] This out-of-equilibrium ap-
proach is not a minor adjunct to standard economic the-
ory; it is economics done in a more general way. [...]
The static equilibrium approach suﬀers two characteris-
tic indeterminacies: it cannot easily resolve among multi-
ple equilibria; nor can it easily model individuals’ choices
of expectations. Both problems are ones of formation (of
an equilibrium and of an ‘ecology’ of expectations, respec-
tively), and when analyzed in formation – that is, out of
equilibrium – these anomalies disappear”.
As we have seen, continuous market clearing is as-
sumed by the mainstream. It is a necessary condition to
obtain “eﬃciency and optimality” and it is quite curious
to read of a theory assuming the explenandum. In such
a way, every out of equilibrium dynamics or path depen-
dency is ruled out and initial conditions do not matter.
The GE model assumes that transactions happen only af-
ter the vector of the equilibrium prices has been reached:
instead of being the result of the exchange, it foresees it
par tatonnement in a logical, ﬁctitious time. Because the
real markets operate in real, historical, time and the ex-
change process determines prices, the GE model is not able
to describe any real economy [9]. Clower [39] suggested
(resemblance Edgeworth, [57]) that exchange might hap-
pen out of equilibrium (at false prices). In such a case,
agents will be quantity-rationed in their supply of-demand
for: because of it, the intertemporal maximization prob-
lem has to be quantity-constraints (the so-called Clower
constraint) and if the economy would reach equilibrium,
it will be non-optimal and ineﬃcient.
The requirement on rationality is also very strong,
since it requires an inﬁnite computational capacity (the
ability of processing tons of data in a inﬁnitesimal amount
of time) and all the information. In fact, if information is
limited, the outcome of a rational choice may be non-opti-
mal. Once again, all the ABM approach is much more par-
simonious, since it requires that the agents do not com-
mit systematic errors. Moreover, given the limited infor-
mation setting, the economic environment might change
aﬀecting, and being aﬀected by, agents’ behavior: learning
and adaptive behavior are therefore contemplated.
Finally, according to Beinhocker [26], the approaches
diﬀer also as regard dynamics (Complex Systems are open,
dynamic, non-linear systems, far from equilibrium; Main-
stream economics are closed, static, linear systems in equi-
librium) and evolution (Complex Systems have an evo-
lutionary process of diﬀerentiation, selection and am-
pliﬁcation which provides the system with novelty and is
responsible for its growth in order and complexity, while
Mainstream has no mechanism for endogenously creating
novelty, or growth in order and complexity.
An Ante Litteram Agent-Based Model:
Thomas Schelling’s Segregation Model
One of the early and most well known examples of an
agent-based model is the segregation model proposed by
Thomas Schelling [145,146], who in 2005 received the
Nobel prize for his studies in game theory (surveys of
more recent applications of ABM to economics can be
found in [159,160,161,163]). To correctly assess the im-
portance of the model, it must be evaluated against the
social and historical background of the time. Up to the
end of the 1960s racial segregation was institutionalized in
the United States. Racial laws required that public schools,
public places and public transportation, like trains and
buses, had separate facilities for whites and blacks. Resi-
dential segregation was also prescribed in some States, al-
though it is now widely recognized that it mainly came
about through organized, mostly private eﬀorts to ghet-
toize blacks in the early twentieth century – particularly
the years between the world wars [63,126]. But if the so-
cial attitude was the strongest force in producing residen-
tial segregation, the Civil Rights movement of the 1960s
greatly contributed to a change of climate, with the white
population exhibiting increasing levels of tolerance. Even-
tually, the movement gained such strength to achieve its
main objective, the abolition of the racial laws: this was
sealed in the Civil Rights Act of 1968 which, among many
other things, outlawed a wide range of discriminatory con-
duct in housing markets. Hence, both the general pub-
lic attitude and the law changed dramatically during the
1960s. As a consequence, many observers predicted a rapid
decline in housing segregation. The decline, however, was
almost imperceptible. The question then was why this hap-
pened. Schelling’s segregation model brought an answer,
suggesting that small diﬀerences in tolerance level or ini-
tial location could trigger high level of segregation even
without formal (i. e. legal) constraints, and even for de-
cent levels of overall tolerance. In the model, whites and
blacks are (randomly) located over a grid, each individual
occupying one cell. As a consequence, each individual has
at most eight neighbors (Moore neighborhood), located
on adjacent cells. Preferences over residential patterns are
represented as the maximum quota of racially diﬀerent

Agent Based Models in Economics and Complexity
A
211
Agent Based Models in Economics and Complexity, Figure 2
NETLOGO implementation of Schelling’s segregation model. a Initial (random) pattern. The average shareof racially similar neighbors
is roughly50%. With a tolerancelevelof70% (40%), less than 20% (morethan 80%) oftheindividuals arenotsatisfied. b Finalpattern.
The average share of racially similar neighbors is 72.1%. Everyone is satisfied. c Final pattern. The average share of racially similar
neighbors is 99.7%. Everyone is satisfied
neighbors that an individual tolerates. For simplicity, we
can assume that preferences are identical: a unique num-
ber deﬁnes the level of tolerance in the population. For ex-
ample, if the tolerance level is 50% and an individual has
only ﬁve neighbors, he would be satisﬁed if no more than
two of his neighbors are racially diﬀerent. If an individual
is not satisﬁed by his current location, he tries to move to
a diﬀerent location where he is satisﬁed.
The mechanism that generates segregation is the fol-
lowing. Since individuals are initially located randomly on
the grid, by chance there will be someone who is not satis-
ﬁed. His decision to move creates two externalities: one in
the location of origin and the other in the location of des-
tination. For example, suppose a white individual decides
to move because there are too many black people around.
As he leaves, the ethnic composition of his neighborhood
is aﬀected (there is one white less). This increases the pos-
sibility that another white individual, who was previously
satisﬁed, becomes eager to move. A similar situation oc-
curs in the area of destination. The arrival of a white indi-
vidual aﬀects the ethnic composition of the neighborhood,
possibly causing some black individual to become unsatis-
ﬁed. Thus, a small non-homogeneity in the initial residen-
tial pattern triggers a chain eﬀect that eventually leads to
high levels of segregation. This mechanism is reinforced
when preferences are not homogeneous in the population.
Figure 2, which shows the NETLOGO implementation
of the Schelling model, exempliﬁes [173]. The left panel
depicts the initial residential pattern, for a population of
2000 individuals, evenly divided between green and red,
living on a 51  51 cells torus (hence the population den-
sity is 76.9%). Two values for the tolerance threshold are
tested: in the ﬁrst conﬁguration, tolerance is extremely
high (70%), while in the second it is signiﬁcantly lower
(30%), although at a level that would still be considered
decent by many commentators. The initial residential pat-
tern (obviously) shows no levels of segregation: every in-
dividual has on average 50% of neighbors of a diﬀerent
race. However, after just a few periods the equilibrium
conﬁgurations of the middle (for a tolerance level of 70%)
and right (for tolerance level of 30%) panels are obtained.
The level of segregation is high: more than three quar-
ters of neighbors are on average of the same racial group,
even in case (b), when individuals are actually happy to
live in a neighborhood dominated by a diﬀerent racial
group! Moreover, most people live in perfectly homoge-
neous clusters, with diﬀerent ethnic clusters being often
physically separated from each other by a no man’s land.
Only the relative mix brought by conﬁning clusters keeps
down the measure of overall segregation. Should the over-
all composition of the population be biased in favor of one
ethnic group, we would clearly recognize the formation of
ghettoes.
Note that the formation of racially homogeneous eth-
nic clusters and ghettoes is an emergent property of the
system, which could hardly be deduced by looking at in-
dividual behavior alone, without considering the eﬀects
of interaction. Moreover, the clusters themselves could be
considered as the elementary unit of analysis at a diﬀer-
ent, more aggregate level, and their behavior, e. g. whether
they shrink, expand, merge or vanish, studied with respect
to some exogenous changes in the environment. Not only
a property, i. e. a statistical regularity, has emerged, but
also a whole new entity can be recognized. However, this

212 A
Agent Based Models in Economics and Complexity
new entity is nothing else but a subjective interpretation
by some external observer of an emergent property of the
system.
The Development of Agent-Based Modeling
The early example of the segregation model notwithstand-
ing, the development of agent-based computational eco-
nomics is closely linked with the work conducted at the
Santa Fe Institute for the study of complexity, a private,
non-proﬁt, independent research and education center
founded in 1984 in Santa Fe, New Mexico. The purpose of
the institute has been, since its foundation, to foster multi-
disciplinary collaboration in pursuit of understanding the
common themes that arise in natural, artiﬁcial, and social
systems. This uniﬁed view is the dominant theme of what
has been called the new science of complexity.
The outcomes of this research program are well de-
picted in three books, all bearing the title The economy as
an evolving complex system [4,17,29]. The following quo-
tation, from the preface of the 1997 volume, summarizes
very accurately the approach:
“In September 1987 twenty people came together at
the Santa Fe Institute to talk about ‘the economy as
a evolving, complex system’. Ten were theoretical
economists, invited by Kenneth J. Arrow, and ten
were physicists, biologists and computer scientists,
invited by Philip W. Anderson. The meeting was
motivated by the hope that new ideas bubbling in
the natural sciences, loosely tied together under the
rubric of ‘the sciences of complexity’, might stim-
ulate new ways of thinking about economic prob-
lems. For ten days, economists and natural scien-
tists took turns talking about their respective worlds
and methodologies. While physicists grappled with
general equilibrium analysis and non-cooperative
game theory, economists tried to make sense of spin
glass models, Boolean networks, and genetic algo-
rithms. The meeting left two legacies. The ﬁrst was
the 1988 volume of essays; the other was the found-
ing, in 1988, of the Economics Program at the Santa
Fe Institute, the Institute’s ﬁrst resident research
program. The Program’s mission was to encourage
the understanding of economic phenomena from
a complexity perspective, which involved the devel-
opment of theory as well as tools for modeling and
for empirical analysis. [...] But just what is the com-
plexity perspective in economics? That is not an easy
question to answer. [...] Looking back over the de-
velopments in the past decade, and of the papers
produced by the program, we believe that a coher-
ent perspective – sometimes called the ‘Santa Fe ap-
proach’ – has emerged within economics.”
The work carried out at the Santa Fe Institute greatly
contributed to popularize the complexity approach to
economics, although a similar line of research was ini-
tiated in Europe by chemists and physicists concerned
with emergent structures and disequilibrium dynamics
(more precisely, in Brussels by the group of the Nobel
prize winner physical chemist Ilya Prigogine ([128]) and
in Stuttgart by the group of the theoretical physicist Her-
mann Haken [91], as discussed in length by Rosser [141]).
Two main reasons can help explaining why the Santa
Fe approach gained some visibility outside the restricted
group of people interested in the complexity theory
(perhaps contributing in this way to mount what Hor-
gan [96,97], called an intellectual fad). Together, they of-
fered an appealing suggestion of both what to do and how
to do it. The ﬁrst reason was the ability to present the com-
plexity paradigm as a unitary perspective. This unitary vi-
sion stressed in particular the existence of feedbacks be-
tween functionalities and objectives: individual objectives
determine to some extent the use and modiﬁcation of ex-
isting functionalities, but functionalities direct to some ex-
tent the choice of individual objectives. It is this analytical
focus that proved to be valuable in disciplines as diverse
as the social sciences, the biological sciences and even ar-
chitecture [70]. The second reason has to do with the cre-
ation of a speciﬁc simulation platform that allowed rel-
atively inexperienced researchers to build their own toy
models that, thanks to the enormous and sustained in-
crease in commonly available computing power, could run
quickly even on small PCs. This simulation platform was
called SWARM [18], and consisted in a series of libraries
that implemented many of the functionalities and techni-
calities needed to build an agent-based simulation, e. g. the
schedule of the events, the passing of time and graphical
widgets to monitor the simulation. In addition to oﬀer-
ing a practical tool to write agent-based simulations, the
SWARM approach proposed a protocol in simulation de-
sign, which the SWARM libraries exempliﬁed.
The principles at the basis of the SWARM protocol are:
(i)
The use of an object-oriented programming language
(SWARM was ﬁrst written in OBJECTIVE C, and later
translated into JAVA), with diﬀerent objects (and ob-
ject types) being a natural counterpart for diﬀerent
agents (and agent types);
(ii) A separate implementation of the model and the tools
used for monitoring and conducting experiments on
the model (the so-called observer);

Agent Based Models in Economics and Complexity
A
213
(iii) An architecture that allows nesting models one into
another, in order to build a hierarchy of swarms –
a swarm being a group of objects and a schedule of
actions that the objects execute. One swarm can thus
contain lower-level swarms whose schedules are inte-
grated into the higher-level schedule.
A number of diﬀerent simulation platforms that ad-
hered to the SWARM protocol for simulation design have
been proposed since, the most widespread being REPAST
([129]; see also [135]). However, other alternative ap-
proaches to writing agent-based models exist. Some rely
on general-purpose mathematical software, like MATHE-
MATICA, MATLAB or MATCAD. Others, exempliﬁed by
the STARLOGO/NETLOGO experience [137], are based on
the idea of an agent-based speciﬁc language.
Finally, despite the fact that ABM are most often com-
puter models, and that the methodology could not de-
velop in the absence of cheap and easy-to-handle per-
sonal computers, it is beneﬁcial to remember that one of
the most well-known agent-based models, the segregation
model we have already described, abstracted altogether
from the use of computers. As Schelling recalls, he had
the original idea while seated on plane, and investigated
it with paper and pencil. When he arrived home, he ex-
plained the rules of the game to his son and got him to
move zincs and coppers from the child’s own collection
on a checkerboard, looking for the results: The dynamics
were suﬃciently intriguing to keep my twelve-year-old en-
gaged p. 1643 in [148].
A Recursive System Representation
of Agent-Based Models
Although the complexity theory is, above all, a mathemat-
ical concept, a rather common misunderstanding about
agent-based simulations is that they are not as sound as
mathematical models. In an often-quoted article, Thomas
Ostrom [130] argued that computer simulation is a third
symbol system in its own right, aside verbal description
and mathematics: simulation is no mathematics at all
(see [79]). An intermediate level of abstraction, according
to this view, characterizes computer simulations: they are
more abstract than verbal descriptions, but less abstract
than pure mathematics. Ostrom (p. 384 in [130]) also ar-
gued that any theory that can be expressed in either of
the ﬁrst two symbol systems can also be expressed in the
third symbol system. This implies that there might be ver-
bal theories, which cannot be adequately expressed in the
second symbol system of mathematics, but can be in the
third [79].
This view has become increasingly popular among so-
cial simulators themselves, apparently because it oﬀers
a shield to the perplexity of the mathematicians, while
hinting at a sort of superiority of computer simulations.
Our opinion is that both statements are simply and plainly
wrong. Agent-based modeling – and more in general sim-
ulation – is mathematics, as we argue in this paragraph.
Moreover, the conjecture that any theory can be expressed
via simulation is easily contradicted: think for instance of
simulating Hegel’s philosophical system.
Actually, agent-based simulations are nothing else but
recursive systems [59,110], where the variables s that de-
scribe at time t the state of each individual unit are deter-
mined, possibly in a stochastic way, as a function of the
past states s and some parameters a:
si;t D fi(si;t1; si;t1; ai; ai ; t)
(1)
The individual state variables could include the memory of
past values, as in the case when an unemployed person is
characterized not only by the fact that he is unemployed,
but also by when he last had a job. The function f i and
the parameters ai determine individual behavior. They can
possibly change over time, either in a random way or de-
pending on some lagged variable or on higher-order pa-
rameters (as in the Environment-Rule-Agent framework of
Gilbert and Terna [80]); when this is the case, their ex-
pression can simply be substituted for in Eq. (1). Equa-
tion (1) allows the recursive computation of the system: at
any moment in time the state of each unit can be expressed
as a (possibly stochastic) function of the initial values X0
only, where X0 includes the initial states and parameters
of all the individual units:
si;t D gi(X0; t)
(2)
The aggregate state of the system is simply deﬁned as
St D
X
i
si;t
(3)
Equilibrium in this system is described as a situation where
the aggregate state S, or some other aggregate statistics Y
computed on the individual states or the individual pa-
rameters are stationary.
Notice that this formalization describes both tradi-
tional dynamic micro models and agent-based simula-
tions. In principle an agent-based model, not diﬀerently
from traditional dynamic micro models, could be solved
analytically. The problem is that the expressions involved
quickly become unbearable, as (i) the level of hetero-
geneity, as measured by the distribution of the parame-

214 A
Agent Based Models in Economics and Complexity
ters ai and functional forms f i, increases; (ii) the amount
of interaction, as measured by the dependency of si;t
on si;t1, increases; (iii) the functional forms f become
more complicated, e. g. with the introduction of if-else
conditions, etc.
Hence, the resort to numerical simulation. Traditional
analytical models on the other hand must take great care
that the system can be solved analytically, i. e. by sym-
bolic manipulation. Hence the use of simple functions as
the omnipresent Cobb–Douglas, the assumption of homo-
geneous units (that can then be replaced by a RA), the
choice of simple interaction processes, often mediated by
a centralized coordination mechanism. However, analyti-
cal tractability alone is a poor justiﬁcation of any modeling
choice. As the Nobel laureate Harry Markowitz wrote, “if
we restrict ourselves to models which can be solved an-
alytically, we will be modeling for our mutual entertain-
ment, not to maximize explanatory or predictive power”
(as reported in [112]). Restricting to analytically solvable
modls – as they are called in the not suﬃciently well-
known paper by Axel Leijonhufvud [108] – looks danger-
ously close to the tale of the man who was searching for
his keys under the light of a street lamp at night and, once
asked if he had lost them there, he answered “No, but this
is where the light is”.
Analysis of Model Behavior
Being able to reach a close solution means that it is pos-
sible to connect inputs and outputs of the model, at any
point in time, in a clear way: the input-output transforma-
tion function, or reduced form, implied by the structural
form in which the model is expressed, is analytically ob-
tained (e. g. the equilibrium expression of some aggregate
variable of interest, as a function of the model parameters).
Hence, theorems can be proved and laws expressed.
On the contrary, in a simulation model the reduced
form remains unknown, and only inductive evidence
about the input/output transformation implied by the
model can be collected. Performing multiple runs of the
simulation with diﬀerent parameters does this. In other
words, simulations suﬀer from the problem of stating gen-
eral propositions about the dynamics of the model start-
ing only from point observations. Since scientiﬁc expla-
nations are generally deﬁned as the derivation of general
laws, which are able to replicate the phenomena of inter-
ests [93,94], simulations appear to be less scientiﬁc than
analytical models. As Axelrod [19] points out, “like deduc-
tion, [a simulation] starts with a set of explicit assump-
tions. But unlike deduction, it does not prove theorems.
Instead, a simulation generates data that can be analyzed
inductively”. Induction comes at the moment of explain-
ing the behavior of the model. It should be noted that al-
though induction is used to obtain knowledge about the
behavior of a given simulation model, the use of a sim-
ulation model to obtain knowledge about the behavior
of the real world refers to the logical process of abduc-
tion [109,117]. Abduction [66,132], also called inference to
the best explanation, is a method of reasoning in which one
looks for the hypothesis that would best explain the rele-
vant evidence, as in the case when the observation that the
grass is wet allows one to suppose that it rained.
Being constrained to unveil the underlying input-out-
put transformation function by repetitively sampling the
parameter space, simulations cannot prove necessity, i. e.
they cannot provide in the traditional sense necessary con-
ditions for any behavior to hold. This is because nothing
excludes a priori that the system will behave in a radically
diﬀerent way as soon as the value of some parameter is
changed, while it is generally not possible to sample all val-
ues of the parameter space. In other words, the artiﬁcial
data may not be representative of all outcomes the model
can produce. While analytical results are conditional on
the speciﬁc hypothesis made about the model only, simu-
lation results are conditional both on the speciﬁc hypoth-
esis of the model and the speciﬁc values of the parame-
ters used in the simulation runs: each run of such a model
yields is a suﬃciency theorem, [yet] a single run does not
provide any information on the robustness of such theo-
rems [20].
The sampling problem becomes increasingly harder
as the number of the parameters increase. This has been
referred to as the curse of dimensionality [143]. To eval-
uate its implications, two arguments should be consid-
ered. The ﬁrst one is theoretical: if the impossibility to
gain a full knowledge of the system applies to the artiﬁ-
cial world deﬁned by the simulation model, it also applies
to the real world. The real data generating process being
itself unknown, stylized facts (against which all models
are in general evaluated) could in principle turn wrong,
at some point in time. From an epistemological point of
view, our belief that the sun will rise tomorrow remains
a probabilistic assessment. The second, and more decisive,
consideration is empirical: we should not worry too much
about the behavior of a model for particular evil combina-
tions of the parameters, as long as these combinations re-
main extremely rare (one relevant exception is when rare
events are the focus of the investigation, e. g. in risk man-
agement, see [150]). If the design of the experiments is
suﬃciently accurate, the problem of how imprecise is the
estimated input-output transformation function becomes
marginal:

Agent Based Models in Economics and Complexity
A
215
Agent Based Models in Economics and Complexity, Figure 3
Sensitivity analysis for the Schelling’s segregation model. Segregation is measured as the share of racially similar neighbors. The
reference parameter configuration is population size D 2000, tolerance level D 40%
While the curse of dimensionality places a practical
upper bound on the size of the parameter space that
can be checked for robustness, it is also the case that
vast performance increases in computer hardware
are rapidly converting what was once perhaps a fatal
diﬃculty into a manageable one [20].
In conclusion, extensive experimentation is the only
way to get a full understanding of the simulation behav-
ior. Sampling of the parameter space can be done ei-
ther systematically, i. e. by grid exploration, or randomly.
Following Leombruni et al. [111], we can further distin-
guish between two levels at which sampling can be done:
a global level and a local level. Local sampling is conducted
around some speciﬁc parameter conﬁgurations of interest,
by letting each parameter vary and keeping all the oth-
ers unchanged. This is known as sensitivity analysis, and
is the equivalent to the study of the partial derivatives of
the input-output transformation function in an analytical
model.
As an example, Fig. 3 reports a plot of the equilibrium
level of segregation in the Schelling model, for decreas-
ing values of tolerance (left panel) and increasing popu-
lation density (right panel). Tolerance level is sampled in
the range [0, .7] by increasing steps of .05, while popula-
tion size is sampled in the range [1000, 2000] by increasing
steps of 100. To get rid of random eﬀects (in the initial res-
idential pattern and in the choice of a diﬀerent location of
unsatisﬁed individuals), 100 runs are performed for every
value of the parameter being changed, and average out-
comes are reported. This gives an idea of the local eﬀects
of the two parameters around the central parameter con-
ﬁguration where the population size is equal to 2000 and
the tolerance level is equal to 70%.
For what concerns the eﬀect of tolerance on segrega-
tion (left panel), it should be noted that the somewhat ir-
regular shape of the relationship is a consequence not of
the sample size but of the small neighborhood individu-
als take into consideration (a maximum of eight adjacent
cells, as we have seen), and the discretization it brings. As
the eﬀect of population size on segregation (right panel) is
concerned, it may seem at a ﬁrst glance counter-intuitive
that segregation initially diminishes, as the population
density increases. This is due to the fact that clusters can
separate more if there are more free locations. Of course,
nothing excludes the possibility that these marginal eﬀects
are completely diﬀerent around a diﬀerent parameter con-
ﬁguration. To check whether this is the case, it is necessary
either to repeat the sensitivity analysis around other con-
ﬁgurations, or to adopt a multivariate perspective.
Allowing all parameters to change performs global
sampling, thus removing the reference to any particular
conﬁguration. To interpret the results of such a global
analysis, a relationship between inputs and outputs in the
artiﬁcial data can be estimated, e. g.:
Y D m(X0) :
(4)
Where Y is the statistics of interest (say, the Gini coeﬃ-
cient of wealth), computed in equilibrium, i. e. when it has
reached stationarity, and X0 contains the initial conditions
and the structural parameters of the model: X0 D fs0; ag.
If the (not necessary unique) steady state is independent of
the initial conditions, Eq. 4 simpliﬁes to:
Y D m(A) :
(5)
Where A contains only the parameters of the simulation.
The choice of the functional form m to be estimated, which

216 A
Agent Based Models in Economics and Complexity
Agent Based Models in Economics and Complexity, Table 1
Regression results for Schelling’s segregation model. Instead of repeating the experiment n times for each parameter configuration,
in order to average out the random effects of the model, we preferred to test a number of different parameter configurations n times
higher. Thus, population size is explored in the range [1000, 2000] by increasing steps of 10, and tolerance level is explored in the
range [0, 7] by increasing steps of .05
Source
SS
df
MS
Model
666719.502
6 111119.917
Residual
12033.9282 2108
5.70869461
Total
678753.43
2114
321.075416
Number of obs = 2115
F(6, 2108) = 19465.03
Prob > F = 0.0000
R-squared = 0.9823
Adj R-squared = 0.9822
Root MSE = 2.3893
Segregation Coef.
Std. Err.
t
P > jtj [95% Conf. Interval]
tolerance
3.379668
.0819347
41.25
0.000
3.218987
3.54035
tolerance_2
 .0655574 .0013175
 49.76 0.000
 .0681411  .0629737
tolerance_3
.0003292
6.73e 06 48.94
0.000
.000316
.0003424
density
 23.83033 3.274691
 7.28
0.000
 30.25229  17.40837
density_2
20.05102
2.372174
8.45
0.000
15.39897
24.70306
interaction
 .1745321 .0153685
 11.36 0.000
 .2046712  .144393
_cons
57.31189
1.957341
29.28
0.000
53.47336
61.15041
is sometimes referred to as metamodel [103] is to a certain
extent arbitrary, and should be guided by the usual criteria
for model speciﬁcation for the analysis of real data.
As an example, we performed a multivariate analysis
on the artiﬁcial data coming out of the Schelling’s segre-
gation model, by letting both the population size and the
tolerance threshold to vary. Overall, 2115 parameter con-
ﬁgurations are tested. After some data mining, our pre-
ferred speciﬁcation is an OLS regression of the segregation
level on a third order polynomial of the tolerance thresh-
old, a second order polynomial of population density, plus
an interaction term given by the product of tolerance and
density. The interaction term, that turns out to be highly
signiﬁcant, implies that the local analysis of Fig. 3 has no
general validity.
The regression outcome is reported in Table 1.
Such a model allows predicting the resulting segrega-
tion level for any value of the parameters. Of course, as the
complexity of the model increases (e. g. leading to multiple
equilibria) ﬁnding an appropriate meta-model becomes
increasingly arduous.
Finally, let’s remark that the curse of dimensionality
strongly suggests that the ﬂexibility in model speciﬁcation
characterizing agent-based models is to be used with care,
never neglecting the KISS (Keep it simple, Stupid) prin-
ciple. Schelling’s segregation model is in this respect an
example of simplicity, since it has but a few parameters:
this is not incoherent with the complexity approach, since
it stresses how simple behavioral rules can generate very
complex dynamics.
Validation and Estimation
The previous section has dealt with the problem of inter-
preting the behavior of an agent-based model, and we have
seen that this can be done by appropriately generating and
analyzing artiﬁcial data. We now turn to the relationship
between artiﬁcial and real data, that is (i) the problem of
choosing the parameter values in order to have the be-
havior of the model being as close as possible to the real
data, and (ii) the decision whether a model is good enough,
which often entails a judgment on “how close” as close as
possible is. The ﬁrst issue is referred to as the problem of
calibration or estimation of the model, while the second
one is known as validation.
Note that all models have to be understood. Thus, for
agent-based models analysis of the artiﬁcial data is always
an issue. However, not all models have to be estimated or
validated. Some models are built with a theoretical focus
(e. g. Akerlof’s market for lemons), and thus comparison
with the real data is not an issue – although it could be ar-
gued that some sort of evaluation is still needed, although
of a diﬀerent kind.
Estimation
Although the terms calibration and estimation are some-
times given slightly diﬀerent meanings (e. g. [105]), we
agree with Hansen and Heckman (p. 91 in [92]) that “the
distinction drawn between calibrating and estimating the
parameters of a model is artiﬁcial at best. Moreover, the
justiﬁcation for what is called calibration is vague and con-

Agent Based Models in Economics and Complexity
A
217
fusing. In a profession that is already too segmented, the
construction of such artiﬁcial distinctions is counterpro-
ductive.”
Our understanding is that, too often, calibration sim-
ply refers to a sort of rough estimation, e. g. by means of
visual comparison of the artiﬁcial and real data. However,
not all parameters ought to be estimated by means of for-
mal statistical methods. Some of them have very natural
real counterparts and their value is known (e. g. the inter-
est rate): the simulation is run with empirical data. Un-
known parameters have on the other had to be properly
estimated.
In analytical models the reduced form coeﬃcients, e. g.
the coeﬃcients linking output variables to inputs, can be
estimated in the real data. If the model is identiﬁed, there
is a one-to-one relationship between the structural and the
reduced form coeﬃcients. Thus, estimates for the struc-
tural coeﬃcients can be recovered. In a simulation model
this can’t be done. However, we could compare the out-
come of the simulation with the real data, and change the
structural coeﬃcient values until the distance between the
simulation output and the real data is minimized. This is
called indirect inference [85], and is also applied to analyt-
ical models e. g. when it is not possible to write down the
likelihood. There are many ways to compare real and arti-
ﬁcial data. For instance, simple statistics can be computed
both in real and in artiﬁcial data, and then aggregated in
a unique measure of distance. Clearly, these statistics have
to be computed just once in the real data (which does not
change), and once every iteration until convergence in the
artiﬁcial data, which depends on the value of the structural
parameters. The change in the value of the parameters of
each iteration is determined according to some optimiza-
tion algorithm, with the aim to minimize the distance.
In the method of simulated moments diﬀerent order of
moments are used, and then weighted to take into account
their uncertainty (while the uncertainty regarding the sim-
ulated moments can be reduced by increasing the number
of simulation runs, the uncertainty in the estimation of the
real, population moment on the basis of real sample data
cannot be avoided). The intuition behind this is to allow
parameters estimated with a higher degree of uncertainty
to count less, in the ﬁnal measure of distance between the
real and artiﬁcial data [174]. Having diﬀerent weights (or
no weights at all) impinges on the eﬃciency of the esti-
mates, not on their consistency. If the number of moments
is equal to the number of structural parameters to be esti-
mated, the model is just-identiﬁed and the minimized dis-
tance, for the estimated values of the parameters, is 0. If
the number of moments is higher than the number of pa-
rameters the model is over-identiﬁed and the minimized
distance is greater than 0. If it is lower it is under-iden-
tiﬁed. Another strategy is to estimate an auxiliary model
both in the real and in the artiﬁcial data, and then com-
pare the two sets of estimates obtained. The regression co-
eﬃcients have the same role as the moments in the method
of simulated moments: they are just a way of summarizing
the data. Hence, if the number of coeﬃcients in the auxil-
iary model is the same as the number of structural param-
eters to be estimated the model is just-identiﬁed and the
minimized distance is 0. The speciﬁcation of the auxiliary
model is not too important. It can be proved that misspec-
iﬁcation (e. g. omission of a relevant variable in the rela-
tionship to be estimated) only aﬀects eﬃciency, while the
estimates of the structural parameters remain consistent.
A natural choice is of course the meta-model of Eq. 4.
Validation
A diﬀerent issue is determining “how good” a model is.
Of course, an answer to this question cannot be unique,
but must be made in respect to some evaluation crite-
rion. This in turn depends on the objectives of the analy-
sis [62,102,111,164]. The need for evaluation of the model
is no diﬀerent in agent-based models and in traditional an-
alytical models. However, like all simulations agent-based
models require an additional layer of evaluation: the valid-
ity of the simulator (the program that simulates) relative
to the model (program validity).
Assuming this is satisﬁed and the program has no
bugs, Marks [121] formalizes the assessment of the model
validity as follows: the model is said to be useful if it can
exhibit at least some of the observed historical behaviors,
accurate if it exhibits only behaviors that are compatible
with those observed historically, and complete if it exhibits
all the historically observed behaviors. In particular, letting
R be the real world output, and M be the model output,
four cases are possible:
a. No intersection between R and M (R \ M D ;): the
model is useless;
b. M is a subset of R (M  R): the model is accurate, but
incomplete;
c. R is a subset of M (M  R): the model is complete, but
inaccurate (or redundant, since the model might tell
something about what could yet happen in the world);
d. M is equivalent to R (M , R): the model is complete
and accurate.
Of course, the selection of the relevant historical behav-
iors is crucial, and amounts to deﬁning the criteria against
which the model is to be evaluated. Moreover, the recogni-

218 A
Agent Based Models in Economics and Complexity
tion itself of historical behavior passes through a process of
analysis and simpliﬁcation that leads to the identiﬁcation
of stylized facts, which are generally deﬁned in stochastic
terms. Thus, a model is eventually evaluated according to
the extent to which it is able to statistically replicate the
selected stylized facts.
Finally, let’s note that the behavior of the model might
change signiﬁcantly for diﬀerent values of the parameters.
Hence, the process of validation always regards both the
structure of the model and the values of the parameters.
This explains why and how validation and estimation are
connected: as we have already noted, estimation is an at-
tempt to make the behavior of the model as close as pos-
sible to real behavior; validation is a judgment on how far
the two behaviors (still) are. A model where the parame-
ters have not been properly estimated and are e. g. simple
guesses can of course be validated. However, by deﬁnition
its performance can only increase should the values of the
parameters be replaced with their estimates.
The Role of Economic Policy
Before economics was political economy. According to the
classical economists, the economic science has to be used
to control the real economies and steer them towards de-
sirable outcomes. If one considers the economic system
as an analogue of the physical one, it is quite obvious to
look for natural economic policy prescriptions (one policy
ﬁts all). This is the approach of mainstream (neoclassical)
economists. There is a widespread opinion, well summa-
rized by Brock and Colander [33], that, with respect to the
economic policy analysis of the mainstream, (i) complex-
ity does not add anything new to the box of tools. This
point needs substantial corrections (see also the reﬂections
by Durlauf [3]). The complexity approach showed us that
the age of certainty ended with the non-equilibrium revo-
lution, exempliﬁed by the works of Prigogine. Considering
the economy as an evolving (adaptive) system we have to
admit that our understanding of it is limited (there is no
room for Laplace’ demon in complexity). Individual be-
havioral rules evolve according to their past performance:
this provides a mechanism for an endogenous change of
the environment. As a consequence the rational expec-
tation hypothesis loses signiﬁcance. However, agents are
still rational in that they do what they can in order not
to commit systematic errors [113]. In this setting there is
still room for policy intervention outside the mainstream
myth of a neutral and optimal policy. Because emergent
facts are transient phenomena, policy recommendations
are less certain, and they should be institution and histor-
ically oriented [65,170]. In particular, it has been empha-
sized that complex systems can either be extremely fragile
and turbulent (a slight modiﬁcation in some minor detail
brings macroscopic changes), or relatively robust and sta-
ble: in such a context, policy prescriptions ought to be case
sensitive.
In a heterogenous interacting agents environment,
there is also room for an extension of the Lucas critique.
It is well known that, according to it, because the underly-
ing parameters are not policy-invariant any policy advice
derived from large-scale econometric models that lack mi-
crofoundations would be misleading. The Lucas Critique
implies that in order to predict the eﬀect of a policy ex-
periment, the so-called deep parameters (preferences, tech-
nology and resource constraints) that govern individual be-
havior have to be modeled. Only in this case it is possi-
ble to predict the behaviors of individuals, conditional on
the change in policy, and aggregate them to calculate the
macroeconomic outcome. But here is the trick: aggrega-
tion is a sum only if interaction is ignored. If non-price
interactions (or other non-linearities) are important, then
the interaction between agents may produce very diﬀerent
outcomes. Mainstream models focus on analytical solvable
solutions: to get them, they have to simplify the assump-
tions e. g. using the RA approach or a Gaussian represen-
tation of heterogeneity. At the end, the main objective of
these models is to ﬁt the theory, not the empirics: how
to explain, e. g., the scale-free network of the real econ-
omy represented in Fig. 1c by using the non interacting
network of the mainstream model of Fig. 1a? At a mini-
mum, one should recognize that the mainstream approach
is a very primitive framework and, as a consequence, the
economic policy recommendations derivable from it are
very far from being adequate prescriptions for the real
world.
Real economies are composed by millions of interact-
ing agents, whose distribution is far from being stochastic
or normal. As an example, Fig. 4 reports the distribution
of the ﬁrms’ trade-credit relations in the electronic-equip-
ment sector in Japan in 2003 (see [47]). It is quite evident
that there exist several hubs, i. e. ﬁrms with many connec-
tions: the distribution of the degree of connectivity is scale
free, i. e. there are a lot of ﬁrms with one or two links, and
very a few ﬁrms with a lot of connections. Let us assume
the Central Authority has to prevent a ﬁnancial collapse of
the system, or the spreading of a ﬁnancial crisis (the so-
called domino eﬀect, see e. g. [104] and [157]). Rather than
looking at the average risk of bankruptcy (in power law
distributions the mean may even not exist, i. e. there is an
empirical mean, but it is not stable), and to infer it is a mea-
sure of the stability of the system, by means of a network
analysis the economy can be analyzed in terms of diﬀer-

Agent Based Models in Economics and Complexity
A
219
Agent Based Models in Economics and Complexity, Figure 4
Network of firms (electrical machinery and other machines sec-
tor, Japan). Source: De Masi et al. [47]
ent interacting sub-systems, and local intervention can be
recommended to prevent failures and their spread.
Instead of a helicopter drop of liquidity, one can make
targeted interventions to a given agent or sector of activ-
ity: Fujiwara, [72], show how to calculate the probability
of going bankrupt by solo, i. e. because of idiosyncratic ele-
ments, or domino eﬀect, i. e. because of the failure or other
agents with which there exist credit or commercial links.
One of the traditional ﬁelds of applications of eco-
nomic policy is redistribution. It should be clear that
a sound policy analysis requires a framework built with-
out the RA straight jacket. A redistributive economic pol-
icy has to take into account that individuals are diﬀerent:
not only they behave diﬀerently, e. g. with respect to saving
propensities, but they also have diﬀerent fortunes: the so-
called St.Thomas (13:12) eﬀect (to anyone who has, more
will be given and he will grow rich; from anyone who has
not, even what he has will be taken away), which is the road
to Paradise for Catholics, and to the power-law distribu-
tion of income and wealth for the econophysicists.
Gaﬀeo et al. [75], show that there is a robust link be-
tween ﬁrms’ size distribution, their growth rate and GDP
growth. This link determines the distributions of the am-
plitude frequency, size of recessions and expansion etc.
Aggregate ﬁrms’ size distribution can be well approxi-
mated by a power law [21,74], while sector distribution
is still right skewed, but without scale-free characteris-
tics [22]. Firms’ growth rates are far from being normal:
in the central part of the distribution they are tent shaped
with very fat tails. Moreover, empirical evidence shows
that exit is an inverse function of ﬁrms’ age and size and
proportional to ﬁnancial fragility. In order to reduce the
volatility of ﬂuctuations, policy makers should act on the
ﬁrms’ size distribution, allowing for a growth of their cap-
italization, their ﬁnancial solidity and wealth redistribu-
tion [48,49]. Since these emerging facts are policy sensi-
tive, if the aggregate parameters change the shape of the
curve will shift as well.
Diﬀerently from Keynesian economic policy, which
theorizes aggregate economic policy tools, and main-
stream neoclassical economics, which prescribes individ-
ual incentives because of the Lucas critique but ignores in-
teraction which is a major but still neglected part of that
critique, the ABM approach proposes a bottom up analy-
sis. What generally comes out is not a one-size-ﬁts-all pol-
icy since it depends on the general as well as the idiosyn-
cratic economic conditions; moreover, it generally has to
be conducted at diﬀerent levels (from micro to meso to
macro). In short, ABM can oﬀer new answers to old unre-
solved questions, although it is still in a far too premature
stage to oﬀer deﬁnitive tools.
Future Directions
We have shown that mainstream approach to economics
uses a methodology [71], which is so weak in its assump-
tions as to have been repeatedly ridiculed by the episte-
mologists [37], and dates back to the classical mechani-
cal approach, according to which reductionism is possible.
We have also seen that adopting the reductionist approach
in economics is to say that agents do not interact directly:
this is a very implausible assumption (billions of Robinson
Crusoes who never meet Friday) and cannot explain the
emerging characteristics of our societies, as witnessed by
the empirical evidence. The reductionist approach of the
mainstream is also theoretically incoherent, since it can be
given no sound microfoundations [8,100].
In the fourth edition of his Principles, Marshall wrote,
“The Mecca of the economist is biology”. What he meant
to say was that, because economics deals with learning
agents, evolution and change are the granum salis of our
economic world. A theory built upon the issue of alloca-
tions of given quantities is not well equipped for the anal-
ysis of change. This allocation can be optimal only if there
are no externalities (increasing returns, non-price interac-
tions etc.) and information is complete, as in the case of the
invisible hand parabola. In the history of science, there is
a passage from a view emphasizing centralized intelligent
design to a view emphasizing self organized criticality [27],
according to which a system with many heterogenous in-
teracting agents reaches a statistical aggregate equilibrium,
characterized by the appearance of some (often scale free)
stable distributions. These distributions are no longer op-

220 A
Agent Based Models in Economics and Complexity
timal or eﬃcient according to some welfare criterion: they
are simply the natural outcome of individual interaction.
Because of the above-mentioned internal and exter-
nal inconsistencies of the mainstream approach, a growing
strand of economists is now following a diﬀerent method-
ology based upon the analysis of systems with many het-
erogenous interacting agents. Their interaction leads to
empirical regularities, which emerge from the system as
a whole and cannot be identiﬁed by looking at any sin-
gle agent in isolation: these emerging properties are, ac-
cording to us, the main distinguishing feature of a com-
plex system. The focus on interaction allows the scientist
to abandon the heroic and unrealistic RA framework, in
favor of the ABM approach, the science of complexity pop-
ularized by the SFI. Where did the Santa Fe approach go?
Did it really bring a revolution in social science, as some of
its initial proponents ambitiously believed? Almost twenty
years and two “The economy as an evolving complex sys-
tem” volumes later, Blume and Durlauf summarized this
intellectual Odyssey as follows:
“On some levels, there has been great success. Much
of the original motivation for the Economics Pro-
gram revolved around the belief that economic
research could beneﬁt from an injection of new
mathematical models and new substantive perspec-
tives on human behavior. [...] At the same time,
[...] some of the early aspirations were not met”
(Chaps. 1–2 in [29]).
It is probably premature to try to give deﬁnitive answers.
For sure, ABM and the complexity approach are a very
tough line of research whose empirical results are very
promising (see e. g., Chaps. 2–3 in [77]). Modeling an
agent-based economy however remains in itself a complex
and complicated adventure.
Bibliography
1. Allen PM, Engelen G, Sanglier M (1986) Towards a general dy-
namic model of the spatial evolution of urban systems. In:
Hutchinson B, Batty M (eds) Advances in urban systems mod-
elling. North-Holland, Amsterdam, pp 199–220
2. Anderson PW (1972) More is different. Science 177:4047
3. Anderson PW (1997) Some thoughts about distribution in
economics. In: Arthur WB, Durlaf SN, Lane D (eds) The econ-
omy as an evolving complex system II. Addison-Wesley, Read-
ing
4. Anderson PW, Arrow K, Pines D (eds) (1988) The economy as
an evolving complex system. Addison-Wesley, Redwood
5. Aoki M (1996) New approaches to macroeconomic mod-
elling: evolutionary stochastic dynamics, multiple equilibria,
and externalities as field effects. Cambridge University Press,
Cambridge
6. Aoki M (2002) Modeling aggregate behaviour and fluctua-
tions in economics. Cambridge University Press, Cambridge
7. Aoki M, Yoshikawa H (2006) Reconstructing macroeco-
nomics. Cambridge University Press, Cambridge
8. Aoki M, Yoshikawa H (2007) Non-self-averaging in economic
models. Economics Discussion Papers No. 2007-49, Kiel Insti-
tute for the World Economy
9. Arrow KJ (1959) Towards a theory of price adjustment. In:
Abramovits M (ed) Allocation of economic resources. Stan-
ford University Press, Stanford
10. Arrow KJ (1963) Social choice and individual values, 2nd edn.
Yale University Press, New Haven
11. Arrow KJ (1964) The role of securities in the optimal allocation
of risk-bearing. Rev Econ Stud 31:91–96
12. Arrow KJ (1971) A utilitarian approach to the concept of
equality in public expenditures. Q J Econ 85(3):409–415
13. Arrow KJ (1994) Methodological individualism and social
knowledge. Am Econ Rev 84:1–9
14. Arrow KJ, Debreu G (1954) Existence of an equilibrium for
a competitive economy. Econometrica 22:265–290
15. Arthur WB (2000) Complexity and the economy. In: Colander
D (ed) The complexity vision and the teaching of economics.
Edward Elgar, Northampton
16. Arthur WB (2006) Out-of-equilibrium economics and agent-
based modeling. In: Tesfatsion L, Judd KL (eds) Hand-
book of computational economics, vol 2: Agent-Based Com-
putational Economics, ch 32. North-Holland, Amsterdam,
pp 1551–1564
17. Arthur WB, Durlauf S, Lane D (eds) (1997) The economy as an
evolving complex system II. Addison-Wesley, Reading
18. Askenazi M, Burkhart R, Langton C, Minar N (1996) The swarm
simulation system: A toolkit for building multi-agent simula-
tions. Santa Fe Institute, Working Paper, no. 96-06-042
19. Axelrod R (1997) Advancing the art of simulation in the social
sciences. In: Conte R, Hegselmann R, Terna P (eds) Simulating
social phenomena. Springer, Berlin, pp 21–40
20. Axtell RL (2000) Why agents? On the varied motivations for
agent computing in the social sciences. Proceedings of the
Workshop on Agent Simulation: Applications, Models and
Tools. Argonne National Laboratory, Chicago
21. Axtell RL (2001) Zipf distribution of US firm sizes. Science
293:1818–1820
22. Axtell RL, Gallegati M, Palestrini A (2006) Common compo-
nents in firms’ growth and the scaling puzzle. Available at
SSRN: http://ssrn.com/abstract=1016420
23. Bak P (1997) How nature works. The science of self-organized
criticality. Oxford University Press, Oxford
24. Batten DF (2000) Discovering artificial economics. Westview
Press, Boulder
25. Barone E (1908) Il ministro della produzione nello stato collet-
tivista. G Econ 267–293, 391–414
26. Beinhocker ED (2006) The origin of wealth: Evolution, com-
plexity, and the radical remaking of economics. Harvard Busi-
ness School Press, Cambridge
27. Bénabou R (1996) Heterogeneity, stratification and growth:
Macroeconomic implications of community structure and
school finance. Am Econ Rev 86:584–609
28. Blanchard OJ, Kiyotaki N (1987) Monopolistic competition
and the effects of aggregate demand. Am Econ Rew 77:647–
666
29. Blume L, Durlauf S (eds) (2006) The economy as an evolving

Agent Based Models in Economics and Complexity
A
221
complex system, III. Current perspectives and future direc-
tions. Oxford University Press, Oxford
30. Blundell R, Stoker TM (2005) Heterogeneity and aggregation.
J Econ Lit 43:347–391
31. Bowles S (1998) Endogenous preferences: The cultural conse-
quences of markets and other economic institutions. J Econ
Lit 36:75–111
32. Brock WA (1999) Scaling in economics: a reader’s guide. Ind
Corp Change 8(3):409–446
33. Brock WA, Colander D (2000) Complexity and policy. In:
Colander D (ed) The complexity vision and the teaching of
economics. Edward Elgar, Northampton
34. Brock WA, Durlauf SN (2001) Discrete choice with social inter-
actions. Rev Econ Stud 68:235–260
35. Brock WA, Durlauf SN (2005) Social interactions and macroe-
conomics. UW-Madison, SSRI Working Papers n.5
36. Caballero RJ (1992) A Fallacy of composition. Am Econ Rev
82:1279–1292
37. Calafati AG (2007) Milton Friedman’s epistemology UPM
working paper n.270
38. Caldarelli G (2006) Scale-free networks. Complex webs in na-
ture and technology. Oxford University Press, Oxford
39. Clower RW (1965) The keynesian counterrevolution: A theo-
retical appraisal. In: Hahn F, Brechling F (eds) The theory of
interst rates. Macmillan, London
40. Cohen A, Harcourt G (2003) What ever happened to the Cam-
bridge capital theory controversies. J Econ Perspect 17:199–
214
41. Cole HL, Mailath GJ, Postlewaite A (1992) Social norms, sav-
ings behaviour, and growth. J Political Econ 100(6):1092–
1125
42. Cooper RW (1999) Coordination games: Complementari-
ties and macroeconomics. Cambridge University Press, Cam-
bridge
43. Crutchfield J (1994) Is anything ever new? Considering emer-
gence. In: Cowan G, Pines D, Meltzer D (eds) Complexity:
Metaphors, models, and reality. Addison-Wesley, Reading,
pp 515–537
44. Davis JB (2006) The turn in economics: Neoclassical domi-
nance to mainstream pluralism? J Inst Econ 2(1):1–20
45. Debreu G (1959) The theory of value. Wiley, New York
46. Debreu G (1974) Excess demand functions. J Math Econ
1:15–23
47. De Masi G, Fujiwara Y, Gallegati M, Greenwald B, Stiglitz
JE (2008) Empirical evidences of credit networks in Japan.
mimeo
48. Delli Gatti D, Di Guilmi C, Gaffeo E, Gallegati M, Giulioni G,
Palestrini A (2004) Business cycle fluctuations and firms’ size
distribution dynamics. Adv Complex Syst 7(2):1–18
49. Delli Gatti D, Di Guilmi C, Gaffeo E, Gallegati M, Giulioni G,
Palestrini A (2005) A new approach to business fluctuations:
Heterogeneous interacting agents, scaling laws and financial
fragility. J Econ Behav Organ 56(4):489–512
50. Denzau AT, North DC (1994) Shared mental models: Ideolo-
gies and institutions. Kyklos 47(1):3–31
51. Descartes R (1637) Discours de la méthode pour bien con-
duire sa raison, et chercher la verité dans les sciences, tr. Dis-
course on Method and Meditations. The Liberal Arts Press,
1960, New York
52. Dorogovtsev SN, Mendes JFF (2003) Evolution of networks
from biological nets to the internet and the WWW. Oxford
University Press, Oxford
53. Di Guilmi C, Gallegati M, Landini S (2007) Economic dynam-
ics with financial fragilityand mean-field interaction: a model.
arXiv:0709.2083
54. Durlauf SN (1993) Nonergodic economic growth. Rev Econ
Stud 60:349–366
55. Durlauf SN (1997) What should policymakers know about
economic complexity? Wash Q 21(1):157–165
56. Durlauf SN, Young HP (2001) Social dynamics. The MIT Press,
Cambridge
57. Edgeworth FY (1925) The pure theory of monopoly In: Papers
relating to political economy. McMillan, London
58. Epstein JM (1999) Agent-based computational models and
generative social science. Complexity 4:41–60
59. Epstein JM (2006) Remarks on the foundations of agent-
based generative social science. In: Tesfatsion L, Judd KL (eds)
Handbook of computational economics. Agent-based com-
putational economics, vol 2, ch 34. North-Holland, Amster-
dam, pp 1585–1604
60. Epstein JM (2006) Generative social science: Studies in agent-
based computational modeling. Princeton University Press,
New York
61. Epstein JM, Axtell RL (1996) Growing artificial societies: Social
science from the bottom up. The MIT Press, Cambridge
62. Fagiolo G, Moneta A, Windrum P (2007) A critical guide to
empirical validation of agent-based models in economics:
Methodologies, procedures, and open problems. Comput
Econ 30:195–226
63. Farley R (1986) The residential segregation of blacks from
whites: Trends, causes, and consequences. In: US Commission
on Civil Rights, Issues in housing discrimination. US Commis-
sion on Civil Rights
64. Feller W (1957) An introduction to probability. Theory and its
applications. Wiley, New York
65. Finch J, Orillard M (eds) (2005) Complexity and the economy:
Implications for economy policy. Edward Elgar, Cheltenham
66. Flach PA, Kakas AC (eds) (2000) Abduction and induction. Es-
says on their relation and integration. Kluwer, Dordrecht
67. Flake GW (1998) The computational beauty of nature. The MIT
Press, Cambridge
68. Foellmer H (1974) Random economies with many interacting
agents. J Math Econ 1:51–62
69. Forni M, Lippi M (1997) Aggregation and the micro-founda-
tions of microeconomics. Oxford University Press, Oxford
70. Frazer J (1995) An evolutionary architecture. Architectural As-
sociation Publications, London
71. Friedman M (1953) Essays in positive economics. University of
Chicago Press, Chicago
72. Fujiwara Y (2006) Proceedings of the 9th Joint Conference on
Information Sciences (JCIS), Advances in Intelligent Systems
Research Series. Available at http://www.atlantis-press.com/
publications/aisr/jcis-06/index_jcis
73. Gabaix X (2008) Power laws in Economics and Finance, 11 Sep
2008. Available at SSRN: http://ssrn.com/abstract=1257822
74. Gaffeo E, Gallegati M, Palestrini A (2003) On the size distribu-
tion of firms, additional evidence from the G7 countries. Phys
A 324:117–123
75. Gaffeo E, Russo A, Catalano M, Gallegati M, Napoletano
M (2007) Industrial dynamics, fiscal policy and R&D: Evi-

222 A
Agent Based Models in Economics and Complexity
dence from a computational experiment. J Econ Behav Organ
64:426–447
76. Gallegati M (1993) Composition effects and economic fluctu-
ations. Econ Lett 44(1–2):123–126
77. Gallegati M, Delli Gatti D, Gaffeo E, Giulioni G, Palestrini A
(2008) Emergent macroeconomics. Springer, Berlin
78. Gallegati M, Palestrini A, Delli Gatti D, Scalas E (2006) Aggre-
gation of heterogeneous interacting agents: The variant rep-
resentative agent framework. J Econ Interact Coord 1(1):5–19
79. Gilbert N (ed) (1999) Computer simulation in the social sci-
ences, vol 42. Sage, Thousand Oaks
80. Gilbert N, Terna P (2000) How to build and use agent-based
models in social science. Mind Soc 1:57–72
81. Gilbert N, Troitzsch K (2005) Simulation for the social scientist.
Open University Press, Buckingham
82. Gintis H (2007) The dynamics of general equilibrium. Econ J
117:1280–1309
83. Glaeser E, Sacerdote B, Scheinkman J (1996) Crime and social
interactions. Q J Econ 111:507–548
84. Glaeser J, Dixit J, Green DP (2002) Studying hate crime with
the internet: What makes racists advocate racial violence?
J Soc Issues 58(122):177–194
85. Gourieroux C, Monfort A (1997) Simulation-based economet-
ric methods. Oxford University Press, Oxford
86. Greenwald B, Stiglitz JE (1986) Externalities in economies
with imperfect information and incomplete markets. Q J Econ
101(2):229–264
87. Grossman SJ, Stiglitz JE (1976) Information and competitive
price systems. Am Econ Rev 66:246–253
88. Grossman SJ, Stiglitz JE (1980) On the impossibility of infor-
mationally efficient markets. Am Econ Rev 70(3):393–408
89. Guesnerie R (1993) Successes and failures in coordinating ex-
pectations. Eur Econ Rev 37:243–268
90. Hahn F (1982) Money and inflation. Blackwell, Oxford
91. Haken H (1983) Synergetics. Nonequilibrium phase transi-
tions and social measurement, 3rd edn. Springer, Berlin
92. Hansen L, Heckman J (1996) The empirical foundations of cal-
ibration. J Econ Perspect 10:87–104
93. Hempel CV (1965) Aspects of scientific explanation. Free
Press, London
94. Hempel CV, Oppenheim P (1948) Studies in the logic of expla-
nation. Philos Sci 15:135–175
95. Hildenbrand W, Kirman AP (1988) Equilibrium analysis: Vari-
ations on the themes by edgeworth and walras. North-Hol-
land, Amsterdam
96. Horgan J (1995) From complexity to perplexity. Sci Am
272:104
97. Horgan J (1997) The end of science: Facing the limits of
knowledge in the twilight of the scientific age. Broadway
Books, New York
98. Jerison M (1984) Aggregation and pairwise aggregation of
demand when the distribution of income is fixed. J Econ The-
ory 33(1):1–31
99. Kirman AP (1992) Whom or what does the representative in-
dividual represent. J Econ Perspect 6:117–136
100. Kirman AP (1996) Microfoundations – built on sand? A re-
view of Maarten Janssen’s microfoundations: A Critical In-
quiry. J Econ Methodol 3(2):322–333
101. Kirman AP (2000) Interaction and markets. In: Gallegati M, Kir-
man AP (eds) Beyond the representative agent. Edward Elgar,
Cheltenham
102. Kleijnen JPC (1998) Experimental design for sensitivity anal-
ysis, optimization, and validation of simulation models. In:
Banks J (ed) Handbook of simulation. Wiley, New York,
pp 173–223
103. Kleijnen JPC, Sargent RG (2000) A methodology for the fitting
and validation of metamodels in simulation. Eur J Oper Res
120(1):14–29
104. Krugman P (1998) Bubble, boom, crash: theoretical notes on
Asia’s crisis. mimeo
105. Kydland FE, Prescott EC (1996) The computational experi-
ment: An econometric tool. J Econ Perspect 10:69–85
106. Lavoie D (1989) Economic chaos or spontaneous order? Im-
plications for political economy of the new view of science.
Cato J 8:613–635
107. Leibenstein H (1950) Bandwagon, snob, and veblen effects in
the theory of consumers’ demand. Q J Econ 64:183–207
108. Leijonhufvud A (1973) Life among the econ. Econ Inq 11:327–
337
109. Leombruni R (2002) The methodological status of agent-
based simulations, LABORatorio Revelli. Working Paper
No. 19
110. Leombruni R, Richiardi MG (2005) Why are economists scep-
tical about agent-based simulations? Phys A 355:103–109
111. Leombruni R, Richiardi MG, Saam NJ, Sonnessa M (2005)
A common protocol for agent-based social simulation. J Ar-
tif Soc Simul 9:1
112. Levy M, Levy H, Solomon S (2000) Microscopic simulation of
financial markets. In: From Investor Behavior to Market Phe-
nomena. Academica Press, New York
113. Lewontin C, Levins R (2008) Biology under the influence: Di-
alectical essays on the coevolution of nature and society.
Monthly Review Press, US
114. Lucas RE (1976) Econometric policy evaluation: A critique.
Carnegie-Rochester Conference Series, vol 1, pp 19–46
115. Lucas RE (1987) Models of business cycles. Blackwell, New
York
116. Lucas RE, Sargent T (1979) After keynesian macroeconomics.
Fed Reserv Bank Minneap Q Rev 3(2):270–294
117. Magnani L, Belli E (2006) Agent-based abduction: Being ratio-
nal through fallacies. In: Magnani L (ed) Model-based reason-
ing in science and engineering, Cognitive Science, Epistemol-
ogy, Logic. College Publications, London, pp 415–439
118. Manski CF (2000) Economic analysis of social interactions.
J Econ Perspect 14:115–136
119. Mantel R (1974) On the characterization of aggregate excess
demand. J Econ Theory 7:348–353
120. Mantegna RN, Stanley HE (2000) An introduction to econo-
physics. Cambridge University Press, Cambridge
121. Marks RE (2007) Validating Simulation Models: A general
framework and four applied examples. Comput Econ 30:265–
290
122. May RM (1976) Simple mathematical models with very com-
plicated dynamics. Nature 261:459–467
123. Mas-Colell A, Whinston MD, Green J (1995) Microeconomic
theory. Oxford University Press, Oxford
124. Miller JH, Page SE (2006) Complex adaptive systems: An in-
troduction to computational models of social life. Princeton
University Press, New York
125. Mirowski P (1989) More heat than light. Cambridge University
Press, Cambridge
126. Muth RF (1986) The causes of housing segregation. US Com-

Agent Based Models in Economics and Complexity
A
223
mission on Civil Rights, Issues in Housing Discrimination. US
Commission on Civil Rights
127. Nagel E (1961) The structure of science. Routledge and Paul
Kegan, London
128. Nicolis G, Prigogine I (1989) Exploring complexity: An intro-
duction. WH Freeman, New York
129. North MJ, Howe TR, Collier NT, Vos JR (2005) Repast simphony
runtime system. In: Macal CM, North MJ, Sallach D (eds) Pro-
ceedings of the agent 2005 Conference on Generative Social
Processes, Models, and Mechanisms, 13–15 Oct 2005
130. Ostrom T (1988) Computer simulation: the third symbol sys-
tem. J Exp Soc Psycholog 24:381–392
131. Page S (1999) Computational models from A to Z. Complexity
5:35–41
132. Peirce CS (1955) Abduction and induction. In: J Buchler (ed)
Philosophical writings of peirce. Dover, New York
133. Phelan S (2001) What is complexity science, really? Emer-
gence 3:120–136
134. Pollack R (1975) Interdependent preferences. Am Econ Rev
66:309–320
135. Railsback SF, Lytinen SL, Jackson SK (2006) Agent-based sim-
ulation platforms: Review and development recommenda-
tions. Simulation 82:609–623
136. Rappaport S (1996) Abstraction and unrealistic assumptions
in economics. J Econ Methodol 3(2):215–36
137. Resnick M (1994) Turtles, termites and traffic jams: Explo-
rations in massively parallel microworlds. MIT, Cambidge
138. Richter MK, Wong K (1999) Non-computability of competitive
equilibrium. Econ Theory 14:1–28
139. Rioss Rull V (1995) Models with heterogeneous agents. In:
Cooley TF (ed) Frontiers of business cycle research. Princeton
University Press, New York
140. Rosser JB (1999) On the complexities of complex economic
dynamics. J Econ Perspect 13:169–192
141. Rosser JB (2000) Integrating the complexity vision into the
teaching of mathematical economics. In: Colander D (ed) The
complexity vision and the teaching of economics. Edward El-
gar, Cheltenham, pp 209–230
142. Rosser JB (2003) Complexity in economics. Edward Elgar,
Cheltenham
143. Rust J (1997) Using randomization to break the curse of di-
mensionality. Econometrica 65:487–516
144. Saari DG (1995) Mathematical complexity of simple eco-
nomics. Notices Am Math Soc 42:222–230
145. Schelling TC (1969) Models of segregation. Am Econ Rev
59:488–493
146. Schelling TC (1971) Dynamic models of segregration. J Math
Sociol 1:143–186
147. Schelling TC (1978) Micromotives and macrobehaviour. W.W.
Norton, New York
148. Schelling TC (2006) Some fun, thirty-five years ago. In: Tesfat-
sion L, Judd KL (eds) Handbook of computational economics.
Agent-based computational economics, vol 2, ch 37. North-
Holland, Amsterdam, pp 1639–1644
149. Schumpeter JA (1960) History of economic analysis. Oxford
University Press, Oxford
150. Segre-Tossani L, Smith LM (2003) Advanced modeling, visual-
ization, and data mining techniques for a new risk landscape.
Casualty Actuarial Society, Arlington, pp 83–97
151. Semmler W (2005) Introduction (multiple equilibria). J Econ
Behav Organ 57:381–389
152. Shy O (2001) The economics of network industries. Cam-
bridge University Press, Cambridge
153. Smith A (1776/1937) The wealth of nations. Random House,
New York
154. Solomon S (2007) Complexity roadmap. Institute for Scientific
Interchange, Torino
155. Sonnenschein H (1972) Market excess demand functions.
Econometrica 40:549–563
156. Stiglitz JE (1992) Methodological issues and the new key-
nesian economics. In: Vercelli A, Dimitri N (eds) Macroeco-
nomics: A survey of research strategies. Oxford University
Press, Oxford, pp 38–86
157. Stiglitz JE (2002) Globalization and its discontents. Northon,
New York
158. Stoker T (1995) Empirical approaches to the problem of ag-
gregation over individuals. J Econ Lit 31:1827–1874
159. Tesfatsion L (ed) (2001) Special issue on agent-based compu-
tational economics. J Econ Dyn Control 25
160. Tesfatsion L (ed) (2001) Special issue on agent-based compu-
tational economics. Comput Econ 18
161. Tesfatsion L (2001) Agent-based computational economics:
A brief guide to the literature. In: Michie J (ed) Reader’s guide
to the social sciences. Fitzroy-Dearborn, London
162. Tesfatsion
L
(2002)
Agent-based
computational
eco-
nomics: Growing economies from the bottom up. Artif Life
8:55–82
163. Tesfatsion L (2006) Agent-based computational economics:
A constructive approach to economic theory. In: Tesfatsion L,
Judd KL (eds) Handbook of computational economics. Agent-
based computational economics, vol 2, ch 16. North-Holland,
Amsterdam, pp 831–880
164. Troitzsch KG (2004) Validating simulation models. In: Horton
G (ed) Proceedings of the 18th european simulation multi-
conference. Networked simulations and simulation networks.
SCS Publishing House, Erlangen, pp 265–270
165. Vriend NJ (1994) A new perspective on decentralized trade.
Econ Appl 46(4):5–22
166. Vriend NJ (2002) Was Hayek an ace? South Econ J 68:811–840
167. Velupillai KV (2000) Computable economics. Oxford Univer-
sity Press, Oxford
168. Velupillai KV (2002) Effectivity and constructivity in economic
theory. J Econ Behav Organ 49:307–325
169. Velupillai KV (2005) The foundations of computable general
equilibrium theory. In: Department of Economics Working Pa-
pers No 13. University of Trento
170. Velupillai KV (2007) The impossibility of an effective theory of
policy in a complex economy. In: Salzano M, Colander D (eds)
Complexity hints for economic policy. Springer, Milan
171. von Hayek FA (1948) Individualism and economic order. Uni-
versity of Chicago Press, Chicago
172. von Mises L (1949) Human action: A treatise on economics.
Yale University Press, Yale
173. Wilensky U (1998) NetLogo segregation model. Center for
connected learning and computer-based modeling. North-
western University, Evanston. http://ccl.northwestern.edu/
netlogo/models/Segregation
174. Winker P, Gilli M, Jeleskovic V (2007) An objective function for
simulation based inference on exchange rate data. J Econ In-
teract Coord 2:125–145
175. Wooldridge M (2001) An introduction to multiagent systems.
Wiley, New York

224 A
Aggregation Operators and Soft Computing
Aggregation Operators
and Soft Computing
VICENÇ TORRA
Institut d’Investigació en Intelligència Artiﬁcial – CSIC,
Bellaterra, Spain
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Applications
Aggregation, Integration and Fusion
Aggregation Operators and Their Construction
Some Aggregation Operators
Future Directions
Bibliography
Glossary
Information integration The whole process of obtaining
some information from diﬀerent sources and then us-
ing this information to achieve a concrete task.
Information fusion A general term that deﬁnes the
whole area that studies techniques and methods to
combine information for achieving a particular goal.
Aggregation operators Particular operators that are ac-
tually used for combining the information.
Definition of the Subject
Aggregation operators are the particular functions used
for combining information in systems where several in-
formation sources have to be taken into consideration for
achieving a particular goal.
Formally, aggregation operators are the particular
techniques of information fusion which can be mathemat-
ically expressed in a relatively simple way. They are part of
the more general process of information integration. That
is the process that goes from the acquisition of data to the
accomplishment of the ﬁnal task.
As current systems (from search engines to vision and
robotics systems) need to consider information from dif-
ferent sources, aggregation operators are of major impor-
tance. Such operators permit one to reduce the quantity of
information and improve its quality.
Introduction
Information fusion is a broad ﬁeld that studies computa-
tional methods and techniques for combining data and in-
formation. At present, diﬀerent methods have been devel-
oped. Diﬀerences are on the type of data available and on
the assumptions data and information sources (data sup-
pliers) satisfy. For example, methods exist for combining
numerical data (e. g. readings from sensors) as well as for
combining structured information (e. g. partial plans ob-
tained from AI planners). Besides, the complexity of the
methods ranges from simple functions (as the arithmetic
mean or the voting technique) that combine a few data to
more complex ones (as procedures for dealing with satel-
lite images) that combine a huge burden of data.
Soft computing is an area in computer science that en-
compasses a few techniques with the goal of solving com-
plex problems when there are no analytic or complete so-
lutions. Two typical problems are the control of a variable
(when standard methods, as PID controllers, are not appli-
cable) and the optimization of a function (when standard
methods, as quadratic programming, cannot be applied
because the objective function to be optimized is highly
non linear). Among the techniques in soft computing, we
can underline genetic algorithms and evolutionary com-
putation, neural networks, and fuzzy sets and fuzzy sys-
tems [21].
Some of the techniques included in soft computing
have been used for combining and fusing data. This is the
case of neural networks. It is well known that neural net-
works (when applied in a supervised manner) can be used
to build a system that reproduces up to some extent the be-
havior of a set of examples. So, in the case of aggregation
and data fusion, they can be used to build models when
we have a data set consisting on some examples (pairs of
input data, expected outcome) that are to be reproduced
by the system. Aggregation operators, which can also be
seen from the soft computing perspective, follow a dif-
ferent principle. They are deﬁned mathematically and are
characterized in terms of their properties.
Aggregation operators are often studied in the frame-
work of fuzzy sets and fuzzy systems, one of the compo-
nents of soft computing. Such operators, when operating
on the [0; 1] interval are closely related to with t-norms
and t-conorms, the basic functions in fuzzy logic to
model conjunction and disjunction. Formally, t-norms
and t-conorms are used in fuzzy logic to combine the
membership degrees either in a conjunctive or disjunctive
way. In this setting aggregation operators are also used to
combine information but additional ﬂexibility is permitted
in the combination. Weighted minimum, and the fuzzy
integrals are some examples of aggregation operators that
are typically studied in this framework.
In this work, we present an overview of the area of in-
formation fusion and we focus on the aggregation oper-

Aggregation Operators and Soft Computing
A
225
ators. We review a few of them with some of their prop-
erties. We focus on those that have a tight link with soft
computing, and especially on the ones that belong to the
ﬁeld of fuzzy sets and fuzzy logic.
Applications
Aggregation operators are used in a large variety of con-
texts and applications [8]. We underline a few of them be-
low. For example, there are applications in economy (to
deﬁne all kind of indices as e. g. the retail price index), biol-
ogy (to combine DNA sequences and classiﬁcations, either
expressed as dendrograms – trees – or partitions [7,12]),
in education (to rate students, universities and educational
systems [8]), in bibliometrics (to evaluate the quality of the
researchers and rate them by means of citation indices as
e. g. the number of citations or the h index [16,25]), and in
computer science.
In computer science, aggregation operators are used
for e. g. evaluating hardware and software, and for fus-
ing preferences. In the ﬁrst case, diﬀerent criteria or per-
formance measures are combined to deﬁne an index. For
example, performance measures for evaluating runtime
systems are deﬁned as the aggregation of the execution
times of diﬀerent benchmarks. Some means (e. g. geomet-
ric means) are used for such aggregation. In the second
case, diﬀerent sets of preferences are fused into a new ag-
gregated one. For example, meta-search engines proceed
in this way aggregating the results of diﬀerent search en-
gines. Note that the result of each search engine is a pref-
erence on the links and the ﬁnal ranking is another pref-
erence that is built as an aggregation of the initial prefer-
ences.
In addition, aggregation operators and information fu-
sion are also used in artiﬁcial intelligence, robotics and vi-
sion. Knowledge integration, knowledge revision or belief
merging for knowledge based systems can also be seen as
a case of information fusion at large. In this case, a knowl-
edge base, which is deﬁned using e. g. description logic in
terms of propositions, is checked to detect incoherences
and, eventually, to correct them. In vision, fusion is used
e. g. for improving the quality of images. Fusion is also
used in robotics for a similar purpose. In particular, it is
used for improving the quality of the information obtained
from the sensors [18].
The diﬀerent uses of information fusion in artiﬁcial in-
telligence and robotics can be classiﬁed into two main cat-
egories. On the one hand, we have the uses related to mak-
ing decisions and, on the other hand, the uses related with
the obtainment of a better description of the application
domain (so that the system has a better understanding).
Decision making typically corresponds to the process
of selecting the best alternative. Diﬃculties in decision
making are found when each alternative is evaluated us-
ing diﬀerent (conﬂicting) criteria. This case corresponds
to the multi criteria decision making framework [13,20].
In such framework, depending on the criterion used in the
selection, diﬀerent alternatives are selected. That is, diﬀer-
ent Pareto optimal solutions can be selected according to
the criteria. One approach to solve this situation is to use
aggregation operators to combine the degrees of satisfac-
tion for each criteria into a new aggregated degree of sat-
isfaction (for a new aggregated criterion). Then selection
uses this aggregated criterion to select the best rated alter-
native. Another approach consists of considering diﬀerent
preferences (one for each criterion) and then such pref-
erences are aggregated into a new aggregated preference.
Then, selection is based on the aggregated preference.
A diﬀerent case, also related to decision making, is
when the alternative to be selected (e. g. the solution of
a problem) is constructed from the other ones. For exam-
ple, several partial solutions are retrieved or obtained by
a set of subsystems (maybe because such solutions have
been applied before with success into similar problems, as
in case-based reasoning). Then, if none of them is com-
pletely appropriate in the new situation, the new solution
is built from such partial solutions. Some applications of
case-based reasoning can be seen from this perspective.
This is also the case in plan merging, where partial plans
are combined to build the ﬁnal plan. Another case cor-
responds to the ensemble methods in machine learning.
Such methods assign an outcome (e. g., a class in a classi-
ﬁcation problem) for a given data taking into account the
outcomes of diﬀerent subsystems (e. g., diﬀerent classiﬁers
based on diﬀerent assumptions constructed from the data
in the learning set). Then, a ﬁnal outcome is decided on
the basis of the outcomes of the subsystems (e. g., using
a voting strategy).
The second use of information fusion and aggregation
operators is for improving the understanding of the ap-
plication domain. This tries to solve some of the prob-
lems related to data of low quality, e. g., data that lacks
accuracy due to errors caused by the information source
or due to the errors introduced in the transmission. An-
other diﬃculty is that the data from a single source might
describe only part of the application’s domain, and thus,
several sources are needed to have a more comprehensive
description of the domain, e. g. either several video cam-
eras or a sequence of images from the same video camera
are needed to have a full description of a room or a build-
ing. Knowledge integration and knowledge revision can be
seen from this perspective as the knowledge in a system

226 A
Aggregation Operators and Soft Computing
needs to be updated and reﬁned when the system is under
execution.
The diﬀerent uses of aggregation operators and the
large number of applications have stimulated the deﬁni-
tion, study and characterization of aggregation operators
on diﬀerent types of data. Due to this, there are nowa-
days operators for a large number of diﬀerent domains.
For example, there are operators to combine data in nom-
inal scales (no ordering or structure exists between the el-
ements to be aggregated), ordinal scales (there is order-
ing between pairs of categories but no other operators are
allowed), numerical data (ratio and measure scales), par-
titions (input data correspond to a partition of the ele-
ments of a given set), fuzzy partitions (as in the previous
case, but fuzzy partitions are considered), dendrograms
(e. g., tree-like structures for classifying species), DNA se-
quences (i. e., sequences of A, T, C, and G), images (in
remote sensing applications), and logical propositions (in
knowledge integration).
Aggregation, Integration and Fusion
In the context of information fusion there are three terms
that are in common use. The terms are information in-
tegration, information fusion, and aggregation operator.
Although these terms are often used as synonymous their
meaning is not exactly equivalent. We deﬁne them below.
Information integration This corresponds to the whole
process of obtaining some information from diﬀer-
ent sources and then using this information to achieve
a concrete task. It is also possible that instead of infor-
mation from several sources, information is extracted
from the same source but at diﬀerent times.
Information fusion This is a general term that deﬁnes
the whole area that studies techniques and methods to
combine information for achieving a particular goal. It
is also understood as the actual process of combining
the information (a particular algorithm that uses the
data to obtain a single datum).
Aggregation operators They are some of the particular
operators that are actually used for combining the in-
formation. They can be understood as particular infor-
mation fusion techniques. In fact, such operators are
the ones that can be formalized straightforwardly in
a mathematical way. The most well known aggregation
operators are the means. Other operators as t-norms
and t-conorms, already pointed out above, are also
considered by some as aggregation operators. In gen-
eral, we consider operators that combine N values in
a given domain D obtaining a new single datum in the
same domain D. For example, in the case of the arith-
metic mean, we combine N values on the real line and
obtain a new datum that is also a value in the real line.
Taking all this into account, we would say that a robot that
combines sensory information includes some information
integration processes in its internal system. When sensory
data is combined with a mean, or a similar function, we
would say that it uses a fusion method that is an aggrega-
tion operator. In contrast, if the robot combines the data
using a neural network or another black-box approach, we
would say that it uses an information fusion method but
that no aggregation operator can be distinguished.
As said above, some people distinguish between aggre-
gation operators and means. In this case, a mean is a par-
ticular aggregation operator that satisﬁes unanimity and
monotonicity. Other operators that combine N values but
that do neither satisfy unanimity nor monotonicity are not
means but aggregation operators. Besides, there are ag-
gregation operators that are not required to satisfy mono-
tonicity because they are not deﬁned on ordered domains.
This is the case of operators on nominal scales. The plural-
ity rule or voting is one of such operators.
In this work we consider aggregation in a narrow
sense. Therefore, an aggregation operator is a function
C, from consensus, such that combines N values in
a domain D and returns another value in the same do-
main, i. e., C : DN ! D. Such function satisﬁes unanim-
ity (i. e., C(a; : : : ; a) D a for all a in D) and, if applicable,
monotonicity (i. e., C(a1; : : : ; aN)  C(b1; : : : ; bN) when
ai < bi. i. e., this latter condition is valid, as stated before,
only when there is an order on the elements in D, and, oth-
erwise, the condition is removed.
Operators on ordered domains deﬁned in this way
satisfy internality. That is, the aggregation of values
a1; : : : ; aN is a value between the minimum and the max-
imum of these values:
min
i
ai  C(a1; : : : ; aN)  max
i
ai :
Aggregation Operators and Their Construction
The deﬁnition of an aggregation operator has to take into
account several aspects. On the one hand, it should con-
sider the type of data to be combined (this aspect has been
mentioned before). That is, whether data is numerical, cat-
egorical, etc. On the second hand, it should consider the
level of abstraction. That is, whether aggregation has to be
done at a lower level or at a higher one. Due to the data
ﬂow, the data in a system typically moves from low levels
to higher ones. Then, the fusion of two data can be done

Aggregation Operators and Soft Computing
A
227
at diﬀerent abstraction levels. Note that data at diﬀerent
levels have diﬀerent representations, e. g. information at
a lower level might be numerical (e. g. the values of a mea-
surement), while at a higher level it might be symbolical
(e. g. propositions in logic, or belief measures). The level
of abstraction and the type of data that are appropriate for
fusion depends on a third element, the type of informa-
tion. That is, information from diﬀerent sources might be
redundant or complementary. Redundant information is
usually aggregated at a low level (e. g. the values of measur-
ing the same object using two similar sensors), while com-
plementary information is usually aggregated at a higher
level (e. g., the fusion of images to have a better representa-
tion of 3D objects cannot be done at a pixel–signal–level).
Once the appropriate environment for a function has
been settled, it is the time for the deﬁnition of the function.
Two main cases can be distinguished, (i) deﬁnition from
examples and (ii) deﬁnitions from properties.
(i) The use of examples for deﬁning the operators follows
machine learning theory. Two main cases can be dis-
tinguished. The ﬁrst one is when we assume that the
model of aggregation is known. e. g., we know that
we will use a weighted mean for the aggregation, and
then the problem is to ﬁnd the appropriate parameters
for such weighted mean. The second one is when the
model is not known and only a loose model – black
box model – is assumed. Then the examples are used
to settle such model. The use of neural networks is an
example of this kind of application.
(ii) In the case of deﬁnition from properties, the system
designer establishes the properties that the aggrega-
tion operator needs to satisfy. Then the operator is de-
rived from such properties.
Functional equations [1,2] can be used for this purpose.
Functional equations are equations that have functions as
their unknowns. For example, (x C y) D (x) C (y)
is an example of a functional equation (one of Cauchy’s
equations) with an unknown function . In this equation,
the goal is to ﬁnd the functions  that satisfy such equa-
tion. In this case, when  are continuous, solutions are
functions (x) of the form (x) D ˛x.
In the case of aggregation, we might be interested
in, for instance, functions (x; y) that aggregate two nu-
merical values x and y, and that satisfy unanimity (i. e.,
(x; x) D x), invariance to transformations of the form
(x) D x C t (i.e, (x1 C t; x2 C t) D (x1; x2) C t), and
invariance to transformations of the form (x) D rx –
positive
homogeneity
(i. e.,
(rx1; rx2) D r(x1; x2)).
Such functions are completely characterized and all of
them are of the form (x1; x2) D (1  k)x1 C kx2.
Naturally, the deﬁnition from properties might cause
the deﬁnition of an overconstrained problem. That is, we
may include too restrictive constraints and deﬁne a prob-
lem with no possible solution. This is the case of the prob-
lem of aggregation of preferences [5] that lead to Arrow’s
impossibility theorem [4]. That is, there is a set of natural
properties that no function for the aggregation of prefer-
ences can satisfy.
The deﬁnition of the aggregated value as the one that is
located at a minimum distance from the objects being ag-
gregated is another approach for deﬁning aggregation op-
erators. In this case, we need to deﬁne a distance between
pairs of objects on the domain of application. Let D be the
domain of application, let d be a distance function for pairs
of objects in D (that is, d : D  D ! R). Then, the aggre-
gation function C : DN ! D applied to a1; : : : ; aN is for-
mally deﬁned as
C(a1; : : : ; aN) D arg min
c
(
N
X
iD1
d(c; ai)
)
:
Such deﬁnition is often known as the median rule. In
the numerical setting, i. e., if the domain D is a sub-
set of the real set, when d(a; b) D (a  b)2, the aggrega-
tion operator results to be the arithmetic mean. When
d(a; b) D ja  bj, the aggregation operator results into the
median. Formally, the median is the only solution when N
is of the form 2k C 1 or when N is of the form 2k but the
two central values are equal. Otherwise, (when N is of the
form 2k and the two central values are diﬀerent), the me-
dian is one of the solutions of the equation above but other
solutions might be also valid. For example, let us assume
that N D 4 and that a1  a2 < a3  a4, then another so-
lution is,
C(a1; : : : ; aN) D
(a4a3  a2a1)
(a4 C a3)  (a2 C a1) :
When d(a; b) D ja  bj1 for k ! 1, we have that the
midrange of the elements a1; : : : ; aN is a solution of the
equation above. That is,
C(a1; : : : ; aN) D (mini ai  maxi ai)
2
:
Some Aggregation Operators
There exist a large number of aggregation operators. We
will discuss in this section a few of them, classiﬁed into dif-
ferent families. We begin with the quasi-arithmetic mean,
a family of operators that encompasses some of the most
well-known aggregation operators, the arithmetic mean,
the geometric mean, and the harmonic mean.

228 A
Aggregation Operators and Soft Computing
Quasi-Arithmetic Means
A quasi-arithmetic mean is an aggregation operator C of
the form
QAM(a1; : : : ; aN) D '1
 
1
N
N
X
iD1
'(ai)
!
:
with ' an invertible function.
We list below a few examples of quasi-arithmetic
means.
 Arithmetic mean:
AM(a1; : : : ; aN) D 1
N
N
X
iD1
ai :
 Geometric mean:
GM(a1; : : : ; aN) D
N
v
u
u
t 1
N
N
X
iD1
ai :
 Harmonic mean:
HM(a1; : : : ; aN) D
N
PN
iD1
1
ai
:
 Power mean (Hölder mean, or root-mean-power,
RMP):
RMP(a1; : : : ; aN) D
˛
v
u
u
t 1
N
N
X
iD1
a˛
i :
 Quadratic mean (root-mean-square):
RMS(a1; : : : ; aN) D
2
v
u
u
t 1
N
N
X
iD1
a2
i :
These means are obtained from the quasi-arithmetic mean
using the following deﬁnition for the function ' : '(x) D
x (in the arithmetic mean), '(x) D log x (in the geometric
mean), '(x) D 1/x (in the harmonic mean), '(x) D x˛
(in the power mean), and '(x) D x2 (in the quadratic
mean).
Several
properties
have
been
proved
for
these
functions. In particular, some inequalities have been
proven [10,15]. For example, it is known that the har-
monic mean (HM) always return a value that is smaller
than the one of the geometric mean (GM) and that the
value of the arithmetic mean is smaller than the one of the
arithmetic mean (AM). That is, the following inequality
holds for all x, y,
HM(x; y)  GM(x; y)  AM(x; y) :
In the case of the power means, the following inequality
holds,
RMPr(x; y)  RMPs(x; y)
for all r < s :
That is, the power means is monotonic with respect to its
parameter.
Also related with the power means, we have that when
˛ ! 0, RMP˛ D GM, that when ˛ ! 1, RMP˛ D max
and that when ˛ ! 1, RMP˛ D min.
From the point of view of the applications, it is of rele-
vance whether the outcome of an aggregation operator is,
in general, large or small. That is, whether the aggregation
operator A is larger or smaller than the aggregation oper-
ator B. For example, whether it holds or not that
A(x; y)  B(x; y)
for all x; y:
Due to the importance in real practice of such order-
ing, some indices have been developed for analyzing the
outcomes of the operators. The average value, the orness
and the andness are examples of such indices.
The orness computes the similarity of the operator
with the maximum. Recall that in fuzzy sets, the maximum
is used to model the disjunction, i. e., or. In contrast to
that, the andness computes the similarity of the operator
with the minimum. Recall that in fuzzy sets, the minimum
is used to model the conjunction, i. e., and. For these mea-
sures, it can be proven that (i) the operator maximum has
a maximum orness (an orness equal to one) and a mini-
mum andness (an andness equal to zero), (ii) the orness of
the arithmetic mean is larger than the orness of the geo-
metric mean, and this latter orness is larger than the one
of the harmonic mean. This property follows from the fol-
lowing inequality that holds for all x and y,
HM(x; y)  GM(x; y)  AM(x; y) :
The interpretations for the andness and the orness ease
the selection of the appropriate aggregation function. This
is so because the orness can be interpreted as a degree of
compensation. That is, when diﬀerent criteria are consid-
ered, a high value of orness means that a few good criteria
can compensate a large number of bad ones. In contrast to
that, a low value of orness means that all criteria have to
be satisﬁed in order to have a good ﬁnal score. Taking this
into account, and using the example above, when only low
compensation is permitted, it is advisable to use the geo-
metric mean or the harmonic mean. In contrast, a larger
compensation requires the user to consider the arithmetic
mean.
A similar approach can be applied for the power mean.
The larger the parameter r in the function (i. e., RMPr) the
larger the compensation.

Aggregation Operators and Soft Computing
A
229
Weighted Means and Quasi-weighted Means
The weighted mean is the most well-known aggregation
operator with weights, and the quasi-weighted mean is one
of its generalizations. In such operator, weights are repre-
sented by means of a weighting vector. That is, a vector
w D (w1; : : : ; wN) that satisﬁes
wi  0
and
˙wi D 1 :
 Weighted mean: given a weighting vector w
D
(w1; : : : ; wN), the weighted mean is deﬁned as follows
WM(a1; : : : ; aN) D
N
X
iD1
wiai :
Naturally, when wi D 1/N for all i D 1; : : : ; N, the
weighted mean corresponds to the arithmetic mean.
In this deﬁnition, the weights can be understood as
their reliability. The larger the importance, the larger
the weights.
 Quasi-weighted mean: given a weighting vector w D
(w1; : : : ; wN), the quasi-weighted mean (or quasi-lin-
ear mean) is deﬁned as follows
QWM(a1; : : : ; aN) D '1
 
wi
N
X
iD1
'(ai)
!
;
with ' an invertible function, and w D (w1; : : : ; wN)
a weighting vector.
The quasi-weighted mean generalizes the quasi-arith-
metic mean given above introducing weights wi for
each of the source. Using appropriate functions ',
we obtain the weighted mean (with '(x) D x), the
weighted geometric mean (with '(x) D log x), the
weighted harmonic mean (with '(x) D 1/x), the
weighted power mean (with '(x) D x˛), and the
weighted quadratic mean (with '(x) D x2).
Losonczi’s Means
This family of means, deﬁned in [6], is related to the quasi-
weighted means. While in the former the weights only de-
pend on the information source (there is one weight for
each ai but such weight does not depend on the value ai),
this is not the case in the Losonczi’s means. In this opera-
tor, weights depend on the values being aggregated.
Formally, a Losonczi’s mean is an aggregation operator
C of the form,
C(a1; : : : ; aN) D '1
 PN
iD1 i(ai)'(ai)
PN
iD1 i(ai)
!
;
with ' an invertible function.
Naturally, when i(a) is constant for all a, we have that
the function reduces to a quasi-weighted means. When
i(a) D aq and '(a) D apq with q D p  1, the Loson-
czi’s means reduces to the counter-harmonic mean, also
known by Lehmer mean. That is,
C(a1; : : : ; aN) D
PN
iD1 ap
i
PN
iD1 ap1
i
:
When i(a) D wiaq and '(a) D apq with q D p  1,
the Losonczi’s means reduces to the weighted Lehmer
mean. That is,
C(a1; : : : ; aN) D
PN
iD1 wiap
i
PN
iD1 wiap1
i
:
Linear Combination of Order Statistics
and OWA Operators
Given a1; : : : ; aN the ith order statistics [3] is the element
in a1; : : : ; aN that occupies the ith position when such el-
ements are ordered in increasing order. Formally, let i be
an index i 2 f1; : : : ; Ng. Then, the ith order statistics OSi
of dimension N is deﬁned as follows:
OSi(a1; : : : ; aN) D as(i) :
Where fs(1); : : : ; s(N)g is a permutation of f1; : : : ; Ng
such that as(j)  as(jC1) for all j 2 f1; : : : ; N  1g.
When N is of the form 2k C 1, OS(NC1)/2 D OSkC1 is
the median; when N is of the form 2k, the median corre-
sponds to the mean value of OSN/2 and OSN/2C1. That is,
median(a1; : : : ; aN) D (OSN/2 C OSN/2C1)/2 :
The OWA (ordered weighted averaging) operator is
a linear combination of order statistics (an L-estimator).
Its deﬁnition is as follows.
 OWA operator: given a weighting vector w D (w1; : : : ;
wN) (i. e., a vector such that wi  0 and ˙iwi D 1), the
OWA operator is deﬁned as follows
OWA(a1; : : : ; aN) D
N
X
iD1
wi a
(i) ;
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that a
(i1)  a
(i) for all i 2 f2; : : : ; Ng. Equiv-
alently,
OWA(a1; : : : ; aN) D
N
X
iD1
wi  OSi(a1; : : : ; aN) :

230 A
Aggregation Operators and Soft Computing
Naturally, when wi D 1/N for all i D 1; : : : ; N, the
OWA operator corresponds to the arithmetic mean.
The ordering of the elements, permit the user to ex-
press the importance of some of the values being ag-
gregated (importance of the values with respect to their
position). For example, the OWA permits us to model
situations where importance is given to low values (e. g.
in a robot low values are more important than larger
ones to avoid collisions). So, informally, we have that
the wi for i D N and i D N  1 should be large.
 Geometric OWA operator: given a weighting vector
w D (w1; : : : ; wN), the geometric OWA (GOWA) op-
erator is deﬁned as follows
GOWA(a1; : : : ; aN) D
N
Y
iD1
awi

(i) ;
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that a
(i1)  a
(i) for all i 2 f2; : : : ; Ng. That is,
the GOWA operator is a geometric mean of the order
statistics.
 Generalized OWA operator: given a weighting vector
w D (w1; : : : ; wN), the generalized OWA operator is
deﬁned as follows
Generalized OWA(a1; : : : ; aN) D
 N
X
iD1
wi a˛

(i)
!1/˛
;
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that a
(i1)  a
(i) for all i 2 f2; : : : ; Ng. That is,
the generalized OWA operator is a generalized mean
(root-mean-powers) of the order statistics.
 Quasi-OWA operator: given a weighting vector w D
(w1; : : : ; wN), the quasi-OWA (QOWA) operator is
deﬁned as follows
QOWA(a1; : : : ; aN) D '1
 N
X
iD1
wi'

a
(i)

!
:
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that a
(i1)  a
(i) for all i 2 f2; : : : ; Ng. That is,
the quasi-OWA operator is a quasi-weighted mean of
the order statistics.
OWA(a1; : : : ; aN) D
N
Y
iD1
awi

(i) ;
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that a
(i1)  a
(i) for all i 2 f2; : : : ; Ng.
 BADD-OWA operator: given a weighting vector w D
(w1; : : : ; wN), the BADD-OWA (basic defuzziﬁcation
distribution OWA) corresponds to the counter-har-
monic mean (see above). That is,
C(a1; : : : ; aN) D
PN
iD1 ap
i
PN
iD1 ap1
i
:
 Induced OWA operator: given a weighting vector w D
(w1; : : : ; wN) and a priority vector b D (b1; : : : ; bN),
the induced OWA (IOWA) operator is deﬁned as fol-
lows
IOWAb(a1; : : : ; aN) D
N
X
iD1
wia
(i) ;
where f(1); : : : ; (N)g is a permutation of f1; : : : ; Ng
such that b
(i1)  b
(i) for all i 2 f2; : : : ; Ng. Natu-
rally, the OWA corresponds to the IOWA when a is
used as the priority vector.
The WOWA Operator
Let p and w be two weighting vectors of dimension N.
Then, the WOWA [23] operator (weighted ordered
weighted averaging operator) of dimension N is deﬁned
as
WOWAp;w(a1; : : : ; aN) D
N
X
iD1
!ia
(i) ;
where  is deﬁned as above (i.e, a
(i) is the ith largest el-
ement in the collection a1; : : : ; aN), and the weight !i is
deﬁned as
!i D w
 X
ji
p
(j)

 w
 X
j<i
p
(j)

;
with w being a nondecreasing function that interpolates
the points
( 
i/N;
X
ji
wj
)
iD1;:::;N
[ f(0; 0)g :
The function w is required to be a straight line when the
points can be interpolated in this way.
The WOWA operator generalizes the OWA and the
weighted mean. When p D (1/N; : : : ; 1/N), the WOWA
reduces to an OWA operator and when w D (1/N; : : : ;
1/N), the WOWA reduces to a weighted mean. That is,
WOWAp;w(a1; : : : ; aN) D OWAw(a1; : : : ; aN);
when p D (1/N; : : : ; 1/N);

Aggregation Operators and Soft Computing
A
231
and
WOWAp;w(a1; : : : ; aN) D WMp(a1; : : : ; aN);
when w D (1/N; : : : ; 1/N):
As the OWA and the weighted mean generalize the arith-
metic mean, when the weights are all equal to 1/N, the
same happens with the WOWA. i. e.,
WOWAp;w(a1; : : : ; aN) D AM(a1; : : : ; aN);
when p D (1/N; : : : ; 1/N) and w D (1/N; : : : ; 1/N):
As stated here, the WOWA operator reduces to the OWA
and the weighted mean for appropriate weighting vec-
tors. This property permits the user to interpret the two
weighting vectors p and w used in the WOWA in terms
of the corresponding interpretation in the OWA and the
weighted mean. That is, p corresponds to the reliability
of the information sources and w permits to express the
importance of the values (with respect to their ordering).
Thus, the WOWA permits the user to express which are
the most reliable information sources (e. g., sensors) and
which are the values that have to be considered more rel-
evant (e. g. larger importance to small values in sensors
to avoid collisions, or larger importance to large values if
a high degree of compensation is permitted in multicrite-
ria decision making).
Fuzzy Integrals
Fuzzy integrals can be used in aggregation [14,25]. For-
mally, they integrate a function with respect to a fuzzy
measure. From the aggregation point of view, the func-
tion corresponds to the data to be aggregated and the
fuzzy measure corresponds to some background knowl-
edge (following the jargon in artiﬁcial intelligence) about
the reliability or importance of the information sources. In
this sense, the fuzzy measure can be seen as a generaliza-
tion of the weights as they are used for the same purpose.
The diﬀerence is that while the weights can only be used
for a single information source, the measures are applied
to sets of them.
In order to use the fuzzy integrals in aggregation we
need ﬁrst some formalization. First, we need to deﬁne
the set of information sources, let it be X D fx1; : : : ; xNg.
That is, X is the set of sensors in a robot or the set of cri-
teria in a multicriteria decision making problem. Then, we
need to express the datum (a number) supplied by source
xi. This is represented by a function f from X to R. That is,
f (xi) is the data supplied by sensor xi or the evaluation of
the criterion xi. In terms of the notation used above in this
work, f (xi) D ai. Then, the aggregation of the data sup-
plied by the information sources X can be computed as
the integral of the function f with respect to a fuzzy mea-
sure on X. The measure is a function deﬁned on the sub-
sets of X. So, for any set of information sources, we have
its weight.
Formally, a fuzzy measure is a set function on X into
[0; 1]. That is, a function from }(X) into [0; 1] or, in other
words, a function that assigns a value in [0; 1] to each sub-
set A of X. Furthermore, fuzzy measures are monotonic
with respect to set inclusion. Their deﬁnition is as follows.
A fuzzy measure  is a set function : }(X) ! [0; 1]
that satisﬁes:
 (;) D 0,
 (X) D 1,
 A  B implies (A)  (B).
When the measure of A is understood as its importance,
the last condition in this deﬁnition states that the more
sources we have, the larger their importance. Boundary
conditions (the ﬁrst two conditions) imply that the min-
imum importance (the one of the empty set) is zero and
that the maximum importance (the one of the full set) is
one. The boundary condition (X) D 1 is arbitrary; nev-
ertheless, from the point of view of the aggregation, it can
be justiﬁed as being similar to the one existing for the
weights in the weighted mean, and having a similar ratio-
nale. That is, the weights wi in a weighting vector add to
one: ˙iwi D 1. Note that such restriction is appropriate in
the case of the weighted mean because it implies that the
weighted mean satisﬁes unanimity (WM(a; : : : ; a) D a).
Note that any other arbitrary bound K (i. e., ˙iwi D K)
would just result into a weighted mean such that unanim-
ity does not hold but that WM(a; : : : ; a) D Ka.
Due to the usefulness of the fuzzy measures [14], sev-
eral families have been deﬁned and studied in the litera-
ture. Some of them are the following: Sugeno lambda mea-
sures, decomposable fuzzy measures, hierarchically de-
composable fuzzy measures, k-order additive fuzzy mea-
sures.
As said, fuzzy measures are used in combination with
fuzzy integrals. Several fuzzy integrals have been deﬁned
in the literature. We review a few of them below.
 Choquet integral [11]: The Choquet integral of a func-
tion f : X ! RC with respect to a fuzzy measure  is
deﬁned by:
CI(f ) D (C)
Z
f d
D
N
X
iD1

f

xs(i)

 f

xs(i1)



As(i)

:

232 A
Aggregation Operators and Soft Computing
where f

xs(i)

indicates that the indices have been
permuted so that 0  f xs(1)
      f xs(N)
  1,
where f

xs(0)

D 0 and where As(i)
D
fxs(i); : : : ;
xs(N)g.
The Choquet integral generalizes the arithmetic mean,
the weighted mean, the OWA operator and the
WOWA operator.
(a) It generalizes the arithmetic mean when we have
that the measure is proportional to the cardinality
of the set, i. e., when (A) D jAj/jXj, the Choquet
integral of f corresponds to the arithmetic mean of
ai D f (xi).
(b) Given a weighting vector w D (w1; : : : ; wN), if we
deﬁne a fuzzy measure as follows
(A) D
X
xi2A
wi ;
we have that the Choquet integral of f with respect
to  corresponds to the weighted mean of ai with
respect to w.
(c) Given a weighting vector w D (w1; : : : ; wN), if we
deﬁne a fuzzy measure  as follows
(A) D
jAj
X
iD1
wi ;
we have that the Choquet integral of f with respect
to  corresponds to the OWA of ai with respect
to w.
(d) Given weighting vectors w and p, we can deﬁne
a fuzzy measure in terms of the interpolating func-
tion w. Once such function is built, we deﬁne the
fuzzy measure  as follows:
(A) D w
0
@
jAj
X
iD1
wi
1
A ;
we have that the Choquet integral of f with respect
to such  corresponds to the WOWA of the ai with
respect to weighting vectors w and p.
 Sugeno integral [22]: The Sugeno integral of a function
f : X ! RC with respect to a fuzzy measure  is de-
ﬁned by:
SI(f ) D (S)
Z
f d D
N
max
iD1

min

f xs(i)
; As(i)

:
Here, s and As(i) are deﬁned as above.
Other fuzzy integrals exist. Among them, we have the
twofold integral that generalizes both Choquet and Sugeno
integrals. The generalization uses two fuzzy measures C
and S that correspond, respectively, to the measures used
in the Choquet and Sugeno integrals. The measure of the
Choquet integral has a probabilistic ﬂavor as the Choquet
integral generalizes the weighted mean, that can be seen as
the expectation of the ai with respect to the probability dis-
tribution w D (w1; : : : ; wN). Note that the weighting vec-
tor in the weighted mean is compatible with a probability
distribution because weights are positive and add to one.
Instead, the Sugeno integral has a fuzzy ﬂavor as it gen-
eralizes the weighted min. and the weighted max., which
are used in fuzzy systems. Such operators are deﬁned in
terms of weighting vectors that do not add to one but have
a maximum of one. So, they are like possibility distribu-
tions.
In short, the twofold integral integrates a function with
respect to two fuzzy measures C and S. We deﬁne such
integral below.
 Twofold integral [17,23]: The twofold integral of
a function f : X ! RC with respect to a fuzzy mea-
sure  is deﬁned by:
TIS;C(f ) D
N
X
iD1
 
i
max
jD1

min

f

xs(i)

; S

As(i)



C
As(i)
  C
As(iC1)
!
:
Here, s and As(i) are deﬁned as above. Naturally,
As(nC1) D ;.
When C D , the twofold integral reduces to the
Sugeno integral and when S D , the twofold in-
tegral reduces to the Choquet integral. Here,  rep-
resents ignorance and is deﬁned by (;) D 0 and
(A) D 1 when A ¤ ;. In addition, when C D
S D , we have that the twofold integral reduces
to the maximum.
Future Directions
Aggregation operators have been studied for a long time.
For example, the inequality
HM(x; y)  GM(x; y)  AM(x; y) ;
that involves the harmonic mean (HM), the geometric
mean (GM), and the arithmetic mean (AM) was already
known by Pappus of Alexandria (ﬂ. c. 300–c. 350). It is
given in his book Synagoge (book III). Since then, a large
number of aggregation operators have been proposed fos-
tered by the new applications and the theoretical studies.
From the practical point of view, current research is
oriented towards the embedding of such operators into
real applications. This has motivated the study and the de-

Aggregation Operators and Soft Computing
A
233
velopment of methods and techniques for parameter de-
termination. e. g., the development of methods that per-
mits a user to ﬁnd the appropriate parameters in a given
application. For example, methods have been deﬁned to
determine the weights in the weighted mean, and to de-
termine the fuzzy measure in a fuzzy integral. This is an
active line of research.
Another related topic is the selection of the appropri-
ate function. Now, the methods for parameter determi-
nation mainly presume that the appropriate operator is
known beforehand and that the only elements to be deter-
mined are the parameters. Tools and methods are needed
to help the users to select which is the appropriate func-
tion.
Finally, there is the need for developing architectures
for information integration. That is, architectures that en-
compass all the processes related with fusion: from the in-
put data (acquisition of the data) to the ﬁnal decision or
output.
All these aspects, oriented towards the real application
of aggregation operators are combined with new research
on the theoretical side. Such research includes the deﬁni-
tion of new functions (needed so that systems can work on
new types of data), the characterization of the functions
(needed so that we can know which are the properties of
the functions and so that a user can then select the appro-
priate one based on his/her problem requirements) and
the study of composite models using aggregation opera-
tors (i. e., hierarchical models that combine several aggre-
gation operators).
Bibliography
Primary Literature
1. Aczél J (1966) Lectures on functional equations and their ap-
plications. Academic, New York, London
2. Aczél J (1987) A short course on functional equations. Reidel,
Dordrecht
3. Arnold BC, Balakrishnan N, Nagaraja HN (1992) A first course in
order statistics. Wiley, New York
4. Arrow KJ (1951) Social choice and individual values, 2nd edn.
Wiley, New York
5. Arrow KJ, Sen AK, Suzumura K (eds) (2002) Handbook of social
choice and welfare. Elsevier, Amsterdam
6. Bajraktarevi´c M (1958) Sur une équation fonctionnelle aux
valeurs moyennes. Glasnik Mat Fiz I Astr 13(4):243–248
7. Barthelemy JP, McMorris FR (1986) The median procedure for
n-trees. J Classif 3:329–334
8. Bouyssou D, Marchant T, Pirlot M, Perny P, Tsoukiàs A, Vincke P
(2000) Evaluation and decision models: A critical perspective.
Kluwer’s International Series. Kluwer, Dordrecht
9. Bullen PS (2003) Handbook of means and their inequalities.
Kluwer, Dordrecht
10. Bullen PS, Mitrinovi´c DS, Vasi´c PM (1988) Means and their In-
equalities. Reidel, Dordrecht
11. Choquet G (1953) Theory of capacities. Ann Inst Fourier 5:131–
295
12. Fishburn PC, Rubinstein A (1986) Aggregation of equivalence
relations. J Classif 3:61–65
13. Fodor J, Roubens M (1994) Fuzzy preference modelling and
multicriteria decision support. Kluwer, Dordrecht
14. Grabisch M, Murofushi T, Sugeno M (2000) Fuzzy measures and
integrals: Theory and applications. Physica, Heidelberg
15. Hardy GH, Littlewood JE, Pólya G (1934) Inequalities, 2nd edn.
Cambridge University Press, Cambridge
16. Hirsch JE (2005) An index to quantify an individual’s scientific
research output. Proc Natl Acad Sci 102(45):16569–16572
17. Narukawa Y, Torra V (2004) Twofold integral and Multi-step
Choquet integral. Kybernetika 40(1):39–50
18. Mitchell HB (2007) Multi-sensor data fusion. An introduction.
Springer, Heidelberg
19. Pappus (1982) La collection mathématique. Librairie Scien-
tifique et Technique Albert Blanchard, Paris
20. Roy B (1996) Multicriteria methodology for decision aiding.
Kluwer, Dordrecht
21. Ruspini EH, Bonissone PP, Pedrycz W (1998) Handbook of fuzzy
computation. IOP, London
22. Sugeno M (1974) Theory of fuzzy integrals and its applications.
Ph D dissertation, Tokyo Institute of Technology, Japan
23. Torra V (1997) The weighted OWA operator. Int J Intell Syst
12:153–166
24. Torra V (2003) La integral doble o twofold integral: Una gen-
eralització de les integrals de Choquet i Sugeno. Butlletí de
l’Associació Catalana d’Intelligència Artificial 29:13–19. Pre-
liminary version in English: Twofold integral: A generalization
of Choquet and Sugeno integral. IIIA Technical Report TR-
2003-08
25. Torra V, Narukawa Y (2007) Modeling decisions: Information
fusion and aggregation operators. Springer, Heidelberg
26. Torra V, Narukawa Y (2008) The h-index and the number of
citations: Two fuzzy integrals. IEEE Trans Fuzzy Syst 16(3):
795–797
Books and Reviews
[26] gives a general description of the field of aggregation opera-
tors, it defines the main operators and discusses a few practi-
cal topics about their applications (e. g. parameter determina-
tion). (Calvo et al, 2002) is an edited book that contains state-
of-the-art chapter on different topics related with aggrega-
tion and fusion. A few properties on the aggregation operators
(mainly related with inequalities) can be found in the books by
Bullen [9] and Bullen, Mitrinovi´c and Vasi´c [10], and the excel-
lent book by Hardy, Littlewood and Pólya [34]. [14] is an edited
volume on fuzzy measures and fuzzy integrals.
[18] is an introduction to multisensor data fusion, a topic very much
related with aggregation operators
Alsina C, Frank MJ, Schweizer B (2006) Associative functions: Trian-
gular norms and copulas. World Scientific, Singapore
Calvo T, Mayor G, Mesiar R (2002) Aggregation operators. Physica,
Heidelberg
Pap E (2002) Handbook of measure theory, vols I, II. North-Holland,
Amsterdam

234 A
Air Traffic Control, Complex Dynamics of
Yager RR (1988) On ordered weighted averaging aggregation op-
erators in multi-criteria decision making. IEEE Trans Syst Man
Cybern 18:183–190
Air Traffic Control, Complex
Dynamics of
BANAVAR SRIDHAR, KAPIL SHETH
NASA Ames Research Center, Moﬀet Field, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Complex Network Analysis
Current US Air Traﬃc Network
Future Air Traﬃc Scenarios
Concluding Remarks
Bibliography
Glossary
Air traﬃc ﬂow Air Traﬃc Flow represents the dynamic
distribution of air traﬃc over a region of space. Air
traﬃc is undergoing major changes both in developed
and developing countries. The demand for air traﬃc
depends on population growth and other economic
factors. Air traﬃc in the United States is expected to
grow to 2 or 3 times the current levels of traﬃc in the
next few decades. An understanding of the characteris-
tics of the current and future ﬂows is essential to design
a good traﬃc ﬂow management strategy.
Traﬃc ﬂow management Safety limits the number of
aircraft arriving at an airport or in a region of the
airspace. Airspace Capacity, the maximum number of
aircraft in a region, depends on the technology to keep
aircraft separated by a safe distance and weather con-
ditions. Airspace capacity decreases in the presence of
severe weather and aircraft may have to be rerouted
or delayed on the ground to maintain safety. The im-
balance between airspace capacity and traﬃc ﬂow de-
mand leads to delays. Traﬃc ﬂow management tries
to maintain eﬃciency of the ﬂows while not exceeding
capacity limits.
Complex networks A network connects components of
a system. The connections and the number of compo-
nents vary with the function of the network. It is ex-
tremely diﬃcult to analyze and visualize the behavior
of networks when the number of components in the
system becomes large. Recently, there has been a ma-
jor advance in our understanding of the behavior of
networks with large number of components. Several
theories have been advanced about the evolution of
large biological and engineering networks by authors
in diverse disciplines like physics, mathematics, biol-
ogy and computer science.
Scale-free networks Several large biological and engi-
neering networks exhibit a scale-free property in the
sense that the probabilistic distribution of their nodes
as a function of connections decreases slower than an
exponential. These networks are characterized by the
fact that a small number of components have a dis-
proportionate inﬂuence on the performance of the net-
work. Scale-free networks are tolerant to random fail-
ure of components; but are vulnerable to selective at-
tack on components.
Definition of the Subject
Civil Aviation is a vital sector of the US economy. In 2004,
air transportation and related industries generated an out-
put of $1.37 trillion employing 12.3 million people in the
US [5]. The National Airspace System (NAS) refers to all
the hardware, software and people involved in managing
air traﬃc in the United States. More than 50 000 commer-
cial ﬂights operated in the US airspace alone on a typi-
cal day at the present time. Demand for air transportation
has seen a six-fold increase in the past 30 years and esti-
mates call for a strong average growth rate of 4.7% dur-
ing the next 20 years [1]. This increase in demand will
put a further strain on the airports and airspace resulting
in delays. Air Transport Association, a group represent-
ing airlines, estimated the cost of delays to airlines in 2005
at 5.9 billion dollars. To address the changes required in
the air transportation system, the US has created a multi-
agency Joint Planning and Development Oﬃce to lead the
required transformations. Similar activities are being pur-
sued in Europe and Asia [14].
Introduction
Air traﬃc in the United States continues to grow at
a steady pace except for a dip immediately after the tragic
events of September 11, 2001. There are diﬀerent growth
scenarios associated both with the magnitude and the
composition of the future air traﬃc. The Terminal Area
Forecast (TAF), prepared every year by the Federal Avia-
tion Administration (FAA), projects the growth of traﬃc
in the United States [4]. Both Boeing and Airbus publish
market outlooks for air travel annually. Although predict-
ing the future growth of traﬃc is diﬃcult, there are two sig-

Air Traffic Control, Complex Dynamics of
A
235
niﬁcant trends: (1) heavily congested major airports con-
tinue to see an increase in traﬃc, and (2) the emergence of
regional jets and other smaller aircraft with fewer passen-
gers operating directly between non-major airports. The
interaction between air traﬃc demand and the ability of
the system to provide the necessary airport and airspace
resources can be modeled as a network. The size of the
resulting network varies depending on the choice of its
nodes. It would be useful to understand the properties
of this network to guide future design and development.
Many questions, such as the growth of delay with increas-
ing traﬃc demand and impact of the en route weather on
future air traﬃc, require a systematic understanding of the
properties of the air traﬃc network.
Recently, there has been a major advance in the under-
standing of the behavior of networks with large number of
components. Several theories have been advanced about
the evolution of large biological and engineering networks
by authors in diverse disciplines like physics, mathemat-
ics, biology and computer science [21]. Several networks
exhibit a scale-free property in the sense that the prob-
abilistic distribution of their nodes as a function of con-
nections decreases slower than an exponential. These net-
works are characterized by the fact that a small number of
components have a disproportionate inﬂuence on the per-
formance of the network. Scale-free networks are tolerant
to random failure of components, but are vulnerable to se-
lective attack on components [2,17].
This paper examines two network representations for
the current air traﬃc system. A network deﬁned with the
40 major airports as nodes and with standard ﬂight routes
as links has a characteristic scale: all nodes have 60 or more
links and no node has more than 460 links. Another net-
work deﬁned with current aircraft routing structure ex-
hibits an exponentially truncated scale-free behavior. Its
degree ranges from 2 connections to 2900 connections,
and 217 nodes have more than 250 connections. Further-
more, those high-degree nodes are homogeneously dis-
tributed in the airspace. A consequence of this scale-free
behavior is that the random loss of a single node has little
impact, but the loss of multiple high-degree nodes (such
as occurs during major storms in busy airspace) can ad-
versely impact the system. Two future scenarios of air traf-
ﬁc growth are used to predict the growth of air traﬃc in the
United States. It is shown that a three-times growth in the
overall traﬃc may result in a ten-times impact on the den-
sity of traﬃc in certain parts of the United States.
The paper is organized as follows. The Sect. “Complex
Network Analysis” provides a brief overview and termi-
nology of the complex networks useful in the analysis of
air traﬃc. This is followed by an application of the com-
plex network methodology to the current air traﬃc system.
The Sect. “Future Air Traﬃc Scenarios considers diﬀerent
scenarios for the evolution of air traﬃc in the United States
during the next 25 years and looks for changes in the be-
havior of the network. An analysis of the future air traﬃc
scenarios shows that a three-times growth in overall traﬃc
can result in a tenfold impact on the density of highly con-
nected nodes in certain parts of the United States. Thus,
in addition to bottlenecks at major airports, the risk of
airspace congestion calls for route restructuring and the
introduction of new procedures and automation to in-
crease airspace capacity. Concluding remarks and possible
research in this area are discussed in the last section.
Complex Network Analysis
Complex systems have many agents or components inter-
acting with each other, and their collective behavior is not
a simple combination of the individual behavior. The pat-
tern of interaction between agents can be studied as a net-
work of connections between agents. Networks of many
types are pervasive in modern society, and scientists from
many ﬁelds are trying to broaden their understanding of
the behavior of the networks. A network is made up of ba-
sic components called either vertices or nodes. Each node
is connected to other nodes in the system. The line con-
necting two nodes is referred to as a link or an edge. An
edge is directed if it runs only in one direction and undi-
rected if it runs in both directions. Degree is the number
of edges connected to a node. Figure 1 shows a simple net-
work with nodes A, B, C and D and undirected edges con-
necting them. The degrees of nodes A, B, C and D are 3, 2,
2 and 1, respectively.
The structure of the network strongly inﬂuences the
functions performed by the network. It is possible to ana-
lyze networks of small sizes (less than 100) by drawing the
picture of the network and analyzing its properties. The
number of nodes in a complex engineering system, such
as the worldwide web [10], can easily be as large as a few
million nodes. The behavior of Random Graphs, graphs
Air Traffic Control, Complex Dynamics of, Figure 1
Nodes and edges in a network

236 A
Air Traffic Control, Complex Dynamics of
where nodes are connected to other nodes randomly, as
the number of nodes becomes large has been studied ex-
tensively by mathematicians [13]. The behavior of Regu-
lar Networks, idealized systems where each component is
identical, has been studied by several authors [17]. Net-
works describing real systems are neither random nor reg-
ular. The ability to model real systems and capture the be-
havior of key variables is extremely diﬃcult. Recent de-
velopments in complex networks examine the statistical
properties of large networks and help answer questions
about the dynamics and stability of such networks. How-
ever, studies on the eﬀects of structure on system behavior
are still in their early stages.
A major area of interest is the role played by the dis-
tribution of the degree of nodes in a network. Let pk be
the fraction of nodes in the network that has degree k. The
degree distribution for the network can be computed by
generating a histogram of pk. The degree distribution for
large random graphs, where each edge is present or absent
with equal probability, can be modeled as a Poisson distri-
bution. The distribution tends to peak for a small value of k
and decays exponentially for large values of k. When using
real data, to reduce the eﬀect of noise in small datasets, the
distribution of degree is expressed in terms of the comple-
mentary cumulative distribution function (ccdf), Pk, the
probability of the number of nodes in the graph with de-
gree greater than k. Pk is computed using the expression
Pk D
1
X
iDk
pi :
Figure 2 shows the ccdf of the Poisson distribution,
ek/k!, for  D 4. Figure 3 shows the ccdf for the Pois-
son distribution on a logarithmic scale.
It has been observed that the degree distributions for
some real-world networks, such as the Internet and bio-
logical networks, are highly skewed with tails several times
longer than the mean. These real networks have a small
number of nodes, or hubs, with a high level of connectiv-
ity. Hubs play an important role in inﬂuencing the proper-
ties of the network. Real networks exhibiting a small num-
ber of hubs are referred to as “scale-free” networks [6]. The
distributions of degree for a number of networks, e. g., In-
ternet, World Wide Web, collaboration network of math-
ematicians, etc., show a power law in their tails: Pk  k˛
for some constant value of ˛. Figure 4 shows the ccdf for
˛ D 2. Power law distributions (Fig. 4) appear linear in
a logarithmic scale (Fig. 5).
The appearance of hubs in scale-free networks is ex-
plained in terms of two behaviors both in people and net-
works, namely, real systems-growth and preferential at-
Air Traffic Control, Complex Dynamics of, Figure 2
Poisson distribution
Air Traffic Control, Complex Dynamics of, Figure 3
Poisson distribution in logarithmic scale
Air Traffic Control, Complex Dynamics of, Figure 4
Power law distribution

Air Traffic Control, Complex Dynamics of
A
237
Air Traffic Control, Complex Dynamics of, Figure 5
Power law distribution in logarithmic scale
tachment. There is a tendency for new growth to gravitate
towards desirable locations. Generally, all the desirable lo-
cations already have some existing nodes. The preferential
attachment mechanism leads to the creation of more pow-
erful nodes or hubs. The various hubs compete to attract
new nodes and absorb them, resulting in an increase of the
number of links in the initial hubs.
Scale-free networks with power law distributions ex-
hibit two important properties: (a) remarkable resistance
to random failure of nodes and (b) extreme vulnerability
to targeted attacks. The functionality and performance of
the network depends on the existence of the edges between
pairs of nodes. The distance between the remaining ver-
tices gets longer as nodes are removed from a network and
the removal of a certain number of nodes may result in
a collapse of the entire system. The tolerance of networks
to failures or removal of nodes varies with their level of re-
silience. The nodes from a network could be removed ran-
domly or in a coordinated approach. Several studies have
shown that scale-free networks are generally robust to ran-
dom removal of nodes. However, they are less tolerant to
the selective removal of hubs.
The next section will examine the air traﬃc in the
United States to see if it exhibits the properties of a scale-
free network and draw analogies from the attack vulnera-
bilities of other complex networks. The methodology can
also be used to understand the properties of the Next Gen-
eration Air Transportation System as it evolves over the
next few decades [18].
Current US Air Traffic Network
The National Airspace System (NAS) refers to the collec-
tion of hardware, software and people, including runways,
radars, networks, FAA, airlines, etc., involved in Air Traf-
ﬁc Management (ATM) in the US. It has been pointed out
that the NAS should be modeled as a complex adaptive
system [12]. The NAS can be looked at as a network at
several diﬀerent levels [11]. Some examples of networks
involving components of the NAS are the route network of
an airline connecting diﬀerent airports, a network of sec-
tors (geographical region used to monitor safe separation
between aircraft) interconnected by geographical proxim-
ity, network of airline crew located in diﬀerent cities and
a surface network of ramps, runways and departure gates
at an airport. The network components and links vary with
the planning interval and the modeling problem. The anal-
ysis of current traﬃc uses data from 2006 and constructs
two diﬀerent types of networks for a typical day of oper-
ations in the NAS. The analysis is based on actual traﬃc
for a day in July 2006. The traﬃc data are processed using
Future Air traﬃc management Concept Evaluation Tool
(FACET) simulation software [19] for conducting the net-
work ﬂow analysis.
The major distinctions regarding the two air traﬃc
networks are described in Table 1. The nodes in the Air-
port Network (AN) correspond to the 40 major airports
in the US. Today, a set of predeﬁned alternative routes are
used for ﬂying between particular city pairs. Each node in
network AN is connected to another node through one or
more routes. The Airport and Airspace Network (AAN)
includes all airports in the US and routes connecting these
airports. A route between two airports is deﬁned by a se-
ries of geographical positions or ﬁxes in the airspace. Each
ﬁx together with the airports represent a node in the AAN
network.
Computation of the Degree of a Node
and the Distribution Function
The computation of the degree of a node and the distri-
bution function is illustrated by an example. The FACET
simulation software generates the departure airport, ar-
rival airport and the ﬁxes through which the aircraft trav-
els en route based on air traﬃc data. Figure 6 shows the in-
tersection of diﬀerent routes connecting airports through
common ﬁxes. In the ﬁgure, aircraft traveling from Seat-
tle (SEA) to Atlanta (ATL), aircraft traveling from Oak-
land (OAK) to Newark (EWR), and aircraft traveling from
Los Angeles (LAX) to Boston (BOS) pass through the ﬁx
Garden City, Kansas (GCK). The degree of GCK is equal
to the total number of aircraft traveling along these three
routes during the day. The degree of node GCK is three
if a single aircraft travels between each of these three pairs

238 A
Air Traffic Control, Complex Dynamics of
Air Traffic Control, Complex Dynamics of, Table 1
Different types of Air Traffic Management networks
Network
Nodes
Number of Nodes Edges
Airport Network (AN)
40 major airports
40
Routes between 40 major airports
Airport and Airspace Network (AAN) All airports and fixes along
routes connecting the airports
8170
Routes between all airports
Air Traffic Control, Complex Dynamics of, Figure 6
Example of intersection of routes at common fixes
of airports. Similarly, aircraft traveling from Dallas (DFW)
to Philadelphia (PHL) and Seattle to Atlanta pass through
the ﬁx Memphis, TN (MEM). The degree of ﬁx MEM is
two if a single aircraft travels between each of these city
pairs. The distribution and the ccdf can be computed eas-
ily given the degree associated with all the airports and en
route ﬁxes and are described in the next section.
Behavior of the Degree of Nodes
in Air Traﬃc Management Network
The ccdf for the nodes in network AN is shown in Fig. 7.
The ﬁgure shows the probability that an airport in the net-
work has more than a certain number of routes originat-
ing or terminating at the airport. The number of connec-
tions at all airports exceeds 60 and Chicago O’Hare airport
has the maximum number (460) of connections. The dis-
tribution of nodes in the airport network does not show
scale-free behavior. The airport connection distribution in
the U.S is similar to the distribution of the connections be-
tween the networks of world airports [3].
Figure 8 shows the ccdf of the nodes in network AAN.
The ccdf of the nodes in network AAN shows a rapid decay
of nodes with the degree of the node. However, it has 217
nodes with more than 250 links, indicative of high volumes
Air Traffic Control, Complex Dynamics of, Figure 7
Cumulative distribution function for nodes (40 major airports) in
network AN
Air Traffic Control, Complex Dynamics of, Figure 8
Distribution of nodes in AAN with current traffic
of traﬃc through these nodes. Figure 9 shows the logarith-
mic behavior of the ccdf of the nodes in network AAN.
The nodes of network AAN initially follow the power
law curve similar to several scale-free networks [17]. How-
ever, towards the end of the tail, the distribution shows
a deviation from the power law behavior indicating a limit
on the number of nodes that have a high number of con-
nections. The limit on the growth of the large hub nodes,
in the distribution of the ATM network, is due to the con-

Air Traffic Control, Complex Dynamics of
A
239
Air Traffic Control, Complex Dynamics of, Figure 9
Logarithmic behavior of nodes in network AAN
straints imposed on traﬃc demand by the location of the
cities, economic development and government policies.
The existence of hub and secondary airports charac-
terizes current air traﬃc operations [8]. The network anal-
ysis provides an additional ability to study traﬃc behav-
ior in the en route airspace. Figure 10 shows geographi-
cal distribution of the nodes with degree higher than 250
in network AAN. These nodes will be referred to as 250G
nodes. 250G nodes represent 2.7% of the total nodes in the
network. Table 2 shows the number of 250G nodes in dif-
ferent traﬃc control regions, referred to as Centers. The
Centers, Chicago (ZAU), Boston (ZBW), Atlanta (ZTL),
Kansas City (ZKC), Washington, DC (ZDC), Indianapo-
lis (ZID), Jacksonville (ZJX), Los Angeles (ZLA), Cleve-
land (ZOB) and New York (ZNY), each have more than
ten of the 250G nodes. The Centers in the western part
of the United States have fewer 250G nodes indicative
of the lower traﬃc density in these Centers. It is infor-
mative to express the distribution of the 250G nodes in
units of number of nodes per 10,000 square nautical miles
(10 Ksqnm). Using this measure of nodal traﬃc density,
ZNY has the highest nodal density with six 250G nodes
per 10 Ksqnm. Table 3 shows the nodal traﬃc density for
the 20 Centers in the continental US.
The behavior of the degree of the nodes in an ATM
network should not be surprising, since the ATM network
has evolved to serve population densities in the US. Net-
work analysis helps to visualize and quantify the charac-
teristics of the network.
Resilience of Air Traﬃc Management Networks
The tolerance of complex networks to random and tar-
geted failures depends on their network structure. As ob-
Air Traffic Control, Complex Dynamics of, Figure 10
Geographical distribution of 250G nodes in network AAN with
current traffic and severe weather polygons
served earlier, a scale-free network is tolerant to random
failure, since the hubs are few and the chance of a hub be-
ing selected randomly is low. However, the same network
may be prone to targeted attacks on a small percentage of
vital nodes. In air traﬃc management networks, weather
can be regarded as an agent of attacks on the system.
Convective weather is a major source of uncertainty in
ATM networks. One eﬀect of severe weather is to make
airspace unavailable for the ﬂow of air traﬃc. The re-
moval of airspace may result in the failure of hubs and
increase the average path length in the network. The im-
pact of weather on ATM system performance appears as
delay [20]. The tolerance of the ATM network to weather
depends on the geographical distribution of the weather
and the coverage of nodes with high degree.
Future Air Traffic Scenarios
National and international projections of traﬃc growth
indicate a tripling of passengers by 2025 [7]. There may
be increased traﬃc due to the growing presence of on-
demand air taxis and unmanned air vehicles. It is esti-
mated that 5000 micro jets may be operational by 2010
and 13 500 by 2022 [22]. The TAF provides forecasts for
airports in the NAS. The forecast for the major airports
in the United States receives more detailed modeling that
takes into account local economic conditions and airline
costs. TAF is the basis for most aviation demand fore-
casts. The TAF data published in March 2006 provides
traﬃc growth rates for the period 2004–2020. The AvDe-
mand model [16] provides three times (3X) current traﬃc
scenarios starting with TAF and assuming slightly diﬀer-
ent conditions for traﬃc growth beyond 2025. The Trans-
portation System Analysis Model (TSAM) [23] predicts

240 A
Air Traffic Control, Complex Dynamics of
future traﬃc demand based on demographics and popu-
lation at the county level. It provides a complimentary al-
ternative to the FAA forecast. The results in this study are
based on data generated by AvDemand.
Two future scenarios are considered. These two sce-
narios and the current traﬃc provide a baseline to com-
pare the impact of future traﬃc changes on the ATM sys-
tem. The initial values of aircraft routes and schedules used
in the extrapolation are based on traﬃc data for a day
in May 2002. Assuming traﬃc growth from TAF and the
nominal traﬃc schedule, Fratar algorithm [23] can be used
to create future daily total number of ﬂights between each
origin and destination pair of the current schedule. The
traﬃc growth scenarios using TAF airport growth out to
2025 predict a future NAS-wide traﬃc growth by a factor
of 1.49. To achieve 3-times current day traﬃc (3X) scenar-
ios assumed in future planning [9], consider two diﬀerent
scenarios for post-2025 traﬃc growth. The compound ex-
trapolation approach grows traﬃc until it reaches 3X by
assuming the TAF airport growth rates for traﬃc beyond
2025. As an alternative, the homogeneous extrapolation
approach assumes the same growth rate at all airports until
the traﬃc level reaches 3X. These two scenarios will be re-
ferred to as 3X Compound (3XC) and 3X Homogeneous
(3XH) scenarios in the rest of the chapter. The scenar-
ios are processed to compute the changes in the network
properties of future ATM systems.
Behavior of Future ATM Networks
The two scenarios described earlier can be used to study
the characteristics of the 3X traﬃc under the two assump-
tions. The properties of future ATM networks can be de-
rived similar to the properties of the current ATM net-
works. The computations can be used to compare the ge-
ographical distribution of the hubs compared to the dis-
tribution today. Figures 11 and 12 show the distribution
of the nodes in the AAN network for the 3XC traﬃc sce-
nario. The results are similar for the 3XH traﬃc scenario.
The future ATM networks, under both scenarios, show
exponentially truncated scale-free behavior similar to the
current ATM network. The distribution of the hubs will
have an impact on the growth of delay in future ATM
networks subjected to severe weather. The vulnerability
of the network to reduction in capacity caused by certain
weather patterns will be signiﬁcant and disruptive to the
operation of the system. The impact of the weather on fu-
ture ATM networks will be described subsequently in the
paper.
The geographical distribution of the 250G nodes under
3XC is shown in Fig. 13. Table 2 shows the same distribu-
Air Traffic Control, Complex Dynamics of, Figure 11
Distribution of nodes in AAN using 3X Compound Scenario
Air Traffic Control, Complex Dynamics of, Figure 12
Logarithmic behavior of nodes in network AAN under 3X Com-
pound traffic
Air Traffic Control, Complex Dynamics of, Figure 13
Geographical distribution of 250G nodes in network N5 with 3X
Compound traffic and severe weather polygons

Air Traffic Control, Complex Dynamics of
A
241
Air Traffic Control, Complex Dynamics of, Table 2
Distribution of 250G nodes by Centers under current and future
traffic scenarios
Center Current 3XH
3XC
ZAB
7
70
78
ZAU
15
67
70
ZBW
16
77
87
ZDC
22
134
143
ZDV
8
51
68
ZFW
3
59
68
ZHU
8
60
72
ZID
15
71
71
ZJX
14
71
81
ZKC
11
58
50
ZLA
16
102
119
ZLC
5
29
40
ZMA
4
44
54
ZME
10
38
42
ZMP
6
46
53
ZNY
14
80
88
ZOA
10
57
67
ZOB
13
90
89
ZSE
5
35
41
ZTL
15
59
62
Total
217
1298 1443
tion by Centers for current traﬃc and 3X scenarios. An
examination of Table 2 is helpful in drawing distinctions
between current day traﬃc and 3X demand traﬃc. The 3X
traﬃc demand creates close to a six-fold increase in the
250G traﬃc nodes. Earlier, it was noted that ZNY has the
highest 250G traﬃc nodal density. Under the 3X demand,
as can be seen from Table 3, ZNY nodal density of 2006 is
equaled or exceeded by a majority of the 20 Centers. Even
more alarmingly, the nodal density of 250G nodes in ZNY
is 6 times the current value, and Cleveland and Washing-
ton Centers have twice the nodal density of ZNY in 2006.
The 3X traﬃc also gives rise to nodes with even more con-
nections. The hubs, 500G, 750G and 1000G, are deﬁned
similarly to the 250G hub. The number of 250G, 500G,
750G and 1000G hubs under diﬀerent traﬃc conditions
are shown in Table 4.
Impact of Weather on ATM Networks
Another way to view the growth of traﬃc is to compare
how similar weather patterns may aﬀect current traﬃc and
3X traﬃc. The geographical distribution of the 250G nodes
shown in Fig. 13 is based on traﬃc during July 2, 2006.
This was a calm weather day with a total NAS aggregate
Air Traffic Control, Complex Dynamics of, Table 3
Nodal density of 250G nodes by Centers under current and fu-
ture traffic scenarios
Center Area
1X
3XH 3XC
ZAB
18.04 0.39
3.88
4.32
ZAU
7.60 1.97
8.82
9.21
ZBW
11.62 1.38
6.63
7.49
ZDC
12.69 1.73 10.56
11.27
ZDV
20.18 0.40
2.53
3.37
ZFW
12.32 0.24
4.79
5.52
ZHU
27.64 0.29
2.17
2.61
ZID
7.07 2.12 10.04
10.04
ZJX
14.55 0.96
4.88
5.57
ZKC
13.37 0.82
4.34
3.74
ZLA
13.55 1.18
7.53
8.78
ZLC
31.57 0.16
0.92
1.27
ZMA
28.66 0.14
1.53
1.88
ZME
10.75 0.93
3.53
3.91
ZMP
28.82 0.21
1.60
1.84
ZNY
2.43 5.76 32.92
36.21
ZOA
13.54 0.74
4.21
4.95
ZOB
6.74 1.93 13.36
13.21
ZSE
19.64 0.25
1.78
2.09
ZTL
9.18 1.63
6.43
6.75
Total
309.96 0.70
4.19
4.66
Air Traffic Control, Complex Dynamics of, Table 4
Total number of nodes for different traffic scenarios
Types of nodes Current 3XH
3XC
250G
217
1312 1468
500G
72
620
806
750G
33
304
452
1000G
22
165
262
delay of 11 997 minutes [15]. The traﬃc on July 2, 2006
will be assumed as traﬃc unaﬀected by weather in this dis-
cussion [20].
Whenever there is severe weather in the NAS, airspace
capacity is reduced and traﬃc is rerouted or held on the
ground causing delays in the system. The Collaborative
Convective Forecast Product (CCFP) is a model of severe
weather activity and the areas marked blue in Fig. 10 shows
the CCFP for July 13, 2006. On July 13, 2006 the NAS
experienced a signiﬁcant total delay of 219 350 minutes
with ZNY, ZDC and ZTL Centers contributing a delay of
116 654, 49 400 and 23 822 minutes respectively. Next, as
shown in Fig. 13, the same severe weather is overlaid in
blue polygons on the geographical distribution of the 250G

242 A
Air Traffic Control, Complex Dynamics of
Air Traffic Control, Complex Dynamics of, Table 5
Total number of nodes affected by severe weather
Types of Nodes 1X 3XH 3XC
250G
34
191
207
500G
9
96
118
750G
4
49
67
1000G
3
23
41
nodes under 3XC demand. Table 5 shows hub nodes af-
fected by the weather today and in the future.
A greater number of hubs, between six to ten times,
are aﬀected by the same weather pattern under future traf-
ﬁc scenarios than today. If one considers the non-linear
growth of delay, in regions such as ZNY, ZDC and ZTL
where the demand on the airspace is close to capacity, the
increased density of high traﬃc nodes in these regions will
result in much larger delays compared to 2006.
Concluding Remarks
Air traﬃc in the United States can be modeled as a network
to understand the impact of predicted growth in demand
on the performance of the system. It is demonstrated that
the air traﬃc network with current en route ﬂight plan
intersections as nodes and with the ﬂight plans as links
shows scale-free properties typical of several large engi-
neering and biological networks. A consequence of this
property is the non-linear growth of traﬃc in certain re-
gions of the United States. A preliminary analysis indi-
cates that a three-times growth in the overall traﬃc may
result in a ten-times impact on the density of traﬃc in
certain parts of the United States. The air traﬃc system
currently experiences signiﬁcant delay during periods of
severe weather activity. The impact of weather of same
severity will be magniﬁed several times, especially in the
northeastern parts of the United States, leading to lower
system performance. Research must be conducted to de-
termine whether this risk can be mitigated through re-
structuring routes or by introducing new operational con-
cepts, such as automation assistance to controllers to in-
crease airspace capacity. The network analysis described
in the chapter can be used to guide the development of
various traﬃc ﬂow management concepts to increase the
eﬃciency of air traﬃc systems.
Bibliography
1. Airbus SAS (2002) Airbus Global Market Forecast 2001–2002.
Reference CB 390.0008/02
2. Albert R, Jeong H, Barabasi AL (2000) Error and attack tolerance
of complex networks. Nature 406:378–382
3. Amaral LAN, Scala A, Barthelemy M, Stanley HE (2000) Classes
of Small-world Networks. Proc Natl Acad Sci USA 97:1149–
1152
4. Anonymous (2005) Terminal Area Forecast Summary, Fiscal
Years 2004–2020. Report No. FAA-APO-05-1, US Department
of Transportation, Federal Aviation Administration
5. Anonymous (2006) Commercial Aviation and the American
Economy. The Campbell-Hill Aviation Group Inc, Washing-
ton DC
6. Barabasi AL, Bonabeau E (2003) Scale-Free Networks. Sci Am
288:60–69
7. Boeing (2007) Current Market Outlook. http://www.boeing.
com/commercial/cmo Accessed June 26, 2007
8. Bonnefoy PA, Hansman RJ (2004) Emergence and Impact of
Secondary Airports in the United States. 4th AIAA Aviation
Technology, Integration and Operations Conference, Chicago,
IL, September 2004
9. Borener S, Carr G, Ballard D, Hasan S (2006) Can NGATS Meet
the Demands of the Future? J Air Traffic Control 48(1):34–38
10. Broder A, et al (2000) Graph structure in the web. Comput Netw
33:309–320
11. DeLaurentis D, Han E, Kotegawa T (2006) Establishment of
a Network-based Simulation of Future Air Transportation Con-
cepts. 6th AIAA Aviation Technology, Integration and Opera-
tions Conference, Wichita, KS, September 2006
12. Donohue G (2003) Air Transportation is a complex Adap-
tive System: Not an Aircraft Design. AIAA/ICAS International
Air and Space Symposium and Exposition, Dayton, Ohio,
July 2003
13. Erdos P, Renyi A (1960) On the Evolution of Random Graphs.
Publ Math Inst, Hung Acad Sci 5:17–61
14. Eurocontrol (2007) An Assessment of Air Traffic Management
in Europe during the Calender Year 2006: Performance Review
Report. Eurocontrol, Brussels, Belgium
15. Federal Aviation Administration (2004) Order 7210.55C: Op-
erational Data Reporting Requirements. US Department of
Transportation
16. Huang AS, Schleicher D, Hunter G (2006) Future Flight demand
Generation Tool. 6th AIAA Aviation Technology, Integration
and Operations Conference, Wichita, KS, September 2006
17. Newman MEJ (2003) The structure and function of complex
networks. SIAM Rev 45:167–256
18. Pearce RA (2006) The next generation air transportation sys-
tem: Transformation starts now. J Air Traffic Control 48(1):
7–10
19. Sridhar B, Sheth K, Smith P, Leber W (2005) Migration of FACET
from Simulation Environment to Dispatcher Decision Support
System. Proceedings of Digital Avionics Systems Conference,
Washington, DC, November 2005
20. Sridhar B, Swei SM (2006) Relationship between Weather, Traf-
fic and Delay Based on Empirical Methods. 6th AIAA Aviation
Technology, Integration and Operations Conference, Wichita,
KS, September 2006
21. Strogatz SH (2001) Exploring complex networks. Nature
410:268–276
22. Teal Group (2003) World Unmanned Aerial Vehicle Systems.
Market Profile and Forecast, Fairfax
23. Viken J, et al (2006) NAS Demand Predictions, Transportation
Systems Analysis Model (TSAM) Compared with Other Fore-
casts. 6th AIAA Aviation Technology, Integration and Opera-
tions Conference, Wichita, KS, September 2006

Algorithmic Complexity and Cellular Automata
A
243
Algorithmic Complexity
and Cellular Automata
JULIEN CERVELLE1, ENRICO FORMENTI2
1 Laboratoire d’Informatique de l’Institut
Gaspard–Monge, Université Paris-Est,
Marne la Vallée, France
2 Laboratoire I3S,
Université de Nice-Sophia Antipolis,
Sophia Antipolis, France
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Kolmogorov Complexity
Dynamical Systems and Symbolic Factors
Cellular Automata
Algorithmic Complexity as a Demonstration Tool
Measuring CA Structural Complexity
Measuring the Complexity of CA Dynamics
Future Directions
Acknowledgments
Bibliography
Glossary
Algorithmic complexity of object x shortest
program
for outputting a description for x (w.r.t. a universal
representation system)
Equicontinuity all points are equicontinuity points (in
compact settings)
Equicontinuity point a point for which the orbits of
nearby points remain close
Expansivity from two distinct points orbits eventually
separate
Incompressible word a word for which the shortest pro-
gram outputting it has “almost” the same length as the
word itself
Injectivity the next state function is injective
Kolmogorov complexity see “algorithmic complexity”
Rich conﬁguration a conﬁguration that contains all pos-
sible ﬁnite patterns over a given alphabet
Sensitivity to initial conditions for any point x there ex-
ist arbitrary close points whose orbits eventually sepa-
rate from the orbit of x
Surjectivity the next state function is surjective
Transitivity there always exist points that eventually
move from any arbitrary neighborhood to any other
Definition of the Subject
In the last 10 years the ﬁeld of complex systems has en-
joyed astonishing development with meaningful applica-
tions in most scientiﬁc domains.
Cellular automata (CA) are a very used model for com-
plex systems characterized by a multitude of small identi-
cal agents capable of building a complex behavior on the
basis of local interactions.
The huge variety of CA dynamical behaviors has been
popularized since the early 1980s by the work of S. Wol-
fram (see [18] for an exhaustive review).
Later, researchers started a systematic study of CA.
Most of the results are summarized in the chapters
of this book (see ▷Chaotic Behavior of Cellular Au-
tomata, ▷Dynamics of Cellular Automata in Non-com-
pact Spaces, ▷Topological Dynamics of Cellular Au-
tomata and ▷Ergodic Theory of Cellular Automata for
instance).
However, many questions seem to be intractable
and vanquisch researchers’ eﬀorts (some such eﬀorts
are reported by ▷Chaotic Behavior of Cellular Au-
tomata, ▷Dynamics of Cellular Automata in Non-com-
pact Spaces, ▷Topological Dynamics of Cellular Au-
tomata and [14]). We strongly believe that Kolmogorov
complexity can be a valuable tool in the quest for answers
in this domain.
Introduction
Kolmogorov (or algorithmic) complexity was introduced
to precisely formalize the notion of “randomness.” For-
mer deﬁnitions of randomness were based on probabilistic
concepts: for instance, a sequence of 0 and 1 is random if
it is obtained by repeated coin tosses.
The drawback of such a deﬁnition is that it does not
formally specify what is and what is not a random se-
quence since all sequences (of a ﬁxed length) have the same
probability to be obtained through unbiased coin tosses.
However, if one considers the following two sequences:
00000000000000000000000000000
10010010010111010110101110101
one would say that the ﬁrst one is not random while the
second one is.
The idea of Kolmogorov complexity comes from this
simple reasoning. What makes the ﬁrst sequence sim-
ple is that it can brieﬂy be described by the sentence
“twenty-nine zeros,” while the second cannot be described
in a shorter way than by describing it literally: “the se-
quence one zero zero one zero zero one zero zero one

244 A
Algorithmic Complexity and Cellular Automata
zero one one one zero one zero one one zero one zero one
one one zero one zero one.” If we use regular expressions
instead of English as a description language, the ﬁrst se-
quence is 029 and the second is itself.
A ﬁrst idea to formalize this process is to say that
the complexity of a sequence is the length of its com-
pressed form for a given lossless compressor. The issue
of such a deﬁnition is that there is no universal com-
pression tool as compression usually depends on the type
of data (sound, picture, text). However, Kolmogorov and
Solomonoﬀproved that there exists a universally op-
timal compressor that compresses sequences better up
to some additive constant w.r.t. any other compressor
(Theorem 1).
Kolmogorov complexity is not computable, but its
combinatorial properties make it a useful demonstration
tool. The major technique in Kolmogorov complexity is
the incompressibility method, and it is brieﬂy reviewed in
Sect. “Algorithmic Complexity as a Demonstration Tool”.
For more on the subject see [13].
On the one hand, Kolmogorov complexity allows one
to give a formal context to study randomness in discrete
dynamical systems; on the other hand, it is a helpful tool
for decreasing the combinatorial complexity of problems.
Both of these aspects of Kolmogorov complexity will ﬁgure
prominently in this chapter.
Kolmogorov Complexity
In this section we give the formal deﬁnition of Kol-
mogorov complexity and its main properties. For simplic-
ity sake we restrict ourselves to the alphabet {0,1} although
Kolmogorov complexity can be deﬁned on more general
alphabets and even on integers [13].
Let f0; 1g be the set of words on f0; 1g, jwj the length
of word w and for 0 6 i < jwj, wi the ith letter of w (in-
dexed from 0), and " the empty word.
A partial computable function is a computable func-
tion that is deﬁned on a subset of f0; 1g (being unde-
ﬁned on an input means that the program that computes
the function does not halt). Denote by D' the set of in-
puts on which ' is deﬁned. The composition of partial
computable functions f and g is the partial computable
function f ı g deﬁned on the set fx 2 Dg; g(x) 2 D f g by
f ı g(x) D f (g(x)). If f is injective, the inverse of f is the
partial computable function f 1 deﬁned by
f 1(w)
D
(
x the unique word such that f (x) D w if it exists;
undeﬁned otherwise :
If f is computable, then f 1 is computed by the algorithm,
which tries all possible x and halts when it ﬁnds one whose
image is w. If no such x exists, then the program does not
halt.
A representation system is a partial computable func-
tion ' from (f0; 1g)2 to f0; 1g. It plays the role of a de-
compressor program used to build words from their com-
pressed form. There are no requirements, and, in particu-
lar, it is not mandatory either that each word have a com-
pressed form (i. e., the image set of ' need not be f0; 1g)
or that a word have only one compressed form (i. e., ' need
not be injective).
Deﬁnition 1 (Kolmogorov complexity given a represen-
tation system) Let ' be a representation system. The Kol-
mogorov complexity given ' of a word w knowing v, de-
noted by K'(wjv), is deﬁned by the length of the shortest
word y such that '(y; v) D w or C1 if such a word does
not exist:
K'(wjv) D minfjyj; y 2 f0; 1g s.t. '(y; v) D wg
with the convention min ; D 1 :
If y is such that '(y; v) D w, we say that y is a pro-
gram for w knowing v. If y is the shortest word such
that '(y; v) D w, we say that y is the shortest program
for w knowing v. The Kolmogorov complexity given ' of
a word w, denoted K'(w), is deﬁned by
K'(w) D K'(wj") :
If y is such that '(y; ") D w, we say that y is a program
for w. If y is the shortest word such that '(y; ") D w, we
say that y is the shortest program for w.
In the previous deﬁnition, the name program can be a little
confusing since ' seems to be the program and y seems to
be its input. This name comes from the additively optimal
universal representation system introduced in Theorem 1,
where the input is a program for the universal Turing
machine.
As stated in the introduction, this deﬁnition lacks ro-
bustness since it depends on a particular (de)compressor.
However, the following results prove that we can ﬁnd
a best compressor in order to deﬁne a robust notion of
complexity.
Deﬁnition 2 (Additively optimal universal representa-
tion system) A representation system '0 is additively op-
timal universal if for any representation system ' there ex-
ists a constant c such that for all w and v in f0; 1g
K'(wjv) C c > K'0(wjv) :
(1)

Algorithmic Complexity and Cellular Automata
A
245
This deﬁnition means that a universal representation sys-
tem compresses better than any other compression system
up to some additive constant.
Remark 1
The constant in Inequality (1) is mandatory
since a representation system cannot be better than any
other one because of very specialized compressors: for any
word w, the representation system ıw by ıw(y; v) D w
for all words y and v is one of the best for word w (i. e.,
Kıw(wjv) D 0).
Proposition 1
Let '0 and '1 be two additively optimal
universal representation systems. Then, there is a constant c
such that for all words w and v
jK'0(wjv)  K'1(wjv)j < c :
Proof
From the deﬁnition of additively optimal univer-
sal representation system, there are constants c0 and c1
such that for all words w and v and for all representation
systems '
K'(wjv)Cc0 > K'0(wjv) and K'(wjv)Cc1 > K'1(wjv):
The thesis is obtained by choosing c D maxfc0; c1g.

In order to deﬁne completely Kolmogorov complexity,
one needs an additively optimal universal representation
system. The key idea for conceiving such a system is to
embed the program for decompression in the compressed
form.
In order to join the decompressing program and the
compressed word in one string, we need a special com-
bining function. We denote by w2 a word u of length
2jwj such that for all 0 6 i < jwj, u2i D u2iC1 D wi,
which is word w, where all letters are repeated twice.
Deﬁne
p ˇ w D p201w :
For
instance,
if
p D 001
and
w D 1001011,
then
p ˇ w D 000011 01 1001011.
This way of combining words has two interesting
properties. First, from p ˇ w a program can compute p
and w since, while all bits are repeated twice, we read p,
and after skipping the 01, we read w. Second, it holds that
jp ˇ wj D 2jpj C 2 C jwj :
This last equation is fundamental to proving the additively
optimality property.
Remark 2 The word p201 is called a self-delimiting en-
coding for p since it encodes p in a unique way and the
end of the code is computable. Hence, if one concatenates
this code for p and another word, then one can compute
back p and the second word. Using the same reasoning,
from p201q201w one can compute each of the three
words p, q, and w.
Theorem 1
There exists an additively optimal universal
representation system.
Proof
Let '0 be the function deﬁned from (f0; 1g)2 to
f0; 1g by
'0 :
(
(f0; 1g)2 ! f0; 1g ;
(p ˇ w; v) 7! Tp(w; v) ;
where Tp is the function from (f0; 1g)2 to f0; 1g com-
puted by the pth Turing machine (two tapes for the two in-
puts). From computability theory we know that this func-
tion is computable since there exists a universal Turing
machine.
Let ' be a representation system. Since ' is com-
putable, it is computed by a Turing machine whose num-
ber is p. For all words w and v, let y be a shortest program
for w knowing v with representation system '. Then
'0(p ˇ y; v) D Tp(y; v) D '(y; v) D w ;
and so p ˇ y is a program (perhaps not the shortest) for w
knowing v with representation system '0. Since
jp ˇ yj D 2jpj C 2 C jyj D 2jpj C 2 C K'(wjv) ;
it holds that, for all words w,
K'0(wjv) 6 K'(wjv) C 2jpj C 2 :
As p depends only on ', we have that for all representation
systems ', there is a constant c D 2jpj C 2 such that
K'0(wjv) < K'(wjv) C c :
We conclude that '0 is an additively optimal representa-
tion system.

Now we are able to deﬁne Kolmogorov complexity.
Deﬁnition 3 Let '0 be an additively optimal representa-
tion system. The Kolmogorov complexity of a word w know-
ing v is deﬁned by
K(wjv) D K'0(wjv) ;
and the Kolmogorov complexity of a word w is deﬁned by
K(w) D K'0(w) D K(wj") :
Remark 3 Kolmogorov complexity is deﬁned up to an ad-
ditive constant from Proposition 1. This means that any
meaningful result must include in its statement something
like “there is a constant c such that.”

246 A
Algorithmic Complexity and Cellular Automata
Properties
Basic Relations
Kolmogorov complexity allows one to
express relations that seem natural between complexities
of words. For instance, the complexity of ww is close to the
complexity of w. The complexity of uv is less than the sum
of the complexities of u and v (in fact up to the logarithm
of the size of u or v). They will be used in the sequel.
We give here a few examples of how to prove such
results.
Theorem 2
There exists a constant c such that for all
words w and v,
K(wjv) < jwj C c :
Proof
Let  be the representation system deﬁned by
(w; x) D w. As '0 is additively optimal, there is a con-
stant c such that
K(wjv) < K	(wjv) C c :
Since w is the only word such that (w) D w, one has
K	(wjv) D jwj. We conclude that
K(wjv) < K	(wjv) C c D jwj C c :

This result is interesting since it gives a computable up-
per bound for Kolmogorov complexity. Moreover, in Sub-
sect. “Incompressible Words” we will see that this bound
is tight.
Theorem 3
Let f and g be partial computable functions.
There exists a constant c such that for all words w 2 D f
and all v 2 Dg
K(f (w)jv) < K(wjg(v)) C c :
Proof
As f and g are computable, function h, deﬁned
by h(x; y) D f ('0(x; g(y))), is a representation system.
Hence, by additive optimality of '0, there is a constant c
such that for all w and v
K(f (w)jv) < Kh(f (w)jv) C c :
(2)
Let x be a shortest program for w knowing g(v)
with representation system '0. Since '0(x; g(v)) D w,
h(x; v) D f ('0(x; g(v)) D f (w), and so x is a program
for f (w) given v with representation system h. We con-
clude that
Kh(f (w)jv) 6 jyj D K(wjg(v)) ;
and hence, from Eq. (2),
K(f (w)jv) < Kh(f (w)jv) C c 6 K(wjg(v)) C c :

This proposition formalizes the intuition that if a word w
can be computed from x, then w is simpler than x, of
course up to some additive constant. This means that w
contains less information than x. Of course, if f is injec-
tive, as we can go back from f (w) to w, then w and f (w)
have the same amount of information. This is stated by
the next corollary.
Corollary 1
Let f and g be injective partial computable
functions. There exists a constant c such that for all words w
and v,
jK(f (w)jv)  K(wjg(v))j < c :
Proof
From the previous theorem, as f , g, f 1, and g1
are computable, there are constants c1 and c2 such that for
all w and v
K(f (w)jv) < K(wjg(v)) C c1 and
K(f 1(w)jv) < K(wjg1(v)) C c2 :
Hence, K(wjg(v))<K(f (w)jv)Cc2. Choosing cDmax(c1;
c2) completes the proof.

Using this corollary, one ﬁnds that the complexity of the
representation by binary strings of a mathematical object
depends only on the mathematical object and not on its
representation, provided that an algorithm can compute
one representation from another. Hence, for any math-
ematical object o, we denote by notation abuse K(o) the
complexity of a representation of o with alphabet f0; 1g. In
particular, we use this for integers, ﬁnite sets, graphs, etc.
However, for objects that cannot be represented by
ﬁnite binary strings (for instance, real numbers, inﬁnite
strings), we cannot deﬁne a Kolmogorov complexity. In
this case, a per-case deﬁnition has to be given since for in-
ﬁnite objects several reasonable deﬁnitions of complexity
can be found. For instance, for inﬁnite strings one can use
the limit superior or the limit inferior of the conditional
complexity of the preﬁx given its length. One can use the
limit superior of the complexity of the preﬁx divided by its
length, etc.
Now we state a result about the complexity of a pair
compared to the complexity of each member.
Theorem 4 There is constant c such that, for any words x,
y, and v,
K(hx; yijv) < K(xjv)CK(yjhv; xi)C2 log2(K(xjv))Cc;

Algorithmic Complexity and Cellular Automata
A
247
where h:; :i is any bijective computable function between N2
and N.
Proof
Let b
x; y be deﬁned by b
x; y D `(x)201xy, where
`(x) is the length of x written in binary. Note that x and y
can both be reconstructed from b
x; y and that
jb
x; yj D 2 C 2dlog2 jxje C jxj C jyj :
Let ' be the representation system deﬁned by
'(b
x; y; v) D h'0(x; v); '0(y; hv; '0(x; v)i)i :
As '0 is additively optimal, there is a constant c such that
for all w and v
K(wjv) < K'(wjv) C c :
Let z be a shortest program for x knowing v, and let z0 be
a shortest program for y knowing hv; xi. As c
z; z0 is a pro-
gram for hx; yi knowing v with representation system ',
one has
K(hx; yijv)
< K'(wjv) C c 6 j c
z; z0j C c
6 K(xjv) C K(yjhv; xi) C 2 log2(K(xjv)) C 3 C c ;
which completes the proof.

Of course, this inequality is not an equality since if x D y,
the complexity of K(hx; yi) is the same as K(x) up to some
constant. When the equality holds, it is said that x and y
have no mutual information.
Note that if we can represent the combination of pro-
grams z and z0 in a shorter way, the upper bound is de-
creased by the same amount. For instance, if we choose
b
x; y D `(`(x))201`(x)xy, it becomes
2 log2 log2 jxj C log2 jxj C jxj C jyj :
If we know that a program never contains a special word u,
then withb
x; y D xuy it becomes jxj C jyj.
Examples
The properties seen so far allow one to prove
most of the relations between complexities of words.
For instance, choosing f : w 7! ww and g being the
identity in Corollary 1, we obtain that there is a constant c
such that jK(wwjv)  K(wjv)j < c.
In the same way, letting f be the identity and g be de-
ﬁned by g(x) D " in Theorem 3, we get that there is a con-
stant c such that K(wjv) < K(w) C c.
By Theorem 4, Corollary 1, and choosing f
as
hx; yi 7! xy, and g as the identity, we have that there is
a constant c such that
K(xy) < K(x)CK(y)C2 min log2(K(x)); log2(K(y)))Cc:
Incompressible Words
Kolmogorov complexity gives
a computer-science notion of randomness. By Theorem 2,
we have an upper bound on Kolmogorov complexity: the
length of the word. When this upper bound is reached, we
say that the word is random. However, Kolmogorov com-
plexity is diﬃcult to use since it is not computable (Theo-
rem 2.3.2 in [13]). The good news is that it is approximable
from above.
The following deﬁnition formalizes the notion of in-
compressible word.
Deﬁnition 4 Let c 2 RC. Word w is c-incompressible if
K(w) > jwj  c :
The next result proves the existence of incompressible
words.
Proposition 2 For any c 2 RC, there are at least (2nC1 
2nc) c-incompressible words w such that jwj 6 n.
Proof
Each program produces only one word; therefore
there cannot be more words whose complexity is below n
than the number of programs of size n. Hence, one ﬁnds
jfw; K(w) < ngj 6 2n  1 :
This implies that
jfw; K(w) < jwj  c and jwj 6 ngj
6 jfw; K(w) < ngj < 2nc ;
and, since
fw; K(w) > jwj  c and jwj 6 ng
 fw; jwj 6 ngnfw; K(w) < jwjc and jwj 6 ng;
it holds that
jfw; K(w) > jwj  c and jwj 6 ngj > 2nC1  2nc :

From this proposition one deduces that half of the words
whose size is less than or equal to n reach the upper
bound of their Kolmogorov complexity. This incompress-
ibility method relies on the fact that most words are
incompressible.
Martin–Löf Tests
Martin–Löf tests give another equivalent deﬁnition of ran-
dom sequences. The idea is simple: a sequence is random
if it does not pass any computable test of singularity (i. e.,

248 A
Algorithmic Complexity and Cellular Automata
a test that selects words from a “negligible” recursively
enumerable set). As for Kolmogorov complexity, this no-
tion needs a proper deﬁnition and implies a universal test.
However, in this review we prefer to express tests in terms
of Kolmogorov complexity. (We refer the reader to [13]
for more on Martin–Löf tests.)
Let us start with an example. Consider the test “to have
the same number of 0s and 1s.” Deﬁne the set
E D fw; w has the same number of 0s and 1sg
and order it in military order (length ﬁrst, then lexi-
cographic order), E D fe0; e1; : : : g. Consider the com-
putable function f : x 7! ex (which is in fact computable
for any decidable set E). Note that there are 2n
n
 words
in E of length 2n. Thus, if ex has length 2n, x is less than
2n
n
, whose logarithm is 2n  log2
pn C O(1). Then, us-
ing Theorem 3 with function f , one ﬁnds that
K(ex) < K(x) < log2 x < jexj  log2
r
jexj
2
;
up to an additive constant. We conclude that all members
ex of E are not c-incompressible whenever they are long
enough.
This notion of test corresponds to “belonging to
a small set” in Kolmogorov complexity terms. The next
proposition formalizes these ideas.
Proposition 3 Let E be a recursively enumerable set such
that jE \ f0; 1gnj D o(2n). Then, for all constants c, there
is an integer M such that all words of length greater than M
of E are not c-incompressible.
Proof
In this proof, we represent integers as binary
strings. Let b
x; y be deﬁned, for integers x and y, by
b
x; y D jxj201xy :
Let f be the computable function
f :
8
<
:
f0; 1g
!
f0; 1g ;
b
x; y
7!
the yth word of length x
in the enumeration of E :
From Theorems 2 and 3, there is a constant d such that for
all words u of E
K(e) < jf 1(e)j C d :
Note un D jE \ f0; 1gnj. From the deﬁnition of function f
one has
jf 1(e)j 6 3 C 2 log2(jej) C log2(ujej) :
As un D o(2n), log2(ujej)  jej tends to 1, so there is
an integer M such that when jej > M, K(e) < jf 1(e)j C
d < jejc. This proves that no members of E whose length
is greater than M are c-incompressible.

Dynamical Systems and Symbolic Factors
A (discrete-time) dynamical system is a structure hX; f i
where X is the set of states of the system and f is a func-
tion from X to itself that, given a state, tells which is the
next state of the system. In the literature, the state space
is usually assumed to be a compact metric space and f is
a continuous function.
The study of the asymptotic properties of a dynamical
system is, in general, diﬃcult. Therefore, it can be interest-
ing to associate the studied system with a simpler one and
deduce the properties of the original system from the sim-
ple one. Indeed, under the compactness assumption, one
can associate each system hX; f i with its symbolic factor as
follows.
Consider a ﬁnite open covering fˇ0; ˇ1; : : : ; ˇkg of X.
Label each set ˇi with a symbol ˛i. For any orbit of ini-
tial condition x 2 X, we build an inﬁnite word wx on
f˛0; ˛1; : : : ; ˛kg such that for all i 2 N, wx(i) D ˛j if
f i(x) 2 ˇj for some j 2 f0; 1; : : : ; kg. If f i(x) belongs to
ˇj \ ˇh (for j ¤ h), then arbitrarily choose either ˛j or
˛h. Denote by Sx the set of inﬁnite words associated with
the initial condition x 2 X and set S D S
x2X Sx. The sys-
tem hS; i is the symbolic system associated with hX; f i; 
is the shift map deﬁned as 8x 2 S8i 2 N; (x)i D xiC1.
When fˇ0; ˇ1; : : : ; ˇkg is a clopen partition, then
hS; i is a factor of hX; f i (see [12] for instance).
Dynamical systems theory has made great strides in
recent decades, and a huge quantity of new systems have
appeared. As a consequence, scientists have tried to clas-
sify them according to diﬀerent criteria. From interac-
tions among physicists was born the idea that in order
to simulate a certain physical phenomenon, one should
use a dynamical system whose complexity is not higher
than that of the phenomenon under study. The problem
is the meaning of the word “complexity”; in general it has
a diﬀerent meaning for each scientist. From a computer-
science point of view, if we look at the state space of factor
systems, they can be seen as sets of bi-inﬁnite words on
a ﬁxed alphabet. Hence, each factor hS; i can be associ-
ated with a language as follows. For each pair of words u w
write u  v if u is a factor of w. Given a ﬁnite word u and
a bi-inﬁnite word v, with an abuse of notation we write
u  v if u occurs as a factor in v. The language L(v) asso-
ciated with a bi-inﬁnite word v 2 A is deﬁned as
L(v) D fu 2 A; u  vg :
Finally, the language L(S) associated with the symbolic fac-
tor hS; i is given by
L(S) D fu 2 A; u  vg :

Algorithmic Complexity and Cellular Automata
A
249
The idea is that the complexity of the system hX; f i
is proportional to the language complexity of its symbolic
factor hS; i (see, for example, ▷Topological Dynamics of
Cellular Automata).
In [4,5], Brudno proposes to evaluate the complexity
of symbolic factors using Kolmogorov complexity. Indeed,
the complexity of the orbit of initial condition x 2 X ac-
cording to ﬁnite open covering fˇ0; ˇ1; : : : ; ˇkg is deﬁned
as
K(x; f ; fˇ0; ˇ1; : : : ; ˇkg) D lim sup
n!1
min
w2Sx
K(w0:n)
n
:
Finally, the complexity of the orbit x is given by
K(x; f ) D sup K(x; f ; ˇ) ;
where the supremum is taken w.r.t. all possible ﬁnite open
coverings ˇ of X. Brudno has proven the following result.
Theorem 5 ([4])
Consider a dynamical system hX; f i
and an ergodic measure . For -almost all x 2 X,
K(x; f ) D H(f ), where H(f ) is the measure entropy
of hX; f i (for more on the measure entropy see, for
example, ▷Ergodic Theory of Cellular Automata).
Cellular Automata
Cellular automata (CA) are (discrete-time) dynamical sys-
tems that have received increased attention in the last few
decades as formal models of complex systems based on lo-
cal interaction rules. This is mainly due to their great va-
riety of distinct dynamical behavior and to the possibility
of easily performing large-scale computer simulations. In
this section we quickly review the deﬁnitions and useful
results, which are necessary in the sequel.
Consider the set of conﬁgurations C, which consists
of all functions from ZD into A. The space C is usually
equipped with the Cantor metric dC deﬁned as
8a; b 2 C;
dC(a; b) D 2n ;
with n D min
Ev2ZDfkEvk1 : a(Ev) ¤ b(Ev)g ;
(3)
where kEvk1 denotes the maximum of the absolute value
of the components of Ev. The topology induced by dC co-
incides with the product topology induced by the discrete
topology on A. With this topology, C is a compact, perfect,
and totally disconnected space.
Let N D fEu1; : : : ; Eusg be an ordered set of vectors of
ZD and f : As 7! A be a function.
Deﬁnition 5 (CA) The D-dimensional CA based on the
local rule ı and the neighborhood frame N is the pair
hC; f i, where f : C 7! C is the global transition rule de-
ﬁned as follows:
8c 2 C;
8Ev 2 ZD ;
f (c)(Ev) D ı

c(Ev C Eu1); : : : ; c(Ev C Eus)

:
(4)
Note that the mapping f is (uniformly) continuous with
respect to dC. Hence, the pair hC; f i is a proper (discrete-
time) dynamical system.
In [7], a new metric on the phase space is introduced
to better match the intuitive idea of chaotic behavior with
its mathematical formulation. More results in this research
direction can be found in [1,2,3,15]. This volume dedicates
a whole chapter to the subject (▷Dynamics of Cellular
Automata in Non-compact Spaces). Here we simply recall
the deﬁnition of the Besicovitch distance since it will be
used in Subsect. “Example 2”.
Consider
the
Hamming
distance
between
two
words u v on the same alphabet A, #(u; v) D jfi 2 N j
ui ¤ vigj. This distance can be easily extended to work on
words that are factors of bi-inﬁnite words as follows:
#h;k(u; v) D jfi 2 [h; k] j ui ¤ vigj ;
where h; k 2 Z and h < k. Finally, the Besicovitch pseu-
dodistance is deﬁned for any pair of bi-inﬁnite words u v
as
dB(u; v) D lim sup
n!1
#n;n(u; v)
2n C 1
:
The pseudodistance dB can be turned into a distance when
taking its restriction to AZj :D, where :D is the relation of
“being at null dB distance.” Roughly speaking, dB measures
the upper density of diﬀerences between two bi-inﬁnite
words.
The space-time diagram  is a graphical representa-
tion of a CA orbit. Formally, for a D-dimensional CA f
with state set A,  is a function from AZD  N  Z to A
deﬁned as  (x; i) D f i(x)j, for all x 2 AZD, i 2 N and
j 2 Z.
The limit set ˝f contains the long-term behavior of
a CA on a given set U and is deﬁned as follows:
˝ f (U) D
\
n2N
f n(U) :
Unfortunately, any nontrivial property on the CA limit
set is undecidable [11]. Other interesting information on
a CA’s long-term behavior is given by the orbit limit set 
deﬁned as follows:
f (U) D
[
u2U
O0
f (u) ;

250 A
Algorithmic Complexity and Cellular Automata
where H0 is the set of adherence points of H. Note that in
general the limit set and the orbit limit sets give diﬀerent
information. For example, consider a rich conﬁguration c
i. e., a conﬁguration containing all possible ﬁnite patterns
(here we adopt the terminology of [6]) and the shift map .
Then, 
(fcg) D AZ, while ˝
(fcg) is countable.
Algorithmic Complexity as a Demonstration Tool
The incompressibility method is a valuable formal tool to
decrease the combinatorial complexity of problems. It is
essentially based on the following ideas (see also Chap. 6
in [13]):
 Incompressible words cannot be produced by short
programs. Hence, if one has an incompressible (inﬁ-
nite) word, it cannot be algorithmically obtained.
 Most words are incompressible. Hence, a word taken at
random can usually be considered to be incompressible
without loss of generality.
 If one “proves” a recursive property on incompressible
words, then, by Proposition 3, we have a contradiction.
Application to Cellular Automata
A portion of a space-time diagram of a CA can be com-
puted given its local rule and the initial condition. The part
of the diagram that depends only upon it has at most the
complexity of this portion (Fig. 1). This fact often implies
great computational dependencies between the initial part
and the ﬁnal part of the portion of the diagram: if the ﬁ-
nal part has high complexity, then the initial part must be
at least as complex. Using this basic idea, proofs are struc-
tured as follows: assume that the ﬁnal part has high com-
plexity; use the hypothesis to prove that the initial part
is not complex; then we have a contradiction. This tech-
nique provides a faster and clearer way to prove results
that could be obtained by technical combinatorial argu-
ments.
The ﬁrst example illustrates this fact by rewriting
a combinatorial proof in terms of Kolmogorov complex-
ity. The second one is a result that was directly written in
terms of Kolmogorov complexity.
Example 1
Consider the following result about languages recogniz-
able by CA.
Proposition 4 ([17])
The language L D fuvu; u; v 2
f0; 1g ; juj > 1g is not recognizable by a real-time one-
way CA.
Algorithmic Complexity and Cellular Automata, Figure 1
States in the gray zone can be computed for the states of the
black line: K(gray zone) 6 K(black line) up to an additive con-
stant
Before giving the proof in terms of Kolmogorov complex-
ity, we must recall some concepts. A language L is accepted
in real-time by a CA if is accepted after jxj transitions. An
input word is accepted by a CA if after accepting state.
A CA is one-way if the neighborhood is f0; 1g (i. e., the
central cell and the one on its right).
Proof
One-way CA have some interesting proper-
ties. First, from a one-way CA recognizing L, a com-
puter can build a one-way CA recognizing L˙
D
fuvu; u; v 2 ˙; juj > 1g, where ˙ is any ﬁnite alphabet.
The idea is to code the ith letter of ˙ by 1u01u1    1uk00,
where u0u1    uk is i written in binary, and to apply the
former CA.
Second, for any one-way CA of global rule f we have
that
f jwj(xwy) D f jwj(xw)f jwj(wy)
for all words x, y, and w.
Now, assume that a one-way CA recognizes L in real
time. Then, there is an algorithm F that, for any integer n,
computes a local rule for a one-way CA that recognizes
Lf0;:::;n1g in real time.
Fix an integer n and choose a 0-incompressible word w
of length n(n  1). Let A be the subset of Z of pairs (x; y)
for x; y 2 f0; : : : ; n  1g and let x 6D y be deﬁned by
A D f(x; y); the nx C yth bit of w is 1g :
Since A can be computed from w and vice versa, one
ﬁnds that K(A) D K(w) D jwj D n(n  1) up to an addi-
tive constant that does not depend on n.
Order the set A D f(x0; y0); (x1; y1); : : : ; g and build
a new word u as follows:
u0 D y0x0 ;
uiC1 D ui yiC1xiC1ui ;
u D ujAj1 ;

Algorithmic Complexity and Cellular Automata
A
251
From Lemma 1 in [17], one ﬁnds that for all x; y 2 f0;
: : : ; n  1g, (x; y) 2 A , xuy 2 L. Let ı be the local rule
produced by F(n), Q the set of ﬁnal states, and f the asso-
ciated global rule. Since
f juj(xuy) D f juj(xu)f juj(uy);
one has that
(x; y) 2 A , xuy 2 L , ı(f juj(xu); f juj(uy)) 2 Q :
Hence, from the knowledge of each
f juj(xu) and
f juj(ux) for x 2 f0; : : : ; n  1g, a list of 2n integers of
f0; : : : ; n  1g, one can compute A. We conclude that
K(A) 6 2n log2(n)
up to an additive constant that does not depend on n. This
contradicts K(A) D n(n  1) for large enough n.

Example 2
Notation
Given x 2 AZD, we denote by x!n the word
w 2 S(2nC1)D obtained by taking all the states xi for
i 2 (2n C 1)D in the martial order.
The next example shows a proof that directly uses the
incompressibility method.
Theorem 6 In the Besicovitch topological space there is no
transitive CA.
Recall that a CA f is transitive if for all nonempty sets A B
there exists n 2 N such that f n(A) \ B ¤ ;.
Proof
By contradiction, assume that there exists a tran-
sitive CA f of radius r with C D jSj states. Let x and y be
two conﬁgurations such that
8n 2 N; K(x!njy!n)  n
2 :
A simple counting argument proves that such conﬁgura-
tions x and y always exist. Since f is transitive, there are
two conﬁgurations x0 and y0 such that for all n 2 N
(x!n; x0
!n)  4"n ;
(y!n; y0
!n)  4ın ;
(5)
and an integer u (which only depends on " and ı) such that
f u(y0) D x0 ;
(6)
where " D ı D (4e10 log2 C)
1.
In what follows only n varies, while C, u, x, y, x0, y0, ı,
and " are ﬁxed and independent of n.
By Eq. (6), one may compute the word x0
!n from the
following items:

y0
!n, f , u and n;
 The two words of y0 of length ur that surround y0
!n
and that are missing to compute x0
!n with Eq. (6).
We obtain that
K(x0
!njy0
!n)  2urCK(u)CK(n)CK(f )CO(1)  o(n)
(7)
(the notations O and o are deﬁned with respect to n). Note
that n is ﬁxed and hence K(n) is a constant bounded by
log2 n. Similarly, r and S are ﬁxed, and hence K(f ) is con-
stant and bounded by C2rC1 log2 C C O(1).
Let us evaluate K(y0
!njy!n). Let a1; a2; a3; : : : ; ak be
the positive positions that y!n and y0
!n diﬀer at, sorted
in increasing order. Let b1 D a1 and bi D ai  ai1, for
2  i  k. By Eq. (5) we know that k  4ın. Note that
Pk
iD1 bi D ak  n.
Symmetrically let a0
1; a0
2; a0
3; : : : ; a0
k0 be the absolute
values of the strictly negative positions that y!n and
y0
!n diﬀer at, sorted in increasing order. Let b0
1 D a0
1 and
b0
i D a0
i  a0
i1, where 2  i  k0. Equation (5) states that
k0  4ın.
Since the logarithm is a concave function, one has
X ln bi
k
 ln
P bi
k
 ln n
k ;
and hence
X
ln bi  k ln n
k ;
(8)
which also holds for b0
i and k0.
Knowledge of the bi, b0
i, and k C k0 states of the cells of
y0
!n, where y!n diﬀers from y0
!n, is enough to compute
y0
!n from y!n. Hence,
K(y0
!njy!n) 
X
ln(bi)
C
X
ln(b0
i) C (k C k0) log2 C C O(1) :
Equation (8) states that
K(y0
!njy!n)  k ln n
k Ck0 ln n
k0 C(kCk0) log2 CCO(1):
The function k 7! k ln n
k is increasing on [0; n
e ]. As
k  4ın 
n
e10 log2 C , we have that
k ln n
k  4ın ln n
4ın 
n
e10 log2 C ln e10 log2 C 
10n
e10 log2 C
and that
(k C k0) log2 C  2n log2 C
e10 log2 C :

252 A
Algorithmic Complexity and Cellular Automata
Replacing a, b, and k by a0, b0, and k0, the same se-
quence of inequalities leads to a similar result. One de-
duces that
K(y0
!njy!n)  (2 log2 C C 20)n
e10 log2 C
C O(1) :
(9)
Similarly, Eq. (9) is also true with K(x!njx0
!n).
The triangular mequality for Kolmogorov complex-
ity K(ajb)  K(ajc) C K(bjc) (consequence of theorems
3 and 4) gives:
K(x!njy!n)  K(x!njx0
!n) C K(x0
!njy0
!n)
C K(y0
!njy!n) C O(1) :
Equations (9) and (7) allow one to conclude that
K(x!njy!n)  (2 log2 C C 20)n
e10 log2 C
C o(n) :
The hypothesis on x and y was K(x!njy!n)  n
2 .
This implies that
n
2  (2C C 20)n
e10 log2 C
C o(n) :
The last inequality is false for big enough n.

Measuring CA Structural Complexity
Another use of Kolmogorov complexity in the study of CA
is to understand what maximum complexity they can pro-
duce extracting examples of CA that show high complexity
characteristics. The question is to deﬁne what is the mean-
ing of “show high complexity characteristics” and, more
precisely, what characteristic to consider. This section is
devoted to structural characteristics of CA, that is to say,
complexity that can be observed through static particular-
ities.
The Case of Tilings
In this section, we give the original example that was given
for tilings, often considered as a static version of CA.
In [10], Durand et al. construct a tile set whose tilings
have maximum complexity. This paper contains two main
results. The ﬁrst one is an upper bound for the complexity
of tilings, and the second one is an example of tiling that
reaches this bound. First we recall the deﬁnitions about
tilings of a plane by Wang tiles.
Deﬁnition 6 (Tilings with Wang tiles) Let C be a ﬁnite
set of colors. A Wang tile is a quadruplet (n; s; e; w) of four
colors from C corresponding to a square tile whose top
color is n, left color is w, right color is e, and bottom color
is s. A Wang tile cannot be rotated but can be copied to
inﬁnity.
Given a set of Wang tiles T, we say that a plane can be
tiled by T if one can place tiles from T on the square grid
Z2 such that the adjacent border of neighboring tiles has
the same color. A set of tiles T that can tile the plane is
called a palette.
The notion of local constraint gives a point of view closer
to CA than tilings. Roughly speaking, it gives the local con-
straints that a tiling of the plane using 0 and 1 must satisfy.
Note that this notion can be deﬁned on any alphabet, but
we can equivalently code any letter with 0 and 1.
Deﬁnition 7 (Tilings by local constraints) Let r be a pos-
itive integer called radius. Let C be a set of square patterns
of size 2r C 1 made of 0 and 1 (formally a function from
r; r to f0; 1g). The set is said to tile the plane if there
is a way to put zeros and ones on the 2-D grid (formally
a function from Z2 to f0; 1g) whose patterns of size 2r C 1
are all in C. The possible layouts of zeros and ones on the
(2-D) grid are called the tilings acceptable for C.
Seminal papers on this subject used Wang tiles. We trans-
late these results in terms of local constraints in order to
more smoothly apply them to CA.
Theorem 7 proves that, among tilings acceptable by
a local constraint, there is always one that is not too com-
plex. Note that this is a good notion since the use of a con-
straint of radius 1, f 0 ; 1 g, which allows for all possible
patterns, provides for acceptable high-complexity tilings
since all tilings are acceptable.
Theorem 7 ([10])
Let C be a local constraint. There is
tiling  acceptable for C such that the Kolmogorov complex-
ity of the central pattern of size n is O(n) (recall that the
maximal complexity for a square pattern n  n is O(n2)).
Proof
The idea is simple: if one knows the bits present
in a border of width r of a pattern of size n, there are
ﬁnitely many possibilities to ﬁll the interior, so the ﬁrst
one in any computable order (for instance, lexicographi-
cally when putting all horizontal lines one after the other)
has at most the complexity of the border since an algo-
rithm can enumerate all possible ﬁllings and take the ﬁrst
one in the chosen order.
Then, if one knows the bits in borders of width r of all
central square patterns of size 2n for all positive integers n
(Fig. 2), one can recursively compute for each gap the ﬁrst
possible ﬁlling for a given computable order. The tiling ob-
tained in this way (this actually deﬁnes a tiling since all
cells are eventually assigned a value) has the required com-
plexity: in order to compute the central pattern of size n,

Algorithmic Complexity and Cellular Automata
A
253
Algorithmic Complexity and Cellular Automata, Figure 2
Nested squares of size 2k and border width r
the algorithm simply needs all the borders of size 2k for
k 6 2n, which is of length at most O(n).

The next result proves that this bound is almost reachable.
Theorem 8
Let r be any computable monotone and un-
bounded function. There exists a local constraint Cr such
that for all tilings  acceptable for Cr, the complexity of the
central square pattern of  is O(n/r(n)).
The original statement does not have the O part. However,
note that the simulation of Wang tiles by a local constraint
uses a square of size ` D d
p
log ke to simulate a single tile
from a set of k Wang tiles; a square pattern of size n in
the tiling corresponds to a square pattern of size n
` in the
original tiling.
Function r can grow very slowly (for instance, the
inverse of the Ackermann function) provided it grows
monotonously to inﬁnity and is computable.
Proof
The proof is rather technical, and we only give
a sketch. The basic idea consists in taking a tiling con-
structed by Robinson [16] in order to prove that it is un-
decidable to test if a local constraint can tile the plane.
This tiling is self-similar and is represented in Fig. 3. As
it contains increasingly larger squares that occur period-
ically (note that the whole tiling is not periodic but only
quasiperiodic), one can perform more and more computa-
tion steps within these squares (the periodicity is required
to be sure that squares are present).
Algorithmic Complexity and Cellular Automata, Figure 3
Robinson’s tiling
Using this tiling, Robinson can build a local con-
straint that simulates any Turing machine. Note that in
the present case, the situation is more tricky than it seems
since some technical features must be assured, like the fact
that a square must deal with the smaller squares inside it
or the constraint to add to make sure that smaller squares

254 A
Algorithmic Complexity and Cellular Automata
have the same input as the bigger one. Using the constraint
to forbid the occurrence of any ﬁnal state, one gets that the
compatible tilings will only simulate computations for in-
puts for which it does not halt.
To ﬁnish the proof, Durand et al. build a local con-
straint C that simulates a special Turing machine that halts
on inputs whose complexity is small. Such a Turing ma-
chine exists since, though Kolmogorov complexity is not
computable, testing all programs from " to 1n allows a Tur-
ing machine to compute all words whose complexity is be-
low n and halt if it ﬁnds one. Then all tilings compatible
with C contain in each square an input on which the Tur-
ing machine does not halt yet is of high complexity.
Function r occurs in the technical arguments since
computable zones do not grow as fast as the length of
squares.

The Case of Cellular Automata
As we have seen so far, one of the results of [10] is that a tile
set always produces tilings whose central square patterns
of size n have a Kolmogorov complexity of O(n) and not
n2 (which is the maximal complexity).
In the case of CA, something similar holds for space-
time diagrams. Indeed, if one knows the initial row. One
can compute the triangular part of the space-time diagram
which depends on it (see Fig. 1). Then, as with tilings, the
complexity of an n  n-square is the same as the complex-
ity of the ﬁrst line, i. e., O(n). However, unlike tilings, in
CA, there is no restriction as to the initial conﬁguration,
hence CA have simple space-time diagrams.
Thus in this case Kolmogorov complexity is not of
great help. One idea to improve the results could be to
study how the complexity of conﬁgurations evolves dur-
ing the application of the global rule. This aspect is par-
ticularly interesting with respect to dynamical properties.
This is the subject of the next section.
Consider a CA F with radius r and local rule ı.
Then, its orbit limit set cannot be empty. Indeed, let a
be a state of f . Consider the conﬁguration !a!. Let sa D
ı(a; : : : ; a). Then, f (!a!) D !s!
a . Consider now a graph
whose vertices are the states of the CA and the edges
are (a; sa). Since each vertex has an outgoing edge (ac-
tually exactly one), it must contain a cycle a0 ! a1 !
   ! ak ! a0. Then each of the conﬁgurations !a!
i for
0 6 i 6 k is in the limit set since f k(!a!
i ) D !a!
i .
This simple fact proves that any orbit limit set (and any
limit set) of a CA must contain at least a monochromatic
conﬁguration whose complexity is low (for any reason-
able deﬁnition). However, one can build a CA whose or-
bit limit set contains only complex conﬁgurations, except
for the mandatory monochromatic one using the local-
constraints technique discussed in the previous section.
Proposition 5 There exists a CA whose orbit limit set con-
tains only complex conﬁgurations.
Proof
Let r be any computable monotone and un-
bounded function and Cr the associated local constraint.
Let A be the alphabet that Cr is deﬁned on. Consider the
2-D CA f r on the alphabet A [ f#g (we assume f#g ¤ A).
Let r be the radius of f r (the same radius as Cr). Finally, the
local rule ı of f r is deﬁned as follows:
ı(P) D
(
P0
if P 2 Cr ;
#
otherwise :
Using this local rule, one can verify the following
fact. If the conﬁguration c is not acceptable for Cr, then
(O(c))0 D f!#!g; (O(c))0 D fcg otherwise. Indeed, if c is
acceptable for Cr, then f (c) D c; otherwise there is a po-
sition i such that c(i) is not a valid pattern for Cr. Then
f (c)(i) D #. By simple induction, this means that all cells
that are at a distance less than kr from position i become
# after k steps. Hence, for all n > 0, after k > nC2jij
r
steps,
the Cantor distance between f k(c) and !#! is less than
2n, i. e., O(c) tends to !#!.

Measuring the Complexity of CA Dynamics
The results of the previous section have limited range since
they tell something about the quasicomplexity but noth-
ing about the plain complexity of the limit set. To enhance
our study we need to introduce some general concepts,
namely, randomness spaces.
Randomness Spaces
Roughly speaking, a randomness space is a structure made
of a topological space and a measure that helps in deﬁning
which points of the space are random. More formally we
can give the following.
Deﬁnition 8 ([6])
A randomness space is a structure
hX; B; i where X is a topological space, B: N ! 2X a to-
tal numbering of a subbase of X, and  a Borel measure.
Given a numbering B for a subbase for an open set of X,
one can produce a numbering B0 for a base as follows:
B0(i) D
\
j2DiC1
B(j) ;
where D: N ! fE j E  N and E is ﬁniteg is the bijection
deﬁned by D1(E) D P
i2E 2i. B0 is called the base derived
from the subbase B. Given two sequences of open sets (Vn)

Algorithmic Complexity and Cellular Automata
A
255
and (Un) of X, we say that (Vn) is U-computable if there
exists a recursively enumerable set H 2 N such that
8n 2 N; Vn D
[
i2N;hn;ii2H
Ui ;
where hi; ji D (i C j)(i C j C 1) C j is the classical bi-
jection between N2 and N. Note that this bijec-
tion can be extended to N D (for D > 1) as follows:
hx1; x2; : : : ; xki D hx1; hx2; : : : ; xkii.
Deﬁnition 9 ([6])
Given a randomness space hX; B; i,
a randomness test on X is a B0-computable sequence (Un)
of open sets such that 8n 2 N; (Un)  2n.
Given a randomness test (Un), a point x 2 X is said to pass
the test (Un) if x 2 \n2NUn. In other words, tests select
points belonging to sets of null measure. The computabil-
ity of the tests assures the computability of the selected null
measure sets.
Deﬁnition 10 ([6]) Given a randomness space hX; B; i,
a point x 2 X is nonrandom if x passes some randomness
test. The point x 2 X is randomk if it is not nonrandom.
Finally, note that for any D  1, hAZD; B; i is a random-
ness space when setting B as
B(j C D  hi1; : : : ; iDi) D
n
c 2 AZD; ci1;:::;iD D aj
o
;
where A D fa1; : : : ; aj; : : : ; aDg and  is the classical
product measure built from the uniform Bernoulli mea-
sure over A.
Theorem 9 ([6]) Consider a D-dimensional CA f . Then,
the following statements are equivalent:
1. f is surjective;
2. 8c 2 AZD, if c is rich (i. e., c contains all possible ﬁnite
patterns), then f (c) is rich;
3. 8c 2 AZD, if c is random, then f (c) is random.
Theorem 10 ([6]) Consider a D-dimensional CA f . Then,
8c 2 AZD, if c is not rich, then f (c) is not rich.
Theorem 11 ([6]) Consider a 1-D CA f . Then, 8c 2 AZD,
if c is nonrandom, then f (c) is nonrandom.
Note that the result in Theorem 11 is proved only for 1-D
CA; its generalization to higher dimensions is still an open
problem.
Open problem 1
Do D-dimensional CA for D > 1 pre-
serve nonrandomness?
From Theorem 9 we know that the property of preserv-
ing randomness (resp. richness) is related to surjectivity.
Hence, randomness (resp. richness) preserving is decid-
able in one dimension and undecidable in higher dimen-
sions. The opposite relations are still open.
Open problem 2
Is nonrichness (resp. nonrandomness)
a decidable property?
Algorithmic Distance
In this section we review an approach to the study of CA
dynamics from the point of view of algorithmic complex-
ity that is completely diﬀerent than the one reported in the
previous section. For more details on the algorithmic dis-
tance see ▷Chaotic Behavior of Cellular Automata.
In this new approach we are going to deﬁne a new dis-
tance using Kolmogorov complexity in such a way that two
points x and y are near if it is “easy” to transform x into y or
vice versa using a computer program. In this way, if a CA
turns out to be sensitive to initial conditions, for example,
then it means that it is able to create new information. In-
deed, we will see that this is not the case.
Deﬁnition 11 The algorithmic distance between x 2 AZD
and y 2 AZD is deﬁned as follows:
da(x; y)
D lim sup
n!1
K(x ! njy ! n) C K(y ! njx ! n)
2(2n C 1)D
:
It is not diﬃcult to see that da is only a pseudodistance
since there are many points that are at null distance (those
that diﬀer only on a ﬁnite number of cells, for exam-
ple). Consider the relation Š of “being at null da distance,
i. e., 8x; y 2 AZD; x Š y if and only if da(x; y) D 0. Then,
hAZD/Š; dai is a metric space.
Note that the deﬁnition of da does not depend on
the chosen additively optimal universal description mode
since the additive constant disappears when dividing by
2(2n C 1)D and taking the superior limit. Moreover, by
Theorem 2, the distance is bounded by 1.
The following results summarize the main properties
of this new metric space.
Theorem 12 (▷Chaotic Behavior of Cellular Automata)
The metric space hAZD/Š; dai is perfect, pathwise con-
nected, inﬁnite dimensional, nonseparable, and noncom-
pact.
Theorem 12 says that the new topological space has
enough interesting properties to make worthwhile the
study of CA dynamics on it.
The ﬁrst interesting result obtained by this approach
concerns surjective CA. Recall that surjectivity plays a cen-
tral role in the study of chaotic behavior since it is a nec-

256 A
Algorithmic Complexity and Cellular Automata
essary condition for many other properties used to deﬁne
deterministic chaos like expansivity, transitivity, ergodic-
ity, and so on (see ▷Topological Dynamics of Cellular Au-
tomata and [8] for more on this subject). The following re-
sult proves that in the new topology AZD/Š the situation
is completely diﬀerent.
Proposition 6 (▷Chaotic Behavior of Cellular Au-
tomata)
If f is a surjective CA, then dA(x; f (x)) D 0 for
any x 2 AZD/Š. In other words, every CA behaves like the
identity in AZD/Š.
Proof
In order to compute f (x)!n from x!n, one need
only know the index of f in the set of CA with radius r,
state set S, and dimension d; therefore
K(f (x)!njx!n) 6 2dr(2n C 2r C 1)d1 log2 jSj
C K(f ) C 2 log2 K(f ) C c ;
and similarly
K(x!njf (x)!n) 6 gdr(2n C 1)d1 log2 jSj
C K(f ) C 2 log2 K(f ) C c :
Dividing by 2(2n C 1)d and taking the superior limit one
ﬁnds that da(x; f (x)) D 0.

The result of Proposition 6 means that surjective CA can
neither create new information nor destroy it. Hence, they
have a high degree of stability from an algorithmic point
of view. This contrasts with what happens in the Can-
tor topology. We conclude that the classical notion of de-
terministic chaos is orthogonal to “algorithmic chaos,” at
least in what concerns the class of CA.
Proposition 7 (▷Chaotic Behavior of Cellular Au-
tomata)
Consider a CA f
that is neither surjec-
tive nor constant. Then, there exist two conﬁgura-
tions x; y 2 AZD/Š such that da(x; y) D 0 but da(f (x);
f (y)) 6D 0.
In other words, Proposition 7 says that nonsurjective, non-
constant CA are not compatible with Š and hence not
continuous. This means that, for any pair of conﬁgura-
tions x y, this kind of CA either destroys completely the
information content of x y or preserves it in one, say x,
and destroys it in y. However, the following result says that
some weak form of continuity still persists (see ▷Topo-
logical Dynamics of Cellular Automata and [8] for the def-
initions of equicontinuity point and sensibility to initial
conditions).
Proposition 8 (▷Chaotic Behavior of Cellular Au-
tomata)
Consider a CA f and let a be the conﬁguration
made with all cells in state a. Then, a is both a ﬁxed point
and an equicontinuity point for f .
Even if CA were noncontinuous on AZD/Š, one could still
wonder what happens with respect to the usual properties
used to deﬁne deterministic chaos. For instance, by Propo-
sition 8, it is clear that no CA is sensitive to initial condi-
tions. The following question is still open.
Open problem 3 Is a the only equicontinuity point for CA
on AZD/Š?
Future Directions
In this paper we have illustrated how algorithmic com-
plexity can help in the study of CA dynamics. We essen-
tially used it as a powerful tool to decrease the combina-
torial complexity of problems. These kinds of applications
are only at their beginnings and much more are expected
in the future. For example, in view of the results of Sub-
sect. “Example 1”, we wonder if Kolmogorov complexity
can help in proving the famous conjecture that languages
recognizable in real time by CA are a strict subclass of lin-
ear-time recognizable languages (see [9,14]).
Another completely diﬀerent development would con-
sist in ﬁnding how and if Theorem 11 extends to higher di-
mensions. How this property can be restated in the context
of the algorithmic distance is also of great interest.
Finally, how to extend the results obtained for CA to
other dynamical systems is a research direction that must
be explored. We are rather conﬁdent that this can shed
new light on the complexity behavior of such systems.
Acknowledgments
This work has been partially supported by the ANR Blanc
Project “Sycomore”.
Bibliography
Primary Literature
1. Blanchard F, Formenti E, Kurka P (1999) Cellular automata in
the Cantor, Besicovitch and Weyl topological spaces. Complex
Syst 11:107–123
2. Blanchard F, Cervelle J, Formenti E (2003) Periodicity and
transitivity for cellular automata in Besicovitch topologies.
In: Rovan B, Vojtas P (eds) (MFCS’2003), vol 2747. Springer,
Bratislava, pp 228–238
3. Blanchard F, Cervelle J, Formenti E (2005) Some results about
chaotic behavior of cellular automata. Theor Comput Sci
349(3):318–336
4. Brudno AA (1978) The complexity of the trajectories of a dy-
namical system. Russ Math Surv 33(1):197–198
5. Brudno AA (1983) Entropy and the complexity of the trajecto-
ries of a dynamical system. Trans Moscow Math Soc 44:127
6. Calude CS, Hertling P, Jürgensen H, Weihrauch K (2001)
Randomness on full shift spaces. Chaos, Solitons Fractals
12(3):491–503

Amorphous Computing
A
257
7. Cattaneo G, Formenti E, Margara L, Mazoyer J (1997) A shift-
invariant metric on SZ inducing a non-trivial topology. In: Pri-
vara I, Rusika P (eds) (MFCS’97), vol 1295. Springer, Bratislava,
pp 179–188
8. Cervelle J, Durand B, Formenti E (2001) Algorithmic informa-
tion theory and cellular automata dynamics. In: Mathematical
Foundations of Computer Science (MFCS’01). Lectures Notes
in Computer Science, vol 2136. Springer, Berlin pp 248–259
9. Delorme M, Mazoyer J (1999) Cellular automata as languages
recognizers. In: Cellular automata: A parallel model. Kluwer,
Dordrecht
10. Durand B, Levin L, Shen A (2001) Complex tilings. In: STOC ’01:
Proceedings of the 33rd annual ACM symposium on theory of
computing, pp 732–739
11. Kari J (1994) Rice’s theorem for the limit set of cellular au-
tomata. Theor Comput Sci 127(2):229–254
12. K˚urka P (1997) Languages, equicontinuity and attractors in cel-
lular automata. Ergod Theory Dyn Syst 17:417–433
13. Li M, Vitányi P (1997) An introduction to Kolmogorov complex-
ity and its applications, 2nd edn. Springer, Berlin
14. M Delorme EF, Mazoyer J (2000) Open problems. Research Re-
port LIP 2000-25, Ecole Normale Supérieure de Lyon
15. Pivato M (2005) Cellular automata vs. quasisturmian systems.
Ergod Theory Dyn Syst 25(5):1583–1632
16. Robinson RM (1971) Undecidability and nonperiodicity for
tilings of the plane. Invent Math 12(3):177–209
17. Terrier V (1996) Language not recognizable in real time by one-
way cellular automata. Theor Comput Sci 156:283–287
18. Wolfram S (2002) A new kind of science. Wolfram Media,
Champaign. http://www.wolframscience.com/
Books and Reviews
Batterman RW, White HS (1996) Chaos and algorithmic complexity.
Fund Phys 26(3):307–336
Bennet CH, Gács P, Li M, Vitányi P, Zurek W (1998) Information dis-
tance. EEE Trans Inf Theory 44(4):1407–1423
Calude CS (2002) Information and randomness. Texts in theoretical
computer science, 2nd edn. Springer, Berlin
Cover TM, Thomas JA (2006) Elements of information theory, 2nd
edn. Wiley, New York
White HS (1993) Algorithmic complexity of points in dynamical sys-
tems. Ergod Theory Dyn Syst 13:807–830
Amorphous Computing
HAL ABELSON, JACOB BEAL, GERALD JAY SUSSMAN
Computer Science and Artiﬁcial Intelligence Laboratory,
Massachusetts Institute of Technology, Cambridge, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
The Amorphous Computing Model
Programming Amorphous Systems
Amorphous Computing Paradigms
Primitives for Amorphous Computing
Means of Combination and Abstraction
Supporting Infrastructure and Services
Lessons for Engineering
Future Directions
Bibliography
Glossary
Amorphous computer A collection of computational
particles dispersed irregularly on a surface or through-
out a volume, where individual particles have no a pri-
ori knowledge of their positions or orientations.
Computational particle A (possibly faulty) individual
device for an amorphous computer. Each particle has
modest computing power and a modest amount of
memory. The particles are not synchronized, although
they are all capable of operating at similar speeds, since
they are fabricated by the same process. All particles
are programmed identically, although each particle has
means for storing local state and for generating ran-
dom numbers.
Field A function assigning a value to every particle in an
amorphous computer.
Gradient A basic amorphous computing primitive that
estimates the distance from each particle to the near-
est particle designated as a source of the gradient.
Definition of the Subject
The goal of amorphous computing is to identify orga-
nizational principles and create programming technolo-
gies for obtaining intentional, pre-speciﬁed behavior from
the cooperation of myriad unreliable parts that are ar-
ranged in unknown, irregular, and time-varying ways.
The heightened relevance of amorphous computing today
stems from the emergence of new technologies that could
serve as substrates for information processing systems of
immense power at unprecedentedly low cost, if only we
could master the challenge of programming them.
Introduction
Even as the foundations of computer science were be-
ing laid, researchers could hardly help noticing the con-
trast between the robustness of natural organisms and the
fragility of the new computing devices. As John von Neu-
mann remarked in 1948 [53]:
With our artiﬁcial automata we are moving much
more in the dark than nature appears to be with

258 A
Amorphous Computing
its organisms. We are, and apparently, at least at
present, have to be much more ‘scared’ by the oc-
currence of an isolated error and by the malfunction
which must be behind it. Our behavior is clearly that
of overcaution, generated by ignorance.
Amorphous computing emerged as a ﬁeld in the mid-
1990s, from the convergence of three factors:
 Inspiration from the cellular automata models for fun-
damental physics [13,34].
 Hope that understanding the robustness of biological
development could both help overcome the brittleness
typical of computer systems and also illuminate the
mechanisms of developmental biology.
 The prospect of nearly free computers in vast quanti-
ties.
Microfabrication
One technology that has come to fruition over the past
decade is micro-mechanical electronic component manu-
facture, which integrates logic circuits, micro-sensors, ac-
tuators, and communication on a single chip. Aggregates
of these can be manufactured extremely inexpensively,
provided that not all the chips need work correctly, and
that there is no need to arrange the chips into precise ge-
ometrical conﬁgurations or to establish precise intercon-
nections among them. A decade ago, researchers envi-
sioned smart dust elements small enough to be borne on
air currents to form clouds of communicating sensor par-
ticles [26].
Airborne sensor clouds are still a dream, but networks
of millimeter-scale particles are now commercially avail-
able for environmental monitoring applications [40]. With
low enough manufacturing costs, we could mix such parti-
cles into bulk materials to form coatings like “smart paint”
that can sense data and communicate its actions to the out-
side world. A smart paint coating on a wall could sense
vibrations, monitor the premises for intruders, or cancel
noise. Bridges or buildings coated with smart paint could
report on traﬃc and wind loads and monitor structural in-
tegrity. If the particles have actuators, then the paint could
even heal small cracks by shifting the material around.
Making the particles mobile opens up entire new classes of
applications that are beginning to be explored by research
in swarm robotics [35] and modular robotics [49].
Cellular Engineering
The second disruptive technology that motivates the study
of amorphous computing is microbiology. Biological or-
ganisms have served as motivating metaphors for com-
puting since the days of calculating engines, but it is only
over the past decade that we have begun to see how biol-
ogy could literally be a substrate for computing, through
the possibility of constructing digital-logic circuits within
individual living cells. In one technology logic signals are
represented not by electrical voltages and currents, but by
concentrations of DNA binding proteins, and logic ele-
ments are realized as binding sites where proteins inter-
act through promotion and repression. As a simple exam-
ple, if A and B are proteins whose concentrations repre-
sent logic levels, then an “inverter” can be implemented in
DNA as a genetic unit through which A serves as a repres-
sor that blocks the production of B [29,54,55,61]. Since
cells can reproduce themselves and obtain energy from
their environment, the resulting information processing
units could be manufactured in bulk at a very low cost.
There is beginning to take shape a technology of cel-
lular engineering that can tailor-make programmable cells
to function as sensors or delivery vehicles for pharmaceu-
ticals, or even as chemical factories for the assembly of
nanoscale structures. Researchers in this emerging ﬁeld
of synthetic biology are starting to assemble registries of
standard logical components implemented in DNA that
can be inserted into E. coli [48]. The components have
been engineered to permit standard means of combining
them, so that biological logic designers can assemble cir-
cuits in a mix-and-match way, similar to how electrical
logic designers create circuits from standard TTL parts.
There’s even an International Genetically Engineered Ma-
chine Competition where student teams from universities
around the world compete to create novel biological de-
vices from parts in the registry [24].
Either of these technologies—microfabricated parti-
cles or engineered cells—provides a path to cheaply fab-
ricate aggregates of massive numbers of computing ele-
ments. But harnessing these for computing is quite a dif-
ferent matter, because the aggregates are unstructured.
Digital computers have always been constructed to behave
as precise arrangements of reliable parts, and almost all
techniques for organizing computations depend upon this
precision and reliability. Amorphous computing seeks to
discover new programming methods that do not require
precise control over the interaction or arrangement of the
individual computing elements and to instantiate these
techniques in new programming languages.
The Amorphous Computing Model
Amorphous computing models the salient features of an
unstructured aggregate through the notion of an amor-

Amorphous Computing
A
259
phous computer, a collection of computational particles
dispersed irregularly on a surface or throughout a volume,
where individual particles have no a priori knowledge of
their positions or orientations. The particles are possibly
faulty, may contain sensors and eﬀect actions, and in some
applications might be mobile. Each particle has modest
computing power and a modest amount of memory. The
particles are not synchronized, although they are all capa-
ble of operating at similar speeds, since they are fabricated
by the same process. All particles are programmed iden-
tically, although each particle has means for storing local
state and for generating random numbers. There may also
be several distinguished particles that have been initialized
to particular states.
Each particle can communicate with a few nearby
neighbors. In an electronic amorphous computer the
particles might communicate via short-distance radio,
whereas bioengineered cells might communicate by chem-
ical signals. Although the details of the communication
model can vary, the maximum distance over which two
particles can communicate eﬀectively is assumed to be
small compared with the size of the entire amorphous
computer. Communication is assumed to be unreliable
and a sender has no assurance that a message has been re-
ceived (Higher-level protocols with message acknowledge-
ment can be built on such unreliable channels).
We assume that the number of particles may be very
large (on the order of 106 to 1012). Algorithms appropri-
ate to run on an amorphous computer should be rela-
tively independent of the number of particles: the perfor-
mance should degrade gracefully as the number of parti-
cles decreases. Thus, the entire amorphous computer can
be regarded as a massively parallel computing system, and
previous investigations into massively parallel computing,
such as research in cellular automata, are one source of
ideas for dealing with amorphous computers. However,
amorphous computing diﬀers from investigations into cel-
lular automata, because amorphous mechanisms must be
independent of the detailed conﬁguration, reliability, and
synchronization of the particles.
Programming Amorphous Systems
A central theme in amorphous computing is the search
for programming paradigms that work within the amor-
phous model. Here, biology has been a rich source of
metaphors for inspiring new programming techniques. In
embryonic development, even though the precise arrange-
ments and numbers of the individual cells are highly vari-
able, the genetic “programs” coded in DNA nevertheless
produce well-deﬁned intricate shapes and precise forms.
Amorphous computers should be able to achieve similar
results.
One technique for programming an amorphous com-
puter uses diﬀusion. One particle (chosen by some sym-
metry-breaking process) broadcasts a message. This mes-
sage is received by each of its neighbors, which propa-
gate it to their neighbors, and so on, to create a wave
that spreads throughout the system. The message contains
a count, and each particle stores the received count and
increments it before re-broadcasting. Once a particle has
stored its count, it stops re-broadcasting and ignores fu-
ture count messages. This count-up wave gives each par-
ticle a rough measure of its distance from the original
source. One can also produce regions of controlled size,
by having the count message relayed only if the count is
below a designated bound.
Two such count-up waves can be combined to iden-
tify a chain of particles between two given particles A
and B. Particle A begins by generating a count-up wave
as above. This time, however, each intermediate particle,
when it receives its count, performs a handshake to iden-
tify its “predecessor”—the particle from which it received
the count (and whose own count will therefore be one
less). When the wave of count messages reaches B, B sends
a “successor” message, informing its predecessor that it
should become part of the chain and should send a mes-
sage to its predecessor, and so on, all the way back to A.
Note that this method works, even though the particles
are irregularly distributed, provided there is a path from
A to B.
The motivating metaphor for these two programs is
chemical gradient diﬀusion, which is a foundational mech-
anism in biological development [60]. In nature, biolog-
ical mechanisms not only generate elaborate forms, they
can also maintain forms and repair them. We can mod-
ify the above amorphous line-drawing program so that
it produces self-repairing line: ﬁrst, particles keep re-
broadcasting their count and successor messages. Second,
the status of a particle as having a count or being in the
chain decays over time unless it is refreshed by new mes-
sages. That is, a particle that stops hearing successor mes-
sages intended for it will eventually revert to not being
in the chain and will stop broadcasting its own succes-
sor messages. A particle that stops hearing its count being
broadcast will start acting as if it never had a count, pick up
a new count from the messages it hears, and start broad-
casting the count messages with the new count. Clement
and Nagpal [12] demonstrated that this mechanism can
be used to generate self-repairing lines and other patterns,
and even re-route lines and patterns around “dead” re-
gions where particles have stopped functioning.

260 A
Amorphous Computing
The relationship with biology ﬂows in the other direc-
tion as well: the amorphous algorithm for repair is a model
which is not obviously inconsistent with the facts of angio-
genesis in the repair of wounds. Although the existence of
the algorithm has no bearing on the facts of the matter, it
may stimulate systems-level thinking about models in bi-
ological research. For example, Patel et al. use amorphous
ideas to analyze the growth of epithelial cells [43].
Amorphous Computing Paradigms
Amorphous computing is still in its infancy. Most of lin-
guistic investigations based on the amorphous comput-
ing model have been carried out in simulation. Neverthe-
less, this work has yielded a rich variety of programming
paradigms that demonstrate that one can in fact achieve
robustness in face of the unreliability of individual parti-
cles and the absence of precise organization among them.
Marker Propagation for Amorphous Particles
Weiss’s Microbial Colony Language [55] is a marker prop-
agation language for programming the particles in an
amorphous computer. The program to be executed, which
is the same for each particle, is constructed as a set of rules.
The state of each particle includes a set of binary mark-
ers, and rules are enabled by boolean combinations of the
markers. The rules, which have the form (trigger, condi-
tion, action) are triggered by the receipt of labelled mes-
sages from neighboring particles. A rule may test condi-
tions, set or clear various markers, and it broadcast further
messages to its neighbors. Each message carries a count
that determines how far it will diﬀuse, and each marker
has a lifetime that determines how long its value lasts.
Supporting these language’s rules is a runtime system that
Amorphous Computing, Figure 1
A Microbial Colony Language program organizes a tube into a structure similar to that of somites in the developing vertebrate
(from [55]).
automatically propagates messages and manages the life-
times of markers, so that the programmer need not deal
with these operations explicitly.
Weiss’s system is powerful, but the level of abstrac-
tion is very low. This is because it was motivated by cel-
lular engineering—as something that can be directly im-
plemented by genetic regulatory networks. The language
is therefore more useful as a tool set in which to imple-
ment higher-level languages such as GPL (see below), serv-
ing as a demonstration that in principle, these higher-level
languages can be implemented by genetic regulatory net-
works as well.
Figure 1 shows an example simulation programmed in
this language, that organizes an initially undiﬀerentiated
column of particles into a structure with band of two al-
ternating colors: a caricature of somites in developing ver-
tebrae.
The Growing Point Language
Coore’s Growing Point Language (GPL) [15] demonstrates
that an amorphous computer can be conﬁgured by a pro-
gram that is common to all the computing elements to
generate highly complex patterns, such as the pattern rep-
resenting the interconnection structure of an arbitrary
electrical circuit as shown in Fig. 2.
GPL is inspired by a botanical metaphor based on
growing points and tropisms. A growing point is a lo-
cus of activity in an amorphous computer. A growing
point propagates through the computer by transferring
its activity from one computing element to a neighbor.
As a growing point passes through the computer it ef-
fects the diﬀerentiation of the behaviors of the particles
it visits. Particles secrete “chemical” signals whose count-
up waves deﬁne gradients, and these attract or repel grow-

Amorphous Computing
A
261
Amorphous Computing, Figure 2
A pattern generated by GPL whose shape mimics a chain of CMOS inverters (from [15])
ing points as directed by programmer-speciﬁc “tropisms”.
Coore demonstrated that these mechanisms are suﬃcient
to permit amorphous computers to generate any arbitrary
prespeciﬁed graph structure pattern, up to topology. Un-
like real biology, however, once a pattern has been con-
structed, there is no clear mechanism to maintain it in
the face of changes to the material. Also, from a program-
ming linguistic point of view, there is no clear way to com-
pose shapes by composing growing points. More recently,
Gayle and Coore have shown how GPL may be extended
to produce arbitrarily large patterns such as arbitrary text
strings[21]. D’Hondt and D’Hondt have explored the use
of GPL for geometrical constructions and its relations with
computational geometry [18,19].
Origami-Based Self-Assembly
Nagpal [38] developed a prototype model for control-
ling programmable materials. She showed how to or-
ganize a program to direct an amorphous sheet of de-
formable particles to cooperate to construct a large family
of globally-speciﬁed predetermined shapes. Her method,
which is inspired by the folding of epithelial tissue, allows
a programmer to specify a sequence of folds, where the set
of available folds is suﬃcient to create any origami shape
(as shown by Huzita’s axioms for origami [23]). Figure 3
shows a sheet of amorphous particles, where particles can
cooperate to create creases and folds, assembling itself into
the well-known origami “cup” structure.
Nagpal showed how this language of folds can be com-
piled into a a low-level program that can be distributed
to all of the particles of the amorphous sheet, similar to
Coore’s GPL or Weiss’s MCL. With a few diﬀerences of
initial state (for example, particles at the edges of the sheet
know that they are edge particles) the particles run their
copies of the program, interact with their neighbors, and
fold up to make the predetermined shape. This technique
is quite robust. Nagpal studied the range of shapes that can
be constructed using her method, and on their sensitivity
Amorphous Computing, Figure 3
Folding an envelope structure (from [38]). A pattern of lines is
constructed according to origami axioms. Elements then coordi-
nate to fold the sheet using an actuation model based on epithe-
lial cell morphogenesis. In the figure, black indicates the front
side of the sheet, grey indicates the back side, and the various
colored bands show the folds and creases that are generated by
the amorphous process. The small white spots show gaps in the
sheet cause by “dead” or missing cells—the process works de-
spite these
to errors of communication, random cell death, and den-
sity of the cells.
As a programming framework, the origami language
has more structure than the growing point language, be-
cause the origami methods allow composition of shape
constructions. On the other hand, once a shape has been
constructed, there is no clear mechanism to maintain ex-
isting patterns in the face of changes to the material.

262 A
Amorphous Computing
Dynamic Recruitment
In Butera [10]’s “paintable computing”, processes dynam-
ically recruit computational particles from an amorphous
computer to implement their goals. As one of his examples
Butera uses dynamic recruitment to implement a robust
storage system for streaming audio and images (Fig. 4).
Fragments of the image and audio stream circulate freely
through the amorphous computer and are marshaled to
a port when needed. The audio fragments also sort them-
selves into a playable stream as they migrate to the port.
To enhance robustness, there are multiple copies of
the lower resolution image fragments and fewer copies of
the higher resolution image fragments. Thus, the image is
hard to destroy; with lost fragments the image is degraded
but not destroyed.
Clement and Nagpal [12] also use dynamic recruit-
ment in the development of active gradients, as described
below.
Growth and Regeneration
Kondacs [30] showed how to synthesize arbitrary two-
dimensional shapes by growing them. These computing
units, or “cells”, are identically-programmed and decen-
tralized, with the ability to engage in only limited, local
communication. Growth is implemented by allowing cells
to multiply. Each of his cells may create a child cell and
place it randomly within a ring around the mother cell.
Cells may also die, a fact which Kondacs puts to use for
temporary scaﬀolding when building complex shapes. If
a structure requires construction of a narrow neck be-
tween two other structures it can be built precisely by lay-
ing down a thick connection and later trimming it to size.
Attributes of this system include scalability, robust-
ness, and the ability for self-repair. Just as a starﬁsh can re-
Amorphous Computing, Figure 4
Butera dynamically controls the flow of information through an amorphous computer. In a image fragments spread through the
computer so that a degraded copy can be recovered from any segment; the original image is on the left, the blurry copy on the right
has been recovered from the small region shown below. In b audio fragments sort themselves into a playable stream as they migrate
to an output port; cooler colors are earlier times (from [10]).
generate its entire body from part of a limb, his system can
self-repair in the event of agent death: his sphere-network
representation allows the structure to be grown starting
from any sphere, and every cell contains all necessary in-
formation for reproducing the missing structure.
Abstraction to Continuous Space and Time
The amorphous model postulates computing particles dis-
tributed throughout a space. If the particles are dense, one
can imagine the particles as actually ﬁlling the space, and
create programming abstractions that view the space itself
as the object being programmed, rather than the collection
of particles. Beal and Bachrach [1,7] pursued this approach
by creating a language, Proto, where programmers specify
the behavior of an amorphous computer as though it were
a continuous material ﬁlling the space it occupies. Proto
programs manipulate ﬁelds of values spanning the entire
space. Programming primitives are designed to make it
simple to compile global operations to operations at each
point of the continuum. These operations are approxi-
mated by having each device represent a nearby chunk
of space. Programs are speciﬁed in space and time units
that are independent of the distribution of particles and of
the particulars of communication and execution on those
particles (Fig. 5). Programs are composed functionally,
and many of the details of communication and composi-
tion are made implicit by Proto’s runtime system, allow-
ing complex programs to be expressed simply. Proto has
been applied to applications in sensor networks like target
tracking and threat avoidance, to swarm robotics and to
modular robotics, e. g., generating a planar wave for coor-
dinated actuation.
Newton’s language Regiment [41,42] also takes a con-
tinuous view of space and time. Regiment is organized in
terms of stream operations, where each stream represents

Amorphous Computing
A
263
Amorphous Computing, Figure 5
A tracking program written in Proto sends the location of a target region (orange) to a listener (red) along a channel (small red dots)
in the network (indicated by green lines). The continuous space and time abstraction allows the same program to run at different
resolutions
a time-varying quantity over a part of space, for exam-
ple, the average value of the temperature over a disc of
a given radius centered at a designated point. Regiment,
also a functional language, is designed to gather streams
of data from regions of the amorphous computer and ac-
cumulate them at a single point. This assumption allows
Regiment to provide region-wide summary functions that
are diﬃcult to implement in Proto.
Primitives for Amorphous Computing
The previous section illustrated some paradigms that have
been developed for programming amorphous systems,
each paradigm building on some organizing metaphor.
But eventually, meeting the challenge of amorphous sys-
tems will require a more comprehensive linguistic frame-
work. We can approach the task of creating such a frame-
work following the perspective in [59], which views lan-
guages in terms of primitives, means of combination, and
means of abstraction.
The fact that amorphous computers consist of vast
numbers of unreliable and unsynchronized particles, ar-
ranged in space in ways that are locally unknown, con-
strains the primitive mechanisms available for organizing
cooperation among the particles. While amorphous com-
puters are naturally massively parallel, the kind of com-
putation that they are most suited for is parallelism that
does not depend on explicit synchronization and the use
of atomic operations to control concurrent access to re-
sources. However, there are large classes of useful behav-
iors that can be implemented without these tools. Prim-
itive mechanisms that are appropriate for specifying be-
havior on amorphous computers include gossip, random
choice, ﬁelds, and gradients.
Gossip
Gossip, also known as epidemic communication [17,20],
is a simple communication mechanism. The goal of a gos-
sip computation is to obtain an agreement about the value
of some parameter. Each particle broadcasts its opinion of
the parameter to its neighbors, and computation is per-
formed by each particle combining the values that it re-
ceives from its neighbors, without consideration of the
identiﬁcation of the source. If the computation changes
a particle’s opinion of the value, it rebroadcasts its new
opinion. The process concludes when the are no further
broadcasts.
For example, an aggregate can agree upon the mini-
mum of the values held by all the particles as follows. Each
particle broadcasts its value. Each recipient compares its
current value with the value that it receives. If the received
value is smaller than its current value, it changes its current
value to that minimum and rebroadcasts the new value.
The advantage of gossip is that it ﬂows in all directions
and is very diﬃcult to disrupt. The disadvantage is that the
lack of source information makes it diﬃcult to revise a de-
cision.
Random Choice
Random choice is used to break symmetry, allowing the
particles to diﬀerentiate their behavior. The simplest use
of random choice is to establish local identity of parti-
cles: each particle chooses a random number to identify
itself to its neighbors. If the number of possible choices
is large enough, then it is unlikely that any nearby parti-
cles will choose the same number, and this number can
thus be used as an identiﬁer for the particle to its neigh-
bors. Random choice can be combined with gossip to elect

264 A
Amorphous Computing
leaders, either for the entire system or for local regions.
(If collisions in choice can be detected, then the number
of choices need not be much higher than then number of
neighbors. Also, using gossip to elect leaders makes sense
only when we expect a leader to be long-lived, due to the
diﬃculty of changing the decision to designate a replace-
ment leader.)
To elect a single leader for the entire system, every par-
ticle chooses a value, then gossips to ﬁnd the minimum.
The particle with the minimum value becomes the leader.
To elect regional leaders, we instead use gossip to carry the
identity of the ﬁrst leader a particle has heard of. Each par-
ticle uses random choice as a “coin ﬂip” to decide when to
declare itself a leader; if the ﬂip comes up heads enough
times before the particle hears of another leader, the par-
ticle declares itself a leader and broadcasts that fact to its
neighbors. The entire system is thus broken up into con-
tiguous domains of particles who ﬁrst heard some partic-
ular particle declare itself a leader.
One challenge in using random choice on an amor-
phous computer is to ensure that the particulars of particle
distribution do not have an unexpected eﬀect on the out-
come. For example, if we wish to control the expected size
of the domain that each regional leader presides over, then
the probability of becoming a leader must depend on the
density of the particles.
Fields
Every component of the state of the computational par-
ticles in an amorphous computer may be thought of as
a ﬁeld over the discrete space occupied by those particles.
If the density of particles is large enough this ﬁeld of val-
ues may be thought of as an approximation of a ﬁeld on
the continuous space.
We can make amorphous models that approximate
the solutions of the classical partial-diﬀerential equations
of physics, given appropriate boundary conditions. The
amorphous methods can be shown to be consistent, con-
vergent and stable.
For example, the algorithm for solving the Laplace
equation with Dirichlet conditions is analogous to the way
it would be solved on a lattice. Each particle must repeat-
edly update the value of the solution to be the average of
the solutions posted by its neighbors, but the boundary
points must not change their values. This algorithm will
eventually converge, although very slowly, independent of
the order of the updates and the details of the local con-
nectedness of the network. There are optimizations, such
as over-relaxation, that are just as applicable in the amor-
phous context as on a regular grid.
Katzenelson [27] has shown similar results for the
diﬀusion equation, complete with analytic estimates of
the errors that arise from the discrete and irregularly
connected network. In the diﬀusion equation there is
a conserved quantity, the amount of material diﬀusing.
Rauch [47] has shown how this can work with the wave
equation, illustrating that systems that conserve energy
and momentum can also be eﬀectively modeled with an
amorphous computer. The simulation of the wave equa-
tion does require that the communicating particles know
their relative positions, but it is not hard to establish local
coordinate systems.
Gradients
An important primitive in amorphous computing is the
gradient, which estimates the distance from each particle
to the nearest particle designated as a source. The gradient
is inspired by the chemical-gradient diﬀusion process that
is crucial to biological development. Amorphous comput-
ing builds on this idea, but does not necessarily compute
the distance using diﬀusion because simulating diﬀusion
can be expensive.
The common alternative is a linear-time mechanism
that depends on active computation and relaying of infor-
mation rather than passive diﬀusion. Calculation of a gra-
dient starts with each source particle setting its distance es-
timate to zero, and every other particle setting its distance
estimate to inﬁnity. The sources then broadcast their esti-
mates to their neighbors. When a particle receives a mes-
sage from its neighbor, it compares its current distance es-
timate to the distance through its neighbor. If the distance
through its neighbor is less, it chooses that to be its esti-
mate, and broadcasts its new estimate onwards.
Although the basic form of the gradient is simple, there
are several ways in which gradients can be varied to better
match the context in which they are used. These choices
may be made largely independently, giving a wide vari-
ety of options when designing a system. Variations which
have been explored include:
Active Gradients
An active gradient [12,14,16] moni-
tors its validity in the face of changing sources and de-
vice failure, and maintains correct distance values. For ex-
ample, if the supporting sources disappear, the gradient is
deallocated. A gradient may also carry version informa-
tion, allowing its source to change more smoothly.
Active gradients can provide self-repairing coordinate
systems, as a foundation for robust construction of pat-
terns. Figure 6 shows the “count-down wave” line de-
scribed above in the introduction to this article. The line’s

Amorphous Computing
A
265
Amorphous Computing, Figure 6
A line being maintained by active gradients, from [12]. A line (black) is constructed between two anchor regions (dark grey) based on
the active gradient emitted by the right anchor region (light grays). The line is able to rapidly repair itself following failures because
the gradient actively maintains itself
implementation in terms of active gradients provides for
self-repair when the underlying amorphous computer is
damaged.
Polarity
A gradient may be set to count down from
a positive value at the source, rather than to count up from
zero. This bounds the distance that the gradient can span,
which can help limit resource usage, but may limit the scal-
ability of programs.
Adaptivity
As described above, a gradient relaxes once
to a distance estimate. If communication is expensive and
precision unimportant, the gradient can take the ﬁrst value
that arrives and ignore all subsequent values. If we want
the gradient to adapt to changes in the distribution of par-
ticles or sources, then the particles need to broadcast at
regular intervals. We can then have estimates that con-
verge smoothly to precise estimates by adding a restoring
force which acts opposite to the relaxation, allowing the
gradient to rise when unconstrained by its neighbors. If, on
the other hand, we value adaptation speed over smooth-
ness, then each particle can recalculate its distance esti-
mate from scratch with each new batch of values.
Carrier
Normally, the distance value calculated by the
gradient is the signal we are interested in. A gradient may
instead be used to carry an arbitrary signal outward from
the source. In this case, the value at each particle is the
most recently arrived value from the nearest source.
Distance Measure
A gradient’s distance measure is, of
course, dependent on how much knowledge we have about
the relative positions of neighbors. It is sometimes advan-
tageous to discard good information and use only hop-
count values, since it is easier to make an adaptive gradient
using hop-count values. Non-linear distance measures are
also possible, such as a count-down gradient that decays
exponentially from the source. Finally, the value of a gra-
dient may depend on more sources than the nearest (this is
the case for a chemical gradient), though this may be very
expensive to calculate.
Coordinates and Clusters
Computational particles may be built with restrictions
about what can be known about local geometry. A parti-
cle may know that it can reliably communicate with a few
neighbors. If we assume that these neighbors are all within
a disc of some approximate communication radius then
distances to others may be estimated by minimum hop
count [28]. However, it is possible that more elaborate par-
ticles can estimate distances to near neighbors. For exam-
ple, the Cricket localization system [45] uses the fact that
sound travels more slowly than radio, so the distance is es-
timated by the diﬀerence in time of arrival between simul-
taneously transmitted signals. McLurkin’s swarmbots [35]
use the ISIS communication system that gives bearing and
range information. However, it is possible for a suﬃciently
dense amorphous computer to produce local coordinate
systems for its particles with even the crudest method of
determining distances. We can make an atlas of overlap-
ping coordinate systems, using random symmetry break-
ing to make new starting baselines [3]. These coordinate
systems can be combined and made consistent to form
a manifold, even if the amorphous computer is not ﬂat or
simply connected.

266 A
Amorphous Computing
One way to establish coordinates is to choose two ini-
tial particles that are a known distance apart. Each one
serves as the source of a gradient. A pair of rectangular
axes can be determined by the shortest path between them
and by a bisector constructed where the two gradients are
equal. These may be reﬁned by averaging and calibrated
using the known distance between the selected particles.
After the axes are established, they may source new gra-
dients that can be combined to make coordinates for the
region near these axes. The coordinate system can be fur-
ther reﬁned using further averaging. Other natural coordi-
nate constructions are bipolar elliptical. This kind of con-
struction was pioneered by Coore [14] and Nagpal [37].
Katzenelson [27] did early work to determine the kind of
accuracy that can be expected from such a construction.
Spatial clustering can be accomplished with any of
a wide variety of algorithms, such as the clubs algo-
rithm [16], LOCI [36], or persistent node partitioning [4].
Clusters can themselves be clustered, forming a hierarchi-
cal clustering of logarithmic height.
Means of Combination and Abstraction
A programming framework for amorphous systems re-
quires more than primitive mechanisms. We also need
suitable means of combination, so that programmers can
combine behaviors to produce more complex behaviors,
and means of abstraction so that the compound behaviors
can be named and manipulated as units. Here are a few
means of combination that have been investigated with
amorphous computing.
Spatial and Temporal Sequencing
Several behaviors can be strung together in a sequence.
The challenge in controlling such a sequence is to deter-
mine when one phase has completed and it is safe to move
on to the next. Trigger rules can be used to detect comple-
tion locally.
In Coore’s Growing Point Language [15], all of the se-
quencing decisions are made locally, with diﬀerent grow-
ing points progressing independently. There is no diﬃ-
culty of synchronization in this approach because the only
time when two growing points need to agree is when they
have become spatially coincident. When growing points
merge, the independent processes are automatically syn-
chronized.
Nagpal’s origami language [38] has long-range op-
erations that cannot overlap in time unless they are in
non-interacting regions of the space. The implementation
uses barrier synchronization to sequence the operations:
when completion is detected locally, a signal is propagated
throughout a marked region of the sheet, and the next op-
eration begins after a waiting time determined by the di-
ameter of the sheet.
With adaptive gradients, we can use the presence of
an inducing signal to run an operation. When the induc-
tion signal disappears, the operation ceases and the parti-
cles begin the next operation. This allows sequencing to be
triggered by the last detection of completion rather than
by the ﬁrst.
Pipelining
If a behavior is self-stabilizing (meaning that it converges
to a correct state from any arbitrary state) then we can
use it in a sequence without knowing when the previ-
ous phase completes. The evolving output of the previous
phase serves as the input of this next phase, and once the
preceding behavior has converged, the self-stabilizing be-
havior will converge as well.
If the previous phase evolves smoothly towards its ﬁnal
state, then by the time it has converged, the next phase may
have almost converged as well, working from its partial re-
sults. For example, the coordinate system mechanism de-
scribed above can be pipelined; the ﬁnal coordinates are
being formed even as the farther particles learn that they
are not on one of the two axes.
Restriction to Spatial Regions
Because the particles of an amorphous computer are dis-
tributed in space it is natural to assign particular behaviors
to speciﬁc spatial regions. In Beal and Bachrach’s work,
restriction of a process to a region is a primitive[7]. As
another example, when Nagpal’s system folds an origami
construction, regions on diﬀerent faces may diﬀerentiate
so that they fold in diﬀerent patterns. These folds may, if
the physics permits, be performed simultaneously. It may
be necessary to sequence later construction that depends
on the completion of the substructures.
Regions of space can be named using coordinates, clus-
tering, or implicitly through calculations on ﬁelds. In-
deed, one could implement solid modelling on an amor-
phous computer. Once a region is identiﬁed, a particle can
test whether it is a member of that region when deciding
whether to run a behavior. It is also necessary to specify
how a particle should change its behavior if its member-
ship in a region may vary with time.
Modularity and Abstraction
Standard means of abstraction may be applied in an amor-
phous computing context, such as naming procedures,

Amorphous Computing
A
267
data structures, and processes. The question for amor-
phous computing is what collection of entities is useful to
name.
Because geometry is essential in an amorphous com-
puting context, it becomes appropriate to describe com-
putational processes in terms of geometric entities. Thus
there are new opportunities for combining geometric
structures and naming the combinations. For example, it
is appropriate to compute with, combine, and name re-
gions of space, intervals of time, and ﬁelds deﬁned on
them [7,42]. It may also be useful to describe the propa-
gation of information through the amorphous computer
in terms of the light cone of an event [2].
Not all traditional abstractions extend nicely to an
amorphous computing context because of the challenges
of scale and the fallibility of parts and interconnect. For
example, atomic transactions may be excessively expensive
in an amorphous computing context. And yet, some of the
goals that a programmer might use atomic transactions to
accomplish, such as the approximate enforcement of con-
servation laws, can be obtained using techniques that are
compatible with an amorphous environment, as shown by
Rauch [47].
Supporting Infrastructure and Services
Amorphous computing languages, with their primitives,
means of combination, and means of abstraction, rest on
supporting services. One example, described above, is the
automatic message propagation and decay in Weiss’s Mi-
crobial Colony Language [55]. MCL programs do not need
to deal with this explicitly because it is incorporated into
the operating system of the MCL machine. Experience
with amorphous computing is beginning to identify other
key services that amorphous machines must supply.
Particle Identity
Particles must be able to choose identiﬁers for commu-
nicating with their neighbors. More generally, there are
many operations in an amorphous computation where the
particles may need to choose numbers, with the property
that individual particles choose diﬀerent numbers.
If we are willing to pay the cost, it is possible to
build unique identiﬁers into the particles, as is done with
current macroscopic computers. We need only locally
unique identiﬁers, however, so we can obtain them using
pseudorandom-number generators. On the surface of it,
this may seem problematic, since the particles in the amor-
phous are assumed to be manufactured identically, with
identical programs. There are, however, ways to obtain in-
dividualized random numbers. For example, the particles
are not synchronized, and they are not really physically
identical, so they will run at slightly diﬀerent rates. This
diﬀerence is enough to allow pseudorandom-number gen-
erators to get locally out of synch and produce diﬀerent se-
quences of numbers. Amorphous computing particles that
have sensors may also get seeds for their pseudorandom-
number generators from sensor noise.
Local Geometry and Gradients
Particles must maintain connections with their neighbors,
tracking who they can reliably communicate with, and
whatever local geometry information is available. Because
particles may fail or move, this information needs to be
maintained actively. The geometry information may in-
clude distance and bearing to each neighbor, as well as
the time it takes to communicate with each neighbor. But
many implementations will not be able to give signiﬁcant
distance or bearing information. Since all of this informa-
tion may be obsolete or inaccurately measured, the parti-
cles must also maintain information on how reliable each
piece of information is.
An amorphous computer must know the dimension
of the space it occupies. This will generally be a con-
stant—either the computer covers a surface or ﬁlls a vol-
ume. In rare cases, however, the eﬀective dimension of
a computer may change: for example, paint is three-
dimensional in a bucket and two-dimensional once ap-
plied. Combining this information with how the num-
ber of accessible correspondents changes with distance, an
amorphous process can derive curvature and local density
information.
An amorphous computer should also support gradi-
ent propagation as part of the infrastructure: a program-
mer should not have to explicitly deal with the propaga-
tion of gradients (or other broadcast communications) in
each particle. A process may explicitly initiate a gradient,
or explicitly react to one that it is interested in, but the
propagation of the gradient through a particle should be
automatically maintained by the infrastructure.
Implementing Communication
Communication between neighbors can occur through
any number of mechanisms, each with its own set of prop-
erties: amorphous computing systems have been built that
communicate through directional infrared [35], RF broad-
cast [22], and low-speed serial cables [9,46]. Simulated sys-
tems have also included other mechanisms such as signals
superimposed on the power connections [11] and chemi-
cal diﬀusion [55].

268 A
Amorphous Computing
Communication between particles can be made im-
plicit with a neighborhood shared memory. In this ar-
rangement, each particle designates some of its internal
state to be shared with its neighbors. The particles regu-
larly communicate, giving each particle a best-eﬀort view
of the exposed portions of the states of its neighbors. The
contents of the exposed state may be speciﬁed explic-
itly [10,35,58] or implicitly [7]. The shared memory allows
the system to be tuned by trading oﬀcommunication rate
against the quality of the synchronization, and decreasing
transmission rates when the exposed state is not chang-
ing.
Lessons for Engineering
As von Neumann remarked half a century ago, biological
systems are strikingly robust when compared with our ar-
tiﬁcial systems. Even today software is fragile. Computer
science is currently built on a foundation that largely as-
sumes the existence of a perfect infrastructure. Integrated
circuits are fabricated in clean-room environments, tested
deterministically, and discarded if even a single defect is
uncovered. Entire software systems fail with single-line er-
rors. In contrast, biological systems rely on local computa-
tion, local communication, and local state, yet they exhibit
tremendous resilience.
Although this contrast is most striking in com-
puter science, amorphous computing can provide lessons
throughout engineering. Amorphous computing concen-
trates on making systems ﬂexible and adaptable at the ex-
pense of eﬃciency. Amorphous computing requires an en-
gineer to work under extreme conditions. The engineer
must arrange the cooperation of vast numbers of identical
computational particles to accomplish prespeciﬁed goals,
but may not depend upon the numbers. We may not de-
pend on any prespeciﬁed interconnect of the particles. We
may not depend on synchronization of the particles. We
may not depend on the stability of the communications
system. We may not depend on the long-term survival of
any individual particles. The combination of these obsta-
cles forces us to abandon many of the comforts that are
available in more typical engineering domains.
By restricting ourselves in this way we obtain some ro-
bustness and ﬂexibility, at the cost of potentially ineﬃcient
use of resources, because the algorithms that are appropri-
ate are ones that do not take advantage of these assump-
tions. Algorithms that work well in an amorphous con-
text depend on the average behavior of participating par-
ticles. For example, in Nagpal’s origami system a fold that
is speciﬁed will be satisfactory if it is approximately in the
right place and if most of the particles on the speciﬁed fold
line agree that they are part of the fold line: dissenters will
be overruled by the majority. In Proto a programmer can
address only regions of space, assumed to be populated by
many particles. The programmer may not address individ-
ual particles, so failures of individual particles are unlikely
to make major perturbations to the behavior of the system.
An amorphous computation can be quite immune to
details of the macroscopic geometry as well as to the in-
terconnectedness of the particles. Since amorphous com-
putations make their own local coordinate systems, they
are relatively independent of coordinate distortions. In an
amorphous computation we accept a wide range of out-
comes that arise from variations of the local geometry.
Tolerance of local variation can lead to surprising ﬂexi-
bility: the mechanisms which allow Nagpal’s origami lan-
guage to tolerate local distortions allow programs to dis-
tort globally as well, and Nagpal shows how such varia-
tions can account for the variations in the head shapes of
related species of Drosophila [38]. In Coore’s language one
speciﬁes the topology of the pattern to be constructed, but
only limited information about the geometry. The topol-
ogy will be obtained, regardless of the local geometry, so
long as there is suﬃcient density of particles to support
the topology. Amorphous computations based on a con-
tinuous model of space (as in Proto) are naturally scale in-
dependent.
Since an amorphous computer is composed of un-
synchronized particles, a program may not depend upon
a priori timing of events. The sequencing of phases of
a process must be determined by either explicit termina-
tion signals or with times measured dynamically. So amor-
phous computations are time-scale independent by con-
struction.
A program for an amorphous computer may not de-
pend on the reliability of the particles or the communica-
tion paths. As a consequence it is necessary to construct
the program so as to dynamically compensate for failures.
One way to do this is to specify the result as the satisfaction
of a set of constraints, and to build the program as a home-
ostatic mechanism that continually drives the system to-
ward satisfaction of those constraints. For example, an ac-
tive gradient continually maintains each particle’s estimate
of the distance to the source of the gradient. This can be
used to establish and maintain connections in the face of
failures of particles or relocation of the source. If a system
is speciﬁed in this way, repair after injury is a continuation
of the development process: an injury causes some con-
straints to become unsatisﬁed, and the development pro-
cess builds new structure to heal the injury.
By restricting the assumptions that a programmer can
rely upon we increase the ﬂexibility and reliability of the

Amorphous Computing
A
269
programs that are constructed. However, it is not yet clear
how this limits the range of possible applications of amor-
phous computing.
Future Directions
Computer hardware is almost free, and in the future it will
continue to decrease in price and size. Sensors and actu-
ators are improving as well. Future systems will have vast
numbers of computing mechansisms with integrated sen-
sors and actuators, to a degree that outstrips our current
approaches to system design. When the numbers become
large enough, the appropriate programming technology
will be amorphous computing. This transition has already
begun to appear in several ﬁelds:
Sensor networks The success of sensor network research
has encouraged the planning and deployment of ever-
larger numbers of devices. The ad-hoc, time-varying
nature of sensor networks has encouraged amorphous
approaches, such as communication through directed
diﬀusion [25] and Newton’s Regiment language [42].
Robotics Multi-agent robotics is much like sensor net-
works, except that the devices are mobile and have ac-
tuators. Swarm robotics considers independently mo-
bile robots working together as a team like ants or
bees, while modular robotics consider robots that phys-
ically attach to one another in order to make shapes or
perform actions, working together like the cells of an
organism. Gradients are being used to create “ﬂock-
ing” behaviors in swarm robotics [35,44]. In modular
robotics, Stoy uses gradients to create shapes [51] while
De Rosa et al. form shapes through stochastic growth
and decay [49].
Pervasive computing Pervasive computing seeks to ex-
ploit the rapid proliferation of wireless computing de-
vices throughout our everyday environment. Mamei
and Zambonelli’s TOTA system [32] is an amor-
phous computing implementation supporting a model
of programming using ﬁelds and gradients [33]. Ser-
vat and Drogoul have suggested combining amor-
phous computing and reactive agent-based systems
to produce something they call “pervasive intelli-
gence” [50].
Multicore processors As it becomes more diﬃcult to
increase processor speed, chip manufacturers are
looking for performance gains through increasing
the number of processing cores per chip. Butera’s
work [10] looks toward a future in which there are
thousands of cores per chip and it is no longer rea-
sonable to assume they are all working or have them
communicate all-to-all.
While much of amorphous computing research is inspired
by biological observations, it is also likely that insights
and lessons learned from programming amorphous com-
puters will help elucidate some biological problems [43].
Some of this will be stimulated by the emerging engineer-
ing of biological systems. Current work in synthetic biol-
ogy [24,48,61] is centered on controlling the molecular bi-
ology of cells. Soon synthetic biologists will begin to engi-
neer bioﬁlms and perhaps direct the construction of multi-
cellular organs, where amorphous computing will become
an essential technological tool.
Bibliography
Primary Literature
1. Bachrach J, Beal J (2006) Programming a sensor network as an
amorphous medium. In: DCOSS 2006 Posters, June 2006
2. Bachrach J, Beal J, Fujiwara T (2007) Continuous space-time
semantics allow adaptive program execution. In: IEEE Interna-
tional Conference on Self-Adaptive and Self-Organizing Sys-
tems, 2007
3. Bachrach J, Nagpal R, Salib M, Shrobe H (2003) Experimental
results and theoretical analysis of a self-organizing global co-
ordinate system for ad hoc sensor networks. Telecommun Syst
J, Special Issue on Wireless System Networks 26(2–4):213–233
4. Beal J (2003) A robust amorphous hierarchy from persistent
nodes. In: Commun Syst Netw
5. Beal J (2004) Programming an amorphous computational
medium. In: Unconventional Programming Paradigms Interna-
tional Workshop, September 2004
6. Beal J (2005) Amorphous medium language. In: Large-Scale
Multi-Agent Systems Workshop (LSMAS). Held in Conjunction
with AAMAS-05
7. Beal J, Bachrach J (2006) Infrastructure for engineered emer-
gence on sensor/actuator networks. In: IEEE Intelligent Sys-
tems, 2006
8. Beal J, Sussman G (2005) Biologically-inspired robust spatial
programming. Technical Report AI Memo 2005-001, MIT, Jan-
uary 2005
9. Beebee W M68hc11 gunk api book. http://www.swiss.ai.mit.
edu/projects/amorphous/HC11/api.html. Accessed 31 May
2007
10. Butera W (2002) Programming a Paintable Computer. PhD the-
sis, MIT
11. Campbell J, Pillai P, Goldstein SC (2005) The robot is the tether:
Active, adaptive power routing for modular robots with unary
inter-robot connectors. In: IROS 2005
12. Clement L, Nagpal R (2003) Self-assembly and self-repairing
topologies. In: Workshop on Adaptability in Multi-Agent Sys-
tems, RoboCup Australian Open, January 2003
13. Codd EF (1968) Cellular Automata. Academic Press, New York
14. Coore D (1998) Establishing a coordinate system on an amor-
phous computer. In: MIT Student Workshop on High Perfor-
mance Computing, 1998
15. Coore D (1999) Botanical Computing: A Developmental Ap-
proach to Generating Interconnect Topologies on an Amor-
phous Computer. Ph D thesis, MIT

270 A
Amorphous Computing
16. Coore D, Nagpal R, Weiss R (1997) Paradigms for structure in an
amorphous computer. Technical Report AI Memo 1614, MIT
17. Demers A, Greene D, Hauser C, Irish W, Larson J, Shenker S,
Stuygis H, Swinehart D, Terry D (1987) Epidemic algorithms for
replicated database maintenance. In: 7th ACM Symposium on
Operating Systems Principles, 1987
18. D’Hondt E, D’Hondt T (2001) Amorphous geometry. In: ECAL
2001
19. D’Hondt E, D’Hondt T (2001) Experiments in amorphous geom-
etry. In: 2001 International Conference on Artificial Intelligence
20. Ganesan D, Krishnamachari B, Woo A, Culler D, Estrin D, Wicker
S (2002) An empirical study of epidemic algorithms in large
scale multihop wireless networks. Technical Report IRB-TR-02-
003, Intel Research Berkeley
21. Gayle O, Coore D (2006) Self-organizing text in an amorphous
environment. In: ICCS 2006
22. Hill J, Szewcyk R, Woo A, Culler D, Hollar S, Pister K (2000) Sys-
tem architecture directions for networked sensors. In: ASPLOS,
November 2000
23. Huzita H, Scimemi B (1989) The algebra of paper-folding. In:
First International Meeting of Origami Science and Technol-
ogy, 1989
24. igem 2006: international genetically engineered machine
competition (2006) http://www.igem2006.com. Accessed 31
May 2007
25. Intanagonwiwat C, Govindan R, Estrin D (2000) Directed dif-
fusion: a scalable and robust communication paradigm for
sensor networks. In: Mobile Computing and Networking,
pp 56–67
26. Kahn JM, Katz RH, Pister KSJ (1999) Mobile networking for
smart dust. In: ACM/IEEE Int. Conf. on Mobile Computing and
Networking (MobiCom 99), August 1999
27. Katzenelson J (1999) Notes on amorphous computing. (Un-
published Draft)
28. Kleinrock L, Sylvester J (1978) Optimum transmission radii for
packet radio networks or why six is a magic number. In: IEEE
Natl Telecommun. Conf, December 1978, pp 4.3.1–4.3.5
29. Knight TF, Sussman GJ (1998) Cellular gate technology. In: First
International Conference on Unconventional Models of Com-
putation (UMC98)
30. Kondacs A (2003) Biologically-inspired self-assembly of 2d
shapes, using global-to-local compilation. In: International
Joint Conference on Artificial Intelligence (IJCAI)
31. Mamei M, Zambonelli F (2003) Spray computers: Frontiers of
self-organization for pervasive computing. In: WOA 2003
32. Mamei M, Zambonelli F (2004) Spatial computing: the tota ap-
proach. In: WOA 2004, pp 126–142
33. Mamei M, Zambonelli F (2005) Physical deployment of dig-
ital pheromones through rfid technology. In: AAMAS 2005,
pp 1353–1354
34. Margolus N (1988) Physics and Computation. Ph D thesis, MIT
35. McLurkin J (2004) Stupid robot tricks: A behavior-based dis-
tributed algorithm library for programming swarms of robots.
Master’s thesis, MIT
36. Mittal V, Demirbas M, Arora A (2003) Loci: Local clustering ser-
vice for large scale wireless sensor networks. Technical Report
OSU-CISRC-2/03-TR07, Ohio State University
37. Nagpal R (1999) Organizing a global coordinate system from
local information on an amorphous computer. Technical Re-
port AI Memo 1666, MIT
38. Nagpal R (2001) Programmable Self-Assembly: Constructing
Global Shape using Biologically-inspired Local Interactions
and Origami Mathematics. Ph D thesis, MIT
39. Nagpal R, Mamei M (2004) Engineering amorphous comput-
ing systems. In: Bergenti F, Gleizes MP, Zambonelli F (eds)
Methodologies and Software Engineering for Agent Systems,
The Agent-Oriented Software Engineering Handbook. Kluwer,
New York, pp 303–320
40. Dust Networks. http://www.dust-inc.com. Accessed 31 May
2007
41. Newton R, Morrisett G, Welsh M (2007) The regiment macro-
programming system. In: International Conference on Infor-
mation Processing in Sensor Networks (IPSN’07)
42. Newton R, Welsh M (2004) Region streams: Functional macro-
programming for sensor networks. In: First International Work-
shop on Data Management for Sensor Networks (DMSN), Au-
gust 2004
43. Patel A, Nagpal R, Gibson M, Perrimon N (2006) The emergence
of geometric order in proliferating metazoan epithelia. Nature
442:1038–1041
44. Payton D, Daily M, Estowski R, Howard M, Lee C (2001)
Pheromone robotics. Autonomous Robotics 11:319–324
45. Priyantha N, Chakraborty A, Balakrishnan H (2000) The cricket
location-support system. In: ACM International Conference on
Mobile Computing and Networking (ACM MOBICOM), August
2000
46. Raffle H, Parkes A, Ishii H (2004) Topobo: A constructive assem-
bly system with kinetic memory. CHI
47. Rauch E (1999) Discrete, amorphous physical models. Master’s
thesis, MIT
48. Registry of standard biological parts. http://parts.mit.edu. Ac-
cessed 31 May 2007
49. De Rosa M, Goldstein SC, Lee P, Campbell J, Pillai P (2006)
Scalable shape sculpting via hole motion: Motion planning
in lattice-constrained module robots. In: Proceedings of the
2006 IEEE International Conference on Robotics and Automa-
tion (ICRA ‘06), May 2006
50. Servat D, Drogoul A (2002) Combining amorphous computing
and reactive agent-based systems: a paradigm for pervasive in-
telligence? In: AAMAS 2002
51. Stoy K (2003) Emergent Control of Self-Reconfigurable Robots.
Ph D thesis, University of Southern Denmark
52. Sutherland A (2003) Towards rseam: Resilient serial execution
on amorphous machines. Master’s thesis, MIT
53. von Neumann J (1951) The general and logical theory of au-
tomata. In: Jeffress L (ed) Cerebral Mechanisms for Behavior.
Wiley, New York, p 16
54. Weiss R, Knight T (2000) Engineered communications for mi-
crobial robotics. In: Sixth International Meeting on DNA Based
Computers (DNA6)
55. Weiss R (2001) Cellular Computation and Communications us-
ing Engineered Genetic Regular Networks. Ph D thesis, MIT
56. Welsh M, Mainland G (2004) Programming sensor networks us-
ing abstract regions. In: Proceedings of the First USENIX/ACM
Symposium on Networked Systems Design and Implementa-
tion (NSDI ’04), March 2004
57. Werfel J, Bar-Yam Y, Nagpal R (2005) Building patterned struc-
tures with robot swarms. In: IJCAI
58. Whitehouse K, Sharp C, Brewer E, Culler D (2004) Hood:
a neighborhood abstraction for sensor networks. In: Proceed-
ings of the 2nd international conference on Mobile systems,
applications, and services.

Analog Computation
A
271
59. Abelson H, Sussman GJ, Sussman J (1996) Structure and In-
terpretation of Computer Programs, 2nd edn. MIT Press, Cam-
bridge
60. Ashe HL, Briscoe J (2006) The interpretation of morphogen gra-
dients. Development 133:385–94
61. Weiss R, Basu S, Hooshangi S, Kalmbach A, Karig D, Mehreja
R, Netravali I (2003) Genetic circuit building blocks for cellular
computation, communications, and signal processing. Natural
Computing 2(1):47–84
Books and Reviews
Abelson H, Allen D, Coore D, Hanson C, Homsy G, Knight T, Nagpal
R, Rauch E, Sussman G, Weiss R (1999) Amorphous computing.
Technical Report AIM-1665, MIT
Analog Computation
BRUCE J. MACLENNAN
Department of Electrical Engineering & Computer
Science, University of Tennessee,
Knoxville, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Fundamentals of Analog Computing
Analog Computation in Nature
General-Purpose Analog Computation
Analog Computation and the Turing Limit
Analog Thinking
Future Directions
Bibliography
Glossary
Accuracy The closeness of a computation to the corre-
sponding primary system.
BSS The theory of computation over the real numbers de-
ﬁned by Blum, Shub, and Smale.
Church–Turing (CT) computation The model of com-
putation based on the Turing machine and other
equivalent abstract computing machines; commonly
accepted as deﬁning the limits of digital computation.
EAC Extended analog computer deﬁned by Rubel.
GPAC General-purpose analog computer.
Nomograph A device for the graphical solution of equa-
tions by means of a family of curves and a straightedge.
ODE Ordinary diﬀerential equation.
PDE Partial diﬀerential equation.
Potentiometer A variable resistance, adjustable by the
computer operator, used in electronic analog comput-
ing as an attenuator for setting constants and parame-
ters in a computation.
Precision The quality of an analog representation or
computation, which depends on both resolution and
stability.
Primary system The system being simulated, modeled,
analyzed, or controlled by an analog computer, also
called the target system.
Scaling The adjustment, by constant multiplication, of
variables in the primary system (including time) so
that the corresponding variables in the analog systems
are in an appropriate range.
TM Turing machine.
Definition of the Subject
Although analog computation was eclipsed by digital com-
putation in the second half of the twentieth century, it is
returning as an important alternative computing technol-
ogy. Indeed, as explained in this article, theoretical results
imply that analog computation can escape from the limita-
tions of digital computation. Furthermore, analog compu-
tation has emerged as an important theoretical framework
for discussing computation in the brain and other natural
systems.
Analog computation gets its name from an analogy,
or systematic relationship, between the physical processes
in the computer and those in the system it is intended
to model or simulate (the primary system). For example,
the electrical quantities voltage, current, and conductance
might be used as analogs of the ﬂuid pressure, ﬂow rate,
and pipe diameter. More speciﬁcally, in traditional ana-
log computation, physical quantities in the computation
obey the same mathematical laws as physical quantities
in the primary system. Thus the computational quantities
are proportional to the modeled quantities. This is in con-
trast to digital computation, in which quantities are rep-
resented by strings of symbols (e. g., binary digits) that
have no direct physical relationship to the modeled quan-
tities. According to the Oxford English Dictionary (2nd
ed., s.vv. analogue, digital), these usages emerged in the
1940s.
However, in a fundamental sense all computing is
based on an analogy, that is, on a systematic relation-
ship between the states and processes in the computer and
those in the primary system. In a digital computer, the re-
lationship is more abstract and complex than simple pro-
portionality, but even so simple an analog computer as
a slide rule goes beyond strict proportion (i. e., distance on

272 A
Analog Computation
the rule is proportional to the logarithm of the number). In
both analog and digital computation—indeed in all com-
putation—the relevant abstract mathematical structure of
the problem is realized in the physical states and processes
of the computer, but the realization may be more or less
direct [40,41,46].
Therefore, despite the etymologies of the terms “ana-
log” and “digital”, in modern usage the principal distinc-
tion between digital and analog computation is that the
former operates on discrete representations in discrete
steps, while the later operated on continuous representa-
tions by means of continuous processes (e. g., MacLen-
nan [46], Siegelmann [p. 147 in 78], Small [p. 30 in 82],
Weyrick [p. 3 in 89]).
That is, the primary distinction resides in the topolo-
gies of the states and processes, and it would be more
accurate to refer to discrete and continuous computa-
tion [p. 39 in 25]. (Consider so-called analog and digital
clocks. The principal diﬀerence resides in the continuity
or discreteness of the representation of time; the motion
of the two (or three) hands of an “analog” clock do not
mimic the motion of the rotating earth or the position of
the sun relative to it.)
Introduction
History
Pre-electronic Analog Computation
Just like digital
calculation, analog computation was originally performed
by hand. Thus we ﬁnd several analog computational pro-
cedures in the “constructions” of Euclidean geometry (Eu-
clid, ﬂ. 300 BCE), which derive from techniques used in
ancient surveying and architecture. For example, Prob-
lem II.51 is “to divide a given straight line into two parts,
so that the rectangle contained by the whole and one of
the parts shall be equal to the square of the other part”.
Also, Problem VI.13 is “to ﬁnd a mean proportional be-
tween two given straight lines”, and VI.30 is “to cut a given
straight line in extreme and mean ratio”. These procedures
do not make use of measurements in terms of any ﬁxed
unit or of digital calculation; the lengths and other contin-
uous quantities are manipulated directly (via compass and
straightedge). On the other hand, the techniques involve
discrete, precise operational steps, and so they can be con-
sidered algorithms, but over continuous magnitudes rather
than discrete numbers.
It is interesting to note that the ancient Greeks distin-
guished continuous magnitudes (Grk., megethoi), which
have physical dimensions (e. g., length, area, rate), from
discrete numbers (Grk., arithmoi), which do not [49]. Eu-
clid axiomatizes them separately (magnitudes in Book V,
numbers in Book VII), and a mathematical system com-
prising both discrete and continuous quantities was not
achieved until the nineteenth century in the work of
Weierstrass and Dedekind.
The earliest known mechanical analog computer is the
“Antikythera mechanism”, which was found in 1900 in
a shipwreck under the sea near the Greek island of An-
tikythera (between Kythera and Crete). It dates to the
second century BCE and appears to be intended for as-
tronomical calculations. The device is sophisticated (at
least 70 gears) and well engineered, suggesting that it was
not the ﬁrst of its type, and therefore that other analog
computing devices may have been used in the ancient
Mediterranean world [22]. Indeed, according to Cicero
(Rep. 22) and other authors, Archimedes (c. 287–c. 212
BCE) and other ancient scientists also built analog com-
puters, such as armillary spheres, for astronomical simu-
lation and computation. Other antique mechanical analog
computers include the astrolabe, which is used for the de-
termination of longitude and a variety of other astronom-
ical purposes, and the torquetum, which converts astro-
nomical measurements between equatorial, ecliptic, and
horizontal coordinates.
A class of special-purpose analog computer, which is
simple in conception but may be used for a wide range
of purposes, is the nomograph (also, nomogram, align-
ment chart). In its most common form, it permits the
solution of quite arbitrary equations in three real vari-
ables, f (u; v; w) D 0. The nomograph is a chart or graph
with scales for each of the variables; typically these scales
are curved and have non-uniform numerical markings.
Given values for any two of the variables, a straightedge
is laid across their positions on their scales, and the value
of the third variable is read oﬀwhere the straightedge
crosses the third scale. Nomographs were used to solve
many problems in engineering and applied mathematics.
They improve intuitive understanding by allowing the re-
lationships among the variables to be visualized, and fa-
cilitate exploring their variation by moving the straight-
edge. Lipka (1918) is an example of a course in graphical
and mechanical methods of analog computation, includ-
ing nomographs and slide rules.
Until the introduction of portable electronic calcula-
tors in the early 1970s, the slide rule was the most famil-
iar analog computing device. Slide rules use logarithms for
multiplication and division, and they were invented in the
early seventeenth century shortly after John Napier’s de-
scription of logarithms.
The mid-nineteenth century saw the development of
the ﬁeld analogy method by G. Kirchhoﬀ(1824–1887) and
others [33]. In this approach an electrical ﬁeld in an elec-

Analog Computation
A
273
trolytic tank or conductive paper was used to solve two-
dimensional boundary problems for temperature distribu-
tions and magnetic ﬁelds [p. 34 in 82]. It is an early exam-
ple of analog ﬁeld computation (see ▷Field Computation
in Natural and Artiﬁcial Intelligence).
In the nineteenth century a number of mechanical
analog computers were developed for integration and dif-
ferentiation (e. g., Litka 1918, pp. 246–256; Clymer [15]).
For example, the planimeter measures the area under
a curve or within a closed boundary. While the operator
moves a pointer along the curve, a rotating wheel accumu-
lates the area. Similarly, the integraph is able to draw the
integral of a given function as its shape is traced. Other
mechanical devices can draw the derivative of a curve or
compute a tangent line at a given point.
In the late nineteenth century William Thomson, Lord
Kelvin, constructed several analog computers, including
a “tide predictor” and a “harmonic analyzer”, which com-
puted the Fourier coeﬃcients of a tidal curve [85,86].
In 1876 he described how the mechanical integrators in-
vented by his brother could be connected together in
a feedback loop in order to solve second and higher order
diﬀerential equations (Small [pp. 34–35, 42 in 82], Thom-
son [84]). He was unable to construct this diﬀerential an-
alyzer, which had to await the invention of the torque am-
pliﬁer in 1927.
The torque ampliﬁer and other technical advance-
ments permitted Vannevar Bush at MIT to construct
the ﬁrst practical diﬀerential analyzer in 1930 [pp. 42–
45 in 82]. It had six integrators and could also do addi-
tion, subtraction, multiplication, and division. Input data
were entered in the form of continuous curves, and the
machine automatically plotted the output curves continu-
ously as the equations were integrated. Similar diﬀerential
analyzers were constructed at other laboratories in the US
and the UK.
Setting up a problem on the MIT diﬀerential ana-
lyzer took a long time; gears and rods had to be arranged
to deﬁne the required dependencies among the variables.
Bush later designed a much more sophisticated machine,
the Rockefeller Diﬀerential Analyzer, which became op-
erational in 1947. With 18 integrators (out of a planned
30), it provided programmatic control of machine setup,
and permitted several jobs to be run simultaneously. Me-
chanical diﬀerential analyzers were rapidly supplanted by
electronic analog computers in the mid-1950s, and most
were disassembled in the 1960s (Bowles [10], Owens [61],
Small [pp. 50–45 in 82]).
During World War II, and even later wars, an impor-
tant application of optical and mechanical analog compu-
tation was in “gun directors” and “bomb sights”, which
performed ballistic computations to accurately target ar-
tillery and dropped ordnance.
Electronic Analog Computation in the 20th Century
It
is commonly supposed that electronic analog computers
were superior to mechanical analog computers, and they
were in many respects, including speed, cost, ease of con-
struction, size, and portability [pp. 54–56 in 82]. On the
other hand, mechanical integrators produced higher pre-
cision results (0.1% vs. 1% for early electronic devices) and
had greater mathematical ﬂexibility (they were able to in-
tegrate with respect to any variable, not just time). How-
ever, many important applications did not require high
precision and focused on dynamic systems for which time
integration was suﬃcient.
Analog computers (non-electronic as well as elec-
tronic) can be divided into active-element and passive-
element computers; the former involve some kind of am-
pliﬁcation, the latter do not [pp. 2-1–4 in 87]. Passive-
element computers included the network analyzers, which
were developed in the 1920s to analyze electric power dis-
tribution networks, and which continued in use through
the 1950s [pp. 35–40 in 82]. They were also applied to
problems in thermodynamics, aircraft design, and me-
chanical engineering. In these systems networks or grids of
resistive elements or reactive elements (i. e., involving ca-
pacitance and inductance as well as resistance) were used
to model the spatial distribution of physical quantities
such as voltage, current, and power (in electric distribution
networks), electrical potential in space, stress in solid ma-
terials, temperature (in heat diﬀusion problems), pressure,
ﬂuid ﬂow rate, and wave amplitude [p. 2-2 in 87]. That is,
network analyzers dealt with partial diﬀerential equations
(PDEs), whereas active-element computers, such as the
diﬀerential analyzer and its electronic successors, were re-
stricted to ordinary diﬀerential equations (ODEs) in which
time was the independent variable. Large network ana-
lyzers are early examples of analog ﬁeld computers (see
▷Field Computation in Natural and Artiﬁcial Intelli-
gence).
Electronic analog computers became feasible after the
invention of the DC operational ampliﬁer (“op amp”)
c. 1940 [pp. 64, 67–72 in 82]. Already in the 1930s scien-
tists at Bell Telephone Laboratories (BTL) had developed
the DC-coupled feedback-stabilized ampliﬁer, which is the
basis of the op amp. In 1940, as the USA prepared to en-
ter World War II, DL Parkinson at BTL had a dream in
which he saw DC ampliﬁers being used to control an anti-
aircraft gun. As a consequence, with his colleagues CA
Lovell and BT Weber, he wrote a series of papers on “elec-
trical mathematics”, which described electrical circuits to

274 A
Analog Computation
“operationalize” addition, subtraction, integration, diﬀer-
entiation, etc. The project to produce an electronic gun-
director led to the development and reﬁnement of DC op
amps suitable for analog computation.
The war-time work at BTL was focused primarily on
control applications of analog devices, such as the gun-
director. Other researchers, such as E. Lakatos at BTL,
were more interested in applying them to general-purpose
analog computation for science and engineering, which
resulted in the design of the General Purpose Analog
Computer (GPAC), also called “Gypsy”, completed in
1949 [pp. 69–71 in 82]. Building on the BTL op amp de-
sign, fundamental work on electronic analog computation
was conducted at Columbia University in the 1940s. In
particular, this research showed how analog computation
could be applied to the simulation of dynamic systems and
to the solution of nonlinear equations.
Commercial
general-purpose
analog
computers
(GPACs) emerged in the late 1940s and early 1950s
[pp. 72–73 in 82]. Typically they provided several dozen
integrators, but several GPACs could be connected to-
gether to solve larger problems. Later, large-scale GPACs
might have up to 500 ampliﬁers and compute with 0.01%–
0.1% precision [p. 2-33 in 87].
Besides integrators, typical GPACs provided adders,
subtracters, multipliers, ﬁxed function generators (e. g.,
logarithms, exponentials, trigonometric functions), and
variable function generators (for user-deﬁned func-
tions) [Chaps. 1.3, 2.4 in 87]. A GPAC was programmed
by connecting these components together, often by means
of a patch panel. In addition, parameters could be entered
by adjusting potentiometers (attenuators), and arbitrary
functions could be entered in the form of graphs [pp. 1-
72–81, 2-154–156 in 87]. Output devices plotted data con-
tinuously or displayed it numerically [pp. 3-1–30 in 87].
The most basic way of using a GPAC was in single-
shot mode [pp. 168–170 in 89]. First, parameters and initial
values were entered into the potentiometers. Next, putting
a master switch in “reset” mode controlled relays to apply
the initial values to the integrators. Turning the switch to
“operate” or “compute” mode allowed the computation to
take place (i. e., the integrators to integrate). Finally, plac-
ing the switch in “hold” mode stopped the computation
and stabilized the values, allowing them to be read from
the computer (e. g., on voltmeters). Although single-shot
operation was also called “slow operation” (in comparison
to “repetitive operation”, discussed next), it was in practice
quite fast. Because all of the devices computed in parallel
and at electronic speeds, analog computers usually solved
problems in real-time but often much faster (Truitt and
Rogers [pp. 1-30–32 in 87], Small [p. 72 in 82]).
One common application of GPACs was to explore
the eﬀect of one or more parameters on the behavior
of a system. To facilitate this exploration of the param-
eter space, some GPACs provided a repetitive operation
mode, which worked as follows (Weyrick [p. 170 in 89],
Small [p. 72 in 82]). An electronic clock switched the com-
puter between reset and compute modes at an adjustable
rate (e. g., 10–1000 cycles per second) [p. 280, n. 1 in 2].
In eﬀect the simulation was rerun at the clock rate, but
if any parameters were adjusted, the simulation results
would vary along with them. Therefore, within a few sec-
onds, an entire family of related simulations could be run.
More importantly, the operator could acquire an intuitive
understanding of the system’s dependence on its parame-
ters.
The Eclipse of Analog Computing
A common view is
that electronic analog computers were a primitive prede-
cessor of the digital computer, and that their use was just
a historical episode, or even a digression, in the inevitable
triumph of digital technology. It is supposed that the cur-
rent digital hegemony is a simple matter of technological
superiority. However, the history is much more compli-
cated, and involves a number of social, economic, histori-
cal, pedagogical, and also technical factors, which are out-
side the scope of this article (see Small [81] and Small [82],
especially Chap. 8, for more information). In any case, be-
ginning after World War II and continuing for twenty-ﬁve
years, there was lively debate about the relative merits of
analog and digital computation.
Speed was an oft-cited advantage of analog comput-
ers [Chap. 8 in 82]. While early digital computers were
much faster than mechanical diﬀerential analyzers, they
were slower (often by several orders of magnitude) than
electronic analog computers. Furthermore, although digi-
tal computers could perform individual arithmetic opera-
tions rapidly, complete problems were solved sequentially,
one operation at a time, whereas analog computers oper-
ated in parallel. Thus it was argued that increasingly large
problems required more time to solve on a digital com-
puter, whereas on an analog computer they might require
more hardware but not more time. Even as digital com-
puting speed was improved, analog computing retained its
advantage for several decades, but this advantage eroded
steadily.
Another important issue was the comparative preci-
sion of digital and analog computation [Chap. 8 in 82].
Analog computers typically computed with three or four
digits of precision, and it was very expensive to do much
better, due to the diﬃculty of manufacturing the parts and
other factors. In contrast, digital computers could perform

Analog Computation
A
275
arithmetic operations with many digits of precision, and
the hardware cost was approximately proportional to the
number of digits. Against this, analog computing advo-
cates argued that many problems did not require such high
precision, because the measurements were known to only
a few signiﬁcant ﬁgures and the mathematical models were
approximations. Further, they distinguished between pre-
cision and accuracy, which refers to the conformity of the
computation to physical reality, and they argued that digi-
tal computation was often less accurate than analog, due to
numerical limitations (e. g., truncation, cumulative error
in numerical integration). Nevertheless, some important
applications, such as the calculation of missile trajectories,
required greater precision, and for these, digital computa-
tion had the advantage. Indeed, to some extent precision
was viewed as inherently desirable, even in applications
where it was unimportant, and it was easily mistaken for
accuracy. (See Sect. “Precision” for more on precision and
accuracy.)
There was even a social factor involved, in that the
written programs, precision, and exactness of digital com-
putation were associated with mathematics and science,
but the hands-on operation, parameter variation, and
approximate solutions of analog computation were as-
sociated with engineers, and so analog computing in-
herited “the lower status of engineering vis-à-vis sci-
ence” [p. 251 in 82]. Thus the status of digital comput-
ing was further enhanced as engineering became more
mathematical and scientiﬁc after World War II [pp. 247–
251 in 82].
Already by the mid-1950s the competition between
analog and digital had evolved into the idea that they were
complementary technologies. This resulted in the develop-
ment of a variety of hybrid analog/digital computing sys-
tems [pp. 251–253, 263–266 in 82]. In some cases this in-
volved using a digital computer to control an analog com-
puter by using digital logic to connect the analog com-
puting elements, set parameters, and gather data. This im-
proved the accessibility and usability of analog computers,
but had the disadvantage of distancing the user from the
physical analog system. The intercontinental ballistic mis-
sile program in the USA stimulated the further develop-
ment of hybrid computers in the late 1950s and 1960s [81].
These applications required the speed of analog computa-
tion to simulate the closed-loop control systems and the
precision of digital computation for accurate computation
of trajectories. However, by the early 1970s hybrids were
being displaced by all digital systems. Certainly part of the
reason was the steady improvement in digital technology,
driven by a vibrant digital computer industry, but con-
temporaries also pointed to an inaccurate perception that
analog computing was obsolete and to a lack of education
about the advantages and techniques of analog comput-
ing.
Another argument made in favor of digital computers
was that they were general-purpose, since they could be
used in business data processing and other application do-
mains, whereas analog computers were essentially special-
purpose, since they were limited to scientiﬁc computa-
tion [pp. 248–250 in 82]. Against this it was argued that all
computing is essentially computing by analogy, and there-
fore analog computation was general-purpose because the
class of analog computers included digital computers! (See
also Sect. “Deﬁnition of the Subject” on computing by
analogy.) Be that as it may, analog computation, as nor-
mally understood, is restricted to continuous variables,
and so it was not immediately applicable to discrete data,
such as that manipulated in business computing and other
nonscientiﬁc applications. Therefore business (and even-
tually consumer) applications motivated the computer in-
dustry’s investment in digital computer technology at the
expense of analog technology.
Although it is commonly believed that analog com-
puters quickly disappeared after digital computers became
available, this is inaccurate, for both general-purpose and
special-purpose analog computers have continued to be
used in specialized applications to the present time. For
example, a general-purpose electrical (vs. electronic) ana-
log computer, the Anacom, was still in use in 1991. This
is not technological atavism, for “there is no doubt con-
siderable truth in the fact that Anacom continued to be
used because it eﬀectively met a need in a historically ne-
glected but nevertheless important computer application
area” [3]. As mentioned, the reasons for the eclipse of ana-
log computing were not simply the technological superi-
ority of digital computation; the conditions were much
more complex. Therefore a change in conditions has ne-
cessitated a reevaluation of analog technology.
Analog VLSI
In the mid-1980s, Carver Mead, who al-
ready had made important contributions to digital VLSI
technology, began to advocate for the development of ana-
log VLSI [51,52]. His motivation was that “the nervous
system of even a very simple animal contains comput-
ing paradigms that are orders of magnitude more eﬀective
than are those found in systems made by humans” and
that they “can be realized in our most commonly avail-
able technology—silicon integrated circuits” [pp. xi in 52].
However, he argued, since these natural computation sys-
tems are analog and highly non-linear, progress would re-
quire understanding neural information processing in an-
imals and applying it in a new analog VLSI technology.

276 A
Analog Computation
Because analog computation is closer to the physical
laws by which all computation is realized (which are con-
tinuous), analog circuits often use fewer devices than cor-
responding digital circuits. For example, a four-quadrant
adder (capable of adding two signed numbers) can be fab-
ricated from four transistors [pp. 87–88 in 52], and a four-
quadrant multiplier from nine to seventeen, depending on
the required range of operation [pp. 90–96 in 52]. Intu-
itions derived from digital logic about what is simple or
complex to compute are often misleading when applied
to analog computation. For example, two transistors are
suﬃcient to compute the logarithm or exponential, ﬁve
for the hyperbolic tangent (which is very useful in neu-
ral computation), and three for the square root [pp. 70–
71, 97–99 in 52]. Thus analog VLSI is an attractive ap-
proach to “post-Moore’s Law computing” (see Sect. “Fu-
ture Directions” below). Mead and his colleagues demon-
strated a number of analog VLSI devices inspired by the
nervous system, including a “silicon retina” and an “elec-
tronic cochlea” [Chaps. 15–16 in 52], research that has lead
to a renaissance of interest in electronic analog comput-
ing.
Non-Electronic Analog Computation
As will be ex-
plained in the body of this article, analog computation sug-
gests many opportunities for future computing technolo-
gies. Many physical phenomena are potential media for
analog computation provided they have useful mathemat-
ical structure (i. e., the mathematical laws describing them
are mathematical functions useful for general- or special-
purpose computation), and they are suﬃciently control-
lable for practical use.
Article Roadmap
The remainder of this article will begin by summariz-
ing the fundamentals of analog computing, starting with
the continuous state space and the various processes by
which analog computation can be organized in time. Next
it will discuss analog computation in nature, which pro-
vides models and inspiration for many contemporary uses
of analog computation, such as neural networks. Then we
consider general-purpose analog computing, both from
a theoretical perspective and in terms of practical general-
purpose analog computers. This leads to a discussion of
the theoretical power of analog computation and in partic-
ular to the issue of whether analog computing is in some
sense more powerful than digital computing. We brieﬂy
consider the cognitive aspects of analog computing, and
whether it leads to a diﬀerent approach to computation
than does digital computing. Finally, we conclude with
some observations on the role of analog computation in
“post-Moore’s Law computing”.
Fundamentals of Analog Computing
Continuous State Space
As discussed in Sect. “Introduction”, the fundamental
characteristic that distinguishes analog from digital com-
putation is that the state space is continuous in analog
computation and discrete in digital computation. There-
fore it might be more accurate to call analog and digital
computation continuous and discrete computation, respec-
tively. Furthermore, since the earliest days there have been
hybrid computers that combine continuous and discrete
state spaces and processes. Thus, there are several respects
in which the state space may be continuous.
In the simplest case the state space comprises a ﬁ-
nite (generally modest) number of variables, each hold-
ing a continuous quantity (e. g., voltage, current, charge).
In a traditional GPAC they correspond to the variables in
the ODEs deﬁning the computational process, each typ-
ically having some independent meaning in the analysis
of the problem. Mathematically, the variables are taken to
contain bounded real numbers, although complex-valued
variables are also possible (e. g., in AC electronic analog
computers). In a practical sense, however, their precision
is limited by noise, stability, device tolerance, and other
factors (discussed below, Sect. “Characteristics of Analog
Computation”).
In typical analog neural networks the state space is
larger in dimension but more structured than in the for-
mer case. The artiﬁcial neurons are organized into one or
more layers, each composed of a (possibly large) number
of artiﬁcial neurons. Commonly each layer of neurons is
densely connected to the next layer. In general the layers
each have some meaning in the problem domain, but the
individual neurons constituting them do not (and so, in
mathematical descriptions, the neurons are typically num-
bered rather than named).
The individual artiﬁcial neurons usually perform
a simple computation such as this:
y D (s) ;
where s D b C
n
X
iD1
wixi ;
and where y is the activity of the neuron, x1; : : : ; xn are
the activities of the neurons that provide its inputs, b is
a bias term, and w1; : : : ; wn are the weights or strengths of
the connections. Often the activation function  is a real-
valued sigmoid (“S-shaped”) function, such as the logistic

Analog Computation
A
277
sigmoid,
(s) D
1
1 C es ;
in which case the neuron activity y is a real number, but
some applications use a discontinuous threshold function,
such as the Heaviside function,
U(s) D
(
C1
if s  0
0
if s < 0
in which case the activity is a discrete quantity. The sa-
turated-linear or piecewise-linear sigmoid is also used oc-
casionally:
(s) D
8
ˆ<
ˆ:
C1
if s > 1
s
if 0  s  1
0
if s < 0 :
Regardless of whether the activation function is con-
tinuous or discrete, the bias b and connection weights
w1; : : : ; wn are real numbers, as is the “net input”
s D P
i wixi to the activation function. Analog computa-
tion may be used to evaluate the linear combination s and
the activation function (s), if it is real-valued. The biases
and weights are normally determined by a learning algo-
rithm (e. g., back-propagation), which is also a good can-
didate for analog implementation.
In summary, the continuous state space of a neural
network includes the bias values and net inputs of the neu-
rons and the interconnection strengths between the neu-
rons. It also includes the activity values of the neurons,
if the activation function is a real-valued sigmoid func-
tion, as is often the case. Often large groups (“layers”) of
neurons (and the connections between these groups) have
some intuitive meaning in the problem domain, but typi-
cally the individual neuron activities, bias values, and in-
terconnection weights do not.
If we extrapolate the number of neurons in a layer
to the continuum limit, we get a ﬁeld, which may be de-
ﬁned as a continuous distribution of continuous quantity
(see ▷Field Computation in Natural and Artiﬁcial Intel-
ligence). Treating a group of artiﬁcial or biological neu-
rons as a continuous mass is a reasonable mathematical
approximation if their number is suﬃciently large and if
their spatial arrangement is signiﬁcant (as it generally is in
the brain). Fields are especially useful in modeling cortical
maps, in which information is represented by the pattern
of activity over a region of neural cortex.
In ﬁeld computation the state space in continuous in
two ways: it is continuous in variation but also in space.
Therefore, ﬁeld computation is especially applicable to
solving PDEs and to processing spatially extended infor-
mation such as visual images. Some early analog com-
puting devices were capable of ﬁeld computation [pp. 1-
14–17, 2-2–16 in 87]. For example, as previously men-
tioned (Sect. “Introduction”), large resistor and capacitor
networks could be used for solving PDEs such as diﬀu-
sion problems. In these cases a discrete ensemble of resis-
tors and capacitors was used to approximate a continu-
ous ﬁeld, while in other cases the computing medium was
spatially continuous. The latter made use of conductive
sheets (for two-dimensional ﬁelds) or electrolytic tanks
(for two- or three-dimensional ﬁelds). When they were ap-
plied to steady-state spatial problems, these analog com-
puters were called ﬁeld plotters or potential analyzers.
The ability to fabricate very large arrays of analog
computing devices, combined with the need to exploit
massive parallelism in realtime computation and control
applications, creates new opportunities for ﬁeld compu-
tation [37,38,43]. There is also renewed interest in us-
ing physical ﬁelds in analog computation. For example,
Rubel [73] deﬁned an abstract extended analog computer
(EAC), which augments Shannon’s [77] general purpose
analog computer with (unspeciﬁed) facilities for ﬁeld com-
putation, such as PDE solvers (see Sects. Shannon’s Analy-
sis – Rubel’s Extended Analog Computer below). JW. Mills
has explored the practical application of these ideas in his
artiﬁcial neural ﬁeld networks and VLSI EACs, which use
the diﬀusion of electrons in bulk silicon or conductive gels
and plastics for 2D and 3D ﬁeld computation [53,54].
Computational Process
We have considered the continuous state space, which is
the basis for analog computing, but there are a variety of
ways in which analog computers can operate on the state.
In particular, the state can change continuously in time or
be updated at distinct instants (as in digital computation).
Continuous Time
Since the laws of physics on which
analog computing is based are diﬀerential equations, many
analog computations proceed in continuous real time.
Also, as we have seen, an important application of ana-
log computers in the late 19th and early 20th centuries
was the integration of ODEs in which time is the indepen-
dent variable. A common technique in analog simulation
of physical systems is time scaling, in which the diﬀerential
equations are altered systematically so the simulation pro-
ceeds either more slowly or more quickly than the primary
system (see Sect. “Characteristics of Analog Computation”
for more on time scaling). On the other hand, because ana-
log computations are close to the physical processes that

278 A
Analog Computation
realize them, analog computing is rapid, which makes it
very suitable for real-time control applications.
In principle, any mathematically describable physi-
cal process operating on time-varying physical quantities
can be used for analog computation. In practice, how-
ever, analog computers typically provide familiar opera-
tions that scientists and engineers use in diﬀerential equa-
tions [70,87]. These include basic arithmetic operations,
such as algebraic sum and diﬀerence (u(t) D v(t) ˙
w(t)), constant multiplication or scaling (u(t) D cv(t)),
variable multiplication and division (u(t) D v(t)w(t),
u(t) D v(t)/w(t)), and inversion (u(t) D v(t)). Tran-
scendental functions may be provided, such as the ex-
ponential (u(t) D exp v(t)), logarithm (u(t) D ln v(t)),
trigonometric functions (u(t) D sin v(t), etc.), and re-
solvers for converting between polar and rectangular co-
ordinates. Most important, of course, is deﬁnite integra-
tion (u(t) D v0 C R t
0 v()d), but diﬀerentiation may also
be provided (u(t) D ˙v(t)). Generally, however, direct dif-
ferentiation is avoided, since noise tends to have a higher
frequency than the signal, and therefore diﬀerentiation
ampliﬁes noise; typically problems are reformulated to
avoid direct diﬀerentiation [pp. 26–27 in 89]. As previ-
ously mentioned, many GPACs include (arbitrary) func-
tion generators, which allow the use of functions deﬁned
only by a graph and for which no mathematical deﬁnition
might be available; in this way empirically deﬁned func-
tions can be used [pp. 32–42 in 70]. Thus, given a graph
(x; f (x)), or a suﬃcient set of samples, (xk; f (xk)), the
function generator approximates u(t) D f (v(t)). Rather
less common are generators for arbitrary functions of two
variables, u(t) D f (v(t); w(t)), in which the function may
be deﬁned by a surface, (x; y; f (x; y)), or by suﬃcient sam-
ples from it.
Although analog computing is primarily continuous,
there are situations in which discontinuous behavior is re-
quired. Therefore some analog computers provide com-
parators, which produce a discontinuous result depending
on the relative value of two input values. For example,
u D
(
k
if v  w ;
0
if v < w :
Typically, this would be implemented as a Heaviside (unit
step) function applied to the diﬀerence of the inputs,
w D kU(v  w). In addition to allowing the deﬁnition of
discontinuous functions, comparators provide a primitive
decision making ability, and may be used, for example to
terminate a computation (switching the computer from
“operate” to “hold” mode).
Other operations that have proved useful in analog
computation are time delays and noise generators [Chap. 7
in 31]. The function of a time delay is simply to retard the
signal by an adjustable delay T > 0: u(t C T) D v(t). One
common application is to model delays in the primary sys-
tem (e. g., human response time).
Typically a noise generator produces time-invariant
Gaussian-distributed noise with zero mean and a ﬂat
power spectrum (over a band compatible with the ana-
log computing process). The standard deviation can be ad-
justed by scaling, the mean can be shifted by addition, and
the spectrum altered by ﬁltering, as required by the appli-
cation. Historically noise generators were used to model
noise and other random eﬀects in the primary system, to
determine, for example, its sensitivity to eﬀects such as tur-
bulence. However, noise can make a positive contribution
in some analog computing algorithms (e. g., for symmetry
breaking and in simulated annealing, weight perturbation
learning, and stochastic resonance).
As already mentioned, some analog computing de-
vices for the direct solution of PDEs have been devel-
oped. In general a PDE solver depends on an analogous
physical process, that is, on a process obeying the same
class of PDEs that it is intended to solve. For example,
in Mill’s EAC, diﬀusion of electrons in conductive sheets
or solids is used to solve diﬀusion equations [53,54]. His-
torically, PDEs were solved on electronic GPACs by dis-
cretizing all but one of the independent variables, thus
replacing the diﬀerential equations by diﬀerence equa-
tions [70], pp. 173–193. That is, computation over a ﬁeld
was approximated by computation over a ﬁnite real array.
Reaction-diﬀusion computation is an important exam-
ple of continuous-time analog computing ▷Reaction-D-
iﬀusion Computing. The state is represented by a set
of time-varying chemical concentration ﬁelds, c1; : : : ; cn.
These ﬁelds are distributed across a one-, two-, or three-
dimensional space ˝, so that, for x 2 ˝, ck(x; t) repre-
sents the concentration of chemical k at location x and
time t. Computation proceeds in continuous time accord-
ing to reaction-diﬀusion equations, which have the form:
@c/@t D Dr2c C F(c) ;
where c D (c1; : : : ; cn)T is the vector of concentrations,
D D diag(d1; : : : ; dn) is a diagonal matrix of positive dif-
fusion rates, and F is nonlinear vector function that de-
scribes how the chemical reactions aﬀect the concentra-
tions.
Some neural net models operate in continuous time
and thus are examples of continuous-time analog compu-
tation. For example, Grossberg [26,27,28] deﬁnes the ac-
tivity of a neuron by diﬀerential equations such as this:

Analog Computation
A
279
˙xi D aixi C
n
X
jD1
bi jw(C)
i j
fj(xj)
n
X
jD1
ci jw()
i j g j(xj)CIi :
This describes the continuous change in the activity of
neuron i resulting from passive decay (ﬁrst term), positive
feedback from other neurons (second term), negative feed-
back (third term), and input (last term). The fj and g j are
nonlinear activation functions, and the w(C)
i j
and w()
i j
are
adaptable excitatory and inhibitory connection strengths,
respectively.
The continuous Hopﬁeld network is another exam-
ple of continuous-time analog computation [30]. The out-
put yi of a neuron is a nonlinear function of its inter-
nal state xi, yi D (xi), where the hyperbolic tangent is
usually used as the activation function, (x) D tanh x, be-
cause its range is [1; 1]. The internal state is deﬁned by
a diﬀerential equation,
i ˙xi D aixi C bi C
n
X
jD1
wi jy j ;
where i is a time constant, ai is the decay rate, bi is the
bias, and wij is the connection weight to neuron i from
neuron j. In a Hopﬁeld network every neuron is symmetri-
cally connected to every other (wi j D wji) but not to itself
(wii D 0).
Of course analog VLSI implementations of neural net-
works also operate in continuous time (e. g., [20,52].
Concurrent with the resurgence of interest in analog
computation have been innovative reconceptualizations of
continuous-time computation. For example, Brockett [12]
has shown that dynamical systems can perform a num-
ber of problems normally considered to be intrinsically se-
quential. In particular, a certain system of ODEs (a non-
periodic ﬁnite Toda lattice) can sort a list of numbers
by continuous-time analog computation. The system is
started with the vector x equal to the values to be sorted
and a vector y initialized to small nonzero values; the y
vector converges to a sorted permutation of x.
Sequential Time
computation refers to computation in
which discrete computational operations take place in suc-
cession but at no deﬁnite interval [88]. Ordinary digital
computer programs take place in sequential time, for the
operations occur one after another, but the individual op-
erations are not required to have any speciﬁc duration, so
long as they take ﬁnite time.
One of the oldest examples of sequential analog com-
putation is provided by the compass-and-straightedge
constructions of traditional Euclidean geometry (Sect. “In-
troduction”). These computations proceed by a sequence
of discrete operations, but the individual operations in-
volve continuous representations (e. g., compass settings,
straightedge positions) and operate on a continuous state
(the ﬁgure under construction). Slide rule calculation
might seem to be an example of sequential analog compu-
tation, but if we look at it, we see that although the opera-
tions are performed by an analog device, the intermediate
results are recorded digitally (and so this part of the state
space is discrete). Thus it is a kind of hybrid computation.
The familiar digital computer automates sequential
digital computations that once were performed manually
by human “computers”. Sequential analog computation
can be similarly automated. That is, just as the control unit
of an ordinary digital computer sequences digital com-
putations, so a digital control unit can sequence analog
computations. In addition to the analog computation de-
vices (adders, multipliers, etc.), such a computer must pro-
vide variables and registers capable of holding continuous
quantities between the sequential steps of the computation
(see also Sect. “Discrete Time” below).
The primitive operations of sequential-time analog
computation are typically similar to those in continuous-
time computation (e. g., addition, multiplication, tran-
scendental functions), but integration and diﬀerentiation
with respect to sequential time do not make sense. How-
ever, continuous-time integration within a single step, and
space-domain integration, as in PDE solvers or ﬁeld com-
putation devices, are compatible with sequential analog
computation.
In general, any model of digital computation can be
converted to a similar model of sequential analog compu-
tation by changing the discrete state space to a continuum,
and making appropriate changes to the rest of the model.
For example, we can make an analog Turing machine by
allowing it to write a bounded real number (rather than
a symbol from a ﬁnite alphabet) onto a tape cell. The Tur-
ing machine’s ﬁnite control can be altered to test for tape
markings in some speciﬁed range.
Similarly, in a series of publications Blum, Shub, and
Smale developed a theory of computation over the reals,
which is an abstract model of sequential-time analog com-
putation [6,7]. In this “BSS model” programs are repre-
sented as ﬂowcharts, but they are able to operate on real-
valued variables. Using this model they were able to prove
a number of theorems about the complexity of sequential
analog algorithms.
The BSS model, and some other sequential analog
computation models, assume that it is possible to make ex-
act comparisons between real numbers (analogous to exact
comparisons between integers or discrete symbols in dig-
ital computation) and to use the result of the comparison

280 A
Analog Computation
to control the path of execution. Comparisons of this kind
are problematic because they imply inﬁnite precision in
the comparator (which may be defensible in a mathemat-
ical model but is impossible in physical analog devices),
and because they make the execution path a discontinu-
ous function of the state (whereas analog computation is
usually continuous). Indeed, it has been argued that this is
not “true” analog computation [p. 148 in 78].
Many artiﬁcial neural network models are examples
of sequential-time analog computation. In a simple feed-
forward neural network, an input vector is processed by
the layers in order, as in a pipeline. That is, the output of
layer n becomes the input of layer n C 1. Since the model
does not make any assumptions about the amount of time
it takes a vector to be processed by each layer and to prop-
agate to the next, execution takes place in sequential time.
Most recurrent neural networks, which have feedback, also
operate in sequential time, since the activities of all the
neurons are updated synchronously (that is, signals prop-
agate through the layers, or back to earlier layers, in lock-
step).
Many artiﬁcial neural-net learning algorithms are also
sequential-time analog computations. For example, the
back-propagation algorithm updates a network’s weights,
moving sequentially backward through the layers.
In summary, the correctness of sequential time com-
putation (analog or digital) depends on the order of oper-
ations, not on their duration, and similarly the eﬃciency of
sequential computations is evaluated in terms of the num-
ber of operations, not on their total duration.
Discrete Time
analog computation has similarities to
both continuous-time and sequential analog computation.
Like the latter, it proceeds by a sequence of discrete (ana-
log) computation steps; like the former, these steps occur
at a constant rate in real time (e. g., some “frame rate”).
If the real-time rate is suﬃcient for the application, then
discrete-time computation can approximate continuous-
time computation (including integration and diﬀerentia-
tion).
Some electronic GPACs implemented discrete-time
analog computation by a modiﬁcation of repetitive oper-
ation mode, called iterative analog computation [Chap. 9
in 2]. Recall (Sect. “Electronic Analog Computation in the
20th Century”) that in repetitive operation mode a clock
rapidly switched the computer between reset and com-
pute modes, thus repeating the same analog computation,
but with diﬀerent parameters (set by the operator). How-
ever, each repetition was independent of the others. Iter-
ative operation was diﬀerent in that analog values com-
puted by one iteration could be used as initial values in the
next. This was accomplished by means of an analog mem-
ory circuit (based on an op amp) that sampled an analog
value at the end of one compute cycle (eﬀectively during
hold mode) and used it to initialize an integrator during
the following reset cycle. (A modiﬁed version of the mem-
ory circuit could be used to retain a value over several iter-
ations.) Iterative computation was used for problems such
as determining, by iterative search or reﬁnement, the ini-
tial conditions that would lead to a desired state at a fu-
ture time. Since the analog computations were iterated
at a ﬁxed clock rate, iterative operation is an example of
discrete-time analog computation. However, the clock rate
is not directly relevant in some applications (such as the
iterative solution of boundary value problems), in which
case iterative operation is better characterized as sequen-
tial analog computation.
The principal contemporary examples of discrete-time
analog computing are in neural network applications to
time-series analysis and (discrete-time) control. In each of
these cases the input to the neural net is a sequence of
discrete-time samples, which propagate through the net
and generate discrete-time output signals. Many of these
neural nets are recurrent, that is, values from later layers
are fed back into earlier layers, which allows the net to re-
member information from one sample to the next.
Analog Computer Programs
The concept of a program is central to digital comput-
ing, both practically, for it is the means for programming
general-purpose digital computers, and theoretically, for it
deﬁnes the limits of what can be computed by a universal
machine, such as a universal Turing machine. Therefore it
is important to discuss means for describing or specifying
analog computations.
Traditionally, analog computers were used to solve
ODEs (and sometimes PDEs), and so in one sense a math-
ematical diﬀerential equation is one way to represent an
analog computation. However, since the equations were
usually not suitable for direct solution on an analog com-
puter, the process of programming involved the translation
of the equations into a schematic diagram showing how
the analog computing devices (integrators etc.) should be
connected to solve the problem. These diagrams are the
closest analogies to digital computer programs and may be
compared to ﬂowcharts, which were once popular in digi-
tal computer programming. It is worth noting, however,
that ﬂowcharts (and ordinary computer programs) rep-
resent sequences among operations, whereas analog com-
puting diagrams represent functional relationships among
variables, and therefore a kind of parallel data ﬂow.

Analog Computation
A
281
Diﬀerential equations and schematic diagrams are
suitable for continuous-time computation, but for sequen-
tial analog computation something more akin to a con-
ventional digital program can be used. Thus, as previously
discussed (Sect. “Sequential Time”), the BSS system uses
ﬂowcharts to describe sequential computations over the
reals. Similarly, C. Moore [55] deﬁnes recursive functions
over the reals by means of a notation similar to a program-
ming language.
In principle any sort of analog computation might
involve constants that are arbitrary real numbers, which
therefore might not be expressible in ﬁnite form (e. g.,
as a ﬁnite string of digits). Although this is of theoreti-
cal interest (see Sect. “Real-valued Inputs, Outputs, and
Constants” below), from a practical standpoint these con-
stants could be set with about at most four digits of pre-
cision [p. 11 in 70]. Indeed, automatic potentiometer-
setting devices were constructed that read a series of dec-
imal numerals from punched paper tape and used them
to set the potentiometers for the constants [pp. 3-58–
60 in 87]. Nevertheless it is worth observing that analog
computers do allow continuous inputs that need not be
expressed in digital notation, for example, when the pa-
rameters of a simulation are continuously varied by the
operator. In principle, therefore, an analog program can
incorporate constants that are represented by a real-valued
physical quantity (e. g., an angle or a distance), which
need not be expressed digitally. Further, as we have seen
(Sect. “Electronic Analog Computation in the 20th Cen-
tury”), some electronic analog computers could compute
a function by means of an arbitrarily drawn curve, that
is, not represented by an equation or a ﬁnite set of digi-
tized points. Therefore, in the context of analog comput-
ing it is natural to expand the concept of a program be-
yond discrete symbols to include continuous representa-
tions (scalar magnitudes, vectors, curves, shapes, surfaces,
etc.).
Typically such continuous representations would be
used as adjuncts to conventional discrete representations
of the analog computational process, such as equations
or diagrams. However, in some cases the most natural
static representation of the process is itself continuous, in
which case it is more like a “guiding image” than a tex-
tual prescription [42]. A simple example is a potential sur-
face, which deﬁnes a continuum of trajectories from ini-
tial states (possible inputs) to ﬁxed-point attractors (the
results of the computations). Such a “program” may de-
ﬁne a deterministic computation (e. g., if the computation
proceeds by gradient descent), or it may constrain a non-
deterministic computation (e. g., if the computation may
proceed by any potential-decreasing trajectory). Thus ana-
log computation suggests a broadened notion of programs
and programming.
Characteristics of Analog Computation
Precision
Analog computation is evaluated in terms of
both accuracy and precision, but the two must be distin-
guished carefully [pp. 25–28 in 2,pp. 12–13 in 89,pp. 257–
261 in 82]. Accuracy refers primarily to the relationship
between a simulation and the primary system it is simu-
lating or, more generally, to the relationship between the
results of a computation and the mathematically correct
result. Accuracy is a result of many factors, including the
mathematical model chosen, the way it is set up on a com-
puter, and the precision of the analog computing devices.
Precision, therefore, is a narrower notion, which refers to
the quality of a representation or computing device. In
analog computing, precision depends on resolution (ﬁne-
ness of operation) and stability (absence of drift), and may
be measured as a fraction of the represented value. Thus
a precision of 0.01% means that the representation will
stay within 0.01% of the represented value for a reasonable
period of time. For purposes of comparing analog devices,
the precision is usually expressed as a fraction of full-scale
variation (i. e., the diﬀerence between the maximum and
minimum representable values).
It is apparent that the precision of analog computing
devices depends on many factors. One is the choice of
physical process and the way it is utilized in the device.
For example a linear mathematical operation can be real-
ized by using a linear region of a nonlinear physical pro-
cess, but the realization will be approximate and have some
inherent imprecision. Also, associated, unavoidable physi-
cal eﬀects (e. g., loading, and leakage and other losses) may
prevent precise implementation of an intended mathemat-
ical function. Further, there are fundamental physical lim-
itations to resolution (e. g., quantum eﬀects, diﬀraction).
Noise is inevitable, both intrinsic (e. g., thermal noise) and
extrinsic (e. g., ambient radiation). Changes in ambient
physical conditions, such as temperature, can aﬀect the
physical processes and decrease precision. At slower time
scales, materials and components age and their physical
characteristics change. In addition, there are always tech-
nical and economic limits to the control of components,
materials, and processes in analog device fabrication.
The precision of analog and digital computing devices
depend on very diﬀerent factors. The precision of a (bi-
nary) digital device depends on the number of bits, which
inﬂuences the amount of hardware, but not its quality. For
example, a 64-bit adder is about twice the size of a 32-
bit adder, but can made out of the same components. At
worst, the size of a digital device might increase with the

282 A
Analog Computation
square of the number of bits of precision. This is because
binary digital devices only need to represent two states,
and therefore they can operate in saturation. The fabrica-
tion standards suﬃcient for the ﬁrst bit of precision are
also suﬃcient for the 64th bit. Analog devices, in con-
trast, need to be able to represent a continuum of states
precisely. Therefore, the fabrication of high-precision ana-
log devices is much more expensive than low-precision
devices, since the quality of components, materials, and
processes must be much more carefully controlled. Dou-
bling the precision of an analog device may be expensive,
whereas the cost of each additional bit of digital precision
is incremental; that is, the cost is proportional to the loga-
rithm of the precision expressed as a fraction of full range.
The forgoing considerations might seem to be a con-
vincing argument for the superiority of digital to analog
technology, and indeed they were an important factor in
the competition between analog and digital computers in
the middle of the twentieth century [pp. 257–261 in 82].
However, as was argued at that time, many computer ap-
plications do not require high precision. Indeed, in many
engineering applications, the input data are known to only
a few digits, and the equations may be approximate or de-
rived from experiments. In these cases the very high pre-
cision of digital computation is unnecessary and may in
fact be misleading (e. g., if one displays all 14 digits of a re-
sult that is accurate to only three). Furthermore, many ap-
plications in image processing and control do not require
high precision. More recently, research in artiﬁcial neu-
ral networks (ANNs) has shown that low-precision ana-
log computation is suﬃcient for almost all ANN applica-
tions. Indeed, neural information processing in the brain
seems to operate with very low precision (perhaps less than
10% [p. 378 in 50]), for which it compensates with massive
parallelism. For example, by coarse coding a population of
low-precision devices can represent information with rel-
atively high precision [pp. 91–96 in 74,75].
Scaling
An important aspect of analog computing is
scaling, which is used to adjust a problem to an analog
computer. First is time scaling, which adjusts a problem
to the characteristic time scale at which a computer oper-
ates, which is a consequence of its design and the physical
processes by which it is realized [pp. 37–44 in 62,pp. 262–
263 in 70,pp. 241–243 in 89]. For example, we might want
a simulation to proceed on a very diﬀerent time scale from
the primary system. Thus a weather or economic simu-
lation should proceed faster than real time in order to
get useful predictions. Conversely, we might want to slow
down a simulation of protein folding so that we can ob-
serve the stages in the process. Also, for accurate results
it is necessary to avoid exceeding the maximum response
rate of the analog devices, which might dictate a slower
simulation speed. On the other hand, too slow a compu-
tation might be inaccurate as a consequence of instability
(e. g., drift and leakage in the integrators).
Time scaling aﬀects only time-dependent operations
such as integration. For example, suppose t, time in the
primary system or “problem time”, is related to , time
in the computer, by  D ˇt. Therefore, an integration
u(t) D R t
0 v(t0)dt0 in the primary system is replaced by
the integration u() D ˇ1 R 
0 v(0)d0 on the computer.
Thus time scaling may be accomplished simply by decreas-
ing the input gain to the integrator by a factor of ˇ.
Fundamental to analog computation is the represen-
tation of a continuous quantity in the primary system by
a continuous quantity in the computer. For example, a dis-
placement x in meters might be represented by a poten-
tial V in volts. The two are related by an amplitude or
magnitude scale factor, V D ˛x, (with units volts/meter),
chosen to meet two criteria [pp. 103–106 in 2,Chap. 4 in
62,pp. 127–128 in 70,pp. 233–240 in 89]. On the one hand,
˛ must be suﬃciently small so that the range of the prob-
lem variable is accommodated within the range of values
supported by the computing device. Exceeding the device’s
intended operating range may lead to inaccurate results
(e. g., forcing a linear device into nonlinear behavior). On
the other hand, the scale factor should not be too small,
or relevant variation in the problem variable will be less
than the resolution of the device, also leading to inaccu-
racy. (Recall that precision is speciﬁed as a fraction of full-
range variation.)
In addition to the explicit variables of the primary sys-
tem, there are implicit variables, such as the time deriva-
tives of the explicit variables, and scale factors must be
chosen for them too. For example, in addition to displace-
ment x, a problem might include velocity ˙x and accelera-
tion ¨x. Therefore, scale factors ˛, ˛0, and ˛00 must be cho-
sen so that ˛x, ˛0 ˙x, and ˛00 ¨x have an appropriate range of
variation (neither too large nor too small).
Once a scale factor has been chosen, the primary sys-
tem equations are adjusted to obtain the analog comput-
ing equations. For example, if we have scaled u D ˛x and
v D ˛0 ˙x, then the integration x(t) D
R t
0 ˙x(t0)dt0 would be
computed by scaled equation:
u(t) D ˛
˛0
Z t
0
v(t0)dt0:
This is accomplished by simply setting the input gain of
the integrator to ˛/˛0.
In practice, time scaling and magnitude scaling are not
independent [p. 262 in 70]. For example, if the derivatives

Analog Computation
A
283
of a variable can be large, then the variable can change
rapidly, and so it may be necessary to slow down the com-
putation to avoid exceeding the high-frequency response
of the computer. Conversely, small derivatives might re-
quire the computation to be run faster to avoid integra-
tor leakage etc. Appropriate scale factors are determined
by considering both the physics and the mathematics of
the problem [pp. 40–44 in 62]. That is, ﬁrst, the physics of
the primary system may limit the ranges of the variables
and their derivatives. Second, analysis of the mathemati-
cal equations describing the system can give additional in-
formation on the ranges of the variables. For example, in
some cases the natural frequency of a system can be es-
timated from the coeﬃcients of the diﬀerential equations;
the maximum of the nth derivative is then estimated as the
n power of this frequency [p. 42 in 62,pp. 238–240 in 89].
In any case, it is not necessary to have accurate values for
the ranges; rough estimates giving orders of magnitude are
adequate.
It is tempting to think of magnitude scaling as a prob-
lem unique to analog computing, but before the invention
of ﬂoating-point numbers it was also necessary in digi-
tal computer programming. In any case it is an essential
aspect of analog computing, in which physical processes
are more directly used for computation than they are in
digital computing. Although the necessity of scaling has
been a source of criticism, advocates for analog comput-
ing have argued that it is a blessing in disguise, because it
leads to improved understanding of the primary system,
which was often the goal of the computation in the ﬁrst
place [5,Chap. 8 in 82]. Practitioners of analog computing
are more likely to have an intuitive understanding of both
the primary system and its mathematical description (see
Sect. “Analog Thinking”).
Analog Computation in Nature
Computational processes—that is to say, information
processing and control—occur in many living systems,
most obviously in nervous systems, but also in the self-
organized behavior of groups of organisms. In most cases
natural computation is analog, either because it makes use
of continuous natural processes, or because it makes use of
discrete but stochastic processes. Several examples will be
considered brieﬂy.
Neural Computation
In the past neurons were thought of binary computing de-
vices, something like digital logic gates. This was a conse-
quence of the “all or nothing” response of a neuron, which
refers to the fact that it does or does not generate an ac-
tion potential (voltage spike) depending, respectively, on
whether its total input exceeds a threshold or not (more ac-
curately, it generates an action potential if the membrane
depolarization at the axon hillock exceeds the threshold
and the neuron is not in its refractory period). Certainly
some neurons (e. g., so-called “command neurons”) do act
something like logic gates. However, most neurons are an-
alyzed better as analog devices, because the rate of impulse
generation represents signiﬁcant information. In particu-
lar, an amplitude code, the membrane potential near the
axon hillock (which is a summation of the electrical in-
ﬂuences on the neuron), is translated into a rate code for
more reliable long-distance transmission along the axons.
Nevertheless, the code is low precision (about one digit),
since information theory shows that it takes at least N mil-
liseconds (and probably more like 5N ms) to discriminate
N values [39]. The rate code is translated back to an am-
plitude code by the synapses, since successive impulses re-
lease neurotransmitter from the axon terminal, which dif-
fuses across the synaptic cleft to receptors. Thus a synapse
acts as a leaky integrator to time-average the impulses.
As previously discussed (Sect. “Continuous State
Space”), many artiﬁcial neural net models have real-valued
neural activities, which correspond to rate-encoded axonal
signals of biological neurons. On the other hand, these
models typically treat the input connections as simple real-
valued weights, which ignores the analog signal process-
ing that takes place in the dendritic trees of biological
neurons. The dendritic trees of many neurons are com-
plex structures, which often have thousand of synaptic in-
puts. The binding of neurotransmitters to receptors causes
minute voltage ﬂuctuations, which propagate along the
membrane, and ultimately cause voltage ﬂuctuations at the
axon hillock, which inﬂuence the impulse rate. Since the
dendrites have both resistance and capacitance, to a ﬁrst
approximation the signal propagation is described by the
“cable equations”, which describe passive signal propaga-
tion in cables of speciﬁed diameter, capacitance, and re-
sistance [Chap. 1 in 1]. Therefore, to a ﬁrst approxima-
tion, a neuron’s dendritic net operates as an adaptive lin-
ear analog ﬁlter with thousands of inputs, and so it is ca-
pable of quite complex signal processing. More accurately,
however, it must be treated as a nonlinear analog ﬁlter,
since voltage-gated ion channels introduce nonlinear ef-
fects. The extent of analog signal processing in dendritic
trees is still poorly understood.
In most cases, then, neural information processing
is treated best as low-precision analog computation. Al-
though individual neurons have quite broadly tuned re-
sponses, accuracy in perception and sensorimotor control
is achieved through coarse coding, as already discussed

284 A
Analog Computation
(Sect. “Characteristics of Analog Computation”). Further,
one widely used neural representation is the cortical map,
in which neurons are systematically arranged in accord
with one or more dimensions of their stimulus space, so
that stimuli are represented by patterns of activity over
the map. (Examples are tonotopic maps, in which pitch
is mapped to cortical location, and retinotopic maps, in
which cortical location represents retinal location.) Since
neural density in the cortex is at least 146 000 neurons per
square millimeter [p. 51 in 14], even relatively small cor-
tical maps can be treated as ﬁelds and information pro-
cessing in them as analog ﬁeld computation (see ▷Field
Computation in Natural and Artiﬁcial Intelligence). Over-
all, the brain demonstrates what can be accomplished by
massively parallel analog computation, even if the individ-
ual devices are comparatively slow and of low precision.
Adaptive Self-Organization in Social Insects
Another example of analog computation in nature is pro-
vided by the self-organizing behavior of social insects, mi-
croorganisms, and other populations [13]. Often such or-
ganisms respond to concentrations, or gradients in the
concentrations, of chemicals produced by other members
of the population. These chemicals may be deposited and
diﬀuse through the environment. In other cases, insects
and other organisms communicate by contact, but may
maintain estimates of the relative proportions of diﬀer-
ent kinds of contacts. Because the quantities are eﬀectively
continuous, all these are examples of analog control and
computation.
Self-organizing populations provide many informa-
tive examples of the use of natural processes for analog
information processing and control. For example, diﬀu-
sion of pheromones is a common means of self-organi-
zation in insect colonies, facilitating the creation of paths
to resources, the construction of nests, and many other
functions [13]. Real diﬀusion (as opposed to sequential
simulations of it) executes, in eﬀect, a massively parallel
search of paths from the chemical’s source to its recipi-
ents and allows the identiﬁcation of near-optimal paths.
Furthermore, if the chemical degrades, as is generally the
case, then the system will be adaptive, in eﬀect continually
searching out the shortest paths, so long as source contin-
ues to function [13]. Simulated diﬀusion has been applied
to robot path planning [32,69].
Genetic Circuits
Another example of natural analog computing is provided
by the genetic regulatory networks that control the behavior
of cells, in multicellular organisms as well as single-celled
ones [16]. These networks are deﬁned by the mutually in-
terdependent regulatory genes, promoters, and repressors
that control the internal and external behavior of a cell.
The interdependencies are mediated by proteins, the syn-
thesis of which is governed by genes, and which in turn
regulate the synthesis of other gene products (or them-
selves). Since it is the quantities of these substances that
is relevant, many of the regulatory motifs can be described
in computational terms as adders, subtracters, integrators,
etc. Thus the genetic regulatory network implements an
analog control system for the cell [68].
It might be argued that the number of intracellular
molecules of a particular protein is a (relatively small) dis-
crete number, and therefore that it is inaccurate to treat
it as a continuous quantity. However, the molecular pro-
cesses in the cell are stochastic, and so the relevant quan-
tity is the probability that a regulatory protein will bind to
a regulatory site. Further, the processes take place in con-
tinuous real time, and so the rates are generally the sig-
niﬁcant quantities. Finally, although in some cases gene
activity is either on or oﬀ(more accurately: very low),
in other cases it varies continuously between these ex-
tremes [pp. 388–390 in 29].
Embryological development combines the analog con-
trol of individual cells with the sort of self-organization of
populations seen in social insects and other colonial or-
ganisms. Locomotion of the cells and the expression of
speciﬁc genes is controlled by chemical signals, among
other mechanisms [16,17]. Thus PDEs have proved use-
ful in explaining some aspects of development; for ex-
ample reaction-diﬀusion equations have been used to de-
scribe the formation of hair-coat patterns and related phe-
nomena [13,48,57]; see ▷Reaction-Diﬀusion Computing.
Therefore the developmental process is governed by natu-
rally occurring analog computation.
Is Everything a Computer?
It might seem that any continuous physical process could
be viewed as analog computation, which would make the
term almost meaningless. As the question has been put,
is it meaningful (or useful) to say that the solar system is
computing Kepler’s laws? In fact, it is possible and worth-
while to make a distinction between computation and
other physical processes that happen to be described by
mathematical laws [40,41,44,46].
If we recall the original meaning of analog compu-
tation (Sect. “Deﬁnition of the Subject”), we see that the
computational system is used to solve some mathematical
problem with respect to a primary system. What makes
this possible is that the computational system and the pri-

Analog Computation
A
285
mary system have the same, or systematically related, ab-
stract (mathematical) structures. Thus the computational
system can inform us about the primary system, or be used
to control it, etc. Although from a practical standpoint
some analogs are better than others, in principle any phys-
ical system can be used that obeys the same equations as
the primary system.
Based on these considerations we may deﬁne compu-
tation as a physical process the purpose of which is the
abstract manipulation of abstract objects (i. e., informa-
tion processing); this deﬁnition applies to analog, digital,
and hybrid computation [40,41,44,46]. Therefore, to de-
termine if a natural system is computational we need to
look to its purpose or function within the context of the
living system of which it is a part. One test of whether its
function is the abstract manipulation of abstract objects is
to ask whether it could still fulﬁll its function if realized
by diﬀerent physical processes, a property called multiple
realizability. (Similarly, in artiﬁcial systems, a simulation
of the economy might be realized equally accurately by
a hydraulic analog computer or an electronic analog com-
puter [5].) By this standard, the majority of the nervous
system is purely computational; in principle it could be
replaced by electronic devices obeying the same diﬀeren-
tial equations. In the other cases we have considered (self-
organization of living populations, genetic circuits) there
are instances of both pure computation and computation
mixed with other functions (for example, where the spe-
ciﬁc substances used have other – e. g. metabolic – roles in
the living system).
General-Purpose Analog Computation
The Importance of General-Purpose Computers
Although special-purpose analog and digital computers
have been developed, and continue to be developed, for
many purposes, the importance of general-purpose com-
puters, which can be adapted easily for a wide variety
of purposes, has been recognized since at least the nine-
teenth century. Babbage’s plans for a general-purpose dig-
ital computer, his analytical engine (1835), are well known,
but a general-purpose diﬀerential analyzer was advocated
by Kelvin [84]. Practical general-purpose analog and dig-
ital computers were ﬁrst developed at about the same
time: from the early 1930s through the war years. General-
purpose computers of both kinds permit the prototyping
of special-purpose computers and, more importantly, per-
mit the ﬂexible reuse of computer hardware for diﬀerent
or evolving purposes.
The concept of a general-purpose computer is useful
also for determining the limits of a computing paradigm.
If one can design—theoretically or practically—a univer-
sal computer, that is, a general-purpose computer capable
of simulating any computer in a relevant class, then any-
thing uncomputable by the universal computer will also
be uncomputable by any computer in that class. This is,
of course, the approach used to show that certain func-
tions are uncomputable by any Turing machine because
they are uncomputable by a universal Turing machine. For
the same reason, the concept of general-purpose analog
computers, and in particular of universal analog computers
are theoretically important for establishing limits to analog
computation.
General-Purpose Electronic Analog Computers
Before taking up these theoretical issues, it is worth re-
calling that a typical electronic GPAC would include lin-
ear elements, such as adders, subtracters, constant mul-
tipliers, integrators, and diﬀerentiators; nonlinear ele-
ments, such as variable multipliers and function genera-
tors; other computational elements, such as comparators,
noise generators, and delay elements (Sect. “Electronic
Analog Computation in the 20th Century”). These are, of
course, in addition to input/output devices, which would
not aﬀect its computational abilities.
Shannon’s Analysis
Claude Shannon did an important analysis of the compu-
tational capabilities of the diﬀerential analyzer, which ap-
plies to many GPACs [76,77]. He considered an abstract
diﬀerential analyzer equipped with an unlimited number
of integrators, adders, constant multipliers, and function
generators (for functions with only a ﬁnite number of
ﬁnite discontinuities), with at most one source of drive
(which limits possible interconnections between units).
This was based on prior work that had shown that almost
all the generally used elementary functions could be gen-
erated with addition and integration. We will summarize
informally a few of Shannon’s results; for details, please
consult the original paper.
First Shannon oﬀers proofs that, by setting up the cor-
rect ODEs, a GPAC with the mentioned facilities can gen-
erate any function if and only if is not hypertranscenden-
tal (Theorem II); thus the GPAC can generate any function
that is algebraic transcendental (a very large class), but not,
for example, Euler’s gamma function and Riemann’s zeta
function. He also shows that the GPAC can generate func-
tions derived from generable functions, such as the inte-
grals, derivatives, inverses, and compositions of generable
functions (Thms. III, IV). These results can be generalized
to functions of any number of variables, and to their com-

286 A
Analog Computation
positions, partial derivatives, and inverses with respect to
any one variable (Thms. VI, VII, IX, X).
Next Shannon shows that a function of any number of
variables that is continuous over a closed region of space
can be approximated arbitrarily closely over that region
with a ﬁnite number of adders and integrators (Thms. V,
VIII).
Shannon then turns from the generation of func-
tions to the solution of ODEs and shows that the GPAC
can solve any system of ODEs deﬁned in terms of non-
hypertranscendental functions (Thm. XI).
Finally, Shannon addresses a question that might seem
of limited interest, but turns out to be relevant to the com-
putational power of analog computers (see Sect. “Analog
Computation and the Turing Limit” below). To under-
stand it we must recall that he was investigating the diﬀer-
ential analyzer—a mechanical analog computer—but sim-
ilar issues arise in other analog computing technologies.
The question is whether it is possible to perform an ar-
bitrary constant multiplication, u D kv, by means of gear
ratios. He show that if we have just two gear ratios a and
b(a; b ¤ 0; 1), such that b is not a rational power of a, then
by combinations of these gears we can approximate k ar-
bitrarily closely (Thm. XII). That is, to approximate mul-
tiplication by arbitrary real numbers, it is suﬃcient to be
able to multiply by a, b, and their inverses, provided a and
b are not related by a rational power.
Shannon mentions an alternative method of constant
multiplication, which uses integration, kv D R v
0 kdv, but
this requires setting the integrand to the constant function
k. Therefore, multiplying by an arbitrary real number re-
quires the ability to input an arbitrary real as the integrand.
The issue of real-valued inputs and outputs to analog com-
puters is relevant both to their theoretical power and to
practical matters of their application (see Sect. “Real-val-
ued Inputs, Output, and Constants”).
Shannon’s proofs, which were incomplete, were even-
tually reﬁned by M. Pour-El [63] and ﬁnally corrected by
L. Lipshitz and L.A. Rubel [35]. Rubel [72] proved that
Shannon’s GPAC cannot solve the Dirichlet problem for
Laplace’s equation on the disk; indeed, it is limited to
initial-value problems for algebraic ODEs. Speciﬁcally, the
Shannon–Pour-El Thesis is that the outputs of the GPAC
are exactly the solutions of the algebraic diﬀerential equa-
tions, that is, equations of the form
P[x; y(x); y0(x); y00(x); : : : ; y(n)(x)] D 0 ;
where P is a polynomial that is not identically vanishing
in any of its variables (these are the diﬀerentially alge-
braic functions) [71]. (For details please consult the cited
papers.) The limitations of Shannon’s GPAC motivated
Rubel’s deﬁnition of the Extended Analog Computer.
Rubel’s Extended Analog Computer
The combination of Rubel’s [71] conviction that the brain
is an analog computer together with the limitations of
Shannon’s GPAC led him to propose the Extended Ana-
log Computer (EAC) [73].
Like Shannon’s GPAC (and the Turing machine), the
EAC is a conceptual computer intended to facilitate theo-
retical investigation of the limits of a class of computers.
The EAC extends the GPAC in a number of respects. For
example, whereas the GPAC solves equations deﬁned over
a single variable (time), the EAC can generate functions
over any ﬁnite number of real variables. Further, whereas
the GPAC is restricted to initial-value problems for ODEs,
the EAC solves both initial- and boundary-value problems
for a variety of PDEs.
The EAC is structured into a series of levels, each more
powerful than the ones below it, from which it accepts in-
puts. The inputs to the lowest level are a ﬁnite number of
real variables (“settings”). At this level it operates on real
polynomials, from which it is able to generate the diﬀeren-
tially algebraic functions. The computing on each level is
accomplished by conceptual analog devices, which include
constant real-number generators, adders, multipliers, dif-
ferentiators, “substituters” (for function composition), de-
vices for analytic continuation, and inverters, which solve
systems of equations deﬁned over functions generated
by the lower levels. Most characteristic of the EAC is
the “boundary-value-problem box”, which solves systems
of PDEs and ODEs subject to boundary conditions and
other constraints. The PDEs are deﬁned in terms of func-
tions generated by the lower levels. Such PDE solvers may
seem implausible, and so it is important to recall ﬁeld-
computing devices for this purpose were implemented in
some practical analog computers (see Sect. “History”) and
more recently in Mills’ EAC [54]. As Rubel observed, PDE
solvers could be implemented by physical processes that
obey the same PDEs (heat equation, wave equation, etc.).
(See also Sect. “Future Directions” below.)
Finally, the EAC is required to be “extremely well-
posed”, which means that each level is relatively insensitive
to perturbations in its inputs; thus “all the outputs depend
in a strongly deterministic and stable way on the initial set-
tings of the machine” [73].
Rubel [73] proves that the EAC can compute every-
thing that the GPAC can compute, but also such functions
as the gamma and zeta, and that it can solve the Dirich-
let problem for Laplace’s equation on the disk, all of which

Analog Computation
A
287
are beyond the GPAC’s capabilities. Further, whereas the
GPAC can compute diﬀerentially algebraic functions of
time, the EAC can compute diﬀerentially algebraic func-
tions of any ﬁnite number of real variables. In fact, Rubel
did not ﬁnd any real-analytic (C1) function that is not
computable on the EAC, but he observes that if the EAC
can indeed generate every real-analytic function, it would
be too broad to be useful as a model of analog computa-
tion.
Analog Computation and the Turing Limit
Introduction
The Church–Turing Thesis asserts that anything that is ef-
fectively computable is computable by a Turing machine,
but the Turing machine (and equivalent models, such as
the lambda calculus) are models of discrete computation,
and so it is natural to wonder how analog computing com-
pares in power, and in particular whether it can compute
beyond the “Turing limit”. Superﬁcial answers are easy to
obtain, but the issue is subtle because it depends upon
choices among deﬁnitions, none of which is obviously cor-
rect, it involves the foundations of mathematics and its
philosophy, and it raises epistemological issues about the
role of models in scientiﬁc theories. Nevertheless this is
an active research area, but many of the results are ap-
parently inconsistent due to the diﬀering assumptions on
which they are based. Therefore this section will be limited
to a mention of a few of the interesting results, but without
attempting a comprehensive, systematic, or detailed sur-
vey; Siegelmann [78] can serve as an introduction to the
literature.
A Sampling of Theoretical Results
Continuous-Time Models
P. Orponen’s [59] 1997 sur-
vey of continuous-time computation theory is a good in-
troduction to the literature as of that time; here we give
a sample of these and more recent results.
There are several results showing that—under various
assumptions—analog computers have at least the power
of Turing machines (TMs). For example, M.S. Bran-
icky [11] showed that a TM could be simulated by ODEs,
but he used non-diﬀerentiable functions; O. Bournez et
al. [8] provide an alternative construction using only an-
alytic functions. They also prove that the GPAC com-
putability coincides with (Turing-)computable analysis,
which is surprising, since the gamma function is Turing-
computable but, as we have seen, the GPAC cannot gen-
erate it. The paradox is resolved by a distinction between
generating a function and computing it, with the latter,
broader notion permitting convergent computation of the
function (that is, as t ! 1). However, the computational
power of general ODEs has not been determined in gen-
eral [p. 149 in 78]. MB Pour-El and I Richards exhibit
a Turing-computable ODE that does not have a Turing-
computable solution [64,66]. M. Stannett [83] also deﬁned
a continuous-time analog computer that could solve the
halting problem.
C. Moore [55] deﬁnes a class of continuous-time re-
cursive functions over the reals, which includes a zero-
ﬁnding operator . Functions can be classiﬁed into a hi-
erarchy depending on the number of uses of , with
the lowest level (no s) corresponding approximately to
Shannon’s GPAC. Higher levels can compute non-Turing-
computable functions, such as the decision procedure for
the halting problem, but he questions whether this result
is relevant in the physical world, which is constrained by
“noise, quantum eﬀects, ﬁnite accuracy, and limited re-
sources”. O. Bournez and M. Cosnard [9] have extended
these results and shown that many dynamical systems have
super-Turing power.
S. Omohundro [58] showed that a system of ten cou-
pled nonlinear PDEs could simulate an arbitrary cellu-
lar automaton (see ▷Mathematical Basis of Cellular Au-
tomata, Introduction to), which implies that PDEs have at
least Turing power. Further, D. Wolpert and B.J. MacLen-
nan [90,91] showed that any TM can be simulated by
a ﬁeld computer with linear dynamics, but the construc-
tion uses Dirac delta functions. Pour-El and Richards ex-
hibit a wave equation in three-dimensional space with
Turing-computable initial conditions, but for which the
unique solution is Turing-uncomputable [65,66].
Sequential-Time Models
We will mention a few of the
results that have been obtained concerning the power of
sequential-time analog computation.
Although the BSS model has been investigated exten-
sively, its power has not been completely determined [6,7].
It is known to depend on whether just rational num-
bers or arbitrary real numbers are allowed in its pro-
grams [p. 148 in 78].
A coupled map lattice (CML) is a cellular automaton
with real-valued states ▷Mathematical Basis of Cellular
Automata, Introduction to; it is a sequential-time analog
computer, which can be considered a discrete-space ap-
proximation to a simple sequential-time ﬁeld computer. P.
Orponen and M. Matamala [60] showed that a ﬁnite CML
can simulate a universal Turing machine. However, since
a CML can simulate a BSS program or a recurrent neural
network (see Sect. “Recurrent Neural Networks” below), it
actually has super-Turing power [p. 149 in 78].

288 A
Analog Computation
Recurrent neural networks are some of the most im-
portant examples of sequential analog computers, and so
the following section is devoted to them.
Recurrent Neural Networks
With the renewed inter-
est in neural networks in the mid-1980s, may investi-
gators wondered if recurrent neural nets have super-
Turing power. M. Garzon and S. Franklin showed that
a sequential-time net with a countable inﬁnity of neu-
rons could exceed Turing power [21,23,24]. Indeed, Siegel-
mann and E.D. Sontag [80] showed that ﬁnite neural nets
with real-valued weights have super-Turing power, but W.
Maass and Sontag [36] showed that recurrent nets with
Gaussian or similar noise had sub-Turing power, illustrat-
ing again the dependence on these results on assumptions
about what is a reasonable mathematical idealization of
analog computing.
For recent results on recurrent neural networks, we
will restrict our attention of the work of Siegelmann [78],
who addresses the computational power of these networks
in terms of the classes of languages they can recognize.
Without loss of generality the languages are restricted to
sets of binary strings. A string to be tested is fed to the
network one bit at a time, along with an input that indi-
cates when the end of the input string has been reached.
The network is said to decide whether the string is in the
language if it correctly indicates whether it is in the set or
not, after some ﬁnite number of sequential steps since in-
put began.
Siegelmann shows that, if exponential time is al-
lowed for recognition, ﬁnite recurrent neural networks
with real-valued weights (and saturated-linear activation
functions) can compute all languages, and thus they are
more powerful than Turing machines. Similarly, stochas-
tic networks with rational weights also have super-Turing
power, although less power than the deterministic nets
with real weights. (Speciﬁcally, they compute P/POLY and
BPP/log respectively; see Siegelmann [Chaps. 4, 9 in 78]
for details.) She further argues that these neural networks
serve as a “standard model” of (sequential) analog compu-
tation (comparable to Turing machines in Church-Turing
computation), and therefore that the limits and capabili-
ties of these nets apply to sequential analog computation
generally.
Siegelmann [p. 156 in 78] observes that the super-
Turing power of recurrent neural networks is a conse-
quence of their use of non-rational real-valued weights.
In eﬀect, a real number can contain an inﬁnite number
of bits of information. This raises the question of how the
non-rational weights of a network can ever be set, since it
is not possible to deﬁne a physical quantity with inﬁnite
precision. However, although non-rational weights may
not be able to be set from outside the network, they can
be computed within the network by learning algorithms,
which are analog computations. Thus, Siegelmann sug-
gests, the fundamental distinction may be between static
computational models, such as the Turing machine and its
equivalents, and dynamically evolving computational mod-
els, which can tune continuously variable parameters and
thereby achieve super-Turing power.
Dissipative Models
Beyond the issue of the power of
analog computing relative to the Turing limit, there are
also questions of its relative eﬃciency. For example, could
analog computing solve NP-hard problems in polynomial
or even linear time? In traditional computational com-
plexity theory, eﬃciency issues are addressed in terms of
the asymptotic number of computation steps to compute
a function as the size of the function’s input increases. One
way to address corresponding issues in an analog context
is by treating an analog computation as a dissipative sys-
tem, which in this context means a system that decreases
some quantity (analogous to energy) so that the system
state converges to an point attractor. From this perspec-
tive, the initial state of the system incorporates the input
to the computation, and the attractor represents its out-
put. Therefore, HT Sieglemann, S Fishman, and A Ben-
Hur have developed a complexity theory for dissipative
systems, in both sequential and continuous time, which
addresses the rate of convergence in terms of the un-
derlying rates of the system [4,79]. The relation between
dissipative complexity classes (e. g., Pd, NPd) and corre-
sponding classical complexity classes (P, NP) remains un-
clear [p. 151 in 78].
Real-Valued Inputs, Outputs, and Constants
A common argument, with relevance to the theoretical
power of analog computation, is that an input to an analog
computer must be determined by setting a dial to a num-
ber or by typing a number into digital-to-analog conver-
sion device, and therefore that the input will be a rational
number. The same argument applies to any internal con-
stants in the analog computation. Similarly, it is argued,
any output from an analog computer must be measured,
and the accuracy of measurement is limited, so that the
result will be a rational number. Therefore, it is claimed,
real numbers are irrelevant to analog computing, since any
practical analog computer computes a function from the
rationals to the rationals, and can therefore be simulated
by a Turing machine. (See related arguments by Martin
Davis [18,19].)

Analog Computation
A
289
There are a number of interrelated issues here, which
may be considered brieﬂy. First, the argument is couched
in terms of the input or output of digital representations,
and the numbers so represented are necessarily rational
(more generally, computable). This seems natural enough
when we think of an analog computer as a calculating de-
vice, and in fact many historical analog computers were
used in this way and had digital inputs and outputs (since
this is our most reliable way of recording and reproducing
quantities).
However, in many analog control systems, the inputs
and outputs are continuous physical quantities that vary
continuously in time (also a continuous physical quantity);
that is, according to current physical theory, these quanti-
ties are real numbers, which vary according to diﬀeren-
tial equations. It is worth recalling that physical quantities
are neither rational nor irrational; they can be so classi-
ﬁed only in comparison with each other or with respect to
a unit, that is, only if they are measured and digitally repre-
sented. Furthermore, physical quantities are neither com-
putable nor uncomputable (in a Church-Turing sense);
these terms apply only to discrete representations of these
quantities (i. e., to numerals or other digital representa-
tions).
Therefore, in accord with ordinary mathematical de-
scriptions of physical processes, analog computations can
can be treated as having arbitrary real numbers (in some
range) as inputs, outputs, or internal states; like other con-
tinuous processes, continuous-time analog computations
pass through all the reals in some range, including non-
Turing-computable reals. Paradoxically, however, these
same physical processes can be simulated on digital com-
puters.
The Issue of Simulation by Turing Machines
and Digital Computers
Theoretical results about the computational power, rel-
ative to Turing machines, of neural networks and other
analog models of computation raise diﬃcult issues, some
of which are epistemological rather than strictly technical.
On the one hand, we have a series of theoretical results
proving the super-Turing power of analog computation
models of various kinds. On the other hand, we have the
obvious fact that neural nets are routinely simulated on
ordinary digital computers, which have at most the power
of Turing machines. Furthermore, it is reasonable to sup-
pose that any physical process that might be used to re-
alize analog computation—and certainly the known pro-
cesses—could be simulated on a digital computer, as is
done routinely in computational science. This would seem
to be incontrovertible proof that analog computation is
no more powerful than Turing machines. The crux of the
paradox lies, of course, in the non-Turing-computable re-
als. These numbers are a familiar, accepted, and necessary
part of standard mathematics, in which physical theory
is formulated, but from the standpoint of Church-Turing
(CT) computation they do not exist. This suggests that the
the paradox is not a contradiction, but reﬂects a divergence
between the goals and assumptions of the two models of
computation.
The Problem of Models of Computation
These issues may be put in context by recalling that the
Church-Turing (CT) model of computation is in fact
a model, and therefore that it has the limitations of all
models. A model is a cognitive tool that improves our abil-
ity to understand some class of phenomena by preserv-
ing relevant characteristics of the phenomena while alter-
ing other, irrelevant (or less relevant) characteristics. For
example, a scale model alters the size (taken to be irrele-
vant) while preserving shape and other characteristics. Of-
ten a model achieves its purposes by making simplifying or
idealizing assumptions, which facilitate analysis or simula-
tion of the system. For example, we may use a linear math-
ematical model of a physical process that is only approxi-
mately linear. For a model to be eﬀective it must preserve
characteristics and make simplifying assumptions that are
appropriate to the domain of questions it is intended to
answer, its frame of relevance [46]. If a model is applied
to problems outside of its frame of relevance, then it may
give answers that are misleading or incorrect, because they
depend more on the simplifying assumptions than on the
phenomena being modeled. Therefore we must be espe-
cially cautious applying a model outside of its frame of rel-
evance, or even at the limits of its frame, where the simpli-
fying assumptions become progressively less appropriate.
The problem is aggravated by the fact that often the frame
of relevance is not explicit deﬁned, but resides in a tacit
background of practices and skills within some discipline.
Therefore, to determine the applicability of the CT
model of computation to analog computing, we must con-
sider the frame of relevance of the CT model. This is easiest
if we recall the domain of issues and questions it was orig-
inally developed to address: issues of eﬀective calculability
and derivability in formalized mathematics. This frame of
relevance determines many of the assumptions of the CT
model, for example, that information is represented by ﬁ-
nite discrete structures of symbols from a ﬁnite alphabet,
that information processing proceeds by the application
of deﬁnite formal rules at discrete instants of time, and

290 A
Analog Computation
that a computational or derivational process must be com-
pleted in a ﬁnite number of these steps.1 Many of these as-
sumptions are incompatible with analog computing and
with the frames of relevance of many models of analog
computation.
Relevant Issues for Analog Computation
Analog computation is often used for control. Historically,
analog computers were used in control systems and to
simulate control systems, but contemporary analog VLSI
is also frequently applied in control. Natural analog com-
putation also frequently serves a control function, for ex-
ample, sensorimotor control by the nervous system, ge-
netic regulation in cells, and self-organized cooperation
in insect colonies. Therefore, control systems provide one
frame of relevance for models of analog computation.
In this frame of relevance real-time response is a criti-
cal issue, which models of analog computation, therefore,
ought to be able to address. Thus it is necessary to be able
to relate the speed and frequency response of analog com-
putation to the rates of the physical processes by which the
computation is realized. Traditional methods of algorithm
analysis, which are based on sequential time and asymp-
totic behavior, are inadequate in this frame of relevance.
On the one hand, the constants (time scale factors), which
reﬂect the underlying rate of computation are absolutely
critical (but ignored in asymptotic analysis); on the other
hand, in control applications the asymptotic behavior of
algorithm is generally irrelevant, since the inputs are typi-
cally ﬁxed in size or of a limited range of sizes.
The CT model of computation is oriented around
the idea that the purpose of a computation is to evalu-
ate a mathematical function. Therefore the basic criterion
of adequacy for a computation is correctness, that is, that
given a precise representation of an input to the function,
it will produce (after ﬁnitely many steps) a precise repre-
sentation of the corresponding output of the function. In
the context of natural computation and control, however,
other criteria may be equally or even more relevant. For
example, robustness is important: how well does the system
respond in the presence of noise, uncertainty, imprecision,
and error, which are unavoidable in real natural and ar-
tiﬁcial control systems, and how well does it respond to
defects and damage, which arise in many natural and arti-
ﬁcial contexts. Since the real world is unpredictable, ﬂexi-
bility is also important: how well does an artiﬁcial system
respond to inputs for which it was not designed, and how
well does a natural system behave in situations outside the
1See MacLennan [45,46] for a more detailed discussion of the
frame of relevance of the CT model.
range of those to which it is evolutionarily adapted. There-
fore, adaptability (through learning and other means) is
another important issue in this frame of relevance.2
Transcending Turing Computability
Thus we see that many applications of analog computation
raise diﬀerent questions from those addressed by the CT
model of computation; the most useful models of analog
computing will have a diﬀerent frame of relevance. In or-
der to address traditional questions such as whether ana-
log computers can compute “beyond the Turing limit”, or
whether they can solve NP-hard problems in polynomial
time, it is necessary to construct models of analog com-
putation within the CT frame of relevance. Unfortunately,
constructing such models requires making commitments
about many issues (such as the representation of reals and
the discretization of time), that may aﬀect the answers to
these questions, but are fundamentally unimportant in the
frame of relevance of the most useful applications of the
concept of analog computation. Therefore, being overly
focused on traditional problems in the theory of compu-
tation (which was formulated for a diﬀerent frame of rele-
vance) may distract us from formulating models of analog
computation that can address important issues in its own
frame of relevance.
Analog Thinking
It will be worthwhile to say a few words about the cognitive
implications of analog computing, which are a largely for-
gotten aspect of analog vs. digital debates of the late 20th
century. For example, it was argued that analog computing
provides a deeper intuitive understanding of a system than
the alternatives do [5,Chap. 8 in 82]. On the one hand,
analog computers aﬀorded a means of understanding an-
alytically intractable systems by means of “dynamic mod-
els”. By setting up an analog simulation, it was possible to
vary the parameters and explore interactively the behavior
of a dynamical system that could not be analyzed mathe-
matically. Digital simulations, in contrast, were orders of
magnitude slower and did not permit this kind of interac-
tive investigation. (Performance has improved suﬃciently
in contemporary digital computers so that in many cases
digital simulations can be used as dynamic models, some-
times with an interface that mimics an analog computer;
see [5].)
Analog computing is also relevant to the cognitive
distinction between knowing how (procedural knowledge)
2See MacLennan [45,46] for a more detailed discussion of the
frames of relevance of natural computation and control.

Analog Computation
A
291
and knowing that (factual knowledge) [Chap. 8 in 82].
The latter (“know-that”) is more characteristic of scien-
tiﬁc culture, which strives for generality and exactness, of-
ten by designing experiments that allow phenomena to
be studied in isolation, whereas the former (“know-how”)
is more characteristic of engineering culture; at least it
was so through the ﬁrst half of the twentieth century, be-
fore the development of “engineering science” and the
widespread use of analytic techniques in engineering ed-
ucation and practice. Engineers were faced with analyti-
cally intractable systems, with inexact measurements, and
with empirical relationships (characteristic curves, etc.),
all of which made analog computers attractive for solv-
ing engineering problems. Furthermore, because analog
computing made use of physical phenomena that were
mathematically analogous to those in the primary system,
the engineer’s intuition and understanding of one system
could be transferred to the other. Some commentators
have mourned the loss of hands-on intuitive understand-
ing attendant on the increasingly scientiﬁc orientation of
engineering education and the disappearance of analog
computer [5,34,61,67].
I will mention one last cognitive issue relevant to the
diﬀerences between analog and digital computing. As al-
ready discussed Sect. “Characteristics of Analog Compu-
tation”, it is generally agreed that it is less expensive to
achieve high precision with digital technology than with
analog technology. Of course, high precision may not be
important, for example when the available data are inex-
act or in natural computation. Further, some advocates of
analog computing argue that high precision digital result
are often misleading [p. 261 in 82]. Precision does not im-
ply accuracy, and the fact that an answer is displayed with
10 digits does not guarantee that it is accurate to 10 dig-
its; in particular, engineering data may be known to only
a few signiﬁcant ﬁgures, and the accuracy of digital calcu-
lation may be limited by numerical problems. Therefore,
on the one hand, users of digital computers might fall into
the trap of trusting their apparently exact results, but users
of modest-precision analog computers were more inclined
to healthy skepticism about their computations. Or so it
was claimed.
Future Directions
Certainly there are many purposes that are best served by
digital technology; indeed there is a tendency nowadays to
think that everything is done better digitally. Therefore it
will be worthwhile to consider whether analog computa-
tion should have a role in future computing technologies.
I will argue that the approaching end of Moore’s Law [56],
which has predicted exponential growth in digital logic
densities, will encourage the development of new analog
computing technologies.
Two avenues present themselves as ways toward
greater computing power: faster individual computing
elements and greater densities of computing elements.
Greater density increases power by facilitating parallel
computing, and by enabling greater computing power to
be put into smaller packages. Other things being equal,
the fewer the layers of implementation between the com-
putational operations and the physical processes that re-
alize them, that is to say, the more directly the physical
processes implement the computations, the more quickly
they will be able to proceed. Since most physical processes
are continuous (deﬁned by diﬀerential equations), analog
computation is generally faster than digital. For example,
we may compare analog addition, implemented directly
by the additive combination of physical quantities, with
the sequential process of digital addition. Similarly, other
things being equal, the fewer physical devices required to
implement a computational element, the greater will be
the density of these elements. Therefore, in general, the
closer the computational process is to the physical pro-
cesses that realize it, the fewer devices will be required, and
so the continuity of physical law suggests that analog com-
putation has the potential for greater density than digital.
For example, four transistors can realize analog addition,
whereas many more are required for digital addition. Both
considerations argue for an increasing role of analog com-
putation in post-Moore’s Law computing.
From this broad perspective, there are many physi-
cal phenomena that are potentially usable for future ana-
log computing technologies. We seek phenomena that
can be described by well-known and useful mathematical
functions (e. g., addition, multiplication, exponential, log-
arithm, convolution). These descriptions do not need to
be exact for the phenomena to be useful in many applica-
tions, for which limited range and precision are adequate.
Furthermore, in some applications speed is not an impor-
tant criterion; for example, in some control applications,
small size, low power, robustness, etc. may be more impor-
tant than speed, so long as the computer responds quickly
enough to accomplish the control task. Of course there are
many other considerations in determining whether given
physical phenomena can be used for practical analog com-
putation in a given application [47]. These include stabil-
ity, controllability, manufacturability, and the ease of in-
terfacing with input and output transducers and other de-
vices. Nevertheless, in the post-Moore’s Law world, we will
have to be willing to consider all physical phenomena as
potential computing technologies, and in many cases we

292 A
Analog Computation
will ﬁnd that analog computing is the most eﬀective way
to utilize them.
Natural computation provides many examples of ef-
fective analog computation realized by relatively slow,
low-precision operations, often through massive paral-
lelism. Therefore, post-Moore’s Law computing has much
to learn from the natural world.
Bibliography
Primary Literature
1. Anderson JA (1995) An Introduction to Neural Networks. MIT
Press, Cambridge
2. Ashley JR (1963) Introduction to Analog Computing. Wiley,
New York
3. Aspray W (1993) Edwin L. Harder and the Anacom: Ana-
log computing at Westinghouse. IEEE Ann Hist of Comput
15(2):35–52
4. Ben-Hur A, Siegelmann HT, Fishman S (2002) A theory of com-
plexity for continuous time systems. J Complex 18:51–86
5. Bissell CC (2004) A great disappearing act: The electronic ana-
logue computer. In: IEEE Conference on the History of Electron-
ics, Bletchley, June 2004. pp 28–30
6. Blum L, Cucker F, Shub M, Smale S (1998) Complexity and Real
Computation. Springer, Berlin
7. Blum L, Shub M, Smale S (1988) On a theory of computation
and complexity over the real numbers: NP completeness, re-
cursive functions and universal machines. Bulletin Am Math
Soc 21:1–46
8. Bournez O, Campagnolo ML, Graça DS, Hainry E. The General
Purpose Analog Computer and computable analysis are two
equivalent paradigms of analog computation. In: Theory and
Applications of Models of Computation (TAMC 2006). Lectures
Notes in Computer Science, vol 3959. Springer, Berlin, pp 631–
643
9. Bournez O, Cosnard M (1996) On the computational power
of dynamical systems and hybrid systems. Theor Comput Sci
168(2):417–59
10. Bowles MD (1996) US technological enthusiasm and British
technological skepticism in the age of the analog brain. Ann
Hist Comput 18(4):5–15
11. Branicky MS (1994) Analog computation with continuous
ODEs. In: Proceedings IEEE Workshop on Physics and Compu-
tation, Dallas, pp 265–274
12. Brockett RW (1988) Dynamical systems that sort lists, diagonal-
ize matrices and solve linear programming problems. In: Pro-
ceedings 27th IEEE Conference Decision and Control, Austin,
December 1988, pp 799–803
13. Camazine S, Deneubourg J-L, Franks NR, Sneyd G, Theraulaz
J, Bonabeau E (2001) Self-organization in Biological Systems.
Princeton Univ. Press, New York
14. Changeux J-P (1985) Neuronal Man: The Biology of Mind (trans:
Garey LL). Oxford University Press, Oxford
15. Clymer AB (1993) The mechanical analog computers of Han-
nibal Ford and William Newell. IEEE Ann Hist Comput 15(2):
19–34
16. Davidson EH (2006) The Regulatory Genome: Gene Regulatory
Networks in Development and Evolution. Academic Press, Am-
sterdam
17. Davies JA (2005) Mechanisms of Morphogensis. Elsevier, Ams-
terdam
18. Davis M (2004) The myth of hypercomputation. In: Teuscher C
(ed) Alan Turing: Life and Legacy of a Great Thinker. Springer,
Berlin, pp 195–212
19. Davis M (2006) Why there is no such discipline as hypercom-
putation. Appl Math Comput 178:4–7
20. Fakhraie SM, Smith KC (1997) VLSI-Compatible Implementa-
tion for Artificial Neural Networks. Kluwer, Boston
21. Franklin S, Garzon M (1990) Neural computability. In: Omidvar
OM (ed) Progress in Neural Networks, vol 1. Ablex, Norwood,
pp 127–145
22. Freeth T, Bitsakis Y, Moussas X, Seiradakis JH, Tselikas A, Man-
gou H, Zafeiropoulou M, Hadland R, Bate D, Ramsey A, Allen
M, Crawley A, Hockley P, Malzbender T, Gelb D, Ambrisco W,
Edmunds MG (2006) Decoding the ancient Greek astronom-
ical calculator known as the Antikythera mechanism. Nature
444:587–591
23. Garzon M, Franklin S (1989) Neural computability ii (extended
abstract). In: Proceedings, IJCNN International Joint Confer-
ence on Neural Networks, vol 1. Institute of Electrical and Elec-
tronic Engineers, New York, pp 631–637
24. Garzon M, Franklin S (1990) Computation on graphs. In: Omid-
var OM (ed) Progress in Neural Networks, vol 2. Ablex, Nor-
wood
25. Goldstine HH (1972) The Computer from Pascal to von Neu-
mann. Princeton, Princeton
26. Grossberg S (1967) Nonlinear difference-differential equations
in prediction and learning theory. Proc Nat Acad Sci USA
58(4):1329–1334
27. Grossberg S (1973) Contour enhancement, short term mem-
ory, and constancies in reverberating neural networks. Stud
Appl Math LII:213–257
28. Grossberg S (1976) Adaptive pattern classification and univer-
sal recoding: I. parallel development and coding of neural fea-
ture detectors. Biol Cybern 23:121–134
29. Hartl DL (1994) Genetics, 3rd edn. Jones & Bartlett, Boston
30. Hopfield JJ (1984) Neurons with graded response have collec-
tive computational properties like those of two-state neurons.
Proc Nat Acad Sci USA 81:3088–92
31. Howe RM (1961) Design Fundamentals of Analog Computer
Components. Van Nostrand, Princeton
32. Khatib O (1986) Real-time obstacle avoidance for manipulators
and mobile robots. Int J Robot Res 5:90–99
33. Kirchhoff G (1845) Ueber den Durchgang eines elektrischen
Stromes durch eine Ebene, insbesondere durch eine kreisför-
mige. Ann Phys Chem 140(64)(4):497–514
34. Lang GF (2000) Analog was not a computer trademark! Why
would anyone write about analog computers in year 2000?
Sound Vib 34(8):16–24
35. Lipshitz L, Rubel LA (1987) A differentially algebraic replac-
ment theorem. Proc Am Math Soc 99(2):367–72
36. Maass W, Sontag ED (1999) Analog neural nets with Gaussian
or other common noise distributions cannot recognize arbi-
trary regular languages. Neural Comput 11(3):771–782
37. MacLennan BJ (1987) Technology-independent design of neu-
rocomputers: The universal field computer. In: Caudill M, Butler
C (eds) Proceedings of the IEEE First International Conference
on Neural Networks, vol 3, IEEE Press, pp 39–49

Analog Computation
A
293
38. MacLennan BJ (1990) Field computation: A theoretical frame-
work for massively parallel analog computation, parts I–IV.
Technical Report CS-90-100, Department of Computer Sci-
ence, University of Tennessee, Knoxville. Available from http://
www.cs.utk.edu/~mclennan. Accessed 20 May 2008
39. MacLennan BJ (1991) Gabor representations of spatiotem-
poral
visual
images.
Technical
Report
CS-91-144,
De-
partment of Computer Science, University of Tennessee,
Knoxville. Available from http://www.cs.utk.edu/~mclennan.
Accessed 20 May 2008
40. MacLennan BJ (1994) Continuous computation and the emer-
gence of the discrete. In: Pribram KH (ed) Origins: Brain & Self-
Organization, Lawrence Erlbaum, Hillsdale, pp 121–151.
41. MacLennan BJ (1994) “Words lie in our way”. Minds Mach
4(4):421–437
42. MacLennan BJ (1995) Continuous formal systems: A unifying
model in language and cognition. In: Proceedings of the IEEE
Workshop on Architectures for Semiotic Modeling and Situ-
ation Analysis in Large Complex Systems, Monterey, August
1995. pp 161–172. Also available from http://www.cs.utk.edu/
~mclennan. Accessed 20 May 2008
43. MacLennan BJ (1999) Field computation in natural and artifi-
cial intelligence. Inf Sci 119:73–89
44. MacLennan BJ (2001) Can differential equations compute?
Technical Report UT-CS-01-459, Department of Computer Sci-
ence, University of Tennessee, Knoxville. Available from http://
www.cs.utk.edu/~mclennan. Accessed 20 May 2008
45. MacLennan BJ (2003) Transcending Turing computability.
Minds Mach 13:3–22
46. MacLennan BJ (2004) Natural computation and non-Turing
models of computation. Theor Comput Sci 317:115–145
47. MacLennan BJ (in press) Super-Turing or non-Turing? Extend-
ing the concept of computation. Int J Unconv Comput, in press
48. Maini PK, Othmer HG (eds) (2001) Mathematical Models for Bi-
ological Pattern Formation. Springer, New York
49. Maziarz EA, Greenwood T (1968) Greek Mathematical Philoso-
phy. Frederick Ungar, New York
50. McClelland JL, Rumelhart DE, the PDP Research Group. Paral-
lel Distributed Processing: Explorations in the Microstructure
of Cognition, vol 2: Psychological and Biological Models. MIT
Press, Cambridge
51. Mead C (1987) Silicon models of neural computation. In:
Caudill M, Butler C (eds) Proceedings, IEEE First International
Conference on Neural Networks, vol I. IEEE Press, Piscataway,
pp 91–106
52. Mead C (1989) Analog VLSI and Neural Systems. Addison-
Wesley, Reading
53. Mills JW (1996) The continuous retina: Image processing with
a single-sensor artificial neural field network. In: Proceedings
IEEE Conference on Neural Networks. IEEE Press, Piscataway
54. Mills JW, Himebaugh B, Kopecky B, Parker M, Shue C, Weile-
mann C (2006) “Empty space” computes: The evolution of
an unconventional supercomputer. In: Proceedings of the 3rd
Conference on Computing Frontiers, New York, May 2006.
ACM Press, pp 115–126
55. Moore C (1996) Recursion theory on the reals and continuous-
time computation. Theor Comput Sci 162:23–44
56. Moore GE (1965) Cramming more components onto inte-
grated circuits. Electronics 38(8):114–117
57. Murray JD (1977) Lectures on Nonlinear Differential-Equation
Models in Biology. Oxford, Oxford
58. Omohundro S (1984) Modeling cellular automata with partial
differential equations. Physica D 10:128–34, 1984.
59. Orponen P (1997) A survey of continous-time computation
theory. In: Advances in Algorithms, Languages, and Complex-
ity, Kluwer, Dordrecht, pp 209–224
60. Orponen P, Matamala M (1996) Universal computation by fi-
nite two-dimensional coupled map lattices. In: Proceedings,
Physics and Computation 1996, New England Complex Sys-
tems Institute, Cambridge, pp 243–7
61. Owens L (1986) Vannevar Bush and the differential analyzer:
The text and context of an early computer. Technol Culture
27(1):63–95
62. Peterson GR (1967) Basic Analog Computation. Macmillan,
New York
63. Pour-El MB (1974) Abstract computability and its relation to
the general purpose analog computer (some connections be-
tween logic, differential equations and analog computers).
Trans Am Math Soc 199:1–29
64. Pour-El MB, Richards I (1979) A computable ordinary differen-
tial equation which possesses no computable solution. Ann
Math Log 17:61–90
65. Pour-EL MB, Richards I (1981) The wave equation with com-
putable initial data such that its unique solution is not com-
putable. Adv Math 39:215–239
66. Pour-El MB, Richards I (1982) Noncomputability in models of
physical phenomena. Int J Theor Phys, 21:553–555
67. Puchta S (1996) On the role of mathematics and mathematical
knowledge in the invention of Vannevar Bush’s early analog
computers. IEEE Ann Hist Comput 18(4):49–59
68. Reiner JM (1968) The Organism as an Adaptive Control System.
Prentice-Hall, Englewood Cliffs
69. Rimon E, Koditschek DE (1989) The construction of analytic
diffeomorphisms for exact robot navigation on star worlds.
In: Proceedings of the 1989 IEEE International Conference on
Robotics and Automation, Scottsdale AZ. IEEE Press, New York,
pp 21–26
70. Rogers AE, Connolly TW (1960) Analog Computation in Engi-
neering Design. McGraw-Hill, New York
71. Rubel LA (1985) The brain as an analog computer. J Theor Neu-
robiol 4:73–81
72. Rubel LA (1988) Some mathematical limitations of the general-
purpose analog computer. Adv Appl Math 9:22–34
73. Rubel LA (1993) The extended analog computer. Adv Appl
Math 14:39–50
74. Rumelhart DE, McClelland JL, the PDP Research Group
(1986) Parallel Distributed Processing: Explorations in the
Microstructure of Cognition, vol 1: Foundations. MIT Press,
Cambridge
75. Sanger TD (1996) Probability density estimation for the inter-
pretation of neural population codes. J Neurophysiol 76:2790–
2793
76. Shannon CE (1941) Mathematical theory of the differential an-
alyzer. J Math Phys Mass Inst Technol 20:337–354
77. Shannon CE (1993) Mathematical theory of the differential an-
alyzer. In: Sloane NJA, Wyner AD (eds) Claude Elwood Shan-
non: Collected Papers. IEEE Press, New York, pp 496–513
78. Siegelmann HT (1999) Neural Networks and Analog Computa-
tion: Beyond the Turing Limit. Birkhäuser, Boston
79. Siegelmann HT, Ben-Hur A, Fishman S (1999) Computational
complexity for continuous time dynamics. Phys Rev Lett
83(7):1463–6

294 A
Anisotropic Networks, Elastomers and Gels
80. Siegelmann HT, Sontag ED (1994) Analog computation via
neural networks. Theor Comput Sci 131:331–360
81. Small JS (1993) General-purpose electronic analog computing.
IEEE Ann Hist Comput 15(2):8–18
82. Small JS (2001) The Analogue Alternative: The electronic ana-
logue computer in Britain and the USA, 1930–1975. Routledge,
London & New York
83. Stannett M (19901) X-machines and the halting problem:
Building a super-Turing machine. Form Asp Comput 2:331–
341
84. Thomson W (Lord Kelvin) (1876) Mechanical integration of the
general linear differential equation of any order with variable
coefficients. Proc Royal Soc 24:271–275
85. Thomson W (Lord Kelvin) (1878) Harmonic analyzer. Proc Royal
Soc 27:371–373
86. Thomson W (Lord Kelvin) (1938). The tides. In: The Harvard
Classics, vol 30: Scientific Papers. Collier, New York, pp 274–307
87. Truitt TD, Rogers AE (1960) Basics of Analog Computers. John
F. Rider, New York
88. van Gelder T (1997) Dynamics and cognition. In: Haugeland J
(ed) Mind Design II: Philosophy, Psychology and Artificial Intel-
ligence. MIT Press, Cambridge MA, revised & enlarged edition,
Chap 16, pp 421–450
89. Weyrick RC (1969) Fundamentals of Analog Computers.
Prentice-Hall, Englewood Cliffs
90. Wolpert DH (1991) A computationally universal field computer
which is purely linear. Technical Report LA-UR-91-2937. Los
Alamos National Laboratory, Loa Alamos
91. Wolpert DH, MacLennan BJ (1993) A computationally univer-
sal field computer that is purely linear. Technical Report CS-
93-206. Dept. of Computer Science, University of Tennessee,
Knoxville
Books and Reviews
Fifer S (1961) Analog computation: Theory, techniques and appli-
cations, 4 vols. McGraw-Hill, New York
Bissell CC (2004) A great disappearing act: The electronic analogue
computer. In: IEEE conference on the history of electronics, 28–
30 Bletchley, June 2004
Lipka J (1918) Graphical and mechanical computation. Wiley, New
York
Mead C (1989) Analog VLSI and neural systems. Addison-Wesley,
Reading
Siegelmann HT (1999) Neural networks and analog computation:
Beyond the Turing limit. Birkhäuser, Boston
Small JS (2001) The analogue alternative: The electronic analogue
computer in Britain and the USA, 1930–1975. Routledge, Lon-
don & New York
Small JS (1993) General-purpose electronic analog computing:
1945–1965. IEEE Ann Hist Comput 15(2):8–18
Anisotropic Networks,
Elastomers and Gels
EUGENE M. TERENTJEV
Cavendish Laboratory, University of Cambridge,
Cambridge, UK
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Nematic Elastomers: Reversible Shape Change
Nematic Elastomers: Soft Elasticity and Dynamics
Swelling and Anisotropic Gels
Cholesteric Elastomers: Photonics
New Frontiers
Bibliography
Glossary
Elastomer Nominally equivalent to “rubber”, this deﬁnes
a weakly crosslinked network of polymer chains which
retain high thermal mobility of the strands between
crosslinks. In this case the entropic eﬀects dominate
the response of such networks. Entropic rubber elas-
ticity arises when such a network percolates the whole
macroscopic system and the crosslinks constrain it
such that it remembers its equilibrium shape, and re-
spond elastically to deformations. In this sense, an elas-
tomer is contrasted to a polymer glass: The latter can
refer to a network so densely crosslinked that the in-
dividual chain segments have no signiﬁcant mobility –
or a polymer system below its structural glass transi-
tion. The elastic modulus of elastomers is usually in
the range 10–1000 kPa, the estimate arising from the
thermal energy kT per network strand, while the glass
modulus is usually 0.1–10 GPa, as in most solids.
Gel This word is used in many diﬀerent contexts, in all
cases referring to a soft object that is capable of re-
taining its shape in ambient conditions against, e. g.,
gravity. The latter condition distinguishes a gel from
a “sol”, or a nominal liquid. The softness is a relative
concept; usually it refers to the modulus at or below the
“human” scale, around 1–100 kPa, but there are many
examples of even softer gels. In this article, a gel is con-
trasted to an elastomer, referring to a crosslinked net-
work of polymer chains which is swollen by a good sol-
vent. In this case the eﬀective modulus becomes very
low, nevertheless the system remains elastic as long as
the integrity of percolating network remains intact.
Quenched constraints The term quenched (as opposed to
annealed) refers to objects or systems that are pre-
vented from exploring their full phase or conforma-
tional space during thermal motion. The classical ex-
ample of quenched system is a structural glass, where
the mobility of elements is very limited. In elastomers
and gels, network crosslinks are the quenched objects,
constraining the polymer chains connecting to them.

Anisotropic Networks, Elastomers and Gels
A
295
The eﬀect of randomly quenched constraints is pro-
found in many physical systems, as the local ther-
modynamic equilibrium has to establish among the
mobile (annealed) elements while observing the con-
straints, the random local realization of which is de-
termined by external factors, such as the preparation
history.
Liquid crystal This refers to a group of anisotropic phases
with incomplete translational order (which would rep-
resent a crystalline lattice). Classical examples are: The
nematic phase – a ﬂuid with no translational order
at all, but with an orientational order of anisotropic
molecules, and smectic or lamellar phases, which in
addition to orientational order also have a 1-dimen-
sional density modulation (a stack of parallel layers).
Liquid crystallinity is a state of spontaneous, equilib-
rium anisotropy (breaking of orientational symmetry),
in contrast to anisotropy induced by external factors
such as electric/magnetic ﬁeld or mechanical deforma-
tion.
Shape memory Strictly, any elastic material “remembers”
its equilibrium shape and returns to it after defor-
mation. The term shape-memory was introduced to
distinguish materials that could be made to preserve
their deformed state, until a trigger (e. g. heating above
a transition temperature) induces the return to the
original equilibrium shape. In shape-memory alloys,
this is due to the martensitic transition, in shape-
memory polymers the deformed state can be ﬁxed by
a glass transition or partial crystallization. Liquid crys-
tal elastomers and gels have a reversible, or equilibrium
shape-memory in that the shape of the body is deter-
mined by the current state of order at any given tem-
perature.
Definition of the Subject
Anisotropic (liquid crystalline) elastomers and gels bring
together, as nowhere else, three important ideas: Orienta-
tional order in amorphous soft materials, responsive molec-
ular shape and quenched topological constraints. Acting to-
gether, they create many new physical phenomena that are
brieﬂy reviewed in this article. Classical liquid crystals are
typically ﬂuids of relatively stiﬀrod molecules with long
range orientational order. Long polymer chains, with in-
corporated rigid anisotropic units can also form orienta-
tionally ordered liquid crystalline phases. By contrast with
free rigid rods, these ﬂexible chains change their average
molecular shape, from isotropic spherical to ellipsoidal,
when their component rods align. Linking the polymer
chains together into a network ﬁxes their relative topol-
ogy, and the polymer melt (or solution) becomes an elas-
tic solid – an elastomer (or gel). Radically new properties
arise from the ability to change average molecular shape of
anisotropic polymer chains in the solid state.
In ordinary solids the deformations are created by rel-
ative movement of the same atoms (or molecules) that
form the bonded lattice. Hence, when the deformation is
small, the lattice symmetry is preserved and one obtains
an ordinary elastic response (often anisotropic). There is
a classical elastic response found in glasses as well (either
isotropic or anisotropic), where in place of the crystalline
lattice recording the preferred position of atoms, they are
conﬁned by constraints of local cages. Either way, the ele-
ments of the body “know” their positions and the system
responds with an increase of elastic energy when these ele-
ments are displaced. Large deformations destroy the lattice
(or cage) integrity and simply break the material.
In contrast, in elastomers and gels the macroscopic
elastic response arises from the entropy change of chains
on relative movement of their crosslinked end-points,
which are relatively far apart. Monomers remain highly
mobile and thus liquid-like. What happens to chain seg-
ments (monomer moieties) on a smaller length scale
is a relatively independent matter and such weakly
crosslinked network behaves as a liquid on length scales
smaller than the end-point separation of strands. In partic-
ular, the liquid crystalline order can be established within
these segments. The magnitude of this order can be al-
tered and its director can rotate, in principle, indepen-
dently of deformation of the crosslinking points. Such an
internal degree of freedom within, and coupled to the elas-
tic body provides additional local forces and torques, in-
tricately connected in the overall macroscopic response of
the body.
In the simplest and most straightforward case of ne-
matic elastomers and gels, the uniaxial ellipsoidal aver-
age anisotropy of chain constituting the network leads to
two key physical properties that make these materials so
unique. If one can change the level of nematic (orien-
tational) order, which is not diﬃcult to achieve by e. g.
changing temperature, changes at the molecular level in-
duce a corresponding mechanical strains: A block of rub-
ber can contract or elongate by a large amount on heat-
ing or cooling, respectively. This process is perfectly re-
versible. This leads to a group of possible applications in
artiﬁcial muscles and actuators, which can be driven by
any stimulus that aﬀects the local nematic order: tempera-
ture, solvent intake, irradiation, etc.
It is also possible to rotate the nematic director axis
and the rubber matrix independently, although in con-
trast to ordinary liquid crystals it costs energy to uni-

296 A
Anisotropic Networks, Elastomers and Gels
formly rotate the director within the matrix. This penalty
leads to suppression of orientational ﬂuctuations and high
optical clarity of birefringent elastomers and gels. Local
rotations also yield a subtle and spectacular elastic phe-
nomenon which is called ‘soft elasticity’. Contrary to in-
tuition, there is an inﬁnite number of non-trivial mechan-
ical deformations that can accommodate the rotation of
anisotropic distribution of chains without its distortion.
As a result, the entropy of the chains does not change, in
spite of macroscopic deformations, and the material can
be distorted without any signiﬁcant energy cost! A spe-
cial combination of shears and elongations/compressions
is required, but it turns out not very diﬃcult to achieve.
Elastic softness, or attempts by the material to achieve it,
pervade much of the elasticity of nematic elastomers and
gels. For instance, another unique and spectacular prop-
erty of these systems is anomalously high damping, where
the mechanical energy is dissipated on these soft rotational
deformation modes.
Cholesteric liquid crystals have a helical director tex-
ture. When crosslinked into elastomers or gels, this peri-
odic helical anisotropy is made to couple to the mechanical
state of the whole body. Their optical and mechanical re-
sponses to imposed stress are exceedingly rich as a result.
Cholesteric elastomers are brightly colored due to selective
reﬂection, and change color as they are stretched – their
photonic band structure can be powerfully manipulated
by applied deformations. Such periodic (distributed feed-
back) photonic systems can emit laser radiation, the color
of which can be continuously shifted by mechanical means
Anisotropic Networks, Elastomers and Gels, Figure 1
Arrangement of anisotropic (rod-like) molecular moieties in common liquid crystal phases: a nematic, b cholesteric, c smectic-A and
d smectic-C. The average orientation of long axes is the director n. The director, as well as the cholesteric helix axis ˆp and the smectic
layer normal ˆk, are all directionless “quadrupolar” vectors, e. g. n D n
across the whole visible spectrum. Further, the eﬀect of
topological imprinting can select and extract molecules of
speciﬁc handedness from a mixed solvent. Cholesteric gels
can act as a mechanical separator of chirality.
Smectic or lamellar elastomers and gels have plane-
like, layered modulation of density in one direction
(SmA), or additionally a tilt of the director away from
the layer normal (SmC). Many other more complex smec-
tic phases exist and could also be made into elastomers.
Here the layers are often constrained by crosslinking den-
sity modulation not to move relative to the rubber matrix.
Deformations along the layer normal can be resisted by
a modulus up to 100 times greater than the normal rubber
(shear) modulus of the matrix. Thus smectic elastomers
are rubbery in the two dimensions of their layer planes,
but respond as hard conventional solids in their third di-
mension. Such extreme mechanical anisotropy promises
interesting applications.
The director tilt associated with the transition from
SmA to SmC induces distortion in the polymer chain
shape distribution. Since chain shape is coupled to me-
chanical shape for an elastomer with quite low symme-
try, one expects a large variety of independent soft-elas-
ticity modes. The tilted SmC systems also exist in chiral
forms which must on symmetry grounds be ferroelectric,
with spontaneous polarization pointing along the vector
product [ˆk  n], see Fig. 1d. Ferroelectric elastomer is very
special: Mechanically it is soft, about 104 times lower in
modulus than conventional solid ferro- and piezoelectrics.
Distortions give polarization changes comparable to those

Anisotropic Networks, Elastomers and Gels
A
297
in ordinary ferroelectrics, but the response to an applied
stress must necessarily much larger than in conventional
materials due to the very low mechanical impedance.
It is debatable, whether an anisotropic glass could be
called a liquid crystal, although nominally a glass can be
made to possess orientational but not a translational or-
der. The lack of molecular mobility makes the question of
thermodynamic equilibrium very ambiguous.
Introduction
Within the simplest aﬃne deformation approach one re-
gards the change in each chain end-to-end vector R as
R0 D F  R, when a deformation of the overall polymer
network is characterized by a deformation gradient ten-
sor Fij. In continuum elasticity this tensor is often called
F D r f , a gradient of the geometrical mapping f from
the reference to the current state of deformation. Assum-
ing the chain connecting the two crosslinks is long enough,
the Gaussian approximation for the number of its confor-
mations Z(R) gives for the free energy (per chain): Wch D
kT ln Z(R0) ' (kT
ı
a2N)[FT  F]i jRiRj, where a is the
step length of the chain random walk and N the number of
such steps. In order to ﬁnd the total elastic work function
of all chains aﬃnely deforming in the network, one needs
to add the contributions Wch(R) with the statistical weight
to ﬁnd a chain with a given initial end-to-end distance
R in the system. This procedure, called the quenched av-
eraging, produces the average hRiRji ' 1
3 a2Nıi j in Wch.
The resulting rubber-elastic free energy (per unit volume)
is Wel D 1
2 nckT(FT : F), with nc a number of chains per
unit volume of the network. This is a remarkably robust
expression, with many seemingly relevant eﬀects, such as
the ﬂuctuation of crosslinking points, only contributing
a small quantitative change in the prefactor. It is, how-
ever, incomplete since the simpliﬁed Gaussian statistics of
chains does not take into account their physical volume
and thus does not address the compressibility issue. The
proper way of dealing with it is by adding an additional in-
dependent penalty for the volume change: 1
2 K(det F  1)2
in elastomers, with K the typical bulk modulus of the order
10 GPa in organic materials. This should be modiﬁed into
1
2 K(det F  ˚)2 for gels swollen by a volume fraction ˚ of
solvent.
If one assumes that the rubber elastic modulus is low,
then a very reasonable approximation is to simply im-
pose the volume-conservation constraint on deformation
tensor: det F D 1. The value of the rubber modulus is
found on expanding the deformation gradient tensor in
small strains, say for a small extension, Fzz D 1 C ",
and obtaining Wel '
3
2 nckT"2. This means the exten-
sion (Young) modulus E D 3nckT; the analogous con-
struction for a small simple shear will give the shear mod-
ulus G D nckT, exactly a third of the Young modulus as
required in an incompressible medium. This shear modu-
lus G, having its origin in the entropic eﬀect of reduction of
conformational freedom on polymer chain deformation,
is usually so much smaller than the bulk modulus (de-
termined by the enthalpy of compressing the dense poly-
mer liquid) that the assumption of rubber or gel deform-
ing at constant volume is justiﬁed. This constraint leads to
the familiar rubber-elastic expression Wel D 1
2 nckT(2 C
2ı) where one has assumed that the imposed extension
Fzz D  is accompanied by the symmetric contraction in
both transverse directions, Fxx D Fyy D 1
ıp
 due to the
incompressibility.
When the chains forming the rubbery network are liq-
uid crystalline, their end-to-end distance distribution be-
comes anisotropic. The case of smectic/lamellar order-
ing is much more complicated. For a simple uniaxial ne-
matic one obtains hRkRki D 1
3`kL and hR?R?i D 1
3`?L,
with L D aN the chain contour length and `k
ı`? the ra-
tio of average chain step lengths along and perpendicu-
lar to the nematic director. In the isotropic phase one re-
covers `k D `? D a. The uniaxial anisotropy of polymer
chains has a principal axis along the nematic director n,
with a prolate (`k
ı`? > 1) or oblate (`k
ı`? < 1) ellip-
soidal conformation of polymer backbone. The ability of
this principal axis to rotate independently under the inﬂu-
ence of network strains makes the rubber elastic response
non-symmetric, so that the elastic work function is [6]
Wel D 1
2GTr

FT  `1  F  `0

C 1
2 K(det F  1)2 ; (1)
with ` the uniaxial matrices of chain step-lengths be-
fore (`0) and after the deformation to the current state:
`i j D `?ıi j C [`k  `?]ninj. Note that the deformation
gradient tensor F does no longer enter the elastic energy
in the symmetric combination FT  F, but is “sandwiched”
between the matrices ` with diﬀerent principal axes. This
means that antisymmetric components of strain will now
have a non-trivial physical eﬀect, in contrast to isotropic
rubbers and, more crucially, to elastic solids with uniax-
ial anisotropy. There, the anisotropy axis is immobile and
the response is anisotropic but symmetric in stress and
strain. The uniqueness of nematic rubbers stems from the
competing microscopic interactions and the diﬀerence in
characteristic length scales: The uniaxial anisotropy is es-
tablished on a small (monomer) scale of nematic coher-
ence length, while the strains are deﬁned (and the elastic
response is arising) on a much greater length scale of poly-
mer chain end-to-end distance, see Fig. 2, [67,68].

298 A
Anisotropic Networks, Elastomers and Gels
Anisotropic Networks, Elastomers and Gels, Figure 2
Relation between the equilibrium chain shape and deformations in nematic elastomers. When the network of initially isotropic
chains, each o average forming a spherical shape of gyration a, is brought into the nematic phase, a corresponding spontaneous
deformation of the sample occurs in proportion to the backbone anisotropy, m D (`k
ı
`?)1/3, b. An example of soft deformation
is given in c, when rotating anisotropic chains can affinely accommodate all strains (a combination of compression along the initial
director, extension across it and a shear in the plane of director rotation), not causing any entropic rubber-elastic response
Nematic Elastomers: Reversible Shape Change
The “Trace formula” (1) has proven very successful in de-
scribing many physical properties of nematic and smec-
tic elastomers and gels. One of the most important con-
sequences of coupling the nematic order to the shape of
an elastic body (and perhaps the most relevant for ap-
plications) is the eﬀect of spontaneous uniaxial exten-
sion/contraction. It is a very simple phenomenon, picto-
rially illustrated in transformation between states (a) and
(b) of Fig. 2. Mathematically, it is straightforward to ob-
tain from (1) for the ﬁxed director orientation nz D 1 and
det[F] D 1, that
Wel D 1
2G
0
@2 `(0)
k
`k
C 2

`(0)
?
`?
1
A ;
(2)
minimizing which one obtains the equilibrium uniaxial
extension  along the nematic director. In the case when
the reference state `0 is isotropic, `(0)
k
D `(0)
? D a, this
spontaneous extension along the director takes the espe-
cially simple form [1,69]:
m D (`k/`?)1/3 :
(3)
The particular way `k and `? depend on the value of ne-
matic order parameter Q depends on the molecular struc-
ture of the system. For weak chain anisotropy it is cer-
tainly linear: `k
ı
`? D 1 C ˛Q, however, there are situa-
tions (e. g. in highly anisotropic main-chain nematic poly-
mers) when the ratio `k/`? / exp[1/(1  Q)] takes very
high values. The key relation (3) lies at the heart of ne-
matic elastomers performing as thermal actuators or ar-
tiﬁcial muscles, Fig. 3, [34,54,60]. The analogy is further
enhanced by the fact that the typical stress exerted on such
actuation is in the range of 10–100 kPa, the same as in hu-
man muscle [54]. A large amount of work has been re-
cently reporting on photo-actuation of nematic elastomers
containing e. g. azobenzene moieties, Fig. 4, or on the ef-
fect of adding/removing solvents [22,26,37] – in all cases
the eﬀect is due to the changing of underlying nematic
order parameter Q, aﬀecting the magnitude of the ratio
(`k
ı`?) and thus the length of elastomer sample or the
exerted force if the length is constrained [19]. Finally, cre-
ating an inhomogeneous internal deformation results in
the bending actuation response, Fig. 5, [9,53,71].
However useful, the concise expression (1) does arise
from a highly idealized molecular model based on the
simple Gaussian statistics of polymer chains between
crosslinks and, as such, may seem oversimpliﬁed. It turns
out that the symmetry features, correctly captured in the
Trace formula, are more important than various cor-
rections and generalizations of the model. In particular,
a much more complex theory of nematic rubber elasticity
taking into account chain entanglements (which have to
play a very important role in crosslinked networks, where
the constraint release by diﬀusion is prohibited), devel-

Anisotropic Networks, Elastomers and Gels
A
299
Anisotropic Networks, Elastomers and Gels, Figure 3
Thermal actuation of nematic elastomers. Images a show a strip
of rubber with its director aligned along its length, reversibly
changing its length on heating and cooling. The same underly-
ing mechanism of changing the orientational order parameter
Q(T) translates itself into the mechanical effect differently, de-
pending on the molecular structure of the polymer. Plot b shows
the relative change in sample length (the strain m) for differ-
ent materials, illustrating the range of motion and the different
point of transition into the isotropic phase, where Q D 0 and
L D L0
oped in [33], shows the same key symmetries:
Wel D 2
3G 2M C 1
3M C 1Tr

FT  `1  F  `0

C 3
2G(M  1)2M C 1
3M C 1
ˇˇ`1/2  F  `1/2
0
ˇˇ2
C G(M  1)ln
ˇˇ`1/2  F  `1/2
0
ˇˇ ;
(4)
where M is a number of entanglements on a typical chain
of length N (M D 1 in the ideal Trace formula), and the
Anisotropic Networks, Elastomers and Gels, Figure 4
Photo-induced actuation of nematic elastomers. Plot a shows
the contraction of a strip of azobenzene-containing nematic rub-
ber on UV-irradiation, returning back to its equilibrium value
when the light is turned off. Plot b shows the exerted stress
under illumination, when the sample is mechanically held. The
time-scales of photo-actuation can vary greatly depending on
the molecular structure of chromophores
notation j : : : j denotes an orientational average of the ma-
trix ... applied to an arbitrary tube segment. The impor-
tant feature of the complicated expression (4) is the “sand-
wiching” of the deformation gradient tensor F between
the two orientational tensors deﬁned in the current and
the reference states, respectively. The continuum model of
fully non-linear elasticity is also quite complicated in its
applications, but the fundamental requirements that any
elastic free energy expression must be (i) invariant with re-
spect to body rotations of the reference state and the cur-
rent state, separately, and (ii) reduce to the classical ex-
pression Tr(FT  F) when the underlying chains become
isotropic, requires it to have the form
Wel / TrFT  [`]m  F  [`0]m ;
(5)
where the power is only allowed to be m D 1; 2; 3. Once
again, one recognizes the key symmetry of separating the
two powers of deformation gradient tensor by the orien-

300 A
Anisotropic Networks, Elastomers and Gels
Anisotropic Networks, Elastomers and Gels, Figure 5
Bending of nematic elastomer cantilevers. Image collage a
shows the effect of heat (IR radiation) applied from one side of
the rubber strip, resulting in the near side contracting more than
the far side. Image b shows the similar effect on photo-elastomer
illuminated from one side
tation tensors deﬁned in the current and reference frames,
respectively.
Nematic Elastomers: Soft Elasticity and Dynamics
Two essential consequences of the coupling between the
elastic modes of polymer network, described by the gen-
eral strain tensor F, and the rotational modes of the
nematic director, which are the principal axes of chain
anisotropy tensor `, are the reduction of the eﬀective elas-
tic response and the penalty on the director ﬂuctuations.
The ﬁrst eﬀect has received the name “soft elasticity” and
is the result of the special symmetry of the coupling be-
tween the orientational modes and deformation gradients.
It is easy to see [44] that there is a whole continuous class
of deformations described by the form
Fsoft D `1/2

 V  `1/2
0
;
(6)
with V an arbitrary unitary matrix, which leave the elas-
tic free energy Wel at its constant minimum value (for
an incompressible system). Remarkably, this remains true
whether one examines the Trace formula (1), or any other
expression above, (4) or (5), as long as they respect the cor-
rect symmetries of the two independent degrees of free-
dom [24,50]. Figure 2c illustrates one example of such soft
deformation.
This phenomenon is unique to anisotropic elastomers
and gels, which have an internal microstructure (liquid
crystalline order) that is capable of continuously chang-
ing its tensor structure. It can also be seen in smectic elas-
tomers, although the layer constraints make it more com-
plicated mathematically. Possible the closest elastic system
is the famous shape-memory alloy [5], where the marten-
site transition creates a crystalline unit cell with several
(usually just two) equal-energy states. The ability of the lat-
tice to switch between these states in response to certain
deformation modes gives the “soft deformation” modes
to these alloys, albeit only a few discrete ones. In elas-
tomers and gels the “soft deformation” reﬂects the ability
of anisotropic polymer chains to rotate their long axis to
accommodate some imposed elastic deformations without
changing their shape.
If one instead chooses to focus on the director modes
in a nematic elastomer with a ﬁxed (constrained) shape,
the coupling between the internal mobile order parameter
and the network elasticity provide a large energy penalty
for uniform director rotations ın (with respect to the
elastically constrained network). This penalty, which ap-
pears as a large mass term in the expression for mean-
square director ﬂuctuation, results in the suppression of
such ﬂuctuations and the related scattering of light from
a nematic elastomer [48]. In contrast to optically tur-
bid ordinary liquid nematics, where light is scattered by
long-wavelength director ﬂuctuations, the aligned mon-
odomain anisotropic elastomer or gel is totally transpar-
ent. However, when the elastic deformations in the net-
work are not constrained and are free to relax, there are
certain combinations of polarization and wave vectors of
director ﬂuctuations (corresponding to the soft deforma-
tion modes), for which the “eﬀective mass” vanishes and
the ﬂuctuation spectrum should appear as in ordinary liq-
uid nematics [44].
It is natural to expect that if a sample of monodomain,
uniformly aligned nematic elastomer (which usually im-
plies that it has been crosslinked in the aligned nematic
phase [34,35]) is stretched along the axis perpendicular
to the nematic director n0, the director will switch and
point along the axis of uniaxial extension. The early the-
ory (ignoring the eﬀect of soft elasticity) [6] has predicted,
and the experiment on polyacrylate LCE [43] conﬁrmed

Anisotropic Networks, Elastomers and Gels
A
301
Anisotropic Networks, Elastomers and Gels, Figure 6
a The scheme of stripe domains in nematic elastomers (inset is their image from a polarizing microscope). b The stress-strain curve
during the stripe domain regime: After a threshold strain, the soft plateau extends over the whole region where the internal director
rotation takes place ( 2 0  90o). c The effect of anomalous vibration damping, demonstrated by the consistently high loss factor
across the whole range of nematic phase; the arrow points at the onset of the isotropic phase where the high damping disappears
that this switching may occur in an abrupt discontinuous
fashion when the natural long dimension of anisotropic
polymer chains can ﬁt into the new shape of the sam-
ple, much extended in the perpendicular direction. How-
ever the same experiment performed on a polysiloxane
LCE [31] has shown an unexpected stripe domain pattern.
Further investigation has proven that the nematic director
rotates continuously from n0 towards the new perpendic-
ular axis, over a substantial range of deformations, but the
direction of this rotation alternates in semi-regular stripes
of several microns width oriented along the stretching di-
rection, Fig. 6a. Later the same textures have been ob-
served by other groups and on diﬀerent materials, includ-
ing polyacrylates [32,73], although there also have been re-
ports conﬁrming the single director switching mode [46].
Theoretical description of stripe domains [66] has
been very successful and has led to several long-reach-
ing consequences. First of all, this phenomenon has be-
come a conclusive proof of the soft elasticity principles. As
such, the nematic elastomers have been recognized as part
of a greater family of elastic systems with microstructure
(shape memory alloys being the most famous member of
this family to date), all exhibiting similar phenomena. Fi-
nally, the need to understand the threshold value of exten-
sion c has led to deeper understanding of internal micro-
scopic constraints in LCE and resulted in introduction of
the concept of semi-softness: A class of deformations that
has the “soft” symmetry, as in Eq. (6), penalized by a small
but physically distinct elastic energy due to such random
constraints.
The main result of theoretical model [66] gives the di-
rector angle variation with strain,
() D ˙ arcsin

`k
`k  `?

1  2
c
2
1/2
(7)
with only one free parameter, the threshold strain c. In
fact, if one uses the pure Trace formula (1), the threshold is
not present at all, c D 1. The backbone chain anisotropy
`k/`?, which enters the theory, is an independent exper-
imentally accessible quantity related, e. g. to the sponta-
neous shape change of LCE on heating it into the isotropic
phase, m  (`k
ı`?)1/3 in Fig. 2. This allowed the data for
director angle to be collapsed onto the same master curve,
spanning the whole range of non-linear deformations.
The physical reason for the stretched LCE to break into
stripe domains with the opposite director rotation ˙()
becomes clear when one recalls the idea of soft elastic-
ity [44,67]. The polymer chains forming the network are
anisotropic, in most cases having the average shape of uni-
axial prolate ellipsoid, see Fig. 2. If the assumption of aﬃne
deformation is made, the strain applied to the whole sam-
ple is locally applied to all network strands. The origin of
(entropic) rubber elasticity is the corresponding change of
shape of the chains, away from their equilibrium shape
frozen at network formation, which results in the reduc-
tion in entropy and rise in the elastic free energy. How-
ever, the nematic (e. g. prolate anisotropic) chains may
ﬁnd another way of accommodating the deformation: If
the sample is stretched perpendicular to the director n (the

302 A
Anisotropic Networks, Elastomers and Gels
long axis of chain gyration volume), the chains may ro-
tate their undeformed ellipsoidal shapes – thus providing
an extension, but necessarily in combination with simple
shear – and keep their entropy constant and elastic free en-
ergy zero! This, of course, is unique to nematic elastomers:
Isotropic chains (with spherical shape) have to deform to
accommodate any deformation. It is also important that
with deformations costing no elastic energy, there is no
need for the material to change its preferred nematic order
parameter Q, so the ratio `k
ı`? remains constant for the
duration of soft deformations (this is not the case when,
e. g., one stretched the elastomer along the director axis).
The physical explanation of stripe domains is now clear:
the stretched nematic network attempts to follow the soft
deformation route to reduce its elastic energy, but this re-
quires a global shear deformation which is prohibited by
rigid clamps on two opposite ends of the sample, Fig. 6a.
The resolution of this macroscopic constraint is to break
into small parallel stripes, each with a completely soft de-
formation (and a corresponding shear) but with the sign
of director rotation (and thus – the shear) alternating be-
tween the stripes. Then there is no global shear and the
system can lower its elastic energy in the bulk, although it
now has to pay the penalty for domain walls and for the
non-uniform region of deformation near the clamps. The
balance of gains and losses determines the domain size d.
The argument above seems to provide a reason for
the threshold strain c, which is necessary to overcome
the barrier for creating domain walls between the “soft”
stripes. However, it turns out that the numbers do not
match. The energy of domain walls must be determined by
the gradient Frank elasticity of the nematic (with the Frank
elastic constant independently measured, K  1011 N)
and thus should be very small, since the characteristic
length scale of nematic rubbers is  D
p
K/G  109 m.
Hence the threshold provided by domain walls alone
would be vanishingly small whereas most experiments
have reported c  1:1 or more. This mystery has led to
a whole new concept of what is now called semi-softness
of LCE. The idea is that, due to several diﬀerent micro-
scopic mechanisms [65], a small addition to the classical
nematic rubber-elastic free energy is breaking the symme-
try required for the soft deformations:
Wel  1
2G
h
Tr

FT  `1

 F  `0

C ˛ (ın C n  F  ın)2 i
(8)
(usually ˛ 
 1). The soft-elastic pathways are still repre-
senting the low-energy deformations, but the small penalty
 ˛G provides the threshold for stripe domain formation
(c  1 C ˛) and also makes the slope of the stress-strain
soft-elastic plateau small but nonzero, Fig. 6b. Compare
this with the ordinary extension modulus of rubber before
and after the region of stripe domains. Regardless of small
complications of semi-soft corrections, the main advance
in identifying the whole class of special low-energy soft de-
formations in LCE and proving their existence by direct
experiment is worth noting.
The presence of the underlying nematic order, with
its director capable of independently moving with respect
to the matrix [16], leads to another remarkable feature
of anisotropic elastomers and gels: the anomalous dissi-
pation of mechanical energy, see Fig. 6c. This eﬀect is
represented in very high values of the so-called loss fac-
tor tan ı D G00ıG0, representing the ratio of the imagi-
nary and real parts of the linear modulus of response to
the oscillating shear. tan ı > 1 in a fully elastic material is
a very unusual situation, no doubt to be explored in appli-
cations in vibration and impact damping [15]. Another as-
pect of high internal dissipation due to the rotating direc-
tor modes is the very high amplitude and very long times
of stress relaxation of stress in all liquid crystalline elas-
tomers and gels [14].
Swelling and Anisotropic Gels
Polymer gels are usually isotropic and swell and shrink
equally in all directions. Volume phase transition phe-
nomena of polymer gels have been extensively investigated
and it is known that an inﬁnitesimal change in an environ-
mental intensive variable such as solvent composition, pH
and temperature yields a discontinuous volume change for
some polymer gels [4,27,55]. These volume transitions are
driven by the balance between the repulsive and attractive
forces acting on the network chains such as van der Waals,
hydrophobic, ionic, hydrogen bonding [49].
If gels of anisotropic molecular structure were pre-
pared, anisotropic mechanical behavior will be expected.
Additional volume transition can be triggered by nematic
ordering of anisotropic molecules inside the gel [62]. This
eﬀect adds a new driving molecular force for volume tran-
sition phenomena. Temperature-sensitive gels exhibiting
volume transition at a certain temperature have attracted
much attention of scientist and technologists because of
their applications to drug delivery systems and sensors,
originally concentrating on N-isopropylacrylamide-based
isotropic systems. Spontaneously anisotropic gels under-
going a sharp and large volume change accompanied by
nematic-isotropic transition is a new type of such temper-
ature-sensitive gels, which will extend the potential appli-
cations.

Anisotropic Networks, Elastomers and Gels
A
303
Anisotropic Networks, Elastomers and Gels, Figure 7
Volume transitions and anisotropic swelling of nematic gels. Plot
a shows the generic effect of equilibrium swelling by isotropic
solvent, at different temperatures (with TGel
NI marking the point
of nematic-isotropic transition). The anisotropy of swelling is
indicated by the growing difference between the expanding
length along the nematic director (a factor k) and perpendic-
ular to the director (?), with Q D k2
?. Plot b indicates the
transitions when the network is swollen by the liquid crystalline
solvent, which has its own transition temperature TSol
NI
Monodomain nematic networks in the dry state show
a large spontaneous elongation parallel to the director axis
upon nematic ordering, as discussed in earlier sections.
This shape change in dry anisotropic elastomers occurs
without appreciable volume change. Large anisotropic vol-
ume changes occur in such networks since the addition
of isotropic solvent also has the eﬀect of reducing the ne-
matic order, but also remove the incompressibility con-
straint [29,61,72]. Upon cooling, the swollen isotropic gel
continuously changes into the shrunken monodomain ne-
matic gel around the transition point TGel
NI , see Fig. 7a.
Importantly, the volume decrease upon nematic order-
ing is accompanied by the shape change of a gel with
a considerable degree of anisotropy, as indicated on the
generic plot. The driving force behind the abrupt and
pronounced shrinking of the nematic gel is the ther-
modynamic force (eﬀective chemical potential) that tries
to expel the “wrong” solvent and allow the network to
lower its free energy of the nematic phase. Liquid crys-
talline networks swollen in a liquid crystalline solvent is
an even richer system, expected to act as a soft actua-
tor driven under electric ﬁeld utilizing the strong dielec-
tric anisotropy of mesogenic units controllable by external
ﬁeld [30]. Figure 7b shows a generic plot of the swelling ra-
tio Q D V
ı
V0. This plot indicates that the large shrinking
occurs in the window of temperatures when the solvent is
still isotropic. However, the solvent is taken into the gel
once again when it becomes nematic itself, since this now
promotes the order in the network and lowers the joint
free energy [41,62].
In stark contrast with ordinary liquid crystals, dry
anisotropic elastomers do not show any signiﬁcant re-
sponse to an applied electric or magnetic ﬁeld. The reason
is that the local elastic resistance (measured by the elastic
energy density, essentially the modulus G  1046 Pa) is
much greater than the energy density associated with lo-
cal dielectric or diamagnetic anisotropy of the material.
Essentially, the ﬁelds are too weak to rotate the director
pinned to the elastic matrix. However, in highly swollen
gels the elastic modulus is much lower and one expects
(and indeed ﬁnds) an electro-optical and electro-mechani-
cal response [10,59,63]. An additional interesting possibil-
ity is when a gel sample is mechanically unconstrained and
therefore capable of ﬁnding a soft-deformation route in re-
sponse to a dielectric torque applied to its local director.
In this case the only resistance to dielectric torque would
come from the weak semi-softness. Figure 8 shows what
happens when a monodomain nematic gel, freely ﬂoating
in a solvent, is subjected to an electric ﬁeld perpendicular
to its director [63]. The director rotates, and the shape of
the gel changes accordingly, see Fig. 2c, contracting in the
parallel direction but remaining constant in the perpen-
dicular direction (? D 1) as required by the soft-defor-
mation geometry.

304 A
Anisotropic Networks, Elastomers and Gels
Anisotropic Networks, Elastomers and Gels, Figure 8
Response of an unconstrained nematic gel to an electric field. Image a shows the gel with the director n0 aligned in the plane,
viewed from above between crossed polars. Electric field is applied perpendicular to the image plane and the director rotates out of
the plane, so that image b shows extinction between crossed polars. Note that the sample width remains constant. The associated
contraction of the gel along the original director, k is plotted against applied field for several crosslinking density values. The
arrows on the plot indicate the semi-soft threshold for the director rotation onset, shifting with crosslinking density. (The data by K.
Urayama)
Cholesteric Elastomers: Photonics
When chiral molecular moieties (or dopant) are is
added to nematic polymers, they form cholesteric phases.
Crosslinked networks made from such polymers accord-
ingly have a spontaneously twisted director distribution.
Their director distribution being periodic with character-
istic length scale in the optical range, they display their in-
homogeneous modulation in brilliant colors of selective
reﬂection. Being macroscopically non-centrosymmetric,
such elastic materials possess correspondingly rare phys-
ical properties, such as the piezoelectricity.
As with nematic LCE, in order to access many unique
properties of cholesteric elastomers one needs to form
well-aligned monodomain samples, which would remain
ordered due to crosslinking. As with nematic systems,
the only way to achieve this is to introduce the network
crosslinks when the desired phase is established by either
external ﬁelds or boundary conditions – otherwise a poly-
domain elastomer would result. As with nematics, there
are certain practical uses of cholesteric polydomain state –
for instance, the applications of stereo-selective swelling
(see below) may work even better in a system with micron-
size cholesteric domains [17].
There are two established ways to create monodomain
cholesteric elastomer ﬁlms with the director in the ﬁlm
plane and the helical axis uniformly aligned perpendicu-
lar to the ﬁlm. Each has its advantages, but also unavoid-
able drawbacks. Traditionally, a technique of photopoly-
merization (and network crosslinking when di-functional
molecules are used, such as di-acrylates) was used in cells
where the desired cholesteric alignment was achieved and
maintained by proper boundary conditions. Many elabo-
rate and exciting applications have been developed using
this approach, e. g. [8,25]. However, it is well-known that
the inﬂuence of surface anchoring only extends a maxi-
mum of few microns into the bulk of a liquid crystal (un-
less a main-chain mesogenic polymer, with dramatically
increased planar anchoring strength is employed [57]).
Therefore, only very thin cholesteric LCE ﬁlms can be pre-
pared by this surface-alignment and photopolymerization
technique.
A diﬀerent approach to preparing monodomain
cholesteric elastomers has been developed by Kim and
Finkelmann [28], utilizing the two-step crosslinking prin-
ciple originally used in nematic LCE. One prepares a sam-
ple of cholesteric polymer gel, crosslinked only partially,
and the deposits it on a substrate to de-swell. Since the
area in contact with the substrate remains constrained,
the sample cannot change its two lateral dimensions in
the plane to accommodate the loss of volume; only the
thickness can change. These constraints in eﬀect apply
a large uniaxial compression, equivalent to a biaxial ex-
tension in the ﬂat plane, which serves to align the direc-
tor in the plane and the helical axis perpendicular to it.
After this is achieved, the full crosslinking of the network
is initiated and the well-aligned free-standing cholesteric
elastomer results. This is a delicate technique, which re-
quires a careful balance of crosslinking/deswelling rates,
solvent content and temperature of the phases. In princi-

Anisotropic Networks, Elastomers and Gels
A
305
Anisotropic Networks, Elastomers and Gels, Figure 9
Selective reflection of light by cholesteric elastomers shifts under deformation. The top row of plots shows the results for a biaxial
extension in the plane, leading to uniaxial compression along the helix from 1 to 0.65, separating the transmission of right- and left-
handed circular polarized light. The bottom row of plots shows the analogous results for a uniaxial extension in the plane, from 1 to
1.6. The key difference to note is the emergence of the matching selective reflection for the opposite chirality of light in the uniaxial
extension case [13]
ple it has no signiﬁcant restrictions on the resulting elas-
tomer area or thickness. However, there is a limitation of
a diﬀerent nature: one cannot achieve too long cholesteric
pitch, e. g. in the visible red or infra-red range. The rea-
son is the required high strength of chiral twisting at the
stage of biaxial extension on deswelling: With no or weak
twisting strength the material is close to the ordinary ne-
matic and although the symmetric biaxial extension does
conﬁne the director to its plane, it does nothing to counter
the quenched disorder of the establishing crosslinks and
the resulting propensity to form a 2-dimensional (planar)
polydomain state. Only at suﬃciently high chiral twist-
ing power the cholesteric propensity overcomes this pla-
nar disorder – and the resulting cholesteric pitch will end
up relatively short. Another unavoidable drawback of this
2-step crosslinking technique is the disordering eﬀect of
the portion of crosslinks established at the ﬁrst stage of gel
formation (one has to have them to sustain elastic stress
and prevent ﬂow on deswelling), so the optical quality of
samples prepared in this way is always worse than that in
thin ﬁlms photo-polymerized in a perfect surface-aligned
cholesteric state.
Cholesteric elastomers and gels respond to imposed
mechanical deformations in diﬀerent ways. Bending of
ﬂexible free-standing cholesteric LCE has been used to
great eﬀect [40]. The more mechanically straightforward
idea is to compress the elastomer along the helical axis,
which is easiest to achieve by imposing a symmetric bi-
axial extension in the director plane [21]. This leads to
the aﬃne contraction of the helical pitch and the corre-
sponding dramatic shift of the selective reﬂection band,
see Fig. 9. If a more traditional uniaxial stretching of elas-
tomer ﬁls is applied, with the initial helical pitch perpen-
dicular to the direction of stretching, the modiﬁed di-
rector distribution is usually no longer simply helical al-
though the characteristic length scales also aﬃnely scale
with sample dimensions. On deformation the director
texture remains periodic along the former helical pitch,
but becomes a non-chiral step-wise modulation [12,70],
leading to new photonic bandgaps and selective reﬂec-
tion in both right- and left-handed circular polarized light,
Fig. 9. Laser emission has been predicted and observed in
cholesteric systems when light was excited in their bulk,
e. g. by a photo-dye stimulated by a pumping laser emit-

306 A
Anisotropic Networks, Elastomers and Gels
Anisotropic Networks, Elastomers and Gels, Figure 10
Piezoelectric effects in chiral elastomers. a Polarization induced by the shear applied to helically twisted textures (flexoelectric ef-
fect); b Polarization induced by shearing a uniformly aligned chiral nematic, with the director either along x- or y-axes (the true
piezoelectric effect P D [n  ("  n)]). The sketch illustrates the chiral geometry that produces a polarization vector along z due to
the shear and anisotropy axis lying in the x-y plane
ting near the bandgap edge. The ability to continuously
change the position of bandgap by mechanically deform-
ing cholesteric elastomers has led to an attractive applica-
tion opportunity of tunable lasers [21,40,47].
Another possibility is topological imprinting of helical
director distribution. Being anchored to the rubber ma-
trix due to crosslinking, the director texture can remain
macroscopically chiral even when all chiral dopants are
removed from the material [18,39]. Internally stored me-
chanical twist can cause cholesteric elastomers to interact
selectively with solvents according to its imprinted hand-
edness. Separation of racemic mixtures by stereo-selective
swelling is an interesting application possibility.
The lack of centro-symmetric invariance in chiral elas-
tic materials leads to interesting and rare physical proper-
ties [56], in particular, piezoelectric eﬀect and non-linear
optical properties. Such characteristics of elastomers with
a chiral smectic C order (the ferroelectric liquid crys-
talline elastomers, FLCE) have been studied with some
intensity for several years now. After the permanently
monodomain (ﬁxed by crosslinking) free-standing sam-
ples of FLCE were prepared [20,23], several useful exper-
iments targeting various electro-mechanical and electro-
optical properties, in particular – the piezoelectricity and
the non-linear optical response, have been reported in re-
cent years [7,36]. Clearly, the prospect of important appli-
cations will continue driving this work. In this short re-
view we shall concentrate on the analogous eﬀect in chiral
nematic LCE which do not possess a spontaneous polar-
ization.
With much higher symmetry of nematic and choles-
teric elastomers (the point group D1 in a chiral material,
in contrast to a very low symmetry: C2 plus the transla-
tional eﬀect of layers in ferroelectric smectic C), there is
a real possibility to identify microscopic origins of piezo-
electricity in amorphous polymers or indeed elastomers,
if one aims to have an equilibrium eﬀect in a stress-resis-
tant material. The piezoelectric eﬀect in a uniform chiral
nematic LCE has been described phenomenologically [56]
and by a fully non-linear microscopic theory similar in its
spirit to the basic Trace formula for nematic rubber elastic-
ity [58]. All experimental research so far has concentrated
on the helically twisted cholesteric elastomers [11,42,64].
However, the cholesteric texture under the required shear
deformation [45], Fig. 10a, will produce highly non-uni-
form distortions giving rise to the well-understood ﬂexo-
electric eﬀect and masking the possible chiral piezoelec-
tricity.
The uniform linear piezoelectricity, i. e. the polariza-
tion induced by a uniform strain, Fig. 10b, (with the small
deformation " D   1), is unknown in completely amor-
phous polymers and rubbers. Even the famous PVDF
polymer-based piezoelectric has its response due to in-
homogeneity of crystalline regions aﬀected by macro-
scopic deformation. The molecular theory [58] has exam-
ined the eﬀect of chirality in molecular structure of chain
monomers and the bias in their short-axis alignment when
the chains are stretched at an angle to the average nematic
director n. If the monomers possess a transverse dipole
moment, this bias leads to macroscopic polarization:
P ' 1
2(nc) : T  `1

   `0
 :
(9)
Here, as in the underlying rubber elasticity, nc is the num-
ber of network strands per unit volume (the crosslinking
density) and the parameter  is the measure of monomer
chirality with the transverse dipole moment dt, see [58]
for detail. Compare this with the original Trace formula

Anisotropic Networks, Elastomers and Gels
A
307
of Eq. (1) to note the characteristic “sandwiching” of strain
between the two diﬀerent orientation tensors. This expres-
sion involves the full deformation gradient tensor  and,
therefore can describe large deformations of a chiral ne-
matic rubber. When shear deformations are small, the lin-
ear approximation of (9) gives, for a symmetric shear,
P ' [n  ("  n)] ;
with the linear coeﬃcient  D @Pı@"   1
2 nc(`2
k 
`2
?)
ı
`k`? clearly proportional to the degree of local ne-
matic order through the chain anisotropy `k  `?. Piezo-
electricity in amorphous rubbers is not only interesting
from the point of view of fundamentals of chiral poly-
mer random walks and symmetry breaking. On the prac-
tical side, due to the rubber modulus much lower than
in ordinary solid piezoelectrics (typically G  105 Pa), the
relevant coeﬃcient d D @Pı@ D ıG is much higher
than the corresponding response to stress in, for in-
stance, quartz or PVDF. The corresponding low mechan-
ical impedance should make the piezoelectric rubber at-
tractive for many energy transducing applications.
New Frontiers
When positional ordering, in the form of layering, is added
to the orientational order of nematics, we have the smec-
tic phase, Fig. 1c,d. When made into networks, they re-
main locally liquid-like, can suﬀer large extensions and
have mobile internal degrees of freedom – much like with
translationally homogeneous nematic elastomers and gels.
This freedom, which gave spontaneous distortions and
soft elasticity in nematics, is restricted in smectics by the
layers to which the matrix is often strongly coupled. The
mechanics of smectic elastomers [38,51] is decidedly more
complex than that of nematic elastomers. In many cases
the eﬀect of layers means that the material has the non-
linear mechanics of a 2D rubber in the layer plane, while
perform as a 1-dimensional crystalline solid in the third di-
mension [2] when the layer translation with respect to the
rubbery matrix is penalized by a periodic potential pro-
vided by the lamellar order. Alternatively, concentrating
on the layer structure, one ﬁnds the behavior radically al-
tered from the liquid crystal case. The layers interact via
the eﬀective background provided by the elastomer net-
work and, as a result, no longer display the celebrated Lan-
dau–Peierls loss of long range order in one- or two- di-
mensions. The layer buckling instabilities occur when de-
formations along the layer normal are applied. An even
more rich system is the smectic-C elastomer and its vari-
ants. This phase has the director tilted in the layer planes
and a much lower symmetry, allowing for a great variety
of soft deformation modes [3,52]. The chiral variants of
smectic-C networks are ferroelectric solids: Soft, greatly
extensible and non-crystalline and thus of great techno-
logical signiﬁcance.
This article examines some of the recent and relevant
ﬁndings about the new class of materials – liquid crys-
talline elastomers and gels. Nematic rubbers have already
proved themselves an unusual and exciting system, with
a number of unique optical and mechanical properties, in-
deed a real example of the Cosserat media with couple-
stress elasticity. Polymer networks with cholesteric order
are an equally provocative system promising new physical
properties.
The basic molecular theory of nematic rubber appears
to be exceptionally simple in its foundation and does not
involve any model parameters apart from the backbone
polymer chain anisotropy `k
ı
`?, which can be indepen-
dently measured. This is a great advantage over many
other soft condensed matter systems requiring compli-
cated, sometimes ambiguous theoretical approaches. Of
course, in many real situations and materials one ﬁnds
a need to look deeper into the microscopic properties;
an example of this is the “semi-softness” of nematic net-
works. Most of these systems are characterized by non-
uniform deformations: Even in the simplest experimen-
tal set-up a large portion of the sample near the clamps is
subjected to non-uniform strains and, therefore, responds
with a non-uniform director ﬁeld.
Looking into the future, many challenging and funda-
mental problems in this ﬁeld are still outstanding. The ﬁeld
of dynamics and relaxation in rubbery networks, although
not young by any means, is still not oﬀering an unambigu-
ous physical picture of stress relaxation. Adding the liquid
crystalline order, we ﬁnd an additional (director) ﬁeld un-
dergoing its own relaxation process and coupled to that of
an elastic network. In the particular case of polydomain
(i. e. randomly disordered in equilibrium) elastomers, we
can identify a hierarchical sequence of physical processes
in the underlying network (above its Tg) and the superim-
posed glass-like nematic order. This leads to a particularly
slow relaxation, but much remains to be done to under-
stand the physics of such complex systems.
The general problem of dynamic mechanical proper-
ties, rheology and relaxation in Cosserat-like incompress-
ible solids, also characterized by the eﬀect of soft elasticity,
brings to mind a number of possible applications. An ex-
ample would be the selective attenuation of certain acous-
tic waves, with polarization and propagation direction sat-
isfying the condition for softness, essentially leading to an
acoustic ﬁltering system. Another example of application
of soft elasticity, also related to the problem of relaxation,

308 A
Anisotropic Networks, Elastomers and Gels
is the damping of shear vibrations in an engineering com-
ponent when its surface is covered by a layer of nematic or
smectic rubber, particularly aligned to allow the director
rotation and softness.
Other very important area of applications is based on
polarizational properties of materials with chirality. Most
non-linear optical applications (which have a great tech-
nological potential) deal with elastomers in the ferroelec-
tric smectic C phase. The low symmetry (in particular,
chirality) and large spontaneous polarization of C smec-
tics have a strong eﬀect on the underlying elastic net-
work, and vice versa. This also returns back to the gen-
eral problem of mechanical properties of smectic rubbers
and gels. In conclusion, after the initial period of explo-
ration and material synthesis, liquid crystalline elastomers,
in all their variety, now oﬀer themselves as an exciting
ground for both fundamental research and for technology.
Smectic and lamellar liquid-crystalline elastomers and gels
are a much more recent area of study, with much more
theoretical and experimental work required to underpin
their dramatically anisotropic and non-linear mechanical
properties combining a two-dimensional rubber-elastic
response and a solid-like properties in the third direction.
Bibliography
1. Abramchuk SS, Khokhlov AR (1987) Molecular theory of high
elasticity of the polymer networks with orientational ordering
of links. Doklady Akad Nauk SSSR 297:385
2. Adams JM, Warner M (2005) Elasticity of smectic-A elastomers.
Phys Rev E 71:021708
3. Adams JM, Warner M (2005) Soft elasticity in smectic elas-
tomers. Phys Rev E 72:011703
4. Annaka M, Tanaka T (1992) Multiple phases of polymer gels.
Nature 355:430
5. Bhattacharya K (2003) Microstructure of martensite. Oxford
University Press, Oxford
6. Bladon P, Terentjev EM, Warner M (1994) Deformation-induced
orientational transitions in liquid crystal elastomers. J Phys II
4:75
7. Brehmer M, Zentel R, Giesselmann F, Germer R, Zugemaier P
(1996) Coupling of liquid crystalline and polymer network
properties in LC-elastomers. Liq Cryst 21:589
8. Broer DJ, Lub J, Mol GN (1995) Wide-band reflective polariz-
ers from cholesteric polymer networks with a pitch gradient.
Nature 378:467
9. Camacho-Lopez M, Finkelmann H, Palffy-Muhoray P, Shelley M
(2004) Fast liquid-crystal elastomer swims into the dark. Nat
Mater 3:307
10. Chang C-C, Chien L-C, Meyer RB (1997) Electro-optical study of
nematic elastomer gels. Phys Rev E 56:595
11. Chang C-C, Chien L-C, Meyer RB (1997) Piezoelectric effects in
cholesteric elastomer gels. Phys Rev E 55:534
12. Cicuta P, Tajbakhsh AR, Terentjev EM (2002) Evolution of pho-
tonic structure on deformation of cholesteric elastomers. Phys
Rev E 65:051704
13. Cicuta P, Tajbakhsh AR, Terentjev EM (2004) Photonic
bandgaps and optical activity in cholesteric elastomers. Phys
Rev E 70:011703
14. Clarke SM, Terentjev EM (1998) Slow stress relaxation in ran-
domly disordered nematic elastomers and gels. Phys Rev Lett
81:4436
15. Clarke SM, Tajbakhsh AR, Terentjev EM, Remillat C, Tomlinson
GR, House JR (2001) Soft elasticity and mechanical damping in
liquid crystalline elastomers. J Appl Phys 89:6530
16. Clarke SM, Tajbakhsh AR, Terentjev EM, Warner M (2001)
Anomalous viscoelastic response of nematic elastomers. Phys
Rev Lett 86:4044
17. Courty S, Tajbakhsh AR, Terentjev EM (2003) Phase chirality
and stereo-selective swelling of cholesteric elastomers. Eur
Phys J E 12:617
18. Courty S, Tajbakhsh AR, Terentjev EM (2003) Stereo-selective
swelling of imprinted cholesterics networks. Phys Rev Lett
91:085503
19. Cviklinski J, Tajbakhsh AR, Terentjev EM (2002) UV-isomerisa-
tion in nematic elastomers as a route to photo-mechanical
transducer. Eur Phys J E 9:427
20. Finkelmann H, Benne I, Semmler K (1995) Smectic liquid single-
crystal elastomers. Macromol Symp 96:169
21. Finkelmann H, Kim ST, Munoz A, Palffy-Muhoray P, Taheri B
(2001) Tunable mirrorless lasing in cholesteric liquid crystalline
elastomers. Adv Mater 13:1069
22. Finkelmann H, Nishikawa E, Pereira GG, Warner M (2001) A new
opto-mechanical effect in solids. Phys Rev Lett 87:015501
23. Gebhard E, Zentel R (1998) Freestanding ferroelectric elas-
tomer films. Macromol Rapid Comm 19:341
24. Golubovi´c L and Lubensky TC (1989) Nonlinear elasticity of
amorphous solids. Phys Rev Lett 63:1082
25. Hikmet RAM, Kemperman H (1998) Electrically switchable mir-
rors and optical components made from liquid-crystal gels.
Nature 392:476
26. Hogan PM, Tajbakhsh AR, Terentjev EM (2002) UV-manipula-
tion of order and macroscopic shape in nematic elastomers.
Phys Rev E 65:041720
27. Ilmain F, Tanaka T, Kokufuta E (1991) Volume transition in a gel
driven by hydrogen-bonding. Nature 349:400
28. Kim ST, Finkelmann H (2001) Cholesteric liquid single-crys-
tal elastomers (LSCE) obtained by the anisotropic deswelling
method. Macromol Rapid Comm 22:429
29. Kishi R, Shishido M, Tazuke S (1990) Liquid-crystalline poly-
mer gels: Anisotropic swelling of poly(gamma-benzyl L-gluta-
mate) gel crosslinked under a magnetic field. Macromolecules
23:3868
30. Kishi R, Suzuki Y, Ichijo H, Hirasa H (1997) Electrical deformation
of thermotropic liquid-crystalline polymer gels. Mol Cryst Liq
Cryst 294:411
31. Kundler I, Finkelmann H (1995) Strain-induced director reori-
entation in nematic liquid single-crystal elastomers. Macromol
Rapid Comm 16:679
32. Kundler I, Finkelmann H (1998) Director reorientation via
stripe-domains in nematic elastomers. Macromol Chem Phys
199:677
33. Kutter S, Terentjev EM (2001) Tube model for the elasticity of
entangled nematic rubbers. Eur Phys J E 6:221
34. Küpfer J, Finkelmann H (1991) Nematic liquid single-crystal
elastomers. Macromol Rapid Comm 12:717

Anomalous Diffusion on Fractal Networks
A
309
35. Legge CH, Davis FJ, Mitchell GR (1991) Memory effects in liq-
uid-crystal elastomers. J Phys II 1:1253
36. Lehmann W, Gattinger P, Keck M, Kremer F, Stein P, Eck-
ert T, Finkelmann H (1998) The inverse electromechanical ef-
fect in mechanically oriented SmC*-elastomers. Ferroelectrics
208:373
37. Li MH, Keller P, Li B, Wang XG, Brunet M (2003) Light-driven
side-on nematic elastomer actuators. Adv Mater 15:569
38. Lubensky TC, Terentjev EM, Warner M (1994) Layer-network
coupling in smectic elastomers. J Phys II 4:1457
39. Mao Y, Warner M (2000) Theory of chiral imprinting. Phys Rev
Lett 84:5335
40. Matsui T, Ozaki R, Funamoto K, Ozaki M, Yoshino K (2002) Flexi-
ble mirrorless laser based on a free-standing film of photopoly-
merized cholesteric liquid crystal. Appl Phys Lett 81:3741
41. Matsuyama A, Kato T (2002) Nematic ordering-induced vol-
ume phase transitions of liquid crystalline gels. J Chem Phys
116:8175
42. Meier W, Finkelmann H (1990) Piezoelectricity of cholesteric
elastomers. Macromol Chem Rapid Comm 11:599
43. Mitchell GR, Davis FJ, Guo W (1993) Strain-induced transitions
in liquid-crystal elastomers. Phys Rev Lett 71:2947
44. Olmsted PD (1994) Rotational invariance and goldstone
modes in nematic elastomers and gels. J Phys II 4:2215
45. Pelcovits RA, Meyer RB (1995) Piezoelectricity of cholesteric
elastomers. J Phys II 5:877
46. Roberts PMS, Mitchell GR, Davis FJ (1997) A single director
switching mode for monodomain liquid crystal elastomers.
J Phys II 7:1337
47. Schmidtke J, Stille W, Finkelmann H (2003) Defect mode emis-
sion of a dye doped cholesteric polymer network. Phys Rev Lett
90:083902
48. Schönstein M, Stille W, Strobl G (2001) Effect of the network
on the director fluctuations in a nematic side-group elastomer
analysed by static and dynamic light scattering. Eur Phys J E
5:511
49. Shibayama M, Tanaka T (1993) Volume phase-transition and
related phenomena of polymer gels. Adv Polym Sci 109:1
50. Stenull O, Lubensky TC (2004) Anomalous elasticity of nematic
and critically soft elastomers. Phys Rev E 69:021807
51. Stenull O, Lubensky TC (2005) Phase transitions and soft elas-
ticity of smectic elastomers. Phys Rev Lett 94:018304
52. Stenull O, Lubensky TC (2006) Soft elasticity in biaxial smectic
and smectic-C elastomers. Phys Rev E 74:051709
53. Tabiryan N, Serak S, Dai X-M, Bunning T (2005) Polymer film
with optically controlled form and actuation. Opt Express
13:7442
54. Tajbakhsh AR, Terentjev EM (2001) Spontaneous thermal ex-
pansion of nematic elastomers. Eur Phys J E 6:181
55. Tanaka T (1978) Collapse of gels and criticalendpoint. Phys Rev
Lett 40:820
56. Terentjev EM (1993) Phenomenological theory of non-uniform
nematic elastomers: Free energy of deformations and electric
field effects. Europhys Lett 23:27
57. Terentjev EM (1995) Density functional model of anchoring
energy at a liquid crystalline polymer-solid interface. J Phys II
5:159
58. Terentjev EM, Warner M (1999) Piezoelectricity of chiral ne-
matic elastomers. Eur Phys J B 8:595
59. Terentjev EM, Warner M, Bladon P (1994) Orientation of liquid
crystal elastomers and gels by an electric field. J Phys II 4:667
60. Thomsen DL, Keller P, Naciri J, Pink R, Jeon H, Shenoy D, Ratna
BR (2001) Liquid crystal elastomers with mechanical properties
of a muscle. Macromolecules 34:5868
61. Urayama K, Arai YO, Takigawa T (2005) Volume phase transi-
tion of monodomain nematic polymer networks in isotropic
solvents. Macromolecules 38:3469
62. Urayama K, Okuno Y, Nakao T, Kohjiya S (2003) Volume tran-
sition of nematic gels in nematogenic solvents. J Chem Phys
118:2903
63. Urayama K, Kondo H, Arai YO, Takigawa T (2005) Electrically
driven deformations of nematic gels. Phys Rev E 71:051713
64. Vallerien SU, Kremer F, Fischer EW, Kapitza H, Zentel R, Poths H
(1990) Experimental proof of piezoelectricity in cholesteric and
chiral smectic C* phases of LC-elastomers. Macromol Chem
Rapid Comm 11:593
65. Verwey GC, Warner M (1997) Compositional fluctuations and
semisoftness in nematic elastomers. Macromolecules 30:4189
66. Verwey GC, Warner M, Terentjev EM (1996) Elastic instability
and stripe domains in liquid crystalline elastomers. J Phys II
6:1273
67. Warner M, Bladon P, Terentjev EM (1994) ‘Soft Elasticity’ –
Deformations without resistance in liquid crystal elastomers.
J Phys II 4:93
68. Warner M, Terentjev EM (2007) Liquid crystal elastomers, 2nd
edn. Oxford University Press, Oxford
69. Warner M, Gelling KP, Vilgis TA (1988) Theory of nematic net-
works. J Chem Phys 88:4008
70. Warner M, Terentjev EM, Meyer RB, Mao Y (2000) Untwisting
of a cholesteric elastomer by a mechanical field. Phys Rev Lett
85:2320
71. Yu Y, Nakano M, Ikeda T (2003) Directed bending of a polymer
film by light – miniaturizinga simple photomechanical system.
Nature 425:145
72. Yusuf Y, Ono Y, Sumisaki Y, Cladis PE, Brand HR, Finkelmann
H, Kai S (2004) Swelling dynamics of liquid crystal elastomers
swollen with low molecular weight liquid crystals. Phys Rev E
69:021710
73. Zubarev ER, Talroze RV, Yuranova TI, Vasilets VN, Plate NA
(1996) Influence of crosslinking conditions on the phase be-
havior of a polyacrylate-based liquid-crystalline elastomer.
Macromol Rapid Comm 17:43
Anomalous Diffusion
on Fractal Networks
IGOR M. SOKOLOV
Institute of Physics, Humboldt-Universität zu Berlin,
Berlin, Germany
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Random Walks and Normal Diﬀusion
Anomalous Diﬀusion
Anomalous Diﬀusion on Fractal Structures
Percolation Clusters

310 A
Anomalous Diffusion on Fractal Networks
Scaling of PDF and Diﬀusion Equations
on Fractal Lattices
Further Directions
Bibliography
Glossary
Anomalous diﬀusion An essentially diﬀusive process in
which the mean squared displacement grows, how-
ever, not as hR2i / t like in normal diﬀusion, but
as hR2i / t˛ with ˛ ¤ 0, either asymptotically faster
than in normal diﬀusion (˛ > 1, superdiﬀusion) or
asymptotically slower (˛ < 1, subdiﬀusion).
Comb model A planar network consisting of a backbone
(spine) and teeth (dangling ends). A popular simple
model showing anomalous diﬀusion.
Fractional diﬀusion equation A diﬀusion equation for
a non-Markovian diﬀusion process, typically with
a memory kernel corresponding to a fractional deriva-
tive. A mathematical instrument which adequately de-
scribes many processes of anomalous diﬀusion, for ex-
ample the continuous-time random walks (CTRW).
Walk dimension The fractal dimension of the trajectory
of a random walker on a network. The walk dimension
is deﬁned through the mean time T or the mean num-
ber of steps n which a walker needs to leave for the ﬁrst
time the ball of radius R around the starting point of
the walk: T / Rdw.
Spectral dimension A property of a fractal structure
which substitutes the Euclidean dimension in the ex-
pression for the probability to be at the origin at time t,
P(0; t) / tds/2. Deﬁnes the behavior of the network
Laplace operator.
Alexander–Orbach conjecture A
conjecture
that the
spectral dimension of the incipient inﬁnite cluster in
percolation is equal to 4/3 independently on the Eu-
clidean dimension. The invariance of spectral dimen-
sion is only approximate and holds within 2% accuracy
even in d D 2. The value of 3/4 is achieved in d > 6
and on trees.
Compact visitation A property of a random walk to visit
practically all sites within the domain of the size of the
mean squared displacement. The visitation is compact
if the walk dimension dw exceeds the fractal dimension
of the substrate df.
Definition of the Subject
Many situations in physics, chemistry, biology or in com-
puter science can be described within models related to
random walks, in which a “particle” (walker) jumps from
one node of a network to another following the network’s
links. In many cases the network is embedded into Eu-
clidean space which allows for discussing the displacement
of the walker from its initial site. The displacement’s be-
havior is important e. g. for description of charge transport
in disordered semiconductors, or for understanding the
contaminants’ behavior in underground water. In other
cases the discussion can be reduced to the properties which
can be deﬁned for a whatever network, like return proba-
bilities to the starting site or the number of diﬀerent nodes
visited, the ones of importance for chemical kinetics or for
search processes on the corresponding networks. In the
present contribution we only discuss networks embedded
in Euclidean space.
One of the basic properties of normal diﬀusion is the
fact that the mean squared displacement of a walker grows
proportionally to time, R2 / t, and asymptotic deviations
from this law are termed anomalous diﬀusion. Micro-
scopic models leading to normal diﬀusion rely on the spa-
tial homogeneity of the network, either exact or statisti-
cal. If the system is disordered and its local properties vary
from one its part to another, the overall behavior at very
large scales, larger than a whatever inhomogeneity, can
still be diﬀusive, a property called homogenization.
The ordered systems or systems with absent long-
range order but highly homogeneous on larger scales do
not exhaust the whole variety of relevant physical systems.
In many cases the system can not be considered as homo-
geneous on a whatever scale of interest. Moreover, in some
of these cases the system shows some extent of scale invari-
ance (dilatation symmetry) typical for fractals. Examples
are polymer molecules and networks, networks of pores
in some porous media, networks of cracks in rocks, and
so on. Physically all these systems correspond to a case of
very strong disorder, do not homogenize even at largest
relevant scales, and show fractal properties. The diﬀusion
in such systems is anomalous, exhibiting large deviations
from the linear dependence of the mean squared displace-
ment on time. Many other properties, like return proba-
bilities, mean number of diﬀerent visited nodes etc. also
behave in a way diﬀerent from the one they behave in nor-
mal diﬀusion. Understanding these properties is of pri-
mary importance for theoretical description of transport
in such systems, chemical reactions in them, and in devis-
ing eﬃcient search algorithms or in understanding ones
used in natural search, as taking place in gene expression
or incorporated into the animals’ foraging strategies.
Introduction
The theory of diﬀusion processes was initiated by A. Fick
who was interested in the nutrients’ transport in a living

Anomalous Diffusion on Fractal Networks
A
311
body and put down the phenomenological diﬀusion equa-
tion (1855). The microscopic, probabilistic discussion of
diﬀusion started with the works of L. Bachelier on the the-
ory of ﬁnancial speculation (1900), of A. Einstein on the
motion of colloidal particles in a quiescent ﬂuid (the Brow-
nian motion) (1905), and with a short letter of K. Pearson
to the readers of Nature in the same year motivated by his
work on animal motion. These works put forward the de-
scription of diﬀusion processes within the framework of
random walk models, the ones describing the motion as
a sequence of independent bounded steps in space requir-
ing some bounded time to be completed. These models
lead to larger scales and longer times to a typical diﬀusion
behavior as described by Fick’s laws. The most prominent
property of this so-called normal diﬀusion is the fact that
the mean squared displacement of the diﬀusing particle
from its initial position at times much longer than the typ-
ical time of one step grows as hx2(t)i / t. Another promi-
nent property of normal diﬀusion is the fact that the dis-
tribution of the particle’s displacements tends to a Gaus-
sian (of width hx2(t)i1/2), as a consequence of the inde-
pendence of diﬀerent steps.
The situations described by normal diﬀusion are abun-
dant, but looking closer into many other processes (which
still can be described within a random walk picture, how-
ever, with only more or less independent steps, or with
steps showing a broad distribution of their lengths or
times) revealed that these processes, still resembling or re-
lated to normal diﬀusion, show a vastly diﬀerent behav-
ior of the mean squared displacement, e. g. hx2(t)i / t˛
with ˛ ¤ 1. In this case one speaks about anomalous dif-
fusion [11]. The cases with ˛ < 1 correspond to subdif-
fusion, the ones with ˛ > 1 correspond to superdiﬀusion.
The subdiﬀusion is typical for the motion of charge carri-
ers in disordered semiconductors, for contaminant trans-
port by underground water or for proteins’ motion in cell
membranes. The models of subdiﬀusion are often con-
nected either with the continuous-time random walks or
with diﬀusion in fractal networks (the topic of the present
article) or with combinations thereof. The superdiﬀusive
behavior, associated with divergent mean square displace-
ment per complete step, is encountered e. g. in transport in
some Hamiltonian models and in maps, in animal motion,
in transport of particles by ﬂows or in diﬀusion in porous
media in Knudsen regime.
The diﬀusion on fractal networks (typically subdiﬀu-
sion) was a topic of extensive investigation in 1980s–1990s,
so that the most results discussed here can be considered as
well-established. Therefore most of the references in this
contribution are given to the review articles, and only few
of them to original works (mostly to the pioneering publi-
cations or to newer works which didn’t ﬁnd their place in
reviews).
Random Walks and Normal Diffusion
Let us ﬁrst turn to normal diﬀusion and concentrate on
the mean squared displacement of the random walker.
Our model will correspond to a random walk on a regu-
lar lattice or in free space, where the steps of the random
walker will be considered independent, identically dis-
tributed random variables (step lengths si) following a dis-
tribution with a given symmetric probability density func-
tion (PDF) p(s), so that hsi D 0. Let a be the root mean
square displacement in one step, hs2
i i D a2. The squared
displacement of the walker after n steps is thus
hr2
ni D
* n
X
iD1
si
!2+
D
n
X
iD1
hs2
i i C 2
n
X
iD1
n
X
jDiC1
hsisji :
The ﬁrst sum is simply na2, and the second one vanishes
if diﬀerent steps are uncorrelated and have zero mean.
Therefore
˝
r2
n
˛
D a2n: The mean squared displacement in
a walk is proportional to the number of steps.
Provided the mean time  necessary to complete a step
exists, this expression can be translated into the tem-
poral dependence of the displacement hr2(t)i D (a2/)t,
since the mean number of steps done during the time t is
n D t/. This is the time-dependence of the mean squared
displacement of a random walker characteristic for nor-
mal diﬀusion. The prefactor a2/ in the hr2(t)i / t depen-
dence is connected with the usual diﬀusion coeﬃcient K,
deﬁned through hr2(t)i D 2dKt, where d is the dimen-
sion of the Euclidean space. Here we note that in the the-
ory of random walks one often distinguishes between the
situations termed as genuine “walks” (or velocity models)
where the particle moves, say, with a constant velocity and
changes its direction at the end of the step, and “random
ﬂight” models, when steps are considered to be instanta-
neous (“jumps”) and the time cost of a step is attributed to
the waiting time at a site before the jump is made. The sit-
uations considered below correspond to the ﬂight picture.
The are three important postulates guaranteeing that
random walks correspond to a normal diﬀusion process:
 The existence of the mean squared displacement per
step, hs2i < 1
 The existence of the mean time per step (interpreted as
a mean waiting time on a site)  < 1 and
 The uncorrelated nature of the walk hsisji D hs2iıi j.
These are exactly the three postulates made by Einstein
in his work on Brownian motion. The last postulate can be

312 A
Anomalous Diffusion on Fractal Networks
weakened: The persistent (i. e. correlated) random walks
still lead to normal diﬀusion provided P1
jDihsisji < 1.
In the case when the steps are independent, the central
limit theorem immediately states that the limiting distri-
bution of the displacements will be Gaussian, of the form
P(r; t) D
1
(4Kt)d/2 exp

 dr2
4Kt

:
This distribution scales as a whole: rescaling the dis-
tance by a factor of , x ! x and of the time by
the factor of 2, t ! 2t, does not change the form of
this distribution. The moments of the displacement scale
according to hrki D
R
P(r; t)rkddrk D const(k)tk/2. An-
other important property of the the PDF is the fact that
the return probability, i. e. the probability to be at time t at
the origin of the motion, P(0; t), scales as
P(0; t) / td/2 ;
(1)
i. e. shows the behavior depending on the substrate’s di-
mension.
The PDF P(r; t) is the solution of the Fick’s diﬀusion
equation
@
@t P(r; t) D k
P(r; t) ;
(2)
where 
 is a Laplace operator. Equation 2 is a parabolic
partial diﬀerential equation, so that its form is invariant
under the scale transformation t ! 2t, r ! r discussed
above. This invariance of the equation has very strong im-
plications. If follows, for example, that one can invert the
scaling relation for the mean squared displacement
R2 / t
and interpret this inverse one
T / r2
as e. g. a scaling rule governing the dependence of a char-
acteristic passage time from some initial site 0 to the sites
at a distance r from this one. Such scaling holds e. g. for the
mean time spent by a walk in a prescribed region of space
(i. e. for the mean ﬁrst passage time to its boundary). For
an isotropic system Eq. (2) can be rewritten as an equation
in the radial coordinate only:
@
@t P(r; t) D K
1
rd1
@
@r

rd1 @
@r P(r; t)

:
(3)
Looking at the trajectory of a random walk at scales
much larger than the step’s length one infers that it is
self-similar in statistical sense, i. e. that its whatever por-
tion (of the size considerably larger then the step’s length)
looks like the whole trajectory, i. e. it is a statistical, ran-
dom fractal. The fractal (mass) dimension of the trajec-
tory of a random walk can be easily found. Let us inter-
pret the number of steps as the “mass” M of the object and
˝r2
n
˛1/2 as its “size” L. The mass dimension D is then de-
ﬁned as M / LD, it corresponds to the scaling of the mass
of a solid D-dimensional object with its typical size. The
scaling of the mean squared displacement of a simple ran-
dom walk with its number of steps suggests that the cor-
responding dimension D D dw for a walk is dw D 2. The
large-scale properties of the random walks in Euclidean
space are universal and do not depend on exact form of
the PDF of waiting times and of jumps’ lengths (provided
the ﬁrst possesses at least one and the second one at least
two moments).
This fact can be used for an interpretation of several
known properties of simple random walks. Thus, ran-
dom walks on a one-dimensional lattice (d D 1) are re-
current, i. e. each site earlier visited by the walk is revis-
ited repeatedly afterwards, while for d  3 they are non-
recurrent. This property can be rationalized through ar-
guing that an intrinsically two-dimensional object cannot
be “squeezed” into a one-dimensional space without self-
intersections, and that it cannot ﬁll fully a space of three
or more dimensions. The same considerations can be used
to rationalize the behavior of the mean number of diﬀer-
ent sites visited by a walker during n steps hSni. hSni typi-
cally goes as a power law in n. Thus, on a one-dimensional
lattice hSni / n1/2, in lattices in three and more dimen-
sions hSni / n, and in a marginal two-dimensional situa-
tion hSni / n/ log n. To understand the behavior in d D 1
and in d D 3, it is enough to note that in a recurrent one-
dimensional walk all sites within the span of the walk are
visited by a walker at least once, so that the mean number
of diﬀerent sites visited by a one-dimensional walk (d D 1)
grows as a span of the walk, which itself is proportional
to a mean squared displacement, hSni / L D hr2
ni1/2, i. e.
hSni / n1/2 D n1/dw. In three dimensions (d D 3), on the
contrary, the mean number of diﬀerent visited sites grows
as the number of steps, hSni / n, since diﬀerent sites are
on the average visited only ﬁnite number of times. The
property of random walks on one-dimensional and two-
dimensional lattices to visit practically all the sites within
the domain of the size of their mean square displacement
is called the compact visitation property.
The importance of the number of diﬀerent sites visited
(and its moments like hSni or hS(t)i) gets clear when one
considers chemical reactions or search problems on the
corresponding networks. The situation when each site of

Anomalous Diffusion on Fractal Networks
A
313
a network can with the probability p be a trap for a walker,
so that the walker is removed whenever it visits such
a marked site (reaction scheme A C B ! B, where the
symbol A denotes the walker and symbol B the “immor-
tal” and immobile trap) corresponds to a trapping problem
of reaction kinetics. The opposite situation, corresponding
to the evaluation of the survival probability of an immo-
bile A-particle at one of the network’s nodes in presence of
mobile B-walkers (the same A C B ! B reaction scheme,
however now with immobile A and mobile B) corresponds
to the target problem, or scavenging. In the ﬁrst case the
survival probability ˚(N) for the A-particle after N steps
goes for p small as ˚(t) / hexp(pSN)i, while for the sec-
ond case it behaves as ˚(t) / exp(phSNi). The simple
A C B ! B reaction problems can be also interpreted as
the ones of the distribution of the times necessary to ﬁnd
one of multiple hidden “traps” by one searching agent, or
as the one of ﬁnding one special marked site by a swarm of
independent searching agents.
Anomalous Diffusion
The Einstein’s postulates leading to normal diﬀusion are to
no extent the laws of Nature, and do not have to hold for
a whatever random walk process. Before discussing gen-
eral properties of such anomalous diﬀusion, we start from
a few simple examples [10].
Simple Geometric Models Leading to Subdiﬀusion
Let us ﬁrst discuss two simple models of geometrical
structures showing anomalous diﬀusion. The discussion
of these models allows one to gain some intuition about
the emergence and properties of anomalous diﬀusion in
complex geometries, as opposed to Euclidean lattices, and
discuss mathematical notions appearing in description of
such systems. In all cases the structure on which the walk
takes place will be considered as a lattice with the unit
spacing between the sites, and the walk corresponds to
jumps from a site to one of its neighbors. Moreover, we
consider the random walk to evolve in discrete time; the
time t and the number of steps n are simply proportional
to each other.
Diﬀusion on a Comb Structure
The simplest example
of how the subdiﬀusive motion can be created in a more or
less complex (however yet non-fractal) geometrical struc-
ture is delivered by the diﬀusion on a comb. We con-
sider the diﬀusion of a walker along the backbone of
a comb with inﬁnite “teeth” (dangling ends), see Fig. 1. The
walker’s displacement in x-direction is only possible when
it is on the backbone; entering the side branch switches
Anomalous Diffusion on Fractal Networks, Figure 1
A comb structure: Trapping in the dangling ends leads to the
anomalous diffusion along the comb’s backbone. This kind of
anomalous diffusion is modeled by CTRW with power-law wait-
ing time distribution and described by a fractional diffusion
equation
oﬀthe diﬀusion along the backbone. Therefore the mo-
tion on the backbone of the comb is interrupted by wait-
ing times. The distribution of these waiting times is given
by the power law  (t) / t3/2 corresponding to the ﬁrst
return probability of a one-dimensional random walk in
a side branch to its origin. This situation – simple random
walk with broad distribution of waiting times on sites –
corresponds to a continuous-time random walk model. In
this model, for the case of the power-law waiting time dis-
tributions of the form  (t) / t1˛ with ˛ < 1, the mean
number of steps hn(t)i (in our case: the mean number
of steps over the backbone) as a function of time goes as
hn(t)i / t˛. Since the mean squared displacement along
the backbone is proportional to this number of steps, it is
also governed by
˝
x2(t)
˛
/ t˛, i. e. in our case we have to
do with strongly subdiﬀusive behavior ˝x2(t)˛ / t1/2.
The PDF of the particle’s displacements in CTRW with
the waiting time distribution  (t) is governed by a non-
Markovian generalization of the diﬀusion equation
@
@t P(x; t) D a2 @
@t 
Z 1
0
M(t  t0)P(x; t0) ;
with the memory kernel M(t) which has a physical mean-
ing of the time-dependent density of steps and is given by
the inverse Laplace transform of ˜M(u) D ˜ (u)/[1 ˜ (u)]
(with ˜ (u) being the Laplace-transform of  (t)). For the
case of a power-law waiting-time probability density of
the type  (t) / t1˛ the structure of the right-hand side
of the equation corresponds to the fractional Riemann–
Liouvolle derivative 0Dˇ
t , an integro-diﬀerential operator
deﬁned as
0Dˇ
t f (t) D
1
 (1  ˇ)
d
dt
Z t
0
dt0
f (t0)
(t  t0)ˇ ;
(4)
(here only for 0 < ˇ < 1), so that in the case of the diﬀu-
sion on a backbone of the comb we have
@
@t P(x; t) D K0D1˛
t

P(x; t) ;
(5)

314 A
Anomalous Diffusion on Fractal Networks
(here with ˛ D 1/2). An exhaustive discussion of such
equations is provided in [13,14]. Similar fractional diﬀu-
sion equation was used in [12] for the description of the
diﬀusion in a percolation system (as measured by NMR).
However one has to be cautious when using CTRW and
corresponding fractional equations for the description of
diﬀusion on fractal networks, since they may capture
some properties of such diﬀusion and fully disregard other
ones. This question will be discussed to some extent in
Sect. “Anomalous Diﬀusion on Fractal Structures”.
Equation (5) can be rewritten in a diﬀerent but equiv-
alent form

0 D˛
t
@
@t P(x; t) D K
P(x; t) ;
with the operator 
0 D˛
t , a Caputo derivative, conjugated
to the Riemann–Liouville one, Eq. (4), and diﬀerent from
Eq. (4) by interchanged order of diﬀerentiation and in-
tegration (temporal derivative inside the integral). The
derivation of both forms of the fractional equations and
their interpretation is discussed in [21]. For the sake of
transparency, the corresponding integrodiﬀerential oper-
ators 0Dˇ
t and 
0 Dˇ
t are simply denoted as a fractional
partial derivative @ˇ/@tˇ and interpreted as a Riemann–
Liouville or as a Caputo derivative depending whether they
stand on the left or on the right-hand side of a generalized
diﬀusion equation.
Random Walk on a Random Walk and on a Self-Avoid-
ing Walk
Let us now turn to another source of anoma-
lous diﬀusion, namely the nondecaying correlations be-
tween the subsequent steps, introduced by the structure of
the possible ways on a substrate lattice.
The case where the anomalies of diﬀusion are due to
the tortuosity of the path between the two sites of a frac-
tal “network” is illustrated by the examples of random
walks on polymer chains, being rather simple, topologi-
cally one-dimensional objects. Considering lattice models
for the chains we encounter two typical situations, the sim-
pler one, corresponding to the random walk (RW) chains
and a more complex one, corresponding to self-avoiding
ones. A conformation of a RW-chain of N monomers cor-
responds to a random walk on a lattice, self-intersections
are allowed. In reality, RW chains are a reasonable model
for polymer chains in melts or in -solutions, where the re-
pulsion between the monomers of the same chain is com-
pensated by the repulsion from the monomers of other
chains or from the solvent molecules, so that when not
real intersections, then at least very close contacts between
the monomers are possible. The other case corresponds to
chains in good solvents, where the eﬀects of steric repul-
sion dominate. The end-to-end distance in a random walk
chain grows as R / l1/2 (with l being its contour length).
In a self-avoiding-walk (SAW) chain R / l, with  being
a so-called Flory exponent (e. g.   3/5 in 3d). The cor-
responding chains can be considered as topologically one-
dimensional fractal objects with fractal dimensions df D 2
and df D 1/  5/3 respectively.
We now concentrate on an walker (excitation, enzyme,
transcription factor) performing its diﬀusive motion on
a chain, serving as a “rail track” for diﬀusion. This chain
itself is embedded into the Euclidean space. The contour
length of the chain corresponds to the chemical coordi-
nate along the path, in each step of the walk at a chain
the chemical coordinate changes by ˙1. Let us ﬁrst con-
sider the situation when a particle can only travel along
this chemical path, i. e. cannot change its track at a point
of a self-intersection of the walk or at a close contact be-
tween the two diﬀerent parts of the chain. This walk starts
with its ﬁrst step at r D 0. Let K be the diﬀusion coeﬃ-
cient of the walker along the chain. The typical displace-
ment of a walker along the chain after time t will then be
l / t1/2 D t1/dw. For a RW-chain this displacement along
the chain is translated into the displacement in Euclidean
space according to R D hR2i1/2 / l1/2, so that the typi-
cal displacement in Euclidean space after time t goes as
R / t1/4, a strongly subdiﬀusive behavior known as one
of the regimes predicted by a repetition theory of polymer
dynamics. The dimension of the corresponding random
walk is, accordingly, dw D 4. The same discussion for the
SAW chains leads to dw D 1/2.
After time t the displacement in the chemical space is
given by a Gaussian distribution
pc(l; t) D
1
p
4Kt
exp

 l2
4Kt

:
The change in the contour length l can be interpreted as
the number of steps along the random-walk “rail”, so that
the Euclidean displacement for the given l is distributed
according to
pE(r; l) D

d
2a2jlj

exp

 dr2
2a2jlj

:
(6)
We can, thus, obtain the PDF of the Euclidean displace-
ment:
P(r; l) D
Z 1
0
pE(r; l)pc(l; t)dl :
(7)
The same discussion can be pursued for a self-avoiding

Anomalous Diffusion on Fractal Networks
A
315
walk for which
pE(r; l) '

 r
l
 1

exp

const

 r
l
1/(1)
;
(8)
with  being the exponent describing the dependence of
the number of realizations of the corresponding chains on
their lengths. Equation (6) is a particular case of Eq. (8)
with  D 1 and  D 1/2. Evaluating the integral using the
Laplace method, we get
P(r; t) ' exp
"
const

r
t1/dw
(11/dw)1#
up to the preexponential. We note that in this case the rea-
son for the anomalous diﬀusion is the tortuosity of the
chemical path. The topologically one-dimensional struc-
ture of the lattice allowed us for discussing the problem in
considerable detail.
The situation changes when we introduce the possibil-
ity for a walker to jump not only to the nearest-neighbor
in chemical space by also to the sites which are far away
along the chemical sequence but close to each other in Eu-
clidean space (say one lattice site of the underlying lattice
far). In this case the structure of the chain in the Euclidean
space leads to the possibility to jump very long in chem-
ical sequence of the chain, with the jump length distribu-
tion going as p(l) / l3/2 for an RW and p(l) / l2:2 for
a SAW in d D 3 (the probability of a close return of a SAW
to its original path). The corresponding distributions lack
the second moment (and even the ﬁrst moment for RW
chains), and therefore one might assume that also the dif-
fusion in a chemical space of a chain will be anomalous.
It indeed shows considerable peculiarities. If the chain’s
structure is frozen (i. e. the conformational dynamics of
a chain is slow compared to the diﬀusion on the chain),
the situation in both cases corresponds to “paradoxical dif-
fusion” [22]: Although the PDF of displacements in the
chemical space lacks the second (and higher) moments,
the width of the PDF (described e. g. as its interquartile
distance) grows, according to W / t1/2. In the Euclidean
space we have here to do with the diﬀusion on a static frac-
tal structure, i. e. with a network of fractal dimension df co-
inciding with the fractal dimension of the chain. Allowing
the chain conformation changing in time (i. e. considering
the opposite limiting case when the conformation changes
are fast enough compared to the diﬀusion along the chain)
leads to another, superdiﬀusive, behavior, namely to Lévy
ﬂights along the chemical sequence. The diﬀerence be-
tween the static, ﬁxed networks and the transient ones has
to be borne in mind when interpreting physical results.
Anomalous Diffusion on Fractal Structures
Simple Fractal Lattices: the Walk’s Dimension
As already mentioned, the diﬀusion on fractal structures is
often anomalous, hx2(t)i / t˛ with ˛ ¤ 1. Parallel to the
situation with random walks in Euclidean space, the tra-
jectory of a walk is typically self similar (on the scales ex-
ceeding the typical step length), so that the exponent ˛ can
be connected with the fractal dimension of the walk’s tra-
jectory dw. Assuming the mean time cost per step to be ﬁ-
nite, i. e. t / n, one readily infers that ˛ D 2/dw. The frac-
tal properties of the trajectory can be rather easily checked,
and the value of dw can be rather easily obtained for sim-
ple regular fractals, the prominent example being a Sier-
pinski gasket, discussed below. Analytically, the value of
dw is often obtained via scaling of the ﬁrst passage time to
a boundary of the region (taken to be a sphere of radius r),
i. e. using not the relation R / t1/dw but the inverse one,
T / rdw. The fact that both values of dw coincide witnesses
strongly in favor of the self-similar nature of diﬀusion on
fractal structures.
Diﬀusion on a Sierpinski Gasket
Serpinski gasket (es-
sentially a Sierpinski lattice), one of the simplest regular
fractal network is a structure obtained by iterating a gen-
erator shown in Fig. 2. Its simple iterative structure and
the ease of its numerical implementation made it a popular
toy model exemplifying the properties of fractal networks.
The properties of diﬀusion on this network were inves-
tigated in detail, both numerically and analytically. Since
the structure does not possess dangling ends, the cause of
the diﬀusion anomalies is connected with tortuosity of the
typical paths available for diﬀusion, however, diﬀerently
from the previous examples, these paths are numerous and
non-equivalent.
The numerical method allowing for obtaining exact re-
sults on the properties of diﬀusion is based on exact enu-
meration techniques, see e. g. [10]. The idea here is to cal-
culate the displacement probability distribution based on
the exact number of ways Wi;n the particle, starting at
a given site 0 can arrive to a given site i at step n. For
a given network the number Wi;n is simply a sum of the
numbers of ways Wj;n1 leading from site 0 to the nearest
neighbors j of the site i, Wj;n1. Starting from W0;0 D 1
and Wi;0 D 0 for i ¤ 0 one simply updates the array
of Wi;n according to the rule Wi;n D P
jDnn(i) Wj;n1,
where nn(i) denotes the nearest neighbors of the node i
in the network. The probability Pi;n is proportional to the
number of these equally probably ways, and is obtained
from Wi;n by normalization. This is essentially a very old
method used to solve numerically a diﬀusion equation

316 A
Anomalous Diffusion on Fractal Networks
Anomalous Diffusion on Fractal Networks, Figure 2
A Sierpinski gasket: a its generator b a structure after 4th iteration. The lattice is rescaled by the factor of 4 to fit into the picture.
c A constriction obtained after triangle-star transformation used in the calculation of the walk’s dimension, see Subsect. “The Spectral
Dimension”
in the times preceding the computer era. For regular lat-
tices this gives us the exact values of the probabilities. For
statistical fractals (like percolation clusters) an additional
averaging over the realizations of the structure is neces-
sary, so that other methods of calculation of probabilities
and exponents might be superior. The value of dw can
then be obtained by plotting ln n vs. lnhr2
ni1/2: the corre-
sponding points fall on a straight line with slope dw. The
dimension of the walks on a Sierpinski gasket obtained
by direct enumeration coincides with its theoretical value
dw D ln 5/ ln 2 D 2:322 : : :, see Subsect. “The Spectral Di-
mension”.
Loopless Fractals (Trees)
Another situation, the one
similar the the case of a comb, is encountered in loopless
fractals (fractal trees), as exempliﬁed by a Vicsek’s con-
struction shown in Fig. 3, or by diﬀusion-limited aggre-
gates (DLA). For the structure depicted in Fig. 3, the walk’s
Anomalous Diffusion on Fractal Networks, Figure 3
A Vicsek fractal: the generator (left) and its 4th iteration. This
is a loopless structure; the diffusion anomalies here are mainly
caused by trapping in dangling ends
dimension is dw D log 15/ log 3 D 2:465 : : : [3]. Parallel to
the case of the comb, the main mechanism leading to sub-
diﬀusion is trapping in dangling ends, which, diﬀerent
from the case of the comb, now themselves have a fractal
structure. Parallel to the case of the comb, the time of travel
from one site of the structure to another one is strongly af-
fected by trapping. However, trapping inside the dangling
end does not lead to a halt of the particle, but only to con-
ﬁning its motion to some scale, so that the overall equa-
tions governing the PDF might be diﬀerent in the form
from the fractional diﬀusion ones.
Diﬀusion and Conductivity
The random walks of particles on a network can be de-
scribed by a master equation (which is perfectly valid for
the exponential waiting time distribution on a site and
asymptotically valid in case of all other waiting time dis-
tributions with ﬁnite mean waiting time ): Let p(t) be the
vector with elements pi(t) being the probabilities to ﬁnd
a particle at node i at time t. The master equation
d
dtp D Wp
(9)
gives then the temporal changes in this probability. A sim-
ilar equation with the temporal derivative changed to d/dn
describes the n-dependence for the probabilities pi;n in
a random walk as a function of the number of steps, pro-
vided n is large enough to be considered as a continuous
variable. The matrix W describes the transition probabili-
ties between the nodes of the lattice or network. The non-
diagonal elements of the corresponding matrix are wij, the
transition probabilities from site i to site j per unit time

Anomalous Diffusion on Fractal Networks
A
317
or in one step. The diagonal elements are the sums of
all non-diagonal elements in the corresponding column
taken with the opposite sign: wii D  P
j wji, which rep-
resents the probability conservation law. The situation of
unbiased random walks corresponds to a symmetric ma-
trix W: wi j D wji. Considering homogeneous networks
and putting all nonzero wij to unity, one sees that the dif-
ference operator represented by each line of the matrix is
a symmetric diﬀerence approximation to a Laplacian.
The diﬀusion problem is intimately connected with
the problem of conductivity of a network, as described by
Kirchhoﬀ’s laws. Indeed, let us consider a stationary sit-
uation in which the particles enter the network at some
particular site A at constant rate, say, one per unit time,
and leave it at some site B (or a given set set of sites B). Let
us assume that after some time a stationary distribution of
the particles over the lattice establishes, and the particles’
concentration on the sites will be described by the vector
proportional to the vector of stationary probabilities satis-
fying
Wp D 0 :
(10)
Calculating the probabilities p corresponds then formally
to calculating the voltages on the nodes of the resistor net-
work of the same geometry under given overall current
using the Kirchhoﬀ’s laws. The conductivities of resistors
connecting nodes i and j have to be taken proportional
to the corresponding transition probabilities wij. The con-
dition given by Eq. (10) corresponds then to the second
Kirchhoﬀ’s law representing the particle conservation (the
fact that the sum of all currents to/from the node i is zero),
and the fact that the corresponding probability current is
proportional to the probability diﬀerence is replaced by
the Ohm’s law. The ﬁrst Kirchhoﬀ’s law follows from the
uniqueness of the solution. Therefore calculation of the di-
mension of a walk can be done by discussing the scaling of
conductivity with the size of the fractal object. This typi-
cally follows a power law.
As an example let us consider a Sierpinski lattice and
calculate the resistance between the terminals A and B (de-
picted by thick wires outside of the triangle in Fig. 2c) of
a fractal of the next generation, assuming that the con-
ductivity between the corresponding nodes of the lattice
of the previous generation is R D 1. Using the triangle-
star transformation known in the theory of electric cir-
cuits, i. e. passing to the structure shown in Fig. 2c by
thick lines inside the triangle, with the conductivity of each
bond r D 1/2 (corresponding to the same value of the con-
ductivity between the terminals), we get the resistance of
a renormalized structure R0 D 5/3. Thus, the dependence
of R on the spacial scale L of the object is R / L with
 D log(5/3)/ log 2. The scaling of the conductivity G is
correspondingly G / L.
Using the ﬂow-over-population approach, known in
calculation of the mean ﬁrst passage time in a system, we
get I / N/hti, where I is the probability current through
the system (the number of particles entering A per unit
time), N is the overall stationary number of particles
within the system and hti is the mean time a particle
spends inside the system, i. e. the mean ﬁrst passage time
from A to B. The mean number of particles inside the sys-
tem is proportional to a typical concentration (say to the
probability pA to ﬁnd a particle at site A for the given cur-
rent I) and to the number of sites. The ﬁrst one, for a given
current, scales as the system’s resistivity, pA / R / L,
and the second one, clearly, as Ldf where L is the system’s
size. On the other hand, the mean ﬁrst passage time scales
according to hti / Ldw (this time corresponds to the typi-
cal number of steps during which the walk transverses an
Euclidean distance L), so that
dw D df C  :
(11)
Considering the d-dimensional generalizations of Sier-
pinski gaskets and using analogous considerations we get
 D log[(d C 3)/(d C 1)]/ log 2. Combining this with the
fractal dimension df D log(d C 1)/ log 2 of a gasket gives
us for the dimension of the walks dw D log(d C 3)/ log 2.
The relation between the scaling exponent of the con-
ductivity and the dimension of a random walk on a fractal
system can be used in the opposite way, since dw can easily
be obtained numerically for a whatever structure. On the
other hand, the solutions of the Kirchhoﬀ’s equations on
complex structures (i. e. the solution of a large system of
algebraic equations), which is typically achieved using re-
laxation algorithms, is numerically much more involved.
We note that the expression for  can be rewritten as
 D df(2/ds  1), where ds is the spectral dimension of the
network, which will be introduced in Subsect. “The Spec-
tral Dimension”. The relation between the walk dimension
and this new quality therefore reads
dw D 2df
ds
:
(12)
A diﬀerent discussion of the same problem based on
the Einstein’s relation between diﬀusion coeﬃcient and
conductivity, and on crossover arguments can be found
in [10,20].
The Spectral Dimension
In mathematical literature spectral dimension is deﬁned
through the return probability of the random walk, i. e. the

318 A
Anomalous Diffusion on Fractal Networks
probability to be at the origin after n steps or at time t. We
consider a node on a network and take it to be an origin of
a simple random walk. We moreover consider the proba-
bility P(0; t) to be at this node at time t (a return probabil-
ity). The spectral dimension deﬁnes then the asymptotic
behavior of this probability for n large:
P(0; t) / tds/2 ;
for t ! 1 :
(13)
The spectral dimension ds is therefore exactly a quantity
substituting the Euclidean dimension d is Eq. (1). In the
statistical case one should consider an average of p(t) over
the origin of the random walk and over the ensemble of
the corresponding graphs.
For ﬁxed graphs the spectral dimension and the fractal
(Hausdorﬀ) dimension are related by
2df
1 C df
 ds  df ;
(14)
provided both exist [5]. The same relation is also shown
true for some random geometries, see e. g. [6,7] and refer-
ences therein. The bounds are optimal, i. e. both equalities
can be realized in some structures.
The connection between the walk dimension dw and
the spectral dimension ds of a network can be easily ra-
tionalized by the following consideration. Let the random
walk have a dimension dw, so that after time t the position
of the walker can be considered as more or less homoge-
neously spread within the spatial region of the linear size
R / n1/dw or R / t1/dw, with the overall number of nodes
N / Rdf / tdf/dw. The probability to be at one particular
node, namely at 0, goes then as 1/N, i. e. as tdf/dw. Com-
paring this with the deﬁnition of the spectral dimension,
Eq. (13) we get exactly Eq. (12). The lower bound in the
inequality Eq. (14) follows from Eq. (13) and from the the
observation that the value of  in Eq. (11) is always smaller
or equal to one (the value of  D 1 would correspond to
a one-dimensional wire without shunts). The upper bound
can be obtained using Eq. (12) and noting that the walks’
dimension never gets less than 2, its value for the regular
network: the random walk on a whatever structure is more
compact than one on a line.
We note that the relation Eq. (12) relies on the assump-
tion that the spread of the walker within the r-domain
is homogeneous, and needs reinterpretation for strongly
anisotropic structures like combs, where the spread in
teeth and along the spine are very diﬀerent (see e. g. [2]).
For the planar comb with inﬁnite teeth df D 2, and ds as
calculated through the return time is ds D 3/2, while the
walk’s dimension (mostly determined by the motion in
teeth) is dw D 2, like for random walks in Euclidean space.
The inequality Eq. (14) is, on the contrary, universal.
Let us discuss another meaning of the spectral dimen-
sion, the one due to which it got its name. This one has to
do with the description of random walks within the master
equation scheme. From the spectral (Laplace) representa-
tion of the solution of the master equation, Eq. (9), we can
easily ﬁnd the probability P(0; t) that the walker starting at
site 0 at t D 0 is found at the same site at time t. This one
reads
P(0; t) D
1
X
iD1
ai exp(i t) ;
where i is the ith eigenvalue of the matrix W and ai is
the amplitude of its ith eigenvector at site 0. Considering
the lattice as inﬁnite, we can pass from discrete eigenvalue
decomposition to a continuum
P(0; t) D
Z 1
0
N ()a() exp(t)d :
For long times, the behavior of P(0; t) is dominated by the
behavior of N () for small values of . Here N () is the
density of states of a system described by the matrix W.
The exact forms of such densities are well known for many
Euclidean lattices, since the problem is equivalent to the
calculating of spectrum in tight-binding approximation
used in the solid state physics. For all Euclidean lattices
N () / d/21 for  ! 0, which gives us the forms of fa-
mous van Hove singularities of the spectrum. Assuming
a() to be nonsingular at  ! 0, we get P(0; t) / td/2.
For a fractal structure, the value of d changed for the one of
the spectral dimension ds. This corresponds to the density
of states N () / ds/21 which describes the properties of
spectrum of a fractal analog of a Laplace operator. The di-
mension ds is also often called fracton dimension of the
structure, since the corresponding eigenvectors of the ma-
trix (corresponding to eigenstates in tight-binding model)
are called fractons, see e. g. [16].
In the examples of random walks on quenched poly-
mer chains the dimension dw was twice the fractal dimen-
sion of the underlying structures, so that the spectral di-
mension of the corresponding structures (without inter-
sections) was exactly 1 just like their topological dimen-
sion.
The spectral dimension of the network governs also
the behavior of the mean number of diﬀerent sites vis-
ited by the random walk. A random walk of n steps hav-
ing a property of compact visitation typically visits all
sites within the radius of the order of its typical displace-
ment Rn / n1/dw. The number of these sites is Sn / Rdfn
where df is the fractal dimension of the network, so that
Sn / nds/2 (provided ds  2, i. e. provided the random

Anomalous Diffusion on Fractal Networks
A
319
walk is recurrent and shows compact visitation). The spec-
tral dimension of a structure plays important role in many
other applications [16]. This and many other properties of
complex networks related to the spectrum of W can of-
ten be easier obtained by considering random walk on the
lattice than by obtaining the spectrum through direct di-
agonalization.
Percolation Clusters
Percolation clusters close to criticality are one of the most
important examples of fractal networks. The properties of
these clusters and the corresponding fractal and spectral
dimensions are intimately connected to the critical indices
in percolation theory.
Thus, simple crossover arguments show that the frac-
tal dimension of an incipient inﬁnite percolation clus-
ter in d  6 dimensions is df D d  ˇ/, where ˇ is the
critical exponent of the density of the inﬁnite cluster,
P1 / (p  pc)ˇ and  is the critical exponent of the cor-
relation length,  / jp  pcj, and stagnates at df D 4 for
d > 6, see e. g. [23].
On the other hand, the critical exponent t, describ-
ing the behavior of the resistance of a percolation sys-
tem close to percolation threshold,  / (p  pc)t, is con-
nected with the exponent  via the following crossover
argument. The resistance of the system is the one of the
inﬁnite cluster. For p > pc this cluster can be considered
as fractal at scales L <  and as homogeneous at larger
scales. Our value for the fractal dimension of the cluster
follows exactly from this argument by matching the den-
sity of the cluster P(L) / L(dfd) at L D  / (p  pc)
to its density P1 / (p  pc)ˇ at larger scales. The in-
ﬁnite cluster for p > pc is then a dense regular assem-
bly of subunits of size  which on their turn are frac-
tal. If the typical resistivity of each fractal subunit is
R then the resistivity of the overall assembly goes as
R / R(L/)2d. This is a well-known relation showing
that the resistivity of a wire grows proportional to its
length, the resistivities of similar ﬂat ﬁgures are the same
etc. This relation holds in the homogeneous regime. On
the other hand, in the fractal regime RL / L, so that over-
all R / Cd2 / (p  pc)(Cd2) giving the value of
the critical exponent t D ( C d  2). The value of  can
in its turn be expressed through the values of spectral and
fractal dimensions of a percolation cluster.
The spectral dimension of the percolation cluster is
very close to 4/3 in any dimension larger then one. This
surprising ﬁnding lead to the conjecture by Alexander and
Orbach that this value of ds D 4/3 might be exact [1] (it is
exact for percolation systems in d  6 and for trees). Much
eﬀort was put into proving or disproving the conjecture,
see the discussion in [10]. The latest, very accurate simula-
tions of two-dimensional percolation by Grassberger show
that the conjecture does not hold in d D 2, and the predic-
tion ds D 4/3 is oﬀby around 2% [9]. In any case it can be
considered as a very useful mnemonic rule.
Anomalous diﬀusion on percolation clusters corre-
sponds theoretically to the most involved case, since it
combines all mechanisms generating diﬀusion anomalies.
The inﬁnite cluster of the percolation system consists of
a backbone, its main current-carrying structure, and the
dangling ends (smaller clusters on all scales attached to the
backbone through only one point). The anomalous diﬀu-
sion on the inﬁnite cluster is thus partly caused by trapping
in this dangling ends. If one considers a shortest (chemi-
cal) way between the two nodes on the cluster, this way is
tortuous and has fractal dimension larger than one.
The Role of Finite Clusters
The exponent t of the conductivity of a percolation system
is essentially the characteristic of an inﬁnite cluster only.
Depending on the physical problem at hand, one can con-
sider the situations when only the inﬁnite cluster plays the
role (like in the experiments of [12], where only the inﬁ-
nite cluster is ﬁlled by ﬂuid pressed through the bound-
ary of the system), and situations when the excitations can
be found in inﬁnite as well as in the ﬁnite clusters, as it is
the case for optical excitation in mixed molecular crystals,
a situation discussed in [24].
Let us concentrate on the case p D pc. The structure of
large but ﬁnite clusters at lower scales is indistinguishable
from those of the inﬁnite cluster. Thus, the mean squared
displacement of a walker on a cluster of size L (one with
N / Ldf sites) grows as R2(t) / t2/dw until is stagnates at
the value of R of the order of the cluster’s size, R2 / N2/df.
At a given time t we can subdivide all clusters into two
classes: those whose size is small compared to t1/dw (i. e.
the ones with N < tds/2), on which the mean square dis-
placement stagnates, and those of the larger size, on which
it still grows:
R2(t) /
(
t2/dw ;
t1/dw < N1/ ff
N2/df ;
otherwise:
The characteristic size Ncross(t) corresponds to the
crossover between these two regimes for a given time t.
The probability that a particle starts at a cluster of
size N is proportional to the number of its sites and goes
as P(N) / Np(N) / N1 with  D (2d  ˇ)/(d  ˇ),
see e. g. [23]. Here p(N) is the probability to ﬁnd a cluster

320 A
Anomalous Diffusion on Fractal Networks
of N sites among all clusters. The overall mean squared
displacement is then given by averaging over the corre-
sponding cluster sizes:
hr2(t)i /
Ncross(t)
X
ND1
N2/dfN1 C
1
X
Ncross(t)
t2/dwN1 :
Introducing the expression for Ncross(t), we get
hr2(t)i / t2/dwC(2)df/dw D t(1/dw)(2dfˇ/) :
A similar result holds for the number of distinct sites vis-
ited, where one has to perform the analogous averaging
with
S(t) /
(
tds/2 ;
t1/dw < N1/ ff
N ;
otherwise
giving S(t) / t(ds/2)(1ˇ/df). This gives us the eﬀective
spectral dimension of the system ˜ds D (ds/2)(1  ˇ/df).
Scaling of PDF and Diffusion Equations
on Fractal Lattices
If the probability density of the displacement of a particle
on a fractal scales, the overall scaling form of this displace-
ment can be obtained rather easily: Indeed, the typical dis-
placement R during the time t goes as r / t1/dw, so that the
corresponding PDF has to scale as
P(r; t) D rdfd
tds/2 f

r
t1/dw

:
(15)
The prefactor takes into account the normalization of the
probability density on a fractal structure (denominator)
and also the scaling of the density of the fractal struc-
ture in the Euclidean space (enumerator). The simple scal-
ing assumes that all the moments of the distance scale
in the same way, so that hrki D
R
P(r; t)rkddrk
D
const(k)tk/dw. The overall scaling behavior of the PDF was
conﬁrmed numerically for many fractal lattices like Sier-
pinski gaskets and percolation clusters, and is corrobo-
rated in many other cases by the coincidence of the val-
ues of dw obtained via calculation of the mean squared
displacement and of the mean ﬁrst passage time (or resis-
tivity of the network). The existence of the scaling form
leads to important consequences e. g. for quantiﬁcation of
anomalous diﬀusion in biological tissues by characterizing
the diﬀusion-time dependence of the magnetic resonance
signal [17], which, for example, allows for diﬀerentiation
between the healthy and the tumor tissues. However, even
in the cases when the scaling form Eq. (15) is believed to
hold, the exact form of the scaling function f (x) is hard to
get; the question is not yet resolved even for simple regular
fractals.
For applications it is often interesting to have a kind of
a phenomenological equation roughly describing the be-
havior of the PDF. Such an approach will be analogous
to putting down Richardson’s equation for the turbulent
diﬀusion. Essentially, the problem here is to ﬁnd a cor-
rect way of averaging or coarse-graining the microscopic
master equation, Eq. (9), to get its valid continuous limit
on larger scales. The regular procedure to do this is un-
known; therefore, several phenomenological approaches
to the problem were formulated.
We are looking for an equation for the PDF of the
walker’s displacement from its initial position and assume
the system to be isotropic on the average. We look for
an analog of the classical Fick’s equation,
@
@t P(r; t) D
rK(r)rP(r; t) which in spherical coordinates and for
K D K(r) takes the form of Eq. (3):
@
@t P(r; t) D
1
rd1
@
@r

rd1K(r) @
@r P(r; t)

:
On fractal lattices one changes from the Euclidean di-
mension of space to the fractal dimension of the lattice
df, and takes into account that the eﬀective diﬀusion co-
eﬃcient K(r) decays with distance as K(r) ' Kr with
 D dw  2 to capture the slowing down of anomalous
diﬀusion on a fractal compared to the Euclidean lattice
situation. The corresponding equation put forward by
O’Shaughnessy and Procaccia [18] reads:
@
@t P(r; t) D K
1
rdf1
@
@r

rdfdwC1 @
@r P(r; t)

:
(16)
This equation was widely used in description of anoma-
lous diﬀusion on fractal lattices and percolation clusters,
but can be considered only as a rough phenomenological
approximation. Its solution
P(r; t) D Ardfd
tds/2 exp
 
B rdw
t
!
;
(with B D 1/Kd2
w and A being the corresponding nor-
malization constant) corresponds exactly to the type of
scaling given by Eq. (15), with the scaling function
f (x) D exp(xdw). This behavior of the scaling function
disagrees e. g. with the results for random walks on poly-
mer chains, for which case we had f (x) D exp(x) with
 D (1  1/dw)1 for large enough x.
In literature, several other proposals (based on plausi-
ble assumptions but to no extent following uniquely from

Anomalous Diffusion on Fractal Networks
A
321
the models considered) were made, taking into account
possible non-Markovian nature of the motion. These are
the fractional equations of the type
@1/dw
@t1/dw P(r; t) D K1
1
r(ds1)/2
@
@r
h
r(ds1)/2P(r; t)
i
; (17)
resembling “half” of the diﬀusion equation and containing
a fractional derivative [8], as well as the “full” fractional
diﬀusion equation
@2/dw
@t2/dw P(r; t) D K2
1
rds1
@
@r

rds1 @
@r P(r; t)

;
(18)
as proposed in [15]. All these equation are invariant under
the scale transformation
t ! t ;
r ! 1/dwr ;
and lead to PDFs showing correct overall scaling prop-
erties, see [19]. None of them reproduces correctly the
PDF of displacements on a fractal (i. e. the scaling function
f (x)) in the whole range of distances. Ref. [19] compar-
ing the corresponding solutions with the results of simu-
lation of anomalous diﬀusion an a Sierpinski gasket shows
that the O’Shaughnessy–Procaccia equation, Eq. (16), per-
forms the best for the central part of the distribution
(small displacements), where Eq. (18) overestimates the
PDF and Eq. (17) shows an unphysical divergence. On
the other hand, the results of Eqs. (17) and (18) repro-
duce equally well the PDF’s tail for large displacements,
while Eq. (16) leads to a PDF decaying considerably faster
than the numerical one. This fact witnesses favor of strong
non-Markovian eﬀects in the fractal diﬀusion, however,
the physical nature of this non-Markovian behavior ob-
served here in a fractal network without dangling ends, is
not as clear as it is in a comb model and its more com-
plex analogs. Therefore, the question of the correct equa-
tion describing the diﬀusion in fractal system (or diﬀerent
correct equations for the corresponding diﬀerent classes
of fractal systems) is still open. We note also that in some
cases a simple fractional diﬀusion equation (with the cor-
responding power of the derivative leading to the correct
scaling exponent dw) gives a reasonable approximation to
experimental data, as the ones of the model experiment
of [12].
Further Directions
The physical understanding of anomalous diﬀusion due to
random walks on fractal substrates may be considered as
rather deep and full although it does not always lead to
simple pictures. For example, although the spectral dimen-
sion for a whatever graph can be rather easily calculated, it
is not quite clear what properties of the graph are respon-
sible for its particular value. Moreover, there are large dif-
ferences with respect to the degree of rigor with which dif-
ferent statements are proved. Mathematicians have recog-
nized the problem, and the ﬁeld of diﬀusion on fractal net-
works has become a fruitful ﬁeld of research in probability
theory. One of the examples of recent mathematical devel-
opment is the deep understanding of spectral properties of
fractal lattices and a proof of inequalities like Eq. (14).
A question which is still fully open is the one of the de-
tailed description of the diﬀusion within a kind of (gener-
alized) diﬀusion equation. It seems clear that there is more
than one type of such equations depending on the concrete
fractal network described. However even the classiﬁcation
of possible types of such equations is still missing.
All our discussion (except for the discussion of the role
of ﬁnite clusters in percolation) was pertinent to inﬁnite
structures. The recent work [4] has showed that ﬁnite frac-
tal networks are interesting on their own and opens a new
possible direction of investigations.
Bibliography
Primary Literature
1. Alexander S, Orbach RD (1982) Density of states on fractals–
fractons. J Phys Lett 43:L625–L631
2. Bertacci D (2006) Asymptotic behavior of the simple random
walk on the 2-dimensional comb. Electron J Probab 45:1184–
1203
3. Christou A, Stinchcombe RB (1986) Anomalous diffusion on
regular and random models for diffusion-limited aggregation.
J Phys A Math Gen 19:2625–2636
4. Condamin S, Bénichou O, Tejedor V, Voituriez R, Klafter J (2007)
First-passage times in complex scale-invariant media. Nature
450:77–80
5. Coulhon T (2000) Random Walks and Geometry on Infinite
Graphs. In: Ambrosio L, Cassano FS (eds) Lecture Notes on
Analysis on Metric Spaces. Trento, CIMR, (1999) Scuola Nor-
male Superiore di Pisa
6. Durhuus B, Jonsson T, Wheather JF (2006) Random walks on
combs. J Phys A Math Gen 39:1009–1037
7. Durhuus B, Jonsson T, Wheather JF (2007) The spectral dimen-
sion of generic trees. J Stat Phys 128:1237–1260
8. Giona M, Roman HE (1992) Fractional diffusion equation
on fractals – one-dimensional case and asymptotic-behavior.
J Phys A Math Gen 25:2093–2105; Roman HE, Giona M, Frac-
tional diffusion equation on fractals – 3-dimensional case and
scattering function, ibid., 2107–2117
9. Grassberger P (1999) Conductivity exponent and backbone di-
mension in 2-d percolation. Physica A 262:251–263
10. Havlin S, Ben-Avraham D (2002) Diffusion in disordered media.
Adv Phys 51:187–292
11. Klafter J, Sokolov IM (2005) Anomalous diffusion spreads its
wings. Phys World 18:29–32

322 A
Applications of Physics and Mathematics to Social Science, Introduction to
12. Klemm A, Metzler R, Kimmich R (2002) Diffusion on random-
site percolation clusters: Theory and NMR microscopy experi-
ments with model objects. Phys Rev E 65:021112
13. Metzler R, Klafter J (2000) The random walk’s guide to anoma-
lous diffusion: a fractional dynamics approach. Phys Rep
339:1–77
14. Metzler R, Klafter J (2004) The restaurant at the end of the ran-
dom walk: recent developments in the description of anoma-
lous transport by fractional dynamics. J Phys A Math Gen
37:R161–R208
15. Metzler R, Glöckle WG, Nonnenmacher TF (1994) Fractional
model equation for anomalous diffusion. Physica A 211:13–24
16. Nakayama T, Yakubo K, Orbach RL (1994) Dynamical properties
of fractal networks: Scaling, numerical simulations, and physi-
cal realizations. Rev Mod Phys 66:381–443
17. Özarslan E, Basser PJ, Shepherd TM, Thelwall PE, Vemuri BC,
Blackband SJ (2006) Observation of anomalous diffusion in ex-
cised tissue by characterizing the diffusion-time dependence
of the MR signal. J Magn Res 183:315–323
18. O’Shaughnessy B, Procaccia I (1985) Analyticalsolutions for dif-
fusion on fractal objects. Phys Rev Lett 54:455–458
19. Schulzky C, Essex C, Davidson M, Franz A, Hoffmann KH
(2000) The similarity group and anomalous diffusion equa-
tions. J Phys A Math Gen 33:5501–5511
20. Sokolov IM (1986) Dimensions and other geometrical critical
exponents in the percolation theory. Usp Fizicheskikh Nauk
150:221–255 (1986) translated in: Sov. Phys. Usp. 29:924
21. Sokolov IM, Klafter J (2005) From diffusion to anomalous dif-
fusion: A century after Einstein’s Brownian motion. Chaos
15:026103
22. Sokolov IM, Mai J, Blumen A (1997) Paradoxical diffusion
in chemical space for nearest-neighbor walks over polymer
chains. Phys Rev Lett 79:857–860
23. Stauffer D (1979) Scaling theory of percolation clusters. Phys
Rep 54:1–74
24. Webman I (1984) Diffusion and trapping of excitations on frac-
tals. Phys Rev Lett 52:220–223
Additional Reading
The present article gave a brief overview of what is known about
the diffusion on fractal networks, however this overview is far from
covering all the facets of the problem. Thus, we only discussed
unbiased diffusion (the effects of bias may be drastic due to e. g.
stronger trapping in the dangling ends), and considered only the
situations in which the waiting time at all nodes was the same (we
did not discuss e. g. the continuous-time random walks on fractal
networks), as well as left out of attention many particular systems
and applications. Several review articles can be recommended as
a further reading, some of them already mentioned in the text.
One of the best-known sources is [10] being a reprint of the text
from the “sturm und drang” period of investigation of fractal ge-
ometries. A lot of useful information on random walks models in
general an on walks on fractals is contained in the review by Haus
and Kehr from approximately the same time. General discussion
of the anomalous diffusion is contained in the work by Bouchaud
and Georges. The classical review of the percolation theory is given
in the book of Stauffer and Aharony. Some additional information
on anomalous diffusion in percolation systems can be found in the
review by Isichenko. A classical source on random walks in disor-
dered systems is the book by Hughes.
Haus JW, Kehr K (1987) Diffusion in regular and disordered lattices.
Phys Rep 150:263–406
Bouchaud JP, Georges A (1990) Anomalous diffusion in disordered
media – statistical mechanisms, models and physical applica-
tions. Phys Rep 195:127–293
Stauffer D, Aharony A (2003) Introduction to Percolation Theory.
Taylor & Fransis, London
Isichenko MB (1992) Percolation, statistical topography, and trans-
port in random-media. Rev Mod Phys 64:961–1043
Hughes BD (1995) Random Walks and random Environments. Ox-
ford University Press, New York
Applications of Physics and
Mathematics to Social Science,
Introduction to
ANDRZEJ NOWAK1, URSZULA STRAWI ´NSKA2
1 Institute for Social Studies, University of Warsaw,
Warsaw, Poland
2 Warsaw School for Social Psychology, Warsaw, Poland
The human mind is often regarded as the most complex
structure in the universe. If anything could be argued to
be of higher complexity it is a collection of interacting
minds: social groups and societies. Kluver argues that so-
cial cognitive complexity (see ▷Social Cognitive Com-
plexity) stems from the fact that such systems consist of
multiple (at least two – social and cognitive) levels that are
constantly at ﬂux due to the multiple inﬂuences both be-
tween and within levels.
The complexity of the subject matter of the social sci-
ences has made adequate description using the traditional
models of mathematics and physics diﬃcult. For a long
time it has been argued that tools of mathematics and
physics are not adequate for social processes and therefore
science had to proceed along two independent fronts that
seemed irreconcilable (Snow, 1993). One approach, epito-
mized by such disciplines as sociology and anthropology,
was characterized by deep insights into the nature of psy-
chological social phenomena. These insights reﬂected so-
cially constructed metrics and meanings that emphasized
the contextual aspects of human experience. In the other
scientiﬁc culture, epitomized in the physical sciences by
precise measurement, qualitative laws and formal models
deﬁned the paradigm within which the invariant and gen-
eral features of phenomena are understood.
The bridge between these two approaches consisted
mainly of statistical models of psychological and social
phenomena and models based on the assumption that ra-
tional choice underlies human behavior. The discoveries
in the late 70s and 80s of the physical and mathematical
sciences have produced a profound change in the social

Applications of Physics and Mathematics to Social Science, Introduction to
A
323
sciences by providing concepts and tools that integrate the
deep understanding inherent in social sciences with the
precision of mathematics and physics. This perspective has
provided solutions for many apparent paradoxes of social
sciences, such as how altruism can emerge in self-centered
individuals. It has led to surprising insights, such as the
realization that the avoidance by individuals of being in
the minority leads to full segregation at the level of social
groups.
The core of the complexity approach into social do-
main is the application of mathematics and physics. These
applications, described and explained by Solomon and
Stauﬀer (see ▷Applications of Physics and Mathemat-
ics to Social Science, Introduction to), are often based on
the Ising model, where a binary state of an element cor-
responds to individual opinions (e. g. yes-no opinions),
choices (e. g. cooperate or compete), or behaviors. The
binary representations of individuals are arranged into
structures such as a grid structure of cellular automata or
networks to represent the structure of interactions of so-
cial groups and societies. The characteristics of every indi-
vidual, such as opinions, depend on “neighbors”: adjacent
cells in a lattice, or linked nodes in a network. The tools
of statistical physics, especially the mean ﬁeld approach,
may be used to analyze dynamics of such systems. The
dynamics of such a system is often characterized by dra-
matic changes, phase transitions in response to small and
continuous changes in the factors that inﬂuence the sys-
tem (such as percolation). Physical models of social pro-
cesses, described by Slanina (see ▷Social Processes, Phys-
ical Models of) concentrate on the link between the micro
(individual) and the macro (societies, social groups, orga-
nizations) levels of description of social reality. This ap-
proach can explain how cooperation emerges among self-
centered individuals, how social classes are formed, and
explains properties of behaviors stemming from bounded
conﬁdence models.
In models of opinion dynamics explained by Stauﬀer
(see ▷Opinion Dynamics and Sociophysics), individuals
either move in the social space, usually seeking others with
similar opinions (Schelling’s model), or change their opin-
ions in a way that depends on the opinions of the inter-
acting partners. In a Volter model, for example, everyone
takes an opinion of a random neighbor, in the majority
model one adopts the opinion of the majority.
The primary tool for analyzing dynamics of complex-
ity models of social processes are computer simulations.
Simulation models described by Troitzsch (see ▷Social
Processes, Simulation Models of) allow one to discover dy-
namical properties of models that, due to their complexity
and nonlinearity of interactions, may be unsolvable using
analytical tools. Most computer simulations utilize agent-
based models. In such models individuals are represented
as agents who interact with other agents following the
rules speciﬁed by the model. Models taken from physics
assume extremely simple agents who are often represented
by a single binary variable such as an opinion in a posi-
tion in a social space. In such models the rules assumed on
the level of a single agent are usually trivial; however inter-
action between agents leads to the emergence of complex
spatial and temporal patterns on the social level. The rules
specifying the behavior of a single agent may sometimes
become complex. As Conte points out, an approach orig-
inating from computer science also assumes much richer
representation of individuals (see ▷Rational, Goal-Ori-
ented Agents).
Complexity models of social processes are very dif-
ferent from traditional mathematical models. Traditional
mathematical and physical models describe the tempo-
ral evolution of the model in terms of equations involv-
ing the global characteristics of systems. Yaari, Stauﬀer
and Solomon discuss intermittency and localization, cen-
tral properties of the dynamics of complexity-based mod-
els (see ▷Intermittency and Localization). Most classical
equations describing social and demographic processes,
such as the logistic equation of growth, acquire unexpected
and fascinating properties when they deﬁne local rather
than global dynamics. These models produce a variety of
behaviors that are observed in many real-life systems, for
example in economics, sociology, biology, and ecology.
According to Kelso and Oullier coordination is one of the
pivotal aspects of emergent social processes (see ▷Social
Coordination, from the Perspective of Coordination Dy-
namics). Individuals not only act alone but they coordi-
nate in the achievement of their goals. Understanding how
this coordination occurs and what forms it may take allows
us to gain insight into how individuals are combined into
dyads and higher level social structures.
White suggests that human behavior may be analyzed
at diﬀerent levels (see ▷Human Behavior, Dynamics of).
Other people provide the critical context for actions of
individuals. Moreover, the actions of individuals change
the structure of society, for example by creating connec-
tions between cooperating individuals. Cohesion provides
an important characteristic for group processes. Networks
organize individuals into social groups and societies where
the cohesion is an important characteristic of the resul-
tant aggregates. Whereas the individual level of descrip-
tion may involve rules, the dynamics of aggregates may be
described by equations.
Processes occurring in organizations, social groups
and societies to high degree depend on structure of in-

324 A
Applications of Physics and Mathematics to Social Science, Introduction to
teractions among individuals. Reed–Tsochas explains that
the structure of human interactions and human relations
may be described as a complex network where individuals
correspond to nodes and links to social context or social
relations. Most networks existing in nature as well as in so-
cieties have several common properties such as scale free
distribution: there is a small number of nodes with a very
high number of connections (degree) and a large number
of nodes with a small number of connections. Networks
tend to have a small world structure. In societies it is pos-
sible to go from one node to most of the others by travers-
ing no more than six links. Autoaﬃliate character means
that similar nodes tend to be connected. There is a high
chance of being connected for nodes that are similar than
for nodes that are dissimilar. The network formalism of-
fers the very tool to analyze structure of complex relations
among individuals, groups and organizations in a society.
These network properties have important consequences
for the dynamics of processes occurring in social groups
and organizations.
Complexity models are applied to diﬀerent levels and
within levels to diﬀerent domains of social processes.
Complexity models based in the neurophysiology of the
human brain have provided the ﬁrst precise model of the
functioning of consciousness. As described by Edelman
and Seth, momentary progressive coordination of activ-
ity of selective neuronal groups underlies the phenomena
of consciousness (see ▷Consciousness and Complexity).
This precise complexity-based model suggests that con-
sciousness is not a property of a speciﬁc brain region but
an emergent, collective phenomenon. This model explains
a variety of experimental results from psychology. Al-
though aﬀective responses are produced by speciﬁc brain
regions, they may be linked to the dynamical properties
of patterns of activity of the cognitive system. In partic-
ular, emotions, as described by Winkielman and Huber
in ▷Dynamics and Evaluation: The Warm Glow of Pro-
cessing Fluency, depend on the dynamics of perception.
Positive aﬀect is elicited by the ﬂuent processing of stimuli
resulting from high coherence in the processing mecha-
nisms, whereas negative emotions result from low ﬂuency.
As Port points out, the dynamics of language
(see ▷Dynamics of Language) can be discussed either on
the level of an individual or a society. At the individual
level language can be understood as the skills of individ-
uals processing speech in real time, at the social level as
a common pattern of the speech of speakers in a commu-
nity.
Rational rules governing the interaction of individ-
ual agents can produce moral dynamics in which the
norms of a society evolve among individuals who are
linked by social interdependence. In his article, Hegsel-
man explains how rational choices may underlie the emer-
gence of norms and the evolution of solidarity and trust
(see ▷Moral Dynamics).
Applications of complexity to social psychology in-
volve dynamical models of various individual and group-
level processes described by Vallacher (see ▷Social Psy-
chology, Applications of Complexity to). The attractor is
the key concept for ﬁnding stability in the constant dy-
namics of all psychological processes. Processes oscillate
around stable points which function as attractors for their
dynamics. Dynamical minimalism describes the core of
the approach of complexity – how to build very sim-
ple models of very complex psychological and social pro-
cesses. At the abstract level many aspects of individual in-
teractions may be described in terms of social inﬂuence.
The intimate link between complexity and the dynam-
ics of psychological processes is most clearly seen in de-
velopment. The concept of development may be captured
in terms of the increasing complexity of the human sys-
tem. Van Geert explains one of the main contributions
of the complexity approach to development, namely the
realization that mental and behavioral phenomena result,
not from individual factors, but from the joint impact of
many factors acting at diﬀerent time scales (see ▷Devel-
opment, Complex Dynamic Systems of). The question of
time scales goes beyond individuals to generations when
one considers similarities between development and evo-
lution. Evolutionary principles as applied to human sys-
tems, described by Bjorklund and Gardiner, deﬁne the
subject matter of memetics (see ▷Development, Evolu-
tion, and the Emergence of Novel Behavior). Human opin-
ions, norms and beliefs decide on the ﬁtness of a given in-
dividual to the social environment. Successful individuals,
because of the higher ﬁtness, have a higher chance of trans-
mitting this property to others. Through such processes
human modeling occurs. From the evolutionary point of
view, the characteristics of individuals (such as opinions,
norms, values) may be treated as genes and the process of
transmission of these properties can be captured in terms
of reproduction, which depends on ﬁtness. As explained
by Heylighen and Chielens, evolution of culture can sim-
ply be modeled by genetic algorithms resembling evolu-
tionary processes and the approach taken by memetics
(see ▷Evolution of Culture, Memetics).
The complexity approach provides principles to de-
scribe the evolution of cities in time, and explain pat-
terns of urban dynamics (see ▷Cities as Complex Sys-
tems: Scaling, Interaction, Networks, Dynamics and Ur-
ban Morphologies by Batty). Comprehensive models of
cities show the interrelation between various processes in

Applications of Physics and Mathematics to Social Science, Introduction to
A
325
the city. Bazzani, Giorgini, and Rambaldi present traﬃc
and crowd dynamics that provided focus for studies con-
cerning the dynamics of urban processes (see ▷Traﬃc and
Crowd Dynamics: The Physics of the City).
Interactions within the organization of multiple peo-
ple, factors, and ideas may be understood in terms of
knowledge economy. As described by Marion in ▷So-
cial Organizations with Complexity Theory: A Dramati-
cally Diﬀerent Lens for the Knowledge Economy, creativ-
ity, adaptability, and learning may be described in terms of
knowledge transfers and transformations within an orga-
nization.
The spread of product adoption may be described by
the diﬀusion model. Barriers in communication between
early adopters and followers may result in a dual mar-
ket phenomenon, which may be analyzed using the tools
of complexity modeling. Goldenberg presents models in
which cellular automata describe the geographical spread
of a product (see ▷Marketing: Complexity Modeling,
Theory and Applications in). The interdisciplinary ap-
proach allows one to develop a universal framework for
modeling market penetration dynamics.
The most precise models of complexity are being de-
veloped to model economic processes. Gallegati deﬁnes
agent-based models as the core of complexity modeling of
economic processes (see ▷Agent Based Models in Eco-
nomics and Complexity). These models are usually char-
acterized by heterogeneity, explicit space, and local in-
teractions. Agents’ behaviors are usually governed by the
principles of bounded rationality in that they have limited
information and therefore limited processing capacity.
Minority game, as described by Zheng (see ▷Minor-
ity Games), provides a formal model of how the ﬁnancial
market functions. In this model, individuals in a group
must make a binary choice. Those who are in the minor-
ity and choose the less popular option win. The interesting
feature of the game is that any winning strategy becomes
a losing strategy as soon as it is adopted by the majority of
the group. It forces constant evolution of the rules govern-
ing the market, where each rule is invalidated as soon as
the majority of the agents discovers it.
Agent-based modeling challenges many assumptions
of neoclassical economics. Moss suggests in his critical
analysis (see ▷Agent Based Modeling and Neoclassical
Economics: A Critical Perspective) that the neoclassical
economy describes an idealized society and does not ap-
ply to any real social system in which economic decisions
are made by heterogeneous interacting agents.
Andersen describes models of investment decision
making in ﬁnance that are constructed by assuming diﬀer-
ent decision rules of agents trying to maximize their proﬁt
(see ▷Investment Decision Making in Finance, Models
of). Researchers try to understand patterns of change in ﬁ-
nancial markets in relation to simple rules of agents trying
to maximize their outcomes. These models allow one to
study typical patterns of market dynamics as the link be-
tween decision rules of agents and the resultant dynamics
of the market indexes.
The dynamics of social and political processes are
highly nonlinear and often abrupt. The state of societies
is shaped to a substantial degree by rare extreme events
rather than by the mass of average processes. Keilis–Borok,
Soloviev, and Lichtman claim that predictability of ex-
treme events in socio-economics and politics (see ▷Ex-
treme Events in Socio-economic and Political Complex
Systems, Predictability of) is thus an important consider-
ation for the prediction of any social, political, and eco-
nomic process. New developments concerning the pre-
dictability of extreme events improve the possibility of pre-
dicting the course of dynamics of various social processes
such as unemployment, recession, surges in homicides,
etc.
In contrast to traditional mathematical and physical
models, the approach of complexity allows us to combine
deep understanding of psychological and social phenom-
ena with the precision of the formal sciences. It provides
a platform for the integration of natural and social sci-
ences. It describes phenomena occurring at higher levels
of reality as emergent eﬀects of the interaction of lower
level elements. It allows, therefore, for the integration of
theories describing various levels of psychological and so-
cial reality, from the level of consciousness to high social
levels such as economies, cities, and societies. In addition
to its heuristic value this approach has a strong integrative
potential also between diﬀerent disciplines of behavioral
and social sciences.
Everyone agrees that psychological and social pro-
cesses display the greatest complexity of all the phenomena
research can encounter. The surprising discovery of the
complexity approach is that extremely complex processes
may be produced by very simple rules and interactions.
From this perspective simplicity is a coin ﬂip of complexity
rather than its opposition. The approach of complexity can
therefore provide for simple, precise explanations of ex-
tremely complex psychological and social processes. The
title of this section could be recast as Finding simplicity in
complex psychological and social processes.
Acknowledgment
Preparation of this article was supported by a CO3 grant
from the European NEST PATHFINDER initiative.

326 A
Artificial Chemistry
Artificial Chemistry
PETER DITTRICH
Department of Mathematics and Computer Science,
Friedrich Schiller University Jena, Jena, Germany
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Basic Building Blocks of an Artiﬁcial Chemistry
Structure-to-Function Mapping
Space
Theory
Evolution
Information Processing
Future Directions
Bibliography
Glossary
Molecular species A molecular species is an abstract
class denoting an ensemble of identical molecules.
Equivalently the terms “species”, “compound”, or just
“molecule” are used; in some speciﬁc context also the
terms “substrate” or “metabolite”.
Molecule A molecule is a concrete instance of a molec-
ular species. Molecules are those entities of an artiﬁ-
cial chemistry that react. Note that sometimes the term
“molecule” is used equivalently to molecular species.
Reaction network A set of molecular species together
with a set of reaction rules. Formally, a reaction net-
work is equivalent to a Petri network. A reaction net-
work describes the stoichiometric structure of a reac-
tion system.
Order of a reaction The order of a reaction is the sum of
the exponents of concentrations in the kinetic rate law
(if given). Note that an order can be fractional. If only
the stoichiometric coeﬃcients of the reaction rule are
given (Eq. (1)), the order is the sum of the coeﬃcients
of the left-hand side species. When assuming mass-ac-
tion kinetics, both deﬁnitions are equivalent.
Autocatalytic set A (self-maintaining) set where each
molecule is produced catalytically by molecules from
that set. Note that an autocatalytic may produce
molecules not present in that set.
Closure A set of molecules A is closed, if no combina-
tion of molecules from A can react to form a molecule
outside A. Note that the term “closure” has been also
used to denote the catalytical closure of an autocat-
alytic set.
Self-maintaining A set of molecules is called self-main-
taining, if it is able to maintain all its constituents.
In a purely autocatalytic system under ﬂow condition,
this means that every molecule can be catalytically pro-
duced by at least one reaction among molecule from
the set.
(Chemical) Organization A closed and self-maintaining
set of molecules.
Multiset A multiset is like a set but where elements can
appear more than once; that is, each element has
a multiplicity, e. g., fa; a; b; c; c; cg.
Definition of the Subject
Artiﬁcial chemistries are chemical-like systems or abstract
models of chemical processes. They are studied in or-
der to illuminate and understand fundamental principles
of chemical systems as well as to exploit the chemical
metaphor as a design principle for information process-
ing systems in ﬁelds like chemical computing or nonlinear
optimization.
An artiﬁcial chemistry (AC) is usually a formal (and,
more seldom, a physical) system that consists of objects
called molecules, which interact according to rules called
reactions. Compared to conventional chemical models,
artiﬁcial chemistries are more abstract in the sense that
there is usually not a one-to-one mapping between the
molecules (and reactions) of the artiﬁcial chemistry to real
molecules (and reactions). An artiﬁcial chemistry aims at
capturing the logic of chemistry rather than trying to ex-
plain a particular chemical system. More formally, an ar-
tiﬁcial chemistry can be deﬁned by a set of molecular
species M, a set of reaction rules R, and speciﬁcations of
the dynamics, e. g., kinetic laws, update algorithm, and ge-
ometry of the reaction vessel.
The scientiﬁc motivation for AC research is to build
abstract models in order to understand chemical-like sys-
tems in all kind of domains ranging from physics, chem-
istry, biology, computer science, and sociology. Abstract
chemical models to study fundamental principles, such
as spatial pattern formation and self-replication, can be
traced back to the beginning of modern computer science
in the 40s [91,97]. Since then a growing number of ap-
proaches have tackled questions concerning the origin of
complex forms, the origin of life itself [19,81], or cellular
diversity [48]. In the same way a variety of approaches for
constructing ACs have appeared, ranging from simple or-
dinary diﬀerential equations [20] to complex individual-
based algebraic transformation systems [26,36,51].

Artificial Chemistry
A
327
The engineering motivation for AC research aims at
employing the chemical metaphor as a programming and
computing paradigm. Approaches can be distinguished
according to whether chemistry serves as a paradigm to
program or construct conventional in-silico information
processing systems [4], Or whether real molecules are
used for information processing like in molecular comput-
ing [13] or DNA computing [2] ▷Cellular Computing,
▷DNA Computing.
Introduction
The term artiﬁcial chemistry appeared around 1990 in the
ﬁeld of artiﬁcial life [72]. However, models that fell under
the deﬁnition of artiﬁcial chemistry appeared also decades
before, such as von Neumann’s universal self-replicating
automata [97]. In the 50s, a famous abstract chemical sys-
tem was introduced by Turing [91] in order to show how
spatial diﬀusion can destabilize a chemical system leading
to spatial pattern formation. Turing’s artiﬁcial chemistries
consist of only a handful of chemical species interacting
according to a couple of reaction rules, each carefully de-
signed. The dynamics is simulated by a partial diﬀerential
equation.
Turing’s model possesses the structure of a conven-
tional reaction kinetics chemical model (Subsect. “The
Chemical Diﬀerential Equation”). However it does not
aim at modeling a particular reaction process, but aims at
exploring how, in principle, spatial patterns can be gen-
erated by simple chemical processes. Turing’s artiﬁcial
chemistries allow one to study whether a particular mech-
anism (e. g., destabilization through diﬀusion) can explain
a particular phenomena (e. g., symmetry breaking and pat-
tern formation).
The example illustrates that studying artiﬁcial chem-
istries is more a synthetic approach. That is, understanding
should be gained through the synthesis of an artiﬁcial sys-
tem and its observation under various conditions [45,55].
The underlying assumption is that understanding a phe-
nomena and how this phenomena can be (re-)created
(without copying it) is closely related [96].
Today’s artiﬁcial chemistries are much more complex
than Turing’s model. They consist of a huge, sometimes
inﬁnite, amount of diﬀerent molecular species and their
reaction rules are deﬁned not manually one by one, but
implicitly (Sect. “Structure-to-Function Mapping”) and
can evolve (Sect. “Evolution”). Questions that are tack-
led are the structure and organization of large chemical
networks, the origin of life, prebiotic chemical evolution,
and the emergence of chemical information and its pro-
cessing.
Introductory Example
It is relatively easy to create a complex, inﬁnitely
sized reaction network. Assume that the set of possible
molecules M are strings over an alphabet. Next, we have to
deﬁne a reaction function describing what happens when
two molecules a1; a2 2 M collide. A general approach is
to map a1 to a machine, operator, or function f D fold(a1)
with f : M ! M [ felasticg. The mapping fold can be de-
ﬁned in various ways, such as by interpreting a1 as a com-
puter program, a Turing machine, or a lambda expression
(Sect. “Structure-to-Function Mapping”). Next, we apply
the machine f derived from a1 to the second molecule.
If f (a2) D elastic the molecules collided elastically and
nothing happens. This could be the case if the machine f
does not halt within a predeﬁned amount of time. Other-
wise the molecules react and a new molecule a3 D f (a2)
is produced. How this new molecule changes the reaction
vessel depends on the algorithm simulating the dynam-
ics of the vessel. Algorithm 1 is a–simple, widely applied
algorithm that simulates second-order catalytic reactions
under ﬂow conditions: Because the population size is con-
stant and thus limited, there is competition and only those
molecules that are somehow created will sustain.
First, the algorithm chooses two molecules randomly,
which simulates a collision. Note that a realistic mea-
sure of time takes the amount of collision and not the
amount of reaction events into account. Then, we deter-
mine stochastically whether the selected molecules react.
If the molecules react, a randomly selected molecule from
the population is replaced by the new product, which as-
sures that the population size stays constant. The replaced
molecules constitute the outﬂow. The inﬂow is implicitly
assumed and is generated by the catalytic production of
molecules. From a chemical point of view, the algorithm
assumes that a product is built from an energy-rich sub-
strate, which does not appear in the algorithm and which
is assumed to be available at constant concentration.
The following section (Sect. “Basic Building Blocks
of an Artiﬁcial Chemistry”) describes the basic building
blocks of an artiﬁcial chemistry in more detail. In Sect.
“Structure-to-Function Mapping” we will explore various
techniques to deﬁne reaction rules. The role of space is
brieﬂy discussed in Sect. “Space”. Then the important the-
oretical concepts of an autocatalytic set and a chemical
organization are explained (Sect. “Theory”). Sect. “Evolu-
tion” shows how artiﬁcial chemistries can be used to study
(chemical) evolution. This article does not discuss infor-
mation processing in detail, because there are a series of
other specialized articles on that topic, which are summa-
rized in Sect. “Information Processing”.

328 A
Artificial Chemistry
INPUT: P : population (array of k molecules)
randomPosition() := randomInt(0, k-1);
reaction(r1, r2) := (fold(r1))(r2);
while not terminate do
reactant1 := P[randomPosition()];
reactant2 := P[randomPosition()];
product := reaction(reactant1, reactant2);
if product == not elastic
P[randomPosition()] := product;
fi
t := t + 1/k
;; increment simulated time
od
Artificial Chemistry, Algorithm 1
Basic Building Blocks of an Artificial Chemistry
Molecules
The ﬁrst step in deﬁning an AC requires that we de-
ﬁne the set of all molecular species that can appear in
the AC. The easiest way to specify this set is to enu-
merate explicitly all molecules as symbols. For example:
M D fH2; O2; H2O; H2O2g or, equivalently, M D fa; b;
c; dg. A symbol is just a name without any structure.
Most conventional (bio-)chemical reaction system mod-
els are deﬁned this way. Also many artiﬁcial chemistries
where networks are designed “by hand” [20], are created
randomly [85], or are evolved by changing the reaction
rules [41,42,43] use only symbolic molecules without any
structure.
Alternatively, molecules can posses a structure. In that
case, the set of molecules is implicitly deﬁned. For exam-
ple: M D fall polymers that can be made from the two
monomers a and b}. In this case, the set of all possible
molecules can even become inﬁnite, or at least quite large.
A vast variety of such rule-based molecule deﬁnitions
can be found in diﬀerent approaches. For example, struc-
tured molecules may be abstract character sequences [3,23,
48,64], sequences of instructions [1,39], lambda-expres-
sions [26], combinators [83], binary strings [5,16,90],
numbers [8], machine code [72], graphs [78], swarms [79],
or proofs [28].
We can call a molecule’s representation its structure,
in contrast to its function, which is given by the reaction
rules R. The description of the valid molecules and their
structure is usually the ﬁrst step when an AC is deﬁned.
This is analogous to a part of chemistry that describes what
kind of atom conﬁgurations form stable molecules and
how these molecules appear.
Reaction Rules
The set of reaction rules R describes how molecules from
M D fa1; a2; : : :g interact. A rule  2 R can be written
according to the chemical notation of reaction rules in the
form
la1;a1 C la2;a2 C    ! ra1;a1 C ra2;a2 C    : (1)
The stoichiometric coeﬃcients la; and ra; describe the
amount of molecular species a 2 M in reaction  2 R on
the left-hand and right-hand side, respectively. Together,
the stoichiometric coeﬃcients deﬁne the stoichiometric
matrix
S D (sa;) D (ra;  la;) :
(2)
An entry sa; of the stoichiometric matrix denotes the net
amount of molecules of type a produced in reaction .
A reaction rule determines a multiset of molecules (the
left-hand side) that can react and subsequently be replaced
by the molecules on the right-hand side. Note that the sign
“C” is not an operator here, but should only separate the
components on either side. The set of all molecules M
and a set of reaction rules R deﬁne the reaction network
hM; Ri of the AC. The reaction network is equivalent to
a Petri net [71]. It can be represented by two matrices,
(la;) and (ra;), made up of the stoichiometric coeﬃ-
cients. Equivalently we can represent the reaction network
as a hyper-graph or a directed bipartite graph with two
node types denoting reaction rules and molecular species,
respectively, and edge labels for the stoichiometry.
A rule is applicable only if certain conditions are ful-
ﬁlled. The major condition is that all of the left-hand
side components must be available. This condition can

Artificial Chemistry
A
329
be broadened easily to include other parameters such as
neighborhood, rate constants, probability for a reaction,
inﬂuence of modiﬁer species, or energy consumption. In
such a case, a reaction rule would also contain additional
information or further parameters. Whether or not these
additional predicates are taken into consideration depends
on the objectives of the artiﬁcial chemistry. If it is meant to
simulate real chemistry as accurate as possible, it is neces-
sary to integrate these parameters into the simulation sys-
tem. If the goal is to build an abstract model, many of these
parameters can be omitted.
Like for the set of molecules we can deﬁne the set of
reaction rules explicitly by enumerating all rules symbol-
ically [20], or we can deﬁne it implicitly by referring to
the structure of the molecules. Implicit deﬁnitions of re-
action rules use, for example, string matching/string con-
catenation [3,60,64], lambda-calculus [26,27], Turing ma-
chines [40,90], ﬁnite state machines [98] or machine code
language [1,16,78,87], proof theory [27], matrix multipli-
cation [5], swarm dynamics [79], or simple arithmetic op-
erations [8]. Note that in some cases the reactions can
emerge as the result of the interaction of many atomic
particles, whose dynamics is speciﬁed by force ﬁelds or
rules [79]. Section “Structure-to-Function Mapping” will
discuss implicitly deﬁned reactions in more details.
Dynamics
The third element of an artiﬁcial chemistry is its speciﬁca-
tion of how the reaction rules give rise to a dynamic pro-
cess of molecules reacting in some kind of reaction ves-
sel. It is assumed that the reaction vessel contains multiple
copies of molecules, which can be seen as instances of the
molecular species M.
This section summarizes how the dynamics of a re-
action vessel (which usually contains a huge number of
molecules) can be modeled and simulated. The approaches
can be characterized roughly by whether each molecule is
treated explicitly or whether all molecules of one type are
represented by a number denoting their frequency or con-
centration.
The Chemical Diﬀerential Equation
A reaction net-
work hM; Ri speciﬁes the structure of a reaction system,
but does not contain any notion of time. A common way
to specify the dynamics of the reaction system is by using
a system of ordinary diﬀerential equations of the following
form:
˙x(t) D Sv(x(t)) ;
(3)
where x D (x1; : : : ; xm)T 2 Rm is a concentration vector
depending on time t, S the stoichiometric matrix derived
from R (Eq. (2)), and v D (v1; : : : ; vr)T 2 Rr a ﬂux vec-
tor depending on the current concentration vector. A ﬂux
v  0 describes the velocity or turnover rate of reac-
tion  2 R. The actual value of v depends usually on
the concentration of the species participating in the reac-
tion  (i. e., LHS()  fa 2 M: la; > 0g) and sometimes
on additional modiﬁer species, whose concentration is not
inﬂuenced by that reaction. There are two important as-
sumptions that are due to the nature of reaction systems.
These assumptions relate the kinetic function v to the re-
action rules R:
Assumption 1
A reaction  can only take place if all
species of its left-hand side LHS(()) are present. This im-
plies that for all molecules a 2 M and reactions  2 R
with a 2 LHS(), if xa D 0 then v D 0. The ﬂux v must
be zero, if the concentration xa of a molecule appearing on
the left-hand side of this reaction is zero. This assumption
meets the obvious fact that a molecule has to be present to
react.
Assumption 2 If all species LHS() of a reaction  2 R
are present in the reactor (e. g. for all a 2 LHS(); xa > 0)
the ﬂux of that reaction is positive, (i. e., v > 0). In other
words, the ﬂux v must be positive, if all molecules re-
quired for that reaction are present, even in small quan-
tities. Note that this assumption implies that a modiﬁer
species necessary for that reaction must appear as a species
on the left- (and right-) hand side of .
In short, taking Assumption 1 and 2 together, we demand
for a chemical diﬀerential equation:
v > 0 , 8a 2 LHS(); xa > 0 :
(4)
Note that Assumption 1 is absolutely necessary. Although
Assumption 2 is very reasonable, there might be situations
where it is not true. Assume, for example, a reaction of
the form a C a ! b. If there is only one single molecule a
left in the reaction vessel, the reaction cannot take place.
Note however, if we want to model this eﬀect, an ODE is
the wrong approach and we should choose a method like
those described in the following sections.
There are a large number of kinetic laws fulﬁlling these
assumptions (Eq. (4)), including all laws that are usually
applied in practice. The most fundamental of such kinetic
laws is mass-action kinetics, which is just the product of
the concentrations of the interacting molecules (cf. [32]):
v(x) D
Y
a2M
xla;
a
:
(5)
The sum of the exponents in the kinetic law P
a2M la;
is called the order of the reaction. Most more compli-

330 A
Artificial Chemistry
cated laws like Michaelis–Menten kinetics are derived
from mass-action kinetics. In this sense, mass-action ki-
netics is the most general approach, allowing one to emu-
late all more complicated laws derived from it, but at the
expense of a larger amount of molecular species.
Explicitly Simulated Collisions
The chemical diﬀeren-
tial equation is a time and state continuous approximation
of a discrete system where molecules collide stochastically
and react. In the introduction we saw how this can be sim-
ulated by a rather simple algorithm, which is widely used
in AC research [5,16,26]. The algorithm presented in the
introduction simulates a second-order catalytic ﬂow sys-
tem with constant population size. It is relatively easy to
extend this algorithm to include reaction rates and arbi-
trary orders.
First, the algorithm chooses a subset from P, which
simulates a collision among molecules. Note that the re-
sulting subset could be empty, which can be used to sim-
ulate an inﬂow (i. e., reactions of the form ! A, where
the left-hand side is empty). By deﬁning the probability
of obtaining a subset of size n, we can control the rate
of reactions of order n. If the molecules react, the reac-
tands are removed from the population and the products
are added. Note that this algorithm can be interpreted as
a stochastic rewriting system operating on a multiset of
molecules [70,88].
For large population size k, the dynamics created by
the algorithm tends to the continuous dynamics of the
chemical diﬀerential equation assuming mass-action ki-
netics. For the special case of second-order catalytic ﬂow,
as described by the algorithm in the introduction, the dy-
namics can be described by the catalytic network equa-
tion [85]
˙xk D
X
i;j2M
˛k
i;jxixj  xk˚(x) ;
with
˚(x) D
X
i;j;k2M
˛k
i;jxixj ;
(6)
which is a generalization of the replicator equation [35].
If the reaction function is deterministic, we can set the ki-
netic constants to
˛k
i;j D
(
1
reaction(i; j) D k ;
0
otherwise :
(7)
This dynamic is of particular interest, because competition
is generated due to a limited population size. Note that the
limited population size is equivalent to an unlimited pop-
ulation size with a limited substrate, e. g., [3].
Discrete-Event Simulation
If the copy number of a par-
ticular molecular species becomes high or if the reaction
rates diﬀer strongly, it is more eﬃcient to use discrete-
event simulation techniques. The most famous technique
is Gillespie’s algorithm [32]. Roughly, in each step, the al-
gorithm generates two random numbers depending on the
current population state to determine the next reaction to
occur as well as the time interval t when it will occur.
Then the simulated time is advanced t :D t C t, and the
molecule count of the population updated according to the
reaction that occurred.
The Gillespie algorithm generates a statistically correct
trajectory of the chemical master equation. The chemical
master equation is the most general way to formulate the
stochastic dynamics of a chemical system. The chemical
master equation is a ﬁrst-order diﬀerential equation de-
scribing the time-evolution of the probability of a reaction
system to occupy each one of its possible discrete set of
states. In a well-stirred (non spatial) AC a state is equiva-
lent to a multiset of molecules.
Structure-to-Function Mapping
A fundamental “logic” of real chemical systems is that
molecules posses a structure and that this structure deter-
mines how the molecule interacts with other molecules.
Thus, there is a mapping from the structure of a molecule
to its function, which determines the reaction rules.
In real chemistry the mapping from structure to func-
tion is given by natural or physical laws. In artiﬁcial
chemistries the structure-to-function mapping is usually
given by some kind of algorithm. In most cases, an artiﬁ-
cial chemistry using a structure-to-function mapping does
not aim at modeling real molecular dynamics in detail.
Rather the aim is to capture the fact that there is a struc-
ture, a function, and a relation between them.
Having an AC with a structure-to-function mapping
at hand, we can study strongly constructive dynamical sys-
tems [27,28]. These are systems where, through the inter-
action of its components, novel molecular species appear.
Example: Lambda-Calculus (AlChemy)
This section demonstrates a structure-to-function that
employs a concept from computer science: the -calculus.
The -calculus has been used by Fontana [26] and Fontana
and Buss [27] to deﬁne a constructive artiﬁcial chemistry.
In the so-called AlChemy, a molecule is a normal-
ized -expression. A -expression is a word over an al-
phabet A D f; :; (; )g [ V where V D fx1; x2; : : :g is an
inﬁnite set of available variable names. The set of  expres-

Artificial Chemistry
A
331
while not terminate() do
reactands := choseASubsetFrom(P);
if randomReal(0, 1) < reactionProbability(reactands);
products = reaction(reactands);
P := remove(P, reactands);
P := insert(P, products);
fi
t := t + 1/ sizeOf(P)
;; increment simulated time
od
Artificial Chemistry, Algorithm 2
sions  is deﬁned for x 2 V; s1 2 ; s2 2  by
x 2 
variable name ;
x:s2 2 
abstraction ;
(s2)s1 2 
application :
An abstraction x:s2 can be interpreted as a function def-
inition, where x is the parameter in the “body” s2. The ex-
pression (s2)s1 can be interpreted as the application of s2
on s1, which is formalized by the rewriting rule
(x:s2)s1 D s2[x   s1] ;
(8)
where s2[x   s1] denotes the term which is generated
by replacing every unbounded occurrence of x in s2 by
s1. A variable x is bounded if it appears in in a form like
: : : (x: : : : x : : :) : : :. It is also not allowed to apply the
rewriting rule if a variable becomes bounded. Example: Let
s1 D x1:(x1)x2:x2 and s2 D x3:x3 then we can derive:
(s1)s2 ) (x1:(x1)x2:x2)x3:x3
) (x3:x3)x2:x2 ) x2:x2 :
(9)
The simplest way to deﬁne a set of second-order catalytic
reactions R by applying a molecule s1 to another mole-
cule s2:
s1 C s2 ! s1 C s2 C normalForm((s1)s2) :
(10)
The procedure normalForm reduces its argument term to
normal form, which is in practice bounded by a maximum
of available time and memory; if these resources are ex-
ceeded before termination, the collision is considered to
be elastic. It should be noted that the -calculus allows
an elegant generalization of the collision rule by deﬁn-
ing it by -expression ˚ 2  : s1 C s2 ! s1 C s2 C
normalForm(((˚)s1)s2).
In order to simulate the artiﬁcial chemistry, Fontana
and Buss [27] used an algorithm like the algorithm
described in Subsect. “Explicitly Simulated Collisions”,
which simulates second-order catalytic mass-action kinet-
ics in a well-stirred population under ﬂow condition.
AlChemy possesses a couple of important general
properties: Molecules come in two forms: as passive data
possessing a structure and as active machines operating
on those structures. This generates a “strange loop” like
in typogenetics [36,94], which allows molecules to refer to
themselves and to operate on themselves. Structures op-
erate on structures, and by doing so change the way struc-
tures operate on structures, and so on; creating a “strange”,
self-referential dynamic loop. Obviously, there are always
some laws that cannot be changed, which are here the rules
of the lambda-calculus, deﬁning the structure-to-function
mapping. Thus, we can interpret these ﬁxed and prede-
ﬁned laws of an artiﬁcial chemistry as the natural or phys-
ical laws.
Arithmetic and Logic Operations
One of the most easy ways to deﬁne reaction rules implic-
itly is to apply arithmetic operations taken from mathe-
matics. Even the simplest rules can generate interesting be-
haviors. Assume, for example, that the molecules are nat-
ural numbers: M D f0; 1; 2 : : : ; n  1g and the reaction
rules are deﬁned by division: reaction(a1; a2) D a1/a2 if
a1 is a multiple of a2, otherwise the molecules do not react.
For the dynamics we assume a ﬁnite, discrete population
and an algorithm like the one described in Subsect. “Ex-
plicitly Simulated Collisions”. With increasing population
size the resulting reaction system displays a phase tran-
sition, at which the population is able to produce prime
numbers with high probability (see [8] for details). In
a typical simulation, the initially high diversity of a ran-
dom population is reduced leaving a set of non-reacting
prime numbers.
A quite diﬀerent behavior can be obtained by sim-
ply replacing the division by addition: reaction(a1; a2) D
a1 C a2 mod n with n D jMj the number of molecular
species. Initializing the well-stirred tank reactor only with

332 A
Artificial Chemistry
the molecular species 1, the diversity rapidly increases to-
wards its maximum and the reactor behaves apparently to-
tally chaotic after a very short transient phase (for a suﬃ-
ciently large population size). However, there are still reg-
ularities, which depend on the prime factors of n (cf. [15]).
Matrix-Multiplication Chemistry
More complex reac-
tion operators can be deﬁned that operate on vectors or
strings instead of scalar numbers as molecules. The ma-
trix multiplication chemistry introduce by Banzhaf [5,6,7]
uses binary strings as molecules. The reaction between two
binary strings is performed by folding one of them into
a matrix which then operates on the other string by multi-
plication.
Example of a reaction for 4-bit molecules Assume a reac-
tion s1 C s2 H) s3. The general approach is:
1. Fold s1 to matrix M. Example: s1 D (s1
1; s2
1; s3
1; s4
1)
M D
s1
1
s2
1
s3
1
s4
1

:
(11)
2. Multiply M with subsequences of s2. Example: Let
s2 D s1
2; s2
2; s3
2; s4
2
 be divided into two subsequences
s12
2 D

s1
2; s2
2

and s34
2 D (s3
2; s4
2). Then we can multi-
ply M with the subsequences:
s12
3 D M ˇ s12
2 ;
s34
3 D M ˇ s34
2 :
(12)
3. Compose s3 by concatenating the products. Example:
s3 D s12
3 ˚ s34
3 .
There are various ways of deﬁning the vector matrix prod-
uct ˇ. It was mainly used with the following threshold
multiplication. Given a bit vector x D (x1; : : : ; xn) and
a bit matrix M D (Mi j) then the term y D M ˇ x is de-
ﬁned by:
y j D
(
0
if Pn
iD1 xi Mi;j  ˚ :
1
otherwise :
(13)
The threshold multiplication is similar to the common
matrix-vector product, except that the resulting vector is
mapped to a binary vector by using the threshold ˚.
Simulating the dynamics by an ODE or explicit molec-
ular collisions (Subsect. “Explicitly Simulated Collisions”),
we can observe that such a system would develop into
a steady state where some string species support each other
in production and thus become a stable autocatalytic cycle,
whereas others would disappear due to the competition in
the reactor.
The system has a couple of interesting properties: De-
spite the relatively simple deﬁnition of the basic reaction
mechanism, the resulting complexity of the reaction sys-
tem and its dynamics is surprisingly high. Like in typoge-
netics and Fontana’s lambda-chemistry, molecules appear
in two forms; as passive data (binary strings) and as active
operators (binary matrix). The folding of a binary string to
a matrix is the central operation of the structure-to-func-
tion mapping. The matrix is multiplied with substrings of
the operand and thus some kind of locality is preserved,
which mimics local operation of macromolecules (e. g. ri-
bosomes or restriction enzymes) on polymers (e. g. RNA
or DNA). Locality is also conserved in the folding, because
bits that are close in the string are also close in the matrix.
Autocatalytic Polymer Chemistries
In order to study the emergence and evolution of autocat-
alytic sets [19,46,75] Bagley, Farmer, Fontana, Kauﬀman
and others [3,23,47,48,59] have used artiﬁcial chemistries
where the molecules are character sequences (e. g., M D
fa; b; aa; ab; ba; bb; aaa; aab; : : :g) and the reactions are
concatenation and cleavage, for example:
aa C babb ˛ aababb
(slow) :
(14)
Additionally, each molecule can act as a catalyst enhancing
the rate of a concatenation reaction.
aa C babb C bbb ˛ bbb C aababb
(fast) :
(15)
Figure 1 shows an example of an autocatalytic network
that appeared. There are two time scales. Reactions that
are not catalyzed are assumed to be very slow and not de-
picted, whereas catalyzed reactions, inﬂow, and decay are
fast.
Typical experiments simulate a well-stirred ﬂow reac-
tor using meta-dynamical ODE frame work [3]; that is, the
ODE model is dynamically changed when new molecu-
lar species appear or present species vanish. Note that the
structure of the molecules does not fully determine the re-
action rules. Which molecule catalyzes which reaction (the
dotted arrows in Fig. 1) is assigned explicitly randomly.
An interesting aspect is that the catalytic or autocatalytic
polymer sets (or reaction networks) evolve without hav-
ing a genome [47,48]. The simulation studies show how
small, spontaneous ﬂuctuations can be ampliﬁed by an au-
tocatalytic network, possibly leading to a modiﬁcation of
the entire network [3]. Recent studies by Fernando and
Rowe [24] include further aspects like energy conservation
and compartmentalization.

Artificial Chemistry
A
333
Artificial Chemistry, Figure 1
Example of an autocatalytic polymer network. The dotted lines represent catalytic links, e. g., aa C ba C aaa ! aaba C aaa. All
molecules are subject to a dilution flow. In this example, all polymerization reactions are reversible and there is a continuous inflow
of the monomers a and b. Note that the network contains further auto-catalytic networks, for example, fa; b; aa; bag and fa; aag,
of which some are closed and thus organizations, for example, fa; b; aa; bag. The set fa; aag is not closed, because b and ba can be
produced
Bagley and Farmer [3] showed that autocatalytic sets
can be silhouetted against a noisy background of sponta-
neously reacting molecules under moderate (that is, nei-
ther too low nor too high) ﬂow.
Artiﬁcial Chemistries Inspired by Turing Machines
The concept of an abstract automaton or Turing ma-
chine provides a base for a variety of structure-func-
tion mappings. In these approaches molecules are usu-
ally represented as a sequence of bits, characters, or in-
structions [36,51,63,86]. A sequence of bits speciﬁes the
behavior of a machine by coding for its state transition
function. Thus, like in the matrix-multiplication chem-
istry and the lambda-chemistry, a molecule appears in two
forms, namely as passive data (e. g., a binary string) and
as an active machine. Also here we can call the mapping
from a binary string into its machine folding, which might
be indeterministic and may depend on other molecules
(e. g., [40]).
In the early 1970s Laing [51] argued for abstract, non-
analogous models in order to develop a general theory
for living systems. For developing such general theory,
so-called artiﬁcial organisms would be required. Laing
suggested a series of artiﬁcial organisms [51,52,53] that
should allow one to study general properties of life and
thus would allow one to derive a theory which is not re-
stricted to the instance of life we observe on earth. The ar-
tiﬁcial organisms consist of diﬀerent compartments, e. g.,
a “brain” plus “body” parts. These compartments con-
tain binary strings as molecules. Strings are translated to
a sequence of instructions forming a three-dimensional
shape (cf. [78,86,94]). In order to perform a reaction, two
molecules are attached such that they touch at one position
(Fig. 2). One of the molecules is executed like a Turing ma-
chine, manipulating the passive data molecule as its tape.
Laing proved that his artiﬁcial organisms are able to per-
form universal computation. He also demonstrated diﬀer-
ent forms of self-reproduction, self-description and self-
inspection using his molecular machines [53].
Typogenetics is a similar approach, which was intro-
duced in 1979 by Hofstadter in order to illustrate the “for-
mal logic of life” [36]. Later, typogenetics was simulated
and investigated in more detail [67,94,95]. The molecules
of typogenetics are character sequences (called strands)
over the alphabet A, C, G, T. The reaction rules are
“typographic” manipulations based on a set of predeﬁned
basic operations like cutting, insertion, or deletion of char-
acters. A sequence of such operations forms a unit (called
an enzyme) which may operate on a character sequence
like a Turing machine on its tape. A character string can be
“translated” to an enzyme (i. e., a sequence of operations)
by mapping two characters to an operation according to
a predeﬁned “genetic code”.

334 A
Artificial Chemistry
Artificial Chemistry, Figure 2
Illustration of Laing’s molecular machines. A program molecule
is associated with a data molecule. Figure from [52]
Another artiﬁcial chemistry whose reaction mecha-
nism is inspired by the Turing machine was suggested
by McCaskill [63] in order to study the self-organization
of complex chemical systems consisting of catalytic poly-
mers. Variants of this AC were realized later in a spe-
cially designed parallel reconﬁgurable computer based on
FPGAs – Field Programmable Gate Arrays [12,18,89]. In
this approach, molecules are binary strings of ﬁxed length
(e. g., 20 [63]) or of variable length [12]. As in previous ap-
proaches, a string codes for an automaton able to manip-
ulate other strings. And again, pattern matching is used to
check whether two molecules can react and to obtain the
“binding site”; i. e., the location where the active molecule
(machine) manipulates the passive molecule (tape). The
general reaction scheme can be written as:
sM C sT ! sM C s0
T :
(16)
In experimental studies, self-replicating strings appeared
frequently. In coevolution with parasites, an evolution-
ary arms race started among these species and the self-
replicating string diversiﬁed to an extent that the para-
sites could not coadapt and went extinct. In a spatial envi-
ronment (e. g., a 2-D lattice), sets of cooperating polymers
evolved, interacting in a hypercyclic fashion. The authors
also observed a chemoton-like [30] cooperative behavior,
with spatially isolated, membrane-bounded evolutionary
stable molecular organizations.
Machines with Fixed Tape Size
In order to perform
large-scale systematic simulation studies like the investiga-
tion of noise [40] or intrinsic evolution [16], it makes sense
to limit the study to molecules of ﬁxed tractable length.
Ikegami and Hashimoto [40] developed an abstract arti-
ﬁcial chemistry with two types of molecular species: 7 bit
long tapes, which are mapped to 16 bit long machines.
Tapes and machines form two separated populations, sim-
ulated in parallel. Reactions take place between a tape and
a machine according to the following reaction scheme:
sM C sT ! sM C sT C s0
M C s0
T :
(17)
A machine sM can react with a tape sT, if its head matches
a substring of the tape sT and its tail matches a diﬀerent
substring of the tape sT. The machine operates only be-
tween these two substrings (called reading frame) which
results in a tape s0
T. The tape s0
T is then “translated” (folded)
into a machine s0
M.
Ikegami and Hashimoto [40] showed that under the
inﬂuence of low noise, simple autocatalytic loops are
formed. When the noise level is increased, the reaction
network is destabilized by parasites, but after a relatively
long transient phase (about 1000 generations) a very sta-
ble, dense reaction network appears, called core net-
work [40]. A core network maintains its relatively high
diversity even if the noise is deactivated. The active mu-
tation rate is high. When the noise level is very high, only
small, degenerated core networks emerge with a low di-
versity and very low (even no) active mutation. The core
networks which emerged under the inﬂuence of external
noise are very stable so that their is no further develop-
ment after they have appeared.
Assembler Automata
An Assembler automaton is like a parallel von Neumann
machine. It consists of a core memory and a set of process-
ing units running in parallel. Inspired by Core Wars [14],
assembler automata have been used to create certain arti-
ﬁcial life systems, such as Coreworld [72,73], Tierra [74],
Avida [1], and Amoeba [69]. Although these machines
have been classiﬁed as artiﬁcial chemistries [72], it is in
general diﬃcult to identify molecules or reactions. Fur-
thermore, the assembler automaton Tierra has explicitly

Artificial Chemistry
A
335
been designed to imitate the Cambrian explosion and
not a chemical process. Nevertheless, in some cases we
can interpret the execution of an assembler automaton as
a chemical process, which is especially possible in a clear
way in experiments with Avida. Here, a molecule is a single
assembler program, which is protected by a memory man-
agement system. The system is initialized with manually
written programs that are able to self-replicate. Therefore,
in a basic version of Avida only unimolecular ﬁrst-order
reactions occur, which are of replicator type. The reaction
scheme can be written as a ! a C mutation(a). The func-
tion mutation represents the possible errors that can occur
while the program is self-replicating.
Latticei Molecular Systems
In this section systems are discussed which consist of a reg-
ular lattice, where each lattice site can hold a part (e. g.
atom) of a molecule. Between parts, bonds can be formed
so that a molecule covers many lattice sites. This is dif-
ferent to systems where a lattice site holds a complete
molecule, like in Avida. The important diﬀerence is that
in lattice molecular systems, the space of the molecular
structure is identical to the space in which the molecules
are ﬂoating around. In systems where a molecule covers
just one lattice site, the molecular structure is described in
a diﬀerent space independently from the space in which
the molecule is located.
Lattice molecular systems have been intensively used
to model polymers [92], protein folding [82], and RNA
structures. Besides approaches which intend to model real
molecular dynamics as accurately as possible, there are
also approaches which try to build abstract models. These
models should give insight into statistical properties of
polymers like their energy landscape and folding pro-
cesses [76,77], but should not give insight into questions
concerning origin and evolution self-maintaining orga-
nizations or molecular information processing. For these
questions more abstract systems are studied as described
in the following.
Varela, Maturana, and Uribe introduced in [93] a lat-
tice molecular system to illustrate their concept of au-
topoiesis (cf. [65,99]). The system consists of a 2-D square
lattice. Each lattice site can be occupied by one of the fol-
lowing atoms: Substrate S, catalyst K, and monomer L.
Atoms may form bonds and thus form molecular struc-
tures on the lattice. If molecules come close to each other
they may react according to the following reaction rules:
K C 2S ! K C L
(1) Composition: Formation
of a monomer.
: : : -L-L C L ! : : : -L-L-L (2) Concatenation:
Artificial Chemistry, Figure 3
Illustration of a lattice molecular automaton that contains an
autopoietic entity. Its membrane is formed by a chain of L
monomers and encloses a catalyst K. Only substrate S may dif-
fuse through the membrane. Substrate inside is catalyzed by K
to form free monomers. If the membrane is damaged by disin-
tegration of a monomer L it can be quickly repaired by a free
monomer. See [65,93] for details
L C L ! L-L
Formation
of
a
bond
be-
tween a monomer and another
monomer with no more than
one bond. This reaction is in-
hibited by a double-bounded
monomer [65].
L ! 2S
(3) Disintegration: Decompo-
sition of a monomer
Figure 3 illustrates an autopoietic entity that may arise.
Note that it is quite diﬃcult to ﬁnd the right dynamical
conditions under which such autopoietic structures are
stable [65,68].
Cellular Automata
Cellular automata ▷Mathematical Basis of Cellular Au-
tomata, Introduction to can be used as a medium to sim-
ulate chemical-like processes in various ways. An obvious
approach is to use the cellular automata to model space
where each cell can hold a molecule (like in Avida) or
atom (like in lattice molecular automata). However there
are approaches where it is not clear at the onset what
a molecule or reaction is. The model speciﬁcation does not
contain any notion of a molecule or reaction, so that an
observer has to identify them. Molecules can be equated

336 A
Artificial Chemistry
Artificial Chemistry, Figure 4
Example of mechanical artificial chemistries. a Self-assembling magnetic tiles by Hosokawa et al. [38]. b Rotating magnetic discs by
Grzybowski et al. [33]. Figures taken (and modified) from [33,38]
with self-propagating patterns like gliders, self-reproduc-
ing loops [54,80], or with the moving boundary between
two homogeneous domains [37]. Note that in the lat-
ter case, particles become visible as space-time structures,
which are deﬁned by boundaries and not by connected set
of cells with speciﬁc states. An interaction between these
boundaries is then interpreted as a reaction.
Mechanical Artiﬁcial Chemistries
There are also physical systems that can be regarded
as artiﬁcial chemistries. Hosokawa et al. [38] presented
a mechanical self-assembly system consisting of triangu-
lar shapes that form bonds by permanent magnets. In-
terpreting attachment and detachment as chemical reac-
tions, Hosokawa et al. [38] derived a chemical diﬀerential
equation (Subsect. “The Chemical Diﬀerential Equation”)
modeling the kinetics of the structures appearing in the
system. Another approach uses millimeter-sized magnetic
disks at a liquid-air interface, subject to a magnetic ﬁeld
produced by a rotating permanent magnet [33]. These
magnetic discs exhibit various types of structures, which
might even interact resembling chemical reactions. The
important diﬀerence to the ﬁrst approach is that the rotat-
ing magnetic discs form dissipative structures, which re-
quire continuous energy supply.
Semi-Realistic Molecular Structures
and Graph Rewriting
Recent development in artiﬁcial chemistry aims at more
realistic molecular structures and reactions, like oo chem-
istry by Bersini [10] (see also [57]) or toy chemistry [9].
These approaches apply graph rewriting, which is a pow-
erful and quite universal approach for deﬁning transfor-
mations of graph-like structures [50] In toy chemistry,
molecules are represented as labeled graphs; i. e., by their
structural formulas; their basic properties are derived by
a simpliﬁed version of the extended Hückel MO theory
that operates directly on the graphs; chemical reaction
mechanisms are implemented as graph rewriting rules act-
ing on the structural formulas; reactivities and selectivities
are modeled by a variant of the frontier molecular orbital
theory based on the extended Hückel scheme. Figure 5
shows an example of a graph-rewriting rule for unimolec-
ular reactions.
Space
Many approaches discussed above assume that the reac-
tion vessel is well-stirred. However, especially in living sys-
tems, space plays an important role. Cells, for example,
are not a bag of enzymes, but spatially highly structured.

Artificial Chemistry
A
337
Artificial Chemistry, Figure 5
An example of a graph-rewriting rule (top) and its application to
the synthesis of a bridge ring system (bottom). Figure from [9]
Moreover, it is assumed that space has played an impor-
tant role in the origin of life.
Techniques for Modeling Space
In chemistry, space is usually modeled by assuming an
Euclidean space (partial diﬀerential equation or particle-
based) or by compartments, which we can ﬁnd also in
many artiﬁcial chemistries [29,70]. However, some arti-
ﬁcial chemistries use more abstract, “computer friendly”
spaces, such as a core-memory like in Tierra [74] or a pla-
nar triangular graph [83]. There are systems like MGS [31]
that allow to specify various topological structures eas-
ily. Approaches like P-systems and membrane comput-
ing [70] allow one even to change the spatial structure dy-
namically ▷Membrane Computing.
Phenomena in Space
In general, space delays the extinction of species by ﬁt-
ter species, which is for example, used in Avida to obtain
more complex evolutionary patterns [1]. Space leads usu-
ally to higher diversity. Moreover, systems that are instable
in a well-stirred reaction vessel can become stable in a spa-
tial situation. An example is the hypercycle [20], which
stabilizes against parasites when space is introduced [11].
Conversely, space can destabilize an equilibrium, leading
to symmetry breaking, which has been suggested as an im-
portant mechanism underlying morphogenesis [91]. Space
can support the co-existence of chemical species, which
would not co-exist in a well-stirred system [49]. Space
is also necessary for the formation of autopoietic struc-
tures [93] and the formation of units that undergo Dar-
winian evolution [24].
Theory
There is a large body of theory from domains like chemi-
cal reaction system theory [21], Petri net theory [71], and
rewriting system theory (e. g., P-systems [70]), which ap-
plies also to artiﬁcial chemistries. In this section, however,
we shall investigate in more detail those theoretical con-
cepts which have emerged in artiﬁcial chemistry research.
When working with complex artiﬁcial chemistries, we
are usually more interested in the qualitative than quanti-
tative nature of the dynamics. That is, we study how the set
of molecular species present in the reaction vessel changes
over time, rather than studying a more detailed trajectory
in concentration space.
The most prominent qualitative concept is the auto-
catalytic set, which has been proposed as an important el-
ement in the origin of life [19,46,75]. An autocatalytic set
is a set of molecular species where each species is produced
by at least one catalytic reaction within the set [41,47]
(Fig. 1). This property has also been called self-maintain-
ing [27] or (catalytic) closure [48]. Formally: A set of
species A  M is called an autocatalytic set (or sometimes,
catalytically closed set), if for all species1 a 2 A there is
a catalyst a0 2 A, and a reaction  2 R such that a0 is cat-
alyst in  (i. e., a0 2 LHS() and a0 2 RHS()) and  can
take place in A (i. e., LHS()  A).
Example 1 (autocatalytic sets)
R D fa ! 2a; a !
a C b; a !; b !g. Molecule a catalyzes its own produc-
tion and the production of b. Both molecules are subject
to a dilution ﬂow (or, equivalently, spontaneous decay),
which is usually assumed in ACs studying autocatalytic
sets. In this example there are three autocatalytic sets: two
non-trivial autocatalytic sets fag and fa; bg, and the empty
set fg, which is from a mathematical point of view also an
autocatalytic set.
The term “autocatalytic set” makes sense only in ACs
where catalysis is possible, it is not useful when applied
to arbitrary reaction networks. For this reason Dittrich
and Speroni di Fenizio [17] introduced a general notion of
self-maintenance, which includes the autocatalytic set as
a special case: Formally, given a reaction network hM; Ri
with m D jMj molecules and r D jRj reactions, and let
S D (sa;) be the (m  r) stoichiometric matrix implied
by the reaction rules R, where sa; denotes the number
1For general reaction systems, this deﬁnition has to be reﬁned.
When A contains species that are part of the inﬂow, like a and b in
Fig. 1, but which are not produced in a catalytic way, we might want
them to be part of an autocatalytic set. Assume, for example, the set
A D fa; b; aa; bag from Fig. 1, where aa catalyzes the production of
aa and ba, while using up “substrate” a and b, which are not catalyti-
cally produced.

338 A
Artificial Chemistry
of molecules of type a produced in reaction . A set of
molecules C  M is called self-maintaining, if there ex-
ists a ﬂux vector v 2 Rr such that the following three con-
ditions apply: (1) for all reactions  that can take place in C
(i. e., LHS()  C) the ﬂux v > 0; (2) for all remaining re-
actions  (i. e., LHS() ª C), the ﬂux v D 0; and (3) for
all molecules a 2 C, the production rate (Sv)a  0. v de-
notes the element of v describing the ﬂux (i. e. rate) of re-
action . (Sv)a is the production rate of molecule a given
ﬂux vector v. In Example 1 there are three self-maintaining
(autocatalytic) sets.
Interestingly, there are self-maintaining sets that can-
not make up a stationary state. In our Example 1 only
two of the three self-maintaining sets are species combi-
nations that can make up a stationary state (i. e., a state
x0 for which 0 D Sv(x0) holds). The self-maintaining set
fag cannot make up a stationary state because a gener-
ates b through the reaction a ! a C b. Thus, there is no
stationary state (of Eq. (3) with Assumptions 1 and 2) in
which the concentration of a is positive while the concen-
tration of b is zero. In order to ﬁlter out those less inter-
esting self-maintaining sets, Fontana and Buss [27] intro-
duced a concept taken from mathematics called closure:
Formally, a set of species A  M is closed, if for all re-
actions  with LHS()  A (the reactions that can take
place in A), the products are also contained in A, i. e.,
RHS()  A).
Closure and self-maintenance lead to the important
concept of a chemical organization [17,27]: Given an arbi-
trary reaction network hM; Ri, a set of molecular species
that is closed and self-maintaining is called an organi-
zation. The importance of an organization is illustrated
by a theorem roughly saying that given a ﬁxed point of
the chemical ODE (Eq. (3)), then the species with pos-
itive concentrations form an organization [17]. In other
words, we have only to check those species combinations
for stationary states that are organizations. The set of all
organizations can be visualized nicely by a Hasse-diagram,
which sketches the hierarchical (organizational) structure
of the reaction network (Fig. 6). The dynamics of the arti-
ﬁcial chemistry can then be explained within this Hasse-
diagram as a movement from organization to organiza-
tion [61,84].
Note that in systems under ﬂow condition, the (ﬁnite)
set of all organizations forms an algebraic lattice [17]. That
is, given two organizations, there is always a unique orga-
nization union and organization intersection.
Although the concept of autopoiesis [93] has been de-
scribed informally in quite some detail, a stringent formal
deﬁnition is lacking. In a way, we can interpret the formal
concepts above as an attempt to approach necessary prop-
Artificial Chemistry, Figure 6
Lattice of organizations of the autocatalytic network shown in
Fig. 1. An organization is a closed and self-maintaining set of
species. Two organizations are connected by a line, if one is con-
tained in the other and there is no organization in between. The
vertical position of an organization is determined by the number
of species it contains
erties of an autopoietic system step by step by precise for-
mal means. Obviously, being (contained in) at least one or-
ganization is a necessary condition for a chemical autopoi-
etic system. But it is not suﬃcient. Missing are the notion
of robustness and a spatial concept that formalizes a sys-
tem’s ability to maintain (and self-create) its own identity,
e. g., through maintaining a membrane (Fig. 3).
Evolution
With artiﬁcial chemistries we can study chemical evolu-
tion. Chemical evolution (also called “pre-biotic evolu-
tion”) describes the ﬁrst step in the development of life,
such as the formation of complex organic molecules from
simpler (in-)organic compounds [62]. Because there are
no pre-biotic fossils, the study of chemical evolution has
to rely on experiments [56,66] and theoretic (simulation)
models [19]. Artiﬁcial chemistries aim at capturing the

Artificial Chemistry
A
339
constructive nature of these chemical systems and try to
reproduce their evolution in computer simulations. The
approaches can be distinguished by whether the evolution
is driven by external operators like mutation, or whether
variation and selection appears intrinsically through the
chemical dynamics itself.
Extrinsic Evolution
In extrinsic approaches, an external variation operator
changes the reaction network by adding, removing, or
manipulating a reaction, which may also lead to the ad-
dition or removal of chemical species. In this approach,
a molecule does not need to posses a structure. The fol-
lowing example by Jain and Krishna [41] shows that this
approach allows one to create an evolving system quite el-
egantly:
Let us assume that the reaction network consists of m
species M D f1; : : : ; mg. There are ﬁrst-order catalytic re-
action rules of the form (i ! i C j) 2 R and a general di-
lution ﬂow (a !) 2 R for all species a 2 M. The reaction
network is completely described by a directed graph repre-
sented by the adjacency matrix C D (ci;j); i; j 2 M; ci;j 2
f0; 1g, with ci;j D 1 if molecule j catalyzes the production
of i, and ci;j D 0 otherwise. In order to avoid that self-
replicators dominate the system, Jain and Krishna assume
ci;i D 0 for all molecules i 2 M.
At the beginning, the reaction network is randomly
initialized, that is, (for i ¤ j)ci;j D 1 with probability p
and ci;j D 0, otherwise. In order to simulate the dynam-
ics, we assume a population of molecules represented by
the concentration vector x D (x1; : : : ; xm), where xi rep-
resents the current concentration of species i. The whole
system is simulated in the following way:
Step 1: Simulate the chemical diﬀerential equation
˙xi D
X
i2M
cj;i  xi
X
k;j2M
ck;jxj ;
(18)
until a steady state is reached. Note that this steady
state is usually independent of the initial concentra-
tions, cf. [85].
Step 2: Select the “mutating” species i, which is the species
with smallest concentration in the steady state.
Step 3: Update the reaction network by replacing the mu-
tating species by a new species, which is created ran-
domly in the same way as the initial species. That is, the
entries of the ith row and ith column of the adjacency
matrix C are replaced by randomly chosen entries with
the same probability p as during initialization.
Step 4: Go to Step 1.
Note that there are two explicitly simulated time scales:
a slow, evolutionary time scale at which the reaction net-
work evolves (Step 2 and 3), and a fast time scale at which
the molecules catalytically react (Step 1).
In this model we can observe how an autocatalytic set
inevitably appears after a period of disorder. After its ar-
rival the largest autocatalytic set increases rapidly its con-
nectivity until it spans the whole network. Subsequently,
the connectivity converges to a steady state determined
by the rate of external perturbations. The resulting highly
non-random network is not fully stable, so that we can
also study the causes for crashes and recoveries. For ex-
ample, Jain and Krishna [44] identiﬁed that in the absence
of large external perturbation, the appearance of a new vi-
able species is a major cause of large extinction and recov-
eries. Furthermore, crashes can be caused by extinction of
a “keystone species”. Note that for these observations, the
new species created in Step 3 do not need to inherit any
information from other species.
Intrinsic Evolution
Evolutionary phenomena can also be caused by the intrin-
sic dynamics of the (artiﬁcial) chemistry. In this case, ex-
ternal operators like those mutating the reaction network
are not required. As opposed to the previous approach, we
can deﬁne the whole reaction network at the onset and
let only the composition of molecules present in the re-
action vessel evolve. The reaction rules are (usually) de-
Artificial Chemistry, Figure 7
Illustration of syntactically and semantically closed organiza-
tions. Each organization consists of an infinite number of molec-
ular species (connected by a dotted line). A circle represents
a molecule having the structure: x1:x2: : : : 27

340 A
Artificial Chemistry
ﬁned by a structure-to-function mapping similar to those
described in Sect. “Structure-to-Function Mapping”. The
advantage of this approach is that we need not deﬁne an
external operator changing the reaction network’s topol-
ogy. Furthermore, the evolution can be more realistic be-
cause, for example, molecular species that have vanished
can reenter at a later time, which does not happen in an
approach like the one described previously. Also, when we
have a structure-to-function mapping, we can study how
the structure of the molecules is related to the emergence
of chemical organizations.
There are various approaches using, for example, Tur-
ing machines [63], lambda-calculus [26,27], abstract au-
tomata [16], or combinators [83] for the structure-to-
function mapping.
In all of those approaches we can observe an important
phenomenon: While the AC evolves autocatalytic sets or
more general, chemical organizations become visible. Like
in the system by Jain and Krishna this eﬀect is indicated
also by an increase of the connectivity of the species within
the population.
When an organization becomes visible, the system
has focused on a sub-space of the whole set of possible
molecules. Those molecules, belonging to the emerged or-
Artificial Chemistry, Figure 8
Example of a self-evolving artificial chemistry. The figure shows the concentration of some selected species of a reaction vessel
containing approximately 104 different species. In this example, molecules are binary strings of length 32 bit. Two binary strings
react by mapping one of them to a finite state machine operating on the second binary string. There is only a general, non-selective
dilution flow. No other operators like mutation, variation, or selection are applied. Note that the structure of a new species tends
to be similar to the structure of the species it has been created from. The dynamics simulated by the algorithm of the introduction.
Population size k D 106 molecules. Figure from [16]
ganization, posses usually relatively high concentrations,
because they are generated by many reactions. The closure
of this set is usually smaller than the universe M. Depend-
ing on the setting the emerged organization can consist of
a single self-replicator, a small set of mutually producing
molecules, or a large set of diﬀerent molecules. Note that
in the latter case the organization can be so large (and even
inﬁnite) that not all its molecules are present in the pop-
ulation (see Fig. 7 for an example). Nevertheless the pop-
ulation can carry the organization, if the system can keep
a generating set of molecules within the population.
An emerged organization can be quite stable but can
also change either by external perturbations like randomly
inserted molecules or by internal ﬂuctuations caused
by reactions among molecules that are not part of the
emerged organization, but which have remained in small
quantities in the population, cf. for an example [61].
Dittrich and Banzhaf [16] have shown that it is even
possible to obtain evolutionary behavior without any ex-
ternally generated variation, that is, even without any in-
ﬂow of random molecules. And without any explicitly
deﬁned ﬁtness function or selection process. Selection
emerges as a result of the dilution ﬂow and the limited
population size.

Artificial Chemistry
A
341
Syntactic and Semantic Closure
In
many
“constructive”
implicitly
deﬁned
artiﬁcial
chemistries we can observe species appearing that share
syntactical and functional similarities that are invariant
under the reaction operation. Fontana and Buss [27] called
this phenomenon syntactic and semantic closure.
Syntactic closure refers to the observation that the
molecules within an emerged organization O  M are
structurally similar. That is, they share certain structural
features. This allows one to describe the set of molecules O
by a formal language or grammar in a compact way. If
we would instead pick a subset A from M randomly, the
expected length of A’s description is on the order of its
size. Assume, for example, if we pick one million strings
of length 100 randomly from the set of all strings of length
100, then we would need about 100 million characters to
describe the resulting set. Interestingly, the organizations
that appear in implicitly deﬁned ACs can be described
much more compactly by a grammar.
Syntactic and semantic closure should be illustrated
with an example taken from [27], where O  M is even
inﬁnite in size. In particular experiments with the lambda-
chemistry, molecules appeared that posses the following
structure:
Ai;j  x1:x2: : : : xi:xj
with
j  i ;
(19)
for example x1:x2:x3:x2. Syntactical closure means
that we can specify such structural rules specifying the
molecules of O and that these structural rules are invari-
ant under the reaction mechanism. If molecules with the
structure given by Eq. (19) react, their product can also be
described by Eq. (19).
Semantic closure means that we can describe the reac-
tions taking place within O by referring to the molecule’s
grammatical structure (e. g. Eq. (19)). In our example, all
reactions within O can be described by the following laws
(illustrated by Fig. 7):
8i; j > 1; k; l : Ai;j C Ak;l H) Ai1;j1 ;
8i > 1; j D 1; k; l : Ai;j C Ak;l H) AkCi1;lCi1 :
(20)
For example x1:x2:x3:x4:x3 C x1:x2:x2
H)
x1:x2:x3:x2 and x1:x2:x1Cx1:x2:x3:x4:x3 H)
x1:x2:x3:x4:x5:x4, respectively (Fig. 7). As a result
of the semantic closure we need not to refer to the un-
derlying reaction mechanism (e. g., the lambda-calculus).
Instead, we can explain the reactions within O on a more
abstract level by referring to the grammatical structure
(e. g., Eq. (19)).
Note that syntactical and semantical closure appears
also in real chemical system and is exploited by chemistry
to organize chemical explanations. It might even be stated
that without this phenomenon, chemistry as we know it
would not be possible.
Information Processing
The chemical metaphor gives rise to a variety of com-
puting concepts, which are explained in detail in further
articles of this encyclopedia. In approaches like amor-
phous computing ▷Amorphous Computing chemical
mechanisms are used as a sub-system for communication
between a huge number of simple, spatially distributed
processing units. In membrane computing ▷Membrane
Computing, rewriting operations are not only used to re-
act molecules, but also to model active transport between
compartments and to change the spatial compartment
structure itself.
Beside these theoretical and in-silico artiﬁcial chemis-
tries there are approaches that aim at using real molecules:
Spatial patterns like waves and solitons in excitable me-
dia can be exploited to compute, see ▷Reaction-Diﬀu-
sion Computing and ▷Computing in Geometrical Con-
strained Excitable Chemical Systems. Whereas other ap-
proaches like conformational computing and DNA com-
puting ▷DNA Computing do not rely on spatially struc-
tured reaction vessels. Other closely related topics are
molecular automata ▷Molecular Automata and bacterial
computing ▷Bacterial Computing.
Future Directions
Currently, we can observe a convergence of artiﬁcial
chemistries and chemical models towards more realis-
tic artiﬁcial chemistries [9]. On the one hand, models of
systems biology are inspired by techniques from artiﬁ-
cial chemistries, e. g., in the domain of rule-based mod-
eling [22,34]. On the other hand, artiﬁcial chemistries be-
come more realistic, for example, by adopting more real-
istic molecular structures or using techniques from com-
putational chemistry to calculate energies [9,57]. Further-
more, the relation between artiﬁcial chemistries and real
biological systems is made more and more explicit [25,
45,58].
In the future, novel theories and techniques have to be
developed to handle complex, implicitly deﬁned reaction
systems. This development will especially be driven by the
needs of system biology, when implicitly deﬁned models
(i. e., rule-based models) become more frequent.
Another open challenge lies in the creation of realistic
(chemical) open-ended evolutionary systems. For exam-

342 A
Artificial Chemistry
ple, an artiﬁcial chemistry with “true” open-ended evolu-
tion or the ability to show a satisfying long transient phase,
which would be comparable to the natural process where
novelties continuously appear, has not been presented yet.
The reason for this might be lacking computing power,
insuﬃcient man-power to implement what we know, or
missing knowledge concerning the fundamental mecha-
nism of (chemical) evolution. Artiﬁcial chemistries could
provide a powerful platform to study whether the mecha-
nisms that we believe explain (chemical) evolution are in-
deed candidates.
As sketched above, there is a broad range of cur-
rently explored practical application domains of artiﬁcial
chemistries in technical artifacts, such as ambient comput-
ing, amorphous computing, organic computing, or smart
materials. And eventually, artiﬁcial chemistry might come
back to the realm of real chemistry and inspire the design
of novel computing reaction systems like in the ﬁeld of
molecular computing, molecular communication, bacte-
rial computing, or synthetic biology.
Bibliography
Primary Literature
1. Adami C, Brown CT (1994) Evolutionary learning in the 2D arti-
ficial life system avida. In: Brooks RA, Maes P (eds) Prof artificial
life IV. MIT Press, Cambridge, pp 377–381. ISBN 0-262-52190-3
2. Adleman LM (1994) Molecular computation of solutions to
combinatorical problems. Science 266:1021
3. Bagley RJ, Farmer JD (1992) Spontaneous emergence of
a metabolism. In: Langton CG, Taylor C, Farmer JD, Ras-
mussen S (eds) Artificial life II. Addison-Wesley, Redwood City,
pp 93–140. ISBN 0-201-52570-4
4. Banâtre J-P, Métayer DL (1986) A new computational model
and its discipline of programming. Technical Report RR-0566.
INRIA, Rennes
5. Banzhaf W (1993) Self-replicating sequences of binary num-
bers – foundations I and II: General and strings of length n =
4. Biol Cybern 69:269–281
6. Banzhaf W (1994) Self-replicating sequences of binary num-
bers: The build-up of complexity. Complex Syst 8:215–225
7. Banzhaf W (1995) Self-organizing algorithms derived from RNA
interactions. In: Banzhaf W, Eeckman FH (eds) Evolution and
Biocomputing. LNCS, vol 899. Springer, Berlin, pp 69–103
8. Banzhaf W, Dittrich P, Rauhe H (1996) Emergent computation
by catalytic reactions. Nanotechnology 7(1):307–314
9. Benkö G, Flamm C, Stadler PF (2003) A graph-based toy model
of chemistry. J Chem Inf Comput Sci 43(4):1085–1093. doi:10.
1021/ci0200570
10. Bersini H (2000) Reaction mechanisms in the oo chemistry. In:
Bedau MA, McCaskill JS, Packard NH, Rasmussen S (eds) Artifi-
cial life VII. MIT Press, Cambridge, pp 39–48
11. Boerlijst MC, Hogeweg P (1991) Spiral wave structure in pre-
biotic evolution: Hypercycles stable against parasites. Physica
D 48(1):17–28
12. Breyer J, Ackermann J, McCaskill J (1999) Evolving reaction-dif-
fusion ecosystems with self-assembling structure in thin films.
Artif Life 4(1):25–40
13. Conrad
M
(1992)
Molecular
computing:
The
key-lock
paradigm. Computer 25:11–22
14. Dewdney AK (1984) In the game called core war hostile pro-
grams engage in a battle of bits. Sci Amer 250:14–22
15. Dittrich P (2001) On artificial chemistries. Ph D thesis, Univer-
sity of Dortmund
16. Dittrich P, Banzhaf W (1998) Self-evolution in a constructive bi-
nary string system. Artif Life 4(2):203–220
17. Dittrich P, Speroni di Fenizio P (2007) Chemical organiza-
tion theory. Bull Math Biol 69(4):1199–1231. doi:10.1007/
s11538-006-9130-8
18. Ehricht R, Ellinger T, McCascill JS (1997) Cooperative ampli-
fication of templates by cross-hybridization (CATCH). Eur J
Biochem 243(1/2):358–364
19. Eigen M (1971) Selforganization of matter and the evo-
lution of biological macromolecules. Naturwissenschaften
58(10):465–523
20. Eigen M, Schuster P (1977) The hypercycle: A principle of
natural self-organisation, part A. Naturwissenschaften 64(11):
541–565
21. Érdi P, Tóth J (1989) Mathematical models of chemical reac-
tions: Theory and applications of deterministic and stochastic
models. Pinceton University Press, Princeton
22. Faeder JR, Blinov ML, Goldstein B, Hlavacek WS (2005) Rule-
based modeling of biochemical networks. Complexity. doi:10.
1002/cplx.20074
23. Farmer JD, Kauffman SA, Packard NH (1986) Autocatalytic repli-
cation of polymers. Physica D 22:50–67
24. Fernando C, Rowe J (2007) Natural selection in chemical evo-
lution. J Theor Biol 247(1):152–167. doi:10.1016/j.jtbi.2007.01.
028
25. Fernando C, von Kiedrowski G, Szathmáry E (2007) A stochas-
tic model of nonenzymatic nucleic acid replication: Elongators
sequester replicators. J Mol Evol 64(5):572–585. doi:10.1007/
s00239-006-0218-4
26. Fontana W (1992) Algorithmic chemistry. In: Langton CG, Tay-
lor C, Farmer JD, Rasmussen S (eds) Artificial life II. Addison-
Wesley, Redwood City, pp 159–210
27. Fontana W, Buss LW (1994) ‘The arrival of the fittest’: To-
ward a theory of biological organization. Bull Math Biol 56:
1–64
28. Fontana W, Buss LW (1996) The barrier of objects: From dynam-
ical systems to bounded organization. In: Casti J, Karlqvist A
(eds) Boundaries and barriers. Addison-Wesley, Redwood City,
pp 56–116
29. Furusawa C, Kaneko K (1998) Emergence of multicellular or-
ganisms with dynamic differentiation and spatial pattern. Artif
Life 4:79–93
30. Gánti T (1975) Organization of chemical reactions into di-
viding and metabolizing units: The chemotons. Biosystems
7(1):15–21
31. Giavitto J-L, Michel O (2001) MGS: A rule-based programming
language for complex objects and collections. Electron Note
Theor Comput Sci 59(4):286–304
32. Gillespie DT (1976) General method for numerically simulat-
ing stochastic time evolution of coupled chemical-reaction.
J Comput Phys 22(4):403–434
33. Grzybowski BA, Stone HA, Whitesides GM (2000) Dynamic self-

Artificial Chemistry
A
343
assembly of magnetized, millimetre-sized objects rotating at
a liquid-air interface. Nature 405(6790):1033–1036
34. Hlavacek W, Faeder J, Blinov M, Posner R, Hucka M, Fontana
W (2006) Rules for modeling signal-transduction systems. Sci
STKE 2006:re6
35. Hofbauer J, Sigmund K (1988) Dynamical systems and the the-
ory of evolution. University Press, Cambridge
36. Hofstadter DR (1979) Gödel, Escher, Bach: An eternal golden
braid. Basic Books Inc, New York. ISBN 0-465-02685-0
37. Hordijk W, Crutchfield JP, Mitchell M (1996) Embedded-particle
computation in evolved cellular automata. In: Toffoli T, Biafore
M, Leäo J (eds) PhysComp96. New England Complex Systems
Institute, Cambridge, pp 153–8
38. Hosokawa K, Shimoyama I, Miura H (1994) Dynamics of self-
assembling systems: Analogy with chemical kinetics. Artif Life
1(4):413–427
39. Hutton TJ (2002) Evolvable self-replicating molecules in an ar-
tificial chemistry. Artif Life 8(4):341–356
40. Ikegami T, Hashimoto T (1995) Active mutation in self-repro-
ducing networks of machines and tapes. ArtifLife 2(3):305–318
41. Jain S, Krishna S (1998) Autocatalytic sets and the growth
of complexity in an evolutionary model. Phys Rev Lett
81(25):5684–5687
42. Jain S, Krishna S (1999) Emergence and growth of com-
plex networks in adaptive systems. Comput Phys Commun
122:116–121
43. Jain S, Krishna S (2001) A model for the emergence of coop-
eration, interdependence, and structure in evolving networks.
Proc Natl Acad Sci USA 98(2):543–547
44. Jain S, Krishna S (2002) Large extinctions in an evolutionary
model: The role of innovation and keystone species. Proc Natl
Acad Sci USA 99(4):2055–2060. doi:10.1073/pnas.032618499
45. Kaneko K (2007) Life: An introduction to complex systems biol-
ogy. Springer, Berlin
46. Kauffman SA (1971) Cellular homeostasis, epigenesis and repli-
cation in randomly aggregated macromolecular systems. J Cy-
bern 1:71–96
47. Kauffman SA (1986) Autocatalytic sets of proteins. J Theor Biol
119:1–24
48. Kauffman SA (1993) The origins of order: Self-organization and
selection in evolution. Oxford University Press, New York
49. Kirner T, Ackermann J, Ehricht R, McCaskill JS (1999) Com-
plex patterns predicted in an in vitro experimental model sys-
tem for the evolution of molecular cooperation. Biophys Chem
79(3):163–86
50. Kniemeyer O, Buck-Sorlin GH, Kurth W (2004) A graph grammar
approach to artificial life. Artif Life 10(4):413–431. doi:10.1162/
1064546041766451
51. Laing R (1972) Artificial organisms and autonomous cell rules.
J Cybern 2(1):38–49
52. Laing R (1975) Some alternative reproductive strategies in arti-
ficial molecular machines. J Theor Biol 54:63–84
53. Laing R (1977) Automaton models of reproduction by self-in-
spection. J Theor Biol 66:437–56
54. Langton CG (1984) Self-reproduction in cellular automata.
Physica D 10D(1–2):135–44
55. Langton CG (1989) Artificial life. In: Langton CG (ed) Proc of ar-
tificial life. Addison-Wesley, Redwood City, pp 1–48
56. Lazcano A, Bada JL (2003) The 1953 Stanley L. Miller experi-
ment: Fifty years of prebiotic organic chemistry. Orig Life Evol
Biosph 33(3):235–42
57. Lenaerts T, Bersini H (2009) A synthon approach to artificial
chemistry. Artif Life 9 (in press)
58. Lenski RE, Ofria C, Collier TC, Adami C (1999) Genome complex-
ity, robustness and genetic interactions in digital organisms.
Nature 400(6745):661–4
59. Lohn JD, Colombano S, Scargle J, Stassinopoulos D, Haith GL
(1998) Evolution of catalytic reaction sets using genetic algo-
rithms. In: Proc IEEE International Conference on Evolutionary
Computation. IEEE, New York, pp 487–492
60. Lugowski MW (1989) Computational metabolism: Towards bi-
ological geometries for computing. In: Langton CG (ed) Artifi-
cial Life. Addison-Wesley, Redwood City, pp 341–368. ISBN 0-
201-09346-4
61. Matsumaru N, Speroni di Fenizio P, Centler F, Dittrich P (2006)
On the evolution of chemical organizations. In: Artmann S, Dit-
trich P (eds) Proc of the 7th german workshop of artificial life.
IOS Press, Amsterdam, pp 135–146
62. Maynard Smith J, Szathmáry E (1995) The major transitions in
evolution. Oxford University Press, New York
63. McCaskill JS (1988) Polymer chemistry on tape: A computa-
tional model for emergent genetics. Internal report. MPI for
Biophysical Chemistry, Göttingen
64. McCaskill JS, Chorongiewski H, Mekelburg D, Tangen U, Gemm
U (1994) Configurable computer hardware to simulate long-
time self-organization of biopolymers. Ber Bunsenges Phys
Chem 98(9):1114–1114
65. McMullin B, Varela FJ (1997) Rediscovering computational au-
topoiesis. In: Husbands P, Harvey I (eds) Fourth european con-
ference on artificial life. MIT Press, Cambridge, pp 38–47
66. Miller SL (1953) A production of amino acids under possible
primitive earth conditions. Science 117(3046):528–9
67. Morris HC (1989) Typogenetics: A logic for artificial life. In:
Langton CG (ed) Artif life. Addison-Wesley, Redwood City, pp
341–368
68. Ono N, Ikegami T (2000) Self-maintenance and self-reproduc-
tion in an abstract cell model. J Theor Biol 206(2):243–253
69. Pargellis AN (1996) The spontaneous generation of digital
“life”. Physica D 91(1–2):86–96
70. Pˇaun G (2000) Computing with membranes. J Comput Syst Sci
61(1):108–143
71. Petri CA (1962) Kommunikation mit Automaten. Ph D thesis,
University of Bonn
72. Rasmussen S, Knudsen C, Feldberg R, Hindsholm M (1990) The
coreworld: Emergence and evolution of cooperative structures
in a computational chemistry. Physica D 42:111–134
73. Rasmussen S, Knudsen C, Feldberg R (1992) Dynamics of pro-
grammable matter. In: Langton CG, Taylor C, Farmer JD, Ras-
mussen S (eds) Artificial life II. Addison-Wesley, Redwood City,
pp 211–291. ISBN 0-201-52570-4
74. Ray TS (1992) An approach to the synthesis of life. In: Langton
CG, Taylor C, Farmer JD, Rasmussen S (eds) Artificial life II. Ad-
dison-Wesley, Redwood City, pp 371–408
75. Rössler OE (1971) A system theoretic model for biogenesis.
Z Naturforsch B 26(8):741–746
76. Sali A, Shakhnovich E, Karplus M (1994) How does a protein
fold? Nature 369(6477):248–251
77. Sali A, Shakhnovich E, Karplus M (1994) Kinetics of protein fold-
ing: A lattice model study of the requirements for folding to
the native state. J Mol Biol 235(5):1614–1636
78. Salzberg C (2007) A graph-based reflexive artificial chemistry.
Biosystems 87(1):1–12

344 A
Artificial Intelligence in Modeling and Simulation
79. Sayama H (2009) Swarm chemistry. Artif Life. (in press)
80. Sayama H (1998) Introduction of structural dissolution into
Langton’s self-reproducing loop. In: Adami C, Belew R, Ki-
tano H, Taylor C (eds) Artificial life VI. MIT Press, Cambridge,
pp 114–122
81. Segre D, Ben-Eli D, Lancet D (2000) Compositional genomes:
Prebiotic information transfer in mutually catalytic noncova-
lent assemblies. Proc Natl Acad Sci USA 97(8):4112–4117
82. Socci ND, OnuchicJN (1995) Folding kinetics of proteinlikehet-
eropolymers. J Chem Phys 101(2):1519–1528
83. Speroni di Fenizio P (2000) A less abstract artficial chemistry.
In: Bedau MA, McCaskill JS, Packard NH, Rasmussen S (eds) Ar-
tificial life VII. MIT Press, Cambridge, pp 49–53
84. Speroni Di Fenizio P, Dittrich P (2002) Artificial chemistry’s
global dynamics. Movement in the lattice of organisation.
J Three Dimens Images 16(4):160–163. ISSN 1342-2189
85. Stadler PF, Fontana W, Miller JH (1993) Random catalytic reac-
tion networks. Physica D 63:378–392
86. Suzuki H (2007) Mathematical folding of node chains in
a molecular network. Biosystems 87(2–3):125–135. doi:10.
1016/j.biosystems.2006.09.005
87. Suzuki K, Ikegami T (2006) Spatial-pattern-induced evolution
of a self-replicating loop network. Artif Life 12(4):461–485. doi:
10.1162/artl.2006.12.4.461
88. Suzuki Y, Tanaka H (1997) Symbolic chemical system based on
abstract rewriting and its behavior pattern. Artif Life Robotics
1:211–219
89. Tangen U, Schulte L, McCaskill JS (1997) A parallel hardware
evolvable computer polyp. In: Pocek KL, Arnold J (eds) IEEE
symposium on FPGAs for custopm computing machines. IEEE
Computer Society, Los Alamitos
90. Thürk M (1993) Ein Modell zur Selbstorganisation von Auto-
matenalgorithmen zum Studium molekularer Evolution. Ph D
thesis, Universität Jena
91. Turing AM (1952) The chemical basis of morphogenesis. Phil
Trans R Soc London B 237:37–72
92. Vanderzande C (1998) Lattice models of polymers. Cambridge
University Press, Cambridge
93. Varela FJ, Maturana HR, Uribe R (1974) Autopoiesis: The orga-
nization of living systems. BioSystems 5(4):187–196
94. Varetto L (1993) Typogenetics: An artificial genetic system.
J Theor Biol 160(2):185–205
95. Varetto L (1998) Studying artificial life with a molecular au-
tomaton. J Theor Biol 193(2):257–285
96. Vico G (1710) De antiquissima Italorum sapientia ex linguae
originibus eruenda librir tres. Neapel
97. von Neumann J, Burks A (ed) (1966) The theory of self-repro-
ducing automata. University of Illinois Press, Urbana
98. Zauner K-P, Conrad M (1996) Simulating the interplay of struc-
ture, kinetics, and dynamics in complex biochemical networks.
In: Hofestädt R, Lengauer T, Löffler M, Schomburg D (eds)
Computer science and biology GCB’96. University of Leipzig,
Leipzig, pp 336–338
99. Zeleny M (1977) Self-organization of living systems: A formal
model of autopoiesis. Int J General Sci 4:13–28
Books and Reviews
Adami C (1998) Introduction to artificial life. Springer, New York
Dittrich P, Ziegler J, Banzhaf W (2001) Artificial chemistries – a re-
view. Artif Life 7(3):225–275
Hofbauer J, Sigmund K (1998) Evolutionary games and population
dynamics. Cambridge University Press, Cambridge
Artificial Intelligence in Modeling
and Simulation
BERNARD ZEIGLER1, ALEXANDRE MUZY2,
LEVENT YILMAZ3
1 Arizona Center for Integrative Modeling
and Simulation, University of Arizona, Tucson, USA
2 CNRS, Università di Corsica, Corte, France
3 Auburn University, Alabama, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Review of System Theory and Framework for Modeling
and Simulation
Fundamental Problems in M&S
AI-Related Software Background
AI Methods in Fundamental Problems of M&S
Automation of M&S
SES/Model Base Architecture
for an Automated Modeler/Simulationist
Intelligent Agents in Simulation
Future Directions
Bibliography
Glossary
Behavior The observable manifestation of an interaction
with a system.
DEVS Discrete Event System Speciﬁcation formalism de-
scribes models developed for simulation; applications
include simulation based testing of collaborative ser-
vices.
Endomorphic agents Agents that contain models of them-
selves and/or of other endomorphic Agents.
Levels of interoperability Levels at which systems can in-
teroperate such as syntactic, semantic and pragmatic.
The higher the level, the more eﬀective is information
exchange among participants.
Levels of system speciﬁcation Levels at which dynamic
input/output systems can be described, known, or
speciﬁed ranging from behavioral to structural.
Metadata Data that describes other data; a hierarchical
concept in which metadata are a descriptive abstrac-
tion above the data it describes.

Artificial Intelligence in Modeling and Simulation
A
345
Model-based automation Automation of system devel-
opment and deployment that employs models or sys-
tem speciﬁcations, such as DEVS, to derive artifacts.
Modeling and simulation ontology The SES is inter-
preted as an ontology for the domain of hierarchical,
modular simulation models speciﬁed with the DEVS
formalism.
Net-centric environment Network Centered, typically
Internet-centered or web-centered information ex-
change medium.
Ontology Language that describes a state of the world
from a particular conceptual view and usually pertains
to a particular application domain.
Pragmatic frame A means of characterizing the con-
sumer’s use of the information sent by a producer;
formalized using the concept of processing network
model.
Pragmatics Pragmatics is based on Speech Act Theory
and focuses on elucidating the intent of the seman-
tics constrained by a given context. Metadata tags
to support pragmatics include Authority, Urgency/
Consequences, Relationship, Tense and Complete-
ness.
Predicate logic An expressive form of declarative lan-
guage that can describe ontologies using symbols for
individuals, operations, variables, functions with gov-
erning axioms and constraints.
Schema An advanced form of XML document deﬁnition,
extends the DTD concept.
Semantics Semantics determines the content of messages
in which information is packaged. The meaning of
a message is the eventual outcome of the processing
that it supports.
Sensor Device that can sense or detect some aspect of the
world or some change in such an aspect.
System speciﬁcation Formalism for describing or speci-
fying a system. There are levels of system speciﬁcation
ranging from behavior to structure.
Service-oriented architecture Web service architecture
in which services are designed to be (1) accessed with-
out knowledge of their internals through well-deﬁned
interfaces and (2) readily discoverable and compos-
able.
Structure The internal mechanism that produces the be-
havior of a system.
System entity structure Ontological basis for modeling
and simulation. Its pruned entity structures can de-
scribe both static data sets and dynamic simulation
models.
Syntax Prescribes the form of messages in which infor-
mation is packaged.
UML Uniﬁed Modeling Language is a software develop-
ment language and environment that can be used for
ontology development and has tools that map UML
speciﬁcations into XML.
XML eXtensible Markup Language provides a syntax for
document structures containing tagged information
where tag deﬁnitions set up the basis for semantic in-
terpretation.
Definition of the Subject
This article discusses the role of Artiﬁcial Intelligence (AI)
in Modeling and Simulation (M&S). AI is the ﬁeld of com-
puter science that attempts to construct computer systems
that emulate human problem solving behavior with the
goal of understanding human intelligence. M&S is a mul-
tidisciplinary ﬁeld of systems engineering, software engi-
neering, and computer science that seeks to develop ro-
bust methodologies for constructing computerized models
with the goal of providing tools that can assist humans in
all activities of the M&S enterprise. Although each of these
disciplines has its core community there have been numer-
ous intersections and cross-fertilizations between the two
ﬁelds. From the perspective of this article, we view M&S as
presenting some fundamental and very diﬃcult problems
whose solution may beneﬁt from the concepts and tech-
niques of AI.
Introduction
To state the M&S problems that may beneﬁt from AI we
ﬁrst brieﬂy review a system-theory based framework for
M&S that provides a language and concepts to facilitate
deﬁnitive problem statement. We then introduce some key
problem areas: veriﬁcation and validation, reuse and com-
posability, and distributed simulation and systems of sys-
tems interoperability. After some further review of soft-
ware and AI-related background, we go on to outline some
areas of AI that have direct applicability to the just given
problems in M&S. In order to provide a unifying theme
for the problem and solutions, we then raise the ques-
tion of whether all of M&S can be automated into an in-
tegrated autonomous artiﬁcial modeler/simulationist. We
then proceed to explore an approach to developing such an
intelligent agent and present a concrete means by which
such an agent could engage in M&S. We close with con-
sideration of an advanced feature that such an agent must
have if it is to fully emulate human capability—the ability,
to a limited, but signiﬁcant extent, to construct and em-
ploy models of its own “mind” as well of the “minds” of
other agents.

346 A
Artificial Intelligence in Modeling and Simulation
Review of System Theory and Framework
for Modeling and Simulation
Hierarchy of System Speciﬁcations
Systems theory [1] deals with a hierarchy of system spec-
iﬁcations which deﬁnes levels at which a system may be
known or speciﬁed. Table 1 shows this Hierarchy of Sys-
tem Speciﬁcations (in simpliﬁed form, see [2] for full ex-
position).
 At level 0 we deal with the input and output interface
of a system.
 At level 1 we deal with purely observational record-
ings of the behavior of a system. This is an I/O relation
which consists of a set of pairs of input behaviors and
associated output behaviors.
 At level 2 we have knowledge of the initial state when
the input is applied. This allows partitioning the in-
put/output pairs of level 1 into non-overlapping sub-
sets, each subset associated with a diﬀerent starting
state.
 At level 3 the system is described by state space and
state transition functions. The transition function de-
scribes the state-to-state transitions caused by the in-
puts and the outputs generated thereupon.
 At level 4 a system is speciﬁed by a set of components
and a coupling structure. The components are systems
on their own with their own state set and state transi-
tion functions. A coupling structure deﬁnes how those
interact. A property of a coupled system that is called
“closure under coupling” guarantees that a coupled sys-
tem at level 3 itself speciﬁes a system. This property
allows hierarchical construction of systems, i. e., that
coupled systems can be used as components in larger
coupled systems.
As we shall see in a moment, the system speciﬁcation hi-
erarchy provides a mathematical underpinning to deﬁne
a framework for modeling and simulation. Each of the en-
Artificial Intelligence in Modeling and Simulation, Table 1
Hierarchy of system specifications
Level Name
What we specify at this level
4
Coupled systems System built up by several component systems that are coupled together
3
I/O System
System with state and state transitions to generate the behavior
2
I/O Function
Collection of input/output pairs constituting the allowed behavior partitioned according to the initial state
the system is in when the input is applied
1
I/O Behavior
Collection of input/output pairs constituting the allowed behavior of the system from an external Black Box
view
0
I/O Frame
Input and output variables and ports together with allowed values
tities (e. g., real world, model, simulation, and experimen-
tal frame) will be described as a system known or speciﬁed
at some level of speciﬁcation. The essence of modeling and
simulation lies in establishing relations between pairs of
system descriptions. These relations pertain to the validity
of a system description at one level of speciﬁcation relative
to another system description at a diﬀerent (higher, lower,
or equal) level of speciﬁcation.
On the basis of the arrangement of system levels as
shown in Table 1, we distinguish between vertical and hor-
izontal relations. A vertical relation is called an association
mapping and takes a system at one level of speciﬁcation
and generates its counterpart at another level of speciﬁ-
cation. The downward motion in the structure-to-behav-
ior direction, formally represents the process by which the
behavior of a model is generated. This is relevant in simu-
lation and testing when the model generates the behavior
which then can be compared with the desired behavior.
The opposite upward mapping relates a system de-
scription at a lower level with one at a higher level of speci-
ﬁcation. While the downward association of speciﬁcations
is straightforward, the upward association is much less so.
This is because in the upward direction information is in-
troduced while in the downward direction information is
reduced. Many structures exhibit the same behavior and
recovering a unique structure from a given behavior is not
possible. The upward direction, however, is fundamental
in the design process where a structure (system at level 3)
has to be found that is capable of generating the desired
behavior (system at level 1).
Framework for Modeling and Simulation
The Framework for M&S as described in [2] establishes en-
tities and their relationships that are central to the M&S
enterprise (see Fig. 1. The entities of the Framework are:
source system, model, simulator, and experimental frame;
they are related by the modeling and the simulation rela-
tionships. Each entity is formally characterized as a sys-

Artificial Intelligence in Modeling and Simulation
A
347
Artificial Intelligence in Modeling and Simulation, Figure 1
Framework entities and relationships
tem at an appropriate level of speciﬁcation of a generic dy-
namic system.
Source System
The source system is the real or virtual environment that
we are interested in modeling. It is viewed as a source of
observable data, in the form of time-indexed trajectories of
variables. The data that has been gathered from observing
or otherwise experimenting with a system is called the sys-
tem behavior database. These data are viewed or acquired
through experimental frames of interest to the model de-
velopment and user. As we shall see, in the case of model
validation, these data are the basis for comparison with
data generated by a model. Thus, these data must be suf-
ﬁcient to enable reliable comparison as well as being ac-
cepted by both the model developer and the test agency
as the basis for comparison. Data sources for this pur-
pose might be measurement taken in prior experiments,
mathematical representation of the measured data, or ex-
pert knowledge of the system behavior by accepted subject
matter experts.
Experimental Frame
An experimental frame is a speciﬁcation of the condi-
tions under which the system is observed or experimented
with [3]. An experimental frame is the operational for-
mulation of the objectives that motivate a M&S project.
A frame is realized as a system that interacts with the sys-
tem of interest to obtain the data of interest under speciﬁed
conditions.
An experimental frame speciﬁcation consists of four
major subsections:
Input stimuli Speciﬁcation of the class of admissible in-
put time-dependent stimuli. This is the class from
which individual samples will be drawn and injected
into the model or system under test for particular ex-
periments.
Control Speciﬁcation of the conditions under which the
model or system will be initialized, continued under
examination, and terminated.
Metrics Speciﬁcation of the data summarization func-
tions and the measures to be employed to pro-
vide quantitative or qualitative measures of the in-
put/output behavior of the model. Examples of such
metrics are performance indices, goodness of ﬁt crite-
ria, and error accuracy bound.
Analysis Speciﬁcation of means by which the results of
data collection in the frame will be analyzed to arrive at
ﬁnal conclusions. The data collected in a frame consists
of pairs of input/output time functions.
When an experimental frame is realized as a system to in-
teract with the model or system under test the speciﬁca-
tions become components of the driving system. For ex-
ample, a generator of output time functions implements
the class of input stimuli.
An experimental frame is the operational formulation
of the objectives that motivate a modeling and simula-
tion project. Many experimental frames can be formulated
for the same system (both source system and model) and
the same experimental frame may apply to many systems.
Why would we want to deﬁne many frames for the same
system? Or apply the same frame to many systems? For
the same reason that we might have diﬀerent objectives
in modeling the same system, or have the same objective
in modeling diﬀerent systems. There are two equally valid
views of an experimental frame. One, views a frame as
a deﬁnition of the type of data elements that will go into
the database. The second views a frame as a system that
interacts with the system of interest to obtain the data of
interest under speciﬁed conditions. In this view, the frame
is characterized by its implementation as a measurement
system or observer. In this implementation, a frame typi-
cally has three types of components (as shown in Fig. 2 and
Fig. 3): a generator, that generates input segments to the
system; acceptor that monitors an experiment to see the
desired experimental conditions are met; and transducer
that observes and analyzes the system output segments.
Figure 2b illustrates a simple, but ubiquitous, pattern
for experimental frames that measure typical job process-
ing performance metrics, such as round trip time and
throughput. Illustrated in the web context, a generator
produces service request messages at a given rate. The
time that has elapsed between sending of a request and
its return from a server is the round trip time. A trans-
ducer notes the departures and arrivals of requests allow-

348 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 2
Experimental frame and components
ing it to compute the average round trip time and other
related statistics, as well as the throughput and unsatisﬁed
(or lost) requests. An acceptor notes whether performance
achieves the developer’s objectives, for example, whether
the throughput exceeds the desired level and/or whether
say 99% of the round trip times are below a given thresh-
old.
Objectives for modeling relate to the role of the model
in systems design, management or control. Experimental
frames translate the objectives into more precise experi-
mentation conditions for the source system or its models.
We can distinguish between objectives concerning those
for veriﬁcation and validation of (a) models and (b) sys-
tems. In the case of models, experimental frames translate
the objectives into more precise experimentation condi-
tions for the source system and/or its models. A model
under test is expected to be valid for the source system
in each such frame. Having stated the objectives, there is
presumably a best level of resolution to answer these ques-
tions. The more demanding the questions, the greater the
resolution likely to be needed to answer them. Thus, the
choice of appropriate levels of abstraction also hinges on
the objectives and their experimental frame counterparts.
In the case of objectives for veriﬁcation and validation
of systems, we need to be given, or be able to formulate,
the requirements for the behavior of the system at the IO
behavior level. The experimental frame then is formulated
to translate these requirements into a set of possible ex-
periments to test whether the system actually performs its
Artificial Intelligence in Modeling and Simulation, Figure 3
Experimental frame and its components
required behavior. In addition we can formulate measures
of the eﬀectiveness (MOE) of a system in accomplishing its
goals. We call such measures, outcome measures. In order
to compute such measures, the system must expose rele-
vant variables, we’ll call these output variables, whose val-
ues can be observed during execution runs of the system.
Model
A model is a system speciﬁcation, such as a set of instruc-
tions, rules, equations, or constraints for generating in-

Artificial Intelligence in Modeling and Simulation
A
349
put/output behavior. Models may be expressed in a va-
riety of formalisms that may be understood as means
for specifying subclasses of dynamic systems. The Dis-
crete Event System Speciﬁcation (DEVS) formalism de-
lineates the subclass of discrete event systems and it can
also represent the systems speciﬁed within traditional for-
malisms such as diﬀerential (continuous) and diﬀerence
(discrete time) equations [4]. In DEVS, as in systems the-
ory, a model can be atomic, i. e., not further decomposed,
or coupled, in which case it consists of components that are
coupled or interconnected together.
Simulator
A simulator is any computation system (such as a single
processor, or a processor network, or more abstractly an
algorithm), capable of executing a model to generate its
behavior.
The more general purpose a simulator is the greater the
extent to which it can be conﬁgured to execute a variety of
model types. In order of increasing capability, simulators
can be:
 Dedicated to a particular model or small class of similar
models
 Capable of accepting all (practical) models from a wide
class, such as an application domain (e. g., communica-
tion systems)
 Restricted to models expressed in a particular model-
ing formalism, such as continuous diﬀerential equation
models
 Capable of accepting multi-formalism models (having
components from several formalism classes, such as
continuous and discrete event).
A simulator can take many forms such as on a single com-
puter or multiple computers executing on a network.
Fundamental Problems in M&S
We have now reviewed a system-theory-based framework
for M&S that provides a language and concepts in which to
formulate key problems in M&S. Next on our agenda is to
discuss problem areas including: veriﬁcation and valida-
tion, reuse and composability, and distributed simulation
and systems of systems interoperability. These are chal-
lenging, and heretofore, unsolved problems at the core of
the M&S enterprise.
Validation and Veriﬁcation
The basic concepts of veriﬁcation and validation (V&V)
have been described in diﬀerent settings, levels of details,
and points of views and are still evolving. These con-
cepts have been studied by a variety of scientiﬁc and en-
gineering disciplines and various ﬂavors of validation and
veriﬁcation concepts and techniques have emerged from
a modeling and simulation perspective. Within the mod-
eling and simulation community, a variety of methodolo-
gies for V&V have been suggested in the literature [5,6,7].
A categorization of 77 veriﬁcation, validation and testing
techniques along with 15 principles has been oﬀered to
guide the application of these techniques [8]. However,
these methods vary extensively – e. g., alpha testing, induc-
tion, cause and eﬀect graphing, inference, predicate calcu-
lus, proof of correctness, and user interface testing and are
only loosely related to one another. Therefore, such a cat-
egorization can only serve as an informal guideline for the
development of a process for V&V of models and systems.
Validation and veriﬁcation concepts are themselves
founded on more primitive concepts such as system spec-
iﬁcations and homomorphism as discussed in the frame-
work of M&S [2]. In this framework, the entities system,
experimental frame, model, simulator take on real impor-
tance only when properly related to each other. For exam-
ple, we build a model of a particular system for some ob-
jective and only some models, and not others, are suitable.
Thus, it is critical to the success of a simulation modeling
eﬀort that certain relationships hold. Two of the most im-
portant are validity and simulator correctness.
The basic modeling relation, validity, refers to the rela-
tion between a model, a source system and an experimen-
tal frame. The most basic concept, replicative validity, is
aﬃrmed if, for all the experiments possible within the ex-
perimental frame, the behavior of the model and system
agree within acceptable tolerance. The term accuracy is of-
ten used in place of validity. Another term, ﬁdelity, is often
used for a combination of both validity and detail. Thus,
a high ﬁdelity model may refer to a model that is both
highly detailed and valid (in some understood experimen-
tal frame). However, when used this way, the assumption
seems to be that high detail alone is needed for high ﬁ-
delity, as if validity is a necessary consequence of high de-
tail. In fact, it is possible to have a very detailed model that
is nevertheless very much in error, simply because some
of the highly resolved components function in a diﬀerent
manner than their real system counterparts.
The basic approach to model validation is comparison
of the behavior generated by a model and the source sys-
tem it represents within a given experimental frame. The
basis for comparison serves as the reference Fig. 4 against
which the accuracy of the model is measured.
The basic simulation relation, simulator correctness, is
a relation between a simulator and a model. A simulator

350 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 4
Basic approach to model validation
correctly simulates a model if it is guaranteed to faithfully
generate the model’s output behavior given its initial state
and its input trajectory. In practice, as suggested above,
simulators are constructed to execute not just one model
but also a family of possible models. For example, a net-
work simulator provides both a simulator and a class of
network models it can simulate. In such cases, we must es-
tablish that a simulator will correctly execute the particular
class of models it claims to support. Conceptually, the ap-
proach to testing for such execution, illustrated in Fig. 5, is
to perform a number of test cases in which the same model
is provided to the simulator under test and to a “gold stan-
dard simulator” which is known to correctly simulate the
model. Of course such test case models must lie within the
class supported by the simulat or under test as well as pre-
sented in the form that it expects to receive them. Compar-
ison of the output behaviors in the same manner as with
model validation is then employed to check the agreement
between the two simulators.
If the speciﬁcations of both the simulator and the
model are available in separated form where each can be
accessed independently, it may be possible to prove cor-
rectness mathematically.
The case of system validation is illustrated in Fig. 6.
Here the system is considered as a hardware and/or soft-
ware implementation to be validated against requirements
for its input/output behavior. The goal is to develop test
models that can stimulate the implemented system with
inputs and can observe its outputs to compare them with
those required by the behavior requirements. Also shown
is a dotted path in which a reference model is constructed
that is capable of simulation execution. Construction of
such a reference model is more diﬃcult to develop than
the test models since it requires not only knowing in ad-
vance what output to test for, but to actually to generate
such an output. Although such a reference model is not
required, it may be desirable in situations in which the ex-
tra cost of development is justiﬁed by the additional range
of tests that might be possible and the consequential in-
creased coverage this may provide.
Model Reuse and Composability
Model reuse and composability are two sides of the same
coin—it is patently desirable to reuse models, the fruits
of earlier or others work. However, typically such models
will become components in a larger composite model and
must be able to interact meaningfully with them. While
software development disciplines are successfully applying
a component-based approach to build software systems,
the additional systems dynamics involved in simulation
models has resisted straight forward reuse and composi-
tion approaches. A model is only reusable to the extent
that its original dynamic systems assumptions are con-
sistent with the constraints of the new simulation appli-
cation. Consequently, without contextual information to
guide selection and refactoring, a model may not be reused
to advantage within a new experimental frame. Davis and

Artificial Intelligence in Modeling and Simulation
A
351
Artificial Intelligence in Modeling and Simulation, Figure 5
Basic approach to simulator verification
Artificial Intelligence in Modeling and Simulation, Figure 6
Basic approach to system validation
Anderson [9] argue that to foster such reuse, model rep-
resentation methods should distinguish, and separately
specify, the model, simulator, and the experimental frame.
However, Yilmaz and Oren [10] pointed out that more
contextual information is needed beyond the informa-
tion provided by the set of experimental frames to which
a model is applicable [11], namely, the characterization of
the context in which the model was constructed. These
authors extended the basic model-simulator-experimen-
tal frame perspective to emphasize the role of context in
reuse. They make a sharp distinction between the objec-
tive context within which a simulation model is originally
deﬁned and the intentional context in which the model is
being qualiﬁed for reuse. They extend the system theoretic
levels of speciﬁcation discussed earlier to deﬁne certain be-
havioral model dependency relations needed to formalize
conceptual, realization, and experimental aspects of con-
text.

352 A
Artificial Intelligence in Modeling and Simulation
As the scope of simulation applications grows, it is in-
creasingly the case that more than one modeling paradigm
is needed to adequately express the dynamics of the dif-
ferent components. For systems composed of models with
dynamics that are intrinsically heterogeneous, it is cru-
cial to use multiple modeling formalisms to describe them.
However, combining diﬀerent model types poses a vari-
ety of challenges [9,12,13]. Sarjoughian [14], introduced
an approach to multi-formalism modeling that employs
an interfacing mechanism called a Knowledge Interchange
Broker to compose model components expressed in di-
verse formalisms. The KIB supports translation from the
semantics of one formalism into that of a second to ensure
coordinated and correct execution simulation algorithms
of distinct modeling formalisms.
Distributed Simulation and System
of Systems Interoperability
The problems of model reuse and composability manifest
themselves strongly in the context of distributed simula-
tion where the objective is to enable existing geograph-
ically dispersed simulators to meaningfully interact, or
federate, together. We brieﬂy review experience with in-
teroperability in the distributed simulation context and
a linguistically based approach to the System of Systems
(SoS) interoperability problem [15]. Sage and Cuppan [16]
drew the parallel between viewing the construction of SoS
as a federation of systems and the federation that is sup-
ported by the High Level Architecture (HLA), an IEEE
standard fostered by the DoD to enable composition of
simulations [17,18].
HLA is a network middleware layer that supports
message exchanges among simulations, called federates,
in a neutral format. However, experience with HLA has
been disappointing and forced acknowledging the dif-
ference between technical interoperability and substan-
tive interoperability [19]. The ﬁrst only enables heteroge-
neous simulations to exchange data but does not guaran-
tee the second, which is the desired outcome of exchang-
ing meaningful data, namely, that coherent interaction
among federates takes place. Tolk and Muguirra [20] in-
troduced the Levels of Conceptual Interoperability Model
(LCIM) which identiﬁed seven levels of interoperability
among participating systems. These levels can be viewed
as a reﬁnement of the operational interoperability type
which is one of three deﬁned by [15]. The operational
type concerns linkages between systems in their interac-
tions with one another, the environment, and with users.
The other types apply to the context in which systems are
constructed and acquired. They are constructive—relating
to linkages between organizations responsible for system
construction and programmatic—linkages between pro-
gram oﬃces to manage system acquisition.
AI-Related Software Background
To proceed to the discussion of the role of AI in address-
ing key problems in M&S, we need to provide some further
software and AI-related background. We oﬀer a brief his-
torical account of object-orientation and agent-based sys-
tems as a springboard to discuss the upcoming concepts of
object frameworks, ontologies and endomorphic agents.
Object-Orientation and Agent-Based Systems
Many of the software technology advances of the last 30
years have been initiated from the ﬁeld of M&S. Objects,
as code modules with both structure and behavior, were
ﬁrst introduced in the SIMULA simulation language [21].
Objects blossomed in various directions and became in-
stitutionalized in the widely adopted programming lan-
guage C++ and later in the infrastructure for the web
in the form of Java [22] and its variants. The freedom
from straight-line procedural programming that object-
orientation championed was taken up in AI in two di-
rections: various forms of knowledge representation and
of autonomy. Rule-based systems aggregate modular if-
then logic elements—the rules—that can be activated in
some form of causal sequence (inference chains) by an ex-
ecution engine [23]. In their passive state, rules represent
static discrete pieces of inferential logic, called declara-
tive knowledge. However, when activated, a rule inﬂuences
the state of the computation and the activation of subse-
quent rules, providing the system a dynamic or procedu-
ral, knowledge characteristic as well. Frame-based systems
further expanded knowledge representation ﬂexibility and
inferencing capability by supporting slots and constraints
on their values—the frames—as well as their taxonomies
based on generalization/specialization relationships [24].
Convergence with object-orientation became apparent in
that frames could be identiﬁed as objects and their taxo-
nomic organization could be identiﬁed with classes within
object-style organizations that are based on sub-class hier-
archies.
On the other hand, the modular nature of objects to-
gether with their behavior and interaction with other ob-
jects, led to the concept of agents which embodied in-
creased autonomy and self-determination [25]. Agents
represent individual threads of computation and are typ-
ically deployable in distributed form over computer net-
works where they interact with local environments and
communicate/coordinate with each other. A wide variety

Artificial Intelligence in Modeling and Simulation
A
353
of agent types exists in large part determined by the variety
and sophistication of their processing capacities—ranging
from agents that simply gather information on packet traf-
ﬁc in a network to logic-based entities with elaborate rea-
soning capacity and authority to make decisions (employ-
ing knowledge representations just mentioned.) The step
from agents to their aggregates is natural, thus leading to
the concept of multi-agent systems or societies of agents,
especially in the realm of modeling and simulation [26].
To explore the role of AI in M&S at the present, we
project the just-given historical background to the concur-
rent concepts of object frameworks, ontologies and endo-
morphic agents. The Uniﬁed Modeling Language (UML)
is gaining a strong foothold as the defacto standard for ob-
ject-based software development. Starting as a diagram-
matical means of software system representation, it has
evolved to a formally speciﬁed language in which the fun-
damental properties of objects are abstracted and orga-
nized [27,28]. Ontologies are models of the world relating
to speciﬁc aspects or applications that are typically repre-
sented in frame-based languages and form the knowledge
components for logical agents on the Semantic Web [29].
A convergence is underway that re-enforces the common-
ality of the object-based origins of AI and software engi-
neering. UML is being extended to incorporate ontology
representations so that software systems in general will
have more explicit models of their domains of operation.
As we shall soon see, endomorphic agents refer to agents
that include abstractions of their own structure and behav-
ior within their ontologies of the world [30].
Artificial Intelligence in Modeling and Simulation, Figure 7
M&S framework formulated within UML
The M&S Framework
Within Uniﬁed Modeling Language (UML)
With object-orientation as uniﬁed by UML and some
background in agent-based systems, we are in a position to
discuss the computational realization of the M&S frame-
work discussed earlier. The computational framework is
based on the Discrete Event System Speciﬁcation (DEVS)
formalism and implemented in various object-oriented
environments. Using UML we can represent the frame-
work as a set of classes and relations as illustrated in Figs. 7
and 8. Various software implementations of DEVS sup-
port diﬀerent subsets of the classes and relations. In par-
ticular, we mention a recent implementation of DEVS
within a Service-Oriented Architecture (SOA) environ-
ment called DEVS/SOA [31,32]. This implementation ex-
ploits some of the beneﬁts aﬀorded by the web environ-
ment mentioned earlier and provides a context for consid-
eration of the primary target of our discussion, compre-
hensive automation of the M&S enterprise.
We use one of the UML constructs, the use case dia-
gram, to depict the various capabilities that would be in-
volved in automating all or parts of the M&S enterprise.
Use cases are represented by ovals that connect to at least
one actor (stick ﬁgure) and to other use cases through “in-
cludes” relations, shown as dotted arrows. For example,
a sensor (actor) collects data (use case) which includes
storage of data (use case). A memory actor stores and
retrieves models which include storage and retrieval (re-
spectively) of data. Constructing models includes retriev-

354 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 8
M&S framework classes and relations in a UML representation
ing stored data within an experimental frame. Validating
models includes retrieving models from memory as com-
ponents and simulating the composite model to generate
data within an experimental frame. The emulator-simula-
tor actor does the simulating to execute the model so that
its generated behavior can be matched against the stored
data in the experimental frame. The objectives of the hu-
man modeler drive the model evaluator and hence the
choice of experimental frames to consider as well as mod-
els to validate. Models can be used in (at least) two time
frames [33]. In the long term, they support planning of ac-
tions to be taken in the future. In the short term, models
support execution and control in real-time of previously
planned actions.
AI Methods in Fundamental Problems of M&S
The enterprise of modeling and simulation is charac-
terized by activities such as model, simulator and ex-
perimental frame creation, construction, reuse, composi-
tion, veriﬁcation and validation. We have seen that valid
model construction requires signiﬁcant expertise in all the
components of the M&S enterprise, e. g., modeling for-
malisms, simulation methods, and domain understanding
and knowledge. Needless to say, few people can bring all
such elements to the table, and this situation creates a sig-
niﬁcant bottleneck to progress in such projects. Among
the contributing factors are lack of trained personnel that
must be brought in, expense of such high capability ex-
perts, and the time needed to construct models to the res-
olution required for most objectives. This section intro-
duces some AI-related technologies that can ameliorate
this situation: Service-Oriented Architecture and Seman-
tic Web, ontologies, constrained natural language capa-
bilities, and genetic algorithms. Subsequently we will con-
sider these as components in uniﬁed, comprehensive, and
autonomous automation of M&S.
Service-Oriented Architecture and Semantic Web
On the World Wide Web, a Service-Oriented Architecture
is a market place of open and discoverable web-services

Artificial Intelligence in Modeling and Simulation
A
355
Artificial Intelligence in Modeling and Simulation, Figure 9
UML use case formulation of the overall M&S enterprise
incorporating, as they mature, Semantic Web technolo-
gies [34]. The eXtensible Markup Language (XML) is the
standard format for encoding data sets and there are stan-
dards for sending and receiving XML [35]. Unfortunately,
the problem just starts at this level. There are myriad ways,
or Schemata, to encode data into XML and a good num-
ber of such Schemata have already been developed. More
often than not, they are diﬀerent in detail when applied to
the same domains. What explains this incompatibility?
In a Service-Oriented Architecture, the producer sends
messages containing XML documents generated in accor-
dance with a schema. The consumer receives and inter-
prets these messages using the same schema in which they
were sent. Such a message encodes a world state descrip-
tion (or changes in it) that is a member of a set delin-
eated by an ontology. The ontology takes into account the
pragmatic frame, i. e., a description of how the informa-
tion will be used in downstream processing. In a SOA
environment, data dissemination may be dominated by
“user pull of data,” incremental transmission, discovery
using metadata, and automated retrieval of data to meet
user pragmatic frame speciﬁcations. This is the SOA con-
cept of data-centered, interface-driven, loose coupling be-
tween producers and consumers. The SOA concept re-
quires the development of platform-independent, com-
munity-accepted, standards that allow raw data to be syn-
tactically packaged into XML and accompanied by meta-
data that describes the semantic and pragmatic informa-
tion needed to eﬀectively process the data into increasingly
higher-value products downstream.
Artificial Intelligence in Modeling and Simulation, Figure 10
Interoperability levels in distributed simulation
Ontologies
Semantic Web researchers typically seek to develop intel-
ligent agents that can draw logical inferences from diverse,
possibly contradictory, ontologies such as a web search
might discover. Semantic Web research has led to a focus
on ontologies [34]. These are logical languages that pro-
vide a common vocabulary of terms and axiomatic rela-
tions among them for a subject area. In contrast, the newly
emerging area of ontology integration assumes that hu-
man understanding and collaboration will not be replaced
by intelligent agents. Therefore, the goal is to create con-
cepts and tools to help people develop practical solutions
to incompatibility problems that impede “eﬀective” ex-
change of data and ways of testing that such solutions have
been correctly implemented.
As illustrated in Fig. 10, interoperability of systems can
be considered at three linguistically-inspired levels: syntac-
tic, semantic, and pragmatic. The levels are summarized in
Table 2. More detail is provided in [36].

356 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Table 2
Linguistic levels
Linguistic Level
A collaboration of systems or
services interoperates at this level if:
Examples
Pragmatic – how
information in messages
is used
The receiver reacts to the message
in a manner that the sender intends
An order from a commander is obeyed by the troops in the field as
the commander intended. A necessary condition is that the
information arrives in a timely manner and that its meaning has
been preserved (semantic interoperability)
Semantic – shared
understanding of
meaning of messages
The receiver assigns the same
meaning as the sender did to the
message.
An order from a commander to multi -national participants in
a coalition operation is understood in a common manner despite
translation into different languages. A necessary condition is that
the information can be unequivocally extracted from the data
(syntactic interoperability)
Syntactic – common
rules governing
composition and
transmitting of messages
The consumer is able to receive and
parse the sender’s message
A common network protocol (e. g. IPv4) is employed ensuring that
all nodes on the network can send and receive data bit arrays
adhering to a prescribed format.
Constrained Natural Language
Model development can be substantially aided by enabling
users to specify modeling constructs using some form of
constrained natural language [37]. The goal is to over-
come modeling complexity by letting users with limited
or nonexistent formal modeling or programming back-
ground convey essential information using natural lan-
guage, a form of expression that is natural and intu-
itive. Practicality demands constraining the actual expres-
sions that can be used so that the linguistic processing
is tractable and the input can be interpreted unambigu-
ously. Some techniques allow the user to narrow down
essential components for model construction. Their goal
is to reduce ambiguity between the user’s requirements
and essential model construction components. A natural
language interface allows model speciﬁcation in terms of
a verb phrase consisting of a verb, noun, and modiﬁer,
for example “build car quickly.” Conceptual realization of
a model from a verb phrase ties in closely with Check-
land’s [38] insight that an appropriate verb should be used
to express the root deﬁnition, or core purpose, of a sys-
tem. The main barrier between many people and existing
modeling software is their lack of computer literacy and
this provides an incentive to develop natural language in-
terfaces as a means of bridging this gap. Natural language
expression could create modelers out of people who think
semantically, but do not have the requisite computer skills
to express these ideas. A semantic representation frees the
user to explore the system on the familiar grounds of nat-
ural language and opens the way for brain storming, inno-
vation and testing of models before they leave the drawing
board.
Genetic Algorithms
The genetic algorithm is a subset of evolutionary algo-
rithms that model biological processes to search in highly
complex spaces. A genetic algorithm (GA) allows a popu-
lation composed of many individuals to evolve under spec-
iﬁed selection rules to a state that maximizes the “ﬁtness.”
The theory was developed by John Holland [39] and popu-
larized by Goldberg who was able to solve a diﬃcult prob-
lem involving the control of gas pipeline transmission for
his dissertation [40]. Numerous applications of GAs have
since been chronicled [41,42]. Recently, GAs have been ap-
plied to cutting-edge problems in automated construction
of simulation models, as discussed below [43].
Automation of M&S
We are now ready to suggest a unifying theme for the
problems in M&S and possible AI-based solutions, by
raising the question of whether all of M&S can be au-
tomated into an integrated autonomous artiﬁcial mod-
eler/simulationist. First, we provide some background
needed to explore an approach to developing such an in-
telligent agent based on the System Entity Structure/Model
Base framework, a hybrid methodology that combines el-
ements of AI and M&S.
System Entity Structure
The System Entity Structure (SES) concepts were ﬁrst
presented in [44]. They were subsequently extended
and implemented in a knowledge-based design environ-
ment [45]. Application to model base management orig-
inated with [46]. Subsequent formalizations and imple-

Artificial Intelligence in Modeling and Simulation
A
357
mentations were developed in [47,48,49,50,51]. Applica-
tions to various domains are given in [52].
A System Entity Structure is a knowledge representa-
tion formalism which focuses on certain elements and re-
lationships that relate to M&S. Entities represent things
that exist in the real world or sometimes in an imag-
ined world. Aspects represent ways of decomposing things
into more ﬁne grained ones. Multi-aspects are aspects for
which the components are all of one kind. Specializa-
tions represent categories or families of speciﬁc forms that
a thing can assume provides the means to represent a fam-
ily of models as a labeled tree. Two of its key features are
support for decomposition and specialization. The former
allows decomposing a large system into smaller systems.
The latter supports representation of alternative choices.
Specialization enables representing a generic model (e. g.,
a computer display model) as one of its specialized varia-
tions (e. g., a ﬂat panel display or a CRT display). On the
basis of SES axiomatic speciﬁcations, a family of models
(design-space) can be represented and further automati-
cally pruned to generate a simulation model. Such mod-
els can be systematically studied and experimented with
based on alternative design choices. An important, salient
feature of SES is its ability to represent models not only in
terms of their decomposition and specialization, but also
their aspects. The SES represents alternative decomposi-
tions via aspects. The system entity structure (SES) formal-
ism provides an operational language for specifying such
hierarchical structures. An SES is a structural knowledge
representation scheme that systematically organizes a fam-
ily of possible structures of a system. Such a family charac-
terizes decomposition, coupling, and taxonomic relation-
ships among entities. An entity represents a real world
object. The decomposition of an entity concerns how it
may be broken down into sub-entities. In addition, cou-
pling speciﬁcations tell how sub-entities may be coupled
together to reconstitute the entity and be associated with
an aspect. The taxonomic relationship concerns admissible
variants of an entity. The SES/Model-Base framework [52]
is a powerful means to support the plan-generate-evaluate
paradigm in systems design. Within the framework, entity
structures organize models in a model base. Thus, mod-
eling activity within the framework consists of three sub-
activities: speciﬁcation of model composition structure,
speciﬁcation of model behavior, and synthesis of a simula-
tion model.
The SES is governed by an axiomatic framework in
which entities alternate with the other items. For example,
a thing is made up of parts; therefore, its entity representa-
tion has a corresponding aspect which, in turn, has entities
representing the parts. A System Entity Structure speciﬁes
a family of hierarchical, modular simulation models, each
of which corresponds to a complete pruning of the SES.
Thus, the SES formalism can be viewed as an ontology with
the set of all simulation models as its domain of discourse.
The mapping from SES to the Systems formalism, particu-
larly to the DEVS formalism, is discussed in [36]. We note
that simulation models include both static and dynamic
elements in any application domain, hence represent an
advanced form of ontology framework.
SES/Model Base Architecture
for an Automated Modeler/Simulationist
In this section, we raise the challenge of creating a fully
automated modeler/simulationist that can autonomously
carry out all the separate functions identiﬁed in the M&S
framework as well as the high level management of these
functions that is currently under exclusively human con-
trol. Recall the use case diagrams in Fig. 9 that depict
the various capabilities that would need to be involved in
realizing a completely automated modeler/simulationist.
To link up with the primary modules of mind, we assign
model construction to the belief generator—interpreting
beliefs as models [54]. Motivations outside the M&S com-
ponent drive the belief evaluator and hence the choice
of experimental frames to consider as well as models to
validate. External desire generators stimulate the imag-
iner/envisioner to run models to make predictions within
pragmatic frames and assist in action planning.
The use case diagram of Fig. 9, is itself a model of
how modeling and simulation activities may be carried out
within human minds. We need not be committed to par-
ticular details at this early stage, but will assume that such
a model can be reﬁned to provide a useful representation
of human mental activity from the perspective of M&S.
This provides the basis for examining how such an arti-
ﬁcially intelligent modeler/simulationist might work and
considering the requirements for comprehensive automa-
tion of M&S.
The SES/MB methodology, introduced earlier, pro-
vides a basis for formulating a conceptual architecture for
automating all of the M&S activities depicted earlier in
Fig. 9 into an integrated system. As illustrated in Fig. 11,
the dichotomy into real-time use of models for acting in
the real world and the longer-term development of models
that can be employed in such real-time activities is mani-
fested in the distinction between passive and active mod-
els. Passive models are stored in a repository that can be
likened to long-term memory. Such models under go the
life-cycle mentioned earlier in which they are validated
and employed within experimental frames of interest for

358 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 11
SES/Model base architecture for automated M&S
long-term forecasting or decision-making. However, in
addition to this quite standard concept of operation, there
is an input pathway from the short term, or working,
memory in which models are executed in real-time. Such
execution, in real-world environments, often results in de-
ﬁciencies, which provide impetus and requirements for
instigating new model construction. Whereas long-term
model development and application objectives are char-
acterized by experimental frames, the short-term execu-
tion objectives are characterized by pragmatic frames. As
discussed in [36], a pragmatic frame provides a means of
characterizing the use for which an executable model is be-
ing sought. Such models must be simple enough to execute
within, usually, stringent real-time deadlines.
The M&S Framework Within Mind Architecture
An inﬂuential formulation of recent work relating to mind
and brain [53] views mind as the behavior of the brain,
where mind is characterized by a massively modular ar-
chitecture. This means that mind is, composed of a large
number of modules, each responsible for diﬀerent func-
tions, and each largely independent and sparsely con-
nected with others. Evolution is assumed to favor such dif-
ferentiation and specialization since under suitably weakly
interactive environments they are less redundant and
more eﬃcient in consuming space and time resources. In-
deed, this formulation is reminiscent of the class of prob-
lems characterized by Systems of Systems (SoS) in which
the attempt is made to integrate existing systems, orig-
inally built to perform speciﬁc functions, into a more
comprehensive and multifunctional system. As discussed
in [15], the components of each system can be viewed as
communicating with each other within a common ontol-
ogy, or model of the world that is tuned to the smooth
functioning of the organization. However, such ontologies
may well be mismatched to support integration at the sys-
tems level. Despite working on the results of a long history
of pre-human evolution, the fact that consciousness seems
to provide a uniﬁed and undiﬀerentiated picture of mind,
suggests that human evolution has to a large extent solved
the SoS integration problem.
The Activity Paradigm for Automated M&S
At least for the initial part of its life, a modeling agent
needs to work on a “ﬁrst order” assumption about its en-
vironment, namely, that it can exploit only semantics-free
properties [39]. Regularities, such as periodic behaviors,
and stimulus-response associations, are one source of such
semantics-free properties. In this section, we will focus
on a fundamental property of systems, such as the brain’s
neural network, that have a large number of components.
This distribution of activity of such a system over space
and time provides a rich and semantics-free substrate from
which models can be generated. Proposing structures and

Artificial Intelligence in Modeling and Simulation
A
359
Artificial Intelligence in Modeling and Simulation, Figure 12
Brain module activities
algorithms to track and replicate this activity should sup-
port automated modeling and simulation of patterns of re-
ality. The goal of the activity paradigm is to extract mech-
anisms from natural phenomena and behaviors to auto-
mate and guide the M&S process.
The brain oﬀers a quintessential illustration of activ-
ity and its potential use to construct models. Figure 12
describes brain electrical activities [54]. Positron emission
tomography (PET) is used to record electrical activity in-
side the brain. The PET method scans show what hap-
pens inside the brain when resting and when stimulated
by words and music. The red areas indicate high brain ac-
tivities. Language and music produce responses in oppo-
site sides of the brain (showing the sub-system specializa-
tions). There are many levels of activity (ranging from low
to high.)
There is a strong link between modularity and the ap-
plicability of activity measurement as a useful concept. In-
deed, modules represent loci for activity—a distribution
of activity would not be discernable over a network were
there no modules that could be observed to be in diﬀer-
ent states of activity. As just illustrated, neuroscientists are
exploiting this activity paradigm to associate brain areas
with functions and to gain insight into areas that are ac-
tive or inactive over diﬀerent, but related, functions, such
as language and music processing. We can generalize this
approach as a paradigm for an automated modeling agent.
Component, Activity and Discrete-Event Abstractions
Figure 13 depicts a brain description through components
and activities. First, the modeler considers one brain ac-
Artificial Intelligence in Modeling and Simulation, Figure 13
Hierarchy of components, activity and discrete-event abstrac-
tions
tivity (e. g., listening music.) This ﬁrst level activity corre-
sponds to simulation components at a lower level. At this
level, activity of the components can be considered to be
only on (grey boxes) or oﬀ(white boxes.) At lower levels,
structure and behaviors can be decomposed. The structure
of the higher component level is detailed at lower levels
(e. g., to the neuronal network). The behavior is also de-
tailed through activity and discrete-event abstraction [55].
At the ﬁnest levels, activity of components can be detailed.
Using the pattern detection and quantization meth-
ods continuously changing variables can also be treated
within the activity paradigm. As illustrated in Fig. 14, small
slopes and small peaks can signal low activity whereas high
slopes and peaks can signal high activity levels. To provide
for scale, discrete event abstraction can be achieved using
quantization [56]. To determine the activity level, a quan-
tum or measure of signiﬁcant change has to be chosen. The
quantum size acts as a ﬁlter on the continuous ﬂow. For
example, one can notice that in the ﬁgure, using the dis-
played quantum, the smallest peaks will not be signiﬁcant.
Thus, diﬀerent levels of resolution can be achieved by em-
ploying diﬀerent quantum sizes. A genetic algorithm can
be used to ﬁnd the optimum such level of resolution given
for a given modeling objective [43].

360 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 14
Activity sensitivity and discrete-events
Activity Tracking
Within the activity paradigm, M&S consists of capturing
activity paths through component processing and trans-
formations. To determine the basic structure of the whole
system, an automated modeler has to answer questions
of the form: where and how is activity produced, received,
and transmitted? Figure 15 represents a component-based
view of activity ﬂow in a neuronal network. Activity paths
through components are represented by full arrows. Ac-
tivity is represented by full circles. Components are rep-
resented by squares. The modeler must generate such
a graph based on observed data—but how does it obtain
such data? One approach is characteristic of current use of
PET scans by neuroscientists. This approach exploits a re-
lationship between activity and energy—activity requires
consumption of energy, therefore, observing areas of high
energy consumption signals areas of high activity. Notice
that this correlation requires that energy consumption be
localizable to modules in the same way that activity is so
localized. So for example, current computer architectures
that provide a single power source to all components do
not lend themselves to such observation. How activity is
passed on from component to component can be related
to the modularity styles (none, weak, strong) of the com-
ponents. Concepts relating such modularity styles to ac-
tivity transfer need to be developed to support an activity
Artificial Intelligence in Modeling and Simulation, Figure 15
Activity paths in neurons
tracking methodology that goes beyond reliance on energy
correlation.
Activity Model Validation
Recall that having generated a model of an observed
system (whether through activity tracking or by other
means), the next step is validation. In order to perform
such validation, the modeler needs an approach to gener-
ating activity proﬁles from simulation experiments on the
model and to comparing these proﬁles with those observed
in the real system. Muzy and Nutaro [57] have developed
algorithms that exploit activity tracking to achieve eﬃ-
cient simulation of DEVS models. These algorithms can be
adopted to provide an activity tracking pattern applicable
to a given simulation model to extract its activity proﬁles
for comparison with those of the modeled system.
A forthcoming monograph will develop the activity
paradigm for M&S in greater detail [58].
Intelligent Agents in Simulation
Recent trends in technology as well as the use of simula-
tion in exploring complex artiﬁcial and natural informa-
tion processes [62,63] have made it clear that simulation
model ﬁdelity and complexity will continue to increase
dramatically in the coming decades. The dynamic and dis-
tributed nature of simulation applications, the signiﬁcance
of exploratory analysis of complex phenomena [64], and
the need for modeling the micro-level interactions, col-
laboration, and cooperation among real-world entities is
bringing a shift in the way systems are being conceptual-
ized. Using intelligent agents in simulation models is based
on the idea that it is possible to represent the behavior of
active entities in the world in terms of the interactions of

Artificial Intelligence in Modeling and Simulation
A
361
Artificial Intelligence in Modeling and Simulation, Figure 16
Evolution of the use of intelligent agents in simulation
an assembly of agents with their own operational auton-
omy.
The early pervading view on the use of agents in sim-
ulation stems from the developments in Distributed Ar-
tiﬁcial Intelligence (DAI), as well as advances in agent
architectures and agent-oriented programming. The DAI
perspective to modeling systems in terms of entities that
are capable of solving problems by means of reasoning
through symbol manipulation resulted in various tech-
nologies that constitute the basic elements of agent sys-
tems. The early work on design of agent simulators within
the DAI community focused on answering the question
of how goals and intentions of agents emerge and how
they lead to execution of actions that change the state of
their environment. The agent-directed approach to simu-
lating agent systems lies at the intersection of several dis-
ciplines: DAI, Control Theory, Complex Adaptive Systems
(CAS), and Discrete-event Systems/Simulation. As shown
in Fig. 16, these core disciplines gave direction to technol-
ogy, languages, and possible applications, which then in-
ﬂuenced the evolution of the synergy between simulation
and agent systems.
Distributed Artiﬁcial Intelligence and Simulation
While progress in agent simulators and interpreters re-
sulted in various agent architectures and their computa-
tional engines, the ability to coordinate agent ensembles
was recognized early as a key challenge [65]. The MACE
system [66] is considered as one of the major milestones
in DAI. Speciﬁcally, the proposed DAI system integrated
concepts from concurrent programming (e. g., actor for-
malism [67]) and knowledge representation to symboli-
cally reason about skills and beliefs pertaining to mod-
eling the environment. Task allocation and coordination
were considered as fundamental challenges in early DAI
systems. The contract net protocol developed by [68] pro-
vided the basis for modeling collaboration in simulation of
distributed problem solving.
Agent Simulation Architectures
One of the ﬁrst agent-oriented simulation languages,
AGENT-0 [69], provided a framework that enabled the
representation of beliefs and intentions of agents. Un-
like object-oriented simulation languages such as SIM-

362 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 17
Agent-directed simulation framework
ULA 67 [70], the ﬁrst object-oriented language for spec-
ifying discrete-event systems, AGENT-O and McCarthy’s
Elephant2000 language incorporated speech act theory to
provide ﬂexible communication mechanisms for agents.
DAI and cognitive psychology inﬂuenced the develop-
ment of cognitive agents such as those found in AGENT-0,
e. g., the Belief-Desires-Intentions (BDI) framework [71].
However, procedural reasoning and control theory pro-
vided a basis for the design and implementation of reac-
tive agents. Classical control theory enables the speciﬁca-
tion of a mathematical model that describes the interaction
of a control system and its environment. The analogy be-
tween an agent and control system facilitated the formal-
ization of agent interactions in terms of a formal speciﬁ-
cation of dynamic systems. The shortcomings of reactive
agents (i. e., lack of mechanisms of goal-directed behav-
ior) and cognitive agents (i. e., issues pertaining to com-
putational tractability in deliberative reasoning) led to the
development of hybrid architectures such as the RAP sys-
tem [72].
Agents are often viewed as design metaphors in the
development of models for simulation and gaming. Yet,
this narrow view limits the potential of agents in improv-
ing various other dimensions of simulation. To this end,
Fig. 17 presents a uniﬁed paradigm of Agent-Directed
Simulation that consists of two categories as follows: (1)
Simulation for Agents (agent simulation), i. e., simulation
of systems that can be modeled by agents (in engineering,
human and social dynamics, military applications etc.)
and (2) Agents for Simulation that can be grouped under
two groups: agent-supported simulation and agent-based
simulation.
Agent Simulation
Agent simulation involves the use of agents as design
metaphors in developing simulation models. Agent sim-
ulation involves the use of simulation conceptual frame-
works (e. g., discrete-event, activity scanning) to simulate
the behavioral dynamics of agent systems and incorpo-
rate autonomous agents that function in parallel to achieve
their goals and objectives. Agents possess high-level in-
teraction mechanisms independent of the problem being
solved. Communication protocols and mechanisms for in-
teraction via task allocation, coordination of actions, and
conﬂict resolution at varying levels of sophistication are
primary elements of agent simulations. Simulating agent
systems requires understanding the basic principles, orga-
nizational mechanisms, and technologies underlying such
systems.
Agent-Based Simulation
Agent-based simulation is the use of agent technology to
monitor and generate model behavior. This is similar to
the use of AI techniques for the generation of model be-
havior (e. g., qualitative simulation and knowledge-based
simulation). Development of novel and advanced simula-
tion methodologies such as multisimulation suggests the
use of intelligent agents as simulator coordinators, where
run-time decisions for model staging and updating takes
place to facilitate dynamic composability. The perception
feature of agents makes them pertinent for monitoring
tasks. Also, agent-based simulation is useful for having
complex experiments and deliberative knowledge process-
ing such as planning, deciding, and reasoning. Agents are

Artificial Intelligence in Modeling and Simulation
A
363
also critical enablers to improve composability and inter-
operability of simulation models [73].
Agent-Supported Simulation
Agent-supported simulation deals with the use of agents
as a support facility to enable computer assistance by en-
hancing cognitive capabilities in problem speciﬁcation and
solving. Hence, agent-supported simulation involves the
use of intelligent agents to improve simulation and gaming
infrastructures or environments. Agent-supported simu-
lation is used for the following purposes:
 to provide computer assistance for front-end and/or
back-end interface functions;
 to process elements of a simulation study symbolically
(for example, for consistency checks and built-in relia-
bility); and
 to provide cognitive abilities to the elements of a sim-
ulation study, such as learning or understanding abili-
ties.
For instance, in simulations with defense applications,
agents are often used as support facilities to
 see the battleﬁeld,
 fuse, integrate and de-conﬂict the information pre-
sented by the decision-maker,
 generate alarms based on the recognition of speciﬁc
patterns,
 Filter, sort, track, and prioritize the disseminated infor-
mation, and
 generate contingency plans and courses of actions.
A signiﬁcant requirement for the design and simulation of
agent systems is the distributed knowledge that represents
the mental model that characterizes each agent’s beliefs
about the environment, itself, and other agents. Endomor-
phic agent concepts provide a framework for addressing
the diﬃcult conceptual issues that arise in this domain.
Endomorphic Agents
We now consider an advanced feature that an auton-
omous, integrated and comprehensive modeler/simula-
tionist agent must have if it is fully emulate human capa-
bility. This is the ability, to a limited, but signiﬁcant extent,
to construct and employ models of its own mind as well of
the minds of other agents. We use the term “mind” in the
sense just discussed.
The concept of an endomorphic agent is illustrated in
Fig. 18 and 19 in a sequence of related diagrams. The di-
agram labeled with an oval with embedded number 1 is
that of Fig. 9 with the modiﬁcations mentioned earlier to
match up with human motivation and desire generation
modules. In diagram 2, the label “mind” refers to the set of
M&S capabilities depicted in Fig. 9. As in [30], an agent,
human or technological, is considered to be composed of
a mind and a body. Here, “body” represents the external
manifestation of the agent, which is observable by other
agents. Whereas, in contrast, mind is hidden from view
and must be a construct, or model, of other agents. In
other words, to use the language of evolutionary psychol-
ogy, agents must develop a “theory of mind” about other
agents from observation of their external behavior. An en-
domorphic agent is represented in diagram 3 with a men-
tal model of the body and mind of the agent in diagram 2.
This second agent is shown more explicitly in diagram 4,
with a mental representation of the ﬁrst agent’s body and
mind. Diagram 5 depicts the recursive aspect of endomor-
phism, where the (original) agent of diagram 2 has devel-
oped a model of the second agent’s body and mind. But the
latter model contains the just-mentioned model of the ﬁrst
agent’s body and mind. This leads to a potentially inﬁnite
regress in which—apparently—each agent can have a rep-
resentation of the other agent, and by reﬂection, of him-
self, that increases in depth of nesting without end. Hofs-
tadter [59] represents a similar concept in the diagram on
page 144 of his book, in which the comic character Sluggo
is “dreaming of himself dreaming of himself dreaming of
himself, without end.” He then uses the label on the Mor-
ton Salt box on page 145 to show how not all self reference
involves inﬁnite recursion. On the label, the girl carrying
the salt box obscures its label with her arm, thereby shut-
ting down the regress. Thus, the salt box has a represen-
tation of itself on its label but this representation is only
partial.
Reference [30] related the termination in self-reference
to the agent’s objectives and requirements in construct-
ing models of himself, other agents, and the environment.
Brieﬂy, the agent need only to go as deep as needed to
get a reliable model of the other agents. The agent can
stop at level 1 with a representation of the other’s bodies.
However, this might not allow predicting another’s move-
ments, particularly if the latter has a mind in control of
these movements. This would force the ﬁrst agent to in-
clude at least a crude model of the other agent’s mind. In
a competitive situation, having such a model might give
the ﬁrst agent an advantage and this might lead the second
agent to likewise develop a predictive model of the ﬁrst
agent. With the second agent now seeming to become less
predictable, the ﬁrst agent might develop a model of the
second agent’s mind that restores lost predictive power.
This would likely have to include a reﬂected representation
of himself, although the impending regression could be

364 A
Artificial Intelligence in Modeling and Simulation
Artificial Intelligence in Modeling and Simulation, Figure 18
M&S within mind
Artificial Intelligence in Modeling and Simulation, Figure 19
Emergence of endomorphic agents
halted if this representation did not, itself, contain a model
of the other agent. Thus, the depth to which competitive
endomorphic agents have models of themselves and others
might be the product of a co-evolutionary “mental arms
race” in which an improvement in one side triggers a con-
tingent improvement in the other—the improvement be-
ing an incremental reﬁnement of the internal models by
successively adding more levels of nesting.
Minsky [60] conjectured that termination of the po-
tentially inﬁnite regress in agent’s models of each other
within a society of mind might be constrained by shear
limitations on the ability to martial the resources required
to support the necessary computation. We can go further
by assuming that agents have diﬀering mental capacities to
support such computational nesting. Therefore, an agent
with greater capacity might be able to “out think” one of
lesser capability. This is illustrated by the following real-
life story drawn from a recent newspaper account of a crit-
ical play in a baseball game.
Interacting Models of Others in Competitive Sport
The following account is illustrated in Fig. 20.
A Ninth Inning to Forget
Cordero Can’t Close, Then Base-Running Gaﬀe Ends
Nats’ Rally

Artificial Intelligence in Modeling and Simulation
A
365
Artificial Intelligence in Modeling and Simulation, Figure 20
Interacting models of others in baseball
Steve Yanda – Washington Post StaﬀWriter
Jun 24, 2007
Copyright The Washington Post Company Jun 24,
2007Indians 4, Nationals 3
Nook Logan played out the ending of last night’s
game in his head as he stood on second base in the
bottom of the ninth inning. The bases were loaded
with one out and the Washington Nationals trailed
by one run. Even if Felipe Lopez, the batter at the
plate, grounded the ball, say, right back to Cleveland
Indians closer Joe Borowski, the pitcher merely would
throw home. Awaiting the toss would be catcher Kelly
Shoppach, who would tag the plate and attempt to
nail Lopez at ﬁrst. By the time Shoppach’s throw
reached ﬁrst baseman Victor Martinez, Logan ﬁg-
ured he would be gliding across the plate with the ty-
ing run. Lopez did ground to Borowski, and the closer
did ﬁre the ball home. However, Shoppach elected to
throw to third instead of ﬁrst, catching Logan drifting
too far oﬀthe bag for the ﬁnal out in the Nationals’
4–3 loss at RFK Stadium. “I thought [Shoppach] was
going to throw to ﬁrst,” Logan said. And if the catcher
had, would Logan have scored all the way from sec-
ond? “Easy.”
We’ll analyze this account to show how it throws light
on the advantage rendered by having an endomorphic ca-
pability to process to a nesting depth exceeding that of an
opponent.
The situation starts the bottom of the ninth inning
with the Washington Nationals at bats having the bases
loaded with one out and trailing by one run. The runner on
second base, Nook Logan plays out the ending of the game
in his head. This can be interpreted in terms of endomor-
phic models as follows. Logan makes a prediction using his
models of the opposing pitcher and catcher, namely that
the pitcher would throw home and the catcher would tag
the plate and attempt to nail the batter at ﬁrst. Logan then
makes a prediction using a model of himself, namely, that
he would be able to reach home plate while the pitcher’s
thrown ball was traveling to ﬁrst base.
In actual play, the catcher threw the ball to third and
caught Logan out. This is evidence that the catcher was
able to play out the simulation to a greater depth then was
Logan. The catcher’s model of the situation agreed with
that of Logan as it related to the other actors. The dif-
ference was that the catcher used a model of Logan that
predicted that the latter (Logan) would predict the he (the
catcher) would throw to ﬁrst. Having this prediction, the
catcher decided instead to throw the ball to the second
baseman which resulted in putting Logan out. We note
that the catcher’s model was based on his model of Logan’s
model so it was at one level greater in depth of nesting than
the latter. To have succeeded, Logan would have had to be

366 A
Artificial Intelligence in Modeling and Simulation
able to support one more level, namely, to have a model
of the catcher that would predict that the catcher would
use the model of himself (Logan) to out-think him and
then make the counter move, not to start towards third
base.
The enigma of such endomorphic agents provides ex-
treme challenges to further research in AI and M&S.
The formal and computational framework that the M&S
framework discussed here provides may be of particular
advantage to cognitive psychologists and philosophers in-
terested in an active area of investigation in which the
terms “theory of mind,” “simulation,” and “mind reading”
are employed without much in the way of deﬁnition [61].
Future Directions
M&S presents some fundamental and very diﬃcult prob-
lems whose solution may beneﬁt from the concepts and
techniques of AI. We have discussed some key problem
areas including veriﬁcation and validation, reuse and com-
posability, and distributed simulation and systems of sys-
tems interoperability. We have also considered some ar-
eas of AI that have direct applicability to problems in
M&S, such as Service-Oriented Architecture and Seman-
tic Web, ontologies, constrained natural language, and ge-
netic algorithms. In order to provide a unifying theme
for the problem and solutions, we raised the question of
whether all of M&S can be automated into an integrated
autonomous artiﬁcial modeler/simulationist. We explored
an approach to developing such an intelligent agent based
on the System Entity Structure/Model Base framework,
a hybrid methodology that combines elements of AI and
M&S. We proposed a concrete methodology by which
such an agent could engage in M&S based on activity
tracking. There are numerous challenges to AI that imple-
menting such a methodology in automated form presents.
We closed with consideration of endomorphic modeling
capability, an advanced feature that such an agent must
have if it is to fully emulate human M&S capability. Since
this capacity implies an inﬁnite regress in which models
contain models of themselves without end, it can only be
had to a limited degree. However, it may oﬀer critical in-
sights into competitive co-evolutionary human or higher
order primate behavior to launch more intensive research
into model nesting depth. This is the degree to which an
endomorphic agent can marshal mental resources needed
to construct and employ models of its own “mind” as well
of the ‘minds” of other agents. The enigma of such endo-
morphic agents provides extreme challenges to further re-
search in AI and M&S, as well as related disciplines such
as cognitive science and philosophy.
Bibliography
Primary Literature
1. Wymore AW (1993) Model-based systems engineering: An in-
troduction to the mathematical theory of discrete systems and
to the tricotyledon theory of system design. CRC, Boca Raton
2. Zeigler BP, Kim TG, Praehofer H (2000) Theory of modeling and
simulation. Academic Press, New York
3. Ören TI, Zeigler BP (1979) Concepts for advanced simulation
methodologies. Simulation 32(3):69–82
4. http://en.wikipedia.org/wiki/DEVS Accessed Aug 2008
5. Knepell PL, Aragno DC (1993) Simulation validation: a confi-
dence assessment methodology. IEEE Computer Society Press,
Los Alamitos
6. Law AM, Kelton WD (1999) Simulation modeling and analysis,
3rd edn. McGraw-Hill, Columbus
7. Sargent RG (1994) Verification and validation of simulation
models. In: Winter simulation conference. pp 77–84
8. Balci O (1998) Verification, validation, and testing. In: Winter
simulation conference.
9. Davis KP, Anderson AR (2003) Improving the composability of
department of defense models and simulations, RAND techni-
cal report. http://www.rand.org/pubs/monographs/MG101/.
Accessed Nov 2007; J Def Model Simul Appl Methodol Tech-
nol 1(1):5–17
10. Ylmaz L, Oren TI (2004) A conceptual model for reusable sim-
ulations within a model-simulator-context framework. Con-
ference on conceptual modeling and simulation. Conceptual
Models Conference, Italy, 28–31 October, pp 28–31
11. Traore M, Muxy A (2004) Capturing the dual relationship be-
tween simulation models and their context. Simulation prac-
tice and theory. Elsevier
12. Page E, Opper J (1999) Observations on the complexity of com-
posable simulation. In: Proceedings of winter simulation con-
ference, Orlando, pp 553–560
13. Kasputis S, Ng H (2000) Composable simulations. In: Proceed-
ings of winter simulation conference, Orlando, pp 1577–1584
14. Sarjoughain HS (2006) Model composability. In: Perrone LF,
Wieland FP, Liu J, Lawson BG, Nicol DM, Fujimoto RM (eds) Pro-
ceedings of the winter simulation conference, pp 104–158
15. DiMario MJ (2006) System of systems interoperability types
and characteristics in joint command and control. In: Proceed-
ings of the 2006 IEEE/SMC international conference on system
of systems engineering, Los Angeles, April 2006
16. Sage AP, Cuppan CD (2001) On the systems engineering and
management of systems of systems and federation of systems.
Information knowledge systems management, vol 2, pp 325–
345
17. Dahmann JS, Kuhl F, Weatherly R (1998) Standards for simula-
tion: as simple as possible but not simpler the high level archi-
tecture for simulation. Simulation 71(6):378
18. Sarjoughian HS, Zeigler BP (2000) DEVS and HLA: Complimen-
tary paradigms for M&S? Trans SCS 4(17):187–197
19. Yilmaz L (2004) On the need for contextualized introspective
simulation models to improve reuse and composability of de-
fense simulations. J Def Model Simul 1(3):135–145
20. Tolk A, Muguira JA (2003) The levels of conceptual interoper-
ability model (LCIM). In: Proceedings fall simulation interoper-
ability workshop, http://www.sisostds.org Accessed Aug 2008
21. http://en.wikipedia.org/wiki/SimulaAccessed Aug 2008

Artificial Intelligence in Modeling and Simulation
A
367
22. http://en.wikipedia.org/wiki/Java Accessed Aug 2008
23. http://en.wikipedia.org/wiki/Expert_system
Accessed
Aug
2008
24. http://en.wikipedia.org/wiki/Frame_language Accessed Aug
2008
25. http://en.wikipedia.org/wiki/Agent_based_model
Accessed
Aug 2008
26. http://www.swarm.org/wiki/Main_Page Accessed Aug 2008
27. Unified Modeling Language (UML) http://www.omg.org/
technology/documents/formal/uml.htm
28. Object Modeling Group (OMG) http://www.omg.org
29. http://en.wikipedia.org/wiki/Semantic_web
Accessed
Aug
2008
30. Zeigler BP (1990) Object Oriented Simulation with Hierarchi-
cal, Modular Models: Intelligent Agents and Endomorphic Sys-
tems. Academic Press, Orlando
31. http://en.wikipedia.org/wiki/Service_oriented_architecture
Accessed Aug 2008
32. Mittal S, Mak E, Nutaro JJ (2006) DEVS-Based dynamic model-
ing & simulation reconfiguration using enhanced DoDAF de-
sign process. Special issue on DoDAF. J Def Model Simul, Dec
(3)4:239–267
33. Ziegler BP (1988) Simulation methodology/model manipu-
lation. In: Encyclopedia of systems and controls. Pergamon
Press, England
34. Alexiev V, Breu M, de Bruijn J, Fensel D, Lara R, Lausen H (2005)
Information integration with ontologies. Wiley, New York
35. Kim L (2003) Official XMLSPY handbook. Wiley, Indianapolis
36. Zeigler BP, Hammonds P (2007) Modeling & simulation-based
data engineering: introducing pragmatics into ontologies
for net-centric information exchange. Academic Press, New
York
37. Simard RJ, Zeigler BP, Couretas JN (1994) Verb phrase model
specification via system entity structures. AI and Planning in
high autonomy systems, 1994. Distributed interactive simu-
lation environments. Proceedings of the Fifth Annual Confer-
ence, 7–9 Dec 1994, pp 192–1989
38. Checkland P (1999) Soft systems methodology in action. Wiley,
London
39. Holland JH (1992) Adaptation in natural and artificial systems:
An introductory analysis with applications to biology, control,
and artificial intelligence. MIT Press, Cambridge
40. Goldberg DE (1989) Genetic algorithms in search, optimization
and machine learning. Addison-Wesley Professional, Princeton
41. Davis L (1987) Genetic algorithms and simulated annealing.
Morgan Kaufmann, San Francisco
42. Zbigniew M (1996) Genetic algorithms + data structures = evo-
lution programs. Springer, Heidelberg
43. Cheon S (2007) Experimental frame structuring for automated
model construction: application to simulated weather genera-
tion. Doct Diss, Dept of ECE, University of Arizona, Tucson
44. Zeigler BP (1984) Multifaceted modelling and discrete event
simulation. Academic Press, London
45. Rozenblit JW, Hu J, Zeigler BP, Kim TG (1990) Knowledge-based
design and simulation environment (KBDSE): foundational
concepts and implementation. J Oper Res Soc 41(6):475–489
46. Kim TG, Lee C, Zeigler BP, Christensen ER (1990) System entity
structuring and model base management. IEEE Trans Syst Man
Cyber 20(5):1013–1024
47. Zeigler BP, Zhang G (1989) The system entity structure: knowl-
edge representation for simulation modeling and design. In:
Widman LA, Loparo KA, Nielsen N (eds) Artificial intelligence,
simulation and modeling. Wiley, New York, pp 47–73
48. Luh C, Zeigler BP (1991) Model base management for multi-
faceted systems. ACM Trans Model Comp Sim 1(3):195–218
49. Couretas J (1998) System entity structure alternatives enumer-
ation environment (SEAS). Doctoral Dissertation Dept of ECE,
University of Arizona
50. Hyu C Park, Tag G Kim (1998) A relational algebraic framework
for VHDL models management. Trans SCS 15(2):43–55
51. Chi SD, Lee J, Kim Y (1997) Using the SES/MB framework to an-
alyze traffic flow. Trans SCS 14(4):211–221
52. Cho TH, Zeigler BP, Rozenblit JW (1996) A knowledge based
simulation environment for hierarchical flexible manufactur-
ing. IEEE Trans Syst Man Cyber- Part A: Syst Hum 26(1):81–91
53. Carruthers P (2006) Massively modular mind architecture the
architecture of the mind. Oxford University Press, USA, pp 480
54. Wolpert L (2004) Six impossible things before breakfast: The
evolutionary origin of belief, W.W. Norton London
55. Zeigler BP (2005) Discrete event abstraction: an emerging
paradigm for modeling complex adaptive systems perspec-
tives on adaptation, In: Booker L (ed) Natural and artificial sys-
tems, essays in honor of John Holland. Oxford University Press,
Oxford
56. Nutaro J, Zeigler BP (2007) On the stability and performance
of discrete event methods for simulating continuous systems.
J Comput Phys 227(1):797–819
57. Muzy A, Nutaro JJ (2005) Algorithms for efficient implementa-
tion of the DEVS & DSDEVS abstract simulators. In: 1st Open In-
ternational Conference on Modeling and Simulation (OICMS).
Clermont-Ferrand, France, pp 273–279
58. Muzy A The activity paradigm for modeling and simulation of
complex systems. (in process)
59. Hofstadter D (2007) I am a strange loop. Basic Books
60. Minsky M (1988) Society of mind. Simon & Schuster, Goldman
61. Alvin I (2006) Goldman simulating minds: the philosophy, psy-
chology, and neuroscience of mindreading. Oxford University
Press, USA
62. Denning PJ (2007) Computing is a natural science. Commun
ACM 50(7):13–18
63. Luck M, McBurney P, Preist C (2003) Agent technology: en-
abling next generation computing a roadmap for agent based
computing. Agentlink, Liverpool
64. Miller JH, Page SE (2007) Complex adaptive systems: an intro-
duction to computational models of social life. Princeton Uni-
versity Press, Princeton
65. Ferber J (1999) Multi-Agent systems: an introduction to dis-
tributed artificial intelligence. Addison-Wesley, Princeton
66. Gasser L, Braganza C, Herman N (1987) Mace: a extensible
testbed for distributed AI research. Distributed artificial intel-
ligence – research notes in artificial intelligence, pp 119–152
67. Agha G, Hewitt C (1985) Concurrent programming using ac-
tors: exploiting large-scale parallelism. In: Proceedings of the
foundations of software technology and theoretical computer
science, Fifth Conference, pp 19–41
68. Smith RG (1980) The contract net protocol: high-level commu-
nication and control in a distributed problem solver. IEEE Trans
Comput 29(12):1104–1113
69. Shoham Y (1993) Agent-oriented programming. Artif Intell
60(1):51–92
70. Dahl OJ, Nygaard K (1967) SIMULA67 Common base definiton.
Norweigan Computing Center, Norway

368 A
Astronomical Time Series, Complexity in
71. Rao AS, George MP (1995) BDI-agents: from theory to practice.
In: Proceedings of the first intl. conference on multiagent sys-
tems, San Francisco
72. Firby RJ (1992) Building symbolic primitives with continuous
control routines. In: Procedings of the First Int Conf on AI Plan-
ning Systems. College Park, MD pp 62–29
73. Yilmaz L, Paspuletti S (2005) Toward a meta-level framework
for agent-supported interoperation of defense simulations.
J Def Model Simul 2(3):161–175
Books and Reviews
Alexiev V, Breu M, de Bruijn J, Fensel D, Lara R, Lausen H (2005) In-
formation integration with ontologies. Wiley, New York
Alvin I (2006) Goldman Simulating Minds: The philosophy, psy-
chology, and neuroscience of mindreading. Oxford University
Press, USA
CarruthersP (2006) Massively modular mind architectureThe archi-
tecture of the mind. http://www.amazon.com/exec/obidos/
search-handle-url/102-1253221-6360121
John H (1992) Holland adaptation in natural and artificial systems:
an introductory analysis with applications to biology, control,
and artificial intelligence. The MIT Press Cambridge
Zeigler BP (1990) Object oriented simulation with hierarchical,
modular models: intelligent agents and endomorphic systems.
Academic Press, Orlando
Zeigler BP, Hammonds P (2007) Modeling & simulation-based data
engineering: introducing pragmatics into ontologies for net-
centric information exchange. Academic Press, New York
Zeigler BP, Kim TG, Praehofer H (2000) Theory of modeling and sim-
ulation. Academic Press, New York
Astronomical Time Series,
Complexity in
JEFFREY D. SCARGLE
Space Science Division, NASA Ames Research Center,
Moﬀett Field, USA
Article Outline
Glossary
Prolog: The Story of Goldilocks and the Three Time Series
Deﬁnition of the Subject
Introduction
Complex Time Series in Astronomy
Chaotic Dynamics in Celestial Mechanics
Chaotic Time Series Analysis in Astronomy
Future Directions
Bibliography
Glossary
Dynamical system A mathematical or physical system of
which the evolution forward in time is determined by
certain laws of motion – deterministic, random, or
a combination.
Deterministic system A dynamical system the future of
which, except for noise, can be exactly predicted based
on past data.
Chaotic system A dynamical system the future of which
can be predicted for short times, but due to the expo-
nential growth of noise (“sensitive dependence on ini-
tial conditions”) long term prediction is not possible.
Random system A dynamical system the future of which
cannot be predicted, based on past data.
Linear system Consider a dynamical system described in
part by two variables (representing forces, displace-
ments, or the like), one of which is viewed as input and
the other as output. The part of the system so described
is said to be linear if the output is a linear function of
the input. Examples: (a) a spring is linear in a small
force applied to the spring, if the displacement is small;
(b) the Fourier transform (output) of a time series (in-
put) is linear. Example (b) is connected with the myth
that a linear transform is not useful for “linear data.”
(To describe given time series data as linear is an abuse
of the term. The concept of linearity cannot apply to
data, but rather to a model or model class describing
the data. In general given data can be equally well de-
scribed by linear and nonlinear models.)
Phase portrait A picture of the evolution of a dynami-
cal system, in a geometrical space related to the phase
space of the system. This picture consists of a sampling
of points tracing out the system trajectory over time.
Stationary system If the statistical descriptors of a dy-
namical system do not change with time, the system
is said to be stationary. In practice, with ﬁnite data
streams, the mathematical deﬁnitions of the various
kinds of stationarity cannot be implemented, and one
is forced to consider local or approximate stationarity.
Prolog: The Story of Goldilocks
and the Three Time Series
Once upon a time there was a little girl named
Goldilocks. She went for a walk in the forest. Pretty
soon she came upon an observatory. She knocked
and, when no one answered, she walked right in.
On the server in the computer room, there were
three data sets. Goldilocks was hungry to ﬁnd magi-
cal, simple dynamics underlying an apparently com-
plicated time series. She tested the time series from
the ﬁrst data set.
“This time series is too noisy!” she exclaimed.

Astronomical Time Series, Complexity in
A
369
So, she tested the time series from the second data
set.
“This time series is too short,” she said
So, she analyzed the last time series.
“Ahhh, this time series is just right,” she said happily
and she analyzed it. “Wow, look, I found a ‘chaotic
attractor’ and a small, non-integer embedding di-
mension, so this must be a chaotic astronomical sys-
tem!” Goldilocks began to speculate about the un-
derlying physics, but there were many detailed mod-
els possible. Goldilocks fell asleep. When she awoke
later, she realized that this had all been a fairy-tale
dream.
THE END
Definition of the Subject
The subject of this article is time series analysis as a tool
for analyzing complex astrophysical systems. A time se-
ries is a set of sequential measurements where informa-
tion about the measurement times is contained in the data
record. It is implicit that the goal is to study the varia-
tion of the measured quantity, ruling out measurements
repeated simply to more accurately determine a value, say
be averaging.
A number of developments are combining to markedly
increase the importance of this topic in modern astro-
physics. The ﬁrst is the growing realization that many
astronomical objects are dynamically changing in compli-
cated ways, yielding intensity changes that have an unpre-
dictable, stochastic nature. In short, many of the objects
previously thought to be evolving only slowly and serenely,
and on very long time scales, are in actuality complex sys-
tems – if not chaotic in a rigorous sense, at least behaving
stochastically. This development has obviously been fueled
by the recent rapid progress in complex systems theory.
Less obvious is the role of improved observational tech-
niques. It is now much easier to distinguish randomness
in time series data due to observational errors from in-
trinsic – regular or stochastic – variability in the source it-
self. And in this Age of Digital Astronomy analysis of large
sets of high-quality time series is more and more crucial to
progress.
Introduction
Let’s begin with a brief description of dynamical systems,
in both the mathematical and physical sense. The former
is an algorithm put forth as a model of the latter. Dynamics
refers to the fact that the algorithm advances an initial state
Astronomical Time Series, Complexity in, Figure 1
Black box representation of a discrete, one-dimensional dynam-
ical system
in time, modeling physical forces that drive the temporal
behavior of an astrophysical system.
Now consider some simple dynamical systems that ex-
hibit complex behavior of the sort that has been called
chaos. Figure 1 depicts the essence of a simple, one dimen-
sional dynamical system, namely a procedure for generat-
ing the value of the variable X at the next future time n C 1
(the output), given the value of X at the current time n (the
input). The procedure starts at time zero, with initial con-
dition X0 as input, and is repeated indeﬁnitely, generat-
ing the time series fX0; X1; X2; X3; : : :g. Algebraically, the
above picture is represented as simply
XnC1 D f (Xn) ;
(1)
where f is the function representing the dynamics of the
process. The following ﬁgure plots an example of such
a dynamical system for a speciﬁc function f . The values
of X look rather random, although the behavior for small
values of X are a hint that some non-randomness is lurk-
ing here.
A commonly used test of whiteness is to compute the
autocorrelation function and/or the power spectrum of X.
Here are the results for the same process as above:
Note that the (symmetric) auto-correlation function
is essentially a delta function, signiﬁcantly diﬀerent from
zero only at zero lag. The power spectrum, of course, is
irregular, but it is consistent with being constant with fre-
quency. So our process appears to be white noise. Indeed,
the autocorrelation and power spectrum of this process are
a delta function and constant, respectively. The process X
is indeed white noise!
But you have just been led down a garden path, be-
cause the interpretation of this as being a random process
is wrong. Equation (1) precisely deﬁnes deterministic dy-
namics. The next future value is a deﬁnite, single-valued
function of the past value. So our white process can hardly
be truly random noise.
It is time to reveal X, as well as the fact that there is
a stronger form of randomness than described by the term
uncorrelated. The above ﬁgures all refer to the famous lo-

370 A
Astronomical Time Series, Complexity in
Astronomical Time Series, Complexity in, Figure 2
Time series generated by a simple nonlinear dynamical system. There are N D 128 time samples plotted
gistic map, namely
XnC1 D f (Xn) D Xn(1  Xn) ;
(2)
in what follows  D 4, a special value that makes X max-
imally chaotic. The interested reader should consult one
of the many books discussing the extraordinarily complex
behavior of this nonlinear system as a function of , in-
cluding proof that it is a “white” process, i. e. has a constant
power spectrum and a delta-function autocorrelation, for
 D 4.
The point here is that the simple ﬁrst-order diagnos-
tics – autocorrelations and power spectra – as useful as
they are, are not helpful at identifying the nature of our
process. Let’s turn to basics. The essence of a random pro-
cess is in its joint probability distributions, not in its cor-
relations. The joint distribution of Xn and Xm is deﬁned as
the probability that the two variables take arbitrary values,
thusly:
P(a; b) D Prob(Xn D a and Xm D b) ;
(3)
where the notation here emphasizes that the arguments
of P are dummy variables. The more common, if slightly
misleading, notation is P(Xn; Xm). One can estimate this
quantity with 2D histograms – i. e., simply count pairs
of values in 2D bins. Figure 4 is such an estimate of
P(Xn; XnC1). If the process X were truly random, all of
the variables Xn would be statistically independent of each
other. Independence is deﬁned by the relation
Prob(Xn D a and Xm D b)
D Prob(Xn D a) Prob(Xm D b) :
(4)
In words, two variables are independent if their joint prob-
ability distribution is the product of their individual distri-
butions. It should be clear from the ﬁgure that the logistic
process is very far from independent; on the contrary, the
value of XnC1 is determined completely by the value of Xn.
Unfortunately, the concepts of uncorrelatedness and inde-
pendence are frequently confused. In addition, scientists
often derive results using the correlation function when
they could derive much more powerful results using prop-
erties of joint probability distributions.

Astronomical Time Series, Complexity in
A
371
Astronomical Time Series, Complexity in, Figure 3
Autocorrelation and power spectrum from the same process as in Fig. 2 (but with twice as many time points, N D 256)
Complex Time Series in Astronomy
The ﬁrst time series data in recorded history were prob-
ably astronomical in origin. It was important for the an-
cients to keep track of the motions of celestial objects, if
only for predicting the arrival of each year’s planting sea-
son. This agricultural service involved noting the passage
of time and the location of the sun relative to other objects
in the sky. Well-known examples of this connection be-
tween astronomical observations over time and practical
aspects of agriculture punctuate the history of the ancient
Egyptian the Mayan culture. There are presumably many
other examples that have not survived the ravages of time –
or the ravages of “all the wrong explorers and discoverers –
thieves planting ﬂags, murderers carrying crosses whom
we have been raised to honor” (Peter S. Beagle, preface to
J. R. R. Tolkien’s “The Hobbit”).
Perhaps the most famous historical incident of rele-
vance is Gauss’s successful determination in 1801 of the
orbit of Ceres from a mere 41 days of data, over which
the asteroid moved only 9 degrees on the sky. Gauss had
invented the method of least squares some years earlier,
and it was relatively easy (only 100 hours of computation
time!) to apply this method to the problem of determining
an orbit of an object moving around the sun, from a time
series of its positions on the sky.
“... for it is now clearly shown that the orbit of
a heavenly body may be determined quite nearly
from good observations embracing only a few days;
and this without any hypothetical assumption.”
Carl Friedrich Gauss
From the dawn of recorded astronomical observations,
as perhaps implemented in prehistoric sites such as Stone-
henge, to the Space Age, astronomical time series have
been central to scientiﬁc exploration. The underlying goal
is to be able to understand the dynamical forces driving the
underlying physical processes and evidenced by variability
in the observed time series. Starting with Poincaré in the
later part of the 19th century, whose work was largely in
the context of planetary motions, astronomers faced the
peculiar possibility that simple, deterministic astrophysi-
cal systems could have complex, apparently random be-
havior. For example, celestial mechanicians realized that
some of the planetary motions they had been dealing with

372 A
Astronomical Time Series, Complexity in
Astronomical Time Series, Complexity in, Figure 4
Histogram estimate of the joint probability distribution of Xn and XnC1 using data from the logistic process described in the previous
figures
over the ages are in reality examples of chaos – the evolu-
tion of physical systems in time in a deterministic yet dis-
ordered way, often summarized as “simple systems with
complex behavior.”
How can a simple and perfectly deterministic physi-
cal system produce complex and unpredictable behavior?
The answer lies in a kind of instability – sensitivity to ini-
tial conditions. A nonlinear, chaotic system has this prop-
erty in the sense that evolutionary paths starting from two
arbitrarily close initial conditions separate exponentially
with time. This property leads to an ampliﬁcation of the
inevitable initial uncertainties that forecloses prediction of
the future. In the hope that complex, seemingly random
behaviors observed in some systems could have simple ex-
planations, there was in previous decades a spasm of ac-
tivity in astrophysics, primarily in two complementary ar-
eas: (a) theoretical computations of the nonlinear dynam-
ical models of astrophysical systems, and (b) analysis of
time series and other data from astrophysical systems, in
an eﬀort to uncover the simplicity underlying the appar-
ent complexity. The ﬁrst of these is treated in the article
Chaos and Complexity in Astrophysics, by Oded Regev,
elsewhere in this Encyclopedia and rather brieﬂy in the
next section, while the second – being the current focus –
will be treated in more detail in the rest of this article.
Chaotic Dynamics in Celestial Mechanics
The period around 1980–1990 saw intensive development
in dynamical systems theory, particularly in celestial me-
chanics – i. e. the theoretical study of the dynamics and
kinematics of astronomical bodies, in our solar system, in
our galaxy, and elsewhere in the Universe. Lecar, Franklin,
Holman and Murray [40] review the topic of chaotic dy-
namics in the solar system.
As in the history of earlier developments in dynam-
ics (Galileo, Newton, Einstein, to name a few of the great
minds illuminating this grand passage in human knowl-
edge), astronomy played an important role in the origin
of complex systems theory. Jack Lissauer [41] provides
a comprehensive review of nonlinear dynamics in the solar
system, covering the extensive work of Wisdom, Laskar,
Duncan, and many other workers in this arena. The thread
here is that in many important situations the equations of
motion deﬁning the dynamical system are known exactly,
and the driving forces are known with great precision, so
that highly accurate numerical computations of the dy-
namical evolution of the systems are achievable. These
computations, and accompanying analytic studies, have
detailed a number of key examples of sensitivity to ini-
tial conditions and quasi-random wandering of state space

Astronomical Time Series, Complexity in
A
373
that are the hallmarks of deterministic chaos. One of the
earliest of these success stories was the seminal Ph. D. the-
sis of Jack Wisdom [76], which elucidated the structure
of gaps known in the spatial distribution of asteroids by
showing that the missing regions correspond to chaotic or-
bital behavior.
For the most part, these computations relate to the
very long timescales of many astronomical phenomena.
One exception – the astronomical system showing chaotic
behavior on the shortest time scale – is the fascinating
story of the rotation of Hyperion, an irregularly shaped
satellite of the planet Saturn. This satellite is unusual in
several ways: it shape is very irregular, as can be seen in
the dramatic false-color image in Fig. 5, its orbit is eccen-
tric (eccentricity e D 0:1), and it is relatively close to its
mother planet (the semi-major axis of its orbit is less than
25 times the radius of Saturn). These features of Hyper-
ion and its orbit conspire to produce strong and regularly
varying tidal forces on the satellite. Speciﬁcally, the diﬀer-
ential equation describing the time evolution of the angle 
between the long axis of the satellite and a reference direc-
tion is
d2/dt2 C (!2
0/2r3) sin 2(  f ) D 0 ;
(5)
where f is an angle (the true anomaly) specifying the posi-
tion of the satellite in its orbit, r is the radial distance of the
satellite from the planet, and !0 is a function of the princi-
ple moments of inertia of the satellite [42]. This nonlinear
equation has very interesting properties, and several au-
thors (e. g. Wisdom, Peale and Mignard [77]) have shown
that this system undergoes chaotic motion. The surface of
section, or state space comprised by  and d/dt, consists
of large regular regions, within which are chaotic islands
roughly associated with the spin orbit resonances 1:2, 1:1,
3:2, 2:1, 5:2, etc. This conﬁguration has marked similarities
with the three-body system, although it is of course much
simpler.
Detailed numerical computations of the Hyperion-
Saturn system predicted chaotic rotation of the satellite.
That is to say, the satellite’s rotation axis should tum-
ble chaotically and not remain even roughly pointed in
a ﬁxed direction. In a real sense, the magnitude and direc-
tion of rotation should not even have well deﬁned values
in the short term; further, the meaning of the long-term
averages would be problematic. This peculiar state of mo-
tion, unprecedented among the other satellites of the So-
lar System, was consistent with a number of close-up im-
ages of Hyperion obtained by the Voyager spacecraft, and
more recently by the Cassini spacecraft (Fig. 5). In addi-
tion, it was noted that a consequence of the irregular rota-
tion is that the brightness of Hyperion also be chaotic, be-
Astronomical Time Series, Complexity in, Figure 5
This image of Hyperion shows strange details across the strange,
tumbling moon’s surface. Differences in color could indicate dif-
ferences in the surface composition. This image was taken by the
NASA Cassini spacecraft during a close flyby on Sept. 26, 2005
cause the shape and reﬂectivity of the surface is irregular.
Klavetter [36] made time series measurements of Hyper-
ion’s brightness to study this question. The observations
were diﬃcult, due to the faintness of the satellite and the
background of light scattered in the telescope by the very
bright planet Saturn. In addition, he attempted to elucidate
the state of chaotic motion by ﬁnding parameter values for
a model of the rotational dynamics, by ﬁtting the model to
the time series data [36].
The chaotic rotation of Hyperion is the best case of
the connection between dynamical systems theory and
time series analysis, and the only case where the time
scales are short enough for detailed observational stud-
ies. The reader is referred to the excellent article, Chaos
and Complexity in Astrophysics, by Oded Regev, in this
Encyclopedia, for more details on the relevant theoretical
background of both this and other complex astronomical
systems.
Chaotic Time Series Analysis in Astronomy
We now turn to our main topic, the extraction of infor-
mation about complex astronomical systems by analyzing
observational time series data. Astronomical time series
have much in common with sequential data in other sci-
ences, but also have features of special concern. Astron-
omy is fundamentally an observational science. Direct ex-

374 A
Astronomical Time Series, Complexity in
periments are for the most part not feasible, and in some
cases one cannot even obtain repeated observations. Con-
sider the distribution of galaxies, cosmic background ra-
diation, etc. on the sky. Short of sending an astronomi-
cal observatory to another part of the Universe, there is
only one view of the sky. As a result, it can be very diﬃcult
to judge whether certain peculiar structures are accidental
(e. g. statistical ﬂuctuations connected with observational
errors) or physically signiﬁcant. Indeed, concepts such as
randomness, determinism, and probability need to be in-
terpreted in somewhat special ways in cosmology and the
study of large-scale structure of the Universe. (For an ex-
ample, see the discussion of the term cosmic variance, e. g.,
on Wikipedia.)
More germane to this discussion, many observational
time series cannot be obtained at evenly spaced time in-
tervals, as is assumed in the application of most standard
analysis methods. Gaps or other sampling irregularities
can interfere with the ability to characterize complex vari-
ations. This is especially true of low signal-to-noise obser-
vations with random sample times – for then, the com-
plexities of observational errors, sampling, and true system
behavior are confusingly tangled together. Even worse, the
sampling can be partially random, partially semi-regular,
dependent on previous values or estimated signal compo-
nents, or even dependent on the real-time perceived im-
portance of the observations to the public or to a funding
agency.
Another consideration, in many but by no means all
cases, is simply the quantity and quality of the data. If each
data point requires dedicating a large amount observing
time with an expensive telescope, long time series will not
be common. Too, the object of interest may be faint and af-
fected by instrumental and cosmic backgrounds. The an-
alyst may thus have to be content with short, noisy time
series. Early in the history of this subject most of the data
analysis methods were developed in the context of labora-
tory or computer studies, for which large amounts of data
were readily available and could be repeated (with diﬀer-
ent initial conditions or parameter values) at will. As will
be seen, diagnosing complex or chaotic behavior funda-
mentally requires access to data on the system’s return to
states very close to each other, but at wide range of times.
Hence the diﬃculties described here are particularly limit-
ing. Accordingly many standard methods were ineﬀective
on most astronomical time series data.
Now consider the setting in which most time series
analysis of complex systems is carried out, the embedding
procedure, with which it is possible to derive dynamical
information from observations of a subset of the variables
needed to specify the full dynamical behavior of the sys-
tem, or even from variables that are not even dynamical in
nature. This remarkable mathematical result means that
observations of subsidiary quantities that are not directly
involved in the dynamics can provide clues to underlying
chaotic dynamical processes. For example, observations of
brightness variations in stars can yield understanding of
the dynamics of the stellar interior. An unfortunate feature
of this mathematics is that the lack of direct connection be-
tween observable and the dynamics leaves room for data
analytic results of questionable physical signiﬁcance. The
interested reader should supplement the intuitive, non-
mathematical discussion here, with reference to one of the
many excellent books on this topic, such as Sprott [67].
A primary question facing the analyst is whether the
noisy data at hand were generated by a (simple) determin-
istic, chaotic, or random process – or some combination.
The deterministic nature of chaos means that knowledge
of the dynamical laws of the system and the current state,
as long as they are both known accurately enough, allow
accurate prediction of future states. Of course sensitive
dependence on initial data prevents this kind of predic-
tion from extending very far into the future. In contrast, if
the physical process is random, there is an intrinsic uncer-
tainty of future values even a short time into the future –
perhaps due to quantum mechanical eﬀects or the pres-
ence of complex, unmeasured hidden variables. One of the
methodologies of time series analysis is to use features re-
lating to these kinds of predictability (or lack thereof) to
diagnose the nature of the underlying physical system. For
practical purposes, then, short-term predictability is the
key: chaotic processes may be predicted accurately over
short periods of time, and random ones cannot.
It is important to note that even random noise can be
predicted to some extent. A trivial example is that even
white noise can be predicted by guessing that the next
value will be the mean value, either as determined exter-
nally or from past observations. More germane is the fact
that nonwhite noise can be predicted to an even greater de-
gree. Correlations present in such a process can yield a pre-
diction that is even better than the mean value. This partial
predictability needs to be distinguished from the more ac-
curate, but short term, predictability of a chaotic process.
At this point the reader will appreciate the diﬃculties
of detecting or characterizing the various kinds of com-
plex behavior from time series analysis. The remainder
of this section will describe various properties of time se-
ries data that are useful in diagnosing chaotic dynamics
and other complex behavior. Some are merely consistent
with chaotic dynamics and do not really prove that chaos
is present. Others are properties that of necessity must be
present if the underlying process is chaotic.

Astronomical Time Series, Complexity in
A
375
An obvious ﬁrst question is: To what extent the visual
appearance of a plot of the time series indicate the pres-
ence of chaos? Rather easily one can visually distinguish
stochastic processes from linearly deterministic ones, such
as periodic oscillations or polynomial trends. (Here the
term stochastic refers to the quality of disorder that is com-
mon to random and chaotic processes.) But the distinc-
tion between chaos and randomness is more subtle. As is
evident in Fig. 1, plots of chaotic and random data with
the same degree of correlation (e. g. white noise against
the output of the logistic map for  D 4) are nearly indis-
tinguishable. The reader is encouraged to make such plots
using Eq. (2) for white chaos and a random number gen-
erator for white noise.
The eye seems good at distinguishing order from dis-
order. Why do we have so much diﬃculty visualizing the
diﬀerence between chaos and randomness? The problem
lies not with the abilities of the human eye but largely with
the way the data are presented. The distinction between
various kinds of disorder, or stochastic behavior, reside in
a diﬀerent space than the standard time series plot, as will
soon be evident.
Part of the problem with visual recognition is that the
dependence of successive values of the observable X is not
revealed very well by plotting them, next to each other on
a time axis. This standard time series plot places too much
emphasis on the time at which the measurements were
made. What matters are the relationships between obser-
vations at successive times, not the absolute times of the
observations. This suggests plotting Xn against XnC1, as
opposed to Xn against n. Compare Figs. 2 and 4.
This simple idea has been around in many guises for
a long time, and probably cannot be credited to any one
person. A charming introduction to the resulting plot,
sometimes called a return map was given by Shaw [66] –
with the slight twist that he plotted successive time inter-
vals between discrete events (the falling of a water drop
from a drippy faucet) against each other, rather than the
values of an observable. This simple idea has grown into
the most powerful tool of chaotic time series analysis. To
see why this is so, we make what seems like a diversion –
but we’ll return soon.
This diversion is state space. It is generally useful to
study a physical system in this abstract space, the coordi-
nates of which are a complete set of independent dynam-
ical variables. The use of this space lies in the fact that
any conﬁguration of the system corresponds to a unique
point in this space, and vice versa. The system’s complete
evolution similarly corresponds to a trajectory or orbit in
state space. This surprisingly useful geometrical view is old
(cf. Poincare, whose studies of the three-body problem of
celestial mechanics, in the late 19th century, are consid-
ered by some to be the origin of the idea of chaos, and
certainly of the geometric view of dynamics) and well de-
scribed in the modern literature of dynamical systems, e. g.
Wiggins [75].
Unfortunately, it is rare that an experimentalist can
measure all the physical coordinates necessary to specify
the state of the system. Practical considerations often per-
mit measurement of only a few state variables or, even
worse, only peripheral quantities that depend indirectly
on the state. An example of this relation is the case of the
chaotic tumbling of Saturn’s satellite Hyperion [36], dis-
cussed above, where the overall brightness of the satellite
must serve as a surrogate for the set of variables needed to
specify the rotational state (rotation direction and magni-
tude, plus orbital parameters). The passive nature of astro-
nomical observations exacerbates this disconnect between
observables and dynamically relevant variables. A chemist
or physicist can insert a probe and measure things directly,
whereas the astronomer must accept those photons, parti-
cles, or perturbations of space-time that Nature chooses to
send our way.
Remarkably, extensive measurements of just one vari-
able can reveal the structure of the orbits in the full mul-
tivariate state space. Packard, Crutchﬁeld, Farmer, and
Shaw [51] and Takens [70] show that under certain con-
ditions on the dynamical system there is a multidimen-
sional embedding space, the coordinates of which can be
derived from a single observed variable. The time series
data generate trajectories in this space that are simply re-
lated to the system’s state-space orbits. This relation is in
the form of a smooth map, from the state space to the em-
bedding space, that preserves the topology of the evolu-
tionary paths. Thus the essential features of the unobserv-
able state-space orbits can be understood by studying the
accessible trajectories in the embedding space. In this anal-
ysis, one does not necessarily need to know the dimension
of the state space – or even the identity of its coordinates.
Of the many choices of embedding space coordinates
the most common is the observed variable evaluated at
a set of lagged times:
X D (Xn; XnCk; XnC2k; XnC3k; : : : ; XnC(M1)k) : (6)
The lag k and the embedding dimension M are positive
integers. In principle the lags need not be equal, but in
practice they are almost always chosen to be so. As time
goes by, points deﬁned by Eq. (6) fall on embedding-space
trajectories topologically equivalent to the system’s state-
space orbits. The trajectories can be traced out using time
series data, albeit crudely if the noise level is large. When
the system satisﬁes the requisite mathematical conditions

376 A
Astronomical Time Series, Complexity in
(and M is high enough) one says that one has constructed
a suitable embedding space.
We thus arrive at a most important tool for the analy-
sis of chaos, the phase portrait, also called the state-space
portrait. The phase portrait plays a key role in the search
for order in chaos. It can be constructed directly from
the time series data. In simplest form it is just a plot of
XnC1 against Xn. In other cases the process must be de-
scribed in a space of higher dimensions, and we add co-
ordinates XnC2; XnC3; : : : Also, the lag k need not be one
unit. A caution: the example described by Harding, Shin-
brot and Cordes [32] shows that one can be fooled by
the appearance of the phase portrait. They exhibit purely
random, non-chaotic models for their pulsar data which
produce phase portraits that show the interesting struc-
ture that is sometimes thought to uniquely characterize
chaos.
In practice, with real data, there are further compli-
cations. Observational errors always produce noise that
somewhat blurs the phase portrait. Yet with a suﬃciently
large signal-to-noise ratio this plot will reveal the nature of
the dynamics in spite of noise. One does not know a priori
the value of the lag. In the theorems justifying the embed-
ding procedure – namely inﬁnite, noise-free data streams –
the value of k does not matter. In practice k does matter,
and one must ﬁgure out a good value from the data. One
does not know a priori the value of the dimension M of the
embedding space. In theory M must be larger than 1 C 2d
(d is the dimension of the physical state space), but in prac-
tice considerably smaller values provide suitable embed-
dings. However, it may well be that the correct dimen-
sion for viewing the underlying process is higher than 3,
in which case there are problems displaying and visualiz-
ing the data.
The attractor of a physical system is, simply put, the
part of state space toward which the system tends to
evolve. The attractor ranges from simple closed curves in
the case of periodic motion to highly complex, high-di-
mensional fractal sets for chaotic motion [2].
Another view of the embedding procedure is that it is
the experimentalist’s view of the attractor of the system.
That is, the smooth transformation that maps state space
into the embedding space maps the physical attractor into
a set contained in the embedding space. The experimen-
talist cannot deal with the attractor itself, and must learn
about it by studying its image in the embedding space.
An important point about the phase portrait is sometimes
overlooked: the time sequence with which the sample
points traverse the embedding space is important. In
simple terms, if the points smoothly trace out a ﬁgure, no
matter how complex it is, the process is indicated to be
regular. On the other hand, if the points jump randomly,
from one part of the embedding space to another, and back
again, the process is indicated to be chaotic. While this be-
havior can be depicted with an animated display on a com-
puter screen, and captured in a video ﬁle, it is much harder
to display on a piece of paper or in a publication.
The dimension of the attractor (which clearly cannot
be larger than the embedding dimension) is an important
quantity. The relation between the attractor dimension,
the embedding dimension, the dimension of the embed-
ding-space image of the attractor, and a host of other di-
mension quantities that have been introduced is a complex
subject. See [23,70] for the pioneering work, and textbooks
such as [67] for up-to-date discussions.
In turn, estimation of the dimension of the attrac-
tor from the time series data is a complex topic. There
are many diﬀerent kinds of dimensions that can be de-
ﬁned, most of which can be estimated from data. There
are many diﬀerent techniques for estimating a given di-
mension. And a single number does not always char-
acterize fully the dimension of an attractor. Generalized
dimensions and a function which represents the distribu-
tion of dimensions have been introduced (e. g., Mandel-
brot [43,44], Theiler [72]).
The literature on this subject is too large to review
here. An overview [72] contains 143 references. The clas-
sic paper is by the Santa Cruz group [28]; see also [23]
for a very nice physical introduction. The book [45] con-
tains many articles on dimension and dimension estima-
tion. The statistics of dimension estimates (e. g. Holzfuss
and Meyer-Kress [34], Theiler [72], Smith 1990, Ramsey
and Yuan [54]) is of interest to experimentalists, for they
need to assess the reliability of their results. Unfortunately
the uncertainties due to other eﬀects [14], some of which
we will soon meet, may often overwhelm purely statistical
uncertainties.
The technique most commonly used by experimen-
talists provides an estimate of the correlation dimen-
sion [30,31]. This method implements the notion that the
rate at which the volume contained in a hyper sphere em-
bedded in M dimensional space grows with radius as RM.
Unfortunately there are severe problems with the use
of dimension estimates to diagnose chaotic dynamics. It
is fair to say that the technique is essentially useless for
detecting the presence of chaos in most astronomical ap-
plications. To start with, large amounts of data are nec-
essary to determine dimensions with any reliability. Next,
even modest noise can aﬀect the results [49]. Smooth-
ing of the data, due to either experimental eﬀects (e. g.,
ﬁnite-time-constant instrumentation) or physical eﬀects
(e. g., radiative transfer eﬀects in the case of astronom-

Astronomical Time Series, Complexity in
A
377
ical objects of variable luminosity) distorts the dimen-
sion [6,7,15,47,52,59,60,61,62].
Furthermore, there are more fundamental diﬃculties
with extracting useful dimensional information from time
series. Contrary to assumptions frequently made by early
workers in this ﬁeld, determination of a low dimension,
fractional or otherwise, does not prove the existence of
a chaotic process underlying the time series. In a seminal
paper, Osborne and Provenzale [50] showed that purely
non-chaotic colored noise has a low dimension as deter-
mined with the Grassberger–Procaccia and similar algo-
rithms. They showed that the dimension is a simple func-
tion of the steepness (quantiﬁed by a power-law index) of
the power spectrum. Accordingly, essentially any dimen-
sion can be produced by a purely random (no chaos need
apply!) process, with the appropriate power spectrum. But
see [53] for some methods to address the problem of dis-
tinguishing chaos from randomness, and [74] for a broad
overview of the problems encountered with astronomical
time series.
A second fundamental issue was raised by Ruelle and
Eckmann [58], who showed that there is a simple upper
limit to the value that can be obtained using the Grass-
berger–Procaccia or any other approach to dimension es-
timation where one regresses counts of points in boxes
(hyper cubes) in the embedding space against the box vol-
ume. By noting the limiting cases of one point per box and
all N points in the box, one sees that the maximum loga-
rithmic slope (which is the dimension estimate) of the re-
gression curve is log(N), approximately. This is an order
of magnitude estimate, and it hardly matters whether the
base of the logarithm is e or 10.
Hence if the true dimension is any value greater than
the logarithm of the number of data points, one will es-
timate a dimension D  log(N). Unfortunately the astro-
nomical literature is littered with dimension estimates in
the range D  2–3 or so, based on 100 to 1000 or so data
points, accompanied by breathless claims of detection of
chaos. See also [57] for other comments aimed at damp-
ening some of the enthusiasm in this arena.
In summary, for a number of reasons, estimates of the
“fractal dimension” of time series data is nearly useless
as a litmus test for nonlinear deterministic dynamics, or
chaos.
Period doubling and bifurcations are sometimes associ-
ated with chaotic dynamics. The idea is as follows: suppose
a dynamical system has periodic behavior, and the value
of the period changes as some parameter of the system
changes. An example is the logistic process of Eq. (2), for
some values of  less than 4. It is well known that this ex-
ceedingly simple system has exceedingly complex behav-
ior, including critical parameter values when one period
becomes two. Although some variable stars and other pe-
riodic astronomical systems exhibit this kind of behavior,
as a function of time and possibly related to underlying
changes in physical parameters, these phenomena are not
very common in astronomy, and therefore will not be dis-
cussed here. In any case, much of the theoretical research
in this area focuses on control – that is, where the param-
eter or parameters can be adjusted more or less arbitrarily.
Once again the passive, observational nature of astronomy
comes in to play. Nevertheless, there is at least one fea-
ture of time series data that has been taken as indicative
of possible period doubling, namely systematic amplitude
changes of an otherwise periodic process. For example, in
the light curve of a variable star, “the alternation of deep
and shallow minima” is some indication of period dou-
bling [13]. Clearly such a conclusion is speculative, and
would need to be supported by further observations, e. g.,
of the light curve as conditions in the stellar interior or at-
mosphere undergo secular changes.
It was remarked earlier that sensitivity to initial con-
ditions (SIC) is a key feature of chaos. There is a mea-
sure of the exponential divergence of trajectories, namely
Lyapunov exponents. There is a huge literature on this
concept (e. g. early papers such as, Eckmann, Kamphorst,
Ruelle and Ciliberto [21], Wolf, Swift, Swinney and Vas-
tano [78], and Sprott [67]) that measure this sensitivity.
To assess divergence of nearby trajectories from time se-
ries, it is necessary to have data describing more than one
trajectory, or a trajectory that returns rather close to ear-
lier points of itself. Measuring Lyapunov exponents, either
averaged over the trajectory or as a function of position
in the embedding space is obviously a very diﬃcult prob-
lem. As the Goldilocks tale at the beginning of this article
hinted, the short, noisy time series available to many ﬁelds
of observational astronomy are simply not suﬃcient to de-
termine the exponential divergences contained in the Lya-
punov exponent. There are no cases in astronomy where
a reliable Lyapunov exponent has been determined from
time series data.
Related analytic topics are prediction of time series
and determination of the equations of motion of the ob-
served system, to assess possible nonlinearity, random-
ness, nonstationarity, etc. Prediction of a process from ob-
servations of its past is as important in the analysis of
chaotic time series as it is for random ones. Accordingly
a large literature has developed (e. g. Abarbanel, Brown
and Kadtke [1], Crutchﬁeld [16], Crutchﬁeld and Mc-
Namara [18], Crutchﬁeld and Packard [19], Crutchﬁeld
and Young [20], Farmer and Sidorowich [24]). Various
authors have pursued the related topic of noise reduc-

378 A
Astronomical Time Series, Complexity in
tion [25,38,69]. Relevant scaling properties are discussed
by Atmanspacher, Scheingraber and Voges [4]. Hempel-
mann and Kurths [33] discuss analysis of the luminos-
ity of cataclysmic variables (SS Cygni in particular) us-
ing symbolic dynamics, an approach that has considerable
promise in this area [20]. But again, data limitations have
prevented much success in the application of these meth-
ods in astronomy.
Let’s end with a brief discussion of variability in
brightness of astronomical objects, in various wavelength
ranges. Regev [55] and Buchler and Regev [13] reviewed
theoretical work on possible chaotic dynamics of stel-
lar pulsations and analysis of variable star data. Numer-
ical work that suggests chaotic behavior in stellar mod-
els includes [3,11,12,39,48]. Gillet [29] on the other hand
does not conﬁrm the period-doubling bifurcation scenario
suggested by Kovács and Buchler [39] and intermittency
and period-doubling bifurcations suggested in the work of
Aikawa [4]. Serre, Buchler, and Goupil [65] study forecast-
ing of variable star light curves using nonlinear methods.
See also Kollath [37].
A ﬁnal study demonstrates a data-driven approach.
Scargle, Steiman-Cameron, Young, Donoho, Crutchﬁeld
and Imamura [64] started with a beautiful time series of
X-ray observations of the bright x-ray source in Scorpius
(Sco X-1) from the EXOSAT satellite. The observations
covered nearly 10 hours, with a time resolution of 2 ms –
thus covering 7 decades of time-scales. The only signiﬁ-
cant source of noise in these observations was the ﬂuc-
tuations associated with photon counting statistics, often
called Poisson noise. Above such noise background this
source evidences random variations of large amplitude
and over a wide range of time scales.
In carrying out simple exploratory time series plots, it
became evident that the general appearance of this vari-
ability is independent of time scale. This fractal or scal-
ing behavior is common astrophysical systems (see Man-
delbrot [43,44], and Flandrin [26] for the mathematics).
This led to consideration of nonlinear physical models
with outputs having such behavior. The classical dripping
faucet system, the ﬁrst archetypical chaotic system [66],
had been generalized from a point system to a spatially ex-
tended one [17], the dripping handrail – so-called because
the physical metaphor involved condensation of dew on
a horizontal rail. Substituting accreting hot plasma for wa-
ter, the inner edge of the accretion disk for the handrail,
and a density driven instability for the drip mechanism,
we hypothesized that the stochastic behavior in Sco X-1
can be described in terms of this kind of nonlinear dynam-
ical system (see Young and Scargle [79] for a more detailed
exposition of the model).
We studied the x-ray luminosity time series using the
wavelet spectrum (called the scalegram), an analog of the
ordinary Fourier power spectrum better suited to dis-
playing power-law power spectra (“1/f noise”) associated
with self-scaling in time series data. The basic idea was
to compare scalegrams from the data against ones com-
puted from the output of numerical simulations of the
dripping handrail model. It was natural that the model
produced power-law wavelet spectra, but astonishingly it
also produced a local peak in the scalegram – a spec-
tral feature indicating what astronomers call a quasi-pe-
riodic oscillation, or QPO. Astonishing because we were
not even trying to ﬁt this feature, and because the con-
ventional wisdom was that the 1/f noise and the QPO are
produced by completely diﬀerent phenomena. Here a sin-
gle, incredibly simple, physical model yielded both spectral
features, ﬁtting the scalegram estimated from the time se-
ries data quite well. Note that wavelets [22,73] are useful
tools for diagnosing stochastic (random or chaotic) time
series. Since the corresponding transforms are linear, this
is a counterexample to the myth that linear tools are not
useful for “nonlinear data.”
A similar model to the dripping handrail was proposed
by Mineshige and co-workers (Mineshige, Takeuchi, and
Nishimori [46] and earlier references therein), based on
another non-linear dynamical paradigm called self-orga-
nized criticality. The physical metaphor for their work was
the instabilities when sand is gradually dropped onto a ﬂat
surface, ultimately generating avalanches with interesting
self-scaling properties, similar to those of some accretion
systems [8]. Hence this is sometime called the sand pile
model. Accretion systems in astronomy are complex and
still not well understood. While there is not general accep-
tance of nonlinear dynamics as explaining the scaling and
QPO features in the power spectra, this approach is com-
petitive with the others under study.
There reader may wish to consult several general col-
lections of papers dealing with chaotic phenomena in as-
trophysics [9,10,56].
Future Directions
Admitedly this article has a somewhat gloomy tone, since
it has emphasized the diﬃculties of applying many of the
standard data analysis methods to astronomical time se-
ries, including problems with the data as well as limitations
on existing analysis methods. But there will be great future
progress in this topic, due to both increased data quan-
tity and quality and the development of more sophisticated
data analysis technology. As we enter the Age of Digital
Astronomy, the improved detectors, telescopes, and space

Astronomical Time Series, Complexity in
A
379
platforms are already producing large-scale surveys of the
sky with good time coverage and resolution, coupled with
photometric accuracy. These platforms will yield cornu-
copias of data which – coupled with advanced machine
learning and data mining techniques, adapted and applied
to the study of nonlinear dynamical systems from time se-
ries data – will greatly improve our knowledge of the as-
tonishingly violent, irregular processes occurring through-
out the Universe. [As a speciﬁc aside, it may be noted that
“projection pursuit” [27,35] provides ways to deal with
multidimensional data of any kind. While most workers
have used it only to search for clustering in multidimen-
sional data, projection pursuit may prove to be useful for
state space structures.] In addition, multivariate visualiza-
tion techniques, especially those aimed at elucidating the
time evolution of states of a complex system, will be cru-
cial to future progress.
Bibliography
Primary Literature
1. Abarbanel H, Brown R, Kadtke J (1989) Prediction and system
identification in chaotic nonlinear systems: Time series with
broadband spectra. Phys Lett A 138:401–408
2. Abraham R, Shaw C (1992) Dynamics, the Geometry of Behav-
ior. Addison Wesley, Reading
3. Aikawa T (1987) The Pomeau–Manneville Intermittent Transi-
tion to Chaos in Hydrodynamic Pulsation Models. Astrophys
Space Sci 139:218–293
4. Aikawa T (1990) Intermittent chaos in a subharmonic bifurca-
tion sequence of stellar pulsation models. Astrophys Space Sci
164:295
5. Atmanspacher H, Scheingraber H, Voges W (1988) Global scal-
ing properties of a chaotic attractor reconstructed from exper-
imental data. Phys Rev A 37:1314
6. Badii R, Broggi G, Derighetti B, Ravani M, Cilberto S, Politi A, Ru-
bio MA (1988) Dimension Increase in Filtered Chaotic Signals.
Phys Rev Lett 60:979–982
7. Badii R, Politi A (1986) On the Fractal Dimension of Filtered
Chaotic Signals. In: Mayer-Kress G (ed) Dimensions and En-
tropies in Chaotic Systems, Quantification of Complex Be-
havior. Springer Series in Synergetics, vol 32. Springer, New
York
8. Bak P, Tang C, Wiesenfeld K (1987) Self-organized criticality: An
explanation of 1/f noise. Phys Rev Lett 59:381–384
9. Buchler J, Eichhorn H (1987) Chaotic Phenomena in Astro-
physics. Proceedings of the Second Florida Workshop in Non-
linear Astronomy. Annals New York Academy of Sciences,
vol 497. New York Academy of Science, New York
10. Buchler J, Perdang P, Spiegel E (1985) Chaos in Astrophysics.
Reidel, Dordrecht
11. Buchler JR, Goupil MJ, Kovács G (1987) Tangent Bifurcations
and Intermittency in the Pulsations of Population II Cepheid
Models. Phys Lett A 126:177–180
12. Buchler JR, Kovács G (1987) Period-Doubling Bifurcations and
Chaos in W Virginis Models. Ap J Lett 320:L57–62
13. Buchler JR, Regev O (1990) Chaos in Stellar Variability. In: Kras-
ner S (ed) The Ubiquity of Chaos. American Association for the
Advancement of Science, Washington DC, pp 218–222
14. Caswell WE, York JA (1986) Invisible Errors in Dimension Calcu-
lation: geometric and systematic effects. In: Mayer-Kress G (ed)
Dimensions and Entropies in Chaotic Systems, Quantification
of Complex Behavior. Springer Series in Synergetics, vol 32.
Springer, New York
15. Chennaoui A, Pawelzik K, Liebert W, Schuster H, Pfister G (1988)
Attractor reconstruction from filtered chaotic time series. Phys
Rev A 41:4151–4159
16. Crutchfield J (1989) Inferring the dynamic, quantifying physical
Complexity. In: Abraham N et al (eds) Measures of Complexity
and Chaos. Plenum Press, New York
17. Crutchfield J, Kaneko K (1988) Are Attractors Relevant to Fluid
Turbulence? Phys Rev Let 60:2715–2718
18. Crutchfield J, McNamara B (1987) Equations of motion from
a data series, Complex Syst 1:417–452
19. Crutchfield J, Packard NH (1983) Symbolic Dynamics of Noisy
Chaos. Physica D 7:201–223
20. Crutchfield J, Young K (1989) Inferring Statistical Complexity.
Phys Rev Lett 63:105–108
21. Eckmann J-P, Kamphorst SO, Ruelle D, Ciliberto S (1986) Lia-
punov Exponents from Time Series. Phys Rev A 34:4971–4979
22. Fang L-Z, Thews RL (1998) Wavelets in Physics. World Scien-
tific, Singapore. A physics-oriented treatment of wavelets, with
several astronomical applications, mostly for spatial or spectral
data
23. Farmer JD, Ott E, Yorke JA (1983) The dimension of chaotic at-
tractors. Physica D 7:153–180
24. Farmer JD, Sidorowich J (1987) Predicting chaotic time series.
Phys Rev Lett 59:845–848
25. Farmer JD, Sidorowich J (1988) Exploiting chaos to predict the
future and reduce noise. In: Lee Y (ed) Evolution, Learning and
Cognition. World Scientific Pub Co, Singapore
26. Flandrin P (1999) Time-Frequency/Time-Scale Analysis. Aca-
demic Press, San Diego . Volume 10 in an excellent series,
Wavelet Analysis and Its Applications. The approach adopted
by Flandrin is well suited to astronomical and physical applica-
tions
27. Friedman JH, Tukey JW (1974) A Projection Pursuit Algorithm
for Exploratory Data Analysis. IEEE Trans Comput 23:881–890
28. Froehling H, Crutchfield J, Farmer JD, Packard NH, Shaw R
(1981) On Determining the Dimension of Chaotic Flows. Phys-
ica D 3:605–617. C Herculis. Astron Astrophys 259:215–226
29. Gillet D (1992) On the origin of the alternating deep and shal-
low light minima in RV Tauri stars: R Scuti and A. Astron Astro-
phys 259:215
30. Grassberger P, Procaccia I (1983) Characterization of strange
attractors. Phys Rev Lett 50:346–349
31. Grassberger P, Procaccia I (1983) Estimation of the Kolmogorov
entropy from a chaotic signal. Phys Rev A 28:2591–2593
32. Harding A, Shinbrot T, Cordes J (1990) A chaotic attractor in
timing noise from the VELA pulsar? Astrophys J 353:588–596
33. Hempelmann A, Kurths J (1990) Dynamics of the Outburst Se-
ries of SS Cygni. Astron Astrophys 232:356–366
34. Holzfuss J, Mayer-Kress G (1986) An Approach to Error-Estima-
tion in the Application of Dimension Algorithms. In: Mayer-
Kress G (ed) Dimensions and Entropies in Chaotic Systems,
Quantification of Complex Behavior. Springer Series in Syner-
getics, vol 32. Springer, New York

380 A
Astronomical Time Series, Complexity in
35. Jones MC, Sibson R (1987) What is Projection Pursuit? J Roy
Statis Soc A 150:1–36
36. Klavetter JJ (1989) Rotation of Hyperion. I – Observations. As-
tron J 97:570. II – Dynamics. Astron J 98:1855
37. Kollath Z (1993) On the observed complexity of chaotic stellar
pulsation. Astrophys Space Sci 210:141–143
38. Kostelich E, Yorke J (1988) Noise reduction in dynamical sys-
tems. Phys Rev A 38:1649–1652
39. Kovács G and Buchler J R (1988) Regular and Irregular Pulsa-
tions in Population II Cepheids. Ap J 334:971–994
40. Lecar M, Franklin F, Holman M, Murray N (2001) Chaos in the
Solar System. Ann Rev Astron Astrophys 39:581–631
41. Lissauer JJ (1999) Chaotic motion in the Solar System. Rev Mod
Phys 71:835–845
42. Lissauer JJ, Murray CD (2007) Solar System Dynamics: Regular
and Chaotic Motion. Encyclopedia of the Solar System, Aca-
demic Press, San Diego
43. Mandelbrot B (1989) Multifractal Measures, Especially for the
Geophysicist. Pure Appl Geophys 131:5–42
44. Mandelbrot B (1990) Negative Fractal Dimensions and Multi-
fractals. Physica A 163:306–315
45. Mayer-Kress G (ed) (1986) Dimensions and Entropies in Chaotic
Systems, Quantification of Complex Behavior. Springer Series
in Synergetics, vol 32. Springer, New York
46. Mineshige S, Takeuchi M, Nishimori H (1994) Is a Black Hole Ac-
cretion Disk in a Self-Organized Critical State. ApJ 435:L12
47. Mitschke F, Moeller M, Lange W (1988), Measuring Filtered
Chaotic Signals. Phys Rev A 37:4518–4521
48. Moskalik P, Buchler JR (1990) Resonances and Period Doubling
in the Pulsations of Stellar Models. Ap J 355:590–601
49. Norris JP, Matilsky TA (1989) Is Hercules X-1 a Strange Attrac-
tor? Ap J 346:912–918
50. Osborne AR, Provenzale A (1989) Finite correlation dimen-
sion for stochastic systems with power-law spectra. Physica D
35:357–381
51. Packard NH, Crutchfield JP, Farmer JD, Shaw RS (1980) Geom-
etry from a Time Series. Phys Rev Lett 45:712–716
52. Paoli P, Politi A, Broggi G, Ravani M, Badii R (1988) Phase Tran-
sitions in Filtered Chaotic Signals. Phys Rev Lett 62:2429–2432
53. Provenzale A, Smith LA, Vio R, Murante G (1992) Distinguishing
between low-dimensional dynamics and randomness in mea-
sured time series. Physica D 58:31
54. Ramsey JB, Yuan H-J (1990) The Statistical Properties of Dimen-
sion Calculations Using Small Data Sets. Nonlinearlity 3:155–
176
55. Regev O (1990) Complexity from Thermal Instability. In: Kras-
ner S (ed) The Ubiquity of Chaos. American Association for
the Advancement of Science, Washington DC, pp 223–232. Re-
lated work in press, MNRAS
56. Regev O (2006) Chaos and complexity in astrophysics, Cam-
bridge University Press, Cambridge
57. Ruelle D (1990) Deterministic Chaos: The Science and the Fic-
tion (the Claude Bernard Lecture for 1989). Proc Royal Soc Lon-
don Ser A Math Phys Sci 427(1873):241–248
58. Ruelle D, Eckmann J-P (1992) Fundamental limitations for es-
timating dimensions and Lyapunov exponents in dynamical
systems. Physica B 56:185–187
59. Scargle JD (1989) An introduction to chaotic and random time
series analysis. Int J Imag Syst Tech 1:243–253
60. Scargle JD (1989) Random and Chaotic Time Series Analysis:
Minimum Phase-Volume Deconvolution. In: Lam L, Morris H
(eds) Nonlinear Structures in Physical Systems. Proceedings
of the Second Woodward Conference. Springer, New York,
pp 131–134
61. Scargle JD (1990) Astronomical Time Series Analysis: Model-
ing of Chaotic and Random Processes. In: Jaschek C, Murtagh
F (eds) Errors, Bias and Uncertainties in Astronomy. Cambridge
U Press, Cambridge, pp 1–23
62. Scargle JD (1990) Studies in astronomical time series analysis.
IV: Modeling chaotic and random processes with linear filters.
Ap J 343:469–482
63. Scargle JD (1992) Predictive deconvolution of chaotic and ran-
dom processes. In: Brillinger D, Parzen E, Rosenblatt M (eds)
New Directions in Time Series Analysis. Springer, New York
64. Scargle JD, Steiman-Cameron T, Young K, Donoho D, Crutch-
field J, Imamura J (1993) The quasi-periodic oscillations and
very low frequency noise of Scorpius X-1 as transient chaos –
A dripping handrail? Ap J Lett 411(part 2):L91–94
65. Serre T, Buchler JR, Goupil M-J (1991) Predicting White Dwarf
Light Curves. In: Vauclair G, Sion E (eds) Proceedings of the 7th
European Workshop. NATO Advanced Science Institutes (ASI)
Series C, vol 336. Kluwer, Dordrecht, p 175
66. Shaw R (1984) The Dripping Faucet as a Model Chaotic System.
Aerial Press, Santa Cruz
67. Sprott JC (2003) Chaos and Time-Series Analysis, Oxford Uni-
versity Press, Oxford. This is a comprehensive and excellent
treatment, with some astronomical examples
68. Steiman-Cameron T, Young K, Scargle J, CrutchfieldJ, Imamura
J, Wolff M, Wood K (1994) Dripping handrails and the quasi-
periodic oscillations of the AM Herculis. Ap J 435:775–783
69. Sugihara G, May R (1990) Nonlinear forecasting as a way of dis-
tinguishing chaos from measurement error in time series. Na-
ture 344:734
70. Takens F (1981) Detecting strange attractors in turbulence. In:
Rand DA, Young L-S (eds) Dynamical Systems and Turbulence,
Lecture Notes in Mathematics, vol 898. Springer, pp 366–381
71. Theiler J (1986) Spurious Dimension from Correlation Al-
gorithms Applied to Limited Time-series data. Phys Rev A
34:2427–2432
72. Theiler J (1990) Statistical Precision of Dimension Estimators.
Phys Rev A 41:3038–3051
73. van den Berg JC (1998) Wavelets in Physics, Cambridge Uni-
versity Press, Cambridge. A physics-oriented treatment of
wavelets, with a chapter by A Bijaoui on astrophysical appli-
cations, some relating to complex systems analysis
74. Vio R, Cristiani S, Lessi O, Provenzale A (1992) Time Series Anal-
ysis in Astronomy: An Applicationto Quasar VariabilityStudies.
Ap J 391:518–530
75. Wiggins S (2003) Introduction to Applied Nonlinear Dynamical
Systems and Chaos, 2nd edn. Springer, New York
76. Wisdom J (1981) The origin of the Kirkwood gaps: A mapping
for asteroidal motion near the 3/1 commensurability. The reso-
nance overlap criterion and the onset of stochastic behavior in
the restricted three-body problem. Ph D Thesis, California Inst
of Technology
77. Wisdom J, Peale SJ, Mignard F (1984) Icarus 58:137
78. Wolf A, Swift JB, Swinney HL, Vastano JA (1985) Determin-
ing Lyapunov exponents from a time series, Physica D 16:
285–317
79. Young K, Scargle J (1996) The Dripping Handrail Model: Tran-
sient Chaos in Accretion Systems. Ap J 468:617

Astrophysics, Chaos and Complexity in
A
381
Books and Reviews
Heck A, Perdang JM (eds) (1991) Applying Fractals in Astronomy.
Springer, New York. A collection of 10 papers
Jaschek C, Murtagh F (eds) (1990) Errors, Bias and Uncertainties in
Astronomy. Cambridge U Press, Cambridge
Lowen SB, Teich MC (2005) Fractal-Based Point Processes, Wiley-
Interscience, Hoboken. An excellent treatment of the unusual
subject of fractal characteristics of event data generated by
complex processes
Maoz D, Sternverg A, Leibowitz EM (1997) Astronomical Time
Series, Kluwer, Dordrecht. Proceedings of the Florence and
George Wise Observatory 25th Anniversary Symposium
Ruelle D (1989) Chaotic Evolution and Strange Attractors: The Sta-
tistical Analysis of Time Series for Deterministic Nonlinear Sys-
tems. Cambridge U Press, Cambridge
Statistical Challenges in Modern Astronomy, a series of books
derived from the excellent Penn State conference series,
with many papers on time series or related topics. http://
astrostatistics.psu.edu/scma4/index.html
Subba Rao T, Priestly MB, Lessi O (eds) (1997) Applications of
Time Series Analysis in Astronomy and Meteorology. Chapman
& Hall, London. Conference proceedings; sample data were
available to the participants
Astrophysics, Chaos
and Complexity in
ODED REGEV1,2
1 Department of Physics, Technion–Israel – Institute
of Technology, Haifa, Israel
2 Department of Astronomy, Columbia University,
New York, USA
Article Outline
Glossary
Deﬁnition of the Subject
Introduction
Hamiltonian Chaos in Planetary, Stellar
and Galactic Dynamics
Chaotic Time Variability of Astronomical Sources
Spatio-Temporal Patterns and Complexity
in Extended Systems
Future Directions
Bibliography
Glossary
Accretion disk A ﬂat gaseous structure into which mat-
ter endowed with angular momentum settles when it
is being gravitationally attracted to a relatively com-
pact object. Dissipative processes, which are needed to
allow for the extraction of angular momentum from
the orbiting gas so as to allow for it to be accreted by
the star, heat the disk and make it observable by the
outgoing radiation.
Attractor A geometrical object in the state space of a dy-
namical system to which the system’s trajectories tend
for large times (or iteration numbers). Attractors of
dissipative systems have a dimension that is lower than
that of the state space and when this dimension is frac-
tal, the attractor is called strange.
Bifurcation A qualitative change in the essential behavior
of a mathematical system (e. g., algebraic equations, it-
erated maps, diﬀerential equations), as a parameter is
varied. Usually involves an abrupt appearance and/or
disappearance of solutions.
Close binary stars Two stars in a Keplerian orbit around
each other, having a small enough separation, so that
they may aﬀect each other during their evolution (for
example by mass transfer). When the pair contains, in
addition to a normal unevolved star, a compact object
(in cataclysmic binaries it is a white dwarf and in X-
ray binaries a neutron star or black hole) the system
may be observationally prominent. Mass transfer from
the normal star via an accretion disk onto the compact
object may result in violent phenomena, such as nova
explosions or X-ray bursts.
Dynamical system (DS) A set of rules by application of
which the state of a physical (or some other, well de-
ﬁned) system can be found, if an initial state is known.
Iterated maps (IM) in which the evolution proceeds in
discrete steps, i. e., one deﬁned by appropriate itera-
tive formula, and diﬀerential equations, both ordinary
(ODE) and partial (PDE) are typical DS.
DS ﬂow All trajectories (or orbits) of a DS in its state
space, which is spanned by the relevant state variables.
The ﬂow is fully determined by the DS and may serve
as its geometrical representation.
Hubble time Estimate for the age of the universe ob-
tained from taking the inverse of the Hubble constant,
the constant ratio between the recession velocity of
galaxies and the distance to them (Hubble law). Cur-
rent estimates give the Hubble time as  14 Gyr.
Interstellar medium (ISM) Diﬀuse matter of low den-
sity, ﬁlling the space between stars in a galaxy.
The average number density of the ISM is typically
n  1 cm3 but its distribution is highly non-uniform,
with the densest clouds reaching n  106 cm3.
Fractal A set whose (suitably deﬁned) geometrical di-
mension is non-integral. Typically, the set appears self-
similar on all scales. A number of geometrical objects
associated with chaos (e. g. strange attractors) are frac-
tals.

382 A
Astrophysics, Chaos and Complexity in
Integrable DS A DS whose exact solution may be cal-
culated analytically in terms of elementary or spe-
cial functions and possibly quadratures. In integrable
Hamiltonian DS the motion proceeds on a set of in-
variant tori in phase space.
Integral of motion A function of a DS’s state variables
which remains constant during the evolution. The ex-
istence of integrals of motion allows for a dimensional
reduction of the DS. A suﬃcient number of indepen-
dent integrals of motion guarantees integrability.
KAM theorem An important theorem by Kolmogorov,
Arnold and Moser (KAM), elucidating the loss of in-
tegrability and transition to chaos in Hamiltonian DS
by the breakdown of invariant tori.
Liapunov exponents A quantitative measure of the local
divergence of nearby trajectories in a DS. If the largest
such exponent, , is positive, trajectories diverge lo-
cally exponentially, giving rise to chaotic behavior. In
that case a measure for the time that chaos is mani-
fested, the Liapunov time, is / 1/.
Poincaré section A geometrical construction with the
help of which the behavior of a multi-dimensional DS
can be examined by means of a two-dimensional IM.
The IM is obtained by ﬁnding the system’s trajectory
successive crossings of a given surface cutting the state
space of the DS (surface of section). Essential features
of the dynamics can be captured if the surface is placed
in an appropriate position.
Pulsating stars Stars that, due to an instability, exhibit ra-
dial or non-radial pulsation. Most have periodic vari-
ations, with periods ranging from a few days to years,
but some are irregular. The classical Cepheids, which
are the most common regular pulsators, were instru-
mental in determining cosmological distances by using
their known period-luminosity relation.
Shilnikov scenario One of the known mathematical
structures that guarantee chaos in dissipative DS. Let
an unstable ﬁxed point exist in the DS state space and
have a fast unstable direction and a stable subspace of
trajectories that slowly spiral in. If a homoclinic (con-
necting the point to itself) orbit exists then there are
chaotic orbits around it.
Definition of the Subject
Astronomy is the science that deals with the origin,
evolution, composition, distance to, and motion of all
bodies and scattered matter in the universe. It includes
astrophysics, which is usually considered to be the theo-
retical part of astronomy, and as such focuses on the phys-
ical properties and structure of cosmic bodies, scattered
matter and the universe as a whole. Astrophysics exploits
the knowledge acquired in physics and employs the latter’s
methods, in an eﬀort to model astronomical systems and
understand the processes taking place in them.
In recent decades a new approach to nonlinear dy-
namical systems (DS) has been introduced and applied to
a variety of disciplines in which DS are used as mathe-
matical models. The theory of chaos and complexity, as
this approach is often called, evolved from the study of di-
verse DS which behave unpredictably and exhibit complex
characteristics, despite their seeming simplicity and deter-
ministic nature. The complex behavior is attributed to the
property of sensitivity to initial conditions (SIC), whereby
despite their completely deterministic behavior, two iden-
tical systems in initial states diﬀering only by a minute
amount, relatively rapidly develop in very diﬀerent ways.
The theory has been most fruitful in physics, but it is im-
portant to note that its ﬁrst paradigms were actually deeply
related to astrophysical systems.
A large majority of astrophysical systems are theoret-
ically modeled by nonlinear DS. The application of the
ideas and methods of chaos and complexity theory to as-
trophysics seems thus to be natural. Indeed, these meth-
ods have already been exploited in the study of some sys-
tems on a vast variety of scales – from planetary satellites
through pulsating stars and up to the large scale structure
of the universe. The main importance of these approaches
is in their ability to provide new analytical insights into the
intricate physical processes taking place in cosmic matter,
shaping the various astronomical objects and causing the
prominent observable phenomena that occur in them.
Introduction
Newton’s laws of mechanics and gravitation had their
most prominent application in the signiﬁcant work of
Laplace, Celestial Mechanics. It appeared in ﬁve volumes
between 1798 and 1827 and summarized his mathematical
development and application of Newton’s work. Laplace
oﬀered a complete mechanical interpretation of the Solar
System – planets and their satellites, including the eﬀects
of perturbations and tidal interactions. This work had im-
mediately been adopted as an unequivocal manifestation
of Nature’s determinism, that is, the possibility to pre-
cisely determine the future of any DS – a set of ordinary
diﬀerential equations (ODE) in this case – if only appro-
priate initial conditions are known. Laplace himself was
so conﬁdent in his results that, according to one story,
when Napoleon asked him what is the role of the Cre-
ator in his theory, he replied that “this hypothesis” was
redundant.

Astrophysics, Chaos and Complexity in
A
383
Remarkably, the same system in celestial mechanics
(and actually its simplest form – containing just three
bodies) which had been considered to be the primary
paradigm of determinism gave rise, almost a century af-
ter Laplace’s work, to profound analytical diﬃculties. An-
other great French mathematician, Henri Poincaré, made
the fundamental discovery that the gravitational n-body
system is non-integrable, that is, its general solution can
not be expressed analytically, already for n > 2. This work
initiated the development of the theory of chaos in Hamil-
tonian systems, which culminated, about 50 years ago,
in the Kolmogorov–Arnold–Moser (KAM) theorem. Sec-
tion “Hamiltonian Chaos in Planetary, Stellar and Galac-
tic Dynamics” of this article reviews the application of this
theory to planetary, stellar and galactic dynamics.
In unrelated investigations of dissipative DS (Hamil-
tonian systems are conservative) in the 1960s, aperiodic
behavior was observed in numerical studies of simplis-
tic models of thermal convection in geophysical and as-
trophysical contexts by Lorentz [40] and Moore and
Spiegel [44]. It was conjectured that this behavior is due to
SIC, with which these systems were endowed. By the same
time, the American mathematician Stephen Smale discov-
ered a new class of “strange” attractors on which the dy-
namics is chaotic and which naturally arise in DS – sets of
ODE or iterated maps (IM) – endowed with SIC. In the
early 1980s, it was realized, following the work of Mitchell
Feigenbaum and others on bifurcations in quadratic IM,
that such DS possess a universal behavior as they approach
chaos. Some applications of these developments to as-
trophysical systems are discussed in Sect. “Chaotic Time
Variability of Astronomical Sources”.
A variety of important physical (and astrophysical)
systems, notably ﬂuids, are modeled by partial diﬀerential
equations (PDE). This class of DS provides the spatio-tem-
poral information, necessary for the description and un-
derstanding of the relevant phenomena. These DS, espe-
cially when they are nonlinear (as is often the case), are sig-
niﬁcantly more complicated than ODE and IM, but appro-
priate reduction techniques can sometimes help in apply-
ing to them the knowledge gained for ODE and IM. These
and a variety of other techniques constitute what is called
today pattern theory. Spatio-temporal patterns have been
identiﬁed and categorized and they help to understand,
using analytical and semi-analytical (usually perturbative)
approaches, some essential properties of the solutions. It is
of interest to properly quantify, describe and understand
the processes that create spatio-temporal complexity, like
the shapes of interstellar clouds, patterns in thermal con-
vection, turbulent ﬂows and other astrophysically relevant
topics. Section “Spatio-Temporal Patterns and Complex-
ity in Extended Systems” is devoted to a review of these
issues.
The quest for the universal and the generic, from
which an understanding of complicated and seemingly er-
ratic and heterogenous natural phenomena can emerge,
is central to the applications of the theory of chaos, pat-
terns and complexity in DS to astrophysics. It may provide
a powerful investigative tool supplementing fully ﬂedged
numerical simulations, which have traditionally been em-
ployed in the study of nonlinear DS in astrophysics.
Hamiltonian Chaos in Planetary, Stellar
and Galactic Dynamics
Planetary systems, star clusters of various richness, galax-
ies, galaxy clusters and super-clusters (clusters of clus-
ters) can be approached most directly by considering the
gravitational n-body problem (GNBP). In this approach
all other interactions, save the gravitational one, are ne-
glected, and the bodies are considered to be point masses.
This model is viable, at least approximately, when the dis-
tances between the bodies are much larger than their typ-
ical size and any scattered matter between the bodies has
only a very small eﬀect on the dynamics.
The GNBP is one of the paradigms of classical dynam-
ics, whose development originated in Newton’s laws and
through the implementation of techniques of mathemat-
ical analysis (mainly by Lagrange in his work Analytical
Mechanics, published in 1788) ﬁnally acquired a powerful
abstract formulation in the form of Hamiltonian canon-
ical formalism. Hamiltonian systems, that is, those that
obey the Hamilton equations are endowed with impor-
tant conservation properties linked to symmetries of the
Hamiltonian, a function that completely describes the sys-
tem. This function is deﬁned on the conﬁguration space
which consists of the phase space (spanned by the general-
ized coordinates and momenta) and the time coordinate.
A Hamiltonian system conserves phase volume and some-
times also other quantities, notably the total energy (when
the Hamiltonian does not depend explicitly on time, as is
the case in the GNBP).
The Hamilton equations consist of two ﬁrst order ODE
for each degree of freedom, thus the GNBP in a three-di-
mensional physical space yields, in general, 6n equations
and so for large n it is quite formidable. The n D 2 case
is reducible to an equivalent one body problem, known as
the Kepler problem. The complete solution of this prob-
lem was ﬁrst given by Johann Bernoulli in 1710, quite
long before the Lagrange–Hamilton formalism was intro-
duced. The gravitational two-body problem has been suc-
cessfully applied to various astrophysical systems, e. g. the

384 A
Astrophysics, Chaos and Complexity in
motion of planets and their satellites and the dynamics of
binary stars. The quest for a similar reduction for systems
with n > 2, was immediately undertaken by several great
mathematicians of the time. The side beneﬁt of these stud-
ies, conducted for almost two centuries, was a signiﬁcant
progress in mathematics (mainly in the theory of ODE),
but deﬁnite answers were found in only some particular,
rather limited cases. Attempts to treat even the simplest
problem of this kind, (the restricted, planar, three-body,
i. e, where one of the bodies is so light that its gravity has
no eﬀect on the dynamics and all the orbits are restricted
to a plane) ended in a failure. All general perturbative ap-
proaches invariably led to diverging terms because of the
appearance of “small divisors” in the perturbation series.
The ﬁrst real breakthrough came only in the 1880s,
when Henri Poincaré worked on the GNBP, set for a prize
by King Oscar II of Sweden. Poincaré did not provide a so-
lution of the problem, but he managed to understand why
it is so hard to solve. By ingenious geometrical arguments,
he showed that the orbits in the restricted three-body
problem are too complicated to be described by any ex-
plicit formula. In more technical terms, Poincaré showed
that the restricted three-body problem, and therefore the
general GNBP, is non-integrable. He did so by introducing
a novel idea, now called a Poincaré section, with the help of
which he was able to visualize the essentials of the dynam-
ics by means of a two-dimensional area preserving IM.
Figure 1 shows schematically a typical mathematical struc-
ture, a homoclinic tangle in this case, that is behind chaotic
behavior in systems like the restricted three-body problem
(and, incidentally, also the forced pendulum). The exis-
tence of a hyperbolic ﬁxed point in the appropriate equiv-
alent IM and the transversal intersection of its stable and
unstable manifolds gives rise to the complex behavior. Re-
markably, Poincaré was able to visualize such a structure
without the aid of computer graphics.
The
non-integrability
of
the
GNBP
naturally
prompted the question of the Solar System stability. No
deﬁnite answer to this problem could however be rea-
sonably expected on the basis of analytical work alone.
Before electronic computers became available, this work
had largely been based on perturbation or mean ﬁeld
methods. Despite the diﬃculties, these eﬀorts yielded new
and deep insights on chaotic behavior in Hamiltonian
systems after intensive work of close to 50 years. First and
foremost among these is the KAM theorem, which elu-
cidated the mathematical process by which an integrable
Hamiltonian system transits to chaotic behavior by losing
its integrability, when a suitably deﬁned control parameter
(e. g. the relative size of the non-integrable perturbation
to an integrable Hamiltonian) is gradually increased from
Astrophysics, Chaos and Complexity in, Figure 1
Schematic illustration of the homoclinic tangle of the unsta-
ble saddle fixed point U. Ws and Wu are, respectively, the sta-
ble and unstable invariant sets (manifolds) of the saddle point.
A transversal intersection of these manifolds (0) necessarily
leads to an infinite number of subsequent intersections, result-
ing in chaotic behavior of the DS
zero. In particular, the crucial role of resonant tori in
this process has been recognized, the transition starting
with the increasing distortion of these tori. The resonant
tori become ultimately corrugated on all scales, acquiring
a fractal shape and allowing orbits to break-up from them
and diﬀuse in the regions between the surviving non-res-
onant tori. Over 20 years after the ﬁnal formulation of
the KAM theorem, Boris Chirikov suggested a diagnostic
criterion for the onset of chaos in Hamiltonian systems.
He studied numerically the standard map
IjC1 D Ij C K sin 	j
	jC1 D 	j C Ij ;
(1)
a particular area preserving two-dimensional IM, where I
and 	 are typically action-angle variables of a Hamilto-
nian system and K is a constant. He showed that fully-
spread chaos arises when K > 1 and found that this hap-
pens because resonances overlap. This is schematically il-
lustrated in Fig. 2.
The basic deﬁning property of deterministic chaos is
SIC (due to the divergence of initially arbitrarily close
phase space trajectories of the DS) and it is quantiﬁed
by the positivity of the largest Liapunov exponent, which
guarantees chaotic behavior in all DS. In Hamiltonian sys-
tems, additional subdivision of diﬀerent degrees of chaotic
motion is available. As the Hamiltonian DS is driven far-
ther into the chaotic regime the motion becomes more
strongly irregular, in the sense that the DS trajectories
in phase space explore progressively larger volumes. All

Astrophysics, Chaos and Complexity in
A
385
Astrophysics, Chaos and Complexity in, Figure 2
Non-resonant KAM tori and chaotic regions (shaded) in the stan-
dard map. Schematic drawing for and an almost integrable case
(K . 1) (a) and the fully chaotic case (K > 1) (b). The broken up
resonant tori, displayed in a which is expanded so as to show
more details, overlap and merge, giving the situation shown
in b – widespread chaos
Hamiltonian DS are recurrent, i. e., their trajectories re-
turn inﬁnitely many times arbitrarily close to the initial
point (according to the Poincaré recurrence theorem this
is true for almost all orbits). A DS is called ergodic when
long-time averages of a variable is equivalent to its phase-
space average. Hamiltonian system are ergodic on non-
resonant tori. If any initial phase-space volume eventu-
ally spreads over the whole space, then the chaotic sys-
tem is said to have the property of mixing. Deeper into the
chaotic regime a Hamiltonian system becomes a K-system.
This happens when trajectories in a connected neighbor-
hood diverge exponentially on average and ﬁnally, when
every trajectory has a positive Liapunov exponent, there is
global instability and the system is called a C-system.
The ﬁrst Hamiltonian system, directly relevant to as-
trophysics, which was explicitly shown to exhibit deter-
ministic chaos, was that studied by Hénon and Heiles [29].
They numerically investigated the orbits of a point mass
(a star) in a model potential (approximating that of an axi-
ally symmetric galaxy). Casting the gravitational inﬂuence
of all the other masses in a system into a mean ﬁeld and
computing the orbits of a point mass in it, is signiﬁcantly
easier than a full n-body integration and has always been
one of the primary techniques of galactic dynamics. The
question addressed in this study involved the possible ex-
istence of a third isolating integral, which would guarantee
integrability in this case. The fact that the motion in the az-
imuthal (with respect to the symmetry axis) direction can
be separated out and that total energy and angular mo-
mentum are two independent integrals of motion leaves
out a three-dimensional phase volume that is accessible to
the system. A third isolating (i. e., independent of the other
two) integral would thus guarantee motion on a two-di-
mensional phase surface only, that is, integrability. Using
a model axi-symmetric potential, whose non-dimensional
form
V(x; y) D 1
2
x2 C y2 C

x2y  y3
3

;
(2)
is clearly seen to contain a simple nonlinear part in ad-
dition to the harmonic one, Hénon and Heiles integrated
the Hamilton equations of motion for several values of E,
the non-dimensional energy parameter. By examining the
Poincaré section in each case, they found that the system
is integrable and the trajectory proceeds on tori (Fig. 3)
as long as E is small enough. For a certain, larger value
of E some of the tori are destroyed, with a chaotic “sea”
appearing in between the surviving ones, but ﬁnally above
some other, still higher, value of E, almost all tori are de-
stroyed and chaos becomes widespread (Fig. 4). These re-
sults largely follow the scenario set forth by the KAM theo-
rem. They also settled the issue: A third integral of motion
of galactic dynamics does not exist in general. The impli-
cations of these ﬁndings have since then been thoroughly
investigated (see also below), primarily by George Con-
Astrophysics, Chaos and Complexity in, Figure 3
Surface of section in the x–y plane for the Hénon–Heiles system
with a relatively low value of the energy parameter E D 1/12.
The system is integrable and the motion proceeds on invariant
tori

386 A
Astrophysics, Chaos and Complexity in
Astrophysics, Chaos and Complexity in, Figure 4
Same as Fig. 3, but for values E D 1/8 (left panel) and E D 1/16. Chaotic orbits that exist between the surviving tori eventually spread
and fill almost all of the available space, with only limited islands of regular motions surviving
topoulos and his collaborators. Moreover, the pioneering
work of Hénon and Heiles could perhaps not directly pre-
dict the properties of the stellar orbits and their distribu-
tion in some particular galaxy. It has, however, remained
to this day a fundamental contribution to the theory of
Hamiltonian DS.
The Hénon and Heiles system is actually a one-body
problem (in a smooth external potential) and owes its
chaotic behavior to the nonlinearity brought about by
the anharmonic terms in the potential. Perhaps the sim-
plest and most intriguing astrophysical realizations of the
GNBP involve the motion of bodies in the Solar Sys-
tem. There exists today ample theoretical evidence, based
largely on series of gradually improving numerical studies,
that chaos permeates the Solar System and can be found
essentially everywhere one looks. The numerical calcula-
tions have recently been supplemented by analytical and
semi-analytical work.
The earliest deﬁnite ﬁnding (theoretical and observa-
tional) of chaotic motion among the permanent bodies
in the Solar System was in the rotational dynamics of the
small and rather inconspicuous Saturn’s moon, Hyperion.
In 1981 the Voyager 2 mission to Saturn discovered that
Hyperion’s orbit, residing well outside the Saturnian rings,
is quite eccentric and the moon itself is unusually ﬂat and
its spin behavior is quite strange. It was found, for exam-
ple, that Hyperion is spinning along neither its longest axis
nor the shortest one; thus its spinning motion must be un-
stable. On the theoretical side, even though this problem
is not a realization of the GNBP with n > 2, the inclu-
sion of this moon’s rotation around its axis may still allow
for complicated dynamics in the system. The problem was
analyzed by Wisdom, Peale and Mignard [63], who em-
ployed a known nonlinear equation for the time evolution
of the angle (t) between the moon’s principal axis, hav-
ing the smallest moment of inertia, and a ﬁxed direction in
space. The equation,
1
n2
d2
dt2 C 2
2

 a
r
3
sin

2(  )

D 0 ;
(3)
includes the orbital parameters of the moon a(t); (t) and
the orbital mean motion frequency n 
p
GMS/a3, as-
sumed to be given, and also the “asphericity” parameter of
the moon’s ﬁgure,   p3(IB  IA)/IC, with IC  IB > IA
being the principal moments of inertia of the moon.
Equation 3 can be perceived as describing a nonlinear
oscillator subject to periodic parametric forcing, a non-in-
tegrable Hamiltonian system, and thus it could not be an-
alytically solved, save for special cases that allow for per-
turbative treatment. Wisdom et al. integrated the equa-
tion numerically and examined the appropriate Poincaré
section for various solutions. Re-scaling the time variable
(formally t ! nt, so that the orbital period becomes 2),
one can easily see that the equation of motion results from
a time-dependent Hamiltonian
H(x; p; t) D 1
2 p2 C 1
22˛2(t) cosf[2x  (t)]g ;
(4)
where x  , p  ˙ and ˛(t)  21/2[a/r(t)]3/2. Now,
choosing orbital parameters appropriate for Hyperion’s
orbit Wisdom et al. found that even for the relatively
small value of  D 0:2 (Hyperion actually has  D 0:89)
narrow disconnected chaotic layers appear in the p  x
plane of section and they reside close to the resonances
p D 1/2; 1; 3/2. For  D 0:89 there appears an extended
chaotic zone. These characteristics of the surface of the
section are reminiscent of the transition to chaos in the
generic case depicted in Fig. 2, where the resonance over-

Astrophysics, Chaos and Complexity in
A
387
lap criterion is illustrated. The direction of Hyperion’s axis
of rotation is, thus, bound to vary chaotically and calcula-
tions show that signiﬁcant changes can occur during just
a few orbital times.
Chaotic tumbling of the kind found for Hyperion can-
not be expected for planets orbiting the Sun, because for
them IA  IB. Complex behavior in the rotational dynam-
ics may arise, however, from the perturbations of the or-
bit caused by the presence of other planets. Laskar and
Robutel [35] explored this possibility for all the inner So-
lar System planets and found that the obliquity of Mars
is currently varying chaotically between 0ı and  60ı on
a time-scale of a few million years, and it has probably
always been varying in this manner. The rotational dy-
namics of Mercury and Venus do not appear to be in the
chaotic regime, but it is reasonable that these planets may
have experienced, in their past, slow chaotic changes in the
obliquity of the rotational axis. The Earth’s case is quite
intriguing because this analysis indicates that it should be
now in the state of chaotic variation of its obliquity be-
tween 0ı and  80ı with a time-scale of a few million
years. Laskar, Joutel and Robutel [36] repeated the calcula-
tions for the Earth, including also the eﬀects of the Moon,
and obtained the comforting result that the Moon’s torque
appears to stabilize the Earth’s rotational state, with the
obliquity varying in a regular manner and by no more than
˙1:3ı about the mean value of 23:3ı.
Just as the periodic orbital motion in the two-body
problem may induce complex rotational dynamics in one
of the bodies, tidal interaction due to the bodies’ ﬁnite size
may aﬀect the orbit in a manner that is far from being
simple. Mardling [41] investigated this possibility in the
context of binary stars, formulating the problem by means
of a Hamiltonian system including stellar, tidally induced,
oscillations. She found that for some speciﬁc initial orbital
parameters the radial separation and eccentricity may ex-
perience chaotic variations, giving rise to the possibility
that an initially unbound orbit may change so as to give
rise to a stable bound system (a binary). In subsequent
studies the eﬀect of such tidal captures on an evolution
of a start cluster was investigated. In addition, Mardling
noticed and pointed out the formal similarity of the tidal-
capture problem to the three-body problem.
Since the early work of Daniel Kirkwood, it has been
known that the space distribution of asteroids (also known
as minor planets) is not uniform and, in particular, dis-
plays prominent gaps, which, as already Kirkwood himself
had recognized, appear when the their period would be
equal to a simple fraction of Jupiter’s period (see Fig. 5).
Early studies, based on the restricted three-body problem
(an asteroid moving in the ﬁeld of the Sun and Jupiter, or-
biting around each other in a Keplerian circle), had em-
ployed perturbative averaging techniques to the motion of
the large bodies and failed to predict the Kirkwood gaps.
Wisdom [62] noticed that these studies could not account
for possible spreading of asteroid orbits through the en-
tire chaotic regions accessible to them in phase space, be-
cause the existing KAM tori prevent it. This is a purely di-
mensional eﬀect and such spreading is possible and may
proceed (albeit on a very long time-scale) even if the sys-
tem is close to integrability and resonance overlap does
not occur, but the dimension of the accessible phase space
has to be higher by at least two than that of the tori. To
allow for the possibility of Arnold diﬀusion (so named
after its discoverer), Wisdom suggested abandoning the
previous approach (giving rise to only four-dimensional
phase space) and use, instead, the three-dimensional re-
stricted elliptic problem. He proposed studying an equiva-
lent discrete map (because full numerical integration over
a long enough time was not feasible then), whose structure
near one particular commensurability (the 3:1 ratio with
Jupiter’s motion) is similar to that of the full Hamiltonian
ﬂow. Test particles, initially placed near the 3:1 commen-
surability in low eccentricity orbits, were found to typically
hover around the initial orbit for almost a million years,
and then suﬀered a sudden large increase of eccentricity,
which would put them in a Mars-crossing orbit. Thus an
asteroid could not survive for a long time and be observed
in an orbit close to the 3:1 resonance. It would ultimately
be removed from it by an encounter with Mars. Subse-
quent full numerical integration conﬁrmed this idea and
provided an explanation for the formation of this promi-
nent Kirkwood gap and, in principle, the other gaps as
well. Another important ﬁnding of these studies was that
even though the Liapunov time for this system is relatively
short ( 103 years), the chaotic increase of eccentricity oc-
curred only after a signiﬁcantly longer time. This is typ-
ical for systems close to integrability, when chaotic tra-
jectories can explore the available phase space only very
slowly, in this sense, by means of Arnold diﬀusion. This
situation is sometimes referred to as stable chaos (see also
below).
The outer Solar System is abundant in minor bodies as
well – asteroids and comets. Comets are thought to orig-
inate from icy planetesimals that formed at the outskirts
of the primitive Solar nebula. Some of them found sta-
ble orbits in the Kuiper belt, a relatively narrow and ﬂat
ring-like region starting just outside the orbit of Neptune
and ending  50 AU from the Sun. Other comets were
probably scattered by encounters with nearby stars into
a vast spherical shell-like reservoir, ranging between ap-
proximately 30,000 AU and 50,000 AU from the Sun – the

388 A
Astrophysics, Chaos and Complexity in
Astrophysics, Chaos and Complexity in, Figure 5
The number of asteroids as a function of the size (in AU). The location of the Kirkwood gaps, marked by the corresponding resonances
with Jupiter motion, is clearly apparent. Reproduced, with permission, from [37], © Annual Reviews
Oort cloud. This topic is still quite controversial nowadays,
but extensive numerical calculations suggest that, in any
case, chaos seems to be playing a major role in cometary
dynamics (cf. [19]). Most comets coming from the Oort
cloud (long-period comets) are thought to have chaotic or-
bits, the origin of chaos being attributed in this case to re-
peated close encounters with massive planets. Sagdeev and
Zaslavsky [52] showed that encounters between Jupiter
and a comet on an almost parabolic orbit can account for
a large chaotic zone, extending into the Oort cloud. More
speciﬁcally, attempts to numerically calculate the orbit of
the famous Halley comet, close to the time of its latest visit
in 1985, failed to agree with each other and with obser-
vations. This problem may have been caused by the fact
that Halley’s orbit is in fact chaotic. Chirikov and Vech-
eslavov [15]) supported this suggestion and estimated that
the timescale for practical unpredictability in this case
could be as short as  30 years. Regarding the short-pe-
riod comets, numerical studies have indicated that there
are practically no stable orbits among the outer planets.
Such orbits have, however, been found above  40 AU
from the Sun and it is thus reasonable that planetesimals
indeed could survive for very long times in the Kuiper belt.
Close to the stable regions there exist also regions of insta-
bility, from which comets could be injected into the in-
ner Solar System by means of Arnold diﬀusion and ulti-
mately (but probably only temporarily) be captured into
resonant orbits, by interaction with other planets, and ap-
pear as a short-period comets.
The most diﬃcult dynamical problem in the Solar Sys-
tem results from the inclusion of all (or, at least, a few
massive) planets and the Sun in a full GNBP. Since the
late 1980s a number of numerical integrations, employ-
ing several diﬀerent approaches, have been performed
on this problem. The results of these studies were gen-
erally in rather remarkable agreement, although diﬀer-
ent techniques had been used to allow for long integra-
tion times. Positive Liapunov exponents, indicating chaos,
have been found for the orbits of most planets and the
Liapunov times t have been estimated, as follows. Suss-
man and Wisdom [57] found 10 . t . 20 Myr for Pluto,
t  4 Myr for the inner planets and 3 . t . 30 Myr for

Astrophysics, Chaos and Complexity in
A
389
the outer ones. Murray and Holman [46] reﬁned the re-
sult for the Jovian planets, and determined it more accu-
rately, at t  5 Myr. Laskar [33] found t  4 Myr for
the inner planets and, in addition, pointed out that despite
this value, the eﬀects of chaos on the orbits of these plan-
ets remain small for at least 5 Gyr (with the exception of
Mercury, whose orbit was found to be able to change dras-
tically its eccentricity within  3:5 Gye). Similar conclu-
sions have also been reached for the outer planets. It thus
appears that the disparity between the Liapunov time and
the time after which a dramatic change in the orbit may
actually appear is similar for the planetary orbits to that of
the asteroid orbits and the term “stable chaos” is appropri-
ate to planetary orbits as well.
Stellar dynamics in clusters and galaxies is, in princi-
ple, a realization of the GNBP with large n. Direct numer-
ical simulations of such problems are still, even nowadays,
often requiring prohibitive computational resources. His-
torically, the approach to such problems had largely been
based on statistical methods, inspired by the impressive
success of statistical mechanics in handling systems com-
posed of an enormous number of microscopic particles.
Typical relaxation times, relevant to dynamical astronomy
are, however, too long for stars to have settled into the in-
voked (and observed) statistical equilibria in clusters and
galaxies. The understanding of the underlying mechanism
of a violent relaxation process, that was proposed to solve
this problem, calls for a better understanding of the n-body
dynamics. To circumvent the full GNBP, studies of orbits
in a given mean ﬁeld potential were conducted, but the
question of self-consistency (of the possibly evolving back-
ground potential) posed a nontrivial challenge to this ap-
proach.
Already the pioneering study of Hénon and Heiles [29]
showed that chaotic orbits are expected even in simplistic
steady potentials. In a number of more recent works it has
been found that chaotic orbits in realistic galactic poten-
tials tend to diﬀuse over the entire phase space available
to them, in times shorter than Hubble time. For exam-
ple, Hasan, Pfenniger and Norman [27] showed this for
barred galaxies and Merritt and Fridman [42] for those
having triaxial, strongly cusped mass distributions. In ad-
dition, Merritt and Valluri [43] found that random time-
dependent perturbations in the mean potential are likely
to enhance the rate of diﬀusion of the orbits. Habib, Kan-
drup and Mahon [26] investigated these type of eﬀects
in more detail by explicitly including weak friction and
noise in the calculation of orbits in non-integrable two-di-
mensional potentials. Habib et al. found that even in two-
dimensional, but strongly chaotic, potentials, the statisti-
cal properties of orbit ensembles (and indeed the struc-
ture of phase space itself) were altered dramatically (on
a timescale much shorter than tR) by the presence of fric-
tion and noise.
All these studies have essentially been modiﬁcations
of the mean ﬁeld dynamics, but it is reasonable to con-
clude from them and some similar calculations that the
full GNBP can be expected to contain a signiﬁcant amount
of irregularities and exhibit evolution on a short (com-
pared to tR) timescale. Thus, quite a strong evolution of
the phase space distribution is expected to occur already
in time-independent (but non-integrable) potentials. The
mechanism driving this evolution was identiﬁed to be as-
sociated with chaotic diﬀusion or mixing. The numerical
experiments mentioned above suggest that mixing behav-
ior indeed does occur in some realistic potentials. More-
over, it is signiﬁcantly enhanced by cusps and irregulari-
ties (which have been modeled by noise and friction) in
the potential.
What do these ﬁndings imply for galactic dynamics? It
is quite clear that this question cannot be answered within
the realm of the mean ﬁeld approach. If the stochastic or-
bits are causing a slow (but not too slow) evolution of
the halo, chaos probably plays a role in galactic evolution.
Moreover, it is possible that the underlying potential is
not only evolving in time, but also cannot be assumed to
be smooth. One has to treat thus the full GNBP, which is
clearly the fully self-consistent approach to the problem
of cluster or galactic dynamics. While it is clear that the
system is chaotic, it is still not clear into which category
of chaotic Hamiltonian system it should be classiﬁed. Al-
ready Gurzadyan and Savvidy [25] have shown that the
GNBP does not satisfy the requirements for it to be clas-
siﬁed as a C-system. Subsequently, El-Zant [20] pointed
out that conventional methods used for quantifying phase-
space mixing are not adequate for the GNBP because of
singularities (e. g. collisions), and proposed to use local
geometric properties rather than global ones. He also ad-
dressed, in a more recent study, the inﬂuence of poten-
tial “softening”, usually employed in n-body simulations
to prevent singularities. The role of chaos in cluster and
galactic dynamics and in dynamical astronomy is still not
fully understood and some of the issues remain controver-
sial. The subject is discussed in detail in the recent book by
Contopoulos [16].
Chaotic Time Variability of Astronomical Sources
A variety of astronomical sources emit radiation whose in-
tensity is intrinsically a time variable. This means that the
variability is the object’s inherent property and does not
result from geometrical (e. g. due to the object’s motion) or

390 A
Astrophysics, Chaos and Complexity in
other external (e. g. because of the Earth’s atmosphere or
other obscuring matter) eﬀects. Such variability may pro-
vide a valuable addition to the usual ﬁxed intensity and
spectral information on the source.
Time-dependent signals that are measured in lab
experiments and astronomical observations are usually
recorded as digital time series data. Such data are also pro-
vided when a DS is numerically solved by a computer pro-
gram. Time series analysis is, in general, a non-trivial pro-
cess. In particular, this is so if the signal is contaminated
by observational noise and the amount of data is limited,
as is the case in a typical astronomical observation. Tra-
ditional techniques, like Fourier analysis, work well when
the signal is periodic or multi-periodic, but they fail if it is
irregular. Irregular variability may result from a random
process, but it can also be due to deterministic chaos.
The questions then arise how to distinguish between
a deterministically chaotic signal and a random one, and
what is the relevant information that can be extracted from
the former. Theoretical and practical issues related to this
problem have indeed been addressed in parallel to the de-
velopments in chaos theory and have yielded signiﬁcant
understanding and a number of eﬀective algorithms (see
Abarbanel et al. [1]). The central idea, based on the pio-
neering work of David Ruelle, is based on a reconstruction
of the essential dynamics of the DS which produces the sig-
nal, by embedding it in a multidimensional (pseudo) state
space. The dimension of the set on which this dynamics
takes place can then be found, using an algorithm of the
kind that was originally proposed by Grassberger and Pro-
caccia [24]. Low-dimensional fractal dimension is indica-
tive of deterministic chaos.
A number of attempts have been made to discover
chaos in aperiodic time series arising from photometric
observation of several types of astronomical objects. While
quite reliable results have been achieved for some pulsat-
ing stars, the situation has been less satisfactory for ac-
creting systems (X-ray binaries and active galactic nuclei).
Norris and Matilsky [47] pointed out some of the pit-
falls of a straightforward application of the Grassberger–
Procaccia method on the reconstructed state space. They
and Scargle [53], in a more general work, have reached
the conclusion that an adequate predictive analysis (like
a model or mapping) is most probably indispensable in the
studies of irregular astronomical series which suﬀer from
an insuﬃcient amount of data and a relatively poor signal-
to-noise ratio. Further general caveats and in particular
those relevant to studies of active galactic nuclei variability
were given by Vio et al. [60].
The most convincing evidence of low-dimensional dy-
namics and chaos in an astronomical signal was given by
Buchler et al. [11]. They proposed and used an appropriate
discrete global mapping in the reconstructed pseudo state-
space, which allowed them to use enough data points to
faithfully determine the dimension of the attractor present
in the equivalent dissipative DS, arising from the prop-
erly reduced data set. The aperiodic pulsating star R Scuti,
whose light-curve had been followed for over 30 years,
mainly by amateur observers (see Fig. 6), was analyzed
ﬁrst, giving a positive Liapunov exponent and a low frac-
tal dimension (D D 3:1) of the attractor. In a subsequent
work Buchler, Kolláth and Cadmus [12] presented ana-
lyzes of the light curves of several additional large-ampli-
tude, irregularly pulsating stars (astronomically classiﬁed
as semi-regular). They eliminated multi-periodicity and
stochasticity as possible causes of the irregular behavior,
leaving low-dimensional chaos as the only viable alterna-
tive. They then used their reconstruction technique in an
attempt to extract quantitative information from the light
curves and to uncover common physical features in this
class of irregular variable stars. Similarly to the case of R
Sct discussed above, it was found that the dynamics of at
least three stars (out of the four examined) takes place in
a four-dimensional dynamical state space, suggesting that
two vibrational modes are involved in the pulsation. The
physical nature of the irregular light curves was identi-
ﬁed as resulting from a resonant mechanism with the ﬁrst
overtone mode, having a frequency close to twice that of
the fundamental one. If the fundamental mode is self-ex-
cited while the resonant one is by itself stable, chaotic be-
havior can be expected on the basis of what is known as
the Shilnikov scenario.
Convincing detection of chaotic behavior in signals
from variable accreting systems has not, as yet, been def-
initely found. Contamination by observational, and pos-
sible also inherent, stochastic noise and an insuﬃcient
amount of data constitute the main challenges in this case.
On the theoretical side, some simplistic and idiosyn-
cratic models, as well as quite sophisticated methodical
approaches, have been proposed to account for chaotic
behavior in time-variable astrophysical systems. Buchler
and Regev [10] devised a one-zone pulsator stellar model,
in which ionization and radiative-transfer were incorpo-
rated. Such simplistic models had been quite eﬀective be-
fore in elucidating the mechanisms responsible for pulsa-
tional instability in stars. The model was of an extended
ionization zone atop an inert stellar core, the oscillation
arising from possible dynamical instability due to partial
ionization. At the same time the zone was driven thermally
by the radiation diﬀusing through the envelope, from the
core outward. When the dynamical time was signiﬁcantly
diﬀerent from the thermal time, the model exhibited pe-

Astrophysics, Chaos and Complexity in
A
391
Astrophysics, Chaos and Complexity in, Figure 6
Typical observed light-curve segments of the irregularly pulsating star R Scuti. The raw data points are indicated by the dots and the
smoothed filtered signal by the continuous line. Reproduced, with permission, from [11]
riodic oscillations. However if these two time scales were
set to be comparable, Buchler and Regev found chaotic
behavior. The mathematical structure of this model was
quite similar to the simplistic model of thermal convec-
tion, in which Moore and Spiegel [44] had found aperiodic
variability, proposing also (together with Baker, whose
one-zone model was instrumental in understanding pul-
sational instability in general) that a similar model can be
used to get irregular stellar pulsation. In Fig. 7 a projection
of the system’s trajectory in its three-dimensional state-
space on the R– ˙R (outer radius–outer velocity) plane is
shown for four cases. In cases (b) and (c) there is chaotic
behavior. The layered structure in those cases is reminis-
cent of chaotic attractors in typical DS. Using some stan-
dard criteria, Buchler and Regev have shown that this vari-
ability has indeed the properties of deterministic chaos.
Other models of this kind have also discovered chaotic be-
havior, resulting from other mechanisms – e. g. a nonlin-
ear non-adiabatic oscillator, but without dynamical insta-
bility and relaxation oscillations; a dynamically driven (by
the inner core) Hamiltonian oscillator, etc. It is diﬃcult
to decide which of the above mechanisms, if any, is rele-
vant to a realistic stellar model or indeed a pulsating star.
Detailed numerical modeling of pulsating stars have, how-
ever, also shown transition to chaotic behavior via a se-
quence of period-doubling bifurcations, when a parame-
ter (in this case the eﬀective temperature) was varied. It
has been suggested by Buchler and his collaborators that
the transition to chaos is associated, in this case, with the
excitation of vibrational overtones, brought about by res-
onances.
Astrophysics, Chaos and Complexity in, Figure 7
Projections of the attractor on the R  ˙R plane in the model of
Buchler and Regev [10]. In cases a and d the oscillations are pe-
riodic while in b and c there is chaos. Reproduced, with permis-
sion, from [10]
A systematic approach to stellar pulsation, based on
DS theory has also been put forward. It employs the idea
of dimensional reduction (using either perturbative analy-
sis or a projection technique). The use of such approaches
is quite widespread, in particular when the original DS is

392 A
Astrophysics, Chaos and Complexity in
formulated in terms of rather complicated PDE, as is the
case in stellar pulsation theory. The idea is to capture the
essentials of the dynamics in the form of a simpler math-
ematical system like a model PDE or a set of ODE – am-
plitude equations, or even IM. This is usually feasible close
to the instability threshold, where the existence of a rel-
atively low-dimensional center manifold to which the dy-
namics of the dissipative system is attracted, is guaranteed.
Spiegel [56] and Buchler [8] discussed the possible tech-
niques which can be applicable to stellar pulsation and
Goupil and Buchler [23] summarized their work on am-
plitude equations, by giving their formal form for a rather
general (non-radial, non-adiabatic) case. By studying the
relevant amplitude equations in the various cases, it is pos-
sible to identify the nature of the possible mode couplings
that are expected to occur in the nonlinear regime. In par-
ticular, the excitation of multi-mode pulsation and the ef-
fect of resonances can be investigated and then applied to
phenomenological ﬁndings on pulsating stars (cf. Buch-
ler [9]). For example, the amplitude equations, derived
in this manner for a case in which two, almost resonant
(having frequencies (˝1 and ˝2  2˝1) radial modes are
taken into account are
dA1
dt
D 1A1 C i ˝
2 A1 C P1A
1 A2
dA2
dt
D 2A2  i˝A2 C P2A2
2 ;
(5)
where Ai are the complex amplitudes of the modes, i their
linear growth-rates, ˝  (2˝1 C ˝2)/2,   (2˝1 C
˝2)/(2˝) and Pi are dependent on the speciﬁc model and
can be derived from its structure. These equations capture
the essential slow dynamics for the weakly nonlinear case.
Amplitude equations derived by perturbative tech-
niques based on the separation of time-scales (as above)
are not adequate for the case in which chaotic pulsation
occurs with the dynamical and thermal timescales being
comparable. However, dimensional reduction is still pos-
sible in this case, but more sophisticated approaches, e. g.,
the one suggested by Spiegel [56], are called for.
In the case of variable accreting sources several sim-
plistic models, based on DS that can give rise to com-
plex time-variability, were also proposed. Celnikier [13]
pointed out that the essential features of the, so called,
rapid X-ray burster, can be reproduced by a determinis-
tic simple nonlinear equation, of the kind used in pop-
ulation dynamics studies. The IM employed was similar
to the one used by Feingenbaum in his pioneering study,
which demonstrated the universal period-doubling bifur-
cation route to chaos. The rapid burster belongs to a class
of variable X-ray sources, thought to originate in a close
binary system in which a normal star is transferring mass
onto its companion neutron star, giving rise to thermonu-
clear ﬂashes on the latter. A simplistic two-zone model
that captures some main characteristics of regular X-ray
bursters has been proposed by Regev and Livio [51] and
such models are still popular nowadays (e. g. [17]). For the
rapid burster Livio and Regev [39] extended the model so
as to include a third degree of freedom, thus providing
the possibility of positive and negative feedback via mass
accretion, and obtained a chaotic series of bursts, whose
characteristics were very similar to the rapid burster.
Accreting close binary systems are known to exhibit
irregularities in their light-curves in a number of spectral
windows. Among them are quasi-periodic oscillations and
short-time ﬂickering. Young and Scargle [66] proposed
a model, in this context, based on a coupled map lattice –
an IM in which the value of a variable is dependent on
the values of this variable in the previous iteration step,
not only at the same location (lattice point), but also on
this value at adjacent spatial locations. For some range of
the parameters, quasi-periodic oscillations similar to the
ones observed in several accreting binaries, were found.
Young and Scargle found that persistent positive value
of the largest Liapunov exponent cannot be guaranteed
in this case, however chaotic behavior can be guaranteed
during long transients. Such behavior was given the name
transient chaos. Another type of a DS, a cellular automaton
(essentially a coupled map lattice, but also with the dynam-
ical variable allowed to take discrete values only) was pro-
posed by Yonehara, Mineshige and Welsh [65] to model
an accretion ﬂow through a disk in close binary systems.
The DS they invoked was in general found to evolve to-
wards a characteristic state, whose qualitative behavior –
mass accumulation during an extended time interval fol-
lowed by rapid avalanches – resembled other well-stud-
ied DS. In most cases the avalanches are localized and
quite small and terminate rapidly, but occasionally a local
avalanche may trigger some very prominent events. This
generic behavior has been found to occur in a variety of
open dissipative systems and the state in which it occurs
was called by Bak, Tang and Weisenfeld [4] self-organized
criticality. Such states generally do not have positive Lia-
punov exponents and are thus not truly chaotic, but they
clearly exhibit scaling behavior. The most famous of these
systems is the sand pile model, reminding some perhaps
also of the behavior of stock markets. In Fig. 8 the ﬂicker-
ing light-curves of a cataclysmic variable in two diﬀerent
wavelengths are shown along with the power spectral den-
sity (PSD) as calculated by Yonehara et al. for one particu-
lar choice of the parameters.

Astrophysics, Chaos and Complexity in
A
393
Astrophysics, Chaos and Complexity in, Figure 8
Light-curves and power spectral densities produced by the cellular automaton model of Yonehara et al. [65] for the flickering of
Cataclysmic Variables. The upper panels correspond to 2000 Å emission, while the lower ones to emission at 6000Å. Reproduced,
with permission, from [65]
Spatio-Temporal Patterns and Complexity
in Extended Systems
Extended astrophysical systems, that is, those that can be
spatially resolved in observations often display very com-
plex structures. The distribution of scattered matter in
galaxies, the interstellar medium (ISM), is far from be-
ing uniform. Observations in a variety of wavelengths in-
dicate that clouds of diﬀerent densities are common and
exhibit very complicated spatial forms (see Fig. 9 for one
example). Observable phenomena in more compact ob-
jects than interstellar clouds, e. g. accretion disks of dif-
ferent types and stars (which are usually not spatially re-
solvable in direct observation), often require consideration
of spatial processes as well. The spatial distribution of the
stars themselves (seen as point sources) appears in the ob-
servations to be highly non-uniform. Hierarchical cluster-
ing, from star clusters of varying richness through galaxies
and galactic clusters and up to the largest super-clusters of
galaxies, is apparent on a variety of spatial scales, up to that
of the entire visible universe. It is of interest to quantify the
observational data and identify the physical processed re-
sponsible for shaping the various structures.
The time evolution of spatially extended astrophysical
objects is naturally modeled by PDE. These DS are often
very complex and include the equations of (magneto)ﬂuid
dynamics and also of thermal and radiative processes. At-
Astrophysics, Chaos and Complexity in, Figure 9
Contours of infra-red radiation intensity in some clouds ob-
served by IRAS (detail). Adapted, by permission, from [6]
tempts to actually solve such nonlinear PDE (usually re-
ferred to as microscopic equations) can only be made by
using numerical simulation. Despite the recent signiﬁcant
progress in computer power and visualization methods,
such numerical calculations are often still inadequate, in
particular because the Reynolds numbers of typical as-
trophysical ﬂows are so high, such that a faithful resolu-
tion of the whole dynamical range is impossible. Modern
approaches to spatio-temporal complexity utilize mathe-
matical techniques to reduce the usually formidable mi-

394 A
Astrophysics, Chaos and Complexity in
croscopic PDE to signiﬁcantly simpler and analytically
(or semi-analytically) tractable DS (simple generic model
PDE, ODE and IM). These approaches constitute what is
now called theory of patterns and complexity and have so
far been successfully employed in the theoretical study of
laboratory and atmospheric ﬂuid dynamical phenomena,
chemical reactor systems, as well as in other diverse appli-
cations (see Cross and Hohenberg [18] for a review).
Physical processes that are instrumental in shaping
the ISM complexity undoubtedly include ﬂuid turbulence,
which unfortunately still deﬁes adequate understanding.
Dynamical system approaches to turbulence have yielded
some success, but only for rather limited simple issues, like
transition (e. g. Bohr et al. [7]). In an eﬀort to identify sim-
pler physical mechanisms inducing ISM complexity, Elph-
ick, Regev and Spiegel [21] began to examine the eﬀects
that thermal bi-stability (alone) may have on an extended
system like the ISM. They used a one-dimensional simpli-
ﬁed version for the relevant non-dimensional microscopic
equation,
@tZ D F(Z; p) C Zˇ@2
x Z ;
(6)
where the dependent variable Z(x; t) is a positive power
of the temperature, ˇ is determined from the temperature
dependence of the heat-diﬀusion coeﬃcient and F(Z; p) is
the relevant cooling function with the pressure p consid-
ered to be a ﬁxed parameter. Approximating F(Z; p) by
a suitable polynomial, they were able to reduce the prob-
lem to a set of ODE for the motion of fronts (between the
two stable phases). In pattern theory this kind of approach
is called defect dynamics. Elphick et al. found that an ini-
tially random distribution of front/anti-front pairs (which
can be regarded as “clouds”) develops by front – anti-front
annihilation and gives rise to an inverse cascade of larger
and larger clouds, on the way to thermal phase separa-
tion. This process is similar to what is known in condensed
matter physics as spinodal decomposition or roughening.
When the simplest spatial forcing (as it should exist in the
ISM, due to the non-uniform distribution of the heating
sources) was introduced, possible complex steady states
were found. Such states can be described by the following
pattern map
IjC1 D Ij  K sin 	j
	jC1 D 	j C L log Ij ;
(7)
where 	 and I are simple functions of the front position
and separation (cloud size), respectively. The constants K
and L are dependent on the system speciﬁcs. This IM bears
resemblance to the standard map (Eq. 1) and similarly to
the latter is chaotic. This means that thermal instability
(and some forcing) alone can produce spatial complexity.
In subsequent studies Aharonson, Regev and Sha-
viv [3] used a multi-dimensional extension of Eq. 6 and
solved it numerically in two dimensions, starting from
a random distribution. It is well known in pattern the-
ory that front curvature is driving evolution of structures
in systems governed by reaction-diﬀusion equations, like
Eq. 6, and indeed the system advanced relatively fast to-
wards phase-separation. However, the inclusion of simple
and quite slight spatio-temporal forcing atop the cooling
function resulted in a ﬁnal locked steady pattern with com-
plex cloud boundaries (see the left panels of Fig. 10). Sha-
viv and Regev [54] included also ﬂuid motion to account
for mass conservation through the moving fronts, but this
did not change the result qualitatively (the right panels of
Fig. 10). Calculation of the cloud boundaries’ fractal di-
mension in the ﬁnal state of the above numerical simu-
lations gave a result that was similar to the one found in
observations by Bazel and Désert [6]; Falgarone, Philips
and Walker [22] and others. These studies analyzed ob-
servational data in a number of wavelengths and reported
fractal dimensions 1:2 . D . 1:5 for the various cloud
boundaries. These observational claims have, however, re-
mained quite controversial and several researchers have
claimed that fractal structures can not be actually faithfully
inferred from such observations. Still, the fact that thermal
instability and diﬀusive spatial coupling alone can give rise
to complicated cloud forms may be important in ongoing
eﬀorts to understand ISM complexity.
Ideas from pattern theory can also be used for astro-
physical systems governed by microscopic equations that
are considerably more complicated than those describ-
ing just a thermally bi-stable diﬀusive medium. Among
the ﬁrst paradigms of deterministic chaos were the dissi-
pative ODE systems of Lorentz, Moore and Spiegel, de-
rived by some drastic simpliﬁcations of the equations of
ﬂuid dynamics in order to model thermal convection.
More judicious reduction procedures have been developed
since then and successfully applied to diﬀerent spatio-
temporal systems. In addition to the mathematical tech-
niques that allow one to extract the dynamics of coher-
ent structures (defects) in spatio-temporal systems, when
such structures do indeed exist, there are a number of
other methods for the reduction of the original PDE sys-
tem. An approach of this kind, which works well for a sys-
tem close to its instability threshold, was recently applied,
by Umurhan, Menou and Regev [58], to the magneto-
rotational instability which is thought to be of primary
importance in accretion disks. Employing a perturbative
asymptotic analysis, valid close to the instability threshold

Astrophysics, Chaos and Complexity in
A
395
Astrophysics, Chaos and Complexity in, Figure 10
Evolution of cloud patterns in a reaction-diffusion system of the kind (6) but in 2D. The left panels show the purely thermal case and
the right one display the effect of the inclusion of fluid motion. The rightmost low picture shows the ultimate state that persists due
to forcing. Reproduced, with permission from [3] and [54]
and for a weakly nonlinear case, Umurhan et al. reduced
the full microscopic magneto-hydrodynamical equations
to a well-known, generic PDE for the spatio-temporal am-
plitude of a disturbance – the real Ginzburg–Landau equa-
tion. Using then-known solutions of this equation, one
could estimate various important ﬂow properties as the in-
stability develops and nonlinearly saturates.
The dynamo problem (i. e., the creation of magnetic
ﬁelds in ﬂows of ionized ﬂuids) is of obvious astrophysi-
cal interest and it too can be approached, in addition to
numerical calculations, by methods of nonlinear DS the-
ory. The work of Balmforth et al. [5], in which the advec-
tion of vector ﬁelds by chaotic ﬂows was investigated, is an
example of such an approach. Childress and Gilbert [14]
summarize a number of ideas along these lines, that can
be fruitful for future research on the dynamo problem.
The nature of the distribution of stars in the universe,
with its hierarchial clustering, has raised some important
questions. Virtually all the viable cosmological models are
derived from Einstein’s general theory of relativity and
are based on the cosmological principle, which asserts that
the universe is homogeneous and isotropic. However, di-
rect observations of luminous matter have always revealed
a substantial degree of clumpiness. Straightforward exam-
ination of the pictures reveals galaxy groups, clusters, su-
per-clusters, voids, ﬁlaments and walls (see Fig. 11). Hi-
erarchial clustering is typical of fractal sets and thus the
question of whether the large scale distribution of mat-
ter in the universe is fractal or homogeneous has nat-
urally been raised. Wu, Lahav and Rees [64] have re-
cently compiled observational surveys of the galaxy dis-
tribution and concluded that relevant statistical measures,
which enable the determination of dimension, point to
the fact that the luminous matter distribution’s dimen-
sion is scale dependent. On relatively small scales, less
than about 50 h1 Mpc (h is the Hubble constant in units
of 100 km/s/Mpc), the distribution is indeed undoubtedly
fractal. It is the smoothness of the galaxy distribution (or
lack thereof) on the largest scales that is still the key point
in the debate between the proponents of a “fractal uni-
verse” and the more traditional cosmologists.
The cosmic microwave background (CMB) radiation,
for which years of data (since the launch of the COBE
satellite) are now available, can provide a measure of the
spatial ﬂuctuations in the universe on the grandest scales –
of the order of  1000 h1 Mpc. Fairly recent results indi-
cate that the dimension of the mass distribution at these
scales is not diﬀerent from the value of three by more
than a few tens of 106, indicating that the universe is
quite smooth on the largest scale. Various attempts to eval-
uate the mass distribution in the universe are based on
the observation of diﬀerent entities, from luminous mat-
ter (galaxies) and up to the CMB (the universe at a very
early epoch, when galaxies were still absent). Some very re-
cent optical surveys aim at an impressively deep coverage
(up to  300–400 h1 Mpc) but the grandest scales can be
probed, at the present time, only by other means. Mod-
ern observations in radio, microwave, infra-red and X-ray
bands also enabled a deeper look, well beyond the exist-
ing and upcoming deep optical surveys. Some recent radio
surveys and X-ray background (XRB) data can probably
probe scales of up to  500 h1 Mpc and thus bridge the

396 A
Astrophysics, Chaos and Complexity in
Astrophysics, Chaos and Complexity in, Figure 11
An image of close to one million of the brightest galaxies detected by the Two Micron All Sky Survey -2MASS (detail). The non-
uniform distribution with clusters, voids and filaments, is clearly visible. The blurred strip is due to the Galactic plane [59].
gap between the optical surveys and the largest CMB scale.
Other probing tools, like the Lyman ˛ forest, may also be
valuable in this respect.
The question of how (and at what scale) the transi-
tion from a fractal distribution to a smooth one occur, is
still quite open. There seems even to be no agreement on
the question of what the relevant statistical measures are
to ﬁnd the scale this transition. The recent suggestion by
Pietronero, Gabrielli and Sylos Labini [48] to deﬁne it as
the crossover scale between power-law behavior and ex-
ponential decay of the correlation function appears to be
the most meaningful.
Theoretical eﬀorts, aimed at ﬁnding and understand-
ing the physical mechanism behind the above results on
the spatial cosmic distributions, have also been made quite
recently. A model for nonlinear gravitational clustering
was proposed by Provenzale et al. [49], who introduced
a simple, dynamically motivated family of multi-fractals
(sets whose fractal dimension changes with scale), which
are well suited for a bottom-up scenario of gravitational
evolution. Murante et al. [45] suggested that a galaxy dis-
tribution whose dimension varies with scale may naturally
result from the presence of a number of very strong lo-
cal density maxima. They randomly superimposed a few
tens of singularities and let them computationally generate
point sets, whose dimension could be estimated as a func-
tion of scale. A result resembling real galaxy data, on small
enough scales, was obtained for suﬃciently strong singu-
larities. Provenzale, Spiegel and Thieberger [50] suggested
that additional statistical measures of the galaxy distribu-
tion may be useful for a meaningful characterization and
perhaps deeper understanding of the scaling behavior at
intermediate scales and found that lacunarity, a well-de-
ﬁned mathematical property found in multi-fractals and
giving rise to oscillations around the mean value of their
generalized dimensions, may have important astrophysi-
cal consequences.
Future Directions
Astronomy counted among the main disciplines in which
the ﬁrst studies on chaos and complexity arose, in both
Hamiltonian and dissipative nonlinear DS. The most sig-
niﬁcant mathematical developments in the theory took
place in the 20th century, mainly in its second half, and the
eﬀorts to extend it have continued until the present time.
Detailed applications to particular astrophysical systems
have, however, always been marred by the diﬃculty of ob-
taining a suﬃcient amount of high-quality data. Carefully
controlled experiments, which are possible in the labora-
tory, are impossible when cosmic phenomena are merely
passively observed.
The ongoing fast development of modern means for
astronomical data acquisition – sophisticated telescopes,
both earthbound and on dedicated satellites, increasingly
powerful computers and new data reduction techniques –
have recently opened new possibilities. On the theoretical
side, intensive eﬀorts are also being made to understand
the behavior of the extremely complicated DS as those en-
countered in the study of astrophysical systems. Thus the
prospects for fruitful and signiﬁcant future developments
appear to be very good.
Among the possible future directions, research on the
following topics seems to be the most promising:

Astrophysics, Chaos and Complexity in
A
397
 Near the end of the 20th century, it was observationally
conﬁrmed that stars other than the Sun, some of them
in binary systems, have orbiting objects around them
that appear to be planets. In the last decade over 200
of such exo-planets have been discovered and there is
growing evidence that chaos plays a role in both their
formation as well as in the evolution of exo-planetary
systems. Holman, Touma and Tremaine [30] studied
such newly discovered exo-planets and concluded that
their results imply that planetary orbits in binary stellar
systems commonly experience periods of high eccen-
tricity and dynamical chaos, and that such planets may
occasionally collide with the primary star. Lissauer [38]
pointed out the possible role of chaos in the formation
of planets, in the Solar Systems and exo-planetary sys-
tems and Kiseleva-Eggleton et al. [32] found that within
a few known exo-planetary systems at least one has
chaotic orbits. The issue of planet migration, which ac-
quired importance after the discovery of exo-planets,
and the possible role of chaos in this process, has been
addressed by Adams and Laughlin [2]. It appears, thus,
that the inclusion of chaotic processes is indispensable
in the important quest to understand the formation of
planetary systems and their evolution. Such studies are
expected to shed new light on the probability of ﬁnding
exo-planets within “habitable” zones and to have impli-
cations for possible existence and survival of life in the
universe.
 The most direct approach to the study of star cluster
and galaxy dynamics is the one employing the GNBP
with a large value of n. The direct problem can only
be approached numerically and because chaos is bound
to be present in this patently non-integrable Hamilto-
nian DS, it is important to assess its possible role in
the simulations. This issue has not been fully resolved
yet, but there are some recent ﬁndings which may elu-
cidate outstanding questions and future directions in
this context. For example, Kandrup and Sideris [31] ad-
dressed the diﬀerence between the behavior found in
full n-body simulations and that of orbits in a smooth
potential and Hayes [28] compared the results of
n-body calculations with softened potential to the exact
ones. Much remains to be done in this line of research,
because the theoretical eﬀorts towards understanding
the formation of structure in the universe, on a large
range of scales, heavily rely on n-body simulations.
Recent work on galactic dynamics, employing self-con-
sistent models, indicates that it is important to care-
fully examine the orbits, distinguishing between the
regular and chaotic ones, and then try to understand
what the role is that the chaotic orbits play in shap-
ing the galaxy. The work of Voglis, Stavropoulos and
Kalapotharakos [61], for example, yielded a surprising
new result – that long living spiral arms are excited on
the disk, composed almost completely of chaotic or-
bits. Other studies of this kind, led mainly by George
Contopoulos and his collaborators and students, have
already provided additional important results in this
context. It is reasonable to expect that new understand-
ing on the role of chaos in galactic dynamics as well
as on ways to handle its inﬂuence on n-body simula-
tions will emerge as a result of future studies. Such ef-
forts may also contribute to new results in the theory of
Hamiltonian DS of this sort.
 Astrophysical ﬂuid dynamics is a broad subject, in
which input from DS and pattern theory may provide
signiﬁcant progress in the future. Most existing stud-
ies of ﬂuid systems in astrophysics have been based,
in addition to linear stability analyzes, on very sim-
pliﬁed models or brute-force numerical simulations.
New knowledge and understanding of nonlinear pro-
cesses that numerical simulations alone can not pro-
vide, can be expected, as it has been achieved in other
ﬂuid dynamical applications, if nonlinear DS and pat-
tern theory is applied to astrophysical ﬂuids as well.
Ed Spiegel and his collaborators have advocated ap-
proaches of this kind for some time (e. g. Spiegel [55])
and interesting results have already been obtained. Se-
lected examples are given in the book edited by Zahn
and Zinn-Justin [67] and more recent such contribu-
tions have already found their way to the astrophysi-
cal and general ﬂuid-dynamical literature. They mark
a promising new direction in the theoretical studies of
important, but not yet fully understood, astrophysical
(magneto)ﬂuid-dynamical processes as accretion, con-
vection, explosions, jets and so forth.
Bibliography
Primary Literature
1. Abarbanel HDI, Brown R, Sidorovich JJ, Tsimring SS (1993) The
analysis of observed chaotic data in physical systems. Rev Mod
Phys 65:1331–1392
2. Adams FC, Laughlin G (2003) Migration and dynamical relax-
ation in crowded systems of giant planets. Icarus 163:290–306
3. Aharonson V, Regev O, Shaviv N (1994) Pattern evolution in
thermally bistable media. Astrophys J 426:621–628
4. Bak P, Tang C, Weisenfeld K (1987) Self-organized criticality –
An explanation of 1/f noise. Phys Rev Lett 59:381–384
5. Balmforth NJ, Cvitanovi´c P, Ierley GR, Spiegel EA, Vattay G
(1993) Ann NY Acad Sci 706:148–154
6. Bazell D, Désert FX (1988) Fractal structure of interstellar cirrus.
Astrophys J 333:312–319

398 A
Astrophysics, Chaos and Complexity in
7. Bohr T, Jensen MH, Paladin G, Vulpiani M (1998) Dynamical
system approach to turbulence. Cambridge University Press,
Cambridge
8. Buchler JR (1993) A dynamical systems approach to nonlinear
stellar pulsations. Astrophys Space Sci 210:9–31
9. Buchler JR (1998) Nonlinear pulsations. ASP Conference Series
135:220–230
10. Buchler JR, Regev O (1982) Oscillations of an extended ioniza-
tion region in a star. Astrophys J 263:312–319
11. Buchler JR, Serre T, Kolláth Z, Mattei J (1996) Nonlinear anal-
ysis of the light curve of the variable star R Scuti. Astrophys J
462:489–501
12. Buchler JR, Kolláth Z, Cadmus RR Jr (2004) Evidence for low-
dimensional chaos in semiregular variable stars. Astrophys J
613:532–547
13. Celnikier LM (1977) A simple mathematical simulacrum of the
X-ray burster phenomenon. Astron Astrophys 60:421–423
14. Childress S, Gilbert AD (1995) Stretch, twist and fold: The fast
dynamo. Springer, New York
15. Chirikov RV, Vecheslavov VV (1989) Chaotic dynamics of Comet
Halley. Astron Astrophys 221:146–154
16. Contopoulos G (2002) Order and chaos in dynamical astron-
omy. Springer, New York
17. Cooper RL, Narayan R (2006) A two-zone model for type I X-ray
bursts on accreting neutron stars. Astrophys J 652:584–596
18. Cross MC, Hohenberg PC (1993) Pattern formation outside
equilibrium. Rev Mod Phys 65:851–1112
19. Duncan MJ, Quinn T (1993) The long-term dynamical evolution
of the Solar System. Ann Rev Astron Astrophys 31:265–295
20. El-Zant AA (1997) On the stability of motion of N-body systems:
A geometric approach. Astron Astrophys 326:113–129
21. Elphick C, Regev O, Spiegel EA (1991) Complexity from thermal
instability. Mon Not Roy Astr Soc 250:617–628
22. Falgarone E, Phillips TG, Walker CK (1991) The edges of molec-
ular clouds – Fractal boundaries and density structure. Astro-
phys J 378:186–201
23. Goupil M-J, Buchler JR (1994) Amplitude equations for nonadi-
abatic nonradial pulsators. Astron Astrophys 291:481–499
24. Grassberger P, Procaccia I (1983) Measuring the strangeness of
strange attractors. Physica 9D:189–208
25. Gurzadyan VG, Savvidy GK (1986) Collective relaxation of stel-
lar systems. Astron Astrophys 160:203–210
26. Habib S, Kandrup HE, Mahon ME (1997) Chaos and noise in
galactic potentials. Astrophys J 480:155–166
27. Hasan H, Pfeniger C, Norman C (1993) Galactic bars with cen-
tral mass concentrations – Three-dimensional dynamics. Astro-
phys J 409:91–109
28. Hayes WB (2003) Shadowing high-dimensional Hamiltonian
systems: The gravitational N-body problem. Phys Rev Lett
90(1–4):054104
29. Hénon M, Heiles C (1964) The applicability of the third inte-
gral of motion: Some numerical experiments. Astronomical J
69:73–79
30. Holman M, Touma J, Tremaine S (1995) Chaotic variations
in the eccentricity of the planet orbiting 16 Cygni B. Nature
386:254–256
31. Kandrup HE, Sideris IV (2003) Smooth potential chaos and
N-body simulations. Astrophys J 585:244–249
32. Kiseleva-Eggleton L, Bois L, Rambaux N, Dvorak R (2002) Global
dynamics and stability limits for planetary systems around HD
12661, HD 38529, HD 37124, and HD 160691. Astrophys J
578:L145–L148
33. Laskar J (1994) Large-scale chaos in the Solar System. Astron
Astrophys 64:115–162
34. Laskar J (1996) Large scale chaos and marginal stability in
the Solar System. Celestial Mech Dynamical Astron Astrophys
287:L9–L12
35. Laskar J, Robutel P (1993) The chaotic obliquity of planets. Na-
ture 361:608–611
36. Laskar J, Joutel F, Robutel P (1993) Stabilization of the earth’s
obliquity by the moon. Nature 361:615–617
37. Lecar M, Franklin FA, Holman MJ, Murray NJ (2001) Chaos in
the Solar System. Ann Rev Astron Astrophys 39:581–631
38. Lissauer JJ (1999) Chaotic motion in the Solar System. Rev Mod
Phys 71:835–845
39. Livio M, Regev O (1985) Xray burst sources: A model for chaotic
behavior. Astron Astrophys J 148:133–137
40. Lorentz EN (1963) Deterministic nonperiodic flow. J Atmos Sci
20:130–141
41. Mardling RA (1995) The role of chaos in the circularization of
tidal capture binaries – I. The chaos boundary. II. Long-time
evolution. Astrophys J 450:722–731, 732–747
42. Merritt D, Fridman T (1996) Triaxial galaxies with cusps. Astro-
phys J 460:136–162
43. Merritt D, Valluri M (1996) Chaos and mixing in triaxial stellar
systems. Astrophys J 471:82–105
44. Moore DW, Spiegel EA (1966) A thermally excited non-linear
oscillator. Astrophys J 143:871–887
45. Murante G, Provenzale A, Spiegel EA, Thieberger R (1997) Den-
sity singularities and cosmic structures. Mon Not Roy Astr Soc
291:585–592
46. Murray N, Holman M (1999) The origin of chaos in the outer
Solar System. Science 283:1877–1881
47. Norris N, Matilsky T (1989) Is Hercules X-1 a strange attractor?
Astrophys J 346:912–918
48. Pietronero L, GabrielliA, Sylos Labini F (2002) Statisticalphysics
for cosmic structures. Physica A 306:395–401
49. Provenzale A, Galeotti P, Murante G, Villone B (1992) A simple
multifractal model of nonlinear gravitational clustering. Astro-
phys J 401:455–460
50. Provenzale A, Spiegel EA, Thieberger R (1997) Cosmic lacunar-
ity. Chaos 7:82–88
51. Regev O, Livio M (1984) Thermal cycles from a two-zone ac-
creting model: X-ray bursts and shell flashes. Astron Astrophys
134:123–128
52. Sagdeev RZ, Zaslavsky GM (1987) Stochasticity in the Kepler
problem and a model of possible dynamics of comets in the
Oort cloud. Nuovo Cimento B 97:119–130
53. Scargle JD (1990) Studies in astronomical time series analysis
IV – Modelling chaotic and random processes with linear filters.
Astrophys J 359:469–482
54. Shaviv NJ, Regev O (1994) Interface dynamics and domain
growth in thermally bistable fluids. Phys Rev E 50:2048–2056
55. Spiegel EA (1980) Fluid dynamical form of the linear and non-
linear Schrödinger equations. Physica D 1:236–240
56. Spiegel EA (1993) Patterns of aperiodic pulsation. Astrophys
Space Sci 210:33–49
57. Sussman GJ, Wisdom J (1992) Chaotic evolution of the Solar
System. Science 257:56–62
58. Umurhan M, Menou K, Regev O (2007) Weakly nonlinear anal-

Astrophysics: Dynamical Systems
A
399
ysis of the magnetorotational instability in a model channel
flow. Phys Rev Lett 98(1–4):034501
59. University of Massachusetts and Infrared Processing and Anal-
ysis Center/California Institute of Technology (2003) Joint
project: Two Micron All Sky Survey. http://www.ipac.caltech.
edu/2mass/releases/allsky. Accessed 2008
60. Vio R, Cristiani S, Lessi O, Provenzale A (1992) Time series anal-
ysis in astronomy – An application to quasar variabilitystudies.
Astrophys J 391:518–530
61. Voglis N, Stavropoulos I, Kalapotharakos C (2006) Chaotic mo-
tion and spiral structure in self-consistent models of rotating
galaxies. Mon Not Roy Astr Soc 372:901–922
62. Wisdom J (1982) The origin of the Kirkwood gaps – A mapping
for asteroidal motion near the 3/1 commensurability. Astron J
87:577–593
63. Wisdom J, Peale SJ, Mignard F (1984) The chaotic rotation of
Hyperion. Icarus 58:137–152
64. Wu KKS, Lahav O, Rees MJ (1999) The large-scale smoothness
of the universe. Nature 397:225–230
65. Yonehara A, Mineshige S, Welsh WF (1997) Cellular-automa-
ton model for flickering of cataclysmic variables. Astrophys J
486:388–396
66. Young K, Scargle JD (1996) The dripping handrail model: Tran-
sient chaos in accretion systems. Astrophys J 468:617–632
67. Zahn J-P, Zinn-Justin J (eds) (1993) Astrophysical fluid dynam-
ics (Les-Houches – Session XLVII). North-Holland, Amsterdam
Books and Reviews
Barnsley M (1998) Fractals everywhere. Academic, San Diego
Heggie D, Hut P (2003) The gravitational million-body problem:
A multidisciplinary approach to star cluster dynamics. Cam-
bridge University Press, Cambridge
Manneville P (2004) Instability, chaos and turbulence. Imperial Col-
lege Press, London
Pismen LM (2006) Patterns and interfaces in dissipative dynamics.
Springer, Berlin
Regev O (2006) Chaos and complexity in astrophysics. Cambridge
University Press, Cambridge
Shore SN (1992) An introduction to astrophysical fluid dynamics.
Academic, San Diego
Sussman GJ, Wisdom J (2001) Structure and interpretation of clas-
sical mechanics. MIT Press, Cambridge
Astrophysics: Dynamical Systems
GEORGE CONTOPOULOS
Academy of Athens, Research Center for Astronomy,
Athens, Greece
Article Outline
Deﬁnition of the Subject
Introduction
Classiﬁcation
Integrable Systems
Formal Integrals. KAM and Nekhoroshev Theory
Periodic Orbits
Transition from Order to Chaos
Dynamical Spectra
Application: Order and Chaos in Galaxies
Future Directions
Bibliography
Definition of the Subject
Dynamical systems is a broad subject, where research has
been very active in recent years. It deals with systems that
change in time according to particular laws. Examples of
such systems appear in astronomy and astrophysics, in
cosmology, in various branches of physics and chemistry,
and in all kinds of other applications, like meteorology,
geodynamics, electronics and biology. In the present ar-
ticle we deal with the mathematical theory of dynamical
systems from the point of view of dynamical astronomy.
This theory is generic in the sense that its mathematics can
be applied to diverse problems of physics and related sci-
ences, ranging from elementary particles to cosmology.
Introduction
The theory of dynamical systems has close relations with
astronomy and astrophysics, in particular with dynami-
cal astronomy. Dynamical astronomy followed two quite
diﬀerent traditions until the middle of the 20th century,
namely celestial mechanics and statistical mechanics.
Celestial mechanics was the prototype of order. The
use of perturbation theories in the solar system was quite
eﬀective for predicting the motion of the planets, satellites
and comets. Despite the fact that the perturbation series
were in most cases extremely long, their eﬀectiveness was
never doubted. One of the most painstaking developments
in this ﬁeld was the “Theory of the Motion of the Moon” of
Delaunay [29] and his successors. Today the theory of the
Moon has been extended to an unprecedented accuracy by
computer algebra, giving the position of the Moon with an
accuracy of a few centimeters [8,30].
Celestial mechanics had proved its eﬀectiveness al-
ready by the 19th century with the discovery of the planet
Neptune by Leverrier and Adams. Its subsequent devel-
opment reached its culmination in the work of Poincaré:
“Méthodes Nouvelles de la Mécanique Céleste” [66]. But
at the same time it reached its limits. Poincaré proved
that most of the series of celestial mechanics are divergent.
Therefore their accuracy is limited. On the other hand, it
was made clear that the most important problem of celes-
tial mechanics, the N-body problem, cannot be solved an-
alytically. The work of Poincaré contains the basic ideas of
most of the recent theories of dynamical systems, in par-
ticular what we call today the “Theory of Chaos”.

400 A
Astrophysics: Dynamical Systems
A completely diﬀerent approach of the theory of dy-
namical systems was based on statistical mechanics. This
approach dealt with the N-body problem, but from a quite
diﬀerent point of view. Instead of a dominant central
body (the Sun) and many small bodies (planets), it dealt
with N-bodies of equal masses (or of masses of the same
order) with large N.
Statistical mechanics, developed by Boltzmann, Gibbs
and others, dominated physics in general. In this approach
the notion of individual orbits of particles became not only
secondary, but even irrelevant. One might consider diﬀer-
ent types of orbits, but their statistical properties should be
the same.
Nevertheless, a basic problem was present in the foun-
dations of statistical mechanics. If the N-body systems
are deterministic, how is it possible to derive the random
properties of the ensembles of statistical mechanics? This
question led to the development of a new theoretical ap-
proach that is called “ergodic” theory. If almost all orbits
are ergodic, then the random behavior of the system can
be proved, and not simply assumed. As an example, con-
sider a room of gas of constant density. The constancy of
the density may be considered as a probabilistic eﬀect. But
if the orbits of the molecules are ergodic, all particles stay
on the average equal intervals of time in equal volumes in-
side this room. Thus, the constancy of density is due to the
properties of the orbits of all particles.
In dynamical astronomy there was a complete di-
chotomy between celestial mechanics and stellar dynam-
ics. Celestial mechanics continued its work with perturba-
tion series, and no chaos at all, while the new branch of
stellar dynamics (and galactic dynamics in particular) was
completely inﬂuenced by statistical considerations, as in
statistical mechanics. In particular, one dealt with distribu-
tion functions of stellar velocities that were well described
by a velocity ellipsoid. But it was realized that the time
required for the establishment of a statistical equilibrium
was much longer than the age of the universe. Thus the
stars had not enough time to settle into a statistical equilib-
rium. These problems led to a reconsideration of the foun-
dations of galactic dynamics, namely to the study of the
individual orbits of the stars. Such studies started only in
the sixties. The ﬁrst time when celestial mechanicians and
galactic astronomers met at a common meeting was dur-
ing the 1964 Thessaloniki Symposium on the “Theory of
Orbits in the Solar System and in Stellar Systems” [16]. At
that time a completely new tool was available in dynamical
astronomy, namely the use of computers.
The ﬁrst computer experiment with a N-body system
was the so-called Fermi–Pasta–Ulam paradox, in 1955.
These authors considered particles along a line attracted
Astrophysics: Dynamical Systems, Figure 1
The first published orbits (1958) in the meridian plane of an
axisymmetric galaxy inside the curve of zero velocity (CZV =
equipotential)
by linear forces, plus a small nonlinear force. The energy
was initially given to a few modes. The energy of each
mode cannot change if the forces are only linear. But be-
cause of the non linearities, the energies of all the modes of
the system were expected to change continually, in a way
consistent with a statistical mechanics prediction. Instead
of that, the computer results have shown that the energy
changes were almost periodic and no tendency to a ﬁ-
nal statistical situation ever appeared. This was a paradox,
a great surprise for people grown up with statistical me-
chanics.
A little later (1956), a computer was used by Per-Olof
Lindblad to calculate orbits of stars in a plane galaxy, in
order to ﬁnd the formation of spiral arms [51].
At about the same time, we calculated [13] two orbits
of stars in three dimensions. Based on the prevalent as-
sumptions of statistical mechanics, we expected these or-
bits to be ergodic and ﬁll all the space inside the energy
surface. Instead, we found that the orbits did not ﬁll all the
available space, but ﬁlled only curvilinear parallelograms,
like deformed Lissajous ﬁgures (Fig. 1). Later it was shown
that such orbits can be explained, qualitatively and quan-
titatively, by a formal third integral of motion [14].
The theory of the third integral goes back to the work
of Birkhoﬀ[5] and Whittaker [77,78]. In particular Whit-
taker found the so-called adelphic integral, i. e. an integral
similar to the energy in simple dynamical systems. In the
case of two harmonic oscillators this integral is reduced
to the energy of one oscillator only. In more general cases
higher order terms have to be added to ﬁnd a constant of
motion. However, in resonant cases the form of the third
integral may be quite diﬀerent. For example, in the case of
two equal frequencies it is a generalization of the angular
momentum.
The series giving the third integral are in general diver-
gent [70]. But even so in numerical applications the third
integral is better conserved if it is truncated at higher and
higher orders, up to a certain maximum order. The useful-
ness of these series was emphasized by Moser [61].

Astrophysics: Dynamical Systems
A
401
The most important result of these studies was that,
in general, dynamical systems are neither integrable nor
ergodic. This was a surprise, because it was generally as-
sumed that generic dynamical systems are either inte-
grable or ergodic [48]. This change of paradigm was em-
phasized by Lynden Bell [54].
The existence of ordered domains in generic dynam-
ical systems is the basic content of the famous Kolmo-
gorov–Arnold–Moser (KAM) theorem. Kolmogorov [47]
announced a theorem proving the existence of invariant
tori in dynamical systems. Such tori contain quasi-peri-
odic motions with frequencies whose ratios are far from all
rationals (see Sect. “Formal Integrals. KAM and Nekhoro-
shev Theory”).
The details of the proof were given by Arnold [2,3] in
the analytical case, and independently by Moser [59,60]
in suﬃciently diﬀerentiable cases (with 333 derivatives!).
More recently the existence of such tori was proven for
systems that are diﬀerentiable only a few times. Further-
more, invariant tori appear generically near stable periodic
orbits. Such orbits appear even in systems with arbitrarily
large perturbations. Thus complete lack of ordered regions
containing invariant tori is rather exceptional.
Classification
There are two main types of dynamical systems:
(1) Maps (or mappings) of the form ExnC1 D Ef (Exn), where
Exn is a vector of N-dimensions and Ef is a set of
N-functions.
(2) Systems of diﬀerential equations of the form E˙x D
Ef (Ex; t), where Ex is a N-dimensional vector, and the dot
denotes derivative with respect to a continuous time t.
Another separation of dynamical systems, both of
maps and diﬀerential equations, is in conservative and dis-
sipative systems. Conservative systems preserve the vol-
ume in phase space, while in dissipative systems the vol-
ume decreases on the average. If we reverse the time direc-
tion we have systems with increasing volume.
A large class of systems of diﬀerential equations
are the Hamiltonian systems, with conjugate variables
Ex (x1; x2; : : : ; xN) and Ey (y1; y2; : : : ; yN) that satisfy equa-
tions of the form:
E˙x D @H
@Ey ;
E˙y D @H
@Ex
(1)
where H is the Hamiltonian function H D H(Ex; Ey; t)
and N is the number of degrees of freedom. The equations
of motion (1) are called canonical, or Hamiltonian equa-
tions. The space of the variables Ex and Ey is called phase
space. The phase space of a map and of a system of diﬀer-
ential equations is N-dimensional, while the phase space
of a Hamiltonian is 2N-dimensional. A change of variables
(Ex; Ey) ! ( Ex0; Ey0) is called canonical if the equations of mo-
tion in the new variables are also canonical.
The most important separation of dynamical systems
is between integrable systems and chaotic systems.
The most simple integrable systems are the solvable
systems, i. e. systems that can be solved explicitly, to give
the variables as functions of time. This deﬁnition is too
restricted, if by functions we mean known functions of
time. A more general deﬁnition is in terms of single-val-
ued functions of time [79], even if such functions can be
given only numerically.
A more general deﬁnition of integrable systems is sys-
tems that have N independent analytic integrals of motion
in involution Ii(Ex; t) D Ii(Ex0; t0), i D 1; 2; : : : ; N. (By in-
volution we mean that any two such integrals have a zero
Poisson bracket (see Eq. 6)).
As regards chaotic systems, their deﬁnition and clas-
siﬁcation is a diﬃcult task. The basic property of chaos
is sensitive dependence on initial conditions. Namely two
orbits of a compact system starting very close to each other
deviate considerably later on (exponentially fast in time).
A particular class of chaotic systems are the ergodic
systems, in which the orbits go everywhere on a surface
of constant energy. Namely, the orbits pass through the
neighborhood of every point of the energy surface. In an
ergodic system the time average of a function is equal to
its phase average, according to Birkhoﬀ’s theorem [6]. An
ergodic system is called “mixing” if two nearby particles on
diﬀerent (ergodic) orbits can go very far from each other.
A further distinction of mixing systems is related to
the speed of deviation of nearby orbits. If the deviation is
exponential in time the system is called “Kolmogorov” or
K-system. The deviation E of nearby orbits that are initially
(at time t0) at an inﬁnitesimal distance E0 is measured by
the Lyapunov characteristic number (LCN) (or simply the
Lyapunov exponent).
For almost all deviations E0 from the initial condition
Ex0 of an orbit the Lyapunov characteristic number is the
same, equal to
LCN D lim
t!1
ln jE/E0j
t
(2)
In a system of N degrees of freedom one can deﬁne N
Lyapunov characteristic numbers. If not speciﬁed explic-
itly otherwise the term Lyapunov characteristic number
means the maximal LCN. A system is said to have sensitive
dependence on the initial conditions if its LCN is positive.

402 A
Astrophysics: Dynamical Systems
A simple system with positive LCN is given by  D
0 exp(qt). In this system ln(/0) D qt, therefore LCN D
q (constant). A system in which the deviation  increases
linearly in time (or as a power of time) is not Kolmogorov
although it is mixing. For example, if  D 0 C˛t, we have
ln(/0)  ln t, and the limit LCN D limt!1(ln t/t) is
zero.
If in a Kolmogorov system the LCN for all orbits is
between certain positive limits, i. e. if C1  LCN  C2 the
system is called “Anosov”, or C-system.
A C-system has inﬁnite periodic orbits, but none of
them is stable. Furthermore the Anosov systems are hyper-
bolic, i. e. the stable and unstable manifolds of each unsta-
ble periodic orbit intersect transversally. The Anosov sys-
tems are structurally stable, i. e. a small perturbation of an
Anosov system leads to another Anosov system.
The usual classiﬁcation [50] of dynamical systems is
the following: Integrable systems, ergodic systems, mixing
systems, Kolmogorov systems, Anosov systems.
This classiﬁcation represents the view that if a system
is not integrable it is at least ergodic. The ﬁrst edition of the
book of Landau and Lifshitz [48] separated all dynamical
systems into two classes, integrable and ergodic. (However
after the 3rd edition this part was omitted). This classiﬁca-
tion misses the most important property of chaos, the fact
that in general chaos co-exists with order in the same sys-
tem.
In fact, in most systems that have been studied numer-
ically up to now, one ﬁnds both chaotic and ordered or-
bits. Such systems are called systems with divided phase
space. Chaotic orbits have a positive Lyapunov character-
istic number, while ordered orbits have LCN D 0. Both
chaotic and ordered orbits cover parts of the phase space.
This is best seen in numerical studies of systems of two
degrees of freedom. The ordered and chaotic domains are
intricately mixed. However, there are regions where order
is predominant, and other regions where chaos is predom-
inant.
Systems of two degrees of freedom that are close to
integrable have only small regions of chaos and most or-
bits are ordered. On the other hand, systems that are far
from integrable may seem completely chaotic, but after
a closer look we usually ﬁnd small islands of stability in
them.
Completely integrable systems are exceptional. On the
other hand, truly ergodic systems are also exceptional. For
example, some piecewise linear maps, that are given mod-
ulo a constant, or systems with abrupt reﬂections, like the
stadium have been proven to be ergodic. But if we add
generic nonlinear terms in these maps the structure of
phase space changes and islands of stability appear.
The present day classiﬁcation of dynamical systems is
Ordered
Chaotic
Random
Compact
Integrable (General
Case)
Systems
with Divided
Phase Space
(Limiting
Cases)
Ergodic Mixing
Kolmogorov
Anosov
Non-
compact
Integrable
with
escapes
Noninte-
grable with
escapes
–
A particular class are the noncompact systems (i. e. sys-
tems with escapes). In such systems some initial condi-
tions lead to escapes, while others do not. These systems
are either integrable, or chaotic. In the latter case chaos ap-
pears in the form of chaotic scattering. But properly speak-
ing chaos refers only to compact systems. Finally, the ran-
dom systems are limiting cases of Anosov systems, because
they have inﬁnitely large Lyapunov characteristic num-
bers.
Integrable Systems
The simplest integrable systems can be solved explicitly.
Autonomous Hamiltonian equations of the form (1) with
one degree of freedom (N D 1) are always solvable.
A very useful set of variables are the action-angle vari-
ables (EJ; E). In particular the change of variables
xi D
p
2Ji sin i ;
yi D
p
2Ji cos i ;
(i D 1; 2 : : : N)
(3)
is canonical and the new variables (EJ; E) satisfy canonical
equations of the form
˙ D @H
@J ;
˙J D @H
@
(4)
In the particular case that H is a function of the
actions only H D H(EJ) the actions EJ (J1; J2; : : : JN) are
integrals of motion, because ˙Ji D 0, and consequently
the derivatives of H with respect to Ji are constant,
i. e. @H/@EJ D E!(EJ) where E! represents the frequencies
(!1; !2; : : : ; !N). Then the angles (1; 2; : : : ; N) are lin-
ear functions of the time i D !i(t  ti0). In such a case
the motion takes place on a N-dimensional torus.
A function I(Ex; Ey; t) in a Hamiltonian system is called
an integral of motion if it remains constant along any or-
bit, i. e. its total derivative is zero. Then
dI
dt D @I
@Ex
dEx
dt C @I
@Ey
dEy
dt C @I
@t D [I; H] C @I
@t D 0
(5)

Astrophysics: Dynamical Systems
A
403
Astrophysics: Dynamical Systems, Figure 2
Regions filled by the various types of orbits in a Stäckel potential
where Ex, Ey are additive and
[I; H]  @I
@Ex
@H
@Ey  @I
@Ey
@H
@Ex
(6)
is called the Poisson bracket between the functions I and H.
An important class of integrable systems are the
Stäckel potentials [33,71,76]. They are given in elliptical
coordinates (; ) by potentials of the form
V D [F1()  F2()]
  
(7)
where  and  are the roots of the second degree equation
in 
x2
  a2 C
y2
  b2 D 1
(8)
with Cartesian coordinates x, y and a2  b2. The Eq. (8)
represents confocal ellipses and hyperbolas. From ev-
ery point (x, y) passes an ellipse (deﬁned by ) and
Astrophysics: Dynamical Systems, Figure 3
Orbits in a rotating Stäckel potential
a hyperbola (deﬁned by ). Both have the same foci at
y D a D ˙(a2  b2)1/2. The orbits in the plane (x, y) ﬁll
regions between such ellipses and hyperbolas (Fig. 2).
The Stäckel potentials are in general non-rotating. Up
to now only one rotating Stäckel potential has been found,
besides the trivial case of a homogeneous ellipsoid, namely
the potential
V D 
k1
[(x2 C y2)2 C 2a2(x2  y2) C a4]1/2 C 1
2˝2
s r2
(9)
The orbits in the rotating case are in general tubes
around both foci, or tubes around one focus only [24]
(Fig. 3).
Stäckel potentials in three degrees of freedom were
considered by many authors like [31,45,53]. There are 32
possible forms of 3-D Stäckel orbits [21].
The Stäckel potentials have been used extensively in
recent years in constructing self-consistent models of el-
liptical galaxies [7,32,46,72]. Such models represent well
real galaxies.
Formal Integrals. KAM and Nekhoroshev Theory
If a system is close to an integrable one, we may use pertur-
bation methods to ﬁnd approximate integrals that are valid
for long intervals of time, and sometimes for all times.
Usually such integrals are in the form of formal power se-
ries. Formal integrals for polynomial Hamiltonians were
ﬁrst derived by Whittaker [77,78].
Another form of such integrals was derived by
Birkhoﬀ[5] and by Cherry [9]. Formal integrals appro-
priate for galactic dynamics were derived by Contopou-
los [14,15]. In the case of an axisymmetric galaxy an inte-
gral of this form is called a third integral (the ﬁrst integral

404 A
Astrophysics: Dynamical Systems
being the energy and the second the z component of the
angular momentum).
Birkhoﬀ[5] and Cherry ([9] use Hamiltonians of the
form)
H D i!111Ci!222C  Ci!NNN CH3CH4C: : :
(10)
where Hk is of degree k in ( ; ), and  ;  are com-
plex variables ( D 1; 2 ; : : : N).
By successive canonical changes of variables the
Hamiltonian takes the normal form H D H2 C H4 C
: : : where Hk is a function of the products ˝ D ()
only (in the nonresonant case), thus it is an even function
of the variables. Then the quantities ˝ are integrals of
motion, and the equations of motion can be solved explic-
itly.
The third integral in a galaxy refers to a Hamiltonian
of the form [14]
H D 1
2
2
X
D1

y2
 C !2
x2


C H3 C H4 C : : :
(11)
where Hk is a function of degree k in x; y ( D 1; 2).
This Hamiltonian represents a galactic potential on a plane
of symmetry. If r D r0 represents a circular orbit on this
plane, we have x1 D r  r0, x2 D z and y1 D ˙x1, y2 D ˙x2.
Then the term H3 of the Hamiltonian takes the form
H3 D x1x2
2  0
3 x3
1
(12)
An integral of motion has its total derivative equal to
zero. The successive terms of the third integral ˚ can be
found from the equation
d˚
dt D [˚; H] D 0
(13)
(where ˚ D ˚2 C ˚3 C : : :, and [˚; H] is the Poisson
bracket) by equating terms of equal degree.
The Hamiltonian H2 has two independent integrals
of motion ˚2 D 1
2
y2
1 C !2
1x2
1
 and ˚0
2 D H2  ˚2. Then,
we ﬁnd
˚3 D 
1
(!2
1  4!2
2)
h !2
1  2!2
2
 x1x2
22x1y2
2C2y1x2y2
i
(14)
etc. The expression (14) and the higher order terms ˚k
of the third integral contain denominators of the form
(m!1  n!2) that become zero if !1/!2 D n/m (rational).
Then ˚ contains secular terms. For example, the form (14)
of ˚3 cannot be valid if !1  2!2 D 0. It is not valid also if
!1  2!2 is small (case of a “small divisor”) because then
˚3 is large, while the series expansion of ˚ implies that the
successive terms ˚2, ˚3, etc. become smaller as the order
increases.
In such resonance cases we can construct a third inte-
gral by a diﬀerent method. If !1/!2 is rational there are
further isolating integrals of motion of the lowest order
Hamiltonian H2 beyond the partial energies ˚2, ˚0
2. For
example, if !1/!2 D n/m then H2 has also the integrals
S0
C0
D (2˚2)m/2(2˚02)n/2 sin
cos

n!2T  m!1(T  T0)

D constant
(15)
which can be written as polynomials of degree m C n in
the variables x; y [15]. The integrals S0; C0 can be used
for constructing series of the form S D S0 CS1 CS2 C: : : ,
C D C0 C C1 C C2 C : : :. These series also contain secu-
lar terms. However, we can prove that there are combina-
tions of the three integrals ˚2, ˚0
2 and S0 (or C0) that do
not contain secular terms of any order, thus they are for-
mal integrals of motion.
The series of the third integral are in general not con-
vergent. However, they may give very good applications,
if truncated at an appropriate level. A simple example of
such a series was provided by Poincaré ([66], Vol. II). The
series
f1 D
1
X
nD0
n!
(1000)n
(16)
is divergent (i. e. f 1 is inﬁnite), but if it is truncated at any
large order n smaller than 1000, it gives approximately the
same value, and it seems to converge, as n increases, to a ﬁ-
nite value. On the other hand the series
f2 D
1
X
nD0
(1000)n
n!
(17)
is convergent, but in order to give an approximate value of
f 2 one has to take more than 1000 terms.
There is an important theorem establishing the ex-
istence of N-dimensional invariant tori in systems of N
degrees of freedom. This is the celebrated KAM theo-
rem, announced by Kolmogorov [47]. The details of the
proof were given by Arnold [2,3] and independently by
Moser [59,60].
The KAM theorem states that in an autonomous
Hamiltonian system, close to an integrable system ex-
pressed in action-angle variables, there are N-dimensional

Astrophysics: Dynamical Systems
A
405
invariant tori, i. e. tori containing quasi-periodic mo-
tions with frequencies !i, satisfying a diophantine condi-
tion (after the ancient Greek mathematician Diophantos)
j E
!kj > /jkjNC1, where E
!k D !1k1 C!2k2 C  C!NkN
with k1; k2; : : : ; kN integers and jkj D jk1j C jk2j C    C
jkNj ¤ 0, while  is a small quantity depending on the
perturbation. The set of such tori has a measure diﬀerent
from zero if  is small.
In the case of two degrees of freedom the ratio !1/!2 is
far from all resonances, if it satisﬁes a diophantine condi-
tion j!1/!2  n/mj > /m3. If we exclude the sets of all ir-
rationals that are near every rational n/m, on both its sides,
the set of the excluded irrationals is smaller than
1
X
mD1
m1
X
nD1
2
m3 < 2
1
X
mD1
1
m2 D 2C
(18)
where C D 2/6  1:64. Therefore the set of diophantine
numbers !1/!2 in the interval (0, 1) has a measure larger
than 1  C and if  is small this is not only positive, but it
can even be close to 1.
The formal integrals ˚ are convergent only in in-
tegrable cases. In other cases ˚ are asymptotic se-
ries that may not represent a particular functions. The
question then arises for how long the formal integrals
˚ D ˚2 C ˚3 C : : : are valid within a given approxima-
tion. The answer to this question is provided by the the-
ory of Nekhoroshev [62]. This theory establishes that the
formal integrals are valid over exponentially long times.
Namely, if the deviation from an integrable system is of
order  the Nekhoroshev time is of order
t D t exp
 M
m

(19)
where t; M and m are positive constants. The formal in-
tegrals must be truncated at an optimal order, such that
the variation of the truncated integrals is minimum. An
analytical estimate of the optimal order of truncation was
provided recently by Efthymiopoulos et al. [36].
Let us consider the orbits in the phase space of an au-
tonomous Hamiltonian system of two degrees of freedom
H  H(x; y; px; py) D h. As the energy of the system, h,
is an integral of motion, one variable, say py, can be derived
from H if the other three variables are known. All orbits
with ﬁxed energy equal to h, lie on the above 3-D surface,
in the 4-D phase space. Thus, we may consider the orbits
in the 3 dimensional phase space (x; y; px).
If every orbit is intersected by a particular sur-
face S(px; py; x; y) D const within every ﬁxed interval of
time T, then this surface is called a Poincaré surface of sec-
tion. The intersections of the points of S are called con-
sequents; they deﬁne a map on the surface of section S
that is called a Poincaré map. In particular, a periodic or-
bit of multiplicity three is represented by three points (e. g.
O1; O2; O3 in Fig. 4).
Periodic Orbits
The most important orbits in a dynamical system are the
periodic orbits. If the periodic orbits are stable they trap
around them a set of nonperiodic orbits with the same av-
erage topology. If they are unstable they repel the orbits
in their neighborhood. In integrable systems the main un-
stable orbits separate the resonant orbits from the nonres-
onant orbits. In nonintegrable systems the unstable peri-
odic orbits introduce chaos in the system. Thus, the study
of a dynamical system usually starts with an exploration of
its periodic orbits.
In the case of an area preserving map x1 D f (x0; y0; ),
y1 D g(x0; y0; ), where  is a parameter, a periodic or-
bit is an invariant point x1 D x0, y1 D y0. Its stability
is found by linearizing the equations around this point.
We have x1 D ax0 C by0, y1 D cx0 C dy0,
where a D @f /@x0, b D @f /@y0, c D @g/@x0, d D @g/@y0.
The conservation of areas implies ad  bc D 1.
In a Hamiltonian system of two degrees of freedom
the periodic orbits and the deviations E D (x ; y) are
deﬁned in the 4-D phase space (x; y; ˙x; ˙y). Along partic-
ular directions in the 4-D phase space the original de-
viation E0 D (x0; y0) becomes E1 D (x1; y1) after
one iteration, where E1 is equal to E0 multiplied by a fac-
tor . Such vectors E0 are called eigenvectors (with arbi-
trary length) and the factors  are called eigenvalues. The
eigenvector forms an angle ˚ with the x-axis, given by
tan ˚ D (  a)/b D c/(  d), that has two opposite so-
lutions ˚ and ˚ C 180ı for each real eigenvalue .
The two eigenvalues 1; 2 are found from the equa-
tion
ˇˇˇˇ
a  
b
c
d  
ˇˇˇˇ D 2  (a C d) C 1 D 0
(20)
The roots 1; 2 of this equation are inverse. If ja C dj > 2
the roots  are real (j1j > 1 and j2j < 1). Then, the orbit
is unstable and the eigenvectors E01; E02 form the angles ˚
with the x-axis. On the other hand if ja C dj < 2 the roots
are complex conjugate with jij D 1 and the orbit is stable.
Ordered orbits lie on invariant tori that intersect a sur-
face of section along invariant curves. Orbits that do not
lie on closed invariant curves and do not escape to inﬁnity
are chaotic. On a surface of section such orbits are rep-
resented by the irregular distribution of their consequents.

406 A
Astrophysics: Dynamical Systems
Astrophysics: Dynamical Systems, Figure 4
Distribution of periodic orbits of various multiplicities in the po-
tential V D 1
2(!2
1x2
1 C !2
2x2
2)  x1x2
2 with !2
1 D 1:6 , !2
2 D 0:9,
 D 0:3
Such chaotic orbits appear near every unstable periodic or-
bit of a nonintegrable system. The domains ﬁlled by the
chaotic orbits surround the corresponding islands of sta-
bility of the same chaotic zone that contains the unstable
periodic orbits. For example, the chaotic domains of the
Hamiltonian (11) with H3 D x1x2
2 are near the unsta-
ble points O1; O2; O3 of type n/m D 2/3 in Fig. 4. These
chaotic domains surround also the islands around the sta-
ble periodic orbit (O1O2O3).
For relatively small perturbations the various reso-
nances are well separated by invariant curves that close
around the central periodic orbit O, therefore chaos is
limited. But when the perturbation increases the various
chaotic domains increase and join each other, destroying
the separating invariant curves, and producing a large con-
nected chaotic domain (Fig. 5) (for the same Hamiltonian
but with  D 4:5). This is a manifestation of a “resonance
overlap”, or “resonance interaction”.
The asymptotic curves of a hyperbolic point of an inte-
grable system join into one separatrix, unless these curves
extend to inﬁnity. The separatrix may be considered as
an orbit (homoclinic orbit), or as an invariant curve con-
taining the images of the initial points on this curve. An
orbit starting very close to the origin along the unstable
branch U will move far from O, but eventually it will re-
turn close to O along the stable branch S.
However, in a nonintegrable system there are no
closed separatrices, and the unstable and stable asymptotic
curves intersect at inﬁnite points called homoclinic points.
All initial points on an asymptotic curve generate
asymptotic orbits, i. e. orbits approaching the periodic or-
bit O either in the forward or in the backward direction of
Astrophysics: Dynamical Systems, Figure 5
As in Fig. 4 for  D 4:5
time. The homoclinic points deﬁne doubly asymptotic or-
bits because they approach the orbit O both for t ! 1
and for t ! 1.
The intersecting asymptotic curves of an unstable peri-
odic orbit form the so-called homoclinic tangle. If we start
an orbit in this region, its consequents ﬁll the region in
a practically random way, forming the chaotic domain that
we see near every unstable periodic orbit of a noninte-
grable system.
Transition from Order to Chaos
Dissipative Systems. The Logistic Map
A simple one dimensional dissipative system, where we
can see the transition from order to chaos, is the logistic
map
xiC1 D f (xi) D 4xi(1  xi)
(21)
This is the prototype of a quadratic map, and it is stud-
ied extensively in the pioneering article of May [56] and
in books on chaos (see e. g. [1,12]). Thus, we will only
describe here its main properties. We consider only val-
ues of  between 0 and 1 in order to have x always be-
tween 0 and 1. The logistic map has two simple peri-
odic orbits on the intersection of the parabola (21) with
the diagonal xiC1 D xi, namely the points x D 0 and
x D x0 D 1 
1
4.
A periodic orbit x0 is stable if the derivative f 0 D
df /dx0 is absolutely smaller than 1. This happens if 1/4 <
 < 1 D 3/4.
It is easily found that when the orbit x0 D 1  (1/4)
of period 1 becomes unstable, then a period 2 family of

Astrophysics: Dynamical Systems
A
407
periodic orbits (xa; xb) bifurcates, which is stable if 1 <
 < 2 D (1 C
p
6)/4.
There is an inﬁnite set of period doubling bifurcations
(Fig. 6a). The intervals between successive bifurcations de-
crease almost geometrically at every period doubling, i. e.
lim
n!1
n  n1
nC1  n
D ı D 4:669201609 : : :
(22)
This
relation
is
asymptotically
exact
(i. e.
for
n ! 1) [28,37]. The number ı is universal, i. e. it is
the same in generic dissipative systems.
The curves Fm(), with m D 2n, starting at the nth pe-
riod doubling bifurcation, are similar to each other, de-
creasing by a factor ˛ D 2:50 : : : in size at each successive
period doubling bifurcation.
As the ratios (22) decrease almost geometrically, the
value n converges to the limiting value 1 D 0:893.
In 2-D maps the areas are reduced at every iteration in
the dissipative case and conserved in the conservative case.
An example is provided by the Hénon map [42]
xiC1 D 1  Kx2
i C yi; yiC1 D bxi
(mod 1)
(23)
(the original map was not given modulo 1). The Jacobian
of the transformation is J D b. If 0 < b < 1 the system is
dissipative and if b D 1 it is conservative.
In the dissipative case there are attractors, i. e. mani-
folds to which tend many orbits as t ! 1 (or i ! 1 in
maps). These attractors may be points, curves (limit cy-
cles), or strange attractors [39,43,52], which are composed
of inﬁnite lines.
Other mechanisms of transition to chaos in dissipa-
tive systems are reviewed by Contopoulos (Section 2.6.3
in [21]).
Conservative Systems
The ﬁrst numerical study of the transition to chaos
in a conservative system was provided by Hénon and
Heiles [44]. The transition to chaos follows a number of
scenarios.
Inﬁnite Period Doubling Bifurcations
This scenario is
similar to the corresponding scenario of the dissipative
case. In fact a large degree of chaos is introduced after an
inﬁnite number of period doubling bifurcations along the
characteristics of the periodic orbits (x D x()). However,
there are three diﬀerences in the conservative case.
(1) Inﬁnite period doubling bifurcations appear generi-
cally only in systems of two degrees of freedom.
(2) The bifurcation ratio is diﬀerent (ı Š 8:72 in conser-
vative systems versus ı Š 4:67 in dissipative systems).
Also the scaling factors are diﬀerent.
(3) The introduction of chaos follows a diﬀerent pattern.
While in the dissipative case chaos appears only after
inﬁnite bifurcations (Fig. 6a), in the conservative sys-
tems some chaos appears around every unstable or-
bit. As the perturbation increases the chaotic domains
increase in size and they merge to form a large con-
nected domain (Fig. 6b).
In some cases the appearance of inﬁnite bifurcations may
be followed by their corresponding disappearance, as the
perturbation increases further. Then the inﬁnite unstable
families terminate in the opposite way forming inﬁnite
bubbles.
Inﬁnite Bifurcations from the same Family
While in
the inﬁnite bifurcations scenario we have pitchfork bifur-
cations from the successive bifurcating families (Fig. 6), in
the present case the original family of periodic orbits be-
comes successively stable and unstable, an inﬁnite num-
ber of times. For example, this happens in the case of the
Hamiltonian
H D 1
2(˙x2 C ˙y2 C x2 C y2) C xy2 D h
(24)
[26]. The stable bifurcating families undergo further pe-
riod doubling bifurcations. The successive stable and un-
stable intervals along the original family have a bifurcation
ratio ı Š 9:22. But this bifurcation ratio is not universal.
It depends on the particular dynamical system considered.
In the case (24) it was proven analytically by Heggie [41]
that this ratio is ı D exp (/
p
2) but in other systems it
is diﬀerent. In all cases near the limiting point there is an
inﬁnity of unstable families that produce a large degree of
chaos.
Inﬁnite Gaps
In a dynamical system representing a ro-
tating galaxy, the central periodic family, which consists of
circular orbits in the unperturbed case, has two basic fre-
quencies. These are the rotational frequency in the rotat-
ing frame (˝  ˝s) (where ˝ is the angular velocity along
a circular orbit and ˝s the angular velocity of the system,
e. g. of the spiral arms), and the epicyclic frequency  of
radial oscillations from the circular orbit.
The ratio of the two frequencies (rotation number)
goes through inﬁnite rational values /(˝  ˝s) D n/m
(resonances) as the energy increases.
The most important resonances are those with m D 1.
At every even resonance (n D 2n1, m D 1), we have a bi-
furcation of a resonant family of periodic orbits in the

408 A
Astrophysics: Dynamical Systems
Astrophysics: Dynamical Systems, Figure 6
Bifurcations and chaos: a in a dissipative system (the logistic map) and b in a conservative system
Astrophysics: Dynamical Systems, Figure 7
a Gaps appear along the characteristic of the family x1 at all even resonances 2n1/m. b Spiral characteristics near corotation (L4)
unperturbed case, which becomes a gap in the perturbed
case (when a spiral, or a bar, is added to the axisymmet-
ric background, Fig. 7a). The gap is formed by separating
the main family into two parts and joining the ﬁrst part
with a branch of the resonant family, and the second part
containing the other branch of the resonant family.
As we approach corotation (where ˝ D ˝s) the ro-
tation number tends to inﬁnity. Therefore, an inﬁnity of
gaps are formed, followed by the appearance of an inﬁnity
of unstable periodic orbits, and this leads to a large degree
of chaos [19].
Inﬁnite Spirals
There is an inﬁnite number of fami-
lies of periodic orbits whose characteristics form spirals
near corotation (Fig. 7b). These families contain an inﬁ-
nite number of unstable periodic orbits that interact with
each other, producing chaos [21,65].
Resonance Overlap
In a nonintegrable Hamiltonian we have several types of
resonant islands and their size can be found by means of
the third integral, calculated for each resonance separately.
We consider a Hamiltonian of the form (11) with
!1/!2 near a resonance n/m. The maximum size D of an
island of type n/m is of O((mCn4)/2) [17]. The same is
true for the area covered by the islands of the resonance
n/m.
If the islands of various nearby resonances are well sep-
arated, then there is no resonance overlap and no large de-
gree of chaos. However, as the perturbation increases the
various islands increase in size and the theoretical islands
overlap. As this overlapping is not possible in reality, what
really happens is that the region between the islands be-
comes chaotic.

Astrophysics: Dynamical Systems
A
409
There are two ways to estimate the critical perturba-
tion of the resonance overlap.
(1) We calculate the areas of the various islands. When
this quantity becomes equal to the total area of phase
space on the surface of section, we start to have large
degree of chaos [17].
(2) We may ﬁnd the critical value for the production of
large chaos between two main resonances of the sys-
tem, say n/m D 4/3 and n/m D 3/2. The positions of
the triple and double periodic orbits are given by the
corresponding forms of the third integral as functions
of the perturbation " (characteristics). The theoretical
characteristics intersect for a certain value of ". On the
other hand, the real characteristics cannot intersect.
Instead, between the two characteristics, a chaotic re-
gion is formed, due to the resonance overlap.
The resonance overlap criterion for large chaos was con-
sidered ﬁrst by Rosenbluth et al. [67] and Contopou-
los [17]. It was later described in detail by Walker and
Ford [75], Chirikov et al. [11], Chirikov [10], and many
others.
Arnold Diﬀusion
Nearly integrable autonomous Hamiltonian systems of N
degrees of freedom have a 2N  1 dimensional phase
space of constant energy, and a large set of N-dimen-
sional invariant surfaces (invariant tori), according to the
KAM theorem. If N D 2 there are N D 2 dimensional sur-
faces separating the 2N  1 D 3 dimensional phase space,
and diﬀusion through these surfaces is impossible. But
if N  3 the N-dimensional surfaces do not separate the
2N  1 dimensional phase space, and diﬀusion is possi-
ble all over the phase space. This is called “Arnold diﬀu-
sion” [3]. Such a diﬀusion is possible even if the perturba-
tion is inﬁnitesimal.
In order to understand better this phenomenon con-
sider a system of three degrees of freedom, that has 3-D
tori in a 5-D phase space of constant energy. If we reduce
the number of both dimensions by two we have 1-D tori
(lines) in a 3-D space. In an integrable case there is a line
passing through every point of the space and all motions
are regular. However, in a nonintegrable case there are
gaps between the lines and these gaps communicate with
each other. Therefore, if a chaotic orbit starts in such a gap
it may go everywhere in the 3-D space.
Extensive numerical studies (e. g. [49]) in a 4-D map
have shown that in the same dynamical system there
are both ordered domains where diﬀusion is very slow
(Arnold diﬀusion), and chaotic domains, where diﬀusion
is dominated by resonance overlap.
As an example we consider two coupled standard maps
x0
1 D x1 C y0
1 ;
y0
1 D y1 C K
2 sin 2x1  ˇ
 sin 2(x2  x1)
(mod 1)
x0
2 D x2 C y0
2 ;
y0
2 D y2 C K
2 sin 2x2  ˇ
 sin 2(x1  x2)
(25)
Here K is the nonlinearity and ˇ the coupling con-
stant [25].
Taking the same initial conditions, we calculate the
time of diﬀusion from an ordered region to the large
chaotic sea. If the coupling ˇ is larger than a critical value
ˇ D ˇc Š 0:305, the diﬀusion time T increases exponen-
tially with decreasing ˇ. If however, ˇ < ˇc, the time T
increases superexponential as ˇ decreases. We can identify
the exponential increase case as due to resonance overlap
diﬀusion and the superexponential increase as due Arnold
diﬀusion.
Cantori
When the perturbation increases and a particular torus is
destroyed, it becomes a cantorus, i. e. a Cantor set of points
that is nowhere dense [4,64]. The cantorus has a countable
inﬁnity of gaps, but it contains a noncountable inﬁnity of
orbits. The most surprising property of the cantori is that
their measure is zero and their fractal dimension is also
zero.
In the case of the standard map, cantori with golden
rotation number rot D (
p
5  1)/2 (expressed in continu-
ous fraction representation as [1; 1; 1; : : :]) exist for all val-
ues of K larger than a critical value Kcr D 0:972 ([55]).
The cantorus is formed when all the periodic orbits
corresponding to the various truncations of the golden
number become unstable. In the limit K D Kcr the invari-
ant curve (torus) is a fractal with self-similar structure on
all scales [40,69].
What happens in general around an island of stability
is the following. Before the destruction of the last KAM
curve surrounding the island, there is a chaotic layer, just
inside the last KAM curve, while outside the last KAM
curve there is a large chaotic sea. When the last KAM curve
becomes a cantorus the orbits from the inner chaotic layer
can escape to the large chaotic sea, but only after a long
time. Thus, the region just inside a cantorus with small
holes is diﬀerentiated by the larger density of the points
inside it (Fig. 8). This is the phenomenon of stickiness.

410 A
Astrophysics: Dynamical Systems
Astrophysics: Dynamical Systems, Figure 8
Stickiness in the standard map for K D 5. The sticky region (dark)
surrounds an island of stability, and is surrounded by a large
chaotic sea
Stickiness
This phenomenon was found numerically by Contopou-
los [18]. Stickiness has been observed in practically all
nonintegrable dynamical systems that contain islands [57].
In Fig. 8 the sticky zone (the dark region surrounding the
island O1) is limited on its inner side by the outer bound-
ary of the island O1 and on its outer side by a set of cantori,
but mainly by the cantorus that has the smallest gaps. An
orbit starting in the sticky zone requires a long time to es-
cape outside. The details of the escape can be found if we
calculate the asymptotic curves of unstable periodic orbits
just inside the main cantorus.
As an example consider an unstable asymptotic curve
from the periodic orbit of period 215 in the case of the
standard map for K D 5 (Fig. 9; [35]). If we start with
a small interval along the asymptotic curve, we ﬁnd an im-
age of this curve that crosses the cantorus after two iter-
ations along the same asymptotic curve. In Fig. 9 we see
that the asymptotic curve starts from P by going ﬁrst to
the left and downwards and then it moves to the right and
upwards, crossing the cantorus in one of its largest gaps
(marked A in Fig. 9). But then the asymptotic curve makes
several oscillations back and forth, entering again inside
the cantorus several times, before going to a large distance
in the chaotic sea. Then it moves around for a long time
before coming back and getting trapped again in the sticky
zone close to the island.
Another type of stickiness refers to the unstable
asymptotic curves that extend far into the chaotic sea (dark
lines in Fig. 8). These asymptotic curves attract nearby or-
bits and thus thick dark lines are formed [23].
Astrophysics: Dynamical Systems, Figure 9
An asymptotic curve of the periodic orbit P in the standard map
with K D 5 makes several oscillations inside the cantorus (stars)
and then escapes. We mark some islands inside the cantorus (on
the right) and outside it (on the left)
Dynamical Spectra
Despite the importance of the Lyapunov characteristic
numbers in distinguishing between order and chaos, their
practical application is limited by the very long calcula-
tions that are usually required for their evaluation. In fact,
if we calculate the ﬁnite time LCN
 D ln j/0j
t
(26)
the variation of  is irregular for relatively small t and
only for large t the value of  of a chaotic orbit stabi-
lizes and tends to a constant limiting value (curve (2)
of Fig. 10), which is the Lyapunov characteristic number
Astrophysics: Dynamical Systems, Figure 10
The variation of (t) of (1) an ordered orbit, and (2) a chaotic
orbit

Astrophysics: Dynamical Systems
A
411
Astrophysics: Dynamical Systems, Figure 11
The spectra of stretching numbers: a of the standard map and b of the Hénon map
LCN D limt!1 . If, on the other hand,  varies approxi-
mately as 1/t, the LCN is zero (ordered orbit (1) of Fig. 10).
Of special interest is to ﬁnd a ﬁnite time LCN after the
shortest possible time. In the case of a map the shortest
time is one iteration (t D 1). Thus one ﬁnds the quantities
ai D ln jiC1/ij
(27)
which are called Lyapunov indicators by Froeschlé et
al. [38], or stretching numbers by Voglis and Contopou-
los [73].
The distribution of successive values of the stretch-
ing numbers ai along an orbit is called the “spectrum
of stretching numbers”. Namely the spectrum gives the
proportion dN/N of the values of ai in a given interval
(a; a C da) divided by da, i. e.
S(a) D dN/(Nda)
(28)
where da is a small quantity and N a large number of iter-
ations.
The main properties of the spectra of stretching num-
bers are [73]:
(1) The spectrum is invariant along the same orbit.
(2) The spectrum does not depend on the initial deviation
E0 from the same initial point in the case of two di-
mensional maps.
(3) The spectrum does not depend on the initial condi-
tions of orbits in the same connected chaotic domain.
(4) In the case of ordered orbits in two dimensional maps
the spectrum does not depend on the initial conditions
on the same invariant curve.
The Lyapunov characteristic number is equal to LCN D
R S(a) a da. The spectrum gives much more information
about a system than LCN. E. g. the standard map
x0 D x C y0 ;
y0 D y C K
2 sin 2x
(mod 1) (29)
Astrophysics: Dynamical Systems, Figure 12
The average value of the stretching number, h˛i, for 10 itera-
tions (beyond the first 10 transients) as a function of x for con-
stant y
for K D 10 and the Hénon map (23) for K´D 7:407 and
b D 1 give two apparently random distributions of points
with the same LCN D 1:62. However, their spectra are
quite diﬀerent (Fig. 11).
The use of the stretching numbers allows a fast dis-
tinction between ordered and chaotic orbits. E.g. while the
calculation of the LCN requires 105  106 iterations, in
Fig. 12 we can separate the chaotic from the ordered orbits
after only 20 iterations. In Fig. 12 the noise is large, nev-
ertheless the chaotic orbits have roughly the same value
of LCN D hai > 0, while the ordered orbits have hai very
close to zero. Further details about this topic can be found
in the book of Contopoulos [21].
Application: Order and Chaos in Galaxies
Galactic Orbits
Galaxies are composed of stars, gas (including dust), and
dark matter. The stars and the dark matter produce the

412 A
Astrophysics: Dynamical Systems
Astrophysics: Dynamical Systems, Figure 13
The basic types of Galaxies a M59/NGC 4621 Elliptical [E], b M51/NGC 5194 normal spiral [Sb] and c NGC1300 barred spiral [SBb]
main part of the galactic potential and force, while the con-
tribution of the gas in the potential and force is small.
In the Hubble classiﬁcation of galaxies, the elliptical
galaxies (E) and the early type spiral galaxies (Sa, Sb)
and barred galaxies (SBa, SBb) have less gas than the
late-type galaxies (Sc, SBc) and the irregular galaxies (I)
(Fig. 13).
Galaxies that have a well developed spiral structure
(usually two symmetric spiral arms) are called “grand de-
sign”, while galaxies with irregular and multiple fragments
of spirals are called “ﬂocculent”.
The study of stellar (or mainly stellar) galaxies is based
on a systematic exploration of their orbits.
The exploration of orbits in galaxies is very important,
because the orbits are needed in constructing self-consis-
tent models of galaxies.
Self-consistency is a new type of problem that does
not appear, e. g. in accelerators, or in the solar system. It
requires the construction of appropriate sets of orbits of
stars, such that their superposition gives a response den-
sity that matches the imposed density of the model. Such
a construction is done in many cases by taking a grid of
initial conditions and calculating the orbits numerically in
a computer and then populating these orbits with stars.
But it is much more helpful and illuminating if one stud-
ies the types of orbits that form the building blocks of the
galaxies.
Thus, the ﬁrst step in understanding the structure and
dynamics of a galaxy is to calculate its orbits, periodic,
quasi-periodic and chaotic.
The most important orbits in a galactic system are the
periodic orbits. The stable orbits trap around them sets
of quasi-periodic orbits, that give the main features of the
galaxy. On the other hand the unstable orbits separate the
various types of ordered orbits, and also characterize the
chaotic domains in a galaxy.
The simplest galactic orbits are the circular periodic
orbits on the plane of symmetry of an axisymmetric galaxy.
The circular orbits in such a galaxy are in general stable.
Astrophysics: Dynamical Systems, Figure 14
a A rosette orbit in an axisymmetric galaxy. b The corresponding
epicyclic orbit
Orbits close to the stable circular periodic orbits are
called epicyclic orbits. Such orbits ﬁll circular rings in the
plane of symmetry of the galaxy and are called “rosette”
orbits (Fig. 14a). In a frame of reference rotating with the
angular velocity (frequency) ˝ of the center of the epicycle
(epicenter, or guiding center) these orbits are closed and
they are approximately ellipses around the center of the
epicycle (Fig. 14b).
If
V0
is
the
axisymmetric
potential,
we
have
˝2 D V0
0/r and 2 D V00
0 C 3V0
0/r. The frequency  along
the epicycle is called “epicyclic frequency”.
In most cases (spiral or barred galaxies) the frame
of reference is not inertial, but rotates with angular ve-
locity ˝s. This is called “pattern velocity”. In this ro-
tating frame the form of the galaxy is stationary. Then
the two basic frequencies of a moving star are (˝  ˝s)
and . If the ratio /(˝  ˝s) D (n/m) is rational, then
we have a resonant periodic orbit in the rotating system.
Two most important resonances are the Lindblad reso-
nances /(˝  ˝s) D ˙2/1 (+ inner Lindblad resonance,
ILR,  outer Lindblad resonance, OLR). A third most im-
portant resonance is corotation (or particle resonance),
where ˝ D ˝s. The angular velocity ˝ of an axisymmet-

Astrophysics: Dynamical Systems
A
413
Astrophysics: Dynamical Systems, Figure 15
The curves ˝ and ˝  /2 in two cases: (1) with only one ILR
(dotted) and (2) two ILR’s (solid), for two different values of the
pattern velocity ˝s
ric family is a function of the distance r. ˝ is a monotoni-
cally decreasing function of r (Fig. 15).
Corotation and the inner and outer Lindblad reso-
nances appear at the intersections of the line ˝ D ˝s
(Fig. 15) with the curves ˝, ˝  /2 and ˝ C /2. There
is only one distance corresponding to corotation and one
distance corresponding to the outer Lindblad resonance
OLR. However, depending on the model used, we may
have one inner Lindblad resonance, or two ILRs, or no ILR
at all.
The study of nonperiodic orbits on the plane of sym-
metry of a galaxy is realized by means of a surface of sec-
tion.
The invariant curves of the ordered nonperiodic orbits
surround either the point x1 or the point x4, that repre-
sent direct or retrograde periodic orbits that are reduced
to circular orbits in the circular case.
The periodic orbits of type x1 in the case of a spiral are
like ellipses close to the inner Lindblad resonance (Fig. 16),
but become like squares near the 4/1 resonance. These or-
bits and the nonperiodic orbits around them support the
spiral up to the 4/1 resonance. However, beyond the 4/1
resonance these orbits are out of phase and do not support
the spiral (Fig. 16). Thus, self-consistent spirals should ter-
minate near the 4/1 resonance [20]. On the other hand,
in strong bars we may have spirals outside corotation (see
Sect. “Chaotic Orbits”). More details about orbits in galax-
ies can be found in [63] and [21].
Integrals of Motion in Spiral and Barred Galaxies
The Hamiltonian on the plane of symmetry of a galaxy
(spiral or barred) rotating with angular velocity ˝s is
H D ˙r2
2 C J2
0
2r2 C V0(r)  ˝sJ0 C V1
(30)
Astrophysics: Dynamical Systems, Figure 16
Periodic orbits in a spiral galaxy support the spiral up to the 4/1
resonance, but not beyond it
where ˙r is the radial velocity, J0 the angular momentum,
V0(r) the axisymmetric potential and V1 the spiral (or
barred) perturbation. The value of H is conserved (Jacobi
constant). The Hamiltonian in action-angle variables is of
the form H D H0 C V1, where
H0 D !1I1 C !2I2 C aI2
1 C 2bI1I2 C cI2
2 C : : :
(31)
and
V1 D Re fA(r) exp[i(˚(r)  2)]g
D Re
X
mn
Vmn(I1; I2) exp[i(m1  n2)]
(32)
The expansion in action-angle variables is found by ex-
panding A, ˚ and  in terms of two angles, 1 (epicyclic
angle) and 2 (azimuth of the epicenter measured from
a certain direction), and r  rc D (2I1/!1)1/2, where rc is
the radius of a circular orbit that has the same Jacobi con-
stant as the real orbit.
The main terms of V1 are of order A and contain
the trigonometric terms cos(1  22), cos(1 C 22) and
cos 22. Away from the main resonances of the galaxy (in-
ner and outer Lindblad resonances and corotation) it is
possible to eliminate these terms by means of a canonical
transformation that gives V1 as a function of new action
variables I
1 , I
2 . Thus, in this case the Hamiltonian H is
a function of I
1 , I
2 only, i. e. I
1 and I
2 are integrals of mo-
tion.
The usual density wave theory of spiral structure deals
with the forms of the integrals I
1 , I
2 , truncated after the
ﬁrst order terms.
However, the transformation from Ii to I
i contains de-
nominators of the forms !1  2!2, !1 C 2!2 or !2, which

414 A
Astrophysics: Dynamical Systems
tend to zero close to the inner and outer Lindblad reso-
nances and corotation respectively. For example, near the
inner Lindblad resonance we cannot eliminate the term
with cos(1  22), because the transformation would
have the denominator !1  2!2 which is close to zero. In
this case we eliminate only the terms with cos(1 C 22)
and cos 22. Instead of eliminating cos(1  22), we write
this term as cos  1, introducing now resonant action-
angle variables J1 D I
1 , J2 D I
2 C 2I
1 ,  1 D 
1  2
2 ,
 2 D 
2 . Then H is expressed in terms of J1, J2 and
 1, but does not contain  2. Therefore, the conjugate
action J2 is an integral of motion. Thus, we have again
two integrals of motion, namely J2 and the Jacobi con-
stant H.
The resonant form of H explains the forms of the or-
bits near the inner Lindblad resonance. In particular, near
this resonance we have not one but two periodic orbits
roughly perpendicular to each other.
The resonant theory is an extension of the linear den-
sity wave theory and it is applicable near the ILR. In a sim-
ilar way we ﬁnd the forms of the integrals near the outer
Lindblad resonance and near corotation.
The three resonant forms of the Hamiltonian tend to
the nonresonant form away from all resonances. In fact,
if the amplitude of the spiral, or the bar, is small, the res-
onant eﬀects are restricted in small regions around each
resonance. However, if the amplitude of a spiral, or a bar,
is large, we have an overlapping of resonances and the sec-
ond integral is no more applicable.
Chaotic Orbits
Chaos in galaxies is always present near corotation, be-
cause this region contains an inﬁnite number of higher or-
der resonances that interact with each other.
Besides corotation, chaos may appear near the center
of the galaxy when there is a large mass concentration (e. g.
a central black hole). Furthermore large scale chaos ap-
pears when the amplitude of the spiral, or the bar, is large.
Appreciable chaos has been found in N-body simulations
of galaxies [27,58,68].
Chaos in barred galaxies near corotation appears
around the unstable Lagrangian points L1 and L2. Around
these points there are short period unstable periodic or-
bits for values of the Jacobi constant larger than the value
H(L1) appropriate for L1. Every unstable periodic orbit,
PL1, PL2 of this family has a stable (S etc) and an unsta-
ble (U etc) manifold attached to it. Orbits near L1 or L2
follow the unstable manifolds of the corresponding orbits
PL1 and PL2 and form trailing spiral arms along U and
U0 (Fig. 17), which are clearly seen in strong bars [22,74].
Astrophysics: Dynamical Systems, Figure 17
Orbits starting close to the unstable orbits PL1 , PL2 move to-
wards PL1 , PL2 along the stable asymptotic curves S , SS and
S0, SS0 and away from PL1 , PL2 along the unstable asymptotic
curves U , UU and U0 , UU0
Thus, in strong bars chaos is important in explaining the
outer spiral arms.
Future Directions
The subject of dynamical systems in astrophysics is pro-
gressing along three main directions: (1) Exploration of
further basic phenomena, especially in systems of more
than two degrees of freedom, (2) Rigorous mathematical
theorems, and (3) Applications on various problems of as-
trophysics.
(1) Much exploratory work exists now on systems of two
degrees of freedom, but relatively little work has been
done with systems of three or more degrees of free-
dom. In particular it is important to ﬁnd the appli-
cability of Arnold diﬀusion in various cases. A better
separation of ordered and chaotic domains in phase-
space is also necessary. Extensions of this work to ﬁnd
ordered and chaotic domains in quantum mechan-
ical systems is also important. ([34] and references
therein). Finally N-body problems with large N are
also of great interest, especially as regards the validity
and applications of statistical mechanics. In particular
problems connected with the evolution of dynamical
systems, are quite important.
(2) Many problems that have been explored numerically
up to now require rigorous mathematical proofs. For
example, the use of formal integrals of motion, the
application of basic theorems, like the KAM and
the Nekhorosev theorems and the applicability of
many statistical methods require a better mathemat-
ical foundation.

Astrophysics: Dynamical Systems
A
415
(3) Presently much is being done with astrophysical ap-
plications of the theory of dynamical systems. For ex-
ample, galactic dynamics has experienced an impor-
tant expansion in recent years, with the exploration of
the role of chaos in galaxies, especially in the forma-
tion and evolution of the outer spiral arms, and in the
evolution of the central black holes. There are many
other astrophysical problems where order and chaos
play an important role, like the structure and evolu-
tion of stars, stellar variability, solar and stellar activ-
ity, the role of magnetic ﬁelds in stars and galaxies,
the properties of extrasolar planetary systems, and the
evolution of the whole Universe.
As regards dynamical astronomy a list of 72 problems
of current interest that require further serious analytical
and numerical work (possible theses topics) is provided
at the end of the book of Contopoulos [21], together with
many references.
Bibliography
1. Argyris J, Faust G, Haase M (1994) An Exploration of Chaos.
North Holland, Amsterdam
2. Arnold VI (1961) Sov Math Dokl 2:245; (1963) Russ Math Surv
18(5):9
3. Arnold VI (1964) Sov Math Dokl 5:581
4. Aubry S (1978) In: Bishop AR, Schneider T (eds) Solitons and
Condensed Matter Physics. Springer, New York, p 264
5. Birkhoff GD (1927) Dynamical Systems. Am Math Soc, Provi-
dence
6. Birkhoff GD (1931) Proc Nat Acad Sci 17:656
7. Bishop JL (1987) Astrophys J 322:618
8. Chapront-Touzé M, Henrard J (1980) Astron Astrophys 86:221
9. Cherry TM (1924) Mon Not R Astr Soc 84:729
10. Chirikov BV (1979) Phys Rep 52:263
11. Chirikov BV, Keil E, Sessler AM (1971) J Stat Phys 3:307
12. Collet P, Eckmann J-P (1980) Iterated Maps on the Interval as
Dynamical Systems. Birkhäuser, Boston
13. Contopoulos G (1958) Stockholm Ann 20(5)
14. Contopoulos G (1960) Z Astrophys 49:273
15. Contopoulos G (1963) Astron J 68:763
16. Contopoulos G (ed) (1966) The Theory of Orbits in the Solar
System and in Stellar Systems. IAU Symp 25. Academic Press,
London
17. Contopoulos G (1967) In: Hénon M, Nahon F (eds) Les Nou-
velles Méthodes de la Dynamique Stellaire. Bull Astron 2(3):223
18. Contopoulos G (1971) Astron J 76:147
19. Contopoulos G (1983) Lett Nuovo Cim 37:149
20. Contopoulos G (1985) Comments Astrophys 11:1
21. Contopoulos G (2002) Order and Chaos in Dynamical Astron-
omy. Springer, New York; Reprinted 2004
22. Contopoulos G (2008) In: Contopoulos G, Patsis P (eds) Chaos
in Astronomy. Springer, New York, p 3
23. Contopoulos G, Harsoula M (2008) Int J Bif Chaos, in press
24. Contopoulos G, Vandervoort P (1992) Astrophys J 389:118
25. Contopoulos G, Voglis N (1996) Cel Mech Dyn Astron 64:1
26. Contopoulos G, Zikides M (1980) Astron Astrophys 90:198
27. Contopoulos G, Efthymiopoulos C, Voglis N (2000) Cel Mech
Dyn Astron 78:243
28. Coullet P, Tresser C (1978) J Phys 39:C5–25
29. Delaunay C (1867) Theorie du Mouvement de la Lune. Paris
30. Deprit A, Henrard J, Rom A (1971) Astron J 76:273
31. de Zeeuw T (1985) Mon Not R Astr Soc 216:273
32. de Zeeuw T, Hunter C, Schwarzschild M (1987) Astrophys J
317:607
33. Eddington AS (1915) Mon Not R Astr Soc 76:37
34. Efthymiopoulos C, Contopoulos G (2006) J Phys A 39:1819
35. Efthymiopoulos C, Contopoulos G, Voglis N, Dvorak R (1997)
J Phys A 30:8167
36. Efthymiopoulos C, Giorgilli A, Contopoulos G (2004) J Phys A
37:1831
37. Feigenbaum M (1978) J Stat Phys 19:25
38. Froeschlé C, et al. (1993) Cel Mech Dyn Astron 56:307
39. Grassberger P, Procaccia I (1983) Phys D 9:189
40. Greene JM et al. (1981) Phys D 3:468
41. Heggie DC (1983) Cel Mech 29:207
42. Hénon M (1969) Quart J Appl Math 27:291
43. Hénon M (1976) Commun Math Phys 50:69
44. Hénon M, Heiles C (1964) Astron J 69:73
45. Hunter C (1988) In: Buchler et al (eds) Integrability in Dynamical
Systems. N Y Acad Sci Ann 536:25
46. Hunter C (1990) In: Buchler et al (eds) GalacticModels. N Y Acad
Ann 596:187
47. Kolmogorov AN (1954) Dokl Akad Nauk SSSR 98:527
48. Landau LD, Lifshitz EM (1960) Mechanics, 1st edn; 3rd edn
(1976) Pergamon Press, Oxford
49. Laskar J (1993) Cel Mech Dyn Astron 56:191
50. Lichtenberg AJ, Lieberman MA (1992) Regular and Chaotic Dy-
namics, 2nd edn. Springer, New York
51. Lindblad PO (1960) Stockholm Ann 21:3; 21:4
52. Lorentz EN (1963) J Atmos Sci 20:130
53. Lynden-Bell D (1962) Mon Not R Astr Soc 124:1
54. Lynden-Bell D (1998) In: Buchler et al (eds) Nonlinear Dynamics
and Chaos in Astrophysics. NY Acad Sci Ann 867:3
55. Mather JN (1982) Erg Theory Dyn Syst 2:397; (1982) Topology
21:457
56. May RM (1976) Nature 261:459
57. Menjuk CR (1985) Phys Rev A 31:3282
58. Merritt D, Valluri M (1999) Astron J 118:1177
59. Moser J (1962) Nachr Acad Wiss Göttingen II. Math Phys Kl:1
60. Moser J (1967) Math Ann 169:136
61. Moser J (1968) Mem Amer Math Soc 81:1
62. Nekhoroshev NN (1977) Russ Math Surv 32(6):1
63. Ollongren A (1965) In: Blaauw A et al (eds) Stars and Stellar
Systems V Galactic Structure. Univ of Chicago Press, Chicago,
p 501
64. Percival IC (1979) In: Month M, Herrera JC (eds) Nonlinear Dy-
namics and the Beam-Beam Interaction. Am Inst Phys, New
York, p 302
65. Pinotsis A (1988) In: Roy AE (ed) Long Term Behaviour of Natu-
ral and Artificial N-Body Systems. Kluwer, Dordrecht, p 465
66. Poincaré H (1892) Les Méthodes Nouvelles de la Mécanique
Céleste Gauthier Villars. Paris I (1892); Paris II (1893); Paris III
(1899); Dover edn (1957)
67. Rosenbluth MN, Sagdeev RA, Taylor JB, Zaslavsky GM (1966)
Nucl Fusion 6:217
68. Schwarzschild M (1993) Astrophys J 409:563

416 A
Astrophysics: Dynamical Systems
69. Shenker SJ, Kadanoff LP (1982) J Stat Phys 27:631
70. Siegel
CL
(1956)
Vorlesungen
über
Himmelsmechanik.
Springer, Berlin
71. Stäckel P (1890) Math Ann 35:91
72. Statler TS (1987) Astrophys J 321:113
73. Voglis N, Contopoulos G (1994) J Phys A 27:4899
74. Voglis N, Tsoutsis P, Efthymiopoulos C (2006) Mon Not R Astr
Soc 373:280
75. Walker GH, Ford J (1969) Phys Rev 188:416
76. Weinacht J (1924) Math Ann 91:279
77. Whittaker ET (1916) Proc Roy Soc Edinburgh 37:95
78. Whittaker ET (1937) A Treatise on the Analytical Dynamics of
Particles and Rigid Bodies, 4th edn. Cambridge Univ Press,
Cambridge
79. Wintner A (1947) Analytical Foundations of Celestial Mechan-
ics. Princeton Univ Press, Princeton

