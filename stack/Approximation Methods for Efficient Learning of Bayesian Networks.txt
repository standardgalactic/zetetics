APPROXIMATION METHODS FOR EFFICIENT 
LEARNING OF BAYESIAN NETWORKS 

Frontiers in Artificial Intelligence and Applications 
Volume 168 
Published in the subseries 
Dissertations in Artificial Intelligence 
Recently published in this series 
Vol. 167. P. Buitelaar and P. Cimiano (Eds.), Ontology Learning and Population: Bridging the 
Gap between Text and Knowledge 
Vol. 166. H. Jaakkola, Y. Kiyoki and T. Tokuda (Eds.), Information Modelling and Knowledge 
Bases XIX 
Vol. 165. A.R. Lodder and L. Mommers (Eds.), Legal Knowledge and Information Systems – 
JURIX 2007: The Twentieth Annual Conference 
Vol. 164. J.C. Augusto and D. Shapiro (Eds.), Advances in Ambient Intelligence 
Vol. 163. C. Angulo and L. Godo (Eds.), Artificial Intelligence Research and Development 
Vol. 162. T. Hirashima et al. (Eds.), Supporting Learning Flow Through Integrative 
Technologies 
Vol. 161. H. Fujita and D. Pisanelli (Eds.), New Trends in Software Methodologies, Tools and 
Techniques – Proceedings of the sixth SoMeT_07 
Vol. 160. I. Maglogiannis et al. (Eds.), Emerging Artificial Intelligence Applications in 
Computer Engineering – Real World AI Systems with Applications in eHealth, HCI, 
Information Retrieval and Pervasive Technologies 
Vol. 159. E. Tyugu, Algorithms and Architectures of Artificial Intelligence  
Vol. 158. R. Luckin et al. (Eds.), Artificial Intelligence in Education – Building Technology 
Rich Learning Contexts That Work 
Vol. 157. B. Goertzel and P. Wang (Eds.), Advances in Artificial General Intelligence: 
Concepts, Architectures and Algorithms – Proceedings of the AGI Workshop 2006 
Vol. 156. R.M. Colomb, Ontology and the Semantic Web 
Vol. 155. O. Vasilecas et al. (Eds.), Databases and Information Systems IV – Selected Papers 
from the Seventh International Baltic Conference DB&IS’2006 
Vol. 154. M. Duží et al. (Eds.), Information Modelling and Knowledge Bases XVIII 
Vol. 153. Y. Vogiazou, Design for Emergence – Collaborative Social Play with Online and 
Location-Based Media 
Vol. 152. T.M. van Engers (Ed.), Legal Knowledge and Information Systems – JURIX 2006: 
The Nineteenth Annual Conference 
Vol. 151. R. Mizoguchi et al. (Eds.), Learning by Effective Utilization of Technologies: 
Facilitating Intercultural Understanding 
ISSN 0922-6389 

Approximation Methods for Efficient 
Learning of Bayesian Networks 
Carsten Riggelsen 
Institute of Geosciences, University of Potsdam, Golm b. Potsdam, Germany 
 
Amsterdam • Berlin • Oxford • Tokyo • Washington, DC 

© 2008 The author and IOS Press. 
All rights reserved. No part of this book may be reproduced, stored in a retrieval system, 
or transmitted, in any form or by any means, without prior written permission from the publisher. 
ISBN 978-1-58603-821-2 
Library of Congress Control Number: 2007942192 
Publisher 
IOS Press 
Nieuwe Hemweg 6B 
1013 BG Amsterdam 
Netherlands 
fax: +31 20 687 0019 
e-mail: order@iospress.nl 
Distributor in the UK and Ireland 
Distributor in the USA and Canada 
Gazelle Books Services Ltd. 
IOS Press, Inc. 
White Cross Mills 
4502 Rachael Manor Drive 
Hightown 
Fairfax, VA 22032 
Lancaster LA1 4XS 
USA 
United Kingdom 
fax: +1 703 323 3668 
fax: +44 1524 63232 
e-mail: iosbooks@iospress.com 
e-mail: sales@gazellebooks.co.uk 
LEGAL NOTICE 
The publisher is not responsible for the use which might be made of the following information. 
PRINTED IN THE NETHERLANDS 

Contents
Foreword
ix
1.
INTRODUCTION
1
2.
PRELIMINARIES
5
1
Random variables and conditional independence
5
2
Graph theory
6
3
Markov properties
7
3.1
The Markov blanket
11
3.2
Equivalence of DAGs
12
4
Bayesian networks
14
5
Bayesian network speciﬁcation
14
5.1
Bayesian parameter speciﬁcation
15
3.
LEARNING BAYESIAN NETWORKS
FROM DATA
19
1
The basics
19
2
Learning parameters
20
2.1
The Maximum-Likelihood approach
20
2.2
The Bayesian approach
21
3
Learning models
24
3.1
The penalised likelihood approach
25
3.2
The Bayesian approach
26
3.2.1
Learning via the marginal likelihood
26
3.2.2
Determining the hyper parameter
29
3.3
Marginal and penalised likelihood
32

vi
EFFICIENT LEARNING OF BAYESIAN NETWORKS
3.4
Search methodologies
33
3.4.1
Model search space
34
3.4.2
Traversal strategy
35
4.
MONTE CARLO METHODS AND MCMC SIMULATION
37
1
Monte Carlo methods
37
1.1
Importance sampling
38
1.1.1
Choice of the sampling distribution
40
2
Markov chain Monte Carlo—MCMC
41
2.1
Markov chains
42
2.1.1
The invariant target distribution
43
2.1.2
Reaching the invariant distribution
43
2.1.3
Metropolis-Hastings sampling
44
2.1.4
Gibbs sampling
46
2.1.5
Mixing, burn-in and convergence of MCMC
49
2.1.6
The importance of blocking
51
3
Learning models via MCMC
53
3.1
Sampling models
54
3.2
Sampling edges
54
3.3
Blocking edges
56
3.3.1
Blocks and Markov blankets
58
3.3.2
Sampling blocks
59
3.3.3
Validity of the sampler
62
3.3.4
The MB-MCMC model sampler
62
3.3.5
Evaluation
64
3.3.6
Conclusion
69
5.
LEARNING FROM INCOMPLETE DATA
71
1
The concept of incomplete data
72
1.1
Missing data mechanisms
73
2
Learning from incomplete data
75
2.1
Likelihood decomposition
76
2.1.1
Complications for learning parameters
78
2.1.2
Bayesian sequential updating
79
2.1.3
Complications for learning models
80
3
Principled iterative methods
81
3.1
Expectation Maximisation—EM
81

Contents
vii
3.1.1
Structural EM—SEM
85
3.2
Data Augmentation—DA
87
3.2.1
DA and eliminating the P-step—DA-P
90
3.3
DA-P and model learning—MDA-P
92
3.4
Eﬃciency issues of MDA-P
93
3.4.1
Properties of the sub-MCMC samplers
93
3.4.2
Interdependence between samplers
95
3.5
Imputation via importance sampling
97
3.5.1
The general idea
97
3.5.2
Importance sampling in the I-step—ISMDA-P
98
3.5.3
Generating new population vs. re-weighing
101
3.5.4
The marginal likelihood as predictive distribution
102
3.5.5
The eMC4 sampler
103
3.5.6
Evaluation—proof of concept
104
3.5.7
Conclusion
109
4
Ad-hoc and heuristic methods
110
4.1
Available cases analysis
110
4.2
Bound and Collapse—BC
111
4.3
Markov Blanket Predictor—MBP
114
4.3.1
The general idea
115
4.3.2
Approximate predictive distributions
115
4.3.3
Parameter estimation
117
4.3.4
Prediction and missing parents
119
4.3.5
Predictive quality
120
4.3.6
Selecting predictive variables
120
4.3.7
Implementation of MBP
123
4.3.8
Parameter estimation
124
4.3.9
Model learning
126
4.3.10 Conclusion and discussion
128
6.
CONCLUSION
131
References
133

This page intentionally left blank

Foreword
This dissertation is the result of 4 years at Utrecht University as a
Ph.D-student at the Department of Information and Computing Sci-
ences. The work presented in this thesis is mainly based on the research
published in various papers during that time. However, it is not merely
a bundle of research articles. In order to provide a coherent treatment
of matters, thereby helping the reader to gain a thorough understanding
of the whole concept of learning Bayesian networks from (in)complete
data, this thesis combines in a clarifying way all the issues presented in
the papers with previously unpublished work. I hope that my eﬀorts
have been worthwhile!
My gratitude goes to my supervisor and co-promotor, dr. Ad Feelders.
I am thankful to my promotor, prof.dr. Arno Siebes and to Jeroen De
Knijf, Edwin de Jong and all the other colleagues in the Algorithmic
Data Analysis group, and at the computer science department.
I would like to thank the members of the outstanding reading com-
mittee: prof.dr.ir. Linda van der Gaag, prof.dr. Richard D. Gill, prof.dr.
Finn V. Jensen, prof.dr. Bert Kappen and prof.dr. Pedro Larra˜naga.
The present dissertation was successfully defended on October 23,
2006 at Utrecht University. For publication in the series “Frontiers in
Artiﬁcial Intelligence and Applications”, IOS Press, only minor details
have been corrected and very few additions have been made.
Carsten Riggelsen
Berlin, October 2007

This page intentionally left blank

Chapter 1
INTRODUCTION
Learning from data
Several approaches to learning from data have emerged through time,
many of them based on very diﬀerent theories and philosophies. Data
mining, machine learning, knowledge discovery, etc. all share the fun-
damental desire to extract in a principled fashion some essentials from
data (e.g., local patterns, nuggets, (ir)regularities, etc.) or even learn
about, or gain insight into, the underlying data generating process.
Which approach to learning is the most appropriate one is open for
discussion, and may depend on pragmatic considerations or more pro-
found beliefs. In any case, it is almost certain that computer science will
play a central role, directly or indirectly, in the learning task. Computer
science has a major impact on all ﬁelds of scientiﬁc research. It pushes
the boundaries of “what’s possible” in most areas. In many ways, ad-
vances in computer science have altered the way we perceive scientiﬁc
research today.
We consider the task of learning from data as a challenge that is in-
herently statistical in nature. Like most branches of science, statistics
is built upon well-founded principles that stipulates lines of valid rea-
soning. However, in learning from data, the statistical approach is the
natural choice because it provides methods and tools that have been
designed speciﬁcally for the purpose of analysing (observational) data.
Although our departure point to learning is a statistical one, there is an
important pragmatic aspect to this; we do not want to constrain our-
selves to a pure statistical approach when the statistical theory leads

2
EFFICIENT LEARNING OF BAYESIAN NETWORKS
to computationally intractable implementations. In that case we resort
to methods that are perhaps less well-founded or principled in nature,
yet are tractable from a computational point of view. Fortunately, with
the recent advances in computer science, statistical analysis has proven
itself in practice as well.
The role of computation
Improvements in computational power means that Monte Carlo (MC)
simulation has become an viable approach to many statistical problems,
that could not be tackled easily before. In the so-called Bayesian ap-
proach, statistical inference is based on entire distributions on various
unknown quantities. The distributions usually have a form that makes
it diﬃcult to formulate them as a closed expression. By way of Markov
chain Monte Carlo (MCMC) simulation, empirical evidence can be pro-
duced from these distributions. These empirical samples approximate
the posterior distributions, and inference is now based on these sam-
ples rather than the exact distributions.
MCMC methods are sound
and valid from a statistical point of view, but it is only by the help of
computers that it has become feasible to actually implement the tech-
niques. This in turn has meant that the Bayesian paradigm, treating
“everything” we are uncertain about as a random variable, has received
quite a lot of attention recently, and to a large extent is just as feasible
as the classical statistical approach. Bayesian statistics is usually more
informative compared to the classical approach, as entire distributions
convey more information than a few summary statistics.
Learning from incomplete data
Most methods for performing statistical data analysis or learning re-
quire complete data in order to work or produce valid results. Unfor-
tunately real-life data(bases) are rarely complete. For statistical analy-
sis of incomplete data the standard tools and algorithms developed for
complete data often don’t suﬃce anymore. Incomplete data is struc-
tured in a way that makes principled data analysis very diﬃcult and
sometimes even intractable. Valid statistical analysis of incomplete data
often leads to high computational complexity compared to a complete
data scenario. Learning from incomplete data is a non-trivial extension
of existing methods developed for learning from complete data.
Also in this context, MCMC simulation techniques provide a feasible
means for learning within the Bayesian paradigm. However, the added
computational burden of the learning task with incomplete data means

Introduction
3
that for large amounts of data that even MCMC techniques may be too
expensive. In that case the only feasible thing to do is to slightly relax
a few assumption about interaction eﬀects in the data, apply heuristics
and resort to summary statistics.
An unfortunate side eﬀect of this
is that we have to trade in the Bayesian approach to data analysis.
However, for large amounts of data, posterior distributions are close
to Normal distributions anyway, and therefore summary statistics often
suﬃce to describe the distributions.
Bayesian network learning
Learning from data ranges between extracting essentials from the
data, to the more fundamental and very challenging task of learning the
underlying data generating process. The former only captures certain
aspects of the data, whereas the latter captures the very “thing” that
gave rise to the data. Obviously, learning the data generating process
yields the strongest result in the sense that it reveals “everything” there
is to learn.
From a statistical point of view, the assumption is that we can cap-
ture the underlying data generating process in terms of a probability
distribution. This gives rise to the question which statistical formalism
is expressive and ﬂexible enough to be able to capture a broad range
of data generating processes. The Bayesian network (BN) formalism is
one possibility. Informally speaking BNs capture the interaction eﬀects
that hold between random variables in terms of a graph. Put in an-
other way, BNs are so-called directed graphical models which is a class
of statistical models deﬁned by a collection of conditional independences
between variables. This graph oﬀers an appealing way of structuring an
otherwise confusing number of equations expressing the (in)dependences
between variables.
BNs occupy a prominent position in decision support environments
where they are used for diagnostic and prediction purposes. In terms
of interpretability, the directed graphical structure (model) of a BN is
attractive, because explicit insight is gained into relationships between
variables. Learning the underlying data generating process in terms of
a BN means that explicit insight is gained into the “workings” of the
data generating process.
Research objectives
The title of the thesis “Approximation Methods for Eﬃcient Learn-
ing of Bayesian Networks” summarises what this thesis is about: We

4
EFFICIENT LEARNING OF BAYESIAN NETWORKS
develop and investigate eﬃcient Monte Carlo simulation methods in or-
der to realise a Bayesian approach to approximate learning of Bayesian
networks from both complete and incomplete data. For large amounts of
incomplete data when Monte Carlo methods are ineﬃcient, the approx-
imations mentioned in Section 1 are implemented, such that learning
remains feasible, albeit non-Bayesian.
Thesis outline
This thesis is divided into six chapters. The topics treated in each
chapter are as follows:
Chapter 2 In the next chapter the notation and some basic concepts
about probabilities, graph theory and conditional independence are
introduced.
We present the Bayesian network formalism, and we
illustrate how to perform parameter learning taking a Bayesian sta-
tistical approach.
Chapter 3 This chapter is about Bayesian network learning from data.
We continue where we left oﬀin Chapter 2, and focus on the Bayesian
approach to model and parameter learning. In that context the so-
called marginal likelihood is also introduced which plays a crucial
role when learning from both complete and incomplete data.
Chapter 4 Here we treat Monte Carlo simulation techniques, impor-
tance sampling and Markov chain Monte Carlo (MCMC) methods.
These techniques are used in Chapter 5. We also develop an eﬃcient
MCMC algorithm called MB-MCMC for sampling Bayesian network
models from complete data.
Chapter 5 In this chapter we introduce the concept of incomplete data,
and we illustrate the diﬃculties in connection with Bayesian network
learning. We go on and treat principled methods for learning, and
discuss iterative methods for model and parameter learning. Var-
ious Bayesian approaches to learning based on MCMC simulation
are also introduced. We combine importance sampling and MCMC,
and develop an eﬃcient algorithm for improved model learning from
incomplete data called eMC4. Finally, we treat less principled, yet
very fast methods for learning, and a new algorithm called MBP is
developed.
Chapter 6 We draw conclusions and summarise the main contributions
of this thesis.

Chapter 2
PRELIMINARIES
In this chapter the notation is presented and the basic concepts related
to the Bayesian network formalism are treated.
Towards the end of
the chapter, we introduce the Bayesian statistical approach to learning.
Obviously there is much more to say about these issues than what is
mentioned here. Thorough introductions and in-depth treatment can
be found elsewhere in the literature such as Cowell et al., 1999; Jensen,
2001; Whittaker, 1990.
1.
Random variables and conditional
independence
Essential to the Bayesian network formalism is the notion of condi-
tional independence between random variables. In this section we focus
on the statistical/probabilistic properties of these variables and the al-
gebraic laws of the (in)dependences between variables; see for instance
Feller, 1970 for more details.
Throughout we use upper case letters to denote (random) variables,
X, Y, Z. The corresponding lower case letters denote values of the vari-
ables, x, y, z.
Bold letters denote vectors, X, Y , Z for variables and
x, y, z for values. The term vector is used somewhat loosely, and in
some places the term set would be more appropriate. Coordinate pro-
jection is sometimes used to extract the required sub-vector of X, e.g,
X{2,5} = X25 denotes the vector (X2, X5). Any deviation from these
conventions will be clear from the context or will be explicitly mentioned
when needed.

6
EFFICIENT LEARNING OF BAYESIAN NETWORKS
A random variable X has a state-space ΩX consisting of the possible
values x.
The state-space of a vector of random variables X is the
Cartesian product of the individual state-spaces of the variables Xi in
X, i.e., ΩX = 
i ΩXi.
In the paragraphs to follow, summations should be replaced by inte-
grals in the case of continuous random variables.
A joint probability distribution Pr(X) over X is a mapping ΩX →
[0; 1] with 
x Pr(x) = 1.
The marginal distribution over Y where
X = Y ∪Z is Pr(Y ) = 
z Pr(z, Y ), i.e., the marginal distribution
is obtained by summing out all variables except Y . We say that Y is
independent of Z if Pr(Y , Z) = Pr(Y ) · Pr(Z).
The conditional probability distribution Pr(Z|y) is deﬁned as Pr(Z,y)
Pr(y)
for Pr(y) > 0, i.e., re-normalise over Z for ﬁxed Y .
Everything is now in place to introduce the concept of conditional in-
dependence. Conditional independence between random variables cap-
tures a notion of irrelevance between variables that comes about when
the values of some other variables are given.
Formally we say that Y is conditionally independent of Z given S if
(not surprisingly) Pr(Z, Y |S) = Pr(Z|S) · Pr(Y |S). For Pr(Y |S) > 0
we may instead write this as Pr(Z|Y , S) = Pr(Z|S), i.e., once S is
given, Z becomes independent of Y .
The interpretation of the latter equation is conveniently captured by
the operator ⊥⊥; the independence statement is then written as Z⊥⊥Y |S.
Given a set of conditional independence statements, other conditional
independence statements are entailed, and they can be derived using
the so-called (semi-)graphoid axioms provided by Pearl, 1988. There
are however many entailed statements, and we therefore introduce a
diﬀerent algebraic structure that captures many of these statements “at
a glance”; this structure takes the form of a graph.
2.
Graph theory
A graph is a pair (X, E), where X = {X1, . . . , Xp} is a set of p
vertices and E is a subset of the ordered pair X × X. If (Xi, Xj) ∈E
and (Xj, Xi) ∈E then vertex Xi and Xj are connected by an undirected
edge, and we write this as Xi−Xj. When (Xi, Xj) ∈E and (Xj, Xi) ̸∈E
then Xi and Xj are connected by a directed edge (also referred to as an
arc) and we write Xi →Xj. When (Xi, Xj) ̸∈E and (Xj, Xi) ̸∈E
we write Xi ̸−Xj. When two vertices are joined by a directed or an
undirected edge, the two vertices are adjacent.

Preliminaries
7
When Xi →Xj then Xi is a parent of Xj, and Xj is a child of Xi. All
parents of Xi belong to the parent set Xpa(i), where the indices of the
parent vertices are returned by the function pa(i). Similarly all children
of Xi belong to the child set Xch(i), where the indices are returned by
ch(i). A graph is empty if the set E is empty, and it is complete if any
pairs of vertices are adjacent.
A graph where all edges are directed is called a directed graph, and
a graph without directed edges is called an undirected graph. A path
of length n between Xi and Xj is a sequence of distinct vertices Xi =
X(1), X(2), . . . , X(n) = Xj, where X(k−1) and X(k) are adjacent, k =
2, . . . , n.
For a directed path it must hold that X(k−1) →X(k), i.e.,
every edge on the path in the directed graph points in the same forward
direction.
In an undirected graph with U, S, Y ⊂X, the set S separates U
from Y if every path between vertices from U and Y intersects at least
one vertex in S.
A directed cycle is a directed path where begin and end vertices are the
same, X(1) = X(n). A directed acyclic graph (DAG) is a directed graph
with no directed cycles. The ancestors, Xan(i), of Xi are all vertices that
precede Xi on directed paths passing/to Xi. The ancestral set of a set
S is 
Xi∈S Xan(i). The descendants, Xde(i), of Xi are all vertices that
succeed Xi on directed paths starting from Xi. The non-descendants is
Xnd(i) = X \ (Xde(i) ∪{Xi}).
A serial connection is Xi →Xk →Xj, and a diverging connection
is Xi ←Xk →Xj. A converging connection is Xi →Xk ←Xj where
Xk is called a collider, and if additionally Xi ̸−Xj the connection is
called a v-connection. A DAG without v-connections is called moral (the
parents of Xk are “married” via an arc). A DAG can be moralised by
marrying the oﬀending parents of common children via an undirected
edge, followed by dropping directions of all arcs. The moralised graph
is thus an undirected graph.
3.
Markov properties
We now associate random variables X = {X1, . . . , Xp} with the ver-
tices of a graph, and use the graph to encode conditional independence
restrictions for a joint probability distribution Pr(X).
The so-called
Markov properties specify how these independence statements relate to
the graph: how to (en)code them and how to extract them.
A number of Markov properties can be ascribed to graphs. There are
properties for DAGs and for undirected graphs. We will concentrate on

8
EFFICIENT LEARNING OF BAYESIAN NETWORKS
the three Markov properties for DAGs: The directed pairwise (DP), di-
rected local (DL) and the directed global (DG) Markov properties. They
are deﬁned as follows:
DP For any non-adjacent vertices Xi and Xj with Xj ∈Xnd(i), it holds
that:
Xi⊥⊥Xj|Xnd(i) \ {Xj}
Hence, Xi is independent of a non-descendant Xj given the remaining
non-descendants, unless Xi and Xj are directly joined.
DL For any Xi it holds that:
Xi⊥⊥Xnd(i) \ Xpa(i)|Xpa(i)
Hence, Xi is independent of its non-descendants given its parents.
DG For any disjoint sets U, S, Y ⊆X where S separates U from Y
in the moralised graph of the ancestral set containing U ∪S ∪Y , it
holds that:
U⊥⊥Y |S
As an example consider the DAG in ﬁgure 2.1, top. If we want to
investigate whether X1⊥⊥X6|{X3, X4} the graph containing the ances-
tral set of X1, X6, X3, X4 is formed, and the graph is moralised. The
undirected graph in ﬁgure 2.1, bottom, left, is the result. From this we
see that any path between X1 and X6 passes X3, and the independence
statement is correct.
To check if X1⊥⊥X4|X5, the graph containing the ancestral set of
X1, X4, X5 is formed and moralised resulting in the graph in ﬁgure 2.1,
bottom, right. From this we see that the independence statement does
not hold: there is a path from X1 to X4 via X3 that does not pass X5.
The d-separation (DS) criterion by Pearl and Verma, 1987 provides
and alternative to the directed global Markov property. Given S ⊆X
and any pair of vertices Xi, Xj ̸∈S a path between Xi and Xj is active
if:
1 Every non-collider in the path is not in S.
2 Every collider in the path is in S or has descendants in S.
If S creates an active path between Xi and Xj then they cannot be
conditionally independent given S. When the path is not active, the
path is blocked by S.

Preliminaries
9
X1
X2
X3
X4
X5
X6
X1
X2
X3
X4
X5
X6
X1
X2
X3
X4
X5
Figure 2.1.
The DG Markov property. Top: a DAG model. Bottom: two moralised
graphs.
DS For disjoint sets U, S, Y ⊆X, U is d-separated from Y by S if
every path between vertices in U and Y is blocked by S.
This reading captures a notion of ﬂow between variables when the
paths between them aren’t blocked.
Lauritzen et al., 1990 show that DS is equivalent to DG. Additionally
it turns out that DG is equivalent to DL which in turn imply DP:
DS ⇔DG ⇔DL ⇒DP

10
EFFICIENT LEARNING OF BAYESIAN NETWORKS
P2
P1
P4
P3
P2
P1
P4
P3
P2
P1
P4
P3
Figure 2.2.
The “Yawning philosophers problem”: The shaded circle in the middle
represents the pillar, and P1, . . . , P4 are the yawning philosophers standing around it.
The arcs/edges are the lines of sight. In the picture to the right, the pillar necessarily
has to be ignored (!) as a consequence of a DAG model representation.
A probability distribution Pr(X) obeying the DG Markov property
with respect to a DAG, factorises in the following way:
Pr(X) =
p

i=1
Pr(Xi|Xpa(i))
(2.1)
It is not necessarily so that all conditional independences of a given
probability distribution can be captured by a DAG model, or, equiva-
lently, that the distribution can be factorised as in eq. 2.1. When the
independences induced by the DAG model are a subset of the indepen-
dences holding for a given joint distribution, we say that the DAG model
is an I-map (independence map) of the probability distribution. There
may be more independences holding in the probability distribution than
what can be read oﬀthe DAG.
As an example consider the following “Yawning philosophers prob-
lem”: Suppose that Pi, i = 1, . . . , 4 denotes the binary random variable
representing whether philosopher i is yawning or not.
For mammals
it is well-known that the probability of yawning is increased when one
sees other mammals yawning. Position the philosophers around a Greek
pillar starting with P1 followed by P2 etc., such that Pi cannot see the
philosopher on the other side of the pillar but only the two philoso-
phers on his left and right side; ﬁgure 2.2, left, depicts this. Hence for
Pi it holds that he has direct visual contact with his two neighbours.
The probability of Pi yawning depends on his neighbours, but is inde-
pendent on the philosopher on the other side given the yawing state
of his neighbours. Hence the following independence statements hold:
P1⊥⊥P3|{P2, P4}, P2⊥⊥P4|{P1, P3}.

Preliminaries
11
No DAG model can capture the two independence assumptions simul-
taneously. The associations between the four variables via four directed
arcs will necessarily introduce a v-connection because no cycles are al-
lowed; see ﬁgure 2.2, middle, for an example. The moralised version
of this DAG will join either P4 −P2 or P1 −P3; see ﬁgure 2.2, right.
Using the DG Markov property this in eﬀect means that one of the con-
ditional independences actually holding cannot be read oﬀthe DAG. In
more general terms, when a v-connection is unavoidable, all indepen-
dence statements that hold cannot be encoded simultaneously; e.g., for
many grid like structures where each intersection represents a random
variable, the “Yawning philosophers problem” exists.
We say that a DAG model is a D-map (dependence map) of a given
joint probability distribution, if all independences that hold for the joint
distribution can be read oﬀthe DAG model. A DAG model is a P-map
(perfect map) of a probability distribution if it is both an I-map and a
D-map of the joint distribution; equivalently we say that the probability
distribution is faithful with respect to the DAG model. In that case not
only does the absence of edges encode independences, but the presence
of edges also encode dependences.
It remains unclear whether most “real-life” distributions or processes
are faithful with respect to DAG models. Strictly speaking this means
that when we learn DAG models from empirical observations, as we will
in Chapter 3, we may not interpret the presence of arcs in a DAG model
as dependences. However, for large DAG models some parts of the DAG
model may be a perfect map and other parts may not.
Admittedly,
although this remains rather vague, it is our hope that a substantial
part of the DAG model indeed does capture the dependences and only a
few dependences we read oﬀare wrong. We would like to stress that the
underlying assumption here is that the objective is to interpret the DAG
model in terms of the (in)dependences it encodes. From the perspective
of prediction one may not be interested in interpreting the DAG model
at all, but merely use it as a concise way of representing a probability
distribution in terms of eq. 2.1.
3.1
The Markov blanket
An important notion is that of the Markov blanket of a random vari-
able Xi. This term is used throughout this thesis, as it plays an impor-
tant role in many learning contexts. The interesting thing about this set

12
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Xi
Xi
Figure 2.3.
The Markov blanket of Xi. Left: the DAG model. Right: the moralised
graph.
is that Xi is independent of all other variables given its Markov blanket;
the blanket so to speak shields oﬀinﬂuence from outside.
The Markov blanket of Xi is:
Xmb(i) = Xpa(i) ∪Xch(i)

j∈ch(i)
Xpa(j) \ {Xi}
i.e., the parents, the children, and the parents of the children.
The
graph in ﬁgure 2.3, left, illustrates the Markov blanket of Xi; the shaded
vertices belong to the Markov blanket of Xi. The graph in ﬁgure 2.3,
right, is a part of the moralised graph of the DAG. From this graph
it is easy to see that all paths to Xi necessarily have to pass a shaded
vertex. Using the DG Markov property we then have that Xi⊥⊥X \
(Xmb(i) ∪{Xi})|Xmb(i). It follows that the Markov blanket consists of
those variables that are immediately relevant for prediction of Xi.
3.2
Equivalence of DAGs
Diﬀerent DAGs can represent the same set of conditional indepen-
dence restrictions; in that case we say that DAGs are equivalent. The
DAG models are partitioned into equivalence classes. Two DAGs are
equivalent if the following two conditions both hold (Verma and Pearl,
1990):
1 By dropping directions, the resulting undirected graphs are the same.
2 The two DAGs have the same v-connections.
Edges of DAGs that have the same direction in all DAGs in a partic-
ular equivalence class are compelled; the other edges are reversible. For

Preliminaries
13
instance from the DAG X1 →X2 →X3 we can read oﬀX1⊥⊥X3|X2,
but the two alternatives X1 ←X2 →X3 and X1 ←X2 ←X3 also
encode X1⊥⊥X3|X2; all these DAGs are equivalent because by dropping
directions they are the same undirected graph, and they all have the
same v-connections (none). In this example both arcs are reversible.
A canonical representation of an equivalence class is obtained via a
partially directed-undirected graph. For all reversible edges the direction
is dropped and for compelled edges the direction is kept. The resulting
graph is referred to as an essential graph (Andersson et al., 1997). For
X1 →X2 →X3, the essential graph is X1 −X2 −X3. Given a DAG, the
essential graph of its equivalence class can be obtained in polynomial
time via an algorithm given in Chickering, 1995.
Given two diﬀerent DAGs, mi and mj, in the same equivalence class,
it is possible to transform mi into mj, through a series of elementary
graph operations, without leaving the equivalence class.
The notion
of covered arc reversal is required for this purpose (Chickering, 1995).
An arc Xi →Xj is covered if the vertices pertaining to the arc have the
same parent set, i.e., Xpa(i) = Xpa(j)\{Xi}. Reversing a covered arc will
not alter the v-connections, and by dropping directions, the undirected
graph will remain unchanged. This means that the DAG obtained by a
single covered arc reversal is equivalent to the initial DAG. Furthermore,
there exists a sequence of DAGs, mi = m(1), . . . , m(n) = mj, where
m(k+1) is obtained from m(k), k = 1, . . . , n−1 by reversing a covered arc
in m(k). Consequently, all DAGs of an equivalence class can be reached
by repeated covered arc reversals. Note that a compelled arc can never
be covered, but also that a reversible arc is not necessarily covered for a
given DAG in the sequence; reversible arcs are only covered in a given
graph in the equivalence class. In the example X1 →X2 →X3 arc
X1 →X2 is covered, but X2 →X3 is not even though X2 →X3 is
reversible. Reversing the covered arc X1 →X2 we get the equivalent
DAG, X1 ←X2 →X3. Now both arcs are covered, and by reversing
X2 →X3 we obtain another equivalent DAG, X1 ←X2 ←X3.
The fact that many arcs are reversible suggests that arcs should not
be interpreted as causal inﬂuences.
From a statistical point of view
equivalent DAGs are statistically indistinguishable, and in order to assign
any causal semantics to arcs, certain assumptions deﬁnitely have to be
made. We refer to Pearl, 2000; Spirtes et al., 2000 for treatment of these
matters. In this thesis we assign no particular semantics to edges except
for the conditional independence restrictions that they encode via the
Markov properties.

14
EFFICIENT LEARNING OF BAYESIAN NETWORKS
4.
Bayesian networks
Up until now, the focus has been on the conditional independences
and the factorisation of a joint probability distribution according to some
DAG model. In the remainder we distinguish the qualitative constraints
dictated by the DAG model from the quantitative part consisting the lo-
cal conditional probabilities deﬁned “along the arcs” of the DAG model.
We use the term Bayesian Network (BN) to denote the pair (m, θ)
(Pearl, 1988) consisting of:
1 The DAG model m = (X, E) encoding conditional independence
statements. Xi is discrete random variable associated with a vertex
of m.
2 The parameter of m denoted by θ = (θ1, . . . , θp), where θi consists
of the local probabilities θXi|Xpa(i) = Pr(Xi|Xpa(i)) in eq. 2.1.
Moreover, we make m and θ explicit, and write eq. 2.1 as:
Pr(X|m, θ) =
p

i=1
Pr(Xi|m, Xpa(i), θi) =
p

i=1
θXi|Xpa(i)
(2.2)
Via the usual algebraic laws of probability theory, any marginal or
conditional probability (distribution) can be computed. However, by ex-
ploiting the independences induced by the DAG, the eﬃciency of these
computations can be improved. Several exact inference algorithms have
been developed that use the DAG directly as computational backbone for
these calculations, such as the method described in Pearl, 1986. Other
inference algorithms use the moralised graph as a departure point plus
some additional transformations to perform these computations, such as
the algorithm by Lauritzen and Spiegelhalter, 1988 and Jensen et al.,
1990. In general however, inference in a BN is NP-hard (Cooper, 1990),
and, even for feasible problems, inference is considered a computation-
ally demanding operation.
5.
Bayesian network speciﬁcation
We can either specify a BN by hand, learn it from data, or, com-
bine both approaches, i.e., use prior knowledge and complement this
knowledge with what can be learned from data. The Bayesian statis-
tical paradigm oﬀers a principled approach for dealing with knowledge
updating; it describes how to combine and update expert knowledge
with observational data. It provides a sound framework for representing

Preliminaries
15
and reasoning with uncertainty and imprecision. Within the scope of
learning these concepts are of great importance; the ﬁnite sample size
and missing data are sources that introduce uncertainty or imprecision
with respect to what is subject to learning.
Another advantage of the Bayesian approach is that it is typically very
clear about the diﬀerent assumptions made by explicitly conditioning
on these assumptions; in order to get rid of them, they have to be
marginalised out again.
A common misconception is that the Bayesian statistical approach is
inherent to the Bayesian network formalism; this is not the case. In fact,
when specifying a BN, by hand or otherwise, the classical (frequentist)
approach is usually taken. We refer to Gelman et al., 2004 for more on
the Bayesian approach to data analysis in general.
5.1
Bayesian parameter speciﬁcation
This section serves as an illustration of how “to be a Bayesian”. In
particular we show how the parameter of a BN is treated from a Bayesian
point of view. Additionally this section is a departure point for the next
chapter about learning BNs.
We assume that m is ﬁxed (given), but that the parameter is subject
to uncertainty and is considered as a random variable Θ. The uncer-
tainty of Θ is captured by a distribution Pr(Θ|m). This distribution
conveys all information we have about Θ given m.
Since the parameter is not assumed given anymore as in eq. 2.2 but
is subject to learning, it is integrated out with respect to Pr(Θ|m):
Pr(X|m) =

Pr(X, θ|m)dθ =

Pr(X|m, θ) · Pr(θ|m)dθ
(2.3)
The equation is a kind of “averaging” over possible values that Θ can
take on, depending on the uncertainty of the possible parameter values
captured by Pr(Θ|m). At this point we see that Pr(Θ|m) is conditional
on the ﬁxed model m only; it is the prior distribution before having
been presented with any information that inﬂuences the “knowledge”
about the parameter.
If we have that:
Pr(Θ|m) =
p

i=1
Pr(Θi|m)
(2.4)

16
EFFICIENT LEARNING OF BAYESIAN NETWORKS
i.e., that parameters Θi are globally independent (Spiegelhalter and Lau-
ritzen, 1990) we get from eq. 2.3 and the BN factorisation in eq. 2.2:
Pr(X|m)
=

p

i=1
Pr(Xi|Xpa(i), m, θi) · Pr(θi|m)dθ
=
p

i=1
E[Θi]Xi|Xpa(i)
=
p

i=1
Pr(Xi|Xi, θi = E[Θi], m)
(2.5)
where E[Θi] =

θi · Pr(θi|m)dθi. A consequence of this assumption
is that in order to exploit what is (currently) known about the param-
eter and the associated uncertainty for computing any conditional or
marginal probability of X, we should use the expected parameter val-
ues per vertex with respect to the parameter distributions that capture
the parameter uncertainty.
Furthermore, from the integrand in eq. 2.3 we see that:
Pr(X, Θ|m) =
p

i=1
Pr(Xi|m, Xpa(i), Θi) · Pr(Θi|m)
i.e., Θi can be considered an extra parent of Xi in m; the result is an
augmented BN. Using the DG Markov property, it is then possible to
check which variables inﬂuence the parameter Θi. Hence, in order to
update the parameters once additional observations are presented, the
inﬂuence restrictions determine how this evidence is distributed towards
these parameters, i.e., what impact new information has on the distribu-
tion on the parameter space. For instance, given some evidence O ⊂X
in the form of a new incoming “fact”, we are interested in the joint
distribution on X given this new fact, Pr(X|m, O) = 	p
i=1 E[Θi|O].
Here the expectation is taken with respect to the posterior parameter
distribution Pr(Θ|m, O), i.e., the parameter distribution conditional on
O. The question that can be answered via the DG Markov property is
then, how the posterior distribution of Θ depends on O.
In particular, observe that Θi is independent of all other vertices
given Xi and Xpa(i), because this conditioning set is exactly the Markov
blanket of Θi (the vertex has no parents). Hence, given that Xi and
Xpa(i) are in O, we have that Pr(Θi|m, O) = Pr(Θi|m, Xi, Xpa(i)).
In ﬁgure 2.4, left, an example of an augmented BN is shown. Suppose
that we observe O = (X4, X5), and that we wonder if this has an impact

Preliminaries
17
X1
X2
X3
X4
X5
Θ1
Θ2
Θ3
Θ4
Θ5
X1
X2
X3
X4
X5
Θ1
Θ2
Θ3
Θ4
Θ5
Figure 2.4.
The Bayesian approach to parameter learning. Left: an augmented BN.
Right: the moralised graph.
on the posterior distribution for Θ3. We create the moralised graph in
ﬁgure 2.4, right. From this graph we can read oﬀthat Θ3 indirectly
depends on X5. This is also the case for X4 even though it is far from
being “local” to Θ3. Had X3 been part of our observation, then inﬂuence
from X4 and X5 would have been blocked. The fact that this is not the
case means that the posterior parameter distribution for Θ3 is updated
via a BN inference method, that is, O = (X4, X5) gives rise to a certain
amount of information that is added to our knowledge. This example
illustrates that X and Θ are essentially treated in the same way.
In the ideal situation, all assumptions should be regarded as random
variables and belong on the conditional side of a distribution. Once we
decide to marginalise them out we are forced to quantify their uncer-
tainty in terms of distributions as in eq. 2.3. When learning BNs from
data, usually only the quantities that depend on the data sample are
treated as random variables. In this section only the parameter of a BN
was treated in a Bayesian fashion; in Chapter 4, Section 3 we go one step
further, and treat the edges of the BN DAG model as random variables.
In this regard it is interesting to observed that “learning” is intrin-
sic to Bayesian statistics. When confronted with new observations, the
Bayesian applies an updating rule based on Bayes’ law which revises
existing distributions thereby capturing a notion of “learning from ex-
perience”; our belief is dynamically changed as more information is ab-
sorbed, and at any given time, we are able to exploit what has been
gathered.

This page intentionally left blank

Chapter 3
LEARNING BAYESIAN NETWORKS
FROM DATA
In this chapter we discuss how to learn Bayesian network models
and the parameters from data. In Chapter 2, Section 5.1 the Bayesian
approach to “learning from experience” was introduced. In this chap-
ter we continue along these lines, and we apply the Bayesian updating
approach given a whole set of observations. There are however alterna-
tives to the Bayesian approach; some of these alternatives are treated
as well, though in less detail.
For model learning we do not discuss
the so-called constraint-based approaches based on independence tests
that are conducted on the data, from which the BN models are derived
(Spirtes et al., 2000; Bromberg et al., 2006).
1.
The basics
The goal is to learn from an independent and identically distributed
(iid) data sample d = (d1, . . . , dc), consisting of c cases (or records).
Each record dj = (xj
1, . . . , xj
p), contains instantiations xj
i of variable Xi
for i = 1, . . . , p for the jth case.
We reduce the data sample to contingency tables representing vectors
with observational counts distributed according to a product multino-
mial distribution parameterised by θ = (θ1, . . . , θp) where θi consists of
the local parameters θXi|Xpa(i) that determine the probability of a case
falling into a particular cell associated with vertex Xi. The cell counts
are extracted from the contingency tables using the function n(·), e.g.,
n(xi, xpa(i)) returns the number of occurrences in the data sample of
Xi = xi and Xpa(i) = xpa(i). Note that by maintaining the counts only,

20
EFFICIENT LEARNING OF BAYESIAN NETWORKS
we seem to lose information about the ordering of the data records.
However, there is no information loss, the iid assumption implies that
the ordering is completely random.
The likelihood function of the product multinomial distribution is:
L(m, θ|d) ∝Pr(d|m, θ)
=
p

i=1

xpa(i)

 
xi
θ
n(xi,xpa(i))
xi|xpa(i)

(3.1)
=
c

j=1
p

i=1
Pr(xj
i|m, xj
pa(i), θi)
(3.2)
Henceforth, we ignore the proportionality factor, and refer to Pr(d|m, θ)
instead of L(m, θ|d) as the likelihood function. The expression in eq. 3.2
explicitly shows that each record j is an independent realisation from
the BN (θ, m).
The likelihood function plays a crucial role in learning BNs. It de-
scribes the process giving rise to the data in terms of the model and
parameter.
2.
Learning parameters
Learning the parameter of a BNs is computationally less demand-
ing than learning the DAG model. For complete data, iteration or ap-
proximation is not required. Closed version solutions are available as
functions of the counts, n(·).
2.1
The Maximum-Likelihood approach
The likelihood function Pr(d|m, θ) should be read as “the likelihood of
(m, θ) given d”. Following this reading, for a ﬁxed model, one approach
to learning the parameter, is to maximise the likelihood with respect to
θ. The intuition behind this, is that the parameter hypothesis that has
the maximum support from data, is the best hypothesis possible:
ˆθ = arg max
θ
Pr(d|m, θ)
For the multinomial distribution, it is easy to determine this estimate.
The ﬁrst moment of the multinomial distribution is:
E[n(xi, xpa(i))|θi, m] = θxi|xpa(i) ·

xi
n(xi, xpa(i))
(3.3)
The multinomial distribution belongs to the regular exponential family,
i.e., it may be written in a particular functional form. The exponential

Learning Bayesian Networks from Data
21
family is a function of the so-called suﬃcient statistics that are func-
tions of the data, T (d); given those statistics, d brings no additional
information about θ (Casella and Berger, 2002). For the multinomial
distribution T (d) coincides with the counts, n(xi, xpa(i)). For the ex-
ponential family, the Maximum-Likelihood (ML) estimator is found by
solving E[T (d)|θ, m] with respect to θ (Cox and Hinkley, 1974; Lehmann
and Casella, 2001). Thus for the multinomial, equating the cell counts
to their expectations using eq. 3.3, it follows that the ML estimators
are:
ˆθxi|xpa(i) =
n(xi, xpa(i))

xi n(xi, xpa(i))
(3.4)
2.2
The Bayesian approach
The Bayesian does not admit to the reading “best support given
data”, because it implies that our knowledge about θ is fully determined
by d, irrespectively of what is known beyond d. Any common, accumu-
lated or prior knowledge is essentially joined together with d forming
a batch using ad-hoc principles or methods that are less “elegant” and
principled than one could hope for.
As discussed in Chapter 2, Section 5.1, the Bayesian treats the param-
eter as a random variable, and deﬁnes a distribution on the parameter
space. This prior distribution captures the uncertainty with respect to
the parameter. As we saw in Chapter 2, Section 5.1, the expectation
of the posterior distribution is the logical choice when calculating any
marginal or conditional distribution for X.
In general however, the
Bayesian is interested in much more than merely the expectation of the
posterior distribution; the entire distribution is interesting. Speciﬁcally,
ultimately we are interested in the posterior distribution Pr(Θ|m, d),
since this contains everything we know about Θ.
The actual updating of prior to posterior distribution goes via Bayes’
law. Bayes’ law combines the information contained in the likelihood
function with the prior parameter distribution, and transforms it into a
posterior parameter distribution:
Pr(Θ|m, d)
=
Pr(d|m, Θ) · Pr(Θ|m)

Pr(d|m, θ) · Pr(θ|m)dθ
(3.5)
∝
p

i=1

xpa(i)

 
xi
θ
n(xi,xpa(i))
xi|xpa(i)

· Pr(Θ|m)
(3.6)

22
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The denominator in eq. 3.5 is the normalising term. Alternatively, in-
stead of considering d as a single batch, we may update the posterior on
a per case basis; ﬁrst we are presented with d1 resulting in the posterior
Pr(Θ|m, d1). This posterior acts as the prior for the next update when
d2 is presented, resulting in the new posterior Pr(Θ|m, d1, d2), and so
on. In the end, this will give us the same posterior as Pr(Θ|m, d); facts
have accumulated over time.
For complete data the ordering of the
records is unimportant.
Although not necessarily a requirement, for computational conve-
nience it is advantageous if the prior and posterior distribution have
the same functional form with a simple update rule. Such a distribution
is called a conjugate prior. From a theoretical point of view conjugacy
may seem as a severe constraint, but unfortunately it is a necessary one
from a practical point of view.
Previously in Chapter 2, Section 5.1 the notion of global parameter in-
dependence was introduced as the decomposition given in eq. 2.4, that is,
the assumption that the conditional probabilities for each child variable
Xi can be speciﬁed or learned separately from each other. If in addi-
tion to global independence we also have local parameter independence
(Spiegelhalter and Lauritzen, 1990), that is:
Pr(Θi|m) =

xpa(i)
Pr(ΘXi|xpa(i)|m)
when combined with global independence, the overall parameter distri-
bution becomes:
Pr(Θ|m) =
p

i=1
Pr(Θi|m) =
p

i=1

xpa(i)
Pr(ΘXi|xpa(i)|m)
which states that each parameter distribution conditional on a parent
conﬁguration, can be updated independently of each other.
Similar to the requirement of conjugacy, assuming parameter inde-
pendence is from a computational point of view the only feasible ap-
proach.
However, it is not an entirely unrealistic assumption.
From
a domain perspective it makes sense that parameter speciﬁcation can
be constrained to particular “regions” of a BN. Perhaps the most ques-
tionable assumption is therefore that of local parameter independence.
After all these parameters pertain to the same child/parent vertices,
and consequently one may argue that it is unreasonable that they can
be determined independently of each other.

Learning Bayesian Networks from Data
23
Since we are dealing with a product multinomial distribution, it fol-
lows from eq. 3.1 that the required likelihood simply becomes a product
of multinomial likelihoods. In fact, the decomposition per vertex and
parent set conﬁguration coincides with the decomposition of the overall
parameter distribution of Θ under global and local parameter indepen-
dence. The conjugate prior for the multinomial is the Dirichlet distri-
bution. Hence we deﬁne a Dirichlet distribution for each multinomial:
Pr(ΘXi|xpa(i)|m) = Dir(ΘXi|xpa(i)|α) =

xi
Θ
α(xi,xpa(i))−1
xi|xpa(i)
· Z(xpa(i), α)
where α > 0 is the vector of prior hyper parameters of the Dirichlet.
The normalising factor is:
Z(xpa(i), α) =
1
 	
xi θ
α(xi,xpa(i))−1
xi|xpa(i)
dθXi|xpa(i)
= Γ(
xi α(xi, xpa(i)))
	
xi Γ(α(xi, xpa(i)))
(3.7)
Here Γ(·) is the gamma function. Note that the normalising factor is a
function of the parent set of Xi and the hyper parameter α.
Applying Bayes’ rule from eq. 3.6, it is easy to see that the posterior
is a product Dirichlet:
Pr(Θ|m, d)
∝
p

i=1

xpa(i)

xi
Θ
n(xi,xpa(i))
xi|xpa(i)
·
p

i=1

xpa(i)

xi
Θ
α(xi,xpa(i))−1
xi|xpa(i)
=
p

i=1

xpa(i)

xi
Θ
α(xi,xpa(i))+n(xi,xpa(i))−1
xi|xpa(i)
(3.8)
where the normalising term for the posterior is updated with the suﬃ-
cient statistics n from d:
p

i=1

xpa(i)
Z(xpa(i), α + n) =
p

i=1

xpa(i)
Γ(
xi α(xi, xpa(i)) + n(xi, xpa(i)))
	
xi Γ(α(xi, xpa(i)) + n(xi, xpa(i)))
(3.9)
For sequential updating the same holds; depending on the conﬁguration
dj only those statistics are increased that correspond to the observation
in record j.
The prior hyper parameters, α, are often considered imaginary counts
corresponding to cell counts prior to observing data. However, inter-
preting α(xi, xpa(i)) = 0 as non-informative in the sense that no prior

24
EFFICIENT LEARNING OF BAYESIAN NETWORKS
counts are available does not result in a posterior Dirichlet unless all
n(xi, xpa(i)) > 0.
From an inference point of view assigning a small
value > 0 to α(xi, xpa(i)) may be helpful in making summary statistics
more stable. For instance for large models, relatively many cell counts
are typically zero, in which case adding a small number of prior obser-
vations makes analysis easier.
It is worth noticing that for large samples, the prior counts will have
a minimal impact on the posterior. For small samples, the prior counts
do have a substantial impact, and should be chosen with some care.
To a Bayesian, the posterior distribution is the object of inference,
and in some cases it may even be “the answer” itself. However, from
a practical point of view, summary statistics of the distribution are
often used to get at least a partial impression of the distribution. De-
note α(xi, xpa(i)) + n(xi, xpa(i)) = s(xi, xpa(i)) and 
xi s(xi, xpa(i)) =
s(xpa(i)). The expectation and the variance of the (posterior) Dirichlet
are given by:
E[Θxi|xpa(i)]
=
s(xi, xpa(i))
s(xpa(i))
(3.10)
V[Θxi|xpa(i)]
=
s(xpa(i)) · s(xi, xpa(i)) −s(xi, xpa(i))2
s(xpa(i))3 + s(xpa(i))2
(3.11)
Fixing the parameter expectations, we see that the variance decreases
as s(xpa(i)) increases and the Dirichlet becomes more tight around the
parameter expectations, indicating that the uncertainty decreases.
The multinomial likelihood and the Dirichlet are the same up to a
constant when we subtract one from all the counts in the Dirichlet. The
ML estimator for the multinomial is given in eq. 3.4 as a function of
the counts n(xi, xpa(i)). If we use the counts s(xi, xpa(i)) −1 instead, it
follows that for α(xi, xpa(i)) ≥1 that the mode of the Dirichlet is located
at:
s(xi, xpa(i)) −1
s(xpa(i)) −|ΩXi|
3.
Learning models
The absence of arcs in the DAG m encodes assumptions about inde-
pendences and therefore a complete DAG encodes no assumptions about
independences at all—it is called the saturated model. A saturated model
can capture any distribution for X, because any minor interaction ef-
fects between variables can be modeled by joining variables. The more

Learning Bayesian Networks from Data
25
dense the graph, the more detail can be captured. A model that is too
dense will pick up spurious and noisy relationships in the data at hand.
We seek the model that captures the interactions of any data set gen-
erated by the underlying process of d, not a model that only captures
all detail of d in particular. The term overﬁtting refers to the problem
pertaining to learning models that ﬁt the data in every detail versus
learning models that generalise the data because a form of regularisa-
tion has been applied while learning (Geman et al., 1992). By applying
regularisation during learning, spurious details are smoothed out, and
the impact of noise is reduced.
3.1
The penalised likelihood approach
As we saw, the straight-forward approach to learning parameters via
the likelihood Pr(d|m, θ), is to ﬁx the model, and then use the ML
estimates to actually ﬁt the model. However, using the same approach
for learning the model, it is well-known that this will result in overﬁtting.
The learned model will have no (or very few) conditional independence
restrictions.
In order to counteract overﬁtting, a measure of model complexity is
introduced. The number of free parameters:
κ(m) =
p

i=1
(|ΩXi| −1) · |ΩXpa(i)|
is the number of parameters that need to be estimated for a given model;
for every parent set conﬁguration, all conﬁgurations of the child need
to be determined (except one, because the probability sums to unity).
Hence, for dense DAG models, there are more free parameters.
The
model selected is then:
ˆm = arg max
m log Pr(d|m, ˆθ) + f(m)
Here f(m) is a penalty term that is responsible for regularisation, and
ˆθ is the ML estimate of θ.
If f(m) = −κ(m) then the AIC criterion (Akaike, 1974) is obtained,
derived from decision theory.
Setting f(m) = −κ(m)
2
log c (where c
refers to the number of cases in the data, d) we get the BIC crite-
rion (Schwarz, 1978), based on the large sample approximation from
Bayesian theory, or equivalently, the (negative) MDL criterion based on
coding theory (Gr¨unwald et al., 2005). We refer to Bouckaert, 1995 for a

26
EFFICIENT LEARNING OF BAYESIAN NETWORKS
more thorough treatment of these metrics for learning Bayesian network
models.
For all scores hold that they decompose, i.e., they are a sum (or
product prior to taking the logarithm) of penalised terms, one term for
each vertex. This is easy to see, since both the logarithm of the product
likelihood in eq. 3.1 and f(m) consists of a sum for i = 1, . . . , p, i.e., one
term per vertex.
3.2
The Bayesian approach
A Bayesian does not approach model learning directly from a pe-
nalised likelihood point of view. Moreover, model selection is not even
in line with the Bayesian paradigm.
In the same way that we were
interested in the posterior parameter distribution when learning the pa-
rameter, the entire posterior model distribution Pr(M|d) is of interest
when learning models. In Chapter 4, Section 3 we return to this issue.
For computational reasons or when large amounts of data is available,
a full Bayesian treatment may not always be desireable. In that case a
Bayesian may decide to do model selection instead; normally the MAP
(maximum a posteriori) model from the posterior model distribution
is the selected as the single most probable model. Hence, the model
selected is merely a summary statistic of the entire posterior model dis-
tribution.
As was the case with parameter learning, the posterior model distri-
bution is obtained via Bayes’ law:
Pr(M|d) =
Pr(d|M) · Pr(M)

m Pr(d|m) · Pr(m) ∝Pr(d|M) · Pr(M)
(3.12)
The MAP model is the model for which Pr(M|d) is maximal. In Bayes’
law, we may disregard the normalising term because it is constant for a
ﬁxed data set. It therefore suﬃces to maximise the numerator Pr(d, M)
with respect to M. The distribution Pr(M) can model any constraint
one may have on the independences, directions or otherwise. Angelopou-
los and Cussens, 2001 discuss how this could be done. In the absence
of prior knowledge, a uniform prior can be used, i.e., all models are as-
signed the same prior probability. In that case it suﬃces to to maximise
Pr(d|M) because the prior is the same for all models.
3.2.1
Learning via the marginal likelihood
The term Pr(d|M) is the marginal likelihood not to be confused with
the likelihood function Pr(d|M, Θ). In the marginal likelihood the pa-

Learning Bayesian Networks from Data
27
rameter is integrated out with respect to the prior parameter distribu-
tion:
Pr(d|M) =

Pr(d|M, θ) · Pr(θ|M)dθ
(3.13)
Hence, the marginal likelihood is the expectation of the likelihood with
respect to the prior. Note that the marginal likelihood coincides with
the normalising term in eq. 3.5. By rearranging and isolating the de-
nominator we get:
Pr(d|M)
=
Pr(d|M, Θ) · Pr(Θ|M)
Pr(Θ|d, M)
=
p

i=1

xpa(i)

xi
Θ
n(xi,xpa(i))+α(xi,xpa(i))−1
xi|xpa(i)
· Z(xpa(i), α)
p

i=1

xpa(i)

xi
Θ
n(xi,xpa(i))+α(xi,xpa(i))−1
xi|xpa(i)
· Z(xpa(i), α + n)
=
p

i=1

xpa(i)

xi
Z(xpa(i), α)
Z(xpa(i), α + n)
(3.14)
The equation reduces to the product of the ratios of the normalising
factors of the prior Dirichlet and the posterior Dirichlet. Using eq. 3.7
it directly follows that we may write eq. 3.14 as:
p

i=1

xpa(i)
Γ

α(xpa(i))

Γ

α(xpa(i)) + n(xpa(i))


xi
Γ

α(xi, xpa(i)) + n(xi, xpa(i))

Γ

α(xi, xpa(i))

This formula gives the probability of the data under model M. The for-
mula was derived from a somewhat diﬀerent point of view in Cooper and
Herskovits, 1992; Heckerman et al., 1995 where the marginal likelihood
scoring metric was referred to as the Bayesian Dirichlet scoring metric
(BD-metric).
The marginal likelihood in eq. 3.13 is quite diﬀerent from the penalised
likelihood scoring approach, where the parameter isn’t integrated out,
but is taken to be the ML estimate. The marginal likelihood takes the
entire range of possible parameter assignments into consideration by ex-
plicitly weighing according to the parameter distribution. In contrast
to the penalised likelihood approach, there is no explicit penalty term
in the Bayesian approach; implicitly overﬁtting is still taken care of.

28
EFFICIENT LEARNING OF BAYESIAN NETWORKS
In Section 3.2.2 and Section 3.3 we return to the issue, but here we
present an intuitive explanation for why this is: ΩΘ contains all possi-
ble parameters, and via Pr(Θ|M) each parameter-“point” is assigned a
density. The expectation of the likelihood as given in eq. 3.13 reduces
to a product of expectation-terms, E[Θ
n(xi,xpa(i))
xi|xpa(i)
]. This expectation ef-
fectively smoothes out the impact of extreme values by averaging over
all “points”, such that no single value will have the ultimate say like
in the ML approach; metaphorically speaking, all “potential parameter
values in ΩΘ compete”. This “competition” is more pronounced when
the volume of ΩΘ is large, which indeed is the case for dense DAG
models—there, more parameters need to be determined than for less
dense models. Hence, for large volume ΩΘ and a uniform Pr(Θ|M),
we a priori state there is no single best parameter such as the ML es-
timate ˆθ, but rather there is a large collection of competing values that
all should be considered, and hence averaged over.
Another and perhaps more intuitively attractive way of reading the
marginal likelihood goes via the so-called predictive-sequential approach
also referred to as the prequential interpretation; see for instance Seillier-
Moiseiwitsch et al., 1992; Dawid, 1984.
From the iid assumption, it
follows that the marginal likelihood can be written as a product of con-
ditional probabilities:
Pr(d|M)
=
c

j=1
Pr(dj|d1, . . . , dj−1, M)
(3.15)
=
c

j=1
Pr(dj|θ = E[Θ|d1, . . . , dj−1], M)
=
c

j=1
p

i=1
nj(xj
i, xj
pa(i)) + α(xj
i, xj
pa(i))
nj(xj
pa(i)) + α(xj
pa(i))
(3.16)
where nj(·) is the function returning the counts from d until record
j.
In terms of n(·), it corresponds to the counts extracted from the
contingency tables based on the data sample (d1, . . . , dj).
In eq. 3.15, each term is the probability of an instantiation of X con-
ditional on previous observations. From eq. 2.5 we know that each term
reduces to a product of expectations of the parameters with respect to
the posterior Dirichlet, where the posterior is conditional on all records
“seen thus far”. Hence, by plugging in the expectation of the Dirichlet
given in eq. 3.10, eq. 3.16 is obtained. Notice that the decomposition

Learning Bayesian Networks from Data
29
given in eq. 3.15 resembles a form of sequential cross-validation, i.e., use
what has been seen thus far to test how well a particular model can
predict the next observation. Cross-validation is a well-know technique
for avoiding overﬁtting in a model selection context.
The marginal likelihood admits to a recursive factorisation according
to M in the same way that the BN does, viz. eq. 2.1:
Pr(d|M) =
p

i=1

xpa(i)

xi
Z(xpa(i), α)
Z(xpa(i), α + n) =
p

i=1
Pr(di|dpa(i), M) (3.17)
where di denotes the column of d referring to Xi. Each term in this
product is a predictive measure: given the c realisations (from the data
sample) for the parent variables, what is the probability of the c reali-
sations for the child variable? Model selection says that we should per
variable choose the parent set for which this prediction is best, because
this is the best model from a prediction point of view.
The decomposition in eq. 2.1 is perhaps not surprising because the
Dirichlet distribution is deﬁned for each multinomial contingency table
(due to local and global parameter independence), yet it is an impor-
tant property when actually learning models. Recall that the penalised
likelihood scoring criteria in Section 3.1 also are decomposable.
The marginal likelihood plays a signiﬁcant role, not only when learn-
ing models, but as we will see in Chapter 5, also when dealing with
incomplete data.
3.2.2
Determining the hyper parameter
The product Dirichlet is parameterised by the hyper parameter α.
The hyper parameter captures the prior belief, before observing data.
In the context of model learning, it turns out that the hyper parameter
primarily is responsible for the degree of regularisation imposed.
For learning parameters, the hyper parameter can be speciﬁed in ad-
vance given a particular DAG model. For learning models rather than
parameters, we would have to specify the hyper parameter for every pos-
sible DAG model. This is obviously infeasible to do by a human expert,
for one because of the large number of diﬀerent models, and also because
from a domain perspective many models make no sense to the expert.
Rather we would like to derive priors “automatically” for an arbitrary
DAG model given that we have speciﬁed a probable DAG model m′ and
corresponding parameter θm′ (thus a full BN) that we think captures
the prior quantitative knowledge.

30
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Along with such a reference BN, an equivalent sample size (ESS)
should be given, then α(xi, xpa(i)) is deﬁned by (with pa(i) being the
indices of the parents in m):
α(xi, xpa(i)) = ESS · Pr(xi, xpa(i)|m′, θm′)
(3.18)
As the name suggests, the ESS is the number of prior observations
on which the prior knowledge of the expert is based, i.e., what is the
equivalent to the prior knowledge in terms of prior observations? From
an intuitive point of view this perhaps makes sense, but formalising
this relationship or mapping is pretty much impossible, and therefore
remains rather vague. The name equivalent sample size and the corre-
sponding interpretation is however rather deceptive, because in practice
the ESS is mainly responsible for the degree of regularisation imposed
when learning models from data (Steck and Jaakkola, 2002).
Proposition 3.1 The ESS determines the overall degree of regulari-
sation imposed when learning, and is distributed via Pr(X|m′, θm′) over
diﬀerent parts of M.
To see why this is, note that the prior distribution Pr(Θ|M) ≡
Pr(Θ|M, α) may be regarded as a function of α. In section 3.2.1 we
already established that when the volume of ΩΘ increases, Pr(Θ|M)
assigns a small density to each “point” in ΩΘ. However, for decreasing
α, Pr(Θ|M) will also assign a smaller density to each “point” in ΩΘ,
since the normalising factor given in eq. 3.7 decreases when α goes to
zero.
The ESS is distributed over all conﬁgurations xi, xpa(i) via eq. 3.18.
For a complex DAG model M there are many conﬁgurations (large par-
ent sets), and each α(xi, xpa(i)) is small because they share the ESS.
This means that the normalising factor will be small, hence less complex
DAG models are preferred. Thus, a large ESS implies weak regularisa-
tion, and a small ESS implies strong regularisation.
From the above discussion it follows that Pr(X|m′, θm′) really dis-
tributes the “amount of regularisation” over diﬀerent parts of M, i.e.,
the degree of regularisation for the vertices of M. In this respect it may
be very diﬃcult to specify such a BN in advance (even though only a
single BN needs to be speciﬁed), because the notion of “distributing the
regularisation” is very vague. In particular if we expect an expert to
be able to specify such a BN, she will probably not be able to do so let
alone grasp the very notion of regularisation.

Learning Bayesian Networks from Data
31
In the literature it has been proposed to choose the prior hyper pa-
rameters according to the following metrics and methods:
The Bayesian Dirichlet equivalent (BDe) is the method just de-
scribed, where an ESS is chosen, and a distribution is deﬁned that
assigns α(xi, xpa(i)) to each Dirichlet.
The BDe-metric is score equivalent which means that equivalent DAG
models have the same marginal likelihood (Heckerman et al., 1995;
Buntine, 1991); the reason is that the prior counts are “consistent”
because they are linked to each other through the distribution deﬁned
in terms of the BN, (m′, θm′).
The Bayesian Dirichlet equivalent uniform (BDeu) is the same as
the BDe-metric, but m′ is the empty graph, and θm′ consists of
parameters:
Pr(Xi|m′, θm′) =
1
|ΩXi|
Consequently the joint over child and parents amounts to:
Pr(Xi, Xpa(i)|m′, θm′) =
1
|ΩXi|

Xj∈Xpa(i)
1
|ΩXj|
This metric is used when no prior information is available, or one is
unable to specify in any detail the BN (m′, θm′); only the ESS needs
to be chosen to “tune” the overall degree of regularisation. The BDeu
yields a uniform distribution of the ESS over Xi, Xpa(i).
Because the BDe-metric is score equivalent, so is the BDeu-metric.
The K2-metric simply says that all α(xi, xpa(i)) = 1.
The K2-metric (Cooper and Herskovits, 1992) is not score equiva-
lent, i.e., models that are equivalent do not have the same marginal
likelihood; the prior counts are in no way related to each other. In
general this is an undesirable property when learning models.
The K2-metric was originally used for learning with the K2-algorithm,
presented in Cooper and Herskovits, 1992. This algorithm assumes that
an ordering of the vertices is given a priori, and therefore score equiva-
lence was not crucial. The BDeu-metric with an ESS of 1 is probably
the most widely used metric in learning algorithms that are based on
the marginal likelihood scoring criterion.

32
EFFICIENT LEARNING OF BAYESIAN NETWORKS
3.3
Marginal and penalised likelihood
The marginal likelihood is equivalent to the BIC/MDL penalised like-
lihood score, for an unlimited amount of data (Chickering and Hecker-
man, 1997; Bouckaert, 1995). For a ﬁnite amount of data, this does not
hold, but for model selection, it is still possible to cast the Bayesian
marginal likelihood in a functional form similar to the penalised likeli-
hood scoring criteria, i.e., as the log-likelihood plus a penalty term. The
functional similarity between all scores means that they can be used in-
terchangeably in diﬀerent theoretical contexts; the main diﬀerence lies
in the regularisation term used.
The joint distribution over BNs may be written as:
Pr(M, Θ|d) = Pr(Θ|d, M) Pr(M|d)
(3.19)
When learning BNs, we seek the MAP of that joint distribution, i.e., the
most probable BN given data. The decomposition in eq. 3.19 means that
we may ﬁrst select the model M for which Pr(M|d) ∝Pr(d|M) Pr(M)
is maximal using the marginal likelihood, after which we may compute
the MAP of Pr(Θ|d, M) conditional on the model just selected.
Theorem 3.1 For model selection, the marginal likelihood can be trans-
formed into an expression that has the functional form of a penalised
(log-)likelihood.
To obtain the MAP, we may reduce eq. 3.19 in the following way:
Pr(M, Θ|d)
∝
Pr(Θ|d, M) Pr(d|M) Pr(M)
=
Pr(Θ|d, M)Pr(d|M, Θ) Pr(Θ|M)
Pr(Θ|d, M)
Pr(M)
=
Pr(d|M, Θ) Pr(Θ|M) Pr(M)
By ﬁlling in Pr(d|M, Θ) and Pr(Θ|M) and slightly rewriting, we get:
p

i=1

xpa(i)

xi
Θ
n(xi,xpa(i))
xi|xpa(i)
p

i=1

xpa(i)

xi
Θ
α(xi,xpa(i))−1
xi|xpa(i)
Z(xpa(i), α) Pr(M)
=
p

i=1

xpa(i)

xi
Θ
n(xi,xpa(i))+α(xi,xpa(i))−1
xi|xpa(i)



likelihood
p

i=1

xpa(i)
Z(xpa(i), α) Pr(M)



penalty
The likelihood in this expression is that of the data plus the prior ob-
servations (minus 1), i.e., Pr(d, α −1|M, Θ). It is worth observing that

Learning Bayesian Networks from Data
33
the penalty term is a combination of the prior model distribution Pr(M)
and the normalising factor of the prior product Dirichlet distribution,
which in turn is also dependent on M; we reduce this to the penalty term
f(M, α). If we leave Pr(M) out of the equation, we have that similar
to BIC/MDL and the AIC scores, the rewrite of the marginal likelihood
remains decomposable on a per vertex basis. If Pr(M) is included, then
this prior needs to decompose as well.
We know the closed form solution for the parameter that maximises
the likelihood based on (d, α−1), namely the ML estimator ˆθ (equivalent
to the MAP given d), and since the penalty term does not depend on the
choice of parameter, this solution also maximises Pr(M, Θ|d). Hence,
we have that:
ˆm = arg max
m log Pr(d, α −1|m, ˆθ) + log f(m, α) = arg max
m log Pr(m|d)
Note that for a ﬁxed model, increasing hyper parameters increases
the score. This is in line with the comments made about regularisation
in Sections 3.2.1 and 3.2.2.
It is interesting to observe that the role of the prior hyper parameters
in fact is twofold: they act as prior counts by adding to d the number
of instances observed “in the past” and they determine the degree of
regularisation.
For large samples, we have for the likelihood term that Pr(d, α −
1|M, Θ) ≈Pr(d|M, Θ), since α will have an insigniﬁcant inﬂuence. This
means that the main role of α is to determine the degree of regularisation
(again leaving Pr(M) out of the picture). That the prior counts have
this eﬀect is rather surprising, especially when considering the semantics
of the counts in the Bayesian parameter learning context. There, α is
treated in the same way as the suﬃcient statistics from d; they are
simply added together, and they are essentially treated as a batch. For
model learning on the other hand, this “adding together” only plays
negligible role; here α mainly determines the regularisation behavior.
3.4
Search methodologies
For parameter learning, both the Maximum-Likelihood approach as
well as the Bayesian approach provide optimal solutions that can be
found as closed functions of the suﬃcient statistics; no search procedure
needs to be employed in order to establish the parameter estimates. For
model selection, things are quite diﬀerent.
Given a suitable search space, a search strategy and and some scoring
metric, we are able to implement methods that perform model selection.

34
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Hence, the “optimal” model is searched for by traversing the search
space, and the scoring metric (which to choose depends on the desired
type and degree of regularisation) is the measure that determines the
degree of optimality.
It is the search strategy that speciﬁes how the
space is traversed.
3.4.1
Model search space
The search space deﬁnes the space of models that encode conditional
independence statements. As previously discussed in Chapter 3, Section
3.2, several DAGs encode the same conditional independence assump-
tions, meaning that the scoring metrics (except the marginal likelihood
together with the K2-metric) for these DAGs are the same. This sug-
gests that a search space with DAGs is larger than strictly necessary.
The canonical representation of essential graphs would form a more suit-
able search space. However, the scoring metrics are deﬁned on DAG
models, and the computational burden of converting between DAG and
essential graphs can be substantial; we note that recently (Chickering,
2002a) improvements have been made in this direction.
In Castelo and Kocka, 2003; Chickering, 2002a the impact of choosing
the essential graph space instead of the DAG space is discussed. In that
respect, the ratio of number of DAGs over the number of essential graphs
is important, and is reported to be about 4 (Gillispie and Perlman,
2001; Kocka, 2001). This means that the average number of equivalent
DAG models per essential graph is about 4, i.e., for a set of conditional
independence statements encoded via an essential graph, there are on
average 4 DAG models that can encode the same set of conditional
independences.
Moreover, for dense essential graphs, there are more
equivalent DAGs, but the number of equivalent DAGs decreases rapidly
as the essential graphs become less dense.
The covered arc reversal discussed in Chapter 2, section 3.2 provides a
mechanism for moving between equivalent DAG models; by performing
covered arc reversals, all (on average) 4 equivalent DAGs are visited.
Hence a DAG search space can from an operational point of view sim-
ulate an essential graph space via a sub-process performing repeated
covered arc reversals as the search strategy is employed.
The search
space remains the space of DAGs, but via the repeated covered arc re-
versals the search space eﬀectively resembles the smaller essential graph
space.
The covered arc reversal process is implemented in a non-deterministic
way: For every DAG model, a covered arc is picked at random, and is

Learning Bayesian Networks from Data
35
reversed. After the reversal an equivalent DAG model is obtained, and
once again a covered arc is reversed, etc. After a number of covered arc
reversals, all DAGs in the equivalence class have been visited. Because
the average number of equivalent DAGs is about 4, the number of cov-
ered arc reversals may be kept relatively small. For dense DAG models
a larger number of reversals is preferred because more equivalent DAGs
exist.
3.4.2
Traversal strategy
The search strategy is intimately linked to the basic transformational
operations one can perform on a DAG with limited computational costs,
including the costs of computing the score after such a transformation.
From a computational point of view it is advantageous to move around
the DAG space in small steps. This corresponds to only modifying the
current DAG model (corresponding to some “location” in the search
space), by an arc transformation between two vertices Xi and Xj:
Arc addition: Change Xi ̸−Xj to Xi →Xj.
Arc removal: Change Xi →Xj to Xi ̸−Xj.
Arc reversal: Change Xi →Xj to Xi ←Xj.
Of these 3 operations, arc addition and arc reversal are non-local because
they may introduce a cycle. Hence, more vertices need to be considered
than Xi and Xj alone to check if these transformations are legal. A
simple depth-ﬁrst search reveals if a cycle is introduced.
From a scoring perspective, all 3 arc operations are local to Xi and
Xj, because the metrics enjoy the property of decomposition. For two
DAG models, only those terms in the marginal likelihood that pertain
to vertices that have diﬀerent parent sets are diﬀerent. Hence, adding
(removing) an arc to Xj means that all terms in the marginal likelihood
remain unchanged, except the term pertaining to Xj, which has to be
re-computed because its parent set changes. For reversal the two terms
pertaining to Xi and Xj need to be re-computed.
Diﬀerent learning algorithms have been proposed for learning DAG
models, both deterministic and non-deterministic methods, see for in-
stance Pelikan et al., 1999; Larra˜naga et al., 1996; Heckerman, 1998;
Buntine, 1991. Although the 3 elementary arc operations computation-
ally provide an eﬃcient way of moving around in the search space, these
basic operations may not be the best or most logical choice for a given

36
EFFICIENT LEARNING OF BAYESIAN NETWORKS
learning algorithm.
Moreover, the elementary arc operations do not
necessarily provide the best decomposition of the DAG model for reach-
ing the highest scoring model, especially if the search space is severely
spiked or otherwise non-smooth. In order to avoid getting stuck in local
optima, the 3 basic operations may not be “aggressive” enough to leave
such an area.
A learning method that theoretically speaking only requires arc ad-
dition and arc removal to do optimal model selection, is based on the
so-called inclusion-driven approach, in which the search space is tra-
versed in a way that respects the inclusion boundary.
The inclusion
boundary of a model from the essential graph space, consists of all es-
sential graphs that have one edge more or one edge less than the current
model. The inclusion boundary translates to the DAG model space via
the covered arc reversals simulating the essential graph space. Given
an inﬁnite amount of data, the scoring metrics will always increase if
addition or removal of an arc of a DAG model is “the right thing to do”
with respect to the underlying distribution (the score is consistent), but
will remain unchanged for equivalent DAG models. This means that we
are guaranteed to reach the best model, if at every stage we move (in
terms of a single arc addition or arc removal), to a model that is better
than the current model. Several model selection algorithms have been
developed that respect the inclusion boundary, such as GES by Chicker-
ing, 2002a; Chickering, 2002b, KES by Nielsen et al., 2003 and HCMC
by Kocka and Castelo, 2001; Castelo and Kocka, 2003.
Unfortunately, for a ﬁnite data sample the optimality is not guaran-
teed, and it is no longer entirely clear whether adhering to an inclusion-
driven learning approach is a big advantage over other learning ap-
proaches that also employ covered arc reversal to simulate the space
of essential graphs. It might very well be that the performance of exist-
ing algorithms, that do not explicitly obey the inclusion boundary, can
be improved by searching in the simulated essential graph space instead
of the DAG space. This may not necessarily be because covered arc
reversals also yield a smaller search space compared to the DAG search
space, but because the covered arc reversals in fact make the search
space smoother when we restrict the search strategy to one that only
employs the 3 elementary arc operations.

Chapter 4
MONTE CARLO METHODS AND MCMC
SIMULATION
In this chapter we treat the theory required for learning about pos-
terior distributions, and we lay the foundation needed in order to learn
BNs from incomplete data.
We also develop a Markov chain Monte
Carlo sampler called MB-MCMC for obtaining DAG models from the
posterior model distribution given complete data.
Monte Carlo methods is a broad topic, and in this chapter we only
present the essentials. See for instance Robert and Casella, 2002; Gilks
et al., 1996 for a thorough treatment of diﬀerent Monte Carlo tech-
niques, including Markov chain Monte Carlo simulation and practical
applications.
1.
Monte Carlo methods
We investigate the generic problem of computing the following sum-
mation (replace sums with integrals for continuous variables):
EPr[h(X)] =

x
h(x) Pr(x)
(4.1)
In particular, h(X) can be a probability distribution, for instance the
conditional distribution Pr(Y |X), in which case the expectation coin-
cides with the distribution Pr(Y ).
If eq. 4.1 is diﬃcult to solve analytically or is infeasible to compute
because of an extremely large cardinality of ΩX, it can be approximated.
Sample X(1), . . . , X(n) from Pr(X), i.e., X(t) ∼Pr(X), and compute

38
EFFICIENT LEARNING OF BAYESIAN NETWORKS
the empirical average of eq. 4.1:
EPr[h(X)] ≈1
n
n

t=1
h(x(t))
(4.2)
The Law of Large Numbers guarantees that the approximation converges
towards the expectation in eq. 4.1 as n →∞.
For one reason or another it might be the case that sampling from
Pr(X) is undesirable. For instance, it may be computationally expensive
to produce realisations, or the distribution may be such, that it can be
evaluated only up to a normalising term, making it diﬃcult or impossible
to sample from. Many computational problems encountered in Bayesian
statistics in fact boil down to not being able to determine the normalising
term, as we will see in section 2.
This means that even solving the
approximation of eq. 4.1 in terms of eq. 4.2 may be problematic.
In order to tackle these problems, more sophisticated Monte Carlo
techniques may help out. We start oﬀby introducing another Monte
Carlo method called importance sampling, and then discuss Markov
chain Monte Carlo sampling.
1.1
Importance sampling
A slightly more advanced Monte Carlo method than the approxima-
tion in eq. 4.2 is the so-called importance sampler. Suppose that we don’t
sample from Pr(X), but instead we sample from another distribution
Pr′(X), e.g., it may be computationally cheap to do so.
First oﬀ, rewrite the summation in eq. 4.1:

x
h(x) Pr(x)
=
1

x
Pr(x)

x
h(x) Pr(x)
=
1

x
Pr(x)
Pr′(x)Pr′(x)

x
h(x) Pr(x)
Pr′(x)Pr′(x)
=
1
EPr′[ Pr(X)
Pr′(X)]
EPr′[h(X) Pr(X)
Pr′(X)]
(4.3)
Sample X(1), . . . , X(n) from Pr′(X), and the empirical average of eq. 4.3
becomes:
1
n
t=1 wt
·
n

t=1
wt · h(x(t))
(4.4)

Monte Carlo Methods and MCMC Simulation
39
Figure 4.1.
The dotted line is Pr′(X), and the solid line is Pr(X).
with weights wt:
wt = Pr(x(t))
Pr′(x(t))
(4.5)
When Pr′(X) > 0 whenever Pr(X) > 0, it holds that as n increases
the approximation becomes more accurate. An essential observation is
that the constant normalising term of Pr(X) cancels out because the
weights as given in eq. 4.5 are normalised in eq. 4.4. This implies that
we need only be able to evaluate Pr(X) up to this normalising constant.
In fact, the normalising term of Pr′(X) is eliminated as well.
Theoretically speaking, importance sampling puts very little restric-
tion on the choice of sampling distribution; in particular, any strictly
positive sampling distribution can be used. When using a uniform sam-
pling distribution, the denominator of wt is the same for all weights t,
and are eliminated by normalisation. Also note that when Pr′(X) and
Pr(X) are proportional, the sampler reduces to the empirical average in
eq. 4.2.
The weights wt in eq. 4.5 are called importance weights and compen-
sate for the diﬀerence between the real distribution and the sampling
distribution. Intuitively, for the standard empirical average in eq. 4.2,
all samples drawn from Pr(X) have the same weight in the approxima-
tion. For importance sampling, any sample from Pr′(X) is, compared
to Pr(X), either “over-sampled” (too frequently sampled) and receives
a weight less than 1, or “under-sampled” (too infrequently sampled) and
receives a weight larger than 1. Figure 4.1 illustrates the principle be-
hind importance sampling. The dotted line is the sampling distribution,
and the solid line is from the target distribution, from which we can’t
sample. Samples drawn at locations where the dotted line lies above the

40
EFFICIENT LEARNING OF BAYESIAN NETWORKS
solid plot, will be drawn more often than necessary, and vice versa. To
correct for that mismatch, the importance weights are required.
1.1.1
Choice of the sampling distribution
Although for large n the importance sampling approximation will be
good, the sampling distribution has a major impact on the performance
of importance sampling. In fact, choosing an inappropriate sampling
distribution can have disastrous eﬀects (see for instance Geweke, 1989).
The rewrite of the expectation in terms of Pr′(X) results in the variance:
VarPr′[h(X) Pr(X)
Pr′(X)]
=
EPr′[h(X)2 Pr(X)2
Pr′(X)2 ] −EPr′[h(X) Pr(X)
Pr′(X)]2
=

x
h(x)2 Pr(x)2
Pr′(x)2 Pr′(x) −EPr[h(X)]2
=

x
h(x)2 Pr(x)
Pr′(x) Pr(x) −EPr[h(X)]2
=
EPr[h(X)2 Pr(X)
Pr′(X)] −EPr[h(X)]2
=

x
h(x)2 Pr(x)2
Pr′(x) −

 
x
h(X) Pr(X)
2
(4.6)
The second term in eq. 4.6 is independent of Pr′(X), so our choice
of Pr′(·) only aﬀects the ﬁrst term. Assuming that we want to be able
to use a wide range of functions h(X) that we don’t know a priori, we
restrict attention to the eﬀect that the ratio Pr(X)2/ Pr′(X) has on the
variance in the ﬁrst term. When this fraction is unbounded, the variance
for many functions is inﬁnite. This leads to general instability and slows
convergence. Notice that the ratio becomes extremely large in the tails
when Pr(X) is larger than Pr′(X) in that region. A bounded ratio is
the best choice, and in particular, in the tails Pr′(X) should dominate
Pr(X).
Suppose a single x(t) is drawn from Pr′(X) from an area of very low
probability (density), and Pr(x(t)) ≫Pr′(x(t)). Such a sample can have
a major impact on the empirical average via importance sampling. The
sample is assigned far too much importance compared to the remaining
samples because the ratio Pr(x(t))/ Pr′(x(t)) is very large. Now suppose
that Pr′(X) is a reasonable approximation of Pr(X) almost everywhere
except in a few areas, where the importance weights are oﬀ-scale. Even
though the majority of samples contribute to a reasonable approximation

Monte Carlo Methods and MCMC Simulation
41
of the expectation, as soon as a sample is obtained from “a bad area”,
the approximation seriously deteriorates because the importance weight
is so much larger compared to the importance weights associated with
the samples from the “good areas”. In such a case, it may be better to
discard such a sample entirely. This should be done with some caution
though. Deletion will generally not introduce bias if the large weight is
due to a very small denominator (compared to the denominator of the
other weights). If it turns out that the deletion of a large-weight proposal
results in a more sensible mass distribution over the remaining sample
proposals, then this indeed does indicate that the sample just deleted
was “an accidental outlier” and can be deleted without problem. On the
other hand, if the numerator in a large-weight sample is large (compared
to the numerator of the other weights), one may want to keep such a
sample, since it then comes from a “region of relatively high impact”
on the empirical approximation.
In Hesterberg, 1995 it is suggested
to use a mixture distribution as proposal distribution to overcome the
problem pertaining to large importance weights.
Each component of
Pr′(X) is Pr′
i(X) with weight vi. From an operational point of view,
drawing X(t) from Pr′(X) means that with probability vi we draw from
Pr′
i(X). In calculating the importance weights however, we use Pr′(x(t))
rather than Pr′
i(x(t)). This way we may deﬁne distributions Pr′
i(X) that
cover diﬀerent areas of Pr(X), such that the importance weights remain
bounded.
A general recommendation is to monitor the running variance of the
importance weights because it gives a good indication of the mutual
diﬀerences between the importance weights of the proposals sampled
from Pr′(X).
When the variance suddenly jumps or is instable, one
should at least investigate if the sampling distribution is appropriate,
or if deletion of a few samples may correct the problem. Large variance
may indicate that the performance of the sampler is poor and that the
approximation will be sub-optimal.
2.
Markov chain Monte Carlo—MCMC
Importance sampling oﬀers a way of approximating E[h(X)] with re-
spect to a distribution Pr(X) only known up to the normalising term,
via some sampling distribution Pr′(X).
Markov chain Monte Carlo
(MCMC) is a whole range of methods that provide an alternative way of
solving the generic problem of approximating the expectation in eq. 4.1.
Importance sampling is a so-called direct sampling method where at
least partial knowledge of the distribution from which samples are re-

42
EFFICIENT LEARNING OF BAYESIAN NETWORKS
quired is needed to get a reasonable approximation. These methods are
by nature non-iterative, and they do not adapt to the target distribu-
tion. MCMC on the other hand is a more ﬂexible and iterative method
for handling very awkward distributions. See Neal, 1993 and Andrieu
et al., 2003 for a walkthrough of basic MCMC theory.
The problems that we want to solve are usually cast in a Bayesian
framework, and we will therefore slightly reformulate the original prob-
lem. Consider Bayes’ law:
Pr(X|Y ) =
Pr(Y |X) Pr(X)

x Pr(Y |x) Pr(x)
(4.7)
The denominator Pr(Y ) = 
x Pr(Y |x) Pr(x) is responsible for normal-
isation. Hence, if the numerator can be computed, then sampling from
Pr(X|Y ), and for instance approximating E[h(X)] with respect to that
distribution, can be solved via importance sampling or MCMC. In gen-
eral h(X) may be any function deﬁned on ΩX. Henceforth we leave the
function h(X) out of the picture, and address the main problem, namely
sampling from Pr(X|Y ). Sometimes we skip the conditional, and the
problem is thus how to sample from Pr(X), when this distribution is
known up to a normalising term.
2.1
Markov chains
MCMC is based on the construction of a Markov chain, where the
X(t)’s are produced sequentially, beginning from X(0), such that X(t+1)
depends on X(t) only, but is independent of X(t−1), X(t−2), . . . , X(0) (a
so-called one-step memory sequence):
X(t+1)⊥⊥X(0), . . . , X(t−1)|X(t)
The chain is constructed via transition probabilities T(X(t+1)|X(t))
corresponding to a conditional distribution for X(t+1) given X(t), and
an initial distribution Pr0(X(0)). The distribution Prt+1(X(t+1)) is then
deﬁned in terms of X(t) via the transition:
Prt+1(X(t+1)) =

x(t)
T(X(t+1)|x(t))Prt(x(t))
When the transition probabilities as deﬁned here do not depend on t,
the Markov chain is called (time) homogeneous.

Monte Carlo Methods and MCMC Simulation
43
2.1.1
The invariant target distribution
An invariant distribution Pr∗(X) for a Markov chain, is one that
persists forever once reached:
Pr∗(X) =

x′
T(X|x′)Pr∗(x′)
(4.8)
The invariant distribution, is the target distribution, i.e., the distribution
from which we want to draw samples. An invariant distribution exists
if the Markov chain satisﬁes the detailed balance condition. Detailed
balance states that the transition probability is symmetric between X
and X′, when they are sampled from a distribution Pr(X). Formally,
it should hold that:
T(X′|X) Pr(X) = T(X|X′) Pr(X′)
(4.9)
In fact, this means that Pr(X) = Pr∗(X), i.e., the distribution must
necessarily be an invariant distribution viz. eq. 4.8:

x′
T(X|x′) Pr(x′)
=

x′
T(x′|X) Pr(X)
=
Pr(X)
We stress that detailed balance is a suﬃcient but not necessary condition
for ensuring invariance.
2.1.2
Reaching the invariant distribution
To guarantee that the invariant distribution is reached, the Markov
chain needs to be irreducible and aperiodic (Tierney, 1994). The chain
is irreducible if starting from X(0) every region of the space from which
we need to sample can eventually be reached with a positive probability.
In ﬁgure 4.2, left, this is illustrated. The outline of the whole ﬁgure
illustrates the actual state space, and the gray area corresponds to the
area explored by an irreducible Markov chain. No matter where X(0) is
located, all states communicate and can be reached; no region is isolated
and the chain is free to move to other states via other intermediate states.
To the right, the chain is reducible. Again, the whole ﬁgure illustrates
the state space from which samples are required. The two gray areas
are not connected, indicating that only the region from which the initial
X(0) is located is explored. The whole state space from which we wish to
sample is not covered because there is no way to reach the other region.

44
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Figure 4.2.
State spaces explored by the Markov chain. Left: irreducible. Right:
reducible.
Aperiodicity guarantees that the chain does not get trapped in cycles,
preventing the chain from getting the opportunity to reach other areas.
It is quite easy to make sure that this does not happen; as long as there
is a strictly positive probability of remaining in the current state, i.e.,
the probability of X(t) and X(t+1) having the same value is > 0, the
chain will not get stuck in cycles.
To sum up, if a homogeneous Markov chain. . .
has an invariant distribution, Pr(X),
it is irreducible, and
it is aperiodic,
the Markov chain will in the limit produce realisations from the invariant
distribution, regardless of the starting point X(0):
|Prt+1(X) −Pr(X)| →0 for t →∞
Such a Markov chain is said to be ergodic.
In the next two sections we discuss two well-known MCMC methods
that produce samples from some desired invariant distribution by build-
ing an ergodic Markov chain: the Metropolis-Hastings sampler, and the
Gibbs sampler. Sometimes we say that a MCMC sampler is ergodic, in
which case it refers to the Markov chain that is produced by the sampler.
2.1.3
Metropolis-Hastings sampling
Metropolis-Hastings MCMC (Metropolis et al., 1953; Hastings, 1970)
by construction produces samples from the distribution, Pr(X), which
is the invariant distribution for the Markov chain.
Quite similar to
importance sampling, a proposal distribution, Pr′(Y |X), from which

Monte Carlo Methods and MCMC Simulation
45
we can sample exists. The Metropolis-Hastings algorithm produces a
Markov chain X(0), X(1), . . . in the following way:
1 Draw Y ∼Pr′(Y |X(t))
2 Take
X(t+1) =

y
with probability ρ(x(t), y)
x(t)
with probability 1 −ρ(x(t), y)
where
ρ(X, Y ) = min
 Pr(Y ) Pr′(X|Y )
Pr(X) Pr′(Y |X), 1

The transition probability is the probability of proposing Y times
the probability of accepting this candidate as X(t+1); additionally, it
includes the probability of proposing any state X′ and rejecting it, re-
maining in the current state, Y = X(t). Thus the transition T(Y |X(t))
becomes:
ρ(X(t), Y )Pr′(Y |X(t)) + I(Y = X(t))

x′
(1 −ρ(X(t), x′))Pr′(x′|X(t))
where I(·) is the indicator function. We indeed have that Pr(X) is the
invariant distribution, because detailed balance holds. For the second
term this is easy to see. For the ﬁrst term we distinguish cases. In the
case Pr(Y ) Pr′(X|Y )
Pr(X) Pr′(Y |X) > 1, we have that ρ(X, Y ) = 1, and by applying
eq. 4.9 it follows:
Pr(X)ρ(X, Y )Pr′(Y |X)
=
Pr(X)Pr′(Y |X)
=
Pr(X)Pr′(Y |X)Pr(Y ) Pr(X|Y )
Pr(Y ) Pr(X|Y )
=
Pr(Y )Pr′(X|Y )Pr(X) Pr(Y |X)
Pr(Y ) Pr(X|Y )
=
Pr(Y )ρ(Y , X)Pr′(X|Y )
In case
Pr(Y ) Pr′(X|Y )
Pr(X) Pr′(Y |X) < 1 we have ρ(X, Y ) =
Pr(Y ) Pr′(X|Y )
Pr(X) Pr′(Y |X) and
ρ(Y , X) = 1, and it follows:
Pr(X)ρ(X, Y )Pr′(Y |X)
=
Pr(X)Pr′(Y |X) Pr(Y ) Pr′(X|Y )
Pr(X) Pr′(Y |X)

46
EFFICIENT LEARNING OF BAYESIAN NETWORKS
=
Pr(Y )Pr′(X|Y )
=
Pr(Y )Pr′(X|Y )ρ(Y , X)
Hence, the Markov chain has invariant distribution Pr(X).
The Metropolis-Hastings sampler depends on the ratios Pr(Y )/ Pr(X)
and Pr′(X|Y )/ Pr′(Y |X), implying that the normalising term for both
the proposal distribution and, more importantly, for Pr(·) cancels out.
Irreducibility is guaranteed when Pr′(Y |X) covers the entire sampling
area on which the invariant distribution Pr(X) is deﬁned, since then any
point can be reached via a proposal. Aperiodicity is also guaranteed,
since there is a non-zero probability of remaining in the current state. If
the proposal distribution is strictly positive this means that the invariant
distribution will be reached in the limit. Notice that this is quite similar
to importance sampling, where a suﬃcient condition for convergence was
that the sampling distribution was positive.
A special case of the Metropolis-Hastings MCMC occurs when we have
that Pr′(Y |X) = Pr′(Y ), i.e., the proposal distribution is independent
of the last state like in importance sampling. This leads to:
ρ(X, Y ) = min
Pr(Y ) Pr′(X)
Pr(X) Pr′(Y ), 1

= min
 Pr(Y )/ Pr′(Y )
Pr(X)/ Pr′(X), 1

(4.10)
The numerator and the denominator also arise in importance sampling
as the importance weights.
With importance sampling the samples
are weighted in order to correct for the diﬀerence between Pr(X) and
Pr′(X), whereas for Metropolis-Hastings MCMC, the unweighted sam-
ples can be used, as they provably do come from Pr(X) (at least in the
limit), and the weights are not required in the direct sense. However,
indirectly the weights are still needed for the ratio given in eq. 4.10.
2.1.4
Gibbs sampling
Gibbs sampling (Geman and Geman, 1984) can be seen as special case
of Metropolis-Hastings sampling, where the transition probabilities are
deﬁned in terms of the conditionals of the invariant distribution. Often
we are able to draw from these conditionals, without being able to draw
from the joint invariant distribution.
The Gibbs sampler we present is deﬁned in terms of blocks of vari-
ables. Each block is drawn conditional on the variables not part of the
current block. Our presentation deviates from the customary treatment
of Gibbs sampling. The reason is that the “normal” Gibbs sampler can

Monte Carlo Methods and MCMC Simulation
47
be regarded as a special case of block Gibbs sampling, and that the
added value of blocking is beneﬁcial in many respects (Liu, 1994; Liu
et al., 1994). We illustrate this in Section 2.1.6, and in Section 3.3 we
exploit blocking for learning DAG models.
By construction, the Gibbs sampler needs at least two blocks. We
write X = Y ∪Z, indicating that we partition X into two blocks: block
one, Y and block two, Z. For ease of exposition, we assume that the
blocks are disjoint, but in general they do not need to be. The proposal
only changes block Y :
Y ∼Pr′(Y |Z) = Pr(Y |Z)
where the conditional side Z is unchanged between moves, and uncon-
ditional on the last state of block Y . Again, this is similar to drawing
from the sampling distribution in importance sampling.
The relationship between the proposal distribution and the invariant
distribution is constant through the ratio:
Pr(Z) = Pr(Y , Z)
Pr(Y |Z) = Pr(Y , Z)
Pr′(Y |Z)
Similar to eq. 4.10, the acceptance ratio becomes:
ρ(U, Y ) = min
Pr(Y , Z)/ Pr′(Y |Z)
Pr(U, Z)/ Pr′(U|Z), 1

= 1
yielding an acceptance rate of 1, meaning that all proposals are accepted.
Obviously only sampling Y means that the Markov chain can’t be
irreducible, since the proposal distribution only proposes changes to one
block.
By only sampling in this lower-dimensional space, it immedi-
ately follows that any point not in that dimension will remain ﬁxed—it
can’t be reached. A minimal conditional for ensuring irreducibility is to
propose changes to block Z as well, i.e., sample Z from the conditional.
Formally speaking, this is achieved by combining several Gibbs samplers,
one per block. This amounts to applying transitions in turn, one transi-
tion per block. The chain remains invariant because each separate block
transition leaves the chain invariant. To see why this is, suppose that
we start the sampler from the invariant distribution. Each block is now
sampled from the conditional of the invariant distribution. This transi-
tion leaves the marginal distribution (that coincides with the marginal
of the invariant distribution) of the other blocks intact. For the block
that is sampled, the transition obviously also leaves the chain invariant.

48
EFFICIENT LEARNING OF BAYESIAN NETWORKS
This argument holds for all the blocks and therefore we may conclude
that once the invariant distribution has been reached, a combination of
block transitions leaves the chain invariant.
The Gibbs sampler can be summarized in the following way:
Assign Xi ∈X to some block Bj, j = 1, . . . , k, such that Xi is part
of at least one block.
Let Pr(Bj|Br̸=j) be the conditional invariant
distribution:
B(t+1)
1
∼
Pr(B1|b(t)
2 , . . . , b(t)
k )
B(t+1)
2
∼
Pr(B2|b(t+1)
1
, b(t)
3 , . . . , b(t)
k )
...
B(t+1)
k
∼
Pr(B2|b(t+1)
1
, . . . , b(t+1)
k−1 )
B(t+2)
1
∼
Pr(B1|b(t+1)
2
, . . . , b(t+1)
k
)
...
The realisations of X thus obtained, are coming from the invariant
distribution, Pr(X).
In particular if Xi is assigned to the singleton
set Bi and k = p (number of variables in X), then the Gibbs sampler
reduces to drawing from the so-called full conditionals; each draw is
univariate conditional on X \ {Xi}. This is also referred to as a single-
site Gibbs sampler.
The visitation scheme as suggested above is not crucial for conver-
gence. Random visitation, a systematic sweep or any other combination
is possible. Depending on the problem at hand, one scheme may be
better than the other. As long as all Xi of X are sampled “inﬁnitely”
often, the invariant distribution will be reached.
The Markov chain is also aperiodic, because there is a probability > 0
of remaining in the current state (of a particular block). All dimensions
of the state space are considered by sampling from the corresponding
conditional, providing a minimal condition for irreducibility. Together
with the so-called positivity requirement, this provides a suﬃcient con-
dition for irreducibility.
The positivity requirement says that all the
conditionals must be strictly positive. Hence, not only are all dimen-
sions visited, but all values along those dimensions can be reached as
well.

Monte Carlo Methods and MCMC Simulation
49
We illustrate an instance of Gibbs sampling in the context of sam-
pling from a BN. Suppose we are given the BN (m, θ) representing
the joint distribution Pr(X|m, θ), and that the distribution required
is Pr(Z|m, θ) for only a subset of the variables, Z ⊆X. Since Gibbs
sampling returns realisations from Pr(X|m, θ), any marginal distribu-
tion of any subset can be estimated by way of counting the realisations.
That is, estimate Pr(Z|m, θ) by using the empirical average of the real-
isations of Z, i.e.:
Pr(z|m, θ) ≈1
n
n

t=1
I(z ⊆x(t))
By employing a univariate Gibbs sampler drawing from the full condi-
tionals, the Markov blanket makes the sampling process easy. The full
conditional distribution reduces to Pr(Xj|Xj−1, Xj+1, . . . , Xp, m, θ) =
Pr(Xj|Xmb(j), m, θ) because the Markov blanket shields oﬀall inﬂuence
from variables outside the Markov blanket. Following the BN decompo-
sition, the univariate distribution is:
Pr(Xj|Xmb(j), m, θ) =
θXj|Xpa(j)
	
i∈ch(j) θXi|Xpa(i)

xj θxj|Xpa(j)
	
i∈ch(j) θXi|Xpa(i)
from which each variable in Z is drawn in turn according to the nor-
mal Gibbs procedure. Notice, that any conditional distribution over a
set of variables from the BN can be calculated similarly. In that case
simply ﬁx the variables on the conditional side (the so-called evidence),
and proceed with the Gibbs sampling procedure by updating the other
variables. In that respect, Gibbs sampling is a BN inference method,
albeit an approximate one.
2.1.5
Mixing, burn-in and convergence of MCMC
From a computational point of view, the most important properties
of MCMC samplers pertain to the following:
the mixing of the chain,
the burn-in time, and
the convergence.
It is crucial to understand that MCMC produces correlated samples.
In this regard it may be beneﬁcial to see MCMC as a way of “walking

50
EFFICIENT LEARNING OF BAYESIAN NETWORKS
around” some state space, such that locations with high probability
(density) are passed relatively often. At every step, the current location
is returned, and this corresponds to a draw. MCMC is adaptive in the
sense that it will have a tendency to seek areas of “mass” or “interest”
rather than just walk around aimlessly.
Mixing refers to the long-term correlations between the states of the
chain. It refers to how fast the states “forget” about the previous states,
i.e., how far from an iid sample the state of the chain is. This captures a
notion of how large the “steps” are when traversing the state space. In
general we want consecutive realisations to be as close to iid as possible.
Slow mixing implies long-term drifts or trends. The terms mobility or
acceleration of a chain, refer to the mixing properties.
When starting an ergodic chain at time t = 0 with a realisation of
X(0) not sampled from the invariant distribution, the time it takes to
reach the invariant distribution is referred to as the burn-in, i.e., the
time it takes before samples can be regarded as coming from the target
distribution. After the burn-in, we say that the chain has converged; the
realisations from then on may be considered samples from the invariant
distribution.
The question is how long the chain needs to be before the realisations
are suﬃciently close to the invariant distribution. Obviously, we would
like to reach the invariant distribution with a minimum amount of com-
putational eﬀort usually meaning that we want to keep the “wasteful”
burn-in realisations at a minimum. At the same time we also would like
to be certain that the invariant distribution indeed has been reached.
Prematurely assuming that samples come from the invariant distribution
when in fact they are just from a burn-in is less than useful.
With poor mobility of the chain, the sampler explores the state space
in an ineﬃcient way, which in eﬀect means that the burn-in period in-
creases. Moreover, assuming that the chain indeed has converged, poor
mixing means that more realisations are required in order to gather
enough empirical information to get an impression of the invariant dis-
tribution. In that respect, the mixing properties of a chain is the most
signiﬁcant factor that determines the performance of MCMC.
Unless we can provide an initial X(0) from the invariant distribu-
tion (no burn-in is required at all), diagnosing convergence of MCMC
is necessary for a trustworthy result. Unfortunately this is a non-trivial
problem. In practice diagnosing convergence using an automated pro-
cedure is quite diﬃcult and problem dependent. This means that in
general such automated methods are unsafe on their own. Many diag-

Monte Carlo Methods and MCMC Simulation
51
nostics are very technical, and for many problems they do not directly
apply, or are diﬃcult to implement; see for instance Gelman and Rubin,
1992.
Monitoring the realisations against t gives a reasonable impression of
the range of values that are “possible” for the invariant distribution.
When realisations are multivariate, scalar functions of the realisations
may be monitored. A burn-in will lie outside this range and stand out
in comparison to the remaining realisations. With poor mixing, a burn-
in may be diﬃcult to discern, since a certain trend may last for many
iterations.
We may be lead to think that the chain has reached the
invariant distribution when it in fact has not. However, if we are able
accelerate the chain, then we may monitor the realisations over a shorter
time frame, and trends are easier to discern.
Admittedly, a graphical criterion for assessing convergence may be
deceptive because it does obviously not say anything about “where the
chain will be in a few moments from now”. This is especially so if only
a single chain is used. Running MCMC with diﬀerent initial so-called
overdispersed values with respect to the invariant distribution, and com-
paring the plots by visual inspection provides a better way of checking
if the chain is stuck “by accident” or if all runs result in similar bands
of realisations. Of course similar plots is no guarantee for convergence,
but it indicates a certain robustness, because the diﬀerent chains even-
tually all reach the same region meaning that minor non-deterministic
disturbances have not resulted in “oﬀ-course” behaviour of the chains.
However, running several parallel chains is from a computational point
of view not the preferred choice for assessing convergence.
2.1.6
The importance of blocking
In the previous section the importance of the Markov chain mixing
properly was stressed. In this section we address the issue of mixing in
the context of the Gibbs sampler. Blocking plays an important role in
this regard.
Suppose that we apply (block) Gibbs sampling to obtain samples from
the invariant distribution Pr(Y , Z), X = Y ∪Z. We ﬁrst sample from
Pr(Y |Z) and then from Pr(Z|Y ) etc. The values of the variables behind
the conditional constrain the probability of sampling certain values from
the distribution, e.g., when sampling from Pr(Z|Y ), the value that Z
can take on, is constrained by the value of the last realisation of Y .
Although irreducibility is guaranteed when the conditionals are strictly
positive, an unfortunate value on the conditional side may imply that

52
EFFICIENT LEARNING OF BAYESIAN NETWORKS
that the variance of the conditional distribution is very small. It may
be highly improbable (though not impossible) to sample certain values
“in the tail”, eﬀectively having a negative impact on the mobility of
the chain. Theoretically the chain will reach the entire sampling space,
in practice this may take very long, because the chain has maneuvered
itself into a region that is comparable to a local optimum. Figure 4.2,
right, also illustrates this issue: The outline of the whole ﬁgure is the
state space we wish to explore. Theoretically there is a path between
the two larger regions (shades of gray), but it may be diﬃcult to reach
one region from the other since we have to pass a very narrow band. So,
strictly speaking irreducibility is guaranteed, but in practice it may be
diﬃcult to sample all over the state space.
When dependence between Z and Y is strong, then the situation
sketched may very well occur. If the dependence is absent, thus when
Pr(Y , Z) = Pr(Y ) · Pr(Z), we have that the conditionals reduce to
Pr(Y ) and Pr(Z), and the interaction eﬀects are gone. For instance,
when sampling from Pr(Z|Y ) = Pr(Z), a previous value assigned to Y
does not inﬂuence the probability of drawing a particular value z.
The above discussion suggests that we should look for blocks that
are only weakly dependent since this prevents the chain from getting
trapped (see also Roberts and Sahu, 1997). It may be diﬃcult to devise
such blocks where this is the case for all values one can assign to the
blocks. In this respect a notion of context (in)dependence also plays a
role. Two blocks may be strongly dependent for some values, yet for
other values less. Hence, we seek blocks that are context independent
on each other for many block assignments.
Additionally, Gibbs sampling allows for dynamic creation of blocks
while running. The blocks need not a priori be deﬁned before starting
the Gibbs sampler, but the individual blocks may change. Also, the
blocks need not be disjoint.
We may proﬁt from this dynamic way
of deﬁning blocks since it oﬀers a large degree of freedom in order to
accommodate the desire to sample blocks that don’t constrain each other
strongly.
A block we may regard as a set of variables that inherently belong to-
gether, or alternatively, as something that should be sampled together,
because the individual variables strongly depend on each other, and
splitting them has a negative impact on the mixing of the chain (Jensen
et al., 1995). It may not always be easy to determine these blocks, but
for some problems there is a natural decomposition which follows al-
most immediately from the problem description. Although a diﬀerent

Monte Carlo Methods and MCMC Simulation
53
branch of computer science, in evolutionary computation the importance
of the right decomposition has also been acknowledged as being very im-
portant, and deceptive decompositions have been investigated (Pelikan
et al., 2003). In many other ways there are actually striking similarities
between Monte Carlo methods and the population based paradigm on
which evolutionary computation is based. In Section 3.3 we will sam-
ple edges from the posterior model distribution, and we investigate how
edges of a DAG model are best blocked to improve mixing.
As an example of blocking, suppose that samples are required from
some distribution Pr(X), X = {X1, X2, X3, X4}; MCMC is employed
for that purpose. Assume that at some iteration, X1 and X2 “naturally
belong together” in the context of the current x3 and x4; they should not
be split, because in the current context they are intimately linked from
a domain perspective. This mutual relevance translates into a statistical
context dependence, and consequently the variables are joined in a block
(X1, X2). We draw the values for (X1, X2) (jointly) from the conditional
Pr(X1, X2|x3, x4). In the next iteration, the context might have changed,
and the situation may be such that the variables (X2, X3, X4) depend on
each other; the variables should be joined, and sampled as a block from
Pr(X2, X3, X4|x1), and so on. As long as each Xi is sampled “inﬁnitely
often”, jointly or individually, samples from Pr(X) are obtained in the
limit.
3.
Learning models via MCMC
MCMC oﬀers a feasible way of learning BN models from a Bayesian
perspective. In Chapter 3, Section 3.2, model learning was treated from
a model selection point of view.
To a Bayesian, a single model is a
rather poor summary statistic of the posterior model distribution. A
Bayesian is interested in the entire distribution Pr(M|d). Also, if we are
interested in some feature over models quantiﬁed by Δ, we can average
the feature with respect to the model posterior:
E[Δ(M)|d] =

m
Δ(m) · Pr(m|d)
In this section we discuss two methods for learning models via MCMC
using the marginal likelihood scoring criterion; the discussion is based on
Riggelsen, 2005. Other MCMC approaches for learning BN models exist,
see for instance Friedman and Koller, 2003. An MCMC approach for
model learning which does not employ the marginal likelihood criterion is

54
EFFICIENT LEARNING OF BAYESIAN NETWORKS
given in Green, 1998. An alternative to MCMC is described in Madigan
and Raftery, 1994.
The posterior distribution Pr(M|d) is obtained via Bayes’ law, hence
has the form of eq. 4.7, thus is precisely an instance of what can be
solved via MCMC. The denominator 
m Pr(d|m) Pr(m) in eq. 3.12 is
due to the large number of models impossible to evaluate.
3.1
Sampling models
MCMC Metropolis-Hastings sampling of models is discussed in Madi-
gan and York, 1995; Kocka and Castelo, 2001. Here the proposal dis-
tribution, guides the incremental changes of the models by proposing
to somehow change the current model. This proposal is produced by
drawing M′ ∼Pr′(M|M(t)). With probability:
ρ(M(t), M′) = min

1, Pr′(M(t)|M′) Pr(M′|d)
Pr′(M′|M(t)) Pr(M(t)|d)

the proposal is accepted and M(t+1) = m′, otherwise M(t+1) = m(t).
For t →∞models from the invariant distribution are obtained.
The usual proposal distribution changes the current model in a single
adjacency, by selecting two vertices at random, and either adds, reverses
or removes the edge (arc) between the vertices. Since these proposals are
uniform, the proposal fraction Pr′(M(t)|M′)/ Pr′(M′|M(t)) cancels out,
and the sampler is driven by the marginal likelihood ratio. Furthermore,
since models consecutive diﬀer in a single edge, it follows from the de-
composition of the marginal likelihood, that all terms cancel out except
those terms for which the parent set changes.
3.2
Sampling edges
In the following we apply a single-site Gibbs MCMC for sampling
models from the posterior model distribution. Instead of considering M
as a single random variable, we suggest to split the model into sepa-
rate edges, each of which we regard as a random variable. This is the
natural decomposition of a DAG model for applying Gibbs sampling,
and is equivalent to applying the single adjacency Metropolis-Hastings
sampler discussed in the previous section. However, the Gibbs sampling
technique explicitly gives rise to the question if single edges is the best
decomposition from an eﬃciency point of view: how does it inﬂuence
the mixing of the chain? This is investigated in the sections to come,
beginning from Section 3.3.

Monte Carlo Methods and MCMC Simulation
55
Deﬁne for all r = 1, . . . , p·(p−1)
2
possible edges of DAG model M the
random variables Er with state space ΩEr = {←, →, ̸−}, i.e., every
edge of the graph can take on a direction, or can be absent.
If the
conﬁguration of all edges forms a DAG, the posterior joint distribution,
Pr(E1, . . . , E p·(p−1)
2
|d) is well-deﬁned.
Via Gibbs sampling, models from the posterior are obtained as joint
realisations of the edge assignments. The process goes as follows: draw
edges at iteration t from the full conditional given the data:
E(t)
1
∼
Pr(E1|e(t−1)
2
, . . . , e(t−1)
p·(p−1)
2
, d)
...
E(t)
p·(p−1)
2
∼
Pr(E p·(p−1)
2
|e(t)
1 , . . . , e(t)
p·(p−1)
2
−1, d)
E(t+1)
1
∼
Pr(E1|e(t)
2 , . . . , e(t)
p·(p−1)
2
, d)
...
Each draw is subject to the constraint that all edges together must
form an acyclic graph.
There are always at least two possible edge
assignments.
Removal is always an option since it can’t introduce a
cycle. Also, it is always possible to add an arc in at least one direction
between any two vertices that are not adjacent.
The fact that not all 3 edge assignments are allowed at every drawing
stage, means that irreducibility can’t be guaranteed by using the posi-
tivity argument, i.e., that Pr(El|E \ {El}, d) > 0. However, the Markov
chain deﬁned here is irreducible, because at every draw the state ̸−
is a possibility and this state never introduces a cycle. Hence, there is
a non-zero probability of removing arcs that obstruct the addition of
other edges in any direction in the graph (obstruct in the sense that the
graph would become cyclic). Consequently all DAGs can be reached by
“breaking down” the current DAG (a kind of backtracking) and rebuild-
ing another one thereby reaching another DAG.
In order to draw edge El from the full conditional given the data we
calculate:
Pr(El|E \ {El}, d)
=
Pr(E1, . . . , El, . . . , E p·(p−1)
2
|d)

el
Pr(E1, . . . , el, . . . , E p·(p−1)
2
|d)

56
EFFICIENT LEARNING OF BAYESIAN NETWORKS
=
Pr(d|E1, . . . , El, . . . , E p·(p−1)
2
)

el
Pr(d|E1, . . . , el, . . . , E p·(p−1)
2
)
(4.11)
where a uniform model prior Pr(M) on the model space, and the de-
nominator Pr(d) = 
m Pr(d|m) Pr(m) both cancel out. When drawing
an edge from the Gibbs sampler, say El, at most two terms are aﬀected,
namely the terms pertaining to the vertices of edge El; denote those
vertices by Xi and Xj. It follows from the marginal likelihood decom-
position given in eq. 3.17 that:
Pr(d|E) =

r̸={i,j}
Pr(dr|dpa(r), E) · Pr(di|dpa(i), E) Pr(dj|dpa(j), E)
such that all factors 	
r̸={i,j} Pr(dr|dpa(r), E) cancel out in eq. 4.11 be-
cause the parent sets for the corresponding vertices are unchanged.
To approximate the expected value of model features, we use the
empirical average:
E[Δ(M)|d] ≈1
n
n

t=1
Δ(m(t))
(4.12)
where n denotes the total number of samples from the Markov chain.
Often this kind of averaging is done over features one can read directly
oﬀa model, e.g., Markov blanket features of vertices, but theoretically
any statement that the model entails can be averaged.
3.3
Blocking edges
In Section 2.1.6, the impact of blocking on the performance of MCMC
was treated. Therefore, we need to analyse if there are edges that inher-
ently belong together, and should be considered jointly when sampling.
Although the univariate edge sampler theoretically does what it is sup-
posed to do, it is from a practical point of view advantageous to seek
blocks that can improve the mixing of the chain and prevent it from
getting trapped for long periods of time.
The state of a single edge El between Xi and Xj is determined by
those terms in the marginal likelihood that correspond to Xi and Xj,
that is, in eq. 3.17 the terms Pr(di|dpa(i), E) and Pr(dj|dpa(j), E). These
terms indirectly depend on Xpa(i) and Xpa(j) in terms of their realisa-
tions in the (ﬁxed) data sample. The marginal likelihood does not de-
compose into independent terms of parent set variables, and every time

Monte Carlo Methods and MCMC Simulation
57
El
Xi
Xj
Figure 4.3.
Dashed-dotted edge is the centre edge El. Solid edges (arcs) are the
edges on which El currently depends; they “span” the block. All edges, including
the dashed ones, potentially constrain each other (though in a varying degree) when
assigned directions.
the parent set changes for a particular vertex, the corresponding term in
the marginal likelihood needs to be recomputed. This means that when
drawing an edge state for El, the current parent sets of Xi and Xj con-
strain the distribution of El. It follows that El depends on all incoming
edges (arcs) to Xi and Xj; this is a form of context dependence, because
when those same edges are outgoing or absent, the dependence is not
present. From the perspective of El, the dependence just described is a
direct one, but the dependency goes even further, because the edges to
Xi and Xj in turn depend on edges in the same way—this is an indirect
dependence. When the parent sets Xpa(i) and Xpa(j) are large, El is
inﬂuenced by many edges. This also means that in relatively dense areas
of a DAG, the edges strongly depend on each other. Edge El can be
considered the “centre” of a dependence region.
These considerations suggest that when we have to determine El, all
edges in a region around El should be joined in a block, and be drawn
“as one”. First of all we notice that the dependence is a context depen-
dence and therefore a block dynamically changes over time depending
on the states of the edges in the vicinity of El. Also, the blocks over-
lap because the state of many edges is constrained by the same set of
edges; this is especially so in dense regions where edges are close to each
other. We suggest to apply a Gibbs sampler that focuses per draw on
one dependence region with one particular edge at the centre instead of
focusing on a single edge. By considering a region around El, we ac-
knowledge that these edges may constrain the assignment of the state
of the centre edge strongly, and to a lesser extent constrain each other.

58
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The question that arises is then how large, or, alternatively, how far
from the centre edge El, edges should be joined in a block. We suggest
to join all edges on which El directly depends (along with El itself) into
block l. However, this set of edges only determines the current situation
but not necessarily “the right one” which is exactly what is object to
learning. Therefore we also include in the block all edge variables between
the vertices spanned by the currently dependent edges, i.e., not only
those pointing to Xi and Xj; see ﬁgure 4.3 for an illustration. All the
edges in that block are “close” and either currently constrain each other
(and most strongly constrain El), or potentially will constrain each other
if assigned a direction.
A consequence of this dynamic way of blocking, is that areas that are
currently dense, receive more attention and are sampled more frequently
than less dense regions. In dense regions edges constrain each other,
hence spending more time in these regions helps solving unfortunate edge
assignments that negatively impact the edge conﬁgurations of dependent
edges.
3.3.1
Blocks and Markov blankets
The suggested way of blocking coincides with the Markov blanket:
the solid edges (arcs) in ﬁgure 4.3 connect Xi with the vertices in the
Markov blanket of Xi. Hence, initially a block is deﬁned by the edges
in the Markov blanket of Xi.
The Markov blanket contains exactly those variables are the “most
relevant” vertices for determining Xi. For that reason, it is reasonable to
think that these variables potentially are related (adjacent in the graph)
as well, since they represent concepts that are “close” from a domain
perspective. Of course, this argument only is valid in the heuristic sense.
The dashed edges in ﬁgure 4.3 can be thought of as representing those
edges that potentially connect the Markov blanket vertices.
In that
respect including those edges in the block a priori is not a bad idea. We
refer to the edges within a block as the currently relevant edges, and
those not within the block as currently irrelevant edges.
Formally, these sets of edges are deﬁned in the following way:
The set of relevant edges of Xi is:
E′
i = {E = (Xs, Xl)|Xs, Xl ∈Xmb(i) ∪{Xi}}
The set of irrelevant edges of Xi is:
E′′
i = {E = (Xi, Xl)|Xl ∈X} \ E′
i

Monte Carlo Methods and MCMC Simulation
59
Xi
Figure 4.4.
A block, E i. Dashed edges belong to the relevant edges, E′
i. Dotted
edges belong to the irrelevant edges, E′′
i .
A block of edges for Xi is: Ei = E′
i ∪E′′
i
In ﬁgure 4.4 the 3 sets are illustrated.
The general approach is now as follows: either shrink or grow the
Markov blanket of a vertex by adding or removing vertices via currently
irrelevant edges, or change the internal relationships between the ver-
tices of the Markov blanket via the currently relevant edges. Observe
that E′
i actually captures the notion of a sub-graph, and that the pro-
posed approach boils down to learning sub-graphs that, when combined,
form the overall DAG model.
3.3.2
Sampling blocks
Block Gibbs sampling in terms of Ei, follows the sampling scheme:
E(t)
1
∼
Pr(E1|¯ε1, d)
...
E(t)
p
∼
Pr(Ep|¯εp, d)
E(t+1)
1
∼
Pr(E1|¯ε1, d)
...
Here ¯εj is the current conﬁguration of the edges in the complement
of the set Ej. As previously remarked in Section 2.1.4, the visitation

60
EFFICIENT LEARNING OF BAYESIAN NETWORKS
scheme is not crucial, and random selection of blocks may be beneﬁcial
for computational reasons.
Sampling a block of edges may seem diﬃcult at ﬁrst sight. In contrast
to single edges, entire blocks have many diﬀerent conﬁgurations. The
probability of each possible conﬁguration of Ei needs to be determined
before we can draw one. Additionally, the relevant and irrelevant edges
play diﬀerent roles.
Adding a vertex not currently belonging to the
Markov blanket via the irrelevant edges should have a lower priority than
changing the internal relationships between those vertices that already
belong to the Markov blanket via the relevant edges.
Metropolis-Hastings sampling provides a way to deal with these issues.
We use a “Metropolis-within-Gibbs” MCMC sampler, where Gibbs sam-
pling and Metropolis-Hastings sampling are combined in order to sample
models from the invariant model posterior. Each draw from the Gibbs
sampler is performed by a Metropolis-Hastings sampler. The proposal
distribution is deﬁned as a mixture distribution, where one component,
f(·), deals with the edges in the relevant edge set, and the other com-
ponent, g(·), deals with the edges in the irrelevant edge set, that is:
Pr′(E(t+1)|E(t)) = w · f(E(t+1)|E′(t)) + (1 −w) · g(E(t+1)|E′′(t))
(4.13)
where 0 < w < 1 determines the mixture weights.
When f(·) is applied, values for the edges in the relevant set are drawn,
i.e., MCMC is run in order to obtain the posterior distribution of those
edges. The cardinality of the set of relevant edges is kept ﬁxed once the
Metropolis-Hastings sampler is applied—we merely assign (new) values
to the edge variables. Hence, the set of relevant edges is determined
before entering the Metropolis-Hastings sampler, and does not change
until control is given back to the overall Gibbs sampler. For g(·) the
same holds, but here assignments are considered to the variables in the
irrelevant set.
The distribution f(·) produces uniform proposals such that each edge
Er ∈E′ has probability 1/|E′| of being drawn. Depending on the cur-
rent value er, a state change is proposed to one of the (at most) two
alternatives with probability 0.5. For example, if E(t)
r
≠−then either
E(t+1)
r
= →or E(t+1)
r
= ←is proposed. For the distribution g(·) the
same holds, but here we have Er ∈E′′ with probability 1/|E′′|. Notice
that edges not in either of these two sets remain unchanged.
The weight w in the mixture varies how much “attention to pay”
to the conﬁguration of the edges in the Markov blanket of the current

Monte Carlo Methods and MCMC Simulation
61
vertex. We want to try out several diﬀerent conﬁgurations of the edges
in the relevant edge set before deciding to grow or shrink it via the
irrelevant edge set. This implies that f(·) should be applied more often
than g(·).
Uniform proposals are not the only option, but it makes the Metropolis-
Hastings acceptance ratio easy to compute. For Metropolis-Hastings,
the acceptance probability depends on the following fraction, with the
proposal distribution deﬁned in eq. 4.13:
Pr′(E(t)|E(t+1))
Pr′(E(t+1)|E(t))
For both the mixture in the numerator and in the denominator, the
weights are the same, the conditional distributions select edges with
equal probability and there is always the same number of alternative
edge assignments, i.e., the distributions are uniform, hence, the ratio
cancels out.
If non-uniform proposals are used, then the proposal ratio does not
cancel out, and it has to be computed explicitly. It is questionable if
one is able to deﬁne such non-uniform proposals unless prior knowledge
is available. Moreover, prior knowledge is rarely expressed in terms of
relevant and irrelevant edge sets, so specifying a non-uniform proposal
distribution is in no way trivial.
There seems to be something counterintuitive as to why the com-
bined MCMC sampler approach works: why sample single edges via
Metropolis-Hastings when the performance gain lies in the fact that
all edges in a block should be considered “one entity”? One could ar-
gue that the problem with the edge dependences has just been pushed
down one level to another MCMC sub-process, and that this process
suﬀers from exactly the same thing as the original single edge Gibbs
MCMC. The reason why the proposed “Metropolis-within-Gibbs” ap-
proach works is that edges that are part of several Markov blankets are
sampled relatively often. This entails that edges in dense regions of the
DAG are sampled more often. More time is spent in regions where edges
constrain each other strongly, meaning that potential obstructions are
“solved” simply because improbable yet not impossible alternatives edge
assignments are eventually accepted because they have been proposed
many times. In doing so we take into account that sampling models is
more diﬃcult in some parts of the model space.

62
EFFICIENT LEARNING OF BAYESIAN NETWORKS
3.3.3
Validity of the sampler
In using such a “Metropolis-within-Gibbs” sampler, the question arises
if the convergence towards the invariant model distribution is still guar-
anteed. Combining MCMC samplers boils down to combining several
transition probabilities via a mixture or product of transitions. It is well-
known (Tierney, 1994) that such combinations leave the overall chain er-
godic with the desired invariant distribution, provided the sub-MCMC
samplers created via Metropolis-Hastings sampling in the separate Gibbs
sampling steps are ergodic with the invariant distributions Pr(Ei| ¯Ei, d).
Theorem 4.1 The MB-MCMC model sampler produces an ergodic Mar-
kov chain with the invariant distribution Pr(M|d).
It is not diﬃcult to see that this is the case, because it follows from
the fact that Metropolis-Hastings MCMC produces an ergodic Markov
chain: The proposal distribution of the Metropolis-Hastings sampler
will with a non-zero probability propose a state change to any edge
in Ei, which guarantees irreducibility. With a non-zero probability it
will remain in the current state for any edge implying aperiodicity. We
may thus conclude that the Metropolis-Hastings sampler, for all i =
1, . . . , p, in the limit returns realisations from the invariant distribution
Pr(Ei| ¯Ei, d), i.e., realisations for the edges in Ei given all other edges.
We note that formally it is not even an requirement that Markov chains
created by the Metropolis-Hastings samplers converges for every i before
going on to the next Gibbs sampling step.
Since blocks dynamically change over time, we need to check if all
edges are sampled. The Gibbs sampler makes a call to a Metropolis-
Hastings sampler for every vertex, and it trivially follows that every
edge eventually will be part of an irrelevant edge set. We can’t guarantee
that the edges will be part of a relevant edge set however, but as long as
all edges are considered (and sampled via Metropolis-Hastings MCMC),
we have that the realisations drawn come from Pr(E1, . . . , E p·(p−1)
2
|d) as
t →∞.
3.3.4
The MB-MCMC model sampler
Algorithm 1 contains the pseudocode of the Markov blanket MCMC
(MB-MCMC) sampler. Line 3 determines the block to pay attention to;
here a systematic sweep is shown. Line 4 calls the algorithm for reversing
covered arcs; we refer to Kocka and Castelo, 2001 for the implementation
of this step. Lines 5–6 determines the edges to consider, and in lines 8–
12 the edges are drawn from the sets of relevant edges. The proposals

Monte Carlo Methods and MCMC Simulation
63
are accepted or rejected in line 13–16. In line 17 the conﬁguration of
all edges is recorded, i.e., here the actual models from the posterior are
saved. One may decide to sub-sample the Markov chain of models by
only recording the draws once in a while.
Algorithm 1: MB-MCMC
Input
: k, MH-steps; w, prop. expand/change block conf.
Output: Edges from Pr(E|d) (requires burn-in).
M ←G = (X = {X1, . . . , Xp}, E = {E1 ≠−, . . . , E p·(p−1)
2
≠−})
1
for r ←0 to ∞do
2
i ←(r mod p) + 1
3
rcar(10)
4
/* Define relevant and irrelevant sets/blocks
*/
E′
i ←{E = (Xs, Xl)|Xs, Xl ∈Xmb(i) ∪{Xi}}
5
E′′
i ←{E = (Xi, Xl)|Xl ∈X} \ E′
i
6
for t ←0 to k do
7
draw U ∼U[0, 1] /* Uniform draw
*/
8
/* Draw edge values for the block
*/
if u < w and E′
i ̸= ∅then
9
draw E(t+1)
i
∼f(Ei|ε′(t)
i
)
10
else
11
draw E(t+1)
i
∼g(Ei|ε′′(t)
i
)
12
ρ ←min{1, Pr(d|ε(t+1)
i
)/ Pr(d|ε(t)
i )}
13
draw U ∼U[0, 1]
14
if u ≥ρ then
15
E(t+1)
i
←ε(t)
i
16
record(e1, . . . , e p·(p−1)
2
)
17
The algorithm takes two arguments: k determines the number of times
the Metropolis-Hastings sampler is run, and w determines the probabil-
ity of changing the internal conﬁguration of a component vs. adding or
removing new vertices. Parameter k need not be large for the overall
invariant model distribution to be reached, i.e., the Metropolis-Hastings
sampler need not converge at every call. In fact we have found it to be
beneﬁcial for the convergence rate to assign k a small value; too large a
value may lead to slow mixing and convergence. In our experiments we
have set k = 5, and w = 0.95. Letting the Metropolis-Hastings sampler

64
EFFICIENT LEARNING OF BAYESIAN NETWORKS
converge before the overall Gibbs sampler reaches its invariant distribu-
tion, means that the invariant distributions of the Metropolis-Hastings
samplers are reached and they now coincide with the marginal (at that
time still) sub-optimal distribution of the Gibbs sampler. There is no
reason to exhaustively explore the state space of the marginals of the
Gibbs sampler, unless we are close to its invariant distribution. Doing
so has a negative impact on the acceleration of the chain.
When every vertex is assigned a cache that keeps the suﬃcient statis-
tics indexed by the parent set, we may drastically improve the speed
of MCMC by querying the cache before querying the data. We have
implemented the Markov blanket sampler in C++ using STL, and for
the experiments in the next section we were able to reach what we be-
lieve are the invariant distributions in less than 10 minutes on a 2 GHz
machine.
3.3.5
Evaluation
We considered two BNs for the experiments: The ALARM network
with 37 vertices and 46 arcs (Beinlich et al., 1989) about intensive care
patient monitoring, and the Insurance network with 27 vertices and 52
arcs (Binder et al., 1997) classifying car insurance applications.
We
used the BDeu metric for the counts α with an ESS of 1. All experi-
ments were run for 1,000,000 iterations. As convergence diagnostic we
monitored the number of edges as suggested in for instance Giudici and
Green, 1999. We compared the Markov blanket MCMC with eMC3,
a single edge MCMC sampler that also employs the RCAR algorithm
(Kocka and Castelo, 2001; Castelo, 2002).
In ﬁgure 4.5 the results of the ALARM network are illustrated for
1000 and 5000 samples. With 1000 samples, we see that two indepen-
dent runs of the MB-MCMC both converge towards models with about
50–53 edges. There is no signiﬁcant diﬀerence in the convergence be-
haviour. For eMC3 two runs produce diﬀerent behaviour and result in
models with 68–77 edges. For 5000 records similar observations hold,
but overall the number of edges is lower: 45–51 for MB-MCMC and
57–70 for eMC3. We notice that eMC3 seems sensitive to the starting
point of the chain. To show this more clearly, we ran both samplers
starting from the empty graph, and from the actual ALARM graph for
7000 samples. The results are illustrated in ﬁgure 4.6. For the 7000
records we would expect that the number of edges on average should
converge to 46, i.e. there is enough data to support the data generating
model. For MB-MCMC, both chains converge towards models with 44–

Monte Carlo Methods and MCMC Simulation
65
0
50000
100000
150000
200000
250000
300000
10
30
50
70
90
No. edges
Run 1
Run 2
Iteration
0
50000
100000
150000
200000
250000
300000
10
30
50
70
90
110
No. edges
Run 1
Run 2
Iteration
Figure 4.5.
ALARM network. Convergence behaviour given 1000 (top) and 5000
(bottom) records for two independent runs.
The lower lines are from the Markov
blanket MCMC, and the upper lines from eMC3.
50 edges. The most frequently sampled model is similar to the ALARM
network ±2 arcs. For eMC3 there is a big diﬀerence.
The chain started from the actual network stays at around 50–55
edges, but the chain started from the empty graph gets stuck at 63–70.
The most frequently sampled model is in both situations less similar to

66
EFFICIENT LEARNING OF BAYESIAN NETWORKS
0
50000
100000
150000
200000
250000
300000
20
30
40
50
60
70
No. edges
Empty
Generating
Iteration
0
50000
100000
150000
200000
250000
300000
40
60
80
100
No. edges
Empty
Generating
Iteration
Figure 4.6.
ALARM network. Convergence behaviour of the Markov blanket MCMC
(top) and eMC3 (bottom) given 7000 records starting from the empty and the data
generating model.
the actual ALARM network than in the MB-MCMC case (excess of ±10
and ±25 arcs).
Next we consider results of the Insurance network in ﬁgure 4.7 for
500 samples. We would like to note that the association between several
parent-child variables in the Insurance network is rather weak and that
even for large data sets these associations will be deemed absent by the

Monte Carlo Methods and MCMC Simulation
67
0
50000
100000
150000
200000
250000
300000
Iteration
30
35
40
45
50
No. edges
Run 1
Run 2
0
50000
100000
150000
200000
250000
300000
Iteration
30
35
40
45
50
No. edges
Run 1
Run 2
Figure 4.7.
Insurance network. Convergence behaviour given 500 records for two
independent runs of the Markov blanket MCMC (top) and eMC3 (bottom).
marginal likelihood score. For 500 records the MB-MCMC converges to
an invariant distribution where models are sampled with 36–40 edges.
The two runs meet at around 150,000 iterations. For eMC3 however, the
two chains don’t quite agree in the number of edges: somewhere between
37–46. We also ran both samplers beginning from the empty and the
actual Insurance graph for which the results are illustrated in ﬁgure 4.8.

68
EFFICIENT LEARNING OF BAYESIAN NETWORKS
0
100000
200000
300000
400000
Iteration
35
40
45
50
55
No. edges
Empty
Generating
0
100000
200000
300000
400000
Iteration
42
47
52
57
62
67
No. edges
Empty
Generating
Figure 4.8.
Insurance network.
Convergence behaviour of the Markov blanket
MCMC (top) and eMC3 (bottom) given 10,000 records starting from the empty and
the data generating model.
For MB-MCMC both starting points produce models with 45–47 edges.
Also here we see that eMC3 is sensitive to the initial model. Starting
from the data generating model, the sampler converges to an invariant
distribution where models with 45–47 edges are sampled. Starting from
the empty graph, models with 54–56 edges are sampled. We see that

Monte Carlo Methods and MCMC Simulation
69
even with 10,000 records, there is not enough information in the data
sample to support the 52 arcs in the data generating Insurance network.
Observe that for large amounts of data, the ﬂuctuation in the number
of edges is larger. This is especially noticeable when comparing the plots
in ﬁgures 4.7 and 4.8; the variability of the plots for 500 records is larger
than for 10,000 records.
This is to be expected because there is no
pronounced “best” model with merely 500 records. For large amounts
of data, the models do not diﬀer a lot, and model selection may in that
case be an eﬃcient alternative to MCMC. After all, MB-MCMC is from
a computational point of view generally more demanding than most
algorithms developed for model selection.
3.3.6
Conclusion
MB-MCMC is a Bayesian approach to learning Bayesian network
models. Being Bayesian is beneﬁcial when model learning is based on a
relatively small amount of data; in practice this may often be the case.
When data is scarce, there may be several models that are structurally
quite diﬀerent from each other, yet are almost equally likely given the
data. In other words, the data supports several models that diﬀer widely,
yet from a scoring perspective are very close. Model selection methods
will only return “the best” model, but give no clue as to how and in
what respect models diﬀer that score almost equally well.
For large data sets—where “large” of course is related to the number
of variables of our domain—the best model is much more likely than any
other model, and model selection may be adequate.
By employing MB-MCMC there is no added computational burden
compared to existing MCMC approaches. The improvement in mixing
and convergence is only due to a wiser decomposition of the joint distri-
bution. From a local perspective, the edges in the Markov blanket of the
vertices form a natural dependence relationship in the sense that they
constrain each other. This local dependence also from a more global
perspective makes sense: dense areas of the DAG is “tougher” to alter
than less dense regions.

This page intentionally left blank

Chapter 5
LEARNING FROM INCOMPLETE DATA
Real-life data is rarely complete. Unfortunately most learning algo-
rithms assume that the data is complete, or handle missing values in an
ad-hoc way. In this chapter we introduce the concept of incomplete data
and how to learn BNs from incomplete data. We illustrate why this is a
non-trivial problem, and why existing complete data methods will not
work “out of the box” anymore. Incomplete data often renders an exact
analysis intractable making approximation or iteration the only feasible
approach to such problems.
We present various principled approaches that require iteration and
apply the theory treated in the previous chapters to learning BNs from
incomplete data. We focus on the Bayesian approach to learning BNs
and several MCMC samplers are presented for approximating posterior
distributions. In particular, we develop an algorithm called eMC4 that
combines importance sampling, Gibbs sampling and model MCMC sam-
pling for drawing from the posterior model distribution given incomplete
data.
Less principled non-iterative approaches are discussed as well;
they are fast, but only approximate results are obtained. Towards the
end of this chapter a non-iterative algorithm is developed, called MBP.
By relaxing assumptions about interactions between variables and by
exploiting the “blocking inﬂuence from outside” eﬀects of Markov blan-
kets, the algorithm is very fast, but still accurate in comparison with
iterative approaches.

72
EFFICIENT LEARNING OF BAYESIAN NETWORKS
1.
The concept of incomplete data
Although “missing data” and “incomplete data” usually are synony-
mous, in the current context the latter is preferred, because it suggests
that a completion of the data exists—somehow the complete data has
been censored or otherwise restricted from being fully observed. In fact
the notion of there existing a potential completion of the incomplete
data is central to the analysis of such data.
To get an intuitive idea of the concept of incomplete data, consider
the following example from credit scoring (see for instance Hand and
Henley, 1997) about applicants applying for a loan at a bank.
The
applicants are either rejected or accepted depending on characteristics
of the applicant such as age, income and marital status. The vector
X = (X1, . . . , Xk) refers to those completely observed application vari-
ables. Repayment behaviour of the accepted applicants is observed by
the creditor, usually leading after some time to a classiﬁcation as either
a good or bad (defaulted) loan; let the binary variable Y be this clas-
siﬁcation label. Repayment behaviour of rejects is for obvious reasons
not observed; complete data is available only for accepted applicants.
The response indicator RY , indicates if the applicant was accepted or
rejected.
We can write the distribution of the outcome of the loan as:
Pr(Y |X) =
1

i=0
Pr(Y |X, RY = i) Pr(RY = i|X)
Note that the sampling process identiﬁes the acceptance/rejection prob-
ability Pr(RY |X) and the outcome conditional on acceptance Pr(Y |X, RY
= 1), but provides no information about the outcome conditional on
rejection Pr(Y |X, RY = 0).
Hence we have a problem in estimating
Pr(Y |X) from observed data.
Now suppose the acceptance policy is such that the probability of
acceptance depends exclusively on X; for example, values of X corre-
sponding to a high perceived default risk have a low acceptance proba-
bility (and vice versa), but otherwise acceptance is completely random.
Then we have:
Pr(Y |X, RY = 1) = Pr(Y |X, RY = 0) = Pr(Y |X)
Now Pr(Y |X) coincides with the observable distribution Pr(Y |X, RY =
1), which we can therefore use to estimate Pr(Y |X). In other words,

Learning from Incomplete Data
73
valid statistical analysis can be performed by considering the observed
part of the data only.
Suppose instead that acceptance is based on additional characteristics
Z other than X, that is Z ̸⊆X, for example the “general impression”
that the loan oﬃcer has of the applicant. If Pr(Y |X, Z) ̸= Pr(Y |Z),
that is Y indeed depends on Z even when we condition on X, this leads
to:
Pr(Y |X, RY = 1) ̸= Pr(Y |X, RY = 0)
(5.1)
i.e., at any particular value of X, the distribution of the observed Y
diﬀers from the distribution of the missing Y . Note that if the loan
oﬃcer does his job well, we expect that Pr(Y = 1|X, RY = 1) > Pr(Y =
1|X, RY = 0) (where Y = 1 denotes a good loan). The inequality in
eq. 5.1 means that we cannot perform valid statistical analysis by only
considering the observed part of the data.
1.1
Missing data mechanisms
As we saw in the previous example, the variable RY is used to indicate
if Y is observed or not.
The distribution of RY models the process
giving rise to incompletion. This mechanism may depend on observed
and possibly on unobserved entries as well.
In general we consider the vector R that has the same dimensions
as D, where Rj
i is the binary indicator variable of Xj
i , i.e., for record
j, variable Xi. When Xj
i is missing, then Rj
i = 0, otherwise Rj
i = 1.
Additionally we partition D = (O, U), where O is the observed part
and U the unobserved (missing) part.
Rubin, 1976, deﬁnes three basic missing data mechanisms: Missing At
Random (MAR), Missing Completely At Random (MCAR) and Missing
Not At Random (MNAR). They are deﬁned as follows:
MAR Pr(R|O, U) = Pr(R|O), i.e., given O the probability that an
observation is missing does not depend on U.
MCAR Pr(R|O, U) = Pr(R), i.e., the probability that an observation
is missing depends neither on O nor U.
It follows that this is a
special case of MAR.
MNAR The probability that an observation is missing depends on U
(and possibly on O).
MAR and MNAR form disjoint classes—a
missing data mechanism is either MAR or MNAR.

74
EFFICIENT LEARNING OF BAYESIAN NETWORKS
In general we are interested in the so-called predictive distribution
Pr(U|O), which is based solely on the observed part of the data:
Pr(U|O) =

r
Pr(U|O, r) Pr(r|O)
(5.2)
The MAR assumption exactly formalises the idea of being able to predict
(“restore”) the missing data, when response is independent of missing
values. From the MAR assumption it follows by swapping U and R in
Pr(R|O, U):
Pr(U|O, R) = Pr(U|O)
That is, for predicting U we need not consider the response indicator
variable for unbiased statistical analysis. We may now write eq. 5.2 as:

r
Pr(U|O, r) Pr(r|O) =

r
Pr(U|O) Pr(r|O)
and we say that the missing data mechanism is ignorable. Strictly speak-
ing ignorability also requires distinctness of parameters (Little and Ru-
bin, 1987). In Bayesian terms, this means that the parameter of the
distribution of the complete data (O, U) should be independent of the
parameter of the distribution governing the missing data mechanism. In
practice this usually holds, but if violated, it “only” comes at the price
of eﬃciency, but the predictions remain unbiased.
If data is missing according to an MNAR mechanism, then the MAR
independence statement does not hold, and we need to predict accord-
ing to Pr(U|O, R). In that case the missing data mechanism has to be
modeled explicitly, for instance by someone with domain knowledge. In
the latter part of the example given in Section 1, the acceptance (rejec-
tion), depends on an unobserved set of characteristics, imposed by the
“domain expert”; this clearly violates the MAR assumption, because
Pr(RY |O = X, U = Z) ̸= Pr(RY |O = X). If Z would have been ob-
served in conjunction with X, then the missing data mechanism would
have been MAR, because the probability of acceptance is no longer de-
pendent on variables that are unobserved. This suggests that when a
missing data mechanism is MNAR it in fact may be become MAR if
additional data is obtained; in this case if we could get hold of Z. Also,
MAR vs. MNAR is really a matter of degree, since we may be able to
get hold of a subset of Z thus coming “closer to MAR”.
Unfortunately, it is not possible by analysing the incomplete data to
determine if data is missing according to MAR or MNAR, unless we

Learning from Incomplete Data
75
have knowledge about the missing data mechanism.
However, as we
mentioned above, MAR and MNAR are the two extremes, i.e., between
Pr(U|O, R) and Pr(U|O) there are several levels of dependence on R.
In fact Pr(U|O, R) ̸= Pr(U|R) covers the entire interval between “al-
most equal” or “entirely diﬀerent”, e.g., with regard to context indepen-
dence. Eﬀectively this means that even though the MAR assumption
may be violated to some extent, analysis may still yield approximately
valid results. Although we cannot test for ignorability in general, an
option is to test how sensitive inference results are to diﬀerent missing
data mechanisms.
In general it is impossible to develop fully automated learning proce-
dures, when data is missing non-ignorable. In that case prior knowledge
has to be speciﬁed about the nature of the missing data mechanism. In
this thesis we develop methods that assume ignorability, in which case
the missing data mechanism need not be speciﬁed.
Under the ignorability assumption, a valid (unbiased) prediction of
U can be obtained based on O. Thus, all information about the miss-
ing data, necessary for performing valid statistical analysis, is already
contained in the observed data O prior to predicting U; the predictions
made for U do not produce information not already implicitly contained
in O. Combining O and the predictions of U merely makes the sample
look complete, yet the actual sample size is really that of O. In every
respect it suﬃces to analyse O only, and not predict U at all.
Although all required information is implicitly contained in O, the
observed data may be structured in a way that complicates the extrac-
tion of this information. In an MCAR setting the structural diﬃculties
can be circumvented at the price of loss of eﬃciency. MCAR implies
that records with missing values can be skipped; since MCAR implies
that the response indicator is independent of the data values, skipping
records with missing values translates into sub-sampling the incomplete
data. What remains is a complete sample, and any complete data sta-
tistical technique can be applied. In Section 4 we return to this issue.
When data is not MCAR, then this process of skipping records obviously
introduces bias, and another approach is required.
2.
Learning from incomplete data
Principled statistical analysis of MAR incomplete data needs to con-
sider all of O, not just subsets as in an MCAR analysis.

76
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The challenges and diﬃculties with incomplete data pervades all ar-
eas of statistical data analysis. An exact analysis is usually out of the
questions because it leads to analytical intractability and high computa-
tional complexity. In the remainder of the thesis we restrict attention to
learning BNs from incomplete data; see also Chickering and Heckerman,
1997 for interesting discussions on large sample approximations when
learning BNs.
In the following, we are given a data sample with incomplete records.
Each record contains instantiations of the observed variables such that
for record j we have that (oj, U j) = (oj, (Uj
1, . . . , Uj
r(j))), where oj is the
observed part of the record, and U j is the r(j)-dimensional unobserved
part.
2.1
Likelihood decomposition
The likelihood function plays a crucial role in learning BNs, no matter
if the classical (penalised) likelihood approach is used or the Bayesian
approach.
The learning methods introduced in Chapter 3 rely on the fact that
the functional form of the likelihood is a product of terms. For incom-
plete data this is unfortunately no longer true.
Via the likelihood decomposition given in eq. 3.2, it follows that the
likelihood of observed data is given by:
Pr(o|m, θ) =
c

j=1

uj
p

i=1
θxj
i |xj
pa(i)
That is, the unobserved variable are summed out for each record: the
sum is taken over all possible completions of the unobserved part. The
likelihood is no longer a simple product, but includes summations as well.
For a given record there will for observed variables be as many terms
as there are completions of the ancestral variables in m. In general this
means that the likelihood of incomplete data has a number of summation
terms roughly exponential in the number of missing items in the sample.
As an example, suppose we are given the following model m with 3
binary variables, X1 →X2 →X3, and assume that we are given the
following incomplete data sample (‘?’ denotes a missing observation):
X1
X2
X3
d1
0
1
1
d2
1
?
1

Learning from Incomplete Data
77
After seeing d1, the likelihood Pr(d1|θ, m) is still a simple product
(where 1 −θXi=1|pa(i) = θXi=0|pa(i)):
(1 −θX1=1)1θ1
X2=1|X1=0θ1
X3=1|X2=1
but once d2 is observed, we need to sum out X2, and the likelihood
Pr(d1, o2|θ, m) becomes:
(1 −θX1=1)1θ1
X1=1θ1
X2=1|X1=0θ1
X2=1|X1=1θ2
X3=1|X2=1 +
(1 −θX1=1)1θ1
X1=1θ1
X2=1|X1=0(1 −θX2=1|X1=1)1θ1
X3=1|X2=1θ1
X3=1|X2=0
a summation over likelihoods of complete data, one per completion.
In rare cases the likelihood of incomplete data remains a product,
namely if all descendants of missing items in the DAG model are miss-
ing as well. Hence, if this holds for all the records, then the resulting
likelihood has the same functional form as the likelihood of complete
data, i.e., it contains no summations.
This leads to similar simpliﬁ-
cations as the so-called monotone pattern of missingness described in
Little and Rubin, 1987; see also Didelez and Pigeot, 1998. In the previ-
ous example, if X3 was missing as well in record two, then the likelihood
Pr(d1, o2|m, θ) would be the product:
(1 −θX1=1)1θ1
X2=1|X1=0θ1
X3=1|X2=1 · θ1
X1=1

x2,x3
θx3|x2θx2|X1=1
=
(1 −θX1=1)1θ1
X1=1θ1
X2=1|X1=0θ1
X3=1|X2=1
Note that when learning the model of a BN, we in fact “build” (DAG)
models by traversing the DAG search space. This means that the func-
tional form of the likelihood will change as well. Hence, the above sim-
pliﬁcation in practice is of little use when learning models. Even when
learning the parameter of BNs the pattern of missingness rarely is such
that a signiﬁcant reduction of summation terms can be achieved. One
can attempt to reduce the number of terms by using an equivalent DAG
model with alternative child/parents relations. For instance with the
above data and the model X2 →X3, the likelihood consists of a sum
of two complete data likelihoods; for the equivalent model X3 →X2
this can be reduced to a likelihood consisting of just one term. Unfortu-
nately, for larger DAG models it is rarely possible to reduce the number
of summation terms signiﬁcantly.

78
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The Bayesian approach is intimately related to the likelihood via
Bayes’ law, and may be seen as a kind of “wrapper” around the like-
lihood where the unobserved items are summed out. Starting with a
product Dirichlet prior distribution, recall that the posterior is obtained
as:
Pr(Θ|m, o) ∝Pr(o|m, Θ) Pr(Θ|m)
The likelihood of incomplete data is a sum of likelihoods of complete
data. This means that the posterior parameter distribution Pr(Θ|m, o)
becomes a mixture distribution; each component in the mixture is a
product Dirichlet distribution corresponding to a completion of the data.
In the previous example, the mixture consists of two components,
where each component is a product Dirichlet with normalising factors
Z1 and Z2; here Zi is the posterior normalising factor as function of the
suﬃcient statistics from completion i as given in eq. 3.9. Z−1
i
is the
normalising term for the product Dirichlet for completion i (the “mass
under” the unnormalised product Dirichlet), and 
j Z−1
j
is the normal-
ising term for the entire unnormalised posterior (the “mass under” the
sum of the unnormalised product Dirichlets). It follows that in order
to normalise the resulting mixture as a whole, the mixture weights are
determined as:
wi =
Z−1
i

j Z−1
j
⇔wi =

j Zj
Zi
(5.3)
and the exact posterior is given by:
Pr(Θ|m, o) =

j
wj Pr(Θ|o, u(j), m)
Here u(j) denotes the jth completion of the data. Hence, the mixture
weights simply redistribute the mass according to the inverse of the
normalising factors of the Dirichlets.
2.1.1
Complications for learning parameters
In order to learn the parameter of a BN the ML estimates can be used.
For complete data these are given by eq. 3.4. With incomplete data,
however, ﬁnding the parameter value that maximises the likelihood is no
trivial task; we then have to turn to numerical optimisation techniques.
For the Bayesian approach to parameter learning the posterior mix-
ture distribution may in itself be the solution. However, the huge number
of mixture components makes it practically impossible to perform any

Learning from Incomplete Data
79
kind of statistical analysis, such as computing summary statistics. The
main problem is not only that all possible completions actually have to
be generated, but also that we in general need to retain the completions
in memory. Although for a ﬁxed model it suﬃces to save the suﬃcient
statistics in memory and not the actual data completions, in general
this is unmanageable and from a storage capacity point of view nearly
impossible.
2.1.2
Bayesian sequential updating
For complete data, we saw that Bayesian updating per record yields
the same result as batch updating. For incomplete data this is no longer
the case (Spiegelhalter and Lauritzen, 1990; Cowell et al., 1995).
In
fact, for incomplete data, diﬀerent orders results in diﬀerent posteri-
ors. The reason is that the Bayesian approach incorporates a predictive
feature during the learning process, based on what has been gathered
thus far. Once a record is observed where some items are missing, they
are predicted using the available information that has been “learned”
from the previous records. As an example, consider the incomplete data
from the previous example. First, d1 is observed, resulting in the pos-
terior Pr(Θ|m, d1), where conditioning on d1 means that the Dirichlet
is updated with the counts extracted from record 1. Next, record two is
considered, but this time there are items missing:
Pr(Θ|m, d1, o2) =

u2
Pr(Θ|d1, o2, u2) Pr(u2|m, d1, o2)
where Pr(Θ|d1, o2, u2) is the posterior Dirichlet distribution updated
with complete data from record 2, and Pr(u2|m, d1, o2) is the predictive
distribution based on the posterior Dirichlet after seeing record 1, con-
ditional on the items that actually have been observed in record 2. In
this example X2 is missing, and has two possible completions. The pos-
terior Pr(Θ|m, d1, o2) becomes a mixture distribution consisting of two
components, with mixing weights corresponding to the probability of
completing the record with ‘1’ and ‘0’ based on what has been gathered
from d1.
To compute the probability Pr(u2|m, d1, o2), inference in the BN is
required where the parameter values are the expectation of the posterior
Dirichlet Pr(Θ|m, d1), and evidence o2; this was discussed in Chapter
2, Section 5.1. After having processed record 2, the next time we are
presented with an incomplete record, we require the expectation of a

80
EFFICIENT LEARNING OF BAYESIAN NETWORKS
mixture of two product Dirichlets.
This is equal to the sum of the
expectations of each Dirichlet component times their mixture weights.
This predictive behaviour goes on: the records are processed sequen-
tially, and predictions are conditional on the records that have already
been processed.
Suppose now that d2 and d1 are swapped. In that case the missing
item X2 needs to be predicted prior to seeing any other records. The
prediction is based on the hyper parameter of the Dirichlet only. The
next record does not contain any missing items, and once it has been
observed, it follows that the posterior is a diﬀerent one than had we
been presented with the completely observed record ﬁrst. The updating
procedure just described is in distribution, i.e., the posterior parameter
distribution is updated as new records are processed.
The Dirichlet
allows updating of the expectation only (Cowell, 1998). Hence, each new
incoming record “moves” the expectation slightly without the need of
maintaining the entire posterior mixture in memory. Still, the resulting
posterior expectation depends on the order of the records.
Intuitively, the sensitivity to the ordering of the records is to be ex-
pected; after all not all of o is used for predictions, but only subsets
thereof. In that sense, this sequential Bayesian way of updating does not
fulﬁll our requirement of being a principled approach for analysing in-
complete data. However, this does not mean that the approach is wrong
as such. In fact, for dynamic systems where observations are sequen-
tially produced as time passes, a Bayesian will only revise the “corpus”
slightly when presented with a new observation. When presented with a
batch of iid records however, the records are time-interchangeable, and
the sequential approach strictly speaking does not apply.
2.1.3
Complications for learning models
Model learning methods rely on the likelihood function or the poste-
rior parameter distribution (that in turn also depends on the likelihood
function).
The functional form of the likelihood for incomplete data
makes it diﬃcult to perform model learning.
In order to apply the penalised likelihood scoring criteria, the ML
estimates are required.
However, these are diﬃcult to determine for
incomplete data, hence the penalised likelihood scoring metrics can not
be applied without further ado.
The Bayesian marginal likelihood is the normalising factor for the
prior product Dirichlet over the normalising factor for the posterior prod-

Learning from Incomplete Data
81
uct Dirichlet in eq. 3.14. This derivation remains valid for incomplete
data, with the exception that for incomplete data the denominator is
then the normalising factor for the posterior mixture distribution. This
normalising factor is only available given that the mixture is known,
requiring that (almost) all completions are generated after which the
normalisation factor for the posterior mixture can be computed as:
1

j Z−1
j
where Zj is the normalising factor for the posterior product Dirichlet
as function of the suﬃcient statistics from the complete data (o, u(j)),
with u(j) being the jth completion.
3.
Principled iterative methods
Looking at the exact (batch) analysis, there is no “predictive” ele-
ment; the unobserved items are summed out, without the interference
by a predictive distribution (with exception to the Bayesian sequential
approach). In a way this would be the “naive” approach that would
come to mind as a ﬁrst attempt to deal with incomplete data in a prin-
cipled fashion: simply sum out the missing items, and use what is left
(the observed part of the data) for further statistical analysis. As we
sketched in the previous sections, this turns out to be a rather unwise
approach.
In this section we discuss other principled algorithms for handling
incomplete data problems.
They provide an alternative to the exact
methods discussed until now, and are from a computational point of
view more tractable. The methods are iterative in nature, and converge
towards (a) completion(s) of the incomplete data and consequently they
are best interpreted as prediction-based approaches.
3.1
Expectation Maximisation—EM
In the following we investigate the well-known deterministic EM-
algorithm by Dempster et al., 1977 for computing ML estimates. The
EM-algorithm can be applied to a variety of problems that may concep-
tually be considered incomplete data problems or that can be formu-
lated as such, e.g., for ﬁtting mixture models. However, in the context
of learning BN parameters, the problem formulation is straightforward.
In Shafer, 1997; McLachlan and Krishnan, 1997; Lauritzen, 1995 the
EM-algorithm is treated in more detail.

82
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The iterative EM-algorithm, in contrast to the approaches discussed
in section 2.1, employs a predictive distribution; in fact it makes use of
several predictive distributions. The general idea is to predict missing
items using the best predictive distribution that we are able to derive
given what we know at a given point in time (iteration).
Such a predictive distribution is Pr(U|m, o, θ(t)), where θ(t) is the
parameter based on information we have gathered up until now. This
parametrisation enables us to produce the best predictions possible at
time t.
The EM-algorithm departs from the likelihood decomposition:
Pr(o|m, θ) = Pr(o, U|m, θ)
Pr(U|m, o, θ)
(5.4)
Recall that the ML estimate is found by:
ˆθ = arg max
θ
Pr(o|m, θ) = arg max
θ
Pr(o, U|m, θ)
Pr(U|m, o, θ)
Because the logarithm is a positive monotonic transformation, we may
equivalently do the following:
ˆθ = arg max
θ
log Pr(o, U|m, θ) −log Pr(U|m, o, θ)
(5.5)
Hence, taking the logarithm will not have any inﬂuence on the estimation
of θ. In the limit EM returns ˆθ essentially solving eq. 5.5.
The missing values are predicted by computing the expectation of
(the logarithm of) the likelihood given in eq. 5.4. This results in the
observed log-likelihood:

u

log Pr(o|m, θ)

Pr(u|m, o, θ(t))
=
log Pr(o|m, θ)
=
Q(θ, θ(t)) −H(θ, θ(t))
where:
Q(θ, θ(t)) =

u

log Pr(o, u|m, θ)

Pr(u|m, o, θ(t))
H(θ, θ(t)) =

u

log Pr(u|m, o, θ)

Pr(u|m, o, θ(t))

Learning from Incomplete Data
83
Let θ(t+1) be the value of θ that maximises Q(θ, θ(t)). θ(t+1) is then
a better estimate than θ(t) because Pr(o|m, θ(t+1)) −Pr(o|m, θ(t)) ≥0.
This can be seen from the following:
Q(θ(t+1), θ(t)) −Q(θ(t), θ(t)) + H(θ(t), θ(t)) −H(θ(t+1), θ(t)) ≥0
In this equation Q(θ(t+1), θ(t)) ≥Q(θ(t), θ(t)) because θ(t+1) was chosen
to accomplish exactly that. Notice that actually we need not even max-
imise Q(θ, θ(t)), but we need only choose a θ(t+1) that yields a larger
Q-value than θ(t). Also H(θ(t), θ(t)) ≥H(θ(t+1), θ(t)) since:
H(θ(t+1), θ(t)) −H(θ(t), θ(t)) = E[log Pr(U|m, o, θ(t+1))
Pr(U|m, o, θ(t))
|θ(t)]
≤log E[Pr(U|m, o, θ(t+1))
Pr(U|m, o, θ(t))
|θ(t)]
= log

u
Pr(u|m, o, θ(t+1))
Pr(u|m, o, θ(t))
Pr(u|m, o, θ(t))
= log 1 = 0
Here Jensen’s Inequality E[log(·)] ≤log E[·] with the concave log-function
has been applied.
The key lies in the Q-function since only that function is relevant for
selecting θ(t+1); the H-function we may leave out of the picture. The
Q-function is deﬁned as the expectation of log Pr(o, U|m, θ)—the log-
likelihood of the complete data. Because the multinomial distribution
belongs to the regular exponential family, the log-likelihood is linear in
the suﬃcient statistics. Exploiting linearity of expectation, this means
that the expectation of the log-likelihood reduces to the expectation of
the suﬃcient statistics. Due to the iid assumption the expected suﬃ-
cient statistics E[n(xi, xpa(i))] with respect to the predictive distribution
become:
E[n(xi, xpa(i))|θ(t)] =
c

j=1
Pr(Xi = xi, Xpa(i) = xpa(i)|oj, m, θ(t))
If Xi and Xpa(i) are observed in record j, and have values xi and xpa(i)
then Pr(xi, xpa(i)|oj, m, θ(t)) = 1 (and equal to 0 for any other value of
Xi and Xpa(i)), and inference in the BN is not required. Otherwise the
result of the inference is added to the suﬃcient statistics. It is now easy

84
EFFICIENT LEARNING OF BAYESIAN NETWORKS
to ﬁnd the parameter θ(t+1); it is the ML estimate using the expected
suﬃcient statistics.
The EM-algorithm is usually presented using the likelihood approach
the way just described. It is not diﬃcult to prove that EM also holds
in a Bayesian setting. The only essential diﬀerence is that the param-
eter estimates are based not only on the data, but also on the hyper
parameter α.
In summary, the EM-algorithm consists of two steps:
1 E-step: Predict missing values given the current best estimate of θ.
In the case of a multinomial sample, this corresponds to predicting
the cell counts.
2 M-step: Calculate the parameter estimate using the statistics from
the E-step, and consider it the new best estimate of θ.
Repeated application of the above E- and M-step produces a sequence
of statistics, i.e., complete data samples, and parameters. The parame-
ters will generally converge towards the true parameter of the BN given
the observed data o. The rate of convergence is related to the so-called
fraction of missing information in the data sample. Usually it will take
longer to converge if more data is missing (McLachlan and Krishnan,
1997), but not necessarily so. The fraction of missing information need
not change dramatically for diﬀerent fractions of missing items.
The rate of convergence of course also depends on the initial estimate
at time t = 0.
If a qualiﬁed guess is available, it can be used as a
plausible starting point. If the likelihood function is multimodal and
the curvature contains regions with ridges, convergence may be slow,
and local optima may be found. To diagnose problems like these several
diﬀerent starting points should be tried.
EM converges monotonically to the (local) optimal parameter esti-
mate. We may monitor its progress by studying the diﬀerence between
the estimated parameter values between each iteration. If the absolute
diﬀerence is below a certain threshold it means that we may stop the
algorithm, and consider the last estimate as the required parameter es-
timate given o.
The idea behind EM is quite appealing, and easy to understand. The
application of the E-step however includes an inference step, which for

Learning from Incomplete Data
85
BNs is known to be an NP-hard problem (Cooper, 1990). The inference
is performed on a per record basis, and repeated iteration of the E-
and M-step means that we potentially have to perform many inference
calculations before convergence.
3.1.1
Structural EM—SEM
EM is traditionally used for parameter learning, however, it is pos-
sible to use the statistics obtained from the E-step for model selection
as well.
Hence, once EM has converged, the statistics corresponding
to completions of the incomplete data can be used together with the
complete data model scoring metrics discussed in chapter 3. Unfortu-
nately it is computationally expensive to use EM for model selection
in this way because even a minor change in model structure requires a
complete re-run of the EM-algorithm for the new structure.
It is in fact possible to do model selection within EM such that
the best model is selected in the limit. This algorithm is referred to
as Structural Expectation Maximisation (SEM), originally proposed by
Friedman, 1997; Friedman, 1998. A slightly ineﬃcient forerunner of the
SEM-algorithm was proposed by Singh, 1997.
In Friedman, 1997 SEM was suggested and proved based on the pe-
nalised likelihood model scoring metric, MDL/BIC. In Friedman, 1998
the marginal likelihood scoring metric was used instead, for which SEM
was shown to be at least approximately correct. In fact, we are now able
to make a stronger statement:
Proposition 5.1 SEM can be made exact when using the marginal
likelihood as a scoring criterion, and it converges to (a local) maximum
of Pr(M|o).
We exploit the fact that for model selection the marginal likelihood
can be rewritten as a penalised likelihood score; as we saw, it is basically
just another penalised likelihood model scoring metric.
We seek the most probable BN given o, i.e., we seek the MAP of:
Pr(M, Θ|o) = Pr(Θ|M, o) Pr(M|o)
From Chapter 3, Section 3.3, we know that we may equivalently max-
imise the penalised log-likelihood:
log Pr(o, α −1|M, Θ) + log f(M, α)

86
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Similar to the EM-algorithm, we introduce the predictive distributions
Pr(U|o, (m, θ)(t)), but now the model and the parameter are dependent
on time t.
Following the same line of reasoning as we did for the EM derivation,
we may restrict attention to the following Q-function, Q((m, θ), (m, θ)(t)):

u

log Pr(o, u, α −1|m, θ) + log f(m, α)

Pr(u|o, (m, θ)(t))
As before, the log-likelihood is linear in the suﬃcient statistics from
the data, and by adding the logarithm of the penalty term, the whole
expression remains a linear function of the suﬃcient statistics. As with
EM, this means that the expectation of the entire expression reduces to
the expectation of the suﬃcient statistics plus the penalty term.
Using the same argument as for EM, it follows that by letting (m, θ)(t+1)
be the value of (m, θ) that maximises Q((m, θ), (m, θ)(t)), it is a better
estimate than (m, θ)(t).
It is not diﬃcult to ﬁnd (m, θ)(t+1), because we know that for any
given m, log Pr(o, u, α −1|m, θ) + log f(m, α) is maximised by the ML
estimate ˆθ based on the statistics from (o, u, α −1). Thus, we select
a model m(t+1) = ˆm that maximises this penalised log-likelihood using
θ(t+1) = ˆθ, and then use the pair ( ˆm, ˆθ)(t+1) in the predictive distribu-
tion for the next iteration.
The SEM-algorithm thus consists of the following steps:
1 E-step: Predict missing values in terms of the cell counts given the
current estimate of (m, θ).
2 M-step: Select the new best pair (m, θ):
(a) Using the statistics from the E-step together with the marginal
likelihood scoring criterion, select the best model ˆm.
(b) Calculate the parameter estimate ˆθ for ˆm using the statistics
from the E-step.
So how is this diﬀerent from the naive model selection approach in
EM? Instead of using the same model for the predictive distribution at
time t+1, the model is changed such that it captures the independences
of the completed data at time t. In the naive approach there are too

Learning from Incomplete Data
87
many wasteful iterations of ﬁtting the parameter to models that are far
from being correct.
3.2
Data Augmentation—DA
In the next few sections, we work towards an MCMC model sampler,
that produces realisations from Pr(M|o). We introduce several interme-
diate MCMC samplers on our way. We take a Bayesian approach, and
treat both the parameter and the model as random variables. A clear
distinction between the model learning phase and the parameter learn-
ing phase is in the Bayesian approach a bit awkward. In the following
we therefore can’t leave Θ entirely out of the picture even though our
ultimate goal is to learn about models only. We depart from an MCMC
sampler called Data Augmentation (DA).
DA is an MCMC Gibbs sampler that resembles EM in many ways. DA
ﬁts perfectly into the Bayesian paradigm in the sense that it accounts for
the uncertainty due to missing data; the increased variance that missing
data introduces. It does so by returning realisations from distributions,
rather than returning a single summary statistic like EM does. As the
name suggests, DA augments the incomplete data, i.e., it augments o
with U (t) thus creating complete data sets. In the standard form, DA
as a side-eﬀect also creates the corresponding parameters Θ(t).
It should be noted that although we in this section reduce DA to a
special kind of Gibbs sampling, the original presentation and motivation
by Tanner and Wong, 1987 was slightly diﬀerent.
Consider the joint distribution Pr(U, Θ|m, o), i.e., the joint distri-
bution of missing data and parameter Θ. Recall that Gibbs sampling
produces draws from a joint distribution, given that we are able to draw
from the full conditionals. In this case this means that we may obtain
realisations from the above joint, if we can draw from the distributions
Pr(U|m, Θ, o) and Pr(Θ|m, o, U).
Consider now the following Gibbs sampling set-up:
U (t+1)
∼
Pr(U|m, θ(t), o)
Θ(t+1)
∼
Pr(Θ|m, o, u(t+1))
...
For t →∞, the realisations come from the invariant joint distribution
Pr(U, Θ|m, o). If we restrict attention to the realisations of U or Θ on

88
EFFICIENT LEARNING OF BAYESIAN NETWORKS
their own, we may consider those as coming from the predictive distri-
bution Pr(U|m, o) or the posterior parameter distribution Pr(Θ|m, o).
Using the theory presented in Chapter 4, we can now for instance
compute the empirical average of some function over the n augmented
data sets:
E[h(U, o)] =

u
h(u, o) Pr(u|m, o) ≈1
n
n

t=1
h(u(t), o)
(5.6)
For example h(·) may be the function that counts the suﬃcient statistics.
The expression in eq. 5.6 will in that case give us the expected suﬃcient
statistics.
These statistics can then be used as input for some other
algorithm.
If the initial realisations of U are not from the invariant
distribution, it is a good idea to skip those samples when computing the
empirical average in eq. 5.6 (although this is not a requirement because
for (very) large n the burn-in has a negligible eﬀect on the approximation
anyway).
Performing DA in BNs is reasonably easy to do thanks to the BN
decomposition.
In this regard the Markov blanket plays an impor-
tant role.
If we focus on one single record, we require samples from
Pr(U l, Θ|ol, m). If we create the augmented BN as we did in ﬁgure 2.4
in Chapter 2, Section 5.1, we see that if we apply a single-site Gibbs sam-
pler, we may for each variable restrict attention to its Markov blanket,
no matter if it is a parameter variable or a variable in U l.
In our context we have c records, and drawing from Pr(U|m, Θ(t), o)
in the Gibbs sampler means drawing a large multivariate sample. Ex-
ploiting the iid assumption, each record with missing items may be con-
sidered separately, i.e., for each record we draw U l given ol and the
current θ = Θ(t). Draw U l using the following univariate Gibbs sam-
pler:
Ul,(t+1)
1
∼
Pr(Ul
1|m, ul,(t)
2
, . . . , ul,(t)
r(l) , ol, θ)
...
Ul,(t+1)
r(l)
∼
Pr(Ul
r(l)|m, ul,(t+1)
1
, . . . , ul,(t+1)
r(l)−1 , ol, θ)
...
In the limit this process generates samples from Pr(U l|m, θ, ol), i.e.,
realisations from the predictive distribution for record l, given θ. For
drawing Ui from the full conditional it suﬃces to only include those

Learning from Incomplete Data
89
variables on the conditional side that are part of the Markov blanket of
Ui. Observe that we actually apply a sub-MCMC Gibbs sampler when
we need to draw from Pr(U|m, Θ(t), o); hence we should not expect
the initial full completions to be multivariate samples from the desired
target distribution.
Once a U (t+1) has been generated, realisations from Pr(Θ|m, o, U (t+1))
are required, which is now a distribution conditional on complete data.
Because of parameter independence, we may draw the parameter vectors
ΘXi|xpa(i) from the posterior Dirichlet Pr(ΘXi|xpa(i)|m, o, U (t+1)) inde-
pendently of each other, i.e., for all Xi and each parent set conﬁguration
xpa(i).
Sampling from the Dirichlet amounts to drawing from the Gamma dis-
tributions (Gelman et al., 2004). Given parent set conﬁguration xpa(i),
for each value xi, draw v(xi) from the Gamma distribution with shape
parameter s(xi, xpa(i)) = α(xi, xpa(i)) + n(xi, xpa(i)) and common scale
parameter (e.g. 1). Let θxi|xpa(i) = v(xi)/ 
xi v(xi) be the normalised
elements of θXi|xpa(i), then this vector will have a Dirichlet distribution
ΘXi|xpa(i) ∼Dir(ΘXi|xpa(i)|s).
The required steps for drawing from
the Gamma distribution are well-described in literature; see for instance
Kennedy and Gentle, 1980.
In summary, DA consists of the following two steps:
1 I-step: The imputation step predicts missing values by drawing from
Pr(U|m, θ(t), o), where θ(t) was drawn from the last parameter pos-
terior.
2 P-step: The posterior step draws the BN parameter from the pos-
terior Pr(Θ|m, o, u(t+1)), where u(t+1) was the prediction drawn for
the missing values from the last I-step.
The strong resemblance with EM is evident. The I-step corresponds
to the E-step in EM and the P-step corresponds to the M-step. The
more fundamental diﬀerence with EM, is that DA produces realisations
from Pr(U, Θ|m, o), rather than just returning the most probable pa-
rameter and corresponding completion of the data. In that sense DA is
a Bayesian approach because entire distributions are approximated via
empirical samples.

90
EFFICIENT LEARNING OF BAYESIAN NETWORKS
The chain of realisations evolving by repeated application of the I-
step is also the key to multiple imputation (MI) (Little and Rubin, 1987;
Rubin, 1987). MI is a technique that combines in a non-Bayesian way
several of the data completions such that the increased uncertainty due
to missing data is accounted for, when further statistical analysis of
incomplete data is necessary. It does so by combining the so-called in-
between variance of independent completions obtained from the I-step
of DA. Details can be found in for instance Brand, 1999.
3.2.1
DA and eliminating the P-step—DA-P
From a predictive point of view, the goal is to complement the in-
complete data sample; with DA we get several probable completions,
such that we can average any function over all those completions as in
eq. 5.6. In this respect only the I-step is interesting, since this is where
the realisations from the predictive distribution are generated. The re-
alisations of the parameter in the P-step are not directly relevant for our
purpose, but the P-step is necessary for standard DA to work. As an
alternative to standard DA we suggest the following alternative Gibbs
sampler (Riggelsen, 2004; Riggelsen, 2006b), where the P-step is omitted
eﬀectively eliminating the overhead of having to draw from the product
Dirichlet distribution. Additionally, the Markov chain converges faster
to Pr(U|m, o) than to the joint Pr(U, Θ|m, o) because there is one di-
mension less “to explore”.
First oﬀ, we require the following:
Proposition 5.2 For a completion U (t), we have Pr(U|U (t), o, m) ∝
Pr(U|o, (U (t), o), m) = Pr(U|o, θ = E[Θ|U (t), o], m) where in the last
distribution o is evidence and (θ = E[Θ|U (t), o], m) a BN.
The equivalence between the 1st and the 2nd equation is up to the
constant factor 1/ Pr(o|m).
The equality follows from the predictive
property of Bayesian statistics viz. eq. 2.5 after seeing d, a whole set of
completely observed records:
Pr(X|d, m)
=

p

i=1
Pr(Xi|Xpa(i), d, θ, m) Pr(θ|d, m)dθ
=
Pr(X|θ = E[Θ|d], m)
In particular, for record l we have Dl = (Ol, U l), we consider Ol as
the evidence, and we may write:
Pr(U l|ol, d, m) = Pr(U l|ol, θ = E[Θ|d], m)

Learning from Incomplete Data
91
Consider now the following Gibbs sampler. Draw the t + 1th imputa-
tion:
U (t+1)
∼
Pr(U|o, u(t), m) ∝Pr(U|o, θ = E[Θ|o, u(t)], m)
...
Thus a new imputation is drawn at time t + 1 based on the last
completed data sample at time t. In the limit we obtain realisations
from the predictive distribution, Pr(U|m, o). We refer to this sampler
as DA-P (Data Augmentation minus the P-step).
The proposed sampling procedure is almost similar to normal DA.
However, instead of drawing from the posterior parameter distribution
as one would do in DA (the P-step of DA), an expectation step can be
performed instead as a consequence of the Bayesian way of marginal-
ising out the parameter space. The Markov chain does not converge
to Pr(U, Θ|m, o) but to Pr(U|m, o), and the parameter space is not
explored by the MCMC sampler. We thus only need to focus on the
so-called primary chain (or the chain of interest) if we are interested
in completions of the incomplete data. In the next section on learning
models, this is exactly the chain we are interested in.
Although the parameter realisations are not explicitly drawn anymore
as in DA, we may (for instance afterwards) approximate the posterior
parameter distribution using the empirical average based on the com-
pletions drawn in the I-step of DA-P:
Pr(Θ|m, o) =

u
Pr(Θ|m, o, u) Pr(u|m, o) ≈1
n
n

t=1
Pr(Θ|m, o, u(t))
If we instead of uniform mixture weights, 1/n, assign weights according
to eq. 5.3, we obtain a better approximation, which may be regarded
as a ﬁnite mixture distribution with product Dirichlet components. We
may now analyse the approximation as a real mixture distribution and
compute summary statistics; for instance the variance may be computed
giving an indication of the uncertainty, including uncertainty due to
missing data (Riggelsen, 2006b); see Titterington et al., 1985 for more
on statistical analysis of ﬁnite mixtures.
Theorem 5.1 The DA-P sampler produces an ergodic Markov chain
with invariant distribution Pr(U|o, m).
We need to make sure that the chain is irreducible, and this we do
by checking the positivity requirement.
We draw from Pr(U|o, θ =

92
EFFICIENT LEARNING OF BAYESIAN NETWORKS
E[Θ|o, U (t)], m), which indeed is strictly positive; there are no logically
impossible combinations of X because from eq. 3.10 we see that θ > 0,
and the BN is just some product of the elements from θ. Aperiodicity
holds because there is a probability > 0 of drawing the same conﬁgura-
tion as the last one, no matter the restrictions imposed by the model.
3.3
DA-P and model learning—MDA-P
When applying DA-P, we assume that the model is ﬁxed indicated by
the lower-case m on the conditional side. We now turn to the problem of
ﬁnding models that can explain the data, but in a way that uncertainty
due to missing data is taken into account when learning. The approach
is somewhat similar to the deterministic SEM-algorithm.
The sampler is built around a Gibbs sampler, where samples in the
limit come from the joint distribution Pr(U, M|o). We omit the P-step,
and draw realisations as follows:
U (t+1)
∼
Pr(U|o, m(t), u(t)) ∝Pr(U|m(t), θ = E[Θ|o, u(t)], o)
M(t+1)
∼
Pr(M|o, u(t+1))
...
In the limit, the models thus obtained can be regarded as realisations
from the posterior model distribution, Pr(M|o). We see that the com-
pletions drawn using DA-P augment the incomplete part of the data,
and the now complete sample is used for model learning.
In Section 3.2.1 we already discussed how to draw from the distri-
bution Pr(U|M(t), E[Θ|o, U (t)], o), by ﬁlling in the unobserved part
of the data sample on a per record basis.
However, drawing from
Pr(M|o, U (t+1)) is more complicated, and requires the application of a
separate MCMC sampler. In Chapter 3, Section 3, an eﬃcient MCMC
model sampler was presented, called MB-MCMC. This means that when
we need draws from the Pr(M|o, U (t+1)), we call the model sampler,
which we know has Pr(M|o, U (t+1)) as the invariant distribution.
In summary, the MDA-P sampler (Model Data Augmentation minus
the P-step) goes as follows:
1 I-step: The imputation step predicts missing values by drawing from
Pr(U|m(t), E[Θ|o, u(t)], o), where u(t) was drawn from the last I-step,
and m(t) was drawn from last the model posterior.

Learning from Incomplete Data
93
2 MP-step: The model posterior step draws from Pr(M|o, u(t+1)), where
u(t+1) was drawn from the last I-step.
Theorem 5.2 The MDA-P sampler produces an ergodic Markov chain
with invariant distribution Pr(M|o).
MDA-P combines two MCMC samplers. Provided that both sam-
plers produce ergodic chains, the desired joint distribution of the overall
sampler is reached in the limit (Tierney, 1994). We know that DA-P
is ergodic with the target distribution Pr(U|M(t), E[Θ|o, U (t)], o) and
the same holds for the model sampler which has the target distribution
Pr(M|o, U (t+1)).
If we compare MDA-P to the SEM-algorithm, then the I-step would
correspond to the E-step in SEM, and the MP-step would correspond
to the model selection step.
3.4
Eﬃciency issues of MDA-P
The eﬃciency of the MDA-P sampler presented in the previous sec-
tion, depends on several factors. Although MDA-P provably will pro-
duce draws from the model posterior in the limit, this is only a theoreti-
cal guarantee, and the remarks made about MCMC sampling in Chapter
4 apply. Essentially we are dealing with a non-trivial MCMC sampler,
and in the following two sections we investigate the sampler focusing on
eﬃciency issues.
3.4.1
Properties of the sub-MCMC samplers
In the MDA-P sampler, we have marginalised out the parameter space
such that the Gibbs sampler need not waste time in exploring this di-
mension. Moreover, as a side-eﬀect of this, we need not implement a
procedure for drawing from the product Dirichlet distribution. What
remains is to draw imputations in the I-step, and generating models in
the MP-step. Since both of these steps employ MCMC samplers on their
own, the performance of MDA-P largely depends on the properties of
those sub-MCMC samplers.
By calling the MB-MCMC algorithm in the MP-step, we made an
attempt to perform reasonably well (in terms of mixing) in this sub-
MCMC sampler by way of blocking edges. However, this does not mean

94
EFFICIENT LEARNING OF BAYESIAN NETWORKS
X1
X3
X2
X4
X5
X6
Figure 5.1.
Gibbs sample from Pr(X1, X4|m, θ, X2, X3, X5, X6). The shaded vertices
are observed, and block inﬂuence between X1 and X4 eﬀectively improving the mixing
properties of the Gibbs sampler.
that this step is easy. From a computational point of view, model sam-
pling via MCMC is quite expensive.
The I-step consists of ﬁlling-in the unobserved part of the data. This
step calls a rather simple Gibbs sampler using a univariate Gibbs sampler
on a per record basis. The performance properties of this sub-MCMC
sampler depends heavily on the fraction of missing data, and also on the
model M at that point in time.
Recall from Section 3.2 that producing realisations in the I-step a-
mounts to drawing from a Gibbs sampler, where each draw is from a
univariate distribution with on the conditional side the variables of the
Markov blanket and a ﬁxed parameter. When missing items never occur
on the conditional side at any time in the process of drawing multivariate
realisations, then the convergence is immediate; the variables for which
no values have been observed, are fully separated from each other given
the remaining observed variables in the Markov blanket.
As an example consider the model m in ﬁgure 5.1. Suppose that for a
given record, the shaded vertices are observed, and that X1 and X4 are
missing. We require realisations from Pr(X1, X4|m, θ, X2, X3, X5, X6).
Using the univariate Gibbs sampler we draw according to the following
procedure:
X(t+1)
1
∼Pr(X1|X2, X3, x(t)
4 , X5, X6, m, θ) = Pr(X1|X2, X3, m, θ)
X(t+1)
4
∼Pr(X4|x(t+1)
1
, X2, X3, X5, X6, m, θ) = Pr(X4|X3, X5, X6, m, θ)
...

Learning from Incomplete Data
95
Because the shaded vertices form the Markov blanket of X1 and X4, the
two variables are independent of each other. The Gibbs sampler reaches
the invariant distribution in one single iteration. This means that the
pair (x(t+1)
1
, x(t+1)
4
) is a sample from Pr(X1, X4|X2, X3, X5, X6, m, θ). In
fact we have a perfectly mixing chain, since samples from t to t + 1 are
iid.
From the above discussion it follows that for records with only few
missing items, there are less interaction eﬀects between variables with
missing observations, and Gibbs sampling performs well. For records
with many missing values, there are potentially many more interactions,
and as a result the Gibbs sampler needs a longer burn-in period, and
may have poor mixing properties.
The performance of the sampler is also dependent on the model, be-
cause it is the model that determines the Markov blanket, and thus the
way variables depend on each other. A sparse DAG model has less inter-
actions that a more dense DAG model, and therefore the Gibbs sampler
performs better with the former than the latter.
In summary, the I-step may seem quite easy from an operational point
of view, but from a computational point of view producing imputations
via MCMC may be rather expensive.
We may conclude that each of the sub-MCMC samplers may be po-
tentially problematic in terms of eﬃciency.
3.4.2
Interdependence between samplers
The performance of each of the two sub-MCMC samplers is important,
but the dependence between the two sub-MCMC samplers also plays
a signiﬁcant role in the performance of MDA-P; afterall the two sub-
MCMC samplers are conditional on the output of each other.
Spending too much time at each sub-MCMC sampler is potentially
worthless if the overall chain is far from the invariant distribution; there
is no need to let either sub-MCMC sampler converge if the overall sam-
pler is still far from its target. The “classical” way of running interleaved
MCMC samplers, such as MDA-P, is to stay for some time at each of the
sub-MCMC samplers, and then “leave” the samplers, returning the last
realisation as the result of the draw. The reason is that although con-
vergence is not required every time, we don’t want to baby-step through
the state-space either. It is only reasonable to let both chains adapt a
bit to the ever changing value on the conditional side, model or impu-

96
EFFICIENT LEARNING OF BAYESIAN NETWORKS
tation, by giving the chain the opportunity to move slightly towards a
new region in the state space, before returning with the deﬁnite draw.
In our MDA-P context, the classical approach implies entering the
I-step conditional on the output of the last MP-step, staying for some
time at the I-step letting the chain adapt, and then return the last
imputation as the result. The MP-step is now entered conditional on
the imputation just returned from the I-step. We stay at the MP-step
for while letting the chain adapt to the new input, and then return the
last model as the result of the draw, and so on. Obviously, for MDA-P
this classical approach is computationally expensive, because both sub-
MCMC samplers are non-trivial MCMC samplers on their own. If we
opt for the alternative, i.e., create only one realisation at every step,
the overall sampler will mix extremely slowly, and there is hardly any
progress. In practice this makes MDA-P useless because the sampler will
get “lost in detail” and it is more prone to getting stuck in sub-optimal
regions of the state space for long periods of time.
In the classical approach, the realisations generated between entering
and leaving the sub-MCMC samplers are “wasted” in the sense that they
are not used by the other sub-MCMC sampler. Those samples serve
only as interim results for the ﬁnal “return value”. There is however an
important observation with regard to those interim samples produced:
the correlation between the realisations (imputations or models) means
that there is only a gradual improvement in the samples as time passes;
there is no clear threshold from which point in time samples may be
considered as being “correct samples”. The value returned by the sub-
MCMC samplers is therefore to some degree rather arbitrary. In fact,
there may be several of the interim draws that would be equally well
suited as the return value. As such these samples have not been exploited
to their full extent, which is unfortunate because we may have put a lot
of computational eﬀort into generating them. Moreover, many samples
may be suitable across several iterations of the MDA-P sampler because
MCMC works by step-wise exploration of the state space rather than
taking sudden jumps. If we use the MCMC “walking around the state
space” metaphor, we have that the chain within a certain time interval
visits the same states several times, before gradually wandering oﬀto
some other region; there are always several samples that are suitable for
a certain number of iterations.
Once we draw models in the MP-step, a lot of eﬀort has been put
into generating completions of the data in the I-step, before returning
the ﬁnal U (t) that acts as input for Pr(M|U (t), o). After returning this

Learning from Incomplete Data
97
last imputation as the ﬁnal result of the I-step, the intermediate results
are discarded, although they may be quite similar (in distribution that
is) to the realisations that would have produced for the next call of
the I-step given the next model. Any interim result may prove to be a
reasonable “return value” that could serve as input for the next MP-step,
especially because the models only diﬀer slightly between the MP-steps
due to correlation.
3.5
Imputation via importance sampling
The approach we propose in this section deals with the issue of “wasted
imputations” by re-using imputations generated the I-phase. We collect
and save the imputation samples we come across in a region that is cur-
rently being explored by the Markov chain, because these samples are
all “good” samples for a certain amount of time (iterations). Obviously
we can’t save all samples we come across, so only a few imputations are
saved. Using importance sampling we are able to use the same imputa-
tion for some time as the chain gradually moves away from the region to
explore other parts of the space. The proposed method allows us to run
MDA-P the classical way because we don’t have to generate several com-
pletions after each model draw anymore, but re-use many imputations.
This section is based on Riggelsen and Feelders, 2005.
3.5.1
The general idea
The general idea is based on the notion of a “population” of real-
isations, where each realisation is an imputation or completion of the
incomplete data drawn from the predictive distribution. To a realisa-
tion, a weight is attached that determines its importance compared to
the other realisations in the population. For model learning from incom-
plete data, the model changes dynamically, and therefore the weight of
each realisation in the population is re-evaluated such that the pop-
ulation at any time reﬂects the correct predictive distribution.
This
re-weighing allows us to reuse the same realisations as new models are
drawn.
Re-weighing is less expensive than producing new realisations all the
time. Combined with the fact that imputations may be useful for several
neighbouring models, it provides a way of alleviating the problem of
“wasted imputations”.

98
EFFICIENT LEARNING OF BAYESIAN NETWORKS
3.5.2
Importance sampling in the I-step—ISMDA-P
The I-step of MDA-P consist of drawing from Pr(U|o, M(t), ˆθ (t)),
where ˆΘ(t) = E[Θ|o, U(t)]. Instead of drawing from Pr(U|o, M(t), ˆθ(t)),
consider the sampling distribution Pr(U|o, M(t−q), ˆΘ(t−q)), q < t, i.e.,
the “correct” distribution at time t −q is used as the sampling distribu-
tion (the “wrong” distribution) at a later time t. We assume that the
distributions at time t and time t −q do not diﬀer substantially—more
on this in Section 3.5.3.
We create a “population” of empirical samples from the sampling dis-
tribution at time t −q, consisting of u(1), . . . , u(n). At time t, we want
to use these samples as if they were sampled from Pr(U|o, M(t), ˆΘ(t)),
and we therefore correct for the mismatch between realisations from
Pr(U|o, M(t), ˆΘ (t)) and Pr(U|o, M(t−q), ˆΘ (t−q)) via the following im-
portance weights:
wi =
Pr(u(i)|o, M(t), ˆΘ(t))
Pr(u(i)|o, M(t−q), ˆΘ(t−q))
The normalised importance weight for the ith draw becomes:
wi
W =
wi
n
r=1 wr
(5.7)
By ﬁlling in, we may write eq. 5.7 as:
wi
n
r=1 wr
=
Pr(o, u(i)|M(t), ˆΘ(t))/ Pr(o|M(t), ˆΘ(t))
Pr(o, u(i)|M(t−q), ˆΘ(t−q))/ Pr(o|M(t−q), ˆΘ(t−q))
n

r=1
Pr(o, u(r)|M(t), ˆΘ(t))/ Pr(o|M(t), ˆΘ(t))
Pr(o, u(r)|M(t−q), ˆΘ(t−q))/ Pr(o|M(t−q), ˆΘ(t−q))
=
Pr(o, u(i)|M(t), ˆΘ(t))/ Pr(o, u(i)|M(t−q), ˆΘ(t−q))
n
r=1 Pr(o, u(r)|M(t), ˆΘ(t))/ Pr(o, u(r)|M(t−q), ˆΘ(t−q))
It follows that we may reduce the importance weights to the likelihood
ratio:
wi =
Pr(o, u(i)|M(t), ˆΘ(t))
Pr(o, u(i)|M(t−q), ˆΘ(t−q))
(5.8)
We have a closed form for the likelihood of complete data given in eq. 3.1,
and therefore this ratio is easy to compute.
From Chapter 4, Section 1.1 it follows that we indeed approximate
Pr(U|o, M(t), ˆΘ (t)) for n →∞because Pr(U|o, M(t−q), ˆΘ (t−q)) > 0

Learning from Incomplete Data
99
whenever Pr(U|o, M(t), ˆΘ(t)) > 0 since Θ > 0, i.e., the parameter is
always strictly positive.
The sampling process, which we refer to as ISMDA-P (Importance
Sampling MDA-P), now proceeds as follows:
1 ISI-step: In the importance sampling I-step perform the following:
(a) If the distributions at time t −q and t diﬀer substantially, then
replace population by u(1), . . . , u(n) via Gibbs sampling U (i) ∼
Pr(U|o, m(t), ˆθ(t)), and compute ρi = Pr(o, u(i)|m(t), ˆθ(t)).
Otherwise reuse population (and likelihoods) from last ISI-step.
(b) For all u(i) compute the likelihood τi = Pr(o, u(i)|m(t), ˆθ(t)).
(c) Compute the importance weights wi = τi/ρi, and W = n
r=1 wr.
(d) With probability wi/W select u(i), i.e., select a completion ac-
cording to the normalised importance weights.
2 MP-step: In the model posterior step draw M(t+1) ∼Pr(M|o, u(i))
via MCMC, by providing the model m(t) as the seed. Also compute
ˆθ(t+1) = E[Θ|o, u(i)].
Theorem 5.3 ISMDA-P produces an ergodic Markov chain with invari-
ant distribution Pr(M|o) when in the ISI-step n →∞.
For the MP-step ISMDA-P is the same as for MDA-P. To test for
irreducibility in the ISI-step, we need to makes sure that all of ΩU can
be reached. Since both the numerator Pr(U|o, M(t−q), ˆΘ(t)) > 0 and
the denominator Pr(U|o, M(t−q), ˆΘ(t)) > 0, because Θ > 0, we have
that for n →∞, the entire ﬁnite state space is reached. Aperiodicity is
guaranteed because there is a non-zero probability of sampling the same
value again.
We emphasise that this result is quite uninteresting from a practical
point of view, since n is usually kept relatively small. Intuitively speak-
ing, we should just make sure that the n samples at any time represents
the distribution in “area” we are exploring with the sampler. More on
this in the next section.

100
EFFICIENT LEARNING OF BAYESIAN NETWORKS
M (t+1) ∼Pr(M|o, , m(t))
∼Pr(U|o, m(t−q), ˆθ
(t−q))
∼Pr(U|o, m(t), ˆθ
(t))
∼Pr(U|o, m(t+1), ˆθ
(t+1))
M (t+2) ∼Pr(M|o, , m(t+1))
∼Pr(U|o, m(t+2), ˆθ
(t+2))
∼Pr(U|o, m(t+3), ˆθ
(t+3))
M (t+3) ∼Pr(M|o, , m(t+2))
M (t+4) ∼Pr(M|o, , m(t+3))
u(1)
u(n)
. . .
. . .
. . .
. . .
. . .
Figure 5.2.
Population based re-weighing scheme. By way of importance sampling
the imputations are re-used for diﬀerent DAG models as time passes. A new popula-
tion of imputations is generated after some time.
Figure 5.2 illustrates the process. The dashed line in the top plot
depicts the sampling distribution at time t −q, and the solid line the
correct distribution at time t, i.e., the distribution from for which we
require samples. Initially n draws are made from Pr(U|o, m(t−q), ˆθ(t−q)),
and they are assigned an importance weight (depicted as a solid circle
indicating mass). Depending on the weight, u(i) is selected, and the
complete data (o, u(i)) is used for drawing a model at time t + 1. This
model gives rise to the middle plot, which is the new distribution from
which we require samples. We reuse the samples from time t −q from
the ﬁrst plot, and only compute the new importance weights, draw a
completion, and draw a model at time t + 2. After having drawn that
model, a new population is sampled from Pr(U|o, m(t+2), ˆθ(t+2)), and
the process continues: a model is drawn at time t + 3 giving rise to the
distribution Pr(U|o, m(t+3), ˆθ(t+3)). This distribution is approximated
by using the population that was sampled at time t−2. A completion is

Learning from Incomplete Data
101
selected depending on the importance weights, and ﬁnally at time t + 4
a model is drawn again.
3.5.3
Generating new population vs. re-weighing
Since ISMDA-P employs importance sampling, the remarks made in
Chapter 4, Section 1.1.1 apply here as well. The population together
with the importance weights is the empirical evidence we have of the
distribution from which we require realisations. Depending on the im-
portance weights, the population may be an extremely crude approxi-
mation. As discussed in Chapter 4, Section 1.1.1, the similarity between
the sampling distribution and the distribution from which we want em-
pirical samples is crucial for a decent approximation. This is exactly
why we require that the distributions at time t −q and t, t > q should
be “close”. For large q, the diﬀerence may be substantial, in which case
it is insuﬃcient to merely re-weigh the population, as none of its mem-
bers may lie in a region of mass, but in some tail region. In that case a
new population should be generated, after which it for some time again
suﬃces to re-weigh the population.
The question is then how we should monitor the population and di-
agnose a “dying” one. The following alternatives seem plausible:
Deﬁne a similarity measure between models δ(M1, M2) in terms of
the edges of the DAG models. If the diﬀerence between M1 and M2
is above a certain threshold then the number of (in)dependences may
be too large, and the distributions may be very dissimilar.
Monitor the variance of the normalised importance weights. If the
variance becomes too large, it indicates that only few weights “carry
the burden”.
Generate a new population for every qth iteration, i.e., cycle between
the ISI-step and the MP-step q times, and then create a new pop-
ulation. Due to the nature of MCMC samplers, it may be a good
idea to change q dynamically. For instance, in the burn-in phase the
population should be renewed more often.
We adopt the last option, because it is easy to implement in practice.
This simple approach in general means that the sampler has to be run
a couple of times such that q can be chosen empirically.
It should be stressed that even when a population does not seem
to be dying according to any diagnostic, there is still a need to renew
the population once in while.
Eventually the completions have been

102
EFFICIENT LEARNING OF BAYESIAN NETWORKS
exploited exhaustively, and in order to reach the entire space of possible
completions, the space should also be explored, and new realisations
should be generated. If the samples are not renewed we risk that the
ISMDA-P sampler becomes reducible, because certain regions of the
state space are never considered.
3.5.4
The marginal likelihood as predictive distribution
As we saw in Chapter 3, Section 3.2.1, the marginal likelihood plays a
central role in model learning within the Bayesian approach. For model
learning the intended reading of Pr(o, U|M) is “the marginal likelihood
of M given o, U”.
However, as it stands, the marginal likelihood is
precisely the probability of (o, U) given model M.
This means that
given M, we can measure how well a given data sample adheres to the
(in)dependences that M encodes. This measure is a probability that
depends solely on the (in)dependence assumptions between the variables
in X, but is independent of the “strengths” that these (in)dependences
have, i.e., Θ is irrelevant.
Hence, if we have a method that produces data sets, we can measure
how probable this sample is given M. In particular, because o is given
already, we need only ﬁll in U to produce a full data sample, and the
overall probability of the augmented sample is Pr(o, u|M). The prob-
ability of u only given M is obtained by normalisation over possible
assignments to U.
The above discussion gives rise to an alternative to the ISMDA-P
sampling scheme presented in Section 3.5.2. Up until now the distribu-
tion in the ISI-step was approximated with importance sampling using
a sampling distribution from an earlier iteration. The approximation
is of Pr(U|o, M(t), ˆΘ(t)), i.e., a distribution conditional on Θ. Via the
importance weights given in eq. 5.8, we need not compute the actual
probability of the imputations, i.e., Pr(u(i)|o, M(t), ˆΘ(t)) but it suﬃces
to compute Pr(u(i), o|M(t), ˆΘ(t)). This probability can be interpreted
as a measure of imputation quality given the current ˆΘ (t). However,
we actually have a closed expression for imputation quality uncondi-
tional on Θ, namely the marginal likelihood Pr(U (i), o|M(t)). By plug-
ging in Pr(u(i), o|M(t)) as the numerator in eq. 5.8, we approximate
Pr(U|o, M(t)) at every ISI-step:
wi =
Pr(o, u(i)|M(t))
Pr(o, u(i)|M(t−q), ˆΘ(t−q))

Learning from Incomplete Data
103
This means that we eﬀectively skip many intermediate approximations
of Pr(U|o, M(t), ˆΘ (t)) “on our way to” to Pr(U|o, M(t)). Instead we
approximate Pr(U|o, M(t)) directly at every iteration. In other words,
we are now actually applying the following Gibbs sampler:
U (t+1)
∼
Pr(U|o, m(t))
M(t+1)
∼
Pr(M|o, u(t+1))
...
where Pr(U|o, M(t)) is approximated via importance sampling.
Theorem 5.4 ISMDA-P with the marginal likelihood in the ISI-step
produces an ergodic Markov chain with invariant distribution Pr(M|o)
when in the ISI-step n →∞.
Since Pr(U, o|M) > 0 we may use the same reasoning as we did
for ISMDA-P. However, we stress again that from a practical point of
view the signiﬁcance of this theorem is less interesting because n is kept
relatively small in practice.
3.5.5
The eMC4 sampler
Following the tradition of the MCMC model sampling algorithms for
complete data, called (e)MC3, we refer to marginal likelihood version of
ISMDA-P as (enhanced) Markov Chain Monte Carlo Model Composition
with Missing Components, (e)MC4.
Algorithm 2 contains the pseudocode of the sampler. Lines 5–9 cre-
ate a new population when required, by drawing from the current model
and the current expected parameter value. Also the likelihood is com-
puted and saved. In lines 10–14 the completions are evaluated via the
marginal likelihood using the current model, and the importance weights
are computed and normalised. In line 15 a completion is selected from
the population depending on its importance weight. The selected com-
pletion is used in conjunction with the observed part of the data to call
an MCMC model sampler in line 16, where a new model is drawn. The
initial seed for the MCMC model sampler is the last model at time t.
Finally in line 17, the model is saved as a realisation from Pr(M|O).
There is one important point about line 12 that we would like to em-
phasise. At ﬁrst sight it may seem rather expensive to even compute τi
for each imputation U (i) at every iteration. However, since M(t−1) and
M(t) are correlated, they diﬀer in only a few adjacencies. This means

104
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Algorithm 2: eMC4
Input : n, population size (no. of completions kept).
Onput: DAG models from Pr(M|o) (requires burn-in).
m(0) ←G = (X = {X1, . . . , Xp}, E = ∅)
1
u(0) ←valid random completion
2
r ←0
3
for t ←0 to ∞do
4
/* MCMC draw imputations (population)
*/
if population-is-dead or exhaustively-exploited then
5
ˆθ ←E[Θ|o, u(r)]
6
for i ←1 to n do
7
draw U (i) ∼Pr(U|o, m(t), ˆθ)
8
ρi ←Pr(u(i), o|m(t), ˆθ)
9
/* Compute importance weights
*/
W ←0
10
for i ←1 to n do
11
τi ←Pr(o, u(i)|m(t))
12
wi ←τi/ρi
13
W ←W + wi
14
draw R ∼Pr(i) ←wi/W
15
/* Few MCMC iterations, return last as M(t+1)
*/
draw M(t+1) ∼Pr(M|o, u(r); m(t)) /* Seed with m(t)
*/
16
record(m(t+1))
17
that most vertices have the same parent sets. Since τi (the marginal like-
lihood) decomposes, we only need to re-compute the imputation quality
score for the vertices that have diﬀerent parent sets.
Hence, for all
i = 1, . . . , n, τi = 	p
j=1 τi(Xj), save between iterations the imputation
quality score τi(Xj) for each vertex. Only when the entire population is
renewed τi needs to be computed for every vertex.
3.5.6
Evaluation—proof of concept
In this section we perform a small experimental evaluation of eMC4
and brieﬂy discuss the results.
We used a data set from Edwards and Havr´anek, 1985 about proba-
ble risk factors of coronary heart disease. The data set consists of 1841

Learning from Incomplete Data
105

	
F

	
B


	
E



	
D

	
C


	
A



	
F

	
B

	
E


	
D

	
C

	
A
Figure 5.3.
Generating model. Left: DAG model. Right: essential graph model.

	
RF

	
B










	
RE

	
D



	
RC

	
RA

	
RF

	
RB

	
RE

	
RD

	
C
	


	
A
	









Figure 5.4.
Missing data mechanisms. Left: mechanism 1. Right: mechanism 2.
records and 6 binary variables, A: smoking, B: strenuous mental work,
C: strenuous physical work, D: blood pressure under 140, E: ratio
β to α proteins less than 3, F: family history of coronary heart dis-
ease. Because several DAGs encode the same set of assumptions about
independence, we depict results as essential graphs.
Based on an eMC3 run using the 1841 complete records, the model
on the left in ﬁgure 5.3 is a highly probable model (although edge F −B
is not strongly supported). The model on the right is the corresponding
essential graph of the DAG. The parameter corresponding to the DAG
model was determined based on the aforementioned data set, and 1800
new records were sampled from the BN.
Incomplete sets were generated by applying missingness mechanism
one in ﬁgure 5.4 on the complete sample. This graph explicitly deﬁnes
how response Ri of variable i depends on observed variables. Since for all
Xi, Ri only depends on completely observed variables, the missingness
mechanism is clearly ignorable. Three incomplete sets were generated
with 5–10%, 10–15% and 15–20% missing components. The probability
of non-response of variable Xi conditional on a parent conﬁguration of
Ri was selected from the speciﬁed interval.
On the basis of the generating model and the missingness mechanism,
we would expect the following results. Since response of C only depends
on B and the association C −B is strong, a big fraction of components
can be deleted for C without destroying support for the edge in the
data. Association D −E is also strong so discarding components for E

106
EFFICIENT LEARNING OF BAYESIAN NETWORKS
will probably not have a major impact on the edge either. Association
E −A is inﬂuenced by B and D because the response is determined by
those variables. Values for E and A may be absent often and therefore
information about the association might have changed. This may also
be the case for the edges C −E and C −A.
We ran eMC4 using each incomplete data set. For drawing a model
from the model posterior, 150 model Metropolis-Hastings steps were
performed, and the last model was returned as the ﬁnal result of the
draw. The size of the population was set to 25, and it was refreshed,
i.e., old population discarded and new one drawn, after every 40th model
drawn, i.e., q = 40.
In ﬁgure 5.5 the top four models are depicted along with their sam-
pling frequencies. Notice the presence of the strong associations C −B
and D −E everywhere, as expected. When the fraction of missing com-
ponents for two associated variables increases it has a big impact on
the support of such an association. Indeed, from the ﬁgure we see that
the support for associations between variables A, C and E has changed.
The sample frequencies and the number of visited models also suggest
that the variance of the posterior distribution becomes bigger when more
components are deleted. There is no longer a pronounced ‘best’ model.
The plot in ﬁgure 5.6 shows this more clearly. Here the cumulative
frequencies are plotted against models (sorted on frequency in descend-
ing order). A steep plot indicates a small variance. For complete data
the 10 best models account for 90% of the distribution whereas for 15–
20% missing components only 50% of the distribution is accounted for
by the best 10 models.
To investigate the similarity of the models between the three incom-
plete sets, we used eq. 4.12, where Δ(M) is set to 1 when there is an
edge between two vertices of interest in M. This results in the expected
probability of the presence of edges as seen in ﬁgure 5.6, bottom. We
can see, as we would expect, that the distance between points of com-
plete data and incomplete data is dependent on the fraction of missing
components. Diamonds (15–20%) have the biggest distance to triangles
(complete), and pluses (5–10%) the smallest.
As we saw for mechanism one, discarding components for two associ-
ated variables can have a big impact on the presence of the corresponding
edge in sampled models. For strongly associated variables the impact is
less pronounced. We created another incomplete data set using mech-
anism two in ﬁgure 5.4. For the associated variables C, E and A the
mechanism only discards components that we think will not severely

Learning from Incomplete Data
107
F
B
E

D
C
A
27%
F
B
E

D
C
A
21%
F
B
E
D
C
A
9%
F
B
E

D
C
A
9%
Complete (total 60 models visited)
F
B
E

D
C
A
25%
F
B
E

D
C
A
11%
F
B
E

D
C
A
7%
F
B
E

D
C
A
5%
Mechanism 1: 5–10% missing (total 137 models visited)
F
B
E

D
C
A
18%
F
B
E

D
C
A
7%
F
B
E

D
C
A
6%
F
B
E

D
C
A
5%
Mechanism 1: 10–15% missing (total 168 models visited)
F
B
E
D
C
A
7%
F
B
E
D
C
A
6%
F
B
E

D
C
A
5%
F
B
E
D
C
A

5%
Mechanism 1: 15–20% missing (total 188 models visited)
F
B
E
D
C
A
12%
F
B
E
D
C
A
12%
F
B
E

D
C
A
9%
F
B
E
D
C
A
8%
Mechanism 2: 20–30% missing (total 152 models visited)
Figure 5.5.
Top four visited models and sampling frequencies. Note that all edges
are reversible.
impact these associations. For the strong association E −D discarding
components on both should not matter. We expect that we are able to
remove a substantial fraction of components and still obtain reasonable
models. We selected the fraction of missing components in the interval
20–30%. In the last row in ﬁgure 5.5 we see that although a substan-
tial fraction of components were deleted, the models learned are quite
similar to the models from the complete set.
To illustrate that it is not the fraction of missing components that
determines the variance but rather the fraction of missing information
(Little and Rubin, 1987), we plotted the cumulative frequency in ﬁg-
ure 5.6, top. The variance of the posterior distribution is similar to the
variance of the posterior for mechanism one with 5–10% missing com-
ponents. This means that although the fraction of missing components

108
EFFICIENT LEARNING OF BAYESIAN NETWORKS
0
10
20
30
40
50
60
0.0
0.2
0.4
0.6
0.8
1.0
Complete
Mech1 5-10
Mech2 20-30
Mech1 10-15
Mech1 15-20
B-F
E-F
C-F
A-F
D-F
E-B
C-B
A-B
D-B
C-E
A-E
D-E
A-C
D-C
D-A
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1.0
Complete
Mech1 5-10
Mech1 10-15
Mech1 15-20
Figure 5.6.
Top: cumulative frequencies. Bottom: expected probability of edges.
is much higher than 5–10%, the uncertainty due to missing data has not
changed substantially.
Finally we compare eMC4 in a model selection setting. The four most
probable models in ﬁgure 5.5 would be the models selected as the MAP
of Pr(M|o). For the data sets with 5–10%, 10–15% and 15–20% missing
components, we ran the SEM implementation by Friedman and Elidan,
2005. The SEM-algorithm was run with default parameters, except for
the ESS which we set to 1 (the default is 5). In ﬁgure 5.7 the models
selected are shown.

Learning from Incomplete Data
109
F
B
E

D
C
A
5–10%
F
B
E

D
C
A
10–15%
F
B
E
D
C
A
15–20%
Figure 5.7.
The models selected with the SEM-algorithm.
For the two data sets with 5–10% and 10–15% missing components,
the models selected coincide with the two most frequently sampled mod-
els using eMC4. For 15–20% missing components, SEM selects the 2nd
most frequently sampled model. We see that the top 4 most probable
models are visited almost equally often using eMC4, and SEM selects
an arbitrary one. This nicely illustrates the added beneﬁt of taking a
Bayesian approach, since we now get an impression of the structural
diﬀerences between models that score almost equally well.
3.5.7
Conclusion
To our knowledge eMC4 is the ﬁrst and only Bayesian approach to
learning Bayesian network models from incomplete data. The reason for
this we believe is that a Bayesian approach via “straightforward” MCMC
simulation is computationally extremely expensive. An even larger ob-
stacle is that it is in practice almost impossible to get such MCMC sam-
plers to converge and mix properly. Even with a more clever MCMC
approach such as eMC4, it is still relatively expensive to sample models
from the posterior model distribution, but the mixing is improved.
Valuable insight is gained when sampling models from the posterior
model distribution; an illustration of the kind of information one can
derive from posterior realisations is given in the previous section.
A
posterior distribution is more informative than just a single model. This
is especially true in the case of incomplete data, since the increased un-
certainty due to missing data is reﬂected in the probability distribution.
The question is then if the insight a Bayesian statistical approach oﬀers
is “worth the eﬀort” from computational point of view.
In the context of parameter learning of BNs, a related approach to
eMC4 can be employed, but then for a ﬁxed DAG model. This means
that a broad range of approximate predictive distributions can be used,
and the “error” that the approximation introduces is corrected for by
importance sampling in the ISI-step. We refer to Riggelsen, 2006b for
details on this.

110
EFFICIENT LEARNING OF BAYESIAN NETWORKS
4.
Ad-hoc and heuristic methods
There are several techniques for handling the problem of missing data
that are less principled in nature; see for instance Allison, 2001. The
methods are often ad-hoc, and for several techniques it is not evident
what is actually gained or lost by applying them without a thorough
analysis of the assumptions underlying the technique. Sometimes ad-
hoc methods may provide a good enough approximation of what would
have been obtained by a principled analysis.
A major problem with
many ad-hoc techniques is that they fail to preserve the dependence
relationship between the variables. It is not enough to focus only on
the “column” for which data items are missing in order to predict the
unobserved items. This in general is the easiest thing to do, but the
multivariate relationship among variables is then distorted and severe
bias may be the result.
There may be several reasons as to why heuristics or ad-hoc ap-
proaches are used. The methods and algorithms discussed so far are it-
erative in nature, they require several passes through the data. At every
iteration, an ever “better” predictive distribution is used for obtaining
the suﬃcient statistics corresponding to completions of the incomplete
data. Generating these completions either requires exact inference in
a BN or application of a Gibbs sampler, which generally speaking is
considered computationally demanding. Most heuristic and ad-hoc ap-
proaches require far less passes, because they are generally not based on
the notion of “reﬁning” a predictive distribution such that the predic-
tions become more and more “correct”.
If for some reason it is undesirable to traverse the data too often, we
probably also have to sacriﬁce a Bayesian approach to data analysis,
because a Bayesian approach normally implies many traversals. This
is not as bad as it seems because for large amounts of data, posteriors
will approximate Normal distributions (Carlin and Louis, 2000) anyway.
In that case we may settle for summary statistics, such as the MAP or
the expectation. This means that model selection becomes interesting
again.
Most ad-hoc approaches exactly produce summary statistics,
albeit approximate ones.
4.1
Available cases analysis
In Section 1.1 we brieﬂy mentioned the very simple approach referred
to as complete case analysis, which simply deletes the cases in d that
contain missing values. This approach is only valid, albeit ineﬃcient,

Learning from Incomplete Data
111
if the missing data mechanism is MCAR (Allison, 2001; Brand, 1999).
A slight variation on this theme called available cases analysis takes
advantage of the fact that diﬀerent parameters require diﬀerent data
items in d. For estimating θXi|Xpa(i) only variable Xi and the variables
in the parent set Xpa(i) are needed. The estimation is then performed by
considering cases for which these variables are observed simultaneously,
and the remaining cases are discarded (hence also assumes MCAR). This
is done for all parameters of the BN. The advantage of this approach is
that less information is discarded than when entire records are neglected.
Once the parameter has been estimated as ˆθ, the predictive distri-
bution becomes Pr(U|o, m, ˆθ). Using this distribution, the incomplete
data can be completed. Alternatively, if it is only the parameter of the
BN that is required, we can use ˆθ directly as a rough approximation of
θ.
4.2
Bound and Collapse—BC
The Bound and Collapse algorithm (BC) (Ramoni and Sebastiani,
1997a; Ramoni and Sebastiani, 1997b) is a method claimed to be able
to return not necessarily the ML estimate but still qualiﬁed guesses as
to what the parameter of a BN could be. In that sense BC is not really
a valid statistical method of learning the BN parameter. The algorithm
is a deterministic non-iterative two-phase process: The bound phase
calculates the extreme bounds for the suﬃcient statistics consistent with
the data sample. On the basis on these statistics, bound parameters are
easily calculated (they are simply a function of the bound statistics).
The collapse phase is a convex combination of these extreme parameters
(for a given child/parents conﬁguration) where the coeﬃcients for the
bounds are based on an available cases analysis.
The suﬃcient statistics n(xi, xpa(i)) is the number of occurrences of
the conﬁguration xi, xpa(i) in the data. When dealing with incomplete
data, this function is not deﬁned for cases with unknown entries either
because Xi is missing, Xpa(i) is (perhaps partly) missing or a combina-
tion of the two.
We now re-deﬁned n(xi, xpa(i)) to pertain to that part of the incom-
plete sample, where Xi, Xpa(i) is fully observed.
For the other part,
where Xi, Xpa(i) is not fully observed, we deﬁne ˜n(xi, xpa(i)). This func-
tion counts the records that can be completed such that they match
the conﬁguration xi, xpa(i). Hence, ˜n(xi, xpa(i)) returns the number of
cases that are consistent with xi, xpa(i), neglecting the cases for which

112
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Xi, Xpa(i) are fully observed. We deﬁne ˜n(xpa(i)|xi) in a similar fashion,
but the bar means that we ﬁx Xi at xi; i.e., consider those records with
incomplete observation for Xpa(i) that are consistent with xpa(i) given
that Xi is observed and has value xi.
In order to ﬁnd the ML estimates for the multinomial distribution
we need only the suﬃcient statistics.
Thus we can ﬁnd the upper
bound ML estimate for θxi|xpa(i), called θ•
xi|xpa(i), by using the statis-
tics ˜n(xi, xpa(i)) + n(xi, xpa(i)). This estimate is simply:
θ•
xi|xpa(i) = n(xi, xpa(i)) + ˜n(xi, xpa(i))
n(xpa(i)) + ˜n(xi, xpa(i))
(5.9)
which gives us the parameter when all incomplete cases consistent with
xi, xpa(i) are indeed assigned this conﬁguration.
In Ramoni and Sebastiani, 2001 the lower bound ML estimate called
θ•xi|xpa(i) is deﬁned such that the number of completions ascribed to
xpa(i) is maximal, without increasing the frequency of the conﬁguration
xi, xpa(i), i.e., for all records complete xpa(i) except for records where Xi
is observed as xi:
θ•xi|xpa(i) =
n(xi, xpa(i))
n(xpa(i)) + ˜n(xpa(i)) −˜n(xpa(i)|xi)
(5.10)
The actual estimate ˆθxi|xpa(i) lies somewhere between the upper bound
θ•
xi|xpa(i) and the lower bound θ•xi|xpa(i).
We collapse the bounds in
eq. 5.9 and eq. 5.10 into one single estimate:
ˆθxi|xpa(i) = (1 −v(xi)) · θ•xi|xpa(i) + v(xi) · θ•
xi|xpa(i)
The ﬁrst term corresponds to any completion of Xi other than xi, and
the second term to the completion xi. The weight v(·) determines the
exact location between the bounds (see bellow).
In Ramoni and Sebastiani, 1998 several lower bounds are deﬁned for
one upper bound. The lower bounds occur when the number of comple-
tions ascribed to yi, xpa(i), yi ̸= xi is maximal, i.e., for each completion
yi of Xi other than xi:
θyi•xi|xpa(i) =
n(xi, xpa(i))
n(xpa(i)) + ˜n(yi, xpa(i))
(5.11)

Learning from Incomplete Data
113
The collapse is deﬁned as a convex combination of all the lower bounds
and the upper bound:
ˆθxi|xpa(i) =

yi̸=xi
v(yi, xpa(i)) · θyi•xi|xpa(i) + v(xi, xpa(i)) · θ•
xi|xpa(i)
The observed part of the data is considered a “representative” sample
of the entire data. The weights v(·) is the relative (conditional) frequency
of some conﬁguration v(xi, xpa(i)) = n(xi, xpa(i))/ 
yi n(yi, xpa(i)) only
considering cases where xi, xpa(i) is fully observed. For the unconditional
version, Xpa(i) is empty.
If we want to use BC in a Bayesian setting, we have to account for
prior counts α. This is done by adding the corresponding prior counts
to the numerator and denominator of eq. 5.9, eq. 5.10 and eq. 5.11. This
results in the expected parameter bounds rather than the ML parameter
bounds.
In summary, BC consists of the following two steps:
1 B-phase: In the bound phase, the bounds of the parameter estimates
are computed:
(a) The upper bound is the maximum number of records consistent
with xi, xpa(i).
(b) The lower bound(s) depend(s) on the number of records that are
consistent with yi, xpa(i), yi ̸= xi. Either each completion with
yi ̸= xi can be regarded as a lower bound, or a single lower bound
can be deﬁned, where the maximum number of records that can
be ascribed to xpa(i) is counted excluding those records for which
Xi is observed with value xi.
2 C-phase: In the collapse phase, determine the precise location of the
parameter estimate between the lower and upper parameter estimate
bounds. The collapse is based on the records for which v(xi, xpa(i))
is fully observed.
BC is not an iterative procedure, and diﬀers radically from EM and
the MCMC methods previously discussed. The estimates found are not
ML estimates, but are based on an ad-hoc approach that from an intu-
itive point of view is attractive.

114
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Essentially BC is a sophisticated available cases analysis, where at-
tention is restricted to include only the child/parents pertaining to a
particular parameter. Like available cases analysis, there is no predic-
tive element involved in the process of obtaining the parameters with
BC.
The interval obtained in the B-phase of BC may on its own be quite
valuable. There is however no distribution or function deﬁned on that
interval that indicates the most probable area(s) within this interval
that could form the basis of a parameter point estimate. The collapse
phase is based on the frequency of Xi (and for the conditional collapse
also on Xpa(i)), estimated by taking only those records of the data into
consideration where the variables are observed. When the weights in
the collapse phase are based on only the marginal distribution of the
child variable, i.e., by v(xi), it is questionable if this provides enough
information to collapse the intervals pertaining to all the parameters
θxi|Xpa(i) irrespective of the conﬁguration assigned to Xpa(i). Moreover,
in determining v(·) the underlying assumption is that the data is missing
according to the MCAR assumption.
When this does not hold, the
collapse phase may become unreliable.
BC seems works for some missing data mechanisms, but unfortunately
due to the points mentioned above, it can produce unpredictable results
for others. Suppose that we want to estimate θxi|xpa(i) from incomplete
data, under the quite realistic MAR missing data mechanism where Ri
depends on Xch(i). BC may perform bad in this situation. The reason
is that BC neglects any information beyond Xi and Xpa(i), which in this
situation is required to determine ˆθxi|xpa(i).
A desirable property of any incomplete data method, is that for equiv-
alent DAG models, the same results should be expected. For available
cases analysis and BC this does not hold.
4.3
Markov Blanket Predictor—MBP
Like BC, the method we describe in this section is a non-iterative
procedure. To some degree the method resembles BC, but on crucial
points it diﬀers; most notably, we approach the missing data problem
from a prediction point of view. Moreover, for equivalent models, the
same results are obtained. This section is based on Riggelsen, 2006a.

Learning from Incomplete Data
115
4.3.1
The general idea
The basic idea behind our method, is to predict missing values on a
per variable basis, by focusing only on those observed variables that di-
rectly inﬂuence the variables that are missing. The BN model explicitly
identiﬁes the relevant predictor variables as those variables that form
the Markov blanket of a given variable. By looking for cases in the data
that have similar values on the observed predictor variables, a predic-
tive distribution for the missing data is created. When the number of
predictors is too large, a selection is made, and only the best predictors
(those that have the strongest inﬂuence) are considered when generating
the predictive distribution.
4.3.2
Approximate predictive distributions
In this section we propose an approximation of the predictive distri-
bution. The way the approximation is deﬁned, leads to a non-iterative
algorithm in a natural way. Additionally, there is no need to perform
exact inference as in EM, or draw several realisations as in DA and its
derivatives.
We assume that for any record l the following factorisation holds:
Pr(U l|ol, M) =
r(l)

k=1
Pr(Ul
k|ol, m)
(5.12)
That is, we are able to predict single missing values independently of
each other through separate predictive distributions.
If the indepen-
dence assumptions expressed in eq. 5.12 actually do hold, then the de-
pendence of Ul
k on ol is in fact only on a subset of the observed variables,
namely the Markov blanket of Ul
k.
If Xl
i = Ul
k, we can thus predict it if all variables in the Markov
blanket are observed:
Pr(Xl
i|ol, m) = Pr(Xl
i|xl
mb(i), m)
If not all variables of the Markov blanket are observed, then there are
other observed variables outside the blanket that inﬂuence the predic-
tion; this was illustrated in the context of Gibbs sampling in Section
3.4.1, by ﬁgure 5.1. We return to this issue in Section 4.3.4, and from
now on re-deﬁne the approximate predictive distribution for record l to

116
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Xi
Xi
Figure 5.8.
New model m′ derived from m. All variables in the Markov blanket of
Xi in m (left) are directed towards Xi in m′ (right).
be:
Pr(U l|ol, m) def
=
r(l)

k=1
Pr(Ul
k|xl
mb(k), m)
where Pr(Xi|Xmb(i), m) is the so-called univariate predictive distribution
for Xi.
The BN decomposition of this predictive distribution according to m
is then:
Pr(Xi|Xmb(i), m) ∝ˆθXi|Xpa(i)

j∈ch(i)
ˆθXj|Xpa(j)
(5.13)
Hence, in order to predict the missing value, we need ﬁrst compute the
estimates ˆθXj|Xpa(j). To avoid iteration, we could instead do available
cases analysis, or apply BC for that purpose. The problem with these
approaches is that they in one way or another only consider observations
in the data pertaining directly to Xj and Xpa(j); they neglect the fact
that other variables also inﬂuence the estimation when Xj or Xpa(j)
are (partly) missing. Disregarding this inﬂuence eﬀectively means that
valuable information is discarded that otherwise could help in making a
better and perhaps unbiased prediction of Xi via eq. 5.13.
We propose to change the model m such that the parametrisation
of the predictive distribution of Xi remains dependent on the Markov
blanket of Xi, but in a way that predictions are less sensitive to slightly
inaccurate parameter estimates.
Given the variables in the Markov blanket of m, the univariate predic-
tive distribution based on this new model m′ captures no extra assump-
tion about independence compared to the predictive distribution based
on m. Speciﬁcally, deﬁne m′ such that Xm′
pa(i) = Xm
mb(i) and Xm′
ch(i) = ∅,

Learning from Incomplete Data
117
i.e., extend the parent set of Xi to include all variables of the Markov
blanket of Xi and remove any children. Figure 5.8 illustrates m′ derived
from m. We note that an m′ is deﬁned for each univariate predictive dis-
tribution; m′ is only concerned with Xi and the variables in Xmb(i) (of
m). The independences actually holding in m, are disregarded without
further ado for predicting missing values:
Pr(Xi|Xpa(i), m′)
=
θm′
Xi|Xpa(i)
(5.14)
≤CI
Pr(Xi|Xmb(i), m) ∝ˆθm
Xi|Xpa(i)

j∈ch(i)
ˆθm
Xj|Xpa(j)
Here ≤CI means that the distribution on the left is less restrictive in
its conditional independence assumptions compared to the distribution
on the right.
Obtaining ˆθm′
Xi|Xpa(i) is now done in a way related to available cases
analysis, but all variables highly relevant for determining this parameter
of the predictive distribution are considered jointly. In m′ we thus ex-
plicitly consider all relevant variables for the parametrisation together,
in contrast to m where separate child-parent variables are considered for
estimating the parameters required according to decomposition m.
In a sense m′ allows for a rather direct prediction approach; there is no
need to perform inference because the most relevant variables for making
a prediction are considered jointly. For the predictive distribution based
on m, it is indirectly assumed that the correct parametrisation of the
actual BN is given prior to predicting the missing values; it is by means
of (simple) inference that the missing values are predicted, that is, by
applying eq. 5.13. However, the only way of estimating these parameters
accurately is to have knowledge about the missing values, but of course
these missing values are exactly what is subject to prediction.
Note that across equivalent DAG models, vertices have the same
Markov blankets, i.e., the Markov blanket of vertices does not change
for equivalent DAG models. Thus m′ is unique across equivalent DAG
models. This entails that the set of predictors is the same no matter
which DAG model is considered within an equivalence class. From a sta-
tistical point of view this is reassuring, since we explicitly do not want
to be able to distinguish predictions based on equivalent DAG models.
4.3.3
Parameter estimation
For the parametrisation of the predictive distribution, a similar cases
approach is used, based on cases where Xi is observed.

118
EFFICIENT LEARNING OF BAYESIAN NETWORKS
As we did for BC, we re-deﬁne n(xi, xpa(i)) to pertain to that part of
the data where Xi, Xpa(i) is fully observed. Hence this function counts
the number of occurrences of xi, xpa(i) in o where child and parents are
observed. For the part where Xi is observed but Xpa(i) is only partially
observed, we deﬁne ˜n(xpa(i)|xi). This function is deﬁned in terms of
match(xl
pa(i)|xpa(i)) capturing the degree of match between xl
pa(i) and
xpa(i), when Xl
pa(i) is only partially observed in record l.
If the ob-
served parents all match with the corresponding values in xpa(i), then it
returns the fraction 1/number-of-possible-conﬁgurations of the missing
parent(s). Formally match(xl
pa(i)|xpa(i)) is deﬁned as:
⎧
⎪
⎨
⎪
⎩
1
|ΩX
l
pa(i)∩U
l|
if

(xpa(i) ∩ol) = (xl
pa(i) ∩ol)

∧

|ΩX
l
pa(i)∩U
l| > 0

0
otherwise
The function ˜n(xpa(i)|xi) is now deﬁned in the following way:
˜n(xpa(i)|xi) =
c

l=1
match(xl
pa(i)|xpa(i)) · I(xl
i = xi)
Also we deﬁne the following total counts:
s∗(xi, xpa(i)) = n(xi, xpa(i)) + ˜n(xpa(i)|xi)
(5.15)
As an example, assume that we have a model {X2, X3, X4} →X1
with binary variables, and that the following data is given:
X1
X2
X3
X4
d1
0
?
1
?
d2
?
0
1
0
d3
1
?
0
1
d4
1
1
0
1
Suppose we require s∗(X1 = 1, Xpa(1) = {X2 = 1, X3 = 0, X4 = 1}).
Record 4 is fully observed, and results in a count of 1 returned by n(·).
Any record with X1 = 1 is now considered, and all those records are
counted for which the parent set is incomplete, but can be be consistently
completed as Xpa(1) = {X2 = 1, X3 = 0, X4 = 1}. In record 3 there is a
partial match with the required parent set conﬁguration, and since X2
has two possible values, 0.5 is returned as the degree of match by ˜n(·).
This means that s∗(X1 = 1, Xpa(1) = {X2 = 1, X3 = 0, X4 = 1}) = 1.5.

Learning from Incomplete Data
119
For s∗(X1 = 0, Xpa(1) = {X2 = 1, X3 = 1, X4 = 0}) only record
1 is relevant, and since X2 and X4 are missing, there are 4 possible
completions, so s∗(X1 = 0, Xpa(1) = {X2 = 1, X3 = 1, X4 = 0}) = 0.25.
Using the degree of a match, several possible conﬁgurations “share
the single count” between them which is only fair given the fact that
the missing values could have been any of the conﬁgurations—the sin-
gle count is uniformly spread over all possible conﬁgurations.
When
there is a perfect match, n(·) returns a full count, and the other parent
conﬁgurations receive no share at all.
Gathering the suﬃcient statistics based on (a subset of) similar ob-
servations from the observed records is valid for a broad range of MAR
mechanisms, but is not necessarily optimal for arbitrary missing data
mechanisms. In contrast to BC and available case analysis, the decom-
position according to m′ is less sensitive to the actual underlying MAR
mechanism because of the larger dependence components captured by
considering jointly all variables of the Markov blanket of m.
Obtaining the statistics s∗(·) requires no iteration, and can be done
by running through the data sample only once.
For each record, all
statistics for all vertices Xi are collected simultaneously.
Finally, to estimate θm′
xi|xpa(i), we use the following point estimate:
ˆθm′
xi|xpa(i) =
s∗(xi, xpa(i))

xi s∗(xi, xpa(i))
For complete data, this estimate is the ML estimate, since ˜n(·) will be
zero.
Note that we may add prior counts α as to obtain the MAP
estimates.
4.3.4
Prediction and missing parents
For every single variable in X a predictive distribution can be created
in the way discussed in the previous sections and applied in all records
where Xi is missing and all parents are observed. However, in some
records not all parents of Xi in m′ are observed, and consequently the
absent parent variables can’t be used as predictors. We therefore deﬁne
the predictive distribution in a slightly diﬀerent way than in the fully
observed parent case.
The variables with missing values have to be
“summed out” such that only the observed variables act as predictors.
For instance, if in record l the variable Xl
i is missing and needs to be
predicted, and a subset of predictors V l ⊆Xpa(i) is missing (so V l ⊂

120
EFFICIENT LEARNING OF BAYESIAN NETWORKS
U l), the predictive distribution for Xl
i is Pr(Xl
i|Xl
pa(i) \ V l, M′). The
parameter for m′ is obtained in terms of s∗(·) deﬁned in eq. 5.15 by
summing out the missing variables:
s∗(xl
i, xl
pa(i) \ vl) =

vl
s∗(xl
i, xl
pa(i))
This means that when the parent set is not fully observed the parame-
ter estimates are functions of these marginal statistics rather than the
“original” statistics.
In summary, we need to traverse the sample once for obtaining the
statistics s∗(·). From these statistics we can create the required univari-
ate predictive distributions for both complete and incomplete parent
sets.
4.3.5
Predictive quality
As stated in eq. 5.14, the predictive distributions based on m and m′
diﬀer in the conditional independence assumptions, but in a way that
the distribution based on m′ can capture any distribution that m can
capture. The predictions we can make for the variables with missing
values using the distribution based on m can also be made using the
distribution based on m′, under the assumption that the parameters are
estimated from a suﬃciently large data sample.
To estimate θm′
xi|xpa(i) we need several “examples” with xpa(i) and xi
observed to get an accurate prediction. If the cardinality of Xpa(i) is
large there are potentially many diﬀerent conﬁgurations; there may not
be enough examples of a particular conﬁguration xpa(i) to make a reliable
estimate of θm′
xi|xpa(i). Consequently the prediction of Xi may suﬀer.
The problem is thus that predictions for vertices with a large Markov
blanket may be unreliable if the sample size is small.
Therefore, we
propose to reduce the number of predictors. Instead of taking all vertices
of a Markov blanket in m as the parents of Xi in m′, we suggest to select
as parents the best b variables of the Markov blanket of Xi in m; best
in terms of predictive power.
4.3.6
Selecting predictive variables
First we need to distinguish between the vertices of the Markov blan-
ket in terms of direct dependences and induced dependences. We have
that Xpa(i)̸⊥⊥Xi and Xch(i)̸⊥⊥Xi, i.e., Xi depends on both parents and
children; this is a direct dependence because in m we have the serial

Learning from Incomplete Data
121
Xi
Figure 5.9.
The potential predictors of Xi from m. The dotted boxes indicate the
predictors that are checked for their predictive quality.
connection Xpa(i) →Xi →Xch(i). The induced dependence says that
for every Xj ∈Xch(i) we have that Xpa(j) \ {Xi}̸⊥⊥Xi|Xj, i.e., Xi de-
pends on the parents of a child of Xi given the child.
This induced
dependence occurs because of the v-connection, Xi →Xj ←Xk with
Xj ∈Xch(i), Xk ∈Xpa(j). This means that if a child of Xi is not chosen
to be part of the predictive variables, then in fact we know that the
parents of that child are irrelevant as well; we need not consider any of
them. Therefore a parent Xk of a child Xj (the collider) is only con-
sidered in conjunction with Xj as a predictor of Xi. Hence, the set of
potential predictors for Xi is:
Ψi = Xpa(i) ∪Xch(i) ∪{(Xk, Xj)|Xk ∈Xpa(j) \ {Xi}, Xj ∈Xch(i)}
from which the b best predictors have to be chosen as the parent set of
Xi in m′. Figure 5.9 illustrates the diﬀerent predictors. We thus have
to test each predictor, and select the b best.
If we want to check how well a predictor, say Xj, is able to predict
Xi, we compute the following probability:
Pr(di|dj, Xpa(i) = {Xj})
(5.16)
where di denotes the ith column of the data sample referring to Xi.
Hence, given that Xj is parent of Xi (thus act as a predictor in m′),
and given the values for the predictor from the data, how well can we
predict the values of Xi from the data sample?
The predictive measure in eq. 5.16 coincides with the terms in the
recursive factorisation of the marginal likelihood given in eq. 3.17; i.e.,

122
EFFICIENT LEARNING OF BAYESIAN NETWORKS
the marginal likelihood is not used for model learning in the current
context, but is now used as a measure of how well a set of observations
for Xi can be predicted given another set of observations for Xj.
As mentioned previously, eq. 5.16 has a simple functional form for
complete data only. To be more precise, for models like Xj →Xi, the
probability in eq. 5.16 is a simple function of the suﬃcient statistics of
the projection (dj, di) when for every record in the projection it holds
that whenever Xi is observed, then Xj is observed too. Obviously, in
general we can’t guarantee that this is the case. As an alternative we
propose to only consider those records where we have observations on
Xi as well as on the predictor (in this case Xj).
Under the MCAR
assumption, this corresponds to an available case analysis of (dj, di), in
which case eq. 5.16 is an unbiased probability estimate. Under the more
general MAR assumption the probability may be biased when a similar
approach is taken. We should however not forget that we are really not
interested an entirely correct probability estimate; we merely rank the
predictors according to this score, and select the b best predictors. As
long as the ranking is not aﬀected in a disastrous way, a biased estimate
does not aﬀect the result. Hence, we use eq. 5.16 for arbitrary ignorable
missing data mechanisms based on the suﬃcient statistics collected by
an available cases analysis.
Another point which we need to mention is that although we check
each predictor on an individual basis, there is no real guarantee that the
b best predictors jointly are good predictors. The general idea is how-
ever that b predictors that score well individually, when taken together
produce even better predictions. One may think of alternatives; for in-
stance a “greedy approach” where we maintain a set of predictors, and
then add those predictors to the set that increase the predictive score
the most until the cardinality of the set is b.
In order to calculate the predictive quality, we need to collect the
suﬃcient statistics from the data sample.
This can be done by one
traversal through the sample. The statistics thus obtained are not the
same as the ones required in the previous section; the b best predictors
need to be determined before we can collect s∗(·).
In summary, the so-called MBP (Markov Blanket Predictor) for pro-
ducing the approximate predictive distribution consists of the steps:
1 G-step: In the generation step, the univariate predictive distributions
are deﬁned:

Learning from Incomplete Data
123
(a) For each Xi, rank the predictive score of each potential predictor
in Xmb(i), respecting the induced dependences. Select the top b
predictors.
(b) Create m′ with the b predictors as parents of Xi, and collect the
statistics s∗(xi, xpa(i)).
2 E-step: In the expectation step, the expected completion of a record
in terms of a count is computed using the univariate predictive dis-
tributions. These distributions are parameterised based on s∗(·). If
the parent set of a variable that needs prediction is partially missing,
sum out the missing parents from s∗(·) ﬁrst.
4.3.7
Implementation of MBP
Actually using the approximate predictive distribution provided by
MBP is part of the “counting-mechanism” employed by the complete
data method. When a record is consulted with missing data, instead of
counting, the predictive distribution is called, which returns the fraction
of a count corresponding to the prediction of the missing value(s) in that
particular record.
The pseudo-code in algorithm 3 is an example of how all the statistics
are obtained from incomplete data, given a ﬁxed model, i.e., it returns
the counts one would obtain in the E-step.
Lines 1–12 correspond to the G-step, where the statistics s∗(·) are col-
lected. These counts are used in the E-step in lines 13–21. Between lines
2–6, the predictors are determined as functions of the Markov blankets.
All predictors are scored using the conditional marginal likelihood based
on an available cases analysis, and depending on the limit we impose on
the maximum number of predictors, the b best predictors are selected.
The lines 14–15 cover the situation where all variables required for
obtaining the statistics are observed for a given case. The lines 17–20
covers the situation where some variables are unobserved. These unob-
served variables are predicted separately conditional on the predictors
from the G-step. If some of the predictors are missing, then they are
summed out from the s∗(·) statistics in line 20. Finally, in line 21, the
fractional statistics are added.
In the next section we evaluate the MBP algorithm. MBP was im-
plemented in C++ using STL, and was run on a 2 GHz machine under
Windows 2000.

124
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Algorithm 3: MBP
Input
: b, max. number of predictors.
Output: All n(xi, xpa(i)) wrt. a BN model
for i ←1 to p do
1
/* Select predictors for Xi and save in Ψi
*/
Ψi ←predictors(Xmb(i))
2
forall Ψi do
3
q ←Pr(di|dΨi, Xpa(i) = Ψi) /* Only available cases */
4
P ←P ∪{(q, Ψi)}
5
Ψi ←selectBest(P, b)
6
/* Get counts s∗(Xi, Ψi) via match-function
*/
for j ←1 to c do
7
if ({Xj
i } ∪Ψj
i) ∈Oj then
8
s∗(xj
i, ψj
i) ←s∗(xj
i, ψj
i) + 1
9
else
10
if Xj
i ∈Oj then
11
forall ψi do
12
s∗(xj
i, ψi) ←s∗(xj
i, ψi) + match(ψj
i|ψi)
/* Get counts n(Xi, Xpa(i)) via predictions
*/
for j ←1 to c do
13
if Xj
i , Xj
pa(i) ∈Oj then
14
n(xj
i, xj
pa(i)) ←n(xj
i, xj
pa(i)) + 1
15
else
16
Π ←({Xj
i } ∪Xj
pa(i)) ∩U j /* Get unobserved variables
17
*/
forall Π do
18
V ←ΨΠ ∩U j /* Get unobserved predictors
*/
19
forall π do Pr(π|oj) ←
v s∗(π, ψj
Π)/ 
π,v s∗(π, ψj
Π)
20
n(xj
i, xj
pa(i)) ←n(xj
i, xj
pa(i)) + 	
π∈{xj
i }∪xj
pa(i) Pr(π|oj)
21
4.3.8
Parameter estimation
First we compare MBP with standard EM for ﬁtting a BN. We use
the same data set as we did with the eMC4 sampler, about risk factors

Learning from Incomplete Data
125
Table 5.1.
Parameter estimates for the model in ﬁgure 5.3 for diﬀerent fractions of
missing data according to the mechanism to the left in ﬁgure 5.4. A bar indicates
negation of the binary variable.
ˆθ
0–10%
10–20%
20–30%
30–40%
EM
MBP
EM
MBP
EM
MBP
EM
MBP
AC
ˆθA|C
0.475
0.475
0.467
0.466
0.484
0.484
0.482
0.482
0.477
ˆθB|C
0.130
0.130
0.132
0.132
0.131
0.131
0.133
0.134
0.122
ˆθB| ¯
C
0.290
0.290
0.289
0.289
0.284
0.285
0.282
0.284
0.312
ˆθC|A,E
0.510
0.510
0.488
0.489
0.509
0.504
0.477
0.482
0.501
ˆθC| ¯
A, ¯
E
0.481
0.481
0.469
0.472
0.491
0.490
0.504
0.507
0.545
ˆθC|A, ¯
E
0.615
0.613
0.617
0.598
0.598
0.580
0.626
0.588
0.569
ˆθC| ¯
A,E
0.368
0.369
0.407
0.420
0.392
0.413
0.383
0.417
0.440
ˆθD|E
0.470
0.470
0.469
0.469
0.480
0.480
0.472
0.473
0.501
ˆθD| ¯
E
0.604
0.604
0.603
0.603
0.610
0.610
0.606
0.607
0.577
ˆθE|A
0.478
0.477
0.473
0.465
0.467
0.460
0.448
0.444
0.458
ˆθE| ¯
A
0.622
0.621
0.628
0.620
0.620
0.613
0.585
0.582
0.600
ˆθF |B
0.163
0.162
0.175
0.174
0.163
0.163
0.154
0.154
0.158
ˆθF | ¯
B
0.874
0.875
0.870
0.870
0.871
0.871
0.873
0.874
0.867
of coronary heart disease (see Section 3.5.6). The model consists of 6
binary variables, and is depicted to the left in ﬁgure 5.3. Four incomplete
sets were generated with 0–10%, 10–20%, 20–30% and 30–40% missing
values according to the missing data mechanism to the left in ﬁgure 5.4.
The probability of non-response of variable Xi conditional on a parent
conﬁguration of Ri was selected from the speciﬁed interval.
The expected suﬃcient statistics obtained via MBP were used for
estimating the parameters of the model. EM was run until a conver-
gence threshold of 0.001. In table 5.1 the MBP and the EM estimates
are shown. MBP requires 3 passes through the sample to estimate the
parameters, whereas EM requires several passes, every time performing
expensive inference in the BN on a per record basis. As the table sug-
gests, the diﬀerences between estimates obtained with EM and MBP are
small. At 30–40% missing data, only θC|A, ¯E and θC| ¯
A,E suﬀer slightly.
The reason for this is that A, C and E are all required for determining
this parameter (E and A are parents of C), and all 3 variables have
incomplete observations as can be seem from the model to the left in
ﬁgure 5.4.

126
EFFICIENT LEARNING OF BAYESIAN NETWORKS
For 30–40% missing data, we included for comparison, the parameter
estimates obtained with available case analysis (column AC). AC relies
heavily on the MCAR assumption.
We see that MBP on the other
hand can cope with MAR missing data as in this example.
MBP is
more robust against departures from the MCAR assumption compared
to available cases analysis.
Estimating the parameters using MBP is almost instantaneous no
matter how large the fraction of missing data is. For EM the number
of passes through the data depends on the fraction of missing items;
more passes were required for larger fractions of missing items. In this
regard we should mention that the convergence rate of EM depends on
the fraction of missing information which does not necessarily change
dramatically for diﬀerent fractions of missing items.
4.3.9
Model learning
Next we evaluate MBP in a model selection context. We compare
MBP to the SEM implementation of Friedman and Elidan, 2005. For
MBP we implemented a greedy search hill-climber as the one described
in Kocka and Castelo, 2001.
Our hill-climber traverses the essential
graph space through repeated covered arc reversals. For MBP we select
the 5 best predictors.
We considered two benchmark BNs for the experiments: The ALARM
network with 37 vertices and 46 arcs Beinlich et al., 1989 and the Insur-
ance network with 27 vertices and 52 arcs Binder et al., 1997.
For the ALARM network 1000 and 5000 records were sampled. Incom-
plete sets were generated by applying a missingness mechanism where 18
variables (selected at random) could be missing, half of these according
to an MCAR mechanism and the other half according to a MAR mech-
anism. Three incomplete sets were generated with 0–10%, 10–20% and
20–30% missing values on the 18 variables. For the Insurance network
1000 and 2500 records were sampled, and incomplete sets were generated
where 14 variables could be missing, half of them according to a MAR
mechanism and the other half according to an MCAR mechanism.
The models learned using SEM and MBP were compared using the
marginal likelihood score from 10,000 records sampled from the BN
learned (DAG model plus parameter) from the smaller complete data
sample (1000 and 5000 for ALARM, 1000 and 2500 for Insurance).
Hence, the larger this score, the better a learned model is able to predict
the 10,000 records that represent the real underlying distribution. We do
not consider the original networks as the golden standards because the

Learning from Incomplete Data
127
Table 5.2.
Log marginal likelihood score for the ALARM network given 1000 and
5000 records. The score is based on 10,000 records sampled from the BNs learned
from the 1000 and 5000 complete records.
% missing
1000 records
5000 records
SEM
MBP
SEM
MBP
0–10%
-99973
-98906
-110105
-107173
10–20%
-99824
-98792
-110669
-107012
20–30%
-99849
-98800
-111044
-107617
Table 5.3.
Log marginal likelihood score for the Insurance network given 1000 and
2500 records. The score is based on 10,000 record sampled from the BNs learned from
the 1000 and 2500 complete records.
% missing
1000 records
2500 records
SEM
MBP
SEM
MBP
0–10%
-160251
-156628
-144174
-143189
10–20%
-161129
-156610
-149236
-145711
20–30%
-159087
-158944
-144979
-144887
relatively small learning samples do not necessarily support the original
data generating networks anymore. It is more reasonable to compare to
the models learned from these smaller complete data samples; after all
we can’t expect to do better than what actually can be learned from
these smaller complete samples.
In table 5.2 the results for the ALARM network are shown. At no
time does MBP produce worse models than SEM from a prediction point
of view; in fact, the score is better for both 1000 and 5000 records. The
same conclusion holds for the Insurance network in table 5.3.
That MBP scores better we partly attribute to the fact that MBP
does not get stuck in local optima while traversing DAG space due to
covered arc reversals. Also, in contrast to (S)EM, MBP will not get
trapped in local optima due to an entirely diﬀerent approach to solving
the missing data problem compared to EM. We observed that the models
learned using SEM were more complex compared to models learned with
MBP.

128
EFFICIENT LEARNING OF BAYESIAN NETWORKS
F
B
E

D
C
A
5–10%
F
B
E

D
C
A
10–15%
F
B
 E
D
C
A
15–20%
Figure 5.10.
The models selected by MBP using the same incomplete data set as for
eMC4.
Finally we ran model selection using MBP on the 3 incomplete data
sets that were generated for eMC4 in Section 3.5.6 with 5–10%, 10–15%
and 15–20% missing components. In ﬁgure 5.7 the models selected using
SEM are shown, and in ﬁgure 5.5 the MAP models from eMC4 are shown
(most frequently visited models on the far left). Figure 5.10 shows the
models that were selected using MBP. The models selected for the data
with 5–10% and 10–15% missing components were the same as the ones
selected by SEM in ﬁgure 5.7, and also the most frequently sampled
models using eMC4. For the data set with 15-20% missing components,
the model selected was diﬀerent from the model selected by SEM that
in turn was diﬀerent from the most frequently sampled model by eMC4.
However, we see from ﬁgure 5.5 that the 4 most frequently visited models
are sampled almost equally often (5% to 7%). From a model selection
point of view, it hardly matters which model is selected in the end. All
in all we may thus conclude that MBP works just as well for model
selection as SEM.
4.3.10
Conclusion and discussion
The MBP sampler is an eﬃcient method for generating approximate
predictive distributions for incomplete data when learning Bayesian net-
works. It integrates easily with existing learning approaches for complete
data, both for model learning and parameter learning, provided that the
methods are functions of the suﬃcient statistics.
The experiments show that the method works well for both MCAR
and the more general MAR missing data mechanisms. It is a very fast
method, yet the experiments indicate that the results obtained are com-
parable to those of EM and SEM. The gain in eﬃciency is especially
noticeable for large data sets where EM and SEM are relatively slow.
For massive data sets we believe that MBP provides a real alternative
to EM, and in any case is a better choice than available cases analy-
sis and BC. However, we do want to stress that we do not claim that

Learning from Incomplete Data
129
MBP is better or even is able to perform equally well as EM under all
circumstances.
On the implementation side, MBP beneﬁts from being very simple to
program. In contrast to EM and SEM, no Bayesian networks inference
engine is required, which in general is known to be rather tedious to
implement.

This page intentionally left blank

Chapter 6
CONCLUSION
In this thesis we have discussed and summarised state-of-the-art meth-
ods for learning Bayesian networks. In recent years several dissertations
have been written about learning Bayesian networks (BNs) from com-
plete data.
This thesis picks up where the previous research on BN
learning left oﬀ.
The main contributions of this thesis can be sum-
marised as follows:
The marginal likelihood was derived and analyzed. For model se-
lection, it was shown to have a similar functional form as a pe-
nalised likelihood, making explicit the penalty term. In existing lit-
erature on BN learning a clear distinction has been made between
MDL/BIC/AIC scores on the one hand and the marginal likelihood
score on the other due to the functional dissimilarity. In contrast to
the penalised likelihood scores, the functional form of the marginal
likelihood (which includes gamma functions) often forms an obstacle
from an analytical point of view.
The rewrite also clearly shows the eﬀects that the prior counts have
on the marginal likelihood, namely that they primarily determine the
regularisation of the models learned.
Structural EM (SEM) was discussed and by exploiting the marginal
likelihood rewrite as a penalised likelihood, it was shown that SEM is
not only approximately correct, but exactly. The original derivation
of SEM in conjunction with the marginal likelihood relies on diﬀerent
approximations of the marginal likelihood. We have shown that this
is not necessary.

132
EFFICIENT LEARNING OF BAYESIAN NETWORKS
For MCMC learning of BN models, Gibbs sampling and Metropolis-
Hastings sampling were combined in the MB-MCMC algorithm. By
analysing the marginal likelihood, we were able to identify the edges
that strongly depend on each other. By way of blocking those edges,
improved mixing of the chain is achieved and this eﬀectively makes
the Markov chain less prone to getting stuck for long periods of time
in contrast to a single edge MCMC approach. Moreover, there is no
added computational burden, compared to existing MCMC model
samplers.
Data Augmentation (DA) was investigated in the context learning
BNs, where the Markov blanket was shown to play an important role.
Various DA derivatives were proposed for eﬃcient learning of BNs
from incomplete data, where the P-step was eliminated. A MCMC
model sampler was presented that employs two interleaved non-trivial
MCMC samplers, and the eﬃciency issues were discussed.
As a solution to the eﬃciency problems of the interleaved model
MCMC sampler, an importance sampling algorithm called eMC4 was
developed that reused previous imputation. The algorithm is based
on the observation that MCMC produces correlated samples, and
that the Markov chain is visiting the same states several times in a
particular region of the state space before moving on to new regions.
The Markov Blanket Predictor (MBP) was developed as a non-itera-
tive 2-pass algorithm for generating approximate predictive distribu-
tions that can be used for both model learning as well as parameter
estimation. In identifying the most relevant predictors, attention is
restricted to the variables in the Markov blanket. When the Markov
blanket is too large, a subset of variables is chosen depending on their
ranking deﬁned in terms of the marginal likelihood. The algorithm
outperforms existing ad-hoc methods that only consider the parents
of variables as predictors, and the results were comparable to those
of EM and SEM.
We have shown that learning from incomplete data is more than a
trivial extension to existing learning methods for complete data. From a
practical point of view we claim that the assumption of complete data is
unrealistic. In that respect this thesis paves the road towards a practical
approach to learning Bayesian networks from real-life data.

References
Akaike, H. (1974). A new look at the statistical model identiﬁcation. IEEE Transac-
tions on Auto Control, AC-19:716–723.
Allison, P.D. (2001). Missing Data. Number 07-136 in Quantitative Applications in
the Social Science. Sage Publications.
Andersson, S., Madigan, D., and Perlman, M. (1997). A characterization of Markov
equivalence classes for acyclic digraphs. Annals of Statistics, 25:505–541.
Andrieu, C., de Freitas, N., Doucet, A., and Jordan, M.I. (2003). An introduction to
MCMC for machine learning. Machine Learning, 50(1-2):5–43.
Angelopoulos, N. and Cussens, J. (2001). Markov chain Monte Carlo using trees-based
priors on model structure. In Breese, J. and Koller, D., editors, Proc. of the Conf.
on Uncertainty in AI, pages 16–23.
Beinlich, I.A, Suermondt, H.J, Chavez, R.M, and Cooper, G.F (1989). The ALARM
monitoring system: A case study with two probabilistic inference techniques for
belief networks. In Proc. of the European Conf. on AI in Medicine.
Binder, J., Koller, D., Russell, S.J., and Kanazawa, K. (1997). Adaptive probabilistic
networks with hidden variables. Machine Learning, 29:213–244.
Bouckaert, R. (1995). Bayesian Belief Networks: From Construction to Inference.
PhD thesis, Utrecht University.
Brand, J. P. L. (1999). Development, Implementation and Evaluation of Muliple Im-
putation Strategies for the Statistical Analysis of Incomplete Data Sets. PhD thesis,
Erasmus University Rotterdam.
Bromberg, F., Margaritis, D., and Honavar, V. (2006). Eﬃcient markov network struc-
ture discovery using independence tests. In SIAM International conf. on data min-
ing, pages 141–152.
Buntine, W. (1991). Theory reﬁnemement on Bayesian networks. In D’Ambrosio, B.,
Smets, P., and Bonissone, P., editors, Proc. of the Conf. on Uncertainty in AI.
Carlin, Bradley P. and Louis, Thomas A. (2000). Bayes and empirical bayes methods
for data analysis. Chapman & Hall/CRC, 2nd edition.
Casella, G. and Berger, R.L. (2002). Statistical Inference. Paciﬁc Grove, 2nd edition.
Castelo, R. (2002). The Discrete Acyclic Digraph Markov Model in Data Mining. PhD
thesis, Utrecht University.

134
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Castelo, R. and Kocka, T. (2003). On inclusion-driven learning of Bayesian networks.
J. of Machine Learning Research, 4:527–574.
Chickering, D.M. (1995). A transformational characterization of equivalent Bayesian
networks. In Besnard, P. and Hanks, S., editors, Proc. of the Conf. on Uncertainty
in AI, pages 87–98.
Chickering, D.M. (2002a). Learning equivalence classes of Bayesian-network struc-
tures. Journal of Machine Learning Research, 2:445–498.
Chickering, D.M. (2002b). Optimal structure identiﬁcation with greedy search. J. of
Machine Learning Research, 3:507–554.
Chickering, D.M. and Heckerman, D. (1997). Eﬃcient approximations for the marginal
likelihood of Bayesian networks with hidden variables. Machine Learning, 29:181–
212.
Cooper, G. (1990). The computational complexity of probabilistic inference using
Bayesian belief networks. Artiﬁcial Intelligence Journal, 42:393–405.
Cooper, G. and Herskovits, E. (1992). A Bayesian method for the induction of prob-
abilistic networks from data. Machine Learning, 9(4):309–347.
Cowell, R.G. (1998). Mixture reduction via predictive scores. Statistics and Comput-
ing, 8(2):97–103.
Cowell, R.G., Dawid, A.P., Lauritzen, S.L., and Spiegelhalter, D.J. (1999). Probabilis-
tic Networks and Expert Systems. Springer.
Cowell, R.G., Dawid, A.P., and Sebastiani, P. (1995). A comparison of sequential
learning methods for incomplete data. Bayesian Statistics, 5:533–541.
Cox, D.R. and Hinkley, D.V. (1974). Theoretical Statistics. Chapman & Hall.
Dawid, A. P. (1984). Statistical theory: The prequential approach. J. of Royal Stat.
Society (Series A), pages 278–292.
Dempster, A.P., Laird, N. M., and Rubin, D.B. (1977). Maximum likelihood from
incomplete data via the EM algorithm. J. of the Royal Statistical Society, Series
B, 34:1–38.
Didelez, V. and Pigeot, I. (1998). Maximum likelihood estimation in graphical models
with missing values. Biometrika, 85(4):960–966.
Edwards, D. and Havr´anek, T. (1985). A fast procedure for model search in multidi-
mensional contingency tables. Biometrika, 72(2):339–351.
Feller, W. (1970). An Introduction to Probability Theory and Its Applications. Wiley.
Friedman, N. (1997). Learning Bayesian networks in the presence of missing values
and hidden variables. In Intl. Conf. on Machine Learning, pages 125–133.
Friedman, N. (1998). The Bayesian structural EM algorithm. In Cooper, G. F. and
Moral, S., editors, Proc. of the Conf. on Uncertainty in AI, pages 129–138.
Friedman, N. and Elidan, G. (2005). LibB for Windows/Linux programs 2.1. Can be
downloaded at http://www.cs.huji.ac.il/labs/compbio/LibB.
Friedman, N. and Koller, D. (2003). Being Bayesian about network structure. A
Bayesian approach to structure discovery in Bayesian networks. Machine Learning,
50(1–2):95–125.
Gelman, A., Carlin, J.B., Stern, H.S., and Rubin, D. B. (2004). Bayesian data anal-
ysis. Chapman & Hall/CRC, 2nd edition.
Gelman, A. and Rubin, D.B. (1992). Inference from iterative simulation using multiple
sequences. Statistical Science, 7(4):457–472.

REFERENCES
135
Geman, S., Bienenstock, E., and Doursat, R. (1992). Neural networks and the bi-
as/variance dilemma. Neural Computation, 6:1–58.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distributions, and the
Bayesian restoration of images. IEEE Trans. on Pattern Analysis and Machine
Intelligence, 6(6):721–741.
Geweke, J. (1989). Bayesian inference in econometric models using Monte Carlo in-
tegration. Econometrica, 74(6):1317–1339.
Gilks, W., Richardson, S., and Spiegelhalter, D. (1996). Markov Chain Monte Carlo
in Practice. Chapman and Hall.
Gillispie, S. and Perlman, M. (2001). Enumerating Markov Equivalence Classes of
Acyclic Digraph Models. In Proc. of the Conf. on Uncertainty in AI.
Giudici, P. and Green, P. (1999). Decomposable graphical gaussian model determi-
nation. Biometrika, 86(4):785–801.
Gr¨unwald, P.D., Myung, I.J., and Pitt, M.A. (2005). Advances in Minimum Descrip-
tion Length: Theory and Applications (Neural Information Processing). The MIT
Press.
Green, P. (1998). Reversible jump Markov chain Monte Carlo computation and Bayesi-
an model determination. Biometrika, 82:711–732.
Hand, D.J. and Henley, W.E. (1997). Statistical classiﬁcation methods in consumer
credit scoring: A review. J. of the Royal Stat. Soc., Series A, 160(3):523–541.
Hastings, W.K. (1970). Monte Carlo sampling methods using Markov chains and their
applications. Biometrika, 57(1):97–109.
Heckerman, D. (1998). A tutorial on learning with Bayesian networks. In Jordan, M.,
editor, Learning in Graphical Models. MIT Press.
Heckerman, D., Geiger, D., and Chickering, D.M. (1995). Learning Bayesian networks:
The combination of knowledge and statistical data. Machine Learning, 20:197–243.
Hesterberg, T. (1995). Weighted average importance sampling and defensive mixture
distributions. Technometrics, 37(2):185–194.
Jensen, C. S., Kong, A., and Kjaerulﬀ, U. (1995). Blocking-Gibbs sampling in very
large probabilistic expert systems. Intl. J. Human-Computer Studies, pages 647–
666.
Jensen, F., Lauritzen, S. L., and Olesen, K. G. (1990). Bayesian updating in causal
probabilistic networks by local computations. Computational Statistics Quarterly,
4:269–282.
Jensen, F. V. (2001). Bayesian Networks and Decision Graphs. Springer-Verlag.
Kennedy, W. and Gentle, J. (1980). Statistical Computing. Marcel Dekker, Inc.
Kocka, T. (2001). Graphical Models: learning and applications. PhD thesis, Faculty
of Informatics and Statistics, University of Economics, Prague.
Kocka, T. and Castelo, R. (2001). Improved learning of Bayesian networks. In Koller,
D. and Breese, J., editors, Proc. of the Conf. on Uncertainty in AI, pages 269–276.
Larra˜naga, P., Poza, M., Yurramendi, Y., Murga, R., and Kuijpers, C. (1996). Struc-
ture learning of Bayesian networks by genetic algorithms: A performance analysis
of control parameters. IEEE Trans. on Pattern Analysis and Machine Intelligence,
26(4):487–493.
Lauritzen, S. L. (1995). The EM algorithm for graphical association models with
missing data. Computational Statistics and Data Analysis, 19:191–201.

136
EFFICIENT LEARNING OF BAYESIAN NETWORKS
Lauritzen, S. L., Dawid, A. P., Larsen, B., and Leimer, H. (1990). Independence
properties of directed Markov ﬁelds. Networks, 20:491–505.
Lauritzen, S. L. and Spiegelhalter, D. J. (1988). Local computations with probabilities
on graphical structures and their applicatins to expert systems. J. R. Stat. Soc.
B, B(50):127–224.
Lehmann, E.L. and Casella, G. (2001). Theory of point estimation. Springer, 2nd
edition.
Little, R.J. and Rubin, D.B. (1987). Statistical analysis with missing data. Wiley and
Sons.
Liu, J.S. (1994). The collapsed Gibbs sampler in Bayesian computations with appli-
cations to a gene regulation problem. J. of the Am. Stat. Assoc., 89:958–288.
Liu, J.S., Wong, W.H., and Kong, A. (1994). Covariance structure of the Gibbs sam-
pler with applications to the comparisons of estimators and augmentation schemes.
Biometrika, 81:27–40.
Madigan, D. and Raftery, A. (1994). Model selection and accounting for model un-
certainty in graphical models using Occam’s window. J. of the Am. Stat. Assoc.,
89:1535–1546.
Madigan, D. and York, J. (1995). Bayesian graphical models for discrete data. Intl.
Statistical Review, 63:215–232.
McLachlan, G.J. and Krishnan, T. (1997). The EM algorithm and extensions. Wiley.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M.N., Teller, A.H., and Teller, E.
(1953). Equations of state calculations by fast computing machine. J. Chem. Phys.,
21:1087–1091.
Neal, R. (1993). Probabilistic Inference Using Markov Chain Monte Carlo Methods.
Technical report, Univ. Toronto.
Nielsen, J., Kocka, T., and Pena, J. (2003). On local optima in learning Bayesian
networks. In Proc. of the Conf. on Uncertainty in AI, pages 435–442.
Pearl, J. (1986). Fusion, propagation, and structuring in belief networks. Artiﬁcial
Intelligence, 29(3):241–288.
Pearl, J. (1988). Probabilistic Reasoning in Intelligent Systems: Networks of Plausible
Inference. Morgan Kaufmann.
Pearl, J. (2000). Causality: Models, Reasoning and Inference. Cambridge Univ. Press.
Pearl, J. and Verma, T. (1987). The logic of representing dependencies by directed
graphs. In Proc. of the Conf. of the American Association of AI, pages 374–379.
Pelikan, M., Goldberg, D.E., and Cant´u-Paz, E. (1999). BOA: The Bayesian opti-
mization algorithm. In Proc. of the Genetic and Evolutionary Computation Conf.,
volume I, pages 525–532.
Pelikan, M., Goldberg, D.E., Ocenasek, J., and Trebst, S. (2003). Robust and scalable
black-box optimization, hierarchy, and Ising spin glasses. Technical Report IlliGAL
No. 2003019, Genetic Algorithms Laboratory, University of Illinois.
Ramoni, M. and Sebastiani, P. (1997a). Eﬃcient parameter learning in Bayesian net-
works from incomplete databases. Technical Report KMI-TR-41, Knowledge Media
Institue, The Open University.
Ramoni, M. and Sebastiani, P. (1997b). Learning Bayesian networks from incomplete
databases. In Geiger, D. and Shenoy, P., editors, Proc. of the Conf. on Uncertainty
in AI, pages 401–408.

REFERENCES
137
Ramoni, M. and Sebastiani, P. (1998). Parameter Estimation in Bayesian networks
from incomplete databases. Intelligent Data Analysis Journal, 2(1).
Ramoni, M. and Sebastiani, P. (2001). Robust learning with missing data. Machine
Learning, 45(2):147–170.
Riggelsen, C. (2004). Learning Bayesian network parameters from incomplete data
using importance sampling. In European Workshop on Probabilistic Graphical Mod-
els, pages 169–176.
Riggelsen, C. (2005). MCMC learning of Bayesian network models by Markov blanket
decomposition. In Gama, J., Camacho, R., Bazdil, P., Jorge, A., and Torgo, L.,
editors, European Conf. on Machine Learning, pages 329–340.
Riggelsen, C. (2006a). Learning Bayesian networks from incomplete data: An eﬃcient
method for generating approximate predictive distributions. In SIAM International
conf. on data mining, pages 130–140.
Riggelsen, C. (2006b). Learning parameters of Bayesian networks from incomplete
data via importance sampling. Intl. J. of Approximate Reasoning, 42(1-2):69–83.
Riggelsen, C. and Feelders, A. (2005). Learning Bayesian network models from in-
complete data using importance sampling. In Cowell, R. G. and Ghahramani, Z.,
editors, Proc. of Artiﬁcial Intelligence and Statistics, pages 301–308.
Robert, C.P. and Casella, G. (2002). Monte Carlo statistical methods. Springer-Verlag,
3rd edition.
Roberts, G.O. and Sahu, S.K. (1997). Updating schemes, correlation structures, block-
ing and parametrization for the Gibbs sampler. J. of the Royal Statistical Society,
Series B, 59:291–317.
Rubin, D.B. (1976). Inference and missing data. Biometrika, 63:581–592.
Rubin, D.B. (1987). Multiple imputation for nonresponse in surveys. Wiley and Sons.
Schwarz, G. (1978). Estimating dimensions of a model. Annals of Stats., 6(2):461–464.
Seillier-Moiseiwitsch, F., Sweeting, T.J., and Dawid, A.P. (1992). Prequential tests of
model ﬁt. Scand. J. of Statistics, 19:45–60.
Shafer, J.L. (1997). Analysis of Incomplete Multivariate Data, volume 72 of Mono-
graphs on Statistics and Applied Probability. Chapman & Hall/CRC.
Singh, M. (1997). Learning Bayesian networks from incomplete data. In American
Assoc. for AI, pages 534–539.
Spiegelhalter, D.J. and Lauritzen, S.L. (1990). Sequential updating of conditional
probabilities on directed graphical structures. Networks, 20:579–605.
Spirtes, P., Glymour, C., and Scheines, R. (2000). Causation, Prediction, and Search.
MIT Press. 2nd edition.
Steck, H. and Jaakkola, T. (2002). On the Dirichlet prior and Bayesian regularization.
In Advances in Neural Info. Proc. Systems, 15.
Tanner, M. and Wong, W. (1987). The calculation of posterior distributions by data
augmentation. J. of the Am. Stat. Assoc., 82(398):528–540.
Tierney, L. (1994). Markov chains for exploring posterior distributions. The Annals
of Statistics, 22:1701–1762.
Titterington, D.M., Smith, A.F.M., and Makov, U.E. (1985). Statistical analysis of
ﬁnite mixture distributions. Wiley.
Verma, T. and Pearl, J. (1990). Equivalence and synthesis of causal models. In Proc.
of the Conf. on Uncertainty in AI.
Whittaker, J. (1990). Graphical Models in Applied Multivariate Statistics. Wiley.

This page intentionally left blank

