
Undergraduate Texts in Mathematics
Editors
S. Axler
K.A. Ribet

Undergraduate Texts in Mathematics
Abbott: Understanding Analysis.
Anglin: Mathematics: A Concise History and
Philosophy.
Readings in Mathematics.
Anglin/Lambek: The Heritage of Thales.
Readings in Mathematics.
Apostol: Introduction to Analytic Number Theory.
Second edition.
Armstrong: Basic Topology.
Armstrong: Groups and Symmetry.
Axler: Linear Algebra Done Right. Second edition.
Beardon: Limits: A New Approach to Real
Analysis.
Bak/Newman: Complex Analysis. Second edition.
Banchoff/Wermer: Linear Algebra Through
Geometry. Second edition.
Beck/Robins: Computing the Continuous
Discretely
Berberian: A First Course in Real Analysis.
Bix: Conics and Cubics: A Concrete Introduction to
Algebraic Curves. Second edition.
Br`emaud: An Introduction to Probabilistic
Modeling.
Bressoud: Factorization and Primality Testing.
Bressoud: Second Year Calculus.
Readings in Mathematics.
Brickman: Mathematical Introduction to Linear
Programming and Game Theory.
Browder: Mathematical Analysis: An Introduction.
Buchmann: Introduction to Cryptography. Second
Edition.
Buskes/van Rooij: Topological Spaces: From
Distance to Neighborhood.
Callahan: The Geometry of Spacetime: An
Introduction to Special and General Relavitity.
Carter/van Brunt: The Lebesgue– Stieltjes
Integral: A Practical Introduction.
Cederberg: A Course in Modern Geometries.
Second edition.
Chambert-Loir: A Field Guide to Algebra
Childs: A Concrete Introduction to Higher Algebra.
Second edition.
Chung/AitSahlia: Elementary Probability Theory:
With Stochastic Processes and an Introduction to
Mathematical Finance. Fourth edition.
Cox/Little/O’Shea: Ideals, Varieties, and
Algorithms. Second edition.
Croom: Basic Concepts of Algebraic Topology.
Cull/Flahive/Robson: Difference Equations. From
Rabbits to Chaos
Curtis: Linear Algebra: An Introductory Approach.
Fourth edition.
Daepp/Gorkin: Reading, Writing, and Proving:
A Closer Look at Mathematics.
Devlin: The Joy of Sets: Fundamentals
of-Contemporary Set Theory. Second edition.
Dixmier: General Topology.
Driver: Why Math?
Ebbinghaus/Flum/Thomas: Mathematical Logic.
Second edition.
Edgar: Measure, Topology, and Fractal Geometry.
Second edition.
Elaydi: An Introduction to Difference Equations.
Third edition.
Erd˜os/Sur´anyi: Topics in the Theory of Numbers.
Estep: Practical Analysis on One Variable.
Exner: An Accompaniment to Higher Mathematics.
Exner: Inside Calculus.
Fine/Rosenberger: The Fundamental Theory
of Algebra.
Fischer: Intermediate Real Analysis.
Flanigan/Kazdan: Calculus Two: Linear and
Nonlinear Functions. Second edition.
Fleming: Functions of Several Variables. Second
edition.
Foulds: Combinatorial Optimization for
Undergraduates.
Foulds: Optimization Techniques: An Introduction.
Franklin: Methods of Mathematical
Economics.
Frazier: An Introduction to Wavelets Through
Linear Algebra.
Gamelin: Complex Analysis.
Ghorpade/Limaye: A Course in Calculus and Real
Analysis
Gordon: Discrete Probability.
Hairer/Wanner: Analysis by Its History.
Readings in Mathematics.
Halmos: Finite-Dimensional Vector Spaces.
Second edition.
Halmos: Naive Set Theory.
H¨ammerlin/Hoffmann: Numerical Mathematics.
Readings in Mathematics.
Harris/Hirst/Mossinghoff: Combinatorics and
Graph Theory.
Hartshorne: Geometry: Euclid and Beyond.
Hijab: Introduction to Calculus and Classical
Analysis. Second edition.
Hilton/Holton/Pedersen: Mathematical
Reﬂections: In a Room with Many Mirrors.
Hilton/Holton/Pedersen: Mathematical Vistas:
From a Room with Many Windows.
Iooss/Joseph: Elementary Stability and Bifurcation
Theory. Second Edition.
(continued after index)

John Stillwell
Naive Lie Theory
123

John Stillwell
Department of Mathematics
University of San Francisco
San Francisco, CA 94117
USA
stillwell@usfca.edu
Editorial Board
S. Axler
K.A. Ribet
Mathematics Department
Department of Mathematics
San Francisco State University
University of California
San Francisco, CA 94132
at Berkeley
USA
Berkeley, CA 94720
axler@sfsu.edu
USA
ribet@math.berkeley.edu
ISBN: 978-0-387-78214-0
e-ISBN: 978-0-387-78215-7
DOI: 10.1007/978-0-387-78214-0
Library of Congress Control Number: 2008927921
Mathematics Subject Classiﬁcation (2000): 22Exx:22E60
c⃝2008 Springer Science+Business Media, LLC
All rights reserved. This work may not be translated or copied in whole or in part without the written
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York,
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. Use
in connection with any form of information storage and retrieval, electronic adaptation, computer
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden.
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they
are not identiﬁed as such, is not to be taken as an expression of opinion as to whether or not they are
subject to proprietary rights.
Printed on acid-free paper
9 8 7 6 5 4 3 2 1
springer.com

To Paul Halmos
In Memoriam

Preface
It seems to have been decided that undergraduate mathematics today rests
on two foundations: calculus and linear algebra. These may not be the
best foundations for, say, number theory or combinatorics, but they serve
quite well for undergraduate analysis and several varieties of undergradu-
ate algebra and geometry. The really perfect sequel to calculus and linear
algebra, however, would be a blend of the two—a subject in which calcu-
lus throws light on linear algebra and vice versa. Look no further! This
perfect blend of calculus and linear algebra is Lie theory (named to honor
the Norwegian mathematician Sophus Lie—pronounced “Lee ”). So why
is Lie theory not a standard undergraduate topic?
The problem is that, until recently, Lie theory was a subject for mature
mathematicians or else a tool for chemists and physicists. There was no
Lie theory for novice mathematicians. Only in the last few years have there
been serious attempts to write Lie theory books for undergraduates. These
books broke through to the undergraduate level by making some sensible
compromises with generality; they stick to matrix groups and mainly to the
classical ones, such as rotation groups of n-dimensional space.
In this book I stick to similar subject matter. The classical groups
are introduced via a study of rotations in two, three, and four dimensions,
which is also an appropriate place to bring in complex numbers and quater-
nions.
From there it is only a short step to studying rotations in real,
complex, and quaternion spaces of any dimension. In so doing, one has
introduced the classical simple Lie groups, in their most geometric form,
using only basic linear algebra. Then calculus intervenes to ﬁnd the tan-
gent spaces of the classical groups—their Lie algebras—and to move back
and forth between the group and its algebra via the log and exponential
functions. Again, the basics sufﬁce: single-variable differentiation and the
Taylor series for ex and log(1+x).
vii

viii
Preface
Where my book diverges from the others is at the next level, the mirac-
ulous level where one discovers that the (curved) structure of a Lie group is
almost completely captured by the structure of its (ﬂat) Lie algebra. At this
level, the other books retain many traces of the sophisticated approach to
Lie theory. For example, they rely on deep ideas from outside Lie theory,
such as the inverse function theorem, existence theorems for ODEs, and
representation theory. Even inside Lie theory, they depend on the Killing
form and the whole root system machine to prove simplicity of the classical
Lie algebras, and they use everything under the sun to prove the Campbell–
Baker–Hausdorff theorem that lifts structure from the Lie algebra to the Lie
group. But actually, proving simplicity of the classical Lie algebras can be
done by basic matrix arithmetic, and there is an amazing elementary proof
of Campbell–Baker–Hausdorff due to Eichler [1968].
The existence of these little-known elementary proofs convinced me
that a naive approach to Lie theory is possible and desirable. The aim of
this book is to carry it out—developing the central concepts and results of
Lie theory by the simplest possible methods, mainly from single-variable
calculus and linear algebra. Familiarity with elementary group theory is
also desirable, but I provide a crash course on the basics of group theory in
Sections 2.1 and 2.2.
The naive approach to Lie theory is due to von Neumann [1929], and it
is now possible to streamline it by using standard results of undergraduate
mathematics, particularly the results of linear algebra. Of course, there is a
downside to naivet´e. It is probably not powerful enough to prove some of
the results for which Lie theory is famous, such as the classiﬁcation of the
simple Lie algebras and the discovery of the ﬁve exceptional algebras.1 To
compensate for this lack of technical power, the end-of-chapter discussions
introduce important results beyond those proved in the book, as part of an
informal sketch of Lie theory and its history. It is also true that the naive
methods do not afford the same insights as more sophisticated methods.
But they offer another insight that is often undervalued—some important
theorems are not as difﬁcult as they look! I think that all mathematics
students appreciate this kind of insight.
In any case, my approach is not entirely naive. A certain amount of
topology is essential, even in basic Lie theory, and in Chapter 8 I take
1I say so from painful experience, having entered Lie theory with the aim of under-
standing the exceptional groups. My opinion now is that the Lie theory that precedes the
classiﬁcation is a book in itself.

Preface
ix
the opportunity to develop all the appropriate concepts from scratch. This
includes everything from open and closed sets to simple connectedness, so
the book contains in effect a minicourse on topology, with the rich class
of multidimensional examples that Lie theory provides. Readers already
familiar with topology can probably skip this chapter, or simply skim it to
see how Lie theory inﬂuences the subject. (Also, if time does not permit
covering the whole book, then the end of Chapter 7 is a good place to stop.)
I am indebted to Wendy Baratta, Simon Goberstein, Brian Hall, Ro-
han Hewson, Chris Hough, Nathan Jolly, David Kramer, Jonathan Lough,
Michael Sun, Marc Ryser, Abe Shenitzer, Paul Stanford, Fan Wu and the
anonymous referees for many corrections and comments. As usual, my
wife, Elaine, served as ﬁrst proofreader; my son Robert also served as the
model for Figure 8.7. Thanks go to Monash University for the opportunity
to teach courses from which this book has grown, and to the University of
San Francisco for support while writing it.
Finally, a word about my title. Readers of a certain age will remember
the book Naive Set Theory by Paul Halmos—a lean and lively volume
covering the parts of set theory that all mathematicians ought to know.
Paul Halmos (1916–2006) was my mentor in mathematical writing, and I
dedicate this book to his memory. While not attempting to emulate his style
(which is inimitable), I hope that Naive Lie Theory can serve as a similar
introduction to Lie groups and Lie algebras. Lie theory today has become
the subject that all mathematicians ought to know something about, so I
believe the time has come for a naive, but mathematical, approach.
John Stillwell
University of San Francisco, December 2007
Monash University, February 2008

Contents
1
Geometry of complex numbers and quaternions
1
1.1
Rotations of the plane . . . . . . . . . . . . . . . . . . . .
2
1.2
Matrix representation of complex numbers . . . . . . . . .
5
1.3
Quaternions . . . . . . . . . . . . . . . . . . . . . . . . .
7
1.4
Consequences of multiplicative absolute value . . . . . . .
11
1.5
Quaternion representation of space rotations . . . . . . . .
14
1.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . .
18
2
Groups
23
2.1
Crash course on groups . . . . . . . . . . . . . . . . . . .
24
2.2
Crash course on homomorphisms . . . . . . . . . . . . . .
27
2.3
The groups SU(2) and SO(3) . . . . . . . . . . . . . . . .
32
2.4
Isometries of Rn and reﬂections
. . . . . . . . . . . . . .
36
2.5
Rotations of R4 and pairs of quaternions . . . . . . . . . .
38
2.6
Direct products of groups . . . . . . . . . . . . . . . . . .
40
2.7
The map from SU(2)×SU(2) to SO(4) . . . . . . . . . . .
42
2.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3
Generalized rotation groups
48
3.1
Rotations as orthogonal transformations . . . . . . . . . .
49
3.2
The orthogonal and special orthogonal groups . . . . . . .
51
3.3
The unitary groups
. . . . . . . . . . . . . . . . . . . . .
54
3.4
The symplectic groups
. . . . . . . . . . . . . . . . . . .
57
3.5
Maximal tori and centers . . . . . . . . . . . . . . . . . .
60
3.6
Maximal tori in SO(n), U(n), SU(n), Sp(n) . . . . . . . . .
62
3.7
Centers of SO(n), U(n), SU(n), Sp(n) . . . . . . . . . . . .
67
3.8
Connectedness and discreteness
. . . . . . . . . . . . . .
69
3.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . .
71
xi

4
The exponential map
74
4.1
The exponential map onto SO(2) . . . . . . . . . . . . . .
75
4.2
The exponential map onto SU(2) . . . . . . . . . . . . . .
77
4.3
The tangent space of SU(2) . . . . . . . . . . . . . . . . .
79
4.4
The Lie algebra su(2) of SU(2) . . . . . . . . . . . . . . .
82
4.5
The exponential of a square matrix . . . . . . . . . . . . .
84
4.6
The afﬁne group of the line . . . . . . . . . . . . . . . . .
87
4.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . .
91
5
The tangent space
93
5.1
Tangent vectors of O(n), U(n), Sp(n) . . . . . . . . . . . .
94
5.2
The tangent space of SO(n) . . . . . . . . . . . . . . . . .
96
5.3
The tangent space of U(n), SU(n), Sp(n) . . . . . . . . . .
99
5.4
Algebraic properties of the tangent space . . . . . . . . . . 103
5.5
Dimension of Lie algebras
. . . . . . . . . . . . . . . . . 106
5.6
Complexiﬁcation . . . . . . . . . . . . . . . . . . . . . . 107
5.7
Quaternion Lie algebras . . . . . . . . . . . . . . . . . . . 111
5.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 113
6
Structure of Lie algebras
116
6.1
Normal subgroups and ideals . . . . . . . . . . . . . . . . 117
6.2
Ideals and homomorphisms . . . . . . . . . . . . . . . . . 120
6.3
Classical non-simple Lie algebras
. . . . . . . . . . . . . 122
6.4
Simplicity of sl(n,C) and su(n) . . . . . . . . . . . . . . . 124
6.5
Simplicity of so(n) for n > 4 . . . . . . . . . . . . . . . . 127
6.6
Simplicity of sp(n)
. . . . . . . . . . . . . . . . . . . . . 133
6.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7
The matrix logarithm
139
7.1
Logarithm and exponential . . . . . . . . . . . . . . . . . 140
7.2
The exp function on the tangent space . . . . . . . . . . . 142
7.3
Limit properties of log and exp . . . . . . . . . . . . . . . 145
7.4
The log function into the tangent space . . . . . . . . . . . 147
7.5
SO(n), SU(n), and Sp(n) revisited
. . . . . . . . . . . . . 150
7.6
The Campbell–Baker–Hausdorff theorem
. . . . . . . . . 152
7.7
Eichler’s proof of Campbell–Baker–Hausdorff . . . . . . . 154
7.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 158
Contents
xii

8
Topology
160
8.1
Open and closed sets in Euclidean space . . . . . . . . . . 161
8.2
Closed matrix groups . . . . . . . . . . . . . . . . . . . . 164
8.3
Continuous functions . . . . . . . . . . . . . . . . . . . . 166
8.4
Compact sets
. . . . . . . . . . . . . . . . . . . . . . . . 169
8.5
Continuous functions and compactness . . . . . . . . . . . 171
8.6
Paths and path-connectedness . . . . . . . . . . . . . . . . 173
8.7
Simple connectedness . . . . . . . . . . . . . . . . . . . . 177
8.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 182
9
Simply connected Lie groups
186
9.1
Three groups with tangent space R . . . . . . . . . . . . . 187
9.2
Three groups with the cross-product Lie algebra . . . . . . 188
9.3
Lie homomorphisms
. . . . . . . . . . . . . . . . . . . . 191
9.4
Uniform continuity of paths and deformations . . . . . . . 194
9.5
Deforming a path in a sequence of small steps . . . . . . . 195
9.6
Lifting a Lie algebra homomorphism . . . . . . . . . . . . 197
9.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . 201
Bibliography
204
Index
207
Contents
xiii

1
Geometry of complex
numbers and quaternions
PREVIEW
When the plane is viewed as the plane C of complex numbers, rotation
about O through angle θ is the same as multiplication by the number
eiθ = cosθ +isinθ.
The set of all such numbers is the unit circle or 1-dimensional sphere
S1 = {z : |z| = 1}.
Thus S1 is not only a geometric object, but also an algebraic structure;
in this case a group, under the operation of complex number multiplication.
Moreover, the multiplication operation eiθ1 ·eiθ2 = ei(θ1+θ2), and the inverse
operation (eiθ)−1 = ei(−θ), depend smoothly on the parameter θ. This
makes S1 an example of what we call a Lie group.
However, in some respects S1 is too special to be a good illustration of
Lie theory. The group S1 is 1-dimensional and commutative, because mul-
tiplication of complex numbers is commutative. This property of complex
numbers makes the Lie theory of S1 trivial in many ways.
To obtain a more interesting Lie group, we deﬁne the four-dimensional
algebra of quaternions and the three-dimensional sphere S3 of unit quater-
nions. Under quaternion multiplication, S3 is a noncommutative Lie group
known as SU(2), closely related to the group of space rotations.
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 1,
1
c⃝Springer Science+Business Media, LLC 2008

2
1
The geometry of complex numbers and quaternions
1.1
Rotations of the plane
A rotation of the plane R2 about the origin O through angle θ is a linear
transformation Rθ that sends the basis vectors (1,0) and (0,1) to (cosθ,
sinθ) and (−sinθ,cosθ), respectively (Figure 1.1).
O
θ
(1,0)
(cosθ,sinθ)
cosθ
sinθ
(0,1)
(−sinθ,cosθ)
θ
Figure 1.1: Rotation of the plane through angle θ.
It follows by linearity that Rθ sends the general vector
(x,y) = x(1,0)+y(0,1)
to
(xcosθ −ysinθ, xsinθ +ycosθ),
and that Rθ is represented by the matrix
cosθ
−sinθ
sinθ
cosθ

.
We also call this matrix Rθ. Then applying the rotation to (x,y) is the same
as multiplying the column vector (xy) on the left by matrix Rθ, because
Rθ
x
y

=
cosθ
−sinθ
sinθ
cosθ
x
y

=
xcosθ −ysinθ
xsinθ +ycosθ

.
Since we apply matrices from the left, applying Rϕ then Rθ is the same
as applying the product matrix RθRϕ. (Admittedly, this matrix happens
to equal RϕRθ because both equal Rθ+ϕ. But when we come to space
rotations the order of the matrices will be important.)

1.1
Rotations of the plane
3
Thus we can represent the geometric operation of combining succes-
sive rotations by the algebraic operation of multiplying matrices. The main
aim of this book is to generalize this idea, that is, to study groups of linear
transformations by representing them as matrix groups. For the moment
one can view a matrix group as a set of matrices that includes, along with
any two members A and B, the matrices AB, A−1, and B−1. Later (in Sec-
tion 7.2) we impose an extra condition that ensures “smoothness” of matrix
groups, but the precise meaning of smoothness need not be considered yet.
For those who cannot wait to see a deﬁnition, we give one in the subsection
below—but be warned that its meaning will not become completely clear
until Chapters 7 and 8.
The matrices Rθ, for all angles θ, form a group called the special or-
thogonal group SO(2). The reason for calling rotations “orthogonal trans-
formations” will emerge in Chapter 3, where we generalize the idea of
rotation to the n-dimensional space Rn and deﬁne a group SO(n) for each
dimension n. In this chapter we are concerned mainly with the groups
SO(2) and SO(3), which are typical in some ways, but also exceptional
in having an alternative description in terms of higher-dimensional “num-
bers.”
Each rotation Rθ of R2 can be represented by the complex number
zθ = cosθ +isinθ
because if we multiply an arbitrary point (x,y) = x+iy by zθ we get
zθ(x+iy) = (cosθ +isinθ)(x+iy)
= xcosθ −ysinθ +i(xsinθ +ycosθ)
= (xcosθ −ysinθ, xsinθ +ycosθ),
which is the result of rotating (x,y) through angle θ. Moreover, the ordi-
nary product zθzϕ represents the result of combining Rθ and Rϕ.
Rotations of R3 and R4 can be represented, in a slightly more compli-
cated way, by four-dimensional “numbers” called quaternions. We intro-
duce quaternions in Section 1.3 via certain 2×2 complex matrices, and to
pave the way for them we ﬁrst investigate the relation between complex
numbers and 2×2 real matrices in Section 1.2.
What is a Lie group?
The most general deﬁnition of a Lie group G is a group that is also a smooth
manifold. That is, the group “product” and “inverse” operations are smooth

4
1
The geometry of complex numbers and quaternions
functions on the manifold G. For readers not familiar with groups we give
a crash course in Section 2.1, but we are not going to deﬁne smooth mani-
folds in this book, because we are not going to study general Lie groups.
Instead we are going to study matrix Lie groups, which include most
of the interesting Lie groups but are much easier to handle.
A matrix
Lie group is a set of n × n matrices (for some ﬁxed n) that is closed un-
der products, inverses, and nonsingular limits. The third closure condition
means that if A1,A2,A3,... is a convergent sequence of matrices in G, and
A = limk→∞Ak has an inverse, then A is in G. We say more about the limit
concept for matrices in Section 4.5, but for n×n real matrices it is just the
limit concept in Rn2.
We can view all matrix Lie groups as groups of real matrices, but it is
natural to allow the matrix entries to be complex numbers or quaternions
as well. Real entries sufﬁce in principle because complex numbers and
quaternions can themselves be represented by real matrices (see Sections
1.2 and 1.3).
It is perhaps surprising that closure under nonsingular limits is equiv-
alent to smoothness for matrix groups. Since we avoid the general con-
cept of smoothness, we cannot fully explain why closed matrix groups are
“smooth” in the technical sense. However, in Chapter 7 we will construct
a tangent space T1(G) for any matrix Lie group G from tangent vectors
to smooth paths in G. We ﬁnd the tangent vectors using only elementary
single-variable calculus, and it can also be shown that the space T1(G) has
the same dimension as G. Thus G is “smooth” in the sense that it has a
tangent space, of the appropriate dimension, at each point.
Exercises
Since rotation through angle θ +ϕ is the result of rotating through θ, then rotating
through ϕ, we can derive formulas for sin(θ +ϕ) and cos(θ +ϕ) in terms of sinθ,
sinϕ, cosθ, and cosϕ.
1.1.1 Explain, by interpreting zθ+ϕ in two different ways, why
cos(θ +ϕ)+isin(θ +ϕ) = (cosθ +isinθ)(cosϕ +isinϕ).
Deduce that
sin(θ +ϕ) = sinθ cosϕ +cosθ sinϕ,
cos(θ +ϕ) = cosθ cosϕ −sinθ sinϕ.
1.1.2 Deduce formulas for sin2θ and cos2θ from the formulas in Exercise 1.1.1.

1.2
Matrix representation of complex numbers
5
1.1.3 Also deduce, from Exercise 1.1.1, that
tan(θ +ϕ) = tanθ +tanϕ
1 −tanθ tanϕ .
1.1.4 Using Exercise 1.1.3, or otherwise, write down the formula for tan(θ −ϕ),
and deduce that lines through O at angles θ and ϕ are perpendicular if and
only if tanθ = −1/tanϕ.
1.1.5 Write down the complex number z−θ and the inverse of the matrix for rota-
tion through θ, and verify that they correspond.
1.2
Matrix representation of complex numbers
A good way to see why the matrices Rθ =
 cosθ −sinθ
sinθ
cosθ

behave the same
as the complex numbers zθ = cosθ + isinθ is to write Rθ as the linear
combination
Rθ = cosθ
1
0
0
1

+sinθ
0
−1
1
0

of the basis matrices
1 =
1
0
0
1

,
i =
0
−1
1
0

.
It is easily checked that
12 = 1,
1i = i1 = i,
i2 = −1,
so the matrices 1 and i behave exactly the same as the complex numbers 1
and i.
In fact, the matrices
a
−b
b
a

= a1+bi,
where
a,b ∈R,
behave exactly the same as the complex numbers a + bi under addition
and multiplication, so we can represent all complex numbers by 2×2 real
matrices, not just the complex numbers zθ that represent rotations. This
representation offers a “linear algebra explanation” of certain properties of
complex numbers, for example:
• The squared absolute value, |a+bi|2 = a2 +b2 of the complex num-
ber a+bi is the determinant of the corresponding matrix
 a −b
b a

.

6
1
The geometry of complex numbers and quaternions
• Therefore, the multiplicative property of absolute value, |z1z2| =
|z1||z2|, follows from the multiplicative property of determinants,
det(A1A2) = det(A1)det(A2).
(Take A1 as the matrix representing z1, and A2 as the matrix repre-
senting z2.)
• The inverse z−1 = a−bi
a2+b2 of z = a+bi ̸= 0 corresponds to the inverse
matrix
a
−b
b
a
−1
=
1
a2 +b2
 a
b
−b
a

.
The two-square identity
If we set z1 = a1 + ib1 and z2 = a2 + ib2, then the multiplicative property
of (squared) absolute value states that
(a2
1 +b2
1)(a2
2 +b2
2) = (a1a2 −b1b2)2 +(a1b2 +a2b1)2,
as can be checked by working out the product z1z2 and its squared abso-
lute value. This identity is particularly interesting in the case of integers
a1,b1,a2,b2, because it says that
(a sum of two squares)×(a sum of two squares) = (a sum of two squares).
This fact was noticed nearly 2000 years ago by Diophantus, who men-
tioned an instance of it in Book III, Problem 19, of his Arithmetica. How-
ever, Diophantus said nothing about sums of three squares—with good rea-
son, because there is no such three-square identity. For example
(12 +12 +12)(02 +12 +22) = 3×5 = 15,
and 15 is not a sum of three integer squares.
This is an early warning sign that there are no three-dimensional num-
bers. In fact, there are no n-dimensional numbers for any n > 2; however,
there is a “near miss” for n = 4. One can deﬁne “addition” and “multipli-
cation” for quadruples q = (a,b,c,d) of real numbers so as to satisfy all
the basic laws of arithmetic except q1q2 = q2q1 (the commutative law of
multiplication). This system of arithmetic for quadruples is the quaternion
algebra that we introduce in the next section.

1.3
Quaternions
7
Exercises
1.2.1 Derive the two-square identity from the multiplicative property of det.
1.2.2 Write 5 and 13 as sums of two squares, and hence express 65 as a sum of
two squares using the two-square identity.
1.2.3 Using the two-square identity, express 372 and 374 as sums of two nonzero
squares.
The absolute value |z| =
√
a2 +b2 represents the distance of z from O, and
more generally, |u −v| represents the distance between u and v. When combined
with the distributive law,
u(v−w) = uv−uw,
a geometric property of multiplication comes to light.
1.2.4 Deduce, from the distributive law and multiplicative absolute value, that
|uv−uw| = |u||v−w|.
Explain why this says that multiplication of the whole plane of complex
numbers by u multiplies all distances by |u|.
1.2.5 Deduce from Exercise 1.2.4 that multiplication of the whole plane of com-
plex numbers by cosθ +isinθ leaves all distances unchanged.
A map that leaves all distances unchanged is called an isometry (from the
Greek for “same measure”), so multiplication by cosθ + isinθ is an isometry of
the plane. (In Section 1.1 we deﬁned the corresponding rotation map Rθ as a linear
map that moves 1 and i in a certain way; it is not obvious from this deﬁnition that
a rotation is an isometry.)
1.3
Quaternions
By associating the ordered pair (a,b) with the complex number a+ib or the
matrix
a −b
b a

we can speak of the “sum,” “product,” and “absolute value”
of ordered pairs. In the same way, we can speak of the “sum,” “product,”
and “absolute value” of ordered quadruples by associating each ordered
quadruple (a,b,c,d) of real numbers with the matrix
q =
a+id
−b−ic
b−ic
a−id

.
(*)
We call any matrix of the form (*) a quaternion. (This is not the only
way to associate a matrix with a quadruple. I have chosen these complex

8
1
The geometry of complex numbers and quaternions
matrices because they extend the real matrices used in the previous sec-
tion to represent complex numbers. Thus complex numbers are the special
quaternions with c = d = 0.)
It is clear that the sum of any two matrices of the form (*) is another
matrix of the same form, and it can be checked (Exercise 1.3.2) that the
product of two matrices of the form (*) is of the form (*). Thus we can
deﬁne the sum and product of quaternions to be just the matrix sum and
product. Also, if the squared absolute value |q|2 of a quaternion q is de-
ﬁned to be the determinant of q, then we have
detq = det
a+id
−b−ic
b−ic
a−id

= a2 +b2 +c2 +d2.
So |q|2 is the squared distance of the point (a,b,c,d) from O in R4.
The quaternion sum operation has the same basic properties as addition
for numbers, namely
q1 +q2 = q2 +q1,
(commutative law)
q1 +(q2 +q3) = (q1 +q2)+q3,
(associative law)
q+(−q) = 0
where 0 is the zero matrix,
(inverse law)
q+0 = q.
(identity law)
The quaternion product operation does not have all the properties of
multiplication of numbers—in general, the commutative property q1q2 =
q2q1 fails—but well-known properties of the matrix product imply the fol-
lowing properties of the quaternion product:
q1(q2q3) = (q1q2)q3,
(associative law)
qq−1 = 1
for q ̸= 0,
(inverse law)
q1 = q,
(identity law)
q1(q2 +q3) = q1q2 +q1q3.
(left distributive law)
Here 0 and 1 denote the 2 × 2 zero and identity matrices, which are also
quaternions. The right distributive law (q2 +q3)q1 = q2q1 +q3q1 of course
holds too, and is distinct from the left distributive law because of the non-
commutative product.
The noncommutative nature of the quaternion product is exposed more
clearly when we write
a+di
−b−ci
b−ci
a−di

= a1+bi+cj+dk,

1.3
Quaternions
9
where
1 =
1
0
0
1

,
i =
0
−1
1
0

,
j =
 0
−i
−i
0

,
k =
i
0
0
−i

.
Thus 1 behaves like the number 1, i2 = −1 as before, and also j2 = k2 =
−1. The noncommutativity is concentrated in the products of i, j, k, which
are summarized in Figure 1.2. The product of any two distinct elements is
i
j
k
Figure 1.2: Products of the imaginary quaternion units.
the third element in the circle, with a + sign if an arrow points from the
ﬁrst element to the second, and a −sign otherwise. For example, ij = k,
but ji = −k, so ij ̸= ji.
The failure of the commutative law is actually a good thing, because it
enables quaternions to represent other things that do not commute, such as
rotations in three and four dimensions.
As with complex numbers, there is a linear algebra explanation of some
less obvious properties of quaternion multiplication.
• The absolute value has the multiplicative property |q1q2| = |q1||q2|,
by the multiplicative property of det: det(q1q2) = det(q1)det(q2).
• Each nonzero quaternion q has an inverse q−1, namely the matrix
inverse of q.
• From the matrix (*) for q we get an explicit formula for q−1. If
q = a1+bi+cj+dk ̸= 0 then
q−1 =
1
a2 +b2 +c2 +d2(a1−bi−cj−dk).
• The quaternion a1−bi−cj−dk is called the quaternion conjugate
q of q = a1+bi+cj+dk, and we have qq = a2 +b2 +c2 +d2 = |q|2.

10
1
The geometry of complex numbers and quaternions
• The quaternion conjugate is not the result of taking the complex con-
jugate of each entry in the matrix q. In fact, q is the result of taking
the complex conjugate of each entry in the transposed matrix qT.
Then it follows from (q1q2)T = qT
2qT
1 that (q1q2) = q2 q1.
The algebra of quaternions was discovered by Hamilton in 1843, and
it is denoted by H in his honor. He started with just i and j (hoping to
ﬁnd an algebra of triples analogous to the complex algebra of pairs), but
later introduced k = ij to escape from apparently intractable problems with
triples (he did not know, at ﬁrst, that there is no three-square identity). The
matrix representation was discovered in 1858, by Cayley.
The 3-sphere of unit quaternions
The quaternions a1+bi+cj+dk of absolute value 1, or unit quaternions,
satisfy the equation
a2 +b2 +c2 +d2 = 1.
Hence they form the analogue of the sphere, called the 3-sphere S3, in the
space R4 of all 4-tuples (a,b,c,d). It follows from the multiplicative prop-
erty and the formula for inverses above that the product of unit quaternions
is again a unit quaternion, and hence S3 is a group under quaternion mul-
tiplication. Like the 1-sphere S1 of unit complex numbers, the 3-sphere
of unit quaternions encapsulates a group of rotations, though not quite so
directly. In the next two sections we show how unit quaternions may be
used to represent rotations of ordinary space R3.
Exercises
When Hamilton discovered H he described quaternion multiplication very con-
cisely by the relations
i2 = j2 = k2 = ijk = −1.
1.3.1 Verify that Hamilton’s relations hold for the matrices 1, i, j, and k. Also
show (assuming associativity and inverses) that these relations imply all
the products of i, j, and k shown in Figure 1.2.
1.3.2 Verify that the product of quaternions is indeed a quaternion. (Hint: It helps
to write each quaternion in the form
q =
α
−β
β
α

,
where α = x−iy is the complex conjugate of α = x+iy.)

1.4
Consequences of multiplicative absolute value
11
1.3.3 Check that q is the result of taking the complex conjugate of each entry in
qT, and hence show that q1q2 = q2 q1 for any quaternions q1 and q2.
1.3.4 Also check that qq = |q|2.
Cayley’s matrix representation makes it easy (in principle) to derive an amaz-
ing algebraic identity.
1.3.5 Show that the multiplicative property of determinants gives the complex
two-square identity (discovered by Gauss around 1820)
(|α1|2 +|β1|2)(|α2|2 +|β2|2) = |α1α2 −β1β2|2 +|α1β2 +β1α2|2.
1.3.6 Show that the multiplicative property of determinants gives the real four-
square identity
(a2
1 +b2
1 +c2
1 +d2
1)(a2
2 +b2
2 +c2
2 +d2
2) =
(a1a2 −b1b2 −c1c2 −d1d2)2
+(a1b2 +b1a2 +c1d2 −d1c2)2
+(a1c2 −b1d2 +c1a2 +d1b2)2
+(a1d2 +b1c2 −c1b2 +d1a2)2.
This identity was discovered by Euler in 1748, nearly 100 years before the dis-
covery of quaternions! Like Diophantus, he was interested in the case of integer
squares, in which case the identity says that
(a sum of four squares)×(a sum of four squares) = (a sum of four squares).
This was the ﬁrst step toward proving the theorem that every positive integer is
the sum of four integer squares. The proof was completed by Lagrange in 1770.
1.3.7 Express 97 and 99 as sums of four squares.
1.3.8 Using Exercise 1.3.6, or otherwise, express 97×99 as a sum of four squares.
1.4
Consequences of multiplicative absolute value
The multiplicative absolute value, for both complex numbers and quater-
nions, ﬁrst appeared in number theory as a property of sums of squares. It
was noticed only later that it has geometric implications, relating multipli-
cation to rigid motions of R2, R3, and R4. Suppose ﬁrst that u is a complex
number of absolute value 1. Without any computation with cosθ and sinθ,
we can see that multiplication of C = R2 by u is a rotation of the plane as
follows.

12
1
The geometry of complex numbers and quaternions
Let v and w be any two complex numbers, and consider their images,
uv and uw under multiplication by u. Then we have
distance from uv to uw = |uv−uw|
= |u(v−w)|
by the distributive law
= |u||v−w|
by multiplicative absolute value
= |v−w|
because |u| = 1
= distance from v to w.
In other words, multiplication by u with |u| = 1 is a rigid motion, also
known as an isometry, of the plane. Moreover, this isometry leaves O
ﬁxed, because u× 0 = 0. And if u ̸= 1, no other point v is ﬁxed, because
uv = v implies u = 1. The only motion of the plane with these properties
is rotation about O.
Exactly the same argument applies to quaternion multiplication, at least
as far as preservation of distance is concerned: if we multiply the space
R4 of quaternions by a quaternion of absolute value 1, then the result is
an isometry of R4 that leaves the origin ﬁxed. It is in fact reasonable to
interpret this isometry of R4 as a “rotation,” but ﬁrst we want to show that
quaternion multiplication also gives a way to study rotations of R3. To see
how, we look at a natural three-dimensional subspace of the quaternions.
Pure imaginary quaternions
The pure imaginary quaternions are those of the form
p = bi+cj+dk.
They form a three-dimensional space that we will denote by Ri+Rj+Rk,
or sometimes R3 for short. The space Ri + Rj + Rk is the orthogonal
complement to the line R1 of quaternions of the form a1, which we will
call real quaternions. From now on we write the real quaternion a1 simply
as a, and denote the line of real quaternions simply by R.
It is clear that the sum of any two members of Ri + Rj + Rk is itself
a member of Ri + Rj + Rk, but this is not generally true of products. In
fact, if u = u1i + u2j + u3k and v = v1i + v2j + v3k then the multiplication
diagram for i, j, and k (Figure 1.2) gives
uv =−(u1v1 +u2v2 +u3v3)
+(u2v3 −u3v2)i−(u1v3 −u3v1)j+(u1v2 −u2v1)k.

1.4
Consequences of multiplicative absolute value
13
This relates the quaternion product uv to two other products on R3 that are
well known in linear algebra: the inner (or “scalar” or “dot”) product,
u·v = u1v1 +u2v2 +u3v3,
and the vector (or “cross”) product
u×v =

i
j
k
u1
u2
u3
v1
v2
v3

= (u2v3 −u3v2)i−(u1v3 −u3v1)j+(u1v2 −u2v1)k.
In terms of the scalar and vector products, the quaternion product is
uv = −u·v+u×v.
Since u· v is a real number, this formula shows that uv is in Ri + Rj + Rk
only if u·v = 0, that is, only if u is orthogonal to v.
The formula uv = −u· v + u× v also shows that uv is real if and only
if u × v = 0, that is, if u and v have the same (or opposite) direction. In
particular, if u ∈Ri+Rj+Rk and |u| = 1 then
u2 = −u·u = −|u|2 = −1.
Thus every unit vector in Ri+Rj+Rk is a “square root of −1.” (This, by
the way, is another sign that H does not satisfy all the usual laws of algebra.
If it did, the equation u2 = −1 would have at most two solutions.)
Exercises
The cross product is an operation on Ri+Rj+Rk because u×v is in Ri+Rj+Rk
for any u,v ∈Ri+Rj+Rk. However, it is neither a commutative nor associative
operation, as Exercises 1.4.1 and 1.4.3 show.
1.4.1 Prove the antisymmetric property u ×v = −v×u.
1.4.2 Prove that u ×(v×w) = v(u ·w)−w(u ·v) for pure imaginary u,v,w.
1.4.3 Deduce from Exercise 1.4.2 that × is not associative.
1.4.4 Also deduce the Jacobi identity for the cross product:
u ×(v×w)+w×(u ×v)+v×(w×u) = 0.
The antisymmetric and Jacobi properties show that the cross product is not com-
pletely lawless. These properties deﬁne what we later call a Lie algebra.

14
1
The geometry of complex numbers and quaternions
1.5
Quaternion representation of space rotations
A quaternion t of absolute value 1, like a complex number of absolute value
1, has a “real part” cosθ and an “imaginary part” of absolute value sinθ,
orthogonal to the real part and hence in Ri+Rj+Rk. This means that
t = cosθ +usinθ,
where u is a unit vector in Ri+Rj+Rk, and hence u2 = −1 by the remark
at the end of the previous section.
Such a unit quaternion t induces a rotation of Ri + Rj + Rk, though
not simply by multiplication, since the product of t and a member q of
Ri + Rj + Rk may not belong to Ri + Rj + Rk. Instead, we send each
q ∈Ri+Rj+Rk to t−1qt, which turns out to be a member of Ri+Rj+Rk.
To see why, ﬁrst note that
t−1 = t/|t|2 = cosθ −usinθ,
by the formulas for q−1 and q in Section 1.3.
Since t−1 exists, multiplication of H on either side by t or t−1 is an
invertible map and hence a bijection of H onto itself. It follows that the
map q →t−1qt, called conjugation by t, is a bijection of H. Conjugation by
t also maps the real line R onto itself, because t−1rt = r for a real number
r; hence it also maps the orthogonal complement Ri+Rj+Rk onto itself.
This is because conjugation by t is an isometry, since multiplication on
either side by a unit quaternion is an isometry.
It looks as though we are onto something with conjugation by t =
cosθ +usinθ, and indeed we have the following theorem.
Rotation by conjugation. If t = cosθ + usinθ, where u ∈Ri + Rj + Rk
is a unit vector, then conjugation by t rotates Ri+Rj+Rk through angle
2θ about axis u.
Proof. First, observe that the line Ru of real multiples of u is ﬁxed by the
conjugation map, because
t−1ut = (cosθ −usinθ)u(cosθ +usinθ)
= (ucosθ −u2 sinθ)(cosθ +usinθ)
= (ucosθ +sinθ)(cosθ +usinθ)
because u2 = −1
= u(cos2 θ +sin2 θ)+sinθ cosθ +u2 sinθ cosθ
= u
also because u2 = −1.

1.5
Quaternion representation of space rotations
15
It follows, since conjugation by t is an isometry of Ri+Rj+Rk, that
its restriction to the plane through O in Ri+Rj+Rk orthogonal to the line
Ru is also an isometry. And if the restriction to this plane is a rotation, then
conjugation by t is a rotation of the whole space Ri+Rj+Rk.
To see whether this is indeed the case, choose a unit vector v orthogonal
to u in Ri + Rj + Rk, so u · v = 0. Then let w = u × v, which equals uv
because u·v = 0, so {u,v,w} is an orthonormal basis of Ri+Rj+Rk with
uv = w, vw = u, wu = v, uv = −vu and so on. It remains to show that
t−1vt = vcos2θ −wsin2θ,
t−1wt = vsin2θ +wcos2θ,
because this means that conjugation by t rotates the basis vectors v and w,
and hence the whole plane orthogonal to the line Ru, through angle 2θ.
This is conﬁrmed by the following computation:
t−1vt = (cosθ −usinθ)v(cosθ +usinθ)
= (vcosθ −uvsinθ)(cosθ +usinθ)
= vcos2 θ −uvsinθ cosθ +vusinθ cosθ −uvusin2 θ
= vcos2 θ −2uvsinθ cosθ +u2vsin2 θ
because vu = −uv
= v(cos2 θ −sin2 θ)−2wsinθ cosθ
because u2 = −1, uv = w
= vcos2θ −wsin2θ.
A similar computation (try it) shows that t−1wt = vsin2θ + wcos2θ, as
required.
□
This theorem shows that every rotation of R3, given by an axis u and
angle of rotation α, is the result of conjugation by the unit quaternion
t = cos α
2 +usin α
2 .
The same rotation is induced by −t, since (−t)−1s(−t) = t−1st. But ±t
are the only unit quaternions that induce this rotation, because each unit
quaternion is uniquely expressible in the form t = cos α
2 +usin α
2 , and the
rotation is uniquely determined by the two (axis, angle) pairs (u,α) and
(−u,−α). The quaternions t and −t are said to be antipodal, because they
represent diametrically opposite points on the 3-sphere of unit quaternions.
Thus the theorem says that rotations of R3 correspond to antipodal
pairs of unit quaternions. We also have the following important corollary.

16
1
The geometry of complex numbers and quaternions
Rotations form a group. The product of rotations is a rotation, and the
inverse of a rotation is a rotation.
Proof. The inverse of the rotation about axis u through angle α is obviously
a rotation, namely, the rotation about axis u through angle −α.
It is not obvious what the product of two rotations is, but we can show
as follows that it has an axis and angle of rotation, and hence is a rotation.
Suppose we are given a rotation r1 with axis u1 and angle α1, and a rotation
r2 with axis u2 and angle α2. Then
r1 is induced by conjugation by t1 = cos α1
2 +u1 sin α1
2
and
r2 is induced by conjugation by t2 = cos α2
2 +u2 sin α2
2 ,
hence the result r1r2 of doing r1, then r2, is induced by
q →t−1
2 (t−1
1 qt1)t2 = (t1t2)−1q(t1t2),
which is conjugation by t1t2 = t. The quaternion t is also a unit quaternion,
so
t = cos α
2 +usin α
2
for some unit imaginary quaternion u and angle α. Thus the product rota-
tion is the rotation about axis u through angle α.
□
The proof shows that the axis and angle of the product rotation r1r2 can
in principle be found from those of r1 and r2 by quaternion multiplication.
They may also be described geometrically, by the alternative proof of the
group property given in the exercises below.
Exercises
The following exercises introduce a small fragment of the geometry of isometries:
that any rotation of the plane or space is a product of two reﬂections. We begin
with the simplest case: representing rotation of the plane about O through angle
θ as the product of reﬂections in two lines through O.
If L is any line in the plane, then reﬂection in L is the transformation of the
plane that sends each point S to the point S′ such that SS′ is orthogonal to L and
L is equidistant from S and S′.

1.5
Quaternion representation of space rotations
17
L
P
S
α
S′
α
Figure 1.3: Reﬂection of S and its angle.
1.5.1 If L passes through P, and if S lies on one side of L at angle α (Figure 1.3),
show that S′ lies on the other side of L at angle α, and that |PS| = |PS′|.
1.5.2 Deduce, from Exercise 1.5.1 or otherwise, that the rotation about P through
angle θ is the result of reﬂections in any two lines through P that meet at
angle θ/2.
1.5.3 Deduce, from Exercise 1.5.2 or otherwise, that if L , M , and N are lines
situated as shown in Figure 1.4, then the result of rotation about P through
angle θ, followed by rotation about Q through angle ϕ, is rotation about R
through angle χ (with rotations in the senses indicated by the arrows).
R
Q
P
M
L
N
θ/2
ϕ/2
χ/2
Figure 1.4: Three lines and three rotations.
1.5.4 If L and N are parallel, so R does not exist, what isometry is the result of
the rotations about P and Q?
Now we extend these ideas to R3. A rotation about a line through O (called
the axis of rotation) is the product of reﬂections in planes through O that meet
along the axis. To make the reﬂections easier to visualize, we do not draw the
planes, but only their intersections with the unit sphere (see Figure 1.5).
These intersections are curves called great circles, and reﬂection in a great
circle is the restriction to the sphere of reﬂection in a plane through O.

18
1
The geometry of complex numbers and quaternions
P
θ/2
L
Q
ϕ/2
M
R
N
Figure 1.5: Reﬂections in great circles on the sphere.
1.5.5 Adapt the argument of Exercise 1.5.3 to great circles L , M , and N shown
in Figure 1.5. What is the conclusion?
1.5.6 Explain why there is no exceptional case analogous to Exercise 1.5.4. De-
duce that the product of any two rotations of R3 about O is another rotation
about O, and explain how to ﬁnd the axis of the product rotation.
The idea of representing isometries as products of reﬂections is also useful in
higher dimensions. We use this idea again in Section 2.4, where we show that any
isometry of Rn that ﬁxes O is the product of at most n reﬂections in hyperplanes
through O.
1.6
Discussion
The geometric properties of complex numbers were discovered long before
the complex numbers themselves. Diophantus (already mentioned in Sec-
tion 1.2) was aware of the two-square identity, and indeed he associated a
sum of two squares, a2+b2, with the right-angled triangle with perpendicu-
lar sides a and b. Thus, Diophantus was vaguely aware of two-dimensional
objects (right-angled triangles) with a multiplicative property (of their hy-
potenuses). Around 1590, Vi`ete noticed that the Diophantus “product”
of triangles with sides (a,b) and (c,d)—namely, the triangle with sides
(ac−bd,bc+ad)—also has an additive property, of angles (Figure 1.6).

1.6
Discussion
19
a
b
√
a2 +b2
×
θ
c
d
√
c2 +d2
=
ϕ
ac−bd
bc+ad

(a2 +b2)(c2 +d2)
θ +ϕ
Figure 1.6: The Diophantus “product” of triangles.
The algebra of complex numbers emerged from the study of polyno-
mial equations in the sixteenth century, particularly the solution of cu-
bic equations by the Italian mathematicians del Ferro, Tartaglia, Cardano,
and Bombelli. Complex numbers were not required for the solution of
quadratic equations, because in the sixteenth century one could say that
x2 + 1 = 0, for example, has no solution. The formal solution x = √−1
was just a signal that no solution really exists. Cubic equations force the
issue because the equation x3 = px+q has solution
x =
3

q
2 +
	
q
2
2
−

 p
3
3
+
3

q
2 −
	
q
2
2
−

 p
3
3
(the “Cardano formula”). Thus, according to the Cardano formula the so-
lution of x3 = 15x+4 is
x =
3
2+

22 −53 +
3
2−

22 −53 =
3√
2+11i +
3√
2−11i.
But the symbol i =
√
−1 cannot be signaling NO SOLUTION here, because
there is an obvious solution x = 4. How can
3√
2+11i + 3√
2−11i be the
solution when 4 is?
In 1572, Bombelli resolved this conﬂict, and launched the algebra of
complex numbers, by observing that
(2+i)3 = 2+11i,
(2−i)3 = 2−11i,

20
1
The geometry of complex numbers and quaternions
and therefore
3√
2+11i+
3√
2−11i = (2+i)+(2−i) = 4,
assuming that i obeys the same rules as ordinary, real, numbers. His calcu-
lation was, in effect, an experimental test of the proposition that complex
numbers form a ﬁeld—a proposition that could not have been formulated,
let alone proved, at the time. The ﬁrst rigorous treatment of complex num-
bers was that of Hamilton, who in 1835 gave deﬁnitions of complex num-
bers, addition, and multiplication that make a proof of the ﬁeld properties
crystal clear.
Hamilton deﬁned complex numbers as ordered pairs z = (a,b) of real
numbers, and he deﬁned their sum and product by
(a1,b1)+(a2,b2) = (a1 +a2,b1 +b2),
(a1,b1)(a2,b2) = (a1a2 −b1b2,a1b2 +b1a2).
Of course, these deﬁnitions are motivated by the interpretation of (a,b) as
a + ib, where i2 = −1, but the important point is that the ﬁeld properties
follow from these deﬁnitions and the properties of real numbers. The prop-
erties of addition are directly “inherited” from properties of real number
addition. For example, for complex numbers z1 = (a1,b1) and z2 = (a2,b2)
we have
z1 +z2 = z2 +z1
because
a1 +a2 = a2 +a1 and b1 +b2 = b2 +b1 for real numbers a1,a2,b1,b2.
Indeed, the properties of addition are not special properties of pairs, they
also hold for the vector sum of triples, quadruples, and so on. The ﬁeld
properties of multiplication, on the other hand, depend on the curious deﬁ-
nition of product of pairs, which has no obvious generalization to a product
of n-tuples for n > 2.
This raises the question; is it possible to deﬁne a “product” operation on
Rn that, together with the vector sum operation, makes Rn a ﬁeld? Hamil-
ton hoped to ﬁnd such a product for each n. Indeed, he hoped to ﬁnd a
product with not only the ﬁeld properties but also the multiplicative abso-
lute value
|uv| = |u||v|,

1.6
Discussion
21
where the absolute value of u = (x1,x2,...,xn) is |u| =

x2
1 +x2
2 +···+x2n.
As we have seen, for n = 2 this property is equivalent to the Diophantus
identity for sums of two squares, so a multiplicative absolute value in gen-
eral implies an identity for sums of n squares.
Hamilton attacked the problem from the opposite direction, as it were.
He tried to deﬁne the product operation, ﬁrst for triples, before worrying
about the absolute value. But after searching fruitlessly for 13 years, he
had to admit defeat. He still had not noticed that there is no three square
identity, but he suspected that multiplying triples of the form a + bi + c j
requires a new object k = i j. Also, he began to realize that there is no hope
for the commutative law of multiplication. Desperate to salvage something
from his 13 years of work, he made the leap to the fourth dimension. He
took k = i j to be a vector perpendicular to 1, i, and j, and sacriﬁced the
commutative law by allowing i j = −ji, jk = −k j, and ki = −ik. On Octo-
ber 16, 1843 he had his famous epiphany that i, j, and k must satisfy
i2 = j2 = k2 = i jk = −1.
As we have seen in Section 1.3, these relations imply all the ﬁeld prop-
erties, except commutative multiplication. Such a system is often called
a skew ﬁeld (though this term unfortunately suggests a specialization of
the ﬁeld concept, rather than what it really is—a generalization). Hamil-
ton’s relations also imply that absolute value is multiplicative—a fact he
had to check, though the equivalent four-square identity was well known
to number theorists.
In 1878, Frobenius proved that the quaternion algebra H is the only
skew ﬁeld Rn that is not a ﬁeld, so Hamilton had found the only “algebra
of n-tuples” it was possible to ﬁnd under the conditions he had imposed.
The multiplicative absolute value, as stressed in Section 1.4, implies
that multiplication by a quaternion of absolute value 1 is an isometry of
R4. Hamilton seems to have overlooked this important geometric fact, and
the quaternion representation of space rotations (Section 1.5) was ﬁrst pub-
lished by Cayley in 1845. Cayley also noticed that the corresponding for-
mulas for transforming the coordinates of R3 had been given by Rodrigues
in 1840. Cayley’s discovery showed that the noncommutative quaternion
product is a good thing, because space rotations are certainly noncommu-
tative; hence they can be faithfully represented only by a noncommutative
algebra. This ﬁnding has been enthusiastically endorsed by the computer
graphics profession today, which uses quaternions as a standard tool for

22
1
The geometry of complex numbers and quaternions
rendering 3-dimensional motion.
The quaternion algebra H plays two roles in Lie theory. On the one
hand, H gives the most understandable treatment of rotations in R3 and R4,
and hence of the rotation groups of these two spaces. The rotation groups
of R3 and R4 are Lie groups, and they illustrate many general features of
Lie theory in a way that is easy to visualize and compute. On the other
hand, H also provides coordinates for an inﬁnite series of spaces Hn, with
properties closely analogous to those of the spaces Rn and Cn. In particular,
we can generalize the concept of “rotation group” from Rn to both Cn and
Hn (see Chapter 3). It turns out that almost all Lie groups and Lie algebras
can be associated with the spaces Rn, Cn, or Hn, and these are the spaces
we are concerned with in this book.
However, we cannot fail to mention what falls outside our scope: the
8-dimensional algebra O of octonions. Octonions were discovered by a
friend of Hamilton, John Graves, in December 1843. Graves noticed that
the algebra of quaternions could be derived from Euler’s four-square iden-
tity, and he realized that an eight-square identity would similarly yield a
“product” of octuples with multiplicative absolute value. An eight-square
identity had in fact been published by the Danish mathematician Degen in
1818, but Graves did not know this. Instead, Graves discovered the eight-
square identity himself, and with it the algebra of octonions. The octonion
sum, as usual, is the vector sum, and the octonion product is not only non-
commutative but also nonassociative. That is, it is not generally the case
that u(vw) = (uv)w.
The nonassociative octonion product causes trouble both algebraically
and geometrically. On the algebraic side, one cannot represent octonions
by matrices, because the matrix product is associative. On the geometric
side, an octonion projective space (of more than two dimensions) is im-
possible, because of a theorem of Hilbert from 1899. Hilbert’s theorem
essentially states that the coordinates of a projective space satisfy the asso-
ciative law of multiplication (see Hilbert [1971]). One therefore has only
O itself, and the octonion projective plane, OP2, to work with. Because of
this, there are few important Lie groups associated with the octonions. But
these are a very select few! They are called the exceptional Lie groups, and
they are among the most interesting objects in mathematics. Unfortunately,
they are beyond the scope of this book, so we can mention them only in
passing.

2
Groups
PREVIEW
This chapter begins by reviewing some basic group theory—subgroups,
quotients, homomorphisms, and isomorphisms—in order to have a basis
for discussing Lie groups in general and simple Lie groups in particular.
We revisit the group S3 of unit quaternions, this time viewing its rela-
tion to the group SO(3) as a 2-to-1 homomorphism. It follows that S3 is
not a simple group. On the other hand, SO(3) is simple, as we show by a
direct geometric proof.
This discovery motivates much of Lie theory. There are inﬁnitely many
simple Lie groups, and most of them are generalizations of rotation groups
in some sense. However, deep ideas are involved in identifying the simple
groups and in showing that we have enumerated them all.
To show why it is not easy to identify all the simple Lie groups we
make a special study of SO(4), the rotation group of R4. Like SO(3),
SO(4) can be described with the help of quaternions. But a rotation of
R4 generally depends on two quaternions, and this gives SO(4) a special
structure, related to the direct product of S3 with itself. In particular, it
follows that SO(4) is not simple.
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 2,
23
c⃝Springer Science+Business Media, LLC 2008

24
2
Groups
2.1
Crash course on groups
For readers who would like a reminder of the basic properties of groups,
here is a crash course, oriented toward the kind of groups studied in this
book. Even those who have not seen groups before will be familiar with the
computational tricks—such as canceling by multiplying by the inverse—
since they are the same as those used in matrix computations.
First, a group G is a set with “product” and “inverse” operations, and
an identity element 1, with the following three basic properties:
g1(g2g3) = (g1g2)g3
for all g1,g2,g3 ∈G,
g1 = 1g = g
for all g ∈G,
gg−1 = g−1g = 1
for all g ∈G.
It should be mentioned that 1 is the unique element g′ such that gg′ = g
for all g ∈G, because multiplying the equation gg′ = g on the left by g−1
gives g′ = 1. Similarly, for each g ∈G, g−1 is the unique element g′′ such
that gg′′ = 1.
The above notation for “product,” “inverse,” and “identity” is called
multiplicative notation. It is used (sometimes with I, e, or 1 in place of 1)
for groups of numbers, quaternions, matrices, and all other groups whose
operation is called “product.” There are a few groups whose operation is
called “sum,” such as Rn under vector addition. For these we use additive
notation: g1 +g2 for the “sum” of g1,g2 ∈G, −g for the inverse of g ∈G,
and 0 (or 0) for the identity of G. Additive notation is used only when G is
abelian, that is, when g1 +g2 = g2 +g1 for all g1,g2 ∈G.
Since groups are generally not abelian, we have to speak of multiplying
h by g “on the left” or “on the right,” because gh and hg are generally
different. If we multiply all members g′ of a group G on the left by a
particular g ∈G, we get back all the members of G, because for any g′′ ∈G
there is a g′ ∈G such that gg′ = g′′ (namely g′ = g−1g′′).
Subgroups and cosets
To study a group G we look at the groups H contained in it, the subgroups
of G. For each subgroup H of G we have a decomposition of G into disjoint
pieces called the (left or right) cosets of H in G. The left cosets (which we
stick with, for the sake of consistency) are the sets of the form
gH = {gh : h ∈H}.

2.1
Crash course on groups
25
Thus H itself is the coset for g = 1, and in general a coset gH is “H trans-
lated by g,” though one cannot usually take the word “translation” literally.
One example for which this is literally true is G the plane R2 of points
(x,y) under vector addition, and H the subgroup of points (0,y). In this
case we use additive notation and write the coset of (x,y) as
(x,y)+H = {(x,y) : y ∈R},
where x is constant.
Then H is the y-axis and the coset (x,y) + H is H translated by the vector
(x,y) (see Figure 2.1). This example also illustrates how a group G decom-
x
0
H
(1,0)+H
(1,0)
(2,0)+H
(2,0)
Figure 2.1: Subgroup H of R2 and cosets.
poses into disjoint cosets (decomposing the plane into parallel lines), and
that different g ∈G can give the same coset gH. For example, (1,0) + H
and (1,1)+H are both the vertical line x = 1.
Each coset gH is in 1-to-1 correspondence with H because we get back
each h ∈H from gh ∈gH by multiplying on the left by g−1. Different
cosets are disjoint because if g ∈g1H and g ∈g2H then
g = g1h1 = g2h2
for some h1,h2 ∈H,
and therefore g1 = g2h2h−1
1 . But then
g1H = g2h2h−1
1 H = g2(h2h−1
1 H) = g2H
because h2h−1
1
∈H and therefore h2h−1
1 H = H by the remark at the end of
the last subsection (that multiplying a group by one of its members gives
back the group). Thus if two cosets have an element in common, they are
identical.

26
2
Groups
This algebraic argument has surprising geometric consequences; for
example, a ﬁlling of S3 by disjoint circles known as the Hopf ﬁbration.
Figure 2.2 shows some of the circles, projected stereographically into R3.
The circles ﬁll nested torus surfaces, one of which is shown in gray.
Figure 2.2: Some circles in the Hopf ﬁbration.
Proposition: S3 can be decomposed into disjoint congruent circles.
Proof. As we saw in Section 1.3, the quaternions a+ bi + cj + dk of unit
length satisfy
a2 +b2 +c2 +d2 = 1,
and hence they form a 3-sphere S3. The unit quaternions also form a group
G, because the product and inverse of unit quaternions are also unit quater-
nions, by the multiplicative property of absolute value.
One subgroup H of G consists of the unit quaternions of the form
cosθ + isinθ, and these form a unit circle in the plane spanned by 1 and
i. It follows that any coset qH is also a unit circle, because multiplica-
tion by a quaternion q of unit length is an isometry, as we saw in Section
1.4. Since the cosets qH ﬁll the whole group and are disjoint, we have a
decomposition of the 3-sphere into unit circles.
□

2.2
Crash course on homomorphisms
27
Exercises
An important nonabelian group (in fact, it is the simplest example of a nonabelian
Lie group) is the group of functions of the form
fa,b(x) = ax+b,
where a,b ∈R and a > 0.
The group operation is function composition.
2.1.1 If fa,b(x) = fa2,b2(fa1,b1(x)), work out a,b in terms of a1,b1,a2,b2, and
check that they are the same as the a,b determined by
a
b
0
1

=
a2
b2
0
1
a1
b1
0
1

.
2.1.2 Also show that the inverse function f −1
a,b (x) exists, and that it corresponds to
the inverse matrix
a
b
0
1
−1
.
This correspondence between functions and matrices is a matrix representation of
the group of functions fa,b. We have already seen examples of matrix representa-
tions of groups—such as the rotation groups in two and three dimensions—and,
in fact, most of the important Lie groups can be represented by matrices.
The unit complex numbers, cosθ +isinθ, form a group SO(2) that we began
to study in Section 1.1. We now investigate its subgroups.
2.1.3 Other than the trivial group {1}, what is the smallest subgroup of SO(2)?
2.1.4 Show that there is exactly one n-element subgroup of SO(2), for each natu-
ral number n, and list its members.
2.1.5 Show that the union R of all the ﬁnite subgroups of SO(2) is also a subgroup
(the group of “rational rotations”).
2.1.6 If z is a complex number not in the group R described in Exercise 2.1.5,
show that the numbers ...,z−2,z−1,1,z,z2,... are all distinct, and that they
form a subgroup of SO(2).
2.2
Crash course on homomorphisms
Normal subgroups
Since hg ̸= gh in general, it can also be that gH ̸= Hg, where
Hg = {hg : h ∈H}

28
2
Groups
is the right coset of H. If gH = Hg for all g ∈G, we say that H is a normal
subgroup of G. An equivalent statement is that H equals
g−1Hg = {g−1hg : h ∈H}
for each g ∈G.
(Because of this, it would be more sensible to call H “self-conjugate,” but
unfortunately the overused word “normal” has stuck.)
The good thing about a normal subgroup H is that its cosets themselves
form a group when “multiplied” by the rule that “the coset of g1, times the
coset of g2, equals the coset of g1g2”:
g1H ·g2H = g1g2H.
This rule makes sense for a normal subgroup H because if g′
1H = g1H and
g′
2H = g2H then g′
1g′
2H = g1g2H as follows:
g′
1g′
2H = g′
1Hg′
2
since g′
2H = Hg′
2 by normality,
= g1Hg′
2
since g′
1H = g1H by assumption,
= g1g′
2H
since g′
2H = Hg′
2 by normality,
= g1g2H
since g′
2H = g2H by assumption.
The group of cosets is called the quotient group of G by H, and is
written G/H. (When G and H are ﬁnite, the size of G/H is indeed the size
of G divided by the size of H.) We reiterate that the quotient group G/H
exists only when H is a normal subgroup. Another, more efﬁcient, way to
describe this situation is in terms of homomorphisms: structure-preserving
maps from one group to another.
Homomorphisms and isomorphisms
When H is a normal subgroup of G, the map ϕ : G →G/H deﬁned by
ϕ(g) = gH
for all g ∈G
preserves products in the sense that
ϕ(g1g2) = ϕ(g1)·ϕ(g2).
This follows immediately from the deﬁnition of product of cosets, because
ϕ(g1g2) = g1g2H = g1H ·g2H = ϕ(g1)·ϕ(g2).

2.2
Crash course on homomorphisms
29
In general, a map ϕ : G →G′ of one group into another is called a ho-
momorphism (from the Greek for “similar form”) if it preserves products.
A group homomorphism indeed preserves group structure, because it not
only preserves products, but also the identity and inverses. Here is why:
• Since g = 1g for any g ∈G, we have
ϕ(g) = ϕ(1g) = ϕ(1)ϕ(g)
because ϕ preserves products.
Multiplying both sides on the right by ϕ(g)−1 then gives 1 = ϕ(1).
• Since 1 = gg−1 for any g ∈G, we have
1 = ϕ(1) = ϕ(gg−1) = ϕ(g)ϕ(g−1)
because ϕ preserves products.
This says that ϕ(g−1) = ϕ(g)−1, because the inverse of ϕ(g) is
unique.
Thus the image ϕ(G) is of “similar” form to G, but we say that G′ is
isomorphic (of the “same form”) to G only when the map ϕ is 1-to-1 and
onto (in which case we call ϕ an isomorphism). In general, ϕ(G) is only a
shadow of G, because many elements of G may map to the same element
of G′. The case furthest from isomorphism is that in which ϕ sends all
elements of G to 1.
Any homomorphism ϕ of G onto G′ can be viewed as the special type
ϕ : G →G/H. The appropriate normal subgroup H of G is the so-called
kernel of ϕ:
H = ker ϕ = {g ∈G : ϕ(g) = 1}.
Then G′ is isomorphic to the group G/ker ϕ of cosets of ker ϕ because:
1. ker ϕ is a group, because
h1,h2 ∈ker ϕ ⇒ϕ(h1) = ϕ(h2) = 1
⇒ϕ(h1)ϕ(h2) = 1
⇒ϕ(h1h2) = 1
⇒h1h2 ∈ker ϕ

30
2
Groups
and
h ∈ker ϕ ⇒ϕ(h) = 1
⇒ϕ(h)−1 = 1
⇒ϕ(h−1) = 1
⇒h−1 ∈ker ϕ.
2. ker ϕ is a normal subgroup of G, because, for any g ∈G,
h ∈ker ϕ ⇒ϕ(ghg−1) = ϕ(g)ϕ(h)ϕ(g−1) = ϕ(g)1ϕ(g)−1 = 1
⇒ghg−1 ∈ker ϕ.
Hence g(ker ϕ)g−1 = ker ϕ, that is, ker ϕ is normal.
3. Each g′ = ϕ(g) ∈G′ corresponds to the coset g(ker ϕ).
In fact, g(ker ϕ) = ϕ−1(g′), because
k ∈ϕ−1(g′) ⇔ϕ(k) = g′
(deﬁnition of ϕ−1)
⇔ϕ(k) = ϕ(g)
⇔ϕ(g)−1ϕ(k) = 1
⇔ϕ(g−1k) = 1
⇔g−1k ∈ker ϕ
⇔k ∈g(ker ϕ).
4. Products of elements of g′
1,g′
2 ∈G′ correspond to products of the
corresponding cosets:
g′
1 =ϕ(g1),g′
2 =ϕ(g2) ⇒ϕ−1(g′
1)=g1(ker ϕ),ϕ−1(g′
2)=g2(ker ϕ)
by step 3. But also
g′
1 = ϕ(g1),g′
2 = ϕ(g2) ⇒g′
1g′
2 = ϕ(g1)ϕ(g2) = ϕ(g1g2)
⇒ϕ−1(g′
1g′
2) = g1g2(ker ϕ),
also by step 3. Thus the product g′
1g′
2 corresponds to g1g2(ker ϕ),
which is the product of the cosets corresponding to g′
1 and g′
2 respec-
tively.
To sum up: a group homomorphism ϕ of G onto G′ gives a 1-to-1 corre-
spondence between G′ and G/(ker ϕ) that preserves products, that is, G′
is isomorphic to G/(ker ϕ).
This result is called the fundamental homomorphism theorem for
groups.

2.2
Crash course on homomorphisms
31
The det homomorphism
An important homomorphism for real and complex matrix groups G is the
determinant map
det : G →C×,
where C× denotes the multiplicative group of nonzero complex numbers.
The determinant map is a homomorphism because det is multiplicative—
det(AB) = det(A)det(B)—a fact well known from linear algebra.
The kernel of det, consisting of the matrices with determinant 1, is
therefore a normal subgroup of G. Many important Lie groups arise in
precisely this way, as we will see in Chapter 3.
Simple groups
A many-to-1 homomorphism of a group G maps it onto a group G′ that
is “simpler” than G (or, at any rate, not more complicated than G). For
this reason, groups that admit no such homomorphism, other than the ho-
momorphism sending all elements to 1, are called simple. Equivalently, a
nontrivial group is simple if it contains no normal subgroups other than
itself and the trivial group.
One of the main goals of group theory in general, and Lie group theory
in particular, is to ﬁnd all the simple groups. We ﬁnd the ﬁrst interesting
example in the next section.
Exercises
2.2.1 Check that z →z2 is a homomorphism of S1. What is its kernel? What are
the cosets of the kernel?
2.2.2 Show directly (that is, without appealing to Exercise 2.2.1) that pairs {±zα},
where zα = cosα +isinα, form a group G when pairs are multiplied by the
rule
{±zα} ·{±zβ} = {±(zαzβ)}.
Show also that the function ϕ : S1 →G that sends both zα,−zα ∈S1 to the
pair {±zα} is a 2-to-1 homomorphism.
2.2.3 Show that z →z2 is a well-deﬁned map from G onto S1, where G is the
group described in Exercise 2.2.2, and that this map is an isomorphism.
The space that consists of the pairs {±zα} of opposite (or “antipodal”) points
on the circle is called the real projective line RP1. Thus the above exercises

32
2
Groups
show that the real projective line has a natural group structure, under which it is
isomorphic to the circle group S1.
In the next section we will consider the real projective space RP3, consisting
of the antipodal point pairs {±q} on the 3-sphere S3. These pairs likewise have
a natural product operation, which makes RP3 a group—in fact, it is the group
SO(3) of rotations of R3. We will show that RP3 is not the same group as S3,
because SO(3) is simple and S3 is not.
We can see right now that S3 is not simple, by ﬁnding a nontrivial normal
subgroup.
2.2.4 Show that {±1} is a normal subgroup of S3.
However, it turns out that {±1} is the only nontrivial normal subgroup of S3.
In particular, the subgroup S1 that we found in Section 2.1 is not normal.
2.2.5 Show that S1 is not a normal subgroup of S3.
2.3
The groups SU(2) and SO(3)
The group SO(2) of rotations of R2 about O can be viewed as a geometric
object, namely the unit circle in the plane, as we observed in Section 1.1.
The unit circle, S1, is the ﬁrst in the series of unit n-spheres Sn, the nth
of which consists of the points at distance 1 from the origin in Rn+1. Thus
S2 is the ordinary sphere, consisting of the points at distance 1 from the
origin in R3. Unfortunately (for those who would like an example of an
easily visualized but nontrivial Lie group) there is no rule for multiplying
points that makes S2 a Lie group. In fact, the only other Lie group among
the n-spheres is S3. As we saw in Section 1.3, it becomes a group when
its points are viewed as unit quaternions, under the operation of quaternion
multiplication.
The group S3 of unit quaternions can also be viewed as the group of
2×2 complex matrices of the form
Q =
a+di
−b−ci
b−ci
a−di

,
where
det(Q) = 1,
because these are precisely the quaternions of absolute value 1. Such matri-
ces are called unitary, and the group S3 is also known as the special unitary
group SU(2). Unitary matrices are the complex counterpart of orthogonal
matrices, and we study the analogy between the two in Chapters 3 and 4.
The group SU(2) is closely related to the group SO(3) of rotations
of R3. As we saw in Section 1.5, rotations of R3 correspond 1-to-1 to

2.3
The groups SU(2) and SO(3)
33
the pairs ±t of antipodal unit quaternions, the rotation being induced on
Ri+Rj+Rk by the conjugation map q →t−1qt. Also, the group operation
of SO(3) corresponds to quaternion multiplication, because if one rotation
is induced by conjugation by t1, and another by conjugation by t2, then
conjugation by t1t2 induces the product rotation (ﬁrst rotation followed by
the second). Of course, we multiply pairs ±t of quaternions by the rule
(±t1)(±t2) = ±t1t2.
We therefore identify SO(3) with the group RP3 of unit quaternion
pairs ±t under this product operation. The map ϕ : SU(2) →SO(3) deﬁned
by ϕ(t) = {±t} is a 2-to-1 homomorphism, because the two elements t and
−t of SU(2) go to the single pair ±t in SO(3). Thus SO(3) looks “simpler”
than SU(2) because SO(3) has only one element where SU(2) has two.
Indeed, SO(3) is “simpler” because SU(2) is not simple—it has the normal
subgroup {±1}—and SO(3) is. We now prove this famous property of
SO(3) by showing that SO(3) has no nontrivial normal subgroup.
Simplicity of SO(3). The only nontrivial subgroup of SO(3) closed under
conjugation is SO(3) itself.
Proof. Suppose that H is a nontrivial subgroup of SO(3), so H includes a
nontrivial rotation, say the rotation h about axis l through angle α.
Now suppose that H is normal, so H also includes all elements g−1hg
for g ∈SO(3). If g moves axis l to axis m, then g−1hg is the rotation about
axis m through angle α. (In detail, g−1 moves m to l, h rotates through
angle α about l, then g moves l back to m.) Thus the normal subgroup H
includes the rotations through angle α about all possible axes.
Now a rotation through α about P, followed by rotation through α
about Q, equals rotation through angle θ about R, where R and θ are as
shown in Figure 2.3. As in Exercise 1.5.6, we obtain the rotation about
P by successive reﬂections in the great circles PR and PQ, and then the
rotation about Q by successive reﬂections in the great circles PQ and QR.
In this sequence of four reﬂections, the reﬂections in PQ cancel out, leaving
the reﬂections in PR and QR that deﬁne the rotation about R.
As P varies continuously over some interval of the great circle through
P and Q, θ varies continuously over some interval. (R may also vary, but
this does not matter.) It follows that θ takes some value of the form
mπ
n ,
where m is odd,

34
2
Groups
R
θ/2
P
α/2
Q
α/2
Figure 2.3: Angle of the product rotation.
because such numbers are dense in R. The n-fold product of this rotation
also belongs to H, and it is a rotation about R through mπ, where m is odd.
The latter rotation is simply rotation through π, so H includes rotations
through π about any point on the sphere (by conjugation with a suitable g
again).
Finally, taking the product of rotations with α/2 = π/2 in Figure 2.3,
it is clear that we can get a rotation about R through any angle θ between
0 and 2π. Hence H includes all the rotations in SO(3).
□
Exercises
Like SO(2), SO(3) contains some ﬁnite subgroups. It contains all the ﬁnite sub-
groups of SO(2) in an obvious way (as rotations of R3 about a ﬁxed axis), but
also three more interesting subgroups called the polyhedral groups. Each poly-
hedral group is so called because it consists of the rotations that map a regular
polyhedron into itself.
Here we consider the group of 12 rotations that map a regular tetrahedron
into itself. We consider the tetrahedron whose vertices are alternate vertices of the
unit cube in Ri + Rj + Rk, where the cube has center at O and edges parallel to
the i, j, and k axes (Figure 2.4).
First, let us see why there are indeed 12 rotations that map the tetrahedron
into itself. To do this, observe that the position of the tetrahedron is completely
determined when we know
• Which of the four faces is in the position of the front face in Figure 2.4.

2.3
The groups SU(2) and SO(3)
35
• Which of the three edges of that face is at the bottom of the front face in
Figure 2.4.
Figure 2.4: The tetrahedron and the cube.
2.3.1 Explain why this observation implies 12 possible positions of the tetrahe-
dron, and also explain why all these positions can be obtained by rotations.
2.3.2 Similarly, explain why there are 24 rotations that map the cube into itself
(so the rotation group of the tetrahedron is different from the rotation group
of the cube).
The 12 rotations of the tetrahedron are in fact easy to enumerate with the help
of Figure 2.5. As is clear from the ﬁgure, the tetrahedron is mapped into itself by
two types of rotation:
• A 1/2 turn about each line through the centers of opposite edges.
• A 1/3 turn about each line through a vertex and the opposite face center.
2.3.3 Show that there are 11 distinct rotations among these two types. What
rotation accounts for the 12th position of the tetrahedron?
Now we make use of the quaternion representation of rotations from Section
1.5. Remember that a rotation about axis u through angle θ corresponds to the
quaternion pair ±q, where
q = cos θ
2 +usin θ
2 .
2.3.4 Show that the identity, and the three 1/2 turns, correspond to the four quater-
nion pairs ±1,±i,±j,±k.

36
2
Groups
1/2 turn
1/3 turn
Figure 2.5: The tetrahedron and axes of rotation.
2.3.5 Show that the 1/3 turns correspond to the eight antipodal pairs among the
16 quaternions
±1
2 ± i
2 ± j
2 ± k
2 .
The 24 quaternions obtained in Exercises 2.3.4 and 2.3.5 form an exceptionally
symmetric conﬁguration in R4. They are the vertices of a regular ﬁgure called the
24-cell, copies of which form a “tiling” of R4.
2.4
Isometries of Rn and reﬂections
In this section we take up an idea that appeared brieﬂy in the exercises
for Section 1.5: the representation of isometries as products of reﬂections.
There we showed that certain isometries of R2 and R3 are products of
reﬂections. Here we represent isometries of Rn as products of reﬂections,
and in the next section we use this result to describe the rotations of R4.
We actually prove that any isometry of Rn that ﬁxes O is the product
of reﬂections in hyperplanes through O, and then specialize to orientation-
preserving isometries. A hyperplane H through O is an (n−1)-dimensional
subspace of Rn, and reﬂection in H is the linear map of Rn that ﬁxes the
elements in H and reverses the vectors orthogonal to H.

2.4
Isometries of Rn and reﬂections
37
Reﬂection representation of isometries. Any isometry of Rn that ﬁxes O
is the product of at most n reﬂections in hyperplanes through O.
Proof. We argue by induction on n. For n = 1 the result is the obvious one
that the only isometries of R ﬁxing O are the identity and the map x →−x,
which is reﬂection in O.
Now suppose that the result is true for n = k −1 and that f is an isom-
etry of Rk ﬁxing O. If f is not the identity, suppose that v ∈Rk is such
that f(v) = w ̸= v. Then the reﬂection ru in the hyperplane orthogonal to
u = v −w maps the subspace Ru of real multiples of u onto itself and the
map ru f (“f followed by ru”) is the identity on the subspace Ru of Rk.
The restriction of ru f to the Rk−1 orthogonal to Ru is, by induction,
the product of ≤k −1 reﬂections. It follows that f = rug, where g is the
product of ≤k −1 reﬂections.
Therefore, f is the product of ≤k reﬂections, and the result is true for
all n by induction.
□
It follows in particular that any orientation-preserving isometry of R3
is the product of 0 or 2 reﬂections (because the product of an odd number
of reﬂections reverses orientation). Thus any such isometry is a rotation
about an axis passing through O.
This theorem is sometimes known as the Cartan–Dieudonn´e theorem,
after a more general theorem proved by Cartan [1938], and generalized
further by Dieudonn´e. Cartan’s theorem concerns “reﬂections” in spaces
with real or complex coordinates, and Dieudonn´e’s extends it to spaces
with coordinates from ﬁnite ﬁelds.
Exercises
Assuming that reﬂections are linear, the representation of isometries as products
of reﬂections shows that all isometries ﬁxing the origin are linear maps. In fact,
there is nice direct proof that all such isometries (including reﬂections) are linear,
pointed out to me by Marc Ryser. We suppose that f is an isometry that ﬁxes O,
and that u and v are any points in Rn.
2.4.1 Prove that f preserves straight lines and midpoints of line segments.
2.4.2 Using the fact that u + v is the midpoint of the line joining 2u and 2v, and
Exercise 2.4.1, show that f(u +v) = f(u)+ f(v).
2.4.3 Also prove that f(ru) = r f(u) for any real number r.
It is also true that reﬂections have determinant −1, hence the determinant detects
the “reversal of orientation” effected by a reﬂection.

38
2
Groups
2.4.4 Show that reﬂection in the hyperplane orthogonal to a coordinate axis has
determinant −1, and generalize this result to any reﬂection.
2.5
Rotations of R4 and pairs of quaternions
A linear map is called orientation-preserving if its determinant is positive,
and orientation-reversing otherwise. Reﬂections are linear and orientation-
reversing, so a product of reﬂections is orientation-preserving if and only
if it contains an even number of terms. We deﬁne a rotation of Rn about O
to be an orientation-preserving isometry that ﬁxes O.
Thus it follows from the Cartan–Dieudonn´e theorem that any rotation
of R4 is the product of 0, 2, or 4 reﬂections. The exact number is not impor-
tant here—what we really want is a way to represent reﬂections by quater-
nions, as a stepping-stone to the representation of rotations by quaternions.
Not surprisingly, each reﬂection is speciﬁed by the quaternion orthogonal
to the hyperplane of reﬂection. More surprisingly, a rotation is speciﬁed
by just two quaternions, regardless of the number of reﬂections needed to
compose it. Our proof follows Conway and Smith [2003], p. 41.
Quaternion representation of reﬂections. Reﬂection of H = R4 in the
hyperplane through O orthogonal to the unit quaternion u is the map that
sends each q ∈H to −uqu.
Proof. First observe that the map q →−uqu is an isometry. This is because
• q →−q reverses the real part of q and keeps the imaginary part ﬁxed,
hence it is reﬂection in the hyperplane spanned by i, j, and k.
• Multiplication on the left by the unit quaternion u is an isometry
by the argument in Section 1.4, and there is a similar argument for
multiplication on the right.
Next notice that the map q →−uqu sends
vu to −u(vu)u = −uu vu
because (vu) = u v,
= −vu
because uu = |u|2 = 1.
In particular, the map sends u to −u, so vectors parallel to u are reversed.
And it sends iu to iu, because i = −i, and similarly ju to ju and ku to ku.
Thus the vectors iu, ju, and ku, which span the hyperplane orthogonal to
u, are ﬁxed. Hence the map q →−uqu is reﬂection in this hyperplane. □

2.5
Rotations of R4 and pairs of quaternions
39
Quaternion representation of rotations. Any rotation of H = R4 about
O is a map of the form q →vqw, where v and w are unit quaternions.
Proof. It follows from the quaternion representation of reﬂections that the
result of successive reﬂections in the hyperplanes orthogonal to the unit
quaternions u1,u2,...,u2n is the map
q →u2n ···u3u2u1 q u1u2u3 ···u2n,
because an even number of sign changes and conjugations makes no
change.
The pre- and postmultipliers are in general two different unit
quaternions, u2n ···u3u2u1 = v and u1u2u3 ···u2n = w, say, so the general
rotation of R4 is a map of the form
q →vqw,
where v and w are unit quaternions.
Conversely, any map of this form is a rotation, because multiplication
of H = R4 on either side by a unit quaternion is an orientation-preserving
isometry. We already know that multiplication by a unit quaternion is an
isometry, by Section 1.4. And it preserves orientation by the following
argument.
Multiplication of H = R4 by a unit quaternion
v =
a+id
−b−ic
b−ic
a−id

,
where
a2 +b2 +c2 +d2 = 1,
is a linear transformation of R4 with matrix
Rv =
⎛
⎜
⎝
a −d −b
c
d
a
−c −b
b
c
a
d
−c b
−d a
⎞
⎟
⎠,
where the 2×2 submatrices represent the complex-number entries in v. It
can be checked that det(Rv) = 1. So multiplication by v, on either side,
preserves orientation.
□
Exercises
The following exercises study the rotation q →iq of H = R4, ﬁrst expressing it as a
product of “plane rotations”—of the planes spanned by 1, i and j, k respectively—
then breaking it down to a product of four reﬂections.

40
2
Groups
2.5.1 Check that q →iq sends 1 to i, i to −1 and j to k, k to −j. How many points
of R4 are ﬁxed by this map?
2.5.2 Show that the rotation that sends 1 to i, i to −1 and leaves j, k ﬁxed is
the product of reﬂections in the hyperplanes orthogonal to u1 = i and u2 =
(i−1)/
√
2.
2.5.3 Show that the rotation that sends j to k, k to −j and leaves 1, i ﬁxed is the
product of reﬂections in the hyperplanes orthogonal to u3 = k and u4 =
(k−j)/
√
2.
It follows, by the formula q →−uqu for reﬂection, that the product of rota-
tions in Exercises 2.5.2 and 2.5.3 is the product
q →u4u3u2u1 q u1u2u3u4
of reﬂections in the hyperplanes orthogonal to u1,u2,u3,u4 respectively.
2.5.4 Check that u4u3u2u1 = i and u1u2u3u4 = 1, so the product of the four re-
ﬂections is indeed q →iq.
2.6
Direct products of groups
Before we analyze rotations of R4 from the viewpoint of group theory, it is
desirable to review the concept of direct product or Cartesian product of
groups.
Deﬁnition. If A and B are groups then their direct product A×B is the set
of ordered pairs (a,b), where a ∈A and b ∈B, under the “product of pairs”
operation deﬁned by
(a1,b1)(a2,b2) = (a1a2,b1b2).
It is easy to check that this product operation is associative, that the
identity element of A × B is the pair (1A,1B), where 1A is the identity of
A and 1B is the identity of B, and that (a,b) has inverse (a−1,b−1). Thus
A×B is indeed a group.
Many important groups are nontrivial direct products; that is, they have
the form A×B where neither A nor B is the trivial group {1}. For example:
• The group R2, under vector addition, is the direct product R × R.
More generally, Rn is the n-fold direct product R×R×···×R.

2.6
Direct products of groups
41
• If A and B are groups of n×n matrices, then the matrices of the form
a
0
0
b

,
where
a ∈A
and
b ∈B,
make up a group isomorphic to A × B under matrix multiplication,
where 0 is the n × n zero matrix. This is because of the so-called
block multiplication of matrices, according to which
a1
0
0
b1
a2
0
0
b2

=
a1a2
0
0
b1b2

.
• It follows, from the previous item, that Rn is isomorphic to a 2n×2n
matrix group, because R is isomorphic to the group of matrices
1
x
0
1

where
x ∈R.
• The group S1 ×S1 is a group called the (two-dimensional) torus T2.
More generally, the n-fold direct product of S1 factors is called the
n-dimensional torus Tn.
We call S1 × S1 a torus because its elements (θ,φ), where θ,φ ∈S1,
can be viewed as the points on the torus surface (Figure 2.6).
θ
φ
(θ,φ)
Figure 2.6: The torus S1 ×S1.
Since the groups R and S1 are abelian, the same is true of all their
direct products Rm ×Tn. It can be shown that the latter groups include all
the connected abelian matrix Lie groups.

42
2
Groups
Exercises
If we let x1,x2,x3,x4 be the coordinates along mutually orthogonal axes in R4,
then it is possible to “rotate” the x1 and x2 axes while keeping the x3 and x4 axes
ﬁxed.
2.6.1 Write a 4 × 4 matrix for the transformation that rotates the (x1,x2)-plane
through angle θ while keeping the x3- and x4-axes ﬁxed.
2.6.2 Write a 4 × 4 matrix for the transformation that rotates the (x3,x4)-plane
through angle φ while keeping the x1- and x2-axes ﬁxed.
2.6.3 Observe that the rotations in Exercise 2.6.1 form an S1, as do the rotations
in Exercise 2.6.2, and deduce that SO(4) contains a subgroup isomorphic
to T2.
The groups of the form Rm ×Tn may be called “generalized cylinders,” based
on the simplest example R×S1.
2.6.4 Why is it appropriate to call the group R×S1 a cylinder?
The notation Sn is unfortunately not compatible with the direct product nota-
tion (at least not the way the notation Rn is).
2.6.5 Explain why S3 = SU(2) is not the same group as S1 ×S1 ×S1.
2.7
The map from SU(2)×SU(2) to SO(4)
In Section 2.5 we showed that the rotations of R4 are precisely the maps
q →vqw, where v and w run through all the unit quaternions. Since v−1
is a unit quaternion if and only if v is, it is equally valid to represent each
rotation of R4 by a map of the form q →v−1qw, where v and w are unit
quaternions. The latter representation is more convenient for what comes
next.
The pairs of unit quaternions (v,w) form a group under the operation
deﬁned by
(v1,w1)·(v2,w2) = (v1v2,w1w2),
where the products v1v2 and w1w2 on the right side are ordinary quaternion
products. Since the v come from the group SU(2) of unit quaternions, and
the w likewise, the group of pairs (v,w) is the direct product SU(2)×SU(2)
of SU(2) with itself.
The map that sends each pair (v,w) ∈SU(2) × SU(2) to the rotation
q →v−1qw in SO(4) is a homomorphism ϕ : SU(2) × SU(2) →SO(4).
This is because

2.7
The map from SU(2)×SU(2) to SO(4)
43
• the product of the map q →v−1
1 qw1 corresponding to (v1,w1)
• with the map q →v−1
2 qw2 corresponding to (v2,w2)
• is the map q →v−1
2 v−1
1 qw1w2,
• which is the map q →(v1v2)−1q(w1w2) corresponding to the product
(v1v2,w1w2) of (v1,w1) and (v2,w2).
This homomorphism is onto SO(4), because each rotation of R4 can
be expressed in the form q →v−1qw, but one might expect it to be very
many-to-one, since many pairs (v,w) of unit quaternions conceivably give
the same rotation. Surprisingly, this is not so. The representation of ro-
tations by pairs is “unique up to sign” in the following sense: if (v,w)
gives a certain rotation, the only other pair that gives the same rotation is
(−v,−w).
To prove this, it sufﬁces to prove that the kernel of the homomorphism
ϕ : SU(2)×SU(2) →SO(4) has two elements.
Size of the kernel. The homomorphism ϕ : SU(2) × SU(2) →SO(4) is
2-to-1, because its kernel has two elements.
Proof. Suppose that (v,w) is in the kernel, so q →v−1qw is the identity
rotation. In particular, this rotation ﬁxes 1, so
v−11w = 1;
hence
v = w.
Thus the map is in fact q →v−1qv, which we know (from Section 1.5) ﬁxes
the real axis and rotates the space of pure imaginary quaternions. Only if
v = 1 or v = −1 does the map ﬁx everything; hence the kernel of ϕ has
only two elements, (1,1) and (−1,−1).
The left cosets of the kernel are therefore the 2-element sets
(v,w)(±1,±1) = (±v,±w),
and each coset corresponds to a distinct rotation of R4, by the fundamental
homomorphism theorem of Section 2.2.
□
This theorem shows that SO(4) is “almost” the same as SU(2)×SU(2),
and the latter is far from being a simple group. For example, the subgroup
of pairs (v,1) is a nontrivial normal subgroup, but clearly not the whole of
SU(2)×SU(2). This gives us a way to show that SO(4) is not simple.

44
2
Groups
SO(4) is not simple. There is a nontrivial normal subgroup of SO(4), not
equal to SO(4).
Proof. The subgroup of pairs (v,1) ∈SU(2)×SU(2) is normal; in fact, it
is the kernel of the map (v,w) →(1,w), which is clearly a homomorphism.
The corresponding subgroup of SO(4) consists of maps of the form
q →v−1q1, which likewise form a normal subgroup of SO(4). But this
subgroup is not the whole of SO(4). For example, it does not include the
map q →qw for any w ̸= ±1, by the “unique up to sign” representation of
rotations by pairs (v,w).
□
Exercises
An interesting subgroup Aut(H) of SO(4) consists of the continuous automor-
phisms of H = R4. These are the continuous bijections ρ : H →H that preserve
the quaternion sum and product, that is,
ρ(p +q) = ρ(p)+ρ(q),
ρ(pq) = ρ(p)ρ(q)
for any p,q ∈H.
It is easy to check that, for each unit quaternion u, the ρ that sends q →u−1qu
is an automorphism (ﬁrst exercise), so it follows from Section 1.5 that Aut(H)
includes the SO(3) of rotations of the 3-dimensional subspace Ri + Rj + Rk of
pure imaginary quaternions. The purpose of this set of exercises is to show that
all continuous automorphisms of H are of this form, so Aut(H) = SO(3).
2.7.1 Check that q →u−1qu is an automorphism of H for any unit quaternion u.
Now suppose that ρ is any automorphism of H.
2.7.2 Use the preservation of sums by an automorphism ρ to deduce in turn that
• ρ preserves 0, that is, ρ(0) = 0,
• ρ preserves differences, that is, ρ(p −q) = ρ(p)−ρ(q).
2.7.3 Use preservation of products to deduce that
• ρ preserves 1, that is, ρ(1) = 1,
• ρ preserves quotients, that is, ρ(p/q) = ρ(p)/ρ(q) for q ̸= 0.
2.7.4 Deduce from Exercises 2.7.2 and 2.7.3 that ρ(m/n) = m/n for any integers
m and n ̸= 0. This implies ρ(r) = r for any real r, and hence that ρ is a
linear map of R4. Why?
Thus we now know that a continuous automorphism ρ is a linear bijection
of R4 that preserves the real axis, and hence ρ maps Ri+Rj+Rk onto itself. It
remains to show that the restriction of ρ to Ri+Rj+Rk is a rotation, that is, an
orientation-preserving isometry, because we know from Section 1.5 that rotations
of Ri+Rj+Rk are of the form q →u−1qu.

2.8
Discussion
45
2.7.5 Prove in turn that
• ρ preserves conjugates, that is, ρ(q) = ρ(q),
• ρ preserves distance,
• ρ preserves inner product in Ri+Rj+Rk,
• ρ(p × q) = ρ(p) × ρ(q) in Ri + Rj + Rk, and hence ρ preserves
orientation.
The appearance of SO(3) as the automorphism group of the quaternion al-
gebra H suggests that the automorphism group of the octonion algebra O might
also be of interest. It turns out to be a 14-dimensional group called G2—the ﬁrst
of the exceptional Lie groups mentioned (along with O) in Section 1.6. This link
between O and the exceptional groups was pointed out by Cartan [1908].
2.8
Discussion
The concept of simple group emerged around 1830 from Galois’s theory
of equations. Galois showed that each polynomial equation has a ﬁnite
group of “symmetries” (permutations of its roots that leave its coefﬁcients
invariant), and that the equation is solvable only if its group decomposes
in a certain way. In particular, the general quintic equation is not solvable
because its group contains the nonabelian simple group A5—the group of
even permutations of ﬁve objects. The same applies to the general equation
of any degree greater than 5, because An, the group of even permutations
of n objects, is simple for any n ≥5.
With this discovery, Galois effectively closed the classical theory of
equations, but he opened the (much larger) theory of groups.
Speciﬁ-
cally, by exhibiting the nontrivial inﬁnite family An for n ≥5, he raised
the problem of ﬁnding and classifying all ﬁnite simple groups. This prob-
lem is much deeper than anyone could have imagined in the time of Galois,
because it depends on solving the corresponding problem for continuous
groups, or Lie groups as we now call them.
Around 1870, Sophus Lie was inspired by Galois theory to develop an
analogous theory of differential equations and their “symmetries,” which
generally form continuous groups. As with polynomial equations, simple
groups raise an obstacle to solvability. However, at that time it was not
clear what the generalization of the group concept from ﬁnite to continuous
should be. Lie understood continuous groups to be groups generated by
“inﬁnitesimal” elements, so he thought that the rotation group of R3 should

46
2
Groups
include “inﬁnitesimal rotations.” Today, we separate out the “inﬁnitesimal
rotations” of R3 in a structure called so(3), the Lie algebra of SO(3). The
concept of simplicity also makes sense for so(3), and is somewhat easier
to establish. Indeed, the inﬁnitesimal elements of any continuous group G
form a structure g now called the Lie algebra of G, which captures most
of the structure of G but is easier to handle. We discuss “inﬁnitesimal
elements,” and their modern counterparts, further in Section 4.3.
It was a stroke of luck (or genius) that Lie decided to look at inﬁnitesi-
mal elements, because it enabled him to prove simplicity for whole inﬁnite
families of Lie algebras in one fell swoop. (As we will see later, most of
the corresponding continuous groups are not quite simple, and one has to
tease out certain small subgroups and quotient by them.) Around 1885 Lie
proved results so general that they cover all but a ﬁnite number of simple
Lie algebras—namely, those of the exceptional groups mentioned at the
end of Chapter 1 (see Hawkins [2000], pp. 92–98).
In the avalanche of Lie’s results, the special case of so(3) and SO(3)
seems to have gone unnoticed. It gradually came to light as twentieth-
century books on Lie theory started to work out special cases of geometric
interest by way of illustration. In the 1920s, quantum physics also directed
attention to SO(3), since rotations in three dimensions are physically sig-
niﬁcant. Still, it is remarkable that a purely geometric argument for the
simplicity of SO(3) took so long to emerge. Perhaps its belated appear-
ance is due to its topological content, namely, the step that depends purely
on continuity. The argument hinges on the fact that θ is a continuous func-
tion of distance along the great circle PQ, and that such a function takes
every value between its extreme values: the so-called intermediate value
theorem.
The theory of continuity (topology) came after the theory of continuous
groups—not surprisingly, since one does not bother to develop a theory
of continuity before seeing that it has some content—and applications of
topology to group theory were rare before the 1920s. In this book we will
present further isolated examples of continuity arguments in Sections 3.2,
3.8, and 7.5 before taking up topology systematically in Chapter 8.
Another book with a strongly geometric treatment of SO(3) is Berger
[1987]. Volume I of Berger, p. 169, has a simplicity proof for SO(3) similar
to the one given here, and it is extended to a simplicity result about SO(n),
for n ≥5, on p. 170: SO(2m+1) is simple and the only nontrivial normal
subgroup of SO(2m) is {±1}. We arrive at the same result by a different

2.8
Discussion
47
route in Section 7.5. (Our route is longer, but it also takes in the complex
and quaternion analogues of SO(n).) Berger treats SO(4) with the help
of quaternions on p. 190 of his Volume II, much as we have done here.
The quaternion representation of rotations of R4 was another of Cayley’s
discoveries, made in 1855.
Lie observed the anomalous structure of SO(4) at the inﬁnitesimal
level. He mentions it, in scarcely recognizable form, on p. 683 of Volume
III of his 1893 book Theorie der Transformationsgruppen. The anomaly of
SO(4) is hidden in some modern treatments of Lie theory, where the con-
cept of simplicity is superseded by the more general concept of semisim-
plicity. All simple groups are semisimple, and SO(4) is semisimple, so an
anomaly is removed by relaxing the concept of “simple” to “semisimple.”
However, the concept of semisimplicity makes little sense before one has
absorbed the concept of simplicity, and our goal in this book is to under-
stand the simple groups, notwithstanding the anomaly of SO(4).

3
Generalized rotation groups
PREVIEW
In this chapter we generalize the plane and space rotation groups SO(2)
and SO(3) to the special orthogonal group SO(n) of orientation-preserving
isometries of Rn that ﬁx O. To deal uniformly with the concept of “rota-
tion” in all dimensions we make use of the standard inner product on Rn
and consider the linear transformations that preserve it.
Such transformations have determinant +1 or −1 according as they
preserve orientation or not, so SO(n) consists of those with determinant 1.
Those with determinant ±1 make up the full orthogonal group, O(n).
These ideas generalize further, to the space Cn with inner product de-
ﬁned by
(u1,u2,...,un)·(v1,v2,...,vn) = u1v1 +u2v2 +···+unvn.
(*)
The group of linear transformations of Cn preserving (*) is called the uni-
tary group U(n), and the subgroup of transformations with determinant 1
is the special unitary group SU(n).
There is one more generalization of the concept of isometry—to the
space Hn of ordered n-tuples of quaternions. Hn has an inner product de-
ﬁned like (*) (but with quaternion conjugates), and the group of linear
transformations preserving it is called the symplectic group Sp(n).
In the rest of the chapter we work out some easily accessible properties
of the generalized rotation groups: their maximal tori, centers, and their
path-connectedness. These properties later turn out to be crucial for the
problem of identifying simple Lie groups.
48
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 3,
c⃝Springer Science+Business Media, LLC 2008

3.1
Rotations as orthogonal transformations
49
3.1
Rotations as orthogonal transformations
It follows from the Cartan–Dieudonn´e theorem of Section 2.4 that a rota-
tion about O in R2 or R3 is a linear transformation that preserves length
and orientation. We therefore adopt this description as the deﬁnition of a
rotation in Rn. However, when the transformation is given by a matrix,
it is not easy to see directly whether it preserves length or orientation. A
more practical criterion emerges from consideration of the standard inner
product in Rn, whose geometric properties we now summarize.
If u = (u1,u2,...,un) and v = (v1,v2,...,vn) are two vectors in Rn,
their inner product u·v is deﬁned by
u·v = u1v1 +u2v2 +···+unvn.
It follows immediately that
u·u = u2
1 +u2
2 +···+u2
n = |u|2,
so the length |u| of u (that is, the distance of u from the origin 0) is deﬁn-
able in terms of the inner product. It also follows (as one learns in linear
algebra courses) that u · v = 0 if and only if u and v are orthogonal, and
more generally that
u·v = |u||v|cosθ,
where θ is the angle between the lines from 0 to u and 0 to v. Thus angle
is also deﬁnable in terms of inner product. Conversely, inner product is
deﬁnable in terms of length and angle. Moreover, an angle θ is determined
by cosθ and sinθ, which are the ratios of lengths in a certain triangle, so
inner product is in fact deﬁnable in terms of length alone.
This means that a transformation T preserves length if and only if T
preserves the inner product, that is,
T(u)·T(v) = u·v
for all
u,v ∈Rn.
The inner product is a more convenient concept than length when one is
working with linear transformations, because linear transformations are
represented by matrices and the inner product occurs naturally within ma-
trix multiplication: if A and B are matrices for which AB exists then
(i, j)-element of AB = (row i of A)·(column j of B).

50
3
Generalized rotation groups
This observation is the key to the following concise and practical criterion
for recognizing rotations, involving the matrix A and its transpose AT. To
state it we introduce the notation 1 for the identity matrix, of any size,
extending the notation used in Chapter 1 for the 2×2 identity matrix.
Rotation criterion. An n× n real matrix A represents a rotation of Rn if
and only if
AAT = 1
and
det(A) = 1.
Proof. First we show that the condition AAT = 1 is equivalent to preserva-
tion of the inner product by A.
AAT = 1 ⇔(row i of A)·(col j of AT) = δij
where δij = 1 if i = j and δij = 0 if i ̸= j
⇔(row i of A)·(row j of A) = δij
⇔rows of A form an orthonormal basis
⇔columns of A form an orthonormal basis
because AAT = 1 means AT = A−1, so 1 = ATA = AT(AT)T,
and hence AT has the same property as A
⇔A-images of the standard basis form an orthonormal basis
⇔A preserves the inner product
because Aei ·Aej = δij = ei ·ej, where e1 =
 1
0
...
0

, ..., en =
 0
...
0
1

are the
standard basis vectors of Rn.
Second, the condition det(A) = 1 says that A preserves orientation, as
mentioned at the beginning of Section 2.5. Standard properties of determi-
nants give
det(AAT) = det(A)det(AT)
and
det(AT) = det(A),
so we already have
1 = det(1) = det(AAT) = det(A)det(AT) = det(A)2.
And the two solutions det(A) = 1 and det(A) = −1 occur according as A
preserves orientation or not.
□
A rotation matrix is called a special orthogonal matrix, presumably
because its rows (or columns) form an orthonormal basis. The matrices

3.2
The orthogonal and special orthogonal groups
51
that preserve length, but not necessarily orientation, are called orthogonal.
(However, orthogonal matrices are not the only matrices that preserve or-
thogonality. Orthogonality is also preserved by the dilation matrices k1 for
any nonzero constant k.)
Exercises
3.1.1 Give an example of a matrix in O(2) that is not in SO(2).
3.1.2 Give an example of a matrix in O(3) that is not in SO(3), and interpret it
geometrically.
3.1.3 Work out the matrix for the reﬂection of R3 in the plane through O orthog-
onal to the unit vector (a,b,c).
3.2
The orthogonal and special orthogonal groups
It follows from the deﬁnition of special orthogonal matrices that:
• If A1 and A2 are orthogonal, then A1AT
1 = 1 and A2AT
2 = 1. It follows
that the product A1A2 satisﬁes
(A1A2)(A1A2)T = A1A2AT
2AT
1
because (A1A2)T = AT
2AT
1,
= A1AT
1
because A2AT
2 = 1,
= 1
because A1AT
1 = 1.
• If A1 and A2 are special orthogonal, then det(A1) = det(A2) = 1, so
det(A1A2) = det(A1)det(A2) = 1.
• If A is orthogonal, then AAT = 1, hence A−1 = AT. It follows that
(A−1)T = (AT)T = A, so A−1 is also orthogonal. And A−1 is special
orthogonal if A is because
det(A−1) = det(A)−1 = 1.
Thus products and inverses of n×n special orthogonal matrices are special
orthogonal, and hence they form a group. This group (the “rotation” group
of Rn) is called the special orthogonal group SO(n).
If we drop the requirement that orientation be preserved, then we get
a larger group of transformations of Rn called the orthogonal group O(n).

52
3
Generalized rotation groups
An example of a transformation that is in O(n), but not in SO(n), is re-
ﬂection in the hyperplane orthogonal to the x1-axis, (x1,x2,x3,...,xn) →
(−x1,x2,x3,...,xn), which has the matrix
⎛
⎜
⎜
⎜
⎝
−1
0
...
0
0
1
...
0
...
0
0
...
1
⎞
⎟
⎟
⎟
⎠,
obviously of determinant −1. We notice that the determinant of a matrix
A ∈O(n) is ±1 because (as mentioned in the previous section)
AAT = 1 ⇒1 = det(AAT) = det(A)det(AT) = det(A)2.
Path-connectedness
The most striking difference between SO(n) and O(n) is a topological one:
SO(n) is path-connected and O(n) is not. That is, if we view n×n matrices
as points of Rn2 in the natural way—by interpreting the n2 matrix entries
a11,a12,...,a1n,a21,...,a2n,...,an1,...,ann as the coordinates of a point—
then any two points in SO(n) may be connected by a continuous path in
SO(n), but the same is not true of O(n). Indeed, there is no continuous
path in O(n) from
⎛
⎜
⎜
⎜
⎝
1
1
...
1
⎞
⎟
⎟
⎟
⎠
to
⎛
⎜
⎜
⎜
⎝
−1
1
...
1
⎞
⎟
⎟
⎟
⎠
(where the entries left blank are all zero) because the value of the determi-
nant cannot jump from 1 to −1 along a continuous path.
The path-connectedness of SO(n) is not quite obvious, but it is inter-
esting because it reconciles the everyday concept of “rotation” with the
mathematical concept. In mathematics, a rotation of Rn is given by speci-
fying just one conﬁguration, usually the ﬁnal position of the basis vectors,
in terms of their initial position. This position is expressed by a matrix
A. In everyday speech, a “rotation” is a movement through a continuous
sequence of positions, so it corresponds to a path in SO(n) connecting the
initial matrix 1 to the ﬁnal matrix A.

3.2
The orthogonal and special orthogonal groups
53
Thus a ﬁnal position A of Rn can be realized by a “rotation” in the
everyday sense of the word only if SO(n) is path-connected.
Path-connectedness of SO(n). For any n, SO(n) is path-connected.
Proof.
For n = 2 we have the circle SO(2), which is obviously path-
connected (Figure 3.1). Now suppose that SO(n −1) is path-connected
and that A ∈SO(n). It sufﬁces to ﬁnd a path in SO(n) from 1 to A, because
if there are paths from 1 to A and B then there is a path from A to B.
1
cosθ +isinθ
θ
Figure 3.1: Path-connectedness of SO(2).
This amounts to ﬁnding a continuous motion taking the basis vectors
e1,e2,...,en to their ﬁnal positions Ae1,Ae2,...,Aen (the columns of A).
The vectors e1 and Ae1 (if distinct) deﬁne a plane P, so, by the path-
connectedness of SO(2), we can move e1 continuously to the position Ae1
by a rotation R of P. It then sufﬁces to continuously move Re2,...,Ren to
Ae2,...,Aen, respectively, keeping Ae1 ﬁxed. Notice that
• Re2,...,Ren are all orthogonal to Re1 = Ae1, because e2,...,en are
all orthogonal to e1 and R preserves angles.
• Ae2,...,Aen are all orthogonal to Ae1, because e2,...,en are all or-
thogonal to e1 and A preserves angles.
Thus the required motion can take place in the Rn−1 of vectors orthogonal
to Ae1, where it exists by the assumption that SO(n−1) is path-connected.
Performing the two motions in succession—taking e1 to Ae1 and then
Re2,...,Ren to Ae2,...,Aen—gives a path from 1 to A in SO(n).
□
The idea of path-connectedness will be explored further in Sections 3.8
and 8.6. In the meantime, the idea of continuous path is used informally in

54
3
Generalized rotation groups
the exercises below to show that path-connectedness has interesting alge-
braic implications.
Exercises
The following exercises study the identity component in a matrix group G, that is,
the set of matrices A ∈G for which there is a continuous path from 1 to A that lies
inside G.
3.2.1 Bearing in mind that matrix multiplication is a continuous operation, show
that if there are continuous paths in G from 1 to A ∈G and to B ∈G then
there is a continuous path in G from A to AB.
3.2.2 Similarly, show that if there is a continuous path in G from 1 to A, then
there is also a continuous path from A−1 to 1.
3.2.3 Deduce from Exercises 3.2.1 and 3.2.2 that the identity component of G is
a subgroup of G.
3.3
The unitary groups
The unitary groups U(n) and SU(n) are the analogues of the orthogonal
groups O(n) and SO(n) for the complex vector space Cn, which consists
of the ordered n-tuples (z1,z2,...,zn) of complex numbers. The sum oper-
ation on Cn is the usual vector addition:
(u1,u2,...,un)+(v1,v2,...,vn) = (u1 +v1,u2 +v2,...,un +vn).
And the multiple of (z1,z2,...,zn) ∈Cn by a scalar c ∈C is naturally
(cz1,cz2,...,czn). The twist comes with the inner product, because we
would like the inner product of a vector v with itself to be a real number—
the squared distance |v|2 from the zero matrix 0 to v. We ensure this by
the deﬁnition
(u1,u2,...,un)·(v1,v2,...,vn) = u1v1 +u2v2 +···+unvn.
(*)
With this deﬁnition of u·v we have
v·v = v1v1 +v2v2 +···+vnvn = |v1|2 +|v2|2 +···+|vn|2 = |v|2,
and |v|2 is indeed the squared distance of v = (v1,v2,...,vn) from 0 in the
space R2n that equals Cn when we interpret each copy of C as R2.

3.3
The unitary groups
55
The kind of inner product deﬁned by (*) is called Hermitian (after the
nineteenth-century French mathematician Charles Hermite). Just as one
meets ordinary inner products of rows when forming the product
AAT,
for a real matrix A,
so too one meets the Hermitian inner product (*) of rows when forming the
product
AAT,
for a complex matrix A.
Here A denotes the result of replacing each entry aij of A by its complex
conjugate aij.
With this adjustment the arguments of Section 3.1 go through, and one
obtains the following theorem.
Criterion for preserving the inner product on Cn. A linear transforma-
tion of Cn preserves the inner product (*) if and only if its matrix A satisﬁes
AA
T = 1, where 1 is the identity matrix.
□
As in Section 3.1, one ﬁnds that the rows (or columns) of A form an
orthonormal basis of Cn. The rows vi are “normal” in the sense that |vi| =
1, and “orthogonal” in the sense that vi · vj = 0 when i ̸= j, where the dot
denotes the inner product (*).
It is clear that if linear transformations preserve the inner product (*)
then their product and inverses also preserve (*), so the set of all transfor-
mations preserving (*) is a group. This group is called the unitary group
U(n). The determinant of an A in U(n) has absolute value 1 because
AA
T = 1 ⇒1 = det(AA
T) = det(A)det(A
T) = det(A)det(A) = |det(A)|2,
and it is easy to see that det(A) can be any number with absolute value 1.
The subgroup of U(n) whose members have determinant 1 is called the
special unitary group SU(n).
We have already met one SU(n), because the group of unit quaternions
α
−β
β
α

,
where α,β ∈C and |α|2 +|β|2 = 1,
is none other than SU(2). The rows (α,−β) and (β,α) are easily seen
to form an orthonormal basis of C2. Conversely, (α,−β) is an arbitrary
unit vector in C2, and (β,α) is the unique unit vector orthogonal to it that
makes the determinant equal to 1.

56
3
Generalized rotation groups
Path-connectedness of SU(n)
We can prove that SU(n) is path-connected, along similar lines to the proof
for SO(n) in the previous section. The proof is again by induction on n,
but the case n = 2 now demands a little more thought. It is helpful to use
the complex exponential function eix, which we take to equal cosx+isinx
by deﬁnition for now. (In Chapter 4 we study exponentiation in depth.)
Given

 α −β
β
α

in SU(2), ﬁrst note that (α,β) is a unit vector in C2,
so α = ucosθ and β = vsinθ for some u,v in C with |u| = |v| = 1. This
means that u = eiφ and v = eiψ for some φ,ψ ∈R.
It follows that
α(t) = eiφt cosθt,
β(t) = eiψt sinθt,
for
0 ≤t ≤1,
gives a continuous path

α(t) −β(t)
β(t) α(t)

from 1 to

 α −β
β
α

in SU(2). Thus
SU(2) is path-connected.
Exercises
Actually, SU(2) is not the only special unitary group we have already met, though
the other one is less interesting.
3.3.1 What is SU(1)?
The following exercises verify that a linear transformation of Cn, with matrix
A, preserves the Hermitian inner product (*) if and only if AAT = 1. They can be
proved by imitating the corresponding steps of the proof in Section 3.1.
3.3.2 Show that vectors form an orthonormal basis of Cn if and only if their
conjugates form an orthonormal basis, where the conjugate of a vector
(u1,u2,...,un) is the vector (u1,u2,...,un).
3.3.3 Show that AAT = 1 if and only if the row vectors of A form an orthonormal
basis of Cn.
3.3.4 Deduce from Exercises 3.3.2 and 3.3.3 that the column vectors of A form
an orthonormal basis.
3.3.5 Show that if A preserves the inner product (*) then the columns of A form
an orthonormal basis.
3.3.6 Show, conversely, that if the columns of A form an orthonormal basis, then
A preserves the inner product (*).

3.4
The symplectic groups
57
3.4
The symplectic groups
On the space Hn of ordered n-tuples of quaternions there is a natural inner
product,
(p1, p2,..., pn)·(q1,q2,...,qn) = p1q1 + p2q2 +···+ pnqn.
(**)
This of course is formally the same as the inner product (*) on Cn, ex-
cept that the pi and qj now denote arbitrary quaternions. The space Hn
is not a vector space over H, because the quaternions do not act correctly
as “scalars”: multiplying a vector on the left by a quaternion is in general
different from multiplying it on the right, because of the noncommutative
nature of the quaternion product.
Nevertheless, quaternion matrices make sense (thanks to the associa-
tivity of the quaternion product, we still get an associative matrix product),
and we can use them to deﬁne linear transformations of Hn. Then, by spe-
cializing to the transformations that preserve the inner product (**), we get
an analogue of the orthogonal group for Hn called the symplectic group
Sp(n). As with the unitary groups, preserving the inner product implies
preserving length in the corresponding real space, in this case in the space
R4n corresponding to Hn.
For example, Sp(1) consists of the 1×1 quaternion matrices, multipli-
cation by which preserves length in H = R4. In other words, the members
of Sp(1) are simply the unit quaternions. Because we deﬁned quaternions
in Section 1.3 as the 2×2 complex matrices
a+id
−b−ic
b−ic
a−id

,
it follows that
Sp(1) =
a+id
−b−ic
b−ic
a−id

: a2 +b2 +c2 +d2 = 1

= SU(2).
Thus we have already met the ﬁrst symplectic group.
The quaternion matrices A in Sp(n), like the complex matrices in
SU(n), are characterized by the condition AAT = 1, where the bar now
denotes the quaternion conjugate. The proof is the same as for SU(n).
Because of this formal similarity, there is a proof that Sp(n) is path-
connected, similar to that for SU(n) given in the previous section.

58
3
Generalized rotation groups
However, we avoid imposing the condition det(A) = 1, because there
are difﬁculties in the very deﬁnition of determinant for quaternion matrices.
We sidestep this problem by interpreting all n × n quaternion matrices as
2n×2n complex matrices.
The complex form of Sp(n)
In Section 1.3 we deﬁned quaternions as the complex 2×2 matrices
q =
a+id
−b−ic
b−ic
a−id

=
α
−β
β
α

for α,β ∈C.
Thus the entries of a quaternion matrix are themselves 2 × 2 matrices q.
Thanks to a nice feature of the matrix product—that it admits block multi-
plication—we can omit the parentheses of each matrix q. Then it is natural
to deﬁne the complex form, C(A), of a quaternion matrix A to be the result
of replacing each quaternion entry q in A by the 2×2 block
α
−β
β
α
.
Notice also that the transposed complex conjugate of this block corre-
sponds to the quaternion conjugate of q:
q =
 a−id
b+ic
−b+ic
a+id

=
 α
β
−β
α

.
Therefore, if A is a quaternion matrix such that AA
T = 1, it follows by
block multiplication (and writing 1 for any identity matrix) that
C(A)C(A)
T = C(AAT) = C(1) = 1.
Thus C(A) is a unitary matrix.
Conversely, if A is a quaternion matrix for which C(A) is unitary, then
AAT = 1. This follows by viewing the product AAT of quaternion matrices
as the product C(A)C(A)
T of complex matrices. Therefore, the group Sp(n)
consists of those n×n quaternion matrices A for which C(A) is unitary.
It follows, if we deﬁne the complex form of Sp(n) to be the group of
matrices C(A) for A ∈Sp(n), that the complex form of Sp(n) consists of the
unitary matrices of the form C(A), where A is an n×n quaternion matrix.
In particular, the complex form of Sp(n) is a subgroup of U(2n).

3.4
The symplectic groups
59
Many books on Lie theory avoid the use of quaternions, and deﬁne
Sp(n) as the group of unitary matrices of the form C(A). This gets around
the inconvenience that Hn is not quite a vector space over H (mentioned
above) but it breaks the simple thread joining the orthogonal, unitary, and
symplectic groups: they are the “generalized rotation” groups of the spaces
with coordinates from R, C, and H, respectively.
Exercises
It is easy to test whether a matrix consists of blocks of the form
α
−β
β
α
.
Nevertheless, it is sometimes convenient to describe the property of “being of the
form C(A)” more algebraically. One way to do this is with the help of the special
matrix
J =
 0
1
−1
0

.
3.4.1 If B =
α
−β
β
α

show that JBJ−1 = B.
3.4.2 Conversely, show that if JBJ−1 = B and B =

c
d
e
f

then we have c = f
and d = −e, so B has the form
α
−β
β
α

.
Now suppose that B2n is any 2n ×2n complex matrix, and let
J2n =
⎛
⎜
⎜
⎜
⎝
J
0
0 ... 0
0
J
0 ... 0
...
...
...
...
...
0
0 ... 0
J
⎞
⎟
⎟
⎟
⎠,
where 0 is the 2 ×2 zero matrix.
3.4.3 Use block multiplication, and the results of Exercises 3.4.1 and 3.4.2, to
show that B2n has the form C(A) if and only if J2nB2nJ−1
2n = B2n.
The equation satisﬁed by J and B2n enables us to derive information about det(B2n)
(thus getting around the problem with the determinant of a quaternion matrix).
3.4.4 By taking det of both sides of the equation in Exercise 3.4.3, show that
det(B2n) is real.
3.4.5 Assuming now that B2n is in the complex form of Sp(n), and hence is uni-
tary, show that det(B2n) = ±1.

60
3
Generalized rotation groups
One can prove Sp(n) is path-connected by an argument like that used for
SU(n) in the previous section. First prove path-connectedness of Sp(2) as for
SU(2), using a result from Section 4.2 that each unit quaternion is the exponential
of a pure imaginary quaternion.
3.4.6 Deduce from the path-connectedness of Sp(n) that det(B2n) = 1.
This is why there is no “special symplectic group”—the matrices in the symplectic
group already have determinant 1, under a sensible interpretation of determinant.
3.5
Maximal tori and centers
The main key to understanding the structure of a Lie group G is its maximal
torus, a (not generally unique) maximal subgroup isomorphic to
Tk = S1 ×S1 ×···×S1
(k-fold Cartesian product)
contained in G. The group Tk is called a torus because it generalizes the
ordinary torus T2 = S1×S1. An obvious example is the group SO(2) = S1,
which is its own maximal torus. For the other groups SO(n), not to mention
SU(n) and Sp(n), maximal tori are not so obvious, though we will ﬁnd
them by elementary means in the next section. To illustrate the kind of
argument involved we ﬁrst look at the case of SO(3).
Maximal torus of SO(3)
If we view SO(3) as the rotation group of R3, and let e1, e2, and e3 be the
standard basis vectors, then the matrices
R′
θ =
⎛
⎝
cosθ
−sinθ
0
sinθ
cosθ
0
0
0
1
⎞
⎠
form an obvious T1 = S1 in SO(3). The matrices R′
θ are simply rotations
of the (e1,e2)-plane through angle θ, which leave the e3-axis ﬁxed.
If T is any torus in G that contains this T1 then, since any torus is
abelian, any A ∈T commutes with all R′
θ ∈T1. We will show that if
AR′
θ = R′
θA
for all
R′
θ ∈T1
(*)
then A ∈T1, so T = T1 and hence T1 is maximal. It sufﬁces to show that
A(e1), A(e2) ∈(e1,e2)-plane,

3.5
Maximal tori and centers
61
because in that case A is an isometry of the (e1,e2)-plane that ﬁxes O. The
only such isometries are rotations and reﬂections, and the only ones that
commute with all rotations are rotations themselves.
So, suppose that
A(e1) = a1e1 +a2e2 +a3e3.
By the hypothesis (*), A commutes with all R′
θ, and in particular with
R′
π =
⎛
⎝
−1
0
0
0
−1
0
0
0
1
⎞
⎠.
Now we have
AR′
π(e1) = A(−e1) = −a1e1 −a2e2 −a3e3,
R′
πA(e1) = R′
π(a1e1 +a2e2 +a3e3) = −a1e1 −a2e2 +a3e3,
so it follows from AR′
π = R′
πA that a3 = 0 and hence
A(e1) ∈(e1,e2)-plane.
A similar argument shows that
A(e2) ∈(e1,e2)-plane,
which completes the proof that T1 is maximal in SO(3).
□
An important substructure of G revealed by the maximal torus is the
center of G, a subgroup deﬁned by
Z(G) = {A ∈G : AB = BA for all B ∈G}.
(The letter Z stands for “Zentrum,” the German word for “center.”) It is
easy to check that Z(G) is closed under products and inverses, and hence
Z(G) is a group. We can illustrate how the maximal torus reveals the center
with the example of SO(3) again.
Center of SO(3)
An element A ∈Z(SO(3)) commutes with all elements of SO(3), and in
particular with all elements of the maximal torus T1. The argument above
then shows that A ﬁxes the basis vector e3. Interchanging basis vectors, we
likewise ﬁnd that A ﬁxes e1 and e2. Hence A is the identity rotation 1.
Thus Z(SO(3)) = {1}.
□

62
3
Generalized rotation groups
Exercises
The 2-to-1 map from SU(2) to SO(3) ensures that the maximal torus and center
of SU(2) are similar to those of SO(3).
3.5.1 Give an example of a T1 in SU(2).
3.5.2 Explain why a T2 in SU(2) yields a T2 in SO(3), so T1 is maximal in
SU(2). (Hint: Map each element g of the T2 in SU(2) to the pair ±g in
SO(3), and look at the images of the S1 factors of T2.)
3.5.3 Explain why Z(SU(2)) = {±1}.
The center of SO(3) can also be found by a direct geometric argument.
3.5.4 Suppose that A is a rotation of R3, about the e1-axis, say, that is not the
identity and not a half-turn. Explain (preferably with pictures) why A does
not commute with the half-turn about the e3-axis.
3.5.5 If A is a half-turn of R3 about the e1-axis, ﬁnd a rotation that does not
commute with A.
In Section 3.7 we will show that Z(SO(2m + 1)) = {1} for all m. However,
the situation is different for SO(2m).
3.5.6 Give an example of a nonidentity element of Z(SO(2m)) for each m ≥2.
3.6
Maximal tori in SO(n), U(n), SU(n), Sp(n)
The one-dimensional torus T1 = S1 appears as a matrix group in several
different guises:
• as a group of 2×2 real matrices
Rθ =
cosθ
−sinθ
sinθ
cosθ

,
• as a group of complex numbers (or 1×1 complex matrices)
zθ = cosθ +isinθ,
• as a group of quaternions (or 1×1 quaternion matrices)
qθ = cosθ +isinθ.
Each of these incarnations of T1 gives rise to a different incarnation of Tk:

3.6
Maximal tori in SO(n), U(n), SU(n), Sp(n)
63
• as a group of 2k×2k real matrices
Rθ1,θ2,...,θk =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
cosθ1
−sinθ1
sinθ1
cosθ1
cosθ2
−sinθ2
sinθ2
cosθ2
...
cosθk
−sinθk
sinθk
cosθk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
where all the blank entries are zero,
• as a group of k×k unitary matrices
Zθ1,θ2,...,θk =
⎛
⎜
⎜
⎜
⎝
eiθ1
eiθ2
...
eiθk
⎞
⎟
⎟
⎟
⎠,
where all the blank entries are zero and eiθ = cosθ +isinθ,
• as a group of k ×k symplectic matrices
Qθ1,θ2,...,θk =
⎛
⎜
⎜
⎜
⎝
eiθ1
eiθ2
...
eiθk
⎞
⎟
⎟
⎟
⎠,
where all the blank entries are zero and eiθ = cosθ + isinθ. (This
generalization of the exponential function is justiﬁed in the next
chapter. In the meantime, eiθ may be taken as an abbreviation for
cosθ +isinθ.)
We can also represent Tk by larger matrices obtained by “padding” the
above matrices with an extra row and column, both consisting of zeros
except for a 1 at the bottom right-hand corner (as we did to produce the
matrices R′
θ in SO(3) in the previous section). Using this idea, we ﬁnd the
following tori in the groups SO(2m), SO(2m+1), U(n), SU(n), and Sp(n).

64
3
Generalized rotation groups
In SO(2m) we have the Tm consisting of the matrices Rθ1,θ2,...,θm. In
SO(2m+1) we have the Tm consisting of the “padded” matrices
R′
θ1,θ2,...,θk =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
cosθ1
−sinθ1
sinθ1
cosθ1
cosθ2
−sinθ2
sinθ2
cosθ2
...
cosθk
−sinθk
sinθk
cosθk
1
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
In U(n) we have the Tn consisting of the matrices Zθ1,θ2,...,θn. In SU(n) we
have the Tn−1 consisting of the Zθ1,θ2,...,θn with θ1 +θ2 +···+θn = 0. The
latter matrices form a Tn−1 because
⎛
⎜
⎜
⎜
⎝
eiθ1
...
eiθn−1
eiθn
⎞
⎟
⎟
⎟
⎠= eiθn
⎛
⎜
⎜
⎜
⎝
ei(θ1−θn)
...
ei(θn−1−θn)
1
⎞
⎟
⎟
⎟
⎠,
and the matrices on the right clearly form a Tn−1. Finally, in Sp(n) we
have the Tn consisting of the matrices Qθ1,θ2,...,θn.
We now show that these “obvious” tori are maximal. As with SO(3),
used as an illustration in the previous section, the proof in each case con-
siders a matrix A ∈G that commutes with each member of the given torus
T, and shows that A ∈T.
Maximal tori in generalized rotation groups. The tori listed above are
maximal in the corresponding groups.
Proof. Case (1): Tm in SO(2m), for m ≥2.
If we let e1,e2,...,e2m denote the standard basis vectors for R2m, then
the typical member Rθ1,θ2,...,θm of Tm is the product of the following plane
rotations, each of which ﬁxes the basis vectors orthogonal to the plane:
rotation of the (e1,e2)-plane through angle θ1,
rotation of the (e3,e4)-plane through angle θ2,
...
rotation of the (e2m−1,e2m)-plane through angle θm.

3.6
Maximal tori in SO(n), U(n), SU(n), Sp(n)
65
Now suppose that A ∈SO(2m) commutes with each Rθ1,θ2,...,θm. We are
going to show that
A(e1),A(e2) ∈(e1,e2)-plane,
A(e3),A(e4) ∈(e3,e4)-plane,
...
A(e2m−1),A(e2m) ∈(e2m−1,e2m)-plane,
from which it follows that A is a product of rotations of these planes, and
hence is a member of Tm. (The possibility that A reﬂects some plane P is
ruled out by the fact that A commutes with all members of Tm, including
those that rotate only the plane P. Then it follows as in the case of SO(3)
that A rotates P.)
To show that A maps the basis vectors into the planes claimed, it suf-
ﬁces to show that A(e1) ∈(e1,e2)-plane, since the other cases are similar.
So, suppose that
ARθ1,θ2,...,θm = Rθ1,θ2,...,θmA
for all
Rθ1,θ2,...,θm ∈T,
and in particular that
ARπ,0,...,0(e1) = Rπ,0,...,0A(e1).
Then if A(e1) = a1e1 +a2e2 +···+a2me2m. we have
ARπ,0,...,0(e1) = A(−e1) = −a1e1 −a2e2 −a3e3 ···−a2me2m,
but
Rπ,0,...,0A(e1) = −a1e1 −a2e2 +a3e3 +···+e2me2m,
whence a3 = a4 = ··· = a2m = 0, as required.
The argument is similar for any other ek. Hence A ∈Tm, as claimed.
Case (2): Tm in SO(2m+1).
In this case we generalize the argument for SO(3) from the previous
section, using maps such as R′
π,0,...,0 in place of R′
π.
Case (3): Tn in U(n).
Let e1,e2,...,en be the standard basis vectors of Cn, and suppose that
A commutes with each element Zθ1,θ2,...,θn of Tn. In particular, A commutes
with
Zπ,0,...,0 =
⎛
⎜
⎜
⎜
⎝
−1
1
...
1
⎞
⎟
⎟
⎟
⎠.

66
3
Generalized rotation groups
Then if A(e1) = a1e1 +···+anen we have
AZπ,0,...,0(e1) = A(−e1) = −a1e1 −···−anen
Zπ,0,...,0A(e1) = Zπ,0,...,0(a1e1 +···+anen) = −a1e1 +···+anen,
whence it follows that a2 = ··· = an = 0.
Thus A(e1) = c1e1 for some c1 ∈C, and a similar argument shows that
A(ek) = ckek for each k. Also, A(e1),...A(en) are an orthonormal basis,
since A ∈U(n). Hence each |ck| = 1, so ck = eiϕk and therefore A ∈Tn.
Case (4): Tn−1 in SU(n).
For n > 2 we can argue as for U(n), except that we need to commute
A with both Zπ,π,0,...,0 and Zπ,0,π,...,0 to conclude that A(e1) = c1e1. This is
because Zπ,0,0,...,0 is not in SU(n), since it has determinant −1.
For n = 2 we can argue as follows.
Suppose A =
 a b
c d

commutes with each Zθ,−θ ∈T1. In particular, A
commutes with
Zπ/2,−π/2 =
i
0
0
−i

,
which implies that
ai
−bi
ci
−di

=
 ai
bi
−ci
−di

.
It follows that b = c = 0 and hence A ∈T1.
Case (5): Tn in Sp(n).
Here we can argue exactly as in Case (3).
□
Exercises
3.6.1 Viewing Cn as R2n, show that Zθ1,θ2,...,θn is the same isometry as Rθ1,θ2,...,θn.
3.6.2 Use Exercise 3.6.1 to give another proof that Tn is a maximal torus of U(n).
3.6.3 Show that the maximal tori found above are in fact maximal abelian sub-
groups of SO(n), U(n), SU(n), Sp(n).
We did not look for a maximal torus in O(n) because the subgroup SO(n) is of
more interest to us, but in any case it easy to ﬁnd a maximal torus in O(n).
3.6.4 Explain why a maximal torus of O(n) is also a maximal torus of SO(n).

3.7
Centers of SO(n), U(n), SU(n), Sp(n)
67
3.7
Centers of SO(n), U(n), SU(n), Sp(n)
The arguments in the previous section show that an element A in G =
SO(n),U(n),SU(n),Sp(n) that commutes with all elements of a maximal
torus T in G is in fact in T. It follows that if A commutes with all elements
of G then A ∈T. Thus we can assume that elements A of the center Z(G)
of G have the special form known for members of T. This enables us to
identify Z(G) fairly easily when G = SO(n),U(n),SU(n),Sp(n).
Centers of generalized rotation groups. The centers of these groups are:
(1) Z(SO(2m)) = {±1}.
(2) Z(SO(2m+1)) = {1}.
(3) Z(U(n)) = {ω1 : |ω| = 1}.
(4) Z(SU(n)) = {ω1 : ωn = 1}.
(5) Z(Sp(n)) = {±1}.
Proof. Case (1): A ∈Z(SO(2m)) for m ≥2.
In this case A = Rθ1,θ2,...,θn for some angles θ1,θ2,...,θn, and A com-
mutes with all members of SO(2m). Now Rθ1,θ2,...,θn is built from a se-
quence of 2×2 blocks (placed along the diagonal) of the form
Rθ =
cosθ
−sinθ
sinθ
cosθ

.
We notice that Rθ does not commute with the matrix
I∗=
1
0
0
−1

unless sinθ = 0 and hence cosθ = ±1. Therefore, if we build a matrix
I∗
2m ∈SO(2m) with copies of I∗on the diagonal, Rθ1,θ2,...,θn will commute
with I∗
2m only if each sinθk = 0 and cosθk = ±1.
Thus a matrix A in Z(SO(2m)) has diagonal entries ±1 and zeros else-
where. Moreover, if both +1 and −1 occur we can ﬁnd a matrix in SO(2m)
that does not commute with A; namely, a matrix with Rθ on the diagonal at
the position of an adjacent +1 and −1 in A, and otherwise only 1’s on the
diagonal. So, in fact, A = 1 or A = −1. Both 1 and −1 belong to SO(2m),
and they obviously commute with everything, so Z(SO(2m)) = {±1}.

68
3
Generalized rotation groups
Case (2): A ∈Z(SO(2m+1)).
The argument is very similar to that for Case (1), except for the last
step. The (2m+1)×(2m+1) matrix −1 does not belong to SO(2m+1),
because its determinant equals −1. Hence Z(SO(2m+1)) = {1}.
Case (3): A ∈Z(U(n)).
In this case A = Zθ1,θ2,...,θn for some θ1,θ2,...,θn and A commutes with
all elements of U(n). If n = 1 then U(n) is isomorphic to the abelian group
S1 = {eiθ : θ ∈R}, so U(1) is its own center. If n ≥2 we take advantage
of the fact that
eiθ1
0
0
eiθ2

does not commute with
0
1
1
0

unless eiθ1 = eiθ2. It follows, by building a matrix with
 0 1
1 0

somewhere
on the diagonal and otherwise only 1s on the diagonal, that A = Zθ1,θ2,...,θn
must have eiθ1 = eiθ2 = ··· = eiθn.
In other words, elements of Z(U(n)) have the form eiθ1. Conversely,
all matrices of this form are in U(n), and they commute with all other
matrices. Hence
Z(U(n)) = {eiθ1 : θ ∈R} = {ω1 : |ω| = 1}.
Case (4): A ∈Z(SU(n)).
The argument for U(n) shows that A must have the form ω1, where
|ω| = 1. But in SU(n) we must also have
1 = det(A) = ωn.
This means that ω is one of the n “roots of unity”
e2iπ/n,
e4iπ/n,
...,
e2(n−1)π/n,
1.
All such matrices ω1 clearly belong to SU(n) and commute with every-
thing, hence Z(SU(n)) = {ω1 : ωn = 1}.
Case (5): A ∈Z(Sp(n)).
In this case A = Qθ1,θ2,...,θn for some θ1,θ2,...,θn and A commutes
with all elements of Sp(n). The argument used for U(n) applies, up to the
point of showing that all matrices in Z(Sp(n)) have the form q1, where
|q| = 1. But now we must bear in mind that quaternions q do not generally
commute. Indeed, only the real quaternions commute with all the others,
and the only real quaternions q with |q| = 1 are q = 1 and q = −1. Thus
Z(Sp(n)) = {±1}.
□

3.8
Connectedness and discreteness
69
Exercises
It happens that the quotient of each of the groups SO(n), U(n), SU(n), Sp(n) by
its center is a group with trivial center (see Exercise 3.8.1). However, it is not
generally true that the quotient of a group by its center has trivial center.
3.7.1 Find the center Z(G) of G = {1,−1,i,−i,j,−j,k,−k} and hence show that
G/Z(G) has nontrivial center.
3.7.2 Prove that U(n)/Z(U(n)) = SU(n)/Z(SU(n)).
3.7.3 Is SU(2)/Z(SU(2)) = SO(3)?
3.7.4 Using the relationship between U(n), Z(U(n)), and SU(n), or otherwise,
show that U(n) is path-connected.
3.8
Connectedness and discreteness
Finding the centers of SO(n), U(n), SU(n), and Sp(n) is an important step
towards understanding which of these groups are simple. The center of
any group G is a normal subgroup of G, hence G cannot be simple unless
Z(G) = {1}. This rules out all of the groups above except the SO(2m+1).
Deciding whether there are any other normal subgroups of SO(2m + 1)
hinges on the distinction between discrete and nondiscrete subgroups.
A subgroup H of a matrix Lie group G is called discrete if there is a
positive lower bound to the distance between any two members of H, the
distance between matrices (aij) and (bij) being deﬁned as
	
∑
i,j
|aij −bij|2.
(We say more about the distance between matrices in the next chapter.) In
particular, any ﬁnite subgroup of G is discrete, so the centers of SO(n),
SU(n), and Sp(n) are discrete. On the other hand, the center of U(n) is
clearly not discrete, because it includes elements arbitrarily close to the
identity matrix.
In ﬁnding the centers of SO(n), SU(n), and Sp(n) we have in fact found
all their discrete normal subgroups, because of the following remarkable
theorem, due to Schreier [1925].
Centrality of discrete normal subgroups. If G is a path-connected matrix
Lie group with a discrete normal subgroup H, then H is contained in the
center Z(G) of G.

70
3
Generalized rotation groups
Proof. Since H is normal, BAB−1 ∈H for each A ∈H and B ∈G. Thus
B →BAB−1 deﬁnes a continuous map from G into the discrete set H. Since
G is path connected, and a continuous map sends paths to paths, the image
of the map must be a single point of H. This point is necessarily A because
1 →1A1−1 = A.
In other words, each A ∈H has the property that BA = AB for all B ∈G.
That is, A ∈Z(G).
□
The groups SO(n), SU(n), and Sp(n) are path-connected, as we have
seen in Sections 3.2, 3.3, and 3.4, so all their discrete normal subgroups
are in their centers, determined in Section 3.7. In particular, SO(2m + 1)
has no nontrivial discrete normal subgroup, because its center is {1}.
It follows that the only normal subgroups we may have missed in
SO(n), SU(n), and Sp(n) are those that are not discrete. In Section 7.5
we will establish that such subgroups do not exist, so all normal sub-
groups of SO(n), SU(n), and Sp(n) are in their centers.
In particular,
the groups SO(2m + 1) are all simple, and it follows from Exercise 3.8.1
below that the rest are simple “modulo their centers.” That is, for G =
SO(2m),SU(n),Sp(n), the group G/Z(G) is simple.
Exercises
3.8.1 If Z(G) is the only nontrivial normal subgroup of G, show that G/Z(G) is
simple.
The result of Exercises 3.2.1, 3.2.2, 3.2.3 can be improved, with the help of
some ideas used above, to show that the identity component is a normal subgroup
of G.
3.8.2 Show that, if H is a subgroup of G and AHA−1 ⊆H for each A ∈G, then H
is a normal subgroup of G.
3.8.3 If G is a matrix group with identity component H, show that AHA−1 ⊆H
for each matrix A ∈G.
The proof of Schreier’s theorem assumes only that there is no path in H be-
tween two distinct members, that is, H is totally disconnected. Thus we have
actually proved: if G is a path-connected group with a totally disconnected nor-
mal subgroup H, then H is contained in Z(G). We can give examples of totally
disconnected subgroups that are not discrete.
3.8.4 Show that the subgroup H = {cos2πr + isin2πr : r rational} of the circle
SO(2) is totally disconnected but dense, that is, each arc of the circle con-
tains an element of H.

3.9
Discussion
71
This example is also a normal subgroup. However, normal, dense, totally
disconnected subgroups are rare.
3.8.5 Explain why there is no normal, dense, totally disconnected subgroup of
SO(n) for n > 2.
3.9
Discussion
The idea of treating orthogonal, unitary, and symplectic groups uniformly
as generalized isometry groups of the spaces Rn, Cn, and Hn seems to
be due to Chevalley [1946]. Before the appearance of Chevalley’s book,
the symplectic group Sp(n) was generally viewed as the group of unitary
transformations of C2n that preserve the symplectic form
(α1α′
1 −β1β ′
1)+···+(αnα′n −βnβ ′n),
where (α1,β1,...,αn,βn) is the typical element of C2n. This element cor-
responds to the element (q1,...,qn) of Hn, where
qk =
αk
−βk
βk
αk

.
The invariance of the quaternion inner product
q1q′
1 +···+qnq′n
is therefore equivalent to the invariance of the matrix product
α1
−β1
β1
α1
α′
1
−β ′
1
β ′
1
α′
1

+···+
αn
−βn
βn
αn
α′
n
−β ′
n
β ′n
α′n

,
which turns out to be equivalent to the invariance of the symplectic form.
The word “symplectic” itself was introduced by Hermann Weyl in his book
The Classical Groups, Weyl [1939], p. 165:
The name “complex group” formerly advocated by me in al-
lusion to line complexes, as these are deﬁned by the vanishing
of antisymmetric bilinear forms, has become more and more
embarrassing through collision with the word “complex” in
the connotation of complex number. I therefore propose to re-
place it with the corresponding Greek adjective “symplectic.”

72
3
Generalized rotation groups
Maximal tori were also introduced by Weyl, in his paper Weyl [1925].
In this book we use them only to ﬁnd the centers of the orthogonal, unitary,
and symplectic groups, since the centers turn out to be crucial in the inves-
tigation of simplicity. However, maximal tori themselves are important for
many investigations in the structure of Lie groups.
The existence of a nontrivial center in SO(2m), SU(n), and Sp(n)
shows that these groups are not simple, since the center is obviously a
normal subgroup. Nevertheless, these groups are almost simple, because
the center is in each case their largest normal subgroup. We have shown in
Section 3.8 that the center is the largest normal subgroup that is discrete,
in the sense that there is a minimum, nonzero, distance between any two
of its elements. It therefore remains to show that there are no nondiscrete
normal subgroups, which we do in Section 7.5.
It turns out that the quotient groups of SO(2m), SU(n), and Sp(n) by
their centers are simple and, from the Lie theory viewpoint, taking these
quotients makes very little difference. The center is essentially “invisible,”
because its tangent space is zero. We explain “invisibility” in Chapter 5,
after looking at the tangent spaces of some particular groups in Chapter 4.
It should be mentioned, however, that the quotient of a matrix group
by a normal subgroup is not necessarily a matrix group. Thus in taking
quotients we may leave the world of matrix groups. The ﬁrst example was
discovered by Birkhoff [1936]. It is the quotient (called the Heisenberg
group) of the group of upper triangular matrices of the form
⎛
⎝
1
x
y
0
1
z
0
0
1
⎞
⎠,
where
x,y,z ∈R,
by the subgroup of matrices of the form
⎛
⎝
1
0
n
0
1
0
0
0
1
⎞
⎠,
where
n ∈Z.
The Heisenberg group is a Lie group, but not isomorphic to a matrix group.
One of the reasons for looking at tangent spaces is that we do not have
to leave the world of matrices. A theorem of Ado from 1936 shows that
the tangent space of any Lie group G—the Lie algebra g—can be faithfully
represented by a space of matrices. And if G is almost simple then g is truly

3.9
Discussion
73
simple, in a sense that will be explained in Chapter 6. Thus the study of
simplicity is, well, simpliﬁed by passing from Lie groups to Lie algebras.
The importance of topology in Lie theory—and particularly paths and
connectedness—was ﬁrst realized by Schreier in 1925. Schreier published
his results in the journal of the Hamburg mathematical seminar—a well-
known journal for algebra and topology at the time—but they were not
noticed by Lie theorists until after Schreier’s untimely death in 1929 at the
age of 28. In 1929, ´Elie Cartan became aware of Schreier’s results and
picked up the torch of topology in Lie theory.
In the 1930s, Cartan proved several remarkable results on the topol-
ogy of Lie groups. One of them has the consequence that S1 and S3 are
the only spheres that admit a continuous group structure. Thus the Lie
groups SO(2) and SU(2), which we already know to be spheres, are the
only spheres that actually occur among Lie groups. Cartan’s proof uses
quite sophisticated topology, but his result is related to the theorem of
Frobenius mentioned in Section 1.6, that the only skew ﬁelds Rn are R,
R2 = C, and R4 = H. In particular, there is a continuous and associa-
tive “multiplication”—necessary for continuous group structure—only in
R, R2, and R4. For more on the interplay between topology and algebra in
Rn, see the book Ebbinghaus et al. [1990].

4
The exponential map
PREVIEW
The group S1 = SO(2) studied in Chapter 1 can be viewed as the image of
the line Ri = {iθ : θ ∈R} under the exponential function, because
exp(iθ) = eiθ = cosθ +isinθ.
This line is (in a sense we explain below) the tangent to the circle at its
identity element 1. And, in fact, any Lie group has a linear space (of the
same dimension as the group) as its tangent space at the identity.
The group S3 = SU(2) is also the image, under a generalized exp func-
tion, of a linear space. This linear space—the tangent space of SU(2) at
the identity—is three-dimensional and has an interesting algebraic struc-
ture. Its points can be added (as vectors) and also multiplied in a way that
reﬂects the nontrivial conjugation operation g1,g2 →g1g2g−1
1
in SU(2).
The algebra su(2) on the tangent space is called the Lie algebra of the Lie
group SU(2), and it is none other than R3 with the vector product.
As we know from Chapter 1, complex numbers and quaternions can
both be viewed as matrices. The exponential function exp generalizes to
arbitrary square matrices, and we will see later that it maps the tangent
space of any matrix Lie group G into G. In many cases exp is onto G, and in
all cases the algebraic structure of G has a parallel structure on the tangent
space, called the Lie algebra of G. In particular, the conjugation operation
on G, which reﬂects the departure of G from commutativity, corresponds
to an operation on the tangent space called the Lie bracket.
We illustrate the exp function on matrices with the simplest nontrivial
example, the afﬁne group of the line.
74
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 4,
c⃝Springer Science+Business Media, LLC 2008

4.1
The exponential map onto SO(2)
75
4.1
The exponential map onto SO(2)
The relationship between the exponential function and the circle,
eiθ = cosθ +isinθ,
was discovered by Euler in his book Introduction to the Analysis of the
Inﬁnite of 1748. One way to see why this relationship holds is to look at
the Taylor series for ex, cosx, and sinx, and to suppose that the exponential
series is also meaningful for complex numbers.
ex = 1+ x
1! + x2
2! + x3
3! + x4
4! + x5
5! +··· ,
cosx = 1−x2
2! + x4
4! −··· ,
sinx = x
1! −x3
3! + x5
5! −··· .
The series for ex is absolutely convergent, so we may substitute iθ for x and
rearrange terms. This gives a deﬁnition of eiθ and justiﬁes the following
calculation:
eiθ = 1+ iθ
1! −θ2
2! −iθ3
3! + θ4
4! + iθ5
5! −···
=

1−θ2
2! + θ4
4! −···

+i
 θ
1! −θ3
3! + θ5
5! −···

= cosθ +isinθ.
Thus the exponential function maps the imaginary axis Ri of points iθ onto
the circle S1 of points cosθ +isinθ in the plane of complex numbers.
The operations of addition and negation on Ri carry over to multipli-
cation and inversion on S1, since
eiθ1eiθ2 = ei(θ1+θ2)
and

eiθ−1
= e−iθ.
There is not much more to say about S1, because multiplication of
complex numbers is a well-known operation and the circle is a well-known
curve. However, we draw attention to one triﬂing fact, because it proves to
have a more interesting analogue in the case of S3 that we study in the next
section. The line of points iθ mapped onto S1 by the exponential function
can be viewed as the tangent to S1 at the identity element 1 (Figure 4.1). Of

76
4
The exponential map
course, the points on the tangent are of the form 1+iθ, but we ignore their
constant real part 1. The essential coordinate of a point on the tangent is its
imaginary part iθ, giving its height θ above the x-axis. Note also that the
point iθ at height θ is mapped to the point cosθ + isinθ at arc length θ.
Thus the exponential map preserves the length of sufﬁciently small arcs.
O
x
y
i
iθ
eiθ
1
Figure 4.1: S1 and its tangent at the identity.
Euler’s discovery that the exponential function can be extended to the
complex numbers, and that it can thereby map a straight line onto a curve,
was just the beginning. In the next section we will see that a further exten-
sion of the exponential function can map the ﬂat three-dimensional space
R3 onto a curved one, S3, and in the next chapter we will see that such
exponential mappings exist in arbitrarily high dimensions.
Exercises
The fundamental property of the exponential function is the addition formula,
which tells us that exp maps sums to products, that is,
eA+B = eAeB.
However, we are about to generalize the exponential function to objects that do
not enjoy all the algebraic properties of real or complex numbers, so it is important
to investigate whether the equation eA+B = eAeB still holds. The answer is that it
does, provided AB = BA.
We assume that
eX = 1 + X
1! + X2
2! +··· ,
where 1 is the identity object.

4.2
The exponential map onto SU(2)
77
4.1.1 Assuming that AB = BA, show that
(A+B)m = Am+
m
1

Am−1B+
m
2

Am−2B2+···+
 m
m−1

ABm−1+Bm,
where
m
l

denotes the number of ways of choosing l things from a set of
m things.
4.1.2 Show that
m
l

= m(m−1)(m−2)···(m−l+1)
l!
=
m!
l!(m−l)!.
4.1.3 Deduce from Exercises 4.1.1 and 4.1.2 that the coefﬁcient of Am−lBl in
eA+B = 1 + A+B
1!
+ (A+B)2
2!
+ (A+B)3
3!
+···
is 1/l!(m−l)! when AB = BA.
4.1.4 Show that the coefﬁcient of Am−lBl in

1 + A
1! + A2
2! + A3
3! +···

1 + B
1! + B2
2! + B3
3! +···

is also 1/l!(m−l)!, and hence that eA+B = eAeB when AB = BA.
4.2
The exponential map onto SU(2)
If u = bi + cj + dk is a unit vector in Ri + Rj + Rk, then u2 = −1 by the
argument at the end of Section 1.4. This leads to the following elegant
extension of the exponential map from pure imaginary numbers to pure
imaginary quaternions.
Exponentiation theorem for H. When we write an arbitrary element of
Ri+Rj+Rk in the form θu, where u is a unit vector, we have
eθu = cosθ +usinθ
and the exponential function maps Ri+Rj+Rk onto S3 = SU(2).
Proof. For any pure imaginary quaternion v we deﬁne ev by the usual
inﬁnite series
ev = 1+ v
1! + v2
2! + v3
3! +··· .
This series is absolutely convergent in H for the same reason as in C: for
sufﬁciently large n, |v|n/n! < 2−n. Thus ev is meaningful for any pure

78
4
The exponential map
imaginary quaternion v. If v = θu, where u is a pure imaginary and |u| = 1,
then u2 = −1 by the remark above, and we get
eθu = 1+ θu
1! −θ2
2! −θ3u
3! + θ4
4! + θ5u
5! −θ6
6! −···
=

1−θ2
2! + θ4
4! −···

+u
 θ
1! −θ3
3! + θ5
5! −···

= cosθ +usinθ.
Also, a point a+bi+cj+dk ∈S3 can be written in the form
a+ bi+cj+dk
√
b2 +c2 +d2

b2 +c2 +d2 = a+u

b2 +c2 +d2,
where u is a unit pure imaginary quaternion. Since a2 + b2 + c2 + d2 = 1
for a quaternion a+bi+cj+dk ∈S3, there is a real θ such that
a = cosθ,

b2 +c2 +d2 = sinθ.
Thus any point in S3 is of the form cosθ + usinθ, and so the exponential
map is from Ri+Rj+Rk onto S3.
□
Up to this point, we have a beautiful analogy with the exponential map
in C. The three-dimensional space Ri+Rj+Rk is the tangent space of the
3-sphere S3 = SU(2) at the identity element 1, as we will see in the next
section.
But the algebraic situation on S3 is more interesting (if you like, more
complex) than on S1. For a pair of elements u,v ∈S3 we generally have
uv ̸= vu, and hence uvu−1 ̸= v. Thus the element uvu−1, the conjugate of v
by u−1, detects failure to commute. Remarkably, the conjugation operation
on S3 = SU(2) is reﬂected in a noncommutative operation on the tangent
space Ri+Rj+Rk that we uncover in the next section.
Exercises
4.2.1 Show that the exponential function maps any line through O in Ri+Rj+Rk
onto a circle of radius 1 in S3.
Since we can have uv ̸= vu for quaternions u and v, it can be expected, from the
previous exercise set, that we can have euev ̸= eu+v.
4.2.2 Explain why i = eiπ/2 and j = ejπ/2.
4.2.3 Deduce from Exercise 4.2.2 that at least one of eiπ/2ejπ/2, ejπ/2eiπ/2 is not
equal to eiπ/2+jπ/2.

4.3
The tangent space of SU(2)
79
4.3
The tangent space of SU(2)
The space Ri + Rj + Rk mapped onto SU(2) by the exponential function
is the tangent space at 1 of SU(2), just as the line Ri is the tangent line
at 1 of the circle SO(2). But SU(2), unlike SO(2), cannot be viewed from
“outside” by humans, so we need a method for ﬁnding tangent vectors from
“inside” SU(2). This method will later be used for the higher-dimensional
groups SO(n), SU(n), and so on.
The idea is to view a tangent vector at 1 as the “velocity vector” of
a smoothly moving point as it passes through 1. To be precise, consider
a differentiable function of t, whose values q(t) are unit quaternions, and
suppose that q(0) = 1. Then the “velocity” q′(0) at t = 0 is a tangent vector
to SU(2), and all the tangent vectors to SU(2) at 1 are obtained in this way.
The assumption that q(t) is a unit quaternion for each t in the domain
of q means that
q(t)q(t) = 1,
(*)
because qq = |q|2 for each quaternion q, as we saw in Section 1.3. By
differentiating (*), using the product rule, we ﬁnd that
q′(t)q(t) +q(t)q′(t) = 0.
(The usual proof of the product rule applies, even though quaternions do
not necessarily commute—it is a good exercise to check why this is so.)
Then setting t = 0, and bearing in mind that q(0) = 1, we obtain
q′(0)+q′(0) = 0.
So, every tangent vector q′(0) to SU(2) satisﬁes
q′(0)+q′(0) = 0,
which means that q′(0) is a pure imaginary quaternion p. Conversely, if
p is any pure imaginary quaternion, then pt ∈Ri + Rj + Rk for any real
number t, and we know from the previous section that ept ∈SU(2). Thus
q(t) = ept is a path in SU(2). This path passes through 1 when t = 0, and
it is smooth because it has the derivative
q′(t) = pept.
(To see why, differentiate the inﬁnite series for ept.) Finally, q′(0) = p,
because e0 = 1. Thus every pure imaginary quaternion is a tangent vector
to SU(2) at 1, and so the tangent space of SU(2) at 1 is Ri+Rj+Rk.

80
4
The exponential map
This construction of the tangent space to SU(2) at 1 provides a model
that we will follow for the so-called classical Lie groups in Chapter 5. In all
cases it is easy to ﬁnd the general form of a tangent vector by differentiating
the deﬁning equation of the group, but one needs the exponential function
(for matrices) to conﬁrm that each matrix X of the form in question is in
fact a tangent vector (namely, the tangent to the smooth path etX).
The Lie bracket
The great idea of Sophus Lie was to look at elements “inﬁnitesimally close
to the identity” in a Lie group, and to use them to infer behavior of ordi-
nary elements. The modern version of Lie’s idea is to infer properties of
the Lie group from properties of its tangent space. A commutative group
operation, as on SO(2), is completely captured by the sum operation on the
tangent space, because ex+y = exey. The real secret of the tangent space is
an extra structure called the Lie bracket operation, which reﬂects the non-
commutative content of the group operation. (For a commutative group,
such as SO(2), the Lie bracket on the tangent space is always zero.)
In the case of SU(2) we can already see that the sum operation on
Ri + Rj + Rk is commutative, so it cannot adequately reﬂect the product
operation on SU(2). Nor can the product on SU(2) be captured by the
quaternion product on Ri+Rj+Rk, because the quaternion product is not
always deﬁned on Ri + Rj + Rk. For example, i belongs to Ri + Rj + Rk
but the product i2 does not. What we ﬁnd is that the noncommutative
content of the product on SU(2) is captured by the Lie bracket of pure
imaginary quaternions U, V deﬁned by
[U,V] = UV −VU.
This comes about as follows. Suppose that u(s) and v(t) are two smooth
paths through 1 in SU(2), with u(0) = v(0) = 1. For each ﬁxed s we con-
sider the path
ws(t) = u(s)v(t)u(s)−1.
This path also passes through 1, and its tangent there is
w′
s(0) = u(s)v′(0)u(s)−1 = u(s)Vu(s)−1,
where V = v′(0) is the tangent vector to v(t) at 1. Now w′
s(0) is a tangent
vector at 1 for each s, so (letting s vary)
x(s) = u(s)Vu(s)−1

4.3
The tangent space of SU(2)
81
is a smooth path in Ri+Rj+Rk. The tangent x′(0) to this path at s = 0 is
also an element of Ri + Rj + Rk, because x′(0) is the limit of differences
between elements of Ri+Rj+Rk, and Ri+Rj+Rk is closed under dif-
ferences and limits. By the product rule for differentiation, and because
u(0) = 1, the tangent vector x′(0) is
d
ds

s=0
u(s)Vu(s)−1 = u′(0)Vu(0)−1 +u(0)V

−u′(0)

= UV −VU,
where U = u′(0) is the tangent vector to u(s) at 1.
It follows that if U,V ∈Ri+Rj+Rk then [U,V] ∈Ri+Rj+Rk. It is
possible to give a direct algebraic proof of this fact (see exercises). But the
proof above shows the connection between the conjugate of v(t) by u(s)−1
and the Lie bracket of their tangent vectors, and it generalizes to a proof
that U,V ∈T1(G) implies [U,V] ∈T1(G) for any matrix Lie group G. In
fact, we revisit this proof in Section 5.4.
Exercises
The deﬁnition of derivative for any function c(t) of a real variable t is
c′(t) = lim
Δt→0
c(t +Δt)−c(t)
Δt
.
4.3.1 By imitating the usual proof of the product rule, show that if c(t) = a(t)b(t)
then
c′(t) = a′(t)b(t)+a(t)b′(t).
(Do not assume that the product operation is commutative.)
4.3.2 Show also that if c(t) = a(t)−1, and a(0) = 1, then c′(0) = −a′(0), again
without assuming that the product is commutative.
4.3.3 Show, however, that if c(t) = a(t)2 then c′(t) is not equal to 2a(t)a′(t) for a
certain quaternion-valued function a(t).
To investigate the Lie bracket operation on Ri + Rj + Rk, it helps to know what
it has in common with more familiar product operations, namely bilinearity: for
any real numbers a1 and a2,
[a1U1+a2U2,V] = a1[U1,V]+a2[U2,V],
[U,a1V1+a2V2] = a1[U,V1]+a2[U,V2].
4.3.4 Deduce the bilinearity property from the deﬁnition of [U,V].
4.3.5 Using bilinearity, or otherwise, show that U,V ∈Ri + Rj + Rk implies
[U,V] ∈Ri+Rj+Rk.

82
4
The exponential map
4.4
The Lie algebra su(2) of SU(2)
The tangent space Ri+Rj+Rk of SU(2) is a real vector space, or a vector
space over R. That is, it is closed under the vector sum operation, and also
under multiplication by real numbers. The additional structure provided by
the Lie bracket operation makes it what we call su(2), the Lie algebra of
SU(2).2 In general, a Lie algebra is a vector space with a bilinear operation
[, ] satisfying
[X,Y]+[Y,X] = 0,
[X,[Y,Z]]+[Y,[Z,X]]+[Z,[X,Y]] = 0.
These algebraic properties look like poor relations of the commutative and
associative laws, and no doubt they seem rather alien at ﬁrst. Nevertheless,
they are easily seen to be satisﬁed by the Lie bracket [U,V] =UV −VU on
Ri+Rj+Rk and, more generally, on any vector space of matrices closed
under the operation U,V →UV −VU (see exercises). In the next chapter
we will see that the tangent space of any so-called classical group is a Lie
algebra for much the same reason that su(2) is.
What makes su(2) particularly interesting is that it is probably the only
nontrivial Lie algebra that anyone meets before studying Lie theory. Its
Lie bracket is not as alien as it looks, being essentially the cross product
operation on R3 that one meets in vector algebra.
To see why, consider the Lie brackets of the basis vectors i, j, and k of
Ri+Rj+Rk, which are
[i,j] = ij−ji = k+k = 2k,
[j,k] = jk−kj = i+i = 2i,
[k,i] = ki−ik = j+j = 2j.
Then, if we introduce the new basis vectors
i′ = i/2,
j′ = j/2,
k′ = k/2,
we get
[i′,j′] = k′,
[j′,k′] = i′,
[k′,i′] = j′.
2It is traditional to denote the Lie algebra of a Lie group by the corresponding lower case
Fraktur (also called German or Gothic) letter. Thus the Lie algebra of G will be denoted by
g, the Lie algebra of SU(n) by su(n), and so on.

4.4
The Lie algebra su(2) of SU(2)
83
The latter equations are precisely the same as those deﬁning the cross prod-
uct on the usual basis vectors.
This probably makes it clear that the cross product on R3 is “the same”
as the Lie bracket on Ri + Rj + Rk, but we can spell out precisely why
by setting up a 1-to-1 correspondence between Ri + Rj + Rk and R3 that
preserves the vector sum and scalar multiples (the vector space operations),
while sending the Lie bracket to the cross product.
The map ϕ : bi+cj+dk →(2b,2c,2d) is a 1-to-1 correspondence that
preserves the vector space operations, and it also sends i′, j′, k′ and their
Lie brackets to i, j, k and their cross products, respectively. It follows that
ϕ sends all Lie brackets to the corresponding cross products, because the
Lie bracket of arbitrary vectors, like the cross product of arbitrary vectors,
is determined by its values on the basis vectors (by bilinearity).
Exercises
The second property of the Lie bracket is known as the Jacobi identity, and all
beginners in Lie theory are asked to check that it follows from the deﬁnition
[X,Y] = XY −YX.
4.4.1 Prove the Jacobi identity by using the deﬁnition [X,Y] = XY −YX to ex-
pand [X,[Y,Z]] + [Y,[Z,X]] + [Z,[X,Y]]. Assume only that the product is
associative and that the usual laws for plus and minus apply.
4.4.2 Using known properties of the cross product, or otherwise, show that the
Lie bracket operation on su(2) is not associative.
In the words of Kaplansky [1963], p. 123,
...the commutative and associative laws, so sadly lacking in the Lie
algebra itself, are acquired under the mantle of f.
By f he means a certain inner product, called the Killing form. A special case of
it is the ordinary inner product on R3, for which we certainly have commutativity:
u ·v = v·u. “Associativity under the mantle of the inner product” means
(u ×v)·w = u ·(v×w).
4.4.3 Show that if
u = u1i+u2j+u3k,
v = v1i+v2j+v3k,
w = w1i+w2j+w3k,
then
u ·(v×w) =

u1
u2
u3
v1
v2
v3
w1
w2
w3

.
4.4.4 Deduce from Exercise 4.4.3 that (u ×v)·w = u ·(v×w).

84
4
The exponential map
4.5
The exponential of a square matrix
We deﬁne the matrix absolute value of A = (aij) to be
|A| =
	
∑
i,j
|aij|2.
For an n × n real matrix A the absolute value |A| is the distance from the
origin O in Rn2 of the point
(a11,a12,...,a1n,a21,a22,...,a2n,...,an1,...,ann).
If A has complex entries, and if we interpret each copy of C as R2 (as in
Section 3.3), then |A| is the distance from O of the corresponding point in
R2n2. Similarly, if A has quaternion entries, then |A| is the distance from O
of the corresponding point in R4n2.
In all cases, |A −B| is the distance between the matrices A and B, and
we say that a sequence A1,A2,A3,... of n × n matrices has limit A if, for
each ε > 0, there is an integer M such that
m > M =⇒|Am −A| < ε.
The key property of the matrix absolute value is the following inequal-
ity, a consequence of the triangle inequality (which holds in the plane and
hence in any Rk) and the Cauchy–Schwarz inequality.
Submultiplicative property. For any two real n × n matrices A and B,
|AB| ≤|A||B|.
Proof. If A = (aij) and B = (bij), then it follows from the deﬁnition of
matrix product that
|(i, j)-entry of AB| = |ai1b1j +ai2b2j +···+ainbnj|
≤|ai1b1j|+|ai2b2j|+···+|ainbnj|
by the triangle inequality
= |ai1||b1j|+|ai2||b2j|+···+|ain||bnj|
by the multiplicative property of absolute value
≤

|ai1|2 +···+|ain|2

|b1j|2 +···+|bnj|2
by the Cauchy–Schwarz inequality.

4.5
The exponential of a square matrix
85
Now, summing the squares of both sides, we get
|AB|2 = ∑
i,j
|(i, j)-entry of AB|2
≤∑
i,j

|ai1|2 +···+|ain|2
|b1j|2 +···+|bnj|2
= ∑
i

|ai1|2 +···+|ain|2∑
j

|b1j|2 +···+|bnj|2
= |A|2|B|2,
as required
□
It follows from the submultiplicative property that |Am| ≤|A|m. Along
with the triangle inequality |A+B| ≤|A|+|B|, the submultiplicative prop-
erty enables us to test convergence of matrix inﬁnite series by comparing
them with series of real numbers. In particular, we have:
Convergence of the exponential series. If A is any n×n real matrix, then
1+ A
1! + A2
2! + A3
3! +··· ,
where 1 = n×n identity matrix,
is convergent in Rn2.
Proof. It sufﬁces to prove that this series is absolutely convergent, that is,
to prove the convergence of
|1|+ |A|
1! + |A2|
2! + |A3|
3! +··· .
This is a series of positive real numbers, whose terms (except for the ﬁrst)
are less than or equal to the corresponding terms of
1+ |A|
1! + |A|2
2! + |A|3
3! +···
by the submultiplicative property. The latter series is the series for the real
exponential function e|A|; hence the original series is convergent.
□
Thus it is meaningful to make the following deﬁnition, valid for real,
complex, or quaternion matrices.

86
4
The exponential map
Deﬁnition. The exponential of any n×n matrix A is given by the series
eA = 1+ A
1! + A2
2! + A3
3! +··· .
The matrix exponential function is a generalization of the complex and
quaternion exponential functions.
We already know that each complex
number z = a+bi can be represented by the 2×2 real matrix
Z =
a
−b
b
a

,
and it is easy to check that ez is represented by eZ. We deﬁned the quater-
nion q = a+bi+cj+dk to be the 2×2 complex matrix
Q =
a+di
−b+ci
b+ci
a−di

,
so the exponential of a quaternion matrix may be represented by the expo-
nential of a complex matrix.
From now on we will often denote the exponential function simply by
exp, regardless of the type of objects being exponentiated.
Exercises
The version of the Cauchy–Schwarz inequality used to prove the submultiplicative
property is the real inner product inequality |u ·v| ≤|u||v|, where
u = (|ai1|,|ai2|,...,|ain|)
and
v =

|b j1|,|b j2|,...,|b jn|

.
It is probably a good idea for me to review this form of Cauchy–Schwarz, since
some readers may not have seen it.
The proof depends on the fact that w·w = |w|2 ≥0 for any real vector w.
4.5.1 Show that 0 ≤(u + xv) · (u + xv) = |u|2 + 2(u · v)x + x2|v|2 = q(x), for any
real vectors u, v and real number x.
4.5.2 Use the positivity of the quadratic function q(x) found in Exercise 4.5.1 to
deduce that
(2u ·v)2 −4|u|2|v|2 ≤0,
that is, |u ·v| ≤|u||v|.

4.6
The afﬁne group of the line
87
Matrix exponentiation gives another proof that eiθ = cosθ +isinθ, since we
can interpret iθ as a 2 ×2 real matrix A.
4.5.3 Show, directly from the deﬁnition of matrix exponentiation, that
A =
0
−θ
θ
0

=⇒
eA =
cosθ
−sinθ
sinθ
cosθ

.
The exponential of an arbitrary matrix is hard to compute in general, but easy
when the matrix is diagonal, or diagonalizable.
4.5.4 Suppose that D is a diagonal matrix with diagonal entries λ1,λ2,...,λk. By
computing the powers Dn show that eD is a diagonal matrix with diagonal
entries eλ1,eλ2,...,eλk.
4.5.5 If A is a matrix of the form BCB−1, show that eA = BeCB−1.
4.5.6 By term-by-term differentiation, or otherwise, show that d
dt etA = AetA for
any square matrix A.
4.6
The afﬁne group of the line
Transformations of R of the form
fa,b(x) = ax+b,
where
a,b ∈R
and
a > 0,
are called afﬁne transformations. They form a group because the product of
any two such transformations is another of the same form, and the inverse
of any such transformation of another of the same form. We call this group
Aff(1), and we can view it as a matrix group. The function fa,b corresponds
to the matrix
Fa,b =
a
b
0
1

, applied on the left to
x
1

,
because
a
b
0
1
x
1

=
ax+b
1

.
Thus Aff(1) can be viewed as a group of 2 × 2 real matrices, and hence
it is a geometric object in R4. On the other hand, Aff(1) is intrinsically
two-dimensional, because its elements form a half-plane.
To see why,
consider ﬁrst the two-dimensional subspace of R4 consisting of the points
(a,b,0,0). This is a plane, and hence so is the set of points (a,b,0,1)

88
4
The exponential map
obtained by translating it by distance 1 in the direction of the fourth coor-
dinate. Finally, we get half of this plane by restricting the ﬁrst coordinate
to a > 0.
Aff(1) is closed under nonsingular limits; hence it is a two-dimensional
matrix Lie group, like the real vector space R2 under vector addition, and
the torus S1 ×S1. Unlike these two matrix Lie groups, however, Aff(1) is
not abelian. For example,
f2,1 f1,2(x) = 1(2x+1)+2 = 2x+3,
whereas
f1,2 f2,1(x) = 2(1x+2)+1 = 2x+5.
Aff(1)is infact the onlyconnected, nonabeliantwo-dimensional Lie group.
This makes it interesting, yet still amenable to computation. As we will
see, it is easy to compute its tangent vectors, and to exponentiate them,
from ﬁrst principles. But ﬁrst note that there are two ways in which Aff(1)
differs from the Lie groups studied in previous chapters.
• As a geometric object, Aff(1) is an unbounded subset of R4 (because
b can be arbitrary and a is an arbitrary positive number). We say that
it is a noncompact Lie group, whereas SO(2), SO(3), and SU(2)
are compact. In Chapter 8 we give a more precise discussion of
compactness.
• As a group, it admits an ∞-to-1 homomorphism onto another inﬁnite
group. The homomorphism ϕ in question is
ϕ :
a
b
0
1

→
a
0
0
1

.
This sends the inﬁnitely many matrices Fa,b, as b varies, to the matrix
Fa,0, and it is easily checked that
ϕ(Fa1,b1Fa2,b2) = ϕ(Fa1,b1)ϕ(Fa2,b2).
It follows, in particular, that Aff(1) is not a simple group. Also, the nor-
mal subgroup of matrices in the kernel of ϕ is itself a matrix Lie group.
The kernel consists of all the matrices that ϕ sends to the identity matrix,
namely, the group of matrices of the form
1
b
0
1

for
b ∈R.

4.6
The afﬁne group of the line
89
Geometrically, this subgroup is a line, and the group operation corresponds
to addition on the line, because
1
b1
0
1
1
b2
0
1

=
1
b1 +b2
0
1

.
The Lie algebra of Aff(1)
Since Aff(1) is half of a plane in the space R4 of 2 × 2 matrices, it is
geometrically clear that its tangent space at the identity element is a plane.
However, to ﬁnd explicit matrices for the elements of the tangent space
we look at the vectors from the identity element
 1 0
0 1

of Aff(1) to nearby
points of Aff(1).
These are the vectors
1+α
β
0
1

−
1
0
0
1

=
α
β
0
0

= α
1
0
0
0

+β
0
1
0
0

for small values of α and β. Normally, one needs to ﬁnd the limiting
directions of these vectors (the “tangent vectors”) as α,β →0, but in this
case all such directions lie in the plane spanned by the vectors
J =
1
0
0
0

,
K =
0
1
0
0

.
The Lie bracket [u,v] = uv −vu on this two-dimensional space is deter-
mined by the Lie bracket of the basis vectors:
[J,K] = K.
The exponential function maps the tangent space 1-to-1 onto Aff(1), as
one sees from some easy calculations with a general matrix

α β
0 0

in the
tangent space. First, induction shows that
α
β
0
0
n
=
αn
βαn−1
0
0

,
or, in terms of J and K,
(αJ+βK)n = αnJ+βαn−1K.

90
4
The exponential map
Then substituting these powers in the exponential series (and writing 1 for
the identity matrix) gives
eαJ+βK
= 1+ 1
1!(αJ+βK)+ 1
2!(αJ+βK)2 +···+ 1
n!(αJ+βK)n +···
= 1+ 1
1!(αJ+βK)+ 1
2!(α2J+βαK)+···+ 1
n!(αnJ+βαn−1K)+···
= 1+
 α
1! + α2
2! +···+ αn
n! +···

J+β
 1
1! + α
2! +···+ αn−1
n!
+···

K
=

eα
β
α (eα −1)
0
1

or
1
β
0
1

if
α = 0.
The former matrix equals
 a b
0 1

, where a > 0, for a unique choice of α
and β. First choose α so that a = eα; then choose β so that
b = β
α (eα −1)
or
b = β
if
α = 0.
Exercises
Exponentiation of matrices does not have all the properties of ordinary exponen-
tiation, because matrices do not generally commute. However, exponentiation
works normally on matrices that do commute, such as powers of a ﬁxed matrix.
Here is an example in Aff(1).
4.6.1 Work out
 a b
0 1
2 and
a b
0 1
3, and then prove by induction that
a
b
0
1
n
=

an
b an−1
a−1
0
1

.
4.6.2 Use the formula in Exercise 4.6.1 to work out the nth power of the matrix
eαJ+βK, and compare it with the matrix enαJ+nβK obtained by exponenti-
ating nαJ +nβK.
4.6.3 Show that the matrices

an b an−1
a−1
0
1

, for n = 1,2,3,..., lie on a line in R4.
Also show that the line passes through the point
1 0
0 1

.

4.7
Discussion
91
4.7
Discussion
The ﬁrst to extend the exponential function to noncommuting objects was
Hamilton, who applied it to quaternions almost as soon as he discovered
them in 1843. In the paper Hamilton [1967], a writeup of an address to the
Royal Irish Academy on November 13, 1843, he deﬁnes the exponential
function for a quaternion q on p. 207,
eq = 1+ q
1 + q2
2! + q3
3! +··· ,
and observes immediately that
eq′eq = eq′+q
when
qq′ = q′q.
On p. 225 he evaluates the exponential of a pure imaginary quaternion,
stating essentially the result of Section 4.2, that
eθu = cosθ +usinθ
when
|u| = 1.
The exponential map was extended to Lie groups in general by Lie
in 1888. From his point of view, exponentiation sends “inﬁnitesimal” el-
ements of a continuous group to “ﬁnite” elements (see Hawkins [2000],
p. 82). A few mathematicians in the late nineteenth century brieﬂy noted
that exponentiation makes sense for matrices, but the theory of matrix ex-
ponentiation did not ﬂourish until Wedderburn [1925] proved the submulti-
plicative property of the matrix absolute value that guarantees convergence
of the exponential series for matrices. The trailblazing investigation of von
Neumann [1929] takes Wedderburn’s result as its starting point.
The matrix exponential function has many properties in common with
the ordinary exponential, such as
eX = lim
n→∞

1+ X
n
n
.
We do not need this property in this book, but it nicely illustrates the idea
of Lie (and, before him, Jordan [1869]), that the “ﬁnite” elements of a
continuous group may be “generated” by its “inﬁnitesimal” elements. If X
is a tangent vector at 1 to a group G and n is “inﬁnitely large,” then 1+ X
n
is an “inﬁnitesimal” element of G. By iterating this element n times we
obtain the “ﬁnite” element eX of G.

92
4
The exponential map
It was discovered by Lie’s colleague Engel in 1890 that, in the group
SL(2,C) of 2×2 complex matrices with determinant 1, not every element
is an exponential. In particular, the matrix
 −1 1
0 −1

is not the exponen-
tial of any matrix tangent to SL(2,C) at 1; hence it is not “generated by
an inﬁnitesimal element” of SL(2,C). (We indicate a proof in the exer-
cises to Section 5.6.) The result was considered paradoxical at the time
(see Hawkins [2000], p. 86), and its mystery was dispelled only when the
global properties of Lie groups became better understood. In the 1920s it
was realized that the topology of a Lie group is the key to its global behav-
ior. For example, the paradoxical behavior of SL(2,C) can be attributed
to its noncompactness, because it can be shown that every element of a
connected, compact Lie group is the exponential of a tangent vector. We
do not prove this theorem about exponentiation in this book, but we will
discuss compactness and connectedness further in Chapter 8.
For a noncompact, but connected, group G the next best thing to sur-
jectivity of exp is the following: every g ∈G is the product eX1eX2 ···eXk of
exponentials of ﬁnitely many tangent vectors X1,X2,...,Xk. This result is
due to von Neumann [1929], and we give a proof in Section 8.6.
For readers acquainted with differential geometry, it should be men-
tioned that the exponential function can be generalized even beyond matrix
groups, to Riemannian manifolds. In this setting, the exponential function
maps the tangent space TP(M) at point P on a Riemannian manifold M
into M by mapping lines through O in TP(M) isometrically onto geodesics
of M through P. The Riemannian manifolds S1 = {z ∈C : |z| = 1} and
S3 = {q ∈H : |q| = 1}, and their tangent spaces R and R3, nicely illustrate
the geodesic aspect of exponentiation. The exponential map sends straight
lines through O in the tangent space isometrically to geodesic circles in
the manifolds (to S1 itself in C, and to the unit circles cosθ +usinθ in H,
which are geodesic because they are the largest possible circles in S3).

5
The tangent space
PREVIEW
The miracle of Lie theory is that a curved object, a Lie group G, can be
almost completely captured by a ﬂat one, the tangent space T1(G) of G at
the identity. The tangent space of G at the identity consists of the tangent
vectors to smooth paths in G where they pass through 1. A path A(t) in G
is called smooth if its derivative A′(t) exists, and if A(0) = 1 we call A′(0)
the tangent or velocity vector of A(t) at 1. T1(G) consists of the velocity
vectors of all smooth paths through 1.
It is quite easy to determine the form of the matrix A′(0) for a smooth
path A(t) through 1 in any of the classical groups, that is, the generalized
rotation groups of Chapter 3 and the general and special linear groups,
GL(n,C) and SL(n,C), we will meet in Section 5.6. For example, any
tangent vector of SO(n) at 1 is an n × n real skew-symmetric matrix—a
matrix X such that X +XT = 0. The problem is to ﬁnd smooth paths in the
ﬁrst place. It is here that the exponential function comes to our rescue.
As we saw in Section 4.5, eX is deﬁned for any n× n matrix X by the
inﬁnite series used to deﬁne ex for any real or complex number x. This ma-
trix exponential function provides a smooth path with prescribed tangent
vector at 1, namely the path A(t) = etX, for which A′(0) = X. In particular,
it turns out that if X is skew-symmetric then etX ∈SO(n) for any real t, so
the potential tangent vectors to SO(n) are the actual tangent vectors.
In this way we ﬁnd that T1(SO(n)) = {X ∈Mn(R) : X +XT = 0}, where
Mn(R) is the space of n× n real matrices. The exponential function simi-
larly enables us to ﬁnd the tangent spaces of all the classical groups: O(n),
SO(n), U(n), SU(n), Sp(n), GL(n,C), and SL(n,C).
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 5,
93
c⃝Springer Science+Business Media, LLC 2008

94
5
The tangent space
5.1
Tangent vectors of O(n), U(n), Sp(n)
In a space S of matrices, a path is a continuous function t →A(t) ∈S, where
t belongs to some interval of real numbers, so the entries aij(t) of A(t) are
continuous functions of the real variable t. The path is called smooth, or
differentiable, if the functions aij(t) are differentiable.
For example, the function
t →B(t) =
cost
−sint
sint
cost

is a smooth path in SO(2), while the function
t →C(t) =
cos|t|
−sin|t|
sin|t|
cos|t|

is a path in SO(2) that is not smooth at t = 0.
The derivative A′(t) of a smooth A(t) is deﬁned in the usual way as
lim
Δt→0
A(t +Δt)−A(t)
Δt
,
and one sees immediately that A′(t) is simply the matrix with entries a′
ij(t),
where aij(t) are the entries of A(t). Tangent vectors at 1 of a group G of
matrices are matrices X of the form
X = A′(0),
where A(t) is a smooth path in G with A(0) = 1 (that is, a path “passing
through 1 at time 0”). Tangent vectors can thus be viewed as “velocity
vectors” of points moving smoothly through the point 1, as in Section 4.3.
For example, in SO(2),
A(t) =
cosθt
−sinθt
sinθt
cosθt

is a smooth path through 1 because A(0) = 1. And since
A′(t) =
−θ sinθt
−θ cosθt
θ cosθt
−θ sinθt

,
the corresponding tangent vector is
A′(0) =
0
−θ
θ
0

.

5.1
Tangent vectors of O(n), U(n), Sp(n)
95
In fact, all tangent vectors are of this form, so they form the 1-dimensional
vector space of real multiples of the matrix i =
 0 −1
1 0

. This conﬁrms what
we already know geometrically: SO(2) is a circle and its tangent space at
the identity is a line.
We now ﬁnd the form of tangent vectors for all the groups O(n), U(n),
Sp(n) by differentiating the deﬁning equation AAT = 1 of their members
A. (In the case of O(n), A is real, so A = A. In the cases of U(n) and Sp(n),
A is the complex and quaternion conjugate, respectively.)
Tangent vectors of O(n), U(n), Sp(n). The tangent vectors X at 1 are
matrices of the following forms (where 0 denotes the zero matrix):
(a) For O(n), n×n real matrices X such that X +XT = 0.
(b) For U(n), n×n complex matrices X such that X +XT = 0.
(c) For Sp(n), n×n quaternion matrices X such that X +XT = 0.
Proof. (a) The matrices A ∈O(n) satisfy AAT = 1. Let A = A(t) be a
smooth path originating at 1, and take d/dt of the equation
A(t)A(t)T = 1.
The product rule holds as for ordinary functions, as does d
dt 1 = 0 because
1 is a constant. Also, d
dt (AT) =
 d
dt A
T by considering matrix entries. So
we have
A′(t)A(t)T +A(t)A′(t)T = 0.
Since A(0) = 1 = A(0)T, for t = 0 this equation becomes
A′(0)+A′(0)T = 0.
Thus any tangent vector X = A′(0) satisﬁes X +XT = 0.
(b) The matrices A ∈U(n) satisfy AA
T = 1. Again let A = A(t) be a
smooth path with A(0) = 1 and now take d/dt of the equation AA
T = 1. By
considering matrix entries we see that d
dt A(t) = A′(t). Then an argument
like that in (a) shows that any tangent vector X satisﬁes X +XT = 0.
(c) For the matrices A ∈Sp(n) we similarly ﬁnd that the tangent vectors
X satisfy X +XT = 0.
□

96
5
The tangent space
The matrices X satisfying X + XT = 0 are called skew-symmetric, be-
cause the reﬂection of each entry in the diagonal is its negative. That is,
xji = −xij. In particular, all the diagonal elements of a skew-symmetric
matrix are 0. Matrices X satisfying X +XT = 0 are called skew-Hermitian.
Their entries satisfy xji = −xij and their diagonal elements are pure imag-
inary.
It turns out that all skew-symmetric n × n real matrices are tangent,
not only to O(n), but also to SO(n) at 1. To prove this we use the matrix
exponential function from Section 4.5, showing that eX ∈SO(n) for any
skew-symmetric X, in which case X is tangent to the smooth path etX in
SO(n).
Exercises
To appreciate why smooth paths are better than mere paths, consider the following
example.
5.1.1 Interpret the paths B(t) and C(t) above as paths on the unit circle, say for
−π/2 ≤t ≤π/2.
5.1.2 If B(t) or C(t) is interpreted as the position of a point at time t, how does
the motion described by B(t) differ from the motion described by C(t)?
5.2
The tangent space of SO(n)
In this section we return to the addition formula of the exponential function
eA+B = eAeB
when
AB = BA,
which was previously set as a series of exercises in Section 4.1. This for-
mula can be proved by observing the nature of the calculation involved,
without actually doing any calculation. The argument goes as follows.
According to the deﬁnition of the exponential function, we want to
prove that

1+ A+B
1!
+···+ (A+B)n
n!
+···

=

1+ A
1! +···+ An
n! +···

1+ B
1! +···+ Bn
n! +···

.
This could be done by expanding both sides and showing that the coefﬁ-
cient of AlBm is the same on both sides. But if AB = BA the calculation

5.2
The tangent space of SO(n)
97
involved is the same as the calculation for real numbers A and B, in which
case we know that eA+B = eAeB by elementary calculus. Therefore, the
formula is correct for any commuting variables A and B.
Now, the beauty of the matrices X and XT appearing in the condition
X +XT = 0 is that they commute! This is because, under this condition,
XXT = X(−X) = (−X)X = XTX.
Thus it follows from the above property of the exponential function that
eXeXT = eX+XT = e0 = 1.
But also, eXT = (eX)T because (XT)m = (Xm)T and hence all terms in the
exponential series get transposed. Therefore
1 = eXeXT = eX(eX)T.
In other words, if X +XT = 0 then eX is an orthogonal matrix.
Moreover, eX has determinant 1, as can be seen by considering the path
of matrices tX for 0 ≤t ≤1. For t = 0, we have tX = 0, so
etX = e0 = 1,
which has determinant 1.
And, as t varies from 0 to 1, etX varies continuously from 1 to eX. This
implies that the continuous function det(etX) remains constant, because
det = ±1 for orthogonal matrices, and a continuous function cannot take
two (and only two) values. Thus we necessarily have det(eX) = 1, and
therefore if X is an n×n real matrix with X +XT = 0 then eX ∈SO(n).
This allows us to complete our search for all the tangent vectors to
SO(n) at 1.
Tangent space of SO(n). The tangent space of SO(n) consists of precisely
the n×n real vectors X such that X +XT = 0.
Proof. In the previous section we showed that all tangent vectors X to
SO(n) at 1 satisfy X +XT = 0. Conversely, we have just seen that, for any
vector X with X +XT = 0, the matrix eX is in SO(n).
Now notice that X is the tangent vector at 1 for the path A(t) = etX in
SO(n). This holds because
d
dt etX = XetX,

98
5
The tangent space
as in ordinary calculus. (This can be checked by differentiating the series
for etX.) It follows that A(t) has the tangent vector A′(0) = X at 1, and
therefore each X such that X +XT = 0 occurs as a tangent vector to SO(n)
at 1, as required.
□
As mentioned in the previous section, a matrix X such that X +XT = 0
is called skew-symmetric.
Important examples are the 3 × 3 skew-
symmetric matrices, which have the form
X =
⎛
⎝
0
−x
−y
x
0
−z
y
z
0
⎞
⎠.
Notice that sums and scalar multiples of these skew-symmetric matrices
are again skew-symmetric, so the 3 × 3 skew-symmetric matrices form a
vector space. This space has dimension 3, as we would expect, since it is
the tangent space to the 3-dimensional space SO(3). Less obviously, the
skew-symmetric matrices are closed under the Lie bracket operation
[X1,X2] = X1X2 −X2X1.
Later we will see that the tangent space of any Lie group G is a vector space
closed under the Lie bracket, and that the Lie bracket reﬂects the conjugate
g1g2g−1
1
of g2 by g−1
1
∈G. This is why the tangent space is so important
in the investigation of Lie groups: it “linearizes” them without obliterating
much of their structure.
Exercises
According to the theorem above, the tangent space of SO(3) consists of 3×3 real
matrices X such that X = −XT. The following exercises study this space and the
Lie bracket operation on it.
5.2.1 Explain why each element of the tangent space of SO(3) has the form
X =
⎛
⎝
0
−x
−y
x
0
−z
y
z
0
⎞
⎠= xI+yJ +zK,
where
I =
⎛
⎝
0
−1
0
1
0
0
0
0
0
⎞
⎠,
J =
⎛
⎝
0
0
−1
0
0
0
1
0
0
⎞
⎠,
K =
⎛
⎝
0
0
0
0
0
−1
0
1
0
⎞
⎠.

5.3
The tangent space of U(n), SU(n), Sp(n)
99
5.2.2 Deduce from Exercise 5.2.1 that the tangent space of SO(3) is a real vector
space of dimension 3.
5.2.3 Check that [I,J] = K, [J,K] = I, and [K,I] = J. (This shows, among other
things, that the 3 × 3 real skew-symmetric matrices are closed under the
Lie bracket operation.)
5.2.4 Deduce from Exercises 5.2.2 and 5.2.3 that the tangent space of SO(3) un-
der the Lie bracket is isomorphic to R3 under the cross product operation.
5.2.5 Prove directly that the n ×n skew-symmetric matrices are closed under the
Lie bracket, using XT = −X and Y T = −Y.
The argument above shows that exponentiation sends each skew-symmetric
X to an orthogonal eX, but it is not clear that each orthogonal matrix is obtainable
in this way. Here is an argument for the case n = 3.
5.2.6 Find the exponential of the matrix B =
⎛
⎝
0
−θ
0
θ
0
0
0
0
0
⎞
⎠.
5.2.7 Show that AeBAT = eABAT for any orthogonal matrix A.
5.2.8 Deduce from Exercises 5.2.6 and 5.2.7 that each matrix in SO(3) equals eX
for some skew-symmetric X.
5.3
The tangent space of U(n), SU(n), Sp(n)
We know from Sections 3.3 and 3.4 that U(n) and Sp(n), respectively, are
the groups of n×n complex and quaternion matrices A satisfying AA
T = 1.
This equation enables us to ﬁnd their tangent spaces by essentially the same
steps we used to ﬁnd the tangent space of SO(n) in the last two sections.
The outcome is also the same, except that, instead of skew-symmetric ma-
trices, we get skew-Hermitian matrices. As we saw in Section 5.1, these
matrices X satisfy X +XT = 0.
Tangent space of U(n) and Sp(n). The tangent space of U(n) consists of
all the n × n complex matrices satisfying X + XT = 0. The tangent space
of Sp(n) consists of all n×n quaternion matrices X satisfying X +XT = 0,
where X denotes the quaternion conjugate of X.
Proof. From Section 5.1 we know that the tangent vectors at 1 to a space
of matrices satisfying AAT = 1 are matrices X satisfying X +XT = 0.

100
5
The tangent space
Conversely, suppose that X is any n×n complex (respectively, quater-
nion) matrix such that X +XT = 0. It follows that
XT = −X
and therefore
XXT = X(−X) = (−X)X = XTX.
This implies, by the addition formula for the exponential function for com-
muting matrices, that
1 = e0 = eX+XT
= eXeXT
.
It is also clear from the deﬁnition of eX that eXT = (eX)T. So if X is any
n×n complex (respectively, quaternion) matrix satisfying X +XT = 0 then
eX is in U(n) (respectively, Sp(n)). It follows in turn that any such X is a
tangent vector at 1. Namely, X = A′(0) for the smooth path A(t) = etX. □
In Section 5.1 we found the form of tangent vectors to O(n) at 1, but
in Section 5.2 we were able to show that all vectors of this form are in
fact tangent to SO(n), so we actually had the tangent space to SO(n) at 1.
An identical step from U(n) to SU(n) is not possible, because the tangent
space of U(n) at 1 is really a larger space than the tangent space to SU(n).
Vectors X in the tangent space of SU(n) satisfy the additional condition that
Tr(X), the trace of X, is zero. (Recall the deﬁnition from linear algebra:
the trace of a square matrix is the sum of its diagonal entries.)
To prove that Tr(X) = 0 for any tangent vector X to SU(n), we use the
following lemma about the determinant and the trace.
Determinant of exp. For any square complex matrix A,
det(eA) = eTr(A).
Proof. We appeal to the theorem from linear algebra that for any complex
matrix A there is an invertible complex3 matrix B and an upper triangular
complex matrix T such that A = BTB−1.
The nice thing about putting A in this form is that
(BTB−1)m = BTB−1BTB−1···BTB−1 = BT mB−1
3The matrix B may be complex even when A is real. We then have an example of a
phenomenon once pointed out by Jacques Hadamard: the shortest path between two real
objects—in this case, det(eA) and eTr(A)—may pass through the complex domain.

5.3
The tangent space of U(n), SU(n), Sp(n)
101
and hence
eA = ∑
m≥0
Am
m! = B

∑
m≥0
T m
m!

B−1 = BeTB−1.
It therefore sufﬁces to prove det(eT) = eTr(T) for upper triangular T, be-
cause this implies
det(eA) = det(BeTB−1) = det(eT) = eTr(T) = eTr(BTB−1) = eTr(A).
Here we are appealing to another theorem from linear algebra, which states
that Tr(BC) = Tr(CB) and hence Tr(BCB−1) = Tr(C) (exercise).
To obtain the value of det(eT) for upper triangular T, suppose that
T =
⎛
⎜
⎜
⎜
⎜
⎜
⎝
t11
∗
∗
···
∗
0
t22
∗
···
∗
0
0
t33
···
∗
...
...
0
0
···
0
tnn
⎞
⎟
⎟
⎟
⎟
⎟
⎠
,
where the entries marked * are arbitrary. From this one can see that
• T 2 is upper triangular, with ith diagonal entry equal to t2
ii,
• T m is upper triangular, with ith diagonal entry equal to tm
ii ,
• eT is upper triangular, with ith diagonal entry equal to etii,
and hence
det(eT) = et11et22 ···etnn = et11+t22+···+tnn = eTr(T),
as required.
□
Tangent space of SU(n). The tangent space of SU(n) consists of all n×n
complex matrices X such that X +XT = 0 and Tr(X) = 0.
Proof.
Elements of SU(n) are, by deﬁnition, matrices A ∈U(n) with
det(A) = 1. We know that the A ∈U(n) are of the form eX with X +XT = 0.
The extra condition det(A) = 1 is therefore equivalent to
1 = det(A) = det(eX) = eTr(X)
by the theorem just proved. It follows that, given any A ∈U(n),
A ∈SU(n) ⇔det(A) = 1 ⇔eTr(X) = 1 ⇔Tr(X) = 0.
Thus the tangent space of SU(n) consists of the n×n complex matrices X
such that X +XT = 0 and Tr(X) = 0.
□

102
5
The tangent space
Exercises
Another proof of the crucial result det(eA) = eTr(A) uses less linear algebra but
more calculus. It goes as follows (if you need help with the details, see Tapp
[2005], p. 72 and p. 88).
Suppose B(t) is a smooth path of n × n complex matrices with B(0) = 1, let
bij(t) denote the entry in row i and column j of B(t), and let Bij(t) denote the
result of omitting row i and column j.
5.3.1 Show that
det(B(t)) =
n
∑
j=1
(−1)j+1b1 j(t)det(B1 j(t)),
and hence
d
dt

t=0
det(B(t)) =
n
∑
j=1
(−1)j+1

b′
1 j(0)det(B1 j(0))+b1 j(0) d
dt

t=0
det(B1 j(t))

.
5.3.2 Deduce from Exercise 5.3.1, and the assumption B(0) = 1, that
d
dt

t=0
det(B(t)) = b′
11(0)+ d
dt

t=0
det(B11(t)).
5.3.3 Deduce from Exercise 5.3.2, and induction, that
d
dt

t=0
det(B(t)) = b′
11(0)+b′
22(0)+···+b′
nn(0) = Tr(B′(0)).
We now apply Exercise 5.3.3 to the smooth path B(t) = etA, for which B′(0) = A,
and the smooth real function
f(t) = det(etA),
for which
f(0) = 1.
By the deﬁnition of derivative,
f ′(t) = lim
h→0
1
h

det(e(t+h)A)−det(etA)

.
5.3.4 Using the property det(MN) = det(M)det(N) and Exercise 5.3.3, show that
f ′(t) = det(etA) d
dt

t=0
det(etA) = f(t)Tr(A).
5.3.5 Solve the equation for f(t) in Exercise 5.3.4 by setting f(t) = g(t)et·Tr(A)
and showing that g′(t) = 0, hence g(t) = 1. (Why?)
Conclude that det(eA) = eTr(A).

5.4
Algebraic properties of the tangent space
103
The tangent space of SU(2) should be the same as the space Ri + Rj + Rk
shown in Section 4.2 to be mapped onto SU(2) by the exponential function. This
is true, but it requires some checking.
5.3.6 Show that the skew-Hermitian matrices in the tangent space of SU(2) can
be written in the form bi + cj + dk, where b,c,d ∈R and i, j, and k are
matrices with the same multiplication table as the quaternions i, j, and k.
5.3.7 Also ﬁnd the tangent space of Sp(1) (which should be the same).
Finally, it should be checked that Tr(XY) = Tr(YX), as required in the proof
that det(eA) = eTr(A). This can be seen almost immediately by meditating on the
sum
x11y11 +x12y21 +···+x1nyn1
+x21y12 +x22y22 +···+x2nyn2
...
+xn1y1n +xn2y2n +···+xnnynn.
5.3.8 Interpret this sum as both Tr(XY) and Tr(YX).
5.4
Algebraic properties of the tangent space
If G is any matrix group, we can deﬁne its tangent space at the identity,
T1(G), to be the set of matrices of the form X = A′(0), where A(t) is a
smooth path in G with A(0) = 1.
Vector space properties. T1(G) is a vector space over R; that is, for any
X,Y ∈T1(G) we have X +Y ∈T1(G) and rX ∈T1(G) for any real r.
Proof. Suppose X = A′(0) and Y = B′(0) for smooth paths A(t),B(t) ∈G
with A(0) = B(0) = 1, so X,Y ∈T1(G). It follows that C(t) = A(t)B(t) is
also a smooth path in G with C(0) = 1, and hence C′(0) is also a member
of T1(G).
We now compute C′(0) by the product rule and ﬁnd
C′(0) = d
dt

t=0
A(t)B(t) = A′(0)B(0)+A(0)B′(0)
= X +Y
because A(0) = B(0) = 1.
Thus X,Y ∈T1(G) implies X +Y ∈T1(G).

104
5
The tangent space
To see why rX ∈T1(G) for any real r, consider the smooth path D(t) =
A(rt). We have D(0) = A(0) = 1, so D′(0) ∈T1(G), and
D′(0) = rA′(0) = rX.
Hence X ∈T1(G) implies rX ∈T1(G), as claimed.
□
We see from this proof that the vector sum is to some extent an image
of the product operation on G. But it is not a very faithful image, because
the vector sum is commutative and the product on G generally is not.
We ﬁnd a product operation on T1(G) that more faithfully reﬂects the
product on G by studying the behavior of smooth paths A(s) and B(t) near
1 when s and t vary independently.
Lie bracket property. T1(G) is closed under the Lie bracket, that is, if
X,Y ∈T1(G) then [X,Y] ∈T1(G), where [X,Y] = XY −YX.
Proof. Suppose A(0) = B(0) = 1, A′(0) = X,B′(0) = Y, so X,Y ∈T1(G).
Now consider the path
Cs(t) = A(s)B(t)A(s)−1
for some ﬁxed value of s.
Then Cs(t) is smooth and Cs(0) = 1, so C′
s(0) ∈T1(G). But also,
C′
s(0) = A(s)B′(0)A(s)−1 = A(s)YA(s)−1
is a smooth function of s, because A(s) is. So we have a whole smooth path
A(s)YA(s)−1 in T1(G), and hence its tangent (velocity vector) at s = 0 is
also in T1(G). (This is because the tangent is the limit of certain elements
of T1(G), and T1(G) is closed under limits.)
This tangent is found by differentiating D(s) = A(s)YA(s)−1 with re-
spect to s at s = 0 and using A(0) = 1:
D′(0) = A′(0)YA(0)−1 +A(0)Y(−A′(0))
= XY −YX = [X,Y],
since A′(0) = X and A(0) = 1. Thus X,Y ∈T1(G) implies [X,Y] ∈T1(G),
as claimed.
□
The tangent space of G, together with its vector space structure and
Lie bracket operation, is called the Lie algebra of G, and from now on we
denote it by g (the corresponding lower case Fraktur letter).

5.4
Algebraic properties of the tangent space
105
Deﬁnition. A matrix Lie algebra is a vector space of matrices that is closed
under the Lie bracket [X,Y] = XY −YX.
All the Lie algebras we have seen so far have been matrix Lie algebras,
and in fact there is a theorem (Ado’s theorem) saying that every Lie algebra
is isomorphic to a matrix Lie algebra. Thus it is not wrong to say simply
“Lie algebra” rather than “matrix Lie algebra,” and we will usually do so.
Perhaps the most important idea in Lie theory is to study Lie groups
by looking at their Lie algebras. This idea succeeds because vector spaces
are generally easier to work with than curved objects—which Lie groups
usually are—and the Lie bracket captures most of the group structure.
However, it should be emphasized at the outset that g does not always
capture G entirely, because different Lie groups can have the same Lie
algebra. We have already seen one class of examples. For all n, O(n) is
different from SO(n), but they have the same tangent space at 1 and hence
the same Lie algebra. There is a simple geometric reason for this: SO(n)
is the subgroup of O(n) whose members are connected by paths to 1. The
tangent space to O(n) at 1 is therefore the tangent space to SO(n) at 1.
Exercises
If, instead of considering the path Cs(t) = A(s)B(t)A(s)−1 in G we consider the
path
Ds(t) = A(s)B(t)A(s)−1B(t)−1
for some ﬁxed value of s,
then we can relate the Lie bracket [X,Y] of X,Y ∈T1(G) to the so-called commu-
tator A(s)B(t)A(s)−1B(t)−1 of smooth paths A(s) and B(t) through 1 in G.
5.4.1 Find D′
s(t), and hence show that D′
s(0) = A(s)YA(s)−1 −Y.
5.4.2 D′
s(0) ∈T1(G) (why?) and hence, as s varies, we have a smooth path E(s) =
D′
s(0) in T1(G) (why?).
5.4.3 Show that the velocity E′(0) equals XY −YX, and explain why E′(0) is in
T1(G).
The tangent space at 1 is the most natural one to consider, but in fact all
elements of G have the “same” tangent space.
5.4.4 Show that the smooth paths through any g ∈G are of the form gA(t), where
A(t) is a smooth path through 1.
5.4.5 Deduce from Exercise 5.4.4 that the space of tangents to G at g is isomor-
phic to the space of tangents to G at 1.

106
5
The tangent space
5.5
Dimension of Lie algebras
Since the tangent space of a Lie group is a vector space over R, it has a
well-deﬁned dimension over R. We can easily compute the dimension of
so(n),u(n),su(n), and sp(n) by counting the number of independent real
parameters in the corresponding matrices.
Dimension of so(n),u(n),su(n), and sp(n). As vector spaces over R,
(a) so(n) has dimension n(n−1)/2.
(b) u(n) has dimension n2.
(c) su(n) has dimension n2 −1.
(d) sp(n) has dimension n(2n+1).
Proof. (a) We know from Section 5.2 that so(n) consists of all n× n real
skew-symmetric matrices X. Thus the diagonal entries are zero, and the
entries below the diagonal are the negatives of those above. It follows that
the dimension of so(n) is the number of entries above the diagonal, namely
1+2+···+(n−1) = n(n−1)
2
.
(b) We know from Section 5.3 that u(n) consists of all n× n complex
skew-Hermitian matrices X. Thus X has n(n−1)/2 complex entries above
the diagonal and n pure imaginary entries on the diagonal, so the number
of independent real parameters in X is
n(n−1)+n = n2.
(c) We know from Section 5.3 that su(n) consists of all n×n complex
skew-Hermitian matrices with Tr(X) = 0. Without the Tr(X) = 0 condi-
tion, there are n2 real parameters, as we have just seen in (b). The condition
Tr(X) = 0 says that the nth diagonal entry is the negative of the sum of the
remaining diagonal entries, so the number of independent real parameters
is n2 −1.
(d) We know from Section 5.3 that sp(n) consists of all n× n quater-
nion skew-Hermitian matrices X. Thus X has n(n−1)/2 quaternion entries
above the diagonal and n pure imaginary quaternion entries on the diago-
nal, so the number of independent real parameters is
2n(n−1)+3n = n(2n−2+3) = n(2n+1).
□

5.6
Complexiﬁcation
107
It seems geometrically natural that a matrix group G should have the
same dimension as its tangent space T1(G) at the identity, but to put this
result on a ﬁrm basis we need to construct a bijection between a neigh-
borhood of 1 in G and a neighborhood of 0 in T1(G), continuous in both
directions—a homeomorphism. This can be achieved by a deeper study of
the exponential function, which we carry out in Chapter 7 (for other pur-
poses). But then one faces the even more difﬁcult problem of proving the
invariance of dimension under homeomorphisms. Fortunately, Lie theory
has another way out, which is simply to deﬁne the dimension of a Lie group
to be the dimension of its Lie algebra.
Exercises
The extra dimension that U(n) has over SU(n) is reﬂected in the fact that the quo-
tient group U(n)/SU(n) exists and is isomorphic to the circle group S1. Among
other things, this shows that U(n) is not a simple group. Here is how to show that
the quotient exists.
5.5.1 Consider the determinant map det : U(n) →C. Why is this a homomor-
phism? What is its kernel?
5.5.2 Deduce from Exercise 5.5.1 that SU(n) is a normal subgroup of U(n).
Since the dimension of U(n) is 1 greater than the dimension of SU(n), we
expect the dimension of U(n)/SU(n) to be 1. The elements of U(n)/SU(n) cor-
respond to the values of det(A), for matrices A ∈U(n), by the homomorphism
theorem of Section 2.2. So these values should form a 1-dimensional group—
isomorphic to either R or S1. Indeed, they are points on the unit circle in C, as the
following exercises show.
5.5.3 If A is an n ×n complex matrix such that AAT = 1, show that |det(A)| = 1.
5.5.4 Give an example of a diagonal unitary matrix A, with det(A) = eiθ.
5.6
Complexiﬁcation
The Lie algebras we have constructed so far have been vector spaces over
R, even though their elements may be matrices with complex or quaternion
entries. Each element is an initial velocity vector A′(0) of a smooth path
A(t), which is a function of the real variable t. It follows that, along with
each velocity vector A′(0), we have its real multiples rA′(0) for each r ∈R,
because they are the initial velocity vectors of the paths A(rt). Thus the
elements A′(0) of the Lie algebra admit multiplication by all real numbers

108
5
The tangent space
but not necessarily by all complex numbers. One can easily give examples
(Exercise 5.6.1) in which a complex matrix A is in a certain Lie algebra but
iA is not.
However, it is certainly possible for a Lie algebra to be a vector space
over C. Indeed, any real matrix Lie algebra g over R has a complexiﬁcation
g+ig = {A+iB : A,B ∈g}
that is a vector space over C. It is clear that g + ig is closed under sums,
because g is, and it is closed under multiples by complex numbers because
(a+ib)(A+iB) = aA−bB+i(bA+aB)
and aA−bB,bA+aB ∈g for any real numbers a and b.
Also, g+ig is closed under the Lie bracket because
[A1 +iB1,A2 +iB2] = [A1,A2]−[B1,B2]+i([B1,A2]+[A1,B2])
by bilinearity, and [A1,A2],[B1,B2],[B1,A2],[A1,B2] ∈g by the closure of g
under the Lie bracket. Thus g+ig is a Lie algebra.
Complexifying the Lie algebras u(n) and su(n), which are not vector
spaces over C, gives Lie algebras that happen to be tangent spaces—of the
general linear group GL(n,C) and the special linear group SL(n,C).
GL(n,C) and its Lie algebra gl(n,C)
The group GL(n,C) consists of all n×n invertible complex matrices A. It
is clear that the initial velocity A′(0) of any smooth path A(t) in GL(n,C) is
itself an n×n complex matrix. Thus the tangent space gl(n,C) of GL(n,C)
is contained in the space Mn(C) of all n×n complex matrices.
In fact, gl(n,C) = Mn(C). We ﬁrst observe that exp maps Mn(C) into
GL(n,C) because, for any X ∈Mn(C) we have
• eX is an n×n complex matrix.
• eX is invertible, because it has e−X as its inverse.
It follows, since tX ∈Mn(C) for any X ∈Mn(C) and any real t, that etX is
a smooth path in GL(n,C). Then X is the tangent vector to this path at 1,
and hence the tangent space gl(n,C) equals Mn(C), as claimed.

5.6
Complexiﬁcation
109
Now we show why gl(n,C) is the complexiﬁcation of u(n):
gl(n,C) = Mn(C) = u(n)+iu(n).
It is clear that any member of u(n) + iu(n) is in Mn(C). So it remains to
show that any X ∈Mn(C) can be written in the form
X = X1 +iX2
where
X1,X2 ∈u(n),
(*)
that is, where X1 and X2 are skew-Hermitian. There is a surprisingly simple
way to do this:
X = X −XT
2
+iX +XT
2i
.
We leave it as an exercise to check that X1 = X−XT
2
and X2 = X+XT
2i
satisfy
X1 +X1
T = 0 = X2 +X2
T, which completes the proof.
As a matter of fact, for each X ∈gl(N,C) the equation (*) has a unique
solution with X1,X2 ∈u(n). One solves (*) by ﬁrst taking the conjugate
transpose of both sides, then forming
X +XT = X1 +X1
T +i(X2 −X2
T)
= i(X2 −X2
T)
because X1 +X1
T = 0
= 2iX2
because X2 +X2
T = 0.
X −XT = X1 −X1
T +i(X2 +X2
T)
= X1 −X1
T
because X2 +X2
T = 0
= 2X1
because X1 +X1
T = 0.
Thus X1 = X−XT
2
and X2 = X+XT
2i
are in fact the only values X1,X2 ∈u(n)
that satisfy (*).
SL(n,C) and its Lie algebra sl(n,C)
The group SL(n,C) is the subgroup of GL(n,C) consisting of the n × n
complex matrices A with det(A) = 1. The tangent vectors of SL(n,C) are
among the tangent vectors X of GL(n,C), but they satisfy the additional
condition Tr(X) = 0. This is because eX ∈GL(n,C) and
det(eX) = eTr(X) = 1 ⇔Tr(X) = 0.

110
5
The tangent space
Conversely, if X has trace zero, then so has tX for any real t, so a
matrix X with trace zero gives a smooth path etX in SL(n,C). This path
has tangent X at 1, so
sl(n,C) = {X ∈Mn(C) : Tr(X) = 0}.
We now show that the latter set of matrices is the complexiﬁcation of
su(n), su(n)+isu(n). Since any X ∈su(n) has trace zero, any member of
su(n) + isu(n) also has trace zero. Conversely, any X ∈Mn(C) with trace
zero can be written as
X = X1 +iX2,
where
X1,X2 ∈su(n).
We use the same trick as for u(n)+iu(n); namely, write
X = X −XT
2
+iX +XT
2i
.
As before, X1 = X−XT
2
and X2 = X+XT
2i
are skew-Hermitian. But also, X1
and X2 have trace zero, because X has.
Thus, sl(N,C) = {X ∈Mn(C) : Tr(X) = 0} = su(n) + isu(n), as
claimed.
Also, by an argument like that used above for gl(n,C), each X∈sl(n,C)
corresponds to a unique ordered pair X1, X2 of elements of su(n) such that
X = X1 +iX2.
This equation therefore gives a 1-to-1 correspondence between the ele-
ments X of sl(n,C) and the ordered pairs (X1,X2) such that X1,X2 ∈su(n).
Exercises
5.6.1 Show that u(n) and su(n) are not vector spaces over C.
5.6.2 Check that X1 = X−XT
2
and X2 = X+XT
2i
are skew-Hermitian, and that X1 and
X2 have trace zero when X has.
5.6.3 Show that the groups GL(n,C) and SL(n,C) are unbounded (noncompact)
when the matrix with (j,k)-entry (a jk +ib jk) is identiﬁed with the point
(a11,b11,a12,b12,...,a1n,b1n,...,ann,bnn) ∈R2n2
and distance between matrices is the usual distance between points in R2n2.

5.7
Quaternion Lie algebras
111
The following exercises show that the matrix A =
 −1 1
0 −1

in SL(2,C) is not
equal to eX for any X ∈sl(2,C), the 2×2 matrices with trace zero. Thus exp does
not map the tangent space onto the group in this case. The idea is to calculate eX
explicitly with the help of the Cayley–Hamilton theorem, which for 2×2 matrices
X says that
X2 −(Tr(X))X +det(X)1 = 0.
Therefore, when Tr(X) = 0 we have X2 = −det(X)1.
5.6.4 When X2 = −det(X)1, show that
eX = cos(

det(X))1 + sin(

det(X))

det(X)
X.
5.6.5 Using Exercise 5.6.4, and the fact that Tr(X) = 0, show that if
eX =

−1
1
0
−1

then cos(

det(X)) = −1, in which case sin(

det(X)) = 0, and there is a
contradiction.
5.6.6 It follows not only that exp does not map sl(2,C) onto SL(2,C) but also
that exp does not map gl(2,C) onto GL(2,C). Why?
This is not our ﬁrst example of a Lie algebra that is not mapped onto its
group by exp. We have already seen that exp cannot map o(n) onto O(n) because
o(n) is path-connected and O(n) is not. What makes the sl(n,C) and gl(n,C)
examples so interesting is that SL(n,C) and GL(n,C) are path-connected. We
gave some results on path-connectedness in Sections 3.2 and 3.8, and will give
more in Section 8.6, including a proof that GL(n,C) is path-connected.
5.6.7 Find maximal tori, and hence the centers, of GL(n,C) and SL(n,C).
5.6.8 Assuming path-connectedness, also ﬁnd their discrete normal subgroups.
5.7
Quaternion Lie algebras
Analogous to GL(n,C), there is the group GL(n,H) of all invertible n×n
quaternion matrices.
Its tangent vectors lie in the space Mn(H) of all
n × n quaternion matrices, and indeed each X ∈Mn(H) is a tangent vec-
tor, because the quaternion matrix etX has the inverse e−tX and hence lies
in GL(n,H). So, for each X ∈Mn(H) we have the smooth path etX in
GL(n,H) with tangent X.
Thus the Lie algebra gl(n,H) of GL(n,H) is precisely Mn(H).

112
5
The tangent space
However, there is no “sl(n,H)” of quaternion matrices of trace zero.
This set of matrices is closed under sums and scalar multiples but, because
of the noncommutative quaternion product, not under the Lie bracket. For
example, we have the following matrices of trace zero in M2(H):
X =
i
0
0
−i

,
Y =
j
0
0
−j

.
But their Lie bracket is
XY −YX =
k
0
0
k

−
−k
0
0
−k

= 2
k
0
0
k

,
which does not have trace zero.
The quaternion Lie algebra that interests us most is sp(n), the tangent
space of Sp(n). As we found in Section 5.3,
sp(n) = {X ∈Mn(H) : X +XT = 0},
where X denotes the result of replacing each entry of X by its quaternion
conjugate.
There is no neat relationship between sp(n) and gl(n,H) analogous
to the relationship between su(n) and sl(n,C). This can be seen by con-
sidering dimensions: gl(n,H) has dimension 4n2 over R, whereas sp(n)
has dimension 2n2 + n, as we saw in Section 5.5. Therefore, we cannot
decompose gl(n,H) into two subspaces that look like sp(n), because the
dimensions do not add up.
As a result, we need to analyze sp(n) from scratch, and it turns out to
be “simpler” than gl(n,H), in a sense we will explain in Section 6.6.
Exercises
5.7.1 Give three examples of subspaces of gl(n,H) closed under the Lie bracket.
5.7.2 What are the dimensions of your examples?
5.7.3 If your examples do not include one of real dimension 1, give such an ex-
ample.
5.7.4 Also, if you have not already done so, give an example g of dimension n
that is commutative. That is, [X,Y] = 0 for all X,Y ∈g.

5.8
Discussion
113
5.8
Discussion
The classical groups were given their name by Hermann Weyl in his 1939
book The Classical Groups. Weyl did not give a precise enumeration of the
groups he considered “classical,” but it seems plausible from the content of
his book that he meant the general and special linear groups, the orthogonal
groups, and the unitary and symplectic groups. Weyl brieﬂy mentioned
that the concept of orthogonal group can be extended to include the group
O(p,q) of transformations of Rp+q preserving the (not positive deﬁnite)
inner product deﬁned by
(u1,u2,...,up,u′
1,u′
2,...,u′
q)·(v1,v2,...,vp,v′
1,v′
2,...,v′
q)
= u1v1 +u2v2 +···+upvp −u′
1v′
1 −u′
2v′
2 −···−u′
qv′
q.
An important special case is the Lorentz group O(1,3), which deﬁnes
the geometry of Minkowski space—the “spacetime” of special relativity.
There are also “p,q generalizations” of the unitary and symplectic groups,
and today these groups are often considered “classical.” However, in this
book we apply the term “classical groups” only to the general and special
linear groups, and O(n), SO(n), U(n), SU(n), and Sp(n).
Weyl also introduced the term “Lie algebra” (in lectures at Princeton in
1934–35, at the suggestion of Nathan Jacobson) for the collection of what
Lie had called the “inﬁnitesimal elements of a continuous group.”
The Lie algebras of the classical groups were implicitly known by Lie.
However, the description of Lie algebras by matrices was taken up only
belatedly, alongside the late-dawning realization that linear algebra is a
fundamental part of mathematics. As we have seen, the serious study of
matrix Lie groups began with von Neumann [1929], and the ﬁrst examples
of nonmatrix Lie groups were not given until 1936. At about the same
time, I. D. Ado showed that linear algebra really is an adequate basis for
the theory of Lie algebras, in the sense that any Lie algebra can be viewed
as a vector space of matrices.
As late as 1946, Chevalley thought it worthwhile to point out why it is
convenient to view elements of matrix groups as exponentials of elements
in their Lie algebras:
The property of a matrix being orthogonal or unitary is deﬁned
by a system of nonlinear relationships between its coefﬁcients;
the exponential mapping gives a parametric representation of

114
5
The tangent space
the set of unitary (or orthogonal) matrices by matrices whose
coefﬁcients satisfy linear relations.
Chevalley [1946] is the ﬁrst book, as far as I know, to explicitly describe
the Lie algebras of orthogonal, unitary, and symplectic groups as the spaces
of skew-symmetric and skew-Hermitian matrices.
The idea of viewing the Lie algebra as the tangent space of the group
goes back a little further, though it did not spring into existence fully
grown. In von Neumann [1929], elements of the Lie algebra of a ma-
trix groups G are taken to be limits of sequences of matrices in G, and von
Neumann’s limits can indeed be viewed as tangents, though this fact is not
immediately obvious (see Section 7.3). The idea of deﬁning tangent vec-
tors to G via smooth paths in G seems to originate with Pontrjagin [1939],
p. 183. The full-blooded deﬁnition of Lie groups as smooth manifolds and
Lie algebras as their tangent spaces appears in Chevalley [1946].
In this book I do not wish to operate at the level of generality that
requires a deﬁnition of smooth manifolds. However, a few remarks are
in order, since the concept of smooth manifold includes some objects that
do not look “smooth” at ﬁrst sight. For example, a single point is smooth
and so is any ﬁnite set of points. This has the consequence that {1,−1}
is a smooth subgroup of SU(2), and also of SO(n) for any even n. The
reason is that a smooth group should have a tangent space at every point,
but nobody said the tangent space has to be big!
“Smoothness” of a k-dimensional group G should imply that G has a
tangent space isomorphic to Rk at 1 (and hence at any point), but this in-
cludes the possibility that the tangent space is R0 = {0}. We must therefore
accept groups as “smooth” if they have zero tangent space at 1, which is
the case for {1}, {1,−1}, and any other ﬁnite group. In fact, ﬁnite groups
are included in the deﬁnition of “matrix Lie group” stated in Section 1.1,
since they are closed under nonsingular limits.
Nevertheless, the presence of nontrivial groups with zero tangent space,
such as {1,−1}, complicates the search for simple groups. If a group G is
simple, then its tangent space g is a simple Lie algebra, in a sense that will
be deﬁned in the next chapter. Simple Lie algebras are generally easier to
recognize than simple Lie groups, so we ﬁnd the simple Lie algebras g ﬁrst
and then see what they tell us about the group G. A good idea—except that
g cannot “see” the ﬁnite subgroups of G, because they have zero tangent
space. Simplicity of g therefore does not rule out the possibility of ﬁnite
normal subgroups of G, because they are “invisible” to g. This is why we

5.8
Discussion
115
took the trouble to ﬁnd the centers of various groups in Chapter 3. It turns
out, as we will show in Chapter 7, that g can “see” all the normal subgroups
of G except those that lie in the center, so in ﬁnding the centers we have
already found all the normal subgroups.
The pioneers of Lie theory, such as Lie himself, were not troubled by
the subtle difference between simplicity of a Lie group and simplicity of its
Lie algebra. They viewed Lie groups only locally and took members of the
Lie algebra to be members of the Lie group anyway (the “inﬁnitesimal” el-
ements). For the pioneers, the problem was to ﬁnd the simple Lie algebras.
Lie himself found almost all of them, as Lie algebras of classical groups.
But ﬁnding the remaining simple Lie algebras—the so-called exceptional
Lie algebras—was a monumentally difﬁcult problem. Its solution by Wil-
helm Killing around 1890, with corrections by ´Elie Cartan in 1894, is now
viewed as one of the greatest achievements in the history of mathematics.
Since the 1920s and 1930s, when Lie groups came to be viewed as
global objects and Lie algebras as their tangent spaces at 1, the question of
what to say about simple Lie groups has generally been ignored or fudged.
Some authors avoid saying anything by deﬁning a simple Lie group to be
one whose Lie algebra is simple, often without pointing out that this con-
ﬂicts with the standard deﬁnition of simple group. Others (such as Bour-
baki [1972]) deﬁne a Lie group to be almost simple if its Lie algebra is
simple, which is another way to avoid saying anything about the genuinely
simple Lie groups.
The ﬁrst paper to study the global properties of Lie groups was Schreier
[1925]. This paper was overlooked for several years, but it turned out to
be extremely prescient. Schreier accurately identiﬁed both the general role
of topology in Lie theory, and the special role of the center of a Lie group.
Thus there is a long-standing precedent for studying Lie group structure as
a topological reﬁnement of Lie algebra structure, and we will take up some
of Schreier’s ideas in Chapters 8 and 9.

6
Structure of Lie algebras
PREVIEW
In this chapter we return to our original motive for studying Lie algebras:
to understand the structure of Lie groups. We saw in Chapter 2 how normal
subgroups help to reveal the structure of the groups SO(3) and SO(4). To
go further, we need to know exactly how the normal subgroups of a Lie
group G are reﬂected in the structure of its Lie algebra g.
The focus of attention shifts from groups to algebras with the following
discovery. The tangent map from a Lie group G to its Lie algebra g sends
normal subgroups of G to substructures of g called ideals. Thus the ideals
of g “detect” normal subgroups of G in the sense that a nontrivial ideal of
g implies a nontrivial normal subgroup of G.
Lie algebras with no nontrivial ideals, like groups with no nontrivial
normal subgroups, are called simple. It is not quite true that simplicity of
g implies simplicity of G, but it turns out to be easier to recognize simple
Lie algebras, so we consider that problem ﬁrst.
We prove simplicity for the “generalized rotation” Lie algebras so(n)
for n > 4, su(n), sp(n), and also for the Lie algebra of the special linear
group of Cn. The proofs occupy quite a few pages, but they are all vari-
ations on the same elementary argument. It may help to skip the details
(which are only matrix computations) at ﬁrst reading.
116
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 6,
c⃝Springer Science+Business Media, LLC 2008

6.1
Normal subgroups and ideals
117
6.1
Normal subgroups and ideals
In Chapter 5 we found the tangent spaces of the classical Lie groups: the
classical Lie algebras. In this chapter we use the tangent spaces to ﬁnd
candidates for simplicity among the classical Lie groups G. We do so by
ﬁnding substructures of the tangent space g that are tangent spaces of the
normal subgroups of G. These are the ideals,4 deﬁned as follows.
Deﬁnition. An ideal h of a Lie algebra g is a subspace of g closed under
Lie brackets with arbitrary members of g. That is, if Y ∈h and X ∈g then
[X,Y] ∈h.
Then the relationship between normal subgroups and ideals is given by
the following theorem.
Tangent space of a normal subgroup. If H is a normal subgroup of a
matrix Lie group G, then T1(H) is an ideal of the Lie algebra T1(G).
Proof. T1(H) is a vector space, like any tangent space, and it is a subspace
of T1(G) because any tangent to H at 1 is a tangent to G at 1. Thus it
remains to show that T1(H) is closed under Lie brackets with members of
T1(G). To do this we use the property of a normal subgroup that B ∈H and
A ∈G implies ABA−1 ∈H.
It follows that A(s)B(t)A(s)−1 is a smooth path in H for any smooth
paths A(s) in G and B(t) in H. As usual, we suppose A(0) = 1 = B(0), so
A′(0) = X ∈T1(G) and B′(0) = Y ∈T1(H). If we let
Cs(t) = A(s)B(t)A(s)−1,
then it follows as in Section 5.4 that
D(s) = C′
s(0) = A(s)YA(s)−1
4This terminology comes from algebraic number theory, via ring theory. In the 1840s,
Kummer introduced some objects he called “ideal numbers” and “ideal primes” in order to
restore unique prime factorization in certain systems of algebraic numbers where ordinary
prime factorization is not unique. Kummer’s “ideal numbers” did not have a clear meaning
at ﬁrst, but in 1871 Dedekind gave them a concrete interpretation as certain sets of numbers
closed under sums, and closed under products with all numbers in the system. In the 1920s,
Emmy Noether carried the concept of ideal to general ring theory. Roughly speaking, a
ring is a set of objects with sum and product operations. The sum operation satisﬁes the
usual properties of sum (commutative, associative, etc.) but the product is required only
to “distribute” over sum: a(b+c) = ab+ac. A Lie algebra is a ring in this general sense
(with the Lie bracket as the “product” operation), so Lie algebra ideals are included in the
general concept of ideal.

118
6
Structure of Lie algebras
is a smooth path in T1(H). It likewise follows that
D′(0) = XY −YX ∈T1(H),
and hence T1(H) is an ideal, as claimed.
□
Remark. In Section 7.5 we will sharpen this theorem by showing that
T1(H) ̸= {0} provided H is not discrete, that is, provided there are points
in H not equal to 1 but arbitrarily close to it. Therefore, if g has no ideals
other than itself and {0}, then the only nontrivial normal subgroups of G
are discrete. We saw in Section 3.8 that any discrete normal subgroup of
a path-connected group G is contained in Z(G), the center of G. For the
generalized rotation groups G (which we found to be path-connected in
Chapter 3, and which are the main candidates for simplicity), we already
found Z(G) in Section 3.7. In each case Z(G) is ﬁnite, and hence discrete.
This remark shows that the Lie algebra g = T1(G) can “see” normal
subgroups of G that are not too small. T1(G) retains an image of a normal
subgroup H as an ideal T1(H), which is “visible” (T1(H) ̸= {0}) provided
H is not discrete. Thus, if we leave aside the issue of discrete normal
subgroups for the moment, the problem of ﬁnding simple matrix Lie groups
essentially reduces to ﬁnding the Lie algebras with no nontrivial ideals.
In analogy with the deﬁnition of simple group (Section 2.2), we deﬁne
a simple Lie algebra to be one with no ideals other than itself and {0}.
By the remarks above, we can make a big step toward ﬁnding simple Lie
groups by ﬁnding the simple Lie algebras among those for the classical
groups. We do this in the sections below, before returning to Lie groups to
resolve the remaining difﬁculties with discrete subgroups and centers.
Simplicity of so(3)
We know from Section 2.3 that SO(3) is a simple group, so we do not
really need to investigate whether so(3) is a simple Lie algebra. However,
it is easy to prove the simplicity of so(3) directly, and the proof is a model
for proofs we give for more complicated Lie algebras later in this chapter.
First, notice that the tangent space so(3) of SO(3) at 1 is the same as
the tangent space su(2) of SU(2) at 1. This is because elements of SO(3)
can be viewed as antipodal pairs ±q of quaternions q in SU(2). Tangents
to SU(2) are determined by the q near 1, in which case −q is not near 1,
so the tangents to SO(3) are the same as the tangents to SU(2).

6.1
Normal subgroups and ideals
119
Thus the Lie algebra so(3) equals su(2), which we know from Section
4.4 is the cross-product algebra on R3. (Another proof that so(3) is the
cross-product algebra on R3 is in Exercises 5.2.1–5.2.3.)
Simplicity of the cross-product algebra. The cross-product algebra is
simple.
Proof. It sufﬁces to show that any nonzero ideal equals R3 = Ri+Rj+Rk,
where i, j, and k are the usual basis vectors for R3.
Suppose that I is an ideal, with a nonzero member u = xi + yj + zk.
Suppose, for example, that x ̸= 0. By the deﬁnition of ideal, I is closed
under cross products with all elements of R3. In particular,
u×j = xk−zi ∈I,
and hence
(xk−zi)×i = xj ∈I.
Then x−1(xj) = j ∈I also, since I is a subspace. It follows, by taking cross
products with k and i, that i,k ∈I as well.
Thus I is a subspace of R3 that includes the basis vectors i, j, and k,
so I = R3. There is a similar argument if y ̸= 0 or z ̸= 0, and hence the
cross-product algebra on R3 is simple.
□
The algebraic argument above—nullifying all but one component of
a nonzero element to show that a nonzero ideal I includes all the basis
vectors—is the model for several simplicity proofs later in this chapter. The
later proofs look more complicated, because they involve Lie bracketing
of a nonzero matrix to nullify all but one basis element (which may be a
matrix with more than one nonzero entry). But they similarly show that a
nonzero ideal includes all basis elements, and hence is the whole algebra,
so the general idea is the same.
Exercises
Another way in which T1(G) may misrepresent G is when T1(H) = T1(G) but H
is not all of G.
6.1.1 Show that T1(O(n)) = T1(SO(n)) for each n, and that SO(n) is a normal
subgroup of O(n).
6.1.2 What are the cosets of SO(n) in O(n)?

120
6
Structure of Lie algebras
An example of a matrix Lie group with a nontrivial normal subgroup is U(n).
We determined the appropriate tangent spaces in Section 5.3.
6.1.3 Show that SU(n) is a normal subgroup of U(n) by describing it as the kernel
of a homomorphism.
6.1.4 Show that T1(SU(n)) is an ideal of T1(U(n)) by checking that it has the
required closure properties.
6.2
Ideals and homomorphisms
If we restrict attention to matrix Lie groups (as we generally do in this
book) then we cannot assume that every normal subgroup H of a Lie group
G is the kernel of a matrix group homomorphism G →G/H. The problem
is that the quotient G/H of matrix groups is not necessarily a matrix group.
This is why we derived the relationship between normal subgroups and
ideals without reference to homomorphisms.
Nevertheless, some important normal subgroups are kernels of matrix
Lie group homomorphisms. One such homomorphism is the determinant
map G →C×, where C× denotes the group of nonzero complex numbers
(or 1×1 nonzero complex matrices) under multiplication. Also, any ideal
is the kernel of a Lie algebra homomorphism—deﬁned to be a map of
Lie algebras that preserves sums, scalar multiples, and the Lie bracket—
because in fact any Lie algebra is isomorphic to a matrix Lie algebra.
An important Lie algebra homomorphism is the trace map,
Tr(A) = sum of diagonal elements of A,
for real or complex matrices A. We verify that Tr is a Lie algebra homo-
morphism in the next section.
The general theorem about kernels is the following.
Kernel of a Lie algebra homomorphism. If ϕ : g →g′ is a Lie algebra
homomorphism, and
h = {X ∈g : ϕ(X) = 0}
is its kernel, then h is an ideal of g.

6.2
Ideals and homomorphisms
121
Proof. Since ϕ preserves sums and scalar multiples, h is a subspace:
X1,X2 ∈h ⇒ϕ(X1) = 0,ϕ(X2) = 0
⇒ϕ(X1 +X2) = 0
because ϕ preserves sums
⇒X1 +X2 ∈h,
X ∈h ⇒ϕ(X) = 0
⇒cϕ(X) = 0
⇒ϕ(cX) = 0
because ϕ preserves scalar multiples
⇒cX ∈h.
Also, h is closed under Lie brackets with members of g because
X ∈h ⇒ϕ(X) = 0
⇒ϕ([X,Y]) = [ϕ(X),ϕ(Y)] = [0,ϕ(Y)] = 0
for any Y ∈g because ϕ preserves Lie brackets
⇒[X,Y] ∈h
for any Y ∈g.
Thus h is an ideal, as claimed.
□
It follows from this theorem that a Lie algebra is not simple if it admits
a nontrivial homomorphism. This points to the existence of non-simple Lie
algebras, which we should look at ﬁrst, if only to know what to avoid when
we search for simple Lie algebras.
Exercises
There is a sense in which any homomorphism of a Lie group G “induces” a homo-
morphism of the Lie algebra T1(G). We study this relationship in some depth in
Chapter 9. Here we explore the special case of the det homomorphism, assuming
also that G is a group for which exp maps T1(G) onto G.
6.2.1 If we map each X ∈T1(G) to Tr(X), where does the corresponding member
eX of G go?
6.2.2 If we map each eX ∈G to det(eX), where does the corresponding X ∈T1(G)
go?
6.2.3 In particular, why is there a well-deﬁned image of X when eX = eX′?

122
6
Structure of Lie algebras
6.3
Classical non-simple Lie algebras
We know from Section 2.7 that SO(4) is not a simple group, so we expect
that so(4) is not a simple Lie algebra. We also know, from Section 5.6,
about the groups GL(n,C) and their subgroups SL(n,C). The subgroup
SL(n,C) is normal in GL(n,C) because it is the kernel of the homomor-
phism
det : GL(n,C) →C×.
It follows that GL(n,C) is not a simple group for any n, so we expect that
gl(n,C) is not a simple Lie algebra for any n. We now prove that these Lie
algebras are not simple by ﬁnding suitable ideals.
An ideal in gl(n,C)
We know from Section 5.6 that gl(n,C) = Mn(C) (the space of all n × n
complex matrices), and sl(n,C) is the subspace of all matrices in Mn(C)
with trace zero. This subspace is an ideal, because it is the kernel of a Lie
algebra homomorphism.
Consider the trace map
Tr : Mn(C) →C.
The kernel of this map is certainly sl(n,C), but we have to check that this
map is a Lie algebra homomorphism. It is a vector space homomorphism
because
Tr(X +Y) = Tr(X)+Tr(Y)
and
Tr(zX) = zTr(X)
for any z ∈C,
as is clear from the deﬁnition of trace.
Also, if we view C as the Lie algebra with trivial Lie bracket [u,v] =
uv−vu = 0, then Tr preserves the Lie bracket. This is due to the (slightly
less obvious) property that Tr(XY) = Tr(YX), which can be checked by
computing both sides (see Exercise 5.3.8). Assuming this property of Tr,
we have
Tr([X,Y]) = Tr(XY −YX)
= Tr(XY)−Tr(YX)
= 0
= [Tr(X),Tr(Y)].
Thus Tr is a Lie bracket homomorphism and its kernel, sl(n,C), is neces-
sarily an ideal of Mn(C) = gl(n,C).

6.3
Classical non-simple Lie algebras
123
An ideal in so(4)
In Sections 2.5 and 2.7 we saw that every rotation of H = R4 is a map of
the form q →v−1qw, where u,v ∈Sp(1) (the group of unit quaternions,
also known as SU(2)). In Section 2.7 we showed that the map
Φ : Sp(1)×Sp(1) →SO(4)
that sends (v,w) to the rotation q →v−1qw is a 2-to-1 homomorphism onto
SO(4). This is a Lie group homomorphism, so by Section 6.1 we expect it
to induce a Lie algebra homomorphism onto so(4),
ϕ : sp(1)×sp(1) →so(4),
because sp(1) × sp(1) is surely the Lie algebra of Sp(1)× Sp(1). Indeed,
any smooth path in Sp(1)×Sp(1) has the form u(t) = (v(t),w(t)), so
u′(0) = (v′(0),w′(0)) ∈sp(1)×sp(1).
And as (v(t),w(t)) runs through all pairs of smooth paths in Sp(1)×Sp(1),
(v′(0),w′(0)) runs through all pairs of velocity vectors in sp(1)×sp(1).
Moreover, the homomorphism ϕ is 1-to-1. Of the two pairs (v(t),w(t))
and (−v(t),−w(t)) that map to the same rotation q →v(t)−1qw(t), exactly
one goes through the identity 1 when t = 0 (the other goes through −1).
Therefore, the two pairs between them yield only one velocity vector in
sp(1) × sp(1), either (v′(0),w′(0)) or (−v′(0),−w′(0)). Thus ϕ is in fact
an isomorphism of sp(1)× sp(1) onto so(4). (For a matrix description of
this isomorphism, see Exercise 6.5.4.)
But sp(1)×sp(1) has a homomorphism with nontrivial kernel, namely,
(v′(0),w′(0)) →(0,w′(0)),
with kernel
sp(1)×{0}.
The subspace sp(1) × {0} is therefore a nontrivial ideal of so(4). Since
sp(1) is isomorphic to so(3), and so(3)×{0} is isomorphic to so(3), this
ideal can be viewed as an so(3) inside so(4).
Exercises
A more concrete proof that sl(n,C) is an ideal of gl(n,C) can be given by checking
that the matrices in sl(n,C) are closed under Lie bracketing with any member of
gl(n,C). In fact, the Lie bracket of any two elements of gl(n,C) lies in sl(n,C),
as the following exercises show.

124
6
Structure of Lie algebras
We let
X =
⎛
⎜
⎜
⎜
⎝
x11
x12
...
x1n
x21
x22
...
x2n
...
...
xn1
xn2
...
xnn
⎞
⎟
⎟
⎟
⎠
be any element of gl(n,C), and consider its Lie bracket with eij, the matrix with
1 as its (i, j)-entry and zeros elsewhere.
6.3.1 Describe Xeij and eijX. Hence show that the trace of [X,eij] is xji −xji = 0.
6.3.2 Deduce from Exercise 6.3.1 that Tr([X,Y]) = 0 for any X,Y ∈gl(n,C).
6.3.3 Deduce from Exercise 6.3.2 that sl(n,C) is an ideal of gl(n,C).
Another example of a non-simple Lie algebra is u(n), the algebra of n × n
skew-hermitian matrices.
6.3.4 Find a 1-dimensional ideal I in u(n), and show that I is the tangent space
of Z(U(n)).
6.3.5 Also show that the Z(U(n)) is the image, under the exponential map, of the
ideal I in Exercise 6.3.4.
6.4
Simplicity of sl(n,C) and su(n)
We saw in Section 5.6 that sl(n,C) consists of all n× n complex matrices
with trace zero. This set of matrices is a vector space over C, and it has
a natural basis consisting of the matrices eij for i ̸= j and eii −enn for
i = 1,2,...,n−1, where eij is the matrix with 1 as its (i, j)-entry and zeros
elsewhere. These matrices span sl(n,C). In fact, for any X ∈sl(n,C),
X = (xij) = ∑
i̸= j
xijeij +
n−1
∑
i=1
xii(eii −enn)
because xnn = −x11−x22−···−xn−1,n−1 for the trace of X to be zero. Also,
X is the zero matrix only if all the coefﬁcients are zero, so the matrices eij
for i ̸= j and eii −enn for i = 1,2,...,n−1 are linearly independent.
These basis elements are convenient for Lie algebra calculations be-
cause the Lie bracket of any X with an eij has few nonzero entries. This
enables us to take any nonzero member of an ideal I and manipulate it
to ﬁnd a nonzero multiple of each basis element in I, thus showing that
sl(n,C) contains no nontrivial ideals.

6.4
Simplicity of sl(n,C) and su(n)
125
Simplicity of sl(n,C). For each n, sl(n,C) is a simple Lie algebra.
Proof. If X = (xij) is any n × n matrix, then Xeij has all columns zero
except the jth, which is occupied by the ith column of X, and −eijX has
all rows zero except the ith, which is occupied by −(row j) of X.
Therefore, since [X,eij] = Xeij −eijX, we have
column j of [X,eij] =
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x1i
...
xi−1,i
xii −xj j
xi+1,i
...
xni
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
and
row i of [X,eij] =

−xj1
...
−xj,j−1
xii −xj j
−xj,j+1
...
−xjn

,
and all other entries of [X,eij] are zero. In the (i, j)-position, where the
shifted row and column cross, we get the element xii −xj j.
We now use such bracketing to show that an ideal I with a nonzero
member X includes all the basis elements of sl(n,C), so I = sl(n,C).
Case (i): X has nonzero entry xji for some i ̸= j.
Multiply [X,eij] by eij on the right. This destroys all columns except
the ith, whose only nonzero element is −xji in the (i,i)-position, moving it
to the (i, j)-position (because column i is moved to column j position).
Now multiply [X,eij] by −eij on the left. This destroys all rows except
the jth, whose only nonzero element is xji at the (j, j)-position, moving it
to the (i, j)-position and changing its sign (because row j is moved to row
i position, with a sign change).
It follows that [X,eij]eij −eij[X,eij] = [[X,eij],eij] contains the nonzero
element −2xji at the (i, j)-position, and zeros elsewhere.
Thus the ideal I containing X also contains eij. By further bracket-
ing we can show that all the basis elements of sl(n,C) are in I.
For
a start, if eij ∈I then eji ∈I, because the calculation above shows that
[[eij,eji],eji] = −2eji. The other basis elements can be obtained by using
the result
[eij,ejk] =
 eik
if i ̸= k,
eii −ej j
if i = k,

126
6
Structure of Lie algebras
which can be checked by matrix multiplication (Exercise 6.4.1).
For example, suppose we have e12 and we want to get e43. This is
achieved by the following pair of bracketings, from right and left:
[e12,e23] = e13,
[e41,e13] = e43.
All ekl with k ̸= l are obtained similarly. Once we have all of these, we
obtain the remaining basis elements of sl(n,C) by
[ein,eni] = eii −enn.
Case (ii). All the nonzero entries of X are among x11,x22,...,xnn.
Not all these elements are equal (otherwise, Tr(X) ̸= 0), so we can
choose i and j such that xii −xj j ̸= 0. Now, for this X, the calculation of
[X,eij] gives
[X,eij] = (xii −xj j)eij.
Thus I includes a nonzero multiple of eij, and hence eij itself. We can
now repeat the rest of the argument in case (i) to conclude again that I =
sl(n,C), so sl(n,C) is simple.
□
An easy corollary of this result is the following:
Simplicity of su(n). For each n, su(n) is a simple Lie algebra.
Proof. We use the result from Section 5.6, that
sl(n,C) = su(n)+isu(n) = {A+iB : A,B ∈su(n)}.
It follows that if I is a nontrivial ideal of su(n) then
I+iI = {C +iD : C,D ∈I}
is a nontrivial ideal of sl(n,C). One only has to check that
1. I+iI is not all of sl(n,C), which is true because of the 1-to-1 corre-
spondence X = X1 +iX2 between elements X of sl(n,C) and ordered
pairs (X1,X2) such that X1,X2 ∈su(n).
If I+iI includes each X ∈sl(n,C) then I includes each Xj ∈su(n),
contrary to the assumption that I is not all of su(n).

6.5
Simplicity of so(n) for n > 4
127
2. I+iI is a vector subspace (over C) of sl(n,C). Closure under sums
is obvious. And the scalar multiple (a+ ib)(C + iD) of any C + iD
in I+iI is also in I+iI for any a+ib ∈C because
(a+ib)(C +iD) = (aC −bD)+i(bC +aD)
and aC −bD,bC +aD ∈I by the vector space properties of I.
3. I + iI is closed under the Lie bracket with any A + iB ∈sl(n,C).
This is because, if C +iD ∈I+iI, then
[C +iD,A+iB] = [C,A]−[D,B]+i([D,A]+[C,B]) ∈I+iI
by the closure properties of I.
Thus a nontrivial ideal I of su(n) gives a nontrivial ideal of sl(n,C). There-
fore I does not exist.
□
Exercises
6.4.1 Verify that
[eij,ejk] =

eik
if i ̸= k,
eii −ej j
if i = k.
6.4.2 More generally, verify that [eij,ekl] = δjkeil −δliek j.
In Section 6.6 we will be using multiples of the basis vectors emm by the quaternion
units i, j, and k. Here is a taste of the kind of result we require.
6.4.3 Show that [i(epp −eqq),j(epp −eqq)] = 2k(epp +eqq).
6.4.4 Show that an ideal of quaternion matrices that includes iemm also includes
jemm and kemm.
6.5
Simplicity of so(n) for n > 4
The Lie algebra so(n) of real n × n skew-symmetric matrices has a basis
consisting of the n(n−1) matrices
Eij = eij −eji
for
i < j.
Indeed, since Eij has 1 in the (i, j)-position and −1 in the ( j,i)-position,
any skew symmetric matrix is uniquely expressible in the form
X = ∑
i< j
xijEij.

128
6
Structure of Lie algebras
Our strategy for proving that so(n) is simple is like that used in Section 6.4
to prove that sl(n,C) is simple. It involves two stages:
• First we suppose that X is a nonzero member of some ideal I and
take Lie brackets of X with suitable basis vectors until we obtain a
nonzero multiple of some basis vector in I.
• Then, by further Lie bracketing, we show that all basis vectors are
in fact in I, so I = so(n).
The ﬁrst stage, as with sl(n,C), selectively nulliﬁes rows and columns until
only a nonzero multiple of a basis vector remains. It is a little trickier to
do this for so(n), because multiplying by Eij leaves intact two columns
(or rows, if one multiplies on the left), rather than one. To nullify all but
two, symmetrically positioned, entries we need n > 4, which is no surprise
because so(4) is not simple.
In the ﬁrst stage we need to keep track of matrix entries as columns
and rows change position, so we introduce a notation that provides number
labels to the left of rows and above columns. For example, we write
Eij =
i
j
i
j
⎛
⎜
⎜
⎜
⎜
⎝
1
−1
⎞
⎟
⎟
⎟
⎟
⎠
to indicate that Eij has 1 in the (i, j)-position, −1 in the (j,i)-position, and
zeros elsewhere.
Now suppose X is the n×n matrix with (i, j)-entry xij. Multiplying X
on the right by Eij and on the left by −Eij, we ﬁnd that
XEij =
i
j
⎛
⎜
⎜
⎜
⎝
−x1j
x1i
−x2j
x2i
...
...
−xnj
xni
⎞
⎟
⎟
⎟
⎠

6.5
Simplicity of so(n) for n > 4
129
and
−EijX = i
j
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−xj1
−xj2
···
−xjn
xi1
xi2
···
xin
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Thus, right multiplication by Eij preserves only column i, which goes to
position j, and column j, which goes to position i with its sign changed.
Left multiplication by −Eij preserves row i, which goes to position j, and
row j, which goes to position i with its sign changed.
The Lie bracket of X with Eij is the sum of XEij and −EijX, namely
[X,Eij] =
i
j
i
j
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−x1j
x1i
−x2j
x2i
...
...
−xj1
−xj2
···
−xji −xij
···
−xj j +xii
···
−xjn
...
...
xi1
xi2
···
xii −xj j
···
xij +xji
···
xin
...
...
−xnj
xni
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Note that the (i, j)- and ( j,i)-entries are zero when X ∈so(n) because xii =
xj j = 0 in a skew-symmetric matrix. Likewise, the (i,i)- and (j, j)-entries
are zero for a skew-symmetric X, so for X ∈so(n) we have the simpler
formula (*) below. In short, the rule for bracketing a skew-symmetric X
with Eij is:
• Exchange rows i and j, giving the new row i a minus sign.
• Exchange columns i and j, giving the new column i a minus sign.
• Put 0 where the new rows and columns meet and 0 everywhere else.

130
6
Structure of Lie algebras
[X,Eij] =
i
j
i
j
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−x1j
x1i
−x2j
x2i
...
...
−xj1
−xj2
···
0
···
0
···
−xjn
...
...
xi1
xi2
···
0
···
0
···
xin
...
...
−xnj
xni
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
(*)
We now make a series of applications of formula (*) for [X,Eij] to
reduce a given nonzero X ∈so(n) to a nonzero multiple of a basis vector.
The result is the following theorem.
Simplicity of so(n). For each n > 4, so(n) is a simple Lie algebra.
Proof. Suppose that I is a nonzero ideal of so(n), and that X is a nonzero
n×n matrix in I. We will show that I contains all the basis vectors Eij, so
I = so(n).
In the ﬁrst stage of the proof, we Lie bracket X with a series of four
basis elements to produce a matrix (necessarily skew-symmetric) with just
two nonzero entries. The ﬁrst bracketing produces the matrix X1 = [X,Eij]
shown in (*) above, which has zeros everywhere except in columns i and j
and rows i and j.
For the second bracketing we choose a k ̸= i, j and form X2 = [X1,E jk],
which has row and column j of X1 moved to the k position, row and column
k of X1 moved to the j position with their signs changed, and zeros where
these rows and columns meet. Row and column k in X1 = [X,Eij] have
at most two nonzero entries (where they meet row and column i and j),
so row and column j in X2 = [X1,E jk] each have at most one, since the

6.5
Simplicity of so(n) for n > 4
131
(j, j)-entry −xik −xki is necessarily zero. The result is that
[X1,E jk] =
i
j
k
i
j
k
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x1i
x2i
...
xjk
0
...
xk j
0
0
...
xi1
xi2
···
0
···
0
···
0
···
xin
...
xni
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
Now choose l ̸= i, j,k and bracket X2 = [X1,E jk] with Eil. The only
nonzero elements in row and column l of X2 are xli at position (l,k) in row
l and xil at position (k,l) in column k. Therefore, X3 = [X2,Eil] is given by
[X2,Eil] =
i
j
k
l
i
j
k
l
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
−xli
xk j
−xil
xjk
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
.
To complete this stage we choose m ̸= i, j,k,l and bracket X3 = [X2,Eil]
with Elm. Since row and column m are zero, the result X4 = [X3,Elm] is the
matrix with xk j in the ( j,m)-position and xjk in the (m, j)-position; that is,
[X3,Elm] = xk jE jm.
Now we work backward. If X is a nonzero member of the ideal I, let xk j
be a nonzero entry of X. Provided n > 4, we can choose i ̸= j,k, then

132
6
Structure of Lie algebras
l ̸= i, j,k and m ̸= i, j,k,l, and construct the nonzero element xk jE jm of I
by a sequence of Lie brackets as above. Finally, we multiply by 1/xk j and
obtain E jm ∈I.
The second stage obtains all other basis elements of so(n) by forming
Lie brackets of E jm with other basis elements. This proceeds exactly as
for sl(n,C), because the Eij satisfy relations like those satisﬁed by the eij,
namely
[Eij,E jk] = Eik
if
i ̸= k,
[Eij,Eki] = E jk
if
j ̸= k.
Thus, when n > 4, any nonzero ideal of so(n) is equal to so(n), as
required.
□
The ﬁrst stage of the proof above may seem a little complicated, but
I doubt that it can be substantially simpliﬁed. If it were much simpler it
would be wrong! We need to use ﬁve different values i, j,k,l,m because
so(4) is not simple, so the result is false for a 4×4 matrix X.
Exercises
6.5.1 Prove that
[Eij,Ejk] = Eik
if
i ̸= k,
[Eij,Eki] = Ejk
if
j ̸= k.
6.5.2 Also show that [Eij,Ekl] = 0 if i, j,k,l are all different.
6.5.3 Use Exercises 6.5.1 and 6.5.2 to give another proof that [X3,Elm] = xk jEjm.
(Hint: Write X3 as a linear combination of Eik and Ejl.)
6.5.4 Prove that each 4×4 skew-symmetric matrix is uniquely decomposable as
a sum
⎛
⎜
⎜
⎝
0
−a
−b
−c
a
0
−c
b
b
c
0
−a
c
−b
a
0
⎞
⎟
⎟
⎠+
⎛
⎜
⎜
⎝
0
−x
−y
−z
x
0
z
−y
y
−z
0
x
z
y
−x
0
⎞
⎟
⎟
⎠.
6.5.5 Setting I = −E12 −E34, J = −E13 +E24, and K = −E14 −E23, show that
[I,J] = 2K, [J,K] = 2I, and [K,I] = 2J.
6.5.6 Deduce from Exercises 6.5.4 and 6.5.5 that so(4) is isomorphic to the direct
product so(3)×so(3) (also known as the direct sum and commonly written
so(3)⊕so(3)).

6.6
Simplicity of sp(n)
133
6.6
Simplicity of sp(n)
If X ∈sp(n) we have X + XT = 0, where X is the result of replacing each
entry in the matrix X by its quaternion conjugate. Thus, if X = (xij) and
xij = aij +biji+cijj+dijk,
then
xij = aij −biji−cijj−dijk
and hence
xji = −aij +biji+cijj+dijk,
where aij,bij,cij,dij ∈R. (And, of course, the quaternion units i, j, and
k are completely unrelated to the integers i, j used to number rows and
columns.) In particular, each diagonal entry xii of X is pure imaginary.
This gives the following obvious basis vectors for sp(n) as a vector
space over R. The matrices eii and Eij are as in Sections 6.4 and 6.5.
• For i = 1,2,...,n, the matrices ieii, jeii, and keii.
• For each pair (i, j) with i < j, the matrices Eij.
• For each pair (i, j) with i < j, the matrices i ˜Eij, j ˜Eij, and k ˜Eij, where
˜Eij is the matrix with 1 in the (i, j)-position, 1 in the (j,i)-position,
and zeros elsewhere.
To prove that sp(n) is simple we suppose that I is an ideal of sp(n) with
a nonzero element X = (xij). Then, as before, we reduce X to an arbitrary
basis element by a series of Lie bracketings and vector space operations.
Once we have found all the basis elements in I, we know that I = sp(n).
We have a more motley collection of basis elements than ever before, but
the job of ﬁnding them is made easier by the presence of the very simple
basis elements ieii, jeii, and keii.
In particular, I includes
[X,ieii] =
i
i
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎝
x1ii
...
−ixi1
···
xiii−ixii
···
−ixin
...
xnii
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎠
,
(*)

134
6
Structure of Lie algebras
and hence also, if i ̸= j,
[[X,ieii],iej j] =
i
j
i
j
⎛
⎜
⎜
⎜
⎜
⎝
−ixiji
−ixjii
⎞
⎟
⎟
⎟
⎟
⎠
,
where all entries are zero except those explicitly shown.
This gets the essential matrix calculations out of the way, and we are
ready to prove our theorem.
Simplicity of sp(n). For all n, sp(n) is a simple Lie algebra.
Proof. When n = 1, we have sp(1) = su(2), which we proved to be simple
in Section 6.4. Thus we can assume n ≥2, which allows us to use the
computations above.
Suppose that I is an ideal of sp(n), with a nonzero element X = (xij).
Case (i). All nonzero entries xij of X are on the diagonal.
In this case (*) gives the element of I
[X,ieii] = (xiii−ixii)eii,
and we can similarly obtain the further elements
[X,jeii] = (xiij−jxii)eii,
[X,keii] = (xiik−kxii)eii.
Now if xii = biii+ciij+diik we ﬁnd
xiii−ixii = −2ciik+2diij,
xiij−jxii = 2biik−2diii,
xiik−kxii = −2biij+2ciii.
So, by the closure of I under Lie brackets and real multiples, we have
(−ciik+diij)eii,
(biik−diii)eii,
(−biij+ciii)eii
in
I.
Lie bracketing these three elements with k1, i1, j1 respectively gives us
diiieii,
biijeii,
ciikeii
in
I.

6.6
Simplicity of sp(n)
135
Thus if xii is a nonzero entry in X we have at least one of the basis vectors
ieii, jeii, keii in I. Lie bracketing the basis vector in I with the other two
then gives us all three of ieii, jeii, keii in I. (Here, the facts that jk = −kj = i
and so on work in our favor.)
Until now, we have found ieii, jeii, keii in I only for one value of i. To
complete our collection of diagonal basis vectors we ﬁrst note that
[Eij,ieii] = i ˜Eij,
[Eij,jeii] = j ˜Eij,
[Eij,keii] = k ˜Eij,
(**)
as special cases of the formula (*). Thus we have
i ˜Eij,
j ˜Eij,
k ˜Eij,
in
I
for some i and arbitrary j ̸= i. Then we notice that
[i ˜Eij,j ˜Eij] = 2k(eii +ej j).
So k(eii + ej j) and keii are both in I, and hence their difference kej j is in
I, for any j. We then ﬁnd iej j in I by Lie bracketing jej j with kej j, and
jej j in I by Lie bracketing iej j with kej j.
Now that we have the diagonal basis vectors ieii, jeii, keii in I for all
i, we can reapply the formulas (**) to get the basis vectors i ˜Eij, j ˜Eij, and
k ˜Eij for all i and j with i < j. Finally, we get all the Eij in I by the formula
[i ˜Eij,ieii] = Eij,
which also follows from (*). Thus all the basis vectors of sp(n) are in I,
and hence I = sp(n).
Case (ii). X has a nonzero entry of the form xij = aij +biji+cijj+dijk,
for some i < j.
Our preliminary calculations show that the element [[X,ieii],iej j] of I
has zeros everywhere except for −ixiji in the (i, j)-position, and its nega-
tive conjugate −ixjii in the (j,i)-position. Explicitly, the (i, j)-entry is
−ixiji = aij +biji−cijj−dijk,
so we have
[[X,ieii],iej j] = aijEij +(biji−cijj−dijk) ˜Eij ∈I.

136
6
Structure of Lie algebras
If aij is the only nonzero coefﬁcient in [[X,ieii],iej j] we have Eij ∈I.
Then, writing Eij = eij −eji, ˜Eij = eij + eji, we ﬁnd from the formula
[eij,eji] = eii −ej j of Section 6.4 the following elements of I:
[Eij,i ˜Eij] = 2i(eii −ej j),
[Eij,j ˜Eij] = 2j(eii −ej j),
[Eij,k ˜Eij] = 2k(eii −ej j).
The ﬁrst two of these elements give us
[i(eii −ej j),j(eii −ej j)] = 2k(eii +ej j) ∈I
(Another big “thank you” to noncommutative quaternion multiplication!)
Adding the last two elements found, we ﬁnd keii ∈I, so I = sp(n) for the
same reasons as in Case (i).
Finally, if one of the coefﬁcients bij, cij, or dij is nonzero, we simplify
aijEij +(biji−cijj−dijk) ˜Eij by Lie bracketing with i1, j1, and k1. Since
[Eij,i1] = 0,
[i ˜Eij,i1] = 0,
[i ˜Eij,j1] = 2k ˜Eij,
and so on, we can nullify all terms in aijEij +(biji−cijj+dijk)˜Eij except
one with a nonzero coefﬁcient. This gives us, say, i ˜Eij ∈I. Then we apply
the formula
[i ˜Eij,ieii] = Eij,
which follows from (*), and we again have Eij ∈I, so we can reduce to
Case (i) as above.
□
Exercises
It was claimed in Section 5.7 that sp(n) is “simpler” than the Lie algebra gl(n,H)
of all n × n quaternion matrices. What was meant is that gl(n,H) is not a simple
Lie algebra—it contains two nontrivial ideals:
R = {X : X = r1 for some r ∈R}
of dimension 1,
T = {X : re(Tr(X)) = 0}
of dimension 4n2 −1,
where re denotes the real part of the quaternion.
6.6.1 Prove that R is an ideal of gl(n,H).
6.6.2 Prove that, for any two quaternions p and q, re(pq) = re(qp).

6.9
Discussion
137
6.6.3 Using Exercise 6.6.2 or otherwise, check that T is an ideal of the real Lie
algebra gl(n,H).
6.6.4 Show that each X ∈gl(n,H) has a unique decomposition of the form X =
R+T, where R ∈R and T ∈T.
It turns out that R and T are the only nontrivial ideals of gl(n,H). This can be
shown by taking the 4n2 basis vectors eij, ieij, jeij, keij for gl(n,H), and consid-
ering a nonzero ideal I.
6.6.5 If I has a member X with a nonzero entry xij, where i ̸= j, show that I
equals T or gl(n,H).
6.6.6 Show in general that I equals either R, T, or gl(n,H).
6.7
Discussion
As mentioned in Section 5.8, the classical simple Lie algebras were known
to Lie in the 1880s, the exceptional simple algebras were discovered by
Killing soon thereafter, and by 1894 Cartan had completely settled the
question by an exhaustive proof that they are the only exceptions. The
number of exceptional algebras, in complex form, is just ﬁve. All this be-
fore it was realized that Lie algebras are quite elementary objects! (namely,
vector spaces of matrices closed under the Lie bracket operation). It has
been truly said that the Killing–Cartan classiﬁcation of simple Lie alge-
bras is one of the great mathematical discoveries of all time. But it is not
necessary to use the sophisticated theory of “root systems,” developed by
Killing and Cartan, merely to prove that the classical algebras so(n), su(n),
and sp(n) are simple. As we have shown in this chapter, elementary matrix
calculations sufﬁce.
The matrix proof that sl(n,C) is simple is sketched in Carter et al.
[1995], p. 10, and the simplicity of su(n) follows from it, but I have
nowhere seen the corresponding elementary proofs for so(n) and sp(n).
It is true that the calculations become a little laborious, but it is not a good
idea to hide all matrix calculations. Many results were ﬁrst discovered
because somebody did such a calculation.
The simplicity proofs in Sections 6.4 to 6.6 are trivial in the sense that
they can be discovered by anybody with enough patience. Given that sp(n),
say, is simple, we know that the ideal generated by any nonzero element
X is the whole of sp(n). Therefore, if we apply enough Lie bracket and
vector space operations to X, we will eventually obtain all the basis vectors

138
6
Structure of Lie algebras
of sp(n). In other words, brute force search gives a proof that any nonzero
ideal of sp(n) equals sp(n) itself.
The Lie algebra so(4) is close to being simple, because it is the direct
product so(3) × so(3) of simple Lie algebras. Direct products of simple
Lie algebras are called semisimple. Sophisticated Lie theory tends to focus
on the broader class of semisimple Lie algebras, where so(4) is no longer
an anomaly. With this approach, one can also avoid the embarrassment of
using the term “complex simple Lie algebras” for algebras such as sl(n,C),
replacing it by the slightly less embarrassing “complex semisimple Lie al-
gebras.” (Of course, the real mistake was to call the imaginary numbers
“complex” in the ﬁrst place.)

7
The matrix logarithm
PREVIEW
To harness the full power of the matrix exponential we need its inverse
function, the matrix logarithm function, log. Like the classical log, the
matrix log is deﬁned by a power series that converges only in a certain
neighborhood of the identity. This makes results involving the logarithm
more “local” than those involving the exponential alone, but in this chapter
we are interested only in local information.
The central result is that log and exp give a 1-to-1 correspondence,
continuous in both directions, between a neighborhood of 1 in any matrix
Lie group G and a neighborhood of 0 in its Lie algebra g = T1(G). Thus
the log function produces tangents. The proof relates the classical limit
process deﬁning tangents to the inﬁnite series deﬁning the logarithm. The
need for limits motivates the deﬁnition of a matrix Lie group as a matrix
group that is suitably closed under limits.
The correspondence shows that elements of G sufﬁciently close to 1
are all of the form eX, where X ∈g. When two such elements, eX and eY,
have a product of the form eZ it is natural to ask how Z is related to X and
Y. The answer to this question is the Campbell–Baker–Hausdorff theorem,
which says that Z equals an inﬁnite sum of elements of the Lie algebra g,
namely X +Y plus elements built from X and Y by Lie brackets.
We give a very elementary, but little-known, proof of the Campbell–
Baker–Hausdorff theorem, due to Eichler. The proof depends entirely on
manipulation of polynomials in noncommuting variables.
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 7,
139
c⃝Springer Science+Business Media, LLC 2008

140
7
The matrix logarithm
7.1
Logarithm and exponential
Motivated by the classical inﬁnite series
log(1+x) = x−x2
2 + x3
3 −x4
4 +··· ,
valid for real x with |x| < 1,
we deﬁne the logarithm of a square matrix 1+A with |A| < 1 by
log(1+A) = A−A2
2 + A3
3 −A4
4 +··· .
This series is absolutely convergent (by comparison with the geometric se-
ries) for |A| < 1, and hence log(1+A) is a well-deﬁned continuous function
in this neighborhood of 1.
The fundamental property of the matrix logarithm is the same as that
of the ordinary logarithm: it is the inverse of the exponential function.
The proof involves a trick we used in Section 5.2 to prove that eAeB =
eA+B when AB = BA. Namely, we predict the result of a computation with
inﬁnite series from knowledge of the result in the real variable case.
Inverse property of matrix logarithm. For any matrix eX within distance
1 of the identity,
log(eX) = X.
Proof. Since eX = 1+ X
1! + X2
2! + X3
3! +··· and |eX −1| < 1 we can write
log(eX) = log

1+
 X
1! + X2
2! +···

=
 X
1! + X2
2! +···

−1
2
 X
1! + X2
2! +···
2
+ 1
3
 X
1! + X2
2! +···
3
−···
by the deﬁnition of the matrix logarithm. Also, the series is absolutely
convergent, so we can rearrange terms so as to collect all powers of Xm
together, for each m. This gives
log(eX) = X +
 1
2! −1
2

X2 +
 1
3! −1
2 + 1
3

X3 +··· .
It is hard to describe the terms that make up the coefﬁcient of Xm, for
arbitrary m > 1, but we know that their sum is zero! Why? Because exactly

7.1
Logarithm and exponential
141
the same terms occur in the expansion of log(ex), when |ex −1| < 1, and
their sum is zero because log(ex) = x under these conditions.
Thus log(eX) = X as required.
□
The inverse property allows us to derive certain properties of the matrix
logarithm from corresponding properties of the matrix exponential. For
example:
Multiplicative property of matrix logarithm. If AB = BA, and log(A),
log(B), and log(AB) are all deﬁned, then
log(AB) = log(A)+log(B).
Proof. Suppose that log(A) = X and log(B) = Y, so eX = A and eY = B by
the inverse property of log. Notice that XY = YX because
X = log(1+(A−1)) = (A−1)−(A−1)2
2
+ (A−1)3
3
−··· ,
Y = log(1+(B−1)) = (B−1)−(B−1)2
2
+ (B−1)3
3
−··· ,
and the series commute because A and B do. Thus it follows from the
addition formula for exp proved in Section 5.2 that
AB = eXeY = eX+Y.
Taking log of both sides of this equation, we get
log(AB) = X +Y = log(A)+log(B)
by the inverse property of the matrix logarithm again.
□
Exercises
The log series
log(1 +x) = x−x2
2 + x3
3 −x4
4 +···
was ﬁrst published by Nicholas Mercator in a book entitled Logarithmotechnia in
1668. Mercator’s derivation of the series was essentially this:
log(1 +x) =
 x
0
dt
1 +t =
 x
0 (1 −t +t2 −t3 +···)dt = x−x2
2 + x3
3 −x4
4 +··· .
Isaac Newton discovered the log series at about the same time, but took the idea
further, discovering the inverse relationship with the exponential series as well.
He discovered the exponential series by solving the equation y = log(1 + x) as
follows.

142
7
The matrix logarithm
7.1.1 Supposing x = a0 +a1y+a2y2 +··· (the function we call ey −1), show that
y =
(a0 +a1y+a2y2 +···)
−1
2(a0 +a1y+a2y2 +···)2
+1
3(a0 +a1y+a2y2 +···)3
···
(*)
7.1.2 By equating the constant terms on both sides of (*), show that a0 = 0.
7.1.3 By equating coefﬁcients of y on both sides of (*), show that a1 = 1.
7.1.4 By equating coefﬁcients of y2 on both sides of (*), show that a2 = 1/2.
7.1.5 See whether you can go as far as Newton, who also found that a3 = 1/6,
a4 = 1/24, and a5 = 1/120.
Newton then guessed that an = 1/n! “by observing the analogy of the series.”
Unlike us, he did not have independent knowledge of the exponential function
ensuring that its coefﬁcients follow the pattern observed in the ﬁrst few.
As with exp, term-by-term differentiation and series manipulation give some
familiar formulas.
7.1.6 Prove that d
dt log(1 +At) = A(1 +At)−1.
7.2
The exp function on the tangent space
For all the groups G we have seen so far it has been easy to ﬁnd a general
form for tangent vectors A′(0) from the equation(s) deﬁning the members
A of G. We can then check that all the matrices X of this form are mapped
into G by exp, and that etX lies in G along with eX, in which case X is a
tangent vector to G at 1. Thus exp solves the problem of ﬁnding enough
smooth paths in G to give the whole tangent space T1(G) = g.
But if we are not given an equation deﬁning the matrices A in G, we
may not be able to ﬁnd tangent matrices in the form A′(0) in the ﬁrst place,
so we need a different route to the tangent space. The log function looks
promising, because we can certainly get back into G by applying exp to a
value X of the log function, since exp inverts log.
However, it is not clear that log maps any part of G into T1(G), except
the single point 1 ∈G. We need to make a closer study of the relation
between the limits that deﬁne tangent vectors and the deﬁnition of log.
This train of thought leads to the realization that G must be closed under
certain limits, and it prompts the following deﬁnition (foreshadowed in
Section 1.1) of the main concept in this book.

7.2
The exp function on the tangent space
143
Deﬁnition. A matrix Lie group G is a group of matrices that is closed
under nonsingular limits. That is, if A1,A2,A3,... is a convergent sequence
of matrices in G, with limit A, and if det(A) ̸= 0, then A ∈G.
This closure property makes possible a fairly immediate proof that exp
indeed maps T1(G) back into G.
Exponentiation of tangent vectors. If A′(0) is the tangent vector at 1 to
a matrix Lie group G, then eA′(0) ∈G. That is, exp maps the tangent space
T1(G) into G.
Proof. Suppose that A(t) is a smooth path in G such that A(0) = 1, and
that A′(0) is the corresponding tangent vector at 1. By deﬁnition of the
derivative we have
A′(0) = lim
Δt→0
A(Δt)−1
Δt
= lim
n→∞
A(1/n)−1
1/n
,
where n takes all natural number values greater than some n0. We compare
this formula with the deﬁnition of logA(1/n),
logA(1/n) = (A(1/n)−1)−(A(1/n)−1)2
2
+ (A(1/n)−1)3
3
−··· ,
which also holds for natural numbers n greater than some n0. Dividing
both sides of the log formula by 1/n we get
nlogA(1/n) = logA(1/n)
1/n
= A(1/n)−1
1/n
−A(1/n)−1
1/n
A(1/n)−1
2
−(A(1/n)−1)2
3
+···

.
(*)
Now, taking n0 large enough that |A(1/n)−1| < ε < 1/2, the series in
square brackets has sum of absolute value less than ε +ε2 +ε3 +··· < 2ε,
so its sum tends to 0 as n tends to ∞. It follows that the right side of (*) has
the limit
A′(0)−A′(0)[0] = A′(0)
as n →∞. The left side of (*), nlogA(1/n), has the same limit, so
A′(0) = lim
n→∞nlogA(1/n).
(**)

144
7
The matrix logarithm
Taking exp of equation (**), we get
eA′(0) = elimn→∞nlogA(1/n)
= lim
n→∞enlogA(1/n)
because exp is continuous
= lim
n→∞

elogA(1/n)n
because eA+B = eAeB when AB = BA
= lim
n→∞A(1/n)n
because exp is the inverse of log.
Now A(1/n) ∈G by assumption, so A(1/n)n ∈G because G is closed under
products. We therefore have a convergent sequence of members of G, and
its limit eA′(0) is nonsingular because it has inverse e−A′(0). So eA′(0) ∈G,
by the closure of G under nonsingular limits.
In other words, exp maps the tangent space T1(G) = g into G.
□
The proof in the opposite direction, from G into T1(G), is more subtle.
It requires a deeper study of limits, which we undertake in the next section.
Exercises
7.2.1 Deduce from exponentiation of tangent vectors that
T1(G) = {X : etX ∈G for all t ∈R}.
The property T1(G) = {X : etX ∈G for all t ∈R} is used as a deﬁnition of T1(G)
by some authors, for example Hall [2003]. It has the advantage of making it clear
that exp maps T1(G) into G. On the other hand, with this deﬁnition, we have to
check that T1(G) is a vector space.
7.2.2 Given X as the tangent vector to etX, and Y as the tangent vector to etY,
show that X +Y is the tangent vector to A(t) = etXetY .
7.2.3 Similarly, show that if X is a tangent vector then so is rX for any r ∈R.
The formula A′(0) = limn→∞nlogA(1/n) that emerges in the proof above can
actually be used in two directions. It can be used to prove that exp maps T1(G)
into G when combined with the fact that G is closed under products (and hence
under nth powers). And it can be used to prove that log maps (a neighborhood of
1 in) G into T1(G) when combined with the fact that G is closed under nth roots.
Unfortunately, proving closure under nth roots is as hard as proving that log
maps into T1(G), so we need a different approach to the latter theorem. Never-
theless, it is interesting to see how nth roots are related to the behavior of the log
function, so we develop the relationship in the following exercises.

7.3
Limit properties of log and exp
145
7.2.4 Suppose that, for each A in some neighborhood N of 1 in G, there is a
smooth function A(t), with values in G, such that A(1/n) = A1/n for n =
1,2,3, .... Show that A′(0) = logA, so logA ∈T1(G).
7.2.5 Suppose, conversely, that log maps some neighborhood N of 1 in G into
T1(G). Explain why we can assume that N is mapped by log onto an
ε-ball Nε(0) in T1(G).
7.2.6 Taking N as in Exercise 7.2.4, and A ∈N , show that t logA ∈T1(G) for
all t ∈[0,1], and deduce that A1/n exists for n = 1,2,3, ....
7.3
Limit properties of log and exp
In 1929, von Neumann created a new approach to Lie theory by conﬁn-
ing attention to matrix Lie groups. Even though the most familiar Lie
groups are matrix groups (and, in fact, the ﬁrst nonmatrix examples were
not discovered until the 1930s), Lie theory began as the study of general
“continuous” groups and von Neumann’s approach was a radical simpliﬁ-
cation. In particular, von Neumann deﬁned “tangents” prior to the concept
of differentiability—going back to the idea that a tangent vector is the limit
of a sequence of “chord” vectors—as one sees tangents in a ﬁrst calculus
course (Figure 7.1).
P
A1
A2
A3
Figure 7.1: The tangent as the limit of a sequence.
Deﬁnition. X is a sequential tangent vector to G at 1 if there is a sequence
⟨Am⟩of members of G, and a sequence ⟨αm⟩of real numbers, such that
Am →1 and (Am −1)/αm →X as m →∞.

146
7
The matrix logarithm
If A(t) is a smooth path in G with A(0) = 1, then the sequence of points
Am = A(1/m) tends to 1 and
A′(0) = lim
m→∞
Am −1
1/m ,
so any ordinary tangent vector A′(0) is a sequential tangent vector. But
sometimes it is convenient to arrive at tangent vectors via sequences rather
than via smooth paths, so it would be nice to be sure that all sequential
tangent vectors are in fact ordinary tangent vectors. This is conﬁrmed by
the following theorem.
Smoothness of sequential tangency. Suppose that ⟨Am⟩is a sequence in
a matrix Lie group G such that Am →1 as m →∞, and that ⟨αm⟩is a
sequence of real numbers such that (Am −1)/αm →X as m →∞.
Then etX ∈G for all real t (and therefore X is the tangent at 1 to the
smooth path etX).
Proof. Let X = limm→∞Am−1
αm . First we prove that eX ∈G. Then we indicate
how the proof may be modiﬁed to show that etX ∈G.
Given that (Am −1)/αm →X as m →∞, it follows that αm →0 as
Am →1, and hence 1/αm →∞. Then if we set
am = nearest integer to 1/αm,
we also have am(Am −1) →X as m →∞. Since am is an integer,
log(Aam
m ) = am log(Am)
by the multiplicative property of log
= am(Am −1)−am(Am −1)
Am −1
2
−(Am −1)2
3
+···

.
And since Am →1 we can argue as in Section 7.2 that the series in square
brackets tends to zero. Then, since limm→∞am(Am −1) = X, we have
X = lim
m→∞log(Aam
m ).
It follows, by the inverse property of log and the continuity of exp, that
eX = lim
m→∞Aam
m .
Since am is an integer, Aam
m ∈G by the closure of G under products. And
then, by the closure of G under nonsingular limits,
eX = lim
m→∞Aam
m ∈G.

7.4
The log function into the tangent space
147
To prove that etX ∈G for any real t one replaces 1/αm in the above
argument by t/αm. If
bm = nearest integer to t/αm,
we similarly have bm(Am −1) →tX as m →∞. And if we consider the
series for
log(Abm
m ) = bm log(Am)
we similarly ﬁnd that
etX = lim
m→∞Abm
m ∈G
by the closure of G under nonsingular limits.
□
This theorem is the key to proving that log maps a neighborhood of 1 in
G onto a neighborhood of 0 in T1(G), as we will see in the next section. It
is also the core of the result of von Neumann [1929] that matrix Lie groups
are “smooth manifolds.” We do not deﬁne or investigate smooth manifolds
in this book, but one can glimpse the emergence of “smoothness” in the
passage from the sequence ⟨Am⟩to the curve etX in the above proof.
Exercises
Having proved that sequential tangents are the same as the smooth tangents we
considered previously, we conclude that sequential tangents have the real vector
space properties. Still, it is interesting to see how the vector space properties
follow from the deﬁnition of sequential tangent.
7.3.1 If X and Y are sequential tangents to a group G at 1, show that X +Y is also.
7.3.2 If X is a sequential tangent to a group G at 1, show that rX is also, for any
real number r.
7.4
The log function into the tangent space
By a “neighborhood” of 1 in G we mean a set of the form
Nδ(1) = {A ∈G : |A−1| < δ},
where |B| denotes the absolute value of the matrix B, deﬁned in Section 4.5.
We also call Nδ(1) the δ-neighborhood of 1. Then we have the following
theorem.

148
7
The matrix logarithm
The log of a neighborhood of 1. For any matrix Lie group G there is a
neighborhood Nδ(1) mapped into T1(G) by log.
Proof. Suppose on the contrary that no Nδ(1) is mapped into T1(G) by log.
Then we can ﬁnd A1,A2,A3,... ∈G with Am →1 as m →∞, and with each
logAm ̸∈T1(G).
Of course, G is contained in some Mn(C). So each logAm is in Mn(C)
and we can write
logAm = Xm +Ym,
where Xm is the component of logAm in T1(G) and Ym ̸= 0 is the component
in T1(G)⊥, the orthogonal complement of T1(G) in Mn(C). We note that
Xm,Ym →0 as m →∞because Am →1 and log is continuous.
Next we consider the matrices Ym/|Ym| ∈T1(G)⊥. These all have ab-
solute value 1, so they lie on the sphere S of radius 1 and center 0 in
Mn(C). It follows from the boundedness of S that the sequence ⟨Ym/|Ym|⟩
has a convergent subsequence, and the limit Y of this subsequence is also
a vector in T1(G)⊥of length 1. In particular, Y ̸∈T1(G).
Taking the subsequence with limit Y in place of the original sequence
we have
lim
m→∞
Ym
|Ym| = Y.
Finally, we consider the sequence of terms
Tm = e−XmAm.
Each Tm ∈G because −Xm ∈T1(G); hence e−Xm ∈G by the exponentiation
of tangent vectors in Section 7.2, and Am ∈G by hypothesis. On the other
hand, Am = eXm+Ym by the inverse property of log, so
Tm = e−XmeXm+Ym
=

1−Xm + X2
m
2! +···

1+Xm +Ym + (Xm +Ym)2
2!
+···

= 1+Ym +higher-order terms.
Admittedly, these higher-order terms include X2
m, and other powers of Xm,
that are not necessarily small in comparison with Ym. However, these pow-
ers of Xm are those in
1 = e−XmeXm,

7.4
The log function into the tangent space
149
so they sum to zero. (I thank Brian Hall for this observation.) Therefore,
lim
m→∞
Tm −1
|Ym|
= lim
m→∞
Ym
|Ym| = Y.
Since each Tm ∈G, it follows that the sequential tangent
lim
m→∞
Tm −1
|Ym|
= Y
is in T1(G) by the smoothness of sequential tangents proved in Section 7.3.
But Y ̸∈T1(G), as observed above. This contradiction shows that our
original assumption was false, so there is a neighborhood Nδ(1) mapped
into T1(G) by log.
□
Corollary. The log function gives a bijection, continuous in both direc-
tions, between Nδ(1) in G and logNδ(1) in T1(G).
Proof. The continuity of log, and of its inverse function exp, shows that
there is a 1-to-1 correspondence, continuous in both directions, between
Nδ(1) and its image logNδ(1) in T1(G).
□
If Nδ(1) in G is mapped into T1(G) by log, then each A ∈Nδ(1) has the
form A = eX, where X = logA ∈T1(G). Thus the paradise of SO(2) and
SU(2)—where each group element is the exponential of a tangent vector—
is partly regained by the theorem above. Any matrix Lie group G has at
least a neighborhood of 1 in which each element is the exponential of a
tangent vector.
The corollary tells us that the set logNδ(1) is a “neighborhood” of 0
in T1(G) in a more general sense—the topological sense—that we will
discuss in Chapter 8. The existence of this continuous bijection between
neighborhoods ﬁnally establishes that G has a topological dimension equal
to the real vector space dimension of T1(G), thanks to the deep theorem
of Brouwer [1911] on the invariance of topological dimension. This gives
a broad justiﬁcation for the Lie theory convention, already mentioned in
Section 5.5, of deﬁning the dimension of a Lie group to be the dimension
of its Lie algebra. In practice, arguments about dimension are made at the
Lie algebra level, where we can use linear algebra, so we will not actually
need the topological concept of dimension.
Exercises
The continuous bijection between neighborhoods of 1 in G and of 0 in T1(G)
enables us to show the existence of nth roots in a matrix Lie group.

150
7
The matrix logarithm
7.4.1 Show that each A ∈Nδ (1) has a unique nth root, for n = 1,2,3, ....
7.4.2 Show that the 2×2 identity matrix 1 has two square roots in SO(2), but that
one of them is “far” from 1.
7.5
SO(n), SU(n), and Sp(n) revisited
In Section 3.8 we proved Schreier’s theorem that any discrete normal sub-
group of a path-connected group lies in its center. This gives us the discrete
normal subgroups of SO(n), SU(n), and Sp(n), since the latter groups are
path-connected and we found their centers in Section 3.7. What remains is
to ﬁnd out whether SO(n), SU(n), and Sp(n) have any nondiscrete normal
subgroups. We claimed in Section 3.9 that the tangent space would enable
us to see any nondiscrete normal subgroups, and we are ﬁnally in a position
to explain why.
For convenience we assume a plausible result that will be proved rigor-
ously in Section 8.6: if Nδ(1) is a neighborhood of 1 in a path-connected
group G, then any element of G is a product of members of Nδ(1). We say
that Nδ(1) generates the whole group G. With this assumption, we have
the following theorem.
Tangent space visibility. If G is a path-connected matrix Lie group with
discrete center and a nondiscrete normal subgroup H, then T1(H) ̸= {0}.
Proof. Since the center Z(G) of G is discrete, and H is not, we can ﬁnd a
neighborhood Nδ(1) in G that includes elements B ̸= 1 in H but no member
of Z(G) other than 1. If B ̸= 1 is a member of H in Nδ(1), then B does not
commute with some A ∈Nδ(1). If B commutes with all elements of Nδ(1)
then B commutes with all elements of G (because Nδ(1) generates G), so
B ∈Z(G), contrary to our choice of Nδ(1).
By taking δ sufﬁciently small we can ensure, by the theorem of the
previous section, that A = eX for some X ∈T1(G). Indeed, we can ensure
that the whole path A(t) = etX is in Nδ(1) for 0 ≤t ≤1.
Now consider the smooth path C(t) = etXBe−tXB−1, which runs from
1 to eXBe−XB−1 = ABA−1B−1 in G. A calculation using the product rule
for differentiation (exercise) shows that the tangent vector to C(t) at 1 is
C′(0) = X −BXB−1.
Since H is a normal subgroup of G, and B ∈H, we have etXBe−tX ∈H.

7.5
SO(n), SU(n), and Sp(n) revisited
151
Then etXBe−tXB−1 ∈H as well, so C(t) is in fact a smooth path in H and
C′(0) = X −BXB−1 ∈T1(H).
Thus to prove that T1(H) ̸= {0} it sufﬁces to show that X −BXB−1 ̸= 0.
Well,
X −BXB−1 = 0 ⇒BXB−1 = X
⇒eBXB−1 = eX
⇒BeXB−1 = eX
⇒BeX = eXB
⇒BA = AB,
contrary to our choice of A and B.
This contradiction proves that T1(H) ̸= {0}.
□
Corollary. If H is a nontrivial normal subgroup of G under the conditions
above, then T1(H) is a nontrivial ideal of T1(G).
Proof. We know from Section 6.1 that T1(H) is an ideal of T1(G), and
T1(H) ̸= {0} by the theorem.
If T1(H) = T1(G) then H ﬁlls Nδ(1) in G, by the log-exp bijection
between neighborhoods of the identity in G and T1(G). But then H = G
because G is path-connected and hence generated by Nδ(1). Thus if H ̸= G,
then T1(H) ̸= T1(G).
□
It follows from the theorem that any nondiscrete normal subgroup H
of G = SO(n),SU(n),Sp(n) gives a nonzero ideal T1(H) in T1(G). The
corollary says that T1(H) is nontrivial, that is, T1(H) ̸= T1(G) if H ̸= G.
Thus we ﬁnally know for sure that the only nontrivial normal subgroups of
SO(n), SU(n), and Sp(n) are the subgroups of their centers. (And hence
all the nontrivial normal subgroups are ﬁnite cyclic groups.)
SO(3) revisited
In Section 2.3 we showed that SO(3) is simple—the result that launched
our whole investigation of Lie groups—by a somewhat tricky geometric
argument. We can now give a proof based on the easier facts that the center
of SO(3) is trivial, which was proved in Section 3.5 (also in Exercises 3.5.4
and 3.5.5), and that so(3) is simple, which was proved in Section 6.1. The
hard work can be done by general theorems.

152
7
The matrix logarithm
By the theorem in Section 3.8, any discrete normal subgroup of SO(3)
is contained in Z(SO(3)), and hence is trivial. By the corollary above,
and the theorem in Section 6.1, any nondiscrete normal subgroup of SO(3)
yields a nontrivial ideal of so(3), which does not exist.
Exercises
7.5.1 If C(t) = etXBe−tXB−1, check that C′(0) = X −BXB−1.
7.5.2 Give an example of a connected matrix Lie group with a nondiscrete normal
subgroup H such that T1(H) = {0}.
7.5.3 Prove that U(n) has no nontrivial normal subgroup except Z(U(n)).
7.5.4 The tangent space visibility theorem also holds if G is not path-connected.
Explain how to modify the proof in this case.
7.6
The Campbell–Baker–Hausdorff theorem
The results of Section 7.4 show that, in some neighborhood of 1, any two
elements of G have the form eX and eY for some X,Y in g, and that the
product of these two elements, eXeY, is eZ for some Z in g. The Campbell–
Baker–Hausdorff theorem says that more than this is true, namely, the Z
such that eXeY = eZ is the sum of a series X +Y+ Lie bracket terms com-
posed from X and Y. In this sense, the Lie bracket on g “determines” the
product operation on G.
To give an inkling of how this theorem comes about, we expand eX
and eY as inﬁnite series, form the product series, and calculate the ﬁrst few
terms of its logarithm, Z. By the deﬁnition of the exponential function we
have
eX = 1+ X
1! + X2
2! + X3
3! +··· ,
eY = 1+ Y
1! + Y 2
2! + Y 3
3! +··· ,
and therefore
eXeY = 1+X +Y +XY + X2
2! + Y 2
2! +···+ XmY n
m!n! +···
with a term for each pair of integers m,n ≥0. It follows, since
log(1+W) = W −W 2
2 + W 3
3 −W 4
4 +··· ,

7.6
The Campbell–Baker–Hausdorff theorem
153
that
Z = log(eXeY) =

X +Y +XY + X2
2! + Y 2
2! +···

−1
2

X +Y +XY + X2
2! + Y 2
2! +···
2
+ 1
3

X +Y +XY + X2
2! + Y 2
2! +···
3
−···
= X +Y + 1
2XY −1
2YX +higher-order terms
= X +Y + 1
2[X,Y]+higher-order terms.
The hard part of the Campbell-Baker-Hausdorff theorem is to prove that
all the higher-order terms are composed from X and Y by Lie brackets.
Campbell attempted to do this in 1897. His work was amended by
Baker in 1905, with further corrections by Hausdorff producing a com-
plete proof in 1906. However, these ﬁrst proofs were very long, and many
attempts have since been made to derive the theorem with greater economy
and insight. Modern textbook proofs are typically only a few pages long,
but they draw on differentiation, integration, and specialized machinery
from Lie theory.
The most economical proof I know is one by Eichler [1968]. It is only
two pages long and purely algebraic, showing by induction on n that all
terms of order n > 1 are linear combinations of Lie brackets. The algebra
is very simple, but ingenious (as you would expect, since the theorem is
surely not trivial). In my opinion, this is also an insightful proof, showing
as it does that the theorem depends only on simple algebraic facts. I present
Eichler’s proof, with some added explanation, in the next section.
Exercises
7.6.1 Show that the cubic term in log(eXeY) is
1
12(X2Y +XY2 +YX2 +Y 2X −2XYX −2YXY).
7.6.2 Show that the cubic polynomial in Exercise 7.6.1 is a linear combination of
[X,[X,Y]] and [Y,[Y,X]].

154
7
The matrix logarithm
The idea of representing the Z in eZ = eXeY by a power series in noncommuting
variables X and Y allows us to prove the converse of the theorem that XY = YX
implies eXeY = eX+Y.
7.6.3 Suppose that eXeY = eYeX. By appeal to the proof of the log multiplicative
property in Section 7.1, or otherwise, show that XY = YX.
7.6.4 Deduce from Exercise 7.6.3 that eXeY = eX+Y if and only if XY = YX.
7.7
Eichler’s proof of Campbell–Baker–Hausdorff
To facilitate an inductive proof, we let
eAeB = eZ,
Z = F1(A,B)+F2(A,B)+F3(A,B)+··· ,
(*)
where Fn(A,B) is the sum of all the terms of degree n in Z, and hence is a
homogeneous polynomial of degree n in the variables A and B. Since the
variables stand for matrices in the Lie algebra g, they do not generally com-
mute, but their product is associative. From the calculation in the previous
section we have
F1(A,B) = A+B,
F2(A,B) = 1
2(AB−BA) = 1
2[A,B].
We will call a polynomial p(A,B,C,...) Lie if it is a linear combination
of A,B,C,... and (possibly nested) Lie bracket terms in A,B,C, . . . . Thus
F1(A,B) and F2(A,B) are Lie polynomials, and the theorem we wish to
prove is:
Campbell–Baker–Hausdorff theorem. For each n ≥1, the polynomial
Fn(A,B) in (*) is Lie.
Proof. Since products of A,B,C,... are associative, the same is true of
products of power series in A,B,C,..., so for any A,B,C we have
(eAeB)eC = eA(eBeC),
and therefore, if eAeBeC = eW,
W =
∞
∑
i=1
Fi

∞
∑
j=1
Fj(A,B),C

=
∞
∑
i=1
Fi

A,
∞
∑
j=1
Fj(B,C)

.
(1)
Our induction hypothesis is that Fm is a Lie polynomial for m < n, and we
wish to prove that Fn is Lie.

7.7
Eichler’s proof of Campbell–Baker–Hausdorff
155
The induction hypothesis implies that all homogeneous terms of degree
less than n in both expressions for W in (1) are Lie, and so too are the
homogeneous terms of degree n resulting from i > 1 and j > 1. The only
possible exceptions are the polynomials
Fn(A,B)+Fn(A+B,C)
on the left (from i = 1, j = n and i = n, j = 1),
Fn(A,B+C)+Fn(B,C)
on the right (from i = n, j = 1 and i = 1, j = n).
Therefore, equating terms of degree n on both sides of (1), we ﬁnd that the
difference between the exceptional polynomials is a Lie polynomial. This
property is a congruence relation between polynomials that we write as
Fn(A,B)+Fn(A+B,C) ≡Lie Fn(A,B+C)+Fn(B,C).
(2)
Relation (2) yields many consequences, by substituting special values of
the variables A, B, and C, and from it we eventually derive Fn(A,B) ≡Lie 0,
thus proving the desired result that Fn is Lie.
Before we start substituting, here are three general facts concerning
real multiples of the variables.
1. Fn(rA,sA) = 0, because the matrices rA and sA commute and hence
erAesA = erA+sA. That is, Z = F1(rA,sA), so all other Fn(rA,sA) = 0.
2. In particular, r = 1 and s = 0 gives Fn(A,0) = 0.
3. Fn(rA,rB) = rnFn(A,B) because Fn is homogeneous of degree n.
These facts guide the following substitutions in the congruence (2).
First, replace C by −B in (2), obtaining
Fn(A,B)+Fn(A+B,−B) ≡Lie Fn(A,0)+Fn(B,−B)
≡Lie 0
by facts 2 and 1.
Therefore
Fn(A,B) ≡Lie −Fn(A+B,−B).
(3)
Then replace A by −B in (2), obtaining
Fn(−B,B)+Fn(0,C) ≡Lie Fn(−B,B+C)+Fn(B,C),
which gives, by facts 1 and 2 again,
0 ≡Lie Fn(−B,B+C)+Fn(B,C).

156
7
The matrix logarithm
Next, replacing B, C by A, B respectively gives
0 ≡Lie Fn(−A,A+B)+Fn(A,B),
and hence
Fn(A,B) ≡Lie −Fn(−A,A+B).
(4)
Relations (3) and (4) allow us to relate Fn(A,B) to Fn(B,A) as follows:
Fn(A,B) ≡Lie −Fn(−A,A+B)
by (4)
≡Lie −(−Fn(−A+A+B,−A−B))
by (3)
≡Lie Fn(B,−A−B)
≡Lie −Fn(−B,−A)
by (4)
≡Lie −(−1)nFn(B,A)
by fact 3.
Thus the relation between Fn(A,B) and Fn(B,A) is
Fn(A,B) ≡Lie −(−1)nFn(B,A).
(5)
Second, we replace C by −B/2 in (2), which gives
Fn(A,B)+Fn(A+B,−B/2) ≡Lie Fn(A,B/2)+Fn(B,−B/2)
≡Lie Fn(A,B/2)
by fact 1,
so
Fn(A,B) ≡Lie Fn(A,B/2)−Fn(A+B,−B/2).
(6)
Next, replacing A by −B/2 in (2) gives
Fn(−B/2,B)+Fn(B/2,C) ≡Lie Fn(−B/2,B+C)+Fn(B,C),
and therefore, by fact 1,
Fn(B/2,C) ≡Lie Fn(−B/2,B+C)+Fn(B,C).
Then, replacing B, C by A, B respectively gives
Fn(A/2,B) ≡Lie Fn(−A/2,A+B)+Fn(A,B),
that is,
Fn(A,B) ≡Lie Fn(A/2,B)−Fn(−A/2,A+B).
(7)

7.7
Eichler’s proof of Campbell–Baker–Hausdorff
157
Relations (6) and (7) allow us to pass from polynomials in A, B to
polynomials in A/2, B/2, paving the way for another application of fact 3
and a new relation, between Fn(A,B) and itself.
Relation (6) allows us to rewrite the two terms on the right side of (7)
as follows:
Fn(A/2,B) ≡Lie Fn(A/2,B/2)−Fn(A/2+B,−B/2)
by (6)
≡Lie Fn(A/2,B/2)+Fn(A/2+B/2,B/2)
by (3)
≡Lie 2−nFn(A,B)+2−nFn(A+B,B)
by fact 3,
Fn(−A/2,A+B)
≡Lie Fn(−A/2,A/2+B/2)−Fn(A/2+B,−A/2−B/2) by (6)
≡Lie −Fn(A/2,B/2)+Fn(B/2,A/2+B/2)
by (4) and (3)
≡Lie −2−nFn(A,B)+2−nFn(B,A+B)
by fact 3.
So (7) becomes
Fn(A,B) ≡Lie 21−nFn(A,B)+2−nFn(A+B,B)−2−nFn(B,A+B),
and, with the help of (5), this simpliﬁes to
(1−21−n)Fn(A,B) ≡Lie 2−n (1+(−1)n)Fn(A+B,B).
(8)
If n is odd, (8) already shows that Fn(A,B) ≡Lie 0.
If n is even, we replace A by A−B in (8), obtaining
(1−21−n)Fn(A−B,B) ≡Lie 21−nFn(A,B).
(9)
The left side of (9)
(1−21−n)Fn(A−B,B) ≡Lie −(1−21−n)Fn(A,−B)
by (3),
so, making this replacement, (9) becomes
−Fn(A,−B) ≡Lie
21−n
1−21−n Fn(A,B).
(10)
Finally, replacing B by −B in (10), we get
−Fn(A,B) ≡Lie
21−n
1−21−n Fn(A,−B)
≡Lie −

21−n
1−21−n
2
Fn(A,B)
by (10),
and this implies Fn(A,B) ≡Lie 0, as required.
□

158
7
The matrix logarithm
Exercises
The congruence relation (6)
Fn(A,B) ≡Lie −(−1)nFn(B,A)
discovered in the above proof can be strengthened remarkably to
Fn(A,B) = −(−1)nFn(B,A).
Here is why.
7.7.1 If Z(A,B) denotes the solution Z of the equation eAeB = eZ, explain why
Z(−B,−A) = −Z(A,B).
7.7.2 Assuming that one may “equate coefﬁcients” for power series in noncom-
muting variables, deduce from Exercise 7.7.1 that
Fn(A,B) = −(−1)nFn(B,A).
7.8
Discussion
The beautiful self-contained theory of matrix Lie groups seems to have
been discovered by von Neumann [1929]. In this little-known paper5 von
Neumann deﬁnes the matrix Lie groups as closed subgroups of GL(n,C),
and their “tangents” as limits of convergent sequences of matrices. In this
chapter we have recapitulated some of von Neumann’s results, streamlin-
ing them slightly by using now-standard techniques of calculus and linear
algebra. In particular, we have followed von Neumann in using the ma-
trix exponential and logarithm to move smoothly back and forth between
a matrix Lie group and its tangent space, without appealing to existence
theorems for inverse functions and the solution of differential equations.
The idea of using matrix Lie groups to introduce Lie theory was sug-
gested by Howe [1983]. The recent texts of Rossmann [2002], Hall [2003],
and Tapp [2005] take up this suggestion, but they move away from the ideas
of von Neumann cited by Howe. All put similar theorems on center stage—
viewing the Lie algebra g of G as both the tangent space and the domain
of the exponential function—but they rely on analytic existence theorems
rather than on von Neumann’s rock-bottom approach through convergent
sequences of matrices.
5The only book I know that gives due credit to von Neumann’s paper is Godement
[2004], where it is described on p. 69 as “the best possible introduction to Lie groups” and
“the ﬁrst ‘proper’ exposition of the subject.”

7.8
Discussion
159
Indeed, von Neumann’s purpose in pursuing elementary constructions
in Lie theory was to explain why continuity apparently implies differentia-
bility for groups, a question raised by Hilbert in 1900 that became known as
Hilbert’s ﬁfth problem. It would take us too far aﬁeld to explain Hilbert’s
ﬁfth problem more precisely than we have already done in Section 7.3,
other than to say that von Neumann showed that the answer is yes for com-
pact groups, and that Gleason, Montgomery, and Zippin showed in 1952
that the answer is yes for all groups.
As mentioned in Section 4.7, Hamilton made the ﬁrst extension of the
exponential function to a noncommutative domain by deﬁning it for quater-
nions in 1843. He observed almost immediately that it maps the pure imag-
inary quaternions onto the unit quaternions, and that eq+q′ = eqeq′ when
qq′ = q′q. He took the idea further in his Elements of Quaternions of
1866, realizing that eq+q′ is not usually equal to eqeq′, because of the non-
commutative quaternion product. On p. 425 of Volume I he actually ﬁnds
the second-order approximation to the Campbell–Baker–Hausdorff series:
eq+q′ −eqeq′ = qq′ −q′q
2
+ terms of third and higher dimensions.
The early proofs (or attempted proofs) of the general Campbell–Baker–
Hausdorff theorem around 1900 were extremely lengthy—around 20 pages.
The situation did not improve when Bourbaki developed a more concep-
tual approach to the theorem in the 1960s. See for example Serre [1965], or
Section 4 or Bourbaki [1972], Chapter II. Bourbaki believes that the proper
setting for the theorem is in the framework of free magmas, free algebras,
free groups, and free Lie algebras, all of which takes longer to explain
than the proofs by Campbell, Baker, and Hausdorff. It seems to me that
these proofs are totally outclassed by the Eichler proof I have used in this
chapter, which assumes only that the variables A, B, C have an associative
product, and uses only calculations that a high-school student can follow.
Martin Eichler (1912–1992) was a German mathematician (later living
in Switzerland) who worked mainly in number theory and related parts of
algebra and analysis. A famous saying, attributed to him, is that there are
ﬁve fundamental operations of arithmetic: addition, subtraction, multipli-
cation, division, and modular forms. Some of his work involves orthogonal
groups, but nevertheless his 1968 paper on the Campbell–Baker–Hausdorff
theorem seems to come out of the blue. Perhaps this is a case in which an
outsider saw the essence of a theorem more clearly than the experts.

8
Topology
PREVIEW
One of the essential properties of a Lie group G is that the product and in-
verse operations on G are continuous functions. Consequently, there comes
a point in Lie theory where it is necessary to study the theory of continuity,
that is, topology. Our journey has now reached that point.
We introduce the concepts of open and closed sets, in the concrete set-
ting of k-dimensional Euclidean space Rk, and use them to explain the re-
lated concepts of continuity, compactness, paths, path-connectedness, and
simple connectedness. The ﬁrst fruit of this development is a topological
characterization of matrix Lie groups, deﬁned in Section 7.2 through the
limit concept.
All such groups are subgroups of the general linear group GL(n,C)
of invertible complex matrices, for some n. They are precisely the closed
subgroups of GL(n,C).
The concepts of compactness and path-connectedness serve to reﬁne
this description. For example, O(n) and SO(n) are compact but GL(n,C)
is not; SO(n) is path-connected but O(n) is not.
Finally, we introduce the concept of deformation of paths, which al-
lows us to deﬁne simple connectivity. A simply connected space is one in
which any two paths between two points are deformable into each other.
This reﬁnes the qualitative description of Lie groups further—for exam-
ple, SU(2) is simply connected but SO(2) is not—but simply connected
groups have a deeper importance that will emerge when we reconnect with
Lie algebras in the next chapter.
160
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 8,
c⃝Springer Science+Business Media, LLC 2008

8.1
Open and closed sets in Euclidean space
161
8.1
Open and closed sets in Euclidean space
The geometric setting used throughout this book is the Euclidean space
Rk = {(x1,x2,...,xk) : x1,x2,...,xk ∈R},
with distance d(X,Y) between points
X = (x1,x2,...,xk)
and
Y = (y1,y2,...,yk)
deﬁned by
d(X,Y) =

(x1 −y1)2 +(x2 −y2)2 +···+(xk −yk)2.
This is the distance on Rk that is invariant under the transformations in
the group O(k) and its subgroup SO(k). Also, when we interpret Cn as
R2n by letting the point (x1 + ix′
1,x2 + ix′
2,...,xn + ix′
n) ∈Cn correspond
to the point (x1,x′
1,x2,x′
2,...,xn,x′
n) ∈R2n then the distance deﬁned by the
Hermitian inner product on Cn is the same as the Euclidean distance on
R2n, as we saw in Section 3.3. Likewise, the distance on Hn deﬁned by its
Hermitian inner product is the same as the Euclidean distance on R4n.
As in Section 4.5 we view an n × n real matrix A with (i, j)-entry aij
as the point (a11,a12,...,a1n,a21,...,ann) ∈Rn2, and deﬁne the absolute
value |A| of A as the Euclidean distance

∑i,j a2
ij of this point from 0 in
Rn2. We similarly deﬁne the absolute value of n× n complex and quater-
nion matrices by interpreting them as points of R2n2 and R4n2, respectively.
Then if we take the distance between matrices A and B of the same size
and type to be |A−B|, we can speak of a convergent sequence of matrices
A1,A2,A3,... with limit A, or of a continuous matrix-valued function A(t)
by using the usual deﬁnitions in terms of distance ε from the limit.
Topology gives a general language for the discussion of limits and con-
tinuity by expressing them in terms of open sets.
Open and closed sets
To be able to express the idea of a “neighborhood” concisely we introduce
the notation Nε(P) for the open ε-ball with center P, that is,
Nε(P) = {Q ∈Rk : |P−Q| < ε}.

162
8
Topology
The set Nε(P) is also called the ε-neighborhood of P.
A set O ⊆Rk is called open if, along with any point P ∈O, there is an
ε-neighborhood Nε(P) ⊆O for some ε > 0. Three properties of open sets
follow almost immediately from this deﬁnition.6
1. Both Rk and the empty set {} are open.
2. Any union of open sets is open.
3. The intersection of two (and hence any ﬁnite number of) open sets is
open.
The third property holds because if P ∈O1 and P ∈O2 then we have
P ∈Nε1(P) ⊆O1
and
P ∈Nε2(P) ⊆O2,
so P ∈Nε(P) ⊆O1 ∩O2, where ε is the minimum of ε1 and ε2.
Open sets are the fundamental concept of topology, and all other topo-
logical concepts can be deﬁned in terms of them. For example, a closed
set7 F is one whose complement Rk −F is open. It follows from prop-
erties 1, 2, 3 of open sets that we have the following properties of closed
sets:
1. Both Rk and the empty set {} are closed.
2. Any intersection of closed sets is closed.
3. The union of two (and hence any ﬁnite number of) closed sets is
closed.
The reason for calling such sets “closed” is that they are closed under the
operation of adding limit points. A limit point of a set S is a point P
such that every ε-neighborhood of P includes points of S . A closed set F
includes all its limit points P. This is so because if P is a point not in F
then P is in the open complement Rk −F and hence P has a neighborhood
Nε(P) ⊆Rk −F. But then Nε(P) does not include any points of F, so P
is not a limit point of F.
6In general topology, where Rk is replaced by an arbitrary set S , these three properties
deﬁne what is called a collection of open sets. In general topology there need be no under-
lying concept of “distance,” hence open sets cannot always be deﬁned in terms of ε-balls.
We will make use of the concept of distance where it is convenient, but it will be noticed
that the general topological properties of open sets frequently give a natural proof.
7It is traditional to denote closed sets by the initial letter of “ferm´e,” the French word
for “closed.”

8.1
Open and closed sets in Euclidean space
163
The relative topology
Many spaces S other than Rk have a notion of distance, so the deﬁnition
of open and closed sets in terms of ε-balls may be carried over directly. In
particular, if S is a subset of some Rk we have:
• The ε-balls of S , Nε(P) = {Q ∈S : |P−Q| < ε}, are the intersec-
tions of S with ε-balls of Rk.
• So the open subsets of S are the intersections of S with the open
subsets of Rk.
• So the closed subsets of S are the intersections of S with the closed
subsets of Rk.
The topology resulting from this deﬁnition of open set is called the relative
topology on S . It is important at a few places in this chapter, notably for
the deﬁnition of a matrix Lie group in the next section.
Notice that S is automatically a closed set in the relative topology,
since it is the intersection of S with a closed subset of Rk, namely Rk
itself. This does not imply that S contains all its limit points; indeed, this
happens only if S is a closed subset of Rk.
Exercises
Open sets and closed sets are common in mathematics. For example, an open
interval (a,b) = {x ∈R : a < x < b} is an open subset of R and a closed interval
[a,b] = {x ∈R : a ≤x ≤b} is closed.
8.1.1 Show that a half-open interval [a,b) = {x : a ≤x < b} is neither open nor
closed.
8.1.2 With the help of Exercise 8.1.1, or otherwise, give an example of an inﬁnite
union of closed sets that is not closed.
8.1.3 Give an example of an inﬁnite intersection of open sets that is not open.
Since a random subset T of a space S may not be closed we sometimes
ﬁnd it convenient to introduce a closure operation that takes the intersection of all
closed sets F ⊇T :
closure(T ) = ∩{F ⊆S : F is closed and F ⊇T }.
8.1.4 Explain why closure(T ) is a closed set containing T .
8.1.5 Explain why it is reasonable to call closure(T ) the “smallest” closed set
containing T .
8.1.6 Show that closure(T ) = T ∪{limit points of T } when T ⊆Rk.

164
8
Topology
8.2
Closed matrix groups
In Lie theory, closed sets are important from the beginning, because all
matrix Lie groups are closed sets in the appropriate topology. This has to do
with the continuity of matrix multiplication and the determinant function,
which we assume for now. In the next section we will discuss continuity
and its relationship with open and closed sets more thoroughly.
Example 1. The circle group S1 = SO(2).
Viewed as a set of points in C or R2, the unit circle is a closed set
because its complement (the set of points not on the circle) is clearly open.
Figure 8.1 shows a typical point P not on the circle and an ε-neighborhood
of P that lies in the complement of the circle. The open neighborhood of P
is colored gray and its perimeter is drawn dotted to indicate that boundary
points are not included.
P
Figure 8.1: Why the complement of the circle is open.
Example 2. The groups O(n) and SO(n).
We view O(n) as a subset of the space Rn2 of n×n real matrices, which
we also call Mn(R). The complement of O(n) is
Mn(R)−O(n) = {A ∈Mn(R) : AAT ̸= 1}.
This set is open because if A is a matrix in Mn(R) with AAT ̸= 1 then
some entries of AAT are unequal to the corresponding entries (1 or 0) in 1.
It follows, since matrix multiplication and transpose are continuous, that
BBT also has entries unequal to the corresponding entries of 1 for any B
sufﬁciently close to A. Thus some ε-neighborhood of A is contained in
Mn(R)−O(n), so O(n) is closed.

8.2
Closed matrix groups
165
Matrices A in SO(n) satisfy the additional condition det(A) = 1. The
matrices A not satisfying this condition form an open set because det is
a continuous function. Namely, if det(A) ̸= 1, then det(B) ̸= 1 for any B
sufﬁciently close to A; hence any A in the set where det ̸= 1 has a whole
ε-neighborhood in this set. Thus the matrices A for which det(A) = 1 form
a closed set. The group SO(n) is the intersection of this closed set with the
closed set O(n), hence SO(n) is itself closed.
Example 3. The group Aff(1).
We view Aff(1) as in Section 4.6, namely, as the group of real matrices
of the form A =
a b
0 1

, where a,b ∈R and a > 0. It is now easy to see that
the group is not closed, because it contains the sequence
An =
1/n
0
0
1

,
whose limit
0 0
0 1

is not in Aff(1). However, Aff(1) is closed in the “rel-
ative” sense: as a subset of the largest 2× 2 matrix group that contains it.
This is because Aff(1) is the intersection of a closed set—the set of ma-
trices
 a b
0 1

with a ≥0—with the set of all invertible 2× 2 matrices. This
brings us to our next example.
Example 4. The general linear group GL(n,C).
The group GL(n,C) is the set of all invertible n×n complex matrices.
This set is a group because it is closed under products (since A−1B−1 =
(BA)−1) and under inverses (obviously). It follows that every group of real
or complex matrices is a subgroup of some GL(n,C),8 which is why we
bring it up now. We are about to deﬁne what a “matrix Lie group” is, and
we wish to say that it is some kind of subgroup of GL(n,C).
But ﬁrst notice that GL(n,C) is not a closed subset of the space Mn(C)
of n×n complex matrices. Indeed, if 1 is the n×n identity matrix, then the
matrices 1/2,1/3,1/4,... all belong to GL(n,C) but their limit 0 does not.
We can say only that GL(n,C) is a closed subset of itself, and the deﬁnition
of matrix Lie group turns upon this appeal to the relative topology.
8GL(n,C) was called “Her All-embracing Majesty” by Hermann Weyl in his book The
Classical Groups. Notice that quaternion groups may also be viewed as subgroups of
GL(n,C), thanks to the identiﬁcation of quaternions with certain 2 × 2 complex matrices
in Section 1.3.

166
8
Topology
Matrix Lie groups
With the understanding that the topology of all matrix groups should be
considered relative to GL(n,C), we make the following deﬁnition:
Deﬁnition. A matrix Lie group is a closed subgroup of GL(n,C).
This deﬁnition is beautifully simple, but still surprising. Lie groups are
supposed to be “smooth,” yet closed sets are not usually smooth (think of a
square or a triangle, say). Apparently the group operation has a “smooth-
ing” effect. And again, there are some closed subgroups of GL(n,C) that
do not even look smooth, for example the group {1} consisting of a single
point! The worry about {1} disappears when one takes a sufﬁciently gen-
eral deﬁnition of “smoothness,” as explained in Section 5.8. The real secret
of smoothness is the matrix exponential function, as we saw in Section 7.3.
Exercises
8.2.1 Prove that U(n), SU(n), and Sp(n) are closed subsets of the appropriate
matrix spaces.
The general linear group GL(n,C) is usually introduced alongside the special
linear group. Both are subsets of the space Mn(C) of complex n ×n matrices.
GL(n,C) = {A : det(A) ̸= 0}
and
SL(n,C) = {A : det(A) = 1}.
8.2.2 Show that GL(n,C) is an open subset of Mn(C).
8.2.3 Show that SL(n,C) is a closed subset of Mn(C).
8.2.4 If H is an arbitrary subgroup of a matrix Lie group G, show that
{sequential tangents of H} = T1(closure(H)).
8.3
Continuous functions
As in elementary analysis, we deﬁne a function f to be continuous at a
point A if, for each ε > 0, there is a δ > 0 such that
|B−A| < δ ⇒| f(B)−f(A)| < ε.
If the points A and B belong to Rk and the values f(A) and f(B) belong
to Rl then the ε-δ condition can be restated as follows: for each ε-ball
Nε( f(A)) there is a δ-ball Nδ(A) such that
{f(B) : B ∈Nδ(A)} ⊆Nε(f(A)).
(*)

8.3
Continuous functions
167
It is convenient and natural to introduce the abbreviations
f(S )
for
{f(B) : B ∈S },
f −1(S )
for
{B : f(B) ∈S }.
Then the condition (*) can be restated: f is continuous at A if, for each
ε > 0, there is a δ > 0 such that
f(Nδ(A)) ⊆Nε( f(A)).
Finally, if f is continuous for some domain of argument values A and some
range of function values f(A) then, for each open subset O of the range of
f, we have
f −1(O)
is open.
(**)
This is because f −1(O) contains, along with each point A, a neighborhood
Nδ(A) of A, mapped by f into an neighborhood Nε(f(A)) of f(A), con-
tained in the open set O along with f(A).
Condition (**) is equivalent to condition (*) in spaces such as Rk, and it
serves as the deﬁnition of a continuous function in general topology, since
it is phrased in terms of open sets alone.
Basic continuous functions
As one learns in elementary analysis, the basic functions of arithmetic are
continuous at all points at which they are deﬁned. Also, composites of con-
tinuous functions are continuous. For example, the composite of addition,
subtraction, and division given by
f(a,b) = a+b
a−b
is continuous for all pairs (a,b) at which it is deﬁned—that is, for all pairs
such that a ̸= b.
A matrix function f is called continuous at A if it satisﬁes the ε-δ
deﬁnition for absolute value of matrices. That is, for all ε there is a δ such
that
|B−A| < δ ⇒| f(B)−f(A)| < ε.
This is equivalent to being a continuous numerical function of the matrix
entries. Important examples for Lie theory are the matrix product and the
determinant, both of which are continuous because they are built from ad-
dition and multiplication of numbers. The matrix inverse f(A) = A−1 is

168
8
Topology
also a continuous function of A, built from addition, multiplication, and di-
vision (by det(A)). It is deﬁned for all A with det(A) ̸= 0, which of course
are also the A for which A−1 exists.
Homeomorphisms
Continuous functions might be considered the “homomorphisms” of topol-
ogy, but if so an “isomorphism” is not simply a 1-to-1 homomorphism. A
topological isomorphism should also have a continuous inverse. A contin-
uous function f such that f −1 exists and is continuous is called a homeo-
morphism. We will also call such a 1-to-1 correspondence, continuous in
both directions, a continuous bijection.
We must speciﬁcally demand a continuous inverse because the inverse
of a continuous 1-to-1 function is not necessarily continuous. The simplest
example is the map from the half-open interval [0,2π) to the circle deﬁned
by f(θ) = cosθ +isinθ (Figure 8.2).
[
)
−→
f
[)
Figure 8.2: The interval and the circle.
This map f is clearly continuous and 1-to-1, but f −1 is not continuous.
For example, f −1(O), where O is a small open arc of the circle between
angle −α and α, is (2π −α,2π)∪[0,α), which is not an open set. (More
informally, f −1 sends points that are near each other on the circle to points
that are far apart on the interval.)
It is clear that f is not an “isomorphism” between [0,2π) and the circle,
because the two spaces have different topological properties. For example,
the circle is compact but [0,2π) is not. (For the deﬁnition of compactness,
see the next section.)
Exercises
If homeomorphisms are the “isomorphisms” of topological spaces, what operation
do they preserve? The answer is that homeomorphisms are the 1-to-1 functions f
that preserve closures, where “closure” is deﬁned in the exercises to Section 8.1:
f(closure(S )) = closure(f(S )).

8.4
Compact sets
169
8.3.1 Show that if P is a limit point of S and f is a continuous function deﬁned
on S and P, then f(P) is a limit point of f(S ).
8.3.2 If f is a continuous bijection, deduce from Exercise 8.3.1 that
f(closure(S )) = closure(f(S )).
8.3.3 Give examples of continuous functions f on subsets of R such that f(open)
is not open and f(closed) is not closed.
8.3.4 Also, give an example of a continuous function f on R and a set S such
that
f(closure(S )) ̸= closure(f(S )).
8.4
Compact sets
A compact set in Rk is one that is closed and bounded. Compact sets
are somewhat better behaved than unbounded closed sets; for example, on
a compact set a continuous function is uniformly continuous, and a real-
valued continuous function attains a maximum and a minimum value. One
learns these results in an introductory real analysis course, but we will
prove one version of uniform continuity below. In Lie theory, compact
groups are better behaved than noncompact ones, and fortunately most of
the classical groups are compact.
We already know from Section 8.2 that O(n) and SO(n) are closed. To
see why they are compact, recall from Section 3.1 that the columns of any
A ∈O(n) form an orthonormal basis of Rn. This implies that the sum of
the squares of the entries in any column is 1, hence the sum of the squares
of all entries is n. In other words, |A| = √n, so O(n) is a closed subset of
Rn2 bounded by radius √n.
There are similar proofs that U(n), SU(n), and Sp(n) are compact.
Compactness may also be deﬁned in terms of open sets, and hence it is
meaningful in spaces without a concept of distance. The deﬁnition is moti-
vated by the following classical theorem, which expresses the compactness
of the unit interval [0,1] in terms of open sets.
Heine–Borel theorem. If [0,1] is contained in a union of open intervals
Ui, then the union of ﬁnitely many Ui also contains [0,1].
Proof. Suppose, on the contrary, that no ﬁnite union of the Ui contains
[0,1]. Then at least one of the subintervals [0,1/2] or [1/2,1] is not con-
tained in a ﬁnite union of Ui (because if both halves are contained in the
union of ﬁnitely many Ui, so is the whole).

170
8
Topology
Pick, say, the leftmost of the two intervals [0,1/2] and [1/2,1] not con-
tained in a ﬁnite union of Ui and divide it into halves similarly. By the same
argument, one of the new subintervals is not contained in a ﬁnite union of
the Ui, and so on.
By repeating this argument indeﬁnitely, we get an inﬁnite sequence of
intervals [0,1] = I1 ⊃I2 ⊃I3 ⊃···. Each In+1 is half the length of In
and none of them is contained in the union of ﬁnitely many Ui. But there
is a single point P in all the In (namely the common limit of their left and
right endpoints), and P ∈[0,1] so P is in some U j.
This is a contradiction, because a sufﬁciently small In containing P is
contained in U j, since U j is open. So in fact [0,1] is contained in the union
of ﬁnitely many Ui.
□
The general deﬁnition of compactness motivated by this theorem is the
following. A set K is called compact if, for any collection of open sets
Oi whose union contains K, there is a ﬁnite subcollection O1,O2,...,Om
whose union contains K . The collection of sets Oi is said to be an “open
cover” of K , and the subcollection O1,O2,...,Om is said to be a “ﬁnite
subcover,” so the deﬁning property of compactness is often expressed as
“any open cover contains a ﬁnite subcover.”
The argument used to prove the Heine–Borel theorem is known as the
“bisection argument,” and it easily generalizes to a “2k-section argument”
in Rk, proving that any closed bounded set has the ﬁnite subcover property.
For example, given a closed, bounded set K in R2, we take a square
that contains K and consider the subsets of K obtained by dividing the
square into four equal subsquares, then dividing the subsquares, and so on.
If K has no ﬁnite subcover, then the same is true of a nested sequence of
subsets with a single common point P, which leads to a contradiction as in
the proof for [0,1].
Exercises
The bisection argument is also effective in another classical theorem about the
unit interval: the Bolzano–Weierstrass theorem, which states that any inﬁnite set
of points {P1,P2,P3,...} in [0,1] has a limit point.
8.4.1 Given an inﬁnite set of points {P1,P2,P3,...} in [0,1], conclude that at least
one of the subintervals [0,1/2], [1/2,1] contains inﬁnitely many of the Pi.
8.4.2 (Bolzano–Weierstrass). By repeated bisection, show that there is a point P
in [0,1], every neighborhood of which contains some of the points Pi.

8.5
Continuous functions and compactness
171
8.4.3 Generalize the argument of Exercise 8.4.2 to show that if K is a closed
bounded set in Rk containing an inﬁnite set of points {P1,P2,P3,...} then
K includes a limit point of {P1,P2,P3,...}.
(We used a special case of this theorem in Section 7.4 in claiming that an inﬁ-
nite sequence of points on the unit sphere has a limit point, and hence a convergent
subsequence.)
The generalized Bolzano–Weierstrass theorem of Exercise 8.4.3 may also be
proved very naturally using the ﬁnite subcover property of compactness. Suppose,
for the sake of contradiction, that {P1,P2,P3,...} is an inﬁnite set of points in a
compact set K, with no limit point in K. It follows that each point Q ∈K has an
open neighborhood N (Q) in K (the intersection of an open set with K ) free of
points Pi ̸= Q.
8.4.4 By taking a ﬁnite subcover of the cover of K by the sets N (Q), show that
the assumption leads to a contradiction.
Not all matrix Lie groups are compact.
8.4.5 Show that GL(n,C) and SL(n,C) are not compact.
8.5
Continuous functions and compactness
We saw in Section 8.3 and its exercises that continuous functions do not
necessarily preserve open sets or closed sets. However, they do preserve
compact sets, so this is another example of “better behavior” of compact
sets. The proof also shows the efﬁciency of the ﬁnite subcover property of
compactness.
Continuous image of a compact set. If K is compact and f is a contin-
uous function deﬁned on K then f(K ) is compact.
Proof. Given a collection of open sets Oi that covers f(K ), we have to
show that some ﬁnite subcollection O1,O2,...,On also covers f(K ).
Well, since f is continuous and Oi is open, we know that f −1(Oi) is
open by Property (**) in Section 8.3. Also, the open sets f −1(Oi) cover
K because the Oi cover f(K ). Therefore, by compactness of K , there
is a ﬁnite subcollection f −1(O1), f −1(O2),..., f −1(Om) that covers K.
But then O1,O2,...,On covers f(K ), as required.
□
It may be thought that a problem arises when the open sets Oi extend
outside f(K ), possibly outside the range of the function f. We avoid this
problem by considering only open subsets relative to K and f(K ), that
is, the intersections of open sets with K and f(K ). For such sets it is still

172
8
Topology
true that f −1(“open”) = “open” when f is continuous, and so the argument
goes through.
A convenient property of continuous functions on compact sets is uni-
form continuity. As always, a continuous f : S →T has the property
that for each ε > 0 there is a δ > 0 such that f maps a δ-neighborhood of
each point P ∈S into an ε-neighborhood of f(P) ∈T . We say that f is
uniformly continuous if δ depends only on ε, not on P.
Uniform continuity. If K is a compact subset of Rm and f : K →Rn is
continuous, then f is uniformly continuous.
Proof. Since f is continuous, for any ε > 0 and any P ∈K there is a neigh-
borhood Nδ(P)(P) mapped by f into Nε/2( f(P)). To create some room to
move later, we cover K with the half-sized neighborhoods Nδ(P)/2(P),
then apply compactness to conclude that K is contained in some ﬁnite
union of them, say
K ⊆Nδ(P1)/2(P1)∪Nδ(P2)/2(P2)∪···∪Nδ(Pk)/2(Pk).
If we let
δ = min{δ(P1)/2,δ(P2)/2,...,δ(Pk)/2},
then each point in K lies in a set Nδ(Pi)/2(Pi) and each of the sets Nδ(Pi)(Pi)
has radius at least 2δ. I claim that |Q−R| < δ implies | f(Q)−f(R)| < ε
for any Q,R ∈K , so f is uniformly continuous on K .
To see why, take any Q,R ∈K such that |Q−R| < δ and a half-sized
neighborhood Nδ(Pi)/2(Pi) that includes Q. Then
|Pi −Q| < δ
and
|Q−R| < δ,
so it follows by the triangle inequality that
|Pi −R| < 2δ,
and hence
R ∈Nδ(Pi)(Pi).
Also, it follows from the deﬁnition of Nδ(Pi)(Pi) that | f(Pi)−f(Q)| < ε/2
and | f(Pi)−f(R)| < ε/2, so
| f(Q)−f(R)| < ε,
again by the triangle inequality.
□

8.6
Paths and path-connectedness
173
Exercises
The above proof of uniform continuity is complicated by the possibility that K is
at least two-dimensional. This forces us to use triangles and the triangle inequality.
If we have K = [0,1] then a more straightforward proof exists.
8.5.1 Suppose that Nδ(P1)(P1) ∪Nδ(P2)(P2) ∪··· ∪Nδ(Pk)(Pk) is a ﬁnite union of
open intervals that contains [0,1].
Use the ﬁnitely many endpoints of these intervals to deﬁne a number δ > 0
such that any two points P,Q ∈[0,1] with |P −Q| < δ lie in the same
interval Nδ(Pi)(Pi).
8.5.2 Deduce from Exercise 8.5.1 that any continuous function on [0,1] is uni-
formly continuous.
8.6
Paths and path-connectedness
The idea of a “curve” or “path” has evolved considerably over the course
of mathematical history. The old term locus (meaning place in Latin),
shows that a curve was once considered to be the (set of) places occupied
by points satisfying a certain geometric condition. For example, a circle is
the locus of points at a constant distance from a particular point, the center
of the circle. Later, under the inﬂuence of dynamics, a curve came to be
viewed as the orbit of a point moving according to some law of motion,
such as Newton’s law of gravitation. The position p(t) of the moving point
at any time t is some continuous function of t.
In topology today, we take the function itself to be the curve. That is,
a curve or path in a space S is a continuous function p : [0,1] →S . The
interval [0,1] plays the role of the time interval over which the point is in
motion—any interval would do as well, and it is sometimes convenient to
allow arbitrary closed intervals, as we will do below. More importantly,
the path is the function p and not just its image. A case in which the
image fails quite spectacularly to reﬂect the function is the space-ﬁlling
curve discovered by Peano in 1890. The image of Peano’s curve is a square
region of the plane, so the image cannot tell us even the endpoints A = f(0)
and B = f(1) of the curve, let alone how the curve makes its way from A
to B.
In Lie theory, paths give a way to distinguish groups that are “all of a
piece,” such as the circle group SO(2), from groups that consist of “sep-
arate pieces,” such as O(2). In Chapter 3 we showed connectedness by

174
8
Topology
describing speciﬁc paths. In the present chapter we wish to discuss paths
more generally, so we introduce the following general deﬁnitions.
Deﬁnitions. A path in a set G is a continuous map p : I →G, where
I = [a,b] is some closed interval9 of real numbers. A set G is called path-
connected if, for any A,B ∈G, there is a path p : [a,b] →G with p(a) = A
and p(b) = B. If p is a path from A to B with domain [a,b] and q is a path
from B to C with domain [b,c] then we call the path p q deﬁned by
p q(t) =
 p(t)
if t ∈[a,b],
q(t)
if t ∈[b,c],
the concatenation of p and q.
Clearly, if there is a path p from A to B with domain [a,b] then there is
a path p′ from A to B with any closed interval as domain. Thus if there are
paths from A to B and from B to C we can always arrange for the domains
of these paths to be contiguous intervals, so the concatenation of the two
paths is deﬁned. Indeed, we can insist that all paths have domain [0,1], at
the cost of a slightly less natural deﬁnition of concatenation (this is often
done in topology books).
Whichever deﬁnition is chosen, one has the following consequences:
• If there is a path from A to B then there is a “reverse” path from B
to A. (If p with domain [0,1] is a path from A to B, consider the
function q(t) = p(1−t).)
• If there are paths in G from A to B, and from B to C, then there is a
path in G from A to C. (Concatenate.)
• If Go is the subset of G consisting of all A ∈G for which there is
a path from 1 to A, then Go is path-connected. (For any B,C ∈G,
concatenate the paths from B to 1 and from 1 to C.)
In a group G, the path-connected subset Go just described is called the
path-component of the identity, or simply the identity component. The set
Go has signiﬁcant algebraic properties. These properties were explored
in some exercises in Chapter 3, but the following theorem and its proof
develop them more precisely.
9We regret that mathematicians use the [ , ] notation for both closed intervals and Lie
brackets, but it should always be clear from the context which meaning is intended.

8.6
Paths and path-connectedness
175
Normality of the identity component. If Go is the identity component of
a matrix Lie group G, then Go is a normal subgroup of G.
Proof. First we prove that Go is a subgroup of G by showing that Go is
closed under products and inverses.
If A,B ∈Go then there are paths A(t) from 1 to A and B(t) from 1 to B.
Since matrix multiplication is continuous, AB(t) is a path in G from A to
AB, so it follows by concatenation of paths from 1 to A and from A to AB
that AB ∈Go. Similarly, A−1A(t) is a path in G from A−1 to 1, so it follows
by path reversal that A−1 is also in Go.
To prove that Go is normal we need to show that AGoA−1 = Go for each
A ∈G. It sufﬁces to prove that AGoA−1 ⊆Go for each A ∈G, because in
that case we have Go ⊆A−1GoA (multiplying the containment on the left
by A−1 and on the right by A), and hence also Go ⊆AGoA−1 (replacing the
arbitrary A by A−1).
It is true that AGoA−1 ⊆Go, because AGoA−1 is a path-connected set—
the image of Go under the continuous maps of left and right multiplication
by A and A−1—and it includes the identity element of G as A1A−1.
□
It follows from this theorem that a non-discrete matrix Lie group is not
simple unless it is path-connected. We know from Chapter 3 that O(n) is
not path-connected for any n, and that SO(n), SU(n), and Sp(n) are path-
connected for all n. Another interesting case, whose proof occurs as an
exercise on p. 49 of Hall [2003], is the following.
Path-connectedness of GL(n,C)
Suppose that A and B are two matrices in GL(n,C), so det(A) ̸= 0 and
det(B) ̸= 0. We wish to ﬁnd a path from A to B in GL(n,C), that is, through
the n×n complex matrices with nonzero determinant.
We look for this path among the matrices of the form (1 −z)A + zB,
where z ∈C. These matrices form a plane, parameterized by the complex
coordinate z, and the plane includes A at z = 0 and B at z = 1. The path
from A to B has to avoid matrices (1−z)A+zB for which
det((1−z)A+zB) = 0.
(*)
Now (1 −z)A + zB is an n × n complex matrix whose entries are linear
terms in z. Its determinant is therefore a polynomial of degree at most n in
z and so, by the fundamental theorem of algebra, equation (*) has at most
n roots.

176
8
Topology
These roots represent n points in the plane of matrices (1−z)A + zB,
not including the points A and B. This allows us to ﬁnd a path, from A to B
in the plane, avoiding the points with determinant zero, as required. Thus
GL(n,C) is path-connected.
□
Generating a path-connected group from a neighborhood of 1
In Section 7.5 we claimed that any path-connected group matrix Lie group
is generated by a neighborhood Nδ(1) of 1, that is, any element of G is a
product of members of Nδ(1). We can now prove this theorem with the
help of compactness.
Generating a path-connected group. If G is a path-connected matrix Lie
group, and Nδ(1) is a neighborhood of 1 in G, then any element of G is a
product of members of Nδ(1).
Proof. Since G is path-connected, for any A ∈G there is a path A(t) in
G with A(0) = 1 and A(1) = A. Also, for each t, multiplication by A(t)
is a continuous map with a continuous inverse (namely, multiplication by
A(t)−1). Hence, if O is any open set that includes 1, the set
A(t)O = {A(t)B : B ∈O}
is an open set that includes the point A(t). As t runs from 0 to 1 the open
sets A(t)O cover the image of the path A(t), which is the continuous image
of the compact set [0,1], hence compact by the ﬁrst theorem in Section 8.5.
So in fact the image of the path lies in a ﬁnite union of sets,
A(t1)O ∪A(t2)O ∪···∪A(tk)O.
We can therefore ﬁnd points 1 = A1,A2,...,Am = A on the path A(t)
such that, for any i, Ai and Ai+1 lie in the same set A(tj)O. Notice that
A = A1 ·A−1
1 A2 ·A−1
2 A3 ·····A−1
m−1Am.
We can arrange that each factor of this product is in Nδ(1) by taking O to be
a subset of Nδ(1) small enough that B−1
i Bi+1 ∈Nδ(1) for any Bi,Bi+1 ∈O.
Then for each i we have
A−1
i Ai+1 = (A(tj)Bi)−1A(tj)Bi+1
for some tj and some Bi,Bi+1 ∈O
= B−1
i Bi+1 ∈Nδ(1).
□

8.7
Simple connectedness
177
Corollary. If G is a path-connected matrix Lie group then each element of
G has the form eX1eX2 ···eXm for some X1,X2,...,Xm ∈T1(G).
Proof. Rerun the proof above with Nδ(1) chosen so that each element of
Nδ(1) has the form eX, as is permissible by the theorem of Section 7.4.
Then each factor in the product
A = A1 ·A−1
1 A2 ·A−1
2 A3 ·····A−1
m−1Am.
has the form eXi.
□
Exercises
The corollary brings to mind the element
−1 1
0 −1

of SL(2,C), shown not to be of
the form eX for X ∈T1(SL(2,C)) in Exercise 5.6.5.
8.6.1 Write
−1 1
0 −1

as the product of two matrices in SL(2,C) with entries 0, i,
or −i.
8.6.2 Deduce from Exercise 8.6.1 and Exercise 5.6.4 that
−1 1
0 −1

= eX1eX2 for
some X1,X2 ∈T1(SL(2,C)).
8.7
Simple connectedness
A space S is called simply connected if it is path-connected and, for any
two paths p and q in S from point A to point B, there is a deformation of
p to q with endpoints ﬁxed. A deformation (or homotopy) of a path p to
path q is a continuous function of two real variables,
d : [0,1]×[0,1] →S
such that
d(0,t) = p(t)
and
d(1,t) = q(t).
And the endpoints are ﬁxed if
d(s,0) = p(0) = q(0)
and
d(s,1) = p(1) = q(1)
for all s.
Here one views the ﬁrst variable as “time” and imagines a continuously
moving curve that equals p at time 0 and q at time 1. So d is a “deformation
from curve p to curve q.”

178
8
Topology
The restriction of d to the bottom edge of the square [0,1]×[0,1] is one
path p, the restriction to the top edge is another path q, and the restriction
to the various horizontal sections of the square is a “continuous series”
of paths between p and q. Figure 8.3 shows several of these sections, in
different shades of gray, and their images under some continuous map d.
These are “snapshots” of the deformation, so to speak.10
−→
d
Im(p)
Im(q)
A
B
Figure 8.3: Snapshots of a path deformation with endpoints ﬁxed.
Simple connectivity is easy to deﬁne, but is quite hard to demonstrate
in all but the simplest case, which is that of Rk. If p and q are paths in
Rk from A to B, then p and q may each be deformed into the line segment
AB, and hence into each other. To deform p, say, one can move the point
p(t) along the line segment from p(t) to the point (1−t)A+tB, traveling
a fraction s of the total distance along this line in time s.
The next-simplest case, that of Sk for k > 1, includes the important Lie
group SU(2) = Sp(1)—the S3 of unit quaternions. On the sphere there
is not necessarily a unique “line segment” from p(t) to the point we may
want to send it to, so the above argument for Rk does not work. One can
project Sk minus one point P onto Rk, and then do the deformation in Rk,
but projection requires a point P not in the image of p, and hence it fails
when p is a space-ﬁlling curve. To overcome the difﬁculty one appeals to
compactness, which makes it possible to show that any path may be divided
into a ﬁnite number of “small” pieces, each of which may be deformed on
10Deﬁning simple connectivity in terms of deformation of paths between any two points
A and B is convenient for our purposes, but there is a common equivalent deﬁnition in terms
of closed paths: S is simply connected if every closed path may be deformed to a point.
To see the equivalence, consider the closed path from A to B via p and back again via q.
(Or, strictly speaking, via the “inverse of path q” deﬁned by the function q(1−t).)

8.7
Simple connectedness
179
the sphere to a “line segment” (a great circle arc). This clears space on the
sphere that enables the projection method to work. For more details see the
exercises below.
Compactness is also important in proving that certain groups are not
simply connected. The most important case is the circle S1 = SO(2), which
we now study in detail, because the idea of “lifting,” introduced here, will
be important in Chapter 9.
The circle and the line
The function f(θ) = (cosθ,sinθ) maps R onto the unit circle S1. It is
called a covering of S1 by R and the points θ +2nπ ∈R are said to lie over
the point (cosθ,sinθ) ∈S1. This map is far from being 1-to-1, because
inﬁnitely many points of R lie over each point of S1. For example, the
points over (1,0) are the real numbers 2nπ for all integers n (Figure 8.4).
0
2π
4π
−2π
−4π
(1,0)
Figure 8.4: The covering of the circle by the line.
However, the restriction of f to any interval of R with length < 2π
is 1-to-1 and continuous in both directions, so f may be called a local
homeomorphism. Figure 8.4 shows an arc of S1 (in gray) of length < 2π
and all the intervals of R mapped onto it by f. The restriction of f to any
one of these gray intervals is a homeomorphism.
The local homeomorphism property of f allows us to relate path defor-
mations in S1 to path deformations in R, which are more easily understood.
The ﬁrst step is the following theorem, relating paths in S1 to paths in R by
a process called lifting.
Unique path lifting. Suppose that p is a path in S1 with initial point P,
and ˜P is a point in R over Q. Then there is a unique path ˜p in R such that
˜p(0) = ˜P and f ◦˜p = p. We call ˜p the lift of p with initial point ˜P.
Proof. The path p is a continuous function from [0,1] into S1, and hence it
is uniformly continuous by the theorem in Section 8.5. This means that we

180
8
Topology
can divide [0,1] into a ﬁnite number of subintervals, say I1,I2,...,Ik in
left-to-right order, each of which is mapped by p into an arc of S1 of length
< 2π. We let pj be the restriction of p to I j and allow the term “path” to
include all continuous functions on intervals of R.
Then, since f has a continuous inverse on intervals of length < 2π:
• There is a unique path ˜p1 : I1 →R, with initial point ˜P, such that
f ◦˜p1 = p1. Namely, ˜p1(t) = f −1(p1(t)), where f −1 is the inverse
of f in the neighborhood of ˜P. Let the ﬁnal point of ˜p1 be ˜P1.
• Similarly, there is a unique path ˜p2 : I2 →R, with initial point ˜P1,
such that f ◦˜p2 = p2, and with ﬁnal point ˜P2 say.
• And so on.
The concatenation of these paths ˜pj in R is the lift ˜p of p with initial point
˜P.
□
There is a similar proof of “unique deformation lifting” that leads to
the following result. Suppose p and q are paths from A to B in S1 and p is
deformable to q with endpoints ﬁxed. Then the lift ˜p of p with initial point
˜A is deformable to the lift ˜q of q with initial point ˜A with endpoints ﬁxed.
Now we are ﬁnally ready to prove that S1 is not simply connected. In
particular, we can prove that the upper semicircle path
p(t) = (cosπt,sinπt)
from (1,0) to (−1,0)
is not deformable to the lower semicircle path
q(t) = (cos(−πt),sin(−πt))
from (1,0) to (−1,0).
This is because the lift ˜p of p with initial point 0 has ﬁnal point π, whereas
the lift ˜q of q with initial point 0 has ﬁnal point −π. Hence there is no de-
formation of ˜p to ˜q with the endpoints ﬁxed, and therefore no deformation
of p to q with endpoints ﬁxed.
□
Exercises
To see why the spheres Sk with k > 1 are simply connected, ﬁrst consider the
ordinary sphere S2.
8.7.1 Explain why, in a sufﬁciently small region of S2, there is a unique “line”
between any two points.

8.7
Simple connectedness
181
8.7.2 Use the uniqueness of “lines” in a small region R to deﬁne a deformation
of any curve p from A to B in R to the “line” from A to B.
Exercises 8.7.1 and 8.7.2, together with uniform continuity, allow any curve p on
S2 to be deformed into a “spherical polygon,” which can then be projected onto a
curve on the plane.
It is geometrically obvious that there is a homeomorphism from S2 −{P}
onto R2 for any point P ∈S2. Namely, choose coordinates so that P is the north
pole (0,0,1) and map S2 −{P} onto R2 by stereographic projection, as shown in
Figure 8.5.
P
Figure 8.5: Stereographic projection.
To generalize this idea to any Sk we have to describe stereographic projection
algebraically. So consider the Sk in Rk+1, deﬁned by the equation
x2
1 +x2
2 +···+x2
k+1 = 1.
We project Sk stereographically from the “north pole” P = (0,0,...,0,1) onto the
subspace Rk with equation xk+1 = 0.
8.7.3 Verify that the line through P and any other point (a1,a2,...,ak+1) ∈Sk has
parametric equations
x1 = a1t,
x2 = a2t,
...,
xk = akt,
xk+1 = 1 +(ak+1 −1)t.
8.7.4 Show that the line in Exercise 8.7.3 meets the hyperplane xk+1 = 0 where
x1 =
a1
1 −ak+1
,
x2 =
a2
1 −ak+1
,
...,
xk =
ak
1 −ak+1
.
8.7.5 By solving the equations in Exercise 8.7.4, or otherwise, show that
a1 =
2x1
x2
1 +···+x2
k +1,
...,
ak =
2xk
x2
1 +···+x2
k +1,
and
ak+1 = x2
1 +···+x2
k −1
x2
1 +···+x2
k +1.
Hence conclude that stereographic projection is a homeomorphism.

182
8
Topology
8.8
Discussion
Closed, connected sets can be extremely pathological, even in R2. For
example, consider the set called the Sierpinski carpet, which consists of
the unit square with inﬁnitely many open squares removed. Figure 8.6
shows what it looks like after several stages of construction. The original
unit square was black, and the white “holes” are where squares have been
removed. In reality, the total area of removed squares is 1, so the carpet is
“almost all holes.” Nevertheless, it is a closed, path-connected set.
Figure 8.6: The Sierpinski carpet
Remarkably, imposing the condition that the closed set be a continu-
ous group removes any possibility of pathology, at least in the spaces of
n× n matrices. As von Neumann [1929] showed, a closed subgroup G of
GL(n,C) has a neighborhood of 1 that can be mapped to a neighborhood
of 0 in some Euclidean space by the logarithm function, so G is certainly
not full of holes. Also G is smooth, in the sense of having a tangent space
at each point.
Thus in the world of matrix groups it is possible to avoid the technical-
ities of smooth manifolds and work with the easier concepts of open sets,
closed sets, and continuous functions.
In this book we avoid the concept of smooth manifold; indeed, this is
one of the great advantages of restricting attention to matrix Lie groups.
But we have, of course, investigated “smoothness” as manifested by the

8.8
Discussion
183
existence of a tangent space at the identity (and hence at every point) for
each matrix Lie group G. As we saw in Chapter 7, every matrix Lie group
G has a tangent space T1(G) at the identity, and T1(G) equals some Rk.
Even ﬁnite groups, such as G = {1}, have a tangent space at the identity;
not surprisingly it is the space R0.
Topology gives a way to describe all the matrix Lie groups with zero
tangent space: they are the discrete groups, where a group H is called
discrete if there is a neighborhood of 1 not containing any elements of G
except 1 itself. Every ﬁnite group is obviously discrete, but there are also
inﬁnite discrete groups; for example, Z is a discrete subgroup of R. The
groups Z and R can be viewed as matrix groups by associating each x ∈R
with the matrix
 1 x
0 1

(because multiplying two such matrices results in
addition of their x entries).
It follows immediately from the deﬁnition of discreteness that T1(H) =
{0} for a discrete group H. It also follows that if H is a discrete subgroup
of a matrix Lie group G then G/H is “locally isomorphic” to G in some
neighborhood of 1. This is because every element of G in some neighbor-
hood of 1 belongs to a different coset. From this we conclude that G/H
and G have the same tangent space at 1, and hence the same Lie alge-
bra. This result shows, once again, why Lie algebras are simpler than Lie
groups—they do not “see” discrete subgroups.
Apart from the existence of a tangent space, there is an algebraic reason
for including the discrete matrix groups among the matrix Lie groups: they
occur as kernels of “Lie homomorphisms.” Since everything in Lie theory
is supposed to be smooth, the only homomorphisms between Lie groups
that belong to Lie theory are the smooth ones. We will not attempt a general
deﬁnition of smooth homomorphism here, but merely give an example: the
map Φ : R →S1 deﬁned by
Φ(θ) = eiθ.
This is surely a smooth map because Φ is a differentiable function of θ.
The kernel of this Φ is the discrete subgroup of R (isomorphic to Z) con-
sisting of the integer multiples of 2π. We would like any natural aspect of a
Lie “thing” to be another Lie “thing,” so the kernel of a smooth homomor-
phism ought to be a Lie group. This is an algebraic reason for considering
the discrete group Z to be a Lie group.
The concepts of compactness, path-connectedness, simple connected-
ness, and coverings play a fundamental role in topology, as a glance at any

184
8
Topology
topology book will show. Their role in Lie theory is also fundamental, and
in fact Lie theory provides some of the best illustrations of these concepts.
The covering of S1 by R is one, and we will see more in the next chapter.
Closed paths in SO(3)
The group SO(3) of rotations of R3 is a striking example of a matrix Lie
group that is not simply connected. We exhibit a closed path in SO(3) that
cannot be deformed to a point in an informal demonstration known as the
“plate trick.”
Imagine carrying a plate of soup in one hand, keeping the plate hori-
zontal to avoid spilling. Now rotate the plate through 360◦, returning it to
its original position in space (ﬁrst three pictures in Figure 8.7).
Figure 8.7: The plate trick.
The series of positions of the plate up to this stage may be regarded as
a continuous path in SO(3). This is because each position is determined by
an “axis” in R3 (the vector from the shoulder to the hand) and an “angle”
(the angle through which the plate has turned). This path in SO(3) is closed
because the initial and ﬁnal points are the same (axis, angle) pair. We can
“deform” the path by varying the position of the arm and hand between
the initial and ﬁnal positions. But it seems intuitively clear that we cannot

8.8
Discussion
185
deform the path to a single point—because the path creates a full twist in
the arm, which cannot be removed by varying the path between the initial
and ﬁnal positions.
However, traversing (a deformation of) the path again, as shown in
the last three pictures, returns the arm and hand to their initial untwisted
state! The topological meaning of this trick is that there is a closed path
p in SO(3) that cannot be deformed to a point, whereas p2 (the result of
traversing p twice) can be deformed to a point. This topological property,
appropriately called torsion, is actually characteristic of projective spaces,
of which SO(3) is one. As we saw in Sections 2.2 and 2.3, SO(3) is the
same as the real projective space RP3.

9
Simply connected Lie groups
PREVIEW
Throughout our exposition of Lie algebras we have claimed that the struc-
ture of the Lie algebra g of a Lie group G captures most, if not all, of the
structure of G. Now it is time to explain what, if anything, is lost when we
pass from G to g. The short answer is that topological information is lost,
because the tangent space g cannot reveal how G may “wrap around” far
from the identity element.
The loss of information is already apparent in the case of R, O(2), and
SO(2), all of which have the line as tangent space. A more interesting case
is that of O(3), SO(3), and SU(2), all of which have the Lie algebra so(3).
These three groups are not isomorphic, and the differences between them
are best expressed in topological language, because the differences persist
even if we distort O(3), SO(3), and SU(2) by continuous 1-to-1 maps.
First, O(3) differs topologically from SO(3) and SU(2) because it is
not path-connected; there are two points in O(3) not connected by a path
in O(3). Second, SU(2) differs topologically from SO(3) in being simply
connected; that is, any closed path in SU(2) can be shrunk to a point.
We elaborate on these properties of O(3), SO(3), and SU(2) in Sec-
tions 9.1 and 9.2. Then we turn to the relationship between homomor-
phisms of Lie groups and homomorphisms of Lie algebras: a Lie group ho-
momorphism Φ : G →H “induces” a Lie algebra homomorphism ϕ : g →h
and if G and H are simply connected then ϕ uniquely determines Φ. This
leads to a deﬁnitive result on the extent to which a Lie algebra g “deter-
mines” its Lie group G: all simply connected groups with the same Lie
algebra are isomorphic.
186
J. Stillwell, Naive Lie Theory, DOI: 10.1007/978-0-387-78214-0 9,
c⃝Springer Science+Business Media, LLC 2008

9.1
Three groups with tangent space R
187
9.1
Three groups with tangent space R
The groups O(2) and SO(2) have the same tangent space, namely the tan-
gent line at the identity in SO(2), because the elements of O(2) not in
SO(2) are far from the identity and hence have no inﬂuence on the tangent
space. Figure 9.1 gives a geometric view of the situation.
The group SO(2) is shown as a circle, because SO(2) can be modeled
by the circle {z : |z| = 1} in the plane of complex numbers. Its complement
O(2)−SO(2) is the coset R·SO(2), where R is any reﬂection of the plane
in a line through the origin. We can also view R · SO(2) as a circle (lying
somewhere in the space of 2× 2 real matrices), since multiplication by R
produces a continuous 1-to-1 image of SO(2). The circle O(2)−SO(2) is
disjoint from SO(2) because distinct cosets are always disjoint. In partic-
ular, O(2)−SO(2) does not include the identity, so the tangent to O(2) at
the identity is simply the tangent to SO(2) at 1:
T1(O(2)) = T1(SO(2)).
O(2)−SO(2)
SO(2)
1
T1(SO(2))
Figure 9.1: Tangent space of both SO(2) and O(2).
As a vector space, the tangent has the same structure as the real line
R (addition of tangent vectors is addition of numbers, and scalar multiples
are real multiples). The tangent also has a Lie bracket operation, but not an
interesting one, because XY = YX for X,Y ∈R, so
[X,Y] = XY −YX = 0
for all X,Y ∈R.
Another Lie group with the same trivial Lie algebra is R itself (under the
addition operation). It is clear that R is its own tangent space.

188
9
Simply connected Lie groups
Thus we have three Lie groups with the same Lie algebra: O(2), SO(2),
and R. These groups can be distinguished algebraically in various ways
(exercises), but the most obvious differences between them are topological:
• O(2) is not path-connected.
• SO(2) is path-connected but not simply connected, that is, there is a
closed path in SO(2) that cannot be continuously shrunk to a point.
• R is path-connected and simply connected.
Another difference is that both O(2) and SO(2) are compact, that is, closed
and bounded, and R is not.
As this chapter unfolds, we will see that the properties of compactness,
path-connectedness, and simple connectedness are crucial for distinguish-
ing between Lie groups with the same Lie algebra. These properties are
“squeezed out” of the Lie group G when we form its Lie algebra g, and
we need to put them back in order to “reconstitute” G from g. In partic-
ular, we will see in Section 9.6 that G can be reconstituted uniquely from
g if we know that G is simply connected. But before looking at simple
connectedness more closely, we study another example.
Exercises
9.1.1 Find algebraic properties showing that the groups O(2), SO(2), and R are
not isomorphic.
From the circle group S1 = SO(2) and the line group R we can construct three
two-dimensional groups as Cartesian products: S1 ×S1, S1 ×R, and R×R.
9.1.2 Explain why it is appropriate to call these groups the torus, cylinder, and
plane, respectively.
9.1.3 Show that the three groups have the same Lie algebra. Describe its under-
lying vector space and Lie bracket operation.
9.1.4 Distinguish the three groups algebraically and topologically.
9.2
Three groups with the cross-product Lie algebra
At various points in this book we have met the groups O(3), SO(3), and
SU(2), and observed that they all have the same Lie algebra: R3 with the
cross product operation. Their Lie algebra may also be viewed as the space

9.2
Three groups with the cross-product Lie algebra
189
Ri+Rj+Rk of pure imaginary quaternions, with the Lie bracket operation
deﬁned in terms of the quaternion product by
[X,Y] = XY −YX.
The groups O(3) and SO(3) differ in the same manner as O(2) and SO(2),
namely, SO(3) is path-connected and O(3) is not. In fact, SO(3) is the
connected component of the identity in O(3): the subset of O(3) whose
members are connected to the identity by paths.
Thus O(3) and SO(3) (like O(2) and SO(2)) have the same tangent
space at the identity simply because all members of O(3) near the identity
are members of SO(3). The reason that SO(3) and SU(2) have the same
tangent space is more subtle, and it involves a phenomenon not observed
among the one-dimensional groups O(2) and SO(2): the covering of one
compact group by another.
As we saw in Section 2.3, each element of SO(3) (a rotation of R3)
corresponds to an antipodal point pair ±q of unit quaternions. If we rep-
resent q and −q by 2 × 2 complex matrices, they are elements of SU(2).
It follows, as we observed in Section 6.1, that SO(3) and SU(2) have the
same tangent vectors at the identity. However, the 2-to-1 map of SU(2)
onto SO(3) that sends the two antipodal quaternions q and −q to the single
pair ±q creates a topological difference between SU(2) and SO(3).
The group SU(2) is the 3-sphere S3 of quaternions q at unit distance
from O in H = R4, and the 3-sphere is simply connected. To see why,
suppose p is a closed path in R3 and suppose that N is a point of S3 not on
p. There is a continuous 1-to-1 map of S3 −{N} onto R3 with a continuous
inverse, namely stereographic projection Π (see the exercises in Section
8.7). It is clear that the loop Π(p) can be continuously shrunk to a point
in R3, for example, by magnifying its size by 1−t at time t for 0 ≤t ≤1.
Hence the same is true of p by mapping the shrinking process back into S3
by Π−1.
In contrast, the space SO(3) of antipodal point pairs ±q, for q ∈S3,
is not simply connected. An informal explanation of this property is the
“plate trick” described in Section 8.8. More formally, consider a path ˜p(s)
in S3 that begins at 1 and ends at −1, that is, ˜p(0) = 1 and ˜p(1) = −1.
Then the point pairs ± ˜p(s) for 0 ≤s ≤1 form a closed path p in SO(3)
because ± ˜p(0) and ± ˜p(1) are the same point pair ±1. Now, if p can be
continuously shrunk to a point, then p can be shrunk to a point keeping the
initial point ±1 ﬁxed (consider the shrinking process relative to this point).

190
9
Simply connected Lie groups
It follows (by “deformation lifting” as in Section 8.7) that the correspond-
ing curve ˜p on S3 can be shrunk to a point, keeping its endpoints 1 and −1
ﬁxed. But this is absurd, because the latter are two distinct points.
To sum up, we have:
• The three compact groups O(3), SO(3), and SU(2) have the same
Lie algebra.
• SO(3) and SU(2) are connected but O(3) is not.
• SU(2) is simply connected but SO(3) is not.
The space SU(2) is said to be a double-covering of SO(3) because there
is a continuous 2-to-1 map of SU(2) onto SO(3) that is locally 1-to-1,
namely the map q →{±q}. This map is locally 1-to-1 because the only
point, other than q, that goes to {±q} is the point −q, and a sufﬁciently
small neighborhood of q does not include −q. Thus the quaternions q′ in a
sufﬁciently small neighborhood of q in SU(2) correspond 1-to-1 with the
pairs {±q′} in a neighborhood of {±q} in SO(3).
It turns out that all the groups SU(n) and Sp(n) are simply connected,
and all the groups SO(n) for n ≥3 are doubly covered by simply connected
groups. Thus simply connected groups arise naturally from the classical
groups. They are the “topologically simplest” among the groups with a
given Lie algebra. The other thing to understand is the relationship between
Lie group homomorphisms (such as the 2-to-1 map of SU(2) onto SO(3)
just mentioned) and Lie algebra homomorphisms. This is the subject of the
next section.
Exercises
A more easily visualized example of a non-simply-connected space with simply
connected double cover is the real projective plane RP2, which consists of the
antipodal point pairs ±P on the ordinary sphere S2. Consider the path p on S2
that goes halfway around the equator, from a point Q to its antipodal point −Q.
9.2.1 Explain why the corresponding path ±p on RP2, consisting of the point
pairs ±P for P ∈p, is a closed path on RP2.
9.2.2 Suppose that ±p can be deformed on RP2 to a single point. Draw a picture
that illustrates the effect of a small “deformation” of ±p on the correspond-
ing set of points on S2.
9.2.3 Explain why a deformation of ±p on RP2 to a single point implies a defor-
mation of p to a pair of antipodal points on S2, which is impossible.

9.3
Lie homomorphisms
191
9.3
Lie homomorphisms
In Section 2.2 we deﬁned a group homomorphism to be a map Φ : G →H
such that Φ(g1g2) = Φ(g1)Φ(g2) for all g1,g2 ∈G. In the case of Lie
groups G and H, where the group operation is smooth, it is appropriate that
Φ preserve smoothness as well, so we deﬁne a Lie group homomorphism
to be a smooth map Φ : G →H such that Φ(g1g2) = Φ(g1)Φ(g2) for all
g1,g2 ∈G.
Now suppose that G and H are matrix Lie groups, with Lie algebras
(tangent spaces at the identity) T1(G) = g and T1(H) = h, respectively. Our
fundamental theorem says that a Lie group homomorphism Φ : G →H
“induces” (in a sense made clear in the statement of the theorem) a Lie
algebra homomorphism ϕ : g →h, that is, a linear map that preserves the
Lie bracket.
The induced map ϕ is the “obvious” one that associates the initial ve-
locity A′(0) of a smooth path A(t) through 1 in G with the initial velocity
(Φ ◦A)′(0) of the image path Φ(A(t)) in H. It is not completely obvious
that this map is well-deﬁned; that is, it is not clear that if A(0) = B(0) = 1
and A′(0) = B′(0) then (Φ◦A)′(0) = (Φ◦B)′(0). But we can sidestep this
problem by deﬁning a smooth map Φ : G →H to be one for which the
correspondence A′(0) →(Φ◦A)′(0) is a well-deﬁned and linear map from
T1(G) to T1(H).
Then it remains only to prove that ϕ preserves the Lie bracket, and we
have already done most of this in proving the Lie algebra properties of the
tangent space in Section 5.4.
For the sake of brevity, we will use the term “Lie homomorphism” for
both Lie group homomorphisms and Lie algebra homomorphisms.
The induced homomorphism. For any Lie homomorphism Φ : G →H of
matrix Lie groups G, H, with Lie algebras g, h, respectively, there is a Lie
homomorphism ϕ : g →h such that
ϕ(A′(0)) = (Φ◦A)′(0)
for any smooth path A(t) through 1 in G.
Proof. Thanks to our deﬁnition of a smooth map Φ, it remains only to
prove that ϕ preserves the Lie bracket, that is,
ϕ[A′(0),B′(0)] = [ϕ(A′(0)),ϕ(B′(0))]

192
9
Simply connected Lie groups
for any smooth paths A(t), B(t) in G with A(0) = B(0) = 1.
We do this, as in Section 5.4, by considering the smooth path in G
Cs(t) = A(s)B(t)A(s)−1
for a ﬁxed value of s.
The Φ-image of this path in H is
Φ(Cs(t)) = Φ

A(s)B(t)A(s)−1
= Φ(A(s))·Φ(B(t))·Φ(A(s))−1
because Φ is a group homomorphism. As we calculated in Section 5.4,
C′
s(0) = A(s)B′(0)A(s)−1 ∈g,
so
ϕ(C′
s(0)) = d
dt

t=0
Φ(A(s))·Φ(B(t))·Φ(A(s))−1
= (Φ◦A)(s)·(Φ◦B)′(0)·(Φ◦A)(s)−1 ∈h.
As s varies, C′
s(0) traverses a smooth path in g and ϕ(C′
s(0)) traverses
a smooth path in h. Therefore, by the linearity of ϕ,
ϕ

tangent to C′
s(0) at s = 0

=

tangent to ϕ(C′
s(0)) at s = 0

.
(*)
Now we know from Section 5.4 that the tangent to C′
s(0) at s = 0 is
A′(0)B′(0)−B′(0)A′(0) = [A′(0),B′(0)].
A similar calculation shows that the tangent to ϕ(C′
s(0)) at s = 0 is
(Φ◦A)′(0)·(Φ◦B)′(0)−(Φ◦B)′(0)·(Φ◦A)′(0)
= [(Φ◦A)′(0),(Φ◦B)′(0)]
= [ϕ(A′(0)),ϕ(B′(0))].
So it follows from (*) that
ϕ[A′(0),B′(0)] = [ϕ(A′(0)),ϕ(B′(0))],
as required.
□
If Φ : G →H is a Lie isomorphism, then Φ−1 : H →G is also a Lie
isomorphism, and it maps any smooth path through 1 in H back to a smooth

9.3
Lie homomorphisms
193
path through 1 in G. Thus Φ maps the smooth paths through 1 in G onto
all the smooth paths through 1 in H, and hence ϕ is onto h.
It follows that the Lie homomorphism ϕ′ induced by Φ−1 is from h
into g. And since ϕ′ sends (Φ◦A)′(0) in h to (Φ−1 ◦Φ◦A)′(0), that is, to
A′(0), we have ϕ′ = ϕ−1. In other words, ϕ is an isomorphism of g onto
h, and so isomorphic Lie groups have isomorphic Lie algebras.
The converse statement is not true, but it is “nearly” true. In Section
9.6 we will show that groups G and H with isomorphic Lie algebras are
themselves isomorphic if they are simply connected. The proof uses paths
in G to “lift” a homomorphism from g in “small steps.” This necessitates
further study of paths and their compactness, which we carry out in the
next two sections.
The trace homomorphism revisited
In Sections 6.2 and 6.3 we have already observed that the map
Tr : g →C
of a real or complex Lie algebra g is a Lie algebra homomorphism. This
result also follows from the theorem above, because the trace is the Lie
algebra map induced by the det homomorphism for real or complex Lie
groups (Section 2.2) thanks to the formula
det(eA) = eTr(A)
of Section 5.3.
Exercises
Deﬁning a smooth map to be one that induces a linear map of the tangent space, so
that we don’t have to prove this fact, is an example of what Bertrand Russell called
“the advantages of theft over honest toil” (in his Introduction to Mathematical
Philosophy, Routledge 1919, p. 71). We may one day have to pay for it by having
to prove that some “obviously smooth” map really is smooth by showing that it
really does induce a linear map of the tangent space.
I made the deﬁnition of smooth map Φ : G →H mainly to avoid proving that
the map ϕ : A′(0) →(Φ ◦A)′(0) is well-deﬁned. (That is, if A′(0) = B′(0) then
(Φ◦A)′(0) = (Φ◦B)′(0).) If we assume that ϕ is well-deﬁned, then, to prove that
ϕ is linear, we need only assume that Φ maps smooth paths to smooth paths. The
proof goes as follows.
Consider the pathC(t) = A(t)B(t), where A(t) and B(t) are smooth paths with
A(0) = B(0) = 1. Then we know from Section 5.4 that C′(0) = A′(0)+B′(0).

194
9
Simply connected Lie groups
9.3.1 Using the fact that Φ is a group homomorphism, show that we also have
(Φ◦C)′(0) = (Φ◦A)′(0)+(Φ◦B)′(0).
9.3.2 Deduce from Exercise 9.3.1 that ϕ(A′(0)+B′(0)) = ϕ(A′(0))+ϕ(B′(0)).
9.3.3 Let D(t) = A(rt) for some real number r. Show that D′(0) = rA′(0) and
(Φ◦D)′(0) = r(Φ◦A)′(0).
9.3.4 Deduce from Exercises 9.3.2 and 9.3.3 that ϕ is linear.
9.4
Uniform continuity of paths and deformations
The existence of space-ﬁlling curves shows that a continuous image of the
unit interval [0,1] may be very “tangled.” Indeed, the image of an arbitrar-
ily short subinterval may ﬁll a whole square in the plane. Nevertheless, the
compactness of [0,1] ensures that the images of small segments of [0,1] are
“uniformly” small. This is formalized by the following theorem, an easy
consequence of the uniform continuity of continuous functions on compact
sets from Section 8.5.
Uniform continuity of paths. If p : [0,1] →Rn is a path, then, for any
ε > 0, it is possible to divide [0,1] into a ﬁnite number of subintervals,
each of which is mapped by p into an open ball of radius ε.
Proof. The interval [0,1] is compact, by the Heine–Borel theorem of Sec-
tion 8.4, so p is uniformly continuous by the theorem of Section 8.5. In
other words, for each ε > 0 there is a δ > 0 such that |p(Q)−p(R)| < ε
for any points Q,R ∈[0,1] such that |Q−R| < δ.
Now divide [0,1] into subintervals of length < δ and pick a point Q in
each subinterval (say, the midpoint). Each subinterval is mapped by p into
the open ball with center p(Q) and radius ε because, if R is in the same
subinterval as Q, we have |Q−R| < δ, and hence |p(Q)−p(R)| < ε.
□
The same proof applies in two dimensions, almost word for word.
Uniform continuity of path deformations. If d : [0,1] × [0,1] →Rn is a
path deformation, then, for any ε > 0, it is possible to divide the square
[0,1] × [0,1] into a ﬁnite number of subsquares, each of which is mapped
by d into an open ball of radius ε.
Proof. The square [0,1] × [0,1] is compact, by the generalized Heine–
Borel theorem of Section 8.4, so d is uniformly continuous by the theorem

9.5
Deforming a path in a sequence of small steps
195
of Section 8.5. In other words, for each ε > 0 there is a δ > 0 such that
|p(Q)−p(R)| < ε for any points Q,R ∈[0,1]×[0,1] such that |Q−R| < δ.
Now divide [0,1] × [0,1] into subsquares of diagonal < δ and pick a
point Q in each subsquare (say, the center). Each subsquare is mapped by
d into the open ball with center p(Q) and radius ε because, if R is in the
same subsquare as Q, we have |Q −R| < δ and hence |p(Q)−p(R)| < ε.
□
Exercises
9.4.1 Show that the function f(x) = 1/x is continuous, but not uniformly contin-
uous, on the open interval (0,1).
9.4.2 Give an example of continuous function that is not uniformly continuous
on GL(2,C).
9.5
Deforming a path in a sequence of small steps
The proof of uniform continuity of path deformations assumes only that d
is a continuous map of the square into Rn. We now need to recall how such
a map is interpreted as a “path deformation.” The restriction of d to the
bottom edge of the square is one path p, the restriction to the top edge is
another path q, and the restriction to the various horizontal sections of the
square is a “continuous series” of paths between p and q—a deformation
from p to q. Figure 9.2 shows the “deformation snapshots” of Figure 8.3
further subdivided by vertical sections of the square, thus subdividing the
square into small squares that are mapped to “deformed squares” by d.
−→
d
Im(p)
Im(q)
Figure 9.2: Snapshots of a path deformation.

196
9
Simply connected Lie groups
The subdivision of the square into small subsquares is done with the
following idea in mind:
• By making the subsquares sufﬁciently small we can ensure that their
images lie in ε-balls of Rn for any prescribed ε.
• The bottom edge of the unit square can be deformed to the top
edge by a ﬁnite sequence of deformations dij, each of which is the
identity map of the unit square outside a neighborhood of the (i, j)-
subsquare.
• It follows that if p can be deformed to q then the deformation can
be divided into a ﬁnite sequence of steps. Each step changes the
image only in a neighborhood of a “deformed square,” and hence in
an ε-ball.
To make this argument more precise, though without deﬁning the dij in
tedious detail, we suppose the effect of a typical dij on the (i, j)-subsquare
to be shown by the snapshots shown in Figure 9.3. In this case, the bottom
and right edges are pulled to the position of the left and top edges, respec-
tively, by “stretching” in a neighborhood of the bottom and right edges and
“compressing” in a neighborhood of the left and top. This deformation
will necessarily move some points in the neighboring subsquares (where
such subsquares exist), but we can make the affected region outside the
(i, j)-subsquare as small as we please. Thus dij is the identity outside a
neighborhood of, and arbitrarily close to, the (i, j)-subsquare.
Figure 9.3: Deformation dij of the (i, j)-subsquare.
Now, if the (1,1)-subsquare is the one on the bottom left and there are
n subsquares in each row, we can move the bottom edge to the top through
the sequence of deformations d11,d12,...,d1n,d2n,...,d21,d31, . . . . Figure
9.4 shows the ﬁrst few steps in this process when n = 4.
Since each dij is a map of the unit square into itself, equal to the identity
outside a neighborhood of an (i, j)-subsquare, the composite map d ◦dij
(“dij then d”) agrees with d everywhere except on a neighborhood of the

9.6
Lifting a Lie algebra homomorphism
197
. . .
Figure 9.4: Sequence deforming the bottom edge to the top.
image of the (i, j)-subsquare. Intuitively speaking, d ◦dij moves one side
of the image subsquare to the other, while keeping the image ﬁxed outside
a neighborhood of the image subsquare.
It follows that if d is a deformation of path p to path q and dij runs
through the sequence of maps that deform the bottom edge of the unit
square to the top, then the sequence of composite maps d ◦dij deforms
p to q, and each d ◦dij agrees with d outside a neighborhood of the image
of the (i, j)-subsquare, and hence outside an ε-ball.
In this sense, if a path p can be deformed to a path q, then p can be
deformed to q in a ﬁnite sequence of “small” steps.
Exercises
9.5.1 If a < 0 < 1 < b, give a continuous map of (a,b) onto (a,b) that sends 0 to
1. Use this map to deﬁne dij when the (i, j)-subsquare is in the interior of
the unit square.
9.5.2 If 1 < b give a continuous map of [0,b) onto [1,b) that sends 0 to 1, and
use it (and perhaps also the map in Exercise 9.5.1) to deﬁne dij when the
(i, j)-subsquare is one of the boundary squares of the unit square.
9.6
Lifting a Lie algebra homomorphism
Now we are ready to achieve the main goal of this chapter: showing that
if g and h are the Lie algebras of simply connected Lie groups G and H,
respectively, then each Lie algebra homomorphism ϕ : g →h is induced by
a Lie group homomorphism Φ : G →H. This is the converse of the theorem
in Section 9.3, and the two theorems together show that the structure of
simply connected Lie groups is completely captured by their Lie algebras.
The idea of the proof is to “lift” the homomorphism ϕ from g to G in small
pieces, with the help of the exponential function and the Campbell–Baker–
Hausdorff theorem of Section 7.7.

198
9
Simply connected Lie groups
We already know, from Section 7.4, that there is a neighborhood N
of 1 in G that is the 1-to-1 image of a neighborhood of 0 in g under the
exponential function. We also know, by Campbell–Baker–Hausdorff, that
the product of two elements eX,eY ∈G is given by a formula
eXeY = eX+Y+ 1
2 [X,Y]+further Lie bracket terms.
Therefore, if we deﬁne Φ on each element eX of G by Φ(eX) = eϕ(X), then
Φ(eXeY) = Φ

eX+Y+ 1
2 [X,Y]+further Lie bracket terms
= eϕ(X+Y+ 1
2 [X,Y]+further Lie bracket terms)
= e(ϕ(X)+ϕ(Y)+ 1
2 [ϕ(X),ϕ(Y)]+further Lie bracket terms)
because ϕ is a Lie algebra homomorphism
= eϕ(X)eϕ(Y)
by Campbell–Baker–Hausdorff
= Φ(eX)Φ(eY).
Thus Φ is a Lie group homomorphism, at least in the region N where
every element of G is of the form eX. However, not all elements of G are
necessarily of this form, so we need to extend Φ to an arbitrary A ∈G by
some other means. This is where we need the simple connectedness of G,
and we carry out a four-stage process, explained in detail below.
1. Connect A to 1 by a path, and show that there is a sequence of points
1 = A1,
A2,
...,
Am = A
along the path such that A1,A−1
1 A2,...,A−1
m−1Am all lie in N , and
hence such that all of Φ(A1),Φ(A−1
1 A2),...,Φ(A−1
m−1Am) are deﬁned.
Motivated by the fact that A = A1 ·A−1
1 A2 · ··· ·A−1
m−1Am, we let
Φ(A) = Φ(A1)Φ(A−1
1 A2)···Φ(A−1
m−1Am).
2. Show that Φ(A) does not change when the sequence A1,A2,...,Am is
“reﬁned” by inserting an extra point. Since any two sequences have
a common reﬁnement, obtained by inserting extra points, the value
of Φ(A) is independent of the sequence of points along the path.
3. Show that Φ(A) is also independent of the path from 1 to A, by show-
ing that Φ(A) does not change under a small deformation of the path.
(Simple connectedness of G ensures that any two paths from 1 to A
may be made to coincide by a sequence of small deformations.)

9.6
Lifting a Lie algebra homomorphism
199
4. Check that Φ is a group homomorphism, and that it induces the Lie
algebra homomorphism ϕ.
Stage 1. Finding a sequence of points 1 = A1,A2,...,Am = A.
In Section 8.6 we showed how to do this, under the title of “gen-
erating a path-connected group from a neighborhood of 1.” We found
1 = A1,A2,...,Am = A so that Ai and Ai+1 lie in the same set A(tj)O,
where O is an open subset of N small enough that C−1
i Ci+1 ∈N for
any Ci,Ci+1 ∈O.
Then Φ(A1) = Φ(1) is deﬁned, and so is Φ(A−1
i Ai+1) for each i.
Stage 2. Independence of the sequence along the path.
Suppose that A′
i is another point on the path A(t), in the same neigh-
borhood A(tj)O as Ai and Ai+1. When the sequence is reﬁned from
A1,...,Ai,Ai+1 ...,An
to
A1,...,Ai,A′
i,Ai+1 ...,Am,
the expression for Φ(A) is changed by replacing the factor Φ(A−1
i Ai+1) by
the two factors Φ(A−1
i A′
i)Φ(A′−1
i
Ai+1). Then both A−1
i A′
i and A′−1
i
Ai+1 are
in O, and so
Φ(A−1
i A′
i)Φ(A′−1
i
Ai+1) = Φ(A−1
i A′
iA′−1
i
Ai+1)
because Φ is a homomorphism on O
= Φ(A−1
i Ai+1).
Hence insertion of an extra point does not change the value of Φ(A).
Stage 3. Independence of the path.
Given paths p and q from 1 to A, we know that p can be deformed to q
because G is simply connected. Let d : [0,1]×[0,1] →G be a deformation
from p to q. Each point P in the unit square has a neighborhood
N(P) = {Q : d(P)−1d(Q) ∈N },
which is open by the continuity of d and matrix multiplication. Inside N(P)
we choose a square neighborhood S(P) with center P and sides parallel
to the sides of the unit square. Then the unit square is contained in the
union of these square neighborhoods, and hence in a ﬁnite union of them,
S(P1)∪S(P2)∪···∪S(Pk), by compactness.

200
9
Simply connected Lie groups
Let ε be the minimum side length of the ﬁnitely many rectangular over-
laps of the squares S(Pj) covering the unit square. Then, if we divide the
unit square into equal subsquares of some width less than ε, each subsquare
lies in a square S(Pj). Therefore, for any two points P, Q in the subsquare,
we have d(P)−1d(Q) ∈N .
This means that we can deform p to q by “steps” (as described in the
previous section) within regions of G where the point d(P) inserted or re-
moved in each step is such that d(P)−1d(Q) ∈N for its neighbor vertices
d(Q) on the path, so Φ(d(P)−1d(Q)) is deﬁned. Consequently, Φ can be
deﬁned along the path obtained at each step of the deformation, and we can
argue as in Stage 2 that the value of Φ does not change.
Stage 4. Veriﬁcation that Φ is a homomorphism that induces ϕ.
Suppose that A,B ∈G and that 1 = A1,A2,...,Am = A is a sequence of
points such that A−1
i Ai+1 ∈O for each i, so
Φ(A) = Φ(A1)Φ(A−1
1 A2)···Φ(A−1
m−1Am).
Similarly, let 1 = B1,B2,...,Bn = B be a sequence of points such that
B−1
i Bi+1 ∈O for each i, so
Φ(B) = Φ(B1)Φ(B−1
1 B2)···Φ(B−1
n−1Bn).
Now notice that 1 = A1,A2,...,Am = AB1,AB2,...,ABn is a sequence of
points, leading from 1 to AB, such that any two adjacent points lie in a
neighborhood of the form CO. Indeed, if the points Bi and Bi+1 both lie in
CO then ABi and ABi+1 both lie in ACO. It follows that
Φ(AB) = Φ(A1)Φ(A−1
1 A2)···Φ(A−1
m−1Am)
×Φ((AB1)−1AB2)Φ((AB2)−1AB3)···Φ((ABn−1)−1ABn)
= Φ(A1)Φ(A−1
1 A2)···Φ(A−1
m−1Am)
×Φ(B−1
1 B2)Φ(B−1
2 B3)···Φ(B−1
n−1Bn)
= Φ(A)Φ(B)
because Φ(B1) = Φ(1) = 1.
Thus Φ is a homomorphism.
To show that Φ induces ϕ it sufﬁces to show this property on N , be-
cause we have shown that there is only one way to extend Φ beyond N .
On N , Φ(A) = eϕ(log(A)), so for the path etX through 1 in G we have
d
dt

t=0
Φ(etX) = d
dt

t=0
etϕ(X) = ϕ(X).

9.7
Discussion
201
Thus Φ induces the Lie algebra homomorphism ϕ.
Putting these four stages together, we ﬁnally have the result:
Homomorphisms of simply connected groups. If g and h are the Lie
algebras of the simply connected Lie groups G and H, respectively, and if
ϕ : g →h is a homomorphism, then there is a homomorphism Φ : G →H
that induces ϕ.
□
Corollary. If G and H are simply connected Lie groups with isomorphic
Lie algebras g and h, respectively, then G is isomorphic to H.
Proof. Suppose that ϕ : g →h is a Lie algebra isomorphism, and let the
homomorphism that induces ϕ be Φ : G →H. Also, let Ψ : H →G be the
homomorphism that induces ϕ−1. It sufﬁces to show that Ψ = Φ−1, since
this implies that Φ is a Lie group isomorphism.
Well, it follows from the deﬁnition of the “lifted” homomorphisms that
Ψ◦Φ : G →G is the unique homomorphism that induces the identity map
ϕ−1 ◦ϕ : g →g, hence Ψ ◦Φ is the identity map on G. In other words,
Ψ = Φ−1.
□
9.7
Discussion
The ﬁnal results of this chapter, and many of the underlying ideas, are due
to Schreier [1925] and Schreier [1927]. In the 1920s, understanding of
the connections between group theory and topology grew rapidly, mainly
under the inﬂuence of topologists, who were interested in discrete groups
and covering spaces. Schreier was the ﬁrst to see clearly that topology is
important in Lie theory and that it separates Lie algebras from Lie groups.
Lie algebras are topologically trivial but Lie groups are generally not, and
Schreier introduced the concept of covering space to distinguish between
Lie groups with the same Lie algebra. He pointed out that every Lie group
G has a universal covering ˜G →G, the unique continuous local isomor-
phism of a simply connected group onto G. Examples are the homomor-
phisms R →S1 and SU(2) →SO(3). In general, the universal covering is
constructed by “lifting,” much as we did in the previous section.
The universal covering construction is inverse to the construction of the
quotient by a discrete group because the kernel of ˜G →G is a discrete sub-
group of ˜G, known to topologists as the fundamental group of G, π1(G).
Thus G is recovered from ˜G as the quotient ˜G/π1(G) = G. Another im-
portant result discovered by Schreier [1925] is that π1(G) is abelian for a

202
9
Simply connected Lie groups
Lie group G. This result strongly constrains the topology of Lie groups,
because the fundamental group of an arbitrary smooth manifold can be any
ﬁnitely presented group. A “random” smooth manifold has a nonabelian
fundamental group.
Like the quotient construction (see Section 3.9), the universal cover-
ing can produce a nonmatrix group ˜G from a matrix group G. A famous
example, essentially due to Cartan [1936], is the universal covering group

SL(2,C) of the matrix group SL(2,C). Thus topology provides another
path to the world of Lie groups beyond the matrix groups.
Topology makes up the information lost when we pass from Lie groups
to Lie algebras, and in fact topology makes it possible to bypass Lie alge-
bras almost entirely. A notable book that conducts Lie theory at the group
level is Adams [1969], by the topologist J. Frank Adams. It should be said,
however, that Adams’s approach uses topology that is more sophisticated
than the topology used in this chapter.
Finite simple groups
The classiﬁcation of simple Lie groups by Killing and Cartan is a remark-
able fact in itself, but even more remarkable is that it paves the way for the
classiﬁcation of ﬁnite simple groups—a much harder problem, but one that
is related to the classiﬁcation of continuous groups. Surprisingly, there are
ﬁnite analogues of continuous groups in which the role of R or C is played
by ﬁnite ﬁelds.11
As mentioned in Section 2.8, ﬁnite simple groups were discovered by
Galois around 1830 as a key concept for understanding unsolvability in the
theory of equations. Galois explained solution of equations by radicals as a
process of “symmetry breaking” that begins with the group of all symme-
tries of the roots and factors it into smaller groups by taking square roots,
cube roots, and so on. The process ﬁrst fails with the general quintic equa-
tion, where the symmetry group is S5, the group of all 120 permutations of
ﬁve things. The group S5 may be factored down to the group A5 of the 60
even permutations of ﬁve things by taking a suitable square root, but it is
not possible to proceed further because A5 is a simple group.
More generally, An is simple for n > 5, so Galois had in fact discovered
an inﬁnite family of ﬁnite simple groups. Apart from the inﬁnite family of
11This brings to mind a quote attributed to Stan Ulam: The inﬁnite we can do right away,
the ﬁnite will take a little longer.

9.7
Discussion
203
cyclic groups of prime order, the ﬁnite simple groups in the other inﬁnite
families are ﬁnite analogues of Lie groups. Each inﬁnite matrix Lie group
G spawns inﬁnitely many ﬁnite groups, obtained by replacing the matrix
entries in elements of G by entries from a ﬁnite ﬁeld, such as the ﬁeld of
integers mod 2. There is a ﬁnite ﬁeld of size q for each prime power q, so
inﬁnitely many ﬁnite groups correspond to each inﬁnite matrix Lie group
G. These are called the ﬁnite groups of Lie type.
It turns out that each simple Lie group yields inﬁnitely many ﬁnite sim-
ple groups in this way. So, alongside the family of alternating groups, we
have a family of simple groups of Lie type for each simple Lie group. The
ﬁnite simple groups that fall outside these families are therefore even more
exceptional than the exceptional Lie groups. They are called the sporadic
groups, and there are 26 of them. The story of the sporadic simple groups
is a long one, ﬁlled with so many amazing episodes that it is impossible
to sketch it here. Instead, I recommend the book Ronan [2006] for an
overview, and Thompson [1983] for a taste of the mathematics.

Bibliography
J. Frank Adams. Lectures on Lie Groups. W. A. Benjamin, Inc., New
York–Amsterdam, 1969.
Marcel Berger. Geometry. I. Springer-Verlag, Berlin, 1987.
Garrett Birkhoff. Lie groups simply isomorphic to no linear group. Bulletin
of the American Mathematical Society, 42:883–888, 1936.
N. Bourbaki.
´El´ements de math´ematique. Fasc. XXXVII. Groupes et
alg`ebres de Lie. Chapitre II: Alg`ebres de Lie libres. Chapitre III:
Groupes de Lie. Hermann, Paris, 1972.
L. E. J. Brouwer. Beweis der Invarianz der Dimensionenzahl. Mathema-
tische Annalen, 71:161–165, 1911.
´Elie Cartan.
Nombres complexes.
In Encyclop´edie des sciences
math´ematiques, I 5, pages 329–468. Jacques Gabay, Paris, 1908.
´Elie Cartan.
La topologie des espaces repr´esentatifs groupes de Lie.
L’Enseignement math´ematique, 35:177–200, 1936.
´Elie Cartan. Lec¸ons sur la Th´eorie des Spineurs. Hermann, Paris, 1938.
Roger Carter, Graeme Segal, and Ian Macdonald. Lectures on Lie Groups
and Lie Algebras. Cambridge University Press, Cambridge, 1995.
Claude Chevalley. Theory of Lie Groups. I. Princeton Mathematical Series,
vol. 8. Princeton University Press, Princeton, N. J., 1946.
John H. Conway and Derek A. Smith. On Quaternions and Octonions.
A K Peters Ltd., Natick, MA, 2003.
204

Bibliography
205
H.-D. Ebbinghaus, H. Hermes, F. Hirzebruch, M. Koecher, K. Mainzer,
J. Neukirch, A. Prestel, and R. Remmert. Numbers. Springer-Verlag,
New York, 1990. With an introduction by K. Lamotke, Translated from
the second German edition by H. L. S. Orde, Translation edited and with
a preface by J. H. Ewing.
M. Eichler. A new proof of the Baker–Campbell–Hausdorff formula. Jour-
nal of the Mathematical Society of Japan, 20:23–25, 1968.
Roger Godement. Introduction `a la th´eorie des groupes de Lie. Springer-
Verlag, Berlin, 2004. Reprint of the 1982 original.
Brian C. Hall. Lie Groups, Lie Algebras, and Representations. Springer-
Verlag, New York, 2003.
Sir William Rowan Hamilton. Researches respecting quaternions. In The
Mathematical Papers of Sir William Rowan Hamilton, Vol. III, pages
159–226. Cambridge University Press, Cambridge, 1967.
Thomas Hawkins.
Emergence of the Theory of Lie Groups.
Springer-
Verlag, New York, 2000.
David Hilbert. Foundations of Geometry. Translated from the tenth Ger-
man edition by Leo Unger. First edition, 1899. Open Court, LaSalle, Ill.,
1971.
Roger Howe. Very basic Lie theory. Amer. Math. Monthly, 90(9):600–623,
1983.
Camille Jordan.
M´emoire sur les groupes de mouvements.
Annali di
matematiche, 2:167–215, 322–345, 1869.
Irving Kaplansky. Lie algebras. In Lectures on Modern Mathematics, Vol.
I, pages 115–132. Wiley, New York, 1963.
L. Pontrjagin. Topological Groups. Princeton University Press, Princeton,
1939.
Mark Ronan. Symmetry and the Monster. Oxford University Press, Oxford,
2006.
Wulf Rossmann. Lie Groups. Oxford University Press, Oxford, 2002.

206
Bibliography
Otto Schreier. Abstrakte kontinuierliche Gruppen. Abhandlungen aus dem
Mathematischen Seminar der Universit¨at Hamburg, 4:15–32, 1925.
Otto Schreier. Die Verwandtschaft stetiger Gruppen im Grossen. Abhand-
lungen aus dem Mathematischen Seminar der Universit¨at Hamburg, 5:
233–244, 1927.
Jean-Pierre Serre. Lie Algebras and Lie Groups. W. A. Benjamin, Inc.,
New York–Amsterdam, 1965.
K. Tapp. Matrix Groups for Undergraduates.
American Mathematical
Society, Providence, RI, 2005.
Thomas M. Thompson.
From Error-Correcting Codes through Sphere
Packings to Simple Groups.
Mathematical Association of America,
Washington, DC, 1983.
John von Neumann.
¨Uber die analytischen Eigenschaften von Grup-
pen linearer Transformationen und ihrer Darstellungen. Mathematische
Zeitschrift, 30:3–42, 1929.
J. H. M. Wedderburn. The absolute value of the product of two matrices.
Bulletin of the American Mathematical Society, 31:304–308, 1925.
Hermann Weyl.
Theorie der Darstellung kontinuierlicher halbeinfacher
Gruppen durch lineare Transformationen. I. Mathematische Zeitschrift,
23:271–301, 1925.
Hermann Weyl. The Classical Groups. Their Invariants and Representa-
tions. Princeton University Press, Princeton, N.J., 1939.

Index
24-cell, 36
A5, 45, 202
An, 45, 202
abelian group, 24, 41, 60
maximal, 66
absolute value
and determinant, 5
as distance, 7, 161
multiplicative property, 6
of complex numbers, 5
of matrices, 84
submultiplicative property, 84
Adams, J. Frank, 202
addition formula, 76, 96, 100, 141
additive notation, 24
Ado, I. D., 72, 105, 113
Aff(1), 87
is noncompact, 88
is not closed, 165
is not simple, 88
Lie algebra of, 89
afﬁne transformations, 87
almost simple, 72, 115
angle, 49
antipodal, 15, 33, 118, 189, 190
antisymmetric property, 13
automorphism, 44
axis of rotation, 14, 17
Baker, H. F., 153
Berger, Marcel, 47
bilinearity, 81, 83, 108
Birkhoff, Garrett, 72
block multiplication, 41, 58, 59
Bombelli, Rafael, 19
Bourbaki, Nicolas, 159
Brouwer, L. E. J., 149
Campbell, J. E., 153
Cardano, Girolamo, 19
Cartan, ´Elie, 37, 45, 73, 115, 137, 202
Cartan–Dieudonn´e theorem, 37
Cauchy–Schwarz inequality, 84, 85
proof, 86
Cayley, Arthur, 10, 21
center, 48, 61, 115
and normal subgroups, 69
and path-connectedness, 69
of a Lie group, 115
of GL(n,C), 111
of SL(n,C), 111
of SO(2m), 67
of SO(2m+1), 68
of SO(3), 61, 151
of Sp(n), 68
of SU(n), 68
of U(n), 68
Chevalley, Claude, 71, 113
closed
interval, 163
matrix group, 143
path, 178, 184
set, 160
deﬁnition, 162
subgroups, 160
closure, 163
commutative law, 6
commutator, 105
compactness, 88, 160
207

208
Index
and continuity, 171
and uniform continuity, 172
in general topology, 170
in Rk, 169
of O(n) and SO(n), 169
of O(n) and SO(n), 188
of U(n), SU(n), and Sp(n), 169
via ﬁnite subcover property, 170
complexiﬁcation, 107, 108
of su(n), 110
of u(n), 109
concatenation, 174, 175, 180
congruence ≡Lie, 155
conjugation, 14
detects failure to commute, 78
in SU(2), 74
reﬂected by Lie bracket, 74,
81, 98
rotation by, 14
continuity, 160
and compactness, 171
deﬁnition, 166
in general topology, 167
of basic functions, 167
of det, 164, 165
of matrix multiplication, 164
of matrix-valued function, 161
theory of, 46
uniform, 172
and compactness, 172
of deformations, 194
of paths, 194
via open sets, 167
continuous groups, 46
continuous groups see Lie groups 45
convergence, 85
coset, 24
cover, 170
and subcover, 170
covering, 179
and quotient, 201
double, 190
of S1 by R, 179
of SO(3) by SU(2), 190
universal, 201
cross product, 13
is an operation on R3, 13
is Lie bracket on su(2), 82
is not associative, 13
satisﬁes Jacobi identity, 13
curve see path 173
cylinder, 42, 188
Dedekind, Richard, 117
deformation, 160, 177
in small steps, 195
in the plate trick, 184
lifting, 180
with endpoints ﬁxed, 180
Degen, Ferdinand, 22
del Ferro, Scipione, 19
determinant
and absolute value, 5
and orientation, 38, 50
and quaternion absolute value, 8
as a homomorphism, 31, 107, 120
is continuous, 164, 165
multiplicative property, 6, 31
of exp, 100, 102
of quaternion matrix, 58, 59
of unitary matrix, 55
dimension, 106
invariance of, 107, 149
of a Lie algebra, 107
of a Lie group, 107
of so(n), 106
of sp(n), 106
of su(n), 106
of u(n), 106
Diophantus, 6
and two-square identity, 18
Arithmetica, 6
direct product, 23, 40, 42, 132, 138
direct sum, 132
discreteness, 69
distance
and absolute value, 7, 161

Index
209
between matrices, 161
in Euclidean space, 161
distributive law, 7
dot product see inner product 13
Eichler, Martin, viii, 139, 153, 159
eight-square identity, 22
Engel, Friedrich, 92
Euclidean space, 161
Euler, Leonhard, 11
exponential formula, 75
four-square identity, 11, 22
exceptional groups, viii, 22, 45, 46
exp see exponential function 74
exponential function, 74
addition formula, 76, 96,
100, 141
complex, 56, 75
of matrices, 74, 84
deﬁnition, 86
provides smooth paths, 93
quaternion, 60, 77
exponential map
into Lie groups, 91
into Riemannian manifolds, 92
is not onto SL(2,C), 92, 111, 177
of tangent vectors, 143
onto SO(2), 75
onto SO(3), 99
onto SU(2), 77
ﬁnite ﬁelds, 202
ﬁnite simple groups, 45, 202
ﬁnite subcover property, 170
four-square identity, 11
discovered by Euler, 11
Frobenius, Georg, 21, 73
G2, 45
Galois theory, 45
Galois, Evariste, 45, 202
Gauss, Carl Friedrich, 11
geodesics, 92
GL(n,C), 108
closed subgroups of, 182
Her All-embracing Majesty, 165
is noncompact, 110
is not simple, 122
is open in Mn(C), 166
is path-connected, 111, 175
not closed in Mn(C), 165
gl(n,C), 108
is not simple, 122
GL(n,H), 111
gl(n,H), 111
subspaces of, 112
Gleason, Andrew, 159
Graves, John, 22
great circle, 17
reﬂection in, 17
group
abelian, 24, 41, 202
additive notation, 24
afﬁne, 74, 87
center, 61
classical, vii, 80, 82, 93, 113
commutative, 1
continuous, 45
generated by inﬁnitesimals, 91
has ﬁnite analogue, 202
coset decomposition of, 25
deﬁnition, 24
direct product, 40
discrete, 69, 72, 118, 183
ﬁnite, 114, 151
of Lie type, 203
simple, 45, 202
fundamental, 201
general linear, 93, 108
Heisenberg, 72
homomorphism
deﬁnition, 29
kernel of, 29
preserves structure, 29
identity component, 54
isomorphism, 29
Lie see Lie groups vii

210
Index
Lorentz, 113
matrix, vii, 3
multiplicative notation, 24
nonabelian, 27
nonmatrix, 72, 202
of generalized rotations, 22
of linear transformations, 3
of rotations of R3, 16
of unit quaternions, 10
orthogonal, 48, 51
path-connected, 52, 56, 118
polyhedral, 34
quotient, 23, 28
simple, 31
simply connected, 201
special linear, 93, 108, 116
special orthogonal, 3, 48, 51
special unitary, 32, 48
sporadic, 203
symplectic, 48, 57
theory, 23
unitary, 48, 55
Hadamard, Jacques, 100
Hall, Brian, 149
Halmos, Paul, ix
Hamilton, Sir William Rowan, 10
and quaternion exponential,
91, 159
deﬁnition of C, 20
Hausdorff, Felix, 153
Hermite, Charles, 55
Hilbert, David, 159
ﬁfth problem, 159
homeomorphism, 107, 168
local, 179
preserves closure, 168
homomorphism
det, 31, 107
induced, 121, 191
Lie, 191
of groups, 23, 28
of Lie algebras, 120, 186, 191
of Lie groups, 183, 186, 191
of S3 ×S3 onto SO(4), 42
of S3 onto SO(3), 23
of simply connected groups, 201
onto quotient group, 28
theorem for groups, 30, 107
trace, 193
homotopy see deformation 177
Hopf ﬁbration, 26
hyperplane, 36
ideal, 116
as image of normal subgroup, 117,
118
as kernel, 120
deﬁnition, 117
in gl(n,C), 122
in ring theory, 117
in so(4), 123
in u(n), 124
origin of word, 117
identity
complex two-square, 11
eight-square, 22
four-square, 11
Jacobi, 13
two-square, 6
identity component, 54, 174
is a subgroup, 54
is normal subgroup, 175
of O(3), 189
inﬁnitesimal elements, 45, 113
inner product, 83
and angle, 49
and distance, 49, 54
and orthogonality, 49
Hermitian, 55
preservation criterion, 55
on Cn, 54
on Hn, 57
on R3, 13
on Rn, 48
deﬁnition, 49
intermediate value theorem, 46
inverse function theorem, viii

Index
211
inverse matrix, 6
invisibility, 72, 114, 150
isometry, 7
and the multiplicative property, 11
as product of reﬂections, 18, 36
is linear, 37
of R4, 12
of the plane, 12
orientation-preserving, 36, 38, 48
orientation-reversing, 38
isomorphism
local, 183
of groups, 23, 29
of simply connected Lie
groups, 186
of sp(1)×sp(1) onto so(4), 123
Jacobi identity, 13
holds for cross product, 13
holds for Lie bracket, 83
Jacobson, Nathan, 113
Jordan, Camille, 91
kernel
of covering map, 201
of group homomorphism, 29
of Lie algebra homomorphism,
120
Killing form, viii, 83
Killing, Wilhelm, 115, 137, 202
Kummer, Eduard, 117
Lagrange, Joseph Louis, 11
length, 49
Lie algebras, vii, 13
are topologically trivial, 201
as “inﬁnitesimal groups”, 46
as tangent spaces, 74, 104, 114
as vector spaces over C, 108
as vector spaces over R, 107
deﬁnition, 82
exceptional, viii, 46, 115, 137
matrix, 105
named by Weyl, 113
non-simple, 122
of classical groups, 113, 118
quaternion, 111
semisimple, 138
simple, 46, 114
deﬁnition, 116, 118
Lie bracket, 74, 80
and commutator, 105
determines group operation, 152
of pure imaginary quaternions, 80
of skew-symmetric matrices, 98
on the tangent space, 104
reﬂects conjugation, 74, 81, 98
reﬂects noncommutative content,
80
Lie groups, vii, 1
abelian, 41
almost simple, 115
as smooth manifolds, 114
classical, 80, 93, 113
compact, 88, 92, 159, 160
deﬁnition of, 3
exceptional, viii, 22, 45, 46
matrix see matrix Lie groups 81
noncommutative, 1
noncompact, 88, 92, 110
nonmatrix, 72, 113, 202
of rotations, 22
path-connected, 160, 175
simple, 48, 115
classiﬁcation of, 202
simply connected, 160, 186
two-dimensional, 88, 188
Lie homomorphisms, 191
Lie polynomial, 154
Lie theory, vii
and quaternions, 22
and topology, 73, 115
Lie, Sophus, 45, 80
and exponential map, 91
concept of simplicity, 115
knew classical Lie algebras, 113,
137

212
Index
knew SO(4) anomaly, 47
Transformationsgruppen, 47
Lie-type ﬁnite groups, 203
lifting, 179
a deformation, 180
a Lie algebra homomorphism, 197
a path, 179
limit point, 4, 162
linear transformations, 2
group of, 3
of H, 39
of Hn, 57
orthogonal, 3
preserving inner product, 48, 49
on Cn, 55
preserving length, 49, 161
preserving orientation, 48, 50
locus, 173
log see logarithm function 139
logarithm function
inverse to exp, 140
multiplicative property,
141, 146
produces tangents, 139
Mn(C), 108
Mn(H), 111
Mn(R), 93
manifold, 3
Riemannian, 92
matrix
absolute value, 84
submultiplicative property, 84
block multiplication, 41
criterion for rotation, 50
dilation, 51
exponential function, 84
deﬁnition, 86
groups, vii
inverse, 6
Lie algebra, 105
Lie group see matrix Lie groups 81
orthogonal, 32, 51, 97
product properties, 8
quaternion, 57, 112
representation of H, 7
discovered by Cayley, 10
representation of C, 5
representation of linear functions,
27, 87
sequence, 161
skew-Hermitian, 96, 99
skew-symmetric, 93, 96, 99
special orthogonal, 50
transpose, 10, 58
unitary, 32
upper triangular, 100
matrix group, 3
abelian, 41
closed, 143, 164
Lie see matrix Lie groups 81
quotient, 72
smoothness of, 3
matrix Lie groups, 4, 81, 113
and topology, 160
are closed, 164
are smooth manifolds, 147
as subgroups of GL(n,C),
160, 165
closed under limits, 4, 88,
139, 147
deﬁned by von Neumann, 158
deﬁnition, 4, 143, 166
include ﬁnite groups, 114
spawn ﬁnite groups, 203
matrix logarithm see logarithm function
139
maximal abelian subgroup, 66
maximal torus, 48, 60
in GL(n,C), 111
in SL(n,C), 111
in SO(2m), 64
in SO(2m+1), 64, 65
in SO(3), 60
in Sp(n), 66
in SU(n), 66

Index
213
in U(n), 65
introduced by Weyl, 72
Mercator, Nicholas, 141
Minkowski space, 113
Montgomery, Deane, 159
multiplicative notation, 24
multiplicative property
of absolute value, 6, 9, 20, 22
and isometries, 11
of determinants, 6, 9
of logarithm, 141, 146
of triangles, 18
neighborhood, 147, 162
topological, 149
Newton, Isaac, 141
Noether, Emmy, 117
nth roots of matrices, 144
O(3) is not path-connected, 186
O(n), 48
deﬁnition, 51
is not path-connected, 52
octonion, 22
automorphisms, 45
projective plane, 22
open
ball, 161
interval, 163
set, 160, 162
in general topology, 162
sets, 161
orientation, 38, 50
and determinant, 38
preservation of, 38
reversal of, 38
orthogonal
complement
in Mn(C), 148
of real quaternions, 12, 14
group, 48, 51
special, 3, 48, 51
matrix, 32, 97
transformation, 3, 49
vectors in R3, 13
orthonormal basis
of Cn, 55
path, 94, 160
as a function, 173
as locus, 173
as orbit, 173
as sequence of positions, 52
closed, 178, 184
and simple connectivity,
178, 184
concatenation, 174
deﬁnition, 174
deformation of, 160
lifting, 179
smooth, 4, 114
deﬁnition, 93, 94
of matrices, 94
of quaternions, 79, 81
path-connectedness, 48, 52, 60,
160, 174
and center, 69
and concept of rotation, 52
of GL(n,C), 111, 175
of SL(n,C), 111
of SO(n), 52
of Sp(n), 57, 60
of SU(n), 56
of U(n), 69
Peano, Giuseppe, 173
plate trick, 184, 189
Pontrjagin, Lev, 114
product
Cartesian, 40
direct, 40
of matrices, 2
of triangles, 18
product rule, 79
projective line
real, 31
projective plane
octonion, 22
real, 190

214
Index
projective space, 185
real, 32, 33, 185
quantum physics, 46
quaternions, vii, 7
absolute value of, 7
is multiplicative, 9
algebra of, 1, 6
discovered by Hamilton, 10
is skew ﬁeld, 21
roles in Lie theory, 22
and reﬂections of R4, 38
and rotations, 10, 14, 39
and SO(4), 23
automorphisms of, 44
conjugate, 9, 58
inverse, 9
matrix representation, 7
product of, 1, 7
is noncommutative, 8
pure imaginary, 12
as tangent vectors, 79
exponentiation of, 60, 77
spaces of, 22
unit, 10, 14
3-sphere of, 10
and SO(3), 33
antipodal, 15
group of, 10
quotient group, 23, 72
deﬁnition, 28
homomorphism onto, 28
R3
as a Lie algebra, 82, 119, 188
as quaternion subspace, 12
rotations of, 10
R4, 10
reﬂections of, 38
rotations of, 23, 36
and quaternions, 39
tiling by 24-cells, 36
Rn, 3
isometries of, 18
as products of reﬂections, 36
rotations of, 3
reﬂections, 16
and isometries of Rn, 36
in great circles, 17
in hyperplanes, 18, 52
linearity of, 38
of R4, 38
reverse orientation, 38
representation theory, viii
Riemannian manifolds, 92
rigid motion see isometry 11
Rodrigues, Olinde, 21
root systems, viii, 137
rotations, vii
and quaternions, 10, 14, 15, 35
are isometries, 38
are orientation-preserving, 38
are orthogonal, 3, 49
as product of reﬂections, 16
form a group, 16
generalized, 59
inﬁnitesimal, 46
of plane, 2
and complex numbers, 3
of R3, 10
and quaternions, 15
of R4, 23
and quaternions, 39
of Rn, 3
deﬁnition, 49
of space, 1
and quaternions, 3, 14
do not commute, 9
of tetrahedron, 34
RP1, 31
RP2, 190
RP3, 32, 33, 185
Russell, Bertrand, 193
Ryser, Marc, 37
S1, 1
as a group, 1, 32
is not simply connected, 180

Index
215
S2, 32
not a Lie group, 32
S3, 10
as a group, 1, 10, 32
as a matrix group, 32
as special unitary group, 32
homomorphism onto SO(3), 23
Hopf ﬁbration of, 26
is not a simple group, 23, 32
is simply connected, 189
Sn, 32
scalar product see inner product 13
Schreier, Otto, 73, 115, 150, 201
semisimplicity, 47
of Lie algebras, 138
Sierpinski carpet, 182
simple connectivity, 160, 177
and isomorphism, 186
deﬁned via closed paths, 178
of Lie groups, 186
of Rk, 178
of Sk, 178
of SU(2), 186, 189
of SU(n) and Sp(n), 190
simplicity
and solvability, 45
Lie’s concept of, 115
of A5, 45, 202
of An, 45, 202
of cross-product algebra, 119
of groups, 31
of Lie algebras, viii, 46, 115
deﬁnition, 116
of Lie groups, 48, 115
of sl(n,C), 125
of SO(2m+1), 46
of SO(3), 33, 118, 151
of so(3), 46, 118, 151
of so(n) for n > 4, 130
of sp(n), 133
of su(n), 126
skew ﬁeld, 21
SL(2,C), 92
is noncompact, 92
not the image of exp, 92, 111, 177
universal covering of, 202
SL(n,C), 108, 109
is closed in Mn(C), 166
is noncompact, 110
is path-connected, 111
sl(n,C), 109
smoothness, 3, 4, 182
and exponential function, 93, 166
and the tangent space, 183
effected by group structure, 166
of ﬁnite groups, 114
of homomorphisms, 183, 191
of manifolds, 3, 114, 182
of matrix groups, 4
of matrix Lie groups, 147
of matrix path, 94
of path, 4, 79, 93, 94
of sequential tangency, 146
SO(2), 3
as image of exp, 74
dense subgroup of, 70
is not simply connected, 179, 188
path-connectedness, 53
SO(2m) is not simple, 46, 72
SO(2m+1) is simple, 46, 70
SO(3), 3
and unit quaternions, 33
as Aut(H), 44
center of, 61, 151
is not simply connected, 184, 186,
189
is simple, 23, 33, 118, 151
Lie algebra of, 46
same tangents as SU(2), 118, 189
so(3), 46
simplicity of, 46, 118, 151
SO(4), 23
and quaternions, 23
anomaly of, 47
is not simple, 23, 44, 122

216
Index
is semisimple, 47
so(4)
is direct product, 132, 138
is not simple, 122
SO(n), 3, 48
deﬁnition, 51
geometric simplicity proof, 46
path-connectedness, 52
Sp(1), 57
equals SU(2), 57
Sp(n), 48, 57
complex form, 58
is not simple, 72
is path-connected, 57
is simply connected, 190
sp(n), 112
is simple, 133
space-ﬁlling curve, 173
special orthogonal group, 48, 51
special relativity, 113
special unitary group, 32, 48, 55
sphere
1-dimensional, 1
group structure, 1
3-dimensional, 1, 10
group structure, 10
n-dimensional, 32
with continuous group structure, 73
stereographic projection, 26, 181, 189
SU(2), 1, 32
as image of exp, 74
homomorphism onto SO(3), 33
is not simple, 33
is simply connected, 186
Lie algebra of, 74
same tangents as SO(3), 118, 189
tangent space, 78, 79, 118, 189
su(2), 74, 82
Lie bracket, 82
SU(n), 48
deﬁnition, 55
is not simple, 72
is simply connected, 190
path-connectedness, 56
subgroup, 23, 24
discrete, 69
discrete normal, 69
nondiscrete, 69
normal, 27
self-conjugate see normal 28
submultiplicative property, 84, 91
symplectic
origin of word, 71
symplectic form, 71
symplectic group, 48, 57
T1(G) see tangent space 93
tangent space, 4, 72, 74, 93
algebraic properties, 103
as indicator of smoothness, 4, 183
closed under Lie bracket, 104
dimension, 4, 107, 149
induced linear map of, 191, 193
is a vector space, 103
linearizes Lie groups, 98
of a normal subgroup, 117
of classical group, 82
of discrete group, 183
of GL(n,C), 108
of Riemannian manifold, 92
of SL(n,C), 110
of SO(2), 74
of SO(3), 98, 118
of SO(n), 96, 97
of Sp(n), 99
of SU(2), 74, 78, 79, 118
of SU(n), 101
of U(n), 99
tangent vector, 4, 93
exponentiation of, 143
of matrix group, 94
of O(n), 95
of SO(n), 93
of Sp(n), 95
of U(n), 95
sequential, 145, 166
smoothness of, 146

Index
217
Tartaglia, Nicolo, 19
tetrahedron, 34
theorem
Ado, 72, 105
Bolzano–Weierstrass, 170, 171
Campbell–Baker–Hausdorff, viii,
139, 152
Cartan–Dieudonn´e, 37
Cayley–Hamilton, 111
four square, 11
Heine–Borel, 169, 194
intermediate value, 46
on group homomorphisms,
30, 107
topology, viii, 160
as theory of continuity,
46, 160
general, 162
in Lie theory, 73, 115
of Lie group, 92
relative, 163, 165
torsion, 185
torus, 41, 188
in S3, 26
maximal, 48
surface, 41
totally disconnected, 70
trace, 100
as Lie algebra homomorphism,
120, 122, 193
homomorphism induced by det,
193
kernel of, 122
same for XY and YX, 103
triangle inequality, 85
two-square identity, 6
complex, 11
U(n), 48
deﬁnition, 55
is not simple, 107
is path-connected, 69
u(n)
is not simple, 124
Ulam, Stan, 202
unitary group, 48, 55
vector product see cross product 13
vector space
over C, 108, 124
over R, 82, 103, 106, 107
velocity vector, 79
Vi`ete, Franc¸ois, 18
von Neumann, John, viii, 158
and Hilbert’s ﬁfth problem, 159
and matrix exponentiation, 91
concept of tangent, 114, 145
theorem on exponentiation, 92
theory of matrix Lie groups, 158
Wedderburn, J. H. M., 91
Weyl, Hermann, 71
introduced maximal tori, 72
introduced word “symplectic”, 71
The Classical Groups, 71, 113
Z(G), 61
Zippin, Leo, 159

Undergraduate Texts in Mathematics (continued from p.ii)
Irving: Integers, Polynomials, and Rings: A Course
in Algebra.
Isaac: The Pleasures of Probability.
Readings in Mathematics.
James: Topological and Uniform Spaces.
J¨anich: Linear Algebra.
J¨anich: Topology.
J¨anich: Vector Analysis.
Kemeny/Snell: Finite Markov Chains.
Kinsey: Topology of Surfaces.
Klambauer: Aspects of Calculus.
Knoebel, Laubenbacher, Lodder, Pengelley:
Mathematical Masterpieces: Further Chronicles
by the Explorers.
Lang: A First Course in Calculus. Fifth edition.
Lang: Calculus of Several Variables. Third edition.
Lang: Introduction to Linear Algebra. Second
edition.
Lang: Linear Algebra. Third edition.
Lang: Short Calculus: The Original Edition of
“A First Course in Calculus.”
Lang: Undergraduate Algebra. Third edition.
Lang: Undergraduate Analysis.
Laubenbacher/Pengelley: Mathematical
Expeditions.
Lax/Burstein/Lax: Calculus with Applications
and Computing. Volume 1.
LeCuyer: College Mathematics with APL.
Lidl/Pilz: Applied Abstract Algebra. Second
edition.
Logan: Applied Partial Differential Equations,
Second edition.
Logan: A First Course in Differential Equations.
Lov´asz/Pelik´an/Vesztergombi: Discrete
Mathematics.
Macki-Strauss: Introduction to Optimal
Control Theory.
Malitz: Introduction to Mathematical Logic.
Marsden/Weinstein: Calculus I, II, III. Second
edition.
Martin: Counting: The Art of Enumerative
Combinatorics.
Martin: The Foundations of Geometry and the
Non-Euclidean Plane.
Martin: Geometric Constructions.
Martin: Transformation Geometry: An
Introduction to Symmetry.
Millman/Parker: Geometry: A Metric
Approach with Models. Second edition.
Moschovakis: Notes on Set Theory. Second
edition.
Owen: A First Course in the Mathematical
Foundations of Thermodynamics.
Palka: An Introduction to Complex Function
Theory.
Pedrick: A First Course in Analysis.
Peressini/Sullivan/Uhl: The Mathematics of
Nonlinear Programming.
Prenowitz/Jantosciak: Join Geometries.
Priestley: Calculus: A Liberal Art. Second edition.
Protter/Morrey: A First Course in Real Analysis.
Second edition.
Protter/Morrey: Intermediate Calculus. Second
edition.
Pugh: Real Mathematical Analysis.
Roman: An Introduction to Coding and
Information Theory.
Roman: Introduction to the Mathematics of
Finance: From Risk management to options
Pricing.
Ross: Differential Equations: An Introduction with
Mathematica R
⃝. Second Edition.
Ross: Elementary Analysis: The Theory of
Calculus.
Samuel: Projective Geometry.
Readings in Mathematics.
Saxe: Beginning Functional Analysis
Scharlau/Opolka: From Fermat to Minkowski.
Schiff: The Laplace Transform: Theory and
Applications.
Sethuraman: Rings, Fields, and Vector Spaces: An
Approach to Geometric Constructability.
Shores: Applied Linear Algebra and Matrix
Analysis.
Sigler: Algebra.
Silverman/Tate: Rational Points on Elliptic Curves.
Simmonds: A Brief on Tensor Analysis. Second
edition.
Singer: Geometry: Plane and Fancy.
Singer: Linearity, Symmetry, and Prediction in the
Hydrogen Atom.
Singer/Thorpe: Lecture Notes on Elementary
Topology and Geometry.
Smith: Linear Algebra. Third edition.
Smith: Primer of Modern Analysis. Second edition.
Stanton/White: Constructive Combinatorics.
Stillwell: Elements of Algebra: Geometry,
Numbers, Equations.
Stillwell: Elements of Number Theory.
Stillwell: The Four Pillars of Geometry.
Stillwell: Mathematics and Its History. Second
edition.
Stillwell: Naive Lie Theory.
Stillwell: Numbers and Geometry.
Readings in Mathematics.
Strayer: Linear Programming and Its Applications.
Toth: Glimpses of Algebra and Geometry. Second
Edition.
Readings in Mathematics.
Troutman: Variational Calculus and Optimal
Control. Second edition.
Valenza: Linear Algebra: An Introduction to
Abstract Mathematics.
Whyburn/Duda: Dynamic Topology.
Wilson: Much Ado About Calculus.

