Diﬀusion Self-Organizing Map on the Hypersphere
M. Andrecut
June 2, 2021
Calgary, Alberta, T3G 5Y8, Canada
mircea.andrecut@gmail.com
Abstract
We discuss a diﬀusion based implementation of the self-organizing map on the unit hyper-
sphere. We show that this approach can be eﬃciently implemented using just linear algebra
methods, we give a python numpy implementation, and we illustrate the approach using the
well known MNIST dataset.
Keywords: machine learning, self-organizing map, SOM
PACS: 07.05.Mh, 02.10.Yn; 02.30.Mv
1
Introduction
The Self-Organizing Map (SOM) is an Artiﬁcial Neural Network (ANN) that can be used to learn
a map that projects a high-dimensional input space into a low-dimensional space, by preserving
the topological structure of the input space, such as the local neighborhood [1], [2].
After the
convergence of the SOM learning process, the nodes (neurons) of the network are self-organized such
that input data samples associated with two nearby nodes are similar. The learning in SOM is based
on biologically inspired competitive learning [3], contrary to the more traditional types of ANNs,
like the feed-forward networks, where learning is based on some variant of the backpropagation
algorithm. Moreover, this type of competitive learning is also unsupervised, that is SOM learns from
unlabeled data, like in the more traditional clustering methods. Due to their combined clustering
and visualization properties SOMs have been successfully applied in almost every scientiﬁc and
engineering domain [4].
Today there exist several variations of the original SOM algorithm, however most of these variants
are based on the Euclidean distance as a similarity measure [5]. Here we discuss a version of SOM
that is implemented on the unit hypersphere, and uses the dot product as a similarity measure. The
nodes of the network also use a diﬀusion mechanism to communicate among them. We show that
this approach can be eﬃciently implemented using just linear algebra methods. A python numpy
implementation is also provided, and the method is illustrated using the MNIST dataset, which is a
well known benchmark frequently used in machine learning.
1
arXiv:2106.00014v1  [cs.NE]  31 May 2021

2
Self-Organizing Map
We assume that X = {x(n) ∈Rd | n = 0, 1, ..., N −1} is the input set of N d-dimensional data
points. A SOM consists of a number of K neurons U = {u(k) ∈Rd | k = 0, 1, ..., K −1}, associated
with K points distributed in a 2-dimensional regular lattice (rectangular or hexagonal).
The original online SOM is a recurrent algorithm where at each time step one input data sample
is presented to the network, and a competitive method is used to select the best matching neuron
(the closest one). The neurons in the neighborhood of the "winning neuron" are updated such that
they are getting closer to the input sample [5]. We should note that here we have two distances,
the ﬁrst one is deﬁned in the high-dimensional input space and it used to compute the "winning
neuron", while the second distance is used in the 2-dimensional space in order to deﬁne the local
neighborhood, and the "interaction" between the neurons. Using the Euclidean distance in both
spaces, the online SOM algorithm can be formulated as following:
1. A randomly selected input x(n)(t) ∈X is presented to the network at the time step t: n =
rand(0, N) (here, rand(0,N) returns a random integer 0 ≤n < N).
2. A "winning node" u(k∗)(t) ∈U that closely matches x(n)(t) is selected:
k∗= arg min
k ∥x(n)(t) −u(k)(t)∥,
(1)
where ∥· ∥is the Euclidean norm.
3. The weights of all nodes are then updated using the following equation:
u(k)(t + 1) = u(k)(t) + α(t)hk∗,k(t)[x(n)(t) −u(k)(t)].
(2)
Here, α(t) > 0 is the learning rate, and hk∗,k(t) is the neighborhood function centered on the
"winning node", at the time step t. The standard choice for h is the Gaussian function:
hk∗,k(t) = exp

−∥u(k∗)(t) −u(k)(t)∥2
2σ2(t)

,
(3)
where σ(t) deﬁnes the neighborhood size at time step t. One can also use other neighborhood func-
tions, like: step, triangular, Mexican hat etc. The main requirements are: 1) hk∗,k(t) is symmetrical,
and the maximum is obtained for k = k∗; 2) hk∗,k(t) decreases when ∥uk∗−uk∥increases.
The learning rate α(t) and neighborhood size σ(t) are usually decreased in time using an annealing
scheme:
α(t + 1) = α(t)θα,
(4)
σ(t + 1) = σ(t)θσ,
(5)
where 0 < θα < 1 and 0 < θσ < 1 are the annealing scaling constants, while α(0) and σ(0) are the
initial values.
2

The above algorithm is repeated until some preset condition is met, for example until the change
in the neurons weight is bellow a threshold 0 < ε < 1:
1
K
sX
k
∥u(k)(t + 1) −u(k)(t)∥2 ≤ε.
(6)
Before training the weights of the SOM neurons must be initialized. The initialization is usually
done using one of the following methods [5]:
1. Independent random values.
2. Randomly drawn samples from the input data.
3. Principal components of the input data.
The above online algorithm is stochastic, and therefore the results depend on the order in which
the input samples are presented to the SOM. A deterministic version of the SOM algorithm is the
batch SOM, which instead of using a single random data point for each iteration, it uses the entire
dataset [6]. The batch update step is given by:
u(k) =
P
j njhj,kx(j)
P
j njhj,k
, k = 0, 1, ..., K −1.
(7)
Here, nj is the number of input samples that have selected the node j as their "winning node", and
x(j) is the average of all these data points. The value hj,k is the value of the neighborhood function,
measuring the distance from node j to node k. We should note that the batch algorithm does not
require a learning rate, also its convergence is much faster than the online algorithm.
An important aspect of SOMs is data visualization. A trained SOM consists of a 2-dimensional
structure that is organized according to the the proximity of the samples in the input space, due to
the neighborhood function role in topology preservation. Typically the nodes are represented by the
cells of a regular grid (rectangular or hexagonal), the color (or intensity) of each cell is proportional
to the average distance in the input space to the neighbors of that node. This can be used to visualize
and identify data clusters in the high-dimensional input space, if for example lighter colors indicate
higher distance (lower density), and darker colors indicate smaller distance (higher density) [5].
3
Diﬀusion Self-Organizing Map on the Unit Hypersphere
In our implementation the input data and the SOM neurons are normalized to the unit hypersphere,
also we replace the distance with the dot product as a similarity measure. That is, we assume that
∥x(n)∥= 1, n = 0, 1, ..., N −1, and ∥u(k)∥= 1, k = 0, 1, ..., K −1. We observe that in this case we
have:
∥x(n)(t) −u(k)(t)∥2 = 2[1 −(x(n)(t))T u(k)(t)],
(8)
3

0
20
40
60
80
0
20
40
60
80
0.2
0.4
0.6
0.8
1.0
(a)
0
20
40
60
80
0
20
40
60
80
0.2
0.4
0.6
0.8
1.0
(b)
Figure 1: The solution of the diﬀusion equation for two diﬀerent winning neurons (L = 100).
and therefore the competitive rule can be rewritten as:
k∗= arg min
k ∥x(n)(t) −u(k)(t)∥⇔k∗= arg max
k (x(n)(t))T uk(t)
(9)
A second aspect is to replace the neighborhood function h() with the solution of the diﬀusion
equation, which is calculated as following. Let us assume that k∗is the index of the winning neuron,
and (i∗, j∗) is its corresponding pair of indices on the SOM grid. We consider the matrix:
hi,j(0) =



1
(i, j) = (i∗, j∗)
0
(i, j) ̸= (i∗, j∗)
(10)
and we calculate the diﬀusion equation solution using the ﬁnite-diﬀerence approach [7]:
hi,j(τ + 1) = hi,j(τ) + D∆τ[hi+1,j(τ) + hi−1,j(τ) + hi,j+1(τ) + hi,j−1(τ) −4hi,j(τ)]/(∆x)2
(11)
where τ is the (diﬀusion) time step, and D is the diﬀusion coeﬃcient. We impose also toroidal
conditions on the grid, that is i ± 1 ≡(i ± 1)modL where L =
√
K (K = L2). Also, we assume
∆τ = 1 and ∆x = 1. During the computation we keep hi∗,j∗= 1 for all time steps τ. We observe
that by increasing the number time of steps we practically increase the radius of the neighborhood
(Fig. 1). Also, if the number of steps T is ﬁxed, then we only have to compute the diﬀusion solution
once for (i∗, j∗) = (0, 0), because we can simply shift the matrix such that the "winning neuron" is
in the desired position (i∗, j∗) = (i, j) (in numpy this can be done using the "roll" function).
For a given h matrix we can formulate the following algorithm:
1. Initialize 0 < ε < 1, δ ←1, L =
√
K.
2. Compute the diﬀusion matrix h ←diﬀusion(L, D, T).
3. Randomly initialize the matrix u = [u(k)] of K neurons, such that each neuron is a normalized
4

row u(k) ∈Rd, u(k) ←u(k)/∥u(k)∥, k = 0, 1, ..., K −1.
4. While ε ≤δ compute:
v ←u
(12)
r ←xuT
(13)
c ←arg max
k
r
(14)
r(n) = f(h, ⌊cn/L⌋, c[n] −L⌊cn/L⌋), n = 0, 1, ..., N −1.
(15)
u ←rT x
(16)
u(k) = u(k)/∥u(k)∥, k = 0, 1, ..., K −1.
(17)
δ = 1 −1
K
X
k
(u(k))T v(k)
(18)
The algorithm parameters are ε, δ and L. Here, ε is the smallest accepted error between two
consecutive estimations of the SOM neurons, δ is the value of the time dependent error between two
consecutive estimations of the SOM neurons. So, the algorithm runs until ε ≤δ. L is the length
of the side of the square grid of SOM neurons, K = L2. The next step is to compute the diﬀusion
matrix h with the parameters L, D and T (the number of diﬀusion time steps). The neurons can
be initialized using the same approach as for the standard SOM algorithm, in this particular case
we just use random initialization.
In (12) we make a copy of the neurons (for future error estimation). In (13) we compute the
dot product between the input data x and the SOM neurons u. We then compute the index of the
"winning neurons" for each input sample (14). In the next step (15) we replace each row r(n) in r
with the shifted and ﬂattened version of the diﬀusion matrix h, this operation is provided by the
function f(h, i, j) (here ⌊·⌋is the ﬂoor function). With the new r we can compute the new values
of the SOM neurons u in (16), which are then normalized in (17). Finally in (18), the "alignment"
error δ between the previous SOM neurons and the current SOM neurons is computed. If δ < ε the
algorithm stops, otherwise it repeats.
4
MNIST dataset
In order to illustrate our approach we consider the well known MNIST data set, which is a large
database of handwritten digits (0,1,...,9), containing 60,000 training images and 10,000 testing images
[8,9]. These are monochrome images with an intensity in the interval [0, 255] and the size of 28 × 28
pixels. The MNIST data set is probably the most frequently used benchmark in image recognition.
We normalize the images as following:
xn ←xn −⟨xn⟩
(19)
xn ←xn/∥xn∥.
(20)
5

0
20
40
60
80
0
20
40
60
80
0.06
0.04
0.02
0.00
0.02
0.04
0.06
(a)
0
20
40
60
80
0
20
40
60
80
0.6
0.7
0.8
0.9
(b)
Figure 2: D-SOM for MNIST data (L = 100): (a) untrained D-SOM (b) trained D-SOM.
In Fig. 2 we give the resulted SOM visualization for the following parameters: L = 102, K = 104,
D = 0.25. Fig. 2(a) shows the initial correlation of the neurons, while Fig. 2(b) shows the ﬁnal
correlation, after training the SOM. One can see clearly how the domain valleys with lower correlation
(light grey) are formed between darker regions occupied by neurons associated with diﬀerent classes.
0
50
100
150
200
250
0
50
100
150
200
250
(a)
0
50
100
150
200
250
0
50
100
150
200
250
(b)
Figure 3: D-SOM neurons: (a) before training (b) after training.
In Fig. 3 we also show a random sample of 100 SOM neurons, before and after training. One
can see that each neuron is actually learning a class of digits. While the SOM is primarily used for
visualization, it can also be used for classiﬁcation. In this case with each neuron u(k) we associate
the class of the input sample that has the highest correlation (dot product). In Fig. 4 we show
the SOM regions corresponding to each class [0,1,...,9]. In order to classify a new sample, one can
just compute the dot product between this sample and all the SOM vectors, and the corresponding
class will be the class of the neuron with the highest correlation. This, simple procedure provides
an accuracy better than 95%, which is not bad considering its simplicity.
6

0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(a) 0
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(b) 1
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(c) 2
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(d) 3
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(e) 4
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(f) 5
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(g) 6
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(h) 7
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(i) 8
0
20
40
60
80
0
20
40
60
80
0.0
0.2
0.4
0.6
0.8
(j) 9
Figure 4: D-SOM class regions for MNIST data.
5
Numerical implementation
The D-SOM can be implemented eﬃciently in python numpy, by exploiting the parallelism provided
by the Intel’s MKL BLAS library. Below we give the implementation for the MNIST data set:
import numpy as np
import
matplotlib . pyplot
as
p l t
def read_data ( imagefile , l a b e l f i l e ,N,M) :
x = np . zeros ((N,M) , dtype=np . f l o a t 3 2 )
images = open( imagefile , ’ rb ’ )
images . read (16)
# skip
the
magic_number
for n in range (N) :
x [ n , : ]
= np . frombuffer ( images . read (M) , dtype=’ uint8 ’ ) . astype (np . f l o a t 3 2 )
images . c l o s e ()
x = x −np . mean(x , axis =0)
a = np . l i n a l g . norm(x , axis =1)
x = x/a [ : , None ]
l a b e l s = open( l a b e l f i l e , ’ rb ’ )
l a b e l s . read (8)
# skip
the
magic_number
xl = np . frombuffer ( l a b e l s . read (N) ,
dtype=’ uint8 ’ )
l a b e l s . c l o s e ()
return
(x ,
xl )
7

def
d i f f u s i o n (L ,T) :
h = np . zeros ((L , L) , dtype=np . f l o a t 3 2 )
h [ 0 , 0 ] ,D = np . f l o a t 3 2 ( 1 . 0 ) , np . f l o a t 3 2 ( 0 . 2 5 )
for
t
in range (T) :
g = np . vstack (( h [ 1 : , : ] , h [ 0 , : ] ) )
g += np . vstack (( h [ L−1 ,:] , h [ : L−1 , : ] ) )
g += np . hstack (( h [ : , 1 : ] , h [ : , 0 ] . reshape (L , 1 ) ) )
g += np . hstack (( h [ : , L−1]. reshape (L , 1 ) , h [ : , : L−1]))
h = D∗g
h [ 0 , 0 ] = np . f l o a t 3 2 ( 1 . 0 )
return h
def dsom(x , u ) :
v , (N,M) ,K = u , x . shape , len (u)
L = int (np . sqrt (K))
for
t
in range (L//2 ,1 , −1):
h = d i f f u s i o n (L , t )
delta , eta = 1 ,0
while
delta > 1e−6 and delta
!=
eta :
eta = delta
r = np . dot (x , u .T)
c = np . argmax ( r , axis =1)
for n in range (N) :
r [ n , : ]
= np . r o l l (h , ( c [ n ]//L , c [ n]−L∗( c [ n ]//L) ) ,
axis =(0 ,1)). f l a t t e n ()
u = np . dot ( r .T, x)
a = np . l i n a l g . norm(u , axis =1)
u = u/a [ : , None ]
delta , v = 1 −np .sum(u∗v)/K, u
print ( " t=" , t , " delta=" , delta )
return u
def
som_correlation (u ) :
(K,M) = u . shape
L = int (np . sqrt (K))
w = np . zeros ((L , L) , dtype=np . f l o a t 3 2 )
for
i
in range (L ) :
for
j
in range (L ) :
n = i ∗L + j
m = (n −L)%K
w[ i , j ] += np . dot (u [ n , : ] , u [m, : ] )
m = (n −1)%K
w[ i , j ] += np . dot (u [ n , : ] , u [m, : ] )
m = (n + 1)%K
w[ i , j ] += np . dot (u [ n , : ] , u [m, : ] )
m = (n + L)%K
w[ i , j ] += np . dot (u [ n , : ] , u [m, : ] )
w[ i , j ] /= 4
return w
8

def
init_som (K,M) :
u = np . random . randn (K,M) . astype (np . f l o a t 3 2 )
for k in range (K) :
u [ k , : ] = u [ k , : ] / np . l i n a l g . norm(u [ k , : ] )
return u
i f __name__ == ’__main__ ’ :
np . random . seed (12345)
print ( "Read␣data" )
(x ,
xl ) = read_data ( " ./ data/ train −images . idx3−ubyte" ,
" ./ data/ train −l a b e l s . idx1−ubyte" ,
60000 ,
784)
(N,M) = x . shape
K = 10000 # number
of SOM neurons
u = init_som (K,M)
print ( " Train ␣SOM" )
u = dsom(x , u)
np . savetxt ( "som−u . csv " ,u , d e l i m i t e r=" , " , fmt=’%f ’ )
print ( " V i s u a l i z e ␣SOM" )
w = som_correlation (u)
f i g = p l t . f i g u r e ( f i g s i z e =(8 ,8))
hmap = p l t . imshow (w,
cmap=’ Greys ’ ,
i n t e r p o l a t i o n=’ nearest ’ )
p l t . colorbar (hmap , f r a c t i o n =0.046 , pad =0.04)
f i g . s a v e f i g ( "som . pdf " , bbox_inches=" t i g h t " )
The implementation is also using 32 bit ﬂoat numbers to minimize the memory footprint. The
"read_data" function reads the data "x" and normalizes it. The number of neurons in SOM is set
to K = 104 and they are initialized randomly in "init_som". The "dsom" function implements the
Diﬀusion-SOM algorithm. As input it takes the data array "x" and the initialized SOM vectors "u".
The diﬀusion time is varied from L/2 to 1, such that initially the neighborhood is large (facilitating
long range interactions between neurons) with a radius of L/2, and then it shrinks to the the ﬁrst
neighbors only. For each diﬀusion time one computes a diﬀusion matrix "h". Here the diﬀusion
coeﬃcient is set to D=0.25. Then for each "h" one applies the algorithm described by (12)-(18).
The algorithm stops when "delta" becomes smaller or equal to ε = 10−6 or if it stops converging.
The SOM vectors are visualized by a square image where each neuron is associated to a pixel,
with the intensity corresponding to the average correlation (dot product) between that neuron and
its immediate four neighbors, which is computed by the function "som_correlation". Finally, the
resulted map is saved in "som.pdf".
Conclusion
In this paper we have presented a version of SOM that is implemented on the unit hypersphere,
and uses the dot product as a similarity measure. The nodes of the network also use diﬀusion to
communicate among them. This approach can be eﬃciently implemented using just linear algebra
methods, and a python numpy version is provided. The method was illustrated using the MNIST
dataset. We should also note that if the number diﬀusion time steps is reduced to one (no diﬀusion),
then the D-SOM algorithm becomes equivalent to the K-Means algorithm described in [10].
9

References
[1] T. Kohonen, Self-organized formation of topologically correct feature maps, Biological Cyber-
netics 66, 59-69 (1982).
[2] T. Kohonen, Self-Organizing Maps, 3rd edition. Springer-Verlag, Berlin, 2001.
[3] S. Haykin, Neural Networks: A Comprehensive Foundation, 2nd ed., Prentice Hall PTR Upper
Saddle River, NJ, USA, 1998.
[4] T. Kohonen, Essentials of the self-organizing map, Neural Networks 37, 52-65 (2013), Twenty-
ﬁfth Anniversary Commemorative Issue.
[5] T. Kohonen, MATLAB Implementations and Applications of the Self-Organizing Map, Unigraﬁa
Oy, Helsinki, Finland, 2014.
[6] T. Kohonen, Comparison of SOM point densities based on diﬀerent criteria, Neural Computa-
tion 11, 2081-2095 (1999).
[7] A. Iserles, A ﬁrst course in the numerical analysis of diﬀerential equations, Cambridge Univer-
sity Press (2008).
[8] Y. Lecun, L. Bottou, Y. Bengio, P. Haﬀner, Gradient-based learning applied to document recog-
nition, Proceedings of the IEEE 86(11), 2278 (1998).
[9] MNIST dataset available at: http://yann.lecun.com/exdb/mnist.
[10] M. Andrecut, K-Means Kernel Classiﬁer, arXiv:2012.13021 (2020).
10

