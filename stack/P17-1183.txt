Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2004–2015
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1183
Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics, pages 2004–2015
Vancouver, Canada, July 30 - August 4, 2017. c⃝2017 Association for Computational Linguistics
https://doi.org/10.18653/v1/P17-1183
Morphological Inﬂection Generation
with Hard Monotonic Attention
Roee Aharoni & Yoav Goldberg
Computer Science Department
Bar-Ilan University
Ramat-Gan, Israel
{roee.aharoni,yoav.goldberg}@gmail.com
Abstract
We present a neural model for morpho-
logical inﬂection generation which em-
ploys a hard attention mechanism, inspired
by the nearly-monotonic alignment com-
monly found between the characters in
a word and the characters in its inﬂec-
tion. We evaluate the model on three pre-
viously studied morphological inﬂection
generation datasets and show that it pro-
vides state of the art results in various se-
tups compared to previous neural and non-
neural approaches. Finally we present an
analysis of the continuous representations
learned by both the hard and soft atten-
tion (Bahdanau et al., 2015) models for the
task, shedding some light on the features
such models extract.
1
Introduction
Morphological inﬂection generation involves gen-
erating a target word (e.g.
“h¨artestem”, the
German word for “hardest”), given a source
word (e.g.
“hart”,
the German word for
“hard”) and the morpho-syntactic attributes of
the target (POS=adjective,
gender=masculine,
type=superlative, etc.).
The task is important for many down-stream
NLP tasks such as machine translation, especially
for dealing with data sparsity in morphologically
rich languages where a lemma can be inﬂected
into many different word forms. Several studies
have shown that translating into lemmas in the tar-
get language and then applying inﬂection gener-
ation as a post-processing step is beneﬁcial for
phrase-based machine translation (Minkov et al.,
2007; Toutanova et al., 2008; Clifton and Sarkar,
2011; Fraser et al., 2012; Chahuneau et al., 2013)
and more recently for neural machine translation
(Garc´ıa-Mart´ınez et al., 2016).
The task was traditionally tackled with hand en-
gineered ﬁnite state transducers (FST) (Kosken-
niemi, 1983; Kaplan and Kay, 1994) which rely
on expert knowledge, or using trainable weighted
ﬁnite state transducers (Mohri et al., 1997; Eisner,
2002) which combine expert knowledge with data-
driven parameter tuning.
Many other machine-
learning based methods (Yarowsky and Wicen-
towski, 2000; Dreyer and Eisner, 2011; Durrett
and DeNero, 2013; Hulden et al., 2014; Ahlberg
et al., 2015; Nicolai et al., 2015) were proposed for
the task, although with speciﬁc assumptions about
the set of possible processes that are needed to cre-
ate the output sequence.
More recently, the task was modeled as neu-
ral sequence-to-sequence learning over character
sequences with impressive results (Faruqui et al.,
2016).
The vanilla encoder-decoder models as
used by Faruqui et al. compress the input sequence
to a single, ﬁxed-sized continuous representation.
Instead, the soft-attention based sequence to se-
quence learning paradigm (Bahdanau et al., 2015)
allows directly conditioning on the entire input se-
quence representation, and was utilized for mor-
phological inﬂection generation with great success
(Kann and Sch¨utze, 2016b,a).
However,
the
neural
sequence-to-sequence
models require large training sets in order to per-
form well: their performance on the relatively
small CELEX dataset is inferior to the latent vari-
able WFST model of Dreyer et al. (2008). Inter-
estingly, the neural WFST model by Rastogi et al.
(2016) also suffered from the same issue on the
CELEX dataset, and surpassed the latent variable
model only when given twice as much data to train
on.
We propose a model which handles the above
issues by directly modeling an almost monotonic
2004

alignment between the input and output charac-
ter sequences, which is commonly found in the
morphological inﬂection generation task (e.g. in
languages with concatenative morphology). The
model consists of an encoder-decoder neural net-
work with a dedicated control mechanism: in each
step, the model attends to a single input state and
either writes a symbol to the output sequence or
advances the attention pointer to the next state
from the bi-directionally encoded sequence, as de-
scribed visually in Figure 1.
This modeling suits the natural monotonic
alignment between the input and output, as the
network learns to attend to the relevant inputs be-
fore writing the output which they are aligned
to. The encoder is a bi-directional RNN, where
each character in the input word is represented
using a concatenation of a forward RNN and a
backward RNN states over the word’s characters.
The combination of the bi-directional encoder and
the controllable hard attention mechanism enables
to condition the output on the entire input se-
quence. Moreover, since each character represen-
tation is aware of the neighboring characters, non-
monotone relations are also captured, which is im-
portant in cases where segments in the output word
are a result of long range dependencies in the in-
put word. The recurrent nature of the decoder, to-
gether with a dedicated feedback connection that
passes the last prediction to the next decoder step
explicitly, enables the model to also condition the
current output on all the previous outputs at each
prediction step.
The hard attention mechanism allows the net-
work to jointly align and transduce while us-
ing a focused representation at each step, rather
then the weighted sum of representations used in
the soft attention model. This makes our model
Resolution Preserving (Kalchbrenner et al., 2016)
while also keeping decoding time linear in the
output sequence length rather than multiplicative
in the input and output lengths as in the soft-
attention model. In contrast to previous sequence-
to-sequence work, we do not require the training
procedure to also learn the alignment. Instead, we
use a simple training procedure which relies on
independently learned character-level alignments,
from which we derive gold transduction+control
sequences. The network can then be trained using
straightforward cross-entropy loss.
To evaluate our model, we perform extensive
experiments on three previously studied morpho-
logical inﬂection generation datasets: the CELEX
dataset (Baayen et al., 1993), the Wiktionary
dataset (Durrett and DeNero, 2013) and the SIG-
MORPHON2016 dataset (Cotterell et al., 2016).
We show that while our model is on par with
or better than the previous neural and non-neural
state-of-the-art approaches, it also performs sig-
niﬁcantly better with very small training sets, be-
ing the ﬁrst neural model to surpass the perfor-
mance of the weighted FST model with latent vari-
ables which was speciﬁcally tailored for the task
by Dreyer et al. (2008). Finally, we analyze and
compare our model and the soft attention model,
showing how they function very similarly with re-
spect to the alignments and representations they
learn, in spite of our model being much simpler.
This analysis also sheds light on the representa-
tions such models learn for the morphological in-
ﬂection generation task, showing how they encode
speciﬁc features like a symbol’s type and the sym-
bol’s location in a sequence.
To summarize, our contributions in this paper
are three-fold:
1. We present a hard attention model for nearly-
monotonic sequence to sequence learning, as
common in the morphological inﬂection set-
ting.
2. We evaluate the model on the task of mor-
phological inﬂection generation, establishing
a new state of the art on three previously-
studied datasets for the task.
3. We perform an analysis and comparison of
our model and the soft-attention model, shed-
ding light on the features such models extract
for the inﬂection generation task.
2
The Hard Attention Model
2.1
Motivation
We would like to transduce an input sequence,
x1:n ∈Σ∗
x into an output sequence, y1:m ∈Σ∗
y,
where Σx and Σy are the input and output vo-
cabularies, respectively. Imagine a machine with
read-only random access to the encoding of the in-
put sequence, and a single pointer that determines
the current read location. We can then model se-
quence transduction as a series of pointer move-
ment and write operations. If we assume the align-
ment is monotone, the machine can be simpli-
2005

ﬁed: the memory can be read in sequential or-
der, where the pointer movement is controlled by
a single “move forward” operation (step) which
we add to the output vocabulary. We implement
this behavior using an encoder-decoder neural net-
work, with a control mechanism which determines
in each step of the decoder whether to write an
output symbol or promote the attention pointer the
next element of the encoded input.
2.2
Model Deﬁnition
In prediction time, we seek the output sequence
y1:m ∈Σ∗
y, for which:
y1:m = arg max
y′∈Σ∗y
p(y′|x1:n, f)
(1)
Where x ∈Σ∗
x is the input sequence and f =
{f1, . . . , fl} is a set of attributes inﬂuencing the
transduction task (in the inﬂection generation task
these would be the desired morpho-syntactic at-
tributes of the output sequence). Given a nearly-
monotonic alignment between the input and the
output, we replace the search for a sequence of let-
ters with a sequence of actions s1:q ∈Σ∗
s, where
Σs = Σy ∪{step}. This sequence is a series of
step and write actions required to go from x1:n
to y1:m according to the monotonic alignment be-
tween them (we will elaborate on the determinis-
tic process of getting s1:q from a monotonic align-
ment between x1:n to y1:m in section 2.4). In this
case we deﬁne: 1
s1:q = arg max
s′∈Σ∗s
p(s′|x1:n, f)
= arg max
s′∈Σ∗s
Y
s′
i∈s′
p(s′
i|s′
1 . . . s′
i−1, x1:n, f)
(2)
which we can estimate using a neural network:
s1:q = arg max
s′∈Σ∗s
NN(x1:n, f, Θ)
(3)
where the network’s parameters Θ are learned us-
ing a set of training examples. We will now de-
scribe the network architecture.
1We note that our model (Eq. 2) solves a different ob-
jective than (Eq 1), as it searches for the best derivation and
not the best sequence. In order to accurately solve (1) we
would need to marginalize over the different derivations lead-
ing to the same sequence, which is computationally challeng-
ing. However, as we see in the experiments section, the best-
derivation approximation is effective in practice.
Figure 1: The hard attention network architecture.
A round tip expresses concatenation of the inputs
it receives. The attention is promoted to the next
input element once a step action is predicted.
2.3
Network Architecture
Notation We use bold letters for vectors and ma-
trices. We treat LSTM as a parameterized func-
tion LSTMθ(x1 . . . xn) mapping a sequence of
input vectors x1 . . . xn to a an output vector hn.
The equations for the LSTM variant we use are
detailed in the supplementary material of this pa-
per.
Encoder For every element in the input sequence:
x1:n = x1 . . . xn, we take the corresponding em-
bedding: ex1 . . . exn, where: exi ∈RE. These
embeddings are parameters of the model which
will be learned during training.
We then feed
the embeddings into a bi-directional LSTM en-
coder (Graves and Schmidhuber, 2005) which re-
sults in a sequence of vectors: x1:n = x1 . . . xn,
where each vector xi
∈
R2H is a concate-
nation of: LSTMforward(ex1, ex2, . . . exi) and
LSTMbackward(exn, exn−1 . . . exi), the forward
LSTM and the backward LSTM outputs when fed
with exi.
Decoder Once the input sequence is encoded, we
feed the decoder RNN, LSTMdec, with three in-
puts at each step:
1. The current attended input, xa ∈R2H, ini-
tialized with the ﬁrst element of the encoded
sequence, x1.
2. A set of embeddings for the attributes that in-
ﬂuence the generation process, concatenated
to a single vector: f = [f1 . . . fl] ∈RF·l.
3. si−1 ∈RE, which is an embedding for the
2006

predicted output symbol in the previous de-
coder step.
Those three inputs are concatenated into a single
vector zi = [xa, f, si−1] ∈R2H+F·l+E, which is
fed into the decoder, providing the decoder output
vector: LSTMdec(z1 . . . zi) ∈RH. Finally, to
model the distribution over the possible actions,
we project the decoder output to a vector of |Σs|
elements, followed by a softmax layer:
p(si = c)
= softmax c(W · LSTMdec(z1 . . . zi) + b)
(4)
Control Mechanism When the most probable ac-
tion is step, the attention is promoted so xa con-
tains the next encoded input representation to be
used in the next step of the decoder. The process
is demonstrated visually in Figure 1.
2.4
Training the Model
For every example: (x1:n, y1:m, f) in the train-
ing data, we should produce a sequence of step
and write actions s1:q to be predicted by the de-
coder. The sequence is dependent on the align-
ment between the input and the output: ideally,
the network will attend to all the input characters
aligned to an output character before writing it.
While recent work in sequence transduction ad-
vocate jointly training the alignment and the de-
coding mechanisms (Bahdanau et al., 2015; Yu
et al., 2016), we instead show that in our case it
is worthwhile to decouple these stages and learn
a hard alignment beforehand, using it to guide the
training of the encoder-decoder network and en-
abling the use of correct alignments for the at-
tention mechanism from the beginning of the net-
work training phase. Thus, our training procedure
consists of three stages: learning hard alignments,
deriving oracle actions from the alignments, and
learning a neural transduction model given the or-
acle actions.
Learning Hard Alignments We use the character
alignment model of Sudoh et al. (2013), based on a
Chinese Restaurant Process which weights single
alignments (character-to-character) in proportion
to how many times such an alignment has been
seen elsewhere out of all possible alignments. The
aligner implementation we used produces either 0-
to-1, 1-to-0 or 1-to-1 alignments.
Deriving Oracle Actions We infer the sequence
of actions s1:q from the alignments by the deter-
ministic procedure described in Algorithm 1. An
example of an alignment with the resulting oracle
action sequence is shown in Figure 2, where a4 is
a 0-to-1 alignment and the rest are 1-to-1 align-
ments.
Figure 2: Top: an alignment between a lemma
x1:n and an inﬂection y1:m as predicted by the
aligner. Bottom: s1:q, the sequence of actions to
be predicted by the network, as produced by Al-
gorithm 1 for the given alignment.
Algorithm 1 Generates the oracle action sequence
s1:q from the alignment between x1:n and y1:m
Require: a, the list of either 1-to-1, 1-to-0 or 0-
to-1 alignments between x1:n and y1:m
1: Initialize s as an empty sequence
2: for each ai = (xai, yai) ∈a do
3:
if ai is a 1-to-0 alignment then
4:
s.append(step)
5:
else
6:
s.append(yai)
7:
if ai+1 is not a 0-to-1 alignment then
8:
s.append(step)
return s
This procedure makes sure that all the source
input elements aligned to an output element are
read (using the step action) before writing it.
Learning a Neural Transduction Model The
network is trained to mimic the actions of the ora-
cle, and at inference time, it will predict the actions
according to the input. We train it using a conven-
tional cross-entropy loss function per example:
L(x1:n, y1:m, f, Θ) = −
X
sj∈s1:q
log softmax sj(d),
d = W · LSTMdec(z1 . . . zi) + b
(5)
Transition System An alternative view of our
model is that of a transition system with AD-
VANCE and WRITE(CH) actions, where the oracle
is derived from a given hard alignment, the input
is encoded using a biRNN, and the next action is
determined by an RNN over the previous inputs
and actions.
2007

3
Experiments
We perform extensive experiments with three pre-
viously studied morphological inﬂection genera-
tion datasets to evaluate our hard attention model
in various settings. In all experiments we com-
pare our hard attention model to the best per-
forming neural and non-neural models which were
previously published on those datasets, and to
our implementation of the global (soft) attention
model presented by Luong et al. (2015) which we
train with identical hyper-parameters as our hard-
attention model. The implementation details for
our models are described in the supplementary
material section of this paper. The source code
and data for our models is available on github.2
CELEX
Our ﬁrst evaluation is on a very small
dataset, to see if our model indeed avoids the ten-
dency to overﬁt with small training sets. We re-
port exact match accuracy on the German inﬂec-
tion generation dataset compiled by Dreyer et al.
(2008) from the CELEX database (Baayen et al.,
1993).
The dataset includes only 500 training
examples for each of the four inﬂection types:
13SIA→13SKE, 2PIE→13PKE, 2PKE→z, and
rP→pA which we refer to as 13SIA, 2PIE, 2PKE
and rP, respectively.3 We ﬁrst compare our model
to three competitive models from the literature that
reported results on this dataset: the Morphologi-
cal Encoder-Decoder (MED) of Kann and Sch¨utze
(2016a) which is based on the soft-attention model
of Bahdanau et al. (2015), the neural-weighted
FST of Rastogi et al. (2016) which uses stacked
bi-directional LSTM’s to weigh its arcs (NWFST),
and the model of Dreyer et al. (2008) which uses
a weighted FST with latent-variables structured
particularly for morphological string transduction
tasks (LAT). Following previous reports on this
dataset, we use the same data splits as Dreyer et al.
(2008), dividing the data for each inﬂection type
into ﬁve folds, each consisting of 500 training,
1000 development and 1000 test examples. We
train a separate model for each fold and report ex-
act match accuracy, averaged over the ﬁve folds.
2https://github.com/roeeaharoni/
morphological-reinflection
3The acronyms stand for: 13SIA=1st/3rd person, singular,
indeﬁnite, past;13SKE=1st/3rd person, subjunctive, present;
2PIE=2nd person, plural, indeﬁnite, present;13PKE=1st/3rd
person, plural, subjunctive, present; 2PKE=2nd person, plu-
ral, subjunctive, present; z=inﬁnitive; rP=imperative, plural;
pA=past participle.
Wiktionary
To neutralize the negative effect of
very small training sets on the performance of
the different learning approaches, we also evalu-
ate our model on the dataset created by Durrett
and DeNero (2013), which contains up to 360k
training examples per language. It was built by
extracting Finnish, German and Spanish inﬂection
tables from Wiktionary, used in order to evalu-
ate their system based on string alignments and
a semi-CRF sequence classiﬁer with linguistically
inspired features, which we use a baseline. We
also used the dataset expansion made by Nicolai
et al. (2015) to include French and Dutch inﬂec-
tions as well. Their system also performs an align-
and-transduce approach, extracting rules from the
aligned training set and applying them in inference
time with a proprietary character sequence classi-
ﬁer. In addition to those systems we also com-
pare to the results of the recent neural approach
of Faruqui et al. (2016), which did not use an at-
tention mechanism, and Yu et al. (2016), which
coupled the alignment and transduction tasks.
SIGMORPHON
As different languages show
different morphological phenomena, we also ex-
periment with how our model copes with these
various phenomena using the morphological in-
ﬂection dataset from the SIGMORPHON2016
shared task (Cotterell et al., 2016).
Here the
training data consists of ten languages, with ﬁve
morphological system types (detailed in Table 3):
Russian (RU), German (DE), Spanish (ES), Geor-
gian (GE), Finnish (FI), Turkish (TU), Arabic
(AR), Navajo (NA), Hungarian (HU) and Maltese
(MA) with roughly 12,800 training and 1600 de-
velopment examples per language. We compare
our model to two soft attention baselines on this
dataset: MED (Kann and Sch¨utze, 2016b), which
was the best participating system in the shared
task, and our implementation of the global (soft)
attention model presented by Luong et al. (2015).
4
Results
In all experiments, for both the hard and soft at-
tention models we implemented, we report results
using an ensemble of 5 models with different ran-
dom initializations by using majority voting on the
ﬁnal sequences the models predicted, as proposed
by Kann and Sch¨utze (2016a). This was done to
perform fair comparison to the models of Kann
and Sch¨utze (2016a,b); Faruqui et al. (2016) which
we compare to, that also perform a similar ensem-
2008

13SIA
2PIE
2PKE
rP
Avg.
MED (Kann and Sch¨utze, 2016a)
83.9
95
87.6
84
87.62
NWFST (Rastogi et al., 2016)
86.8
94.8
87.9
81.1
87.65
LAT (Dreyer et al., 2008)
87.5
93.4
87.4
84.9
88.3
Soft
83.1
93.8
88
83.2
87
Hard
85.8
95.1
89.5
87.2
89.44
Table 1: Results on the CELEX dataset
DE-N
DE-V
ES-V
FI-NA
FI-V
FR-V
NL-V
Avg.
Durrett and DeNero (2013)
88.31
94.76
99.61
92.14
97.23
98.80
90.50
94.47
Nicolai et al. (2015)
88.6
97.50
99.80
93.00
98.10
99.20
96.10
96.04
Faruqui et al. (2016)
88.12
97.72
99.81
95.44
97.81
98.82
96.71
96.34
Yu et al. (2016)
87.5
92.11
99.52
95.48
98.10
98.65
95.90
95.32
Soft
88.18
95.62
99.73
93.16
97.74
98.79
96.73
95.7
Hard
88.87
97.35
99.79
95.75
98.07
99.04
97.03
96.55
Table 2: Results on the Wiktionary datasets
sufﬁxing+stem changes
circ.
sufﬁxing+agg.+v.h.
c.h.
templatic
RU
DE
ES
GE
FI
TU
HU
NA
AR
MA
Avg.
MED
91.46
95.8
98.84
98.5
95.47
98.93
96.8
91.48
99.3
88.99
95.56
Soft
92.18
96.51
98.88
98.88
96.99
99.37
97.01
95.41
99.3
88.86
96.34
Hard
92.21
96.58
98.92
98.12
95.91
97.99
96.25
93.01
98.77
88.32
95.61
Table 3: Results on the SIGMORPHON 2016 morphological inﬂection dataset. The text above each lan-
guage lists the morphological phenomena it includes: circ.=circumﬁxing, agg.=agglutinative, v.h.=vowel
harmony, c.h.=consonant harmony
bling technique.
On the low resource setting (CELEX), our hard
attention model signiﬁcantly outperforms both the
recent neural models of Kann and Sch¨utze (2016a)
(MED) and Rastogi et al. (2016) (NWFST) and the
morphologically aware latent variable model of
Dreyer et al. (2008) (LAT), as detailed in Table
1.
In addition, it signiﬁcantly outperforms our
implementation of the soft attention model (Soft).
It is also, to our knowledge, the ﬁrst model that
surpassed in overall accuracy the latent variable
model on this dataset. We attribute our advantage
over the soft attention models to the ability of the
hard attention control mechanism to harness the
monotonic alignments found in the data. The ad-
vantage over the FST models may be explained by
our conditioning on the entire output history which
is not available in those models. Figure 3 plots
the train-set and dev-set accuracies of the soft and
hard attention models as a function of the training
epoch. While both models perform similarly on
the train-set (with the soft attention model ﬁtting it
slightly faster), the hard attention model performs
signiﬁcantly better on the dev-set. This shows the
soft attention model’s tendency to overﬁt on the
small dataset, as it is not enforcing the monotonic
assumption of the hard attention model.
On the large training set experiments (Wik-
tionary), our model is the best performing model
on German verbs, Finnish nouns/adjectives and
Dutch verbs, resulting in the highest reported av-
erage accuracy across all inﬂection types when
compared to the four previous neural and non-
neural state of the art baselines, as detailed in Ta-
ble 2.
This shows the robustness of our model
also with large amounts of training examples,
and the advantage the hard attention mechanism
provides over the encoder-decoder approach of
Faruqui et al. (2016) which does not employ an
attention mechanism. Our model is also signiﬁ-
cantly more accurate than the model of Yu et al.
(2016), which shows the advantage of using in-
dependently learned alignments to guide the net-
work’s attention from the beginning of the training
process. While our soft-attention implementation
outperformed the models of Yu et al. (2016) and
Durrett and DeNero (2013), it still performed in-
feriorly to the hard attention model.
As can be seen in Table 3, on the SIG-
MORPHON 2016 dataset our model performs
2009

0
10
20
30
40
0
0.5
1
epoch
accuracy
soft-train
hard-train
soft-dev
hard-dev
Figure 3: Learning curves for the soft and hard
attention models on the ﬁrst fold of the CELEX
dataset
Figure 4: A comparison of the alignments as pre-
dicted by the soft attention (left) and the hard at-
tention (right) models on examples from CELEX.
better than both soft-attention baselines for the
sufﬁxing+stem-change languages (Russian, Ger-
man and Spanish) and is slightly less accurate than
our implementation of the soft attention model on
the rest of the languages, which is now the best
performing model on this dataset to our knowl-
edge. We explain this by looking at the languages
from a linguistic typology point of view, as de-
tailed in Cotterell et al. (2016). Since Russian,
German and Spanish employ a sufﬁxing morphol-
ogy with internal stem changes, they are more suit-
able for monotonic alignment as the transforma-
tions they need to model are the addition of suf-
ﬁxes and changing characters in the stem. The
rest of the languages in the dataset employ more
context sensitive morphological phenomena like
vowel harmony and consonant harmony, which re-
quire to model long range dependencies in the in-
put sequence which better suits the soft attention
mechanism. While our implementation of the soft
attention model and MED are very similar model-
wise, we hypothesize that our soft attention model
results are better due to the fact that we trained
the model for 100 epochs and picked the best per-
forming model on the development set, while the
MED system was trained for a ﬁxed amount of 20
epochs (although trained on more data – both train
and development sets).
5
Analysis
The Learned Alignments
In order to see if the
alignments predicted by our model ﬁt the mono-
tonic alignment structure found in the data, and
whether are they more suitable for the task when
compared to the alignments found by the soft at-
tention model, we examined alignment predictions
of the two models on examples from the develop-
ment portion of the CELEX dataset, as depicted in
Figure 4. First, we notice the alignments found
by the soft attention model are also monotonic,
supporting our modeling approach for the task.
Figure 4 (bottom-right) also shows how the hard-
attention model performs deletion (legte→lege)
by predicting a sequence of two step operations.
Another notable morphological transformation is
the one-to-many alignment, found in the top exam-
ple: ﬂog→ﬂiege, where the model needs to trans-
form a character in the input, o, to two characters
in the output, ie. This is performed by two consec-
utive write operations after the step operation of
the relevant character to be replaced. Notice that
in this case, the soft attention model performs a
different alignment by aligning the character i to
o and the character g to the sequence eg, which
is not the expected alignment in this case from a
linguistic point of view.
The Learned Representations
How does the
soft-attention model manage to learn nearly-
perfect monotonic alignments?
Perhaps the the
network learns to encode the sequential position
as part of its encoding of an input element? More
generally, what information is encoded by the soft
and hard alignment encoders? We selected 500
random encoded characters-in-context from input
2010

(a) Colors indicate which character is encoded.
(b) Colors indicate which character is encoded.
(c) Colors indicate the character’s position.
(d) Colors indicate the character’s position.
Figure 5: SVD dimension reduction to 2D of 500 character representations in context from the encoder,
for both the soft attention (top) and hard attention (bottom) models.
words in the CELEX development set, where ev-
ery encoded representation is a vector in R200.
Since those vectors are outputs from the bi-LSTM
encoders of the models, every vector of this form
carries information of the speciﬁc character with
its entire context. We project these encodings into
2-D using SVD and plot them twice, each time
using a different coloring scheme. We ﬁrst color
each point according to the character it represents
(Figures 5a, 5b). In the second coloring scheme
(Figures 5c, 5d), each point is colored according
to the character’s sequential position in the word it
came from, blue indicating positions near the be-
ginning of the word, and red positions near its end.
While both models tend to cluster representa-
tions for similar characters together (Figures 5a,
5b), the hard attention model tends to have much
more isolated character clusters. Figures 5c, 5d
show that both models also tend to learn represen-
tations which are sensitive to the position of the
character, although it seems that here the soft at-
tention model is more sensitive to this information
as its coloring forms a nearly-perfect red-to-blue
transition on the X axis. This may be explained
by the soft-attention mechanism encouraging the
encoder to encode positional information in the
input representations, which may help it to pre-
dict better attention scores, and to avoid collisions
when computing the weighted sum of representa-
tions for the context vector. In contrast, our hard-
attention model has other means of obtaining the
position information in the decoder using the step
actions, and for that reason it does not encode it
as strongly in the representations of the inputs.
This behavior may allow it to perform well even
with fewer examples, as the location information
is represented more explicitly in the model using
the step actions.
6
Related Work
Many previous works on inﬂection generation
used machine learning methods (Yarowsky and
Wicentowski, 2000; Dreyer and Eisner, 2011;
Durrett and DeNero, 2013; Hulden et al., 2014;
Ahlberg et al., 2015; Nicolai et al., 2015) with
assumptions about the set of possible processes
needed to create the output word. Our work was
mainly inspired by Faruqui et al. (2016) which
trained an independent encoder-decoder neural
2011

network for every inﬂection type in the training
data, alleviating the need for feature engineering.
Kann and Sch¨utze (2016b,a) tackled the task with
a single soft attention model (Bahdanau et al.,
2015) for all inﬂection types, which resulted in
the best submission at the SIGMORPHON 2016
shared task (Cotterell et al., 2016).
In another
closely related work, Rastogi et al. (2016) model
the task with a WFST in which the arc weights
are learned by optimizing a global loss function
over all the possible paths in the state graph, while
modeling contextual features with bi-directional
LSTMS. This is similar to our approach, where
instead of learning to mimic a single greedy align-
ment as we do, they sum over all possible align-
ments. While not committing to a single greedy
alignment could in theory be beneﬁcial, we see
in Table 1 that—at least for the low resource
scenario—our greedy approach is more effective
in practice.
Another recent work (Kann et al.,
2016) proposed performing neural multi-source
morphological reinﬂection, generating an inﬂec-
tion from several source forms of a word.
Previous works on neural sequence transduc-
tion include the RNN Transducer (Graves, 2012)
which uses two independent RNN’s over mono-
tonically aligned sequences to predict a distribu-
tion over the possible output symbols in each step,
including a null symbol to model the alignment.
Yu et al. (2016) improved this by replacing the null
symbol with a dedicated learned transition proba-
bility. Both models are trained using a forward-
backward approach, marginalizing over all possi-
ble alignments. Our model differs from the above
by learning the alignments independently, thus en-
abling a dependency between the encoder and de-
coder. While providing better results than Yu et al.
(2016), this also simpliﬁes the model training us-
ing a simple cross-entropy loss. A recent work by
Raffel et al. (2017) jointly learns the hard mono-
tonic alignments and transduction while maintain-
ing the dependency between the encoder and the
decoder. Jaitly et al. (2015) proposed the Neural
Transducer model, which is also trained on exter-
nal alignments. They divide the input into blocks
of a constant size and perform soft attention sepa-
rately on each block. Lu et al. (2016) used a com-
bination of an RNN encoder with a CRF layer to
model the dependencies in the output sequence.
An interesting comparison between ”traditional”
sequence transduction models (Bisani and Ney,
2008; Jiampojamarn et al., 2010; Novak et al.,
2012) and encoder-decoder neural networks for
monotone string transduction tasks was done by
Schnober et al. (2016), showing that in many cases
there is no clear advantage to one approach over
the other.
Regarding task-speciﬁc improvements to the at-
tention mechanism, a line of work on attention-
based speech recognition (Chorowski et al., 2015;
Bahdanau et al., 2016) proposed adding location
awareness by using the previous attention weights
when computing the next ones, and preventing
the model from attending on too many or too
few inputs using “sharpening” and “smoothing”
techniques on the attention weight distributions.
Cohn et al. (2016) offered several changes to the
attention score computation to encourage well-
known modeling biases found in traditional ma-
chine translation models like word fertility, po-
sition and alignment symmetry.
Regarding the
utilization of independent alignment models for
training attention-based networks, Mi et al. (2016)
showed that the distance between the attention-
infused alignments and the ones learned by an in-
dependent alignment model can be added to the
networks’ training objective, resulting in an im-
proved translation and alignment quality.
7
Conclusion
We presented a hard attention model for mor-
phological inﬂection generation. The model em-
ploys an explicit alignment which is used to train
a neural network to perform transduction by de-
coding with a hard attention mechanism.
Our
model performs better than previous neural and
non-neural approaches on various morphological
inﬂection generation datasets, while staying com-
petitive with dedicated models even with very few
training examples. It is also computationally ap-
pealing as it enables linear time decoding while
staying resolution preserving, i.e. not requiring
to compress the input sequence to a single ﬁxed-
sized vector.
Future work may include apply-
ing our model to other nearly-monotonic align-
and-transduce tasks like abstractive summariza-
tion, transliteration or machine translation.
Acknowledgments
This work was supported by the Intel Collabora-
tive Research Institute for Computational Intelli-
gence (ICRI-CI), and The Israeli Science Founda-
tion (grant number 1555/15).
2012

References
Malin Ahlberg, Markus Forsberg, and Mans Hulden.
2015. Paradigm classiﬁcation in supervised learning
of morphology. In NAACL HLT 2015. pages 1024–
1029.
R Harald Baayen, Richard Piepenbrock, and Rijn van
H. 1993. The {CELEX} lexical data base on {CD-
ROM} .
Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Ben-
gio. 2015.
Neural machine translation by jointly
learning to align and translate. Proceedings of the
International Conference on Learning Representa-
tions (ICLR) .
Dzmitry Bahdanau, Jan Chorowski, Dmitriy Serdyuk,
Yoshua Bengio, et al. 2016. End-to-end attention-
based large vocabulary speech recognition.
In
2016 IEEE International Conference on Acous-
tics, Speech and Signal Processing (ICASSP). pages
4945–4949.
Maximilian Bisani and Hermann Ney. 2008.
Joint-
sequence
models
for
grapheme-to-phoneme
conversion.
Speech Commun. 50(5):434–451.
https://doi.org/10.1016/j.specom.2008.01.002.
Victor Chahuneau, Eva Schlinger, Noah A. Smith, and
Chris Dyer. 2013. Translating into morphologically
rich languages with synthetic phrases. In EMNLP.
pages 1677–1687.
Jan
K
Chorowski,
Dzmitry
Bahdanau,
Dmitriy
Serdyuk, Kyunghyun Cho, and Yoshua Bengio.
2015. Attention-based models for speech recogni-
tion. In Advances in Neural Information Processing
Systems 28, pages 577–585.
Ann Clifton and Anoop Sarkar. 2011.
Combin-
ing morpheme-based machine translation with post-
processing morpheme prediction.
In ACL. pages
32–42.
Trevor Cohn, Cong Duy Vu Hoang, Ekaterina Vy-
molova, Kaisheng Yao, Chris Dyer, and Gholam-
reza Haffari. 2016. Incorporating structural align-
ment biases into an attentional neural translation
model.
In Proceedings of the 2016 Conference
of the North American Chapter of the Associa-
tion for Computational Linguistics: Human Lan-
guage Technologies. Association for Computational
Linguistics, San Diego, California, pages 876–885.
http://www.aclweb.org/anthology/N16-1102.
Ryan Cotterell, Christo Kirov, John Sylak-Glassman,
David Yarowsky, Jason Eisner, and Mans Hulden.
2016.
The SIGMORPHON 2016 shared task—
morphological reinﬂection.
In Proceedings of the
2016 Meeting of SIGMORPHON.
Markus Dreyer and Jason Eisner. 2011. Discovering
morphological paradigms from plain text using a
dirichlet process mixture model. In EMNLP. pages
616–627.
Markus Dreyer, Jason R Smith, and Jason Eisner.
2008. Latent-variable modeling of string transduc-
tions with ﬁnite-state methods. In Proceedings of
the conference on empirical methods in natural lan-
guage processing. pages 1080–1089.
Greg Durrett and John DeNero. 2013.
Supervised
learning of complete morphological paradigms. In
NAACL HLT 2013. pages 1185–1195.
Jason Eisner. 2002. Parameter estimation for proba-
bilistic ﬁnite-state transducers.
In Proceedings of
the 40th annual meeting on Association for Compu-
tational Linguistics. pages 1–8.
Manaal Faruqui, Yulia Tsvetkov, Graham Neubig, and
Chris Dyer. 2016. Morphological inﬂection genera-
tion using character sequence to sequence learning.
In NAACL HLT 2016.
Alexander M. Fraser, Marion Weller, Aoife Cahill, and
Fabienne Cap. 2012. Modeling inﬂection and word-
formation in smt. In EACL. pages 664–674.
Mercedes Garc´ıa-Mart´ınez, Lo¨ıc Barrault, and Fethi
Bougares. 2016. Factored neural machine transla-
tion. arXiv preprint arXiv:1609.04621 .
A. Graves and J. Schmidhuber. 2005.
Framewise
phoneme classiﬁcation with bidirectional LSTM and
other neural network architectures.
Neural Net-
works 18(5-6):602–610.
Alex Graves. 2012.
Sequence transduction with
recurrent
neural
networks.
arXiv
preprint
arXiv:1211.3711 .
Mans Hulden, Markus Forsberg, and Malin Ahlberg.
2014. Semi-supervised learning of morphological
paradigms and lexicons. In EACL. pages 569–578.
Navdeep Jaitly, David Sussillo, Quoc V Le, Oriol
Vinyals, Ilya Sutskever, and Samy Bengio. 2015. A
neural transducer. arXiv preprint arXiv:1511.04868
.
Sittichai Jiampojamarn, Colin Cherry, and Grzegorz
Kondrak. 2010.
Integrating joint n-gram fea-
tures into a discriminative training framework. In
Human Language Technologies:
The 2010 An-
nual Conference of the North American Chap-
ter of the Association for Computational Lin-
guistics. Association for Computational Linguis-
tics,
Los Angeles,
California,
pages 697–700.
http://www.aclweb.org/anthology/N10-1103.
Nal Kalchbrenner, Lasse Espeholt, Karen Simonyan,
Aaron van den Oord, Alex Graves, and Koray
Kavukcuoglu. 2016. Neural machine translation in
linear time. arXiv preprint arXiv:1610.10099 .
Katharina Kann, Ryan Cotterell, and Hinrich Sch¨utze.
2016. Neural multi-source morphological reinﬂec-
tion. EACL 2017 .
2013

Katharina Kann and Hinrich Sch¨utze. 2016a.
Med:
The lmu system for the sigmorphon 2016 shared task
on morphological reinﬂection.
Katharina Kann and Hinrich Sch¨utze. 2016b. Single-
model encoder-decoder with explicit morphological
representation for reinﬂection. In ACL.
Ronald M. Kaplan and Martin Kay. 1994.
Regular
models of phonological rule systems.
Computa-
tional Linguistics 20(3):331–378.
Kimmo Koskenniemi. 1983. Two-level morphology:
A general computational model of word-form recog-
nition and production. Technical report.
Liang Lu, Lingpeng Kong, Chris Dyer, Noah A Smith,
and Steve Renals. 2016. Segmental recurrent neural
networks for end-to-end speech recognition. arXiv
preprint arXiv:1603.00223 .
Thang Luong, Hieu Pham, and Christopher D. Man-
ning. 2015. Effective approaches to attention-based
neural machine translation. In Proceedings of the
2015 Conference on Empirical Methods in Natu-
ral Language Processing. Association for Compu-
tational Linguistics, Lisbon, Portugal, pages 1412–
1421. http://aclweb.org/anthology/D15-1166.
Haitao Mi,
Zhiguo Wang,
and Abe Ittycheriah.
2016.
Supervised attentions for neural machine
translation.
In Proceedings of the 2016 Con-
ference on Empirical Methods in Natural Lan-
guage Processing. Association for Computational
Linguistics,
Austin,
Texas,
pages
2283–2288.
https://aclweb.org/anthology/D16-1249.
Einat Minkov, Kristina Toutanova, and Hisami Suzuki.
2007.
Generating complex morphology for ma-
chine translation.
In Proceedings of the 45th
Annual Meeting of the Association of Computa-
tional Linguistics. Association for Computational
Linguistics, Prague, Czech Republic, pages 128–
135. http://www.aclweb.org/anthology/P07-1017.
Mehryar Mohri, Fernando Pereira, and Michael Riley.
1997. A rational design for a weighted ﬁnite-state
transducer library. In International Workshop on Im-
plementing Automata. pages 144–158.
Garrett Nicolai, Colin Cherry, and Grzegorz Kondrak.
2015. Inﬂection generation as discriminative string
transduction. In NAACL HLT 2015. pages 922–931.
Josef R. Novak,
Nobuaki Minematsu,
and Kei-
kichi Hirose. 2012.
WFST-based grapheme-
to-phoneme conversion:
Open source tools for
alignment,
model-building and decoding.
In
Proceedings of the 10th International Workshop
on Finite State Methods and Natural Language
Processing. Association for Computational Lin-
guistics,
Donostia–San Sebastin,
pages 45–49.
http://www.aclweb.org/anthology/W12-6208.
C. Raffel, T. Luong, P. J. Liu, R. J. Weiss, and
D. Eck. 2017. Online and Linear-Time Attention by
Enforcing Monotonic Alignments.
arXiv preprint
arXiv:1704.00784 .
Pushpendre Rastogi, Ryan Cotterell, and Jason Eisner.
2016. Weighting ﬁnite-state transductions with neu-
ral context. In Proc. of NAACL.
Carsten Schnober, Steffen Eger, Erik-Lˆan Do Dinh, and
Iryna Gurevych. 2016. Still not there? comparing
traditional sequence-to-sequence models to encoder-
decoder neural networks on monotone string trans-
lation tasks.
In Proceedings of COLING 2016,
the 26th International Conference on Computational
Linguistics: Technical Papers. The COLING 2016
Organizing Committee, Osaka, Japan, pages 1703–
1714. http://aclweb.org/anthology/C16-1160.
Katsuhito Sudoh, Shinsuke Mori, and Masaaki Nagata.
2013.
Noise-aware character alignment for boot-
strapping statistical machine transliteration from
bilingual corpora. In EMNLP 2013. pages 204–209.
Kristina Toutanova, Hisami Suzuki, and Achim Ruopp.
2008. Applying morphology generation models to
machine translation. In ACL. pages 514–522.
David Yarowsky and Richard Wicentowski. 2000.
Minimally supervised morphological analysis by
multimodal alignment. In ACL.
Lei Yu, Jan Buys, and Phil Blunsom. 2016.
Online
segment to segment neural transduction.
In Pro-
ceedings of the 2016 Conference on Empirical Meth-
ods in Natural Language Processing. Association
for Computational Linguistics, Austin, Texas, pages
1307–1316.
https://aclweb.org/anthology/D16-
1138.
Matthew D Zeiler. 2012. Adadelta: an adaptive learn-
ing rate method. arXiv preprint arXiv:1212.5701 .
2014

Supplementary Material
Training Details, Implementation and Hyper
Parameters
To train our models, we used the train portion of
the datasets as-is and evaluated on the test portion
the model which performed best on the develop-
ment portion of the dataset, without conducting
any speciﬁc pre-processing steps on the data. We
train the models for a maximum of 100 epochs
over the training set. To avoid long training time,
we trained the model for 20 epochs for datasets
larger than 50k examples, and for 5 epochs for
datasets larger than 200k examples. The models
were implemented using the python bindings of
the dynet toolkit.4
We trained the network by optimizing the ex-
pected output sequence likelihood using cross-
entropy loss as mentioned in equation 5. For op-
timization we used ADADELTA (Zeiler, 2012)
without regularization. We updated the weights
after every example (i.e. mini-batches of size 1).
We used the dynet toolkit implementation of an
LSTM network with two layers for all models,
each having 100 entries in both the encoder and
decoder. The character embeddings were also vec-
tors with 100 entries for the CELEX experiments,
and with 300 entries for the SIGMORPHON and
Wiktionary experiments.
The morpho-syntactic attribute embeddings
were vectors of 20 entries in all experiments. We
did not use beam search while decoding for both
the hard and soft attention models as it is signif-
icantly slower and did not show clear improve-
ment in previous experiments we conducted. For
the character level alignment process we use the
implementation provided by the organizers of the
SIGMORPHON2016 shared task.5
LSTM Equations
We used the LSTM variant implemented in the
dynet toolkit, which corresponds to the following
4https://github.com/clab/dynet
5https://github.com/ryancotterell/
sigmorphon2016
equations:
it = σ(Wixxt + Wihht−1 + Wicct−1 + bi)
ft = σ(Wfxxt + Wfhft−1 + Wfcct−1 + bf)
ec = tanh(Wcxxt + Wchht−1 + bc)
ct = ct−1 ◦ft + ec ◦it
ot = σ(Woxxt + Wohht−1 + Woxct + bo)
ht = tanh(ct) ◦ot
(6)
2015

