arXiv:2407.01548v1  [q-bio.OT]  25 Apr 2024
From Cognition to Computation:
A Comparative Review of Human Attention and Transformer Architectures
Minglu Zhao 1 Dehong Xu 1 Tao Gao 2
Abstract
Attention is a cornerstone of human cognition
that facilitates the efﬁcient extraction of informa-
tion in everyday life. Recent developments in
artiﬁcial intelligence like the Transformer archi-
tecture also incorporate the idea of attention in
model designs. However, despite the shared fun-
damental principle of selectively attending to in-
formation, human attention and the Transformer
model display notable differences, particularly
in their capacity constraints, attention pathways,
and intentional mechanisms. Our review aims
to provide a comparative analysis of these mech-
anisms from a cognitive-functional perspective,
thereby shedding light on several open research
questions. The exploration encourages interdisci-
plinary efforts to derive insights from human at-
tention mechanisms in the pursuit of developing
more generalized artiﬁcial intelligence.
1. Introduction
Over the past few years, the ﬁeld of artiﬁcial intelligence
(AI) has experienced a signiﬁcant transformation with the
introduction of the Transformer architectures, which have
rapidly become the cornerstone of many state-of-the-art
models in natural language processing (NLP), computer vi-
sion, and beyond. These architectures incorporate the con-
cept of attention which echos the complex cognitive pro-
cess of human attention, a remarkable ability that enables
us to focus on speciﬁc aspects of our environment while ef-
fectively ﬁltering out extraneous information. Our review
article aims to provide an in-depth comparative analysis of
human attention and Transformer architectures, systemati-
cally examining the similarities and differences from vari-
ous perspectives including vision, language, and agency.
Attention has been one of the most studied topics in
1Department of Statistics, UCLA
2Department of Com-
munication,
UCLA.
Correspondence
to:
Minglu
Zhao
<minglu.zhao@ucla.edu>.
Preprint. Work in progress.
cognitive psychology and inﬂuences a broad range of
cognitive processes, contributing to perception, memory,
and cognitive control.
Classic studies in cognitive psy-
chology have underlined attention’s role as a ﬁltering
mechanism that selectively processes relevant information
from the environment while managing cognitive resources
(Broadbent, 1958; Treisman, 1964; Tanenhaus et al., 1995;
Wolfe, 2000).
Additionally, attention as a mental con-
struct is intertwined with self-regulation and social commu-
nication processes, facilitating not only individuals’ task
commitment (Rueda et al., 2005; McClelland et al., 2007;
Kanfer & Ackerman, 1989) but also cooperative interac-
tions (Bratman, 1987; Tomasello et al., 1995). On the other
hand, AI models with the notion of attention, exempli-
ﬁed by Transformer architectures, have shown great ver-
satility and robustness in various applications. Initially de-
signed for language processing, Transformer models have
profoundly transformed the NLP landscape. By utilizing
self-attention mechanisms to grasp the contextual relation-
ships in sequences, they have made remarkable advance-
ments in various tasks (Brown et al., 2020; OpenAI, 2023),
including machine translation (Vaswani et al., 2017), sen-
timent analysis (Devlin et al., 2018), and text summariza-
tion (Liu & Lapata, 2019).
In the computer vision do-
main, Transformer models have been applied to large-scale
visual tasks and are able to capture long-range depen-
dencies and hierarchies in image data (Dosovitskiy et al.,
2020; Carion et al., 2020; Khan et al., 2022). Several re-
cent architectures further incorporate Transformer architec-
tures in decision-making tasks, treating decisions as a se-
quence generation task and thereby enabling the model to
learn optimal action strategies based on past experiences
(Chen et al., 2021; Janner et al., 2021; Meng et al., 2021).
The shared terminology of “attention” in human cognitive
studies and Transformer architecture has given rise to in-
triguing parallels yet certain ambiguities concerning their
relationship. Although both emphasize the selective pro-
cessing of contextual information, human attention and
the Transformer model present distinct disparities in their
capacity constraints, attentional pathways, and intentional
mechanisms. In this article, we systematically compare the
two domains around these functional attributes. We note
that this article does not serve as an exhaustive review of ei-
1

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
ther the human attention mechanism or the Transformer ar-
chitecture, but rather a focused comparative analysis seek-
ing to identify open directions for better incorporating in-
sights from human attention to attention-based models in
AI. We would also like to point out several reviews for
a more comprehensive review of the topics (Wolfe, 2000;
Carrasco, 2011; Khan et al., 2022; Guo et al., 2022).
2. Attention modeling and Transformer
architecture
The principle of attention has been incorporated into var-
ious domains of AI, underpinning advancements in areas
such as image recognition (Xu et al., 2015), speech recog-
nition (Chorowski et al., 2015), and sequence-to-sequence
prediction models (Bahdanau et al., 2014; Luong et al.,
2015). However, in recognition of the signiﬁcant impact
it induced, this paper will concentrate on analyzing the
Transformer architecture (Vaswani et al., 2017), a model
that heavily relies on the attention mechanism. The Trans-
former is an innovative AI architecture and has been a
game-changer in natural language processing which has
laid the foundation for numerous state-of-the-art models.
The Transformer model leverages self-attention mecha-
nisms to capture contextual information in input sequences,
making it highly effective in handling long-rangedependen-
cies while discarding the use of recurrence in the network.
Self-attention mechanism
In essence, the Transformer
architecture employs an “attention” mechanism that takes
into account different words’ relevance in a sentence when
processing each word. This approach can be compared to
how we, as humans, selectively focus on certain aspects of
a scene or conversation while tuning out the rest. Speciﬁ-
cally, this is achieved through the self-attention mechanism,
which computes the relationships between all pairs of el-
ements in an input sequence. The core idea is to assign
different weights to different elements based on their rel-
evance to the current element being processed. The self-
attention mechanism is deﬁned as follows:
Attention(Q, K, V ) = softmax
QKT
√dk

V,
(1)
where Q (query), K (key), and V (value) are derived from
the input matrix X through linear transformations using
weight matrices W Q, W K, and W V : Q = XW Q, K =
XW K, V = XW V .
The keys (K), queries (Q), and values (V ) in this formula
can be thought of as abstractions that the Transformer uses
to represent different aspects of the words in a sentence.
The process of generating key (K) and query (Q) can be in-
tuitively understood as the Transformer “asking questions”
about certain words (the queries) and “looking up answers”
in other words (the keys). This “questioning” and “answer-
ing” process allows the Transformer to understand the in-
terdependencies between words in a sentence. The rele-
vance between one word’s key and another word’s query
is computed by taking the dot product of the respective
query and key (QKT), which is a measure of their sim-
ilarity. The softmax function transforms these relevance
scores into probability values that sum up to one. √dk is
a scaling factor that helps the model to be more trainable
and stable. Values (V ) are representations of the words
themselves. After the Transformer computes the similar-
ity between all queries and keys, it uses this information to
weigh the importance of each value and creates a weighted
sum of the values, which forms the output of the attention
mechanism. In a way, values are the “content” that the
Transformer wants to focus on, and the role of the keys
and queries is to determine how much attention each value
should be given.
Multi-head attention
The multi-head attention mecha-
nism in the Transformer model is a crucial step that allows
the model to capture different aspects of the meaning of a
sentence. Essentially, multi-head attention means running
not one, but multiple attention mechanisms (or “heads”) in
parallel, each focusing on different “types” or “aspects” of
the input sequence’s information. This is deﬁned in the fol-
lowing equations:
MultiHead(Q, K, V ) = Concat(head1, . . . , headh)W O,
(2)
where each attention head is computed as:
headi = Attention(QW Q
i , KW K
i , V W V
i ).
(3)
Each attention head applies the attention mechanism but
with different learned linear transformations (i.e., different
weight matrices W Q
i , W K
i , W V
i ). Because these transfor-
mations are different for each head, each head learns to
pay attention to different features in the input. After all
the attention heads have processed the input sequence inde-
pendently, the results from each head are concatenated and
then linearly transformed using another learned weight ma-
trix W O. This step integrates the different “perspectives”
from each head into a uniﬁed representation, which is then
passed onto the next layer in the Transformer model.
3. Comparative analysis of human attention
and attention in Transformers
We structure our review around key functions of attention
as understood from a cognitive perspective and distinguish
2

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
these based on their similarities or differences when com-
paring human attention to the Transformer architecture. We
outline a series of inﬂuential studies in cognitive science re-
garding the idea of human attention, from domains includ-
ing vision, language, and agency. We further list out some
recent work built on top of the Transformer architecture for
comparison.
3.1. Similarities
3.1.1. SELECTIVE ATTENTION
From a functional perspective, human attention mechanism
can be considered a selective process which allows us to
focus on a particular object or task while ﬁltering out ir-
relevant or distracting information. This phenomenon is
often described as the “cocktail party effect,” referring to
our ability to concentrate on a single conversation amidst
a noisy environment (Cherry, 1953).
Broadbent’s Filter
Model further suggested that humans tend to focus on spe-
ciﬁc elements while ﬁltering out others (Broadbent, 1958).
The Attenuation Model by Treisman modiﬁed this under-
standing by proposing that unattended information is not
completely ﬁltered out, but rather “turned down” or at-
tenuated (Treisman, 1964).
Later, (Deutsch & Deutsch,
1963) suggested that the selection of relevant stimuli hap-
pens later in the processing chain, indicating selective at-
tention operates not only on the perceptual level.
Sev-
eral lines of work deepened this understanding through re-
searching on spatial attention, where humans ﬁlter out spa-
tially nearby distractors for efﬁcient information process-
ing (Eriksen & Eriksen, 1974; Posner, 1980)
Similarly, the Transformer model applies a form of selec-
tive attention to process sequences of data (Vaswani et al.,
2017). Through its self-attention mechanism, the Trans-
former model calculates attention scores, or weights, for
each element in the input relative to all others. The compu-
tation of these weights is managed by a softmax function,
designed such that higher weights are allocated to larger in-
put values, while ensuring that the aggregate of all weights
sums up to one. In this way, if certain parts receive a higher
attention score, the remaining elements are accordingly as-
signed a smaller fraction of the attention. The mechanism
thus allow the Transformer to focus more on some and
less on others, echoing how humans tend to concentrate on
some inputs while neglecting others.
3.1.2. CONTEXTUAL UNDERSTANDING
Humans typically interpret sensory inputs within their
broader context, both in visual scenes and linguistic utter-
ances, a feature commonly referred to as contextual pro-
cessing. A series of studies demonstrated the effect of con-
text on object recognition (Palmer, 1975). When an object
is presented in a congruent scene (e.g., a loaf of bread in a
kitchen), people recognize it more quickly and accurately
than when it is in an incongruent scene (e.g., a loaf of bread
on a beach). A similar effect was found in linguistics stud-
ies, where researchers showed that when listeners hear an
ambiguous word, they do not perceive both meanings and
then select the appropriate one; instead, they immediately
interpret the word in light of the surrounding sentence con-
text (Tanenhaus et al., 1995). This suggests that context
is integrated in real time during language comprehension.
In studies on human decision-making, researchers revealed
how the phrasing of a problem can drastically change peo-
ple’s decisions, even when the underlying objective infor-
mation is the same. For instance, people may opt for a
surgery when told it has a 90% survival rate, but reject it
when told it has a 10% mortality rate, illustrating how our
decisions are shaped by the context in which information
is presented (Tversky & Kahneman, 1981). In this way, hu-
mans’ interpretation of sensory stimuli is informed not only
by the immediate properties of the stimuli themselves, but
also by their surroundings.
With a similar idea, the Transformer model is designed
to accommodate context in processing sequences of data.
This is often regarded as one of the major reasons why
Transformer models are competitive in multiple domains.
Speciﬁcally, the self-attention mechanism considers the en-
tire input sequence when processing each token. This de-
sign enables the model to accurately capture long-range de-
pendencies in the data, enriching the representation of each
token by incorporating the context provided by all other to-
kens in the sequence (Vaswani et al., 2017). This design
is particularly powerful in tasks such as machine transla-
tion and text summarization, where understanding the con-
textual relationships within the input sequence is critical
(Vaswani et al., 2017).
Recent work further take advan-
tage of this design and apply Transformers to sequential
decision making tasks, where the action-to-taken is depen-
dent on the context of past experience (Chen et al., 2021;
Meng et al., 2021).
3.2. Differences
3.2.1. CAPACITY CONSTRAINTS
One salient distinction between human attention and Trans-
former models comes from their capacity constraints. The
human attention system operates within constraints deﬁned
by perceptual and cognitive boundaries, including a lim-
ited visual ﬁeld and working memory capacity (Broadbent,
1958; Cowan, 2001). These biological constraints necessi-
tate the deployment of attention to prioritize and extract
relevant information.
The spotlight of human attention,
therefore, shifts and scales based on task demands and
environmental cues, enabling efﬁcient processing within
these capacity constraints (Carrasco, 2011). The capacity
3

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
constraint of human attention has been extensively investi-
gated in psychological studies particularly in visual atten-
tion. Starting with Broadbent’s Filter Model (Broadbent,
1958), it was proposed that human attention serves as a bot-
tleneck that allows only certain information to pass for fur-
ther processing due to limited capacity. Researchers further
introduced the concept of “selective looking”, showing that
when engaged in a demanding task, observers can fail to no-
tice unexpected events (Neisser & Becklen, 1975). In the
famous Invisible Gorilla Experiment (Simons & Chabris,
1999), participants watched a video where they were asked
to count basketball passes. During the video, a person in
a gorilla suit walked through the scene. Despite the con-
spicuousness of the event, half of the participants failed
to notice the gorilla, illustrating the limits of human atten-
tion capacity and the phenomenon of inattentional blind-
ness. The Load Theory of attention also suggested that
the level of perceptual load in a task determines the efﬁ-
ciency of selective attention (Lavie, 2005). The capacity
constraint is also evident in the visual search tasks, where
humans’ attentional capacity is guided by feature-based
and spatial-based information (Wolfe & Gray, 2007). From
this perspective, human attention is indeed a double-edged
sword: while it facilitates the extraction of needed informa-
tion from a plethora of sensory input, while simultaneously
imposing limits on what can be attended to at any one time
(Broadbent, 1958; Lavie, 2005). Moreover, due to the lim-
ited capacity of our cognitive resources, human attention
typically operates in a sequential manner, focusing on a re-
stricted set of inputs at any given moment and necessitating
constant shifts to process diverse environmental elements
(Broadbent, 1958; Dux et al., 2006).
In contrast, Transformer architectures, being artiﬁcial con-
structs, do not possess these inherent biological constraints.
Given adequate computational resources, they can process
all parts of the input in parallel, regardless of the input’s
size or complexity (Vaswani et al., 2017).
This parallel
processing capability enables Transformers to consider all
elements simultaneously, capturing dependencies and rela-
tionships across the entire sequence without the need to
sequentially shift focus as in human attention. Thus, at-
tention in Transformers can be viewed as a mechanism that
enables the model to extract contextual relationships within
the data, rather than a solution to a processing limitation.
3.2.2. ATTENTION PATHWAYS
Human attention is characterized by a dynamic in-
terplay
between
top-down
and
bottom-up
processes
(Corbetta & Shulman, 2002). Top-down attention is goal-
directed and driven by cognitive factors like knowledge
and expectations.
It allows us to selectively focus on
information that is relevant to our current task or inten-
tion. This idea is evident in neuroscience studies, where
researchers found that top-down inﬂuences bias neural rep-
resentation towards task-relevant information amidst com-
peting stimuli (Desimone & Duncan, 1995). There is also
enhanced sensory activity when anticipating a target stim-
ulus, thus underscoring the priming effect of top-down at-
tention (Egner & Hirsch, 2005). From an agency perspec-
tive, the top-down attention enables humans to employ var-
ious heuristics, achieving more efﬁcient decision-making
by directing attention towards relevant or expected stimuli
(Tversky & Kahneman, 1974). On the other hand, bottom-
up attention is stimulus-driven and automatic, which is
primarily guided by salient stimuli in the environment,
such as bright colors, loud noises, or sudden movements
(Itti & Koch, 2001; Chun et al., 2011). This type of atten-
tion helps us quickly identify potential threats or opportu-
nities in our surroundings without the need for conscious
control (Yantis, 1998). Together, human attention is indeed
a dual-pathway mechanism with top-down control guiding
the focus of attention, while bottom-up signals determining
instant attention shifts (Buschman & Miller, 2007).
In contrast to human attention, attention allocation in Trans-
former models is an entirely data-driven process.
The
attention weights within these models are learned dur-
ing the training phase, based solely on the input data
(Vaswani et al., 2017).
This design paradigm is limited
to one direction of information processing, wherein the
model aggregates lower-level features to construct higher-
level, semantically rich representations.
Consequently,
the attention focus within Transformers is shaped purely
by previous learning experiences, as opposed to the in-
tricate interplay of cognitive factors observed in humans
(Corbetta & Shulman, 2002).
3.2.3. INTENTIONAL NATURE
In this section, we compare the intentional nature of at-
tention in humans and the Transformers architecture. For
humans, the attention mechanism is not solely a passive
response to limited cognitive resources, but also a delib-
erate, controlled process.
The process of allocating at-
tention is an active decision-making mechanism, reﬂect-
ing an individual’s agency in consciously directing their
cognitive resources in alignment with their beliefs, goals,
and intentions, in line with Theory of Mind (Bratman,
1987). The ability to intentionally ignore extraneous in-
formation and focus on achieving one’s goal is critical to
human decision-making.
Effective self-regulation of at-
tention plays a crucial role in sustaining commitment to
tasks (Rueda et al., 2005; McClelland et al., 2007), espe-
cially for those requiring prolonged effort, such as complex
problem-solving and learning (Kanfer & Ackerman, 1989;
Muraven & Baumeister, 2000), as well as in tasks where
multiple outcomes are desirable (Cheng et al., 2022).
4

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
Furthermore, attention is a mental construct that signiﬁ-
cantly contributes to effective communication by serving as
a crucial signal for interpreting human intention, emotion,
and personality, often indicated by eye gaze. This nonver-
bal cue provides insightful information into an individual’s
thoughts and focus. Evolutionarily, humans have uniquely
developed a high color contrast between the white sclera
and the colored iris, unlike chimpanzees, facilitating dis-
cernment of gaze direction and, subsequently, intention and
emotions (Kobayashi & Kohshima, 1997; Tomasello et al.,
2007). This visibility of human eye gaze communicates
our attention and intention nonverbally with impressive ef-
ﬁcacy. Humans, even at a young age, are highly sensitive
to the gaze direction of others, enabling an understanding
of the intentions and mental states of the person they are
interacting with (Hood et al., 1998; Tomasello et al., 1995).
This sensitivity to gaze direction often culminates in gaze
alternation between the object of interest and the commu-
nication partner, signalling an intention to share attention,
thereby inviting social interaction (Tomasello et al., 2005).
Critically, such shared attention underpins various social
interactions and is instrumental in the development of lan-
guage, social cognition, and theory of mind. Hence, atten-
tion operates as a vital component of social cognition by
shaping our understanding of each other’ intentions.
On the other hand, although the determination of focus
in Transformers depends on the learned attention weights,
which resonates with the idea of a controlled process as
in human attention allocation, it is crucial to note that the
Transformer architecture does not possess inherent cogni-
tive states. The concept of “attention” within this context is
essentially a mathematical construct based on learned data
patterns. It remains unclear whether the representations
output from the attention mechanism coincides with what
one believes to be important from an agency perspective.
4. Potential Directions
Drawing upon the comparative analysis of human attention
and attention in the Transformer architecture presented thus
far, we now turn our focus to identifying salient open re-
search questions. The objective is to explore the extent to
which principles of human attention can guide the model-
ing of attention in artiﬁcial intelligence. Speciﬁcally, we
would like to note that not all characteristics of the human
attention mechanism may be desirable when translated into
the artiﬁcial intelligence domain. Careful judgement is thus
required to integrate only those characteristics of human at-
tention that are beneﬁcial in the context of artiﬁcial intelli-
gence while avoiding potential impediments.
4.1. Is emulating human capacity constraints
beneﬁcial?
Humans possess a limited capacity for cognitive resources
including a limited visual ﬁeld and working memory ca-
pacity (Broadbent, 1958; Cowan, 2001). These limitations
necessitate the need for attention as a cognitive tool that
selectively processes information based on its importance.
In contrast, Transformer architectures do not face such lim-
itations and handle large volumes of data simultaneously.
Despite the disparity, it is important to note that the ab-
sence of capacity constraints in Transformers does not de-
note a deﬁciency. In fact, one of the Transformer’s crucial
strengths lies in its ability to process large amount of in-
formation in parallel, which has proven critical in recent
large-scale models like GPT (Brown et al., 2020; OpenAI,
2023). Such models have demonstrated impressive perfor-
mance across a wide range of tasks by leveraging their ca-
pacity to parse and process massive corpora of textual data.
This computational efﬁciency is indispensable for AI to ef-
fectively aid humans in various ﬁelds, such as autonomous
driving, where it is crucial to augment human capabilities
and overcome cognitive limitations.
4.2. How can models adopt a resource-rational
approach from human attention?
From another perspective, while the Transformer ar-
chitecture is not bounded by the same cognitive re-
source limitations as humans, they may still glean in-
valuable insights from understanding the efﬁciency with
which humans allocate and utilize their limited resources.
From a resource-rational perspective (Grifﬁths et al., 2015;
Lieder & Grifﬁths, 2020), human cognition demonstrates
an important optimization between performance and re-
source allocation that allows us to operate effectively in
complex environments despite our cognitive constraints.
Although recent developments in large language models
(LLMs) have reached great success, training of the mod-
els typically demand substantial computational resources
(Brown et al., 2020; OpenAI, 2023). Unlike humans, who
can generate near-instantaneous responses to diverse stim-
uli, these models require signiﬁcant computation to pro-
duce comparable outputs. Hence, the challenge remains
for AI researchers: to maximize efﬁciency rather than to
merely increase capacity. Such economical use may thus
pave the way for the development of more robust, effective,
and resource-friendly models.
4.3. How can attention modeling generate meaningful
representations?
Human attention comes with inherent cognitive limita-
tions. This is evident in phenomena such as inattentional
blindness (Simons & Chabris, 1999) and change blindness
5

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
(Rensink et al., 1997), wherein humans overlook certain as-
pects of a visual scene.
Beyond indicating a limitation,
these phenomena indeed highlight humans’ ability to dis-
cern the semantically signiﬁcant components within a com-
plicated visual scene. The human visual attention system
does not construct an exhaustive, detailed representation of
the visual world; rather, it selects and processes only the
crucial segments (Treisman & Gelade, 1980; Treue, 2003).
This capacity for discernment underscores the impressive
human capability to extract meaningful information from
inputs, thereby crafting a useful representation. Contrar-
ily, models like Transformers treat processing as a predom-
inantly data-driven operation, and thus whether the pre-
served representation is semantically meaningful is still an
opaque question that relies on subjective judgements. To
develop more generalizable AI algorithms, meaningful rep-
resentation of one’s attention could serve as a critical com-
ponent to make AI more interpretable and thus trustworthy
(Doshi-Velez & Kim, 2017).
4.4. How can attention be formulated as an explicit
component of agency?
Beyond a functional construct of human perception, at-
tention further serves as a key element of human agency
that can be intentionally modulated and controlled.
At
an individual level, it operates as a self-regulation mecha-
nism to shield us from irrelevant and distracting informa-
tion (Kanfer & Ackerman, 1989; Muraven & Baumeister,
2000). In social interactions, attention functions as a con-
duit for understanding the intentions of others and facili-
tating communication (Bratman, 1987). Research in cog-
nitive psychology indicates that joint attention serves as
the foundation of human cooperation, forming a shared
commitment to a task (Tomasello et al., 2005; Tang et al.,
2020; 2022).
Indeed, theories suggest that the primary
objective of conversation is to manipulate each other’s at-
tention, thereby fostering a shared common ground of in-
formation among cooperative agents (Stacy et al., 2020;
2021; Fan et al., 2021). In this way, attention extends be-
yond merely representing an individual’s focus; it serves
as an external and explicit construct intertwined with
one’s agency and implies a more profound role in me-
diating human interactions and shaping social dynamics.
On the other hand, although recent studies incorporating
the Transformer-based architecture have emphasized learn-
ing from past experiences to decide on action strategies
(Chen et al., 2021; Janner et al., 2021), attention in these
models primarily serves to calculate the relationship be-
tween all pairs of elements in the input sequence, rather
than functioning as a mental mechanism to generate inten-
tional behavior. The concept of joint attention, central to
human cooperation from a cognitive science perspective,
is not explicitly incorporated in recent developments of
multi-agent algorithms based on Transformers (Meng et al.,
2021; Wen et al., 2022). As a result, it remains an open
question how we can more effectively incorporate the hu-
man perspective of attention into AI models.
5. Conclusion
In conclusion, our review drew a comparison between the
human attention mechanism and the Transformer architec-
ture, revealing a diverse range of similarities and differ-
ences. We hope the analysis can serve to highlight areas
where artiﬁcial intelligence might draw inspiration from
the intricacies of human attention while also keeping the
unique strengths of Transformer-based models.
References
Bahdanau, D., Cho, K., and Bengio, Y. Neural machine
translation by jointly learning to align and translate.
arXiv preprint arXiv:1409.0473, 2014.
Bratman, M. Intention, plans, and practical reason. 1987.
Broadbent, D. E. Perception and communication. Perga-
mon Press, 1958.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., et al. Language models are few-shot learn-
ers. Advances in neural information processing systems,
33:1877–1901, 2020.
Buschman, T. J. and Miller, E. K. Top-down versus bottom-
up control of attention in the prefrontal and posterior
parietal cortices. science, 315(5820):1860–1862, 2007.
Carion, N., Massa, F., Synnaeve, G., Usunier, N., Kirillov,
A., and Zagoruyko, S. End-to-end object detection with
transformers. In Computer Vision–ECCV 2020: 16th Eu-
ropean Conference, Glasgow, UK, August 23–28, 2020,
Proceedings, Part I 16, pp. 213–229. Springer, 2020.
Carrasco, M. Visual attention: The past 25 years. Vision
research, 51(13):1484–1525, 2011.
Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A.,
Laskin, M., Abbeel, P., Srinivas, A., and Mordatch, I. De-
cision transformer: Reinforcement learning via sequence
modeling. Advances in neural information processing
systems, 34:15084–15097, 2021.
Cheng, S., Zhao, M., Zhu, J., Zhou, J., Shen, M., and Gao,
T. Intentional commitment through an internalized the-
ory of mind: Acting in the eyes of an imagined observer.
In Proceedings of the Annual Meeting of the Cognitive
Science Society, volume 44, 2022.
6

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
Cherry, E. C.
Some experiments on the recognition of
speech, with one and with two ears. The Journal of the
acoustical society of America, 25(5):975–979, 1953.
Chorowski, J. K., Bahdanau, D., Serdyuk, D., Cho, K., and
Bengio, Y. Attention-based models for speech recogni-
tion. Advances in neural information processing systems,
28, 2015.
Chun, M. M., Golomb, J. D., and Turk-Browne, N. B. A
taxonomy of external and internal attention. Annual re-
view of psychology, 62:73–101, 2011.
Corbetta, M. and Shulman, G. L. Control of goal-directed
and stimulus-driven attention in the brain.
Nature re-
views neuroscience, 3(3):201–215, 2002.
Cowan, N. The magical number 4 in short-term memory:
A reconsideration of mental storage capacity. Behavioral
and brain sciences, 24(1):87–114, 2001.
Desimone, R. and Duncan, J. Neural mechanisms of selec-
tive visual attention. Annual review of neuroscience, 18
(1):193–222, 1995.
Deutsch, J. A. and Deutsch, D. Attention: Some theoretical
considerations. Psychological review, 70(1):80, 1963.
Devlin, J., Chang, M.-W., Lee, K., and Toutanova, K. Bert:
Pre-training of deep bidirectional transformers for lan-
guage understanding. arXiv preprint arXiv:1810.04805,
2018.
Doshi-Velez, F. and Kim, B.
Towards a rigorous sci-
ence of interpretable machine learning. arXiv preprint
arXiv:1702.08608, 2017.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer,
M., Heigold, G., Gelly, S., et al.
An image is worth
16x16 words: Transformers for image recognition at
scale. arXiv preprint arXiv:2010.11929, 2020.
Dux, P. E., Ivanoff, J., Asplund, C. L., and Marois, R.
Isolation of a central bottleneck of information process-
ing with time-resolved fmri. Neuron, 52(6):1109–1120,
2006.
Egner, T. and Hirsch, J.
Cognitive control mechanisms
resolve conﬂict through cortical ampliﬁcation of task-
relevant information. Nature neuroscience, 8(12):1784–
1790, 2005.
Eriksen, B. A. and Eriksen, C. W. Effects of noise letters
upon the identiﬁcation of a target letter in a nonsearch
task. Perception & psychophysics, 16(1):143–149, 1974.
Fan, L., Qiu, S., Zheng, Z., Gao, T., Zhu, S.-C., and Zhu, Y.
Learning triadic belief dynamics in nonverbal communi-
cation from videos. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recogni-
tion, pp. 7312–7321, 2021.
Grifﬁths, T. L., Lieder, F., and Goodman, N. D. Rational
use of cognitive resources: Levels of analysis between
the computational and the algorithmic. Topics in cogni-
tive science, 7(2):217–229, 2015.
Guo, M.-H., Xu, T.-X., Liu, J.-J., Liu, Z.-N., Jiang, P.-T.,
Mu, T.-J., Zhang, S.-H., Martin, R. R., Cheng, M.-M.,
and Hu, S.-M. Attention mechanisms in computer vision:
A survey. Computational Visual Media, 8(3):331–368,
2022.
Hood, B. M., Willen, J. D., and Driver, J. Adult’s eyes
trigger shifts of visual attention in human infants. Psy-
chological Science, 9(2):131–134, 1998.
Itti, L. and Koch, C. Computational modelling of visual
attention. Nature reviews neuroscience, 2(3):194–203,
2001.
Janner, M., Li, Q., and Levine, S. Ofﬂine reinforcement
learning as one big sequence modeling problem.
Ad-
vances in neural information processing systems, 34:
1273–1286, 2021.
Kanfer, R. and Ackerman, P. L.
Motivation and cogni-
tive abilities: An integrative/aptitude-treatment interac-
tion approach to skill acquisition.
Journal of applied
psychology, 74(4):657, 1989.
Khan, S., Naseer, M., Hayat, M., Zamir, S. W., Khan, F. S.,
and Shah, M. Transformers in vision: A survey. ACM
computing surveys (CSUR), 54(10s):1–41, 2022.
Kobayashi, H. and Kohshima, S. Unique morphology of
the human eye. Nature, 387(6635):767–768, 1997.
Lavie, N. Distracted and confused?: Selective attention un-
der load. Trends in cognitive sciences, 9(2):75–82, 2005.
Lieder, F. and Grifﬁths, T. L. Resource-rational analysis:
Understanding human cognition as the optimal use of
limited computational resources. Behavioral and brain
sciences, 43:e1, 2020.
Liu, Y. and Lapata, M. Text summarization with pretrained
encoders. arXiv preprint arXiv:1908.08345, 2019.
Luong, M.-T., Pham, H., and Manning, C. D. Effective ap-
proaches to attention-based neural machine translation.
arXiv preprint arXiv:1508.04025, 2015.
7

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
McClelland, M. M., Cameron, C. E., Connor, C. M., Farris,
C. L., Jewkes, A. M., and Morrison, F. J. Links between
behavioral regulation and preschoolers’ literacy, vocabu-
lary, and math skills. Developmental psychology, 43(4):
947, 2007.
Meng, L., Wen, M., Yang, Y., Le, C., Li, X., Zhang,
W., Wen, Y., Zhang, H., Wang, J., and Xu, B.
Of-
ﬂine pre-trained multi-agent decision transformer: One
big sequence model conquers all starcraftii tasks. arXiv
preprint arXiv:2112.02845, 2021.
Muraven, M. and Baumeister, R. F. Self-regulation and de-
pletion of limited resources: Does self-control resemble
a muscle? Psychological bulletin, 126(2):247, 2000.
Neisser, U. and Becklen, R. Selective looking: Attending
to visually speciﬁed events. Cognitive psychology, 7(4):
480–494, 1975.
OpenAI. Gpt-4 technical report. arXiv, 2023.
Palmer, t. E. The effects of contextual scenes on the identi-
ﬁcation of objects. Memory & cognition, 3(5):519–526,
1975.
Posner, M. I. Orienting of attention. Quarterly journal of
experimental psychology, 32(1):3–25, 1980.
Rensink, R. A., O’regan, J. K., and Clark, J. J. To see or
not to see: The need for attention to perceive changes in
scenes. Psychological science, 8(5):368–373, 1997.
Rueda, M. R., Posner, M. I., and Rothbart, M. K. The de-
velopment of executive attention: Contributions to the
emergence of self-regulation. Developmental neuropsy-
chology, 28(2):573–594, 2005.
Simons, D. J. and Chabris, C. F. Gorillas in our midst: Sus-
tained inattentional blindness for dynamic events. per-
ception, 28(9):1059–1074, 1999.
Stacy, S., Zhao, Q., Zhao, M., Kleiman-Weiner, M., and
Gao, T. Intuitive signaling through an “Imagined We”.
In CogSci, 2020.
Stacy, S., Li, C., Zhao, M., Yun, Y., Zhao, Q., Kleiman-
Weiner, M., and Gao, T. Modeling communication to
coordinate perspectives in cooperation. arXiv preprint
arXiv:2106.02164, 2021.
Tanenhaus, M. K., Spivey-Knowlton, M. J., Eberhard,
K. M., and Sedivy, J. C. Integration of visual and lin-
guistic information in spoken language comprehension.
Science, 268(5217):1632–1634, 1995.
Tang, N., Stacy, S., Zhao, M., Marquez, G., and Gao, T.
Bootstrapping an imagined we for cooperation. In Pro-
ceedings of the Annual Meeting of the Cognitive Science
Society (CogSci), 2020.
Tang, N., Gong, S., Zhao, M., Gu, C., Zhou, J., Shen, M.,
and Gao, T. Exploring an imagined “we” in human col-
lective hunting: Joint commitment within shared inten-
tionality. In Proceedings of the annual meeting of the
cognitive science society, volume 44, 2022.
Tomasello, M., Carpenter, M., Call, J., Behne, T., and Moll,
H. Understanding and sharing intentions: The origins of
cultural cognition. Behavioral and brain sciences, 28(5):
675–691, 2005.
Tomasello, M., Hare, B., Lehmann, H., and Call, J. Re-
liance on head versus eyes in the gaze following of great
apes and human infants: the cooperative eye hypothesis.
Journal of human evolution, 52(3):314–320, 2007.
Tomasello, M. et al.
Joint attention as social cognition.
Joint attention: Its origins and role in development,
103130:103–130, 1995.
Treisman, A.
Monitoring and storage of irrelevant mes-
sages in selective attention. Journal of Verbal Learning
and Verbal Behavior, 3(6):449–459, 1964.
Treisman, A. M. and Gelade, G. A feature-integration the-
ory of attention. Cognitive psychology, 12(1):97–136,
1980.
Treue, S. Visual attention: the where, what, how and why
of saliency. Current opinion in neurobiology, 13(4):428–
432, 2003.
Tversky, A. and Kahneman, D.
Judgment under uncer-
tainty: Heuristics and biases: Biases in judgments reveal
some heuristics of thinking under uncertainty. science,
185(4157):1124–1131, 1974.
Tversky, A. and Kahneman, D. The framing of decisions
and the psychology of choice. science, 211(4481):453–
458, 1981.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones,
L., Gomez, A. N., Kaiser, Ł., and Polosukhin, I.
At-
tention is all you need. Advances in neural information
processing systems, 30, 2017.
Wen, M., Kuba, J., Lin, R., Zhang, W., Wen, Y., Wang,
J., and Yang, Y. Multi-agent reinforcement learning is a
sequence modeling problem. Advances in Neural Infor-
mation Processing Systems, 35:16509–16521, 2022.
Wolfe, J. M. Visual attention. Seeing, pp. 335–386, 2000.
Wolfe, J. M. and Gray, W. Guided search 4.0. Integrated
models of cognitive systems, pp. 99–119, 2007.
Xu, K., Ba, J., Kiros, R., Cho, K., Courville, A., Salakhudi-
nov, R., Zemel, R., and Bengio, Y. Show, attend and
8

From Cognition to Computation: A Comparative Review of Human Attention and Transformer Architectures
tell: Neural image caption generation with visual atten-
tion. In International conference on machine learning,
pp. 2048–2057. PMLR, 2015.
Yantis, S. Control of visual attention. attention, 1(1):223–
256, 1998.
9

