Mathematical discoveries from program 
search with large language models
Bernardino R­om­er­a-­Pa­re­de­s, M­oh­am­ma­damin Barekatain, Alexander Novikov, Matej Balog, 
M. Pawan Kumar, Emilien Dupont, Francisco J. R. Ruiz, Jordan S. Ellenberg, Pengming Wang, 
Omar Fawzi, Pushmeet Kohli & Alhussein Fawzi
This is a PDF file of a peer-reviewed paper that has been accepted for publication. 
Although unedited, the content has been subjected to preliminary formatting. Nature 
is providing this early version of the typeset paper as a service to our authors and 
readers. The text and figures will undergo copyediting and a proof review before the 
paper is published in its final form. Please note that during the production process 
errors may be discovered which could affect the content, and all legal disclaimers 
apply.
Received: 12 August 2023
Accepted: 30 November 2023
Accelerated Article Preview 
Published online xx xx xxxx
Cite this article as: Romera-Paredes, B. et al. 
Mathematical discoveries from program 
search with large language models. Nature  
https://doi.org/10.1038/s41586-023-06924-6 
(2023)
https://doi.org/10.1038/s41586-023-06924-6
Nature  |  www.nature.com
Accelerated Article Preview
A
C
C
E
L
E
R
A
T
E
D
 A
R
T
I
C
L
E
 P
R
E
V
I
E
W

Mathematical discoveries from program search with large
1
language models
2
Bernardino Romera-Paredes1∗
Mohammadamin Barekatain1∗
3
Alexander Novikov1∗
Matej Balog1∗
M. Pawan Kumar1∗
4
Emilien Dupont1∗
Francisco J. R. Ruiz1∗
Jordan S. Ellenberg2
5
Pengming Wang1
Omar Fawzi3
Pushmeet Kohli1
Alhussein Fawzi1∗
6
1Google DeepMind, London, UK
7
2University of Wisconsin-Madison, Madison, Wisconsin, USA
8
3Universit´e de Lyon (Inria, ENS Lyon, UCBL, LIP), Lyon, France
9
Abstract
10
Large Language Models (LLMs) have demonstrated tremendous capabilities in solving com-
11
plex tasks, from quantitative reasoning to understanding natural language. However, LLMs
12
sometimes suffer from confabulations (or hallucinations) which can result in them making plau-
13
sible but incorrect statements (Bang et al., 2023; Borji, 2023). This hinders the use of current
14
large models in scientific discovery. Here we introduce FunSearch (short for searching in the
15
function space), an evolutionary procedure based on pairing a pre-trained LLM with a system-
16
atic evaluator. We demonstrate the effectiveness of this approach to surpass the best known re-
17
sults in important problems, pushing the boundary of existing LLM-based approaches (Lehman
18
et al., 2022). Applying FunSearch to a central problem in extremal combinatorics — the cap
19
set problem — we discover new constructions of large cap sets going beyond the best known
20
ones, both in finite dimensional and asymptotic cases. This represents the first discoveries made
21
for established open problems using LLMs. We showcase the generality of FunSearch by apply-
22
ing it to an algorithmic problem, online bin packing, finding new heuristics that improve upon
23
widely used baselines. In contrast to most computer search approaches, FunSearch searches for
24
programs that describe how to solve a problem, rather than what the solution is. Beyond being
25
an effective and scalable strategy, discovered programs tend to be more interpretable than raw
26
solutions, enabling feedback loops between domain experts and FunSearch, and the deployment
27
of such programs in real-world applications.
28
Many problems in mathematical sciences are “easy to evaluate,” despite being typically “hard to
29
solve.” For example, in computer science, NP-complete optimization problems admit a polynomial-
30
time evaluation procedure (measuring the quality of the solution), despite the widespread belief that
31
no polynomial-time algorithms to solve such problems exist. We focus in this paper on problems
32
admitting an efficient evaluate function, which measures the quality of a candidate solution. Promi-
33
nent examples include the maximum independent set problem and maximum constraint satisfaction
34
problems (such as finding the ground state energy of a Hamiltonian). Our goal is to generate a
35
solve program, such that its outputs receive high scores from evaluate (when executed on inputs
36
of interest), and ultimately improve over the best known solutions.
37
∗Equal contributors.
1
ACCELERATED ARTICLE PREVIEW

While Large Language Models (LLMs) have recently seen dramatic improvements in their coding
38
capabilities [5–9], with applications including debugging [10, 11], solving code competitions [12, 13]
39
and improving code performance [14], synthesizing solve programs for open problems requires find-
40
ing new ideas that are verifiably correct. This is very hard for LLMs, as they tend to confabulate or
41
ultimately fall short of going beyond existing results. To surpass the “nominal” capabilities of LLMs,
42
recent works [3] have combined them with evolutionary algorithms [15, 16], leading to important
43
improvements on diverse synthetic problems [17], searching for neural network architectures [18–20],
44
and solving puzzles [21]. Our proposed method, FunSearch, pushes the boundary of LLM-guided
45
evolutionary procedures to a new level: the discovery of new scientific results for established open
46
problems, and the discovery of new algorithms. Surpassing state-of-the-art results on established
47
open problems provides a clear indication that the discoveries are truly new, as opposed to being
48
retrieved from the LLM’s training data.
49
FunSearch (short for searching in the function space) combines a pre-trained (frozen) Large Lan-
50
guage Model, whose goal is to provide creative solutions, with an evaluator, which guards against
51
confabulations and incorrect ideas. FunSearch iterates over these two components, evolving initial
52
low-scoring programs into high-scoring ones discovering new knowledge. Key to the success of this
53
simple procedure is a combination of multiple essential ingredients. First, we sample best performing
54
programs and feed them back into prompts for the LLM to improve on; we refer to this as best-shot
55
prompting. Second, we start with a program in the form of a skeleton (containing boilerplate code
56
and potentially prior structure about the problem), and only evolve the part governing the critical
57
program logic. For example, by setting a greedy program skeleton, we evolve a priority function
58
used to make decisions at every step. Third, we maintain a large pool of diverse programs by using
59
an island-based evolutionary method that encourages exploration and avoids local optima. Finally,
60
leveraging the highly parallel nature of FunSearch, we scale it asynchronously, considerably broad-
61
ening the scope of this approach to find new results, while keeping the overall cost of experiments
62
low.
63
We show the surprising effectiveness of FunSearch on several use-cases. We consider a fundamen-
64
tal problem in extremal combinatorics, namely, the cap set problem [22, 23]. FunSearch demonstrates
65
the existence of hitherto unknown constructions that go beyond existing ones, including the largest
66
improvement in 20 years to the asymptotic lower bound. To the best of our knowledge, this shows
67
the first scientific discovery — a new piece of verifiable knowledge about a notorious scientific prob-
68
lem — using an LLM. Using FunSearch, we also find new algorithms for the online bin packing
69
problem that improve upon traditional ones on well-studied distributions of interest [24, 25], with
70
potential applications to improving job scheduling algorithms.
71
While most computer search techniques output directly what the solution is (e.g., a list of vectors
72
forming a cap set), FunSearch produces programs generating the solution. For structured problems,
73
such programs tend to be more interpretable — facilitating interactions with domain experts —
74
and concise — making it possible to scale to large instances — compared to a mere enumeration
75
of the solution. In addition, decision procedures (such as for bin packing) described by code in a
76
standard programming language are crucially easier to deploy compared to other types of descriptions
77
(e.g., neural networks), which typically require specialized hardware and for which verifying design
78
specifications is notoriously hard.
79
1
FunSearch
80
An overview of FunSearch is shown in Figure 1, and its components are described in more detail
81
below. For more details and ablations showing the importance of each component, see Methods and
82
2
ACCELERATED ARTICLE PREVIEW

Appendix A in Supplementary Information.
83
Specification.
The input to FunSearch is a specification of the problem in the form of an evaluate
84
function, which scores candidate solutions. In addition, we provide an initial program (which can
85
be trivial) to evolve. While in principle these are the minimum requirements, we found that perfor-
86
mance tends to improve significantly if we write the initial solve program in the form of a skeleton
87
(containing boilerplate code and prior knowledge of the problem in the form of a program struc-
88
ture), and only use FunSearch to evolve the critical part that governs its logic. Figure 2 (a) shows
89
an example where the skeleton takes the form of a simple greedy algorithm, and the crucial part to
90
evolve by FunSearch is the priority function that is used to make the greedy decision at every step.
91
This delegates to FunSearch precisely the part that is usually the hardest to come up with. While
92
a fixed skeleton may constrain the space of programs that can be discovered, we find it improves
93
overall results because it focuses the LLM resources on evolving the critical part only, instead of also
94
using the LLM to recreate already known program structures (with more opportunities for mistakes
95
that would render the entire program incorrect). If available, the user can optionally provide addi-
96
tional known information about the problem at hand, in the form of docstrings, relevant primitive
97
functions, or import packages, which FunSearch may use.
98
Pre-trained LLM.
The LLM is the creative core of FunSearch, in charge of coming up with
99
improvements to the functions presented in the prompt and sending these for evaluation. Perhaps
100
surprisingly, we obtain our results with a pre-trained model, i.e., without any fine-tuning on our
101
problems. We use Codey, an LLM built on top of the PaLM2 model family [26], which has been
102
finetuned on a large corpus of code and is publicly accessible through its API [27]. Because FunSearch
103
relies on sampling from an LLM extensively, an important performance-defining tradeoff is between
104
the quality of the samples and the inference speed of the LLM. In practice, we have chosen to work
105
with a fast-inference model (rather than slower-inference, higher-quality), and the results in the
106
paper are obtained using a total number of samples on the order of 106. Beyond this tradeoff, we
107
have empirically observed that the results obtained in this paper are not too sensitive to the exact
108
choice of LLM, as long as it has been trained on a large enough corpus of code. See Appendix A in
109
Supplementary Information for a comparison to StarCoder [7], a state-of-the-art open-source LLM
110
for code.
111
Evaluation.
Programs generated by the LLM are evaluated and scored on a set of inputs. For
112
example, in the cap set problem (Section 2.1) the inputs are the values of the dimensionality n
113
that we are interested in, and in combinatorial optimization (Section 2.2), the inputs correspond
114
to different bin packing instances. The scores across different inputs are then combined into an
115
overall score of the program using an aggregation function, such as the mean. The scored programs
116
are then sent to the programs database. Programs that were incorrect (did not execute within the
117
imposed time and memory limits, or produced invalid outputs) are discarded, and the remaining
118
scored programs are then sent to the programs database.
119
Programs database.
The programs database keeps a population of correct programs, which are
120
then sampled to create prompts. Preserving and encouraging diversity of programs in the database is
121
crucial to enable exploration and avoid being stuck in local optima. To encourage diversity we adopt
122
an islands model, also known as multiple population and multiple-deme model [28, 29], a genetic
123
algorithm approach. A number of islands, or subpopulations, are created and evolved independently.
124
To sample from the program database, we first sample an island and then sample a program within
125
3
ACCELERATED ARTICLE PREVIEW

that island, favoring higher-scoring and shorter programs (see Methods for the exact mechanism).
126
Crucially, we let information flow between the islands by periodically discarding the programs in the
127
worst half of the islands (corresponding to the ones whose best individuals have the lowest scores).
128
We replace the programs in those islands with a new population, initialized by cloning one of the
129
best individuals from the surviving islands.
130
Prompt.
New prompts are created by “best-shot prompting” from the programs database, and
131
are then fed to the LLM to generate a new program. We first sample k programs from a single island
132
in the programs database, according to the procedure described above. Sampled programs are then
133
sorted according to their score, and a version is assigned to each (v0 for the lowest scoring program,
134
v1 for the second lowest scoring, etc.). These programs are then combined into a single prompt —
135
with the version appended as a suffix to the function name; e.g., in the case of Figure 2 (a), this
136
would be priority v0, priority v1, ... — and the header of the function we wish to generate
137
(e.g., priority vk) is added to the end of the prompt. In practice, we set k = 2, as two functions
138
lead to better results compared to just one, with diminishing returns beyond that. Constructing a
139
prompt by combining several programs (as opposed to only one) enables the LLM to spot patterns
140
across the different programs and generalize those. Related approaches to prompt building have
141
been recently considered; e.g., [17], and were shown to perform well on different domains.
142
Distributed approach.
We implement FunSearch as a distributed system that has three types
143
of workers: a programs database, samplers, and evaluators, which communicate asynchronously. The
144
programs database stores and serves programs, samplers generate new functions using the pre-trained
145
LLM, while evaluators assess programs, as shown in Figure F.26 in Supplementary Information. In
146
the example of Figure 2 (a), the programs database stores priority functions, samplers generate
147
new implementations of priority, while evaluators score the proposals by executing the main func-
148
tion on user-specified inputs. Our distributed system offers several advantages: first, it naturally
149
leverages parallelism across different tasks, e.g., LLM sampling and evaluation are performed con-
150
currently. Second, it enables scaling to more than one sampler and evaluator, which would be a
151
very limiting setup, considering that evaluation can take minutes for many problems of interest.
152
Running evaluators in parallel considerably broadens the scope of this approach to such problems.
153
The distributed setting enables running many evaluator nodes on inexpensive CPU hardware, while
154
few samplers run on machines with accelerators for fast LLM inference; this keeps the overall cost
155
and energy usage of experiments low. In our experiments, we typically use 15 samplers and 150 CPU
156
evaluators (can be served on 5 CPU servers each running 32 evaluators in parallel). See Appendix
157
A in Supplementary Information for more details. Also, due to the randomness of LLM sampling
158
and of the evolutionary procedure, for some problems we run several experiments to get the best
159
reported results. See Methods and Appendix A.3 in Supplementary Information for a full statistical
160
analysis.
161
2
Results
162
We now describe some of the new discoveries made by FunSearch in two different fields: pure math-
163
ematics and applied computer science. Additional discoveries on other problems (namely, corners
164
problem and Shannon capacity of cycle graphs) are presented in Appendix B in Supplementary
165
Information. Full discovered programs are available in Appendix C in Supplementary Information.
166
4
ACCELERATED ARTICLE PREVIEW

2.1
Extremal combinatorics
167
We apply FunSearch to two related problems in extremal combinatorics — a branch of mathematics
168
that studies the maximal (or minimal) possible sizes of sets satisfying certain properties.
169
Cap sets.
The cap set problem [22], once described by Terence Tao as “perhaps my favourite open
170
question” [30], refers to the task of finding the largest possible set of vectors in Zn
3 (known as a cap
171
set) such that no three vectors sum to zero. Geometrically, no three points of a cap set lie on a line
172
(see Figure 3 for an example with n = 2).
173
The problem has drawn much interest for a variety of reasons. For one, it is an analogue of
174
the classical number theory problem of finding large subsets of primes in which no three are in
175
arithmetic progression. For another, it differs from many problems in combinatorics in that there
176
is no consensus among mathematicians regarding what the right answer should be. Finally, the
177
problem serves as a model for the many other problems involving “three-way interactions.” For
178
instance, progress towards improved upper bounds for the cap set problem [31, 32] immediately led
179
to a series of other combinatorial results, e.g., on the Erd¨os-Radio sunflower problem [33].
180
The exact size of the largest possible cap set in n dimensions is known only for n ≤6.
A
181
brute force approach is not practical as the search space quickly becomes enormous with growing
182
n, e.g., around 31600 for n = 8. Previous methods impose potentially suboptimal restrictions on the
183
search space [34, 35]. In contrast, we search the full space via an algorithm skeleton that utilises a
184
function priority : Zn
3 →R. Intuitively, this function provides a priority with which each x ∈Zn
3
185
should be included in the cap set. Our algorithm starts with an empty set and iteratively adds the
186
vector x ∈Zn
3 with the highest priority that does not violate the cap set constraint; see Figure 2
187
(a). Starting from a trivial constant function, we evolve the crucial priority component of our
188
approach to result in large cap sets.
189
Using this approach we discovered cap sets of sizes shown in Figure 4 (a). Notably, in dimension
190
n = 8, FunSearch found a larger cap set than what was previously known, thus illustrating the
191
power of FunSearch to discover novel constructions. This also shows the scalability of FunSearch to
192
larger dimensions, where the previously best known construction relied on a complex combination
193
of cap sets in lower dimensions [34, 35]. In contrast, FunSearch discovered a larger cap set from
194
scratch, without having to be explicitly taught any way of combining cap sets. Moreover, we do not
195
just discover the set of 512 8-dimensional vectors in itself, but a program that generates it: we show
196
this program in Figure 4 (b). Through inspecting the code, we obtain a degree of understanding
197
of what this set is: specifically, manual simplification of Figure 4 (b) provides the construction in
198
Figure 4 (c). Some properties of this construction are strikingly similar to the construction of the
199
Hill cap [36, 37], which results in the optimal 112-cap in Z6
3.
200
Admissible sets.
Beyond finding the size of the largest cap set cn in dimension n, a fundamental
201
problem in additive combinatorics [23] is determining the capacity C = supn c1/n
n
. The breakthrough
202
result of [32] established an upper bound of C ≤2.756. In this work, we are interested in lower
203
bounds on C. To this end, we use the framework of constant weight admissible sets (or admissible
204
sets for short) [35], which has established the current state-of-the-art.
205
Formally, admissible sets A(n, w) are collections of vectors in {0, 1, 2}n satisfying two properties:
206
i) each vector has the same number w of non-zero elements but a unique support (thereby implying
207
|A| ≤
 n
w

); ii) for any three distinct vectors there is a coordinate in which their three respective
208
values are {0, 1, 2}, {0, 0, 1}, or {0, 0, 2}. Informally, an admissible set describes how to combine
209
cap sets in smaller dimensions into large cap sets in higher dimensions [35]. We denote the set of
210
5
ACCELERATED ARTICLE PREVIEW

full-size admissible sets (with |A| =
 n
w

) as I(n, w). The current state-of-the-art [39] has relied on
211
SAT solvers to construct large admissible sets.
212
As before, we evolve a function priority : {0, 1, 2}n →R, which is used to iteratively grow
213
admissible sets. Starting from a trivial constant function, we discover one that provides us with
214
an I(12, 7) admissible set; the discovered program is shown in Figure 5 (b). This discovery alone
215
already improves the lower bound on the cap set capacity from 2.2180 [39] to 2.2184. Yet, interpreting
216
the program found by FunSearch (Figure 5 b) helps us significantly push the boundaries of what
217
admissible sets we can construct.
Specifically, we notice that the discovered priority function
218
treats the n coordinates in a highly symmetric way, and indeed it turns out that the admissible set
219
it constructs is preserved under independent cyclic permutations of coordinates within four disjoint
220
groups of coordinate triples. Hereinafter we call such admissible sets symmetric (see Appendix D in
221
Supplementary Information for a formal definition).
222
We now use FunSearch to directly search for symmetric admissible sets. Note that this is a more
223
restricted but also much smaller search space, which allows for significantly higher dimensions and
224
weights than were previously possible. This led us to discovering a full-size I(15, 10) admissible set
225
(implying C ≥2.219486) and a partial admissible set in A(24, 17) of size 237 984, which implies
226
a new lower bound on the cap set capacity of 2.2202 (see Figure 5 a). While this is the largest
227
improvement to the lower bound in the last 20 years, we note it is still far from the upper bound,
228
and we hope our results inspire future work on this problem.
229
Not only does FunSearch scale to much larger instances than traditional combinatorial solvers
230
(see Appendix A.4 in Supplementary Information), it is a unique feature of searching in function
231
space that we were able to inspect the code discovered by FunSearch and infer a new insight into
232
the problem, in the form of a new symmetry. The procedure we followed in this section is a concrete
233
example of how LLM-based approaches can be used in mathematical sciences: FunSearch suggests
234
a solution, which is examined by researchers, who may note features of interest. These features are
235
used to refine the search, leading to better solutions. This process can be iterated, with both human
236
and search consistently in the loop.
237
2.2
Bin packing
238
Combinatorial optimization is a subfield of mathematics which plays an important role across a wide
239
range of areas, from theoretical computer science to practical problems in logistics and scheduling.
240
While many combinatorial optimization problems are provably hard to solve for large instances, it
241
is typically possible to achieve strong performance using heuristics to guide the search algorithm.
242
The choice of a heuristic is crucial for obtaining strong performance, but designing a good heuristic
243
is difficult in practice. In this section, we show that FunSearch can be used to discover effective
244
heuristics for one of the central problems in combinatorial optimization: bin packing [4].
245
The goal of bin packing is to pack a set of items of various sizes into the smallest number of
246
fixed-sized bins. Bin packing finds applications in many areas, from cutting materials to scheduling
247
jobs on compute clusters. We focus on the online setting where we pack an item as soon as it is
248
received (as opposed to the offline setting where we have access to all items in advance). Solving
249
online bin packing problems then requires designing a heuristic for deciding which bin to assign an
250
incoming item to.
251
Heuristics for online bin packing are well studied and several variants exist with strong worst
252
case performance [40–45]. However, they often exhibit poor performance in practice [4]. Instead, the
253
most commonly used heuristics for bin packing are first fit and best fit. First fit places the incoming
254
item in the first bin with enough available space, while best fit places the item in the bin with least
255
available space where the item still fits. Here, we show that FunSearch discovers better heuristics
256
6
ACCELERATED ARTICLE PREVIEW

OR1
OR2
OR3
OR4
Weibull 5k
Weibull 10k
Weibull 100k
First Fit
6.42%
6.45%
5.74%
5.23%
4.23%
4.20%
4.00%
Best Fit
5.81%
6.06%
5.37%
4.94%
3.98%
3.90%
3.79%
FunSearch
5.30%
4.19%
3.11%
2.47%
0.68%
0.32%
0.03%
Table 1: Online bin packing results. Fraction of excess bins (lower is better) for various bin
packing heuristics on the OR and Weibull datasets. FunSearch outperforms first fit and best fit
across problems and instance sizes.
than first fit and best fit on simulated data.
257
To achieve this, we define a heuristic as a program that takes as input an item and an array
258
of bins (containing the remaining capacity of each bin) and returns a priority score for each bin.
259
The solve function picks the bin with the highest score according to the heuristic (see Figure 2 b).
260
FunSearch is then used to evolve this heuristic, starting from best fit.
261
We first evaluate FunSearch on the well-known OR-Library bin packing benchmarks [24], con-
262
sisting of four datasets, OR1 to OR4, containing bin packing instances with an increasing number
263
of items (see Appendix E.4 in Supplementary Information for details). We evolve our heuristic on
264
a training set of generated bin packing instances with the same number of items as those in OR1
265
and, after the evolutionary process is concluded, test it on the OR1 to OR4 datasets. We measure
266
performance as the fraction of excess bins used over the L2 lower bound [46] of the optimal offline
267
packing solution (which is generally not achievable in the online setting).
268
As can be seen in Table 1, FunSearch outperforms both first fit and best fit across all datasets.
269
Further, the learned heuristic generalizes: even though it has only seen instances of the same size as
270
OR1 during training, it generalizes across problem sizes, performing even better on large instances
271
and widening the gap to best fit. In addition to the OR benchmarks, we also use FunSearch to evolve
272
heuristics on bin packing instances sampled from a Weibull distribution, as these closely follow many
273
real-world scheduling problems [25, 47] (see Appendix E.4 in Supplementary Information for details).
274
As shown in Table 1, the performance of FunSearch is very strong on this dataset, significantly
275
outperforming first fit and best fit across instances, as well as scaling gracefully to large instances
276
(being only 0.03% off the lower bound on the optimum for 100 000 items). In addition, FunSearch is
277
robust and consistently outperforms these baselines as shown in the statistical analysis in Appendix
278
A.3 in Supplementary Information.
279
We observed that several heuristics discovered by FunSearch use the same general strategy for
280
bin packing (see Figure 6 for an example). Instead of packing items into bins with the least capacity
281
(like best fit), the FunSearch heuristics assign items to least capacity bins only if the fit is very tight
282
after placing the item. Otherwise, the item is typically placed in another bin which would leave
283
more space after the item is placed. This strategy avoids leaving small gaps in bins that are unlikely
284
to ever be filled (see Appendix E.5 in Supplementary Information for example visualizations of such
285
packings).
286
As this example demonstrates, the benefits of FunSearch extend beyond theoretical and mathe-
287
matical results to practical problems like bin packing. Indeed, bin packing, and related combinatorial
288
optimization problems, are ubiquitous and find applications across a range of industries. We are
289
optimistic that FunSearch could be applied to several such use-cases with potential for real-world
290
impact.
291
7
ACCELERATED ARTICLE PREVIEW

3
Discussion
292
The effectiveness of FunSearch in discovering new knowledge for hard problems might seem intrigu-
293
ing. We believe that the LLM used within FunSearch does not use much context about the problem;
294
the LLM should instead be seen as a source of diverse (syntactically correct) programs with occa-
295
sionally interesting ideas. When further constrained to operate on the crucial part of the algorithm
296
with a program skeleton, the LLM provides suggestions that marginally improve over existing ones
297
in the population, which ultimately results in discovering new knowledge on open problems when
298
combined with the evolutionary algorithm. Another crucial component of the effectiveness of Fun-
299
Search is that it operates in the space of programs: rather than directly searching for constructions
300
(which is typically an enormous list of numbers), FunSearch searches for programs generating those
301
constructions. Because most problems we care about are structured (highly non-random), we hy-
302
pothesize that solutions are described more concisely with a computer program, compared to other
303
representations. For example, the trivial representation of the admissible set A(24, 17) consists of
304
more than 200 000 vectors, but the program generating this set consists only of a few lines of code.
305
Because FunSearch implicitly encourages concise programs, it scales to much larger instances com-
306
pared to traditional search approaches in structured problems. In a loose sense, FunSearch attempts
307
to find solutions that have low Kolmogorov complexity [48–50] (which is the length of the short-
308
est computer program that produces a given object as output), while traditional search procedures
309
have a very different inductive bias. We believe that such Kolmogorov-compressed inductive bias
310
is key to FunSearch scaling up to the large instances in our use-cases. In addition to scale, we
311
have empirically observed that FunSearch outputs programs that tend to be interpretable — that
312
is, they are clearly easier to read and understand compared to a list of numbers. For example, by
313
scrutinizing FunSearch’s output for the admissible set problem, we found a new symmetry, which
314
was then subsequently used to improve the results even further. Despite the rarity of symmetric
315
solutions, we observe that FunSearch preferred symmetric ones, as these are more parsimonious
316
(that is, they require less information to specify), in addition to the natural bias of LLMs (trained
317
on human-produced code) in outputting code with similar traits to human code. This is in contrast
318
to traditional genetic programming which do not have this bias (and in addition require hand-tuning
319
the mutation operators [51]).
320
We note that FunSearch currently works best for problems having the following characteristics:
321
a) availability of an efficient evaluator; b) a “rich” scoring feedback quantifying the improvements
322
(as opposed to a binary signal); c) ability to provide a skeleton with an isolated part to be evolved.
323
For example, the problem of generating proofs for theorems [52–54] falls outside this scope, since
324
it is unclear how to provide a rich enough scoring signal. In contrast, for MAX-SAT, the number
325
of satisfied clauses can be used as a scoring signal. In this paper, we have explicitly striven for
326
simplicity and we are confident that FunSearch can be further extended to improve its performance
327
and be applicable to more classes of problems.
In addition, the rapid development of LLMs is
328
likely to result in samples of far superior quality at a fraction of the cost, making FunSearch more
329
effective at tackling a broad range of problems. As a result, we envision that automatically-tailored
330
algorithms will soon become common practice and deployed in real-world applications.
331
References
332
[1] Y. Bang, S. Cahyawijaya, N. Lee, W. Dai, D. Su, B. Wilie, H. Lovenia, Z. Ji, T. Yu, W. Chung,
333
et al., A multitask, multilingual, multimodal evaluation of ChatGPT on reasoning, hallucina-
334
tion, and interactivity, arXiv preprint arXiv:2302.04023 (2023).
335
8
ACCELERATED ARTICLE PREVIEW

[2] A. Borji, A categorical archive of ChatGPT failures, arXiv preprint arXiv:2302.03494 (2023).
336
[3] J. Lehman, J. Gordon, S. Jain, K. Ndousse, C. Yeh, K. O. Stanley, Evolution through large
337
models, arXiv preprint arXiv:2206.08896 (2022).
338
[4] E. G. Coffman, M. R. Garey, D. S. Johnson, Approximation algorithms for bin-packing—an
339
updated survey, Algorithm design for computer system design (1984) 49–106.
340
[5] M. Chen, J. Tworek, H. Jun, Q. Yuan, H. P. d. O. Pinto, J. Kaplan, H. Edwards, Y. Burda,
341
N. Joseph, G. Brockman, et al., Evaluating large language models trained on code, arXiv
342
preprint arXiv:2107.03374 (2021).
343
[6] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan, E. Jiang, C. Cai, M. Terry,
344
Q. Le, et al., Program synthesis with large language models, arXiv preprint arXiv:2108.07732
345
(2021).
346
[7] R. Li, L. B. Allal, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone, C. Akiki, J. Li,
347
J. Chim, et al., StarCoder: may the source be with you!, arXiv preprint arXiv:2305.06161
348
(2023).
349
[8] D. Fried, A. Aghajanyan, J. Lin, S. Wang, E. Wallace, F. Shi, R. Zhong, W.-t. Yih, L. Zettle-
350
moyer, M. Lewis, Incoder: A generative model for code infilling and synthesis, in: International
351
Conference on Learning Representations, 2022.
352
[9] E. Nijkamp, B. Pang, H. Hayashi, L. Tu, H. Wang, Y. Zhou, S. Savarese, C. Xiong, CodeGen:
353
An open large language model for code with multi-turn program synthesis, in: International
354
Conference on Learning Representations, 2022.
355
[10] X. Chen, M. Lin, N. Sch¨arli, D. Zhou, Teaching large language models to self-debug, arXiv
356
preprint arXiv:2304.05128 (2023).
357
[11] V. Liventsev, A. Grishina, A. H¨arm¨a, L. Moonen, Fully autonomous programming with large
358
language models, arXiv preprint arXiv:2304.10423 (2023).
359
[12] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, T. Eccles, J. Keeling,
360
F. Gimeno, A. Dal Lago, et al., Competition-level code generation with alphacode, Science 378
361
(2022) 1092–1097.
362
[13] E. Zelikman, Q. Huang, G. Poesia, N. D. Goodman, N. Haber, Parsel: A (de-) compositional
363
framework for algorithmic reasoning with language models, arXiv preprint arXiv:2212.10561
364
(2023).
365
[14] A. Madaan, A. Shypula, U. Alon, M. Hashemi, P. Ranganathan, Y. Yang, G. Neubig, A. Yazdan-
366
bakhsh, Learning performance-improving code edits, arXiv preprint arXiv:2302.07867 (2023).
367
[15] D. E. Goldberg, Optimization and machine learning, 1989.
368
[16] J. R. Koza, Genetic programming as a means for programming computers by natural selection,
369
Statistics and computing 4 (1994) 87–112.
370
[17] E. Meyerson, M. J. Nelson, H. Bradley, A. Moradi, A. K. Hoover, J. Lehman, Language model
371
crossover: Variation through few-shot prompting, arXiv preprint arXiv:2302.12170 (2023).
372
9
ACCELERATED ARTICLE PREVIEW

[18] A. Chen, D. M. Dohan, D. R. So,
EvoPrompting: Language models for code-level neural
373
architecture search, arXiv preprint arXiv:2302.14838 (2023).
374
[19] M. Zheng, X. Su, S. You, F. Wang, C. Qian, C. Xu, S. Albanie, Can GPT-4 perform neural
375
architecture search?, arXiv preprint arXiv:2304.10970 (2023).
376
[20] M. U. Nasir, S. Earle, J. Togelius, S. James, C. Cleghorn, LLMatic: Neural architecture search
377
via large language models and quality-diversity optimization, arXiv preprint arXiv:2306.01102
378
(2023).
379
[21] P. Haluptzok, M. Bowers, A. T. Kalai, Language models can teach themselves to program
380
better (2022).
381
[22] J. Grochow, New applications of the polynomial method: the cap set conjecture and beyond,
382
Bulletin of the American Mathematical Society 56 (2019) 29–64.
383
[23] T. Tao, V. H. Vu, Additive combinatorics, volume 105, Cambridge University Press, 2006.
384
[24] J. E. Beasley, Or-library: distributing test problems by electronic mail, Journal of the opera-
385
tional research society 41 (1990) 1069–1072.
386
[25] I. Casti˜neiras, M. De Cauwer, B. O’Sullivan, Weibull-based benchmarks for bin packing, in:
387
International Conference on Principles and Practice of Constraint Programming, Springer, 2012,
388
pp. 207–222.
389
[26] R. Anil, A. M. Dai, O. Firat, M. Johnson, D. Lepikhin, A. Passos, S. Shakeri, E. Taropa,
390
P. Bailey, Z. Chen, et al., Palm 2 technical report, arXiv preprint arXiv:2305.10403 (2023).
391
[27] Code models overview, https://cloud.google.com/vertex-ai/docs/generative-ai/code/
392
code-models-overview, 2023. [Online; accessed July-2023].
393
[28] R. Tanese, Distributed genetic algorithms for function optimization, University of Michigan,
394
1989.
395
[29] E. Cant´u-Paz,
A survey of parallel genetic algorithms,
Calculateurs paralleles, reseaux et
396
systems repartis 10 (1998) 141–171.
397
[30] T. Tao, Open question: best bounds for cap sets, https://terrytao.wordpress.com/2007/
398
02/23/open-question-best-bounds-for-cap-sets/, 2009.
399
[31] E. Croot, V. F. Lev, P. P. Pach, Progression-free sets in are exponentially small, Annals of
400
Mathematics (2017) 331–337.
401
[32] J. S. Ellenberg, D. Gijswijt, On large subsets of F n
q with no three-term arithmetic progression,
402
Annals of Mathematics (2017) 339–343.
403
[33] E. Naslund, W. Sawin, Upper bounds for sunflower-free sets, in: Forum of Mathematics, Sigma,
404
volume 5, Cambridge University Press, 2017, p. e15.
405
[34] Y. Edel, J. Bierbrauer, Large caps in small spaces, Designs, Codes and Cryptography 23 (2001)
406
197–212.
407
[35] Y. Edel, Extensions of generalized product caps, Designs, Codes and Cryptography 31 (2004)
408
5–14.
409
10
ACCELERATED ARTICLE PREVIEW

[36] R. Hill, On the largest size of cap in S5,3, Atti della Accademia Nazionale dei Lincei. Classe di
410
Scienze Fisiche, Matematiche e Naturali. Rendiconti 54 (1973) 378–384.
411
[37] P. J. Cameron, J. H. Van Lint, Designs, graphs, codes and their links, volume 3, Cambridge
412
University Press Cambridge, 1991.
413
[38] A. R. Calderbank, P. C. Fishburn, Maximal three-independent subsets of {0, 1, 2} n, Designs,
414
Codes and Cryptography 4 (1994) 203–211.
415
[39] F. Tyrrell, New lower bounds for cap sets, arXiv preprint arXiv:2209.10045 (2022).
416
[40] C. C. Lee, D. T. Lee, A simple on-line bin-packing algorithm, Journal of the ACM (JACM) 32
417
(1985) 562–572.
418
[41] P. Ramanan, D. J. Brown, C.-C. Lee, D.-T. Lee, On-line bin packing in linear time, Journal of
419
Algorithms 10 (1989) 305–326.
420
[42] S. S. Seiden, On the online bin packing problem, Journal of the ACM (JACM) 49 (2002)
421
640–671.
422
[43] J. Balogh, J. B´ek´esi, G. D´osa, J. Sgall, R. v. Stee,
The optimal absolute ratio for online
423
bin packing, in: Proceedings of the twenty-sixth annual ACM-SIAM symposium on discrete
424
algorithms, SIAM, 2014, pp. 1425–1438.
425
[44] J. Balogh, J. B´ek´esi, G. D´osa, L. Epstein, A. Levin,
A new and improved algorithm for
426
online bin packing, in: 26th Annual European Symposium on Algorithms (ESA 2018), Schloss
427
Dagstuhl–Leibniz-Zentrum fuer Informatik, 2018, pp. 5:1–5:14.
428
[45] E. G. Coffman, J. Csirik, G. Galambos, S. Martello, D. Vigo,
Bin packing approximation
429
algorithms: survey and classification, Handbook of combinatorial optimization (2013) 455–531.
430
[46] S. Martello, P. Toth, Lower bounds and reduction procedures for the bin packing problem,
431
Discrete applied mathematics 28 (1990) 59–70.
432
[47] S. Angelopoulos, S. Kamali, K. Shadkami,
Online bin packing with predictions 36 (2022)
433
4574–4580.
434
[48] G. J. Chaitin, On the length of programs for computing finite binary sequences, Journal of the
435
ACM (JACM) 13 (1966) 547–569.
436
[49] M. Li, P. Vit´anyi, et al., An introduction to Kolmogorov complexity and its applications,
437
volume 3, Springer, 2008.
438
[50] R. J. Solomonoff, A formal theory of inductive inference. part i, Information and control 7
439
(1964) 1–22.
440
[51] M. O’Neill, L. Vanneschi, S. Gustafson, W. Banzhaf, Open issues in genetic programming,
441
Genetic Programming and Evolvable Machines 11 (2010) 339–363.
442
[52] S. Polu, I. Sutskever, Generative language modeling for automated theorem proving, arXiv
443
preprint arXiv:2009.03393 (2020).
444
[53] S. Polu, J. M. Han, K. Zheng, M. Baksys, I. Babuschkin, I. Sutskever, Formal mathematics
445
statement curriculum learning, arXiv preprint arXiv:2202.01344 (2022).
446
11
ACCELERATED ARTICLE PREVIEW

[54] A. Q. Jiang, W. Li, S. Tworkowski, K. Czechowski, T. Odrzyg´o´zd´z, P. Mi lo´s, Y. Wu, M. Jam-
447
nik, Thor: Wielding hammers to integrate language models and automated theorem provers,
448
Advances in Neural Information Processing Systems 35 (2022) 8360–8373.
449
12
ACCELERATED ARTICLE PREVIEW

Figure 1:
Overview of FunSearch. The input to FunSearch is a specification of the problem
in the form of an evaluate function, an initial implementation of the function to evolve, which can
be trivial, and potentially a skeleton. At each iteration, FunSearch builds a prompt by combining
several programs sampled from the programs database (favouring high-scoring ones). The prompt is
then fed to the pre-trained LLM, and new programs are created. Newly created programs are then
scored and stored in the programs database (if correct), thus closing the loop. The user can at any
point retrieve the highest-scoring programs discovered so far.
Figure 2: Examples of FunSearch specifications for two problems. The evaluate function
takes as input a candidate solution to the problem, and returns a score assessing it. The solve
function contains the algorithm skeleton, which calls the function to evolve that contains the crucial
logic. For (a), the function to evolve is called priority, and for (b) it is called heuristic. The main
function implements the evaluation procedure by connecting the pieces together. Specifically, it uses
the solve function to solve the problem, and then scores the resulting solutions using evaluate. In
simplest cases, main just executes solve once and uses evaluate to score the output, e.g., see (a).
In specific settings such as online algorithms, the main function implements some additional logic,
e.g., see (b).
Figure 3: Diagram of a cap set of size 4 in Z2
3. The circles are the elements of Z2
3 with the
ones belonging to the cap set shown in blue. The possible lines in Z2
3 are also shown (with colors
indicating lines that wrap around in arithmetic modulo 3). No three elements of the cap set are in
a line.
Figure 4:
Result of applying FunSearch to the cap set problem. (a) Size of the largest
cap set in Zn
3 for different dimensions n.
(b) The function priority : Zn
3 →R discovered by
FunSearch that results in a cap set of size 512 in n = 8 dimensions. One feature to note is that
the priority is affected by whether the same entry appears in positions i and -i (-i denotes the
i-th position counting from the end). This motivates the notion of reflections, used in (c). (c)
An explicit construction of this new 512-cap, which we were able to manually construct thanks to
having discovered the cap set by searching in function space. See Appendix E.2 in Supplementary
Information for more details and for relation to Hill cap.
Figure 5: Results on the cap set problem via admissible sets. (a) Summary of lower bounds
on the cap set capacity C. (b) The priority function {0, 1, 2}n →R discovered by FunSearch that
results in an I(12, 7) admissible set. The source code reveals that when n = 12, the function treats
the four triples of coordinates {0, 4, 8}, {1, 5, 9}, {2, 6, 10}, and {3, 7, 11} together. We then checked
that the admissible set is in fact symmetric under independent cyclic permutations of coordinates
within each of these four triples. See Appendix D and Appendix E.3 in Supplementary Information
for more details.
13
ACCELERATED ARTICLE PREVIEW

Figure 6: Example of a short online bin packing heuristic discovered by FunSearch for
the OR dataset. This example illustrates frequently observed behavior: instead of always packing
items into the best fit bin, the heuristic encourages packing the item only if the fit is tight (line 11).
Comments in the code were manually added. See Appendix C in Supplementary Information for
more discovered heuristics.
14
ACCELERATED ARTICLE PREVIEW

A
Methods
450
A.1
Implementation details of FunSearch
451
Distributed system.
We implement FunSearch as a distributed system that has three types of
452
workers: a programs database, samplers, and evaluators. The programs database stores the initial
453
user-provided program, as well as all programs received from the evaluators. The samplers are in
454
charge of performing the LLM inference step; to do so they repeatedly query the programs database
455
for prompts. To achieve higher sampling throughput, samplers generate multiple samples from each
456
prompt.
The samples from the LLM (i.e., the generated programs) are sent to the evaluators,
457
which score programs by executing them on inputs of interest and assessing the outputs using
458
evaluate. Programs that are correct are sent to the programs database to be stored. Each of
459
the three FunSearch components is provided as both Python code and pseudocode (Appendix F in
460
Supplementary Information).
461
Prompt building.
When queried for a prompt, the programs database samples k programs to
462
encourage the LLM to merge ideas from them (we typically set k = 2; see Appendix E.1 in Sup-
463
plementary Information). Programs are sorted according to their score in increasing order, starting
464
from “version 0” (v0). Using these k programs, the prompt is built as explained next.
465
For the sake of clarity, we use here the problem specification from Figure 2 (a) to precisely
466
describe the prompting mechanism. The overall structure of the prompt mimics the structure of
467
the program skeleton, with the following differences: (i) The priority function is stripped out, and
468
replaced with the k = 2 programs sampled, first priority v0 and then priority v1. (ii) After
469
that, a priority v2 function with no body is appended — the LLM will be in charge of completing
470
the body of that function. (iii) All other functions that appear before priority v0 are removed.
471
See Extended Data Figure 1 for an example of the structure of a prompt.
472
Evolutionary method and program selection.
Another key feature of FunSearch is the method
473
used for evolution of the population of programs from the programs database, as well as for program
474
selection, i.e., how the programs database samples programs when queried for a prompt. For this,
475
we use the islands model, a parallel genetic algorithm [28, 29]. Specifically, we split the population
476
into m separate groups, or islands. Each island is initialized with a copy of the user-provided initial
477
program and is evolved separately. That is, whenever a prompt is required, we first uniformly sample
478
an island and then sample k = 2 programs from that island to build the prompt. The programs
479
generated from the LLM based on that prompt will later be stored in the same island. Every four
480
hours, we discard all the programs from the m/2 islands whose best instances have the lowest score.
481
Each of these islands is then seeded with a single program, obtained by first choosing one of the
482
surviving m/2 islands uniformly at random, and then retrieving the highest-scoring program from
483
that island (breaking ties in favour of older programs). The evolutionary process is then restarted
484
from this state, in which the reset islands contain one high-performing program each (see Extended
485
Data Figure 2).
486
This method has several advantages. First, drawing the analogy where an island corresponds
487
to an experiment, this approach effectively allows us to run several smaller experiments in parallel,
488
instead of a single large experiment. This is beneficial because single experiments can get stuck in
489
local minima, where the majority of programs in the population are not easily mutated and combined
490
into stronger programs. The multiple island approach allows us to bypass this and effectively kill
491
off such experiments to make space for new ones starting from more promising programs. Second,
492
15
ACCELERATED ARTICLE PREVIEW

promising experiments are run for longer, since the islands that survive a reset are the ones with
493
higher scores.
494
Within each island, we further cluster programs according to their signature. We define the
495
signature of a program as the tuple containing the program’s scores on each of the inputs (e.g., the
496
cap set size for each input n). Programs with the same signature are clustered together. When
497
sampling a program within an island, we first sample an island’s cluster, and then a program within
498
that cluster (see Extended Data Figure 3).
This approach, which aims at preserving diversity
499
[55, 56], is related to Lexicase [57] in that both approaches consider a set of test cases for scoring an
500
individual, and it is related to fitness uniform optimization [58], which also clusters individuals based
501
on their fitness value, however we sample the clusters based on their score instead of uniformly, as
502
detailed next.
503
When sampling a cluster, we favor those with larger score values. Specifically, let si denote the
504
score of the i-th cluster, defined as an aggregation (e.g., mean) of all the scores in the signature that
505
characterizes that cluster. The probability pi of choosing cluster i is
506
pi =
exp (si/Tcluster)
P
i′ exp (si′/Tcluster),
Tcluster = T0 ·

1 −n mod N
N

,
(1)
where Tcluster is the temperature parameter, n is the current number of programs in the island,
507
and T0 and N are hyperparameters (given in Appendix E.1 in Supplementary Information). This
508
approach is sometimes referred to as the Boltzmann selection procedure [59].
509
When sampling a program within a cluster, we favor shorter programs. In particular, let ℓi
510
denote the negative length of the i-th program within the chosen cluster (measured as the number
511
of characters), and let eℓi =
ℓi−mini′ ℓi′
maxi′ ℓi′+10−6 . We set the probability of each program proportional to
512
exp(eℓi/Tprogram), where Tprogram is a temperature hyperparameter.
513
Robustness.
Due to randomness in LLM sampling and in the evolutionary procedure, repeating
514
an experiment can lead to different results. For some problems (e.g. cap set through the admissible
515
set problem, and online bin packing) every single run of FunSearch surpasses the baseline, with only
516
some variation in the magnitude of the difference. For example, all experiments on admissible sets
517
improve upon the previous best capacity lower bound, with 60% of experiments on I(12, 7) finding
518
a full-size admissible set. For other problems, multiple independent repetitions of an experiment
519
may be necessary to improve upon prior best results. In particular, the case of cap set by direct
520
construction in n = 8 dimensions is particularly challenging, with only 4 out of 140 experiments
521
discovering a cap set of size 512. See Appendix A.3 in Supplementary Information for more details.
522
A.2
Related work
523
Large Language Models.
The rise of powerful LLMs such as [60] has been followed by systems
524
in which an LLM core is enveloped by a “programmatic scaffold” [61], and multiple LLM calls are
525
connected together in some way to accomplish larger and more intricate tasks beyond what would be
526
possible using a single prompt and the raw LLM, possibly using external tools or external memory
527
streams [62–66]. LLMs have also been paired with evaluators; for example, [21, 67] fine-tune an
528
LLM on data that has been previously generated by the LLM itself (respectively on puzzle problems
529
and solutions, and on justifications/explanations for answers to questions), and use an evaluator
530
to assess the correctness of this data, ensuring that the fine-tuning dataset contains correct solu-
531
tions/explanations only. More related to our approach is the use of LLMs as a mutation operator
532
on code. [3] was the first work to show that coupling an LLM with a programatic way of scoring a
533
16
ACCELERATED ARTICLE PREVIEW

solution can lead to a self-improvement loop. In [17–20], the LLM is used as a crossover operator
534
rather than a mutation one, i.e., the LLM prompts are composed of several functions, similarly to
535
FunSearch. In [3, 17], the task is to improve code that generates bidimensional virtual robots that
536
can move as far as possible in a given simulated terrain ([17] additionally considers the tasks of
537
symbolic regression, natural language sentences, and image generation), in [18–20] the task is to
538
find neural network architectures (described with Python code), and in [68] the task is continuous
539
exploration in the game of Minecraft. In contrast, in this paper we tackle open problems in math-
540
ematics and algorithm design, and we surpass human-designed constructions. We achieve that by
541
combining multiple ingredients together: a distributed system with multiple samplers and evaluators
542
that communicate asynchronously, a user-provided program specification and skeleton, as well as
543
an evolutionary mechanism based on islands that preserves the diversity of programs. FunSearch
544
achieves that using an off-the-shelf LLM without fine-tuning.
545
More broadly, LLMs have been used for program synthesis as one of its main applications [5–9].
546
There are many use cases being explored, such as automatically editing code to improve performance
547
[14], automatically debugging code [10, 11], generating code from natural language descriptions [69–
548
71], and doing so to solve problems in code competitions [12, 13]. Unlike the above approaches
549
which provide tools to increase the productivity of software engineers, we combine in this paper
550
the creativity of LLMs with the power of evolutionary procedures to push the boundaries of human
551
knowledge through solving open hard problems. Another line of research uses LLMs to guide the
552
search for formal proofs for automatic theorem proving [52–54]. While this approach has the potential
553
of eventually finding new knowledge, the achievements of these methods still lag behind the frontier
554
of human knowledge.
555
Genetic programming.
Genetic programming (GP) is a subfield of computer science concerned
556
with automatically generating or discovering computer programs using evolutionary methods [16,
557
72, 73] and is employed for symbolic regression applications [74, 75] and discovery of optimization
558
algorithms [76] among others. In this broad sense, combining LLMs with evolution can be seen
559
as an instance of GP with the LLM acting as a mutation and crossover operator. However, using
560
an LLM mitigates several issues in traditional GP [51], as shown in Appendix A in Supplementary
561
Information and discussed in [3]. Indeed, GP methods require defining a number of parameters,
562
chief among them the set of allowed mutation operations (or primitives) [16]. Designing such a set
563
of operations is non-trivial and problem-specific, requiring domain knowledge about the problem at
564
hand or its plausible solution [51]. While research has been done to mitigate this limitation, through
565
for example the reuse of subprograms [77] or modeling the distribution of high-performing programs
566
[78], designing effective and general code mutation operators remains difficult. In contrast, LLMs
567
have been trained on vast amounts of code and as such have learned about common patterns and
568
routines from human-designed code. The LLM can leverage this, as well as the context given in the
569
prompt, to generate more effective suggestions than the random ones typically used in GP.
570
Related to GP, the field of hyper-heuristics [79, 80] seeks to design learning methods for gen-
571
erating heuristics applied to combinatorial optimization problems. In practice, these heuristics are
572
often programs discovered through GP, typically by evolving a heuristic on a set of instances of a
573
given combinatorial optimization problem, such as bin packing [81]. Indeed, like FunSearch, hyper-
574
heuristics have also been applied to online bin packing, with the learned heuristics able to match the
575
performance of first fit [82] and best fit [83] on a set of generated bin packing instances. Augmenting
576
the heuristics with memory of previously seen items can even lead to heuristics outperforming best
577
fit [84]. In addition, these evolved heuristics can sometimes generalize to larger instances than the
578
ones they were trained on [85], similar to the learned FunSearch heuristics. However, as is the case
579
with GP, one of the fundamental limitations of hyper-heuristics is that the components of the evolved
580
17
ACCELERATED ARTICLE PREVIEW

heuristic must be manually defined by the user and often need to be tailored to a specific problem
581
to be effective. The LLM in FunSearch allows us to bypass this limitation and learn heuristics for
582
bin packing and job scheduling as well as discovering novel mathematical constructions, all within
583
a single pipeline without problem specific tuning.
584
Program superoptimization and software engineering.
Searching for the best way of mod-
585
ifying source code is a task that appears in multiple branches of computer science and software
586
development. These occurrences can be broadly classified into two groups: first, where the goal is to
587
find semantic-preserving modifications (this arises in program optimization and superoptimization,
588
where the aim is to modify the program so that it executes faster while maintaining its input-output
589
behaviour), and second, where the goal is to find programs with different semantics (this arises, e.g.,
590
in automatic program repair and mutation testing). With some exceptions discussed below, most
591
of these areas use relatively simple and hard-coded mutation operators on either the source code
592
directly (such as deleting or swapping lines) or on the abstract syntax tree (AST).
593
Machine learning approaches have been used for program superoptimization. For example, [86]
594
used reinforcement learning to learn the sampling probabilities used within a hierarchical prob-
595
abilistic model of simple program edits introduced by STOKE [87].
Neural networks have also
596
been proposed as a mutation operator for program optimization in [88]. These works operated on
597
code written in Assembly (perhaps because designing meaningful and rich edit distributions on pro-
598
grams in higher-level languages is challenging). More recently, [14] used LLMs to find performance-
599
improving edits to code written in C++ or Python. We also note that reinforcement learning has
600
recently been applied to discover new faster algorithms for fundamental operations such as matrix
601
multiplication [89] and sorting [90].
602
In this paper, we have not explicitly explored semantic-preserving applications such as discovering
603
performance-improving code edits, but we believe that FunSearch could be an effective method for
604
that setting too. In both use cases presented in Section 2, the goal is to evolve programs with new
605
semantics, but the application is different from program repair or mutation testing: in Section 2.1 we
606
used FunSearch to discover a program that constructs a previously unknown mathematical object,
607
and in Section 2.2 we used FunSearch to discover a program that corresponds to a more efficient
608
heuristic for online bin packing.
609
Data availability.
The experiments carried out in this paper do not require any data corpus other
610
than the publicly available OR-Library bin packing benchmarks [24]. The output functions of interest
611
produced by FunSearch are shown across the main paper and in text files in the Supplementary
612
Information.
613
Code availability.
The discovered functions as well as the evolutionary algorithm, code manipula-
614
tion routines, and a single-threaded implementation of the FunSearch pipeline are available as Python
615
code in the Supplementary information and at https://github.com/google-deepmind/funsearch.
616
Additionally, the software library launchpad [91], and a sandbox for safely executing generated code
617
on our internal distributed system were used. No training or fine-tuning of a large language model
618
is required; API access for inference is sufficient. We used Codey [27], which is available through its
619
API, and StarCoder [7], which is open source.
620
18
ACCELERATED ARTICLE PREVIEW

References
621
[55] J.-B. Mouret, S. Doncieux, Overcoming the bootstrap problem in evolutionary robotics using
622
behavioral diversity, in: 2009 IEEE Congress on Evolutionary Computation, 2009, pp. 1161–
623
1168.
624
[56] J. K. Pugh, L. B. Soros, K. O. Stanley, Quality diversity: A new frontier for evolutionary
625
computation, Frontiers in Robotics and AI 3 (2016) 40.
626
[57] T. Helmuth, L. Spector, J. Matheson, Solving uncompromising problems with lexicase selection,
627
IEEE Transactions on Evolutionary Computation 19 (2015) 630–643.
628
[58] M. Hutter, S. Legg, Fitness uniform optimization, IEEE Transactions on Evolutionary Com-
629
putation 10 (2006) 568–589.
630
[59] M. de la Maza, An analysis of selection procedures with particular attention paid to propor-
631
tional and boltzmann selection, in: Proceedings of the fifth international conference on genetic
632
algorithms, 1993, Morgan Kaufmann, 1993.
633
[60] OpenAI, GPT-4 technical report, 2023. arXiv:2303.08774.
634
[61] B. Millidge, Scaffolded LLMs as natural language computers, https://www.beren.io/
635
2023-04-11-Scaffolded-LLMs-natural-language-computers, 2023. [Online; accessed July-
636
2023].
637
[62] T. Schick, J. Dwivedi-Yu, R. Dess`ı, R. Raileanu, M. Lomeli, L. Zettlemoyer, N. Cancedda,
638
T. Scialom, Toolformer: Language models can teach themselves to use tools, arXiv preprint
639
arXiv:2302.04761 (2023).
640
[63] J. S. Park, J. C. O’Brien, C. J. Cai, M. R. Morris, P. Liang, M. S. Bernstein, Generative agents:
641
Interactive simulacra of human behavior, arXiv preprint arXiv:2304.03442 (2023).
642
[64] J. Wu, L. Ouyang, D. M. Ziegler, N. Stiennon, R. Lowe, J. Leike, P. Christiano, Recursively
643
summarizing books with human feedback, arXiv preprint arXiv:2109.10862 (2021).
644
[65] M. Nye, A. J. Andreassen, G. Gur-Ari, H. Michalewski, J. Austin, D. Bieber, D. Dohan,
645
A. Lewkowycz, M. Bosma, D. Luan, C. Sutton, A. Odena,
Show your work: Scratchpads
646
for intermediate computation with language models, arXiv preprint arXiv:2112.00114 (2021).
647
[66] S. Yao, J. Zhao, D. Yu, N. Du, I. Shafran, K. R. Narasimhan, Y. Cao, ReAct: Synergizing
648
reasoning and acting in language models, in: International Conference on Learning Represen-
649
tations, 2023.
650
[67] E. Zelikman, Y. Wu, J. Mu, N. Goodman,
Star: Bootstrapping reasoning with reasoning,
651
Advances in Neural Information Processing Systems 35 (2022) 15476–15488.
652
[68] G. Wang, Y. Xie, Y. Jiang, A. Mandlekar, C. Xiao, Y. Zhu, L. Fan, A. Anandkumar, Voyager:
653
An open-ended embodied agent with large language models, arXiv preprint arXiv:2305.16291
654
(2023).
655
[69] P. Yin, W.-D. Li, K. Xiao, A. Rao, Y. Wen, K. Shi, J. Howland, P. Bailey, M. Catasta,
656
H. Michalewski, et al., Natural language to code generation in interactive data science note-
657
books, arXiv preprint arXiv:2212.09248 (2022).
658
19
ACCELERATED ARTICLE PREVIEW

[70] A. Ni, S. Iyer, D. Radev, V. Stoyanov, W.-t. Yih, S. Wang, X. V. Lin, Lever: Learning to
659
verify language-to-code generation with execution, in: International Conference on Machine
660
Learning, PMLR, 2023, pp. 26106–26128.
661
[71] S. Zhou, U. Alon, F. F. Xu, Z. Jiang, G. Neubig, Docprompting: Generating code by retrieving
662
the docs, in: International Conference on Learning Representations, 2022.
663
[72] W. Banzhaf, P. Nordin, R. E. Keller, F. D. Francone, Genetic programming: an introduction:
664
on the automatic evolution of computer programs and its applications, Morgan Kaufmann
665
Publishers Inc., 1998.
666
[73] W. B. Langdon, R. Poli, Foundations of genetic programming, Springer Science & Business
667
Media, 2013.
668
[74] H. Ma, A. Narayanaswamy, P. Riley, L. Li, Evolving symbolic density functionals, Science
669
Advances 8 (2022).
670
[75] M. Schmidt, H. Lipson, Distilling free-form natural laws from experimental data, science 324
671
(2009) 81–85.
672
[76] X. Chen, C. Liang, D. Huang, E. Real, K. Wang, Y. Liu, H. Pham, X. Dong, T. Luong, C.-J.
673
Hsieh, et al., Symbolic discovery of optimization algorithms, arXiv preprint arXiv:2302.06675
674
(2023).
675
[77] J. R. Koza, Genetic programming II: automatic discovery of reusable programs, MIT press,
676
1994.
677
[78] R. Salustowicz, J. Schmidhuber, Probabilistic incremental program evolution, Evolutionary
678
computation 5 (1997) 123–141.
679
[79] E. Burke, G. Kendall, J. Newall, E. Hart, P. Ross, S. Schulenburg,
Hyper-heuristics: An
680
emerging direction in modern search technology, Handbook of metaheuristics (2003) 457–474.
681
[80] P. Ross, Hyper-heuristics, Search methodologies: introductory tutorials in optimization and
682
decision support techniques (2005) 529–556.
683
[81] E. K. Burke, M. Gendreau, M. Hyde, G. Kendall, G. Ochoa, E. ¨Ozcan, R. Qu, Hyper-heuristics:
684
A survey of the state of the art, Journal of the Operational Research Society 64 (2013) 1695–
685
1724.
686
[82] E. K. Burke, M. R. Hyde, G. Kendall, Evolving bin packing heuristics with genetic program-
687
ming, in: International Conference on Parallel Problem Solving from Nature, Springer, 2006,
688
pp. 860–869.
689
[83] E. K. Burke, M. R. Hyde, G. Kendall, J. Woodward,
Automatic heuristic generation with
690
genetic programming: evolving a jack-of-all-trades or a master of one, in: Proceedings of the
691
9th annual conference on Genetic and evolutionary computation, 2007, pp. 1559–1565.
692
[84] E. K. Burke, M. R. Hyde, G. Kendall, Providing a memory mechanism to enhance the evo-
693
lutionary design of heuristics, in: IEEE Congress on Evolutionary Computation, IEEE, 2010,
694
pp. 1–8.
695
20
ACCELERATED ARTICLE PREVIEW

[85] E. K. Burke, M. Hyde, G. Kendall, J. R. Woodward, The scalability of evolved on line bin
696
packing heuristics, in: 2007 IEEE Congress on Evolutionary Computation, IEEE, 2007, pp.
697
2530–2537.
698
[86] R. Bunel, A. Desmaison, P. Kohli, P. H. Torr, M. P. Kumar,
Learning to superoptimize
699
programs, in: International Conference on Learning Representations, 2017.
700
[87] E. Schkufza, R. Sharma, A. Aiken, Stochastic superoptimization, ACM SIGARCH Computer
701
Architecture News 41 (2013) 305–316.
702
[88] A. Shypula, P. Yin, J. Lacomis, C. L. Goues, E. Schwartz, G. Neubig, Learning to superoptimize
703
real-world programs, in: Deep Learning for Code Workshop (ICLR 2022 Workshop), 2022.
704
[89] A. Fawzi, M. Balog, A. Huang, T. Hubert, B. Romera-Paredes, M. Barekatain, A. Novikov,
705
F. J. R Ruiz, J. Schrittwieser, G. Swirszcz, et al.,
Discovering faster matrix multiplication
706
algorithms with reinforcement learning, Nature 610 (2022) 47–53.
707
[90] D. J. Mankowitz, A. Michi, A. Zhernov, M. Gelmi, M. Selvi, C. Paduraru, E. Leurent, S. Iqbal,
708
J.-B. Lespiau, A. Ahern, et al., Faster sorting algorithms discovered using deep reinforcement
709
learning, Nature 618 (2023) 257–263.
710
[91] F. Yang, G. Barth-Maron, P. Sta´nczyk, M. Hoffman, S. Liu, M. Kroiss, A. Pope, A. Rrustemi,
711
Launchpad: a programming model for distributed machine learning research, arXiv preprint
712
arXiv:2106.04516 (2021).
713
21
ACCELERATED ARTICLE PREVIEW

Acknowledgments.
We would like to thank Rohan Anil, Vlad Feinberg, Emanuel Taropa, Thomas
714
Hubert, Julian Schrittwieser, Sebastian Nowozin for their LLM support; Tom Schaul, Chrisantha
715
Fernando, Andre Barreto, Prateek Gupta for discussions on evolutionary algorithms; Michael Fig-
716
urnov and Taylan Cemgil for reviewing the paper; Federico Piccinini, Sultan Kenjeyev for their
717
support on job scheduling, Sam Blackwell for technical support; Olaf Ronneberger, Felix Gimeno,
718
Blanca Huergo, Abbas Mehrabian and Ankit Anand for useful advice; George Holland for program
719
management support.
720
Author Contributions.
BRP conceived the project with help from AF and PK. AF scoped
721
problems and developed project vision. BRP and AN developed the initial FunSearch codebase.
722
AN, BRP, M. Balog, FR, M. Barekatain, ED, AF implemented and refined the different components
723
of the system. M. Barekatain, AN imported and experimented with LLMs. M. Barekatain, AN, M.
724
Balog worked on evaluating, debugging, and improving the efficiency of experiments. M. Balog,
725
M. Barekatain, BRP, AN, AF, OF, JE contributed to the cap set problem.
MPK, M. Balog,
726
JE researched and analyzed results about the admissible sets problem. ED, M. Barekatain, PW
727
contributed to the online bin packing problem. FR, OF researched and did experiments on other
728
problems (Shannon capacity and corners problem), PK contributed technical advice and ideas. AF,
729
BRP, ED, FR, MPK, M. Balog, AN, JE, M. Barekatain wrote the paper. These authors contributed
730
equally: BRP, M. Barekatain, AN, M. Balog, MPK, ED, FR, AF.
731
Corresponding authors.
Correspondence to Bernardino Romera-Paredes (brp@google.com), Push-
732
meet Kohli (pushmeet@google.com) or Alhussein Fawzi (afawzi@google.com).
733
Competing interests.
The authors of the paper are planning to file a patent application relating
734
to subject matter contained in this paper in the name of Google DeepMind.
735
Additional information.
Supplementary Information is available for this paper.
736
22
ACCELERATED ARTICLE PREVIEW

Extended Data Figure 1: Example of best-shot prompting, based on the skeleton from
Figure 2 (a). The prompt includes k = 2 implementations sampled from the programs database,
with higher-scoring implementations being more likely to be included.
Extended Data Figure 2: Evolutionary method. The initial programs are separated into
islands and each of them are evolved separately. After a number of iterations, the islands with
the worst score are wiped and the best program from the islands with the best score are placed in
the empty islands. Evolution then proceeds separately again until the next reset. This process is
repeated until termination.
Extended Data Figure 3: Program clusters within islands. Within each island, programs are
grouped into clusters based on their signature (i.e., their scores on several inputs). We first sample
clusters, favoring the ones with higher score. Within the chosen clusters, we sample a program,
favoring shorter programs. The sampled programs are used to prompt the LLM which generates a
new program. If the new program is correct, it is added to the island, either in an existing cluster
or a new one if its signature was not yet present.
23
ACCELERATED ARTICLE PREVIEW

??
Pre-trained LLM
Evaluation
Programs
database
Specification
?
?
Prompt
FunSearch
Novel program
ACCELERATED ARTICLE PREVIEW

"""Finds large cap sets."""
import numpy as np
import utils_capset
# Function to be executed by FunSearch.
def main(n):
"""Runs `solve` on `n`-dimensional cap set and
evaluates the output."""
֒→
solution = solve(n)
return evaluate(solution, n)
def evaluate(candidate_set, n):
"""Returns size of candidate_set if it is a cap
set, None otherwise."""
֒→
if utils_capset.is_capset(candidate_set, n):
return len(candidate_set)
else:
return None
def solve(n):
"""Builds a cap set of dimension `n` using
`priority` function."""
֒→
# Precompute all priority scores.
elements = utils_capset.get_all_elements(n)
scores = [priority(el, n) for el in elements]
# Sort elements according to the scores.
elements = elements[np.argsort(scores,
kind='stable')[::-1]]
֒→
# Build `capset` greedily, using scores for
prioritization.
֒→
capset = []
for element in elements:
if utils_capset.can_be_added(element, capset):
capset.append(element)
return capset
# Function to be evolved by FunSearch.
def priority(element, n):
"""Returns the priority with which we want to add
`element` to the cap set."""
֒→
return 0.0
(a) Cap set.
"""Finds good assignment for online 1d bin
packing."""
֒→
import numpy as np
import utils_packing
# Function to be executed by FunSearch.
def main(problem):
"""Runs `solve` on online 1d bin packing instance,
and evaluates the output."""
֒→
bins = problem.bins
# Packs `problem.items` into `bins` online.
for item in problem.items:
# Extract bins that have space to fit item.
valid_bin_indices =
utils_packing.get_valid_bin_indices(item,
bins)
֒→
֒→
best_index = solve(item,
bins[valid_bin_indices])
֒→
# Add item to the selected bin.
bins[valid_bin_indices[best_index]] -= item
return evaluate(bins, problem)
def evaluate(bins, problem):
"""Returns the negative of the number of bins
required to pack items in `problem`."""
֒→
if utils_packing.is_valid_packing(bins, problem):
return -utils_packing.count_used_bins(bins,
problem)
֒→
else:
return None
def solve(item, bins):
"""Selects the bin with the highest value according
to `heuristic`."""
֒→
scores = heuristic(item, bins)
return np.argmax(scores)
# Function to be evolved by FunSearch.
def heuristic(item, bins):
"""Returns priority with which we want to add
`item` to each bin."""
֒→
return -(bins - item)
(b) Online bin packing.
ACCELERATED ARTICLE PREVIEW

ACCELERATED ARTICLE PREVIEW

n
3
4
5
6
7
8
Best known
9
20
45
112
236
496
FunSearch
9
20
45
112
236
512
(a)
def priority(el: tuple[int, ...],
n: int) -> float:
֒→
score = n
in_el = 0
el_count = el.count(0)
if el_count == 0:
score += n ** 2
if el[1] == el[-1]:
score *= 1.5
if el[2] == el[-2]:
score *= 1.5
if el[3] == el[-3]:
score *= 1.5
else:
if el[1] == el[-1]:
score *= 0.5
if el[2] == el[-2]:
score *= 0.5
for e in el:
if e == 0:
if in_el == 0:
score *= n * 0.5
elif in_el == el_count - 1:
score *= 0.5
else:
score *= n * 0.5 ** in_el
in_el += 1
else:
score += 1
if el[1] == el[-1]:
score *= 1.5
if el[2] == el[-2]:
score *= 1.5
return score
(b)
def build_512_cap() -> list[tuple[int, ...]]:
"""Returns a cap set of size 512 in `n=8` dimensions."""
n = 8
V = np.array(list(itertools.product(range(3), repeat=n)), dtype=np.int32)
support = lambda v: tuple(i for i in range(n) if v[i] != 0)
reflections = lambda v: sum(1 for i in range(1, n // 2) if v[i] == v[-i])
# Add all 128 weight-8 vectors that have >= 2 reflections.
weight8_vectors = [v for v in V
if np.count_nonzero(v) == 8
# Weight is 8.
and reflections(v) >= 2]
# At least 2 reflections.
# Add all 128 weight-4 vectors that have specific support.
supports_16 = [(0, 1, 2, 3), (0, 1, 2, 5), (0, 3, 6, 7), (0, 5, 6, 7),
(1, 3, 4, 6), (1, 4, 5, 6), (2, 3, 4, 7), (2, 4, 5, 7)]
weight4_vectors = [v for v in V
if support(v) in supports_16]
# Add all 128 weight-4 vectors with specific support and 1 reflection.
supports_8 = [(0, 1, 2, 7), (0, 1, 2, 6), (0, 1, 3, 7), (0, 1, 6, 7),
(0, 1, 5, 7), (0, 2, 3, 6), (0, 2, 6, 7), (0, 2, 5, 6),
(1, 2, 4, 7), (1, 2, 4, 6), (1, 3, 4, 7), (1, 4, 6, 7),
(1, 4, 5, 7), (2, 3, 4, 6), (2, 4, 6, 7), (2, 4, 5, 6)]
weight4_vectors_2 = [v for v in V
if support(v) in supports_8
and reflections(v) == 1]
# Exactly 1 reflection.
# Add 128 weight-5 vectors with <= 1 reflections and one more condition.
allowed_zeros = [(0, 4, 7), (0, 2, 4), (0, 1, 4), (0, 4, 6),
(1, 2, 6), (2, 6, 7), (1, 2, 7), (1, 6, 7)]
weight5_vectors = [
v for v in V
if tuple(i for i in range(n) if v[i] == 0) in allowed_zeros
and reflections(v) <= 1
# At most 1 reflection.
and (v[1] * v[7]) % 3 != 1 and (v[2] * v[6]) % 3 != 1]
return weight8_vectors + weight4_vectors + weight4_vectors_2 +
weight5_vectors
֒→
(c)
ACCELERATED ARTICLE PREVIEW

Bound
on C
Admissible set
ingredient
Source
2.2101
I(90, 89)
(Calderbank and Fishburn, 1994)
2.2173
I(10, 5)
(Edel, 2004)
2.2180
I(11, 7)
(Tyrrell, 2022)
2.2184
I(12, 7)
FunSearch
2.2194
I(15, 10)
FunSearch
2.2202
A(24, 17)
FunSearch
(a)
def priority(el: tuple[int, ...], n: int, w: int) -> float:
score = 0.0
for i in range(n):
if
el[i]
== 1:
score -= 0.9 ** ( i % 4 )
if
el[i]
== 2:
score -= 0.98 ** (30 - ( i % 4 ))
if
el[i]
== 1 and
el[i - 4]
== 1:
score -= 0.98 ** (30 - ( i % 4 ))
if
el[i]
== 2 and
el[i - 4]
!= 0:
score -= 0.98 ** (30 - ( i % 4 ))
if
el[i]
== 2 and
el[i - 4]
== 1 and
el[i - 8]
== 2:
score -= 0.98 ** (30 - ( i % 4 ))
score -= 6.3
if
el[i]
== 2 and
el[i - 4]
== 2 and
el[i - 8]
== 1:
score -= 0.98 ** (30 - ( i % 4 ))
if
el[i]
== 2 and
el[i - 4]
== 1 and
el[i - 8]
== 1:
score -= 6.3
if
el[i]
== 2 and
el[i - 4]
== 0 and
el[i - 8]
== 2:
score -= 6.3
if
el[i]
== 1 and
el[i - 4]
== 1 and
el[i - 8]
== 0:
score -= 2.2
return score
(b)
ACCELERATED ARTICLE PREVIEW

def heuristic(item: float, bins: np.ndarray) -> np.ndarray:
"""Online bin packing heuristic discovered with FunSearch."""
score = 1000 * np.ones(bins.shape)
# Penalize bins with large capacities.
score -= bins * (bins - item)
# Extract index of bin with best fit.
index = np.argmin(bins)
# Scale score of best fit bin by item size.
score[index] *= item
# Penalize best fit bin if fit is not tight.
score[index] -= (bins[index] - item)**4
return score
ACCELERATED ARTICLE PREVIEW

"""Finds large cap sets."""
import numpy as np
import utils_capset
def priority_v0(element, n):
"""Returns the priority with which we want to add `element` to the cap set."""
#######
# Code from lowest-scoring sampled program.
return ...
#######
def priority_v1(element, n):
"""Improved version of `priority_v0`."""
#######
# Code from highest-scoring sampled program.
return ...
#######
def priority_v2(element, n):
"""Improved version of `priority_v1`."""
Extended Data Fig. 1
ACCELERATED ARTICLE PREVIEW

Extended Data Fig. 2
ACCELERATED ARTICLE PREVIEW

Extended Data Fig. 3
ACCELERATED ARTICLE PREVIEW

