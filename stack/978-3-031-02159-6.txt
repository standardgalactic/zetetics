
Grammatical Inference for
Computational Linguistics

Synthesis Lectures on
Human Language Technologies
Editor
Graeme Hirst, University of Toronto
Synthesis Lectures on Human Language Technologies is edited by Graeme Hirst of the University of
Toronto. The series consists of 50- to 150-page monographs on topics relating to natural language
processing, computational linguistics, information retrieval, and spoken language understanding.
Emphasis is on important new techniques, on new applications, and on topics that combine two or
more HLT subﬁelds.
Grammatical Inference for Computational Linguistics
Jeffrey Heinz, Colin de la Higuera, and Menno van Zaanen
2015
Automatic Detection of Verbal Deception
Eileen Fitzpatrick, Joan Bachenko, and Tommaso Fornaciari
2015
Natural Language Processing for Social Media
Atefeh Farzindar and Diana Inkpen
2015
Semantic Similarity from Natural Language and Ontology Analysis
S´ebastien Harispe, Sylvie Ranwez, Stefan Janaqi, and Jacky Montmain
2015
Learning to Rank for Information Retrieval and Natural Language Processing, Second Edition
Hang Li
2014
Ontology-Based Interpretation of Natural Language
Philipp Cimiano, Christina Unger, and John McCrae
2014
Automated Grammatical Error Detection for Language Learners, Second Edition
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2014

Web Corpus Construction
Roland Sch¨afer and Felix Bildhauer
2013
Recognizing Textual Entailment: Models and Applications
Ido Dagan, Dan Roth, Mark Sammons, and Fabio Massimo Zanzotto
2013
Linguistic Fundamentals for Natural Language Processing: 100 Essentials from Morphology
and Syntax
Emily M. Bender
2013
Semi-Supervised Learning and Domain Adaptation in Natural Language Processing
Anders Søgaard
2013
Semantic Relations Between Nominals
Vivi Nastase, Preslav Nakov, Diarmuid ´O S´eaghdha, and Stan Szpakowicz
2013
Computational Modeling of Narrative
Inderjeet Mani
2012
Natural Language Processing for Historical Texts
Michael Piotrowski
2012
Sentiment Analysis and Opinion Mining
Bing Liu
2012
Discourse Processing
Manfred Stede
2011
Bitext Alignment
J¨org Tiedemann
2011
Linguistic Structure Prediction
Noah A. Smith
2011
Learning to Rank for Information Retrieval and Natural Language Processing
Hang Li
2011

Computational Modeling of Human Language Acquisition
Afra Alishahi
2010
Introduction to Arabic Natural Language Processing
Nizar Y. Habash
2010
Cross-Language Information Retrieval
Jian-Yun Nie
2010
Automated Grammatical Error Detection for Language Learners
Claudia Leacock, Martin Chodorow, Michael Gamon, and Joel Tetreault
2010
Data-Intensive Text Processing with MapReduce
Jimmy Lin and Chris Dyer
2010
Semantic Role Labeling
Martha Palmer, Daniel Gildea, and Nianwen Xue
2010
Spoken Dialogue Systems
Kristiina Jokinen and Michael McTear
2009
Introduction to Chinese Natural Language Processing
Kam-Fai Wong, Wenjie Li, Ruifeng Xu, and Zheng-sheng Zhang
2009
Introduction to Linguistic Annotation and Text Analytics
Graham Wilcock
2009
Dependency Parsing
Sandra K¨ubler, Ryan McDonald, and Joakim Nivre
2009
Statistical Language Models for Information Retrieval
ChengXiang Zhai
2008

All rights reserved. No part of this publication may be reproduced, stored in a retrieval system, or transmitted in
any form or by any means—electronic, mechanical, photocopy, recording, or any other except for brief quotations
in printed reviews—without the prior permission of the publisher.
Grammatical Inference for Computational Linguistics
Jeffrey Heinz, Colin de la Higuera, Menno van Zaanen
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES
Series ISSN: 1947-4040 print
1947-4059 ebook
Lecture #28
Series Editor: Graeme Hirst, University of Toronto
First Edition
10 9 8 7 6 5 4 3 2 1
© Springer Nature Switzerland AG 2022
Reprint of original edition   Morgan   Claypool 2016
©
&
ISBN:  978-3-031-01031-6
 paperback
ISBN:  978-3-031-02159-6
 ebook
DOI  10.1007/978-3-031-02159-6
A Publication in the Springer series

Grammatical Inference for
Computational Linguistics
Jeffrey Heinz
University of Delaware
Colin de la Higuera
Nantes University
Menno van Zaanen
Tilburg University
SYNTHESIS LECTURES ON HUMAN LANGUAGE TECHNOLOGIES #28

ABSTRACT
This book provides a thorough introduction to the subﬁeld of theoretical computer science known as
grammatical inference from a computational linguistic perspective. Grammatical inference provides
principled methods for developing computationally sound algorithms that learn structure from
strings of symbols. The relationship to computational linguistics is natural because many research
problems in computational linguistics are learning problems on words, phrases, and sentences: What
algorithm can take as input some ﬁnite amount of data (for instance a corpus, annotated or otherwise)
and output a system that behaves “correctly” on speciﬁc tasks?
Throughout the text, the key concepts of grammatical inference are interleaved with illus-
trative examples drawn from problems in computational linguistics. Special attention is paid to the
notion of “learning bias.” In the context of computational linguistics, such bias can be thought to
reﬂect common (ideally universal) properties of natural languages. This bias can be incorporated
either by identifying a learnable class of languages which contains the language to be learned or by
using particular strategies for optimizing parameter values. Examples are drawn largely from two
linguistic domains (phonology and syntax) which span major regions of the Chomsky Hierarchy
(from regular to context-sensitive classes). The conclusion summarizes the major lessons and open
questions that grammatical inference brings to computational linguistics.
KEYWORDS
grammatical inference, language learning, natural languages, formal languages

To our families, friends, and automata

xi
Contents
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xv
List of Tables
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xvii
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . xix
1.
Studying Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
An Overview of Grammatical Inference
. . . . . . . . . . . . . . . . .
2
1.2
Formal and Empirical Grammatical Inference
. . . . . . . . . . . . . . .
3
1.3
Formal Grammatical Inference
. . . . . . . . . . . . . . . . . . . . .
5
1.3.1
Language and Grammar
. . . . . . . . . . . . . . . . . . . .
6
1.3.2
Language Families
. . . . . . . . . . . . . . . . . . . . . .
8
1.3.3
Learning Languages Efﬁciently
. . . . . . . . . . . . . . . . .
9
1.4
Empirical Grammatical Inference . . . . . . . . . . . . . . . . . . . . 10
1.4.1
Languages, Grammars, and Language Families . . . . . . . . . . . 11
1.4.2
Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.6
Formal Preliminaries
. . . . . . . . . . . . . . . . . . . . . . . . . 14
2.
Formal Learning
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 21
2.1.1
The Issues of Learning . . . . . . . . . . . . . . . . . . . . . 21
2.1.2
Learning Scenarios
. . . . . . . . . . . . . . . . . . . . . . 22
2.1.3
Learning Grammars of Languages
. . . . . . . . . . . . . . . . 23
2.2
Learnability: Deﬁnitions and Paradigms
. . . . . . . . . . . . . . . . . 24
2.2.1
Blame the Data, Not the Algorithm
. . . . . . . . . . . . . . . 24
2.2.2
A Non-Probabilistic Setting: Identiﬁcation in the Limit
. . . . . . . 24
2.2.3
An Active Learning Setting . . . . . . . . . . . . . . . . . . . 25
2.2.4
Introducing Complexity
. . . . . . . . . . . . . . . . . . . . 26
2.2.5
A Probabilistic Version of Identiﬁcation in the Limit . . . . . . . . . 28
2.2.6
Probably Approximately Correct (PAC) Learning
. . . . . . . . . . 28
2.3
Grammar Formalisms . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.3.1
Finite-State Machines Recognizing Strings
. . . . . . . . . . . . 31
2.3.2
Probabilistic Finite-State Machines . . . . . . . . . . . . . . . . 34

xii
CONTENTS
2.3.3
Transducers
. . . . . . . . . . . . . . . . . . . . . . . . . 37
2.3.4
More Complex Formalisms . . . . . . . . . . . . . . . . . . . 41
2.3.5
Dealing with Trees and Graphs
. . . . . . . . . . . . . . . . . 46
2.4
Is Grammatical Inference an Instance of Machine Learning?
. . . . . . . . . 47
2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49
3.
Learning Regular Languages
. . . . . . . . . . . . . . . . . . . . . . . . 51
3.1
Introduction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.2
Bias Selection Reduces the Problem Space
. . . . . . . . . . . . . . . . 52
3.3
Regular Grammars . . . . . . . . . . . . . . . . . . . . . . . . . . 53
3.4
State-Merging Algorithms . . . . . . . . . . . . . . . . . . . . . . . 56
3.4.1
The Problem of Learning Stress Patterns
. . . . . . . . . . . . . 56
3.4.2
Merging States . . . . . . . . . . . . . . . . . . . . . . . . 59
3.4.3
Finite-State Representations of Finite Samples
. . . . . . . . . . . 60
3.4.4
The State-Merging Theorem
. . . . . . . . . . . . . . . . . . 62
3.5
State-Merging as a Learning Bias . . . . . . . . . . . . . . . . . . . . 64
3.6
State-Merging as Inference Rules
. . . . . . . . . . . . . . . . . . . . 66
3.7
RPNI
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 67
3.7.1
How It Works
. . . . . . . . . . . . . . . . . . . . . . . . 67
3.7.2
Theoretical Results
. . . . . . . . . . . . . . . . . . . . . . 68
3.8
Regular Relations
. . . . . . . . . . . . . . . . . . . . . . . . . . 69
3.9
Learning Stochastic Regular Languages
. . . . . . . . . . . . . . . . . 71
3.9.1
Stochastic Languages
. . . . . . . . . . . . . . . . . . . . . 72
3.9.2
Structure of the Class Is Deterministic and Known A Priori . . . . . . 73
3.9.3
Structure of the Class Is Deterministic but Not Known A Priori
. . . . 78
3.9.4
Structure of the Class Is Non-Deterministic and Not Known A Priori . . 80
3.10
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.
Learning Non-Regular Languages
. . . . . . . . . . . . . . . . . . . . . . 85
4.1
Substitutability
. . . . . . . . . . . . . . . . . . . . . . . . . . . 86
4.1.1
Identifying Structure
. . . . . . . . . . . . . . . . . . . . . 86
4.1.2
Learning Using Substitutability
. . . . . . . . . . . . . . . . . 88
4.2
Empirical Approaches
. . . . . . . . . . . . . . . . . . . . . . . . 89
4.2.1
Expanding and Reducing Approaches . . . . . . . . . . . . . . . 89
4.2.2
Supervised and Unsupervised Approaches
. . . . . . . . . . . . . 89
4.2.3
Word-Based and POS-Based Approaches
. . . . . . . . . . . . . 90
4.2.4
Description of Empirical Systems
. . . . . . . . . . . . . . . . 90
4.2.5
Comparison of Empirical Systems
. . . . . . . . . . . . . . .
102

CONTENTS
xiii
4.3
Issues for Evaluation
. . . . . . . . . . . . . . . . . . . . . . . .
103
4.3.1
Looks-Good-To-Me Approach
. . . . . . . . . . . . . . . .
104
4.3.2
Rebuilding Known Grammars . . . . . . . . . . . . . . . . .
105
4.3.3
Compare against a Treebank
. . . . . . . . . . . . . . . . .
107
4.3.4
Language Membership . . . . . . . . . . . . . . . . . . . .
109
4.4
Formal Approaches . . . . . . . . . . . . . . . . . . . . . . . . .
111
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
113
5.
Lessons Learned and Open Problems . . . . . . . . . . . . . . . . . . . .
115
5.1
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115
5.2
Lessons
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
5.3
Problems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116
5.3.1
Learning Targets
. . . . . . . . . . . . . . . . . . . . . .
116
5.3.2
Learning Criteria . . . . . . . . . . . . . . . . . . . . . .
118
5.4
Resources
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
120
5.5
Final Words
. . . . . . . . . . . . . . . . . . . . . . . . . . .
120
Bibliography
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
Author Biographies . . . . . . . . . . . . . . . . . . . . . . . . . . . .
137

xv
List of Figures
1.1
General overview of the process of grammatical inference.
. . . . . . . . . . . .
3
1.2
The relationship between strings, languages, and language families. . . . . . . . . .
8
1.3
The Chomsky Hierarchy.
. . . . . . . . . . . . . . . . . . . . . . . . .
9
2.1
Graphical representation of a dfa. . . . . . . . . . . . . . . . . . . . . . .
32
2.2
An nfa.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.3
An nfa with λ-transitions. . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.4
A pfa. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
2.5
A pfa with λ-transitions.
. . . . . . . . . . . . . . . . . . . . . . . . .
36
2.6
An hmm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.7
An hmm using ♯to terminate the generation of strings.
. . . . . . . . . . . . .
38
2.8
A rational transducer. . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.9
A semideterministic transducer. . . . . . . . . . . . . . . . . . . . . . . .
39
2.10
Normalized transducer.
. . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.11
A probabilistic transducer.
. . . . . . . . . . . . . . . . . . . . . . . . .
41
2.12
A probabilistic semideterministic transducer. . . . . . . . . . . . . . . . . . .
41
2.13
Parse tree for John hit the ball.
. . . . . . . . . . . . . . . . . . . . . . . .
43
2.14
Parse tree for the sentence aabaccc for a linear grammar with rules S →aS|bS|aSc|acc.
44
2.15
Parse tree for an even linear grammar.
. . . . . . . . . . . . . . . . . . . .
45
2.16
Skeleton corresponding to the bracketed string (a(a(a(b)a)b)(a(a)b)).
. . . . . . .
47
3.1
A ﬁnite-state acceptor which recognizes the language ´σσ ∗. . . . . . . . . . . . .
54
3.2
A minimal, deterministic, ﬁnite-state acceptor for Pintupi stress. . . . . . . . . . .
57
3.3
A minimal, deterministic, ﬁnite-state acceptor for Kwakwala stress. . . . . . . . . .
59
3.4
Machine B represents the machine obtained by merging states 1 and 2 in Machine A.
.
59
3.5
A preﬁx tree of Pintupi words.
. . . . . . . . . . . . . . . . . . . . . . .
61
3.6
A sufﬁx tree for Pintupi words.
. . . . . . . . . . . . . . . . . . . . . . .
62
3.7
The result of merging states in the preﬁx tree for Pintupi words (Figure 3.5) with the same
incoming paths of length 2.
. . . . . . . . . . . . . . . . . . . . . . . .
65

xvi
LIST OF FIGURES
3.8
A preﬁx-tree whose states have been enumerated in breadth-ﬁrst fashion.
. . . . . .
68
3.9
A a family of distributions with four parameters.
. . . . . . . . . . . . . . . .
74
3.10
The count of the parse of S = ⟨bbc⟩and the probabilities obtained after normalization. .
75
3.11
The structure of a bi-gram model. . . . . . . . . . . . . . . . . . . . . . .
76
4.1
Substitutability in tree structures.
. . . . . . . . . . . . . . . . . . . . . .
87
4.2
Schematic overview of phases in abl. . . . . . . . . . . . . . . . . . . . . .
93
4.3
Alignment of two sentences and the identiﬁcation of hypotheses.
. . . . . . . . .
93
4.4
Initial adios graph for John sees a cat, John walks, and The dog sees Mary.
. . . . . .
95
4.5
The initial adios graph, and the two graphs after rewriting the pattern of nodes.
. . .
97
4.6
Example dependency parse without terminals, which are visualized as dots.
. . . . .
99
4.7
Elementary tree structures in dop. . . . . . . . . . . . . . . . . . . . . . .
100
4.8
Two derivations using left-most substitution that lead to the same parse. . . . . . . .
101
4.9
Schematic representation of the looks-good-to-me evaluation approach. . . . . . .
104
4.10
Schematic representation of the rebuilding known grammars evaluation approach.
.
105
4.11
Schematic representation of the compare against a treebank evaluation approach.
.
107
4.12
Schematic representation of the language membership evaluation approach.
. . . .
110

xvii
List of Tables
2.1
Deterministic ﬁnite-state automata: dfa
. . . . . . . . . . . . . . . . . . .
32
2.2
Non-deterministic ﬁnite-state automata: nfa
. . . . . . . . . . . . . . . . .
34
2.3
Regular expressions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
2.4
Probabilistic ﬁnite-state automata: pfa
. . . . . . . . . . . . . . . . . . . .
37
2.5
Hidden Markov models: hmm
. . . . . . . . . . . . . . . . . . . . . . .
38
2.6
Transducers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
2.7
Probabilistic transducers . . . . . . . . . . . . . . . . . . . . . . . . . .
42
2.8
Context-free grammars
. . . . . . . . . . . . . . . . . . . . . . . . . .
45
2.9
Probabilistic context-free grammars
. . . . . . . . . . . . . . . . . . . . .
46
3.1
Pintupi words . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57
3.2
Pintupi words with a syllabic representation
. . . . . . . . . . . . . . . . . .
57
3.3
All LHOR words up to four syllables in length . . . . . . . . . . . . . . . . .
58
3.4
Words in Samala with sibilant sounds [ʃ, S] . . . . . . . . . . . . . . . . . .
76
3.5
Impossible Samala words with sibilant sounds [s, ʃ] . . . . . . . . . . . . . . .
77
3.6
SP2 probabilities of a sibilant occurring sometime after another one.
. . . . . . . .
78
4.1
Matrix for John walks, John sees Mary and Mary walks . . . . . . . . . . . . . . .
91
4.2
Overview of properties of the two groups of empirical systems . . . . . . . . . . .
102

xix
Preface
Once authors have written the words “The End,” they realize how much has been left out. And this
rule holds if instead of one author, there are three. This is one reason why prefaces are important.
They let readers know (and remind the authors) what the book achieves and what it does not.
The tasks addressed in this book become more formidable with each passing day. It is
becoming more complex and intricate because there are more and more cases where one is delivered
a huge amount of strings, words, or sentences, or has access to some such data, and one is asked
how to build a model summarizing or explaining this information. Furthermore, for many reasons—
for example, the fact that most computer scientists have taken courses on graph theory and formal
languages—the types of models people are seeking will be very often linked with grammars and
automata.
That is why, today, there are people attempting to build or infer grammars or ﬁnite-state
machines in ﬁelds as different as veriﬁcation, pattern recognition, bioinformatics, and linguistics.
That is why techniques of all sorts are being used to infer these models: some rely on statistics, others
on linear algebra, some on formal language theory, and many quite often on a combination of these.
And ﬁnally, that is why certain choices have been made in this book, and therefore some readers
might be frustrated.
Before we explain why some of our choices may frustrate readers, let us state who we
think our readers are. One reason we embarked on this project was because there is no text
which introduces grammatical inference to people working in computational linguistics and natural
language processing. Our hope is that this book helps bridge the gap between the needs of these
researchers and a particular way of thinking about the problems of learning automata and grammars
in machine learning. We sincerely believe grammatical inference can help address problems in
computational linguistics and that problems in computational linguistics can inform and lead to
new developments in grammatical inference (in fact, such mutual beneﬁts exist and are ongoing).
We also have in mind readers who are not encountering automata and grammars for the ﬁrst
time. The kind of background knowledge we expect readers to have is of the type that could be found
in standard textbooks on formal language theory that one might take as an advanced undergraduate
student or a beginning graduate student. We also expect readers to have some familiarity with topics
in computational linguistics and natural language processing, like the kinds discussed in the books
by Jurafsky and Martin [2008] or Manning and Sch¨utze [1999] (or their more recent editions).

xx
PREFACE
So what are the choices that may frustrate readers? The ﬁrst choice we made was to concentrate
on only some tools and techniques, and not attempt to be exhaustive.
The second choice was to cover the tools and techniques which have been developed in what
may informally be called the school of grammatical inference, as represented, over the past 30 years,
by the papers published in the series of conferences called ICGI—International Conference on
Grammatical Inference. These share a certain number of aspects.
.
They build upon well-understood formal language formalisms and avoid, whenever possible,
technical complications in the deﬁnitions of the objects themselves.
.
They either attempt to deliver formal learnability results, independent of some particular
corpus, or, on the other hand, aim to produce a very general algorithm whose proof of concept
will be given by its results on particular corpora without corpus-speciﬁc tweaks.
Consequently, this means the knowledge we present builds from formal language theory and
concentrates on those techniques whose intricate theoretical backbone comes from that ﬁeld.
A third choice is that the book is not self-contained, in the sense that not every algorithm
discussed is presented and proved correct in full detail. Instead, we have chosen to focus on ideas, and
to include only the notation, deﬁnitions, and theorems that we felt important because they support
those main points. We do not include proofs, but we try to point to them and further material
which helps readers ﬁnd detailed descriptions and explanations of the algorithms or formalisms. For
instance, we often refer to de la Higuera [2010], a book on grammatical inference with a general
orientation, which is self-contained.
Together all of this means leaving out certain results, which no doubt deserve closer attention.
For ﬁnite-state machines, one notable area left out is spectral methods, which identify ﬁnite-
state machines with sets of matrices and therefore transforms the learning problem into one which
searches for an optimal set of parameters which ﬁts those matrices. The techniques here are attractive:
they allow the learning of very rich classes of ﬁnite-state machines, rely on linear algebra’s vast
literature, and can be redeﬁned as global optimization problems, for which a large number of
researchers are bettering the algorithms all the time.
For formal grammars, a number of results (sometimes grouped under the name grammar
induction) are based on starting with a backbone grammar, either extremely general or devised from
using data for which the structure is known, and adjusting the parameters by just observing relative
frequencies. The types of grammars will themselves be adapted to better ﬁt the knowledge we have
of natural language.
We do not argue here that the techniques covered in this book work better, just that they
correspond to a uniform set of ideas which, when understood, can allow a number of problems to
be solved.

PREFACE
xxi
Perhaps one argument which we would like to put forward is that of intelligibility. Albeit
informal in most cases, the idea is that the types of techniques proposed in this book rely on
wanting to understand the machines and grammars learned. An undeclared goal is that one should
be able to run a grammatical inference algorithm, obtain perhaps a large automaton or grammar, and
nevertheless be able to observe it and understand it, not just its effects. This helps to explain why
we believe that the issue of learning the structure of the grammar is essential, and why this theme
recurs throughout the book.
One may wonder if this is necessary, as the grammar will often be evaluated through a success
or error rate, not through its capacity to speak to us. On the other hand, there are increasingly many
applications where the user wants more than a black box.
All of this, and the idea of making the book useful to as many readers as possible, was what
the authors had in mind when they launched this adventure.
ACKNOWLEDGMENTS
The authors would like to thank Dion Bot, R´emi Eyraud, Thomas Graf, ´Akos K´ad´ar, Esm´ee
Mertens, Jim Rogers, Eva Ummelen, Merel van de Wiel, and the anonymous reviewers for their
useful feedback.
Jeffrey Heinz gladly thanks Mika, Emma, and Maya for their support.
Colin de la Higuera is grateful to Lindsey for her support.
Menno van Zaanen would like to thank Tanja, Colwin, and Lejla for their support.
September 2015

1
C H A P T E R 1
Studying Learning
Grammatical inference is a subﬁeld of theoretical computer science which aims to characterize,
understand, and solve learning problems in terms of formal languages and grammars. The ﬁeld
of computational linguistics faces many different kinds of tasks which involve natural languages
and learning. Many of these tasks aim to automate decisions and processes that humans accurately
undertake every day with apparently very little conscious effort. Examples include word recognition
and segmentation, the phonological, morphological, syntactic, semantic, and pragmatic analysis of
both speech and written texts, and, at least for multilingual speakers, translation.
Computationally, these are difﬁcult problems, each with their own subproblems and sub-
tleties, but there can be little doubt that solving nearly every one of them goes hand-in-hand with
understanding natural language systems. Natural languages are systems with their own internal log-
ics, rules, constraints, and structures. It is one of the grand mysteries of contemporary times that
humans appear to have this knowledge (as evidenced by their mostly uniform behavior in many lin-
guistic tasks) despite the fact that it is unconscious (humans cannot easily articulate it in any detail)
and untaught (while it is acquired it is not explicitly taught). In fact, it is one of the goals of the
academic disciplines of theoretical and descriptive linguistics to precisely state the types of systems
natural languages are.
The grand mystery may be solved by one of the marvelous promises of modern times. The
development of hardware and software that can make discoveries and learn has changed—and
continues to change—our society and our lives. If the systems that underly natural languages can
be learned by such machines and programs—if these logics, rules, constraints, and structures can
be automatically acquired—then virtually all of the above tasks will be solvable automatically by
machines.
Grammatical inference goes to the heart of this enterprise. The “grammar” in “grammatical
inference” refers to any aspect of the logics, rules, constraints, and structures that compose the sys-
tems underlying natural language. Grammars are models of these systems of knowledge. “Inference”
refers to rational steps made in acquiring knowledge from observations and prior assumptions about
those observations. At its core, grammatical inference is a method of inquiry that tries to understand

2
1. STUDYING LEARNING
the computations involved in making inferences about grammars from observations under a variety
of different learning scenarios.
The purpose of this book is to introduce computational linguists to the major results of
this ﬁeld and to its way of thinking. While the ﬁeld of grammatical inference has much to offer
computational linguistics, there is no doubt that computational linguists can make contributions to
the ﬁeld of grammatical inference as well.
The notion of grammar adopted here is broad enough that it can be used for any generative
system, including non-linguistic ones in other ﬁelds. For example, there can be grammars for DNA
or RNA sequences, for the order in which messages should be sent over a computer network, or for
the structure of web pages. In this book, however, we will either discuss situations that deal with
natural language data or discuss topics that are of general nature (and hence valid for all types of
data).
1.1
AN OVERVIEW OF GRAMMATICAL INFERENCE
Grammatical inference takes a cue from formal language theory. Knowledge regarding a natural
language can be modeled with a formal language. For instance, the knowledge of which sentences
in a natural language are well formed can be modeled with a formal language that contains all and
only those sequences of words which together constitute the set of well-formed sentences. Another
example comes from phonology: the fact that English speakers generally can (and do and will) coin
words like “bling” but generally cannot (and do not and will not) coin words like “gding” can be
modeled with a formal language that contains all and only those sequences of letters (or phonemes
or speech sounds) which make up the possible well-formed words of the language. While these sets
of sentences and words are inﬁnite, grammars are ﬁnite representations of these formal languages.
Henceforth, we will refer to the elements of formal languages—these sequences of symbols—as
strings.
While these examples are simple, more complex knowledge can also be modeled with formal
languages. If well-formedness is stochastic, then probabilistic grammars can be used. If syntactic
constituency is part of the knowledge we wish to model, then we can introduce symbols into
the strings which represent constituent boundary symbols. Whether these abstract symbols are
observable to learners depends on the learning scenario. Since practically anything can be represented
with strings (even a video, even a grammar), the use of formal language theory is sufﬁciently broad
for the form of inquiry undertaken by researchers in grammatical inference.
Grammatical inference construes the learning process broadly as follows. The setup is shown
in Figure 1.1. Information from the language that is to be learned is provided by an abstract entity
called the oracle. The oracle has access to a grammar, description, representation, or model of the

1.2 Formal and Empirical Grammatical Inference
3
Information
Oracle
Requests
MO
ML
Model of
language
Model of
language
Learner
FIGURE 1.1: General overview of the process of grammatical inference.
language (MO). Based on this knowledge, the oracle can provide learners with information, which
typically consists of strings that are valid according to model MO. The task of the learner is now to
create its own model (ML) of the language based on the information (that comes from the model
MO) provided by the oracle. The learner may make requests to the oracle for information.
Within this general framework, several concrete decisions need to be made to establish a
precise learning scenario. What kind of information does the oracle provide the learner? What kind
of requests can the learner make, if any? How close does ML have to be to MO to count as successful
learning?
Typically, the information provided to the learner by the oracle is in the shape of strings
that are part of the language that the model MO describes (or when the learning scenario allows,
explicitly marked as not being part of the language of MO). There are several ways the learner
may ask for additional information. For instance, the learner may simply ask for another string or
speciﬁcally ask whether a string so far unseen is valid or not. These choices change the speciﬁc setting
of the learning process. Additionally, whether certain types of information, such as tree structures
or semantic features, are allowed or not is also a choice of the particular learning scenario adopted.
1.2
FORMAL AND EMPIRICAL GRAMMATICAL INFERENCE
In this book, we will describe two different approaches to grammatical inference as a science.
First, we will discuss formal grammatical inference and, second, empirical grammatical inference.
Roughly speaking, the former addresses general behavior of learning algorithms with theorems and
mathematical proofs and the latter addresses the speciﬁc behavior of learning algorithms by examining
their performance on particular tasks.
It should not be surprising that the ﬁeld of grammatical inference breaks down this way. In
computer science, we want to know our algorithms work as intended. This requires a well-deﬁned
problem space and well-deﬁned criteria for solutions for every problem instance. It is helpful to
compare the situation to a simpler one. In introductory computer science courses we learn which
procedures can sort lists. There are many algorithms that can do this such as (for instance) bubble

4
1. STUDYING LEARNING
sort. Importantly, the problem space is well deﬁned. The inputs to the problem contain any ﬁnite
list of elements in addition to an ordering relation over those elements. The success criteria are also
well deﬁned. The job of the algorithm is to return a new list with the elements of the input list
sorted according to the ordering relation. It is a theoretical result that bubble sort correctly solves
this problem for any of the possible inputs.
We can consider a world before bubble sort (or any other sorting algorithm) had been
discovered. How could we reliably sort lists in such a world? One method may have been to develop
procedures and to test their performance on individual lists. Does the procedure seem to do its job
on lists A, B, and C? If so, we may hope that it does well on lists D and E as well. Of course, without
a theoretical result, there would be no guarantee the procedure performs well on D and E. On the
other hand, efforts to successfully sort lists A, B, and C may well lead to such a theoretical result.
In other words, when trying to address any problem computationally, there are two fronts
with which it can be addressed: the formal front and the empirical one. Grammatical inference is a
research program working at both these levels. The formal approach is concerned with deﬁning the
problem space and proving that an algorithm satisﬁes the solution criteria for any instance of the
problem space. Progress is made when learning problems are identiﬁed and algorithms developed
which provably solve them. The problems can vary in their instance space and their success criteria.
This is why we say formal grammatical inference is concerned with ascertaining the general behavior
of algorithms which are engaged in learning. Given an initial set of assumptions about the input to
the learning algorithm, can we guarantee a certain level of performance?
On the other hand, empirical grammatical inference is more concerned with improving the
speciﬁc behavior of algorithms engaged in learning. These algorithms are usually run for particular
tasks for which particular inputs are already present. For instance, given a particular training corpus
of data as input, can we improve the output of the algorithm so that it performs better according
to some metrics on a particular test set of data? The empirical approach tries to get something to
work well for one case, and then another, and then another. This approach is often motivated by
deploying quickly a system which works for the immediate cases at hand.
Part of the issue researchers face in the computational science of learning is precisely stating
what the instance space and success criteria are. This is one reason why formal grammatical inference
is difﬁcult (but also exciting). Where one chooses to work is a matter of personal preference. But there
can be little doubt about the ultimate importance of the formal work. One only needs to consider
where we would be without theoretical guarantees for sorting algorithms (and many other kinds of
algorithms) to see why. This does not lessen the importance of empirical approaches. Not only are
they often crucial intermediate stages in developing formal results, they are also more immediately

1.3 Formal Grammatical Inference
5
applicable to tasks we wish to automate today. An incomplete solution to a problem is never as good
as a complete solution, but it is much better than no solution whatsoever.
The situation in computational linguistics can also be understood in these terms. There
are many problems which need to be addressed: transliteration, machine translation, anaphora
resolution, etc. On the one hand, we want to deﬁne a problem, understand its instance space and
solutions, and prove that a particular algorithm solves this problem, preferably efﬁciently. This is
formal grammatical inference. On the other hand, however, in the absence of such results that can be
immediately deployed for everyday useful tasks, empirical grammatical inference develops learning
systems that aim to be immediately useful on particular tasks or particular problem instances.
There are many other ways to look at the learning of (natural language) grammars. For
instance, one area of research aims to develop cognitive models of language learning. In this
area, properties of theoretical models are compared against the performance of human learners.
It still is not clear exactly what a cognitively realistic model of language learning should include.
Another approach has been called evolutionary language learning. It models language learning over
generations. Once the learner has learned a model, it becomes the oracle of the next generation.
These models are often investigated as social processes, with multiple oracles and multiple learners.
These kinds of views on learning are beyond the scope of this book.
The next two sections provide an introduction in the two areas discussed in this book. First,
formal grammatical inference will be introduced, followed by empirical grammatical inference. The
rest of the book follows a similar line. In Chapter 2, formal grammatical inference is discussed in
detail. Chapter 3 concentrates on learning regular languages (see below), primarily from the perspec-
tive of formal grammatical inference, although some empirical grammatical inference algorithms
are mentioned. Chapter 4 deals with learning non-regular languages, and mostly in the context
of empirical grammatical inference (though again some formal grammatical inference algorithms
are mentioned). Finally, Chapter 5 summarizes the ﬁeld, describes open questions, and highlights
lessons learned so far.
1.3
FORMAL GRAMMATICAL INFERENCE
During the informal description of grammatical inference in the ﬁrst part of this chapter, we already
mentioned some possible choices for modeling the learning process. In order to allow us to be explicit
about what the entire process looks like, we will have to come up with a way of describing all the
details of the process. Essentially, the same holds for the descriptive power of our grammar (which is
going to describe the language that we are aiming to learn). Fortunately, the language of mathematics
allows us to formally describe the learning process as well as the model of the language.

6
1. STUDYING LEARNING
Having mathematical descriptions of the grammar and the learning process allows us to
reason about the possibilities (and impossibilities) of (efﬁcient) learnability. In other words, we can
mathematically prove whether it is possible that a particular language (or group of languages) is
learnable in that particular learning setting.
Before we can come up with mathematical proofs, we need to formalize all aspects of the
learning process. When modeling language learning in a mathematical way, we need to have formal
descriptions of the language we are trying to learn and a representation (a grammar) that allows us to
represent the language we are learning. Additionally, we will need to describe the learning process,
which consists of a description of how the interaction between the oracle, which provides information
on the language, and the learner takes place as well as how success of learning is measured. Together,
all of these items will determine the instance space of the learning problem.
In the next three sections, we will describe each of the aspects in more detail. First, we will
take a look at the relationship between languages and their representation. Second, we describe how
languages can be grouped in families according to linguistic properties the languages share. Finally,
we will focus on properties of the learning process and indicate that the learnability proofs can be
based on properties of language families, which allows us to generalize learnability from one language
to a family of languages.
1.3.1
LANGUAGE AND GRAMMAR
From a formal perspective, a language is seen as a set of strings. This set may be ﬁnite or inﬁnite
and the corresponding language is called ﬁnite or inﬁnite, respectively. For instance, the language
that describes all English words representing the numbers from 1 to 10 is ﬁnite: {one, two, three,
four, ﬁve, six, seven, eight, nine, ten}. Obviously, ﬁnite languages may still be very large; for instance,
imagine the formal language containing all possible English sentences with fewer than 100 words.1
This language is ﬁnite, but quite large.
Representing a ﬁnite language can be done by simply enumerating all strings in the set. The
case of inﬁnite languages is harder, because we would like to represent the inﬁnite language with
ﬁnite means (time, amount of paper, etc.). In order to represent an inﬁnite set with ﬁnite means, we
need an additional syntax to describe the exact way in which the inﬁnite set is represented.
Imagine we want to describe the language that contains of all strings that consist of any number
of a’s: {a, aa, aaa, . . . }. In the previous sentence, we have already provided two informal “grammars”
which describe the right language in ﬁnite ways. The ﬁrst used a natural language description: “the
language that contains all strings that consist of any number of a’s”. This description, which contains
13 words, is a ﬁnite description of this language. The second relied on some syntax, namely the
1. Technically, one would require a ﬁnite alphabet, so only words “from the dictionary” are allowed.

1.3 Formal Grammatical Inference
7
symbols: . . . , {, and }. Additionally, we have used a comma and a list of example strings. If we
give a description of the language using these symbols, we presume that the reader of the informal
grammar understands what we mean by these symbols. In this informal way, we can communicate
which inﬁnite language we are thinking of with a ﬁnite means using the words and notation above.
Describing a language using a grammar requires a notation and an interpretation of this
notation. In the previous paragraph we have used an informal (natural language) and somewhat
less informal description with some mathematical symbols. Informal descriptions have several
disadvantages. First, they are often ambiguous. In our natural language description, for instance,
it is unclear whether the empty string—the string which has no as or any other symbols—is also
part of the language or not. Second, there are many different ways to describe the same language.
Some descriptions may be quite understandable, but for others it may be very hard to ﬁgure out
exactly what language they represent. Third, informal descriptions can make it hard to identify
important properties of the language.
Using formal descriptions of languages solves most of the problems of informal descriptions.
In the case of formal descriptions, the language is described using symbols that have a predeﬁned
meaning. First, formal descriptions are unambiguous. The “grammar” of mathematics describes
exactly how we should combine symbols into a coherent, meaningful whole. Additionally, each
symbol has its own meaning or interpretation that we all agree on. Note that this requires that we
all need to agree on how these symbols are used. This is why Section 1.6 contains an overview of
the mathematical notations used in this book.2
To summarize, we want to be able to describe languages, which may be inﬁnite. To represent
the languages, we require grammars. Each grammar (which is guaranteed to be ﬁnite) represents
its own (possibly inﬁnite) language through an interpretation. To describe the language in a ﬁnite,
unambiguous way, a grammar is denoted using mathematical symbols.
Having access to mathematical descriptions of formal languages clearly has advantages, as
discussed above. However, if our aim is to say something about learning natural languages, we need to
know which kinds of grammars can describe the aspects of natural languages that we are interested in.
Currently, there is still some debate about which formalism will allow us to describe natu-
ral languages most accurately. A range of formalisms is being used to describe natural languages.
For instance, several context-sensitive formalisms are currently being investigated for their ability
to naturally and efﬁciently describe aspects of natural language syntax, such as multiple context-
free grammars (mcfgs), minimalist grammars (mgs), and Tree-Adjoining Grammars (tags)
2. We realize that this reasoning does not completely hold, because we describe the mathematical symbols using (potentially
ambiguous) natural language. However, the problem of ambiguity is reduced by describing the meaning of the symbols as
much as possible out of context. This means that the meaning is described in a generic way.

8
1. STUDYING LEARNING
String
Strings
Language
Language
Languages
Family
FIGURE 1.2: The relationship between strings, languages, and language families.
[Joshi 1985, Seki et al. 1991, Stabler 1997, 2011]. Also in wide use in natural language processing
are ﬁnite-state acceptors and transducers, which are different (but related) kinds of ﬁnite-state
grammars. Grammar formalisms such as these will be deﬁned as they are needed throughout
the book.
1.3.2
LANGUAGE FAMILIES
So far, we have talked about ways of describing a speciﬁc language. It is tempting to think of a
speciﬁc, particular language as the target of the learning problem. However, this is a mistake akin
to thinking of the sorting problem as the problem of sorting a speciﬁc list. A particular language is
an instance of a more general problem, just like a particular list is an instance of the more general
sorting problem.
A collection of languages is called a family of languages. For instance, a language that only
contains a ﬁnite number of strings is called a ﬁnite language. The family of ﬁnite languages is the
set containing all and only such languages. In a similar line, natural languages are all and only those
languages spoken or written by people. More generally, a family of languages can be deﬁned in terms
of one or more properties.
Figure 1.2 visualizes the relationship between strings, languages and families of languages.
The left square describes the collection of all possible strings. A dot in that square represents a
speciﬁc string. This square should be understood to contain all logically possibly strings (and thus
inﬁnitely many strings). A language, represented by an ellipse in the ﬁgure, describes a (possibly
inﬁnite) set of strings belonging to the language. The right square represents the collection of all
possible languages. A dot in the right square represents one language. One such language corresponds
to an ellipse in the left square. An ellipse in the right square denotes a family of languages, which is
a (possibly inﬁnite) set of languages.
One issue which arises when deﬁning a learning problem is deﬁning the set of learning targets.
One way this has been accomplished is with language families. For example, for any language L in

1.3 Formal Grammatical Inference
9
RL
CFL
CSL
REL
FIGURE 1.3: The Chomsky Hierarchy, with REL: recursively enumerable languages; CSL: context-sensitive
languages; CFL: context-free languages; and RL: regular languages. The semitransparent ellipse represents a
cross-cutting class.
this family, it would be desirable at the end of the learning process that the learning algorithm
outputs a grammar which is an accurate description of L. With regard to natural languages, it would
be desirable to ﬁnd a class of formal languages which is sufﬁciently expressive to describe some aspect
of natural languages, and to make this the set of learning targets.
It is useful to mention at this point that formal language theory has investigated many families
of languages, and has achieved remarkable success in understanding how these families are related
and the different grammatical formalisms that can be used to generate, represent, and distinguish
languages in these families. The Chomsky Hierarchy includes the most well-known languages in
this class and is shown in Figure 1.3. The languages in each family are united by their common
property of being expressible with a particular grammatical formalism. Section 1.6 provides formal
deﬁnitions for these grammars and families of languages.
There are several other, less well-known, families of languages that have been studied and
new families of languages are being identiﬁed. These families cross-cut the families in the Chomsky
Hierarchy and include both subregular classes and non-regular classes. While formal grammatical
inference clearly addresses learning problems where the families of the Chomsky Hierarchy make
up the learning targets, it is also interested in other families of languages as well.
1.3.3
LEARNING LANGUAGES EFFICIENTLY
Thus, one aim of formal grammatical inference can be said to identify families of languages and
develop algorithms that provably efﬁciently learn languages within the family in a particular learning
setting. With respect to computational linguistics, the aim can be said to ﬁnd a formal description
of a family of languages that can be said to contain all natural languages, and at the same time can

10
1. STUDYING LEARNING
be shown to be efﬁciently learnable in a learning setting that corresponds most closely to the real
world (for instance, the learning setting that most closely corresponds to human early life).
Describing a learning setting requires two design choices. First, what does the process of
interaction between the learner and the oracle look like? In one setting, it might be the case that the
oracle simply provides information, which the learner can use. The information may only include
examples of valid strings from a target language (so-called positive data) or it may only include
examples of both valid and invalid strings from the target language (positive and negative data). In
another setting, the learner might be permitted to question the oracle regarding whether strings
belong to the target language and the oracle may be required to answer truthfully. Questions on how
to deal with oracles which are sometimes untruthful (i.e., noise), and implicit or explicit feedback
of the oracle also belong to this design choice.
The second design choice regards what constitutes a successful solution to the learning
problem. One setting might require the grammar output by the learner to represent a language
which is identical to the language of the oracle. Alternatively, the learning setting may allow the
grammar to represent a language which differs from the language of the oracle, but the errors are
limited in some way. Settings can also be used to impose efﬁciency conditions on learning algorithms
which limit the number of computations it can make during the process. Consequently, an algorithm
which is successful at learning a family of languages in a setting with no efﬁciency conditions may
fail to learn the same family in a setting which imposes some. The choice of setting deﬁnes what
“learning” means.
Results in formal grammatical inference are proofs of theorems whose statements are some-
thing like “Algorithm A in learning setting S successfully learns the family of languages L”. The
setting S contains all the important details about the kind of information the learner receives, the
requests it makes, and what counts as success. We ﬁnd it interesting that the proofs of these theorems
often rely on a subtle interplay between the requirements of the learning setting S and the properties
of the family of languages L. Different learning settings, families of language, and learnability results
are described in Chapters 2 and 3.
1.4
EMPIRICAL GRAMMATICAL INFERENCE
Empirical grammatical inference starts with the notion that natural languages are efﬁciently learnable
because people manage to learn them. This means that the language should be learnable given an
appropriate input. If we can develop systems that are able to learn these languages, we can then
analyze properties of these systems. The identiﬁed properties can form the basis of learnability proofs
in the area of formal grammatical inference. At the same time, empirical grammatical inference leads

1.4 Empirical Grammatical Inference
11
to deployable systems that work well enough to be used in tasks involving the learning of natural
languages.
Empirical grammatical inference is also motivated in part by the following concerns. First, for
most, if not all, natural languages, we do not have a complete and correct grammatical representation
available. For grammatical inference, this means that the model of the oracle (MO in Figure 1.1)
cannot be made explicit in any way. It also means that we do not know exactly where the family of
natural languages is located in the hierarchy represented in Figure 1.2.
A second concern is that there is a debate on exactly how the interaction between the oracle
(say, the parent) and the learner (the child) occurs. It is clear that the learner receives example strings,
such as sentences, or words, from the oracle. For instance, the oracle (parent) speaks to the learner
(child). However, the learner may perhaps also receive additional information, in the shape of non-
verbal communication, such as the oracle pointing to objects. Additionally, the learner can also speak
to the oracle and if the oracle reacts in a certain way, the learner may believe that the oracle correctly
interpreted the utterance and this may also be a source of information for the learner.
The combination of language representation with the shape of the interaction leads to addi-
tional choices. Perhaps the language representation should be able to encode semantic information.
If this is possible, it leads to the additional problem that the semantics also need to be learned and
grounded in the real world or the learner’s model of the real world.
The same questions that need to be asked in the ﬁeld of formal grammatical inference to
design learning settings (language family, learning process and evaluation) are just as relevant in the
ﬁeld of empirical grammatical inference. However, due to the different starting point of empirical
grammatical inference with respect to its formal counterpart, some distinct choices are made. These
will be discussed brieﬂy in the following sections and in more detail in Chapters 3 and 4.
1.4.1
LANGUAGES, GRAMMARS, AND LANGUAGE FAMILIES
Like formal grammatical inference, empirical grammatical inference systems learn from example
strings, such as sentences or words. Depending on the type of string, different languages are learned,
for instance, the language of natural language sentences, which corresponds to describing syntax, or
the language of words, which requires a grammar of morphology.
Also, like the algorithms developed in formal grammatical inference, the outputs of empirical
grammatical inference algorithms are grammars. However, unlike formal grammatical inference,
the target of learning is not necessarily a family of languages. The ultimate aim of an empirical
grammatical inference system is to be able to learn the language that underlies the input data it is
given. This means that the learning system should essentially be language independent. This does
not necessarily mean that the same system should be able to learn every aspect (syntax, morphology,

12
1. STUDYING LEARNING
etc.) of every natural language. Some systems may focus on identifying word boundaries in a stream
of phonemes, learning rules of word or sentence formation, or something else. The only requirement
is that the description of the language is done according to a grammar (which, just like in formal
grammatical inference, is a ﬁnite representation of the language).
One difference between formal and empirical grammatical inference is that the target of
empirical grammatical inference can be construed as one or a small number of particular languages,
whereas the target of formal grammatical inference is a class of languages. As we discuss below,
empirical grammatical inference uses data from a small number of speciﬁc languages, and the
performance of the system is measured by comparing the learned grammar against a gold standard,
which is taken to be their grammars, which are known only to an oracle.
1.4.2
EVALUATION
The evaluation methods discussed in the context of formal grammatical inference measures the
performance of learning algorithms in vitro (or perhaps better named in silico). In this situation,
the problem space is known in advance and so are the solutions to the problem instances. What
is unknown is an algorithm which maps problem instances to their solutions. However, once an
algorithm is proposed, evaluation can proceed in part by comparing its output on a problem instance
directly against (a grammar of) the target language. In contrast, research in the area of empirical
grammatical inference measures the performance of empirical systems in vivo. The performance
of the systems is measured in the context of an application. Thus, empirical grammatical inference
systems can be evaluated both extrinsically by using the algorithm as a component in a larger system
and intrinsically by comparing measures like precision and recall to a gold standard.
The aim of empirical grammatical inference systems is to learn the grammar of the language
which performs best on some task, or range of tasks, as possible. Thus, in empirical grammatical
inference, the problem is one of optimization: What algorithm exists that outputs a grammar whose
behavior on a task is optimal?
Obviously then, to know exactly how well the learned grammar behaves, evaluation criteria
are needed. Different situations may require different evaluation criteria to determine how well the
system is doing. For instance, if we are interested in learning only speciﬁc syntactic constructions,
there is no need to evaluate against the entire target language. We only want to know to what extent
the constructions we are interested in are being learned correctly.
Ideally, for the problem of language identiﬁcation, the learned grammar (that of the learner,
ML) should completely cover the target language (described by MO) while at the same time no
additional strings (not part of the target language) should be accepted by the learned grammar.
Even though the system should learn the language completely and correctly, in practical situations
(for example in which no full description of the target grammar is known, such as in the case of

1.5 Summary
13
natural languages) this proves to be difﬁcult. Evaluation metrics that indicate to what extent the
grammar is complete and correct show us how close we are to a particular goal.
Another way empirical grammatical inference algorithms can be evaluated is via their incor-
poration into pre-existing natural language processing systems. Applications that deal with strings,
such as speech-driven dialog systems and information extraction systems, often need to know how
to deal with unexpected input. In the case of natural languages, this may, for instance, be in the form
of new words or syntactic constructions. Since grammatical inference algorithms generalize beyond
their input, incorporating a learning algorithm in an application may be used to make the application
more robust in dealing with unexpected input. If incorporating the grammatical inference system
(or an alternative implementation of such a system) improves the performance of the overall appli-
cation, then we can attribute the increase in performance to the newly added grammatical inference
module.
1.5
SUMMARY
To summarize, both formal and empirical grammatical inference have their roles to play in compu-
tational linguistics. While the two areas of research may seem quite different, there are quite a few
similarities.
At the start of both research processes, one of the ﬁrst questions one encounters is the bias
decision (see Section 2.5 for a discussion on bias), which consists of making a guess about which
grammars correspond best with the language(s) we are trying to learn. Exactly which class of
grammars is selected is based on at least two reasons. First, we need to believe that the class of
grammars we select is going to be strong enough to be able to describe the language(s) we are trying
to learn and that it has other properties, such as probabilistic variants, necessary for the problem
at hand. Second, algorithms which can learn these grammars from some input should either be
inadequate in some way or non-existent. Otherwise, it would not be research!
Both formal and empirical grammatical inference are attempting to learn grammars from
data (either explicitly present or not). The techniques used can be very similar, if not identical. The
primary difference is how success is measured. In formal learning, we are primarily going to measure
success by transforming an ill-posed learning problem into a (mathematically) well-posed one and
proving an algorithm solves this problem. For the result to be useful in a deployable system, it must
be the case that the problem we face in the real world is an instance of this formal problem. If not,
then all bets are off. For example, an algorithm which provably efﬁciently learns a family of languages
probably will not be very effective in a natural language processing system if natural languages do
not belong to this family of formal languages.

14
1. STUDYING LEARNING
In empirical learning we will (usually) be measuring success with respect to a gold standard,
which is a proxy for the “true” answer. This type of evaluation does not make the assumption that
what we had to learn belongs to one family or another: the aim is to learn a grammar whose behavior
on some task is as close as possible to the gold standard (according to the evaluation metric).
1.6
FORMAL PRELIMINARIES
In order to be precise when discussing learning, it is essential that the meaning of all the terminology
is clear. In this section we will introduce several concepts that will be used throughout the book.
However, we will assume the readers are familiar with certain concepts and notation. For
instance, we assume a basic familiarity with set theory and its notation, such as the empty set
(∅), union, intersection, and set difference (∪, ∩, \). Moreover, given a set X, we will write |X|
for the cardinality of X. The symmetric difference between two sets (or languages) A and B is
A ⊕B = (A \ B) ∪(B \ A).
We also assume familiarity with the standard logic and will make use of the logical connectives
representing “and,” “or,” negation, implication, and bi-conditional (∧, ∨, ¬, ⇒, ⇔, respectively),
the universal and existential quantiﬁers (∀, ∃, respectively).
We also assume a basic familiarity with computational complexity theory and an understanding
of O notation and awareness of terms such as P, N P, and N P-complete. For readers unfamiliar
with these terms, we recommend the following texts: Garey and Johnson [1979] and Cormen [2013].
Grammatical inference deals with learning representations of languages. This requires us to
be precise about what a language is. Informally, a language contains strings. These strings are made
up out of symbols. These symbols come from the vocabulary.
Deﬁnition 1.1 (Vocabulary or alphabet)
A vocabulary or alphabet is a ﬁnite, non-empty set of
symbols .
In the natural language context, for instance, a vocabulary might be the set of words, when
describing syntax (with the potential problem that it is possible to generate new words, for instance
through compounding, which might lead to a theoretically inﬁnite set of symbols), or phonemes,
when describing phonological representations.
Based on the vocabulary, we can create strings, which may also be called sequences.
Deﬁnition 1.2 (String)
A (ﬁnite) string w = a1 . . . an is a possibly empty, ﬁnite, ordered list of
symbols. We write λ for the unique string of length 0 (called the empty string) and |w| for the length
of w. Thus n = |w|. ∗denotes the set of all ﬁnite strings over .
With strings we can now deﬁne languages.

1.6 Formal Preliminaries
15
Deﬁnition1.3 (Language)
A language L is a possibly inﬁnite set of strings: L ⊆⋆. Let N denote
the set of non-negative integers. For all k ∈N, let ≤k = {w ∈⋆: |w| ≤k} and >k = {w ∈⋆:
|w| > k}.
We say that u is a subsequence of v, denoted u ⪯v, ifdef
u = a1 . . . a|u| and there exist
v0, . . . , v|u| ∈⋆s.t. v = v0a1v1 . . . a|u|v|u|.
We say that u is a substring of v ifdef there exist two strings l and r, possibly empty, such that
v = lur.
For any ﬁnite set of strings L, we let ∥L∥be the sum of the lengths of the strings in L. We
will write |L| for the cardinality of L.
The concatenation of two languages L1 and L2 is written L1L2 and is deﬁned to be L1L2 =
{wv | w ∈L1 and v ∈L2}. It is also useful to deﬁne the Kleene star operation with respect to
languages. The Kleene star of a language L is another language written L∗and is deﬁned recursively:
λ ∈L∗and for all w ∈L, w ∈L∗(the base cases) and w, v ∈L∗⇒wv ∈L∗(the recursive case).
Note that the formal descriptions of languages, symbols in  and strings from ⋆are simply
formal concepts. Depending on what symbols are available in  and hence can be used in ⋆, people
might assign speciﬁc meaning to these symbols, strings, and languages. However, from a formal
perspective, these deﬁnitions do not require the assignment of a particular meaning. To describe
exactly which strings are an element of a particular language, a representation of the language is
required. This is done using a grammar.
Deﬁnition 1.4 (Grammar)
A grammar GL is a ﬁnite representation that describes a (possibly
inﬁnite) language L.
Exactly how a grammar describes the inﬁnite language depends on how it is interpreted.
While a grammar deﬁnes just one language, languages which admit common grammar formalisms
(i.e., a common notation and interpretation) form a family of languages.3 It is useful in this regard
to deﬁne classes of grammars and the families of languages that are associated with them.
Deﬁnition 1.5 (Language and grammar families)
A class of languages L is represented by the
grammars of a class G. L and G are related by a naming function L : G →L that is total (∀G ∈
G, L(G) ∈L) and surjective (∀L ∈L, ∃G ∈G such that L(G) = L).
In words, any language in L admits a grammar from G. For any string w ∈⋆and language
L ∈L, we will write L |= w ifdef w ∈L. This corresponds to the notion of being able to recognize
3. Technically, there is a difference between a family (the family of regular languages), which is an abstract notion but not a
set, and a class of languages over some ﬁxed alphabet . In the latter case, mathematical manipulations are possible. We will
nevertheless not make this difference in the sequel, and use freely both terms, with an implicit alphabet when so required.

16
1. STUDYING LEARNING
whether string w belongs to language L. Additionally, the grammars should be understood as
allowing a given parser (whatever it may look like) to recognize the strings. For any string w ∈⋆
and grammar G ∈G, we will write G ⊢w if the parser recognizes w. Basically, the parser must be
sound and complete with respect to the interpretation: G ⊢w ⇐⇒L(G) |= w.
There are many ways to write grammars. Mathematically, grammars can be sets of strings, or
tuples of sets or other ﬁnite objects. It is possible to develop universal coding systems; for example,
sets and tuples themselves can be expressed as ﬁnite strings (in fact, every ﬁnite string, and thus
grammar) can be expressed with a unique positive integer [Rogers 1967]). Thus, the naming function
and the ﬁnite grammar itself are deeply interwoven concepts. Together, they allow us to decide what
strings are recognized by the (potentially inﬁnite) language the grammar describes. Consequently,
there are several ways languages can be described. We illustrate this diversity of grammars with the
regular languages.
One particular grammar formalism is that of deterministic ﬁnite-state acceptors.
Deﬁnition 1.6 (Deterministic ﬁnite-state acceptor (dfa))
A deterministic ﬁnite-state acceptor is a
quintuple ⟨, Q, q0, F , δ⟩for which
.
 is the ﬁnite set of input symbols, corresponding to the vocabulary;
.
Q = {q0, q1, . . . , qN−1} is the ﬁnite set of N states;
.
q0 is the start state;
.
F is the ﬁnite set of ﬁnal states (F ⊆Q); and
.
δ : Q ×  →Q is the transition function. Given a state q ∈Q and input symbol i ∈, either
δ(q, i) is undeﬁned or it returns a state q′ ∈Q.
We refer to the class of dfa with GDFA. We also let the size of a dfa be given by its number of
states: ∥⟨, Q, q0, F , δ⟩∥≡|Q|.
How do we identify the strings recognized by a dfa? In other words, what is the naming
function? How are these objects interpreted? These questions are answered as follows. For each
dfa, δ recursively deﬁnes a function δ∗: Q × ⋆→Q. For all q ∈Q, let δ∗(q, λ) = q and, for all
u ∈⋆, a ∈, let δ∗(q, ua) = δ(δ∗(q, u), a). (If δ∗(q, u) is undeﬁned or if for some q ∈Q and
a ∈, δ(q, a) is undeﬁned then δ∗(q, ua) would then also be undeﬁned). For each dfa A, the
language of A is L(A) = {w ∈⋆| δ∗(q0, w) ∈F}. The class of regular languages contain exactly
those languages for which a dfa exists which describes it.
Deﬁnition 1.7 (Family of regular languages)
LRL = {L | (∃A ∈GDFA)[L(A) = L]}
Note that other representations, such as regular expressions (also used in several programming
languages), can also be used to describe exactly all regular languages.
Deﬁnition 1.8 (Regular expression)
Given , a regular expression is deﬁned recursively as follows.

1.6 Formal Preliminaries
17
1. The base cases:
1. ∅∅∅is a regular expression.
2. λ is a regular expression.
3. For all σ ∈, σ is a regular expression.
2. The recursive cases:
1. If R is a regular expression then (R∗) is a regular expression.
2. If R and S are regular expressions then (RS) is a regular expression.
3. If R and S are regular expressions then (R + S) is a regular expression.
3. Nothing else is a regular expression.
While the deﬁnition above is suggestive, it is important to realize regular expressions are just strings
of uninterpretable symbols at this stage. To relate them to languages, we will need to make use of
an explicit naming function. This is accomplished recursively as follows.
1. The base cases:
1. L(∅∅∅) = ∅
2. L(λ) = {λ}
3. ∀σ ∈, L(σ) = {σ}
2. The recursive cases:
1. L(R∗) = (L(R))∗
2. L(RS) = L(R)L(S)
3. L(R + S) = L(R) ∪L(S)
The following theorem is a remarkable fact.
Theorem 1.1 (Kleene’s Theorem)
Every language deﬁnable with a regular expression is deﬁnable
with a dfa and vice versa.
Another grammar formalism is that of context-free grammars (cfgs).
Deﬁnition 1.9 (Context-free grammar)
A context-free grammar is a quadruple G = ⟨V , , S, R⟩
for which
.
V , the ﬁnite set of non-terminals;
.
, the ﬁnite set of terminals;
.
S ∈V , the start non-terminal; and
.
R ⊂V × (V ∪)∗is the set of productions (grammar rules).
For all (A, β) ∈R, we often write A →β. We refer to the class of cfgs with GCFG.

18
1. STUDYING LEARNING
Again, we can ask what is the naming function for context-free grammars? How are they
interpreted? The language of a context-free grammar is deﬁned as follows. The (partial) derivations
of a cfg G = ⟨V , , S, R⟩is written D(G) and is deﬁned recursively as follows.
1. The base case: S belongs to D(G).
2. The recursive case: For all A →β ∈R and for all γ1, γ2 ∈(V ∪)∗, if γ1Aγ2 ∈D(G) then
γ1βγ2 ∈D(G).
3. Nothing else is in D(G).
Then the language of the grammar L(G) is deﬁned as L(G) = {w ∈⋆| w ∈D(G)}.
Based on the deﬁnition of context-free grammars, we can deﬁne context-free languages.
Deﬁnition 1.10 (Family of context-free languages)
LCF = {L | (∃G ∈GCFG)[L(G) = L]}
It is another remarkable fact that every regular language is context-free, but not vice versa.
Theorem 1.2 ([Scott and Rabin 1959])
Regular languages are a proper subset of context-free
languages.
The two theorems above show what is possible when a grammar formalism is introduced.
They help realize what the expressive power of the grammar formalism is.
The Chomsky Hierarchy (Figure 1.3) describes the expressive power between four well-
known families of languages: regular languages (also called type 3), context-free languages (type
2), context-sensitive languages (type 1), and recursively enumerable languages (type 0). The ﬁrst
two have already been deﬁned.
Context-sensitive languages can be described using context-sensitive grammars, which are
very similar to context-free grammars, with the difference that all productions in R are of the
form αAβ →αγβ with A ∈V , α and β ∈(V ∪)∗, and γ ∈(V ∪)+. Recursively enumerable
languages (also called computably enumerable) are languages for which a Turing machine exists that,
for every string in the language, correctly answers “yes” if asked whether the string belongs to the
language.4
If context-free languages are more powerful than regular languages, why even consider or use
regular languages? Or, in that same line of reasoning, why even consider context-free languages and
not go all the way for recursively enumerable languages?
4. The Turing machine can be thought of as a grammar for this language. Note it does not have to answer if asked about
a string which does not belong to the language. Languages for which there exists a Turing machine which has to answer
correctly about the membership of every string in ⋆form the recursive class of languages. This class is a proper subset of
the recursively enumerable languages and properly contains the context-sensitive languages.

1.6 Formal Preliminaries
19
One answer is that there appears to be a trade-off between generative power and efﬁciency. For
instance, recognizing membership of strings in regular languages can be done in linear time, T (n) =
O(n); recognizing membership in context-free languages can be done in cubic time, T (n) = O(n3);
and for more powerful language families, such as context-sensitive languages, this is worse. In fact,
it is P-space complete.5
Another reason comes from the perspective that scientiﬁc theories and hypotheses ought to
be strong and falsiﬁable. So if one’s theory of natural language is that anything computable is a
natural language then that is the weakest theory possible that makes no falsiﬁable predictions (at
least under the Church–Turing thesis). It follows that the hypothesis that all natural languages are
describable with dfas is a stronger scientiﬁc hypothesis than the one that says all natural languages
are describable with cfgs. The evidence is, however, that this hypothesis is not correct [Chomsky
1956, Shieber 1985].
It is for these reasons that the goal to identify the smallest family of languages that contains
all possible human languages (or the smallest family of languages relevant to some aspect of human
language) is reasonable. This may not necessarily be one of the families in the Chomsky Hierarchy;
instead it may be one that cross-cuts those classes as shown in Figure 1.3.
5. In complexity theory, a problem is said to be P-space complete if it is hardest between all problems which can be solved
with a Turing machine which uses memory polynomial in the size of its input. The generally accepted conjecture is that
this means that no reasonable (polynomial-time) algorithm exists.

21
C H A P T E R 2
Formal Learning
2.1
INTRODUCTION
The goal of this chapter is to show a global picture of the formal issues and results in grammatical
inference. A more extensive survey can be found in de la Higuera [2010]. The theory underlying
grammatical inference rests upon a number of pillars such as the theory of languages and automata
[Sudkamp 2006], their probabilistic counterparts such as hidden Markov models [Rabiner 1989],
and basic concepts from computational complexity [Sanjeev and Boaz 2009], computational learning
theory [Kearns and Vazirani 1994], and information theory [Cover and Thomas 1991].
2.1.1
THE ISSUES OF LEARNING
Grammatical inference is about learning a grammar given information about a language. Generally
speaking, the information a learner is going to have access to concerns a language: if in linguistic
terms a language may have a meaning rendered complex by the point of view with which one is
approaching these questions, in mathematics a language is just a set of strings.1 This set may be
inﬁnite, each string having some simple semantic feature associated with it, which may be just a
label indicating if the string belongs to the language or not, or, more informatively, the structure (or
parse tree) of the string, or even its probability depending on some given distribution. The learner
might be given access to a large quantity of sentences (a corpus), which may be organized, and which
may be annotated. The learner is in some settings able to interrogate an expert (or alternative sources
of data) in order to obtain responses to queries: Does this string or sentence belong to the language?
How can we complete this sentence? What are the most frequent sentences in the language? Can
this sentence be annotated? Translated?
With this information the learner’s goal is to build a representation of the language: the
representation will typically be a ﬁnite-state machine (which allows one to recognize the sentences
from the language), a grammar (which can be used to generate sentences from the language), or
another formalism (a regular expression will deﬁne the set of sentences in the language).
1. As described in the previous chapter.

22
2. FORMAL LEARNING
The fact that a learner will build its own formalism of the intended language poses an
interesting question: If two learners build two different-looking grammars, how can they agree that
they are using the same language? The answer to this question is in many cases negative: they cannot
(the problem of deciding equivalence between two grammars, for many classes, is intractable). This
is a serious hint that learning grammars is a difﬁcult task. A puzzling scenario allowing us to solve
the equivalence problem for two grammars G1 and G2 could be the following: a learner tries to learn
from data generated from G1, and obtains G′
1, does the same from G2, obtaining G′
2. Now, should
not G′
1 and G′
2 coincide, in a syntactic sense?
When learning only from strings, which is also usually called unsupervised learning, an
attractive alternative is to build a probabilistic artifact. A probabilistic context-free grammar, a
probabilistic ﬁnite automaton, or a hidden Markov model will each associate a quantity with a
string, typically deﬁning the probability of that string. In this case, the notion of associated language
may not be meaningful: if we just talk about the language of all strings that have non-null probability
then we may have two very different distributions that would be equivalent as far as their associated
languages are concerned. Furthermore, this would not even prove useful. Another deﬁnition would
be to say that a string is in the language when its probability is above a given threshold. This tempting
deﬁnition leads to severe computational problems as, even in the case where the distribution is
produced by a ﬁnite-state machine, a number of associated problems are intractable, like (1) knowing
if such a language is ﬁnite or (2) ﬁnding the most probable string.
Probabilistic ﬁnite-state automata (pfa) can be seen as a special case of transducers: these are
ﬁnite-state machines taking strings as inputs and also as outputs. In the case of pfa the outputs are
probabilities. Generalizing, the output can be a multiplicity or even another string (the translation
of the ﬁrst one). In a broad sense, when grammatical inference deals with transducers, the goal is to
learn functions which take as inputs strings.
2.1.2
LEARNING SCENARIOS
Depending on the intended application, the learner will have access to the data in one way or another.
The way we receive the data, the price we have to pay for it, or the richness of the information received
are all going to matter. Let us explore some typical learning settings.
Batch learning is a situation where we are given a (usually large) number of strings. These strings
may come with extra information: a label for each string in the case of a classiﬁcation task,
tags attached to substrings, brackets inside the string, or a weight. Two typical settings are
those of learning from text where only strings from the language are given to the learner, and
learning from an informant where the strings from which one is to learn are labeled with 1 or

2.1 Introduction
23
0 depending on the fact that they belong or not to the language. The learner is supposed to
build a hypothesis from this sample.
Online learning is a very typical setting from a theoretical point of view: a learning situation
where the learner receives the items of data one after the other and is supposed to build a
new hypothesis after having seen each new learning example. Gold [1967] introduced this
setting in order to show that certain learning algorithms had strong convergence properties
(identiﬁcation in the limit): the inﬁnite process (new example →new hypothesis) is supposed
to converge to just one grammar being produced after a ﬁnite number of steps, provided some
completeness conditions are met concerning the presentation of the examples. It has also been
argued that this learning situation is well suited to represent the language acquisition task a
child has to face.
Active learning is a setting where no data is available at ﬁrst, or where only unlabeled data is
available. The learner then has not only to learn (generalize, induce, etc.) but to query the
environment in order to obtain its data or a labeling of this data. Typically the learner will
attempt to ﬁnd out if a particular string belongs to the language or not (membership query) or
will ask for some extra information about a string present in the data set.
Interactive learning is a special case close to the previous one—the learner classically attempts to
automatically build the model, but will interact with an oracle, through some query system:
typically the oracle can intervene by correcting some decision made by the learner.
2.1.3
LEARNING GRAMMARS OF LANGUAGES
A ﬁrst non-trivial question is that of deciding if we are to learn grammars or languages. If common
knowledge tells us that this is about language, about learning or acquiring a language, we will argue
that since the goal is to study effective ways of learning, there is always a representation issue.
Furthermore, a number of results show that for a given class of languages, there will be considerable
differences between learning one type of representation rather than another.
Having decided that we need to learn a grammar, we now have to choose what type of grammar
we require. We will discuss in Section 2.3 the different arguments that should be considered when
making this choice. For the moment, and without entering into precise deﬁnitions which can be
found in a number of textbooks and research articles, let us list some of the most common and
important types of ﬁnite-state machines and grammars:
.
ﬁnite-state machines that recognize languages: deterministic ﬁnite-state automata and their
non-deterministic counterparts;
.
ﬁnite-state machines that deﬁne distributions over strings: probabilistic automata or hidden
Markov models;

24
2. FORMAL LEARNING
.
ﬁnite-state machines that describe or generate transductions, or translations from one lan-
guage to another; and
.
context-free grammars that are also used to recognize, generate, and describe more complex
structural rules.
2.2
LEARNABILITY: DEFINITIONS AND PARADIGMS
Learning is a complex phenomenon. It has been argued that it is about compression, encoding,
discovery of patterns, and even the capacity of forgetting. Let us explore some mathematical
deﬁnitions in which the convergence of learning, speed, and quantity of resources needed to learn
can be analyzed.
2.2.1
BLAME THE DATA, NOT THE ALGORITHM
At ﬁrst, learning seems to be an ill-posed problem. Suppose we have a learning algorithm which
on some data returns a grammar. Why should one learned grammar be better than another? Why
should this grammar even be the right grammar? After all, the learner has only had access to a ﬁnite
quantity of data!
In order to transform the seemingly ill-posed learning question into a well-posed question
whose solution does give us insights to what learning can mean, we transform it into a convergence
problem. Learning is going to be measured as the convergence toward a stable and good solution.
In an ideal world, one would hope to have this convergence depend on a magic number: as soon as
a given quantity of information or data is available, the intended grammar would be learned. But
many things can go wrong: the data may not be representative or, even when it is, we may be facing
some intractable problem. It is therefore necessary to impose some conditions on the data in order
to secure a learning result, which will therefore always be read as: provided the data available has
a minimal quality (with respect to a target and the criterion we impose), we can ensure that the
solution is good. Ensuring might still depend on some probabilistic notion, and good will often also
be probabilistically founded.
2.2.2
A NON-PROBABILISTIC SETTING: IDENTIFICATION IN THE LIMIT
The ﬁrst important deﬁnition is due to Gold [1967]: identiﬁcationinthelimit. In this learning setting,
the learner has access to a never-ending supply of information about the language to be learned. This
may be the actual strings from the language, or labeled strings indicating if they belong or not, or any
speciﬁcs about the language. The important thing is that the presentation of this information has
to be complete: all the possible information has to be presented eventually. For example, if learning

2.2 Learnability: Deﬁnitions and Paradigms
25
from text, which is the modality of learning from positive examples only, each string in the language
should appear at some point or another.
After receiving each piece of information the learner is to return a hypothesis. For a class of
grammars to be identiﬁable in the limit it is required that given any grammar G in the class and any
complete presentation of L(G), there is a point where the learner begins to systematically output a
hypothesis grammar G′ (convergence) and L(G′) = L(G) (correctness). By a complete presentation
we mean that each admissible information is presented at least once.
Example 2.1
Let us consider the case of the regular languages. These can be represented with
deterministic ﬁnite automata (dfa). If the type of information we are learning from consists
of examples and counter-examples, the setting is called learning from an informant. A complete
presentation pres of a language L will present pairs (w,1) and (w,0) such that on one hand
{w : (w, 1) ∈pres} = L and {w : (w, 0) ∈pres} = ⋆\L. Then an algorithm which systematically
searches for the smallest dfa consistent with the information seen so far2 is going to achieve
identiﬁcation in the limit: at some point all the strings of length up to 2n −1 (where n is the number
of states in the target dfa) will have appeared and theory tells us that any other dfa consistent with
the data is larger than the target. Therefore, dfa are identiﬁable in the limit from an informant.
Main Learning Results
Gold [1967] proved that any recursively enumerable class of languages was identiﬁable in the
limit from an informant. On the other hand, when learning from text (only positive instances are
presented) the situation is drastically different: he proved that any class containing all ﬁnite languages
and at least one inﬁnite language was not identiﬁable in the limit. The proof is not trivial, but the
reader who wishes for himself to try to invent an algorithm should rapidly become convinced of the
impossibility of doing so. This, of course, holds for the regular languages, and most well-known
classes.
2.2.3
AN ACTIVE LEARNING SETTING
In active learning, the learner is not directly presented with information, but has to ask for it. It does
this by querying the oracle. A number of different queries have been introduced since the introduction
of this model by Angluin [1987]: the most important ones are membershipqueries, in which the learner
suggests a string to the oracle and receives the status of this string as answer, and (strong) equivalence
2. Consistency entails that the (smallest) dfa accepts all the positive examples and rejects all the negative ones. The fact
that this particular problem cannot be solved by any polynomial-time algorithm is irrelevant here, but the algorithm rpni
discussed in Section 3.7 does solve this problem efﬁciently.

26
2. FORMAL LEARNING
queries, in which the oracle is presented with a candidate grammar and is to answer “yes” or provide
the learner with a counter-example.
The main difference with the online model is that the learner is now in charge of the learning
session: it decides the next step, including the fact that there is a next step. Therefore, it must decide
at some point to halt. Learning is achieved if it halts with the correct hypothesis (equivalent to the
target).
It should be noted that oracles are both abstract mathematical objects and representative of
practical learning situations: when the learner can interrogate the environment, query a human
expert, or ask the World Wide Web, there may be the chance of using an active learning algorithm.
Main Learning Results
Without further complexity conditions these settings may lead to speciﬁc analyses for researchers
in inductive inference, but, probably, computational linguists will not ﬁnd here what they need.
Indeed, even dfa cannot be learned from positive data and membership queries alone: for a class to
be learnable, the learner must know when to halt, which means that at most one consistent hypothesis
is left in the search space.
2.2.4
INTRODUCING COMPLEXITY
In what precedes we have worried about the capacity of our learning algorithm to converge, some
day. We obviously need something of more practical use. We want to be able to say that learning
takes a reasonable amount of time and energy.
Complexity results usually will depend on the size of an instance of the problem. In the case
of learning, the instance comprises the data which is given to the learner, but also the target itself,
even if the learner never gets to see this target!
What Should We Count?
We ﬁrst need to know what we are counting. We are in a practical situation where we are given data
and are supposed to build a hypothesis. We are concerned not only with the capacity of building
something, but also of doing as well as possible with the data we are given. Moreover, there may be
a situation where even if we had as much time as we required, we may not have enough data to be
able to build a reasonable hypothesis.
Let us explore brieﬂy some of these ideas.
.
The size of what we are learning is obviously an issue: if trying to learn a dfa with three states
over a two-symbol alphabet—not a very interesting task for computational linguistics—we
will certainly need less data than if we are to learn a complex context-free grammar, with
hundreds of rules and over an alphabet made of words from some common language. Note

2.2 Learnability: Deﬁnitions and Paradigms
27
also that since simple (formal) languages can be inﬁnite, the actual cardinality of the language
is not a useful measure for complexity.
.
The size of the information we have been given: The more information we have to process,
the longer we will need.3 This may seem simple, but how should we measure the actual size
of a corpus? Is it the number of strings? The number of different strings? Or, the number of
symbols that intervene in those strings, i.e., the sum of the lengths of all strings in the sample?
A survey of these questions can be found in de la Higuera et al. [2008].
How Should We Count?
In order to be able to talk about the time needed to learn, we have to make a difference between the
real learning problem (usually in the batch learning setting—given a sample S, learn a grammar G)
and the online problem used in the analysis: we will suppose that the data arrives to the learner one
item at the time, and that the learner is required to build a new hypothesis from the data it has had
access to so far.
In this theoretical context one can consider counting a number of things.
.
The number of errors before learning. In an online setting, the learning algorithm is presented
with a new unlabeled piece of data. Its running hypothesis should be able to classify or label this
input string. The implicit prediction error measure used by Pitt [1989] counts this number. A
good learning algorithm is one that will only make a polynomial number of implicit prediction
errors before converging.
.
The number of mind changes is the number of times the learning algorithm has to modify
its hypothesis, before learning. Again, it would seem reasonable that this number varies
polynomially with the size of the target.
.
Another way of measuring concerns the size of a sample sufﬁcient for learning to take place.
This characteristic sample may be seen as provided by a teacher. If small it could mean that the
probability that it appears in a random training sample is high. It should be noticed that some
types of grammars admit small characteristic samples, whereas others do not [de la Higuera
1997].
These questions were analyzed by Pitt [1989], de la Higuera [1997, 2010], and Eyraud et al.
[2015]. Summarizing, it can be shown that in most models even deterministic ﬁnite automata are
not polynomially learnable. In some cases, this is the “hardest” class one can learn.
3. When learning from data streams, a goal is to limit this factor: the learner is not allowed to memorize all the data. For a
ﬁrst example of grammatical inference in this setting, see Balle et al. [2014b].

28
2. FORMAL LEARNING
2.2.5
A PROBABILISTIC VERSION OF IDENTIFICATION IN THE LIMIT
A probabilistic model (or probabilistic language) is a distribution of probabilities over the set of all
strings. When learning a probabilistic model two things change: one still does not have control over
the presentation, but one can measure bad luck, i.e., the fact that some important information has
not yet appeared. On the other hand, what is to be learned has changed, and one is now interested
in learning both a structure and the parameters of this structure.
Horning [1969] proposed to learn probabilistic grammars for natural language processing:
since then, this line has been followed by a number of researchers. Strong arguments in favor have
been proposed by Clark and Lappin [2011].
The notion of identiﬁcation in the limit can be extended to cope with learning distributions.
In this case, instead of being complete, the presentation of the examples is supposed to follow the
distribution which is to be learned. The probability that the empirical distribution of examples is
indeﬁnitely very different from the theoretical one is 0: we cannot get a skewed distribution forever.
Therefore, a class of probabilistic grammars is identiﬁable in the limit with probability 1 if
there exists a learning algorithm which, given any grammar in the class is guaranteed to build a
grammar G′ equivalent to G, after having seen a ﬁnite number of examples, with probability 1.
Main Learning Results
Identiﬁcation in the limit with probability 1 has been studied since Angluin [1988a] who ﬁrst
analyzed the problem and suggested an enumerative algorithm. There are two different issues:
identifying the structure and identifying the probabilities. A number of results related to the ﬁrst
question can be found in de la Higuera and Oncina [2004] and Stern-Brocot trees are used in order
to identify the probabilities in de la Higuera and Thollard [2000].
2.2.6
PROBABLY APPROXIMATELY CORRECT (PAC) LEARNING
The Probably Approximately Correct (pac) paradigm was introduced by Valiant [1984] and has been
widely used in machine learning.
In order to deﬁne pac learning, a number of extra parameters have to be introduced:
.
n, a parameter measuring the size of the target under scrutiny (typically the number of states,
for a ﬁnite-state machine);
.
m, an upper bound on the length of the strings we want to classify;
.
ϵ, the error one is prepared to accept; and
.
δ, the conﬁdence with which we want to be within the error.

2.2 Learnability: Deﬁnitions and Paradigms
29
About Distances
In pac learning, the goal is to approximate a target grammar GT by a hypothesis grammar GH. In
order to measure how close GH is to GT a distance is required. There are two cases to consider.
.
GT is a (classifying) grammar deﬁning a language, and an independent underlying, but
unknown distribution D exists. This distribution reﬂects the importance of the data: we expect
that a string w1 whose probability p1 is double of that of another string w2 would therefore be
twice as frequent in a random sample as w2. It should be noted that this does not mean that
w1 has probability p1 of being in a language, just that it has probability p1 of being randomly
drawn.
Then dD(GH , GT ) = PrD(x ∈L(G) ⊕L(H)). In words, this is the total mass of prob-
ability of the misclassiﬁed strings.
For example, suppose LT = {anbn : n ∈N} and we use the following distribution over
⋆: PrD(u) =
1
22|u|+1. Now if LH = ∅, we have dD(GH , GT ) = PrD(LT ) = 
i∈N
1
24i+1 =
1
2

i∈N ( 1
16)
i = 8
15.
If LH = {anbn : n > 0}, dD(GH , GT ) = PrD(λ) = 1
2.
.
GT is a probabilistic grammar. In this case the examples are drawn following the distribution
deﬁned by the grammar itself. PrGT and PrGH denote the probability functions using the
target grammar and (respectively) the hypothesis grammar. A number of alternative distances
have been proposed:
d∞(GT , GH) = maxx∈⋆| PrGT (x) −PrGH(x)|
dk(GT , GH) =

x∈⋆(| PrGT (x) −PrGH(x)|k)
 1
k
kl(GT , GH) = 
x∈⋆PrGT (x) log(PrGT (x)/ PrGH(x)).
The Kullback and Leibler [1951] (kl) divergence is not a distance but it measures the cross-entropy
between the target grammar and the hypothesis.
Deﬁnition 2.1
Let GT be the target grammar and GH a hypothesis grammar. Let ϵ > 0. We say
that GH is an ϵ-good hypothesis w.r.t. G for distance d ifdef d(GT , GH) < ϵ.
A learning algorithm is now asked to build a grammar given a conﬁdence parameter δ and an
error parameter ϵ. The algorithm is also given an upper bound n on the size of the target grammar
and (sometimes) an upper bound m on the length of the examples it is going to get. The algorithm
can query an oracle for an example randomly drawn according to the distribution D. The query of an
example or a counter-example will be denoted Ex(). When the oracle is only queried for a positive
example, we will write Pos-Ex(). And when the oracle is only queried for strings of length ≤m, we
will write Ex(m) and Pos-Ex(m), respectively. Formally, the oracle will then return a string drawn

30
2. FORMAL LEARNING
from D, or D(L(G)), or D(≤m), or D(L(G) ∩≤m), respectively, where D(L) is the restriction of
D to the strings of L: PrD(L)(x) = PrD(x)/ PrD(L) if x ∈L, 0 otherwise. PrD(L)(x) is not deﬁned
if L = ∅.
Deﬁnition 2.2 (Polynomial pac-learnability for discriminant grammars)
Let G be a class of
grammars. G is pac-learnable ifdef there exists an algorithm A s.t. ∀ϵ, δ > 0, for any distribution
D over ⋆, ∀n ∈N, ∀G ∈G of size ≤n, for any upper bound m ∈N on the size of the examples, if
A has access to Ex(), ϵ, δ, n, and m, then with probability larger than 1 −δ, A returns an ϵ-good
hypothesis w.r.t. G. If A runs in time polynomial in 1
ϵ , 1
δ, n, and m, we say that G is polynomially
pac-learnable.
In the case where the goal is to learn probabilistic grammars, the restriction on the length of
the strings is meaningless and the deﬁnition is adapted as follows.
Deﬁnition 2.3 (Polynomial pac-learnability for probabilistic grammars)
Let G be a class of
probabilistic grammars. G is pac-learnable ifdef there exists an algorithm A s.t. ∀ϵ, δ > 0, ∀n ∈N,
∀G ∈G of size ≤n, if A has access to Ex(), ϵ, δ, n then with probability larger than 1 −δ, A
returns an ϵ-good hypothesis w.r.t. G (and the intended distance). If A runs in time polynomial in
1
ϵ , 1
δ, and n, we say that G is polynomially pac-learnable.
Main Learning Results
The pac-learnability of grammars from strings of unbounded size has always posed technical
difﬁculties. Most results are negative [Kearns and Valiant 1989, Warmuth 1989, Kearns and Vazirani
1994].
When learning probabilistic automata, pac-learning is the dominant setting. One of the
reasons for this is that the identiﬁcation in the limit with probability one has not allowed any
satisfying deﬁnition regarding complexity issues [de la Higuera and Oncina 2004]. For different types
of distances, pac-learning algorithms have been proposed for deterministic probabilistic automata
from text [Clark and Thollard 2004, Palmer and Goldberg 2005, Castro and Gavald`a 2008], with
queries [Balle et al. 2010], and from data streams [Balle et al. 2014b].
A related independent question is that of computing the distances between distributions, i.e.,
between the probabilistic grammars or automata which deﬁne these. Interestingly, for pfa some
distances are tractable while others are not, whereas for probabilistic context-free grammars, no
distance can be computed [Lyngsø and Pedersen 2002, Nederhof and Satta 2004, de la Higuera
et al. 2014].

2.3 Grammar Formalisms
31
2.3
GRAMMAR FORMALISMS
In many grammatical inference situations, the class of grammars is imposed. But there are also several
situations where a careful analysis of the data and the goals of the task may allow one to choose the
hypothesis class. The choice of the class is obviously crucial to success in learning: a discriminant
model may be better than a probabilistic one; a simpler model (which will represent poorer structures)
might do the job as well as a more complex one and, furthermore, be easier to learn.
In this section we present some of the most important classes of grammars and automata. In
order to help the reader to choose the right type of grammars, we will, in each case, measure the
capacity of a grammar (or automaton from that class) to do each of the following:
To parse: How easy is it, given an input string, to obtain the expected output?
To model: What grammatical structures can, or cannot, be modeled by using a grammar from
this class?
To learn: Do we have learning algorithms (or, on the other hand, theoretical negative results)?
Complete formal deﬁnitions can be found in a number of textbooks; we choose here to present the
models informally and rely principally on examples.
2.3.1
FINITE-STATE MACHINES RECOGNIZING STRINGS
Finite-state automata are used to decide if a string belongs to a language or not. A ﬁnite-state
automaton is built using states and transitions between states. These transitions are labeled by
symbols from the chosen (input) alphabet. A particular state is chosen as initial state (but there
can be more than one if the automaton is non-deterministic); certain states are marked as ﬁnal or
accepting states. An automaton recognizes a string if there is a path of transitions leading from an
initial state to a ﬁnal state which reads this string. The language recognized by the automaton is
exactly the set of strings recognized by it. We build upon the deﬁnitions from Section 1.6.
Deterministic Finite-State Automata
The language LA recognized by the automaton A is the set of all strings x for which there exists a
path leading from the initial state to a ﬁnal state which reads string x. In the example represented
in Figure 2.1, a is in LA, whereas ab is not. This machine is deterministic in the following sense:
(1) there is just one initial state and (2) in every state, when having to read any symbol, there is at
most one admissible transition.
Parsing with a dfa is straightforward and can be done in time linear in the length of the string
to be parsed.
dfa admit a minimum canonical form, unique up to the names of the states. This has
important consequences: equivalence can be tested in polynomial time.
Table 2.1 summarizes the parsing, modeling, and learning criteria for dfa.

32
2. FORMAL LEARNING
q2
q0
q2
q1
b
b
b
a
a
a
a
b
FIGURE 2.1: Graphical representation of a dfa.
TABLE 2.1: Deterministic ﬁnite-state automata: dfa
Criterion
Comment
Parsing
Parsing a string of length m can be done easily in time linear in m.
Modeling
A dfa can be used to recognize any regular language.
Learning
There are algorithms to learn dfa from an informant (from both examples and
counter-examples). But to learn from positive examples only, extra bias is
needed: the class of dfa (and therefore of languages) has to be reduced. Study
of dfa learning has been very extensive with algorithms, in the active setting
[Angluin 1988b], or from an informant [Oncina and Garc´ıa 1992]. In order
to obtain positive learning results one can consider subclasses of the regular
languages, leading to constrained types of deterministic ﬁnite automata: k-
reversible languages admit automata which, when reversed, use a look-ahead
of size k to parse [Angluin 1982], and k-testable languages are deﬁned by legal
and illegal substrings of length k [Garc´ıa and Vidal 1990].
Non-Deterministic Finite-State Automata
In a non-deterministic ﬁnite-state automaton (nfa), more freedom is allowed, since there may be
different initial states and, from any particular state and any symbol, there may be several states
reachable when reading a string. In the case of the nfa represented in Figure 2.2, this means that
there are various parses for a particular string. For instance, string aa has three parses, two of which
reach a ﬁnal accepting state.
nfa can even have λ-transitions: these allow to move freely from one state to another. This
is represented in Figure 2.3: with λ-transitions parsing becomes even more complex. But these
λ-transitions can be removed with a polynomial-time algorithm.

2.3 Grammar Formalisms
33
q2
q0
q3
q1
b
b
a
a
a
a
a
a
FIGURE 2.2: An nfa.
q2
q0
q3
q1
b
b
a
a
λ
λ
a
a
a
FIGURE 2.3: An nfa with λ-transitions. Some strings admit an inﬁnity of parses.
nfa also suffer from some algorithmic inconveniences.
.
Checking if two nfa are equivalent is P-space complete. A and B are equivalent ifdef they
recognize the same language, i.e., if LA = LB.
.
Minimizing an nfa is an N P-hard problem. This is linked with the fact that there is no
simple tractable normal or canonical form for nfa.4
Table 2.2 summarizes the parsing, modeling, and learning criteria for nfa.
Regular Expressions
These are used to describe languages in a linear (non-graphic) way using the symbols of the alphabet
 and + (indicating the union) and ∗(for the iteration). The formal deﬁnitions are found in
Deﬁnition 1.8.
4. In complexity theory, an N P-hard problem is the hardest in the class of problems solvable in polynomial time by a non-
deterministic Turing machine. More practically, the hypothesis P ̸= N P is generally believed to be true; as a consequence,
an N P-hard problem does not admit a tractable algorithm.

34
2. FORMAL LEARNING
TABLE 2.2: Non-deterministic ﬁnite-state automata: nfa
Criterion
Comment
Parsing
Parsing a string of length m with an nfa of n states can be done in time in O(mn).
Modeling
A nfa can be used to recognize any regular language. But in certain cases
the smallest dfa equivalent with a given nfa can be exponentially larger.
Furthermore, there is no natural notion of canonical form, which also
corresponds to the fact that there is no straightforward semantics.
Learning
There are few positive results concerning learning nfa. When algorithms exist,
they will learn nfa corresponding to a subclass of the regular languages or
have a complexity function of that of the corresponding smallest dfa.
TABLE 2.3: Regular expressions
Criterion
Comment
Parsing
Parsing a string with a regular expression typically requires transforming the
regular expression into an nfa. The transformation can be expensive.
Modeling
Regular expressions model again the regular languages, like the dfa. But they
can appear as natural modeling tools, for example in web applications, where
xpath expressions are tree-like extensions of regular expressions.
Learning
There are few positive results concerning learning regular expressions. One
exception is by Fernau [2005], who attempts to learn these directly. In
text extraction applications, heuristics allowing to ﬁnd representing regular
expressions have been proposed.
Example 2.2
aba∗b(a + b)∗is a regular expression. abb, ababaa are strings belonging to the
language denoted by this regular expression. On the other hand, abaaa is not in the language.
Table 2.3 summarizes the parsing, modeling, and learning criteria for regular expressions.
2.3.2
PROBABILISTIC FINITE-STATE MACHINES
In the previous formalisms, parsing corresponds to answering the following question: Does this
(candidate) string belong or not to the language? All strings in the language have equal importance.
An alternative is to weigh the strings depending on their importance, which leads to the introduction
of weighted automata. A variant of these where all weights are positive and the sum of total weights
equals 1 leads to probabilistic languages. These can be recognized or generated, for example, by
automata, hidden Markov models, or probabilistic context-free grammars.

2.3 Grammar Formalisms
35
Probabilistic Finite-State Automata
Probabilistic ﬁnite automata (pfa) are generative devices: they are built from a dfa or nfa structure,
upon which three functions are added:
.
IP : Q →R+ ∩[0, 1] (initial probabilities);
.
FP : Q →R+ ∩[0, 1] (ﬁnal probabilities); and
.
δP : Q × ( ∪{λ}) × Q →R+ is a transition function; the function is complete: IP, δP, and
FP are functions such that

q∈Q
IP(q) = 1,
(2.1)
and ∀q ∈Q,
FP(q) +

a∈∪{λ},q′∈Q
δP(q, a, q′) = 1.
(2.2)
The above deﬁnition is inspired by Vidal et al. [2005] and de la Higuera [2010]. A historical
landmark is Paz [1971]. Let x ∈⋆, A(x) be the set of all paths accepting x: a path is a
sequence π = qi0x1qi1x2 . . . xnqin where x = x1 . . . xn, xi ∈ ∪{λ}, and ∀j ≤n, ∃pj ̸= 0 such that
δP(qij−1, xj , qij) = pj. The probability of the path π is
IP(qi0) . 
j∈[n]
pj . FP(qin).
And the probability of the string x is obtained by summing over all the paths in A(x). Note
that this may result in an inﬁnite sum because of λ-transitions (and more problematically λ-cycles).
An effective computation can be done by means of the forward (or backward) algorithm [Vidal
et al. 2005].
A pfa is represented in Figure 2.4; note that in some nodes, two probabilities are given—the
one that the state is initial, and the one of halting in that state. The probability of a given string is
taken by summing over the different accepting paths. On each path, the weights are multiplied. In
Figure 2.5 there are λ-transitions. Since furthermore there is a cycle of λ-transitions this means that
parsing is complex. It should be noted that even if pfa cannot be determinized in the way nfa can,
there are algorithms to eliminate the λ-transitions.
Table 2.4 summarizes the parsing, modeling, and learning criteria for pfa.
Hidden Markov Models
Hidden Markov models (hmms) [Rabiner 1989, Jelinek 1998] are ﬁnite-state machines deﬁned by
(1) a ﬁnite set of states, (2) a probabilistic transition function, (3) a distribution over initial states,
and (4) an output function.

36
2. FORMAL LEARNING
q3 : 0.3
q2 : 0.4
0.6 : q1 : 0.1
b 0.2
a 0.5
b 0.4
b 0.4
a 0.5
a 0.2
a 0.5
0.4 : q0
b 0.5
FIGURE 2.4: A pfa.
q3 : 0.3
q2 : 0.4
0.6 : q1 : 0.1
b 0.2
λ 0.5
λ 0.4
b 0.4
λ 0.5
a 0.2
λ 0.5
0.4 : q0
b 0.5
FIGURE 2.5: A pfa with λ-transitions.
An hmm generates a string by visiting (in a hidden way) states and outputting values when
in those states. An hmm is represented in Figure 2.6:
.
a state denoted 0.6 : q2 : (a, 0.5)(b, 0.5) has probability 0.6 of being chosen as initial, and
when reached will generate a and b with probability 0.5; and
.
the weight labeling a transition initiating in state q indicates the probability of following this
transition, when in state q.
Typical problems include ﬁnding the most probable path corresponding to a particular output
(usually solved by the viterbi algorithm).
Note that in order to obtain a distribution over ⋆and not each m, one solution is to
introduce a unique ﬁnal state in which, once reached, the machine halts. An alternative often used is
to introduce a special symbol (♯) and to only consider the strings terminating with ♯: the distribution
is then deﬁned over ⋆♯. The hmm from Figure 2.6 is transformed into the one represented in
Figure 2.7.
Equivalence results between hmms and pfa can be found in Vidal et al. [2005].
Table 2.5 summarizes the parsing, modeling, and learning criteria for hmms.

2.3 Grammar Formalisms
37
TABLE 2.4: Probabilistic ﬁnite-state automata: pfa
Criterion
Comment
Parsing
Parsing a string of length m with a pfa of n states can be done in time in O(mn2)
using the forward algorithm.
Modeling
The deterministic restriction (dpfa) corresponds to a less powerful class: some
distributions can be represented by pfa, but not by dpfa.
Learning
There are few theoretical positive results concerning learning pfa. The typical
algorithm is em (called baum-welch [Baum et al. 1970, Hulden 2012] in
this setting): it starts with a particular structure and an initial setting of the
parameters, then, iteratively, parses the strings from a sample and counts how
the transitions are used, then updates the weights accordingly. For the special
class of dpfa, there have been a number of algorithms, built on the state-
merging techniques [Carrasco and Oncina 1994, Ron et al. 1995, Thollard
et al. 2000, Clark and Thollard 2004]. The pautomac competition, which
took place in 2012, was won by Shibata and Yoshinaka [2014]. Bayesian
model-merging [Stolcke 1994] and spectral methods [Bailly 2011] are some
other methods used for this task. A complete presentation of the pautomac
ﬁndings can be found in Verwer et al. [2014].
q4 : (a, 1)
q3 : (a, 0.1)(b, 0.9)
0.4 : q0 : (a, 0.4)(b, 0.6)
0.6 : q1 : (a, 0.5)(b, 0.5)
0.2
0.6
0.8
0.4
0.5
0.3
0.7
0.5
FIGURE 2.6: An hmm.
2.3.3
TRANSDUCERS
Transducers take strings as inputs, but also as outputs; they allow one to recognize bi-languages, and
can also include weights.
Finite-State Transducers
Transducers are used for a number of tasks, including morphology [Roark and Sproat 2007] and
automatic translation [Amengual et al. 2001]. A transducer is a ﬁnite-state machine in which there is
not only an input but also an output string. Typically, outputs can be emitted both at the transitions
and the ﬁnal states.

38
2. FORMAL LEARNING
q4 : (a, 1)
q3 : (a, 0.1)(b, 0.5)(#, 0.4)
0.4 : q0 : (a, 0.4)(b, 0.3)(#, 0.3)
0.6 : q1 : (a, 0.5)(b, 0.5)
0.2
0.6
0.8
0.4
0.5
0.3
0.7
0.5
FIGURE 2.7: An hmm using ♯to terminate the generation of strings. It deﬁnes a distribution over ⋆♯, and
therefore can be used also over ⋆.
TABLE 2.5: Hidden Markov models: hmm
Criterion
Comment
Parsing
Parsing a string of length m with an hmm of n states can be done in time in
O(mn2).
Modeling
An hmm does not have ﬁnal states: it therefore deﬁnes a distribution over each
n. Through careful encoding, they can deﬁne the same distributions as those
modeled by pfa.
Learning
There are few theoretical results concerning learning hmm as the main algorithm
is an expectation-maximization method [baum-welch; Baum et al. 1970]
whose convergence is problematic. Other learning methods include spectral
methods [Hsu et al. 2012] and Gibbs sampling [Gelfand and Smith 1990].
q0
le :: the
ballon :: λ
rouge :: red ball
jaune :: yellow ball
balle :: λ
balle :: λ
la :: the
FIGURE 2.8: A rational transducer.
In Figure 2.8, we have represented a rational transducer, with outputs only on the transitions:
we can use this transducer to ﬁnd that the correct translation of input string “la balle rouge” is “the
red ball.”
Subsequential transducers are deterministic with respect to their input: this means that every
input can only be translated into at most one output. Oncina et al. [1993] show that they are learnable

2.3 Grammar Formalisms
39
q0
q1
q2
a :: 1
a :: 1
  0
a :: 10
11
b :: λ
# ::   0
00
# :: 00
01
b ::   0
11
FIGURE 2.9: A semideterministic transducer.
when they describe total functions: every input string has exactly one translation. Vilar [1996] shows
that they are learnable in an active setting by translation queries.
Extensions of this very constrained model exist: Allauzen and Mohri [2002] introduced
p-subsequential transducers: these have multiple outputs at the states. Furthermore, machines
for which inputs are deterministic but the outputs are not are called semideterministic ﬁnite-state
transducers [Beros and de la Higuera 2014]. One such transducer is represented in Figure 2.9: given
an input, there is at most one parse path, but there can be many different outputs.
On the other hand, general transducers can exist (with or without probabilities). They can be
normalized in such a way that all transitions have labels of one of the following forms:
.
input is a symbol, output is the empty string; and
.
input is the empty string, output is a symbol.
In Figure 2.10 we have represented such a transducer. It can be seen that ﬁnding the possible
output strings for a given input string is already a difﬁcult question.
Table 2.6 summarizes the parsing, modeling, and learning criteria for transducers.
Probabilistic Finite-State Transducers
Weighted and probabilistic transducers are becoming increasingly popular. Weighted transducers
have outputs that are weights and strings [Mohri 1997, Mohri et al. 2000].
Probabilistic ﬁnite-state transducers (pfst) are similar to pfa, but in this case two different
alphabets (source  and target ) are involved. Each transition in a pfst has attached a symbol
from the source alphabet (or λ) and a string (possible empty string) of symbols from the target
alphabet. pfsts can be viewed as graphs, as in Figure 2.11:
.
a transition labeled b :: 00, 0.2 will be followed with probability 0.2 and result in translating
the input symbol b into the string 00; and
.
when reaching state q2 there is probability 0.3 of halting.

40
2. FORMAL LEARNING
q1
q2
q7
q11
q13
q14
q10
q12
q6
q4
q15
q16
q8
q5
q9
q3
λ :: 4
λ :: 4
λ :: 2
λ :: 1
λ :: 1
λ :: 1
λ :: 1
λ :: 3
λ :: 3
λ :: 3
a :: λ
a :: λ
a :: λ
a :: λ
a :: λ
λ :: 2
λ :: 2
λ :: 4
λ :: 4
# :: λ
# :: λ
# :: λ
FIGURE 2.10: Normalized transducer.
TABLE 2.6: Transducers
Criterion
Comment
Parsing
In the context of transducers, parsing can lead to several different questions. If
the question is that of checking whether y is a correct translation for x, this
can be solved for general (non-deterministic) transducers in polynomial time.
The question of discovering all the translations for a given input is ill-posed as
the set can be inﬁnite, even if it is a regular language; see Figure 2.10.
Modeling
Subsequential transducers only accept one translation for every input string,
which is clearly a limitation. Extensions in which several possible outputs are
allowed are the p-subsequential and semideterministic models.
Learning
Subsequential transducers can be learned from a sample [Oncina et al. 1993] or
translation queries [Vilar 1996].
The transducer deﬁnes a distribution over bi-languages. One can note that the pair (ab, 110) will
be generated with probability 0.0036 = 0.3 . 0.3 . 0.2 . 0.2. More complex is the case of the pair
(aa, 111), which can be generated in two different ways. The probability of generating this pair is
then 0.0135 = 0.3 . 0.3 . 0.3 . 0.3 + 0.3 . 0.3 . 0.5 . 0.4 . 0.3.
More formally, let x ∈⋆and y ∈⋆. Let T (x, y) be the set of all paths accepting (x, y): a
path is a sequence π = qi0(x1, y1)qi1(x2, y2) . . . (xn, yn)qin where x = x1 . . . xn and y = y1 . . . yn,
with ∀j ∈[n], xj ∈ ∪{λ} and yj ∈⋆, and ∀j ∈[n], ∃pij such that (qij−1, xj, yj, qij , pij) ∈E.
The probability of the path is

2.3 Grammar Formalisms
41
q1 : 0.2
q2 : 0.3
q3 : 0
a :: 1, 0.3
b :: 00, 0.2
λ :: 1, 0.3
b :: 0, 0.2
λ :: 1, 0.5
a :: λ, 0.4
λ :: 0, 0.6
FIGURE 2.11: A probabilistic transducer.
q1
q2
q3
a :: 1, 0.2
a :: 1, 0.4
b :: 00,   0.3
# :: λ,   0.2
1,   0.4
# ::   0,   0.1
01,   0.2
a :: 0,   0.3
 1,   0.4
b ::   0,  0.3
11,  0.2
FIGURE 2.12: A probabilistic semideterministic transducer.
IP(qi0) . 
j∈[n]
pij . FP(qin).
And the probability of the translation pair (x, y) is obtained by summing over all the paths in
T (x, y). The probability of y given x (the probability of y as a translation of x, denoted as
PrT (y|x)) is
PrT (x,y)

z∈⋆PrT (x,z).
The problem of ﬁnding the optimal translation is called optimal decoding: it is N P-hard
[Casacuberta and de la Higuera 2000]. Recent work provides techniques allowing to compute this
string in many cases [de la Higuera and Oncina 2013]. Semideterministic transducers can also be
adapted in order to include probabilities and deﬁne distributions over bi-languages, as in Figure 2.12.
Table 2.7 summarizes the parsing, modeling, and learning criteria for probabilistic transducers.
2.3.4
MORE COMPLEX FORMALISMS
Finite-state machines can only model certain languages. The different models we have surveyed can
be rendered more complex in a number of ways.

42
2. FORMAL LEARNING
TABLE 2.7: Probabilistic transducers
Criterion
Comment
Parsing
There are several parsing problems related to probabilistic transducers:
.
Computing Pr(x, y) can be done by adapting the forward algorithm.
.
The stochastic translation problem of a source sentence is: given input string x, ﬁnd
a target string y that maximizes Pr(y | x) or Pr(x, y). In other words, we are
looking for the most probable translation. This is actually a complex intractable
problem [Casacuberta and de la Higuera 2000] for which a number of heuristics
exist, and an efﬁcient parameterized algorithm has been designed [de la Higuera
and Oncina 2014].
Modeling
The expressiveness of probabilistic transducers depends on the amount of
determinism allowed. There are some rich extensions allowing to deﬁne
distributions: negative and even complex weights have been proposed.
Learning
There are few positive formal results concerning learning pst. In the identiﬁcation
in the limit line, Akram et al. [2012] learn the deterministic ones in an active
setting and Akram and de la Higuera [2012] in a batch setting. In a pac
learning setting, recent results have been obtained by Balle et al. [2014a].
.
General context-free grammars correspond to the second step of the Chomsky Hierarchy.
They model languages which can also be recognized by push-down automata.
.
Probabilistic context-free grammars are the probabilistic version of the above.
.
Bi-grammars are context-free extensions of the transducers: the bi-languages are built by using
context-free like rewriting rules.
.
Whereas all these models deal with strings, there is in many cases a natural albeit technical
extension to trees: tree automata, tree grammars, tree transducers, etc. In certain cases, graph
languages can also be deﬁned.
Context-Free Grammars
A context-free grammar is used to generate strings. It can be used for parsing by algorithms running
in O(m3) time, where m is the length of the string. The two better known algorithms for doing this
are the Earley [1970] and the cyk [Younger 1967] algorithms.
It is often a sound policy to normalize the context-free grammars: when in Chomsky (or
quadratic) normal form, the right hand of rules is of length at most 2; when in Greibach normal
form, the right hands start with a terminal symbol. Grammatical inference specialists should be
aware that when learning a normal form, the actual structure of the strings changes. If what matters

2.3 Grammar Formalisms
43
the
ball
Det
N
NP
hit
John
V
VP
NP
S
FIGURE 2.13: Parse tree for John hit the ball.
is only the language, this is not an issue. But if one is also interested in why a string belongs to
the language, then one will need the derivation or parse tree which is grammar dependent and not
language dependent.
One curious grammatical inference example illustrates this point: Sakakibara [1990] proves
that reversible grammars are learnable from skeletons; skeletons are parse trees with no labels on the
internal nodes of the tree. Furthermore, he shows that any context-free language admits a reversible
normal form (even if the construction can be exponential). It would therefore seem natural to claim
that context-free languages are learnable from skeletons. If so, this would represent great news as
skeletons can easily be built from treebanks by just removing the labels of the internal nodes! The
answer is nevertheless negative, as the sort of skeletons we would need to be able to use Sakakibara’s
result are not those that appear naturally when analyzing natural language. So the language inferred
by this technique would be very far away from the natural language we would expect.
Example2.3
The following grammar generates well-structured bracketed languages: ⟨{N1}, {a, b},
N1, R⟩with R = {N1 →aN1bN1; N1 →λ}.
A typical parse tree for a context-free grammar that may be used for English is represented
in Figure 2.13.
Linear grammars are context-free grammars in which right-hand sides of rules contain at
most one non-terminal. The good news is that parsing strings using linear grammars is in O(n2)
time. The bad news is that there is no real advantage, as far as learnability is concerned, in using
linear grammars: as discussed in Section 2.1.1, the equivalence problem remains undecidable, which
is a barrier for learning. Nevertheless, there are some cases where learning is possible: even linear
grammars [Takada 1988] and deterministic linear grammars [de la Higuera and Oncina 2002] have
been shown to be learnable.

44
2. FORMAL LEARNING
a
S
S
a
S
b
a
c
c
c
S
FIGURE 2.14: Parse tree for the sentence aabaccc for a linear grammar with rules S →aS|bS|aSc|acc.
Example 2.4
In Figure 2.14 is an example of a linear grammar and of a tree.
Even linear grammars are those linear grammars where, on the right-hand side of the rules,
the unique non-terminal symbol, if present, has, to its left and to its right, an identical number of
terminal symbols. If such grammars are used, it is easy to ﬁnd the center of a string and therefore to
produce a skeleton. Therefore, learning such grammars is as difﬁcult as learning ﬁnite-state machines.
Example 2.5
⟨{N1}, {a, b}, N1, R⟩with R = {N1 →aN1a|bN1b|a|b|λ} is a linear grammar which
generates palindromes. Furthermore, this grammar is even linear. Figure 2.15 shows a parse tree for
this grammar.
Deterministic linear grammars were shown to be learnable by de la Higuera and Oncina
[2002]. A probabilistic version is studied by de la Higuera and Oncina [2003]. In such grammars
there is exactly one terminal symbol before the non-terminal in the right-hand side of the rules and
there is a deterministic rule to be followed.
Example 2.6
The following is a deterministic linear grammar:
⟨{N1, N2}, {a, b}, N1, R⟩with R = {N1 →aN1ab|bN2; N2 →aN1a|b}.
Table 2.8 summarizes the parsing, modeling, and learning criteria for cfgs.
Probabilistic Context-Free Grammars
Deﬁnition2.4
A probabilistic context-free grammar (pcfg) G is a quintuple ⟨V , , N, R, P ⟩where
V is a ﬁnite alphabet (of variables or non-terminals),  is a ﬁnite alphabet (of terminal symbols), N
(∈V ) is the start symbol, R ⊂V × (V ∪)∗is a ﬁnite set of production rules, and P : R →R+ is
the probability function. Furthermore, (1) ∀r ∈R, 0 < P (r) ≤1 and (2) ∀A ∈N,  (A, α) ∈R :
P(A, α) = 1.

2.3 Grammar Formalisms
45
a
N1
N1
b
N1
a
b
b
N1
a
a
FIGURE 2.15: Parse tree for an even linear grammar.
TABLE 2.8: Context-free grammars
Criterion
Comment
Parsing
cyk and the Early algorithm are two parsing algorithms, both with cubic
complexity.
Modeling
Context-free grammars allow for the representing of patterns which are not
regular: palindromes and brackets, for instance.
Learning
Sakakibara [1990] has proposed different learning results from bracketed data. In
unsupervised learning, there are no efﬁcient algorithms, one exception being
an active learning result [Clark 2010a]. But a number of alternative approaches
have been studied. Some of these are presented in Chapter 4.
A pcfg is used to generate strings by rewriting iteratively the non-terminals in the string,
beginning with the start symbol. A string may be obtained by different derivations. In this case the
problem is called ambiguity. Parsing with a pcfg is usually done by transforming the pcfg into one
equivalent in quadratic normal form and adapting the Earley or the cyk algorithms.
Table 2.9 summarizes the parsing, modeling, and learning criteria for pcfgs.
Bigrammars
Translation tasks requiring rules that cannot be described through ﬁnite-state machines mechanisms
can make use of a formalism associating context-free grammars and transducers. A synchronous
grammar (or synchronous phrase structure grammar) is made of a set of rules of the form T →
input ; output where T is a non-terminal, input is a string over non-terminals and labeled terminal

46
2. FORMAL LEARNING
TABLE 2.9: Probabilistic context-free grammars
Criterion
Comment
Parsing
cyk and the Early algorithm are two parsing algorithms which can be adapted to
work with probabilistic context-free grammars. A very efﬁcient extension of
the Early algorithm due to Stolcke [1995], which can compute:
.
the probability of a given string x generated by a pcfg G;
.
the single most probable parse for x;
.
the probability that x occurs as a preﬁx of some string generated by G.
Modeling
Context-free grammars allow us to represent patterns which are not regular:
palindromes and brackets, for instance.
Learning
There are two issues when contemplating learning pcfgs. Unsupervised learning
is the task consisting of learning these from just strings. Alternatives are to
start with very general grammars and attempt to estimate the parameters.
This can be done with the inside-outside algorithm [Lari and Young 1990].
Bayesian methods will rely on priors: a knowledge of some characteristics
concerning the distribution which will help the algorithm to converge.
Algorithm comino produces interesting results [Scicluna and de la Higuera
2014b]. The supervised task is simpler, as it consists of learning from the
treebank.
symbols from the input alphabet, and output is a string over non-terminals and labeled terminal
symbols from the output alphabet.
A typical rule might be
NP →el libro grande ; the big book
NP →el NOUN 1 ADJ 2 ; the ADJ 2 NOUN 1.
Notice that only a one-to-one mapping is allowed. Parsing with such machines can be complex. A
natural extension consist in adding probabilities to the rules [Koehn 2010].
2.3.5
DEALING WITH TREES AND GRAPHS
Strings and sequences represent the ﬁrst level of structured information. In a number of applications
much more information (and of a much richer nature) can be represented through trees or even
graphs. The learning problems will obviously be harder, but the beneﬁts will be higher.
The theories of tree automata and graph grammars are out of the scope of this book. However,
here we name just a few:

2.4 Is Grammatical Inference an Instance of Machine Learning?
47
a
•
•
a
•
a
b
b
•
a
•
a
•
b
a
FIGURE 2.16: Skeleton corresponding to the bracketed string (a(a(a(b)a)b)(a(a)b)).
.
If one chooses only to bracket a sentence, the result may be represented as a skeleton. A
skeleton is a tree in which the internal nodes are unlabeled. See Example 2.7 for the simple
idea.
.
Trees will usually be ordered, and the internal nodes of a tree will be marked by valuable
labels. This is the case of the trees one can ﬁnd produced by a parser for natural language.
Tree grammars and automata have been thoroughly studied in the past [Comon et al. 1997].
.
Graphs can be directed or not. Graphs will possibly represent complex dependencies between
the constituents of a sentence. Graph grammars have been studied in detail [Courcelle and
Engelfriet 2012]. When asked to be learnable, the grammars have to be simpliﬁed [Oates
et al. 2002, 2003].
Example 2.7
An example of a skeleton is depicted in Figure 2.16.
2.4
IS GRAMMATICAL INFERENCE AN INSTANCE OF
MACHINE LEARNING?
As grammatical inference deals with learning automata and grammars, the reader familiar with
machine learning will be interested in relating some key machine learning concepts with grammatical
inference. Interestingly, both natural language processing and machine learning were discussed by
Turing [1950] in his work about machine intelligence: he worked on both and partially reported some
of his ﬁndings.
Nowadays, machine learning is a well-established ﬁeld of research, with its journals, con-
ferences, companies, and teams. It also has a theory of its own which is studied in universities
throughout the world. This theory is based on some profound statistically inspired work by Vapnik
and Chervonenkis [1971] and some more combinatorially inspired work by Valiant [1984].

48
2. FORMAL LEARNING
Vapnik’s approach is to characterize the learning problem as one of studying convergence
issues regarding the empirical risk. The empirical risk measures the errors made when ﬁnding a
hypothesis explaining the examples. There are then two sources of errors: a ﬁrst one concerns the
fact that the optimization process may not return the real optimum, and the second concerns the
fact that the set of hypotheses may not be able to capture exactly what has to be learned. Vapnik
studies how the generalization risk (measuring the errors to be made on unseen data) will converge
to the empirical risk.
In Valiant’s framework (see Section 2.2.6), we suppose that what is to be learned does belong
to the hypothesis space. We will then ask if it is possible to explore efﬁciently this space and return
a consistent hypothesis in this space.
Since grammatical inference is seen by many as a particular form of machine learning, one
may believe that it is sufﬁcient to adapt the above well-studied theories to the setting of grammatical
inference. In general, this approach does not work well; here are some simple reasons for this.
.
In classical machine learning, a good measure of the hypotheses class is the V C-dimension
(Vapnik–Chervonenkis). The V C-dimension of the class expresses how easy it is to ﬁnd
a hypothesis that will match a given set of examples and counter-examples. A small V C-
dimension will usually mean that the class of hypotheses is poor, which in turn means that a
good hypothesis inside the class is going to be hard to better. A high V C-dimension means
that the class is rich and that ﬁnding a consistent hypothesis is always possible: this encourages
overﬁtting and will usually result in hardness results, because of the high variance.
Mathematically, the V C-dimension is the size of the largest set which can be shattered by
the hypothesis class; a set X = {x1, x2, . . . , xn} is shattered by a class H if given any partition
of X into X1 and X2 there is a hypothesis h in H which classiﬁes all elements of X1 as 1, and
all those of X2 as 0.
But the V C-dimension for typical classes of automata or grammars is inﬁnite. This is
easy to see: if we are given any ﬁnite language, in most formalisms it is possible to build
an inﬁnity of automata or grammars which recognize/generate exactly the strings from the
ﬁnite language. A usual way around this is to restrict the class by indexing it by the size of the
grammars it contains. The new question is then: What is the V C-dimension for the automata
with at most n states, or for the grammars with at most n rules? Here again, the results are
disappointing, with V C-dimensions of O(n log n) for dfa and O(n2) for nfa: such results
are inconclusive and do not allow one to derive the sort of bounds for which one would hope.
.
Traditional computational learning tools will prove that ﬁnding consistent dfa is a hard
problem, as hard as a number of cryptographic problems [Kearns and Vazirani 1994]. This
in turn results in not allowing for positive pac learning results.

2.5 Summary
49
2.5
SUMMARY
To conclude this chapter, let us ask ourselves how theoretical results can help.
The ﬁrst thing to understand is that a ﬁrst decision people attempting to build a learning
system have to make is what sort of grammar they are thinking of learning. This is going to constitute
the learning bias. Indeed, the learner is not just trying to learn from data: it has to decide what biased
solution it is looking for. More generally, the bias-variance trade-off issue (or the related one of no
free lunch) is very present in grammatical inference and is an issue to be taken seriously!
This may seem frustrating. After all, why not let the learning system ﬁnd an unbiased
grammar? There are many works in learning theory showing that this naive approach is doomed.
It is always possible to ﬁnd a very complex grammar consistent with the data, provided this data is
non-contradictory.
And the choice of grammars will depend on our understanding of what we are looking for, of
the sort of essential rules governing the structure of sentences or words. But also on our understanding
of how grammars model languages, of how easy they are to learn.
The second question we will want to raise is that of the learning paradigm. In the real learning
situation we are to face, the way we receive the data, the way it is generated, and the properties of this
matter will all be elements important to analyze. The theoretical tools from grammatical inference
allow us to do that.
Obviously, the fact that our learning algorithm has nice positive convergence properties will
not ensure that learning will be possible when facing a speciﬁc learning situation. But curiously, there
still is an advantage of having some theorem telling us that the algorithms the learner is using can
learn or identify some unknown targets, although not all. Consider the signiﬁcance of the assessment
“this class is not learnable.” If a class is not learnable, this means that somewhere in the class there
are some targets which are not learnable. It means that if what we were hoping to learn was one of
those targets, then we should forget it or rely on luck. Going further, this signiﬁes that if we divide
the class of grammars under scrutiny into the subclass of the ones the algorithm can learn and the
subclass of those the algorithm cannot learn, there is in fact a hidden bias: the real class our algorithm
is learning is probably quite different from the class we intend to learn from.
In other words, we say that we are using bias A but are really using bias B. The worse part is
that usually we cannot know what B is!

51
C H A P T E R 3
Learning Regular Languages
3.1
INTRODUCTION
This chapter primarily examines how regular patterns can be learned from positive data. It also
emphasizes an approach to learning by selecting the bias carefully. This is because many of the
important, practical grammatical inference techniques were developed in this way. It is also because
many of the insights obtained here can be, have been, and continue to be fruitfully applied to non-
regular classes. And so there is every reason to believe that the lessons here are valuable in ongoing
research on language learning.
The ﬁrst section of this chapter explains why appropriately selected bias is a valuable way to
attend to learning problems in computational linguistics. As explained there, it is not the only way,
and others have and continue to be pursued.
The main learning technique discussed in this chapter is state-merging. State-merging will
be introduced in terms of learning regular sets (i.e., regular languages), although we will also see that
state-merging is used to learn regular relations and probabilistic regular languages as well. Regular
sets are used to exemplify the algorithms because they are simpler and more well studied.
State-merging itself is introduced with respect to an example problem in the acquisition
of phonology. The concrete example is intended to help exposition. The state-merging theorem
(Theorem 3.2), which establishes the soundness of the method, is also presented.
This chapter then discusses rpni, an algorithm which efﬁciently learns any regular language
from positive and negative data in the sense discussed in the previous chapter. rpni is included
because it reinforces the utility of the state-merging theorem (Theorem 3.2) and the importance of
canonical forms as learning targets, and because the idea behind it underlies successful algorithms,
which learn classes stochastic languages and regular transductions.
These other algorithms for learning regular relations and stochastic regular languages are also
discussed, although in less detail. The chapter concludes with suggestions for further reading.

52
3. LEARNING REGULAR LANGUAGES
3.2
BIAS SELECTION REDUCES THE PROBLEM SPACE
Natural language patterns fall across different regions of the Chomsky Hierarchy. When trying to
understand how such patterns could be learned, there are generally two different strategies that have
been adopted.
One strategy is to deﬁne learning so that increasingly larger regions can be learned. An
important idea in this line of work is that different learning frameworks may better characterize
the data presentations learners actually get. For example, in the framework identiﬁcation in the limit
from positive data, the class of data presentations with which learners must succeed has been criticized
as being too broad, antagonistic, and unrealistic [Clark and Lappin 2011]. Frameworks in which
learners are able to learn large regions of the Chomsky Hierarchy succeed in no small part because
they limit the data presentations with which learners must succeed in signiﬁcant ways (e.g., to
classes of computable data presentations) [Gold 1967, Horning 1969, Angluin 1988a]; see also
the discussion in Heinz [2015]).
Another strategy is to identify learnable regions which cross-cut the Chomsky Hierarchy.
The idea here is that important properties of natural language are overlooked by the major regions
of the Chomsky Hierarchy, and by restricting the class of languages to be learned, the additional
knowledge that comes with this target class can be harnessed to solve the learning problem. In the
framework identiﬁcation in the limit from positive data, this means the target class will have to exclude
some ﬁnite languages [Gold 1967] and a deﬁning property of such classes is provided by Angluin
[1980]. Many examples of such classes have been studied in the grammatical inference literature.
Both these two strategies have a common theme at their core. The common theme is this:
hard problems are easier to solve with better characterizations. This is because the instance space of
the problem has been reduced in a meaningful way.
A simple example illustrates this general point. The Hamiltonian path problem is the problem
of ﬁnding a path in an undirected graph which visits each node vertex in the graph exactly once.
This problem is known to be NP-complete [Garey and Johnson 1979]. However, if the graphs
are restricted to linear sequences (like stations along a single rail line), the problem has a trivial
solution. Note that in both the original and restricted versions of this problem, the instance space
of the problem is countably inﬁnite. But the restriction makes the problem solvable. As anticipated
by Gold [1967], research in grammatical inference has shown that meaningful restrictions of either
the class of data presentations or the class of languages that learners are required to succeed on can
make the learning problem solvable.
This chapter focuses on state-merging, which exempliﬁes a sound way bias selection (the
second strategy) can be instantiated in algorithms for learning regular languages.

3.3 Regular Grammars
53
3.3
REGULAR GRAMMARS
In this chapter, there are three kinds of patterns to be discussed: regular sets, regular relations, and
regular distributions over sets or relations (i.e., regular stochastic sets or relations). Although there
are many ways to deﬁne grammars for each of these types of patterns, they are deﬁned here in
terms of ﬁnite-state automata. One reason for this is that many kinds of ﬁnite-state automata admit
canonical forms. Canonical forms are advantageous because they typically directly reﬂect invariant
mathematical properties of the patterns they describe. Particular canonical forms for automata will be
introduced shortly.1 Another reason for using ﬁnite-state automata is that one of the main techniques
for inferring regular grammars relies on the concept of merging the states of these automata; this
technique is called state-merging.
Deterministic ﬁnite-state acceptors (dfa) were deﬁned in Section 1.6. Non-deterministic
ﬁnite-state acceptors (nfa) were introduced in Section 2.3.1. They are deﬁned here both for
completeness and so that we can precisely state important theorems later.
For any set of S, let P(S) denote the powerset of S (the set of all subsets of S).
Deﬁnition 3.1 (Non-deterministic ﬁnite-state acceptor (nfa))
A non-deterministic ﬁnite-state
acceptor is a 5-tuple ⟨, Q, I , F , δ⟩for which
.
 is the ﬁnite set of input symbols, corresponding to the vocabulary;
.
Q is a ﬁnite set of states;
.
I is the ﬁnite set of initial states (I ⊆Q);
.
F is the ﬁnite set of ﬁnal states (F ⊆Q); and
.
δ : Q ×  →P(Q) is the (total) transition function; given a state q ∈Q and input symbol
i ∈, δ(q, i) returns a set of states Q′ ⊆Q.
The transition function is extended recursively so that its domain is P(Q) × ∗. Then the language
generated, recognized, or accepted by a ﬁnite-state acceptor A is
L(A) = {w ∈∗| δ(I , w) ∩F ̸= ∅}.
There are two important facts about the class of nfa. First, it properly includes the class of dfa.
Thus deterministic acceptors are a special type of nfa. Second, the family of languages describable
with nfa is exactly the same as the family of languages describable with dfa: it is the class of regular
languages.
1. In contrast, there are no (computable in polynomial time) canonical (e.g., shortest) regular expressions for regular sets.

54
3. LEARNING REGULAR LANGUAGES
1
0
σ
 σ ´
FIGURE 3.1: A ﬁnite-state acceptor which recognizes the language ´σσ ∗.
We now wish to introduce two canonical forms of regular languages by way of an example.
Consider  ={´σ, `σ, σ}. These symbols denote strongly stressed, weakly stressed, and unstressed
syllables, respectively. Following the linguistic observation that words can have many weakly stressed
syllables but (usually) at most one strongly stressed syllable, we will refer to the weakly stressed
syllables as secondary stressed and the strongly stressed syllables as primary stressed syllables. Now
consider the ﬁnite-state acceptor deﬁned pictorially in Figure 3.1. The states are Q = {0, 1}; the
initial state {0}; the ﬁnal states {1}; and δ = {(0, ´σ) →{1}, (1,σ) →{1}}. Consequently, this acceptor
recognizes the formal language containing all and only those strings which begin with the symbol
´σ and is then followed by zero or more σ symbols. In other words, this formal language represents
a linguistic pattern in which the initial syllables bear the primary (strongest) stress and the other
syllables are unstressed.
For every regular language L, there are inﬁnitely many ﬁnite-state acceptors which recognize
L. However, there is a particular acceptor for L, which is often called the canonical form. This
acceptor is the smallest deterministic acceptor recognizing L.2 An nfa is deterministic provided
|I| = 1 and for all states q ∈Q and symbols σ ∈, it is the case that |δ(q, σ)| ≤1 (so there is at
most one state reachable from q on reading σ).
An important fact about the canonical form of a regular language is that the states are
intimately related to algebraic properties of the language. To explain, it is important to understand
that, for every regular language L, every string w ∈∗can be associated with a residual stringset,
also called the set of good tails. The good tails (or residual) of w with respect to L are all strings v such
that wv ∈L. The good tails are all the ways in which w can be continued so that the resulting string
belongs to L. Formally, TailsL(w) = {v | wv ∈L}. Consider the language of the nfa in Figure 3.1,
which would be written with a regular expression as ´σσ ∗. The good tails of ´σσ with respect to this
language is the set indicated by the regular expression σ ∗. On the other hand, the good tails of σ ´σ
with respect to this language is empty.
2. There are other canonical representations of regular languages, including the syntactic monoid [McNaughton and Papert
1971] and the universal automaton [Lombardy and Sakarovitch 2008].

3.3 Regular Grammars
55
Nerode and Myhill considered an equivalence relation over ∗induced by L: two strings
w1 and w2 are L-equivalent if and only if they have the same set of good tails with respect to L.
Continuing the example above, it is not difﬁcult to verify that the strings ´σσ, ´σσσ, and ´σ are all
tail-equivalent with respect to the language ´σσ ∗.
Nerode and Myhill proved that the L-tail-equivalence relation partitions ∗into ﬁnitely many
blocks if and only if L is a regular language. An important result in their proof is that the states of
the smallest deterministic acceptor recognizing L represent the blocks of this equivalence relation.
In other words, for every canonical acceptor for a regular language, for every state q in this acceptor,
every string which leads to q has exactly the same set of good tails. For this reason, the smallest dfa
is also called the tail canonical acceptor for a regular language L.
In a completely symmetric fashion, one can deﬁne the sufﬁxes of a string, the heads of a string
with respect to a language, and a head-equivalence relation.3 The Myhill–Nerode theorem is easily
adapted to this other construction: the regular languages are exactly those for which the L-head-
equivalence relation partitions ∗into ﬁnitely many blocks. The head canonical acceptor is in fact the
smallest reverse deterministic acceptor for a regular language L. An acceptor is reverse deterministic
provided it is deterministic if its reverse acceptor is deterministic. (The reverse nfa switches the
start states with ﬁnal states, and points the transitions in the other direction. Formally, for an nfa
A = ⟨, Q, I , F , δ⟩, the reverse of A is Ar = ⟨, Q, F , I , δr⟩where δr(q, i) = {q′ | q ∈δ(q′, i)}.)
The head canonical acceptor and tail canonical acceptor have different structures, which reﬂect
their right-to-left and left-to-right orientations, respectively. While the left-to-right orientation
may appear more natural for production (since time moves “left to right”), there is a reason to think
accessing strings right-to-left plays a role in cognition. If strings are stored in memory in a ﬁrst-
in/last-out fashion (like plates stacked on one of those cafeteria-style spring-based storage systems)
then when accessing the string from memory, it will be read right-to-left.
These results for tail and head canonical acceptors are important for learning because it
provides a way to distinguish or not distinguish the underlying states based on information present in
the strings. If there is reason to believe that two observed preﬁxes (sufﬁxes) w1 and w2 of a language
have the same set of tails (heads), then those two preﬁxes (sufﬁxes) will lead to the same state in
the tail (head) canonical acceptor. On the other hand, if there is reason to believe w1 and w2 do not
have the same set of tails (heads) then they should lead to different states in the tail (head) canonical
acceptor. As we will see below, there can be reasons why two observed preﬁxes (sufﬁxes) have the
3. Formally, v is a sufﬁx of w iff there exists u ∈∗such w = uv. The good heads of v with respect to L are all strings u
such that uv ∈L. Two strings v1 and v2 are L-head-equivalent if and only if they have the same set of good heads with
respect to L.

56
3. LEARNING REGULAR LANGUAGES
same set of tails (heads) or not. This connection is made more explicit in Sections 3.5 and 3.6, below,
after its mechanics are introduced.
3.4
STATE-MERGING ALGORITHMS
State-merging is a technique which refers to a class of algorithms. It will be emphasized that one
way different state-merging algorithms can be obtained is by altering the criteria for deciding which
states should be merged.
State-merging is a method of writing smaller and smaller ﬁnite-state descriptions of observed
strings while keeping some property invariant. The general scheme of learners of this type follow a
two-step procedure.
1. A ﬁnite-state representation of the input.
2. Merge states that are equivalent (in some predetermined sense).
Which ﬁnite-state representation of the input is used and how it is decided which states
to merge in this structure are the two key questions involved when developing a state-merging
algorithm. These decisions determine everything: the kinds of generalizations that are made, and
ultimately the kinds of patterns which can be learned.
Below we ﬁrst explain the process of merging states in a ﬁnite-state acceptor—what it is and
how it works. Then we explain how the input to the learning algorithm can be represented as a
ﬁnite-state acceptor. We conclude with the state-merging theorem, which establishes the soundness
of this approach to learning to regular languages. The theorem is possible partly from the fact that
there are canonical representations of regular languages. Along the way, we illustrate these ideas with
examples, drawing in particular on a problem children face when learning the phonology of their
native language.
3.4.1
THE PROBLEM OF LEARNING STRESS PATTERNS
Before continuing further, let us illustrate one problem in phonological acquisition which well help
along the exposition regarding state-merging: the problem of learning the stress pattern of one’s
native language (if one exists) from syllabic representations of words.
Many languages have stress patterns. For example, consider the words in Pintupi shown in
Table 3.1 [Hansen and Hansen 1969, p. 163]. If we abstract to the level of syllables, the pattern
stands out more clearly, as shown in Table 3.2. Hayes [1995, p. 62] described the stress pattern of
Pintupi as follows:
1. primary stress falls on the initial syllable, and
2. secondary stress falls on alternating non-ﬁnal syllables.

3.4 State-Merging Algorithms
57
TABLE 3.1: Pintupi words
a.
p´aŋa
“earth”
b.
tj´uaya
“many”
c.
m´aaw`ana
“through from behind”
d.
p´uiŋk`alatju
“we (sat) on the hill”
e.
tj´amul`ımpatj`uŋku
“our relation”
f.
´ıir`iŋul`ampatju
“the ﬁre for our beneﬁt ﬂared up”
g.
k´uranj`ulul`ımpatj`ua
“the ﬁrst one who is our relation”
h.
y´uma`ıŋkam`aratj`uaka
“because of mother-in-law”
TABLE 3.2: Pintupi words with a
syllabic representation
a. ´σσ
e. ´σσ `σσ `σσ
b. ´σσσ
f. ´σσ `σσ `σσσ
c. ´σσ `σσ
g. ´σσ `σσ `σσ `σσ
d. ´σσ `σσσ
h. ´σσ `σσ `σσ `σσσ
q1
q2
q3
q0
q4
σ
σ
σ
 σ ´
 σ`
FIGURE 3.2: A minimal, deterministic, ﬁnite-state acceptor for Pintupi stress.
These generalizations can be encoded as a ﬁnite-state acceptor, as shown in Figure 3.2.
Here is a different example of a stress pattern found in the world’s languages: the unbounded
stress pattern in Kwakwala [Walker 2000]:
1. Primary stress falls on the left-most heavy syllable in a word, and if there are no heavy syllables,
it falls on the ﬁnal syllable.

58
3. LEARNING REGULAR LANGUAGES
TABLE 3.3: All LHOR words up to four syllables in length
´H
´L
´H L
´H H
L ´H
L ´L
´H L L
´H L H
´H H L
´H H H
L ´H L
L ´H H
L L ´L
L L ´H
L ´H L L
L ´H L H
´H L L L
´H L L H
´H H L L
´H H L H
L ´H H L
L ´H H H
´H L H L
´H L H H
´H H H L
´H H H H
L L ´H L
L L ´H H
L L L ´L
L L L ´H
Many languages, like Kwakwala, distinguish between “light” and “heavy” syllables. The “weight” of
a syllable can be determined by vowel length, presence of a coda, and potentially many other factors
[Gordon 2006].
Following Hayes [1995], we refer to this pattern as the “Leftmost Heavy Otherwise Right-
most” (LHOR) pattern. According to this generalization, in words with syllable proﬁles LLH,
LLHL, and LLHLH, the primary stress will always fall on the third syllable because that is the left-
most heavy syllable in each word.4 Table 3.3 shows all words up to four syllables in length which
exemplify this pattern. This pattern is unbounded because the primary stress could fall arbitrarily
far from either word edge. For example, in words with the syllable proﬁle LLLHLLH, stress is
predicted to fall on the fourth syllable. On the other hand, according to the rule, a word with only
light syllables will have stress fall on the ﬁnal syllable of the word (as in LLLL ´L).
Just as with the Pintupi, it is important to realize that the generalizations above apply equally
well to longer words, even if no such words of that length exist in the lexicon (or are constructible by
word formation rules). The words in Table 3.3 are just among the shortest words drawn from this
set. Letting  ={H, ´H, L, ´L}, the ﬁnite-state acceptor in Figure 3.3 describes this inﬁnite set, and
thus captures the linguistic generalization faithfully.
More generally, the linguistic generalizations that phonologists make when describing the
dominant stress patterns in languages can be thought of as inﬁnite sets. Our interest in the nature of
these phonological generalizations leads us to examine the nature of these mathematical objects—the
inﬁnite sets with which these generalizations are identiﬁed.
These examples are introduced in order to concretely establish the nature of the learning
problem. What algorithm can take the ﬁnite sets of data in Tables 3.2 and 3.3 as input and output
the ﬁnite-state acceptors in Figures 3.2 and 3.3, respectively? State-merging algorithms are one
important method that can solve this problem.
4. Hence a transcription with stress marked would read LL ´H, LL ´HL, and LL ´HLH, respectively. It is important not to
confuse syllable weight with stress. Here, L and H indicate unstressed “light” and “heavy” syllables, whereas ´L and ´H will
be used to indicate light and heavy syllables bearing primary stress, respectively.

3.4 State-Merging Algorithms
59
2
1
0
L
LHOR
L, H
H´
L´
FIGURE 3.3: A minimal, deterministic, ﬁnite-state acceptor for Kwakwala stress.
3
0
1
a
Machine A
a
2
a
a
3
0
1–2
Machine B
a
a
FIGURE 3.4: Machine B represents the machine obtained by merging states 1 and 2 in Machine A.
3.4.2
MERGING STATES
When distinct states are merged, they become a single state. A key concept in state-merging is
that transitions are preserved [Angluin 1982, Hopcroft et al. 2001]. This is one way in which
generalizations may occur—because the post-merged machine accepts everything the pre-merged
machine accepts, possibly more.
For example in Figure 3.4, Machine B is the machine obtained by merging states 1 and 2 in
Machine A. It is necessary to preserve the transitions in Machine A in Machine B. In particular,
there must be a transition from state 1 to state 2 in Machine B. There is such a transition, but because
states 1 and 2 are the same state in Machine B, the transition is now a loop. Whereas Machine A
only accepts one word aaa, Machine B accepts an inﬁnite number of words aa, aaa, aaaa, . . . .
Some observations regarding the example in Figure 3.4 are in order. First, the post-merged
machine may not be deterministic. Second, the merging process does not specify which states should
be merged. It only speciﬁes a mechanism for determining a new machine once it has been decided
which states are to be merged. Thus, the choice of which states are to be merged determines the
kinds of generalizations that occur. A merging strategy is thus a generalization strategy.
Also, observe that once the equivalence of states is determined, this effectively partitions the
states of the acceptor into different regions, or blocks. It follows from the deﬁnition below that the
order in which the states in these regions are merged is inconsequential.

60
3. LEARNING REGULAR LANGUAGES
Formally, let A = ⟨, Q, I , F , δ⟩be any nfa. Consider any partition π of Q, and let B(q, π)
refer to the set of states in the same block of the partition as state q. Then merging states in the same
blocks of A according to π yields another acceptor A/π = ⟨′, Q′, I ′, F ′, δ′⟩deﬁned as follows:
′ = 
Q′ = {B : B(q, π) such that q ∈Q}
I ′ = {B : B(q, π) such that q ∈I}
F ′ = {B : B(q, π) such that q ∈F}
δ′(B0(q0, π), a) = {B1(q1, π) : q1 ∈δ(q0, a)}.
A/π is sometimes called the quotient of A and π. Notice that any block containing at least one
ﬁnal (initial) state is itself a ﬁnal (initial) state in the new machine. Similarly, if there is at least one
transition labeled a from any state in block B0 to another state in block B1 then in the new machine
there is a transition from B0 to B1 labeled a.
The reason state-merging can result in generalization follows from the directly from the
following theorem whose origin is unknown. A proof is given in Heinz [2007].
Theorem 3.1
Let A be any acceptor and π any partition of Q. Then L(A) ⊆L(A/π).
Thus, according to this theorem, any word accepted by the pre-merged machine will also be accepted
by the post-merged machine. The language generated by post-merged machine is necessarily a
superset of the pre-merged machine, and this superset language may be inﬁnite in size. Thus, in
a very direct way, state-merging shows how it is possible to obtain a grammar which represents a
linguistic generalization corresponding to an inﬁnite set from a ﬁnite input sample. So this theorem
indicates that state-merging is fully capable of modeling such a language learning process. Let us
now turn to how the ﬁnite input to these algorithms is represented.
3.4.3
FINITE-STATE REPRESENTATIONS OF FINITE SAMPLES
Preﬁx Trees
A preﬁx tree acceptor (pta) is a structured, ﬁnite-state representation of a ﬁnite sample. The idea is
that each state in the tree corresponds to a unique preﬁx in the sample. Here the word “preﬁx” is not
used in its morphological sense, but in its mathematical sense.
Formally, a string u is a preﬁx of a string w iff there exists a string v ∈∗such that w = uv. For
every word w, the preﬁxes of w are prefixes(w) = {u | u is a preﬁx of w}. This function’s domain
can be extended to languages in the usual way: prefixes(L) = 
w∈L prefixes(w). For every set
of strings S, we let (S) refer to the alphabet of S.
Now preﬁx trees can be deﬁned.

3.4 State-Merging Algorithms
61
3
4
0
1
2
6
7
5
8
σ
σ
σ
σ
σ
 σ ´
 σ`
 σ`
FIGURE 3.5: A preﬁx tree of Pintupi words.
Deﬁnition 3.2
pta(S) is deﬁned to be the nfa ⟨, Q, I , F , δ⟩such that
 = (S)
Q = prefixes(S)
I = {λ}
F = S
δ(u, a) = ua iff u, ua ∈Q.
An example is shown in Figure 3.5, which shows a preﬁx tree of the syllabic proﬁles of the
eight Pintupi words given in Table 3.2.5
Observe that pta(S) can be computed efﬁciently in the size of the sample S. pta(S) can be
computed batchwise from a sample S, or iteratively. In the latter case, as each word is added, an
existing path in the machine is pursued as far as possible. When no further path exists, a new one
is formed. When a word w is added to a preﬁx tree pta(S), we speak of extending the preﬁx tree
acceptor with w.
Observe further that even in the simple example in Figure 3.5, it is possible to see that there is
structure in the preﬁx tree acceptor, and that this structure repeats itself. State-merging can eliminate
this structural redundancy, resulting in generalization.
Sufﬁx Trees
Preﬁx tree acceptors are not the only way to represent a ﬁnite sample as a ﬁnite-state machine.
Another representation is sufﬁx tree acceptors, which are reverse deterministic representations of
the sample.
5. We have enumerated the names of the states for convenience. Strictly speaking, according to the deﬁnition, state 0 is λ,
1 is ´σ, 2 is ´σσ, 3 is ´σσ `σ, and so on.

62
3. LEARNING REGULAR LANGUAGES
4
3
5
6
9
7
10
13
2
8
12
11
16
1
0
σ
σ
σ
σ
σ
 σ ´
 σ ´
 σ ´
 σ ´
 σ ´
 σ ´
 σ`
 σ`
 σ`
FIGURE 3.6: A sufﬁx tree for Pintupi words.
Formally, a string u is a sufﬁx of a string w ifdef
there exists a string v ∈∗such that
w = vu. The sufﬁxes of words and languages are deﬁned analogously as above and is denoted with
suffixes(.).
Deﬁnition 3.3
sta(S) is deﬁned to be the acceptor ⟨, Q, I , F , δ⟩such that
 = (S)
Q = suffixes(S)
I = S
F = {λ}
δ(au, a) = u iff u, ua ∈Q.
As an example, Figure 3.6 shows a sufﬁx tree for the same eight Pintupi words.
Like preﬁx trees, sufﬁx trees can also be constructed efﬁciently with batch or iterative algo-
rithms.
3.4.4
THE STATE-MERGING THEOREM
It has been proved that if a sample of words generated by some nfa is sufﬁcient—that is, exercises
every transition in this nfa—then there exists some way to merge states in the preﬁx tree to recover
the generating nfa [Angluin 1982]. Although we do not know which states should be merged, we
are guaranteed that there is a way to merge such states to recover the original machine. We know
such a partition exists.
The theorem is given below after some helpful deﬁnitions.
Deﬁnition 3.4
Let A = ⟨, Q, q0, F , δ⟩be a tail canonical acceptor, and let w ∈L(A). Then the
transition set of w are those transitions in δ that make up the path of w through A (recall that for

3.4 State-Merging Algorithms
63
each w ∈L(A), there is a unique path since A is tail canonical). We denote the transition set of w
in A with Trans setA(w).
Deﬁnition3.5
Let A = ⟨, Q, q0, F , δ⟩be a canonical ﬁnite-state acceptor. Then S is a sufﬁcient
sample of A ifdef

w∈S Trans setA(w) = δ and for all qf ∈F, there is a word w ∈S such that
δ(q0, w) = qf.
Pictorially, we can imagine, as A computes the path of some word w, coloring the states and
transitions along this path. If a sample S is sufﬁcient for a canonical acceptor then every state and
transition will be colored after every word in S is processed. Additionally, we can imagine marking
ﬁnal states when we reach the end of the string. Importantly, since Q is ﬁnite, there will be sufﬁcient
samples that only contain ﬁnitely many strings.6
Theorem 3.2
Let A = ⟨, Q, I , F , δ⟩be a tail canonical ﬁnite-state acceptor, S a ﬁnite sufﬁcient
sample of A, and pta(S) = ⟨P T , QP T , IP T , FPT , δPT ⟩. Then there exists a partition π over QPT
such that pta(S)/π is isomorphic to A.
A corollary follows that state-merging over sufﬁx trees is also viable.
Corollary 3.1
Let A = ⟨, Q, I , F , δ⟩be a head canonical ﬁnite-state acceptor, S a ﬁnite sufﬁ-
cient sample of A, and sta(S) = ⟨ST , QST , IST , FST , δST ⟩. Then there exists a partition π over
QST such that sta(S)/π is isomorphic to A.
A proof of the theorem and its corollary can be found in Heinz [2007].
The signiﬁcance of this theorem (and corollary) should not be overlooked. Provided the
learning data D exercises every transition in the target ﬁnite-state grammar, there is a way to merge
states in the preﬁx tree built from D which exactly yields the learning target. Since there are only
ﬁnitely many transitions, only a ﬁnite sample is needed to meet this condition. Thus, the possibility
is raised that—for some subclass of the regular languages—there is a state-merging strategy which
identiﬁes that class in the limit from positive data.
State-merging algorithms therefore can be stated very simply. Given a ﬁnite sample S, a state-
merging algorithm ﬁrst computes either the preﬁx or sufﬁx tree of S, and then computes a partition
π of this tree and ﬁnally computes the quotient of this tree according to the partition π. In other
words, the algorithm returns a machine M equal to the following:
M = T (S)/π,
(3.1)
6. Typically, a shortest transition set can be constructed as follows. For each state q, take the shortest string w that reaches
q from the initial state. Then, to this set add wa for each a ∈.

64
3. LEARNING REGULAR LANGUAGES
where T (S) is either a preﬁx or sufﬁx tree acceptor. In this way, state-merging algorithms are able
to determine the global structure of the ﬁnal dfa through a series of local decisions in the preﬁx
tree. This is possible because the canonical form of the dfa provides a sound rationale for such local
decisions to be made. States are merged if there is reason to believe that distinct preﬁxes have the
same set of tails, according to the Myhill–Nerode relation.
The problem of learning stress patterns can now be restated in this context. How can states
be merged in the preﬁx tree for Pintupi (Figure 3.5) to return an acceptor equivalent to the one in
Figure 3.2? Will the same merging strategy yield the stress pattern of Kwakwala (Figure 3.3) when
given a preﬁx tree acceptor for Kwakwala words?
3.5
STATE-MERGING AS A LEARNING BIAS
Theorem 3.2 establishes a key result: Given any tail canonical acceptor A for any regular language
and a sufﬁcient sample S of words generated by this acceptor, there is some way to merge states in the
preﬁx tree of S which returns the acceptor A. This result does not tell us which states to merge for
a particular acceptor. It just says that there is partition of states whose blocks, once merged, would
yield an acceptor isomorphic to the canonical one. Nonetheless, the result is important because it
leaves open the possibility that there is some property of a class of regular sets we may be interested
in for which there is a successful state-merging strategy.
This section reviews state-merging strategies that have been employed for learning regular sets
and regular relations and the kinds of regular sets and relations that are learnable by those strategies.
One strategy for merging states examines structural properties of the tree acceptors. For
example, two states may be deemed equivalent if they share the same incoming paths of length
2. Formally, this means states in the preﬁx tree pta(S) = ⟨, Q, I , F , δ⟩are merged if they have
the same k-length sufﬁx. For all u, v ∈Q:
u ∼v ifdef ∃x, y, w such that |w| = k, u = xw, v = yw.
(3.2)
This state-merging algorithm then is simply the one shown in Equation 3.3:
G = pta(S)/π∼.
(3.3)
To illustrate the algorithm, consider the preﬁx tree for Pintupi words (Figure 3.5). It is easily
seen that states 4 and 7 share the same incoming path (σσ). States 3 and 6 share `σσ, and 5 and 8
share σ `σ. Merging these states yields the Figure 3.7. The acceptor in Figure 3.7 is not the canonical
acceptor for Pintupi, but it does recognize the same language.
In fact, this algorithm provably identiﬁes in the limit from positive data the Strictly (k + 1)-
Local class of languages [Garc´ıa and Vidal 1990]. Strictly k-Local languages are a subregular class

3.5 State-Merging as a Learning Bias
65
3–6
0
2
4–7
5–8
1
σ
σ
σ
σ
 σ ´
 σ`
 σ`
FIGURE 3.7: The result of merging states in the preﬁx tree for Pintupi words (Figure 3.5) with the same
incoming paths of length 2.
of formal languages which are the non-stochastic counterparts to n-gram models (where n = k + 1).
They have been studied extensively [McNaughton and Papert 1971, Rogers and Pullum 2011,
Rogers et al. 2013].
Edlefsen et al. [2008] study whether 109 distinct stress patterns are Strictly k-Local and if so
for what k. These 109 patterns are taken from typological studies [Bailey 1995, Gordon 2002] and
have been encoded as nfa [Heinz 2009].7 They ﬁnd that only 44 of these patterns are Strictly 2-
Local and 81 are Strictly 5-Local. They ﬁnd that the other 28 are not Strictly k-Local for any k. In
other words, even permitting very generous input samples, only 81 of the patterns can be learned
by merging states with the same incoming paths of length 5, and 28 cannot be learned by merging
states with the same incoming paths of length k, for any k. Is there a state-merging strategy that
works for all 109 distinct stress patterns?
There are other ways to merge states. Generally, two strategies are followed. First, if the current
structure is “ill-formed” then merge states to eliminate sources of ill-formedness. The state-merging
strategy described above is an example of this approach. A preﬁx tree with multiple states with the
same incoming path of length k is “ill-formed” and this “defect” is corrected by merging these states.
Angluin [1982] recursively merged states to eliminate reverse non-determinism and proves that this
procedure identiﬁes the 0-reversible languages (and generalizes the procedure to learn the class of
reversible languages). Muggleton [1990] merged states with the same “contexts” of size k and proves
the learnability of the k-contextual languages. Heinz [2008] merged ﬁnal states and proves this
procedure learns the class of left-to-right iterative languages, which are classes related to the zero-
reversible class. This kind of state-merging has also been studied in the context learning stochastic
regular languages. For example, Stolcke [1994] merged states to maximize posterior probability (for
7. These nfa are available at http://st2.ullet.net.

66
3. LEARNING REGULAR LANGUAGES
hmms), and Carrasco and Oncina [1999] merged states in a preﬁx tree if the residual stochastic
language of each preﬁx with respect to the tree are sufﬁciently similar.
The second merging strategy is to enumerate the states in the preﬁx tree and begin to merge
states in that order unless “ill-formed” structures arise. For instance, rpni merges states unless the
resulting generalization is inconsistent with the negative data (the ﬁnite sample to rpni includes
both positive and negative examples). Similarly, Oncina et al. [1993] proved the learnability of
subsequential transducers by merging states unless “onward subsequentiality” is lost. Clark and
Thollard [2004] presented a learnability result for regular stochastic languages by merging states
unless they are “μ-distinguishable.” These algorithms are discussed in further detail below.
To summarize, state-merging strategies instantiate learning biases. This is because distinctions
maintained in the preﬁx tree (or sufﬁx tree) are lost by state-merging, which results in generalizations.
The choice of partition corresponds to the generalization strategy (i.e., which distinctions will be
maintained and which will be lost). As Gleitman [1990, p. 12] wrote:
The trouble is that an observer who notices everything can learn nothing for there is no end of
categories known and constructible to describe a situation. [emphasis in original]
The preﬁx tree keeps track of all the information, and state-merging deliberately ignores some of it,
leading to generalization. Which information should be ignored and which should be kept is at the
heart of learning, and at the heart of state-merging.
3.6
STATE-MERGING AS INFERENCE RULES
One of the important insights that state-merging learning strategies has led to is the relationship of
the Nerode-equivalence relation to other kinds of equivalence relations. Two examples will serve to
illustrate.
Again consider the strictly k-local languages [McNaughton and Papert 1971, Garc´ia et al.
1990], which can be learned by merging states with same incoming paths of length k. Because
these states are merged it means any two preﬁxes of the languages with the same k-length sufﬁx
have exactly the same residuals; that is, they have the same set of good tails. Formally, this can be
stated as an inference rule: ∀u, v, w ∈∗: uv, wv ∈prefixes(L) with |v| = k then TailsL(uv) =
TailsL(wv). This property turns out to be a characteristic property of the strictly local languages
known as the sufﬁx substitution property [Rogers and Pullum 2011]: a language L is Strictly Local
ifdef ∃k such that if uvx, wvy ∈L and |v| = k then uvy, wvx ∈L. The state-merging makes clear
why this property holds—paths uv and wv will lead to the same state.
Another example comes from the 0-reversible languages [Angluin 1982]. Angluin showed
that by merging states to eliminate reverse non-determinism that ∀u, v, w, y ∈∗if uv, wv, uy ∈L
then wy ∈L. In other words, if two preﬁxes u and w share one good tail then they share all good tails.

3.7 RPNI
67
The insights by understanding these state-merging algorithms in terms of inference rules of
the above types have led to a number of algorithms for learning non-regular languages under the
name “distributional learning” [Clark and Eyraud 2007, Yoshinaka 2009, Clark and Lappin 2011].
3.7
RPNI
rpni is an acronym for Regular Positive and Negative Inference. As its name indicates, rpni is
unlike the state-merging algorithms considered so far because it relies on both positive and negative
evidence. A detailed discussion (with theorems and proofs) along with a clear explanation of an
example run of rpni is provided by de la Higuera [2010]. The brief discussion draws highlights
from de la Higuera’s discussion there.
3.7.1
HOW IT WORKS
Like the state-merging algorithms above, rpni ﬁrst builds a preﬁx tree from the positive data. Note
that the negative data is not expressed in the preﬁx tree. There is only one way in which it could in
fact be expressed. If a negative data point w was a preﬁx of a positive data point wv, then the state in
the preﬁx tree corresponding to w could be marked as “deﬁnitively not a ﬁnal state.” This foretells
how the merging procedure in rpni uses the negative evidence.
After the preﬁx tree is built, the states are enumerated in a breadth-ﬁrst fashion. Figure 3.8
illustrates a breadth-ﬁrst enumeration of a preﬁx tree. Pairs of states are merged (and then tested)
according to this enumeration. So in Figure 3.8, rpni ﬁrst merges states 0 and 1 and tests the
consequences (see below). This test determines whether those states should stay merged or if the
merge should be undone. Afterwards, in either case, it considers the next pair of states in the
enumeration. In this example, that would be states 0 and 2, then 0 and 3, then 1 and 2, and so on.
One consequence of this particular enumeration is that whenever two states are merged, one
of those states will always be the root of a subtree. In other words, the tails of one of the two states
being merged in the preﬁx tree will always be ﬁnite.8
The automata obtained from merging of two states is then tested against the negative evidence.
If this automata rejects all the available negative evidence then the test is a success; otherwise it fails.
If the test fails, merging of the two states is undone, and the two states remain distinct henceforth.
To see why the test could fail, recall that if a ﬁnal state is merged with a non-ﬁnal state, the
resulting state is also ﬁnal. If this non-ﬁnal state is one that must be non-ﬁnal given the negative
evidence, then the test will fail. For instance, if trying to learn a regular language L, the positive
examples in Figure 3.8 are provided, and it is known that a ̸∈L, then merging states 1 and 2 will
8. In de la Higuera’s book, these are referred to as the “blue” states.

68
3. LEARNING REGULAR LANGUAGES
9
4
a
a
a
a
a
b
b
b
c
c
c
c
1
10
11
5
6
2
0
12
7
8
3
FIGURE 3.8: A preﬁx-tree whose states have been enumerated in breadth-ﬁrst fashion.
be rejected. This is because the merged state will be ﬁnal which would result in an automata which
misclassiﬁes the string a. This is how rpni uses negative evidence.
Another important detail regarding rpni is how it handles non-determinism. Recall that
state-merging can (and often does) lead to non-determinism. Essentially, once a pair of states has
been merged, the non-determinism is eliminated by “folding” the tree branching from one of the
merged states into the rest of the existing automata, extending it as necessary. Thus, if a merge is
rejected, the folding must also be undone.
3.7.2
THEORETICAL RESULTS
Oncina et al. [1993] proved that the rpni efﬁciently identiﬁes all regular languages in the limit from
positive and negative data. The proof establishes the existence of a (ﬁnite) characteristic sample for
each regular language. This characteristic sample can be determined from the tail canonical acceptor.
Essentially, the positive data comes from a ﬁnite set of words which exercise every transition (and
ﬁnal states) in the automata. The crucial negative evidence comes from data points which show that
every two states in the canonical acceptor do not have the same set of tails. Since both the positive
and negative evidence are ﬁnite, the entire sample itself is ﬁnite as well.
As an example, consider the canonical acceptor for Pintupi stress. A negative data point which
distinguishes states 2 and 3 would be ´σσσ `σσ since `σσ is a tail of ´σσ (which leads to state 2), but
it is not a tail of ´σσσ (which leads to state 3).

3.8 Regular Relations
69
One important aspect of rpni is that it is an efﬁcient algorithm in a couple of different senses.
First, its execution time is polynomial in the size of the sample. Second, and more interestingly, the
size of the characteristic sample is polynomial in the size of the canonical acceptor for the target
regular language.
It is the second property which distinguishes rpni from learners mentioned in Chapter 2.
Recall in Section 2.2.2 that there are learning algorithms that are able to learn the entire computably
enumerable class from positive and negative data. These algorithms enumerated all the logically
possibly grammars and just hypothesized the ﬁrst one in the enumeration consistent with all the
data considered so far. These “enumerative learners” were troubling because while they met the letter
of the deﬁnition of learning, they did not meet its spirit. They were not insightful, and suggested
something about the deﬁnition was not correct.
One way to improve the deﬁnition was to require learning to be efﬁcient in some sense. Of
the two senses mentioned above, with respect to regular languages—polynomial-time computability
in the size of the data and polynomial size of the characteristic sample in the size of the canonical
acceptor for target language—only the latter is meaningful. This is because Pitt [1989] showed that
any enumerative learner can be made polynomial-time computable in the size of the data with a
method now referred to as Pitt’s trick. (See also de la Higuera [1997, 2010], Eyraud et al. [2015].)
No such trick is known to exist for the second sense, which is “polynomial size of the
characteristic sample in the size of the canonical acceptor.” Thus, it is this second sense which makes
rpni an interesting contribution to the problem of inference of regular languages from positive and
negative data.
3.8
REGULAR RELATIONS
Regular relations are those relations that can be described with non-deterministic ﬁnite-state trans-
ducers. Many problems in computational linguistics, such as transliteration, letter-to-phoneme
conversion, and machine translation, are problems about learning relations.
Subsequential relations are a subclass of the regular relations. They are those regular relations
which describe functions for which a ﬁnite-state transducer which processes the input determinis-
tically exists. They can also be described informally as weighted deterministic acceptors where the
weights are strings and multiplication is concatenation.
Formally, a subsequential transducer is a tuple ⟨Q, q0, X, Y , σ , δ⟩, where Q is a ﬁnite set
of states, X and Y are ﬁnite alphabets, q0 ∈Q is the initial state, and σ ⊆Q × Y ∗is the output
function. The transition function δ ⊆Q × X × Y ∗× Q is necessarily deterministic:
(q, a, u, r), (q, a, v, s) ∈δ ⇒u = v ∧r = s.

70
3. LEARNING REGULAR LANGUAGES
The transition function δ is also recursively extended to δ∗. The relation that a subsequential
transducer T = ⟨Q, q0, X, Y , σ , δ⟩recognizes/accepts/generates is
R(t) =

(x, yz) ∈X∗× Y ∗| (∃q ∈F)
[(q0, x, y, q) ∈δ∗∧z = σ(q)]
	
.
(3.4)
Since subsequential transducers are deterministic, the relations they recognize are functions. Sub-
sequential transducers have been generalized to permit up to p outputs for each input and Mohri
[1997] showed that many desirable properties are preserved.
Like nfa, subsequential transducers have a canonical form [Oncina et al. 1993], which
associates the states of the canonical machine to classes of a Nerode-like equivalence. In addition to
the aforementioned properties of subsequential transducers, these canonical machines are “onward,”
which means the transducer, as it reads the input, minimally delays writing the output (so not at all
or as little as possible).
Oncina et al. [1993] presented the Onward Subsequential Transducer Inference Algorithm
(ostia), which provably identiﬁes subsequential functions in the limit from positive data. For every
subsequential function f , the input to the algorithm is a ﬁnite sample of pairs (w, f (w)). ostia is
similar to rpni because pairs (w, f (w)) provide a form of (indirect) negative evidence. If (x, y) ∈f
then for all z ̸= y it must be the case that (x, z) ̸∈f .
ostia ﬁrst builds an onward preﬁx tree. An onward preﬁx tree is constructed based on the input
strings and the outputs are pushed as close to the root of the tree as possible to ensure onwardness.
Then the algorithm merges states in a manner similar to rpni. It enumerates the states in the preﬁx
tree in a breadth-ﬁrst fashion and then greedily merges states unless onward subsequentiality is lost.
Oncina et al. [1993] proved that ostia identiﬁes total subsequential functions in the limit
from positive data. Again, no negative data is required (unlike rpni) because the positive data, in
conjunction with the knowledge that a function is being learned, provides indirect negative evidence.
Interestingly, for partial subsequential functions f , ostia is also guaranteed to succeed in the
sense that it returns a subsequential function f ′ such that for all w in the domain of f it is the case
that f ′(w) = f (w). But, interestingly, if f is not deﬁned on w, f ′ may be! Oncina et al. [1993]
reported an interesting experiment on learning the function converting Roman numerals to Arabic
numerals, where the function returned by ostia correctly translates well-deﬁned Roman numerals
like “XVIII” but returns uninterpretable numbers on ill-deﬁned Roman numerals like “VXIII.”
Also, in later work Oncina and colleagues overcome this challenge by changing the nature
of the learning problem. If the learning problem is to identify a subsequential transduction given a
learning sample and the domain of the transduction (given as a dfa) then they showed even partial

3.9 Learning Stochastic Regular Languages
71
subsequential transduction can be efﬁciently identiﬁed exactly [Oncina and Var´o 1996]. Similarly,
they show how additional knowledge of the range helps in a similar way [Castellanos et al. 1998].
ostia has also been applied to phonological rule learning. Gildea and Jurafsky [1996] show
that ostia does not learn the English tapping rule or German word-ﬁnal devoicing rule from data
present in adapted dictionaries of English or German. They explain that the sufﬁcient sample that
ostia needs to converge (and is guaranteed to receive as input at some ﬁnite point in theory under the
identiﬁcation in the limit from positive data paradigm) is not present in these adapted dictionaries.
This is not just a matter of quantity and needing larger dictionaries. It is also a matter of quality.
The sufﬁcient sample needed by ostia to learn these phonological rules may require words that
violate inviolable constraints in English and German, such as the logically possible word ttt. Gildea
and Jurafsky [1996] went on to apply additional phonologically-motivated heuristics to improve
state-merging choices and obtain signiﬁcantly better results.
More recently, Chandlee [2014] shows local phonological processes can be characterized by
a class of subsequential functions. This subclass is called input strictly local because they are deﬁned
analogously to the strictly local languages. Chandlee et al. [2014] provides a state-merging algorithm
which learns this subclass in the limit from positive data more efﬁciently than ostia or its variants.
Jardine et al. [2014] provides another algorithm for learning this subclass from positive examples in
linear time and data as well as other subclasses whose underlying structure is known and ﬁxed in
advance. These algorithms are similar in ﬂavor to the variants of ostia which also assume additional
a priori knowledge [Oncina and Var´o 1996, Castellanos et al. 1998] (but in terms of the nature of
the transduction in addition to knowledge of the domain or range).
3.9
LEARNING STOCHASTIC REGULAR LANGUAGES
In this section, the grammatical inference of regular stochastic languages is examined. After deﬁning
regular stochastic languages, the ﬁrst problem is considered: how to estimate the parameters, from
a sample, of a stochastic language belonging to a class of subregular distributions describable with a
deterministic ﬁnite-state acceptor (pfa). This problem has a known solution under the Maximum
Likelihood Estimate criteria. After discussing two classes of subregular stochastic distributions,
we move to state-merging algorithms which are able to efﬁciently learn the entire class of regular
deterministic stochastic languages under different learning criteria. Methods that target the larger
class of regular non-deterministic stochastic languages are then discussed. This discussion includes
mention of the results of the recent pautomac competition, where teams competed to develop
algorithms that could best learn deterministic and non-deterministic regular stochastic languages
[Verwer et al. 2014].

72
3. LEARNING REGULAR LANGUAGES
3.9.1
STOCHASTIC LANGUAGES
A stochastic language is a probability distribution over ∗. As explained in Chapter 2, this means
that
1. each word in ∗is assigned some probability between zero and one, and
2. the sum of all the probabilities adds to one.
Like formal (non-stochastic) languages, there are different classes of stochastic languages. As with
non-stochastic languages, the ﬁeld of grammatical inference is interested in ﬁnding algorithms that
can successfully learn every distribution in a class, under some rigorous deﬁnition of successful
learning.
One way to deﬁne classes of stochastic languages is in terms of the support of the stochastic
languages. The support of a stochastic language is the non-stochastic language obtained by the set of
strings with non-zero probabilities. While it is natural to deﬁne regular stochastic languages as those
with regular support (and context-free stochastic languages as those with context-free support and
so on), these are not very useful deﬁnitions. For instance, if regular stochastic languages are deﬁned
as those with regular support then very powerful grammars are necessary to compute them [Kornai
2011]. This result, originally due to Ellis [1969], is because there are logically possible distributions
with regular support where the probabilities contain irrational values. In fact, Kornai (Theorem 1)
actually shows that even probabilistic context-free grammars cannot describe all stochastic languages
with regular support. Consequently, the distinctions afforded by the traditional class boundaries—
regular, context-free, context-sensitive, and computably enumerable—are lost. For this reason we
deﬁne classes of stochastic languages in terms of the grammars, and not in terms of their support.
It is common in computational linguistics for stochastic grammars of a certain type to be
deﬁned as their non-stochastic counterparts. For instance, a probabilistic context-free grammar is
simply a context-free grammar where the production rules have been augmented with probabilities in
an appropriate fashion. Similarly, regular stochastic languages can be deﬁned to be those describable
with ﬁnite-state machines augmented with probabilities on the transitions. (This class turns out
to be exactly the same class of distributions that can be represented with hidden Markov models
[Vidal et al. 2005]; see Chapter 2). When classes of stochastic languages are deﬁned in this way,
the traditional boundaries remain [Kornai 2011, Theorem 2]. For example, with these deﬁnitions,
any distribution deﬁned by a stochastic nfa can be deﬁned by a pcfg, but not vice versa. It is this
(grammar-based) deﬁnition that we use when discussing families of stochastic languages.
As with classes of formal languages, even if two classes are learnable under some deﬁnition
of learning, there is typically a trade-off in the amount of time and data necessary to converge to
the target grammar depending on the nature of the target class. The more structured the class (and
hence typically less expressive), the easier learning tends to be.

3.9 Learning Stochastic Regular Languages
73
As mentioned, regular distributions are those obtained by assigning probabilities to the
transitions of each state in a (possibly non-deterministic) ﬁnite-state acceptor.9 Consequently, there
are two problems when trying to learn a regular distribution: one is trying to learn the structure of the
ﬁnite-state acceptor, and one is trying to learn the probabilities on the transitions. By structure of the
acceptor, we mean the state set and the transitions between the states. Since any missing transition
can be modeled as an existing transition with zero probability, the structure of every acceptor can
be considered to be fully connected (every transition with every letter of the alphabet exists between
every two states). This effectively reduces the structure of the machines to a single number—the
number of states in the acceptor.
Learning the class of regular distributions is not easy, but there has been substantial theoretical
progress in grammatical inference which addresses this problem. We begin this section with a much
easier problem: learning very structured classes of distributions using the Maximum Likelihood
Estimate as the learning criterion.
Many readers are probably familiar with this style of learning since it underlies commonly
used techniques in natural language processing (such as n-grams). An orthodox presentation can be
found in many places, such as Geman and Johnson [2004].
However, the presentation here is unorthodox because it is being presented from the perspec-
tive of grammatical inference. Our focus is on well-deﬁned classes of distributions, well-deﬁned
presentations of data drawn from these distributions, well-deﬁned learning criteria that make clear
what successful learning is, and algorithms that successfully learn any distribution from the class
under this learning criteria.
3.9.2
STRUCTURE OF THE CLASS IS DETERMINISTIC
AND KNOWN A PRIORI
One way a class of stochastic distributions can be described is with a single deterministic ﬁnite-state
acceptor. The dfa represents a class of distributions—the ones obtainable by placing probabilities on
the transitions in the dfa. In order to keep the grammatical representation ﬁnite, it will be important
to constrain the probabilities on the transitions in some fashion. It is common to assume they have
rational values and cannot be any real value. We will abstract away from this issue here.
Clearly, this class is properly contained within the class of regular distributions. For a dfa
M, let DM denote this class of distributions. This class essentially ﬁxes the structure, and thus
the only learning problem is to learn probabilities of the transitions. Figure 3.9 illustrates a class of
distributions and a particular distribution within the class.
9. Here, and in the sequel, we consider the action of ending the generation process at a particular state with a transitional
probability. Formally, this is usually accomplished with a function which maps states (not transitions) to probability.

74
3. LEARNING REGULAR LANGUAGES
b
a
c
c : 1–4
b : 1–4
a : 1–4
A
M
A : 1–4
M′
FIGURE 3.9: M represents a family of distributions with four parameters. M′ represents a particular
distribution in this family.
The statistical model is given by the structure of the dfa and the transitions to be estimated
are the parameters of the model. This problem has a solution under the learning criterion known as
the Maximum Likelihood Estimate (mle).
To explain the mle criterion, it is necessary to ﬁrst deﬁne the likelihood of a sample S
generated by a distribution D. Let D(w) be the probability that D generates w. If S = ⟨w1, . . . wn⟩
then the likelihood of S given D is deﬁned as
LD(S) =

w∈S
D(w).
Note that S is not a set, but a sequence, so the same word can occur multiple times in S (and
thus would occur multiple times in the product). The product above reﬂects the assumption that
S is independent and identically distributed (i.i.d.). Therefore, an element in the sequence S is
independent of the ones that come before and after it.
Given a sample of data S, the mle of S is the distribution DA ∈D that maximizes the
likelihood of the data S with respect to the D. In other words, mle assigns a greater likelihood
to S than every other distribution in D.
The mle learning criterion can then be stated as follows. A learning algorithm A learns a
class of distributions D ifdef , for all D belonging to D, and for any ﬁnite sample of data S generated
by D, the grammar G output by A(S) deﬁnes a distribution DG, which is the mle with respect to D.
One of the reasons the mle criterion is important follows from thinking of the input sample
S as it becomes longer and longer and approaches an inﬁnite sequence of words generated by the
target distribution. For any ϵ > 0, there is a sample size N such that for all S where |S| ≥N, the
difference between the mle of S with respect to D and the true distribution D ∈D is within ϵ.
This property is called consistency in the statistical literature. In other words, we are guaranteed, as

3.9 Learning Stochastic Regular Languages
75
b : 2
a : 0
c : 1
c : 1–4
b : 1–2
a : 0
A : 1
A : 1–4
FIGURE 3.10: The ﬁgure on the left indicates the count of the parse of S = ⟨bbc⟩through M and the ﬁgure
on the right indicates the probabilities obtained after normalization.
the sample size grows, to get arbitrarily close to the true distribution. The difference between two
distributions can be measured in different ways, but the above result is true for any of these ways.10
For any dfa M, a learning algorithm for DM which satisﬁes the mle is known. We do not
know the origins of the following theorem, but it is not difﬁcult to prove.11 For modern treatments,
see Vidal et al. [2005] and de la Higuera [2010].
Theorem 3.3
For a sample S and deterministic ﬁnite-state acceptor M, let algorithm A count the
parse of S through M and normalize at each state. A is a learning algorithm for DM which satisﬁes
the mle.
To count the parse of S through Mmeans the following. Initialize the count of each transition,
and the count of ending at each state to zero. Then, for each word w ∈S, follow the path of w in M
and add one to each transition traversed in this path. When a word ends in a state, add one to that
count as well. Since w was generated by a distribution in DM and since M is deterministic, there is
exactly one such path. In the statistical literature, this kind of learning process is called the relative
frequency estimator.
Figure 3.10 illustrates the learning procedure of DM for S = ⟨bbc⟩. According to Theo-
rem 3.3, the pfa on the right in Figure 3.10 satisﬁes the mle criterion: the likelihood it assigns to
S = ⟨bbc⟩is greater than the likelihood every other distribution in DM assigns to S.
N-gram models are widely used in computational linguistics. These are in fact strictly k-local
distributions. Figure 3.11 shows the structure of a bi-gram model where the alphabet is {a, b, c}.
Such a model has 16 transitional probabilities, given by associating probabilities to each transition
and to ending at each state. These are the 16 parameters of a bi-gram model for this alphabet.
10. de la Higuera [2010] and Clark and Lappin [2011] contain good discussions of different ways measures to measure the
distance between distributions.
11. A typical proof of this theorem solves a system of partial differential equations.

76
3. LEARNING REGULAR LANGUAGES
λ
b·
a·
c·
b
b
b
b
c
c
c
c
a
a
a
a
FIGURE 3.11: The structure of a bi-gram model.
TABLE 3.4: Words in Samala with sibilant sounds [ʃ, S]
1.
[ʃtojonowonowaʃ]
“it stood upright”
[Applegate 1972, p. 72]
2.
[kʃptwaʃ]
“I made acorn mush”
[Applegate 1972, p. 119]
3.
[suslasq]
“he presses it tight”
[Applegate 1972, p. 119]
4.
[swashisin]
“the terrain is rugged”
[Applegate 1972, p. 122]
Leaving aside the important problem of smoothing [Jurafsky and Martin 2008, Ch. 4], training a
bi-gram model proceeds exactly per Theorem 3.3.
It is well known that natural language contains long-distance dependencies, both in syntax
[Chomsky 1956] and in phonology [Odden 1994, Rose and Walker 2004, Hansson 2010, Nevins
2010]. For any n, these dependencies can extend beyond n symbols, and so n-gram models are unable
to express (or model) them.
Here is an example of an unbounded long-distance dependency from Samala,12 a Chumash
language spoken in an area near Santa Barbara, California [Applegate 1972, 2007]. In this language,
there is a long-distance dependency among sibilant sounds. Sibilants are sounds like [ʃ, s], and in
Samala there are words containing sounds like [ʃ] and words containing sounds like [s], but no words
containing both [ʃ, s]. Well-formed words only draw from one group of these sounds or the other
and never from both. Some Samala words are shown in Table 3.4. However, there are no words
which are normally pronounced like those shown in Table 3.5.
This kind of long-distance dependency can be modeled with strictly piecewise languages
[Heinz 2010, Rogers et al. 2010, 2013]. These are a subclass of the regular languages similar to strictly
12. This language was formerly called Inese˜no Chumash.

3.9 Learning Stochastic Regular Languages
77
TABLE 3.5: Impossible Samala
words with sibilant sounds [s, ʃ]
1.
*[stojonowonowaʃ]
2.
*[ʃtojonowonowas]
3.
*[koswashiʃin]
4.
*[koʃwashisin]
local languages, except they are based on subsequences, not substrings. A word u is a subsequence of
another word w the symbols in u occur in the same order in w (but not necessarily contiguously).13
With respect to Samala, this means that words like *[stojonowonowaʃʃʃ] and *[ʃʃʃtojonowonowas] are
ill-formed because the subsequences sʃʃʃ and ʃʃʃs, which these words contain, are ill-formed.14
The strictly piecewise languages have several interesting characterizations in terms of formal
language theory, automata theory, model theory [Rogers et al. 2010, 2013], and the algebraic theory
of automata [Fu et al. 2011]. Like strictly local languages, if there is an upper bound k on the length
of the sequence, the class is also provably efﬁciently learnable in interesting ways [Heinz et al. 2012b,
Heinz and Rogers 2013].
Heinz and Rogers [2010] deﬁned strictly piecewise distributions and present an algorithm
for learning the mle of this class. They represent the class with multiple machines (what Heinz
and Rogers [2013] call a “factored” representation). The distribution itself is then provided by a
variant of the product operation which calculates the co-emission probability [Vidal et al. 2005].
The algorithm ﬁnds the mle of the class of distributions represented by each individual factor per
Theorem 3.3, and then applies the product operation.
The results in Table 3.6 show the parameters obtained when the algorithm is fed a training
corpus of 4800 words from a dictionary of Samala. The results mean *[stojonowonowaʃʃʃ] would be
orders of magnitude less likely than [ʃʃʃtojonowonowaʃʃʃ] because it contains the [sʃ] subsequence.
This example, and the one before it, are examples of subregular classes of distributions, which
can be represented with a single automaton as in the case of the strictly k-local languages, or as a
list of automata which are combined by a special product operation, as in the case of the strictly
k-piecewise languages. What they have in common is that the structure of the class can be ﬁxed
to these automata-theoretic representations. Consequently, learning distributions based on these
13. Formal deﬁnitions are given in Section 1.6. The language ∗u1∗u2 . . . ∗un∗is called the shufﬂe ideal of u.
14. There are languages like Samala except only one of the subsequences {sʃʃʃ, ʃʃʃs} is forbidden. See Heinz [2010] for
discussion.

78
3. LEARNING REGULAR LANGUAGES
TABLE 3.6: SP2 probabilities of a sibilant occurring some-
time after another one (collapsing laryngeal distinctions).
P(x | y <) means the probability of x given y occuring
anywhere before it in the string.
x
P(x | y <)
s
ts
ʃ
tʃ
s
0.0325
0.0051
0.0013
0.0002
ts
0.0212
0.0114
0.0008
0.
y
ʃ
0.0011
0.
0.067
0.0359
tʃ
0.0006
0.
0.0458
0.0314
classes can be reduced to the problem of the estimating the values of the parameters of the model,
which are expressed in the automata as the transitional probabilities. Theorem 3.3 is an important
result allowing this to happen.
This section has focused on learning stochastic languages where the underlying structure is
ﬁxed with a single dfa (as is the case for stochastic strictly k-local languages, also called n-gram
models), or is ﬁxed with a list of dfa (as is the case for stochastic strictly k-piecewise languages)
under the mle learning criterion. Other types of learning criteria, and other types of estimators
exist, which can also be proﬁtably studied when the structure of the is known and ﬁxed a priori.
Chapter 2 already discussed the pac-learning criteria. Bayes estimators and the maximum a posteriori
estimator are also widely used in computational linguistics and natural language processing [Geman
and Johnson 2004].
So far this chapter has motivated the strictly local and strictly piecewise languages from
studies of natural language phonotactics. More generally, it has been hypothesized that all segmental
phonotactic patterns in natural languages can be modeled with strictly local and strictly piecewise
languages (and by extension, distributions) [Heinz 2010]. (See Heinz et al. [2011] for a slightly
weaker hypothesis.)
3.9.3
STRUCTURE OF THE CLASS IS DETERMINISTIC
BUT NOT KNOWN A PRIORI
There are algorithms with theoretical guarantees for larger classes of regular distributions. In
particular, the class of regular deterministic stochastic languages is identiﬁable in the limit with

3.9 Learning Stochastic Regular Languages
79
probability one [de la Higuera and Thollard 2000] and are learnable in modiﬁed-pac setting [Clark
and Thollard 2004].
In both cases, the algorithms presented employ state-merging methods and build on prior
work, notably the algorithm alergia [Carrasco and Oncina 1994], which is the ﬁrst approach
guaranteed to learn the structure underlying any regular deterministic stochastic language (rdsl).
In this section, we choose to describe alergia [Carrasco and Oncina 1994, 1999], because of its
similarity to rpni and ostia.
For non-stochastic regular languages, we already emphasized the importance of the Myhill–
Nerode theorem. A similar theorem exists for regular deterministic stochastic languages [Carrasco
and Oncina 1999, Vidal et al. 2005]. Each rdsl has a canonical representation in terms of a
probabilistic deterministic ﬁnite-state acceptor (dpfa). Just as each state q in the tail canoni-
cal acceptor for a regular language corresponds to a regular language, which is the set of good
tails for each string w that leads to q from the initial state, each state in the canonical dpfa
for a rdsl corresponds to a rdsl, which represents the residual for each string w that leads
to q from the initial state with a non-zero probability. In this case, the residual is a stochastic
language.
Consequently, a state-merging algorithm needs only to decide correctly whether two states
in a preﬁx tree construction have the same stochastic set of good tails. The general form of the
algorithm is given by rlips [Carrasco and Oncina 1994], which is an acronym for Regular Language
Inference from Probabilistic Samples. A statistical test can be used to decide if two ﬁnite samples
are drawn from the same distribution or not (and hence belong to the same stochastic set of tails).
Since a number of different tests can be employed, rlips represents a family of algorithms, of
which alergia is one. alergia uses the Hoeffding statistical test and Carrasco and Oncina [1999]
and de la Higuera and Thollard [2000] showed that this method provably converges to the target
dpfa with probability one. In other words, alergia successfully learns both the structure and the
transitional probabilities, and it does so with polynomial time and data.
Clark and Thollard [2004] also obtained a theoretical learning result for the class of distri-
butions deﬁnable from dpfa, this time in a variant of the Probably Approximately Correct (pac)
framework. Kearns and Vazirani [1994] established that the class of distributions describable with
dpfa is not pac-learnable. Clark and Thollard combine the state-merging insights from alergia
with the insights of Ron et al. [1995], who developed a pac-like learning algorithm for a class of
acyclic dpfa. As before, the idea is to only merge states in a preﬁx tree if the preﬁxes share the same
stochastic set of tails. Instead of using the Hoeffding test, however, Clark and Thollard (follow-
ing Ron et al.) adopt the Kullback–Leibler Divergence as a way to measure the error between two
distributions (see Section 2.2.6). Because this introduces additional parameters into the learning
framework, they refer to their learning criteria as kl-pac.

80
3. LEARNING REGULAR LANGUAGES
de la Higuera [2010] referred to the Clark and Thollard algorithm as dsai for “Distinguishing
Strings Automata Inference.” This is because crucial to their algorithm and its analysis is the notion
of “distinguishing strings” which reveal whether two states have the same stochastic tail set or not.
Speciﬁcally, Clark and Thollard considered some μ > 0 and deﬁne for every pair of states in a dpfa,
a string w to be “μ-distinguishing” if the difference between the probability assigned to w in the
stochastic tail set of one preﬁx and the probability assigned to w in the stochastic tail set of the other
preﬁx is greater than μ.
Together these results show that it is possible to efﬁciently learn, in certain senses, both the
structure and the transitional probabilities of the dpfa that model regular deterministic stochastic
languages.
3.9.4
STRUCTURE OF THE CLASS IS NON-DETERMINISTIC
AND NOT KNOWN A PRIORI
Methods for learning the class of non-deterministic regular stochastic languages face hurdles. Abe
and Warmuth [1992] established that learning the class of non-deterministic regular stochastic
languages is hard.
One clue to why this is the case might come from that the fact that although this class
of distributions can be modeled with both hmms and pfa, there are no canonical forms for the
distributions in this class. Thus, unlike the classes discussed above, it is not possible for the global
structure of the underlying grammar to be determined from a series of local decisions in a preﬁx
tree. Whether or not two preﬁxes correspond to the same state in the underlying representation is
independent from whether they share the same stochastic set of tails or not.
Another related reason for the difﬁculty is the credit problem. When the structure is known
and deterministic, there is exactly one parse for each string in the sample and so it is clear where
and how to adjust the probabilities in the transitions of the automata. However, when the unknown
structure is non-deterministic, there are potentially many distinct parses of a string in the sample. In
this case, it is not clear which transitions in the underlying automata are responsible and how their
probabilities should be adjusted to maximize the likelihood of the sample.
Nonetheless, a number of powerful statistical methods have been, and continue to be, de-
veloped. These methods do not have the same theoretical guarantees as the algorithms mentioned
above. We brieﬂy mention two approaches.
The Expectation-Maximization algorithm offers one way to update probabilities that guaran-
tees reaching a local optimum in the likelihood space. The idea is to ﬁrst make an initial estimation
of the number of states of a fully connected pfa and the weights on the transitions and then iterate
through two steps: (1) estimating the counts obtained from the sample using the weights (expecta-
tion step) and (2) adjusting them in a way that increases the likelihood of the sample (maximization

3.10 Summary
81
step). This approach is implemented via a dynamic programming approach known as the Baum–
Welch method. Readers are referred to Jurafsky and Martin [2008] and de la Higuera [2010] for
details.
A second, more recent approach is called spectral learning, which decomposes the target
function according to its Hankel matrix, a concept borrowed from algebraic theory. For any stochastic
distribution D describable with a pfa over ∗, the rows and columns of its Hankel matrix correspond
to preﬁxes and sufﬁxes, respectively, and values of cell (u, v) are assigned D(uv). While this
representation of D is redundant, it affords several interesting properties. The inﬁnitely sized Hankel
matrix can be partitioned into submatrices with particular properties; the number of blocks (called
rank) of a particular partition relates to the size of the minimal pfa for D. Thus, spectral learning of
an unknown distribution D comes down to identifying the right partition given a sample and then
computing a pfa for D. Provided the rank of D is known, this is possible because the ﬁnite basis of
a particular partition of the Hankel matrix ensures that only ﬁnitely many strings need to be seen
in order to determine the entire Hankel matrix for D. Readers are referred to Hsu et al. [2012] and
Balle et al. [2014a] for details.
The Probabilistic Automata learning Competition (pautomac) was run in 2012 as part
of the biannual International Conference of Grammatical Inference. It was the ﬁrst grammatical
inference challenge that allowed the comparison between methods and algorithms designed to learn
deterministic and non-deterministic regular stochastic languages. Challengers were provided with
artiﬁcial data and tried to estimate the probabilities of unseen strings generated by the underlying
probabilistic models. Results were evaluated by calculating perplexity; see Verwer et al. [2014] for
details.
Perhaps one of the most striking results of the competition was put by the organizers of the
competition this way: “Of course, we cannot be sure [. . . ], but it seems to indicate that it is best to
learn a non-deterministic model when the data is drawn from a non-deterministic distribution, and
that it is best to learn a deterministic model when the data is drawn from a deterministic distribution”
[Verwer et al. 2014, p. 143]. Obviously then, if we are interested in modeling natural language with
stochastic grammars, we would like to know whether the underlying grammars are deterministic or
non-deterministic.
3.10
SUMMARY
This chapter studied the problems of learning classes of regular languages, regular transductions, and
regular stochastic languages. Different types of state-merging algorithms were shown to be able to
learn different classes of these languages. The principles behind state-merging can be summarized
as follows.

82
3. LEARNING REGULAR LANGUAGES
.
Regular languages, subsequential transducers, and deterministic regular stochastic languages
have canonical forms.
.
Each state q in the canonical form itself represents a residual. For regular languages, q
represents the set of good tails of strings w which lead to q. These are the set of strings
which would change the state from q to a ﬁnal state. For subsequential transductions and
stochastic languages, the residuals are characterized similarly. In these cases the residuals are
subsequential functions in the former case and stochastic languages in the latter.
.
Tree representations of the sample are ﬁnite representations of the observed data. Preﬁx (sufﬁx)
trees distinguish each preﬁx (sufﬁx) in the sample with its own state and hence its own residual.
.
Criteria are used to decide when different states have the same residuals. The corresponding
states are merged.
While the class of regular languages cannot be identiﬁed in the limit from positive data alone,
certain subclasses of regular languages can be so identiﬁed. Rogers and Pullum [2011] and Rogers
et al. [2013] discuss several natural subregular classes, some of which appear to characterize certain
natural language phenomenon well.
The state-merging algorithms effectively instantiate (a priori given) inference rules. For
instance, if two preﬁxes share a common k-long sufﬁx then merging the corresponding states in the
preﬁx tree will result in learning strictly (k + 1)-local languages. The bias selection that is undertaken
in these cases can be said to be quite strong. Similarly, in the case of stochastic languages, if the
deterministic structure of an underlying acceptor is known, the bias selection is very strong and
learning the transitional probabilities is straightforward.
The class of regular languages can efﬁciently identiﬁed in the limit from positive and negative
data by the algorithm rpni. rpni merges pairs of states provided the result is consistent with the
sample of positive and negative it is given.
Similarly, the class of subsequential transductions can be efﬁciently identiﬁed in the limit from
positive examples by ostia. Because subsequential transductions are functional, a positive example
also provides implicit negative evidence. Consequently, ostia is very similar in spirit to rpni.
Similarly again, the class of regular deterministic stochastic languages can be efﬁciently iden-
tiﬁed in the limit from positive examples by alergia. Drawing a stochastic sample also allows for
implicit negative evidence to become available. Like regular languages and subsequential transduc-
tions, regular deterministic stochastic languages admit canonical forms. alergia merges states in
the preﬁx tree provided that the residuals of those preﬁxes are considered to be equivalent. There are
different tests available for this, but de la Higuera and Thollard [2000] showed that the Hoeffding
test used by alergia is theoretically sound and leads to convergence in the limit with probability 1.

3.10 Summary
83
The bias selection in these last three cases is also strong (deterministic regular grammars), but
is considerably weaker than trying to learn subclasses of regular languages, functions, or stochastic
regular languages. The development of algorithms that provably and efﬁciently learn these classes
under the criteria provided are some of the key achievements of the ﬁeld of grammatical inference.
This is why de la Higuera [2010] presented detailed treatments of these algorithms with example
runs. Readers are referred to that book for details about these algorithms that are not covered here.
As mentioned, targeting subclasses of the subsequential functions or regular deterministic
stochastic languages strengthens the bias selection. Subclasses of subsequential functions have only
recently been studied and appear to be learnable under stronger, more efﬁcient learning criteria
[Chandlee et al. 2014, Jardine et al. 2014]. On the other hand, subclasses of deterministic stochastic
languages have been studied previously, notably n-gram models, which are the stochastic version of
strictly k-local languages. However, many others remain to be studied carefully.
Furthermore, weakening the bias selection from “deterministic regular” to “non-deterministic
regular” seems to lead to trouble. For instance, the theoretical guarantees for learning the larger class
of non-deterministic regular stochastic languages are much weaker. In general, the true structure of
the underlying automaton is not guaranteed to be discovered with these methods. Other methods
which are guaranteed to identify the grammar in the limit with probability 1, such as the enumerative
methods given by Angluin [1988a], are unfortunately very inefﬁcient. Results for learning the full
class of regular relations—in contrast to the subsequential functions—are also strikingly absent.
Many tasks in computational linguistics use methods that are among the most successful for
learning non-deterministic regular stochastic languages. However, to our knowledge the algorithms
which are among the most successful for learning deterministic regular stochastic languages (rlips,
alergia, dsai) have not been explored. The results of the recent pautomac competition [Verwer
et al. 2014] suggest that if the underlying natural language phenomenon can be described by
deterministic regular stochastic languages, then these would be fruitful algorithms for computational
linguists to apply.

85
C H A P T E R 4
Learning Non-Regular Languages
Research in the ﬁeld of grammatical inference deals with learnability of languages. In general, the
setup is as follows. Given a family of languages, one speciﬁc language is selected and a set of sample
strings is extracted. The learner now has to identify the language, from the family of languages, that
was used to generate the sample strings.
Formal grammatical inference deals with the question whether speciﬁc families of languages
as a whole can be identiﬁed efﬁciently under certain conditions. This is shown by providing formal,
mathematical proofs of learnability.
While formal grammatical inference provides us with proofs of learnability, there are situations
in which it is unclear what family of languages a speciﬁc grammar belongs to. For instance, consider
the task of learning natural language syntax. We do not have a formal representation of the family of
(formal) languages that corresponds to the family of natural languages. However, approximations of
such families of languages have been made. For instance, syntax may be approximated using context-
free grammars. This leads to functional descriptions, but there are valid structures that cannot be
described using a context-free grammar, and context-free grammars may describe constructions that
do not occur in natural languages.1
In contrast to formal grammatical inference, empirical grammatical inference approaches the
problem of learnability of languages from a different starting point. Whereas formal grammatical in-
ference focuses directly on families of languages, empirical grammatical inference deals with learning
speciﬁc languages. Given a set of sample strings from a speciﬁc language, empirical grammatical in-
ference aims to learn the underlying language. Additionally, if identiﬁcation of the exact underlying
language is not possible, an approximation should be given.
Once an empirical grammatical inference system has been developed that can learn from
a set of “interesting” languages, such as natural language syntax, we can analyze the bias of the
algorithm used in the system. This bias may lead to a formal description of the family of languages the
system can practically learn. Given this information, we have evidence that this family of languages
1. The current consensus is that natural language syntax requires at least some descriptive power of context-free and with
high likelihood (mildly) context-sensitive languages [Huybrechts 1984, Shieber 1985].

86
4. LEARNING NON-REGULAR LANGUAGES
is learnable. A follow-up step may be to formulate a formal proof that this family of languages is
indeed efﬁciently learnable.
Empirical grammatical inference systems can be roughly divided into three groups. This
division is based on which aspects of the model are selected, or ﬁxed, beforehand. Essentially, these
groups form a sliding scale based on the amount of ﬂexibility in the model.
First, the potential structures may be ﬁxed completely, which means that the system should
only learn the parameters in the model that belong to each structure. An example of a model that has
ﬁxed, predeﬁned structures is an n-gram model. In n-gram models, the structure describes substrings
of n symbols that can be combined to indicate which longer strings are found in the language.
Second, we can identify models that have ﬁxed structures, similar to that of the ﬁrst type
of model, but the selection of the structures is more ﬁne-grained. For instance, the structures may
be based on the structure identiﬁed from (sub)trees or context-free grammar rules that have been
extracted from a treebank (this requires the input to contain some information on possible structures).
Once extracted, the structures remain ﬁxed and the model can be adjusted by setting the parameters
for each of the structures.
Finally, there are models that allow for the dynamic selection or creation of structures. The
structures in these models are not hard-coded or predeﬁned using external (linguistic) knowledge
as is the case when the structures are extracted from a treebank. The structures can be added or
removed from the model as the system sees ﬁt. Additionally, the model may need to set parameters
for each of the structures.
In the remainder of this chapter, we will provide several examples of systems that can be found
on the more ﬂexible end of the spectrum of grammatical inference models as just described. We will
ﬁrst introduce a principle that allows identifying regularities in the training strings. The systems
we describe in this chapter are all based on this principle; some systems explicitly start from the
principle, whereas others use the notion more implicitly.
4.1
SUBSTITUTABILITY
Many empirical grammatical inference systems that focus on learning context-free grammars build
on a common underlying principle. However, the way the underlying principle is applied or incor-
porated in the learning systems is different for each system.
4.1.1
IDENTIFYING STRUCTURE
Consider the task of learning syntactic structures in natural language sentences. The system receives a
set of example sentences and should output structure. A whole range of questions arises. For instance,
what should this structure look like? On what basis should the structure be assigned? Should the

4.1 Substitutability
87
()
If 
 is a non-terminal, 
can be replaced by
and vice versa in any context.
FIGURE 4.1: Substitutability in tree structures.
learned structure be what linguists would assign or should the structure at least be linguistically
motivated?
If we assume that the structure assigned to the sentences should correspond to the linguistic
notion of constituents2 then we may be able to use tests for constituency to identify the structure in
the sentences. There are several tests that can be used to test for constituency. Some of these tests are
language speciﬁc. For instance, we may use the fact that several Germanic languages have a verb-
second (V2) word order [Santorini and Kroch 2007], which means that the main verb can be found
in the second position of the sentence. In this case, we may identify one or more words before the
main verb, which, given the fact that the language is V2, will necessarily be a constituent.
Language-independent tests for constituency are hard to ﬁnd. The most well known is that
of substitutability. The underlying idea behind this test is that elements of the same type are
substitutable. In other words, if we know that a particular group of words forms a constituent of
a particular type, we may replace this constituent by any other constituent of the same type. For
instance, if we know that in the sentence What is a family fare the phrase a family fare is a noun phrase,
substitutability means that we can replace this phrase by any other noun phrase and end up with a
syntactically correct sentence. If we also know that the payload of an African Swallow is a noun phrase,
this means that What is the payload of an African Swallow is also syntactically correct.
The idea of substitutability is visualized in Figure 4.1. The type of the constituent (e.g., noun,
verb phrase, etc.), indicated by the small circle, can be expanded in (at least) two ways, depicted by
the triangles. The triangles represent subtrees with the words of the constituent as their yield. If one
of the subtrees headed by the constituent type is found in a particular context, this subtree can be
replaced by another subtree that is headed by the same type.
2. A constituent consists of one or more words that function as a single unit (in the context of a sentence).

88
4. LEARNING NON-REGULAR LANGUAGES
Note that the notion of type in this context may be slightly different from what linguists typi-
cally describe using syntactic types. For example, in English, nouns can be preceded by determiners.
However, only nouns beginning with a vowel can follow the determiner “an.” For substitutability
to be able to describe this phenomenon, information on the ﬁrst vowel of the word should be en-
coded using the type, which is phonological information and arguably should not be described using
syntactic types.
To formalize the concept of substitutability, we use the notion of substring. Alternative
deﬁnitions can be found in van Zaanen [2002a].
Deﬁnition 4.1 (Substitutability)
Substrings u and v are substitutable for each other in L if given
any strings l and r in ∗, lur ∈L ⇔lvr ∈L
The above deﬁnition has led to theoretical work on substitutability, as for example Clark and
Eyraud [2007], Clark and Yoshinaka [2014], and Scicluna and de la Higuera [2014a].
The concept of substitutability is not unlike the Nerode equivalence relation which played
a central role in Chapter 3. As with Nerode equivalence, substitutability allows one to develop
inference rules similar to the ones discussed in Section 3.6. There it was asked: When do we know
strings u and v have the same good tails? Here, it is asked: When do we know strings u and v are
substitutable?
4.1.2
LEARNING USING SUBSTITUTABILITY
The notion of substitutability does not automatically lead to systems that learn languages. The fact
that constituents of the same type are substitutable only indicates the usefulness of the concept of
constituency. If one knows that certain words in a sentence form a constituent of a particular type,
the words may be replaced by another constituent of the same type.
Identifying constituents can be attempted by reversing the idea of substitutability. We know
that constituents of the same type can be replaced, so if we can ﬁnd evidence of the replacement
of constituents (for instance, in several sentences), we may assume that the parts of the sentences
occurring in a similar context are possibly constituents of the same type.
If we have evidence that the sentence a b c d has b c as a constituent of type X, we know there
is a context a X d where X can be replaced by any constituent of type X. If we can then ﬁnd other
sentences that have the same context, the words on the X position in that context may indeed form
a constituent of type X. For instance, if we ﬁnd the sentences What is a family fare and What is the
payload of an African Swallow, we may identify What is to be the context of constituents a family fare and
the payload of an African Swallow, which are both of the same type (noun phrases in this case).

4.2 Empirical Approaches
89
4.2
EMPIRICAL APPROACHES
In the last several years, a collection of empirical grammatical inference systems has been developed.
In this section, we will discuss the best-known, context-free grammar learning systems. All these
systems rely on some application of the notion of substitutability. Even though we try to provide a
rather complete overview, there exist empirical grammatical inference systems that focus on speciﬁc
properties, such as cognitive plausibility, and not simply on learning the best ﬁtting grammar given
a collection of strings. These specialized systems are not explicitly described here.
4.2.1
EXPANDING AND REDUCING APPROACHES
Even though all empirical grammatical inference systems that learn context-free grammars rely,
implicitly or explicitly, on the idea of substitutability, we can identify two distinct approaches
to how the complete search space is traversed. The ﬁrst approach starts from the sample strings
and generalizes the grammar by identifying regularities within the strings. We call this approach
expanding as the grammar expands from a tight ﬁt of the training data to more general grammars
that capture a larger language (meaning that more strings are part of the language, even though the
grammar may be smaller).
The second approach is called reducing. These systems start with the assumption that all
strings are possible in the language. Given the valid strings in the training data, the collection of
valid strings is reduced.
These two approaches are discussed in detail by van Zaanen and van Noord [2012] and are
related to comparable approaches in the context of learning ﬁnite-state machines. The expanding
approach corresponds to the model of the state-merging approach, whereas the reducing approach
coincides with the model of the state-splitting approach.
4.2.2
SUPERVISED AND UNSUPERVISED APPROACHES
Empirical grammatical inference systems may also be grouped based on the information contained
in the input. There are many possible values for this parameter.
To illustrate the parameter that indicates the amount of information, we may consider two
extremes, even though, typically, these extremes do not occur in practice. On one end of the spectrum,
no information whatsoever about the language is given. In this case it is extremely hard to learn, as no
assumptions can be made based on the data. On the other end of the spectrum, the full information
about the language is provided. This means that no learning is required at all, as everything is already
known.
Typically, learners of non-regular languages receive a set of strings that are sampled from the
language to be learned. This holds for all systems described in Section 4.2.4. However, alternative

90
4. LEARNING NON-REGULAR LANGUAGES
approaches receive other representations of the strings from the language. Examples of such repre-
sentations include unlabeled tree structures, which are also called skeletons [de la Higuera 2010],
partial tree structures [Sakakibara and Muramatsu 2000], or full tree structures [Charniak 1993].
We will concentrate on learners that receive data from a plain string presentation.
4.2.3
WORD-BASED AND POS-BASED APPROACHES
From a natural language point of view, a plain string presentation may mean different things. Em-
pirical grammatical inference systems that work on real-world data, different linguistic annotation
layers may serve as “plain strings.” For instance, strings of morphemes, written words, or part-of-
speech (pos) tags3 as symbols in the strings lead to representations of natural language sentences. In
this case, the learned grammars represent syntactic structures. Using letters or phonemes as symbols
allows for the learning of morphological structure in natural language words.
The systems described below have been developed to deal with learning natural language syn-
tax. Some systems start with tokenized (written) language in the form of words or tokens, and others
are based on strings of pos tags. The major difference between these two presentations is the size of
the vocabulary. Presentations consisting of strings of tokens may lead to very large vocabularies. For
instance, the Google Web 1T 5-gram Version 1 corpus4 is based on 1,024,908,267,229 words of
running text. This leads to a vocabulary consisting of 13,588,391 unique words (not counting words
that occur less than 200 times). In contrast, presentations resulting in strings of pos tags typically
have much smaller vocabularies. The Brown tag set consists of 87 unique tags [Francis and Kuˇcera
1982], the C5 tag set used in the claws project has 61 tags [Garside et al. 1997], and the Penn
Treebank tag set has 45 tags [Marcus et al. 1993]. A more ﬁne-grained tag set is, for instance, the tag
set used for the Dutch D-Coi corpus [van Eynde 2005]. This tag set consists of 320 tags (grouped
in 12 main tag groups). This is still orders of magnitude smaller than the size of the vocabulary of
words.
4.2.4
DESCRIPTION OF EMPIRICAL SYSTEMS
In this section, several empirical grammatical inference systems are described in some detail. All of
these aim to learn context-free grammars based on unstructured strings of either tokens (words) or
pos tags. Whereas the actual implementation is different in each system, the underlying approach
is comparable. Each system effectively makes use of the notion of substitutability. Some systems do
this explicitly, whereas others rely on the statistics of certain symbols occurring in similar contexts.
3. Obviously, these representations need to be learned as well before they can be used. For instance, phoneme, morpheme, and
word boundaries will need to be identiﬁed from the sound signal, and pos tags already describe some syntactic information.
4. http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html

4.2 Empirical Approaches
91
TABLE 4.1: Matrix for John walks, John sees Mary and Mary walks
(.) walks
John (.)
(.) sees Mary
. . .
contexts
John
x
x
. . .
walks
x
. . .
Mary
x
. . .
sees Mary
x
. . .
...
...
...
...
...
expressions
EMILE
emile is an empirical grammatical inference system that is based on the explicit notion of substi-
tutability. This system originates from research in the area of formal grammatical inference. Adriaans
[1992] showed that shallow context-free grammars (in the shape of categorial grammars) are pac
learnable under simple distributions. Based on the theoretical research, a practical implementation
has been built [Vervoort 2000, Adriaans and Vervoort 2002].
The system identiﬁes context-free grammars that are context-separable and expression-
separable. A grammar is context- or expression-separable if for each non-terminal in the grammar
respectively a characteristic context or expression can be found. A context is characteristic, if it
only appears with expressions of one particular type and an expression is characteristic if it only
occurs within a context of a particular type. These grammars correspond to the family of shallow
context-free grammars used in the formal proofs [Vervoort 2000].
emile starts with a collection of plain sentences (i.e., strings of words). These sentences are
analyzed to identify possible expressions and contexts. All combinations of contexts and expressions
are stored in a matrix containing the corresponding co-occurrence information. A co-occurrence
matrix has one dimension with possible contexts and another dimension with possible expressions.
Table 4.1 is part of such a matrix given example sentences John walks, John sees Mary, and Mary walks.
Analyzing the matrix, emile can identify characteristic expressions and context by comparing
either entries in the rows or in the columns. This process is called one-dimensional clustering. In the
case of the matrix of Table 4.1, it ﬁnds the cluster [John (.), {walks, sees Mary}], which is a characteristic
context, as its expressions only occur in the same contexts. More complex types of clustering can be
deﬁned, such as two-dimensional clustering, which also takes contexts with ambiguous types into
account [Vervoort 2000].

92
4. LEARNING NON-REGULAR LANGUAGES
Given the characteristic contexts and expressions, grammar rules are created. For each ex-
pression e belonging to context T , a grammar rule T →e is introduced. Also, all occurrences of
expressions e in the grammar are replaced by non-terminal T . A start symbol S is introduced and a
grammar is created that makes sure all sentential contexts are reachable: S →T .
Once the grammar rules are created, the same process is repeated. Due to the abstraction
over the expressions in the grammar rules, new characteristic contexts and expressions can be
identiﬁed, leading to deeper hierarchical grammar rules. For instance, if John and Mary are identiﬁed
as expressions of type E, all occurrences of John and Mary are replaced by E. This means that sentences
such as John sees Mary and Mary slaps John, become E sees E and E slaps E. In this situation,
expressions sees and slaps share the same context, which was not the case in the original sentences.
Based on this information, the expressions sees and slaps also receive the same type, which may
again lead to further generalizations.
Originally, emile has been designed to show formal learnability of the family of shallow
context-free languages. As such, the system is designed as an algorithm with known, formal
properties. This does not necessarily mean that the algorithm leads to a practical system that can
learn languages based on real-world data. The practical implementation of emile [Adriaans and
Vervoort 2002] has a wide range of parameters; for instance, to restrict the size of the matrix or the
number of comparisons made to identify the clusters. The choice of the settings of the parameters
has an impact on the results of the system as well as the practical runtime and memory requirements
of the system.
Alignment-Based Learning
Another system based on the idea of substitutability is Alignment-Based Learning (abl) [van
Zaanen 2000a, 2000b, 2000c, 2002a, 2003]. This system is presented as a framework consist-
ing of a pipeline of phases. Based on the framework, a working system corresponds to selecting
a speciﬁc module for each of the phases. A practically usable system is available [van Zaanen
2002b].5
As illustrated in Figure 4.2, abl consists of two main phases. An optional third phase
may be added. The ﬁrst phase is called alignment learning, which generates a hypothesis space
given a collection of strings. (In the implementation, an intermediate phase, called clustering, can
be distinguished. This phase groups common non-terminals within the hypothesis space). The
hypothesis space serves as the input to the second phase, selection learning. This phase selects the best
constituents from the hypothesis space, which leads to a structured version of the input collection
5. http://ilk.uvt.nl/menno/research/software/abl

4.2 Empirical Approaches
93
Alignment
learning
Selection
learning
Grammar
extraction
Hypothesis
space
Structured
corpus
Grammar
Hypothesis
space
Structured
corpus
Corpus
FIGURE 4.2: Schematic overview of phases in abl.
I need (X1 a dinner during the ﬂight)X1
I need (X1to return on Tuesday)X1
FIGURE 4.3: Alignment of two sentences and the identiﬁcation of hypotheses.
of strings. If required, a third phase, called grammar extraction, may be used to extract an explicit
grammar from the structured output.
The system is designed to work on plain sentences, or strings of words. The alignment learning
phase searches for regularities in the input. Each sentence in the training data is compared against
each other sentence. Pairs of sentences are aligned, which indicates equal and unequal parts of the
sentences. This is illustrated in Figure 4.3. According to the idea of substitutability, the unequal parts
of the sentences are considered possible constituents, which are called hypotheses. Each sentence has
an associated hypothesis space that contains the hypotheses for that sentence.
Each sentence is compared against all other sentences in the collection. For each of the
comparisons, the alignment is done on the plain sentences. The hypotheses in the hypothesis space
are not taken into account during the alignment. If hypotheses are identiﬁed, these are added to the
already existing hypotheses in the hypothesis space.
The alignment of sentences can be done in different ways, each leading to a different alignment
learning “module.” Currently, modules using edit distance [Wagner and Fischer 1974] or sufﬁx trees
[Ukkonen 1995, Geertzen 2003, Geertzen and van Zaanen 2004] have been implemented. The edit
distance–based modules align all sentences in pairs, leading to O(|C|2) computation time, with |C|
the number of sentences in the corpus. The edit transcript of pair of sentences is used to identify the
unequal parts (which consist of all edit operations except the match operation).

94
4. LEARNING NON-REGULAR LANGUAGES
As an alternative to the edit distance–based modules, modules that search for words occurring
in multiple sentences using sufﬁx trees [Ukkonen 1995] have been implemented. A sufﬁx tree is built
that contains the entire collection. Branches in the sufﬁx tree indicate positions in sentences that
have the same left-hand side, but differing right-hand sides. Similarly, branches in the preﬁx tree
(which in this context is the sufﬁx tree from the reversed sentences) indicate positions in sentences
that have the same right-hand side, but differing left-hand sides. Combining the branch points leads
to the identiﬁcation of equal and unequal parts in the sentences. The algorithms based on the sufﬁx
tree representation of the sentences lead to different results compared to the edit distance–based
modules. The main advantage of using the sufﬁx tree–based modules is that they can handle larger
collections, as the sufﬁx trees can be built in linear time.
Since hypotheses are added to the hypothesis space of a sentence, it may be the case that two
or more hypotheses with the same opening and closing brackets are added separately. The clustering
step makes sure that the non-terminals belonging to different hypotheses that share the same opening
and closing brackets are merged and the different non-terminals are merged throughout the entire
collection.
Adding hypotheses to the hypothesis space without taking the existing hypotheses in the
hypothesis space into account may lead to overlapping hypotheses. Hypotheses overlap if the opening
bracket of hypothesis x is between the opening and closing bracket of hypothesis y, while the closing
bracket of hypothesis x is after the closing bracket of hypothesis y. For instance, in (X1 a (X2 b)X1
c)X2 the hypotheses of type X1 and X2 overlap.
Overlapping hypotheses are unwanted if the underlying grammar is considered context-free.
In that case, the resulting structure after applying the phases should be seen as a tree structure, or
derivation using the learned grammar. Generating sentences based on a context-free grammar leads
to tree structures.
The aim of the selection learning phase is to select constituents (which are already present as
hypotheses in the hypothesis space) in such a way that none of the remaining constituents overlap.
Currently, several selection learning methods have been implemented. Selection learning
modules exist that select constituents chronologically or based on statistics. With the chronological
selection, earlier hypotheses are considered correct, or in other words, if an alignment learning
module tries to add a hypothesis to the hypothesis space that overlaps with an existing hypothesis,
it is not added.
The statistics-based selection learning methods identify the most likely correct hypothesis.
For each of the hypothesis, a probability is computed according to properties of the hypothesis, such
as the number of occurrences of the words contained in the hypothesis. The most likely structure is
then computed using a viterbi-style optimization [Viterbi 1967].

4.2 Empirical Approaches
95
S
The
dog
John
sees
walks
a
cat
E
Mary
1
1
1
1
1
2
2
2
3
3
3
3
FIGURE 4.4: Initial adios graph for John sees a cat, John walks, and The dog sees Mary.
ADIOS
A system that borrows ideas from ﬁnite-state automata and the notion of substitutability is called
adios (Automatic Distillation of Structure) [Edelman et al. 2004, Solan et al. 2005]. This system
starts by representing the sample sentences from the language as a graph. The graph is compressed,
which resolves non-determinism. Next, adios searches for signiﬁcant patterns, which correspond
to substitutable parts in the sentences. These structures are considered to be constituents.
During the ﬁrst phase, a directed graph (similar to a ﬁnite-state machine) is built with unique
start and end nodes. For each sample sentence, a path is created. Each unique word in the sentence
is represented using a node. A sentence is represented by connecting nodes with the corresponding
words in the sentence. Each sentence leads to a new path, so edges are not shared between sentences.
Figure 4.4 shows an example of such a graph using the sentences: John sees a cat, John walks, and
The dog sees Mary.
Because nodes are shared between sentences but edges are unique for each path, words and
hence parts of sentences that can be found in several sentences are automatically aligned with
each other. This makes the identiﬁcation of equal and unequal parts (essential information for
substitutability) easy.
The second phase segments the graph by identifying subpaths that are shared by a signiﬁcant
number of partially aligned paths. These subpaths correspond to expressions (in a variety of contexts)
which are substitutable. The subpaths are scored using the mex (Motif Extraction) criterion. The
identiﬁcation of subpaths continues until no more signiﬁcant paths can be found.
The mex criterion relies on probabilities that measure changes in in- and out-degree of nodes.
The changes in in- and out-degree indicate words on the boundary of substitutable expressions. For

96
4. LEARNING NON-REGULAR LANGUAGES
instance, a node, say, n, with a large in-degree indicates that many paths use node n. If the paths
going out of node n go to many other nodes then that might indicate that node n is the end of a
pattern. The idea is that a collection of paths following the same nodes indicate a subpath, or pattern,
that occurs in a range of contexts. The moment the collection of paths spread out over a number of
nodes, this indicates the end of the pattern.
The computation of the signiﬁcant patterns is done as follows. First, probabilities are com-
puted that measure the in- and out-degree of nodes. This computation is done separately going from
left to right and right to left (indicated by PR and PL, respectively) to ﬁnd the start and end points
of signiﬁcant patterns. We follow the deﬁnitions of Kunik et al. [2005].
First, we deﬁne probabilities over the out-degree of a node ei:
p(ei) = # paths leaving ei
total # paths
p(ej|ei) = # paths going from ei to ej
total # paths going out of ei
.
The relative probability of the outgoing paths can be extended to longer paths. PR(ei; ej)
indicates the probability of the outgoing paths going from ei to ej:
PR(ei; ej) = p(ej|eiei+1 . . . ej−1) = # paths from ei to ej
# paths from ei to ej−1
.
PR describes the probability of paths going to the right. In the same line, PL can be deﬁned,
which describes similar probabilities, but go from right to left:
PL(ej; ei) = p(ei|ei+1ei+2 . . . ej) = # paths from ej to ei
# paths from ej to ei+1
.
Essentially, PR and PL are normalized in- and out-degrees. The interesting nodes in the graph
display a drop in probability when moving through the graph. This drop is measured by D, which
describes the relative change of the probability between two nodes:
DR(ei; ej) = PR(ei; ej)
PR(ei; ej−1)
DL(ej; ei) = PR(ej; ei)
PR(ej; ei+1).

4.2 Empirical Approaches
97
w1
w2
w3
w4
1
1
1
1
1
2
2
2
2
2
w1
n1
1
1
1
2
2
2
3
w2
w3
3
3
3
3
3
FIGURE 4.5: Above, the initial adios graph. Below, the two graphs after rewriting the pattern of nodes w2,
w3, w4 (indicated by the dotted line in the initial graph) as node n1.
To decide when a node may serve as the starting point or ending point of a pattern, a parameter
η is introduced. If DR(ei; ej) < η, ej−1 is used as the end of a pattern. Similarly, if DL(ej; ei) < η,
ei+1 is used as the begin of a pattern.
A problem with this approach is that the probabilities (and hence the D values) are computed
based on potentially a very small number of paths going through edges. To reduce this problem,
signiﬁcance values are computed. This allows for an additional setting (α) that indicates a form of
certainty. Typical settings for η and α are 0.9 and 0.01, respectively.
Once a start and end point of a pattern is found, a new node is created that encapsulates the
nodes within the pattern. All paths going through all nodes in the pattern now go through this newly
created node. For instance, in Figure 4.5, if nodes w2, w3, and w4 are found to be a pattern, these are
replaced by a new node n1. Essentially, node n1 is now a hierarchical node, as it contains nodes w2,
w3, and w4. Note that when paths do not go through all nodes of a pattern, these paths are retained
separately. Path 3 is such an example in Figure 4.5.
CCM and DMV
The Constituent-Context Model (ccm) and Dependency Model with Valence (dmv) are two
different models that both focus on different aspects of syntactic structure [Klein 2004, 2005]. By
combining the results of both models, results improve. We will discuss the ccm model ﬁrst, followed
by dmv and ﬁnally, brieﬂy, the combination of the two models.
CCM. ccm [Klein and Manning 2002, 2005] builds on the idea of substitutability. The aim is
to identify expressions, which are called spans in ccm, in contexts. Instead of strict decisions on

98
4. LEARNING NON-REGULAR LANGUAGES
whether spans are constituents (as in emile) or hypotheses (as in abl), ccm assigns a measure of
likelihood to each span. This probability is deﬁned as Pspan(s|t) where s is the span, i.e., a string of
part-of-speech tags, and t either has the value “constituent” or “distituent” indicating whether the
span is a constituent or not.
Similarly to the deﬁnition of the probability of a span, ccm deﬁnes the probability of a context
c as Pcontext(c|t). In this case, t (which is again either “constituent” or “distituent”) describes whether
the expression contained in the context is a constituent or a distituent.
The probabilities for spans and contexts are used to deﬁne the probability of a bracketing B
on a string s, P (s, B). A bracketing corresponds to a tree structure deﬁned over the string. ccm
starts with a uniform distribution Pbin over all bracketings that correspond to binary tree structures.
This probability is deﬁned as
P (s, B) = Pbin(B)P (s|B).
P(s|B) can be expanded as
P (s|B) = i,j:i≤jPspan(sij|Bij)Pcontext(si−1, sj|Bij),
where sij is the span over the substring of symbols starting at position i and ending, not including, at
position j. The context (si−1, sj) consists of the symbol before the start of the span (i.e., si−1 which
is before sij) and the symbol following the span sj. To make sure a context can be deﬁned when
the span starts at the beginning of a string or ends at the end of a string, speciﬁc sentence boundary
markers are added at the begin and end of the sentence. Bij has the value “constituent” if the span
over i and j is in the bracketing B and “distituent” otherwise.
Next, the probabilities are re-estimated using the Expectation-Maximization (em) algorithm
[Dempster et al. 1977]. The variables that need to be estimated () are the probabilities Pspan(s|t)
and Pcontext(c|t) and also the probability of the bracketing P (B). In ccm, the probability of the
bracketing is not changed. It is set beforehand using the uniform distribution over all binary
bracketings.
The em algorithm consists of two steps. The E step computes the likelihoods of P (B|s, )
given the current values of the parameters in . The M step searches for new settings of the
parameters ′ that maximizes BP (B|s, ) log P (B, s|′). The em algorithm iterates over these
two steps until the values of the parameters converge.
To start the em process, initial values for P (B|s, ) are needed. Klein and Manning [2005]
indicated that using the uniform distribution over the binary trees has the problem that the trees are
all balanced. To allow unbalanced trees to be identiﬁed during the em process, binary trees are build

4.2 Empirical Approaches
99
Root
FIGURE 4.6: Example dependency parse without terminals, which are visualized as dots.
to the left and right side of a randomly chosen split point. This yields a distribution Psplit, which
has a preference for somewhat unbalanced trees.
DMV. The Dependency Model with Valence (dmv) is an unsupervised dependency parser [Klein
2004]. Instead of a one-to-many mapping between non-terminals (on the left-hand side) and termi-
nals or non-terminals (on the right-hand side) in, for instance, context-free grammars, dependency
models are a one-to-one mapping. Essentially, dependencies describe head-dependent relationships
between the words in a sentence. This results in directed acyclic graphs; for instance, like the one
depicted in Figure 4.6.
dmv aims to learn a dependency structure starting from the root position, recursively adding
new structure until all words in the sentence are covered. First, the dependent of the root node is
selected. From this dependent, the left and right subgraphs are added in a depth-ﬁrst manner. If no
more dependent can be found, a stop condition is selected.
Following the notation of Klein [2004], the task is to learn a dependency structure D. For each
word h, the left and right dependents of h are deﬁned by depsD(h, l) and depsD(h, r), respectively.
The probability of the dependency structure D(h), which has h as its root, can be deﬁned as
P (D(h)) = dir∈{l,r}a∈depsD(h,dir)Pstop(¬stop|h, dir, adj)
Pchoose(a|h, dir)P(D(a))
Pstop(stop|h, dir, adj).
Here, Pstop(stop|h, dir, adj) describes the probability that h has no more dependents; h is the
head, dir describes the direction, and adj describes adjacency (true if in direction dir an argument
has been generated). The probability of the selection of a dependent a as the dependent of h is
described by Pchoose(a|h, dir), and the probability of the dependency structure that has a as its root
is P (D(a)).
The three parameters (Pstop, Pchoose, and Proot, which describe the probability that a speciﬁc
word is pointed to by the root node) are re-estimated using the em algorithm, just like in the ccm
model.

100
4. LEARNING NON-REGULAR LANGUAGES
CCM and DMV. The ccm and dmv models can be combined into a new model that assigns
structure based on the structure found by both systems [Klein 2004]. The probability of a tree
structure in the combined system is the product of the probabilities of the separate systems.
It turns out that the combination of ccm and dmv leads to better results compared against
the results of the separate systems. This illustrates that the structures identiﬁed by ccm and dmv
are complementary.
This direction of research (in particular learning dependency relations) has received consid-
erable attention the last few years. For instance, Spitkovsky [2013] described a range of systems,
Headden III [2012] incorporated lexical features, and Naseem et al. [2010] used language indepen-
dent rules, like language universals.
U-DOP
The u-dop system [Bod 2006a, 2006b] relies on Data-Oriented Parsing (dop) [Bod 1998, Bod et al.
2003] as the underlying formalism. u-dop starts by generating all possible (binary) tree structures
and uses the dop statistical model to decide which tree structures to keep. Because u-dop relies so
much on the dop statistical model, we will discuss dop ﬁrst.
dop is a grammar formalism that is structurally equivalent to context-free grammars. The
difference lies in the fact that the statistical model is stronger compared to stochastic context-free
grammars. Instead of assigning probabilities to context-free grammar rules, dop assigns probabilities
to elementary subtrees. Context-free grammar rules form a subset of all elementary subtrees of a
tree structure. Elementary subtrees are subtrees for which on all levels in the tree either none or all
children are present. This includes the entire tree structure. Figure 4.7 gives an example of a tree
structure (the left-most tree) and all its elementary subtrees (which includes the full tree structure).
During parsing, elementary subtrees are combined using left-most substitution. This process
combines two elementary subtrees by merging the left-most non-terminal in one elementary tree and
the root non-terminal of the other node. A particular property of parsing using elementary subtrees
is that there may be several ways to generate the same parse (which corresponds to a tree structure
over the sentence being parsed). One such way is called a derivation and the resulting structure is
NP
V
VP
NP
V
VP
VP
NP
S
NP
S
NP
V
VP
NP
PN
NP
PN
S
VP
NP
PN
S
FIGURE 4.7: Elementary tree structures in dop.

4.2 Empirical Approaches
101
NP
V
VP
NP
V
VP
NP
S
NP
V
VP
NP
PN
NP
PN
S
VP
NP
PN
S
FIGURE 4.8: Two derivations using left-most substitution that lead to the same parse.
called a parse. Figure 4.8 illustrates two derivations that lead to the same parse using the elementary
subtrees of Figure 4.7.
To compute the probability of a derivation, dop follows the same principle as is used in
probabilistic context-free parsing. The probability of a derivation is the product of the probabilities
of the separate rules ti (elementary subtrees in the case of dop) being used:
P (t1 ◦t2 ◦. . . ◦tn) = n
1P (ti),
where ◦denotes the process of combining elementary subtrees using left-most substitution (or the
application of context-free grammar rules in the case of context-free parsing).
The probability of an elementary subtree is computed using the maximum likelihood estimate
of a tree over all trees with the same root symbol:
P (t) =
|t|
t′:r(t′)=r(t)|t′| ,
where r(t) returns the symbol that can be found at the root of tree t. These probabilities are smoothed
using Good–Turing smoothing [Good 1953].
Since there may be multiple derivations that lead to the same tree structure, the probability of
a parse is computed by combining the probabilities of all derivations D that lead to the same parse T :
P (T ) = D derives T P (D).
u-dop relies heavily on the strong statistical power of the dop framework. The advantage of
using elementary subtrees as items receiving probabilities is that in a probabilistic way, long-distance
dependencies may be modeled. The disadvantage of the elementary subtrees is that given a decent
size treebank, exponentially many elementary subtrees may be generated. This requires decisions,
such as the use of Monte Carlo sampling [Hammersley and Handscomb 1964], to estimate the

102
4. LEARNING NON-REGULAR LANGUAGES
probability of a derivation to practically limit computational effort. Alternatively, one may try to
reduce the exponential size of the grammar into polynomial size using pcfg reduction techniques
[Goodman 1996, 2003].
u-dop starts by generating all possible binary tree structures on a set of example sentences.
Based on these structures, all elementary subtrees are generated. The probabilities of these elemen-
tary subtrees are estimated using the em algorithm.
4.2.5
COMPARISON OF EMPIRICAL SYSTEMS
The systems that have been discussed can be compared according to different aspects. It turns out
that two groups of systems can be identiﬁed. The systems within a group approach the problem in a
similar way, whereas the two groups each have their own approach. Table 4.2 provides an overview
of different properties of the two groups of systems. The ﬁrst group consists of emile, abl, and
adios. The second group contains ccm/dmv and u-dop.
The ﬁrst group consists of systems that start from a collection of strings of words, whereas the
second group requires strings of pos tags. It has to be noted that, for instance, Klein and Manning
[2002] also reported results on a dataset on which pos tags have been induced in an unsupervised way.
The systems in both groups rely on the Zipf distribution that can be found in natural language
[Zipf 1929]. This distribution states that a small selection of words occurs very frequently, whereas
many words only occur sporadically. In the systems that learn structure on strings of words, the
frequently occurring words serve as identiﬁers or markers in contexts. Based on the frequently
occurring contexts, expressions can be identiﬁed. The expressions consist of words that can be found
in the long tail, i.e., words that only occur infrequently.
In contrast, the systems that take string of pos tags as input rely more on the probabilistic
properties of frequently co-occurring symbols. These systems, which both use em estimating to
identify useful probabilities, require patterns that occur frequently enough to allow for reliable
statistics. A pos tag can be seen as an equivalence class for a group of words that all serve the
TABLE 4.2: Overview of properties of the two groups of
empirical systems
Group 1
Group 2
Systems
emile, abl, adios
ccm/dmv, u-dop
Input
Strings of words
Strings of pos tags
Approach
Expanding
Reducing

4.3 Issues for Evaluation
103
same (syntactic) function, such as nouns or verbs. By grouping all these words together, the problem
of computing probabilities for unseen or very infrequently occurring words is reduced.
Another property in which the groups differ is the approach. The systems in group 1 slowly
introduce structure only when enough evidence has been found. The reason for this is that only when
useful contexts and corresponding expressions are found the systems can start identifying structure.
The systems in group 2 are greedy. They start by assigning all possible structures and based on the
frequently occurring structures readjust probabilities. Structures that are not useful will receive a very
low probability and will not be retained.
Comparing the systems within group 1, emile and adios are quite similar. Both systems
introduce structure only when enough evidence if found (either by frequency of the context or by
signiﬁcant paths through the nodes). abl always introduces structure whenever it can, but due to
the greediness of the ﬁrst phase, a second phase that aims to remove incorrect structure is required.
ccm and u-dop both identify structure based on the probabilities of all possible structures.
ccm is based on the context-free grammar formalism, whereas u-dop relies on the statistically
stronger dop formalism.
4.3
ISSUES FOR EVALUATION
Formal grammatical inference results are in the form of mathematical proofs. As such, the evaluation
of the work in that area is done by examining the formal proofs. The algorithms used in the proofs
are typically not implemented and used on real data, but they provide evidence that a particular
family of languages can be learned efﬁciently using the algorithm.
Empirical grammatical inference, on the other hand, starts from the notion that we know
that a particular language (coming from a speciﬁc family of languages) is efﬁciently learnable, but a
formal description of this family of languages is not necessarily or typically available. For instance,
humans are able to learn languages from the family of natural languages, but we do not have a formal
description of this family. Also, so far, no family of formal languages is known that can be shown to
be formally learnable under any learning setting and to contain the family of natural languages.
Note that empirical grammatical inference deals with the evaluation of a learning system based
on data from a speciﬁc language that is known to be a member of a family of languages. This type
of evaluation does not result in (mathematical) proof of the learnability of families of languages.
However, the aim is to investigate in how far a learning system can identify one or more languages.
Based on this information, the algorithm itself may be investigated either to improve the results or
to form the basis of formal grammatical inference proofs.
To be able to compare different empirical grammatical inference systems and to know how
far away the output of a system is from the target language, an evaluation method is needed.

104
4. LEARNING NON-REGULAR LANGUAGES
Learned
grammar
Plain
corpus
Human
evaluator
GI system
Results
FIGURE 4.9: Schematic representation of the looks-good-to-me evaluation approach.
Several evaluation approaches are found in the literature. van Zaanen [2002a, pp. 58–62] provides
an overview of three different evaluation approaches. In van Zaanen et al. [2004] a fourth approach
is identiﬁed as well. Another overview of the four approaches can be found in van Zaanen and de la
Higuera [2011]. We will describe each of them here in turn, together with a brief discussion of their
advantages and disadvantages.
4.3.1
LOOKS-GOOD-TO-ME APPROACH
The looks-good-to-me approach is a subjective evaluation method. The approach is visualized
in Figure 4.9. The grammatical inference system under consideration is applied to an unstructured
collection of sequential data, a corpus. The output of the system, which can either be in the form of
a grammar or a treebank version of the input data, is then evaluated manually. The person (typically
the developer of the system) performing the evaluation investigates the output of the system. The
evaluation may focus on speciﬁc constructions such as recursion or certain grammar rules, but it
may also be a visual inspection of the entire grammar or tree structures to check for coverage. If
the structures are found, the system is considered to be performing well. The evaluation is normally
described in the form of textual comments.
Advantages
The looks-good-to-me approach has several advantages. First, only a collection of plain, unstruc-
tured strings is required in addition to access to an expert who can evaluate the task. This collection
may be extracted from an existing dataset or the sentences may be created (semi)manually. This is
particularly useful if a structured version of data suitable for the task (which would allow for al-
ternative evaluation approaches) is not available or only small amounts can be found. In contrast,
unstructured data is more often available. Natural language corpora without syntactic annotation,
for instance in the form of tree structures, are more readily available than comparable treebanks. For
many natural languages or linguistic domains, no treebanks are available at all.
Second, since the output of the system is analyzed by an expert, special attention may be
given to speciﬁc syntactic constructions. The expert evaluator may simply disregard constructions
that the system is not supposed to learn and only focus on a subset of constructions, such as the
correct identiﬁcation of noun phrases, or speciﬁc types of recursion.

4.3 Issues for Evaluation
105
Disadvantages
The evaluation using the looks-good-to-me approach depends heavily on the expert evaluator.
Due to this inﬂuence, the looks-good-to-me approach has several disadvantages. First, the
evaluator should try to provide results that are as objective as possible. However, as the evaluator is
often the developer of the system, the evaluator may quickly ﬁnd that the output looks good, hence
the name of the approach.
Second, the comparison of multiple systems is difﬁcult. The results of the evaluation are
typically of qualitative nature. The evaluator describes the interesting aspects in natural language.
This means that the evaluation of the output of several systems may still be feasible when the systems
are compared in parallel at the same time, but when the outputs are compared by different experts
or when the output of one system is compared with an existing evaluation, the results may be much
less reliable.
Finally, if an evaluator only concentrates on the ability of a system to learn speciﬁc construc-
tions, this evaluation only holds for those speciﬁc constructions. In other words, the evaluation is
not usable as an overall evaluation of the system.
4.3.2
REBUILDING KNOWN GRAMMARS
The goal in grammatical inference is to design a system that learns a compact representation for a
language given some example strings. In other words, the aim of the task is to learn a grammar. The
rebuilding known grammars evaluation approach starts from the idea that there is an underlying
grammar that describes the language that needs to be learned. Given this grammar, example strings
are generated, which serve as the input to the grammatical inference system. The output of the
system, in the shape of a grammar, can then be compared against the original grammar. This entire
process is shown in Figure 4.10.
Learned
grammar
Plain
corpus
String
generator
Comparison
system
GI system
Grammar
Results
FIGURE 4.10: Schematic representation of the rebuilding known grammars evaluation approach.

106
4. LEARNING NON-REGULAR LANGUAGES
When using the rebuilding known grammars evaluation approach, several choices have
to be made. First, the actual grammar that will be used for the evaluation needs to be selected. Often,
a grammar is selected from a set of well-known grammars [Cook et al. 1976, Hopcroft et al. 2001],
such as the parenthesis language or Dyck language, which contains sentences consisting of balanced
open and close parentheses. The results of the system on these well-known grammars illustrate how
well the system works, because the results can be compared against results of previous evaluations
(of other systems).
Second, a method for generating strings based on the grammar needs to be picked. Different
methods for generating strings may lead to different probability distributions over the generated
strings. The generative process should at the very least use all of the grammar rules at some point.
However, the actual choice of how the strings are to be generated may be more complex, especially
when probabilistic grammars are to be learned.
Third, the learned grammar needs to be compared against the original grammar. There are
two general approaches of doing this. If the languages (i.e., the set of acceptable strings) that can
be generated by the grammars are compared, the evaluation measures weak equivalence or language
equivalence. If the shape of the grammar rules is taken into account as well (which comes down to
comparing the tree structures generated by parsing the strings using the grammars), strongequivalence
or structural equivalence is measured.
Advantages
The rebuilding known grammars approach has several advantages. First, no sequential data is
required at all, as it is generated by the grammar. This means that if a system requires more input
data, this can be generated from the grammar on the ﬂy.
Second, the inﬂuence of the evaluator is reduced (in two places). As the data is generated by
an automated process, the data cannot easily be tuned to the problem. Also, the comparison of the
output can be done in a more objective way, as two grammars can be compared. This leads to a more
objective evaluation (compared to the looks-good-to-me approach).
Disadvantages
Even though the rebuilding known grammars approach solves some of the problems of the
looks-good-to-me approach, this approach has its own problems. First, the process that generates
the training data may still inﬂuence the evaluation. As mentioned earlier, the generation process
should make sure all grammar rules are used at some point, but additional requirements, such as
probabilistic properties of the language (if modeled) should be considered as well.
Second, the comparison of grammars (or their languages) is problematic. With more powerful
families of languages, the problem of language equivalence is undecidable. Intuitively, inﬁnite

4.3 Issues for Evaluation
107
languages require an inﬁnite amount of comparisons before language equivalence can be established.
For small grammars, humans may still be able to do a deep comparison of the grammar rules
to show whether two grammars are equivalent. However, when more complex (and interesting)
grammars, such as wide-coverage natural language grammars, are used, this poses problems. Note
that comparing the grammars by generating strings based on one language and analyzing the
generated strings with the other grammar may provide language equivalence. However, this means
that a proper generation process is needed (as described above) and for more complex grammars, a
large amount of strings need to be generated and analyzed.
Third, the rebuilding known grammars approach can only be applied when the under-
lying grammar of a language is actually known. This is feasible with artiﬁcially created grammars,
which are often used with this evaluation approach. However, this is more difﬁcult when consid-
ering natural languages. Some wide-coverage natural language grammars exist, but the generation
of training data as well as measuring equivalence of a learned grammar and the original grammar is
problematic.
4.3.3
COMPARE AGAINST A TREEBANK
The compare against a treebank evaluation approach starts from the notion that grammars can
be used to structurally annotate sentences. Instead of measuring the learned grammar (which may be
difﬁcult), this approach measures the effectiveness of learning the structure in the form of trees. The
entire process is illustrated in Figure 4.11. Since van Zaanen and Adriaans [2001], this approach
has been one of the main evaluation approaches in the area of empirical grammatical inference.
To perform a compare against a treebank evaluation, a treebank (i.e., a collection of
structured sentences) is required. There are several ways to build such a treebank. For instance, the
treebank may be generated from an artiﬁcial grammar. Alternatively, it may be annotated manually
Learned
treebank
Plain
corpus
Extract
sentences
Compare
treebanks
Base
treebank
GI system
Results
FIGURE 4.11: Schematic representation of the compare against a treebank evaluation approach.

108
4. LEARNING NON-REGULAR LANGUAGES
or semiautomatically. This allows for the evaluation of languages for which the underlying grammar
is not (fully) known.
From the treebank, the plain sentences are extracted, resulting in a plain corpus. No structural
information (apart from the order in which the words occur) is present in the plain corpus. This plain
corpus serves as the input to the grammatical inference system under evaluation.
The grammatical inference system is applied to the plain corpus, which results in a learned
treebank. This learned treebank is a collection of tree structures. The trees are structured versions of
the sentences in the plain corpus. Note that some grammatical inference systems generate a grammar
and do not directly generate a treebank (whereas other systems do). If this is the case, a parser should
be used to analyze the sentences from the plain corpus using the learned grammar.
The actual evaluation compares the learned treebank against the original treebank. The
results of this comparison measure the degree to which the structure found in the learned treebank
corresponds to the structure in the original treebank.
Several metrics exist that each measure different aspects of the two treebanks. The most well-
known metrics stem from the ﬁeld of information retrieval [van Rijsbergen 1979]. Precision provides
a measure of the correctness of the learned structures compared against the original treebank. correct
measures how many structures (which are typically described as brackets) are found in both gold
(original trees) and learned collections:
Precision =

s∈structure |correct(gold(s), learned(s))|

s∈structure |learned(s)|
.
Recall measures of the degree to which all structures of the original treebank are also found in the
learned treebank:
Recall =

s∈structure |correct(gold(s), learned(s))|

s∈structure |gold(s)|
.
In order to have one overall measure, the F-score is used, which is the geometric mean of precision
and recall:
F-score = 2 ∗Precision ∗Recall
Precision + Recall
Advantages
Comparing structures from two treebanks can be done completely objectively. It is easy to evaluate
another system against the same treebank and the numeric results are directly comparable.
To allow other researchers to evaluate using the same settings, standardized treebanks may be
used. Using standardized treebanks also limits the possibility to tune the training data to the system
(which is possible with the looks-good-to-me approach).

4.3 Issues for Evaluation
109
An advantage of this approach with respect to the rebuilding known grammars approach
is that no string generation process is required. This resolves the problem of building a dataset that
measures the coverage of the entire grammar.
With respect to the looks-good-to-me approach, no language expert is required to perform
the evaluation. The evaluation process can be completely automated. This also reduces the evaluation
time and effort.
Disadvantages
The compare against a treebank approach relies on the availability of a treebank. However,
such datasets are not available for a wide range of languages. This limits the possibilities of evaluation
to only those languages for which such datasets have been developed.
The annotations in a treebank are performed based on a linguistic theory. Different linguistic
theories lead to different types of annotation. The resulting tree structures may be quite different
from the tree structures learned by the grammatical inference system. In other words, a grammatical
inference system may perform well on treebanks that are annotated according to one linguistic theory,
but perform badly on treebanks annotated using another theory. One of the reasons why different
linguistic theories exist and are used to annotate data is that the real underlying grammar of natural
language is not known.
Even though the metrics of precision and recall are well known, there are different ways
of applying them to the data. Firstly, there is the difference between micro and macro recall and
precision. With micro metrics, a global contingency table is constructed and used to compute the
results. Macro metrics calculate precision and recall for each tree and the average of these scores lead
to the overall results. Additionally, it may be unclear which structures should be taken into account.
Brackets that completely cover the entire sentence or brackets that only cover one word may or may
not be used (they are in a way trivial to add). These choices have a signiﬁcant impact on the actual
results of a system. For a proper evaluation, it has to be made clear exactly which design choices have
been made.
4.3.4
LANGUAGE MEMBERSHIP
The language membership evaluation approach concentrates on measuring whether the grammar
describes the language in a weak generative sense (in contrast to compare against a treebank,
which measures strong generative equivalence). The approach is illustrated in Figure 4.12.
Initially, the grammatical inference system is trained using the training information. The
grammatical inference system tries to identify regularities within this data that allows it to decide
whether newly seen sentences are either a member of the language under consideration or not.

110
4. LEARNING NON-REGULAR LANGUAGES
Membership
information
Training
information
Compare
membership
Test
sentences
GI system
Results
FIGURE 4.12: Schematic representation of the language membership evaluation approach.
Next, the evaluation starts. The system is fed test sentences. This set should contain sentences
that are member of the language, but also sentences that are not a member of the language to be
learned. The output of the system consists of a tag for each sentence that describes whether each
of the test sentences belongs to the language or not. This membership information is compared
against the real language membership information, leading to the result, which describes how well
the system can describe the overall language disregarding the internal structure.
Within the area of grammatical inference, this approach has been used extensively in com-
petitions (comparable to shared tasks which are common in other areas). Examples of competitions
that used this evaluation approach are Abbadingo [Lang et al. 1998], Gowachin, Omphalos [Starkie
et al. 2005], and Tenjinno [Starkie et al. 2006].
Typically, this approach only measures the precision of the language membership, which is
deﬁned as the percentage of correctly tagged sentences. This means that it can measure how well
the systems identify sentences belonging to the language or not, but it does not measure how well
the language learned by the system covers the entire language under consideration (recall). This can
only be measured properly by evaluating all sentences in the language, which is impossible in case
of inﬁnite-size languages.
Another approach to measuring coverage or recall is to generate sentences based on the
learned grammar (assuming it is a generative grammar). By considering the percentage of generated
sentences that is really member of the language, coverage can be measured.
If probabilistic grammars are learned, one may be more interested in how well the grammar
describes the probability distribution over the sentences in the language. In this case, another
metric may be more useful. Perplexity measures the probability of the test set assigned by the
language model. It is deﬁned as 2H where H is the entropy of probability distribution P : H =
−n
i=1
P(wi) log2 P (wi)
n
. Essentially, it measure the amount of surprise of seeing the next symbol

4.4 Formal Approaches
111
in a string. If the probability distribution of the system (P ) describes the language well, then the
probability of the string will be high and hence the perplexity will be low. Another way of looking
at perplexity is that lower perplexity means that a smaller number of bits is required to describe the
model.
Advantages
The main advantage of the language membership evaluation approach is that it allows for the
comparison of the system disregarding the representation of the grammar. As long as the system is
able to assign tags that indicate whether a sentence is member of the language or not, the evaluation
may be performed. This solves, for instance, the problem of the choice of linguistic theory used in
the annotation of treebanks.
Furthermore, the evaluation may be performed automatically without human interference,
making this another objective evaluation method. The language under consideration may be auto-
matically generated, but naturally occurring data (for which the underlying grammar is unknown)
may be evaluated as well.
Disadvantages
A proper evaluation using this approach requires a good sentence generation method. It has to be
clear that at the some point (up to inﬁnity) in time the entire language is being tested. The choice
of the method that generates test sentences may have an inﬂuence on the evaluation results. This
problem also holds when recall is being measured. For this a generative grammar needs to be learned,
and only using a method that is known to cover the entire grammar at some point can recall be
properly measured.
When using perplexity to measure how well the learned model ﬁts the probabilistic language,
the typical approach is to evaluate the system based on the assigned probabilities per symbol. This
works well for languages described by, for instance, n-gram models (which assign a probability to
each symbol based on previous symbols), but for the evaluation of other types of grammars it may be
necessary to measure perplexity over complete strings (and normalize over the length of the string)
instead of symbols as described above.
4.4
FORMAL APPROACHES
Many of the systems described in this chapter focus on learning natural language syntax. These
systems try to learn grammars that are generatively stronger than, for instance, regular languages.
However, it is unclear whether the power of context-free grammars is strong enough to fully describe
natural language syntax. For example, Huybrechts [1984] and Shieber [1985] indicate that some
syntactic constructions may require grammars from more powerful families.

112
4. LEARNING NON-REGULAR LANGUAGES
Grammatical inference research that concentrates on learning natural language syntax goes
in two directions. First, empirical systems as described in previous sections aim to identify structure
that corresponds to linguistic theories deﬁned by linguists. If we can build better systems, these
improvement may provide further insight into the learning process, the linguistic formalism, and
(perhaps most importantly) the generative capabilities of the formal grammar formalism. Second,
based on the empirical grammatical inference systems, new insights in formal grammatical inference
may be gained.
Formal grammatical inference has shown that learning the family of context-free grammars is
difﬁcult, or impossible, in most learning settings. However, context-free grammars may not even be
powerful enough to describe natural language syntax and some form of context-sensitive grammars
may be required to do so. If learning context-free grammars is already problematic then learning
context-sensitive grammars is clearly also problematic.
The solution to this problem is to identify families of languages that do not completely contain
families of languages that cannot be learned efﬁciently. The family of natural languages may contain
some context-free languages, some context-sensitive languages, but not all. In other words, it may
not be necessary to have access to the full power of context-sensitiveness. In a similar line, the full
power of context-freeness may not be required either.
Following this same line of thinking, there has been some research into the area of learning
context-sensitive languages. For instance, Alqu´ezar and Sanfeliu [1997] described Augmented
Regular Expressions, Yoshinaka [2009] discussed variants of substitutability, and Clark [2010b]
proposed Distributional Lattice Grammars. In general, however, the ﬁeld of learning context-
sensitive languages is still an open research area.
Another line of research in the area of formal grammatical inference, which is based on results
from empirical grammatical inference, deals with formal proofs based on the approaches used in the
empirical systems. In particular, the notion of substitutability has been used as the basis for the family
of languages called Non-Terminally Separated (nts) languages.
The family of nts languages is a subset of the family of deterministic context-free grammars.
An nts grammar is deﬁned as G = ⟨V , , S, R⟩where V is a set of non-terminals,  is a vocabulary,
R is a set of production rules, and S ∈V is the start symbol. Additionally, these grammars follow the
rule that for N ∈V , if N
∗⇒αβγ and M
∗⇒β then N
∗⇒αMγ . In other words, the non-terminals
in the grammar correspond exactly with the notion of substitutability. The family of nts grammars
have been shown to be efﬁciently pac learnable as well as identiﬁable in the limit [Clark and Eyraud
2005, 2007, Clark 2006].
Unfortunately, it is easy to see that natural languages are not nts languages. There are
situations in which substitutability leads to incorrect structures. For instance, if we consider the

4.5 Summary
113
sentences John eats meat and John eats much, according to the notion of substitutability, the words
meat and much belong to the same equivalence class. This means that in all cases the two words
are interchangeable. However, in practice this is not the case. This example illustrates that learning
based on substitutability learns a family of languages that is different from the family of natural
languages.
The ﬁelds of formal and empirical grammatical inference both provide their own view on
learning languages. On the one hand, formal grammatical inference shows learnability of families
of languages under certain conditions. On the other hand, empirical grammatical inference shows
practical possibilities and limitations of learning from real-world data. The ultimate aim of gram-
matical inference of natural languages is to identify a family of languages that can be proved to be
learnable under realistic conditions and at the same time is powerful enough to ﬁt the family of
natural languages.
4.5
SUMMARY
In this chapter, we primarily focused on empirical approaches to grammatical inference. Even though
empirical approaches do not lead to formal proofs of (efﬁcient) learnability, there are situations in
which the underlying family of languages is (not yet) known. For instance, when aiming to learn
natural language syntax, there is still an ongoing discussion on the required generative power.
The empirical grammatical inference approaches that we have investigated here all rely on
a similar principle, that of substitutability. This principle corresponds with (linguistic) tests for
constituency. By identifying parts in the set of example strings that can be substituted for each
other, the learners aim to identify substrings that correspond to the linguistic notion of constituents.
Several practical systems have been treated in some detail and a comparison of these systems
has been made. The systems differ in their use of the notion of substitutability. emile, adios, and
abl directly identify substitutable substrings, whereas ccm and u-dop both implicitly use the notion
in their statistical models. Based on this comparison, the systems have been grouped into expanding
and reducing approaches. These groups correspond broadly with the model-merging and model-
splitting approaches, respectively, used in learning ﬁnite-state machines.
The main problem with empirical grammatical inference systems is that they are used to
provide evidence for the learnability of certain languages. However, the required language family
is (typically) not know beforehand, which means that the performance of the systems may not be
perfect. This requires a different type of evaluation. We cannot rely on the fact that a language is
learned perfectly or even within certain limits. We do like to know which systems perform better
than others and we would also like to have an idea of how far away from the target we are. To measure

114
4. LEARNING NON-REGULAR LANGUAGES
the performance of the systems, several evaluation approaches have been used. Each approach has
its own advantages and disadvantages.
Even though this chapter mainly focused on empirical systems, the ideas that underlie these
systems have led to formal learnability proofs as well.

115
C H A P T E R 5
Lessons Learned
and Open Problems
We conclude the book with a brief summary of what has been covered, the main lessons we wish to
impart, and the open problems where research efforts ought to be directed.
5.1
SUMMARY
In Chapter 1, learning problems were introduced from the perspective of theoretical computer
science. Like other problems in computer science, it can be approached both formally and empirically,
and both have an important role to play in securing new knowledge. In Chapter 2, principles of
grammatical inference were explained and an overview of the formal methods and results were
presented. Several different deﬁnitions of learning were introduced, and several different classes
of grammars and formal languages were presented. Chapter 3 studied how linguistic generalizations
which can be represented with ﬁnite-state grammars can be learned. It was explained how many
formal results are based on state-merging and some a priori knowledge of the underlying ﬁnite-state
structure, which can be partial (the machine is deterministic) or complete (the machine has these
states and transitions). Chapter 4 studied learning problems where the targets cannot be represented
with ﬁnite-state grammars. In contrast to the previous chapters, this chapter focused on empirical
methods, and particular tasks. The important concept of substitutability was introduced and shown
to underly many systems that target context-free languages for learning. Different approaches to
evaluating empirical learning systems were also explained.
It is not an accident that Chapter 3 focused on formal results and Chapter 4 on empirical
results. Generally, there is a greater understanding of the learning problem when the targets of
learning can be represented with ﬁnite-state grammars than when they cannot be. (Of course, this
is not to say that there are no formal learning results for non-regular languages. As discussed in
Chapter 4, the work of Alexander Clark and his colleagues over the past decade formalized and
generalized important insights provided by the concept of substitutability.)

116
5. LESSONS LEARNED AND OPEN PROBLEMS
5.2
LESSONS
We hope that readers have come to appreciate that “learning” can be deﬁned in different ways.
Characterizing the learning problem is just as important as presenting solutions to it. In fact, several
aspects of the learning problem need to be deﬁned: the family of languages that the learner aims
to learn (which deﬁnes the set of targets), the learning process (describing which information is
provided by the oracle to the learner and how this transfer of information takes place), and the
evaluation of the end result (which measures whether the learner’s output counts as success).
We also hope that readers have come to appreciate the role grammatical structure plays in
learning. A priori knowledge of some grammatical structure can really help. This a priori knowledge
can be thought of as a learning bias. We have argued that this bias is present in learning systems,
whether it is implicit or explicit. We believe understanding comes when it is explicit, so its conse-
quences can be studied and evaluated carefully.
5.3
PROBLEMS
The ﬁeld of grammatical inference has been around for over 40 years. Research in the ﬁeld has led to
a range of results. However, there are also still many open problems. de la Higuera [2006] discussed
some open problems in grammatical inference. These are not necessarily linguistically motivated
but may be of interest to the more theoretically oriented reader. In this section, we provide a broad
perspective on currently open problems.
5.3.1
LEARNING TARGETS
Researchers are still identifying classes of stochastic and non-stochastic stringsets, relations, and
functions relevant to natural language. This area of research is likely to continue for the foreseeable
future.
The reason is partly due to the fact that most formal results that use the families of the
Chomsky Hierarchy have been negative: these families of languages are not learnable efﬁciently from
positive data. At the same time, we also know that there are constructions, or patterns, that require
the grammars of natural languages to contain relatively strong syntactic constructions [Shieber 1985],
which seems to clash with the learnability results.
One solution to this problem, anticipated by Gold [1967], is to identify a family of natural
languages which cross-cuts the Chomsky Hierarchy. This has the effect of limiting the learning
problem to some, but not all, context-free or context-sensitive languages. Instead, families of
languages should be identiﬁed that capture the subset of languages that allows us to describe those
constructions and patterns that occur in the natural languages.

5.3 Problems
117
In this vein, here are three areas we perceive as fruitful. We describe these areas in theo-
retical terms, but we stress that progress on these problems can be pursued both theoretically and
empirically.
Subregularformallanguagesandtransductions. Chapter 3 mentioned several subregular classes
of formal languages that appear relevant to natural language. While some of this work was
done over 40 years ago [McNaughton and Papert 1971], it has not really been noticed by the
computational linguistics community, with some exceptions [Heinz et al. 2011]. Furthermore,
very little of this work has been generalized to transductions (one exception is Chandlee et al.
[2014]). It is expected that further research in this area will lead to a better understanding and
better systems that learn certain aspects of natural language phenomena, such as phonology.
Sub-mildly context-sensitive formal languages. Chapter 4 studied the problem of learning
context-free languages. Shieber [1985] has argued that there are natural languages that go
beyond the context-free boundary. Several linguistic formalisms are known to generate lan-
guages which are mildly context sensitive (mcs), including Tree-Adjoining Grammars [Joshi
1985, Vijay Shanker and Weir 1994] and Minimalist Grammars [Michaelis 1998, Stabler
2011]. There seems to be no feasible way to learn the entire class of mcs languages under
a variety of learning criteria, but subclasses can be so learned [Becerra Bonache et al. 2010,
Clark and Yoshinaka 2014]. While these results are formal in nature, both formal and empiri-
cal results on learning sub-mcs languages is one of the cutting-edges of grammatical inference
that has the potential to revolutionize our understanding of the kinds of computations present
in natural language systems.
Subregular tree languages and transductions. There is another interesting approach which can
be pursued which combines elements of the two above. Formal languages are sets of strings
and grammars can be said to generate or recognize these sets. However, when it comes to
natural languages, we are also interested in tree structures. Work in theoretical computer
science has studied sets of trees and grammars which generate or recognize these sets (for
an overview see Rozenberg and Salomaa [1997]). An early result established that yields of
regular tree languages coincide with context-free languages [Thatcher 1967]. Much later it
was realized that in fact the context-free languages are exactly the yields of the strictly 2-local
subclass of regular tree languages [Rogers 1994, 1997] (cf. dop in Chapter 4). Thus, while
regular tree languages properly include strictly 2-local tree languages, the yields are the same.
Regular tree transductions of regular tree languages allows one to move beyond context-free
string languages. In fact, the yield of the image of a regular tree language under a regular tree
transduction can yield a mcs language [Morawietz 2003, M¨onnich 2006, Kobele et al. 2007,

118
5. LESSONS LEARNED AND OPEN PROBLEMS
Graf 2013]. In other words, the study and learning of sub-mcs classes of string languages can
proceed by studying and learning subregular classes of tree languages and tree transductions.
An additional issue relates to the fact that most current formal descriptions of a language
are rather clear-cut. Either a string is in the language or it is not. While many sentences and
words in natural languages clearly belong (or not), there are situations in which it is not clear (to
humans) whether the sentence or word is really part of the language or not. For instance, deep center
embedding makes sentences harder to judge as acceptable. The phrase The bike that a woman rides is
acceptable, but The bike that a woman that a child likes rides is harder to understand. Teasing apart issues
of linguistic performance from linguistic competence is not straightforward, although guidelines do
exist [Sch¨utze 1996]. To resolve this issue, or at least to be able to describe intermediate acceptability
or grammaticality judgments, stochastic grammars may be used. However, they face one signiﬁcant
hurdle: longer strings are eventually going to be worse (less probable) than shorter strings. While
some ideas exist to address this issue [Clark and Lappin 2011, Clark et al. 2013], much remains to
be done.
One very simple argument in favor of learning stochastic grammars is the fact that “absence
of information is information.” When attempting to learn a grammar from a large corpus, should
we use the fact that the string the is absent, or should we only rely on those strings which are
present? Stochastic grammars allow us to determine that an event with frequency 0 does not exist,
and decisions can be made based on this information.
Finally, no research has yet been performed in the area of formally modeling second language
learning or learning dialects. In these situations, there might be (partial) overlap or interaction
between the ﬁrst language and the second language.
5.3.2
LEARNING CRITERIA
The previous section identiﬁed one way of better characterizing the learning problem for natural
language in terms of better characterizing the targets of learning. In this section, we discuss open
areas of research that aim to better characterize the learning problem in terms of better characterizing
the learning criteria itself.
The input to learning. Much work in grammatical inference characterizes the input to the
learner in terms of a sequence or set of positive data, usually strings.
However, there is still an ongoing discussion on the nature of the interaction between
human oracles and learners, and how much interaction or linguistic data is really accessible
to the learner. In order to develop new models of active and interactive learning that describe

5.3 Problems
119
learning settings or oracle–learner interaction more accurately, it would seem that information
from linguists and the language acquisition is essential.
For instance, it seems reasonable to assume that children also have access to some
aspect of the meaning of the sentences they hear. In other words, the input to learning
is not only some linear string of morphemes or sounds, but it is that plus some semantic
representation [Angluin and Becerra-Bonache 2008, Kwiatkowksi et al. 2010, 2012]. Other
types of potentially useful information include the prosodic and intonational contours of
utterances.
Finally, the problem of learning in the absence of noise is already difﬁcult. So how about
the harder problem of learning in the presence of noise? While there have been important
advances in this regard [Angluin and Laird 1988], this is an area where advances can help
bridge the formal and empirical methods.
Measuring efﬁcient learning. de la Higuera [1997] presents a learning paradigm which requires
learning algorithms to be efﬁcient both in time and in data. The former is familiar: the time
required to output a grammar must be polynomial in the size of the input. The second is
less familiar but no less important: informally, the size of the input data needed for the
algorithm to output the correct grammar for each language must be polynomial in the size of
the grammar. Without the latter requirement, any learning algorithm can be transformed into
a time-efﬁcient one [Pitt 1989]. However, de la Higuera’s paradigm makes the most sense
for regular learning targets. It remains unclear how to successfully deﬁne efﬁcient learning
for non-regular targets. Past efforts to bound the number of errors or the number of mind
changes are reviewed along with some more recent ideas by Eyraud et al. [2015].
What counts as successful learning. Finally, as explained in Chapter 2, there is always the
question of what counts as successful learning.
In the case of formal grammatical inference, if exact learning is not required, what kinds
of approximations are? Several inﬂuential ideas have been formulated, but undoubtedly many
inﬂuential ideas remain to be formulated.
In the case of empirical grammatical inference, several ways of evaluating learning systems
were discussed in Chapter 4. Certainly, each of these await improvements (for instance if more
accurate treebanks or gold standards are developed) and other measures of evaluation can be
developed.
To conclude, there is no shortage of research to be done in grammatical inference. Compu-
tational linguistics provides a rich, fertile domain with plenty of speciﬁc tasks and problems, which
in turn provide a natural context for much of this research to take place.

120
5. LESSONS LEARNED AND OPEN PROBLEMS
5.4
RESOURCES
Readers interested in learning more about grammatical inference are directed to three sources.
First, de la Higuera [2010] is a comprehensive and detailed monograph covering many aspects of
grammatical inference. Second, there is a forthcoming collection of chapters by leading researchers
on advanced topics in grammatical inference [Heinz and Sempere 2015]. Topics in that book include
active learning, spectral learning, learning tree languages, and learning context-sensitive languages,
among others. Third, the biannual International Conference of Grammatical Inference (ICGI) has
been meeting regularly in even-numbered years. More information about this conference series, its
published proceeding papers, associated challenges, and software for various algorithms, including
ones discussed in this book, can be found at http://www.grammarlearning.org.
5.5
FINAL WORDS
We hope that this book has provided a broad picture of the goals and methods of grammatical
inference as it relates to computational linguistics. This book has not attempted to be exhaustive,
but instead to provide enough of a sufﬁcient foundation of knowledge that allows readers to engage
the literature in this area from its past and its future.
Perhaps we are dreaming, but if anyone comes to better appreciate the wonder of language
and the wonder of language learning as a result of this book, it will have achieved its purpose.
The End.

121
Bibliography
Abe, N. and Warmuth, M. K. (1992). On the computational complexity of approximating distributions
by probabilistic automata. Machine Learning, 9:205–260. DOI: 10.1007/BF00992677. 80
Adriaans, P., Fernau, H., and van Zaanen, M., Editors (2002). Proceedings of the International Colloquium
on Grammatical Inference (ICGI ’02), volume 2482 of Lecture Notes in Artiﬁcial Intelligence. Springer-
Verlag. 121, 136
Adriaans, P. and Vervoort, M. (2002). The emile 4.1 grammar induction toolbox. In Adriaans et al.
[2002], pages 293–295. 91, 92
Adriaans, P. W. (1992). Language Learning from a Categorial Perspective. Ph.D. thesis, University of
Amsterdam, Amsterdam, the Netherlands. 91
Akram, H. I. and de la Higuera, C. (2012). Learning probabilistic subsequential transducers from positive
data. In Proceedings of the International Conference on Agents and Artiﬁcial Intelligence (ICAART ’13).
42
Akram, H. I., de la Higuera, C., and Eckert, C. (2012). Actively learning probabilistic subsequential
transducers. In Heinz et al. [2012a], pages 19–33. 42
Allauzen, C. and Mohri, M. (2002). p-subsequentiable transducers. In Implementation and Application
of Automata, 7th International Conference (CIAA ’02), Revised Papers, volume 2608 of Lecture Notes
in Computer Science, pages 24–34. Springer-Verlag. 39
Alqu´ezar, R. and Sanfeliu, A. (1997). Recognition and learning of a class of context-sensitive languages
described by augmented regular expressions. Pattern Recognition, 30(1):163–182. 112
Amengual, J. C., Bened´ı, J. M., Casacuberta, F., Casta˜no, A., Castellanos, A., Jim´enez, V. M., Llorens,
D., Marzal, A., Pastor, M., Prat, F., Vidal, E., and Vilar, J. M. (2001). The EuTrans-I speech
translation system. Machine Translation, 15(1):75–103. 37
Angluin, D. (1980). Inductive inference of formal languages from positive data. Information and Control,
45:117–135. DOI: 10.1016/S0019-9958(80)90285-5. 52
Angluin, D. (1982). Inference of reversible languages. Journal of the Association for Computing Machinery,
29(3):741–765. DOI: 10.1023/A:1022860810097. 32, 59, 62, 65, 66
Angluin, D. (1987). Queries and concept learning. Machine Learning, 2:319–342. DOI:
10.1023/A:1007320031970. 25
Angluin, D. (1988a). Identifying languages from stochastic examples. Technical Report Yaleu/Dcs/RR-
614, Yale University. 28, 52, 83

122
BIBLIOGRAPHY
Angluin, D. (1988b). Learning regular sets from queries and counterexamples. Information and Control,
39:337–350. 32
Angluin, D. and Becerra-Bonache, L. (2008). Learning meaning before syntax. In Clark et al. [2008],
pages 1–14. DOI: 10.1007/978-3-540-88009-7_1. 119
Angluin, D. and Laird, P. (1988). Learning from noisy examples. Machine Learning, 2:343–370. 119
Applegate, R. (1972). Inese˜no Chumash Grammar. Ph.D. thesis, University of California, Berkeley. 76
Applegate, R. (2007). Samala-English dictionary: A guide to the Samala language of the Inese˜no Chumash
People. Santa Ynez Band of Chumash Indians. 76
Bailey, T. (1995). Nonmetrical Constraints on Stress. Ph.D. thesis, University of Minnesota. Ann Arbor,
Michigan. Stress System Database available at http://www.cf.ac.uk/psych/ssd/index.html. 65
Bailly, R. (2011). QWA: Spectral algorithm. Journal of Machine Learning Research - Workshop and
Conference Proceedings, Proceedings of the Asian Conference on Machine Learning ACML ’11, 20:147–
163. 37
Balle, B., Carreras, X., Luque, F. M., and Quattoni, A. (2014a). Spectral learning of weighted automata.
Machine Learning, 96(1–2):33–63. DOI: 10.1007/s10994-013-5416-x. 42, 81
Balle, B., Castro, J., and Gavald`a, R. (2010). A lower bound for learning distributions generated by
probabilistic automata. In Proceedings of the International Conference on Algorithmic Learning Theory
(ALT ’10), volume 6331 of Lecture Notes in Computer Science, pages 179–193. Springer-Verlag. DOI:
10.1007/978-3-642-16108-7_17. 30
Balle, B., Castro, J., and Gavald`a, R. (2014b). Adaptively learning probabilistic deterministic automata
from data streams. Machine Learning, 96(1–2):99–127. 27, 30
Baum, L. E., Petrie, T., Soules, G., and Weiss, N. (1970). A maximization technique occurring in the
statistical analysis of probabilistic functions of Markov chains. Annals of Mathematical Statistics,
41:164–171. 37, 38
Becerra Bonache, L., Case, J., Jain, S., and Stephan, F. (2010). Iterative learning of simple external
contextual languages. Theoretical Computer Science, 411:2741–2756. 117
Beros, A. and de la Higuera, C. (2014). A canonical semi-deterministic transducer. In Clark et al.
[2014], pages 33–48. 39
Bod, R. (1998). Beyond Grammar—An Experience-Based Theory of Language, volume 88 of CSLI Lecture
Notes. Center for Study of Language and Information (CSLI) Publications, Stanford, CA, USA.
100
Bod, R. (2006a). An all-subtrees approach to unsupervised parsing. In Proceedings of the International
Conference on Computational Linguistics and of the Annual Meeting of the Association for Computational
Linguistics (COLING and ACL ’06), pages 865–872. Association for Computational Linguistics.
DOI: 10.3115/1220175.1220284. 100

BIBLIOGRAPHY
123
Bod, R. (2006b). Unsupervised parsing with U-DOP. In Proceedings of the Conference on Natural Language
Learning (CoNLL ’06), pages 85–92, Morristown, NJ, USA. Association for Computational
Linguistics. DOI: 10.3115/1596276.1596293. 100
Bod, R., Sima’an, K., and Scha, R., Editors (2003). Data Oriented Parsing. Center for Study of Language
and Information (CSLI) Publications, Stanford, CA, USA. 100, 126
Carrasco, R. C. and Oncina, J. (1994). Learning stochastic regular grammars by means of a state merging
method. In Carrasco, R. C. and Oncina, J., Editors, Proceedings of the International Colloquium on
Grammatical Inference (ICGI ’94), number 862 in Lecture Notes in Artiﬁcial Intelligence, pages
139–150. Springer-Verlag. 37, 79
Carrasco, R. C. and Oncina, J. (1999). Learning deterministic regular grammars from stochastic samples
in polynomial time. RAIRO (Theoretical Informatics and Applications), 33(1):1–20. 66, 79
Casacuberta, F. and de la Higuera, C. (2000). Computational complexity of problems on probabilistic
grammars and transducers. In de Oliveira [2000], pages 15–24. DOI: 10.1007/978-3-540-45257-
7_2. 41, 42
Castellanos, A., Vidal, E., Var´o, M. A., and Oncina, J. (1998). Language understanding and
subsequential transducer learning. Computer Speech and Language, 12:193–228. 71
Castro, J. and Gavald`a, R. (2008). Towards feasible pac-learning of probabilistic deterministic ﬁnite
automata. In Clark et al. [2008], pages 163–174. 30
Chandlee, J. (2014). Strictly Local Phonological Processes. Ph.D. thesis, The University of Delaware. 71
Chandlee, J., Eyraud, R., and Heinz, J. (2014). Learning strictly local subsequential functions.
Transactions of the Association for Computational Linguistics, 2:491–503. 71, 83, 117
Charniak, E. (1993). Statistical Language Learning. Massachusetts Institute of Technology Press,
Cambridge, MA, USA and London, UK. 90
Chomsky, N. (1956). Three models for the description of language. IRE Transactions on Information
Theory, page 113–124. IT-2. 19, 76
Clark, A. (2006). pac-learning unambiguous nts languages. In Sakakibara et al. [2006], pages 59–71.
DOI: 10.1007/11872436_6. 112
Clark, A. (2010a). Distributional learning of some context-free languages with a minimally adequate
teacher. In Sempere, J. and Garc´ıa, P., Editors, Proceedings of the International Colloquium on
Grammatical Inference (ICGI ’10), volume 6339 of Lecture Notes in Computer Science, pages 24–37.
Springer-Verlag. DOI: 10.1007/978-3-642-15488-1_4. 45
Clark, A. (2010b). Efﬁcient, correct, unsupervised learning of context-sensitive languages. In Proceedings
of the Conference on Natural Language Learning (CoNLL ’10), pages 28–37, Stroudsburg, PA, USA.
Association for Computational Linguistics. 112

124
BIBLIOGRAPHY
Clark, A., Coste, F., and Miclet, L., Editors (2008). Proceedings of the International Colloquium on
Grammatical Inference (ICGI ’08), volume 5278 of Lecture Notes in Computer Science. Springer-Verlag.
122, 123, 127, 128
Clark, A. and Eyraud, R. (2005). Identiﬁcation in the limit of substitutable context-free languages.
In Jain, S., Simon, H. U., and Tomita, E., Editors, Proceedings of the International Conference on
Algorithmic Learning Theory (ALT ’05), volume 3734 of Lecture Notes in Computer Science, pages
283–296. Springer-Verlag. DOI: 10.1007/11564089_23. 112
Clark, A. and Eyraud, R. (2007). Polynomial identiﬁcation in the limit of substitutable context-free
languages. Journal of Machine Learning Research, 8:1725–1745. DOI: 10.1007/11564089_23. 67,
88, 112
Clark, A., Giorgolo, G., and Lappin, S. (2013). Statistical representation of grammaticality judgements:
the limits of n-gram models. In Proceedings of the Fourth Annual Workshop on Cognitive Modeling and
Computational Linguistics (CMCL ’13), pages 28–36, Soﬁa, Bulgaria. Association for Computational
Linguistics. 118
Clark, A., Kanazawa, M., and Yoshinaka, R., Editors (2014). Proceedings of the International Conference
on Grammatical Inference (ICGI ’14), volume 34 of JMLR Proceedings. JMLR.org. 122, 129, 133
Clark, A. and Lappin, S. (2011). Linguistic Nativism and the Poverty of the Stimulus. Wiley-Blackwell
Press, Chichester, UK. 28, 52, 67, 75, 118
Clark, A. and Thollard, F. (2004). pac-learnability of probabilistic deterministic ﬁnite state automata.
Journal of Machine Learning Research, 5:473–497. 30, 37, 66, 79
Clark, A. and Yoshinaka, R. (2014). Distributional learning of parallel multiple context-free grammars.
Machine Learning, 96(1-2):5–31. DOI: 10.1007/s10994-013-5403-2. 88, 117
Comon, H., Dauchet, M., Gilleron, R., Jacquemard, F., Lugiez, D., Tison, S., and Tommasi, M. (1997).
Tree automata techniques and applications. Release October 1 2002. http://www.grappa.univ-lille3
.fr/tata. 47
Cook, C. M., Rosenfeld, A., and Aronson, A. R. (1976). Grammatical inference by hill climbing.
Informational Sciences, 10:59–80. DOI: 10.1016/S0020-0255(76)90602-2. 106
Cormen, T. H. (2013). Algorithms Unlocked. MIT Press. 14
Courcelle, B. and Engelfriet, J. (2012). Graph Structure and Monadic Second-Order Logic. A Language-
Theoretic Approach. Cambridge University Press. 47
Cover, T. and Thomas, J. (1991). Elements of Information Theory. John Wiley and Sons, New York, NY.
21
Dempster, A., Laird, N., and Rubin, D. (1977). Maximum likelihood from incomplete data via the EM
algorithm. Journal of the Royal Statistical Society, Series B (Methodological), 39(1):1–38. 98

BIBLIOGRAPHY
125
Earley, J. (1970). An efﬁcient context-free parsing algorithm. Communications of the Association for
Computing Machinery, 13(2):94–102. 42
Edelman, S., Solan, Z., Ruppin, E., and Horn, D. (2004). Learning syntactic constructions from raw
corpora. In Proceedings of the 29th Boston University Conference on Language Development. 95
Edlefsen, M., Leeman, D., Myers, N., Smith, N., Visscher, M., and Wellcome, D. (2008). Deciding
strictly local (SL) languages. In Breitenbucher, J., Editor, Proceedings of the Midstates Conference for
Undergraduate Research in Computer Science and Mathematics, pages 66–73. 65
Ellis, C. (1969). Probabilistic Languages and Automata. Ph.D. thesis, University of Illinois, Urbana. 72
van Eynde, F. (2005). Part of speech tagging en lemmatisering van het D-COI corpus. http://odur.let
.rug.nl/vannoord/Lassy/POS_manual.pdf. 90
Eyraud, R., Heinz, J., and Yoshinaka, R. (2015). Efﬁciency in the identiﬁcation in the limit learning
paradigm. In Heinz, J. and Sempere, J., Editors, Advanced Topics in Grammatical Inference. Springer-
Verlag. To appear. 27, 69, 119
Fernau, H. (2005). Algorithms for learning regular expressions. In Jain et al. [2005], pages 297–311.
DOI: 10.1007/11564089_24. 34
Francis, W. N. and Kuˇcera, H. (1982). Frequency Analysis of English Usage. Lexicon and Grammar.
Houghton Mifﬂin, Boston, USA. 90
Fu, J., Heinz, J., and Tanner, H. G. (2011). An algebraic characterization of strictly piecewise languages.
In Ogihara, M. and Tarui, J., Editors, Theory and Applications of Models of Computation, volume
6648 of Lecture Notes in Computer Science, pages 252–263. Springer-Verlag. DOI: 10.1007/978-3-
642-20877-5_26. 77
Garc´ıa, P. and Vidal, E. (1990). Inference of K-testable languages in the strict sense and applications
to syntactic pattern recognition. Pattern Analysis and Machine Intelligence, 12(9):920–925. DOI:
10.1109/34.57687. 32, 64
Garc´ia, P., Vidal, E., and Oncina, J. (1990). Learning locally testable languages in the strict sense. In
Proceedings of the Workshop on Algorithmic Learning Theory (ALT ’90), pages 325–338. 66
Garey, M. R. and Johnson, D. S. (1979). Computers and Intractability: A Guide to the Theory of NP-
Completeness. W. H. Freeman. 14, 52
Garside, R., Leech, G., and McEnery, A. (1997). Corpus Annotation. Addison Wesley Longman. 90
Geertzen, J. (2003). String alignment in grammatical inference—what sufﬁx trees can do. Master’s
thesis, Tilburg University, Tilburg, the Netherlands. Available as technical report ILK-0311. 93
Geertzen, J. and van Zaanen, M. (2004). Grammatical inference using sufﬁx trees. In Paliouras, G. and
Sakakibara, Y., Editors, Proceedings of the International Colloquium on Grammatical Inference (ICGI
’04), volume 3264 of Lecture Notes in Artiﬁcial Intelligence, pages 163–174. Springer-Verlag. DOI:
10.1007/978-3-540-30195-0_15. 93

126
BIBLIOGRAPHY
Gelfand, A. and Smith, A. (1990). Sampling-based approaches to calculating marginal densities. Journal
of the American Statistical Association, 85(410):pp. 398–409. DOI: 10.2307/2289776. 38
Geman, S. and Johnson, M. (2004). Probability and statistics in computational linguistics, a brief
review. In Johnson, M., Khudanpur, S., Ostendorf, M., and Rosenfeld, R., Editors, Mathematical
Foundations of Speech and Language Processing, volume 138 of The IMA Volumes in Mathematics and
its Applications, pages 1–26. Springer-Verlag. DOI: 10.1007/978-1-4419-9017-4_1. 73, 78
Gildea, D. and Jurafsky, D. (1996). Learning bias and phonological-rule induction. Computational
Linguistics, 24(4):497–530. 71
Gleitman, L. (1990). The structural sources of verb meanings. Language Acquisition, 1(1):3–55. DOI:
10.1207/s15327817la0101_2. 66
Gold, E. (1967). Language identiﬁcation in the limit. Information and Control, 10:447–474. 23, 24, 25,
52, 116
Good, I. J. (1953). The population frequencies of species and the estimation of population parameters.
Biometrika, 40(3–4):237–264. 101
Goodman, J. (1996). Efﬁcient algorithms for parsing the dop model. In Proceedings of the Conference on
Empirical Methods on Natural Language Processing (EMNLP ’96), pages 143–152. Association for
Computational Linguistics. 102
Goodman, J. (2003). Efﬁcient parsing of dop with pcfg-reductions. In Bod et al. [2003], pages 125–146.
ISBN: 1-57586-435-5. 102
Gordon, M. (2002). A factorial typology of quantity-insensitive stress. Natural Language and Linguistic
Theory, 20(3):491–552. Additional appendices available at http://www.linguistics.ucsb.edu/faculty/
gordon/pubs.html. 65
Gordon, M. (2006). Syllable Weight: Phonetics, Phonology, Typology. Routledge. 58
Graf, T. (2013). Local and Transderivational Constraints in Syntax and Semantics. Ph.D. thesis, University
of California, Los Angeles. 118
Hammersley, J. M. and Handscomb, D. C. (1964). Monte Carlo Methods. John Wiley and Sons,
New York. 101
Hansen, K. and Hansen, L. (1969). Pintupi phonology. Oceanic Linguistics, 8:153–170. DOI:
10.2307/3622818. 56
Hansson, G. (2010). Consonant Harmony: Long-Distance Interaction in Phonology. Number 145 in
University of California Publications in Linguistics. University of California Press, Berkeley, CA.
Available on-line (free) at eScholarship.org. 76
Hayes, B. (1995). Metrical Stress Theory. Chicago University Press. 56, 58
Headden III, W. P. (2012). Unsupervised Bayesian Lexicalized Dependency Grammar Induction. Ph.D.
thesis, Brown University. 100

BIBLIOGRAPHY
127
Heinz, J. (2007). The Inductive Learning of Phonotactic Patterns. Ph.D. thesis, University of California,
Los Angeles. 60, 63
Heinz, J. (2008). Left-to-right and right-to-left iterative languages. In Clark et al. [2008], pages 84–97.
DOI: 10.1007/978-3-540-88009-7_7. 65
Heinz, J. (2009). On the role of locality in learning stress patterns. Phonology, 26(2):303–351. 65
Heinz, J. (2010). Learning long-distance phonotactics. Linguistic Inquiry, 41(4):623–661. DOI:
10.1162/LING_a_00015. 76, 77, 78
Heinz, J. (2015). Computational theories of learning and developmental psycholinguistics. In Lidz, J.,
Synder, W., and Pater, J., Editors, The Cambridge Handbook of Developmental Linguistics. Cambridge
University Press. To appear. 52
Heinz, J., de la Higuera, C., and Oates, T., Editors (2012a). Proceedings of the International Conference on
Grammatical Inference (ICGI ’12), volume 21. Jmlr.org. 121, 128, 136
Heinz, J., Kasprzik, A., and K¨otzing, T. (2012b). Learning with lattice-structured hypothesis spaces.
Theoretical Computer Science, 457:111–127. DOI: 10.1016/j.tcs.2012.07.017. 77
Heinz, J., Rawal, C., and Tanner, H. G. (2011). Tier-based strictly local constraints for phonology. In
Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL ’11)., pages
58–64, Portland, Oregon, USA. Association for Computational Linguistics. 78, 117
Heinz, J. and Rogers, J. (2010). Estimating strictly piecewise distributions. In Proceedings of the Annual
Meeting of the Association for Computational Linguistics (ACL ’10), pages 886–896, Uppsala, Sweden.
Association for Computational Linguistics. 77
Heinz, J. and Rogers, J. (2013). Learning subregular classes of languages with factored deterministic
automata. In Kornai, A. and Kuhlmann, M., Editors, Proceedings of the Meeting on the Mathematics
of Language (MoL ’13), pages 64–71, Soﬁa, Bulgaria. Association for Computational Linguistics. 77
Heinz, J. and Sempere, J. M., Editors (2015). Advanced Topics in Grammatical Inference. Springer-Verlag.
To appear. 120
de la Higuera, C. (1997). Characteristic sets for polynomial grammatical inference. Machine Learning,
27:125–138. DOI: 10.1023/A:1007353007695. 27, 69, 119
de la Higuera, C. (2006). Ten open problems in grammatical inference. In Sakakibara, Y., Kobayashi,
S., Sato, K., Nishino, T., and Tomita, E., Editors, Proceedings of the International Colloquium on
Grammatical Inference (ICGI ’06, volume 4201 of Lecture Notes in Artiﬁcial Intelligence, pages 32–44.
Springer-Verlag. DOI: 10.1007/11872436_4. 116
de la Higuera, C. (2010). GrammaticalInference:LearningAutomataandGrammars. Cambridge University
Press. DOI: 10.1007/s10590-011-9086-9. xx, 21, 27, 35, 67, 69, 75, 80, 81, 83, 90, 120

128
BIBLIOGRAPHY
de la Higuera, C., Janodet, J.-C., and Tantini, F. (2008). Learning languages from bounded resources:
the case of the DFA and the balls of strings. In Clark et al. [2008], pages 43–56. DOI:
10.1007/978-3-540-88009-7_4. 27
de la Higuera, C. and Oncina, J. (2002). Inferring deterministic linear languages. In Kivinen, J. and Sloan,
R. H., Editors, Proceedings of the Conference on Leaning Theory (COLT ’02), number 2375 in Lecture
Notes in Artiﬁcial Intelligence, pages 185–200. Springer-Verlag. DOI: 10.1007/3-540-45435-7_13.
43, 44
de la Higuera, C. and Oncina, J. (2003). Identiﬁcation with probability one of stochastic deterministic
linear languages. In Gavald`a, R., Jantke, K., and Takimoto, E., Editors, Proceedings of the
International Conference on Algorithmic Learning Theory (ALT ’03), number 2842 in Lecture Notes
in Computer Science, pages 134–148. Springer-Verlag. 44
de la Higuera, C. and Oncina, J. (2004). Learning probabilistic ﬁnite automata. In Paliouras, G. and
Sakakibara, Y., Editors, Proceedings of the International Colloquium on Grammatical Inference (ICGI
’04), volume 3264 of Lecture Notes in Artiﬁcial Intelligence, pages 175–186. Springer-Verlag. 28, 30
de la Higuera, C. and Oncina, J. (2013). Computing the most probable string with a probabilistic
ﬁnite state machine. In Proceedings of the International Workshop on Finite State Methods and Natural
Language Processing (FSMNLP ’13). https://aclweb.org/anthology/W/W13/W13-1801.pdf. 41
de la Higuera, C. and Oncina, J. (2014). The most probable string: an algorithmic study. Journal of Logic
and Computation, 24(2):311–330. DOI: 10.1093/logcom/exs049. 42
de la Higuera, C., Scicluna, J., and Nederhof, M.-J. (2014). On the computation of distances for
probabilistic context-free grammars. CoRR, abs/1407.1513. 30
de la Higuera, C. and Thollard, F. (2000). Identication in the limit with probability one of stochastic
deterministic ﬁnite automata. In de Oliveira [2000], pages 15–24. DOI: 10.1007/978-3-540-
45257-7_12. 28, 79, 82
Hopcroft, J. E., Motwani, R., and Ullman, J. D. (2001). Introduction to Automata Theory, Languages, and
Computation. Addison-Wesley Publishing Company, Reading, MA, USA. 59, 106
Horning, J. J. (1969). A Study of Grammatical Inference. Ph.D. thesis, Stanford University. 28, 52
Hsu, D., Kakade, S. M., and Zhang, T. (2012). A spectral algorithm for learning hidden Markov models.
Journal of Computer and System Sciences, 78(5):1460–1480. 38, 81
Hulden, M. (2012). Treba: Efﬁcient numerically stable em for pfa. In Heinz et al. [2012a], pages
249–253. 37
Huybrechts, R. M. A. C. (1984). The weak adequacy of context-free phrase structure grammar. In de
Haan, G. J., Trommelen, M., and Zonneveld, W., Editors, Van periferie naar kern, pages 81–99.
Foris, Dordrecht, the Netherlands. 85, 111

BIBLIOGRAPHY
129
Jain, S., Simon, H.-U., and Tomita, E., Editors (2005). Proceedings of the International Conference on
Algorithmic Learning Theory (ALT ’05), volume 3734 of Lecture Notes in Computer Science. Springer-
Verlag. 125, 132
Jardine, A., Chandlee, J., Eyraud, R., and Heinz, J. (2014). Very efﬁcient learning of structured classes
of subsequential functions from positive data. In Clark et al. [2014], pages 94–108. 71, 83
Jelinek, F. (1998). Statistical Methods for Speech Recognition. The Mit Press, Cambridge, Massachusetts.
35
Joshi, A. K. (1985). Tree-adjoining grammars: How much context sensitivity is required to provide rea-
sonable structural descriptions? In Dowty, D., Karttunen, L., and Zwicky, A., Editors, Natural Lan-
guage Parsing, pages 206–250. Cambridge University Press. DOI: 10.1017/CBO9780511597855
.007. 8, 117
Jurafsky, D. and Martin, J. (2008). Speech and Language Processing: An Introduction to Natural Language
Processing, Speech Recognition, and Computational Linguistics. Prentice-Hall, Upper Saddle River,
NJ, second edition. xix, 76, 81
Kearns, M. and Valiant, L. (1989). Cryptographic limitations on learning boolean formulae and ﬁnite
automata. In 21st ACM Symposium on Theory of Computing, pages 433–444. 30
Kearns, M. J. and Vazirani, U. (1994). An Introduction to Computational Learning Theory. MIT Press.
21, 30, 48, 79
Klein, D. (2004). Corpus-based induction of syntactic structure: Models of dependency and constituency.
In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL ’04), pages
478–485. 97, 99, 100
Klein, D. (2005). The Unsupervised Learning of Natural Language Structure. Ph.D. thesis, Stanford
University. 97
Klein, D. and Manning, C. D. (2002). A generative constituent-context model for improved grammar
induction. In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL
’02), pages 128–135. Association for Computational Linguistics. 97, 102
Klein, D. and Manning, C. D. (2005). Natural language grammar induction with a generative
constituent-context model. Pattern Recognition, 38(9):1407–1419. DOI: 10.1016/j.patcog.2004
.03.023. 97, 98
Kobele, G. M., R´etor´e, C., and Salvati, S. (2007). An automata-theoretic approach to minimalism. In
Rogers, J. and Kepser, S., Editors, Model Theoretic Syntax at 10, pages 71–80. 117
Koehn, P. (2010). Statistical Machine Translation. Cambridge University Press. 46
Kornai, A. (2011). Probabilistic grammars and languages. Journal of Logic, Language, and Information,
20:317–328. DOI: 10.1007/s10849-011-9135-z. 72

130
BIBLIOGRAPHY
Kullback, S. and Leibler, R. A. (1951). On information and sufﬁciency. Annals Mathematical Statistics,
22(1):79–86. DOI: 10.1214/aoms/1177729694. 29
Kunik, V., Solan, Z., Edelman, S., and Horn, D. (2005). Motif extraction and protein classiﬁcation. In
Proceedings of Computational Systems Bioinformatics (CSB), pages 80–85. 96
Kwiatkowksi, T., Zettlemoyer, L., Goldwater, S., and Steedman, M. (2010). Inducing probabilistic
CCG grammars from logical form with higher-order uniﬁcation. In Li and M`arquez [2010], pages
1223–1233. 119
Kwiatkowski, T., Goldwater, S., Zettlemoyer, L., and Steedman, M. (2012). A probabilistic model of
syntactic and semantic acquisition from child-directed utterances and their meanings. In Proceedings
of the Conference of the European Chapter of the Association for Computational Linguistics EACL ’12,
pages 234–244, Avignon, France. Association for Computational Linguistics. 119
Lang, K. J., Pearlmutter, B. A., and Price, R. A. (1998). Results of the Abbadingo One dfa learning
competition and a new evidence-driven state merging algorithm. In Honavar, V. and Slutski,
G., Editors, Proceedings of the International Colloquium on Grammatical Inference (ICGI ’98),
number 1433 in Lecture Notes in Artiﬁcial Intelligence, pages 1–12. Springer-Verlag. DOI:
10.1007/BFb0054059. 110
Lari, K. and Young, S. J. (1990). The estimation of stochastic context free grammars using the inside-
outside algorithm. Computer Speech and Language, 4:35–56. 46
Li, H. and M`arquez, L., Editors (2010). Proceedings of the Conference on Empirical Methods on Natural
Language Processing (EMNLP ’10). Association for Computational Linguistics. 130, 131
Lombardy, S. and Sakarovitch, J. (2008). The universal automaton. In Flum, J., Gr¨adel, E., and Wilke,
T., Editors, Logic and Automata, volume 2 of Texts in Logic and Games, pages 457–504. Amsterdam
University Press. 54
Lyngsø, R. B. and Pedersen, C. N. S. (2002). The consensus string problem and the complexity of
comparing hidden Markov models. Journal of Computing and System Science, 65(3):545–569. 30
Manning, C. and Sch¨utze, H. (1999). Foundations of Statistical Natural Language Processing. Cambridge,
MA: MIT Press. xix
Marcus, M. P., Santorini, B., and Marcinkiewicz, M. A. (1993). Building a large annotated corpus of
English: the Penn treebank. Computational Linguistics, 19(2):313–330. 90
McNaughton, R. and Papert, S. (1971). Counter-Free Automata. MIT Press. 54, 65, 66, 117
Michaelis, J. (1998). Derivational minimalism is mildly context-sensitive. In Selected Papers from the Third
International Conference on Logical Aspects of Computational Linguistics (LACL ’98), pages 179–198,
London, UK. Springer-Verlag. DOI: 10.1007/3-540-45738-0_11. 117

BIBLIOGRAPHY
131
Miclet, L. and de la Higuera, C., Editors (1996). Proceedings of the International Colloquium on
Grammatical Inference (ICGI ’96), number 1147 in Lecture Notes in Artiﬁcial Intelligence. Springer-
Verlag. 132, 135
Mohri, M. (1997). Finite-state transducers in language and speech processing. Computational Linguistics,
23(3):269–311. 39, 70
Mohri, M., Pereira, F. C. N., and Riley, M. (2000). The design principles of a weighted ﬁnite-state
transducer library. Theoretical Computer Science, 231(1):17–32. 39
M¨onnich, U. (2006). Grammar morphisms. Unpublished manuscript. 117
Morawietz, F. (2003). Two-Step Approaches to Natural Language Formalisms. Walter de Gruyter, Berlin.
117
Muggleton, S. (1990). Inductive Acquisition of Expert Knowledge. Addison-Wesley. 65
Naseem, T., Chen, H., Barzilay, R., and Johnson, M. (2010). Using universal linguistic knowledge to
guide grammar induction. In Li and M`arquez [2010], pages 1234–1244. 100
Nederhof, M.-J. and Satta, G. (2004). Kullback-Leibler distance between probabilistic context-
free grammars and probabilistic ﬁnite automata. In Proceedings of the International Conference on
Computational Linguistics (COLING ’04), volume 71. Association for Computational Linguistics.
DOI: 10.3115/1220355.1220366. 30
Nevins, A. (2010). Locality in Vowel Harmony. MIT Press, Cambridge, MA. 76
Oates, T., Desai, D., and Bhat, V. (2002). Learning k-reversible context-free grammars from positive
structural examples. In Sammut, C. and Hoffmann, A. G., Editors, Proceedings of the International
Conference on Machine Learning (ICML ’02), pages 459–465. Morgan Kaufmann, San Francisco,
CA. 47
Oates, T., Doshi, S., and Huang, F. (2003). Estimating maximum likelihood parameters for stochastic
context-free graph grammars. In Proceedings of the International Conference on Inductive Logic
Programming ILP ’03, volume 2835 of Lecture Notes in Computer Science, pages 281–298. Springer-
Verlag. 47
Odden, D. (1994). Adjacency parameters in phonology. Language, 70(2):289–330. DOI: 10.2307/
415830. 76
de Oliveira, A. L., Editor (2000). Proceedings of the International Colloquium on Grammatical Inference
(ICGI ’00), volume 1891 of Lecture Notes in Artiﬁcial Intelligence. Springer-Verlag. 123, 128, 133
Oncina, J. and Garc´ıa, P. (1992). Identifying regular languages in polynomial time. In Bunke, H., Editor,
Advances in Structural and Syntactic Pattern Recognition, volume 5 of Series in Machine Perception and
Artiﬁcial Intelligence, pages 99–108. World Scientiﬁc. 32

132
BIBLIOGRAPHY
Oncina, J., Garc´ıa, P., and Vidal, E. (1993). Learning subsequential transducers for pattern recognition
interpretation tasks. Pattern Analysis and Machine Intelligence, 15(5):448–458. DOI: 10.1109/34
.211465. 38, 40, 66, 68, 70
Oncina, J. and Var´o, M. A. (1996). Using domain information during the learning of a subsequential
transducer. In Miclet and de la Higuera [1996], pages 313–325. DOI: 10.1007/BFb0033364. 71
Palmer, N. and Goldberg, P. W. (2005). pac-learnability of probabilistic deterministic ﬁnite state
automata in terms of variation distance. In Jain et al. [2005], pages 157–170. 30
Paz, A. (1971). Introduction to Probabilistic Automata. Academic Press, New York. 35
Pitt, L. (1989). Inductive inference, dfas, and computational complexity. In Analogical and Inductive
Inference, number 397 in Lecture Notes in Artiﬁcial Intelligence, pages 18–44. Springer-Verlag. DOI:
10.1007/3-540-51734-0_50. 27, 69, 119
Rabiner, L. (1989). A tutorial on hidden Markov models and selected applications in speech recoginition.
Proceedings of the IEEE, 77:257–286. DOI: 10.1109/5.18626. 21, 35
van Rijsbergen, C. J. (1979). Information Retrieval. University of Glasgow, Glasgow, UK, second edition.
Printout. 108
Roark, B. and Sproat, R. (2007). Computational Approaches to Syntax and Morphology. Oxford University
Press. 37
Rogers, H. (1967). Theory of Recursive Functions and Effective Computability. McGraw Hill Book
Company. 16
Rogers, J. (1994). Studies in the Logic of Trees with Applications to Grammatical Formalisms. Ph.D. thesis,
University of Delaware. Published as Technical Report 95-04 by the Department of Computer and
Information Sciences. 117
Rogers, J. (1997). Strict LT2 : Regular : Local : Recognizable. In Retor´e, C., Editor, Proceedings of Logical
Aspects of Computational Linguistics: First International Conference (LACL ’96), Selected Papers, volume
1328 of Lecture Notes in Artiﬁcial Intelligence, pages 366–385. Springer-Verlag. 117
Rogers, J., Heinz, J., Bailey, G., Edlefsen, M., Visscher, M., Wellcome, D., and Wibel, S. (2010). On
languages piecewise testable in the strict sense. In Ebert, C., J¨ager, G., and Michaelis, J., Editors,
The Mathematics of Language, volume 6149 of Lecture Notes in Artiﬁcial Intelligence, pages 255–265.
Springer-Verlag. 76, 77
Rogers, J., Heinz, J., Fero, M., Hurst, J., Lambert, D., and Wibel, S. (2013). Cognitive and sub-regular
complexity. In Morrill, G. and Nederhof, M.-J., Editors, Formal Grammar, volume 8036 of Lecture
Notes in Computer Science, pages 90–108. Springer-Verlag. 65, 76, 77, 82
Rogers, J. and Pullum, G. (2011). Aural pattern recognition experiments and the subregular hierarchy.
Journal of Logic, Language and Information, 20:329–342. DOI: 10.1007/s10849-011-9140-2. 65, 66,
82

BIBLIOGRAPHY
133
Ron, D., Singer, Y., and Tishby, N. (1995). On the learnability and usage of acyclic probabilistic ﬁnite
automata. In Proceedings of the Conference on Leaning Theory (Colt ’95), pages 31–40. 37, 79
Rose, S. and Walker, R. (2004). A typology of consonant agreement as correspondence. Language,
80(3):475–531. 76
Rozenberg, G. and Salomaa, A., Editors (1997). Handbook of Formal Languages, Volume III, Beyond
Language. Springer-Verlag. 117
Sakakibara, Y. (1990). Learning context-free grammars from structural data in polynomial time.
Theoretical Computer Science, 76:223–242. DOI: 10.1016/0304-3975(90)90017-C. 43, 45
Sakakibara, Y., Kobayashi, S., Sato, K., Nishino, T., and Tomita, E., Editors (2006). Proceedings of
the International Colloquium on Grammatical Inference (ICGI ’06, number 4201 in Lecture Notes in
Artiﬁcial Intelligence. Springer-Verlag. 123, 134
Sakakibara, Y. and Muramatsu, H. (2000). Learning context-free grammars from partially structured
examples. In de Oliveira [2000], pages 229–240. DOI: 10.1007/978-3-540-45257-7_19. 90
Sanjeev, A. and Boaz, B. (2009). Computational Complexity: A Modern Approach. Cambridge University
Press, New York, NY, USA, ﬁrst edition. 21
Santorini, B. and Kroch, A. (2007). The Syntax of Natural Language: An Online Introduction using the
Trees Program. Online version. 87
Sch¨utze, C. (1996). The Empirical Base of Linguistics: Grammaticality Judgments and Linguistic
Methodology. University of Chicago Press. 118
Scicluna, J. and de la Higuera, C. (2014a). Grammatical inference of some probabilistic context-free
grammars from positive data using minimum satisﬁability. In Clark et al. [2014], pages 139–152. 88
Scicluna, J. and de la Higuera, C. (2014b). pcfg induction for unsupervised parsing and language
modelling. In Proceedings of the Conference on Empirical Methods on Natural Language Processing
(EMNLP ’14), pages 1353–1362. Association for Computational Linguistics. 46
Scott, D. and Rabin, M. (1959). Finite automata and their decision problems. IBM Journal of Research
and Development, 5(2):114–125. 18
Seki, H., Matsumura, T., Fujii, M., and Kasami, T. (1991). On multiple context-free grammars.
Theoretical Computer Science, 88(2):191–229. 8
Shibata, C. and Yoshinaka, R. (2014). A comparison of collapsed bayesian methods for probabilistic
ﬁnite automata. Machine Learning, 96(1–2):155–188. 37
Shieber, S. M. (1985). Evidence against the context-freeness of natural language. Linguistics and
Philosophy, 8(3):333–343. DOI: 10.1007/BF00630917. 19, 85, 111, 116, 117
Solan, Z., Horn, D., Ruppin, E., and Edelman, S. (2005). Unsupervised learning of natural languages.
Proceedings of the National Academy of Sciences of the United States of America, 102(33):11629–11634.
DOI: 10.1073/pnas.0409746102. 95

134
BIBLIOGRAPHY
Spitkovsky, V. (2013). Grammar Induction and Parsing with Dependency-and-Boundary Models. Ph.D.
thesis, Stanford University, Stanford, CA, USA. 100
Stabler, E. P. (1997). Derivational minimalism. In Retor´e, C., Editor, Logical Aspects of Computational
Linguistics, volume 1328 of Lecture Notes in Computer Science, pages 68–195, Berlin. Springer-Verlag.
8
Stabler, E. P. (2011). Computational perspectives on minimalism. In Boeckx, C., Editor, OxfordHandbook
of Linguistic Minimalism. Oxford University Press. DOI: 10.1093/oxfordhb/9780199549368.013
.0027. 8, 117
Starkie, B., Coste, F., and van Zaanen, M. (2005). Progressing the state-of-the-art in grammatical
inference by competition. AI Communications, 18(2):93–115. 110
Starkie, B., van Zaanen, M., and Estival, D. (2006). The Tenjinno machine translation competition. In
Sakakibara et al. [2006], pages 214–226. DOI: 10.1007/11872436_18. 110
Stolcke, A. (1994). Bayesian Learning of Probabilistic Language Models. Ph.D. dissertation, University of
California. 37, 65
Stolcke, A. (1995). An efﬁcient probablistic context-free parsing algorithm that computes preﬁx
probabilities. Computational Linguistics, 21(2):165–201. 46
Sudkamp, A. (2006). Languages and Machines: An Introduction to the Theory of Computer Science. Addison-
Wesley, third edition. 21
Takada, Y. (1988). Grammatical inference for even linear languages based on control sets. Information
Processing Letters, 28(4):193–199. 43
Thatcher, J. W. (1967). Characterizing derivation trees for context-free grammars through a
generalization of ﬁnite automata theory. Journal of Computer and System Sciences, 1:317–322. DOI:
10.1016/S0022-0000(67)80022-9. 117
Thollard, F., Dupont, P., and de la Higuera, C. (2000). Probabilistic dfa inference using Kullback-
Leibler divergence and minimality. In Proceedings of the International Conference on Machine Learning
(ICML ’00), pages 975–982. Morgan Kaufmann, San Francisco, CA. 37
Turing, A. M. (1950). Computing machinery and intelligence. MIND: A Quarterly Review of Pyschology
and Philosophy, 59(236):433–460. 47
Ukkonen, E. (1995). On-line construction of sufﬁx trees. Algorithmica, 14:249–260. DOI: 10.1007/
BF01206331. 93, 94
Valiant, L. G. (1984). A theory of the learnable. Communications of the Association for Computing
Machinery, 27(11):1134–1142. 28, 47
Vapnik, V. N. and Chervonenkis, A. Y. (1971). On the uniform convergence of relative frequencies
of events to their probabilities. Theory of Probability & Its Applications, 16(2):264–280. DOI:
10.1137/1116025. 47

BIBLIOGRAPHY
135
Vervoort, M. R. (2000). Games, Walks and Grammars. Ph.D. thesis, University of Amsterdam,
Amsterdam, the Netherlands. 91
Verwer, S., Eyraud, R., and de la Higuera, C. (2014). Pautomac: A probabilistic automata and hidden
Markov models learning competition. Machine Learning, 96(1–2):129–154. DOI: 10.1007/s10994-
013-5409-9. 37, 71, 81, 83
Vidal, E., Thollard, F., de la Higuera, C., Casacuberta, F., and Carrasco, R. C. (2005). Probabilistic
ﬁnite state automata – part I and II. Pattern Analysis and Machine Intelligence, 27(7):1013–1039.
DOI: 10.1109/TPAMI.2005.147. 35, 36, 72, 75, 77, 79
Vijay Shanker, K. and Weir, D. (1994). The equivalence of four extensions of context-free grammars.
Mathematical Systems Theory, 27:511–546. DOI: 10.1007/BF01191624. 117
Vilar, J. M. (1996). Query learning of subsequential transducers. In Miclet and de la Higuera [1996],
pages 72–83. DOI: 10.1007/BFb0033343. 39, 40
Viterbi, A. (1967). Error bounds for convolutional codes and an asymptotically optimum decoding
algorithm. Institute of Electrical and Electronics Engineers Transactions on Information Theory, 13:260–
269. DOI: 10.1109/TIT.1967.1054010. 94
Wagner, R. A. and Fischer, M. J. (1974). The string-to-string correction problem. Journal of the
Association for Computing Machinery, 21(1):168–173. 93
Walker, R. (2000). Mongolian stress, licensing, and factorial typology. ROA-172, Rutgers Optimality
Archive, http://roa.rutgers.edu/. 57
Warmuth, M. (1989). Towards representation independence in pac-learning. In Jantke, K. P., Editor,
Proceedings of AII ’89, volume 397 of Lecture Notes in Artiﬁcial Intelligence, pages 78–103. Springer-
Verlag. DOI: 10.1007/3-540-51734-0_53. 30
Yoshinaka, R. (2009). Learning mildly context-sensitive languages with multidimensional substitutability
from positive data. In Gavald`a, R., Lugosi, G., Zeugmann, T., and Zilles, S., Editors, Proceedings of
the International Conference on Algorithmic Learning Theory (ALT ’09), volume 5809 of Lecture Notes
in Computer Science, pages 278–292. Springer-Verlag. DOI: 10.1007/978-3-642-04414-4_24. 67,
112
Younger, D. H. (1967). Recognition and parsing of context-free languages in time n3. Information and
Control, 10(2):189–208. DOI: 10.1016/S0019-9958(67)80007-X. 42
van Zaanen, M. (2000a). ABL: Alignment-Based Learning. In Proceedings of the International Conference
on Computational Linguistics (COLING ’00), pages 961–967. Association for Computational
Linguistics. 92
van Zaanen, M. (2000b). Bootstrapping syntax and recursion using Alignment-Based Learning. In
Langley, P., Editor, Proceedings of the International Conference on Machine Learning (ICML ’00),
pages 1063–1070. 92

136
BIBLIOGRAPHY
van Zaanen, M. (2000c). Learning structure using Alignment Based Learning. In Kilgarriff, A., Pearce,
D., and Tiberius, C., Editors, Proceedings of the Third Annual Doctoral Research Colloquium (CLUK),
pages 75–82. Universities of Brighton and Sussex. 92
van Zaanen, M. (2002a). Bootstrapping Structure into Language: Alignment-Based Learning. Ph.D. thesis,
University of Leeds, Leeds, UK. 88, 92, 104
van Zaanen, M. (2002b). Implementing Alignment-Based Learning. In Adriaans et al. [2002], pages
312–314. 92
van Zaanen, M. (2003). Theoretical and practical experiences with Alignment-Based Learning. In
Proceedings of the Australasian Language Technology Workshop, pages 25–32. 92
van Zaanen, M. and Adriaans, P. (2001). Alignment-Based Learning versus emile: A comparison. In
Proceedings of the Belgian-Dutch Conference on Artiﬁcial Intelligence (BNAIC), pages 315–322. 107
van Zaanen, M. and de la Higuera, C. (2011). Computational language learning. In van Benthem, J. and
ter Meulen, A., Editors, Handbook of Logic and Language, pages 765–780. Elsevier, second edition.
104
van Zaanen, M., Roberts, A., and Atwell, E. (2004). A multilingual parallel parsed corpus as gold
standard for grammatical inference evaluation. In Kranias, L., Calzolari, N., Thurmair, G., Wilks,
Y., Hovy, E., Magnusdottir, G., Samiotou, A., and Choukri, K., Editors, Proceedings of the Workshop:
The Amazing Utility of Parallel and Comparable Corpora, pages 58–61. 104
van Zaanen, M. and van Noord, N. (2012). Model merging versus model splitting context-free grammar
induction. In Heinz et al. [2012a], pages 224–236. 89
Zipf, G. K. (1929). Relative frequency as a determinant of phonetic change. Harvard Studies in Classical
Philology, 40:1–95.

137
Author Biographies
JEFFREY HEINZ
Jeffrey Heinz received his Ph.D. from the University of California,
Los Angeles in 2007, and is currently an Associate Professor at the
University of Delaware. His research lies at the intersection of theo-
retical and mathematical linguistics, theoretical computer science, and
computational learning theory, with specializations in phonology, lin-
guistic typology, and grammatical inference. His work in these areas
has appeared in the journals Linguistic Inquiry, Phonology, Theoretical
Computer Science, Topics in Cognitive Science, Transactions of the Asso-
ciation of Computational Linguistics, and Science, among others.
His current research interests are on establishing language-
theoretic, automata-theoretic, model-theoretic, and logical character-
izations of subregular classes of formal languages and transductions in order to better characterize the
computational nature of phonological grammars and to better understand how they can be learned.
He has served as part of the executive committee of the Association for Computational
Linguistics Special Interest Group in Computational Morphology and Phonology (ACL-
SIGMORPHON) since 2007. He has been a member of the steering committee of the Inter-
national Community in Grammatical Inference (ICGI) since 2012. Moving forward, he would like
to also support and strengthen the work of the Association for Mathematics of Language (MOL)
and the Association for Logic, Language, and Information (FoLLI).

138
AUTHOR BIOGRAPHIES
COLIN DE LA HIGUERA
Colin de la Higuera received his Ph.D. at Bordeaux University,
France, in 1989. He has been an Associate Professor at the Uni-
versity of Montpellier, a Professor at Saint-Etienne University, and
is now a Professor at Nantes University. He has been involved in a
number of research themes, including algorithmics, formal language
theory, and pattern recognition. His chief interest lies in grammatical
inference, a ﬁeld in which he has been the author of more than 50
reviewed research papers and a monograph, Grammatical Inference:
Learning Automata and Grammars, published in 2010.
He has developed algorithms, studied learning models, and
has been trying to link classical formal language frameworks with
alternative ways of deﬁning languages, inspired by linguistic considerations or techniques developed
in pattern recognition.
He has been chairman of the International Community in Grammatical Inference (2002–
2007) and president of the SIF: The French Informatics Society (2012–2015).
He is currently a trustee of the Knowledge for All foundation and working toward the usage
of technology for an open dissemination of knowledge and education.

AUTHOR BIOGRAPHIES
139
MENNO VAN ZAANEN
Menno van Zaanen received his Ph.D. from the University of Leeds,
UK in 2002. He holds Master degrees in both computer science and
linguistics. He is currently an Assistant Professor at Tilburg Univer-
sity, the Netherlands. His research concentrates on empirical gram-
matical inference and its applications. He worked and is still work-
ing on several projects dealing with structure in different modalities,
multi-modal information retrieval, question answering, and symbolic
machine learning for language and music. He has taught courses on a
range of topics, including digital heritage, natural language process-
ing, language and speech technology, social intelligence, and infor-
mation search. He has published on several systems that deal with
both clean and noisy linguistic data, such as language independent
syntactic structure induction, boundaries in compounds (of different languages), spelling checkers,
part-of-speech tagging of Twitter messages, and the identiﬁcation of patterns in music and text.
He is a founding member of the International Community in Grammatical Inference and was
chairman between 2007 and 2010. He is International Advisory Committee member of the ACL
Special Interest Group on Finite-State Methods (ACL-SIGFSM), editorial board member of the
CLIN journal, and Associate Editor of the Computational Cognitive Science journal.

