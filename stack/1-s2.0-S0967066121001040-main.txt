Control Engineering Practice 112 (2021) 104827
Contents lists available at ScienceDirect
Control Engineering Practice
journal homepage: www.elsevier.com/locate/conengprac
Quadrotor going through a window and landing: An image-based visual
servo control approach✩
Zhiqi Tang a,b,∗, Rita Cunha a, David Cabecinhas a, Tarek Hamel b,c, Carlos Silvestre d,1
a ISR, IST, Universidade de Lisboa, Portugal
b I3S-CNRS, Université Côte d’Azur, Nice-Sophia Antipolis, France
c Institut Universitaire de France, France
d Faculty of Science and Technology of the University of Macau, Macao, China
A B S T R A C T
This paper considers the problem of controlling a quadrotor to go through a window and land on a planar target, the landing pad, using an Image-Based Visual
Servo (IBVS) controller that relies on sensing information from two on-board cameras and an IMU. The maneuver is divided into two stages: crossing the window
and landing on the pad. For the first stage, a control law is proposed that guarantees that the vehicle will not collide with the wall containing the window
and will go through the window with non-zero velocity along the direction orthogonal to the window, keeping at all times a safety distance with respect to
the window edges. For the landing stage, the proposed control law ensures that the vehicle achieves a smooth touchdown, keeping at all time a positive height
above the plane containing the landing pad. For control purposes, the centroid vectors provided by the combination of the spherical image measurements of a
collection of landmarks (corners) for both the window and the landing pad are used as position measurement. The translational optical flow relative to the wall,
window edges, and landing plane is used as velocity cue. To achieve the proposed objective, no direct measurements nor explicit estimate of position or velocity
are required. Simulation and experimental results are provided to illustrate the performance of the presented controller.
1. Introduction
Navigation of Unmanned Aerial Vehicles (UAVs) using vision sys-
tems has been an important field of research during recent decades.
Especially in indoor environments, where GPS is unavailable, a widely
adopted alternative sensor suite includes an inertial measurement unit
(IMU) and cameras, which are both passive, lightweight, and inex-
pensive sensors (Zingg et al., 2010). Three main solutions have been
proposed for navigation using vision in indoor environments: map-
based navigation, map-building-based navigation and mapless naviga-
tion (DeSouza & Kak, 2002). The first approach depends on a user-
created geometric model or topology map of the environment, e.g. Per-
spective n Point (PnP), and the second requires the use of sensors to
construct their own geometric or topological models, e.g. Simultaneous
localization and mapping (SLAM). The mapless visual navigation, in
which no global representation of the environment is required and the
environment is perceived as the system navigates, can be classified in
accordance with the main vision technique or types of clues used during
the navigation, which are methods based on optical flow, appearance
✩This work was supported by the Macao Science and Technology Development Fund under Grant FDCT/0031/2020/AFJ; by the University of Macau, Macau,
China,
under the Project MYRG2018-00198-FST; by the Fundaçao para a Ciência e a Tecnologia (FCT) through LARSyS - FCT Project UIDB/50009/2020 and
PTDC/EEI-AUT/32107/2017 and by FCT Scientific Employment Stimulus grant CEECIND/04199/2017. The work of T. Hamel was supported by the French
National Research Agency (ANR) under the project DACAR. The work of Z. Tang was supported by FCT through Ph.D. Fellowship PD/BD/114431/2016 under
the FCT-IST NetSys Doctoral Program.
∗Corresponding author at: ISR, IST, Universidade de Lisboa, Portugal.
E-mail addresses: zhiqitang@tecnico.ulisboa.pt (Z. Tang), rita@isr.tecnico.ulisboa.pt (R. Cunha), dcabecinhas@isr.tecnico.ulisboa.pt (D. Cabecinhas),
thamel@i3s.unice.fr (T. Hamel), csilvestre@umac.mo (C. Silvestre).
1 On leave from ISR, IST, Universidade de Lisboa, Portugal.
and feature tracking (DeSouza & Kak, 2002). However, the appearance-
based method has the main problems which are to find an appropriate
algorithm for the representation of the environment and to define the
on-line matching criteria.
Visual servo control is a popular mapless navigation method based
on feature tracking, which can be classified in two main categories:
Image-based visual servo (IBVS) and Position-based visual servo (PBVS)
control. PBVS involves reconstruction of the target pose with respect
to the robot thus a 3-D model of the observed object should be
known. However in IBVS, the control commands are deduced directly
from image features, thus they offer advantages in robustness to cam-
era and target calibration errors, reduced computational complexity,
and simple extension to applications involving multiple cameras com-
pared to PBVS methods (Hutchinson et al., 1996). However, classical
IBVS (Chaumette et al., 2016) suffers from three key problems. First,
it is necessary to determine the depth of each visual feature used in
the image error criterion independently from the control algorithm.
Second, the rigid-body dynamics of the camera ego-motion are highly
https://doi.org/10.1016/j.conengprac.2021.104827
Received 8 September 2020; Received in revised form 9 April 2021; Accepted 13 April 2021
Available online 3 May 2021
0967-0661/© 2021 Elsevier Ltd. All rights reserved.

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 1. 3-D plot of the quadrotor trajectories under different initial conditions.
coupled when expressed as target motion in the image plane. And
last, it uses a simple linearized control on the image kinematics that
leads to complex non-linear dynamics and is not easily extended to the
dynamics.
In order to overcome these problems, a spherical camera geometry
can be used, from which the virtual spherical image points can be
obtained by transforming the image points on the perspective camera to
the view that would be seen by an ideal unified-spherical camera. Novel
IBVS algorithms based on spherical image centroids (e.g. applications
on hovering an autonomous helicopter (Hamel & Mahony, 2002),
landing a quadrotor on moving platform (Hérissé et al., 2012; Serra
et al., 2016) and landing a fixed-wing aircraft on the runway (Le Bras
et al., 2014; Serra et al., 2015; Tang et al., 2018a)), do not require
accurate depth information for observed image features and overcomes
the difficulties associated with the highly coupled dynamics of the
camera ego-motion in the image dynamics.
This paper extends the IBVS control solution based on spherical
image centroids to a specific problem of steering a quadrotor to move
from one room to a second one by crossing a window and then land
on a planar target placed in the second room (see Fig. 1). This appli-
cation has significant practical interest since many tasks (e.g. search
and rescue in an earthquake-damaged building (Michael et al., 2012),
package delivery using UAVs) require UAVs to land on a final des-
tination or to perform intermediate landings for battery recharge or
exchange, or refueling (for larger UAVs) during long missions. The
quadrotor is assumed to be equipped with an IMU and two on-board
cameras: one forward-looking and another downward-looking. Neither
the translational velocity and position of the vehicle nor the location of
the target (window and landing pad) are known. In the proposed IBVS
control laws, the centroid vectors provided by the combination of the
spherical image measurements of a collection of landmarks (corners)
from both the window and the landing pad are used as position cue and
the translational optical flow relative to the plane containing window
and landing pad is used as velocity cue.
The proposed control strategy draws inspiration from (Serra et al.,
2016) which combines a centroid-like feature and the translational
optical flow to perform exponential landing on a desired spot. This
paper considers different control objectives: going through a window
and then landing on a desired target. The control law for going through
a window ensures that no collision with the wall or windows edges
will occur and the vehicle will align with the center line orthogonal
to the window, crossing it with non-zero velocity. The control law for
the landing is an improvement with respect to the one used in Serra
et al. (2016) in which the desired optical flow was chosen constant,
leading to a high-gain controller that fails to achieve a perfect landing
maneuver. Conversely, the desired optical flow adopted in this work
is not constant. It corresponds to the component of the image centroid
in the direction orthogonal to the target plane leading to a vanishing
desired optical flow when the distance to the target approaches zero
and therefore one avoids the high-gain nature of prior work (Serra
et al., 2016).
Following on previous work (Tang et al., 2018b), which presents
the preliminary results with only simulations, this paper presents the
following novel contributions: i) bounded disturbances (e.g. due to
wind, and unmodeled dynamics) are included in the dynamics of the
system; ii) a complete stability analysis shows that convergence to
the desired zero-height equilibrium is guaranteed in all cases and ulti-
mate boundedness of the horizontal position error is guaranteed when
landing in the presence of horizontal disturbances; iii) experimental
results are provided where the controllers run on an onboard computer
together with the image processing for the detection of window and
landing pad and for the computation of the translational optical flow.
The body of the paper consists of seven parts. Section 2 presents
the dynamic model, the fundamental equations of motion, and the
adopted hierarchical control architecture. Section 3 introduces the
environment and presents the image features that are used in the
control laws. Section 4 proposes two control laws: one for the landing
task in obstacle-free environments and the other for flying through the
window. A combination of these two control laws in the practical case
is also presented in this section. Section 5 shows simulation results
obtained with the proposed controller. Section 6 presents and analyzes
the experimental results which validate the proposed controllers. The
paper concludes with some final comments in Section 7.
1.1. Related work
There are several examples in the literature of recent work dedi-
cated to the problem of flying autonomous vehicles in complex envi-
ronment using vision systems. In Falanga et al. (2017), Loianno et al.
(2017) and Guo and Leang (2020), the authors specifically address
the problem of going through a window using only a single camera
and an IMU. However, estimation of vehicle’s position and velocity is
required in Falanga et al. (2017) and Loianno et al. (2017). Besides, the
pose of the window is assumed to be known in Loianno et al. (2017).
Although the work in Guo and Leang (2020) directly uses image feature
as position cue, estimates of the image depth are still required and the
velocity vector is assumed to be known. In general, state estimation
adds computational complexity, and the output is often sensitive to
image noise and camera calibration errors. The limited work on image-
based control approach can be explained by the complexity involved in
obtaining sound proofs of convergence and stability.
Landing in complex environments calls for obstacle avoidance ca-
pabilities, which are naturally provided by the use of optical flow, a
visual feature that draws inspiration from flying insects. Optical flow
measures the pattern of apparent motion of objects, surfaces, and edges
in a visual scene caused by the relative motion between an observer
and a scene (Burton & Radford, 1978). It has been experimentally
shown that the neural system of the insects reacts to optic flow pat-
terns to produce a large variety of flight capabilities, such as obstacle
avoidance, speed maintenance, odometry estimation, wall following
and corridor centering, altitude regulation, orientation control and
landing (Floreano & Wood, 2015; Serres & Ruffier, 2017). Using optical
flow as velocity cue and observed feature expressed in terms of an
unnormalized spherical centroid, a fully nonlinear adaptive visual servo
control design is provided in Mahony et al. (2008). Although estimating
the height of the camera above the landing plane was still required, it
was the first time that an IBVS control using image measurements for
both position and velocity was proposed, going beyond the kinematic
model to consider the dynamics. Based on Mahony et al. (2008) and
using optical flow, the authors proposed IBVS controllers for landing a
quadrotor (Hérissé et al., 2012; Serra et al., 2016) and landing a fixed-
wing aircraft eliminating the need to estimate the height of the vehicle
2

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 2. Reference frames and force for schematic representation of a quadrotor.
above the ground (Le Bras et al., 2014; Serra et al., 2015; Tang et al.,
2018a). Using a distinct paradigm, a novel setup of self-supervised
learning based on optical flow was introduced in Ho et al. (2018).
Using optical flow, the proposed method learns the visual appearance of
obstacles in order to search for a landing spot for micro aerial vehicles.
When compared to related work, this paper proposes simple IBVS
controllers applied in sequence to first go through a window and then
land on a planar target, using only vision measurements and requiring
no estimation of position, velocity, image depth, nor height above the
target. The present work also provides rigorous mathematical proofs for
stability and robustness in the presence of disturbances, complemented
by experimental validation of the proposed controllers.
2. Quadrotor modeling and control architecture
Consider a quadrotor equipped with an IMU and two cameras.
To describe the motion of the quadrotor, two reference frames are
introduced: an inertial reference frame {𝐼} fixed to the earth surface
and a body-fixed frame {𝐵} = (𝑒𝑏
1, 𝑒𝑏
2, 𝑒𝑏
3) attached to the quadrotor’s
center of mass (see Fig. 2). Let 𝑅= 𝐼
𝐵𝑅∈𝑆𝑂(3) denote the orientation
of the frame {𝐵} with respect to {𝐼} and let 𝜉∈R3 be the position
of the origin of the frame {𝐵} with respect to {𝐼}. Let 𝑣
∈
R3
denote the translational velocity expressed in {𝐼} and 𝛺∈R3 the
orientation velocity expressed in {𝐵}. The kinematics and dynamics of
the quadrotor vehicle are then described as
{
̇𝜉= 𝑣
𝑚̇𝑣= −𝐹+ 𝑚𝑔𝑒3+ ▵
(1)
{
̇𝑅= 𝑅𝛺×
I ̇𝛺= −𝛺×I𝛺+ 𝛤
(2)
with 𝑔the gravitational acceleration, 𝑚the mass of the vehicle, 𝑒3 =
[0 0 1]⊤and I its inertia matrix. The matrix 𝛺× denotes the skew-
symmetric matrix associated with the vector product 𝛺×𝑥= 𝛺× 𝑥, for
any 𝑥∈R3.
The vector 𝐹∈R3 expressed in {𝐼} combines the principal non-
conservative forces applied to the quadrotor and generated by the four
rotors. In quasi-hover conditions one can reasonably assume that this
aerodynamic force is always in the direction 𝑒𝑏
3 in {𝐵}, since all the
four thrusters are aligned with 𝑒𝑏
3 and their contribution predominates
over other components. Thus the 𝐹in the direction of 𝑒𝑏
3 expressed in
the inertial frame can be described as follows:
𝐹= 𝐹𝑇𝑅𝑒3
(3)
where the scalar 𝐹𝑇represents the total thrust magnitude generated
by the four motors. It also represents the unique control input for the
translational dynamics.
The term ▵combines the modeling errors and aerodynamic effects
due to the interaction of the rotors wake with the environment causing
random wind and dynamic inflow effects (Peters & HaQuang, 1988).
The vector 𝛤∈R3 expressed in {𝐵} is the torque control for the at-
titude dynamics. It is obtained via the combination of the contributions
of four rotors. The invertible linear map between [𝐹𝑇∈R+, 𝛤∈R3] and
the collection of individual thrusters [𝐹𝑇1, 𝐹𝑇2, 𝐹𝑇3, 𝐹𝑇4] can be found
in Hamel et al. (2002).
Fig. 3. A hierarchical control design strategy.
Fig. 4. Landing plane and window plane.
2.1. Control architecture
A hierarchical control design strategy is adopted in this paper (see
Fig. 3). This choice is motivated by the natural structure of the sys-
tem dynamics and its practical implementation (Bertrand et al., 2011;
Hérissé et al., 2012). For the translational dynamics of the quadrotor
(Eq. (1)), the force 𝐹(Eq. (3)) is used as control input by means of its
thrust direction and its magnitude. This constitutes a high-level outer
loop for the control design. The thrust 𝐹𝑇is directly the magnitude
of the designed force (𝐹𝑇= ‖𝐹‖) and the desired attitude 𝑅𝑑(partly
obtained by the desired direction 𝑅𝑑𝑒3 =
𝐹
‖𝐹‖ complemented by a
desired yaw) can then be reached by considering the body’s angular
velocity 𝛺as an intermediary control input, which constitutes again
a desired angular velocity for the fully actuated orientation dynamics
(Eq. (2)) via the high gain control torque 𝛤. The stabilization of the
orientation dynamics is not the subject of this paper and it is assumed
that a suitable low level robust stabilizing control is implemented, that
satisfactorily regulates the attitude error with a fast dynamics.
3. Environment and image features
In this section adequate image features in relation to the consid-
ered tasks are derived and all required assumptions regarding the
environment and the setup are established.
Assumption 1.
A downward-looking camera and a forward-looking
camera are attached to the center of mass of the vehicle. The
downward-looking camera reference frame coincides with the body-
fixed frame {𝐵}. The rotation matrix from the forward-looking camera
reference frame to the body frame 𝐵
𝐶𝑅∈𝑆𝑂(3) is known.
Assumption 2.
The angular velocity 𝛺is measured and the orien-
tation matrix 𝑅of {𝐵} with respect to {𝐼} is obtained by external
3

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
observer-based IMU measurements. This allows to represent all image
information and the system dynamics in the inertial frame.
Assumption 3. The landing target lies on a textured plane which is
called target plane. Its normal direction 𝜂𝑡∈S2 in the inertial frame is
known (typically 𝜂𝑡≈𝑒3), where S2 ∶= {𝑦∈R2 ∶‖𝑦‖ = 1} denotes the
2-Sphere and ‖.‖ the Euclidean norm.
Assumption 4. The target window has a rectangle shape and lies on a
textured wall which is called window plane. Its width 𝑟𝑤is known but
its normal direction 𝜂𝑤∈S2 is unknown.
Both landing plane and window plane are placed in the environ-
ment, as shown in Fig. 4. It assumed that the vehicle is able to recognize
the landing pad and the window from landmarks on the pad and from
corners and edges of the window respectively. The background texture
on both landing plane and window plane are also exploited to obtain
information about the vehicle’s velocity with respect to the planes and
also to avoid collisions with the wall and the window’s edges.
For any initial position (along with any initial velocity) outside the
room containing the landing pad, the main objective is to design a
feedback controller resorting only to image features that can ensure
automatic landing of the vehicle without any collision.
3.1. Image features on the landing plane
The target on the landing plane is depicted in Fig. 4. The axes of {𝐼}
are given by (𝑢𝑡, 𝜌𝑡, 𝜂𝑡), where 𝜌𝑡= 𝜂𝑡×𝑢𝑡, and the origin of {𝐼} is placed
at the center of the landing pad. As shown in Fig. 4, 𝑠𝑡
𝑖∈R3 denotes the
position of 𝑖th marker (or a corner) of the landing pad relative to the
inertial frame expressed in {𝐼}. Note that 𝜂⊤
𝑡𝑠𝑡
𝑖= 0. Define the position
vector of 𝑖th marker of the target relative to {𝐵} as
𝑃𝑡
𝑖= 𝑠𝑡
𝑖−𝜉.
(4)
The position of the vehicle relative to the center of the landing pad is
defined as
𝜉𝑡= −1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
𝑃𝑡
𝑖= 𝜉−1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
𝑠𝑡
𝑖
(5)
where 𝑛𝑡is the number of observed markers on the landing pad and
1
𝑛𝑡
∑𝑛𝑡
𝑖=1 𝑠𝑡
𝑖is a constant vector. This sum is zero when all markers are in
the camera field of view.
Using the spherical projection model for a calibrated camera, the
spherical image points of landing pad’s markers can be expressed as
𝑝𝑡
𝑖=
𝑃𝑡
𝑖
‖𝑃𝑡
𝑖‖ =
𝑠𝑡
𝑖−𝜉
‖𝑠𝑡
𝑖−𝜉‖
(6)
which can be obtained from the 2D pixel locations (𝑋𝑡
𝑖, 𝑌𝑡
𝑖) of the
camera image, such that
𝑝𝑡
𝑖= 𝑅
̄𝑝𝑡
𝑖
‖ ̄𝑝𝑡
𝑖‖ , with ̄𝑝𝑡
𝑖= 𝐴−1
⎡
⎢
⎢⎣
𝑋𝑡
𝑖
𝑌𝑡
𝑖
1
⎤
⎥
⎥⎦
.
(7)
The matrix 𝐴−1 in the above equation is the camera’s intrinsic parame-
ters that transforms image pixel to perspective coordinates ̄𝑝𝑡
𝑖. Note that
expressions (6) and (7) are the same and hence 𝑝𝑡
𝑖does not depend on
the orientation.
The visual feature used for the landing task is the centroid of the
observed visual feature.
𝑞𝑡∶= −1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
𝑝𝑡
𝑖= −𝑅
(
1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
̄𝑝𝑡
𝑖
‖ ̄𝑝𝑡
𝑖‖
)
(8)
which is the simplest image feature that encodes all information about
the position of the vehicle with respect to the landing plane. It is
not necessary to match observed image points with desired features
as required in classical image based visual servo control. Besides, the
Fig. 5. Window plane and unit directions ℎ𝑖normal to the planes defined by the origin
of camera frame and the 𝑖th window edge.
calculation of the image centroid is highly robust to pixel noise, and
easily computed in real-time in the camera frame and then derotated.
This ensures that 𝑞𝑡is invariant to any orientation motion (Hamel &
Mahony, 2002).
3.2. Image features on the window plane
As shown in Fig. 4, a rectangular window is placed on a textured
wall. Its corners and edges are assumed to be recognized in camera
images. Both information are combined together to extract the nor-
mal direction 𝜂𝑤and provide the feedback information used in the
controller.
Consider first the windows corners and let 𝑠𝑤
𝑖
∈R3 denote the
position of 𝑖th corner of the window expressed in {𝐼}. Define the
position vector of 𝑖th corner of the window relative to {𝐵} as
𝑃𝑤
𝑖
= 𝑠𝑤
𝑖−𝜉.
(9)
From there, one can deduce the position of the vehicle with respect to
the window’s center:
𝜉𝑤= −1
𝑛𝑤
𝑛𝑤
∑
𝑖=1
𝑃𝑤
𝑖
= 𝜉−1
𝑛𝑤
𝑛𝑤
∑
𝑖=1
𝑠𝑤
𝑖,
(10)
with 𝑛𝑤(typically 𝑛𝑤
=
4) number of the window’s corners and
1
𝑛𝑤
∑𝑛𝑤
𝑖=1 𝑠𝑤
𝑖constant vector.
Similarly to Section 3.1 and recalling that the forward-looking
camera is used to detect the window, the spherical image points of the
corners of the window are exploited:
𝑝𝑤
𝑖=
𝑃𝑤
𝑖
‖𝑃𝑤
𝑖‖ = 𝑅𝐵
𝐶𝑅
̄𝑝𝑤
𝑖
‖ ̄𝑝𝑤
𝑖‖ ,
(11)
with ̄𝑝𝑤
𝑖the perspective coordinates of the 𝑖th window’s corner, leading
the following centroid:
𝑞𝑤(𝑡) ∶= −1
𝑛𝑤
𝑛𝑤
∑
𝑖=1
𝑝𝑤
𝑖(𝑡) = −𝑅𝐵
𝐶𝑅
(
1
𝑛𝑤
𝑛𝑤
∑
𝑖=1
̄𝑝𝑤
𝑖
‖ ̄𝑝𝑤
𝑖‖
)
,
(12)
where 𝐵
𝐶𝑅is the rotation matrix from the forward-looking camera
reference frame to the body frame.
Now, to extract the normal direction 𝜂𝑤, recall that the axes rep-
resenting the window are given by (𝜂𝑤, 𝜌𝑤, 𝑢𝑤), with 𝜌𝑤= 𝑢𝑤× 𝜂𝑤
(see Fig. 5). Using the image of 𝑖th line and exploiting the fact that
the window has a rectangular shape, it is straightforward to get the
directions 𝑢𝑤and 𝜌𝑤and consequently 𝜂𝑤. As described in Mahony and
Hamel (2005), in the binormalized Euclidean Plucker coordinates, the
𝑖th line can be represented by its unit direction 𝑢𝑤(resp. 𝜌𝑤) and the
unit direction ℎ𝑖, which is normal to the plane defined by the origin of
the camera/body-fixed frame and the 𝑖th line. The unit vector ℎ𝑖can be
obtained directly from the images of lines which can be identified using
a convenient line detection technique, such as the Hough transform.
4

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 6. The green volume represents the region defined by inequality (17) which
excludes the window edges. (For interpretation of the references to color in this figure
legend, the reader is referred to the web version of this article.)
Using the fact that lines 1 and 3 (resp. lines 2 and 4) are parallel in the
inertial frame, one deduces the measure of the direction 𝑢𝑤(resp. 𝜌𝑤)
from the following relationships:
𝜌𝑤= ± ℎ1 × ℎ3
‖ℎ1 × ℎ3‖
(13)
𝑢𝑤= ± ℎ2 × ℎ4
‖ℎ2 × ℎ4‖ .
(14)
Then the normal vector to the window plane is directly obtained by
𝜂𝑤= ± 𝑢𝑤× 𝜌𝑤
‖𝑢𝑤× 𝜌𝑤‖
(15)
and the sign of Eq. (15) is chosen such that the condition 𝜂⊤
𝑤𝑞𝑤(0) < 0,
with 𝑞𝑤(𝑡) the image centroid of window’s corners in Eq. (12).
To exploit the image of window’s edges, defining the vector from the
vehicle to the closest point on window’s edges as 𝐿𝑒∈R3, its direction
𝑙𝑒=
𝐿𝑒
‖𝐿𝑒‖ can be obtained from the camera
𝑙𝑒= {𝑙𝑒
𝑖∶max{|𝜂⊤
𝑤𝑙𝑒
𝑖|}, 𝑖= {1, 2, 3, 4}}
(16)
where
𝑙𝑒
𝑖= ±(ℎ𝑖× 𝜌𝑤), 𝑖= {1, 3}, 𝑙𝑒
𝑖= ±(ℎ𝑖× 𝑢𝑤), 𝑖= {2, 4}
are the directions from the vehicle to the nearest point on each edge 𝑖.
Form now on, it is able to derive the required information achieving
the double goal of going through the window in the meanwhile avoid-
ing collision with the window edges and wall. We first define the safety
region such that
∶= {𝜉𝑤∶‖𝑞𝑤(𝜉𝑤)‖ ≤𝜖},
(17)
where 𝜖> 0 is chosen such that ∀𝜉𝑤∈, the condition ‖𝜉𝑤‖ < 𝑟𝑤
2 −𝜖
also holds, implying that the region does not contain the window
edges (see Fig. 6).
From there, the chosen visual feature that encodes all required in-
formation about the position of the vehicle with respect to the window
is:
̄𝑞𝑤∶= −1
𝑛𝑤
∑𝑛𝑤
𝑖=1 𝑝𝑤
𝑖
[
𝛼𝑤(𝑡)
1
𝜂⊤𝑤𝑝𝑤
𝑖
+ (1 −𝛼𝑤(𝑡))
𝜂⊤
𝑤𝑙𝑒
𝜂⊤𝑤𝑝𝑤
𝑖
]
,
(18)
where 𝛼𝑤(‖𝑞𝑤(𝜉𝑤)‖) is a weight function ensuring the continuity of ̄𝑞𝑤.
It is defined as follows:
𝛼𝑤(𝑡) =
⎧
⎪
⎨
⎪⎩
0
, if ‖𝑞𝑤‖ ≤𝜖(𝜉𝑤∈)
1
𝛿(‖𝑞𝑤‖ −𝜖)
, if 𝜖< ‖𝑞𝑤‖ < 𝜖+ 𝛿
1
, if ‖𝑞𝑤‖ ≥𝜖+ 𝛿,
(19)
with 𝛿an arbitrary small positive constant. Since 𝜂⊤
𝑤𝑙𝑒=
𝜂⊤
𝑤𝐿𝑒
‖𝐿𝑒‖ =
𝜂⊤
𝑤𝑃𝑤
𝑖
‖𝐿𝑒‖ ,
̄𝑞𝑤can be expressed in terms of the unknown distance 𝑑𝑜and 𝑑𝑒:
̄𝑞𝑤(𝑡) = 𝛼𝑤(𝑡) 𝜉𝑤(𝑡)
𝑑𝑜(𝑡) + (1 −𝛼𝑤(𝑡)) 𝜉𝑤(𝑡)
𝑑𝑒(𝑡)
(20)
where 𝑑𝑜∶= 𝜂⊤
𝑤𝑃𝑤
𝑖
= −𝜂⊤
𝑤𝜉𝑤is the distance from the camera to the wall
and 𝑑𝑒∶= ‖𝐿𝑒‖ =
√
𝑑2
𝑜+ ‖𝜋𝜂𝑤𝐿𝑒‖2 represents the distance from the
camera to the closest window’s edge.
3.3. Image kinematics and translational optical flow
The kinematics of any observed points on the landing plane (includ-
ing markers of the landing pad) can be written as:
̇𝑃𝑡= −̇𝜉= −𝑣
(21)
where 𝑃𝑡expressed in {𝐼} denotes any point on the textured ground of
the landing plane. So the kinematics of the corresponding image point
𝑝𝑡=
𝑃𝑡
‖𝑃𝑡‖ can be expressed as
̇𝑝𝑡= −𝜋𝑝𝑡
𝑣
‖𝑃𝑡‖ .
(22)
with
𝜋𝑦∶= 𝐼3 −𝑦𝑦⊤≥0,
the orthogonal projection operator in R3 onto the 2-dimensional vector
subspace orthogonal to any 𝑦∈S2. Let 𝑑𝑡be the height of the vehicle
above the landing plane:
𝑑𝑡∶= 𝜂⊤
𝑡𝑃𝑡= 𝜂⊤
𝑡𝑃𝑡
𝑖= −𝜂⊤
𝑡𝜉𝑡,
(23)
then Eq. (22) can be rewritten as
̇𝑝𝑡= −cos 𝜃𝑡𝜋𝑝𝑡𝜙𝑡(𝑡)
(24)
where cos 𝜃𝑡=
𝑑𝑡
‖𝑃𝑡‖ = 𝜂⊤
𝑡𝑝𝑡and 𝜙𝑡is the translational optical flow:
𝜙𝑡(𝑡) = 𝑣(𝑡)
𝑑𝑡(𝑡)
(25)
which is the ideal image velocity cue that can be complemented with
the centroid information for designing a pure IBVS controller to per-
form the landing task.
The translational optical flow 𝜙𝑡can be obtained by integrating ̇𝑝𝑡
(24) over a solid angle 𝑆2 of the sphere around the normal direction
𝜂𝑡to the landing plane. It can be shown that the average of the optical
flow is calculated as in Hérissé et al. (2012):
𝜙𝑡(𝑡) = −(𝑅𝑡𝛬−1𝑅⊤
𝑡) ∫∫𝑆2 ̇𝑝𝑡𝑑𝑝𝑡,
(26)
where matrix 𝛬is a constant diagonal matrix depending on parameters
of the solid angle 𝑆2, and 𝑅𝑡represents the orientation matrix of the
landing plane with respect to the inertial frame. Since {𝐼} is chosen
coincident with the target frame one has 𝑅𝑡= 𝐼3.
In practice, the optical flow is first measured in the camera frame
from the 2-D optical flow ̇̄𝑝𝑡obtained from a sequence of images using
the Lucas–Kanade algorithm and then derotated (see Hérissé et al.,
2012 for more detail). Note however that computing the optical flow
from (26) or directly from ̇̄𝑝𝑡in the camera frame and then derotating
it, the result is theoretically the same and does not depend on the
measured 𝛺nor on the estimated 𝑅.
Similarly, the kinematics of any observed points on the window
plane can be written in the inertial frame as
̇𝑃𝑤= −𝑣
(27)
where 𝑃𝑤expressed in {𝐼} denotes the position of a point on the
textured wall of the window plane with respect to {𝐵} expressed in
{𝐼}, not to be confused with 𝑃𝑤
𝑖
in Eq. (9), which is the position of the
𝑖th corner of the window with respect to {𝐵} and also expressed in {𝐼}.
So the kinematics of the corresponding image point 𝑝𝑤=
𝑃𝑤
‖𝑃𝑤‖ can be
written as
̇𝑝𝑤= −cos 𝜃𝑤𝜋𝑝𝑤𝑣
𝑑𝑜
(28)
with cos 𝜃𝑤=
𝑑𝑜
‖𝑃𝑤‖ = 𝜂⊤
𝑤𝑝𝑤. Analogously to the previous case, the
translational optical flow with respect to the textured wall
𝑣
𝑑𝑜can be
5

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
obtained from the integral of ̇𝑝𝑤along the direction 𝜂𝑤over a solid
angle.
Now, to achieve the goal that the vehicle is going through the
window smoothly, the translational optical flow with respect to the
closest window’s edge is also used. The kinematics of any observed
points on the closest window’s edge is
̇𝑃𝑒= −𝑣
(29)
where 𝑃𝑒denotes the position of a point on the closest edge from the
window. The kinematics of the corresponding image point 𝑝𝑒=
𝑃𝑒
‖𝑃𝑒‖
can be written as
̇𝑝𝑒= −cos 𝜃𝑒𝜋𝑙𝑒𝑣
𝑑𝑒
(30)
with cos 𝜃𝑒=
𝑑𝑒
‖𝑃𝑒‖ = 𝑙𝑒⊤𝑝𝑒. The translational optical flow with respect
to the closest window edge,
𝑣
𝑑𝑒, can be obtained from the integral of ̇𝑝𝑒
along the direction 𝑙𝑒over a solid angle.
Analogously to (18), the translational optical flow used for going
through the window is the convex combination of the translational
optical flow with respect to the textured wall and to the closest window
edge, respectively:
𝜙𝑤= 𝛼𝑤(𝑡) 𝑣(𝑡)
𝑑𝑜(𝑡) + (1 −𝛼𝑤(𝑡)) 𝑣(𝑡)
𝑑𝑒(𝑡)
(31)
with 𝛼𝑤(𝑡) defined already by (19).
4. Controller design
4.1. Landing in obstacle free environment
Theorem 1. Consider the system (1) in the nominal case (▵≡0) subjected
to the following feedback control:
𝐹𝑡= 𝐾𝑡
𝑝𝑞𝑡+ 𝐾𝑡
𝑑𝜙𝑡+ 𝑚𝑔𝑒3
(32)
with 𝐾𝑡
𝑝= 𝑘𝑡
𝑝1,2𝜋𝜂𝑡+ 𝑘𝑡
𝑝3𝜂𝑡𝜂⊤
𝑡
and 𝐾𝑡
𝑑= 𝑘𝑡
𝑑1,2𝜋𝜂𝑡+ 𝑘𝑡
𝑑3𝜂𝑡𝜂⊤
𝑡
two constant
positive definite matrices. Then, for any initial condition such that 𝑑𝑡(0) =
−𝜂⊤
𝑡𝜉𝑡(0) ∈R+, the following assertions hold ∀𝑡≥0:
(1) the height 𝑑𝑡(𝑡) = −𝜂⊤
𝑡𝜉𝑡(𝑡) ∈R+ and its derivative ̇𝑑𝑡(𝑡) ∈R are well
defined and uniformly bounded and converge to zero asymptotically,
(2) the acceleration
̇𝑣(𝑡) and the states (𝜉𝑡(𝑡), 𝑣(𝑡)) are bounded and
converge asymptotically to zero.
Proof. See Appendix A.
□
Proposition 1. Consider the system (1) in which ▵and ̇▵are bounded.
(1) If the perturbation ▵is such that:
▵= 𝜋𝜂𝑡▵, or equivalently 𝜂⊤
𝑡▵(𝑡) = 0, ∀𝑡≥0,
then, for any initial condition such that 𝑑𝑡(0) = −𝜂⊤
𝑡𝜉𝑡(0) ∈R+,
direct application of the feedback control (32) ensures that: (i) Item
1 of Theorem 1 holds, (ii) ̇𝑣(𝑡) and 𝑣(𝑡) are bounded and converging
asymptotically to zero, and finally (iii) ‖𝜋𝜂𝑡𝜉𝑡‖ is ultimately bounded
by 𝛥𝜉, solution of ‖𝜋𝜂𝑡𝑞𝑡‖ = ‖▵‖max
𝑘𝑡
𝑑1,2
.
(2) If 𝜂⊤
𝑡
▵(𝑡) ≠0, then, for any initial condition such that 𝑑𝑡(0) =
−𝜂⊤
𝑡𝜉𝑡(0) ∈R+, the following slightly modified feedback control:
𝐹𝑡= 𝐾𝑡
𝑝𝑞𝑡+ 𝐾𝑡
𝑑(𝜙𝑡−𝜂𝑡𝜙∗
𝑡) + 𝑚𝑔𝑒3
(33)
with 𝜙∗
𝑡
≥
1
𝑘𝑡
𝑑3
|𝜂⊤
𝑡▵(𝑡)|max, ensures that the above (i) and (ii)
assertions hold and guarantees that 𝜉𝑡is bounded.
Proof. See Appendix B.
□
Remark 1.
The focus of the above proposition is on robustness and
adaptation of the controller with respect to the bounded perturbation ▵.
It is introduced particularly to show robustness of the proposed control
law with respect to bounded perturbations in the plane orthogonal to
𝜂𝑡and, in the interest of a less complicated presentation, a slightly
modified version of the control law (32) is introduced in (33) to be
able to analyze the robustness of the closed loop system with respect
to any bounded disturbance.
4.2. Going through the center of the window
To accomplish the goal of going through the window, while avoid-
ing the wall and window edges, the following control law is proposed
𝐹𝑤= 𝜎(𝑞𝑤)(𝑘𝑤
𝑝𝜋𝜂𝑤̄𝑞𝑤+ 𝑘𝑤
𝑑𝜋𝜂𝑤𝜙𝑤+ 𝑘𝑤
𝜙𝜂𝑤(𝜂𝑤
⊤𝜙𝑤−𝜙∗
𝑤) + 𝑚𝑔𝑒3),
(34)
with 𝑘𝑤
𝑝, 𝑘𝑤
𝑑and 𝑘𝑤
𝜙positive gains, 𝜙∗
𝑤> 0 and
𝜎(𝑞𝑤) =
{
0
, if 𝜂⊤
𝑤𝑞𝑤≥0
1
, if 𝜂⊤
𝑤𝑞𝑤< 0,
(35)
which indicates that when the vehicle already crossed the window
(𝑑𝑜≤0), 𝐹𝑤= 0. Note that when 𝜂⊤
𝑤𝑞𝑤< 0, the resulting closed-loop
system can be written as
⎧
⎪
⎨
⎪⎩
̇𝜉𝑤= 𝑣
̇𝑣= −𝑘𝑤
𝑝𝜋𝜂𝑤
𝜉𝑤
𝑑𝑤
−𝑘𝑤
𝑑𝜋𝜂𝑤
𝑣
𝑑𝑤
−𝑘𝑤
𝜙𝜂𝑤(𝜂𝑤
⊤𝑣
𝑑𝑤
−𝜙∗
𝑤)+ ▵,
(36)
The unknown term 𝑑𝑤is a convex combination of the unknown dis-
tances 𝑑𝑜and 𝑑𝑒:
1
𝑑𝑤
= (𝛼𝑤
1
𝑑𝑜
+ (1 −𝛼𝑤) 1
𝑑𝑒
)
(37)
which is deduced from (20) and (31) according to the definition of 𝛼𝑤
(19):
𝑑𝑤=
⎧
⎪
⎨
⎪⎩
𝑑𝑒,
if ‖𝑞𝑤‖ ≤𝜖(𝜉𝑤∈)
𝑑𝑜𝑑𝑒
𝛼𝑤𝑑𝑒+(1−𝛼𝑤)𝑑𝑜,
if 𝜖< ‖𝑞𝑤‖ < 𝜖+ 𝛿
𝑑𝑜,
if ‖𝑞𝑤‖ ≥𝜖+ 𝛿.
(38)
Remark 2. Note that the unknown time varying distance 𝑑𝑤involved
in the closed-loop system is due to the use of feedback information
̄𝑞𝑤= 𝜉𝑤
𝑑𝑤and 𝜙𝑤=
𝑣
𝑑𝑤in the control law. It is the key feature to achieve
the double goal of avoiding collision with the wall and window edges
as well, while ensuring the main task of going through the center of
the window. When the vehicle approaches the wall or window edges
outside the region , 𝑑𝑤= 𝑑𝑜. If 𝑑𝑜is decreasing then the motion in the
orthogonal direction to the wall is highly damped while the region 
is highly attractive. In practice, this leads to a bounded high gain in the
feedback control that prevents collision. When the vehicle is inside the
region , 𝑑𝑤= 𝑑𝑒. This later is lower bounded by a positive constant
so that the vehicle is able to go through the center of the window with
a non-zero velocity. More details of analysis will be shown below.
Proposition 2.
Consider the system (1) with the control input given by
(34). If the positive gains 𝑘𝑤
𝑝, 𝑘𝑤
𝑑and 𝑘𝑤
𝜙are such that
𝑘𝑤
𝑑
2
𝑘𝑤𝑝
> 𝑟𝑤
2 and for
any arbitrary small 𝜖> 0, the chosen 𝜙∗
𝑤satisfies:
𝜙∗
𝑤>
|𝜂⊤
𝑤▵(𝑡)|max
𝑘𝑤
𝜙
+ 𝜖, ∀𝑡≥0,
(39)
then for any initial condition satisfying 𝑑𝑤(0)
∈
R+ and as long as
𝜎(𝑞𝑤(𝑡)) = 1, the following assertions hold ∀𝑡≥0:
(1) there exists a finite time 𝑡𝑤≥0 at which the vehicle enters the region
(‖𝑞𝑤(𝑡𝑤)‖ ≤𝜖) and remains there, while 𝑑𝑤(𝑡) ≥𝑑𝑜(𝑡) ∈R+, ∀𝑡<
𝑡𝑤,
6

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
(2) there exists a finite time 𝑡lim > 𝑡𝑤at which the vehicle crosses the
window 𝑑𝑜(𝑡lim) = 0, with strictly negative velocity ̇𝑑𝑜(𝑡lim) such that
the vehicle is inside the region (‖𝑞𝑤(𝑡)‖ ≤𝜖) for all 𝑡∈[𝑡𝑤, 𝑡lim).
Proof. See Appendix C.
□
4.3. Application scenario
The double goal of crossing the window and landing on the landing
pad can be achieved by simply applying the control laws 𝐹𝑤and 𝐹𝑡in
sequence, with an adequate trigger to switch from 𝐹𝑤to 𝐹𝑡. Taking the
limitation of the cameras’ field of view into the consideration, there
will be four different modes during the full process of going through
a window and landing on the pad. When 𝑡∈[𝑇1, 𝑇2), 𝑚𝑜𝑑𝑒= 1 and
𝐹𝑤(34) is active. When the vehicle approaches to the center of the
window, the on-board camera loses the full image of the window and
𝑚𝑜𝑑𝑒changes to 2. When 𝑡∈[𝑇2, 𝑇3), 𝑚𝑜𝑑𝑒2 is active and the open-
loop control 𝜂𝑤|𝜂⊤
𝑤𝐹𝑤(𝑇−
2 )| is applied, where 𝑇−
2 is the last time instance
before the camera loses the image of the window. At the time instance
𝑡= 𝑇3, when the downward-looking camera detects the landing pad,
the 𝑚𝑜𝑑𝑒changes to 3 and the control law 𝐹𝑡(32) is applied when 𝑡∈
[𝑇3, 𝑇4). At time instance 𝑡= 𝑇4, the vehicle is already close to the center
of the landing target and it is safe to slowly shutdown the quadrotor
motors. In order to avoid inadequate behaviors, the switch from 𝑚𝑜𝑑𝑒2
to 3 is only triggered once. Moreover, in practice, due to the limitation
of camera’s field of view, the initial errors should not be large and
should converge to zero fast enough, thereby allowing the vehicle to
almost align with the center of the window before switching to 𝑚𝑜𝑑𝑒
2. Additionally, the position of the landing target should be close
enough to the window so that the quadrotor is able to timely detect the
landing target after it goes through the window. The switching between
different modes is based on the combination of selected frames from
both the downward-looking and forward-looking on-board cameras
obtained in the experiments. The detail on the adopted procedure is
described in Section 6.
5. Simulation results
In this section, simulation results are presented to illustrate the
behavior of the closed-loop system using the proposed controller. A
high-gain inner-loop controller is used to control the attitude dynam-
ics (Tang et al., 2015). It generates the torque inputs in order to
stabilize the orientation of the vehicle to a desired one defined by
the desired thrust direction 𝑅𝑑𝑒3, which is provided by the outer-
loop image-based controller, and the desired yaw chosen to align the
forward-looking camera with direction orthogonal to the wall. The
control algorithm is tested with different initial conditions, always
starting from a position outside the room containing the target (see
Fig. 1). The initial velocity of the quadrotor is 𝑣(0) = [0 0 0]⊤, and
the gains are chosen as 𝐾𝑡
𝑝= diag[4 4 1.75], 𝐾𝑡
𝑑= 4𝐼3, 𝑘𝑤
𝑑= 0.8, 𝑘𝑤
𝑝= 1,
𝑘𝑤
𝜙= 1 and 𝜙∗
𝑤= 0.3
As shown in Fig. 1, with different initial positions the quadrotor
successfully avoids the wall and window, goes through the center of the
window, and then lands on the center of the landing target. Figs. 7–15
show in detail the time evolution of quadrotor’s state variables, virtual
input, and image features for the initial position 𝜉(0) = [−2 0.1 −1.82]⊤.
The time evolution of the active mode is also specified. In 𝑚𝑜𝑑𝑒1,
the quadrotor is approaching the window; in 𝑚𝑜𝑑𝑒2, it is crossing the
window with no image cues; in 𝑚𝑜𝑑𝑒3, it starts detecting the landing
pad and transitions to the landing maneuver; and finally in 𝑚𝑜𝑑𝑒4, the
motors are shutdown.
Fig. 7 shows the time evolution of the vehicle’s position and the
dashed lines are the coordinates of window’s center. From Fig. 7, one
can see the quadrotor first converges to the center line of the window
and then converges to the center point of the target. Fig. 8 shows the
time evolution of the vehicle’s velocity. The virtual control input 𝐹is
Fig. 7. Evolutions of the quadrotor’s position 𝜉and the mode.
Fig. 8. Evolutions of the quadrotor’s velocity 𝑣.
shown in Fig. 9. The angular velocity of the quadrotor is depicted in
Fig. 10 and Fig. 11 depicts the time evolution of Euler angles, which
indicates a good compromise in terms of time-scale separation between
the outer-loop and inner-loop controller. Figs. 12 and 13 show the
translational optical flow used for going through the window in 𝑚𝑜𝑑𝑒1
and for landing in 𝑚𝑜𝑑𝑒3, respectively. The evolution of image features
of ̄𝑞𝑤and 𝑞𝑡are depicted in Figs. 14 and 15, respectively. One can
see that the image features ̄𝑞𝑤and 𝑞𝑡approach to the desired values
[−1 0 0]⊤and [0 0 0]⊤, respectively, before the on-board cameras lose
the image information.
6. Experiments
6.1. Experimental setup
In order to set up the experiment, a movable wall was used to divide
the testing space into two smaller compartments and a landing pad was
placed on the ground of the second one. The partition wall contains
a rectangular window and is textured as a brick wall to provide the
background optical flow, as shown in Fig. 18. The vehicle used for
the experiments is an Asctec Pelican quadrotor (Fig. 16) with weight
1676g and the arm length from the center of mass to each motor is
20 cm. The available commands are thrust force and attitude which
are derived from the force 𝐹provided by the outer-loop controller
(32) (respectively (34)) and the desired yaw angle. The quadrotor
7

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 9. Evolutions of the virtual control input 𝐹.
Fig. 10. Evolution of angular velocity 𝛺.
Fig. 11. Evolution of Euler angles.
is equipped with two wide-angle cameras, one pointing towards the
ground and another is facing the forward direction, pointing at the
wall. Recalling Assumption 1, the downward-looking camera reference
Fig. 12. Translational optical flow using for going through the window during mode
1.
Fig. 13. Translational optical flow using for landing during mode 3.
Fig. 14. Image feature ̄𝑞𝑤during mode 1.
frame coincides with the vehicle’s body fixed frame and the rotation
matrix from the forward-looking camera reference frame to the body
frame is 𝐵
𝐶𝑅= 𝑅𝑍(−𝜋
4 )𝑅𝑋( 𝜋
2 ). These two cameras are uEye UI-122ILE
8

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 15. Image feature 𝑞𝑡during mode 3.
Fig. 16. Asctec Pelican quadrotor.
models featuring a 1/2-in sensor with global shutter which operate
at a resolution of 752 × 480 pixel at 50 frames per second and are
provisioned with 2.2-mm lenses.
In the experiments, a rapid prototyping and testing architecture are
used in which a MATLAB/Simulink environment integrates the sensors
and the cameras, the control algorithm and the communication with the
vehicle. The controller is developed and tuned on a MATLAB/Simulink
environment and C code is generated and compiled to run onboard the
vehicle as a final step. The onboard computer (a 4-core Intel i7-3612QE
at 2.1 GHz, named AscTec Mastermind) is responsible for running in
Linux three major software components that provide:
(1) interface with the camera hardware, image acquisition, feature
detection and optical flow computation;
(2) computation of the vehicle force references from the image
features, translational optical flow, and angular velocity and
rotation matrix estimates provided by the IMU;
(3) interface with microprocessor, receiving IMU data and sending
force references to the inner-loop controller.
A Python program running on the onboard computer performs
detection of the window, detection of landing target, and optical flow
computation using the OpenCV library. ARUCO markers, for which
built-in detection functions exist in the OpenCV library, are used to
define the landmarks on the landing pad. In order to fit the camera’s
field of view during the full process of landing, the landmarks are
composed by 4 groups of ARUCO markers and in each group there
are 4 ARUCO markers with same border size but different identifier
Fig. 17. ARUCO markers on the landing pad.
Fig. 18. Selected frames from the forward-looking camera. (For interpretation of the
references to color in this figure legend, the reader is referred to the web version of
this article.)
(id) as shown in Fig. 17. When the camera is far away from the
markers, the group of larger markers can be seen and when the camera
is near the ground, only the smaller group of the landmarks will be
shown in the field of view. The rectangular window shape is detected
using the library code originally developed for ARUCO marker’s border
detection. The detected window frame (in green) and the window’s
coordinate system overlayed on the image are show in Fig. 18-(1), (2),
and (3). The translational optical flow is also computed onboard. The
computation is based on the conventional image plane optical flow
field provided by a pyramidal implementation of the Lucas–Kanade
algorithm. The detailed description of the computation can be found
in Serra et al. (2016). The small vectors represented in Fig. 19 represent
the translational optical flow of the image pixels.
In order to provide ground truth measurements and evaluate the
performance of the proposed controller, a VICON motion capture sys-
tem (VICON, 2014) which comprises 12 cameras is used together with
markers attached to the quadrotor, window, and landing target. The
motion capture system is able to accurately locate the position of the
markers, from which ground truth position and orientation measure-
ments are gathered. Note that, none of the measurements from the
motion capture system are used in the proposed controller.
6.2. Experimental results
The experiments were conducted with the same control gains as the
simulations. Before the proposed controller is triggered, the vehicle is
hovering at position 𝜉= [0.15, 1.79, −1.76] m, which is outside the space
9

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 19. Selected frames from the downward-looking camera.
containing the landing pad. As mentioned in Section 4.3, there are four
different modes during the full process of going through a window and
landing on the target due to the limitation of the field of view of the on-
board cameras. Fig. 18 shows the selected frames in a timed sequence
from the forward-looking camera. These four frames are a fixed time
step apart and were taken during a 𝑚𝑜𝑑𝑒1 to 𝑚𝑜𝑑𝑒2 transition. In
Fig. 18-(1), (2), and (3), 𝑚𝑜𝑑𝑒1 is active, and one can see that the
window frame is well detected. In Fig. 18-(1), 𝑡= 𝑇1 and the controller
𝐹𝑤is triggered. In Fig. 18-(2) and (3), the vehicle is still in 𝑚𝑜𝑑𝑒1
and approaches the center of the window. As the vehicle approaches
the window, the window frame disappears from the field of view of
the camera and at time 𝑡= 𝑇2 the 𝑚𝑜𝑑𝑒commutes to 2, as shown in
Fig. 18-(4). Note that during transition from 𝑚𝑜𝑑𝑒1 to 2, instead of
losing the window frame, the camera may detect rectangles other then
the target window, as depicted in Fig. 18-(4). In order to avoid this
situation, the 𝑚𝑜𝑑𝑒changes from 1 to 2 if the pixel coordinates change
instantaneously in a way that is incompatible with smooth tracking of
the same window object. Fig. 19 shows the selected frames in a timed
sequence from the downward-looking camera. These four frames were
taken at fixed time steps during a transition from 𝑚𝑜𝑑𝑒3 to 4. At time
instance 𝑡= 𝑇3, as shown in Fig. 19-(1) and (2), the downward-looking
camera detects successfully the landing pad, the 𝑚𝑜𝑑𝑒is switched to 3
and 𝐹𝑡is applied as control input. Recall that the switching from 𝑚𝑜𝑑𝑒2
to 𝑚𝑜𝑑𝑒3 is only triggered once in order to avoid inadequate behavior.
In Fig. 19-(3), the vehicle approaches the target and the 𝑚𝑜𝑑𝑒is still 3.
At the time instance 𝑡= 𝑇4, when the quadrotor has almost reached the
target position (see Fig. 19-(4)), the 𝑚𝑜𝑑𝑒changes to 4 and it is safe to
slowly shutdown the motors.
Figs. 20 and 21 show the position and velocity coordinates of
the vehicle provided by VICON, respectively. We can see that the
vehicle goes through the center of the window at the end of 𝑚𝑜𝑑𝑒
2 and finally lands on the target. Fig. 22 shows the evolution of the
angular velocity and Fig. 23 show the evolution of the Euler angles.
From Fig. 23, one can see that a good compromise in terms of time-
scale separation between the outer-loop and inner-loop controllers is
attained, which indicates that the inner-loop controller is sufficiently
fast to track the outer-loop references, including during the transitions
between different modes. The evolution of the virtual control input
𝐹is depicted in Fig. 24. Fig. 25 shows the image feature ̄𝑞𝑤used
for going through the window. The solid line represents ̄𝑞𝑤computed
from the image sequence and the dashed line represents ̄𝑞𝑤provided
by the VICON system. There are slight differences between these two
computations due to the fact that rotation matrix 𝑅provided by the
IMU is affected by the surrounding magnetic field generated by the
fast rotating motors. Figs. 27 and 26 show the translational optical flow
used for going through the window and for landing respectively. The
solid red lines represent the translational optical flow computed from
Fig. 20. Evolutions of the quadrotor’s position 𝜉and the mode.
Fig. 21. Evolutions of the quadrotor’s velocity 𝑣.
Fig. 22. Evolution of angular velocity 𝛺.
the image sequence and the dashed line represents the translational
optical flow derived from VICON measurements. The video of the
experimental results can be found in https://youtu.be/DbpeGfJMHk0.
10

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Fig. 23. Evolution of Euler angle.
Fig. 24. Evolutions of the vitual control input 𝐹.
Fig. 25. Image features ̄𝑞𝑤computed from the image sequence (solid line) and from
the VICON measurements (dashed line) during mode 1.
Fig. 26. Translational optical flow computed during 𝑚𝑜𝑑𝑒1 (going through window)
from the image sequence (solid line) and from the VICON measurements (dashed line).
Fig. 27. Translational optical flow computed during 𝑚𝑜𝑑𝑒3 (landing) from the image
sequence (solid line) and from the VICON measurements (dashed line).
7. Conclusion
This paper considers the problem of controlling a quadrotor to go
through a window and land on planar target, using an Image-Based
Visual Servo (IBVS) controller. For control purposes, the centroids
vectors provided by the combination of the corresponding spherical
image measurements of landmarks (corners) for both the window and
the target are used as position feedback. The translational optical
flow relative to the wall, window edges, and landing plane is used
as velocity measurement. To achieve the proposed objective, no direct
measurements of position or velocity are used and no explicit estimate
of the height above the landing plane or of the distance to the wall
is required. With the initial position outside the room containing the
target, the proposed control law guarantees that the quadrotor aligns
itself with the center line orthogonal to the window, crosses it with
non-zero velocity and finally lands on the planar target successfully
without colliding the wall or the edges of the window. Rigourous
proofs of convergence and/or piratical stability of closed-loop system
are provided when the system is subjected to unknown bounded dis-
turbances. Simulation and experimental results show the effectiveness
of the overall proposed control scheme.
11

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Appendix A. Proof of Theorem 1
Proof. The proof follows a reasoning very similar to that of Theorem 1
in Rosa et al. (2014). Recalling (1) and applying the control input (32),
the closed-loop system can be written as
⎧
⎪
⎨
⎪⎩
̇𝜉𝑡= 𝑣
̇𝑣= −𝐾𝑡
𝑝𝑞𝑡(𝜉𝑡) −𝐾𝑡
𝑑
𝑣
𝑑𝑡
.
(40)
Before proceeding with the proof of item (1), a positive definite
storage function 2(𝜉𝑡, 𝑣) will be defined and one will show that if 𝑑𝑡(𝑡)
remains positive,
̇2 is negative semi-definite, which implies that the
solutions remain bounded for all 𝑡≥0. Define 2 as
2(𝜉𝑡, 𝑣) = 1(𝜉𝑡) + 1
2𝑣⊤𝐾𝑡
𝑝
−1𝑣
(41)
where 1(𝜉𝑡) is the radially unbounded function given by
1(𝜉𝑡) = 1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
(‖𝑃𝑡
𝑖(𝜉𝑡)‖ −‖𝑃𝑡
𝑖(0)‖).
(42)
To show that 1(𝜉𝑡) is a positive definite function, note that
𝜕1
𝜕𝜉𝑡
= 𝑞⊤
𝑡
(43)
𝜕21
𝜕𝜉2
𝑡
= 𝑄
(44)
where 𝑄=
1
𝑛𝑡
∑𝑛𝑡
𝑖=1
1
‖𝑃𝑡
𝑖‖ 𝜋𝑝𝑡
𝑖is positive definite, as long as at least
two of the vectors 𝑝𝑡
𝑖are non-collinear. It follows that 1 is a convex
function of 𝜉𝑡, with a global minimum attained when 𝜕1
𝜕𝜉𝑡= 𝑞⊤
𝑡= 0, or
equivalently, when 𝜉𝑡= 0. Since 1(0) = 0 is the global minimum of the
function, one concludes that 1(𝜉𝑡) is positive-definite. Noting that,
̇1 = 𝑞⊤
𝑡𝑣,
(45)
it follows that
̇2 = −1
𝑑𝑡
𝑣⊤𝐾𝑡
𝑝
−1𝐾𝑡
𝑑𝑣
(46)
which is negative semi-definite as long as 𝑑𝑡remains positive and im-
plies that the states 𝜉𝑡(𝑡) and 𝑣(𝑡) remain bounded for all 𝑡≥0. The next
steps of the proof consist in proving first Item (1) and then the uniform
continuity of (46) along every system’s solution in order to deduce, by
application of Barbalat’s Lemma, the asymptotic convergence of 𝑣to
zero and from there one deduces the asymptotic convergence of ̇𝑣and
then 𝜉𝑡to zero (Item (2)).
Proof of Item 1: Using (40) and the fact that 𝑑𝑡(𝑡) = −𝜂⊤
𝑡𝜉𝑡and
̇𝑑𝑡= −𝜂𝑡⊤𝑣yields
̈𝑑𝑡= −𝑘𝑡
𝑑3
̇𝑑𝑡
𝑑𝑡
−𝑘𝑡
𝑝3𝛽𝑡
(47)
with
𝛽𝑡(𝑡) = −𝜂⊤
𝑡𝑞𝑡= 1
𝑛𝑡
∑𝑛𝑡
𝑖=1
𝑑𝑡
‖𝑃𝑖
𝑡‖ > 0, ∀𝑡.
(48)
This relation is of course valid as long as 𝑑𝑡(𝑡) > 0. From there, direct
application of Rosa et al. (2014, Th. 1-(2)) shows that if 𝑑𝑡(0) ∈R+,
the solution (𝑑𝑡, ̇𝑑𝑡) ∈(R+, R) exists and uniformly bounded ∀𝑡and
converges asymptotically to (0, 0).
Proof of Item 2: To show that
̈2 = −2
𝑑𝑡
𝑣⊤𝐾𝑡
𝑝
−1𝐾𝑡
𝑑̇𝑣+
̇𝑑𝑡
𝑑𝑡
1
𝑑𝑡
𝑣⊤𝐾𝑡
𝑝
−1𝐾𝑡
𝑑𝑣
is bounded and hence
̇2 (46) is uniformly continuous, it suffices to
show that ‖𝑣‖
𝑑𝑡
is bounded (so is
̇𝑑𝑡
𝑑𝑡). For that purposes, consider the
dynamics of
𝑣
𝑑𝑡∶
𝑑
𝑑𝑡( 𝑣
𝑑𝑡
) = −1
𝑑𝑡
((𝐾𝑡
𝑑+ ̇𝑑𝑡𝐼) 𝑣
𝑑𝑡
+ 𝐾𝑡
𝑝𝑞𝑡).
(49)
Since
̇𝑑𝑡converges asymptotically to zero and 𝑞𝑡is bounded then, by
direct application of Rosa et al. (2014, Lemma 4) one ensures that
𝑣
𝑑𝑡
is bounded. From there one concludes that ̇2 is uniformly continuous
and hence 𝑣converges asymptotically to zero.
To prove that 𝑞𝑡(𝑡) (or equivalently 𝜉𝑡) is asymptotically converging
to zero one has to show first ̇𝑣is converging to zero. From (40), one
can verify that:
̈𝑣= −
𝐾𝑡
𝑑
𝑑𝑡
̇𝑣+ 𝛿0
̇𝑣,
(50)
with 𝛿0
̇𝑣= 𝐾𝑡
𝑑
̇𝑑𝑡
𝑑𝑡
𝑣
𝑑𝑡−𝐾𝑡
𝑝̇𝑞𝑡. Since
𝑣
𝑑𝑡(and hence
̇𝑑𝑡
𝑑𝑡) is bounded and
̇𝑞𝑡= 𝑄𝑣= 𝑄0
𝑣
𝑑𝑡
, with 𝑄0 = 1
𝑛𝑡
𝑛𝑡
∑
𝑖=1
𝑑𝑡
‖𝑃𝑡
𝑖‖ 𝜋𝑝𝑡
𝑖< 𝐼3,
is also a bounded vector, one ensures that 𝛿0
̇𝑣is bounded. Therefore,
direct application of Rosa et al. (2014, Lem. 3) concludes boundedness
and the asymptotic convergence of ̇𝑣to zero and hence one has:
𝑣
𝑑𝑡
= −𝐾𝑡
𝑑
−1𝐾𝑡
𝑝𝑞𝑡+ 𝑜(𝑡)
(51)
with 𝑜(𝑡) a asymptotically vanishing term.
By multiplying both sides of the above equation by the bounded
vector 𝑞⊤
𝑡(the gradient of 1) and using the fact that
̇1 = 𝑞⊤
𝑡𝑣(45),
one obtains:
̇1 = −𝑑𝑡𝑞⊤
𝑡𝐾𝑡
𝑑
−1𝐾𝑡
𝑝𝑞𝑡+ 𝑑𝑡𝑞⊤
𝑡𝑜(𝑡).
(52)
Since 𝑑𝑡(𝑡) converges asymptotically to zero, then by taking the integral
of (47)
̇𝑑𝑡(𝑡) −̇𝑑𝑡(0) = −𝑘𝑡
𝑑3 log( 𝑑𝑡(𝑡)
𝑑𝑡(0) ) −𝑘𝑡
𝑝3 ∫
𝑡
0
𝛽𝑡(𝜏)𝑑𝜏,
(53)
one concludes that
lim
𝑡→∞∫
𝑡
0
𝛽𝑡(𝜏)𝑑𝜏= +∞.
(54)
Combining Eq. (54) with the fact that 𝑑(𝑡) ≥𝛽𝑡(𝑡) (from (48)) and
replacing the time index 𝑡of Eq. (52) by the new time-scale index
𝑠(𝑡) ∶= ∫𝑡
0 𝑑𝑡(𝜏)𝑑𝜏(𝑠tends to infinity if and only if 𝑡tends to infinity),
one has:
𝑑
𝑑𝑠1 = −𝑞⊤
𝑡𝐾𝑡
𝑑
−1𝐾𝑡
𝑝𝑞𝑡+ 𝑞⊤
𝑡𝑜(𝑡),
(55)
from which one concludes that 𝑞𝑡(and 𝜉𝑡) is asymptotically converging
to zero.
□
Appendix B. Proof of Proposition 1
Proof. The proof follows and exploits the same technical steps of the
proof of Theorem 1. Since assertions made are almost the same using
either (33) or (32) (equivalently (33) with 𝜙∗
𝑡= 0) except for the last
item (iii), the proof will be provided using (33) as feedback control and
differences will be specified when necessaries.
When ▵≠0 and 𝜙∗
𝑡≠0, it is straightforward to verify that (46)
becomes:
̇2 = −1
𝑑𝑡
𝑣⊤𝐾𝑡
𝑝
−1𝐾𝑡
𝑑𝑣+ 𝑣⊤𝐾𝑡
𝑝
−1(▵+𝐾𝑡
𝑑𝜙∗
𝑡𝜂𝑡)
(56)
Recall now the dynamics of ̇𝑑𝑡(47), 𝑣
𝑑𝑡(49), and of ¨v (50) in case where
▵≠0 and 𝜙∗
𝑡≠0.
̈𝑑𝑡= −𝑘𝑡
𝑑3
̇𝑑𝑡
𝑑𝑡
−𝑘𝑡
𝑝3𝛽▵
𝑡
(57)
12

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
𝑑
𝑑𝑡( 𝑣
𝑑𝑡
) = −1
𝑑𝑡
((𝐾𝑡
𝑑+ ̇𝑑𝑡𝐼) 𝑣
𝑑𝑡
+ 𝛿▵
𝑣)
(58)
̈𝑣= −
𝐾𝑡
𝑑
𝑑𝑡
̇𝑣+ 𝛿▵
̇𝑣,
(59)
with
𝛽▵
𝑡(𝑡) =
1
𝑘𝑡
𝑝3
(𝑘𝑡
𝑑3𝜙∗
𝑡+ 𝜂⊤
𝑡▵) −𝜂⊤
𝑡𝑞𝑡
(60)
𝛿▵
𝑣= 𝐾𝑡
𝑝𝑞𝑡−▵−𝐾𝑡
𝑑𝜂𝑡𝜙∗
𝑡
(61)
𝛿▵
̇𝑣= 𝐾𝑑
̇𝑑𝑡
𝑑𝑡
𝑣
𝑑𝑡
−𝐾𝑡
𝑝̇𝑞𝑡+ ̇▵.
(62)
Now since 𝛽▵
𝑡(𝑡) > 0, ∀𝑡independently from the value chosen for 𝜙∗
𝑡,
direct application of Rosa et al. (2014, Th. 1-(2)) shows that the solu-
tion (𝑑𝑡, ̇𝑑𝑡) ∈(R+, R) exists and uniformly bounded ∀𝑡and converges
(at least) asymptotically to (0, 0).
By combining this with the fact that all terms involved in 𝛿▵
𝑣(𝑞𝑡, ▵
and 𝜙∗
𝑡) are bounded, direct application of Rosa et al. (2014, Lem. 4)
concludes the boundedness of
𝑣
𝑑𝑡. Since 𝑑𝑡is converging to zero, one
concludes that 𝑣is converging to zero by a direct application of Rosa
et al. (2014, Lem. 3). Using the fact that ̇▵is bounded by assumption,
the proof of boundedness ¨v (59) and its convergence to zero is directly
deduced from to proof the unperturbed case (50). From there and
analogously to the unperturbed case (Theorem 1- proof of Item (2)),
one gets:
𝑣
𝑑𝑡
= −𝐾𝑡
𝑑
−1𝐾𝑡
𝑝𝑞𝑡+ 𝐾𝑡
𝑑
−1 ▵+𝜂𝑡𝜙∗
𝑡+ 𝑜(𝑡)
(63)
with 𝑜(𝑡) an asymptotically vanishing term.
By multiplying both sides of (63) by 𝑞⊤
𝑡
and using the fact that
̇1 = 𝑞⊤
𝑡𝑣(45), one obtains:
̇1
𝑑𝑡
= −𝑞⊤
𝑡𝐾𝑡
𝑑
−1𝐾𝑡
𝑝𝑞𝑡+ 𝑞⊤
𝑡(𝐾𝑡
𝑑
−1 ▵+𝜂𝑡𝜙∗
𝑡+ 𝑜(𝑡)).
(64)
From there one distinguishes between the two issues stated in the
proposition:
(1) 𝜂⊤
𝑡▵(𝑡) = 0, ∀𝑡and 𝜙∗
𝑡= 0 (𝐹𝑡given by (32))
By changing the time scale index and similarly to argument used
at the end of the proof of Theorem 1, one concludes that ‖𝑞𝑡‖ is
ultimately bounded by ‖▵‖max
𝑘𝑡𝑝1,2
. Since 𝑑𝑡= 𝜂⊤
𝑡𝜉𝑡converges to zero, one
concludes that ‖𝜋𝜂𝑡𝜉𝑡‖ is ultimately bounded by 𝛥𝜉which is the solution
of ‖𝜋𝜂𝑡𝑞𝑡‖ = ‖▵‖max
𝑘𝑡
𝑑1,2
.
(2) 𝜂⊤
𝑡▵(𝑡) ≠0, and 𝜙∗
𝑡≠0 (𝐹𝑡given by (33))
In that case one concludes that the storage function 1 is decreasing
as long as the right hand side of the above equation is negative and
𝑑𝑡> 0 and hence 𝜉𝑡is bounded. The argument of changing the time
index is not valid in this case.
□
Appendix C. Proof of Proposition 2
Proof. We will consider hereafter only the case where 𝜎(𝑞𝑤) = 1 (or
equivalently when 𝜂⊤
𝑤𝑞𝑤< 0). That is the situation in which the vehicle
is going through the window while avoiding collision with the wall and
the window edges.
From the dynamics of the closed-loop system (36), the proof focus
first on the evolution of 𝑑𝑤. That is the evolution of the system in the
direction 𝜂𝑤.
When ‖𝑞𝑤(𝑡)‖ ≥𝜖+ 𝛿, one has 𝑑𝑤= 𝑑𝑜= −𝜂𝑇
𝑤𝜉𝑤and hence:
̇𝑑𝑜= −𝜂⊤
𝑤𝑣
̈𝑑𝑜= −𝑘𝑤
𝜙
̇𝑑𝑜
𝑑𝑜
−𝑘𝑤
𝜙𝛽𝑤
(65)
with 𝛽𝑤= 𝜙∗
𝑤+
𝜂⊤
𝑤▵
𝑘𝑤
𝜙
≥𝜖.
When 𝜖< ‖𝑞𝑤(𝑡)‖ < 𝜖+ 𝛿, one has 𝑑𝑤=
𝑑𝑜𝑑𝑒
𝛼𝑤𝑑𝑒+(1−𝛼𝑤)𝑑𝑜with 𝛼𝑤
(defined by (19)) a uniformly continuous and bounded valued function
on [0, 1], and hence one verifies that:
̈𝑑𝑜(𝑡) = −𝑘𝑤
𝜙𝑏(𝑡)
̇𝑑𝑜(𝑡)
𝑑𝑜(𝑡) −𝑘𝑤
𝜙𝛽𝑤
(66)
with 𝑏(𝑡) =
(1−𝛼𝑤(𝑡))𝑑𝑜(𝑡)+𝛼𝑤(𝑡)𝑑𝑒(𝑡)
𝑑𝑒(𝑡)
a positive uniformly continuous and
bounded function as long as 𝜖< ‖𝑞𝑤(𝑡)‖ < 𝜖+ 𝛿and 𝑑𝑜(0) ∈R+.
direct application of Hérissé et al. (2012, Th. 5.1) to both Eqs. (65)
and (66), one can conclude that as long as 𝑑𝑜(0) ∈R+ and ‖𝑞𝑤(𝑡)‖ > 𝜖
(or equivalently 𝜉𝑤∉), 𝑑𝑜(𝑡) ∈R+, ∀𝑡≥0 and 𝑑𝑜(𝑡) converges to
zero exponentially (the exponential convergence of 𝑑𝑜(𝑡) is granted due
to the fact that 𝛽𝑤≥𝜖) but never crosses zero and hence the vehicle
will never touch the wall in a finite time. Additionally, one also proves,
from Hérissé et al. (2012, Th. 5.1), that there exists a finite time 𝑡1 ≥0
such that ̇𝑑𝑜(𝑡) < 0, ∀𝑡≥𝑡1 and hence 𝑑𝑜and 𝑑𝑤are decreasing after 𝑡1.
When ‖𝑞𝑤(𝑡)‖ ≤𝜖(the situation when 𝜉𝑤∈), one has 𝑑𝑤= 𝑑𝑒>
𝑑𝑜. In this case one can easily verify that (65) can be rewritten as:
̈𝑑𝑜= −𝑘(𝑡) ̇𝑑𝑜−𝑘𝑤
𝜙𝛽𝑤
(67)
with 𝑘(𝑡) =
𝑘𝑤
𝜙
𝑑𝑒a upper bounded positive gain as long as 𝑑𝑒(𝑡) is positive.
Due to the fact that 𝛽𝑤≥𝜖, ̇𝑑𝑜is ultimately bounded by −
𝑘𝑤
𝜙𝛽𝑤
𝑘(𝑡) ≤−
𝑘𝑤
𝜙𝜖
𝑘(𝑡)
and hence one immediately ensures that there exists a finite time 𝑡2 ≥0
from which ̇𝑑𝑜(𝑡) < 0, ∀𝑡≥𝑡2. This implies that when ‖𝑞𝑤(𝑡)‖ ≤𝜖(𝜉𝑤∈
), 𝑑𝑜is decreasing ∀𝑡≥𝑡2 and hence 𝑑𝑜crosses zero in a finite time
̄𝑡> 𝑡2. Note that at 𝑡= ̄𝑡, one has 𝜎(𝑞𝑤(̄𝑡)) = 0 according to (35).
Consider now the dynamics of the closed-loop system (36) in the
plane 𝜋𝜂𝑤. That is the dynamics of 𝜉⟂∶= 𝜋𝜂𝑤𝜉𝑤. By defining 𝑣⟂∶= 𝜋𝜂𝑤𝑣
and ▵⟂∶= 𝜋𝜂𝑤▵, one gets:
̇𝜉⟂=𝑣⟂
(68)
̇𝑣⟂= −
𝑘𝑤
𝑑
𝑑𝑤
(𝑣⟂+
𝑘𝑤
𝑝
𝑘𝑤
𝑑
𝜉⟂)+ ▵⟂.
(69)
Define a new state
𝑧= 𝑣⟂+
𝑘𝑤
𝑝
𝑘𝑤
𝑑
𝜉⟂,
(70)
and the following positive definite storage function:
3 = 1
2 ‖𝑧‖2 + 1
2(
𝑘𝑤
𝑝
𝑘𝑤
𝑑
)2‖𝜉⟂‖2,
with time derivative
̇3 = −(
𝑘𝑤
𝑝
𝑘𝑤
𝑑
)3‖𝜉⟂‖2 −(
𝑘𝑤
𝑑
𝑑𝑤
−
𝑘𝑤
𝑝
𝑘𝑤
𝑑
)‖𝑧‖2 + 𝑧⊤▵⟂
≤−(
𝑘𝑤
𝑝
𝑘𝑤
𝑑
)3‖𝜉⟂‖2 −‖𝑧‖
𝑘𝑤
𝑑
((
𝑘𝑤
𝑑
2
𝑑𝑤
−𝑘𝑤
𝑝)‖𝑧‖ −𝑘𝑤
𝑑‖ ▵⟂‖),
(71)
which is negative-definite provided that 0 < 𝑑𝑤<
𝑘𝑤
𝑑
2
𝑘𝑤𝑝
and ‖𝑧‖ ≥
𝑑𝑤𝑘𝑤
𝑑‖▵⟂‖max
𝑘𝑤
𝑑
2−𝑑𝑤𝑘𝑤𝑝
.
Proof of Item 1:
To show there exists a finite time 𝑡𝑤≥0 at which the vehicle enters
the region and remains there as long as 𝜎(𝑞𝑤) = 1, one proceeds
using a proof by contradiction in two steps.
In the first step, assume that 𝜉𝑤is not converging to in a finite
time 𝑡𝑤and hence ‖𝑞𝑤(𝑡)‖ > 𝜖, ∀𝑡. In the second one, assume that 𝜉𝑤is
switching indefinitely between the two regions.
(i) Consider the situation for which the initial condition is such that
‖𝑞𝑤(0)‖ > 𝜖(outside the region ). Using the fact that there exists
a finite time instant 𝑡1 from which 𝑑𝑤is decreasing and converging to
zero but never crosses zero in finite time (see the above discussion), one
concludes that 𝑧(70) is exponentially converging to zero and hence:
𝑣⟂= ̇𝜉⟂= −
𝑘𝑤
𝑝
𝑘𝑤
𝑑
𝜉⟂+ 𝑜(𝑡),
13

Z. Tang, R. Cunha, D. Cabecinhas et al.
Control Engineering Practice 112 (2021) 104827
with 𝑜(𝑡) an exponential vanishing term. This in turn implies that 𝜉⟂
(resp. 𝑣⟂) is converging to zero exponentially. Combining this with the
fact that 𝑑𝑤(𝑡) (resp. 𝑑𝑜(𝑡)) is converging to zero, one concludes that
there exists a finite time 𝑡𝑤at which ‖𝑞𝑤(𝑡𝑤)‖ < 𝜖(𝜉𝑤(𝑡𝑤) ∈), which
contradicts the first part of the assumption.
(ii) Consider the situation for which the vehicle is switching indefinitely
between the two regions. Since 𝑑𝑜(𝑡) (respectively 𝑑𝑤) is decreasing
∀𝑡≥max{𝑡1, 𝑡2} for both cases of ‖𝑞𝑤‖ > 𝜖and ‖𝑞𝑤‖ ≤𝜖with the
fact that (𝜉⟂, 𝑣⟂) converges exponentially to (0, 0) (proof of the step
(i)), one concludes that there exists a finite time 𝑡𝑤≥0 at which the
vehicle enters the region (‖𝑞𝑤(𝑡𝑤)‖ ≤𝜖), and remains there as long
as 𝜎(𝑞𝑤) = 1, which contradicts the assumption.
Combining this with the discussion following (66), one ensures that
there exists 𝜖1 > 0 such that 𝑑𝑤(𝑡) ≥𝑑𝑜(𝑡) > 𝜖1, ∀𝑡< 𝑡𝑤.
Proof of Item 2:
When 𝜉𝑤is inside the region (‖𝑞𝑤‖ ≤𝜖), one guarantees that 3
(71) is decreasing as long as 0 < 𝑑𝑤<
𝑘𝑤
𝑑
2
𝑘𝑤𝑝
and ‖𝑧‖ ≥
𝑑𝑤𝑘𝑤
𝑑‖▵⟂‖max
𝑘𝑤
𝑑
2−𝑑𝑤𝑘𝑤𝑝
. Now
since there exists a time ̄𝑡> 𝑡𝑤such that 𝑑𝑜(̄𝑡) = 0, one concludes that
𝑡lim exists and it is equal to ̄𝑡.
□
Appendix D. Supplementary data
Supplementary material related to this article can be found online
at https://doi.org/10.1016/j.conengprac.2021.104827.
References
Bertrand, S., Guénard, N., Hamel, T., Piet-Lahanier, H., & Eck, L. (2011). A hierarchical
controller for miniature {VTOL} UAVs: Design and stability analysis using singular
perturbation theory. Control Engineering Practice, 19(10), 1099–1108.
Burton, A., & Radford, J. (1978). Thinking in perspective: Critical essays in the study of
thought processes (vol. 646). Routledge.
Chaumette, F., Hutchinson, S., & Corke, P. (2016). Visual servoing. In Springer handbook
of robotics (pp. 841–866). Springer.
DeSouza, G. N., & Kak, A. C. (2002). Vision for mobile robot navigation: A survey.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 24(2), 237–267.
Falanga, D., Mueggler, E., Faessler, M., & Scaramuzza, D. (2017). Aggressive quadrotor
flight through narrow gaps with onboard sensing and computing using active vision.
In 2017 IEEE international conference on robotics and automation (pp. 5774–5781).
IEEE.
Floreano, D., & Wood, R. J. (2015). Science, technology and the future of small
autonomous drones. Nature, 521(7553), 460.
Guo, D., & Leang, K. K. (2020). Image-based estimation, planning, and control for high-
speed flying through multiple openings. International Journal of Robotics Research,
Article 0278364920921943.
Hamel, T., & Mahony, R. (2002). Visual servoing of an under-actuated dynamic
rigid-body system: An image-based approach. IEEE Transactions on Robotics and
Automation, 18(2), 187–198.
Hamel, T., Mahony, R., Lozano, R., & Ostrowski, J. (2002). Dynamic modelling
and configuration stabilization for an X4-flyer. IFAC Proceedings Volumes, 35(1),
217–222.
Hérissé, B., Hamel, T., Mahony, R., & Russotto, F.-X. (2012). Landing a VTOL unmanned
aerial vehicle on a moving platform using optical flow. IEEE Transactions on
Robotics, 28(1), 77–89.
Ho, H., De Wagter, C., Remes, B., & De Croon, G. (2018). Optical-flow based self-
supervised learning of obstacle appearance applied to mav landing. Robotics and
Autonomous Systems, 100, 78–94.
Hutchinson, S., Hager, G., & Corke, P. (1996). A tutorial on visual servo control.
Robotics and Automation, IEEE Transactions on, 12(5), 651–670.
Le Bras, F., Hamel, T., Mahony, R., Barat, C., & Thadasack, J. (2014). Approach
maneuvers for autonomous landing using visual servo control. IEEE Transactions
on Aerospace and Electronic Systems, 50(2), 1051–1065.
Loianno, G., Brunner, C., McGrath, G., & Kumar, V. (2017). Estimation, control, and
planning for aggressive flight with a small quadrotor with a single camera and
IMU. IEEE Robotics and Automation Letters, 2(2), 404–411.
Mahony, R., Corke, P., & Hamel, T. (2008). Dynamic image-based visual servo control
using centroid and optic flow features. Journal of Dynamic Systems, Measurement,
and Control, 130(1), Article 011005.
Mahony, R., & Hamel, T. (2005). Image-based visual servo control of aerial robotic
systems using linear image features. IEEE Transactions on Robotics, 21(2), 227–239.
Michael, N., Shen, S., Mohta, K., Mulgaonkar, Y., Kumar, V., Nagatani, K., Okada, Y.,
Kiribayashi, S., Otake, K., & Yoshida, K. (2012). Collaborative mapping of an
earthquake-damaged building via ground and aerial robots. Journal of Field Robotics,
29(5), 832–841.
Peters, D. A., & HaQuang, N. (1988). Dynamic inflow for practical applications.
Rosa, L., Hamel, T., Mahony, R., & Samson, C. (2014). Optical-flow based strategies
for landing VTOL UAVs in cluttered environments. IFAC Proceedings Volumes, 47(3),
3176–3183.
Serra, P., Cunha, R., Hamel, T., Cabecinhas, D., & Silvestre, C. (2016). Landing of a
quadrotor on a moving target using dynamic image-based visual servo control. IEEE
Transactions on Robotics, 32(6), 1524–1535.
Serra, P., Cunha, R., Hamel, T., Silvestre, C., & Le Bras, F. (2015). Nonlinear image-
based visual servo controller for the flare maneuver of fixed-wing aircraft using
optical flow. IEEE Transactions on Control Systems Technology, 23(2), 570–583.
Serres, J. R., & Ruffier, F. (2017). Optic flow-based collision-free strategies: From insects
to robots. Arthropod Structure & Development, 46(5), 703–717.
Tang, Z., Cunha, R., Hamel, T., & Silvestre, C. (2018). Aircraft landing using dynamic
two-dimensional image-based guidance control. IEEE Transactions on Aerospace and
Electronic Systems, 55(5), 2104–2117.
Tang, Z., Cunha, R., Hamel, T., & Silvestre, C. (2018). Going through a window and
landing a quadrotor using optical flow. In 2018 European control conference (pp.
2917–2922). IEEE.
Tang, Z., Li, L., Serra, P., Cabecinhas, D., Hamel, T., Cunha, R., & Silvestre, C. (2015).
Homing on a moving dock for a quadrotor vehicle. In TENCON 2015–2015 IEEE
region 10 conference (pp. 1–6). IEEE.
VICON (2014). Motion capture systems from vicon. http://www.vicon.com.
Zingg, S., Scaramuzza, D., Weiss, S., & Siegwart, R. (2010). MAV navigation through
indoor corridors using optical flow. In IEEE international conference on robotics and
automation (pp. 3361–3368).
14

