EE 227A: Convex Optimization and Applications
April 24, 2008
Lecture 24: Robust Optimization: Chance Constraints
Lecturer: Laurent El Ghaoui
Reading assignment: Chapter 2 of the book on Robust Optimization1
24.1
Stochastic Optimization
Example 1: Stochastic LP
min cTx
(24.1)
st : A(wi)x ≤b(wi), ∀i
where A(wi), b(wi) are realizations of the random variables A, b.
Example 2:
Stochastic portfolio optimization
max
u1,u2 E(y1u1 + y2u2 −1
2σ2
1u2
1 −1
2σ2
1u2
1 −c1|u1 −u0| −c2|u2 −u1|)
(24.2)
where y1, y2 are random variables. The problem is maximizing the expectation.
The stochastic recourse problem, however, becomes complicated. (Taking the expectation
of the objective function in (24.11), E(c2| ˆ
(u2 + U2y1) −u1|) usually is not in a simple form.)
24.2
Chance Optimization
Consider the LP
min cTx
(24.3)
st : aT
i x ≤bi
where ai ∼N( ˆσi, Σi). We can formulate a chance constrained LP
min cTx
(24.4)
st : Pr{aT
i x ≤bi} ≥1 −ǫ
If we take into account the correlation between ai’s, the problem is
min cTx
(24.5)
st : Pr{Ax ≤b} ≥1 −ǫ
1A. Ben Tal, L. El Ghaoui, A. Nemirovski, Robust Optimization.
24-1

EE 227A
Lecture 24 — April 24, 2008
Spring 2012
where A is the random variable.
If c is also a random variable, the problem can be formulated as
min t
(24.6)
st : Pr{t ≥cTx} ≥1 −ǫ, Pr{Ax ≤b} ≥1 −ǫ
24.3
Robust Optimization topics
In the following lectures, we will cover the following topics about Robust Optimization
• Form eﬃcient convex approximation
• Chance constraints relaxation (including the case with partially known distribution)
• Evaluate quality of relaxation
• Recourse programming
24.4
Chance Constraints
Chance constraints are a probabilistic way of handling probabilistic uncertainty. We would
like to convert chance constraints into robustness constraints, which are easier to deal with.
Consider the LP:
min cTx
(24.7)
s.t. Ax ≤b
(24.8)
Our basic problem with chance constraints is:
min cTx
(24.9)
s.t. Prob{Ax ≤b} ≥1 −ǫ
(24.10)
The technique we use to simplify the the chance constraints is to assume no correlation
between rows of the A-matrix, which allows us to write the chance constraint as:
Prob{aT
i x ≤bi} ≥1 −ǫ,
i = 1, 2, . . . m
(24.11)
This is a less general model, but it is often much easier to solve. The question now becomes:
how do we handle such scalar chance constraints?
We will consider a constraint of the form
Prob{aTx ≤b} ≥1 −ǫ,
where a, b are random, and say that x is ǫ-reliable if it satisﬁes the above constraint.
24-2

EE 227A
Lecture 24 — April 24, 2008
Spring 2012
24.5
Example: Gaussian Case
In this particular case, this scalar constraint is easy to handle. Suppose a ∼N(ˆa, Σ), then
for ﬁxed x:
ξ = aTx −b ∼N(ˆaTx −b, xTΣx)
(24.12)
Scaling with ξ = ˆξ + σu, where ˆξ = ˆaTx −b, σ2 = xTΣx, and u ∼N(0, 1), gives that:
Prob{ξ ≤0} = Prob
(
u ≤−
ˆξ
σ
)
=
Z −
ˆξ
σ
−∞
1
2π exp(−u2/2)du ≥1 −ǫ
(24.13)
Deﬁning Erf(α) =
R ∞
α
1
2π exp(−u2/2)du, we can rewrite the inequality above as Erf

−
ˆξ
σ

≤
ǫ. Note that Erf(·) is a decreasing function and that Erf(0) = 1/2. Further simplifying our
expression, we get that:
Erf
 
−
ˆξ
σ
!
≤ǫ
⇔−
ˆξ
σ ≥Erﬁnv(ǫ) =: ψ(ǫ)
(24.14)
⇔ˆξ + ψ(ǫ)σ ≤0
(24.15)
⇔ˆaTx + ψ(ǫ)
√
xTΣx ≤b
(24.16)
Note that ψ(ǫ) > 0 if ǫ < 1/2, and ψ(ǫ) is like a coeﬃcient that can be precomputed.
Consequently, the inequality above is a SOCP constraint. Making ǫ smaller causes ψ(ǫ)
to become larger, which makes the constraint harder to satisfy.
Also, we can interpret
ψ(ǫ)
√
xTΣx as a risk term and ˆaTx as an average term.
Besides this case, there are very few cases where we can do symbolic calculations. In
practice, assuming Gaussian uncertainty is ﬁne for most cases, even when the randomness is
not Gaussian. Also, notice that the SOCP constraint derived above is the same as the one
obtained using a robust optimization approach with an uncertainty model. Indeed, suppose
that all we know is that:
a ∈U := {ˆa + Σ1/2u |
∥u∥2 ≤ψ(ǫ)}
(24.17)
Thenn, we have that aTx ≤b,
∀a ∈U if and only if ˆaTx + ψ(ǫ)
√
xTΣx. This uncertainty
model seems very restrictive, but it can be interpreted as a probabilistic approach; it is fairly
reasonable.
24-3

EE 227A
Lecture 24 — April 24, 2008
Spring 2012
24.6
Case with Independent, Bounded Random Un-
certainties
Suppose that we have a vector uncertainty ξ ∈RL, which is a random variable, such that
the coeﬃcients ξl’s are mutually independent, and E(ξl) = 0, and |ξl| ≤1, ∀l. The last
constraint is a deterministic bound, which tells us about the support of the random variable
ξ. We consider the chance constraint
Prob{a(ξ)Tx ≤b(ξ)} ≥1 −ǫ
(24.18)
where a, b depend on the random variable ξ in an aﬃne fashion, precisely a(ξ) = a0 + Σξiai
and b(ξ) = b0 + Σξibi, where vector ai and scalars bi, i = 0, . . . , L, are given.
In the case that ξi = 1 with probability 1/2 and ξi = −1 with probability 1/2, computing
the probability in (24.29) for ﬁxed x can be shown to be NP-hard, and so it is quite reasonable
to use an approximation, precisely, a suﬃcient condition for the chance constraint to hold.
Our chance constraint can be rewritten as
Prob{ξTz > b0 −aT
o x} ≤ǫ.
(24.19)
We will show later the following lemma.
Let z ∈RL be a deterministic vector, and
ξ1, . . . , ξL be independent random variables with zero mean taking values in [−1, 1]. Then
Prob{ξTz > Ω∥z∥2} ≤exp(−Ω2/2).
Applying the lemma with zi = aT
i x −bi, we see that the constraint
b0 −aT
o x ≥Ω
v
u
u
t
L
X
i=1
(aT
i x −bi)2
(24.20)
implies that
Prob{ξTz > b0 −aT
o x} ≤Prob{ξTz > Ω∥z∥2} ≤ǫ,
provided Ω≥
p
2 log(1/ǫ) Thus, the SOC condition (24.31) is a suﬃcient for our chance
constraint (24.30) to hold.
Again, the conic constraint (24.31) can be reinterpreted as a robust condition, against
ellipsoidal uncertainties:
aT(ξ)x ≤b(ξ),
∀ξ ∈U
(24.21)
U = {ξ |
∥ξ∥2 ≤Ω}
(24.22)
Deﬁne BΩ= {ξ|
∥ξ∥2 ≤Ω} and Box1 = {ξ|
∥ξ∥∞≤1}. We can impose robustness
against a Box uncertainty:
aT(ξ)x ≤b(ξ), ∀ξ ∈Box1 ⇔b0 ≥aT
0 x + ΣL
i=1
aT
i x −bi

24-4

EE 227A
Lecture 24 — April 24, 2008
Spring 2012
This condition gives us 100% reliability, while a ball uncertainty, which corresponds to the
SOCP condition (24.31) gives a reliability of ǫ. If Ω≥7.44, then exp(−Ω2/2) ≤10−12 = ǫ,
so we get huge reliability for small Ω. In high dimensions, the ball is much smaller in volume
than the box, precisely:
volBΩ
volBox1
=
 
Ω
p
eπ/2
√m
!m
→m→∞0
(24.23)
Hence, in high dimensions, it is better to use BΩ: although it does not give 100% reliability,
it does provide a highly reliable solution. A solution robust against a set that is much smaller
in volume than the box is still highly reliable over the entire box.
We can even arrange that the set over which we enforce a robustness condition is actually
entirely included in the original box (this is not the case with the previous setting). Indeed,
consider ξ ∈U := Box1∩BΩ. It turns out that an equivalent representation of the robustness
constraint a(ξ)Tx ≤b(ξ), ∀ξ ∈U is: there exist z, w such that
∥z∥1 + Ω
q
Σw2
i ≤b0 −aT
0 , zi + wi = bi −aT
i x, ∀i.
(24.24)
Now this robustness constraint still guarantees a high reliability: If x is feasible for the above
robustness condition, and infeasible for the constraint a(ξ)Tx ≤b(ξ) for some realization ξ,
then
Σξi(aT
i x −bi) > b0 −aT
0 x
⇒−Σziξi −Σwiξi > b0 −aT
0 x
⇒Σ|zi| −Σwiξi > b0 −aT
0 x
⇒−Σwiξi > Ω
p
Σw2
i
Consequently, we have that Prob{a(ξ)Tx > b(ξ)} ≤Prob{−wTξ > Ω∥w∥2} ≤exp(−Ω2/2).
When m > Ω2, ξ takes only ±1 values, the set Box1 ∩BΩdoes not even contain a single
realization of ξ. Still, we can ﬁnd a highly reliable solution by enforcing the robustness
constraint (24.35).
This illustrates that a solution that is robust with respect to an uncertainty set that not
only is smaller, but is actually contained in the original box, is actually still highly reliable.
24.7
Example: Investment Problem
We have several assets including cash (or, risk-free asset) and stocks, and denote by r the
n + 1 vector of returns over the investment period, with r0 being the return of cash (say,
r0 = 1.05 for a 5% CD). How should we distribute some sum (say, one dollar) over these
assets, where we take the returns to be a random variable?
We invest amounts y0, y1, . . . , yn, such that Σyi = 1 and yi ≥0. The total return of the
portfolio is given by r0y0 + Σriyi ≥t. We make the further assumption that the returns are
random in the form ri = µi + σiξi, where µi is the mean and σi is the standard deviation.
24-5

EE 227A
Lecture 24 — April 24, 2008
Spring 2012
We have two models, the ﬁrst is the robust case where ξ ∈Box1 and the second is the
ball-box case where ξ ∈Box1 ∩BΩ. Take the tolerance of Ω= 3.255 or ǫ = 0.005 and use
µi = 1.05 + 0.3(200 −i)/199 and σi = 0.05 + 0.6(200 −i)/199. Using the ﬁrst model, we get
y0 = 1 and yi = 0, which means that a purely robust approach leads to the very conservative
investment of putting everything in a risk-free asset. The second model gives a worst-case
return of 10.1% with a 0.5% chance of not getting this return. We thus observe that chance
constraints allow to make the robustness condition much less conservative, at the expense of
a very slight increase in risk.
24-6

